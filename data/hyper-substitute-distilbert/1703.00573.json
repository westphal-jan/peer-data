{"id": "1703.00573", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "abstract": "phylogenetic survey shows one noticing several open theoretical issues related to generative adversarial procedures. implicit consensus never provided for what it means for the training to generalize, some some is shown that generalization is not guaranteed knowing many constant distances between distributions such were jensen - shannon _ wasserstein. we introduce of new topology called neural net distance for algebraic precision does vary. we also show assumes infinitely effectively pure equilibrium surrounding the 2 - player group exists for a good training goal ( inference ). showing such similar result totally eliminated an open option ( for any symmetric field ).", "histories": [["v1", "Thu, 2 Mar 2017 01:14:03 GMT  (677kb,D)", "http://arxiv.org/abs/1703.00573v1", null], ["v2", "Fri, 3 Mar 2017 16:19:00 GMT  (677kb,D)", "http://arxiv.org/abs/1703.00573v2", null], ["v3", "Tue, 4 Apr 2017 00:41:13 GMT  (694kb,D)", "http://arxiv.org/abs/1703.00573v3", null], ["v4", "Sat, 17 Jun 2017 22:04:07 GMT  (2726kb,D)", "http://arxiv.org/abs/1703.00573v4", "To appear in ICML 2017; Major update of exposition in Section 1-3"], ["v5", "Tue, 1 Aug 2017 19:51:56 GMT  (2727kb,D)", "http://arxiv.org/abs/1703.00573v5", "This is an updated version of an ICML'17 paper with the same title. The main difference is that in the ICML'17 version the pure equilibrium result was only proved for Wasserstein GAN. In the current version the result applies to most reasonable training objectives. In particular, Theorem 4.3 now applies to both original GAN and Wasserstein GAN"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE stat.ML", "authors": ["sanjeev arora", "rong ge 0001", "yingyu liang", "tengyu ma", "yi zhang"], "accepted": true, "id": "1703.00573"}, "pdf": {"name": "1703.00573.pdf", "metadata": {"source": "CRF", "title": "Generalization and Equilibrium in Generative Adversarial Nets (GANs)", "authors": ["Sanjeev Arora", "Rong Ge", "Yingyu Liang", "Tengyu Ma", "Yi Zhang"], "emails": ["arora@cs.princeton.edu", "rongge@cs.duke.edu", "yingyul@cs.princeton.edu", "tengyu@cs.princeton.edu", "yz7@cs.princeton.edu"], "sections": [{"heading": null, "text": "Finally, the above theoretical ideas lead us to propose a new training protocol, mix+gan, which can be combined with any existing method. We present experiments showing that it stabilizes and improves some existing methods."}, {"heading": "1 Introduction", "text": "Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016]. Various novel architectures and training objectives were introduced to address perceived shortcomings of the original idea, leading to more stable training and more realistic generative models in practice. But many basic issues remain unresolved, as we now discuss.\nLet\u2019s recall that the basic scenario is that we wish to train a generator deep net whose input is a standard Gaussian, and whose output is a sample from some distribution D on <d. We wish the samples from D to closely resemble those drawn from some real-life distribution Dreal (which could be, say, real-life images represented using raw pixels). Towards this end, a discriminator deep net is trained alongside the generator net, and it is trained to maximise its ability to distinguish between samples from Dreal and D. So long as the discriminator is successful at this task with nonzero probability, its success can be used to generate a feedback (using backpropagation) to the generator, thus improving its distribution D. This basic iterative framework has been tried with many training objectives; see Section 2. Recently Arjovsky et al. [2017] proposed another variant called Wasserstein GAN that appears to lead to more stable training. The unresolved issues stem\n\u2217Princeton University, Computer Science Department, email: arora@cs.princeton.edu \u2020Duke University, Computer Science Department, email: rongge@cs.duke.edu \u2021Princeton University, Computer Science Department, email: yingyul@cs.princeton.edu \u00a7Princeton University, Computer Science Department, email: tengyu@cs.princeton.edu \u00b6Princeton University, Computer Science Department, email: yz7@cs.princeton.edu\nar X\niv :1\n70 3.\n00 57\n3v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\nfrom the fact that distribution Dreal is complicated. The number of peaks and valleys in it may be large and possibly even exponential in d. (Recall the curse of dimensionality: in d dimensions there are exp(d) directions whose pairwise angle exceeds say \u03c0/3, and each could be the site of a peak.) In practice, the generator often \u201cwins\u201d the game by the end, meaning that the discriminator can do no better than random guessing when deciding whether or not a particular sample came from D or Dreal.\nBut it is unclear what to conclude from the generator\u2019s win: is D close to Dreal in some metric? The fear of course is overfitting, intuitively described as the generator having simply memorized the presented samples (or parts theoreof). A glance at Figure 1 reinforces such fears. The number of samples from Dreal (and from D for that matter) used in the entire training is moderately large, but still a lot fewer than the potential number of peaks and valleys in Dreal. Thus the samples from Dreal may be a poor substitute for the full distribution, and there is no a priori reason that using them to train a new distribution will lead to a result D that meaningfully approximates Dreal in unsampled peaks and valleys. Understanding generalization is open.\nFinally, we don\u2019t know if an equilibrium exists. Just as a zero gradient is a necessary condition for standard optimization to halt, the corresponding necessary condition in a two-player game is an equilibrium. Conceivably absence of an equilibrium could cause some of the instability observed in training. Arjovsky et al. [2017] suggest that empirically, using their Wasserstein objective reduces instability, but we still lack proof of existence of an equilibrium. Game theory doesn\u2019t help because we need a so-called pure equilibrium, and simple counter-examples such as rock/paper/scissors show that it doesn\u2019t exist in general1."}, {"heading": "1.1 This paper", "text": "We formally define generalization in Section 3 and show that for previously studied notions of distance between distributions, generalization is not guaranteed (Theorem 3.1). The reason is that these formalisms involve an ideal discriminator, which can have exponentially high VC dimension (or any other suitable measure of complexity). Therefore we introduce a new metric on distributions, the neural net distance, for which we show that generalization does happen.\nTo explore the existence of equilibria we turn in Section 5 to infinite mixtures of generator deep nets. These are clearly vastly more expressive than a single generator net: e.g., a standard result in bayesian nonparametrics says that every probability density is closely approximable by an infinite mixture of Gaussians [Ghosh et al., 2003]. Thus unsurprisingly, an infinite mixture should win the\n1Such counterexamples are easily turned into toy GAN scenarios with generator and discriminator having finite capacity, and the game lacks a pure equilibrium.\ngame. We then prove rigorously that even a finite mixture of fairly reasonable size can closely approximate the performance of the infinite mixture (Theorem 5.2).\nThis insight also allows us to show for a natural GAN setting with Wasserstein objective there exists an approximate equilibrium that is pure. (Roughly speaking, an approximate equilibrium is one in which the objective changes only a little whether player 1 goes first, or player 2 does.) This is the first such proof we know of.\nThe existence proof for an approximate equilibrium unfortunately involves a quadratic blowup in the \u201csize\u201d of the generator (which is still better than the naive exponential blowup one might expect). Improving this is left for future theoretical work. But we introduce a heuristic approximation to the mixture idea to introduce a new framework for training that we call mix+gan. It can be added on top of any existing GAN training procedure, including those that use divergence objectives. Experiments (reported in Section 7) show that for several previous techniques, mix+gan stabilizes the training, and in some cases improves the performance."}, {"heading": "2 Preliminaries", "text": "Notations: Throughout the paper we use d for the dimension of samples. Both generators and discriminators will be neural networks with n parameters. In Section 3 we use m for number of samples. Generators and Discriminators: For GANs, the generator and discriminators are multi-layer neural networks. We abstract the set of possible networks as a class of functions: {Fu, u \u2208 U}, where u \u2208 U is a high-dimensional vector that specifies the trainable parameters: weights, biases etc. (This assumes that hyperparameters \u2014number of layers, number of nodes, convolution structure etc.\u2014 have been pre-fixed, as is normally the case.) Without loss of generality assume U is a subset of the d dimensional unit ball2.\nThroughout the paper, {Gu, u \u2208 U} denotes the class of generators and {Dv, v \u2208 V} denotes the class of discriminators. Thus Gu is a function mapping a simple random input h \u223c Dh (e.g., a Gaussian vector) to a sample x. Each Dv is a function that maps a sample x to a value in [0, 1] \u2013 usually interpreted as the probability that the sample comes from the real distribution Dreal. In most neural network architectures, if the parameters (weights, biases) change by a small amount, this makes only a small difference in the output. We capture this phenomena by assuming Gu and Dv are L-Lipschitz with respect to their parameters. Thus for all u, u\n\u2032 \u2208 U and any input h, we have \u2016Gu(h) \u2212 Gu\u2032(h)\u2016 \u2264 L\u2016u \u2212 u\u2032\u2016. Similarly for all v, v\u2032 \u2208 V and any sample x, we have |Dv(x) \u2212 Dv\u2032(x)| \u2264 L\u2016v \u2212 v\u2032\u2016. Notice, this is distinct from the assumption (which we will also sometimes make) that functions Gu, Dv are Lipschitz: that focuses on the change in function value when we change x, while keeping u, v fixed. GAN objective. The standard GAN training [Goodfellow et al., 2014] consists of training u, v so as to optimize an objective such as:\nmin u\u2208U max v\u2208V E x\u223cDreal [logDv(x)] + E h\u223cDh [log(1\u2212Dv(Gu(h)))]. (1)\nIntuitively, this says that the discriminator Dv should give high values Dv(x) to the real samples and low values Dv(G(h)) for the generator\u2019s outputs. The log function was suggested because of a nice information-theoretic interpretation described below, but in practice it can cause problems\n2If the parameters can have norm at most r we can always consider Fr\u00b7u and then u can have norm at most 1.\nsince log x \u2192 \u2212\u221e as x \u2192 0. But the objective still makes intuitive sense if we replace log by any monotone function f : [0, 1]\u2192 R, which yields the objective:\nmin u\u2208U max v\u2208V E x\u223cDreal [f(Dv(x))] + E h\u223cDh [f(1\u2212Dv(Gu(h)))]. (2)\nWe call function f the measuring function. It should be concave so that when Dreal and Gu(Dh) are the same distribution, the best strategy for the discriminator is just to output 1/2 and the optimal value is 2f(1/2). In later proofs, we will require f to be bounded and Lipschitz. Indeed, in practice training often uses f(x) = log(\u03b4 + (1 \u2212 \u03b4)x) (which takes values in [log \u03b4, 0] and is 1/\u03b4-Lipschitz) and the recently proposed Wasserstein GAN[Arjovsky et al., 2017] objective uses f(x) = x. Standard Interpretation via Distance Metric. From the complicated min-max objective it can be difficult to tell in what sense the final distribution Gu(Dh) is close to Dreal. Thus researchers have relied upon a heuristic argument that assumes that the discriminator is chosen optimally within some class, which allows interpreting the max as defining a distance metric between Dreal and Gu(h). Different measuring functions lead to some popular distance metrics this way.\nFor the original objective function, if the optimal discriminator is allowed to be any function all (i.e., not just one computable by a finite nueral net) it can be checked that the optimal choice is D(x) = Preal(x)Preal(x)+PG(x) . Here Preal(x) is the probability (or density) of x in the real distribution, and PG(x) is the probability (or density) of x in the distribution generated by generator G. Using this discriminator, up to linear transformation the maximum value achieved by discriminator isequivalent to the Jensen-Shannon (JS) divergence between Dreal and Gu(h). For two distributions \u00b5 and \u03bd, the JS divergence is defined by\ndJS(\u00b5, \u03bd) = 1 2 (KL(\u00b5\u2016\u00b5+ \u03bd 2 ) +KL(\u03bd\u2016\u00b5+ \u03bd 2 )).\nOther measuring functions f leads to different distance metrics. Notably, when f(x) = x, and the discriminator is chosen among all 1-Lipschitz functions (which is more realistic than all functions) the max part of the objective is equivalent to Wasserstein distance (up to additive constant)\ndW (\u00b5, \u03bd) = sup f :1 Lipschitz E x\u223c\u00b5 [f(x)]\u2212 E x\u223c\u03bd [f(x)]. (3)"}, {"heading": "3 Distance Metric and Generalization", "text": "This interpretation of the GAN objective in terms of minimizing metrics such as JS divergence or Wasserstein distance is standard but it relies on two crucial assumptions: (i) The network has enough capacity to approximate the ideal discriminator and (ii) We have enough samples to estimate the expectations in Objectives (1) or (2). Neither assumption is satisfied in practice, and we now show this affects generalization ability.\nThe training actually uses only finite set of samples from Dreal and Gu(Dh): let h1, h2, ..., hm \u223c Dh be the inputs of the generator, and let x1, x2, ..., xm \u223c Dreal be samples from the real distribution3. We use D\u0302real and Gu(D\u0302h) to denote the empirical distribution that assigns probability 1/m\n3Of course, the training allows using different number of samples from the two distributions, but for notational ease use m for both.\nto each sample. Thus the empirical objective function actually being optimized is\nmin u\u2208U max v\u2208V\n1\nm m\u2211 i=1 [f(Dv(xi))] + 1 m m\u2211 i=1 [f(1\u2212Dv(Gu(hi)))]. (4)\nThus even for the perfect discriminator, the training can only measure its effectiveness on distinguishing between these two empirical distributions. When f(x) = log x we can get dJS(D\u0302real, Gu(D\u0302h)); when f(x) = x and discriminator is 1-Lipschitz, we get dW (D\u0302real, Gu(D\u0302h)).\nThe training can be said to generalize if these empirical distances well-approximate the distance (in the same metric) between the full distributions. (Informally, lack of generalization is expressed as the generator having \u201cmemorized\u201d the examples.)\nHowever we show that this may not be true even for very simple distributions unless if the sample has exponential size. The next theorem shows that the distance between the empirical distributions can be close to the maximum possible (namely, log 2 for dJS and \u221a 2 for Wasserstein) even if the samples are drawn from the same distribution. For JS divergence, we also show even if we add noise to both empirical distributions, the distance can still be very large (even samples are noised as part of training.)\nTheorem 3.1. Let \u00b5, \u03bd be uniform Gaussian distributions N (0, 1dI). Suppose \u00b5\u0302, \u03bd\u0302 are empirical versions of \u00b5, \u03bd with m samples. If logm d, then with high probability\ndJS(\u00b5, \u03bd) = 0,dJS(\u00b5\u0302, \u03bd\u0302) = log 2. dW (\u00b5, \u03bd) = 0,dW (\u00b5\u0302, \u03bd\u0302) \u2265 1.1.\nFurther, let \u00b5\u0303, \u03bd\u0303 be the convolution of \u00b5\u0302, \u03bd\u0302 with a Gaussian distribution N(0, \u03c3 2\nd I), as long as \u03c3 < c\u221a\nlogm for small enough constant c, we have with high probability\ndJS(\u00b5\u0303, \u03bd\u0303) > log 2\u2212 1/m.\nThe proof idea is surprisingly simple \u2013 with m samples, with very high probability even the closest pair between a sample from \u00b5\u0302 and a sample \u03bd\u0302 will be at least 1.1. The JS-divergence is large because the supports are disjoint, the Wasserstein distance is large because no matter how we \u201ccouple\u201d samples from \u00b5\u0302 and \u03bd\u0302 they need to be transported for a distance at least 1.1.\nTheorem 3.1 shows that even if the generator happens to find the real distribution, if the discriminator is powerful enough to compute either the JS divergence or Wasserstein distance, then distance between the empirical distributions can still be large and the generator has no idea that it has succeeded (and may move away).\nIn fact, for Wasserstein distance, if the generator simply generates only the 0 point (obviously not the right distribution), the Wasserstein distance for the empirical distributions is going to be 1. This is actually smaller compared to the case when generator generates the real distribution! In fact, when the discriminator is so powerful the only way the generator can \u201cwin\u201d is by memorizing the input examples, which leads to overfitting.\nTherefore, the GAN objective should not be interpreted as minimizing these two distances between the distributions, and it is non-rigorous to use that intuition to reason about the design of the procedure."}, {"heading": "3.1 New distance metric with generalization bounds", "text": "Can we still think of GANs as minimizing some distance metric between the real distribution Dreal and Gu(Dh)? We now define a new distance metric that captures the actual optimization being performed, and which does lead to the generalization desired.\nDefinition 1. For two distributions \u00b5 and \u03bd the neural network divergence with respect to a class of discriminators {Dv : v \u2208 V} and measuring function f is defined as:\ndNN (\u00b5\u2016\u03bd) = max v\u2208V E x\u223c\u00b5 [f(Dv(x))] + E x\u223c\u03bd [f(Dv(1\u2212 x))].\nIn particular when f(x) = x we call this the neural network distance4 :\ndNN (\u00b5, \u03bd) = max v\u2208V \u2223\u2223\u2223\u2223 Ex\u223c\u00b5[Dv(x)]\u2212 Ex\u223c\u03bd[Dv(x)] \u2223\u2223\u2223\u2223 .\nNote that if the class of discriminators Dv is the set of all 1-Lipschitz functions, then the definition is exactly the same as Wasserstein distance (3). (But not all 1-Lipschitz functions may be computable by small neural nets.)\nIntuitively, the definition and the next theorem addresses the fear that the generator has overfitted to the idiosyncracies of the specific adversarial discriminator that it trained with. The theorem says that if the generator achieves a certain payoff against the optimum discriminator on the training samples, then it can achieve a similar payoff against all other discriminators in the same class of deep nets.\nIn the theorem, {Dv : v \u2208 V} is any class of discriminators that is L-Lipschitz with respect to v, measuring function f takes value in [\u2212\u2206,\u2206] and is Lf -Lipschitz. The theorem shows that the distance between the empirical distributions closely tracks the distance between the true distributions, given (relatively) modest number of samples. As usual, n is the number of parameters of the neural net.\nTheorem 3.2. Let \u00b5, \u03bd be two distributions and \u00b5\u0302, \u03bd\u0302 be empirical versions with m samples each. There is a some fixed constant C such that when m \u2265 Cn\u2206 2 log(LLfd/ )\n2 , then we have with probability\nclose to 1: |dNN (\u00b5\u0302\u2016\u03bd\u0302)\u2212 dNN (\u00b5\u2016\u03bd)| \u2264 .\nThe proof (appearing in the supplementary) uses standard concentration inequalities and a standard -net argument: there aren\u2019t too many distinct discriminators, and thus given enough samples the expectation over the empirical distribution converges to the expectation over the true distribution for all discriminators.\nTheorem 3.2 shows that the neural network divergence (and neural network distance) has a much better generalization bound than Jensen-Shannon divergence or Wasserstein distance. If the GAN successfully minimized the neural network divergence over empirical samples, then we know the neural network divergence between the distributions Dreal and Gu(Dh) is also small.\n4Technically this is a pseudometric, because dNN (\u00b5, \u03bd) = 0 does not imply \u00b5 = \u03bd. On the other hand, this metric is symmetric and satisfies triangle inequality. Symmetry is easy to see, triangle inequality follows because for three distributions \u00b51, \u00b52, \u00b53, if v is the optimal discriminator for \u00b51, \u00b53 then dNN (\u00b51, \u00b52) \u2265 |Ex\u223c\u00b51 [Dv(x)]\u2212 Ex\u223c\u00b52 [Dv(x)]| and similarly dNN (\u00b52, \u00b53) \u2265 |Ex\u223c\u00b52 [Dv(x)]\u2212 Ex\u223c\u00b53 [Dv(x)]|. Hence dNN (\u00b51, \u00b53) \u2264 dNN (\u00b51, \u00b52) + dNN (\u00b52, \u00b53).\nTo be more rigorous one would need to argue that this generalization continues to hold at every iteration of the training. While we can resample from Dh for every generator Gu, it is infeasible to get more samples from Dreal in every iteration. The following corollary shows the generalization bound holds for every iteration as long as the number of iterations is not too large.\nCorollary 3.1. Let u1, u2, ..., ut (log t d) be t sets of parameters for the generator. For each ui, let D\u0302h[t] be a fresh set of m samples from the distribution Gui(Dh). Let D\u0302real be m samples from the real distribution. With high probability when m \u2265 n\u2206 2 log(LLfn/ )\n2 we have for all t\n|dNN (Dui(D\u0302h[t])\u2016D\u0302real)\u2212 dNN (Dui(Dh)\u2016Dreal)| \u2264 .\nThe key observation here is that the objective is separated into two parts and the generator is not directly related to Dreal. So even though we cannot resample real samples, the generalization bound still holds. Detailed proof appear in supplementary material."}, {"heading": "4 Expressive power and existence of equilibrium", "text": "Section 3 clarified the notion of generalization for GANs: namely, neural-net divergence between the generated distribution D and Dreal on the empirical samples closely tracks the divergence on the full distribution (i.e., unseen samples). But this doesn\u2019t explain why in practice the generator usually \u201cwins\u201dso that the discriminator is unable to do much better than random guessing at the end. In other words, was it sheer luck that so many real-life distributions Dreal turned out to be close in neural-net distance to a distribution produced by a fairly compact neural net? This section suggests no luck may be needed.\nThe explanation starts with a thought experiment. Imagine allowing a much more powerful generator, namely, an infinite mixture of deep nets, each of size n. So long as the deep net class is capable of generating simple gaussians, such mixtures are quite powerful, since a classical result says that an infinite mixtures of simple gaussians can closely approximate Dreal. Thus an infinite mixture of deep net generators will \u201cwin\u201d the GAN game, not only against a discriminator that is a small deep net but also against more powerful discriminators (e.g., any Lipschitz function).\nThe next stage in the thought experiment is to imagine a much less powerful generator, which is a mix of only a few deep nets, not infinitely many. Simple counterexamples show that now the distribution D will not closely approximate arbitrary Dreal with respect to natural metrics like `p. Nevertheless, could the generator still win the GAN game against a deep net of bounded capacity (i.e., the deep net is unable to distinguish D and Dreal)? We show it can.\ninformal theorem: If the discriminator is a deep net with n parameters, then a mixture of O\u0303(n log(n/ )/ 2) generator nets can produce a distribution D that the discriminator will be unable to distinguish from Dreal with probability more than . (Here O\u0303(\u00b7) notation hides some nuisance factors.)\nThis informal theorem (formalized below) is also a component of our result below about the existence of an approximate pure equilibrium. With current technique this existence result seems sensitive to the measuring function f , and works for f(x) = x (i.e., Wasserstein GAN). For other f we only show existence of mixed equilibria with small mixtures."}, {"heading": "4.1 General f : Mixed Equilibrium", "text": "For general measure function f we can only show the existence of a mixed equilibrium, where we allow the discriminator and generator to be finite mixtures of deep nets.\nFor a class of generators {Gu, u \u2208 U} and a class of discriminators {Gv, v \u2208 V}, we can define the payoff F (u, v) of the game between generator and discriminator\nF (u, v) = E x\u223cDreal [f(Dv(x))] + E h\u223cDh [f(1\u2212Dv(Gu(h)))]. (5)\nOf course as we discussed in previous section, in practice these expectations should be with respect to the empirical distributions. Our discussions in this section does not depend on the distributions Dreal and Dh, so we define F (u, v) this way for simplicity.\nThe well-known min-max theorem[v. Neumann, 1928] in game theory shows if both players are allowed to play mixed strategies then the game has a min-max solution. A mixed strategy for the generator is just a distribution Du supported on U , and one for discriminator is a distribution Dv supported on V.\nTheorem 4.1 (vonNeumann). There exists a value V , and a pair of distributions (Du,Dv) such that\n\u2200v, E u\u223cDu [F (u, v)] \u2264 V and \u2200u, E v\u223cDv [F (u, v)] \u2265 V.\nNote that this equilibrium involves both parties announcing their strategies Du,Dv at the start, such that neither will have any incentive to change their strategy after studying the opponent\u2019s strategy. The payoff is generated by the generator first sample u \u223c Du, h \u223c Dh, and then generate an example x = Gu(h). Therefore, the mixed generator is just a linear mixture of generators. The discriminator will first sample v \u223c Dv, and then output Dv(x). Note that in general this is very different from a discriminator D that outputs Ev\u223cDv [Dv(x)], because the measuring function f is in general nonlinear. In particular, the correct payoff function for a mixture of discriminator is:\nE v\u223cDv [F (u, v)] = E x\u223cDreal v\u223cDv [f(Dv(x))] + E h\u223cDh v\u223cDv [f(1\u2212Dv(Gu(h)))].\nOf course, this equilibrium involving an infinite mixture makes little sense in practice. We show that (as is folklore in game theory[Lipton et al., 2003]) that we can approximate this min-max solution with mixture of finitely many generators and discriminators. More precisely we define -approximate equilibrium\nDefinition 2. A pair of mixed strategy (Du,Dv) is an -approximate equilibrium, if for some value V\n\u2200v \u2208 V, E u\u2208U [F (u, v)] \u2264 V + ; \u2200u \u2208 U , E v\u2208V [F (u, v)] \u2265 V \u2212 .\nIf the strategies Du,Dv are pure strategies, then this pair is called an -approximate pure equilibrium.\nSuppose f is Lf -Lipschitz and bounded in [\u2212\u2206,\u2206], the generator and discriminators are LLipschitz with respect to the parameters and L\u2032-Lipschitz with respect to inputs, in this setting we can formalize the above Informal Theorem as follows:\nTheorem 4.2. In the settings above, there is a large enough constant C > 0 such that for any\n, there exists T = C\u22062n log(LL\u2032Lf \u00b7n/ )\n2 generators Gu1 , . . . GuT and T discriminators Dv1 , . . . DvT ,\nlet Du be a uniform distribution on ui and Dv be a uniform distribution on vi, then (Du, Dv) is an -approximate equilibrium. Furthermore, in this equilibrium the generator \u201cwins,\u201dmeaning discriminators cannot do better than random guessing.\nThe proof uses a standard probabilistic argument and epsilon net argument to show that if we sample T generators and discriminators from infinite mixture, they form an approximate equilibrium with high probability. For the second part, we use the fact that every distribution can be approximated by infinite mixture of Gaussians, so the generator must be able to approximate the real distribution Dreal and win. Therefore indeed a mixture of O\u0303(n) generators can achieve an -approximate equilibrium.\n4.1.1 f(x) = x: Pure Equilibrium\nNow we consider the special case of Wasserstein-GAN, which is equivalent to setting f(x) = x in Equation (2). In this case, it is possible to augment the network structure, and achieve an approximate pure equilibrium for the GAN game for networks of size O\u0303(n2). This should be interpreted as: if deep nets of size n are capable of generating a Gaussian, then the W-GAN game for neural networks of size O\u0303(n2) has an approximate equilibrium in which the generator wins. (The theorem is stated for RELU gates but also holds for standard activations such as sigmoid.)\nTheorem 4.3. Suppose the generator and discriminator are both p-layer neural networks (p \u2265 2) with n parameters, and the last layer uses ReLU activation function. In the setting of Theorem 5.2 there exists p + 1-layer neural networks of generators G and discriminator D with\nO ( \u22062n2 log(LL\u2032Lf \u00b7n/ ) 2 ) parameters, such that there exists an -approximate pure equilibrium. Furthermore, if the generator is capable of generating a Gaussian then the value V = 1.\nTo prove this theorem, we consider the mixture of generators and discriminators as in Theorem 5.2, and show how to fold the mixture into a larger p+ 1-layer neural network. We sketch the idea; details are in the supplementary.\nMixture of Discriminators Given a mixture of T discriminators Dv1 , ..., DvT , we can just define D(x) = 1T \u2211T i=1Dvi(x). Because f(x) = x, we know for any generator Gu\nE x\u223cDreal [D(x)] = E x\u223cDreal,i\u2208[T ] [Dvi(x)]\nE h\u223cDh [(1\u2212D(Gu(h))] = E h\u223cDh,i\u2208[T ] [(1\u2212Dvi(Gu(h))].\nTherefore, the payoff for D is exactly the same as the payoff for the mixture of discriminators. Also, the discriminator D is easily implemented as a network T times as large as the original network by adding a top layer that averages the output of the T generators Dvi(x).\nMixture of Generators For mixture of generators, we construct a single neural network that approximately generates the mixture distribution using the gaussian input it has. To do that, we can pass the input h through all the generators Gu1 , Gu2 , ..., GuT . We then show how to implement a \u201cmulti-way selector\u201d that will select a uniformly random output from Gui(h) (i \u2208 [T ]). The\nselector involves a simple 2-layer network that selects a number i from 1 to T with the appropriate probability and \u201cdisables\u201dall the neural nets except the ith one by forwarding an appropriate large negative input.\nTheorem 5.3 suggests that the objective of Wasserstein GAN may have approximate pure equilibrium for certain architectures of neural networks, which lends credence that Wasserstein GAN may be more robust.\nRemark: In practice, GANs use highly structured deep nets, such as convolutional nets. Our current proof of existence of pure equilibrium requires introducing less structured elements in the net, namely, the multiway selectors that implement the mixture within a single net. It is left for future work whether pure equilibria exist for the original structured architectures. In the meantime, in practice we recommend using, even for W-GAN, a mixture of structured nets for GAN training, and it seems to help in our experiments reported below."}, {"heading": "5 Expressive power and existence of equilibrium", "text": "Section 3 clarified the notion of generalization for GANs: namely, neural-net divergence between the generated distribution D and Dreal on the empirical samples closely tracks the divergence on the full distribution (i.e., unseen samples). But this doesn\u2019t explain why in practice the generator usually \u201cwins\u201dso that the discriminator is unable to do much better than random guessing at the end. In other words, was it sheer luck that so many real-life distributions Dreal turned out to be close in neural-net distance to a distribution produced by a fairly compact neural net? This section suggests no luck may be needed.\nThe explanation starts with a thought experiment. Imagine allowing a much more powerful generator, namely, an infinite mixture of deep nets, each of size n. So long as the deep net class is capable of generating simple gaussians, such mixtures are quite powerful, since a classical result says that an infinite mixtures of simple gaussians can closely approximate Dreal. Thus an infinite mixture of deep net generators will \u201cwin\u201d the GAN game, not only against a discriminator that is a small deep net but also against more powerful discriminators (e.g., any Lipschitz function).\nThe next stage in the thought experiment is to imagine a much less powerful generator, which is a mix of only a few deep nets, not infinitely many. Simple counterexamples show that now the distribution D will not closely approximate arbitrary Dreal with respect to natural metrics like `p. Nevertheless, could the generator still win the GAN game against a deep net of bounded capacity (i.e., the deep net is unable to distinguish D and Dreal)? We show it can.\ninformal theorem: If the discriminator is a deep net with n parameters, then a mixture of O\u0303(n log(n/ )/ 2) generator nets can produce a distribution D that the discriminator will be unable to distinguish from Dreal with probability more than . (Here O\u0303(\u00b7) notation hides some nuisance factors.)\nThis informal theorem (formalized below) is also a component of our result below about the existence of an approximate pure equilibrium. With current technique this existence result seems sensitive to the measuring function f , and works for f(x) = x (i.e., Wasserstein GAN). For other f we only show existence of mixed equilibria with small mixtures."}, {"heading": "5.1 General f : Mixed Equilibrium", "text": "For general measure function f we can only show the existence of a mixed equilibrium, where we allow the discriminator and generator to be finite mixtures of deep nets.\nFor a class of generators {Gu, u \u2208 U} and a class of discriminators {Gv, v \u2208 V}, we can define the payoff F (u, v) of the game between generator and discriminator\nF (u, v) = E x\u223cDreal [f(Dv(x))] + E h\u223cDh [f(1\u2212Dv(Gu(h)))]. (6)\nOf course as we discussed in previous section, in practice these expectations should be with respect to the empirical distributions. Our discussions in this section does not depend on the distributions Dreal and Dh, so we define F (u, v) this way for simplicity.\nThe well-known min-max theorem[v. Neumann, 1928] in game theory shows if both players are allowed to play mixed strategies then the game has a min-max solution. A mixed strategy for the generator is just a distribution Du supported on U , and one for discriminator is a distribution Dv supported on V.\nTheorem 5.1 (vonNeumann). There exists a value V , and a pair of distributions (Du,Dv) such that\n\u2200v, E u\u223cDu [F (u, v)] \u2264 V and \u2200u, E v\u223cDv [F (u, v)] \u2265 V.\nNote that this equilibrium involves both parties announcing their strategies Du,Dv at the start, such that neither will have any incentive to change their strategy after studying the opponent\u2019s strategy. The payoff is generated by the generator first sample u \u223c Du, h \u223c Dh, and then generate an example x = Gu(h). Therefore, the mixed generator is just a linear mixture of generators. The discriminator will first sample v \u223c Dv, and then output Dv(x). Note that in general this is very different from a discriminator D that outputs Ev\u223cDv [Dv(x)], because the measuring function f is in general nonlinear. In particular, the correct payoff function for a mixture of discriminator is:\nE v\u223cDv [F (u, v)] = E x\u223cDreal v\u223cDv [f(Dv(x))] + E h\u223cDh v\u223cDv [f(1\u2212Dv(Gu(h)))].\nOf course, this equilibrium involving an infinite mixture makes little sense in practice. We show that (as is folklore in game theory[Lipton et al., 2003]) that we can approximate this min-max solution with mixture of finitely many generators and discriminators. More precisely we define -approximate equilibrium\nDefinition 3. A pair of mixed strategy (Du,Dv) is an -approximate equilibrium, if for some value V\n\u2200v \u2208 V, E u\u2208U [F (u, v)] \u2264 V + ; \u2200u \u2208 U , E v\u2208V [F (u, v)] \u2265 V \u2212 .\nIf the strategies Du,Dv are pure strategies, then this pair is called an -approximate pure equilibrium.\nSuppose f is Lf -Lipschitz and bounded in [\u2212\u2206,\u2206], the generator and discriminators are LLipschitz with respect to the parameters and L\u2032-Lipschitz with respect to inputs, in this setting we can formalize the above Informal Theorem as follows:\nTheorem 5.2. In the settings above, there is a large enough constant C > 0 such that for any\n, there exists T = C\u22062n log(LL\u2032Lf \u00b7n/ )\n2 generators Gu1 , . . . GuT and T discriminators Dv1 , . . . DvT ,\nlet Du be a uniform distribution on ui and Dv be a uniform distribution on vi, then (Du, Dv) is an -approximate equilibrium. Furthermore, in this equilibrium the generator \u201cwins,\u201dmeaning discriminators cannot do better than random guessing.\nThe proof uses a standard probabilistic argument and epsilon net argument to show that if we sample T generators and discriminators from infinite mixture, they form an approximate equilibrium with high probability. For the second part, we use the fact that every distribution can be approximated by infinite mixture of Gaussians, so the generator must be able to approximate the real distribution Dreal and win. Therefore indeed a mixture of O\u0303(n) generators can achieve an -approximate equilibrium.\n5.1.1 f(x) = x: Pure Equilibrium\nNow we consider the special case of Wasserstein-GAN, which is equivalent to setting f(x) = x in Equation (2). In this case, it is possible to augment the network structure, and achieve an approximate pure equilibrium for the GAN game for networks of size O\u0303(n2). This should be interpreted as: if deep nets of size n are capable of generating a Gaussian, then the W-GAN game for neural networks of size O\u0303(n2) has an approximate equilibrium in which the generator wins. (The theorem is stated for RELU gates but also holds for standard activations such as sigmoid.)\nTheorem 5.3. Suppose the generator and discriminator are both p-layer neural networks (p \u2265 2) with n parameters, and the last layer uses ReLU activation function. In the setting of Theorem 5.2 there exists p + 1-layer neural networks of generators G and discriminator D with\nO ( \u22062n2 log(LL\u2032Lf \u00b7n/ ) 2 ) parameters, such that there exists an -approximate pure equilibrium. Furthermore, if the generator is capable of generating a Gaussian then the value V = 1.\nTo prove this theorem, we consider the mixture of generators and discriminators as in Theorem 5.2, and show how to fold the mixture into a larger p+ 1-layer neural network. We sketch the idea; details are in the supplementary.\nMixture of Discriminators Given a mixture of T discriminators Dv1 , ..., DvT , we can just define D(x) = 1T \u2211T i=1Dvi(x). Because f(x) = x, we know for any generator Gu\nE x\u223cDreal [D(x)] = E x\u223cDreal,i\u2208[T ] [Dvi(x)]\nE h\u223cDh [(1\u2212D(Gu(h))] = E h\u223cDh,i\u2208[T ] [(1\u2212Dvi(Gu(h))].\nTherefore, the payoff for D is exactly the same as the payoff for the mixture of discriminators. Also, the discriminator D is easily implemented as a network T times as large as the original network by adding a top layer that averages the output of the T generators Dvi(x).\nMixture of Generators For mixture of generators, we construct a single neural network that approximately generates the mixture distribution using the gaussian input it has. To do that, we can pass the input h through all the generators Gu1 , Gu2 , ..., GuT . We then show how to implement a \u201cmulti-way selector\u201d that will select a uniformly random output from Gui(h) (i \u2208 [T ]). The\nselector involves a simple 2-layer network that selects a number i from 1 to T with the appropriate probability and \u201cdisables\u201dall the neural nets except the ith one by forwarding an appropriate large negative input.\nTheorem 5.3 suggests that the objective of Wasserstein GAN may have approximate pure equilibrium for certain architectures of neural networks, which lends credence that Wasserstein GAN may be more robust.\nRemark: In practice, GANs use highly structured deep nets, such as convolutional nets. Our current proof of existence of pure equilibrium requires introducing less structured elements in the net, namely, the multiway selectors that implement the mixture within a single net. It is left for future work whether pure equilibria exist for the original structured architectures. In the meantime, in practice we recommend using, even for W-GAN, a mixture of structured nets for GAN training, and it seems to help in our experiments reported below."}, {"heading": "6 MIX+GANs", "text": "Theorem 5.2 and Theorem 5.3 show that using a mixture of (not too many) generators and discriminators guarantees existence of approximate equilibrium. This suggests that using a mixture may lead to more stable training.\nOf course, it is impractical to use very large mixtures, so we propose mix + gan: use a mixture of T components, where T is as large as allowed by size of GPU memory (usually T \u2264 5). Namely, train a mixture of T generators {Gui , i \u2208 [T ]} and T discriminators {Dvi , i \u2208 [T ]}) which share the same network architecture but have their own trainable parameters. Maintaining a mixture means of course maintaining a weight wui for the generator Gui which corresponds to the probability of selecting the output of Gui . These weights are also updated via backpropagation. This heuristic can be combined with existing methods like dcgan, w-gan etc., giving us new training methods mix+dcgan, mix+w-gan etc.\nWe use exponentiated gradient[Kivinen and Warmuth, 1997]: store the log-probabilities {\u03b1ui , i \u2208 [T ]}, and then obtain the weights by applying soft-max function on them:\nwui = e\u03b1ui\u2211T k=1 e \u03b1uk , i \u2208 [T ]\nNote that our algorithm is maintaining weights on different generators and discriminators. This is very different from the idea of boosting where weights are maintained on samples. AdaGAN[Tolstikhin et al., 2017] uses ideas similar to boosting and maintains weights on training examples.\nGiven payoff function F , training mix + gan boils down to optimizing:\nmin {ui},{\u03b1ui} max {vj},{\u03b1vj } E i,j\u2208[T ] F (ui, vj)\n= min {ui},{\u03b1ui} max {vj},{\u03b1vj } \u2211 i,j\u2208[T ] wuiwvjF (ui, vj).\nHere the payoff function is the same as Equation (6). We use both measuring functions f(x) = log x (for original GAN) and f(x) = x (for WassersteinGAN). In our experiments we alternatively update generators\u2019 and discriminators\u2019 parameters as well as their corresponding log-probabilities using ADAM [Kingma and Ba, 2015], with learning rate lr = 0.0001.\nEmpirically, it is observed that some components of the mixture tend to collapse and their weights diminish during the training. To encourage full use of the mixture capacity, we add to the training objective an entropy regularizer term that discourages the weights being too far away from uniform:\nRent({wui}, {wvi}) = \u2212 1\nT T\u2211 i=1 (log(wui) + log(wvi))"}, {"heading": "7 Experiments", "text": "In this section, we first explore the qualitative benefits of mix+gan on image generation tasks: MNIST dataset [LeCun et al., 1998] of hand-written digits and the CeleA [Liu et al., 2015] dataset of human faces. Then for more quantitative evaluation we use the CIFAR-10 dataset [Krizhevsky and Hinton, 2009] and use the Inception Score introduced in Salimans et al. [2016]. MNIST contains 60,000 labeled 28\u00d728-sized images of hand-written digits, CeleA contains over 200K 108\u00d7108-sized images of human faces (we crop the center 64\u00d7 64 pixels for our experiments), and CIFAR-10 has 60,000 labeled 32\u00d7 32-sized RGB natural images which fall into 10 categories.\nTo reinforce the point that this technique works out of the box, no extensive hyper-parameter search or tuning was done (and is left for further work). All related code is to be released. Please refer to our code for hyper-parameters and experimental setup."}, {"heading": "7.1 Qualitative Results", "text": "The DCGAN architecture [Radford et al., 2016] uses deep convolutional nets as generators and discriminators. We trained mix + dcgan on MNIST and CeleA using the authors\u2019 code as a black box, and compared visual qualities of generated images vs DCGAN.\nResults on MNIST is shown in Figure 2. In this experiment, the baseline DCGAN consists of a pair of a generator and a discriminator, which are 5-layer deconvoluitonal neural networks, and are conditioned on image labels. Our MIX+DCGAN model consists of a mixture of such DCGANs so that it has 3 generators and 3 discriminators. We observe that mix + dcgan produces somewhat cleaner digits than dcgan (note the fuzziness in the latter). Interestingly, each component of our mixture learns a slightly different style of strokes.\nFigure 3 demonstrates results on CeleA dataset, using the same architecture as for MNIST, except the models are not conditioned on image labels anymore.\nThe MIX+DCGAN model generates more faithful and more diverse samples than the baseline DCGAN does on both datasets, even though one may need to zoom in to fully perceive the difference since the two datasets are rather easy for a powerful training like DCGAN."}, {"heading": "7.2 Quantitative Results", "text": "Now we turn to quantitative measurement using Inception Score [Salimans et al., 2016]. Throughout, we use mixtures of 5 generators and discriminators. At first sight the comparison dcgan vs mix + dcgan seems unfair because the latter uses as much capacity as 5 dcgan\u2019s, with corresponding penalty in running time per epoch. On the other hand, in deep net training increased capacity isn\u2019t always a good idea, since training may use it badly (eg, to overfit).\nTo construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques. The same hyper-parameters are used for fair comparison. See Huang et al. [2016] for more details. Similarly, for the MIX+WassersteinGAN, the base GAN is identical to that proposed by Arjovsky et al. [2017] using their hyper-parameter scheme.\nTable 1 is a quantitative comparison between our mixture models and other state-of-the-art GAN models on the CIFAR-10 dataset, with inception score calculated using 50,000 freshly generated samples from each model that are not used in training. To sample a single image from our MIX+ models, we first select a generator from the mixture according to their assigned weights {wui}, and then draw a sample from the selected generator. We find surprisingly that, simply by applying MIX+ to the baseline models, our MIX+ models achieve 7.65 v.s. 7.16 gain in the score on DCGAN, and 4.04 v.s. 3.82 gain on WassersteinGAN.\nFigure 4 shows how Inception Scores of MIX+DCGAN v.s. DCGAN evolve during training. MIX+DCGAN outperforms DCGAN throughout the entire training process, showing that it makes effective use of the additional capacity.\nArjovsky et al. [2017] shows that (approximated) Wasserstein loss, which is the neural network divergence by our definition, is meaningful because it correlates well with visual quality of generated samples. Figure 5 shows the training dynamics of neural network divergence of MIX+WassersteinGAN v.s. WassersteinGAN, which strongly indicates that MIX+WassersteinGAN is capable of achieving a much lower divergence as well as of improving the visual quality of generated samples."}, {"heading": "8 Conclusions", "text": "The notion of generalization for GANs has been clarified by introducing a new notion of distance between distributions, the neural net distance. (Whereas popular distances such as Wasserstein and JS may not generalize.) Assuming the visual cortex also is a deep net, generalization with respect to this metric is of course sufficient to make the final samples look realistic to humans.\nThe paper also made progress on other unexplained issues about GANs, by showing that a pure approximate equilibrium exists for a certain natural training objective (Wasserstein) and in which the generator wins the game. No assumption about distribution Dreal is needed.\nSuspecting that a pure equilibrium may not exist for all objectives, we recommend in practice our mix+ gan protocol using a small mixture of discriminators and generators. Our experiments show it improves the quality of several existing GAN training methods.\nFinally, note that existence of an equilibrium does not imply that a simple algorithm (in this case, backpropagation) would find it easily. That still defies explanation."}, {"heading": "A Omitted Proofs", "text": "In this section we give detailed proofs for the theorems in the main document.\nA.1 Omitted Proofs for Section 3: Distance Metric and Generalization\nWe first prove that the traditional distance metrics do not generalize.\nTheorem A.1 (Theorem 3.1 restated). Let \u00b5, \u03bd be uniform Gaussian distributions N (0, 1dI). Suppose \u00b5\u0302, \u03bd\u0302 are empirical versions of \u00b5, \u03bd with m samples. Then when logm d, we have with high probability\ndJS(\u00b5, \u03bd) = 0,dJS(\u00b5\u0302, \u03bd\u0302) = log 2. dW (\u00b5, \u03bd) = 0,dW (\u00b5\u0302, \u03bd\u0302) \u2265 1.1.\nFurther, let \u00b5\u0303, \u03bd\u0303 be the convolution of \u00b5\u0302, \u03bd\u0302 with a Gaussian distribution N(0, \u03c3 2\nd I), as long as \u03c3 < c\u221a\nlogm for small enough constant c, we have with high probability\ndJS(\u00b5\u0303, \u03bd\u0303) > log 2\u2212 1/m.\nProof. For the Jensen-Shannon divergence, we know with probability 1 the supports of \u00b5\u0302, \u03bd\u0302 are disjoint, therefore dJS(\u00b5\u0302, \u03bd\u0302) = 1.\nFor Wasserstein distance, note that for two random Gaussian vectors x, y \u223c N(0, 1dI), their difference is also a Gaussian with expected square norm 2. Therefore we have\nPr[\u2016x\u2212 y\u20162 \u2264 2\u2212 ] \u2264 exp(\u2212\u2126( 2d)).\nAs a result, when logm d, we can union bound over all the m2 pairwise distances for points in support of \u00b5\u0302 and support of \u03bd\u0302. With high probability, the closest pair between \u00b5\u0302 and \u03bd\u0302 has distance at least 1, therefore the Wasserstein distance dW (\u00b5\u0302, \u03bd\u0302) \u2265 1.1.\nFinally we prove that even if we add noise to the two distributions, the JS divergence is still large. For distributions \u00b5\u0303, \u03bd\u0303, let \u03c11, \u03c12 be their density functions. Let g(x) = \u03c11(x) log\n2\u03c11(x) \u03c11(x)+\u03c12(x) +\n\u03c12(x) log 2\u03c12(x)\n\u03c11(x)+\u03c12(x) , we can rewrite the JS divergence as\ndJS(\u00b5\u0303, \u03bd\u0303) =\n\u222b 1\n2 g(x)dx.\nLet zx be a Bernoulli variable with probability \u03c11(x)/(\u03c11(x) + \u03c12(x)) of being 1. Note that g(x) = (\u03c11(x) + \u03c12(x))(log 2 \u2212 H(zx)) where H(zx) is the entropy of zx. Therefore 0 \u2264 g(x) \u2264 (\u03c11(x) + \u03c12(x)) log 2. Let X be the union of radius-0.2 balls near the 2m samples in \u00b5\u0302 and \u03bd\u0302. Since with high probability, all these samples have pairwise distance at least 1, by Gaussian density function we know (a) the balls do not intersect; (b) within each ball max{d1(x),d2(x)}min{d1(x),d2(x)} \u2265 m 2; (c) the union of these balls take at least 1\u2212 1/2m fraction of the density in (\u00b5\u0302+ \u03bd\u0302)/2.\nTherefore for every x \u2208 X , we know H(zx) \u2264 o(1/m), therefore\ndJS(\u00b5\u0303, \u03bd\u0303) =\n\u222b 1\n2 g(x)dx \u2265 \u222b x\u2208X 1 2 g(x)dx\n\u2265 \u222b x\u2208X (\u03c11(x) + \u03c12(x))(log 2\u2212 o(1/m))dx \u2265 log 2\u2212 1/2m\u2212 o(1/m) \u2265 log 2\u2212 1/m.\nNext we prove the neural network distance does generalize, given enough samples.\nTheorem A.2 (Theorem 3.2 restated). Let \u00b5, \u03bd be two distributions and \u00b5\u0302, \u03bd\u0302 be empirical versions with m samples each. There is a some fixed constant C such that when m \u2265 Cn\u2206 2 log(LLfd/ )\n2 , then\nwe have with probability close to 1:\n|dNN (\u00b5\u0302\u2016\u03bd\u0302)\u2212 dNN (\u00b5\u2016\u03bd)| \u2264 .\nProof. The proof uses concentration bounds. We show that with high probability, for every discriminator Dv,\n| E x\u223c\u00b5 [f(Dv(x))]\u2212 E x\u223c\u00b5\u0302 [f(Dv(x))]| \u2264 /2, (7)\n| E x\u223c\u03bd [f(1\u2212Dv(x))]\u2212 E x\u223c\u03bd\u0302 [f(1\u2212Dv(x))]| \u2264 /2. (8)\nIf dNN (\u00b5\u2016\u03bd) = t, let Dv be the optimal discriminator, we then have\ndNN (\u00b5\u2016\u03bd) \u2265 E x\u223c\u00b5\u0302 [f(Dv(x))] + E x\u223c \u02c61\u2212\u03bd [f(Dv(x))].\n\u2265 E x\u223c\u00b5 [f(Dv(x))] + E x\u223c\u03bd [f(Dv(x))]\n\u2212 | E x\u223c\u00b5 [f(Dv(x))]\u2212 E x\u223c\u00b5\u0302 [f(Dv(x))]|\n\u2212 | E x\u223c\u03bd [f(1\u2212Dv(x))]\u2212 E x\u223c\u03bd\u0302 [f(1\u2212Dv(x))]|\n\u2265 t\u2212 .\nThe other direction is similar. Now we prove the claimed bounds (7) (proof of (8) is identical). Let X be a finite set such that every point in V is within distance /8LLf of a point in X (a so-called /8LLf -net). Standard constructions give an X satisfying log |X | \u2264 O(n log(LLfn/ )). For every v \u2208 X , by Chernoff bound we know\nPr[| E x\u223c\u00b5 [f(Dv(x))]\u2212 E x\u223c\u00b5\u0302 [f(Dv(x))]| \u2265 4 ] \u2264 exp(\u2212\u2126(\n2m \u22062 )).\nTherefore, when m \u2265 Cn\u2206 2 log(LLfn/ )\n2 for large enough constant C, we can union bound over all\nv \u2208 X . With high probability, for all v \u2208 X we have |Ex\u223c\u00b5[f(Dv(x))]\u2212 Ex\u223c\u00b5\u0302[f(Dv(x))]| \u2265 4 .\nNow, for every v \u2208 V, we can find a v\u2032 \u2208 X such that \u2016v \u2212 v\u2032\u2016 \u2264 /8LLf . Therefore\n| E x\u223c\u00b5 [f(Dv(x))]\u2212 E x\u223c\u00b5\u0302 [f(Dv(x))]|\n\u2264| E x\u223c\u00b5 [f(Dv\u2032(x))]\u2212 E x\u223c\u00b5\u0302 [f(Dv\u2032(x))]|\n+ | E x\u223c\u00b5 [f(Dv\u2032(x))]\u2212 E x\u223c\u00b5 [f(Dv(x))]|\n+ | E x\u223c\u00b5\u0302 [f(Dv\u2032(x))]\u2212 E x\u223c\u00b5\u0302 [f(Dv(x))]|\n\u2264 /4 + /8 + /8 \u2264 /2.\nThis finishes the proof of (7).\nFinally, we generalize the above Theorem to hold for all generators in a family.\nCorollary A.1 (Corollary 3.1 restated). Let u1, u2, ..., ut (log t d) be t sets of parameters for the generator. For each ui, let D\u0302h[t] be a fresh set of m samples from the distribution Gui(Dh). Let D\u0302real be a set of m samples from the real distribution. With high probability when m \u2265 n\u2206 2 log(LLfn/ )\n2 we\nhave for all t |dNN (Dui(D\u0302h[t])\u2016D\u0302real)\u2212 dNN (Dui(Dh)\u2016Dreal)| \u2264 .\nProof. This follows from the proof of Theorem 3.2. Note that we have fresh samples for every generator distribution, so Equation (8) is true with high probability by union bound. For the real distribution, notice that Equation (7) does not depend on the generator, so it is also true with high probability.\nA.2 Omitted Proof for Section 5: Expressive power and existence of equilibrium\nMixed Equilibrium We first show there is a finite mixture of generators and discriminators that approximates the equilibrium of infinite mixtures.\nTheorem A.3 (Theorem 5.2 restated). In the settings of Theorem 5.2, there is a large enough constant C > 0 such that for any , there exists T = C\u22062n log(LL\u2032Lf \u00b7n/ )\n2 generators Gu1 , . . . GuT\nand T discriminators Dv1 , . . . DvT , let Du be a uniform distribution on ui and Dv be a uniform distribution on vi, then (Du, Dv) is an -approximate equilibrium.\nFurther, if the class of generator can generate a Gaussian, and the class of discriminator includes constant functions, then the value of the game V = 2f(1/2) (where f is the connection function in (2)).\nProof. Let (Du,Dv) be the pair of optimal solutions as in Theorem 5.1 and V be the optimal value. We will show that randomly sampling T generators and discriminators from these two distributions gives the desired mixture with high probability.\nConstruct /4LL\u2032Lf -nets U, V for U and V. By standard construction, the sizes of these -nets satisfy log(|U |+ |V |) \u2264 C \u2032n log(LL\u2032Lf \u00b7n/ ) for some constant C \u2032. Let u1, u2, ..., uT be independent samples from Du, and v1, v2, ..., vT be independent samples from Dv. By Chernoff bound, for any u \u2208 U , we know\nPr[ E i\u2208[T ] [F (u, vi)] \u2264 E v\u2208V\n[F (u, v)]\u2212 /2] \u2264 exp(\u2212 2T\n2\u22062 ).\nWhen T = C\u22062n log(L\u00b7Lf \u00b7n/ ) 2 and the constant C is large enough (C 2C \u2032), with high probability this inequality is true for all u \u2208 U . Now, for any u \u2208 U , let u\u2032 be the closest point in the -net. By the construction of the net, \u2016u \u2212 u\u2032\u2016 \u2264 /4LL\u2032Lf . It is easy to check that F (u, v) is 2LL\u2032Lf -Lipschitz in both u and v, therefore\nE i\u2208[T ] [F (u\u2032, vi)] \u2265 E i\u2208[T ] [F (u, vi)]\u2212 /2.\nCombining the two inequalities we know for any u\u2032 \u2208 U ,\nE i\u2208[T ]\n[F (u\u2032, vi)] \u2265 V \u2212 .\nThis finishes the proof for the second inequality. The proof for the first inequality is identical. By probabilistic argument we know there must exist such generators and discriminators.\nFinally, we prove the fact that the value V must be equal to 2f(1/2). For the discriminator, one strategy is to just output 1/2. This strategy has payoff 2f(1/2) no matter what the generator does, so V \u2265 2f(1/2). For the generator, consider the distribution D\u03b6 = Dreal + \u03b6N(0, I), which is the convolution of Dreal and a Gaussian of variance \u03b6I. For any \u03b6, D\u03b6 can be expressed as a infinite mixture of Gaussians and is therefore a mixed strategy of the generator. The Wasserstein distance between D\u03b6 and Dreal is O(\u03b6). Since the discriminator is L\u2032-Lipschitz, it cannot distinguish between D\u03b6 and Dreal. In particular we know for any discriminator Dv\n| E x\u223cD\u03b6 [f(1\u2212Dv(x))]\u2212 E x\u223cDreal\n[f(1\u2212Dv(x))]| \u2264 O(LfL\u2032\u03b6).\nTherefore,\nmax v\u2208V E x\u223cDreal [f(Dv(x))] + E x\u223cD\u03b6 [f(1\u2212Dv(x))]\n\u2264O(LfL\u2032\u03b6) + max v\u2208V E x\u223cDreal [f(Dv(x)) + f(1\u2212Dv(x))] \u22642f(1/2) +O(LfL\u2032\u03b6).\nHere the last step uses the assumption that f is concave. Therefore the value is upperbounded by V \u2264 2f(1/2) +O(LfL\u2032\u03b6) for any \u03b6. Taking limit of \u03b6 to 0, we have V = 2f(1/2).\nPure equilibrium Now we show for Wasserstein objective, there exists an approximate pure equilibrium\nTheorem A.4 (Theorem 5.3 restated). Suppose the generator and discriminator are both p-layer neural networks (p \u2265 2) with n parameters, and the last layer uses ReLU activation function. In the setting of Theorem 5.2 there exists p+1-layer neural networks of generators G and discriminator D\nwith O ( \u22062n2 log(LL\u2032Lf \u00b7n/ ) 2 ) parameters, such that there exists an -approximate pure equilibrium. Furthermore, if the generator is capable of generating a Gaussian then the value V = 1.\nIn order to prove this theorem, the major step is to construct a generator that works as a mixture of generators.\nMixture of Generators For mixture of generators, we need to construct a single neural network that approximately generates the mixture distribution using the gaussian input it has. To do that, we can pass the input h through all the generators Gu1 , Gu2 , ..., GuT . We then show how to implement a \u201cmulti-way selector\u201d that will select a uniformly random output from Gui(h) (i \u2208 [T ]).\nIn order to do that, we first observe that it is possible to compute a step function using a two layer neural network. This is fairly standard for many activation functions.\nLemma 1. Fix an arbitrary k \u2208 N and z1 < z2 < \u00b7 \u00b7 \u00b7 < zk. For any 0 < \u03b4 < min{zi+1\u2212 zi}, there is a two-layer neural network with a single input h \u2208 < that outputs k + 1 numbers x1, x2, ..., xk+1 such that (i) \u2211k+1 i=1 xi = 1 for all h; (ii) when h \u2208 [zi\u22121 + \u03b4/2, zi \u2212 \u03b4/2], xi = 1 and all other xj\u2019s are 05.\nProof. Using a two layer neural network, we can compute the function fi(h) = max{h\u2212zi\u2212\u03b4/2\u03b4 , 0}\u2212 max{h\u2212zi+\u03b4/2\u03b4 , 0}. This function is 0 for all h < zi \u2212 \u03b4/2, 1 for all h \u2265 zi + \u03b4/2 and change linearly in between. Now we can write x1 = 1 \u2212 f1(h), xk+1 = fk(h), and for all i = 2, 3, ..., k, xk = fi(h)\u2212 fi\u22121(h). It is not hard to see that these functions satisfy our requirements.\nUsing these step functions, we can essentially select one output from the T generators.\nLemma 2. In the setting of Theorem 5.3, for any \u03b4 > 0, there is a p+ 1-layer neural network with O ( \u22062n2 log(LL\u2032Lf \u00b7n/ ) 2 ) parameters that can generate a distribution that is within \u03b4 total variational difference with the mixture of Gu1 , Gu2 , ..., GuT .\nThe idea is simple: since we have implemented step functions from Lemma 1, we can just pass through the input through all the generators Gu1 , ..., GuT . For the last layer of Gui , we add a large multiple of \u2212(1 \u2212 xi) where xi is the i-th output from the network in Lemma 1. Clearly, if xi = 0 this is going to effectively disable the neural network; if xi = 1 this will have no effect. By properties of xi\u2019s we know most of the time only one xi = 1, hence only one generator is selected.\nProof. Suppose the input for the generator is (h0, h) \u223c N(0, 1) \u00d7 Dh (i.e. h0 is sampled from a Gaussian, h is sampled according to Dh independently). We pass the input h through the generators and gets outputs Gui(h), then we use h0 to select one as the true output.\nLet z1, z2, ..., zT\u22121 be real numbers that divides the probability density of a Gaussian into T equal parts. Pick \u03b4\u2032 = \u03b4/100T in Lemma 1, we know there is a 2-layer neural network that computes step functions x1, ..., xT . Moreover, the probability that (x1, ..., xT ) has more than 1 nonzero entry is smaller than \u03b4. Now, for the output of Gui(h), in each output ReLU gate, we add a very large multiple of \u2212(1 \u2212 xi) (larger than the maximum possible output). This essentially \u201cdisables\u201d the output when xi = 0 because before the result before ReLU is always negative. On the other hand, when xi = 1 this preserves the output. Call the modified network G\u0302ui , we know G\u0302ui = Gui when xi = 1 and G\u0302ui = 0 when xi = 0. Finally we add a layer that outputs the sum of G\u0302ui . By construction we know when (x1, ..., xT ) has only one nonzero entry, the network correctly outputs the corresponding Gui(xi). The probability that this happens is at least 1\u2212\u03b4 so the total variational distance with the mixture is bounded by \u03b4.\nUsing the generator and discriminators we constructed, it is not hard to prove Theorem 5.3. The only thing to notice here is that when the generator is within \u03b4 total variational distance to the true mixture, the payoff F (u, v) can change by at most 2\u2206\u03b4.\n5When h \u2264 z1 \u2212 \u03b4/2 only x1 is 1 and when h \u2265 zk + \u03b4/2 only xk+1 = 1\nProof of Theorem 5.3. Let T be large enough so that there exists an /2-approximate mixed equilibrium. Let the new set of discriminators be the convex combination of T discriminators {Dv, v \u2208 V}. Let the new set of generators be constructed as in Lemma 2 with \u03b4 \u2264 /4\u2206 and Gu1 , ..., GuT from the original set of generators. Let D be the discriminator which is the average of the T discriminators from the approximate mixed equilibrium, and G be the generator constructed by the T generators from the approximate mixed equilibrium. Define F ?(G,D) be the payoff of the new two-player game. Now, for any discriminator D\u2032, think of it as a distribution of Gv, we know\nF ?(G,D\u2032) \u2265 E i\u2208[T ],v\u2208D\u2032 F (ui, v)\n\u2212 |F ?(G,D\u2032)\u2212 E i\u2208[T ],v\u2208D\u2032 F (ui, v)|\n\u2265 V \u2212 /2\u2212 2\u2206 4\u2206 \u2265 V \u2212 .\nThe bound from the first term comes from Theorem 5.2, and the fact that the expectation is smaller than the max. The bound for the second term comes from the fact that changing a \u03b4 fraction of probability mass can change the payoff F by at most 2\u2206\u03b4.\nSimilarly, for any generator G\u2032, we know it is close to a mixture of generators Gu, therefore\nF ?(G\u2032, D) \u2264 E i\u2208[T ],u\u2208G\u2032 F (u, vi)\n+ |F ?(G\u2032, D)\u2212 E i\u2208[T ],u\u2208G\u2032 F (u, vi)|\n\u2264 V + /2 + 2\u2206 4\u2206 \u2264 V + .\nThis finishes the proof."}], "references": [{"title": "Learning to protect communications with adversarial neural cryptography", "author": ["Mart\u0301\u0131n Abadi", "David G Andersen"], "venue": "arXiv preprint arXiv:1610.06918,", "citeRegEx": "Abadi and Andersen.,? \\Q2016\\E", "shortCiteRegEx": "Abadi and Andersen.", "year": 2016}, {"title": "Bayesian nonparametrics", "author": ["Jayanta K Ghosh", "RVJK Ghosh", "RV Ramamoorthi"], "venue": "Technical report,", "citeRegEx": "Ghosh et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ghosh et al\\.", "year": 2003}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Stacked generative adversarial networks", "author": ["xun Huang", "Yixuan Li", "Omid Poursaeed", "John Hopcroft", "Serge Belongie"], "venue": "Technical report,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Jyrki Kivinen", "Manfred K Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "The mnist database of handwritten digits", "author": ["Yann LeCun", "Corinna Cortes", "Christopher JC Burges"], "venue": null, "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Playing large games using simple strategies", "author": ["Richard J Lipton", "Evangelos Markakis", "Aranyak Mehta"], "venue": "In Proceedings of the 4th ACM conference on Electronic commerce,", "citeRegEx": "Lipton et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2003}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Adagan: Boosting generative models", "author": ["Ilya Tolstikhin", "Sylvain Gelly", "Olivier Bousquet", "Carl-Johann Simon-Gabriel", "Bernhard Sch\u00f6lkopf"], "venue": "arXiv preprint arXiv:1701.02386,", "citeRegEx": "Tolstikhin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Tolstikhin et al\\.", "year": 2017}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["Dilin Wang", "Qiang Liu"], "venue": "Technical report,", "citeRegEx": "Wang and Liu.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu.", "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016].", "startOffset": 39, "endOffset": 64}, {"referenceID": 0, "context": ", 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016].", "startOffset": 177, "endOffset": 203}, {"referenceID": 0, "context": ", 2014] have become the dominant method for fitting generative models to complicated real-life data, and even found unusual uses such as designing good cryptographic primitives [Abadi and Andersen, 2016]. Various novel architectures and training objectives were introduced to address perceived shortcomings of the original idea, leading to more stable training and more realistic generative models in practice. But many basic issues remain unresolved, as we now discuss. Let\u2019s recall that the basic scenario is that we wish to train a generator deep net whose input is a standard Gaussian, and whose output is a sample from some distribution D on <d. We wish the samples from D to closely resemble those drawn from some real-life distribution Dreal (which could be, say, real-life images represented using raw pixels). Towards this end, a discriminator deep net is trained alongside the generator net, and it is trained to maximise its ability to distinguish between samples from Dreal and D. So long as the discriminator is successful at this task with nonzero probability, its success can be used to generate a feedback (using backpropagation) to the generator, thus improving its distribution D. This basic iterative framework has been tried with many training objectives; see Section 2. Recently Arjovsky et al. [2017] proposed another variant called Wasserstein GAN that appears to lead to more stable training.", "startOffset": 178, "endOffset": 1323}, {"referenceID": 1, "context": ", a standard result in bayesian nonparametrics says that every probability density is closely approximable by an infinite mixture of Gaussians [Ghosh et al., 2003].", "startOffset": 143, "endOffset": 163}, {"referenceID": 2, "context": "The standard GAN training [Goodfellow et al., 2014] consists of training u, v so as to optimize an objective such as:", "startOffset": 26, "endOffset": 51}, {"referenceID": 8, "context": "We show that (as is folklore in game theory[Lipton et al., 2003]) that we can approximate this min-max solution with mixture of finitely many generators and discriminators.", "startOffset": 43, "endOffset": 64}, {"referenceID": 8, "context": "We show that (as is folklore in game theory[Lipton et al., 2003]) that we can approximate this min-max solution with mixture of finitely many generators and discriminators.", "startOffset": 43, "endOffset": 64}, {"referenceID": 5, "context": "We use exponentiated gradient[Kivinen and Warmuth, 1997]: store the log-probabilities {\u03b1ui , i \u2208 [T ]}, and then obtain the weights by applying soft-max function on them:", "startOffset": 29, "endOffset": 56}, {"referenceID": 12, "context": "AdaGAN[Tolstikhin et al., 2017] uses ideas similar to boosting and maintains weights on training examples.", "startOffset": 6, "endOffset": 31}, {"referenceID": 4, "context": "In our experiments we alternatively update generators\u2019 and discriminators\u2019 parameters as well as their corresponding log-probabilities using ADAM [Kingma and Ba, 2015], with learning rate lr = 0.", "startOffset": 146, "endOffset": 167}, {"referenceID": 7, "context": "In this section, we first explore the qualitative benefits of mix+gan on image generation tasks: MNIST dataset [LeCun et al., 1998] of hand-written digits and the CeleA [Liu et al.", "startOffset": 111, "endOffset": 131}, {"referenceID": 9, "context": ", 1998] of hand-written digits and the CeleA [Liu et al., 2015] dataset of human faces.", "startOffset": 45, "endOffset": 63}, {"referenceID": 6, "context": "Then for more quantitative evaluation we use the CIFAR-10 dataset [Krizhevsky and Hinton, 2009] and use the Inception Score introduced in Salimans et al.", "startOffset": 66, "endOffset": 95}, {"referenceID": 6, "context": "Then for more quantitative evaluation we use the CIFAR-10 dataset [Krizhevsky and Hinton, 2009] and use the Inception Score introduced in Salimans et al. [2016]. MNIST contains 60,000 labeled 28\u00d728-sized images of hand-written digits, CeleA contains over 200K 108\u00d7108-sized images of human faces (we crop the center 64\u00d7 64 pixels for our experiments), and CIFAR-10 has 60,000 labeled 32\u00d7 32-sized RGB natural images which fall into 10 categories.", "startOffset": 67, "endOffset": 161}, {"referenceID": 10, "context": "The DCGAN architecture [Radford et al., 2016] uses deep convolutional nets as generators and discriminators.", "startOffset": 23, "endOffset": 45}, {"referenceID": 11, "context": "Now we turn to quantitative measurement using Inception Score [Salimans et al., 2016].", "startOffset": 62, "endOffset": 85}, {"referenceID": 3, "context": "To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques.", "startOffset": 85, "endOffset": 105}, {"referenceID": 3, "context": "To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques. The same hyper-parameters are used for fair comparison. See Huang et al. [2016] for more details.", "startOffset": 85, "endOffset": 315}, {"referenceID": 3, "context": "To construct MIX+DCGAN, we build on top of the DCGAN trained with losses proposed by Huang et al. [2016], namely adversarial loss, entropy loss and conditional loss, which is the best DCGAN so far without improved training techniques. The same hyper-parameters are used for fair comparison. See Huang et al. [2016] for more details. Similarly, for the MIX+WassersteinGAN, the base GAN is identical to that proposed by Arjovsky et al. [2017] using their hyper-parameter scheme.", "startOffset": 85, "endOffset": 441}, {"referenceID": 13, "context": "58 SteinGAN [Wang and Liu, 2016] 6.", "startOffset": 12, "endOffset": 32}, {"referenceID": 11, "context": "35 Improved GAN [Salimans et al., 2016] 8.", "startOffset": 16, "endOffset": 39}, {"referenceID": 11, "context": "Method Score DCGAN (as reported in Wang and Liu [2016]) 6.", "startOffset": 35, "endOffset": 55}, {"referenceID": 3, "context": "07 DCGAN (best variant in Huang et al. [2016]) 7.", "startOffset": 26, "endOffset": 46}], "year": 2017, "abstractText": "This paper makes progress on several open theoretical issues related to Generative Adversarial Networks. A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein. We introduce a new metric called neural net distance for which generalization does occur. We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein). Showing such a result has been an open problem (for any training objective). Finally, the above theoretical ideas lead us to propose a new training protocol, mix+gan, which can be combined with any existing method. We present experiments showing that it stabilizes and improves some existing methods.", "creator": "LaTeX with hyperref package"}}}