{"id": "1703.00993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2017", "title": "A Comparative Study of Word Embeddings for Reading Comprehension", "abstract": "the focus of past selective learning prototypes for reading laboratory tasks has been primarily on the design - novel sensory adaptive architectures. currently we imagine instead our minor choices must indicate ( possibly ) indirect use of university - trained developmental algorithms, indicate ( subsequently ) greater representation of out - of - hearing workers at test time, whom turn out to have a lesser impact than architectural choices determine the final performance. interviews systematically explore better options for these choices, and provide recommendations reflecting researchers frequently exploring this area.", "histories": [["v1", "Thu, 2 Mar 2017 23:58:54 GMT  (698kb,D)", "http://arxiv.org/abs/1703.00993v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bhuwan dhingra", "hanxiao liu", "ruslan salakhutdinov", "william w cohen"], "accepted": false, "id": "1703.00993"}, "pdf": {"name": "1703.00993.pdf", "metadata": {"source": "CRF", "title": "A Comparative Study of Word Embeddings for Reading Comprehension", "authors": ["Bhuwan Dhingra", "Hanxiao Liu", "Ruslan Salakhutdinov", "William W. Cohen"], "emails": ["wcohen}@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Systems that can read documents and answer questions about their content are a key language technology. The field, which has been termed Reading Comprehension (RC), has attracted a tremendous amount of interest in the last two years, primarily due to the introduction of largescale annotated datasets, such as CNN (Hermann et al., 2015) and SQuAD (Rajpurkar et al., 2016).\nPowerful statistical models, including deep learning models (also termed as readers), have been proposed for RC, most of which employ the following recipe: (1) Tokens in the document and question are represented using word vectors obtained from a lookup table (either initialized randomly, or from a pre-trained source such as GloVe (Pennington et al., 2014)). (2) A sequence model such as LSTM (Hochreiter and Schmidhuber, 1997), augmented with an attention mechanism (Bahdanau et al., 2014), updates these vectors to produce contextual representations. (3) An output layer uses these contextual representations to locate the answer in the document. The focus so far in the literature has been on steps (2) and\n(3), and several novel architectures have been proposed (see Section 2.1).\nIn this work, we show that seemingly minor choices made in step (1), such as the use of pretrained word embeddings and the handling of outof-vocabulary tokens at test time, can lead to substantial differences in the final performance of the reader. These differences are usually much larger than the gains reported due to architectural improvements. As a concrete example, in Figure 1 we compare the performance of two RC models\u2014Stanford Attentive Reader (AR) (Chen et al., 2016) and Gated Attention (GA) Reader (Dhingra et al., 2016)\u2014on the Who-Did-What dataset (Onishi et al., 2016), initialized with word embeddings trained on different corpora. Clearly, comparison between architectures is meaningful only under a controlled initialization method.\nTo justify our claims, we conduct a comprehensive set of experiments comparing the effect of utilizing embeddings pre-trained on several corpora. We experiment with RC datasets from different domains using different architectures, and obtain consistent results across all settings. Based on our findings, we recommend the use of certain pre-trained GloVe vectors for initialization. These consistently outperform other off-the-shelf\nar X\niv :1\n70 3.\n00 99\n3v 1\n[ cs\n.C L\n] 2\nM ar\n2 01\n7\nembeddings such as word2vec 1 (Mikolov et al., 2013), as well as those pre-trained on the target corpus itself and, perhaps surprisingly, those trained on a large corpus from the same domain as the target dataset.\nAnother important design choice is the handling of out-of-vocabulary (OOV) tokens at test time. A common approach (e.g. (Chen et al., 2016; Shen et al., 2016)) is to replace infrequent words during training with a special token UNK, and use this token to model the OOV words at the test phase. In reading comprehension, where the target answers are often rare words, we find that this approach leads to significantly worse performance in certain cases. A superior strategy is to assign each OOV token either a pre-trained, if available, or a random but unique vector at test time. We discuss and compare these two strategies, as well a mixed variant of the two, in Section 3.2."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 RC Datasets & Models", "text": "Many datasets aimed at measuring the performance of RC have been proposed (Nguyen et al., 2016; Trischler et al., 2016). For our purposes, we pick two of these benchmarks from different domains \u2013 Who-Did-What (WDW) (Onishi et al., 2016) constructed from news stories, and the Children\u2019s Book Test (CBT) (Hill et al., 2015) constructed from children\u2019s books. For CBT we only consider the questions where the answer is a named entity (CBT-NE). Several RC models based on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016). For our experiments we pick two of these models: the simple, but competitive, Stanford AR, and the high-performing GA Reader.\nStanford AR: The Stanford AR consists of single-layer Bidirectional GRU encoders for both the document and the query, followed by a bilinear attention operator for computing a weighted average representation of the document. The original model, which was developed for the anonymized CNN / Daily Mail datasets, used an output lookup table Wa to select the answer. However, without anonymization the number of answer candi-\n1 https://code.google.com/archive/p/word2vec/\ndates can become very large. Hence, we instead select the answer from the document representation itself, followed by an attention sum mechanism (Kadlec et al., 2016). This procedure is very similar to the one used in GA Reader, and is described in detail in Appendix A.\nGA Reader: The GA Reader is a multi-hop architecture which updates the representation of document tokens through multiple bidirectional GRU layers (Cho et al., 2014). At the output of each intermediate layer, the token representations are re-weighted by taking their element-wise product with an attention-weighted representation of the query. The outputs of the final layer are further matched with the query representation with an inner product to produce a distribution over the candidate answers in the document, and multiple mentions are aggregated using attention sum. We use the publicly available code2 with the default hyperparameter settings of (Dhingra et al., 2016), detailed in Appendix B."}, {"heading": "2.2 Word Embedding methods", "text": "The two most popular methods for inducing word embeddings from text corpora are GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013). These packages also provide off-theshelf (OTS) embeddings trained on large corpora3. While the GloVe package provides embeddings with varying sizes (50-300), word2vec only provides embeddings of size 300. This is an important difference, which we discuss in detail later. We also train three additional embeddings, listed in Table 1, including those trained on the target datasets themselves. In summary, we test with two in-domain corpora for WDW: one large (OTS) and one small (WDW), and two in-domain corpora for CBT: one large (BT) and one small (CBT).\n2 https://github.com/bdhingra/ga-reader\n3The word2vec package contains embeddings for both capitalized and lowercase words. We convert all words to lowercase, and if a word has both lowercase and uppercase embeddings we use the lowercase version.\nWhen training embeddings, we set hyperparameters to their default values in the provided packages (see Appendix B for details). This is by no means an optimal choice, in fact previous studies (Levy et al., 2015) have shown that hyperparameter choices may have a significant impact on downstream performance. However, training a single RC model can take anywhere from several hours to several days, and tuning hyperparameters for the embedding method on this downstream task is both infeasible and rarely done in practice. Instead, our objective is to provide guidelines to researchers using these methods out-of-the-box."}, {"heading": "3 Experiments and Results", "text": ""}, {"heading": "3.1 Comparison of Word Embeddings", "text": "We repeat each experiment twice with different random seeds and report the average test set accuracy across the two runs. Figure 2 shows a comparison of the RC performance for GA Reader and Stanford AR after initializing with various pretrained embeddings, and also after initializing randomly. We see consistent results across the two datasets and and the two models.\nThe first observation is that using embeddings trained on the right corpora can improve anywhere from 3-6% over random initialization. However, the corpus and method used for pre-training are important choices: for example word2vec embeddings trained on CBT perform worse than random. Also note that in every single case, GloVe embeddings outperform word2vec embeddings trained on the same corpora. It is difficult to claim that one method is better than the other, since previous studies (Levy et al., 2015) have shown that these methods are sensitive to hyperparameter tuning. However, if used out-of-the-box, GloVe seems to be the preferred method for pre-training.\nThe single best performance is given by off-theshelf GloVe embeddings (d = 100) in each case, which outperform off-the-shelf word2vec embeddings (d = 300). To understand if the difference comes from the differing dimension sizes, we plot the performance of GloVe embeddings as the dimension size is increased in Figure 3 (left). Performance drops as the embedding dimension size is increased (most likely due to over-fitting); however even at d = 300, GloVe embeddings outperform word2vec embeddings.\nOn both test datasets embeddings trained on formal domains, like news (OTS, WDW), perform\nat least as well as those trained on informal ones, like fiction (BT, CBT). This is surprising for CBTNE dataset which is itself constructed from the informal domain of children\u2019s books. For example, WDW (50M tokens) does significantly better than CBT-NE (50M tokens) in 3 out of the 4 cases, and also significantly better than the much larger BT (8B tokens) in one setting (and comparably in other settings). A key distinguishing feature between these two domains is the fraction of text composed of stopwords 4 \u2013 WDW consists of 54% stopwords while BT consists of 68% stopwords. Both GloVe and word2vec induce word vectors by minimizing the Euclidean distance between vectors of frequently co-occurring words. Co-occurrence with stopwords, however, provides little meaningful information about the semantics of a particular word, and hence corpora with a high percentage of these may not produce high-quality vectors. This effect may be mitigated during pretraining by either, (1) removing a fraction of stopwords from the corpus, (2) increasing the window size for counting co-occuring words. Figure 3 (right) shows the effect of both these methods on downstream RC performance. There is an improvement as the fraction of stopwords decreases or the window size increases, upto a certain limit. In fact, with proper tuning, BT embeddings give roughly the same performance as OTS GloVe, emphasizing the importance of hyperparameter tuning when training word vectors. There is also evidence that stopword removal can be beneficial when training word vectors, however this needs further verification on other downstream tasks."}, {"heading": "3.2 Handling OOV tokens", "text": "In this section we study some common techniques for dealing with OOV tokens at test time. Based on the results from the previous section, we conduct this study using only the off-the-shelf GloVe pre-trained embeddings. Let the training, test and GloVe vocabularies be denoted by V T , V E and V G respectively. Also define V Tn = {t \u2208 V T : #t > n} where #t denotes the count of token t in the training corpus. Before training a neural network for RC, the developer must first decide on the set of words V which will be assigned word vectors. Any token outside V is treated as an OOV token (denoted by UNK) and is assigned the same\n4We use the list of stopwords available at http://research. microsoft.com/en-us/um/redmond/projects/mctest/data/ stopwords.txt\nfixed vector. By far the most common technique in NLP literature (e.g. (Chen et al., 2016; Shen et al., 2016)) for constructing this vocabulary is to decide on a minimum frequency threshold n (typically 5- 10) and set V = V Tn . Out of these, vectors for those which also appear in V G are initialized to their GloVe embeddings, and the rest are randomly initialized. Remaining tokens in V T and those in V E \u2212 V T are all assigned the UNK vector, which is itself updated during training. This method ignores the fact that many of the words assigned as UNK may have already trained embeddings available in V G. Hence, here we propose another strategy of constructing the vocabulary as V = V Tn \u222a V G. Then at test time, any new token would be assigned its GloVe vector if it exists, or the vector for UNK. A third approach, used in (Dhingra et al., 2016), is motivated by the fact that many of the RC models rely on computing fine-grained similarity between document and\nquery tokens. Hence, instead of assigning all OOV tokens a common UNK vector, it might be better to assign them untrained but unique random vectors. This can be done by setting the vocabulary to V = V Tn \u222aV E \u222aV G. Hence, at test time any new token will be assigned its GloVe vector if it exists, or a random vector. Note that for this approach access to V E at training time is not needed.\nFigure 4 shows a comparison of all three approaches with varying n for the GA Reader on WDW and CBT-NE datasets. A gap of 3% and 11% between the best and worst setting for WDW and CBT-NE respectively clearly indicates the importance of using the correct setting. The commonly used method of setting V = V Tn is not a good choice for RC, and gets worse as n is increased. It performs particularly poorly for the CBT-NE dataset, where \u223c 20% of the test set answers do not appear in the training set (compared to only \u223c 1.5% in WDW). The other two approaches perform comparably for WDW, but for CBT-NE assigning random vectors rather than UNK to OOV tokens gives better performance. This is also easily explained by looking at fraction of test set answers which do not occur in V T0 \u222aV G \u2013 it is \u223c 10% for CBT-NE, and < 1% for WDW. Since in general it is not possible to compute these fractions without access to the test set, we recommend setting V = V Tn \u222a V E \u222a V G."}, {"heading": "4 Conclusions", "text": "We have shown that the choice of pre-trained embeddings for initializing word vectors has a significant impact on the performance of neural models for reading comprehension. So does the method for handling OOV tokens at test time. We argue that different architectures can only be compared when these choices are controlled for. Based on our experiments, we recommend the use of offthe-shelf GloVe embeddings, and assigning pretrained GloVe vectors, if available, or random but unique vectors to OOV tokens at test time."}, {"heading": "Acknowledgments", "text": "This work was funded by NSF under CCF1414030 and Google Research."}, {"heading": "A Answer Selection for Stanford AR", "text": "Using the notation from (Chen et al., 2016), let p\u03031, p\u03032, . . . , p\u0303m be the contextual embeddings of the tokens in the document, and let o be the attention-weighted document representation, then we compute the probability that token i answers the question as:\nP (a = di|d, q) = si = softmax(p\u0303Ti o) (1)\nThe probability of a particular candidate c \u2208 C as being the answer is then computed by aggregating the probabilities of all document tokens which appear in c and renormalizing over the candidates:\nPr(c|d, q) \u221d \u2211\ni\u2208I(c,d)\nsi (2)\nwhere I(c, d) is the set of positions where a token in c appears in the document d."}, {"heading": "B Hyperparameter Details", "text": "For the WDW dataset we use hidden state size d = 128 for the GRU and dropout with p = 0.3. For CBT-NE dataset we use d = 128 and dropout with p = 0.4. The Stanford AR has only 1 layer as proposed in the original paper, while the GA Reader has 3 layers. For Stanford AR dropout is applied to the input of the layer, and for GA Reader it is applied in between layers. Embeddings sizes for the word vectors were set to dw = 100 for all experiments, except those using off-the-shelf word2vec embeddings. To enable a fair comparison, we utilize the qe-comm feature for Stanford AR, which was used in the implementation of GA Reader. Since our purpose is to study the effect of word vectors, we do not use character embeddings in our experiments.\nWe train the models using the ADAM (Kingma and Ba, 2014) optimizer with an initial learning rate of 0.0005, which is halved every epoch after the first 3 epochs. We track performance on the validation set, and select the model with the highest validation accuracy for testing.\nWhen training word vectors we retain the default settings provided with the GloVe and word2vec packages, with the only exception that window size was set to 10 for both (to ensure consistency). For word2vec, we used skip-gram architecture with hierarchical softmax, and subsampled frequent words with a threshold 10\u22123 (see (Mikolov et al., 2013) for details). For GloVe, we used 15 iterations when training on the small WDW and CBT corpora, and 50 iterations for the large BT corpus. In any corpus, words occurring less than 5 times were filtered before training the word vectors."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.0473 .", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Embracing data abundance: Booktest dataset for reading comprehension", "author": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1610.00956 .", "citeRegEx": "Bajgar et al\\.,? 2016", "shortCiteRegEx": "Bajgar et al\\.", "year": 2016}, {"title": "A thorough examination of the cnn/daily mail reading comprehension task", "author": ["Danqi Chen", "Jason Bolton", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1606.02858 .", "citeRegEx": "Chen et al\\.,? 2016", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Attention-overattention neural networks for reading comprehension", "author": ["Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu."], "venue": "arXiv preprint arXiv:1607.04423 .", "citeRegEx": "Cui et al\\.,? 2016", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-attention readers for text comprehension", "author": ["Bhuwan Dhingra", "Hanxiao Liu", "William W Cohen", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1606.01549 .", "citeRegEx": "Dhingra et al\\.,? 2016", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Tracking the world state with recurrent entity networks", "author": ["Mikael Henaff", "Jason Weston", "Arthur Szlam", "Antoine Bordes", "Yann LeCun."], "venue": "arXiv preprint arXiv:1612.03969 .", "citeRegEx": "Henaff et al\\.,? 2016", "shortCiteRegEx": "Henaff et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems. pages 1684\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "arXiv preprint arXiv:1511.02301 .", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Text understanding with the attention sum reader network", "author": ["Rudolf Kadlec", "Martin Schmid", "Ondrej Bajgar", "Jan Kleindienst."], "venue": "arXiv preprint arXiv:1603.01547 .", "citeRegEx": "Kadlec et al\\.,? 2016", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Dynamic entity representations with max-pooling improves machine reading", "author": ["Sosuke Kobayashi", "Ran Tian", "Naoaki Okazaki", "Kentaro Inui."], "venue": "NAACL-HLT .", "citeRegEx": "Kobayashi et al\\.,? 2016", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural semantic encoders", "author": ["Tsendsuren Munkhdalai", "Hong Yu."], "venue": "arXiv preprint arXiv:1607.04315 .", "citeRegEx": "Munkhdalai and Yu.,? 2016", "shortCiteRegEx": "Munkhdalai and Yu.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268 .", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Who did what: A large-scale person-centered cloze dataset", "author": ["Takeshi Onishi", "Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester."], "venue": "EMNLP .", "citeRegEx": "Onishi et al\\.,? 2016", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pages 1532\u2013 1543. http://www.aclweb.org/anthology/D14-1162.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250 .", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "author": ["Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen."], "venue": "arXiv preprint arXiv:1609.05284 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Iterative alternating neural attention for machine reading", "author": ["Alessandro Sordoni", "Phillip Bachman", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1606.02245 .", "citeRegEx": "Sordoni et al\\.,? 2016", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830 .", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Multi-perspective context matching for machine comprehension", "author": ["Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian."], "venue": "arXiv preprint arXiv:1612.04211 .", "citeRegEx": "Wang et al\\.,? 2016", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "End-to-end answer chunk extraction and ranking for reading comprehension", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou."], "venue": "arXiv preprint arXiv:1610.09996 .", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "years, primarily due to the introduction of largescale annotated datasets, such as CNN (Hermann et al., 2015) and SQuAD (Rajpurkar et al.", "startOffset": 87, "endOffset": 109}, {"referenceID": 19, "context": ", 2015) and SQuAD (Rajpurkar et al., 2016).", "startOffset": 18, "endOffset": 42}, {"referenceID": 18, "context": "Powerful statistical models, including deep learning models (also termed as readers), have been proposed for RC, most of which employ the following recipe: (1) Tokens in the document and question are represented using word vectors obtained from a lookup table (either initialized randomly, or from a pre-trained source such as GloVe (Pennington et al., 2014)).", "startOffset": 333, "endOffset": 358}, {"referenceID": 9, "context": "model such as LSTM (Hochreiter and Schmidhuber, 1997), augmented with an attention mechanism (Bahdanau et al.", "startOffset": 19, "endOffset": 53}, {"referenceID": 0, "context": "model such as LSTM (Hochreiter and Schmidhuber, 1997), augmented with an attention mechanism (Bahdanau et al., 2014), updates these vectors to produce contextual representations.", "startOffset": 93, "endOffset": 116}, {"referenceID": 1, "context": "Corpus 1: BookTest dataset (Bajgar et al., 2016), Corpus 2: Wikipedia + Gigaword.", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "As a concrete example, in Figure 1 we compare the performance of two RC models\u2014Stanford Attentive Reader (AR) (Chen et al., 2016) and Gated Attention (GA) Reader (Dhingra et al.", "startOffset": 110, "endOffset": 129}, {"referenceID": 5, "context": ", 2016) and Gated Attention (GA) Reader (Dhingra et al., 2016)\u2014on the Who-Did-What dataset (Onishi et al.", "startOffset": 40, "endOffset": 62}, {"referenceID": 17, "context": ", 2016)\u2014on the Who-Did-What dataset (Onishi et al., 2016), initialized with word embeddings trained on different corpora.", "startOffset": 36, "endOffset": 57}, {"referenceID": 2, "context": "(Chen et al., 2016; Shen et al., 2016)) is to replace infrequent words during training with a special token UNK, and use this to-", "startOffset": 0, "endOffset": 38}, {"referenceID": 21, "context": "(Chen et al., 2016; Shen et al., 2016)) is to replace infrequent words during training with a special token UNK, and use this to-", "startOffset": 0, "endOffset": 38}, {"referenceID": 16, "context": "Many datasets aimed at measuring the performance of RC have been proposed (Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 74, "endOffset": 119}, {"referenceID": 23, "context": "Many datasets aimed at measuring the performance of RC have been proposed (Nguyen et al., 2016; Trischler et al., 2016).", "startOffset": 74, "endOffset": 119}, {"referenceID": 8, "context": ", 2016) constructed from news stories, and the Children\u2019s Book Test (CBT) (Hill et al., 2015) constructed from children\u2019s books.", "startOffset": 74, "endOffset": 93}, {"referenceID": 4, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 15, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 22, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 21, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 12, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 6, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 24, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 25, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 20, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 26, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 27, "context": "on deep learning have been proposed (Cui et al., 2016; Munkhdalai and Yu, 2016; Sordoni et al., 2016; Shen et al., 2016; Kobayashi et al., 2016; Henaff et al., 2016; Wang and Jiang, 2016; Wang et al., 2016; Seo et al., 2016; Xiong et al., 2016; Yu et al., 2016).", "startOffset": 36, "endOffset": 261}, {"referenceID": 10, "context": "Hence, we instead select the answer from the document representation itself, followed by an attention sum mechanism (Kadlec et al., 2016).", "startOffset": 116, "endOffset": 137}, {"referenceID": 3, "context": "architecture which updates the representation of document tokens through multiple bidirectional GRU layers (Cho et al., 2014).", "startOffset": 107, "endOffset": 125}, {"referenceID": 5, "context": "We use the publicly available code2 with the default hyperparameter settings of (Dhingra et al., 2016), detailed in Appendix B.", "startOffset": 80, "endOffset": 102}, {"referenceID": 18, "context": "The two most popular methods for inducing word embeddings from text corpora are GloVe (Pennington et al., 2014) and word2vec (Mikolov et al.", "startOffset": 86, "endOffset": 111}, {"referenceID": 14, "context": ", 2014) and word2vec (Mikolov et al., 2013).", "startOffset": 21, "endOffset": 43}, {"referenceID": 13, "context": "This is by no means an optimal choice, in fact previous studies (Levy et al., 2015) have shown that hyperparameter choices may have a significant impact on downstream performance.", "startOffset": 64, "endOffset": 83}, {"referenceID": 13, "context": "It is difficult to claim that one method is better than the other, since previous studies (Levy et al., 2015) have shown that these methods are sensitive to hyperparameter tuning.", "startOffset": 90, "endOffset": 109}, {"referenceID": 2, "context": "(Chen et al., 2016; Shen et al., 2016)) for constructing this vocabulary is to decide on a minimum frequency threshold n (typically 510) and set V = V T n .", "startOffset": 0, "endOffset": 38}, {"referenceID": 21, "context": "(Chen et al., 2016; Shen et al., 2016)) for constructing this vocabulary is to decide on a minimum frequency threshold n (typically 510) and set V = V T n .", "startOffset": 0, "endOffset": 38}, {"referenceID": 5, "context": "A third approach, used in (Dhingra et al., 2016), is motivated by the fact that many of the RC models rely on computing fine-grained similarity between document and query tokens.", "startOffset": 26, "endOffset": 48}], "year": 2017, "abstractText": "The focus of past machine learning research for Reading Comprehension tasks has been primarily on the design of novel deep learning architectures. Here we show that seemingly minor choices made on (1) the use of pre-trained word embeddings, and (2) the representation of outof-vocabulary tokens at test time, can turn out to have a larger impact than architectural choices on the final performance. We systematically explore several options for these choices, and provide recommendations to researchers working in this area.", "creator": "LaTeX with hyperref package"}}}