{"id": "1703.01030", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2017", "title": "Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction", "abstract": "recently mainly demonstrated close - piece - way - art operations in sequential numerical logic phenomena ( g. g., impulse control, sequential prediction ) with tandem vertical network search. modeling also has access to near - optimal oracles that achieve excellent performance on random output during training. we evaluate that task - - - a policy objective extension of the imitation learning ( il ) technique simultaneously ( ross & amp ; bagnell, 1965 ) - - - can leverage such an oracle nor achieve faster input productive solutions drawing less training data than similarly less - predictable intelligent intelligence ( spp ) technique. using additional feedforward and recurrent neural network graphs, we conduct stochastic computation operations illustrating a sequential query task, counter - parsing on raw sensor data, thousands of then on various high resolution robotics control tools. we also provide a complementary theoretical study test design! alone we usually tolerate up to 160 nonlinear sample complexity combining learning with diversity than competitors rl algorithms, which backs our prior methodology. this results and theory specify that neither proposed candidate can achieve superior performance with respect versus the complexity when the demonstrator is sub - optimal.", "histories": [["v1", "Fri, 3 Mar 2017 04:12:03 GMT  (283kb,D)", "http://arxiv.org/abs/1703.01030v1", "17 pages"]], "COMMENTS": "17 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["wen sun", "arun venkatraman", "geoffrey j gordon", "byron boots", "j andrew bagnell"], "accepted": true, "id": "1703.01030"}, "pdf": {"name": "1703.01030.pdf", "metadata": {"source": "META", "title": "Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction", "authors": ["Wen Sun", "Arun Venkatraman", "Geoffrey J. Gordon", "Byron Boots", "J. Andrew Bagnell"], "emails": ["WENSUN@CS.CMU.EDU", "ARUNVENK@CS.CMU.EDU", "GGORDON@CS.CMU.EDU", "BBOOTS@CC.GATECH.EDU", "DBAGNELL@RI.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "A fundamental challenge in artificial intelligence, robotics, and language processing is to reason, plan, and make a se-\nquence of decisions to minimize accumulated cost, achieve a long-term goal, or optimize for a loss acquired only after many predictions. Reinforcement Learning (RL), especially deep RL, has dramatically advanced the state of the art in sequential decision making in high-dimensional robotics control tasks as well as in playing video and board games (Schulman et al., 2015; Silver et al., 2016). Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016). Often in sequential prediction tasks, future predictions from the learner are dependent on the history of previous predictions; thus, a poor prediction early on can yield high accumulated loss (cost) for future predictions. Viewing the predictor as a policy \u03c0, deep RL algorithms are able to reason about the future accumulated cost in a sequential decision making process whether it is a traditional robotics control problem or a sequential structured prediction task.\nIn contrast with reinforcement learning methods, wellknown imitation learning (IL) and sequential prediction algorithms such as SEARN (Daume\u0301 III et al., 2009), DaD (Venkatraman et al., 2015), AggreVaTe (Ross & Bagnell, 2014), and LOLS (Chang et al., 2015b) reduce the sequence prediction problem to supervised learning by leveraging one special property of the sequence prediction problem: at training time we usually have a (near) optimal costto-go oracle. At any point along the sequential prediction process (i.e. state or partially completed sequential prediction), the oracle is able to select the next (near)-best action.\nConcretely, the above methods assume access to an oracle1 that provides an optimal or near-optimal action and the future accumulated loss Q\u2217, also called the cost-to-\n1Expert, demonstrator, and oracle are used interchangeably.\nar X\niv :1\n70 3.\n01 03\n0v 1\n[ cs\n.L G\n] 3\nM ar\ngo. For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.g., ground truth). Similarly, for sequential prediction problems, an oracle can be constructed by optimization (e.g., beam search) or by a clairvoyant greedy algorithm (Daume\u0301 III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.g., cumulative reward, IoU, Unlabeled Attachment Score, BLEU) given the training data\u2019s ground truth.\nSuch an oracle, however, is only available during training time (e.g., when there is access to ground truth). Thus, the goal of IL is to learn a policy \u03c0\u0302, with the help of the oracle (\u03c0\u2217, Q\u2217) during the training session, such that \u03c0\u0302 achieves similar quality performance at test time when the oracle is unavailable. In contrast to IL, reinforcement learning methods often initialize with an random policy \u03c00 or cost-to-go (accumulated loss)Q0 predictor which may be far from the optimal. The optimal policy (or cost-to-go) must be found through a tradeoff of dithering, directed exploration, and exploitation.\nThe existence of oracle can be exploited to alleviate blind learning by trial and error: one can imitate the oracle to speed up learning process by significantly reducing exploration. A classic IL method is to collect data from running the demonstrator or oracle and train a regressor or classifier via supervised learning. These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle. A pernicious problem with these methods is that they require the training and test data to be sampled from the same distribution. This is very difficult to enforce in practice, and, as a result, policies learned by these methods can fail spectacularly in theory and in practice (Ross & Bagnell, 2010). Interactive approaches to IL such as SEARN (Daume\u0301 III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross & Bagnell, 2014) interleave learning and testing procedures to overcome the data mismatch issue and, as a result, work well in practical applications. Furthermore, these interactive approaches can provide strong theoretical guarantees between training time loss and test time performance through a reduction to no-regret online learning.\nIn this work, we introduce AggreVaTeD, a differentiable version of AggreVaTe (Aggregate Values to Imitate (Ross & Bagnell, 2014)) which extends interactive IL for use in sequential prediction and challenging continuous robot control tasks. We provide two gradient update procedures:\na regular gradient update developed from Online Gradient Descent (OGD) (Zinkevich, 2003) and a natural gradient update (Kakade, 2002; Bagnell & Schneider, 2003) which we show is closely related to Exponential Gradient Descent (EG), another popular no-regret algorithm that enjoys an almost dimension-free property (Bubeck et al., 2015).2\nAggreVaTeD leverages the oracle to learn rich polices that can be represented by complicated non-linear function approximators. Our experiments with deep neural networks on various robotics control simulators and on a dependency parsing sequential prediction task show that AggreVaTeD can achieve expert-level performance and even super-expert performance when the oracle is sub-optimal, a result rarely achieved by non-interactive IL approaches. The differentiable nature of AggreVaTeD additionally allows us to employ LSTM-based policies to handle partially observable settings (e.g., observe only partial robot state). Empirical results demonstrate that by leveraging an oracle, IL can learn much faster than RL in practice.\nIn addition to providing a set of practical algorithms, we develop a comprehensive theoretical study of IL on discrete MDPs. We construct an MDP that demonstrates exponentially better sample efficiency for IL than any RL algorithm. For general discrete MDPs, we provide a regret upper bound for AggreVaTeD with EG, which shows IL can learn dramatically faster than RL. We provide a regret lower bound for any IL algorithm, which demonstrates that AggreVaTeD with EG is near-optimal. Our experimental and the theoretical results support the proposition:\nImitation Learning is a more effective strategy than Reinforcement Learning for sequential prediction with near optimal cost-to-go oracles."}, {"heading": "2. Preliminaries", "text": "In order to develop algorithms that reason about longterm decision making, it is convenient to cast the problem into the Markov Decision Process (MDP) framework. The MDP framework consists of a set of states, actions (that come from a policy), cost (loss), and a model that transitions states given actions. For robotics control problems, the robot\u2019s configuration is the state, the controls (e.g., joint torques) are the actions, and the cost is related to achieving a task (e.g., distance walked). Though more nuanced, most sequential predictions can be cast into the same framework (Daume\u0301 III et al., 2009). The actions are the learner\u2019s (e.g., RNN\u2019s) predictions. The state is then the result of all the predictions made so far (e.g., the dependency tree constructed so far or the words translated so far). The cumulative cost is the performance metric such as (negative) UAS,\n2i.e., the regret bound depends on poly-log of the dimension of parameter space.\nreceived at the end (horizon) after the final prediction.\nFormally, a finite-horizon Markov Decision Process (MDP) is defined as (S,A, P, C, \u03c10, H). Here, S is a set of S many states and A is a set of A actions; given time step t, Pt is the transition dynamics such that for any st \u2208 S, st+1 \u2208 S, at \u2208 A, Pt(st+1|st, at) is the probability of transiting to state st+1 from state st by taking action at at step t; C is the cost distribution such that a cost ct at step t is sampled from Ct(\u00b7|st, at). Finally, we denote c\u0304t as the expected cost, \u03c10 as the initial distribution of states, and H \u2208 N+ as the finite horizon (max length) of the MDP.\nWe define a stochastic policy \u03c0 such that for any state s \u2208 S, \u03c0(\u00b7|s) \u2208 \u2206(A), where \u2206(A) is a A-dimension simplex, conditioned on state s. \u03c0(a|s) \u2208 [0, 1] outputs the probability of taking action a at state s. The distribution of trajectories \u03c4 = (s1, a1, . . . , aH\u22121, sH) is deterministically dependent on \u03c0 and the MDP, and is defined as\n\u03c1\u03c0(\u03c4) = \u03c10(s1) H\u220f t=2 \u03c0(at\u22121|st\u22121)Pt\u22121(st|st\u22121, at\u22121).\nThe distribution of the states at time step t, induced by running the policy \u03c0 until t, is defined \u2200st:\nd\u03c0t (st) = \u2211\n{si,ai}i\u2264t\u22121\n\u03c10(s1) t\u22121\u220f i=1 \u03c0(ai|si)Pi(si+1|si, ai).\nNote that the summation above can be replaced by an integral if the state or action space is continuous. The average state distribution d\u0304\u03c0(s) = \u2211H t=1 d \u03c0 t (s)/H .\nThe expected average cost of a policy \u03c0 can be defined with respect to \u03c1\u03c0 or {d\u03c0t }:\n\u00b5(\u03c0) = E \u03c4\u223c\u03c1\u03c0 [ H\u2211 t=1 c\u0304t(st, at)] = H\u2211 t=1 E s\u223cd\u03c0t (s),a\u223c\u03c0(a|s) [c\u0304t(s, a)].\nWe define the state-action value Q\u03c0t (s, a) (i.e., cost-to-go) for policy \u03c0 at time step t as:\nQ\u03c0t (st, at) = c\u0304t(st, at) + E s\u223cPt(\u00b7|st,at),a\u223c\u03c0(\u00b7|s) Q\u03c0t+1(s, a).\nwhere the expectation is taken over the randomness of the policy \u03c0 and the MDP.\nWe define \u03c0\u2217 as the expert policy (e.g., human demonstrators, search algorithms equipped with ground-truth) and Q\u2217t (s, a) as the expert\u2019s cost-to-go oracle (note \u03c0\n\u2217 may not be optimal, i.e., \u03c0\u2217 6\u2208 arg min\u03c0 \u00b5(\u03c0)). Throughout the paper, we assume Q\u2217t (s, a) is known or can be estimated without bias (e.g., by rolling out \u03c0\u2217: starting from state s, applying action a, and then following \u03c0\u2217 for H \u2212 t steps).\nWhen \u03c0 is represented by a function approximator, we use the notation \u03c0\u03b8 to represent the policy parametrized by\n\u03b8 \u2208 Rd: \u03c0(\u00b7|s; \u03b8). In this work we specifically consider optimizing policies in which the parameter dimension d may be large. We also consider the partially observable setting in our experiments, where the policy \u03c0(\u00b7|o1, a1, ..., ot; \u03b8) is defined over the whole history of partial observations and actions (ot is generated from the hidden state st). We use an LSTM-based policy (Duan et al., 2016) where the LSTM\u2019s hidden states provide a compressed feature of the history."}, {"heading": "3. Differentiable Imitation Learning", "text": "Policy based imitation learning aims to learn a policy \u03c0\u0302 that approaches the performance of the expert \u03c0\u2217 in testing time when \u03c0\u2217 is not available anymore. In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction. To do this, we leverage the reduction of IL and sequential prediction to online learning as shown in (Ross & Bagnell, 2014) to learn policies represented by expressive differentiable function approximators.\nThe fundamental idea in Ross & Bagnell (2014) is to use a no-regret online learner to update policies using the following loss function at each episode n:\n`n(\u03c0) = 1\nH H\u2211 t=1 E st\u223cd\u03c0nt [ E a\u223c\u03c0(\u00b7|st) [Q\u2217t (st, a)] ] . (1)\nThe loss function intuitively encourages the learner to find a policy that minimize the expert\u2019s cost-to-go under the state distribution resulting from the current learned policy \u03c0n. Specifically, Ross & Bagnell (2014) suggest an algorithm named AggreVaTe (Aggregate Values to Imitate) that uses Follow-the-Leader (FTL) (Shalev-Shwartz et al., 2012) to update policies:\u03c0n+1 = arg min\u03c0\u2208\u03a0 \u2211n i=1 `n(\u03c0), where \u03a0 is a pre-defined convex policy set. When `n(\u03c0) is strongly convex with respect to \u03c0 and \u03c0\u2217 \u2208 \u03a0, after N iterations AggreVaTe with FTL can find a policy \u03c0\u0302:\n\u00b5(\u03c0\u0302) \u2264 \u00b5(\u03c0\u2217)\u2212 N +O(ln(N)/N), (2)\nwhere N = [ \u2211N n=1 `n(\u03c0 \u2217)\u2212min\u03c0 \u2211N n=1 `n(\u03c0)]/N . Note that N \u2265 0 and the above inequality indicates that \u03c0\u0302 can outperform \u03c0\u2217 when \u03c0\u2217 is not (locally) optimal (i.e., n > 0). Our experimental results support this observation.\nA simple implementation of AggreVaTe that aggregates the values (as the name suggests) will require an exact solution to a batch optimization procedure in each episode. When \u03c0 is represented by large, non-linear function approximators, the arg min procedure generally takes more and more computation time as n increases.\nOnline Mirror Descent (OMD) (Shalev-Shwartz et al., 2012) are popular for online learning due to its efficiency.\nTherefore we consider two special cases of OMD for optimizing sequence of losses {`n(\u03c0)}n: Online Gradient Descent (OGD) (Zinkevich, 2003) and Exponential Gradient Descent (EG) (Shalev-Shwartz et al., 2012), which lead to a regular stochastic policy gradient descent algorithm and a natural policy gradient algorithm, respectively. Also, when applying OGD and EG to {`n(\u03c0)}n, one can show that Eq. 2 will hold (with O(1/ \u221a N)), as long as `n(\u03c0) is convex with respect to \u03c0."}, {"heading": "3.1. Online Gradient Descent", "text": "For discrete actions, the gradient of `n(\u03c0\u03b8) (Eq. 1) with respect to the parameters \u03b8 of the policy can be computed as\n\u2207\u03b8`n(\u03b8) = 1\nH H\u2211 t=1 E st\u223cd \u03c0\u03b8n t \u2211 a \u2207\u03b8\u03c0(a|st; \u03b8)Q\u2217t (st, a).\n(3)\nFor continuous action spaces, we cannot simply replace the summation by integration since in practice it is impossible to evaluate Q\u2217t (s, a) for infinitely many a, so, instead, we use importance weighting to re-formulate `n (Eq. 1) as\n`n(\u03c0\u03b8) = 1\nH H\u2211 t=1 E s\u223cd \u03c0\u03b8n t ,a\u223c\u03c0(\u00b7|s;\u03b8n) \u03c0(a|s; \u03b8) \u03c0(a|s; \u03b8n) Q\u2217t (s, a)\n= 1\nH E\n\u03c4\u223c\u03c1\u03c0\u03b8n H\u2211 t=1 \u03c0(at|st; \u03b8) \u03c0(at|st; \u03b8n) Q\u2217t (st, at). (4)\nSee Appendix A for the derivation of the above equation. With this reformulation, the gradient with respect to \u03b8 is\n\u2207\u03b8`n(\u03b8) = 1\nH E\n\u03c4\u223c\u03c1\u03c0\u03b8n H\u2211 t=1 \u2207\u03b8\u03c0(at|st; \u03b8) \u03c0(at|st; \u03b8n) Q\u2217t (st, at). (5)\nThe above gradient computation enables a very efficient update procedure with online gradient descent: \u03b8n+1 = \u03b8n \u2212 \u03b7n\u2207\u03b8`n(\u03b8)|\u03b8=\u03b8n , where \u03b7n is the learning rate."}, {"heading": "3.2. Policy Updates with Natural Gradient Descent", "text": "We derive a natural gradient update procedure for imitation learning inspired by the success of natural gradient descent in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015). First, we show that Exponential Gradient Descent (EG) can be leveraged to speed up imitation learning in discrete MDPs. Then we extend EG to continuous MDPs, where we show that, with three steps of approximation, EG leads to a natural gradient update procedure."}, {"heading": "3.2.1. EXPONENTIAL GRADIENT IN DISCRETE MDPS", "text": "For notational simplicity, for each state s \u2208 S, we represent the policy \u03c0(\u00b7|s) as a discrete probability vector\n\u03c0s \u2208 \u2206(A). We also represent d\u03c0t as a S-dimension probability vector from S-d simplex, consisting of d\u03c0t (s),\u2200s \u2208 S. For each s, we use Q\u2217t (s) to denote the A-dimension vector consisting of the state-action cost-to-go Q\u2217t (s, a) for all a \u2208 A. With this notation, the loss function `n(\u03c0) from Eq. 1 can now be written as: `n(\u03c0) = 1 H \u2211H t=1 \u2211 s\u2208S d \u03c0n t (s)(\u03c0\ns \u00b7Q\u2217t (s)), where a \u00b7 b represents the inner product between vectors a and b. Exponential Gradient updates \u03c0 as follows: \u2200s \u2208 S,\n\u03c0n+1 = arg min \u03c0s\u2208\u2206(A),\u2200s\u2208S\n1\nH H\u2211 t=1 \u2211 s\u2208S d\u03c0nt (s) ( \u03c0s \u00b7Q\u2217t (s) ) + \u2211 s\u2208S d\u0304\u03c0n(s) \u03b7n,s KL(\u03c0s\u2016\u03c0sn), (6)\nwhere KL(q\u2016p) is the KL-divergence between two probability distributions q and p. This leads to the following closed-form update:\n\u03c0sn+1[i] = \u03c0sn[i] exp\n( \u2212 \u03b7n,sQ\u0303es[i] )\u2211|A| j=1 \u03c0 s n[j] exp ( \u2212 \u03b7n,sQ\u0303es[j]\n) , i \u2208 [|A|], (7)\nwhere Q\u0303es = \u2211H t=1 d \u03c0n t (s)Q \u2217 t (s)/(Hd\u0304\n\u03c0n(s)). We refer readers to (Shalev-Shwartz et al., 2012) or Appendix B for the derivations of the above closed-form updates."}, {"heading": "3.2.2. CONTINUOUS MDPS", "text": "We now consider how to update the parametrized policy \u03c0\u03b8 for continuous MDPs. Replacing summations by integrals, Eq. 6 can be written as:\n\u03b8 = arg min \u03b8\n1\nH H\u2211 t=1 E s\u223cd \u03c0\u03b8n t E a\u223c\u03c0(\u00b7|s;\u03b8) [Q\u2217t (s, a)]\n+ E s\u223cd\u0304\u03c0\u03b8n KL(\u03c0\u03b8||\u03c0\u03b8n)/\u03b7n. (8)\nIn order to solve for \u03b8 from Eq. 8, we apply several approximations. We first approximate `n(\u03b8) (the first part of the RHS of the above equation) by its first-order Taylor expansion: `n(\u03b8) \u2248 `n(\u03b8n) +\u2207\u03b8n`n(\u03b8n) \u00b7 (\u03b8\u2212 \u03b8n). When \u03b8 and \u03b8n are close, this is a valid local approximation.\nSecond, we replace KL(\u03c0\u03b8||\u03c0\u03b8n) by KL(\u03c0\u03b8n ||\u03c0\u03b8), which is a local approximation since KL(q||p) and KL(p||q) are equal up to the second order (Kakade & Langford, 2002; Schulman et al., 2015).\nThird, we approximate KL(\u03c0\u03b8n ||\u03c0\u03b8) by a second-order Taylor expansion around \u03b8n, such that we can approximate the penalization using the Fisher information matrix:\nE s\u223cd\u0304\u03c0\u03b8n KL(\u03c0\u03b8n ||\u03c0\u03b8) \u2248 (1/2)(\u03b8 \u2212 \u03b8n)T I(\u03b8n)(\u03b8 \u2212 \u03b8n),\nwhere the Fisher information matrix I(\u03b8n) = Es,a\u223cd\u0304\u03c0\u03b8n \u03c0\u03b8n (a|s) ( \u2207\u03b8n log(\u03c0\u03b8n(a|s)) )( \u2207\u03b8n log(\u03c0\u03b8n(a|s) )T .\nInserting these three approximations into Eq. 8, and solving for \u03b8, we reach the following update rule \u03b8n+1 = \u03b8n \u2212 \u03b7nI(\u03b8n)\u22121\u2207\u03b8`n(\u03b8)|\u03b8=\u03b8n , which is similar to the natural gradient update rule developed in (Kakade, 2002) for the RL setting. Bagnell & Schneider (2003) provided an equivalent representation for Fisher information matrix:\nI(\u03b8n) = 1\nH2 E \u03c4\u223c\u03c1\u03c0\u03b8n \u2207\u03b8n log(\u03c1\u03c0\u03b8n (\u03c4))\u2207\u03b8n log(\u03c1\u03c0\u03b8n (\u03c4)) T ,\n(9)\nwhere \u2207\u03b8 log(\u03c1\u03c0\u03c4 (\u03c4)) is the gradient of the log likelihood of the trajectory \u03c4 which can be computed as\u2211H t=1\u2207\u03b8 log(\u03c0\u03b8(at|st)). In the remainder of the paper, we use this Fisher information matrix representation, which yields much faster computation of the descent direction \u03b4\u03b8, as we will explain in the next section."}, {"heading": "4. Sample-Based Practical Algorithms", "text": "In the previous section, we derived a regular gradient update procedure and a natural gradient update procedure for IL. Note that all of the computations of gradients and Fisher information matrices assumed it was possible to exactly compute expectations including Es\u223cd\u03c0 and Ea\u223c\u03c0(a|s). In this section, we provide practical algorithms where we approximate the gradients and Fisher information matrices using finite samples collected during policy execution."}, {"heading": "4.1. Gradient Estimation and Variance Reduction", "text": "We consider an episodic framework where given a policy \u03c0n at episode n, we roll out \u03c0n K times to collect K trajectories {\u03c4ni }, for i \u2208 [K], \u03c4ni = {s i,n 1 , a i,n 1 , ...}. For gradient \u2207\u03b8`n(\u03b8)|\u03b8=\u03b8n we can compute an unbiased estimate using {\u03c4ni }i\u2208[K]:\n\u2207\u0303\u03b8n = 1\nHK K\u2211 i=1 H\u2211 t=1 \u2211 a \u2207\u03b8n\u03c0\u03b8n(a|s i,n t )Q \u2217 t (s i,n t , a),\n(10)\n\u2207\u0303\u03b8n = 1\nHK K\u2211 i=1 H\u2211 t=1 \u2207\u03b8n\u03c0\u03b8n(a i,n t |s i,n t ) \u03c0\u03b8n(a i,n t |s i,n t ) Q\u2217t (s i,n t , a i,n t ).\n(11)\nfor discrete and continuous setting respectively.\nWhen we can compute V \u2217t (s) (e.g., minaQ \u2217 t (s, a)), we can replace Q\u2217t (s i,n t , a) in Eq. 10 and Eq. 11 by the state-action advantage function A\u2217t (s i,n t , a) = Q \u2217 t (s i,n t , a) \u2212 V \u2217t (s i,n t ), which leads to the following two unbiased and variance-\nreduced gradient estimations (Greensmith et al., 2004):\n\u2207\u0303\u03b8n = 1\nHK K\u2211 i=1 H\u2211 t=1 \u2211 a \u2207\u03b8n\u03c0\u03b8n(a|s i,n t )A \u2217 t (s i,n t , a),\n(12)\n\u2207\u0303\u03b8n = 1\nHK K\u2211 i=1 H\u2211 t=1 \u2207\u03b8n\u03c0\u03b8n(a i,n t |s i,n t ) \u03c0\u03b8n(a i,n t |s i,n t ) A\u2217t (s i,n t , a i,n t ),\n(13)\nwhere Eq. 12 is for discrete action and Eq. 13 for continuous action .\nThe Fisher information matrix (Eq. 9) is approximated as:\nI\u0303(\u03b8n) = 1\nH2K K\u2211 i=1 \u2207\u03b8n log(\u03c1\u03c0\u03b8n (\u03c4i))\u2207\u03b8n log(\u03c1\u03c0\u03b8n (\u03c4i)) T = SnS T n , (14)\nwhere, for notation simplicity, we denote Sn as a d\u00d7K matrix where the i\u2019s th column is\u2207\u03b8n log(\u03c1\u03c0\u03b8n (\u03c4i))/(H \u221a K). Namely the Fisher information matrix is represented by a sum of K rank-one matrices. For large policies represented by neural networks, K d, and hence I\u0303(\u03b8n) a low rank matrix. One can find the descent direction \u03b4\u03b8n by solving the linear system SnS T n \u03b4\u03b8n = \u2207\u0303\u03b8n for \u03b4\u03b8n using Conjugate Gradient (CG) with a fixed number of iterations, which is equivalent to solving the above linear systems using the Partial Least Square (Phatak & de Hoog, 2002). This approach is used in TRPO (Schulman et al., 2015). The difference is that our representation of the Fisher matrix is in the form of SnSTn and in CG we never need to explicitly compute or store SnSTn which requires d2 space and time. Instead, we only compute and store Sn (O(Kd)) and the total computational time is still O(K2d). The learning-rate for natural gradi-\nent descent can be chosen as \u03b7n = \u221a \u03b4KL/(\u2207\u0303T\u03b8n\u03b4\u03b8n), such that KL(\u03c1\u03c0\u03b8n+1 (\u03c4)\u2016\u03c1\u03c0\u03b8n (\u03c4)) \u2248 \u03b4KL \u2208 R +"}, {"heading": "4.2. Differentiable Imitation Learning: AggreVaTeD", "text": "We present the differentiable imitation learning framework AggreVaTeD, in Alg. 1. At every iteration n, the roll-in policy \u03c0\u0302n is a mix of the expert policy \u03c0\u2217 and the current policy \u03c0\u03b8n , with mixing rate \u03b1 (\u03b1n \u2192 0, n \u2192 \u221e): at every step, with probability \u03b1, \u03c0\u0302n picks \u03c0\u2217 and else \u03c0\u03b8n . This mixing strategy with decay rate was first introduced in (Ross et al., 2011) for IL, and later on was used in sequence prediction (Bengio et al., 2015). In Line 6 one can choose Eq. 10 or the corresponding variance reduced estimation Eq. 12 (Eq. 11 and Eq. 13 for continuous actions) to perform regular gradient descent, and choose CG to perform natural gradient descent. Compared with previous well-known IL and sequential prediction algorithms (Ross\nAlgorithm 1 AggreVaTeD (Differentiable AggreVaTe) 1: Input: The given MDP and expert \u03c0\u2217. Learning rate {\u03b7n}. Schedule rate {\u03b1i}, \u03b1n \u2192 0, n\u2192\u221e.\n2: Initialize policy \u03c0\u03b81 (either random or supervised learning). 3: for n = 1 to N do 4: Mixing policies: \u03c0\u0302n = \u03b1n\u03c0\u2217 + (1\u2212 \u03b1n)\u03c0\u03b8n . 5: Starting from \u03c10, roll in by executing \u03c0\u0302n on the given MDP to generate K trajectories {\u03c4ni }. 6: Using Q\u2217 and {\u03c4ni }i, compute the descent direction \u03b4\u03b8n (Eq. 10, Eq. 11, Eq. 12, Eq. 13, or CG). 7: Update: \u03b8n+1 = \u03b8n \u2212 \u03b7n\u03b4\u03b8n . 8: end for 9: Return: the best hypothesis \u03c0\u0302 \u2208 {\u03c0n}n on validation.\net al., 2011; Ross & Bagnell, 2014; Chang et al., 2015b), AggreVaTeD is extremely simple: we do not need to perform any Data Aggregation (i.e., we do not need to store all {\u03c4i}i from all previous iterations); the computational complexity of each episode scales as O(d).\nWhen we use non-linear function approximators to represent the polices, the analysis of AggreVaTe from (Ross & Bagnell, 2014) will not hold, since the loss function `n(\u03b8) is not convex with respect to parameters \u03b8. Nevertheless, as we will show in experiments, in practice AggreVaTeD is still able to learn a policy that is competitive with, and sometimes superior to the oracle\u2019s performance."}, {"heading": "5. Quantify the Gap: An Analysis of IL vs RL", "text": "How much faster can IL learn a good policy than RL? In this section we quantify the gap on discrete MDPs when IL can (1) query for an optimalQ\u2217 or (2) query for a noisy but unbiased estimate ofQ\u2217. To measure the speed of learning, we look at the cumulative regret of the entire learning process, defined as RN = \u2211N n=1(\u00b5(\u03c0n)\u2212 \u00b5(\u03c0\u2217)). A smaller regret rate indicates faster learning. Throughout this section, we assume the expert \u03c0\u2217 is optimal. We consider finite-horizon, episodic IL and RL algorithms."}, {"heading": "5.1. Exponential Gap", "text": "We consider an MDPM shown in Fig. 1 which is a depthK binary tree-structure with S = 2K \u2212 1 states and two\nactions al, ar: go-left and go-right. The transition is deterministic and the initial state s0 (root) is fixed. The cost for each non-leaf state is zero; the cost for each leaf is i.i.d sampled from a given distribution (possibly different distributions per leaf). Below we show that forM, IL can be exponentially more sample efficient than RL.\nTheorem 5.1. ForM, the regret RN of any finite-horizon, episodic RL algorithm is at least:\nE[RN ] \u2265 \u2126( \u221a SN). (15)\nThe expectation is with respect to random generation of cost and internal randomness of the algorithm. However, for the same MDPM, with the access to Q\u2217, we show IL can learn exponentially faster:\nTheorem 5.2. For the MDPM, there exists a policy class such that AggreVaTe with FTL that can achieve the following regret bound:\nRN \u2264 O(ln (S)). (16)\nFig. 1 illustrates the intuition behind the theorem. Assume during the first episode, the initial policy \u03c01 picks the rightmost trajectory (bold black) to explore and the algorithm queries from oracle that for s0 we have Q\u2217(s0, al) < Q\u2217(s0, ar), it immediately learns that the optimal policy will go left (black arrow) at s0. Hence the algorithm does not have to explore the right sub-tree (dotted circle).\nNext we consider a more difficult setting where one can only query for a noisy but unbiased estimate ofQ\u2217 (e.g., by rolling out \u03c0\u2217 finite number of times). The above halving argument will not apply since deterministically eliminating nodes based on noisy estimates might permanently remove good trajectories. However, IL can still achieve a poly-log regret with respect to S, even in the noisy setting:\nTheorem 5.3. With only access to unbiased estimate ofQ\u2217, for the MDPM, AggreVaTeD with EG that can achieve the following regret with probability at least 1\u2212 \u03b4:\nRN \u2264 O ( ln(S)( \u221a ln(S)N + \u221a ln(2/\u03b4)N) ) . (17)\nThe detailed proofs of the above three theorems can be found in Appendix D,E,F respectively. In summary, for MDPM, IL is is exponentially faster than RL."}, {"heading": "5.2. Polynomial Gap and Near-Optimality", "text": "We next quantify the gap in general discrete MDPs and also show that AggreVaTeD is near-optimal. We consider the harder case where we can only access an unbiased estimate of Q\u2217t , for any t and state-action pair. The policy \u03c0 is represented as a set of probability vectors \u03c0s,t \u2208 \u2206(A), for all s \u2208 S and t \u2208 [H]: \u03c0 = {\u03c0s,t}s\u2208S,t\u2208[H].\nTheorem 5.4. With access to unbiased estimates of Q\u2217t , AggreVaTeD with EG achieves the regret upper bound:\nRN \u2264 O ( HQemax \u221a S ln(A)N ) . (18)\nHere Qemax is the maximum cost-to-go of the expert. 3 The total regret shown in Eq. 18 allows us to compare IL algorithms to RL algorithms. For example, the Upper Confidence Bound (UCB) based, near-optimal optimistic RL algorithms from (Jaksch et al., 2010), specifically designed for efficient exploration, admit regret O\u0303(HS \u221a HAN), leading to a gap of approximately \u221a HAS compared to the regret bound of imitation learning shown in Eq. 18.\nWe also provide a lower bound on RN for H = 1 case which shows the dependencies on N,A, S are tight:\nTheorem 5.5. There exists an MDP (H=1), with only access to unbiased estimate ofQ\u2217, any finite-horizon episodic imitation learning algorithm must have:\nE[RN ] \u2265 \u2126( \u221a S ln(A)N). (19)\nThe proofs of the above two theorems regarding general MDPs can be found at Appendix G,H. In summary for discrete MDPs, one can expect at least a polynomial gap and a possible exponential gap between IL and RL."}, {"heading": "6. Experiments", "text": "We evaluate our algorithms on robotics simulations from OpenAI Gym (Brockman et al., 2016) and on Handwritten Algebra Dependency Parsing (Duyck & Gordon, 2015). We report reward instead of cost, since OpenAI Gym by default uses reward and dependency parsing aims to maximize UAS score. As our approach only promises there exists a policy among all of the learned polices that can perform as well as the expert, we report the performance of the best policy so far: max{\u00b5(\u03c01), ..., \u00b5(\u03c0i)}. For regular gradient descent, we use ADAM (Kingma & Ba, 2014) which is a first-order no-regret algorithm, and for natural gradient, we use CG to compute the descent direction. For RL we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TNPG) (Duan et al., 2016)."}, {"heading": "6.1. Robotics Simulations", "text": "We consider CartPole Balancing, Acrobot Swing-up, Hopper and Walker. For generating an expert, similar to previous work (Ho & Ermon, 2016), we used a Deep Q-Network (DQN) to generate Q\u2217 for CartPole and Acrobot (e.g., to simulate the settings where Q\u2217 is available), while using the publicly available TRPO implementation to generate\n3Here we assume Qemax is a constant compared to H . If Qemax = \u0398(H), then the expert is no better than a random policy of which the cost-to-go is around \u0398(H).\n\u03c0\u2217 for Hopper and Walker to simulate the settings where one has to estimate Q\u2217 by Monte-Carlo roll outs \u03c0\u2217.\nDiscrete Action Setting We use a one-layer (16 hidden units) neural network with ReLu activation functions to represent the policy \u03c0 for the Cart-pole and Acrobot benchmarks. The value function Q\u2217 is obtained from the DQN (Mnih et al., 2015) and represented by a multi-layer fully connected neural network. The policy \u03c0\u03b81 is initialized with common ReLu neural network initialization techniques. For the scheduling rate {\u03b1i}, we set all \u03b1i = 0: namely we did not roll-in using the expert\u2019s actions during training. We set the number of roll outs K = 50 and horizon H = 500 for CartPole and H = 200 for Acrobot.\nFig. 4a and 4b shows the performance averaged over 10 random trials of AggreVaTeD with regular gradient descent and natural gradient descent. Note that AggreVaTeD outperforms the experts\u2019 performance significantly: Natural gradient surpasses the expert by 5.8% in Acrobot and 25% in Cart-pole. Also, for Acrobot swing-up, at horizon H = 200, with high probability a randomly initialized neural network policy won\u2019t be able to collect any reward signals. Hence the improvement rates of REINFORCE and TNPG are slow. In fact, we observed that for a short horizon such asH = 200, REINFORCE and Truncated Natural Gradient often even fail to improve the policy at all (failed 6 times among 10 trials). On the contrary, AggreVaTeD does not suffer from the delayed reward signal issue, since the expert will collect reward signals much faster than a randomly initialized policy.\nFig. 2c shows the performance of AggreVaTeD with an LSTM policy (32 hidden states) in a partially observed setting where the expert has access to full states but the learner has access to partial observations (link positions). RL algorithms did not achieve any improvement while AggreVaTeD still achieved 92% of expert\u2019s performance.\nContinuous Action Setting We test our approaches on two robotics simulators with continuous actions: (1) the 2-d Walker and (2) the Hopper from the MuJoCo physics simulator. Following the neural network settings described in Schulman et al. (2015), the expert policy \u03c0\u2217 is obtained from TRPO with one hidden layer (64 hidden states), which is the same structure that we use to represent our policies \u03c0\u03b8. We set K = 50 and H = 100. We initialize \u03c0\u03b81 by collecting K expert demonstrations and then maximize the likelihood of these demonstrations (i.e., supervised learning). We use Eq. 11 instead of the variance reduced equation here since we need to use MC roll-outs to estimate V \u2217 (we simply use one roll-out to estimate Q\u2217).\nFig. 2d and 2e show the performance averaged over 5 random trials. Note that AggreVaTeD outperforms the expert in the Walker by 5.4% while achieving 97% of the expert\u2019s\nperformance in the Hopper problem. After 100 iterations, we see that by leveraging the help from experts, AggreVaTeD can achieve much faster improvement rate than the corresponding RL algorithms."}, {"heading": "6.2. Dependency Parsing on Handwritten Algebra", "text": "We consider a sequential prediction problem: transitionbased dependency parsing for handwritten algebra with raw image data (Duyck & Gordon, 2015). The parsing task for algebra is similar to the classic dependency parsing for natural language (Chang et al., 2015a) where the problem is modelled in the IL setting and the state-of-the-art is achieved by AggreVaTe with FTRL (using Data Aggregation). The additional challenge here is that the inputs are handwritten algebra symbols in raw images. We directly learn to predict parse trees from low level image features (Histogram of Gradient features (HoG)). During training, the expert is constructed using the ground-truth dependencies in training data. The full state s during parsing consists of three data structures: Stack, Buffer and Arcs, which store raw images of the algebraic symbols. Since the sizes of stack, buffer and arcs change during parsing, a common approach is to featurize the state s by taking the features of the latest three symbols from stack, buffer and arcs (e.g., (Chang et al., 2015a)). Hence the problem falls into the partially observable setting, where the feature o is extracted from state s and only contains partial information about s. The dataset consists of 400 sets of handwritten algebra equations. We use 80% for training, 10% for validation, and 10% for testing. We include an example of handwritten algebra equations and its dependency tree in Appendix I. Note that different from robotics simulators where at every episode one can get fresh data from the simulators, the dataset is fixed and sample efficiency is critical.\nThe RNN policy follows the design from (Sutskever et al., 2014). It consists of two LSTMs. Given a sequence of algebra symbols \u03c4 , the first LSTM processes one symbol at a time and at the end outputs its hidden states and memory (i.e., a summary of \u03c4 ). The second LSTM initializes its own hidden states and memory using the outputs of the first LSTM. At every parsing step t, the second LSTM takes the current partial observation ot (ot consists of features of the\nmost recent item from stack, buffer and arcs) as input, and uses its internal hidden state and memory to compute the action distribution \u03c0(\u00b7|o1, ..., ot, \u03c4) conditioned on history. We also tested reactive policies constructed as fully connected ReLu neural networks (NN) (one-layer with 1000 hidden states) that directly maps from observation ot to action a, where ot uses the most three recent items. We use variance reduced gradient estimations, which give better performance in practice. The performance is summarised in Table 1. Due to the partial observability of the problem, AggreVaTeD with a LSTM policy achieves significantly better UAS scores compared to the NN reactive policy and DAgger with a Kernelized SVM (Duyck & Gordon, 2015). Also AggreVaTeD with a LSTM policy achieves 97% of optimal expert\u2019s performance. Fig. 3 shows the improvement rate of regular gradient and natural gradient on both validation set and test set. Overall we observe that both methods have similar performance. Natural gradient achieves a better UAS score in validation and converges slightly faster on the test set but also achieves a lower UAS score on test set."}, {"heading": "7. Conclusion", "text": "We introduced AggreVaTeD, a differentiable imitation learning algorithm which trains neural network policies for sequential prediction tasks such as continuous robot control and dependency parsing on raw image data. We showed that in theory and in practice IL can learn much faster than RL with access to optimal cost-to-go oracles. The IL learned policies were able to achieve expert and sometimes super-expert levels of performance in both fully observable\nand partially observable settings. The theoretical and experimental results suggest that IL is significantly more effective than RL for sequential prediction with near optimal cost-to-go oracles."}, {"heading": "Appendix: Proofs and Detailed Bounds", "text": ""}, {"heading": "A. Derivation of Eq. 4", "text": "Starting from Eq. 1 with parametrized policy \u03c0\u03b8, we have:\n`n(\u03b8) = 1\nH H\u2211 t=1 E st\u223cd \u03c0\u03b8n t [ E at\u223c\u03c0(\u00b7|st;\u03b8) [Q\u2217t (st, at)] ] = 1\nH H\u2211 t=1 E st\u223cd \u03c0\u03b8n t [ \u222b a \u03c0(a|st; \u03b8)Q\u2217t (st, a)da ]\n= 1\nH H\u2211 t=1 E st\u223cd \u03c0\u03b8n t [ \u222b a \u03c0(a|st; \u03b8n) \u03c0(a|st; \u03b8) \u03c0(a|st; \u03b8n) Q\u2217t (st, a)da ]\n= 1\nH H\u2211 t=1 E st\u223cd \u03c0\u03b8n t [ E a\u223c\u03c0(\u00b7|st;\u03b8n) \u03c0(a|st; \u03b8) \u03c0(a|st; \u03b8n) Q\u2217t (st, a) ]\n= 1\nH H\u2211 t=1 E st\u223cd \u03c0\u03b8n t ,at\u223c\u03c0(a|st;\u03b8n) [ \u03c0(at|st; \u03b8) \u03c0(at|st; \u03b8n) Q\u2217t (st, at) ] . (20)"}, {"heading": "B. Derivation of Exponential Gradient Update in Discrete MDP", "text": "We show the detailed derivation of Eq. 7 for AggreVaTeD with EG in discrete MDP. Recall that with KL-divergence as the penalization, one update the policy in each episode as:\n{\u03c0sn+1}s\u2208S = arg min{\u03c0s\u2208\u2206(A),\u2200s} 1 H H\u2211 t=1 \u2211 s\u2208S d\u03c0nt (s) ( \u03c0s \u00b7Q\u2217t (s) ) + \u2211 s\u223cS d\u0304\u03c0n(s) \u03b7n,s KL(\u03c0s\u2016\u03c0sn)\nNote that in the above equation, for a particular state s, optimizing \u03c0s is in fact independent of \u03c0s \u2032 ,\u2200s\u2032 6= s. Hence the optimal sequence {\u03c0s}s\u2208S can be achieved by optimizing \u03c0s independently for each s \u2208 S. For \u03c0s, we have the following update rule:\n\u03c0sn+1 = arg min \u03c0s\u2208\u2206(A)\n1\nH H\u2211 t=1 d\u03c0nt (s)(\u03c0 s \u00b7Q\u2217t (s)) + d\u0304\u03c0n(s) \u03b7n,s KL(\u03c0s\u2016\u03c0sn)\n= arg min \u03c0s\u2208\u2206(A) \u03c0s \u00b7 ( H\u2211 t=1 d\u03c0nt (s)Q \u2217 t (s)/H) + d\u0304\u03c0n(s) \u03b7n,s KL(\u03c0s\u2016\u03c0sn)\n= arg min \u03c0s\u2208\u2206(A) \u03c0s \u00b7 ( H\u2211 t=1 d\u03c0nt (s)Q \u2217 t (s)/(Hd\u0304 \u03c0n(s))) + 1 \u03b7n,s KL(\u03c0s\u2016\u03c0sn)\n= arg min \u03c0s\u2208\u2206(A) \u03c0s \u00b7 Q\u0303e(s) + 1 \u03b7n,s A\u2211 j=1 \u03c0s[j](log(\u03c0s[j])\u2212 log(\u03c0sn[j])) (21)\nTake the derivative with respect to \u03c0s[j], and set it to zero, we get:\nQ\u0303e(s)[j] + 1\n\u03b7n,s (log(\u03c0s[j]/\u03c0sn[j]) + 1) = 0, (22)\nthis gives us:\n\u03c0s[j] = \u03c0sn[j] exp(\u2212\u03b7n,sQ\u0303e(s)[j]\u2212 1). (23)\nSince \u03c0s \u2208 \u2206(A), after normalization, we get:\n\u03c0s[j] = \u03c0sn[j] exp(\u2212\u03b7n,sQ\u0303e(s)[j])\u2211A i=1 \u03c0 s n[i] exp(\u2212\u03b7n,sQ\u0303e(s)[i])\n(24)"}, {"heading": "C. Lemmas", "text": "Before proving the theorems, we first present the Performance Difference Lemma (Kakade & Langford, 2002; Ross & Bagnell, 2014) which will be used later:\nLemma C.1. For any two policies \u03c01 and \u03c02, we have:\n\u00b5(\u03c01)\u2212 \u00b5(\u03c02) = H H\u2211 t=1 Est\u223cd\u03c01t [ Eat\u223c\u03c01(\u00b7|st)[Q \u03c02 t (st, at)\u2212 V \u03c02 t (st)] ] . (25)\nWe refer readers to (Ross & Bagnell, 2014) for the detailed proof of the above lemma.\nThe second known result we will use is the analysis of Weighted Majority Algorithm. Let us define the linear loss function as `n(w) = w \u00b7 yn, for any yn \u2208 Rd, and w \u2208 \u2206(d) from a probability simplex. Running Exponential Gradient Algorithm on the sequence of losses {w \u00b7 yn} to compute a sequence of decisions {wn}, we have: Lemma C.2. The sequence of decisions {wn} computed by running Exponential Gradient descent with step size \u00b5 on the loss functions {w \u00b7 yn} has the following regret bound:\nN\u2211 n=1 wn \u00b7 yn \u2212 min w\u2217\u2208\u2206(d) N\u2211 n=1 w\u2217 \u00b7 yn \u2264 ln(d) \u00b5 + \u00b5 2 N\u2211 n=1 d\u2211 i=1 wn[i]yn[i] 2. (26)\nWe refer readers to (Shalev-Shwartz et al., 2012) for detailed proof."}, {"heading": "D. Proof of Theorem 5.1", "text": "Proof. We construct a reduction from stochastic Multi-Arm Bandits (MAB) to the MDP M\u0303. A stochastic MAB is defined by S arms denoted as I1, ..., IS . Each arm It\u2019s cost ci at any time step t is sampled from a fixed but unknown distribution. A bandit algorithm picks an arm It at iteration t and then receives an unbiased sample of the picked arm\u2019s cost cIt . For any bandit algorithm that picks arms I1, I2, ..., IN in N rounds, the expected regret is defined as:\nE[RN ] = E[ N\u2211 n=1 cIn ]\u2212 min i\u2208[S] N\u2211 n=1 c\u0304i, (27)\nwhere the expectation is taken with respect to the randomness of the cost sampling process and possibly the randomness of the bandit algorithm. It has been shown that there exists a set of distributions from which the arms\u2019 costs sampled from, the expected regret E[RN ] is at least \u2126( \u221a SN) (Bubeck et al., 2012).\nConsider a MAB with 2K arms. To construct a MDP from a MAP, we construct a K + 1-depth binary-tree structure MDP with 2K+1\u2212 1 nodes. We set each node in the binary tree as a state in the MDP. The number of actions of the MDP is two, which corresponds to go left or right at a node in the binary tree. We associate each leaf nodes with arms in the original MAB: the cost of the i\u2019th leaf node is sampled from the cost distribution for the i\u2019th arm, while the non-leaf nodes have cost always equal to zero. The initial distribution \u03c10 concentrates on the root of the binary tree. Note that there are total 2K trajectories from the root to leafs, and we denote them as \u03c41, ...\u03c42K . We consider finite horizon (H = K + 1) episodic RL algorithms that outputs \u03c01, \u03c02, ..., \u03c0N at N episodes, where \u03c0n is any deterministic policy that maps a node to actions left or right. Any RL algorithm must have the following regret lower bound:\nE[ N\u2211 n=1 \u00b5(\u03c0n)]\u2212min \u03c0\u2217 N\u2211 n=1 \u00b5(\u03c0\u2217) \u2265 \u2126( \u221a SN), (28)\nwhere the expectation is taken with respect to the possible randomness of the RL algorithms. Note that any deterministic policy \u03c0 identifies a trajectory in the binary tree when rolling in from the root. The optimal policy \u03c0\u2217 simply corresponds to the trajectory that leads to the leaf with the mininum expected cost. Note that each trajectory is associated with an arm from the original MAB, and the expected total cost of a trajectory corresponds to the expected cost of the associated arm. Hence if there exists an RL algorithm that achieves regret O( \u221a SN), then we can solve the original MAB problem by simply running the RL algorithm on the constructed MDP. Since the lower bound for MAB is \u2126( \u221a SN), this concludes that Eq. 28 holds."}, {"heading": "E. Proof of Theorem 5.2", "text": "Proof. For notation simplicity we denote al as the go-left action while ar is the go-right action. Without loss of generality, we assume that the leftmost trajectory has the lowest total cost (e.g., s3 in Fig. 1 has the lowest average cost). We consider the deterministic policy class \u03a0 that contains all policy \u03c0 : S \u2192 {al, ar}. Since there are S states and 2 actions, the total number of policies in the policy class is 2S . To prove the upper bound RN \u2264 O(log(S)), we claim that for any e \u2264 K, at the end of episode e, AggreVaTe with FTL identifies the e\u2019th state on the best trajectory, i,e, the leftmost trajectory s0, s1, s3, ..., s(2K\u22121\u22121). We can prove the claim by induction.\nAt episode e = 1, based on the initial policy, AggreVaTe picks a trajectory \u03c41 to explore. AggreVaTe with FTL collects the states s at \u03c41 and their associated cost-to-go vectors [Q\u2217(s, al), Q\u2217(s, ar)]. Let us denote D1 as the dataset that contains the state,cost-to-go pairs: D1 = {(s, [Q\u2217(s, al), Q\u2217(s, al)])}, for s \u2208 \u03c41. Since s0 is visited, the state-cost pair (s0, [Q \u2217(s0, al), Q \u2217(s0, ar)]) must be in D1. To update policy from \u03c01 to \u03c02, AggreVaTe with FTL runs cost-sensitive classification D1 as:\n\u03c02 = arg min \u03c0 |D1|\u2211 k=1 Q\u2217(sk, \u03c0(sk)), (29)\nwhere sk stands for the k\u2019th data point collected at dataset D1. Due to the construction of policy class \u03a0, we see that \u03c02 must picks action al at state s0 since Q(s0, al) < Q(s0, ar). Hence at the end of the episode e = 1, \u03c02 identifies s1 (i.e., running \u03c02 from root s0 leads to s1), which is on the optimal trajectory.\nNow assume that at the end of episode n \u2212 1, the newly updated policy \u03c0n identifies the state s(2n\u22121\u22121): namely at the beginning of episode n, if we roll-in \u03c0n, the algorithm will keep traverse along the leftmost trajectory till at least state s(2n\u22121\u22121). At episode n, let Dn as the dataset contains all data points from Dn\u22121 and the new collected state, cost-togo pairs from \u03c4n: Dn = Dn\u22121 \u222a {(s, [Q\u2217(s, al), Q\u2217(s, ar)])}, for all s \u2208 \u03c4n. Now if we compute policy \u03c0n+1 using cost-sensitive classification (Eq. 29) over Dn, we must learn a policy \u03c0n+1 that identifies action al at state s(2j\u22121), since Qe(s(2j\u22121), al) < Q\n\u2217(s(2j\u22121), ar), and s(2j\u22121) is included in Dn, for j = 1, ..., n\u2212 1. Hence at the end of episode n, we identify a policy \u03c0n+1 such that if we roll in policy \u03c0n+1 from s0, we will traverse along the left most trajectory till we reach s(2n\u22121).\nHence by the induction hypothesis, at the end of episodeK\u22121, \u03c0K will reach state s(2K\u22121\u22121), the end of the best trajectory.\nSince AggreVaTe with FTL with policy class \u03a0 identifies the best trajectory with at most K \u2212 1 episodes, the cumulative regret is then at most O(K), which is O(log(S)) (assuming the average cost at each leaf is a bounded constant), as S is the number of nodes in the binary-tree structure MDP M\u0303."}, {"heading": "F. Proof of Theorem 5.3", "text": "Since in Theorem 5.3 we assume that we only have access to the noisy, but unbiased estimate of Q\u2217, the problem becomes more difficult since unlike in the proof of Theorem 5.2, we cannot simply eliminate states completely since the cost-to-go of the states queried from expert is noisy and completely eliminate nodes will potentially result elimination of low cost trajectories. Hence here we consider a different policy representation. We define 2K deterministic base policies \u03c01, ..., \u03c02 K\n, such that rolling in policy \u03c0i at state s0 will traverse along the trajectory ending at the i\u2019th leaf. We define the policy class \u03a0 as the convex hull of the base policies \u03a0 = {\u03c0 : \u22112K i=1 wi\u03c0 i, \u22112K i wi = 1, wi \u2265 0,\u2200i}. Namely each \u03c0 \u2208 \u03a0 is a stochastic policy: when rolling in, with probability wi, \u03c0 execute the i\u2019th base policy \u03c0i from s0. Below we prove that AggreVaTeD with Exponential Gradient Descent achieves the regret bound O( \u221a ln(S)N).\nProof. We consider finite horizon, episodic imitation learning setting where at each episode n, the algorithm can roll in the current policy \u03c0n once and only once and traverses through trajectory \u03c4n . Let us define \u02dc\u0300n(w) =\n1 K+1 \u2211 s\u2208\u03c4n \u22112K j=1 wjQ\u0303 e(s, \u03c0j(s)), where \u03c4n is the trajectory traversed by rolling in policy \u03c0n starting at s0, and Q\u0303e is a noisy but unbiased estimate of Q\u2217. We simply consider the setting where Q\u0303e is bounded |Q\u0303e| \u2264 lmax (note that we can easily extend our analysis to a more general case where Q\u0303e is from a sub-Gaussian distribution). Note that \u02dc\u0300n(w) is simply a linear loss with respect to w:\n\u02dc\u0300 n(w) = w \u00b7 qn, (30)\nwhere qn[j] = \u2211 s\u2208\u03c4n Q\u0303\ne(s, \u03c0j(s))/(K+ 1). AggreVaTeD with EG updates w using Exponential gradient descent. Using the result from lemma C.2, we get:\nN\u2211 n=1 (\u02dc\u0300n(wn)\u2212 \u02dc\u0300n(w\u2217)) = N\u2211 n=1 (wn \u00b7 qn \u2212 w\u2217 \u00b7 qn) \u2264 ln(2K) \u00b5 + \u00b5 2 N\u2211 n=1 2K\u2211 j=1 wn[j]qn[j] 2 \u2264 ln(2 K) \u00b5 + \u00b5 2 N\u2211 n=1 l2max\n= ln(2K) \u00b5 + \u00b5Nl2max 2 \u2264 lmax\n\u221a ln(S)N. (31)\nNote that S = 2K+1 \u2212 1. The above inequality holds for any w\u2217 \u2208 \u2206(2K), including the we that corresponds to the expert (i.e., we[1] = 1, we[i] = 0, i 6= 1 as we assumed without loss of generality the left most trajectory is the optimal trajectory).\nNow let us define `n(w) as follows:\n`n(w) = 1\nK + 1 K+1\u2211 t=1 \u2211 s\u223cS d\u03c0nt (s) 2K\u2211 j=1 wjQ \u2217(s, \u03c0j(s)). (32)\nNote `n(w) can be understood as first rolling in \u03c0n infinitely many times and then querying for the exact cost-to-go Q\u2217 on all the visited states. Clearly \u02dc\u0300n(w) is an unbiased estimate of `n(w): E[\u02dc\u0300n(w)]\u2212`n(w) = 0, where the expectation is over the randomness of the roll-in and sampling procedure of Q\u0303e at iteration n, conditioned on all events among the previous n\u2212 1 iterations. Also note that |\u02dc\u0300n(w)\u2212 `n(w)| \u2264 2lmax, since `n(w) \u2264 lmax. Hence {\u02dc\u0300n(wn)\u2212 `n(wn)} is a bounded martingale difference sequence. Hence by Azuma-Heoffding inequality, we get with probability at least 1\u2212 \u03b4/2:\nN\u2211 n=1 `n(wn)\u2212 \u02dc\u0300n(wn) \u2264 2lmax \u221a log(2/\u03b4)N, (33)\nand with probability at least 1\u2212 \u03b4/2:\nN\u2211 n=1 \u02dc\u0300 n(w e)\u2212 `n(we) \u2264 2lmax \u221a log(2/\u03b4)N. (34)\nCombine the above inequality using union bound, we get with probability at least 1\u2212 \u03b4:\nN\u2211 n=1 (`n(wn)\u2212 `n(we)) \u2264 N\u2211 n=1 (\u02dc\u0300n(wn)\u2212 \u02dc\u0300n(we)) + 4lmax \u221a log(2/\u03b4)N. (35)\nNow let us apply the Performance Difference Lemma (Lemma C.1), we get with probability at least 1\u2212 \u03b4:\nN\u2211 n=1 \u00b5(\u03c0n)\u2212 N\u2211 n=1 \u00b5(\u03c0\u2217) = N\u2211 n=1 (K + 1) ( `n(wn)\u2212 `n(we) ) \u2264 (K + 1)(lmax \u221a ln(S)N + 4lmax \u221a log(2/\u03b4)N), (36)\nrearrange terms we get:\nN\u2211 n=1 \u00b5(\u03c0n)\u2212 N\u2211 n=1 \u00b5(\u03c0\u2217) \u2264 log(S)lmax( \u221a ln(S)N + \u221a log(2/\u03b4)N) \u2264 O(ln(S) \u221a ln(S)N), (37)\nwith probability at least 1\u2212 \u03b4."}, {"heading": "G. Proof of Theorem 5.4", "text": "The proof of theorem 5.4 is similar to the one for theorem 5.3. Hence we simply consider the infinitely many roll-ins and exact query of Q\u2217 case. The finite number roll-in and noisy query of Q\u2217 case can be handled by using the martingale difference sequence argument as shown in the proof of theorem 5.3.\nProof. Recall that in general setting, the policy \u03c0 consists of probability vectors \u03c0s,t \u2208 \u2206(A), for all s \u2208 S and t \u2208 [H]: \u03c0 = {\u03c0s,t}\u2200s\u2208S,t\u2208[H]. Also recall that the loss functions EG is optimizing are {`n(\u03c0)} where:\n`n(\u03c0) = 1\nH H\u2211 t=1 \u2211 s\u2208S d\u03c0nt (s)(\u03c0 s,t \u00b7Q\u2217t (s)) = H\u2211 t=1 \u2211 s\u2208S \u03c0s,t \u00b7 qs,tn (38)\nwhere as we defined before Q\u2217t (s) stands for the cost-to-go vector Q \u2217 t (s)[j] = Q \u2217 t (s, aj), for the j\u2019th action in A, and qs,tn = d\u03c0nt (s) H Q \u2217 t (s).\nNow if we run Exponential Gradient descent on `n to optimize \u03c0s,t for each pair of state and time step independently, we can get the following regret upper bound by using Lemma C.2:\nN\u2211 n=1 `n(\u03c0)\u2212min \u03c0 N\u2211 n=1 `n(\u03c0n) \u2264 H\u2211 t=1 \u2211 s\u2208S ( ln(A) \u00b5 + \u00b5 2 N\u2211 n=1 A\u2211 j=1 \u03c0s,t[j]qs,tn [j] 2 ) . (39)\nNote that we can upper bound (qs,tn [j]) 2 as:\n(qs,tn [j]) 2 \u2264 d\n\u03c0n t (s) 2\nH2 (Q\u2217max)\n2 \u2264 d \u03c0n t (s)\nH2 (Q2max) 2 (40)\nSubstitute it back, we get:\nN\u2211 n=1 (`n(\u03c0n)\u2212 `n(\u03c0\u2217)) \u2264 H\u2211 t=1 \u2211 s\u2208S ( ln(A) \u00b5 + \u00b5 2 N\u2211 n=1 A\u2211 j=1 \u03c0s,t[j]d\u03c0nt (s) (Q\u2217max) 2 H2 )\n= H\u2211 t=1 (S ln(A) \u00b5 + \u00b5(Q\u2217max) 2 2H2 N\u2211 n=1 \u2211 s\u2208S d\u03c0nt (s) A\u2211 j=1 \u03c0s,t[j] ) = H\u2211 t=1 (S ln(A) \u00b5 + \u00b5(Q\u2217max) 2 2H2 N ) \u2264 Q \u2217 max\nH\n\u221a 2S ln(A)N, (41)\nif we set \u00b5 = \u221a\n(Q\u2217max) 2NS ln(A)/(2H2).\nNow let us apply the performance difference lemma (Lemma C.1), we get:\nRN = N\u2211 n=1 \u00b5(\u03c0n)\u2212 N\u2211 n=1 \u00b5(\u03c0\u2217) = H N\u2211 n=1 (`n(wn)\u2212 `n(we)) \u2264 HQemax \u221a S ln(A)N. (42)"}, {"heading": "H. Proof of Theorem 5.5", "text": "Let us use Q\u0303e(s) to represent the noisy but unbiased estimate of Q\u2217(s).\nProof. For notation simplicity, we denote S = {s1, s2, ..., sS}. We consider a finite MDP with time horizon H = 1. The initial distribution \u03c10 = {1/S, ..., 1/S} puts 1/S weight on each state. We consider the algorithm setting where at every episode n, a state sn \u2208 S is sampled from \u03c10 and the algorithm uses its current policy \u03c0snn \u2208 \u2206(A) to pick an action a \u2208 A for sn and then receives a noisy but unbiased estimate Q\u0303e(sn) of Q\u2217(sn) \u2208 R|A|. The algorithm then updates its policy from \u03c0s n\nn to \u03c0 sn n+1 for s n while keep the other polices for other s unchanged (since the algorithm did not receive any\nfeedback regarding Q\u2217(s) for s 6= sn and the sample distribution \u03c10 is fixed and uniform). For expected regret E[RN ] we\nhave the following fact:\nE sn\u223c\u03c10,\u2200n\n[ E\nQ\u0303e(sn)\u223cPsn ,\u2200n [ N\u2211 n=1 (\u03c0s n n \u00b7 Q\u0303e(sn)\u2212 \u03c0\u2217sn \u00b7 Q\u0303e(sn)) ]]\n= E sn\u223c\u03c10,\u2200n [ N\u2211 n=1 E Q\u0303ei (si)\u223cPsi ,i\u2264n\u22121 [ (\u03c0s n n \u00b7Q\u2217(sn)\u2212 \u03c0esn \u00b7Q\u2217(sn)) ]]\n= N\u2211 n=1 E si\u223c\u03c10,i\u2264n\u22121 [ E Q\u0303ei (si)\u223cPsi ,i\u2264n\u22121 [ E s\u223c\u03c10 (\u03c0sn \u00b7Q\u2217(s)\u2212 \u03c0\u2217s \u00b7Q\u2217(s)) ]] = E\n[ N\u2211 n=1 E s\u223c\u03c10 \u03c0sn \u00b7Q\u2217(s)\u2212 E s\u223c\u03c10 \u03c0\u2217s \u00b7Q\u2217(s) ]\n= E N\u2211 n=1 [\u00b5(\u03c0n)\u2212 \u00b5(\u03c0\u2217)], (43)\nwhere the expectation in the final equation is taken with respect to random variables \u03c0i, i \u2208 [N ] since each \u03c0i is depend on Q\u0303ej , for j < i and s j , for j < i.\nWe first consider EQ\u0303e(sn)\u223cPsn ,\u2200n [\u2211N n=1(\u03c0 sn n \u00b7 Q\u0303e(sn) \u2212 \u03c0\u2217sn \u00b7 Q\u0303e(sn)) ]\nconditioned on a given sequence of s1, ..., sN . Let us define that among N episodes, the set of the index of the episodes that state si is sampled as Ni and its cardinality as Ni, and we then have \u2211S i=1Ni = N and Ni \u2229Nj = \u2205,for i 6= j.\nE Q\u0303e(sn)\u223cPsn ,\u2200n [ N\u2211 n=1 (\u03c0s n n \u00b7 Q\u0303e(sn)\u2212 \u03c0\u2217sn \u00b7 Q\u0303e(sn)) ]\n= S\u2211 i=1 \u2211 j\u2208Ni E Q\u0303ej (si)\u223cPsi (\u03c0sij \u00b7 Q\u0303 e j(si)\u2212 \u03c0esiQ\u0303 e j(si)) (44)\nNote that for each state si, at the rounds from Ni, we can think of the algorithm running any possible online linear regression algorithm to compute the sequence of policies \u03c0sij ,\u2200j \u2208 Ni for state si. Note that from classic online linear regression analysis, we can show that for state si there exists a distribution Psi such that for any online algorithm:\nE Q\u0303ej (si)\u223cPsi ,\u2200j\u2208Ni [ \u2211 j\u2208Ni (\u03c0sij \u00b7 Q\u0303 e j(si)\u2212 \u03c0esi \u00b7 Q\u0303 e j(si)) ] \u2265 c \u221a ln(A)Ni, (45)\nfor some non-zero positive constant c. Substitute the above inequality into Eq. 44, we have:\nE Q\u0303e(sn)\u223cPsn ,\u2200n [ N\u2211 n=1 (\u03c0s n n \u00b7 Q\u0303e(sn)\u2212 \u03c0\u2217sn \u00b7 Q\u0303e(sn)) ] \u2265 S\u2211 i=1 c \u221a ln(A)Ni = c \u221a ln(A) S\u2211 i=1 \u221a Ni. (46)\nNow let us put the expectation Esi\u223c\u03c10,\u2200i back, we have:\nE sn\u223c\u03c10,\u2200n\n[ E\nQ\u0303e(sn)\u223cPsn [ N\u2211 n=1 (\u03c0s n n \u00b7 Q\u0303e(sn)\u2212 \u03c0\u2217sn \u00b7 Q\u0303e(sn))|s1, ..., sn ]] \u2265 c \u221a ln(A) N\u2211 i=1 E[ \u221a Ni]. (47)\nNote that each Ni is sampled from a Binomial distribution B(N, 1/S). To lower bound En\u223cB(N,1/S) \u221a n, we use Hoeffding\u2019s Inequality here. Note that Ni = \u2211N n=1 an, where an = 1 if si is picked at iteration n and zero otherwise. Hence ai is from a Bernoulli distribution with parameter 1/S. Using Hoeffding bound, for Ni/N , we get:\nP (|Ni/N \u2212 1/S| <= ) \u2265 1\u2212 exp(\u22122N 2). (48)\nLet = 1/(2S), and substitute it back to the above inequality, we get:\nP (0.5(N/S) \u2264 Ni \u2264 1.5(N/S)) = P ( \u221a 0.5(N/S) \u2264 \u221a Ni \u2264 \u221a 1.5(N/S)) \u2265 1\u2212 exp(\u22122N/S2). (49)\nHence, we can lower bound E[ \u221a Ni] as follows:\nE[ \u221a Ni] \u2265 \u221a 0.5N/S(1\u2212 exp(\u22122N/S2)). (50)\nTake N to infinity, we get:\nlim N\u2192\u221e\nE[ \u221a Ni] \u2265 \u221a 0.5N/S. (51)\nSubstitute this result back to Eq. 47 and use the fact from Eq. 43, we get:\nlim N\u2192\u221e E[RN ] = lim N\u2192\u221e E sn\u223c\u03c10,\u2200n\n[ E\nQ\u0303e(sn)\u223cPsn ,\u2200n [ N\u2211 n=1 (\u03c0s n n \u00b7 Q\u0303e(sn)\u2212 \u03c0\u2217sn \u00b7 Q\u0303e(sn)) ]] \u2265 c \u221a ln(A) S\u2211 i=1 E[ \u221a Ni]\n\u2265 c \u221a ln(A)S \u221a 0.5N/S = \u2126( \u221a S ln(A)N).\nHence we prove the theorem."}, {"heading": "I. Details of Dependency Parsing for Handwritten Algebra", "text": "In Fig. 4, we show an example of set of handwritten algebra equations and its dependency tree from a arc-hybird sequence slssslssrrllslsslssrrslssrlssrrslssrr. The preprocess step cropped individual symbols one by one from left to right and from the top equation to the bottom one, centered them, scaled symbols to 40 by 40 images, and finally formed them as a sequence of images.\nSince in the most common dependency parsing setting, there is no immediate reward at every parsing step, the reward-togo Q\u2217(s, a) is computed by using UAS as follows: start from s and apply action a, then use expert \u03c0\u2217 to roll out til the end of the parsing process; Q\u2217(s, a) is the UAS score of the final configuration. Hence AggreVaTeD can be considered as directly maximizing the UAS score, while previous approaches such as DAgger or SMILe (Ross et al., 2011) tries to mimic expert\u2019s actions and hence are not directly optimizing the final objective."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In ICML, pp", "citeRegEx": "Abbeel et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2004}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["Bahdanau", "Dzmitry", "Brakel", "Philemon", "Xu", "Kelvin", "Goyal", "Anirudh", "Lowe", "Ryan", "Pineau", "Joelle", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "arXiv preprint arXiv:1607.07086,", "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Learning to search for dependencies", "author": ["Chang", "Kai-Wei", "He", "Daum\u00e9 III", "Hal", "Langford", "John"], "venue": "arXiv preprint arXiv:1503.05615,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Learning to search better than your teacher", "author": ["Chang", "Kai-wei", "Krishnamurthy", "Akshay", "Agarwal", "Alekh", "Daume", "Hal", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Chang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2015}, {"title": "Searchbased structured prediction", "author": ["Daum\u00e9 III", "Hal", "Langford", "John", "Marcu", "Daniel"], "venue": "Machine learning,", "citeRegEx": "III et al\\.,? \\Q2009\\E", "shortCiteRegEx": "III et al\\.", "year": 2009}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Predicting structure in handwritten algebra data from low level features", "author": ["Duyck", "James A", "Gordon", "Geoffrey J"], "venue": "Data Analysis Project Report, MLD,", "citeRegEx": "Duyck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Duyck et al\\.", "year": 2015}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Finn", "Chelsea", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "In ICML,", "citeRegEx": "Finn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Finn et al\\.", "year": 2016}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Greensmith", "Evan", "Bartlett", "Peter L", "Baxter", "Jonathan"], "venue": null, "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Generative adversarial imitation learning", "author": ["Ho", "Jonathan", "Ermon", "Stefano"], "venue": "In NIPS,", "citeRegEx": "Ho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2016}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter"], "venue": null, "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Plato: Policy learning using adaptive trajectory optimization", "author": ["Kahn", "Gregory", "Zhang", "Tianhao", "Levine", "Sergey", "Abbeel", "Pieter"], "venue": "arXiv preprint arXiv:1603.00622,", "citeRegEx": "Kahn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kahn et al\\.", "year": 2016}, {"title": "A natural policy gradient", "author": ["Kakade", "Sham"], "venue": null, "citeRegEx": "Kakade and Sham.,? \\Q2002\\E", "shortCiteRegEx": "Kakade and Sham.", "year": 2002}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In ICML,", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik", "Ba", "Jimmy"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Ritter", "Alan", "Galley", "Michel", "Gao", "Jianfeng", "Jurafsky", "Dan"], "venue": "arXiv preprint arXiv:1606.01541,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr"], "venue": "Nature,", "citeRegEx": "Mnih and Volodymyr,? \\Q2015\\E", "shortCiteRegEx": "Mnih and Volodymyr", "year": 2015}, {"title": "Exploiting the connection between pls, lanczos methods and conjugate gradients: alternative proofs of some properties of pls", "author": ["Phatak", "Aloke", "de Hoog", "Frank"], "venue": "Journal of Chemometrics,", "citeRegEx": "Phatak et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Phatak et al\\.", "year": 2002}, {"title": "Sequence level training with recurrent neural networks", "author": ["Ranzato", "Marc\u2019Aurelio", "Chopra", "Sumit", "Auli", "Michael", "Zaremba", "Wojciech"], "venue": "ICLR 2016,", "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Maximum margin planning", "author": ["Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A"], "venue": "In ICML,", "citeRegEx": "Ratliff et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ratliff et al\\.", "year": 2006}, {"title": "Visual chunking: A list prediction framework for region-based object detection", "author": ["Rhinehart", "Nicholas", "Zhou", "Jiaji", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": "In ICRA. IEEE,", "citeRegEx": "Rhinehart et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rhinehart et al\\.", "year": 2015}, {"title": "Efficient reductions for imitation learning", "author": ["Ross", "St\u00e9phane", "Bagnell", "J. Andrew"], "venue": "In AISTATS, pp", "citeRegEx": "Ross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2010}, {"title": "Reinforcement and imitation learning via interactive no-regret learning", "author": ["Ross", "Stephane", "Bagnell", "J Andrew"], "venue": "arXiv preprint arXiv:1406.5979,", "citeRegEx": "Ross et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2014}, {"title": "A reduction of imitation learning and structured prediction to noregret online learning", "author": ["Ross", "St\u00e9phane", "Gordon", "Geoffrey J", "Bagnell", "J.Andrew"], "venue": "In AISTATS,", "citeRegEx": "Ross et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Learning policies for contextual submodular prediction", "author": ["Ross", "Stephane", "Zhou", "Jiaji", "Yue", "Yisong", "Dey", "Debadeepta", "Bagnell", "Drew"], "venue": "In ICML,", "citeRegEx": "Ross et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2013}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In ICML, pp", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["Silver", "David"], "venue": null, "citeRegEx": "Silver and David,? \\Q2016\\E", "shortCiteRegEx": "Silver and David", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Apprenticeship learning using linear programming", "author": ["Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E"], "venue": "In ICML,", "citeRegEx": "Syed et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Syed et al\\.", "year": 2008}, {"title": "Improving multi-step prediction of learned time series models", "author": ["Venkatraman", "Arun", "Hebert", "Martial", "Bagnell", "J Andrew"], "venue": null, "citeRegEx": "Venkatraman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venkatraman et al\\.", "year": 2015}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K"], "venue": "In AAAI,", "citeRegEx": "Ziebart et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ziebart et al\\.", "year": 2008}, {"title": "Online Convex Programming and Generalized Infinitesimal Gradient Ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}, {"title": "Proof of Theorem 5.1 Proof. We construct a reduction from stochastic Multi-Arm Bandits (MAB) to the MDP M\u0303. A stochastic MAB is defined by S arms denoted as I, ..., I . Each arm I\u2019s cost ci at any time step t is sampled from a fixed but unknown distribution", "author": ["D. proof"], "venue": null, "citeRegEx": "proof.,? \\Q2012\\E", "shortCiteRegEx": "proof.", "year": 2012}], "referenceMentions": [{"referenceID": 26, "context": "Reinforcement Learning (RL), especially deep RL, has dramatically advanced the state of the art in sequential decision making in high-dimensional robotics control tasks as well as in playing video and board games (Schulman et al., 2015; Silver et al., 2016).", "startOffset": 213, "endOffset": 257}, {"referenceID": 19, "context": "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).", "startOffset": 213, "endOffset": 275}, {"referenceID": 1, "context": "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).", "startOffset": 213, "endOffset": 275}, {"referenceID": 16, "context": "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).", "startOffset": 213, "endOffset": 275}, {"referenceID": 30, "context": ", 2009), DaD (Venkatraman et al., 2015), AggreVaTe (Ross & Bagnell, 2014), and LOLS (Chang et al.", "startOffset": 13, "endOffset": 39}, {"referenceID": 24, "context": "For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.", "startOffset": 166, "endOffset": 204}, {"referenceID": 12, "context": "For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.", "startOffset": 166, "endOffset": 204}, {"referenceID": 25, "context": ", beam search) or by a clairvoyant greedy algorithm (Daum\u00e9 III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.", "startOffset": 52, "endOffset": 140}, {"referenceID": 21, "context": ", beam search) or by a clairvoyant greedy algorithm (Daum\u00e9 III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.", "startOffset": 52, "endOffset": 140}, {"referenceID": 29, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 20, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 32, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 8, "context": "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy \u03c0\u0302\u2217 or Q\u0302\u2217 from a fixed-size dataset pre-collected from the oracle.", "startOffset": 14, "endOffset": 133}, {"referenceID": 24, "context": ", 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross & Bagnell, 2014) interleave learning and testing procedures to overcome the data mismatch issue and, as a result, work well in practical applications.", "startOffset": 16, "endOffset": 35}, {"referenceID": 6, "context": "We use an LSTM-based policy (Duan et al., 2016) where the LSTM\u2019s hidden states provide a compressed feature of the history.", "startOffset": 28, "endOffset": 47}, {"referenceID": 26, "context": "In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction.", "startOffset": 68, "endOffset": 91}, {"referenceID": 26, "context": "In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction. To do this, we leverage the reduction of IL and sequential prediction to online learning as shown in (Ross & Bagnell, 2014) to learn policies represented by expressive differentiable function approximators. The fundamental idea in Ross & Bagnell (2014) is to use a no-regret online learner to update policies using the following loss function at each episode n:", "startOffset": 69, "endOffset": 431}, {"referenceID": 26, "context": "Policy Updates with Natural Gradient Descent We derive a natural gradient update procedure for imitation learning inspired by the success of natural gradient descent in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015).", "startOffset": 172, "endOffset": 236}, {"referenceID": 26, "context": "Second, we replace KL(\u03c0\u03b8||\u03c0\u03b8n) by KL(\u03c0\u03b8n ||\u03c0\u03b8), which is a local approximation since KL(q||p) and KL(p||q) are equal up to the second order (Kakade & Langford, 2002; Schulman et al., 2015).", "startOffset": 140, "endOffset": 188}, {"referenceID": 9, "context": "11 by the state-action advantage function At (s i,n t , a) = Q \u2217 t (s i,n t , a) \u2212 V \u2217 t (s i,n t ), which leads to the following two unbiased and variancereduced gradient estimations (Greensmith et al., 2004):", "startOffset": 184, "endOffset": 209}, {"referenceID": 26, "context": "This approach is used in TRPO (Schulman et al., 2015).", "startOffset": 30, "endOffset": 53}, {"referenceID": 24, "context": "This mixing strategy with decay rate was first introduced in (Ross et al., 2011) for IL, and later on was used in sequence prediction (Bengio et al.", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": ", 2011) for IL, and later on was used in sequence prediction (Bengio et al., 2015).", "startOffset": 61, "endOffset": 82}, {"referenceID": 11, "context": "For example, the Upper Confidence Bound (UCB) based, near-optimal optimistic RL algorithms from (Jaksch et al., 2010), specifically designed for efficient exploration, admit regret \u00d5(HS \u221a HAN), leading to a gap of approximately \u221a HAS compared to the regret bound of imitation learning shown in Eq.", "startOffset": 96, "endOffset": 117}, {"referenceID": 6, "context": "For RL we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TNPG) (Duan et al., 2016).", "startOffset": 86, "endOffset": 105}, {"referenceID": 26, "context": "Following the neural network settings described in Schulman et al. (2015), the expert policy \u03c0\u2217 is obtained from TRPO with one hidden layer (64 hidden states), which is the same structure that we use to represent our policies \u03c0\u03b8.", "startOffset": 51, "endOffset": 74}, {"referenceID": 28, "context": "The RNN policy follows the design from (Sutskever et al., 2014).", "startOffset": 39, "endOffset": 63}], "year": 2017, "abstractText": "Researchers have demonstrated state-of-the-art performance in sequential decision making problems (e.g., robotics control, sequential prediction) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD \u2014 a policy gradient extension of the Imitation Learning (IL) approach of (Ross & Bagnell, 2014) \u2014 can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recurrent neural predictors, we present stochastic gradient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior performance with respect to the oracle when the demonstrator is sub-optimal.", "creator": "LaTeX with hyperref package"}}}