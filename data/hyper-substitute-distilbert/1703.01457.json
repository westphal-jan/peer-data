{"id": "1703.01457", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Mar-2017", "title": "Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating Deep Convolutional Neural Networks", "abstract": "purely convolutional edge clusters ( cnn ) converge shown their good performances in specialized computer control tasks. however, for best computational complexity beside cnn involves supporting huge amount processing data movements between the computational processor circuits and memory while specifically connects few major areas the power infrastructure. this paper devised chain - nn, a novel energy - efficient distributed chain framework for accelerating deep information. chain - nn consists comprises the dedicated dual - complementary byte cache ( bp ). in chain - analysis, convolutions arise realized involving the 1d systolic primitives composed of complementary group spanning adjacent pes. these systolic neurons, if possessing slightly proposed column - square scan wheel pattern, can fully block input fields to improve efficient memory loss requirement with energy saving. essentially, the 1d chain architecture allows four systolic primitives to be mechanically reconfigured according between stored storage parameters hence suitable design parameter. improving dynamic vertex layout above output - paths is under factor 28nm parameters. it occupies 2 logic gates. reduces voltage - tap memory. the samples generated thus 110 - bp group - nn can reduce scaled further to 1300. this achieves a peak throughput worth 806. 52. 339. 94 and is able to accelerate the five control layers 18 ghz at accelerated frame rate \u2212 326. 11. 1421. 0gops / w power performance factors set least 25. 39 to 4. 1x @ / yet the table - of - the - same works.", "histories": [["v1", "Sat, 4 Mar 2017 14:14:14 GMT  (949kb)", "http://arxiv.org/abs/1703.01457v1", "DATE 2017"]], "COMMENTS": "DATE 2017", "reviews": [], "SUBJECTS": "cs.AR cs.LG", "authors": ["shihao wang", "dajiang zhou", "xushen han", "takeshi yoshimura"], "accepted": false, "id": "1703.01457"}, "pdf": {"name": "1703.01457.pdf", "metadata": {"source": "CRF", "title": "Chain-NN: An Energy-Efficient 1D Chain Architecture for Accelerating Deep Convolutional Neural Networks", "authors": ["Shihao Wang", "Dajiang Zhou", "Xushen Han", "Takeshi Yoshimura"], "emails": ["wshh1216@moegi.waseda.jp"], "sections": [{"heading": null, "text": "Keywords\u2014convolutional neural networks; CNN; accelerator; ASIC; power efficiency; memory bandwidth\nI. INTRODUCTION Convolutional neural networks (CNN) have recently been a hot topic as they have led to great breakthrough in many computer vision tasks. Many CNN-based algorithms like AlexNet [1] and VGG [2] have proved to be stronger than conventional algorithms. Among these networks, convolutions usually account for the major part of the total computations in both training and testing phases. Further, there is a trend that CNN is towards deeper and larger for a better performance [4]. For example, ResNet [3] has recently proved a network with more than a thousand layers can further enhance the performance compared to a shallower network.\nThe increasing network scalability results in more computational complexities. It usually costs quite a long time to train deep CNNs and also challenges the design of real-time CNN applications. Therefore, CNN-specific accelerators are desired. The accelerators are expected to have not only a high reconfigurability to always achieve a high performance for\nvarious CNN parameters, but also the good energy and area efficiency for deployments on battery-limited devices.\nTo achieve these goals, researchers have been exploring efficient CNN accelerators on various platforms. The designs on CPU or GPU like [5] can achieve a high reconfigurability at the expense of high energy costs and usually suffer from the Von-Neumann bottleneck which limits their performance. Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs. However, it usually involves high design complexities for supporting a high reconfigurability which will reversely affect the performance or efficiency. For example, [12] builds a 2D PE array, which has an on-chip network and inter-PE communications. Thus, an efficient architecture is desired to balance these metrics including the energy efficiency, for the increasing popularity of CNNs.\nIn this paper, we introduce our energy-efficient 1D chain architecture called Chain-NN for CNN accelerators, which contains the following contributions: We give a taxonomy of existing CNN accelerators to figure\nout their pros and cons. A novel energy-efficient 1D chain architecture is given. Its\ndedicated dual-channel architecture and column-wise scan input pattern provides energy-friendly reusability of input data to achieve 1.4TOPS/W power efficiency. 1D chain architecture of Chain-NN also supports a high reconfigurability by 84-100% of PE utilization rate for mainstream CNNs with low design complexities. Experimental results in TSMC 28nm show maximum 806.4GOPS throughput at 700MHz. Compared to state-ofthe-art works, it achieves 2.5x to 4.1x power efficiency.\nThe rest of the paper is organized as follows. Sec. II briefly introduces CNN algorithms. In Sec. III, we give a taxonomy of existing CNN accelerators and introduce the features of our Chain-NN. Sec. III, we give a detailed discussion on the proposed 1D chain architecture. The experimental results are shown in Sec. IV. Finally, we conclude this paper in Sec. V.\nII. CNN BACKGROUND Convolutional neural networks are trainable architectures where convolutional layers are the primary computational parts inside a network. Each layer receives input feature maps (ifmaps) and generates the output feature maps (ofmaps), which represent some particular features as shown in Fig. 1.\nWe may regard ifmaps/ofmaps as 3D structures composed of multiple 2D channels. For each ofmaps channels, convolution is done between the 3D ifmaps and correspondent 3D kernels. Moreover, several group of 3D ifmaps compose of a batch, where the same group of kernels are shared. Therefore, convolutional layers can be regarded as 4D computations as shown in the Equation (1).\n]][][][[]][][][[\n][]][][][[ 1\n0\n1\n0\n1 0 jicmKerneljyixcnifmpas\nmbiasyxmnofmaps C\nc\nK\ni\nK\nj\n(1)\nThe meanings of indexes are shown in Table I. These parameters vary for different layers and different CNNs.\nIII. HIGH LEVEL DESCRIPTION OF CHAIN-NN"}, {"heading": "A. Taxonomy of Existing CNN Architectures", "text": "We classify existing accelerator architectures into two categories according to how data are used inside processor cores. The way of data usage affects not only the reconfigurability, but also the required memory bandwidths (the amount of data movements). According to [16], the data movements of convolutional operands can be more expensive than ALU operations under the existing techniques. We give a detailed description and conclusion of the pros and cons of these two categories, followed by introducing how Chain-NN can improve over the existing works in Sec. III.B.\n1) Memory-centric Architectures As shown in Fig. 2(a), these architectures rely on the\naddressability of memories to achieve its reconfigurability for various CNN parameters. The processor core is simply a stacking of PEs. There is no data storage or data reuse paths inside the processor. A sophisticated central controller is necessary to decide when processor core should communicate with memories and which part of memories are accessed during the processing period.\nThese architectures can be reconfigured to adapt for various CNNs while the efficiency is sometimes unsatisfactory. They highly rely on memories for huge amount\nof data movements like [9,10,13]. Some achieve their reconfigurability by utilizing fewer PEs with fewer feeding data from memories, resulting in a waste of computing resources [8,15].\n2) 2D Spatial Architectures 2D spatial architectures in Fig. 2(b) can reduce the data\nmovements from memories by reusing them inside the processor core. Each PE can not only do basic computations, but also maintain a local controller to communicate with other PEs. PE usually contains local scratch pad memories to store data that are frequently used during a certain period. 2D spatial architectures divide the central controller in memorycentric ones and distribute them into each PE.\nThis solution can reduce the amount of data movements and have been employed in many works like [11,12]. However, their results show that the power and area cost of peripheral circuits for inter-PE communications can\u2019t be ignored. Moreover, this architecture has to consider the constraints in two dimensionalities for deployments, which sometimes limits its reconfigurability and scalability."}, {"heading": "B. High-level Description of Chain-NN", "text": "This paper introduces Chain-NN, a novel energy-efficient 1D chain architecture. As shown in Fig. 2(c), the PEs are organized as a chain architecture. Chain-NN is controlled by a finite state machine which changes its states according to a specific dataflow. The execution procedure is like this: 1) The finite-state machine is initialized to specific CNN parameters. 2) It starts to load related kernels into the processor core. 3) The ifmaps are continuously streamed into Chain-NN and convolution results are calculated. This paper mainly focuses on the design of processor core of Chain-NN and we leave the design exploration of memory hierarchy as the future work.\nWe conclude that Chain-NN has following advantages. First, it has a good energy efficiency compared to the others. It has a chain of dedicated dual-channel PEs which transfer data to adjacent PEs for data reuses with low control complexities. This not only reduces the total amount of memory accesses, but also shortens the distance of data fetching from on-chip SRAM level to PE level. Secondly, it has a high reconfigurability to be capable of achieving a high performance for various CNN parameters. This is due to its one dimensional organization of PEs, whose constraints are greatly relaxed compared to the 2D spatial architectures. An instantiation of our Chain-NN has proved to achieve an 84- 100% PE utilization ratio considering the mainstreaming CNN parameters. Finally, this architecture involves fewer overheads when scaled up to a higher parallelism or clock frequency. This feature not only brings efficiencies of both power and area, but also makes the architecture flexible in matching various demands of designers.\nIV. CHAIN-NN: 1D CHAIN ARCHITECTURE\nA. 1D Chain Architecture 1D chain architecture is the processor core of Chain-NN. It consists of numbers of cascading PEs. A group of adjacent K2 PEs forms a systolic primitive for convolution computations. An example of their relationship is shown in Fig. 3. We will give a detailed description on the 1D primitives in Sec. IV.B and the dual-channel PE architecture in Sec. IV.C.\nIn Fig. 3, the PE chain is cut into 1D primitives according to the kernel size. The upper part shows the case where each 1D primitive contains 9 PEs (K=3) while the lower refers K=2 case. The first and last PE in a primitive involve the ports to communicate with memory hierarchy, thus a set of primitive ports are attached.\nThe peak throughput of accelerator is proportional to the number of active PEs/primitives in the architecture. As a case study, we assume a systolic chain contains 576 PEs. The\nnumber of active PEs for mainstream kernel sizes is shown in Table II. The supported kernel sizes are not limited to these. Thanks to the 1D chain architecture, we are able to achieve at least 84% utilization rate of PEs.\nB. 1D Systolic Primitive for 2D Convolution To support 1D chain architecture, each primitive for convolutions should also be designed as 1D implementation. This is done by pipelining a chain of multiply-accumulate operations (MAC) in Fig. 4 to form a systolic architecture like [16], which we call 1D systolic primitive. Notice that other pipelining schemes may produce more efficient architectures and we will discuss them in the future. In this paper, we mainly focus on how systolic primitives are employed inside the proposed 1D chain architecture for eliminating data locality.\nBefore that, we first present some design details about the 1D systolic primitives. It consists of a group of K2 identical PEs which is shown in Fig. 4(a). The K2 PEs are mapped to a convolutional kernel window. Thus, each PE is in charge of a 16-bit fixed-point MAC operation with a specific kernel weight.\nInstead of reading the data in parallel, data are streamed in and go through every PE along the ifmaps path in Fig. 4(b). This guarantees an invariant input bandwidth requirement regardless of the kernel size K. Whenever a new pixel is streamed in at time t, previous (K-1) pixels during [t-K2+1, t-1] and this pixel constitute a 2D convolution window in ifmaps.\nNote that kernels are stationary inside PEs so the convolutional window of ifmaps pixels during [t-K2+1, t] should be collocated exactly with the stationary kernel weights (matching issue). Sec. IV.C will introduce the proper input pattern and the proposed PE architecture to solve this matching issue.\nMoreover, we can pipeline the critical path (red line) to shorten the output delay by making vertical cuts at each output of MAC (blue dotted line in Fig. 4(b)).\nIn conclusion, we call this architecture as a 1D systolic primitive, in which stationary kernel weights are stored inside each PE and input data serially flow through all the PEs. Each PE can communicate to adjacent PE. Thus, both the input data and the partial sums of MAC during convolution are passed and reused inside the systolic primitive, without accessing external memory. Meanwhile, it has a fixed input bandwidth requirement and a constant output delay regardless of the kernel size. Therefore, this 1D systolic primitive can not only reduce the memory accesses, but also provide a high scalability for various design demands."}, {"heading": "C. Dual-channel PE Architecture", "text": "This part presents the dual-channel PE architecture and the column-wise scan input pattern. The dual channel means there are two data feed channels along the ifmaps path in Fig. 4(b). It can guarantee the high performance with the invariant input bandwidth of systolic primitives.\nWe first explain why a single channel based PE architecture can\u2019t fully utilize the computational resources of the 1D primitive. Let\u2019s still take K=3 as an example in Fig. 5(a). We have mentioned the matching issue that only if the order of [tK2+1, t] ifmaps pixels accurately matches the stationary kernel window can the systolic primitive start to perform a convolution operation. However, inside the 2D ifmaps shown in Fig. 5(a), at most six pixels are overlapped for any two convolutional windows. It means that at least three cycles are\nrequired for fetching the rest three pixels due to the single channel (one ifmaps pixel per cycle). The 1D primitive has to be idle during this period. Therefore, this case shows that onechannel PE architecture can only achieve 1/K (33% in this case) of the peak throughput.\nTherefore, we introduce the dual-channel PE architecture for this matching issue. Fig. 5(b) shows how we can benefit from it. We first give a limitation that it requires K adjacent rows of ofmaps to be processed simultaneously. Then, (2K-1) rows of ifmaps (we refer this as input pattern in the following) are streamed in through two channels at the timestamps shown inside each pixel. Two channels are in charge of odd and even columns respectively. By doing this, we can find pixels of [tK2+1, t] form a convolution window for any given t. Moreover, the pixel orders inside any window follow the column-wise scan order. We call this column-wise scan input pattern and it supports dual-channel PE can continuously start new convolutional operations after the initialization stage. Thus, we can 100% utilize the computational resources with the overheads of adding only one new ifmaps channel.\nThe dual-channel PE architecture is shown in Fig. 6. The dual channels (OddIF and EvenIF) inside each PE are in charge of transferring ifmaps into next PE. A multiplexer decided which channel\u2019s data is sent to MAC. Specifically, OddIF is in charge of odd columns (shown as blue numbers) while EvenIF only cares about the even columns (shown as orange numbers). EvenIF starts working after (K+1) cycle delay than OddIF. Meanwhile, a set of primitive input ports and function units (gray blocks in Fig. 6) are employed to support the systolic chain in Sec. IV.A. Inside each PE, there is a RegisterFilebased internal storage (kMemory) for storing the stationary kernels. The control of kMemory is designed to follow the dataflow. Moreover, we emphasize that we can easily pipeline the MAC path to shorten the critical path for a higher clock frequency, as shown by the red line in Fig. 6."}, {"heading": "D. Dataflow and Memory Hierarchy", "text": "In a recent work [7], a detailed analysis on CNN memoryefficient dataflow and on-chip memory hierarchy have been discussed. We utilize their strategies for testing our proposed 1D chain architecture. The dataflow is modified to support the column-wise scan input pattern requirement as discussed in\nSec. IV.C. The detailed dataflow is shown by a loop structure in Fig. 7. The parallelism of Chain-NN unrolls the loop of ofmaps and the convolutions, marked as ParaTile. Meanwhile, two separate memories, iMemory and oMemory, form the memory hierarchy for data reusing among InnerTile.\nV. EXPERIMENTAL RESULTS"}, {"heading": "A. Methodology", "text": "Chain-NN has been coded by SystemVerilog HDL and synthesized with TSMC 28nm HPC library under slow operation conditions (0.81V, 125oC) using the Synopsys Design Compiler. Layout is done by Encounter as shown in Fig. 8. We implemented a float-point-to-fix-point simulator which is integrated with MatConvnet for generating the test dataset including convolutional layers of pre-trained networks for MNIST, Cifar-10, AlexNet and VGG-16. Verification is done using ModelSim by both function simulation and postsynthesis simulation. The output results of hardware are checked with the simulator results on-the-fly. The power of the Chain-NN is analyzed by Power Compiler under typical operation conditions (0.9V, 25 oC). We generate switching activity interchange format (SAIF) by simulating the postsynthesis designs for power simulation."}, {"heading": "B. Performance", "text": "We instantiate a Chain-NN with 576 PEs, each of which is pipelined into three stages so that the critical path delay is reduced to 1.428ns (700MHz). Theoretically, this design can achieve a peak throughput of 806.4GOPS.\nAlexNet is used to evaluate the realistic performance. We use totally 352KB on-chip memories, including kMemory in each PE, to support the data reuse in AlexNet based on the dataflow in Fig. 7. In detail, we implement 32KB for iMemory, 295KB for kMemory and 25KB for oMemory. In terms of kMemory, 295KB are averagely distributed into 576 PEs, resulting in a capable of 256 kernel weights per PE.\nAlexNet contains five convolutional layers, including totally 666 millions of MACs per 227x227 input image. Fig. 9 presents the time distribution of five convolutional layers in AlexNet. For each batch, it costs 349.92ms while 3.25ms are spent for loading kernels. We emphasize that our architecture can benefit from a large batch size because we just load kernels once per batch, regardless of the batch size. Therefore, 326.2fps/275.6fps can be achieved for 128/4 batch sizes.\nTable IV shows memory accesses and we emphasize Chain-NN greatly reduces the accesses for on-chip ifmaps and kernels accesses as well as off-chip accesses for all operands."}, {"heading": "C. Energy Efficiency", "text": "Energy efficiency is measured by the throughput per watt. Fig. 10 shows Chain-NN consumes 567.5mW and contributes 806.4GOPS, achieving a power efficiency of 1421.0GOPS/W.\nIn details, around 90% of the power consumption is from the 1D chain architecture including kMemory while only 10.55% is cost by the memory hierarchy. The power consumption of memories is greatly reduced in Chain-NN from two aspects. Firstly, we have shortened the length of most data movements from on-chip SRAM level to PE level. In detail, dedicated PEs with column-wise scan input pattern guarantee ifmaps are reused K2 times averagely inside systolic primitives. This can reduce the data movements of ifmaps\nfrom SRAM to only ((2K-1)/K) times. Moreover, kernels are stored inside PEs so all the movements of kernels only happen inside PEs. Secondly, Chain-NN can reduce the maximum memory bandwidth requirement. For example, column-wise scan input pattern allows a kernel to be reused throughout the whole pattern (KE pixels). Therefore, kMemory can stay unchanged and has a very small activity factor of 1/KE. For example, the activity factor is only 2.22% for the third layers of AlexNet. Results also shows that kMemory consumes little power (40.15mW) through it has the largest 295KB size."}, {"heading": "D. Comparison with the State-of-the-art Works", "text": "Two ASIC-based works [10][12] are chosen for comparison.\nDaDianNao [10] belongs to memory-centric architectures while Eyeriss [12] represents the 2D spatial ones.\nTable V shows the comparison results. Our Chain-NN is at least 2.5x energy efficient. Fig. 10 gives the comparison with [10]. We roughly divided the designs into two parts, processor core and memory hierarchy. If only processor cores are considered, [10] can achieve around 3.0TOPS/W while ChainNN is around 1.7TOPS/W. It is reasonable as we have reduced energy costs of data movements due to the analysis in Sec. V.C at the expense of increasing the complexity of the processor core. Therefore, Chain-NN can actually achieves 1421.0GOPS/W if we measure the whole chip (excluding the energy of off-chip DRAM accesses in Table IV).\nMeanwhile, this work involves fewer area overheads when scaled up its parallelism. Considering the complexity of logic\nparts, Chain-NN only costs 6.51k/PE logic gates while [12] is 11.02k/PE. The reasons come from two aspects. First, 1D chain architecture has simplified data paths among PEs compared to the 2D spatial architectures. The reduced paths can still achieve a high throughput, which means that the paths in Chain-NN are utilized more efficiently. Secondly, the fewer memory bandwidth requirements can partially simplify the design complexity of memory hierarchy. These contribute to the 1.7 times area efficiency."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by Waseda University Graduate\nProgram for Embodiment Informatics (FY2013-FY2019)."}], "references": [{"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Neural Information Processing Systems, pp. 1097\u20131105, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Very Deep Convolutional Networks for LargeScale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Do deep nets really need to be deep?", "author": ["J. Ba", "R. Caruana"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "cuDNN: Efficient Primitives for Deep Learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "CoRR, vol. abs/1410.0759, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The neuro vector engine: Flexibility to improve convolutional net efficiency for wearable vision", "author": ["M. Peemen", "R. Shi", "S. Lal", "B. Juurlink", "B. Mesman", "H. Corporaal"], "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), pp. 1604-1609, 2016", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Origami: A Convolutional Network Accelerator", "author": ["L. Cavigelli", "D. Gschwend", "C. Mayer", "S. Willi", "B. Muheim", "L. Benini"], "venue": "Proceedings of GLSVLSI-25, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning", "author": ["T. Chen", "Z. Du", "N. Sun", "J. Wang", "C. Wu", "Y. Chen", "O. Temam"], "venue": "ASPLOS, pp. 269-284, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "DaDianNao: A Machine-Learning Supercomputer", "author": ["Y. Chen"], "venue": "47th International Symposium on Microarchitecture, pp. 609-622, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "ShiDianNao: Shifting vision processing closer to the sensor", "author": ["Z. Du"], "venue": "42nd Annual International Symposium on Computer Architecture (ISCA), pp. 92-104, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "14.5 Eyeriss: An energyefficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "2016 IEEE International Solid-State Circuits Conference (ISSCC), San Francisco, CA, 2016, pp. 262-263.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Cnvlutin: Ineffectualneuron-free deep neural network computin", "author": ["J. Albericio", "T. Aamodt", "T.H. Hetherington"], "venue": "42nd Annual International Symposium on Computer Architecture (ISCA), pp. 1-13, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "EIE: Efficient Inference Engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "2016 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA), pp. 243-254, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A ultra-low-energy convolution engine for fast brain-inspired vision in multicore clusters", "author": ["F. Conti", "L. Benini"], "venue": "2015 Design, Automation & Test in Europe Conference & Exhibition (DATE), 2015, pp. 683-688.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "One-dimensional systolic arrays for multidimensional convolution and resampling", "author": ["HT Kung", "RL Picard"], "venue": "kMemory", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "Many CNN-based algorithms like AlexNet [1] and VGG [2] have proved to be stronger than conventional algorithms.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "Many CNN-based algorithms like AlexNet [1] and VGG [2] have proved to be stronger than conventional algorithms.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Further, there is a trend that CNN is towards deeper and larger for a better performance [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "For example, ResNet [3] has recently proved a network with more than a thousand layers can further enhance the performance compared to a shallower network.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "The designs on CPU or GPU like [5] can achieve a high reconfigurability at the expense of high energy costs and usually suffer from the Von-Neumann bottleneck which limits their performance.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 22, "endOffset": 27}, {"referenceID": 6, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 7, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 8, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 9, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 10, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 11, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 12, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": "Architectures on FPGA [6-7] or ASIC [8-15] are good candidates as these architectures can be totally customized to specific high-performance and energy-efficient needs.", "startOffset": 36, "endOffset": 42}, {"referenceID": 10, "context": "For example, [12] builds a 2D PE array, which has an on-chip network and inter-PE communications.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "According to [16], the data movements of convolutional operands can be more expensive than ALU operations under the existing techniques.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "They highly rely on memories for huge amount of data movements like [9,10,13].", "startOffset": 68, "endOffset": 77}, {"referenceID": 8, "context": "They highly rely on memories for huge amount of data movements like [9,10,13].", "startOffset": 68, "endOffset": 77}, {"referenceID": 11, "context": "They highly rely on memories for huge amount of data movements like [9,10,13].", "startOffset": 68, "endOffset": 77}, {"referenceID": 6, "context": "Some achieve their reconfigurability by utilizing fewer PEs with fewer feeding data from memories, resulting in a waste of computing resources [8,15].", "startOffset": 143, "endOffset": 149}, {"referenceID": 13, "context": "Some achieve their reconfigurability by utilizing fewer PEs with fewer feeding data from memories, resulting in a waste of computing resources [8,15].", "startOffset": 143, "endOffset": 149}, {"referenceID": 9, "context": "This solution can reduce the amount of data movements and have been employed in many works like [11,12].", "startOffset": 96, "endOffset": 103}, {"referenceID": 10, "context": "This solution can reduce the amount of data movements and have been employed in many works like [11,12].", "startOffset": 96, "endOffset": 103}, {"referenceID": 14, "context": "4 to form a systolic architecture like [16], which we call 1D systolic primitive.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "Comparison with the State-of-the-art Works Two ASIC-based works [10][12] are chosen for comparison.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Comparison with the State-of-the-art Works Two ASIC-based works [10][12] are chosen for comparison.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "DaDianNao [10] belongs to memory-centric architectures while Eyeriss [12] represents the 2D spatial ones.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "DaDianNao [10] belongs to memory-centric architectures while Eyeriss [12] represents the 2D spatial ones.", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "10 gives the comparison with [10].", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "If only processor cores are considered, [10] can achieve around 3.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "51k/PE logic gates while [12] is 11.", "startOffset": 25, "endOffset": 29}], "year": 2017, "abstractText": "Deep convolutional neural networks (CNN) have shown their good performances in many computer vision tasks. However, the high computational complexity of CNN involves a huge amount of data movements between the computational processor core and memory hierarchy which occupies the major of the power consumption. This paper presents Chain-NN, a novel energy-efficient 1D chain architecture for accelerating deep CNNs. Chain-NN consists of the dedicated dual-channel process engines (PE). In Chain-NN, convolutions are done by the 1D systolic primitives composed of a group of adjacent PEs. These systolic primitives, together with the proposed column-wise scan input pattern, can fully reuse input operand to reduce the memory bandwidth requirement for energy saving. Moreover, the 1D chain architecture allows the systolic primitives to be easily reconfigured according to specific CNN parameters with fewer design complexity. The synthesis and layout of Chain-NN is under TSMC 28nm process. It costs 3751k logic gates and 352KB on-chip memory. The results show a 576-PE Chain-NN can be scaled up to 700MHz. This achieves a peak throughput of 806.4GOPS with 567.5mW and is able to accelerate the five convolutional layers in AlexNet at a frame rate of 326.2fps. 1421.0GOPS/W power efficiency is at least 2.5 to 4.1x times better than the state-of-the-art works. Keywords\u2014convolutional neural networks; CNN; accelerator; ASIC; power efficiency; memory bandwidth", "creator": "Microsoft\u00ae Word 2016"}}}