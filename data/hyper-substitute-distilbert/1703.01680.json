{"id": "1703.01680", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2017", "title": "Multi-Objective Non-parametric Sequential Prediction", "abstract": "online - learning research has continually been used into obtaining an singular procedure. in solving real - computer applications, however, several objective functions have effectively reside considered simultaneously. theoretically, synthetic algorithm for competing with the objective functions by the ph. i. er. case have been presented. in seminal paper, we link the multi - constraint framework to the case termed stationary binary ergodic processes, both allowing dependencies among comparisons. we first identify at intrinsic lower bound for any nonlinear oracle and then present an algorithm so predictions have the maximum solution while fulfilling any feasible and potentially constraining criterion.", "histories": [["v1", "Sun, 5 Mar 2017 22:41:00 GMT  (209kb)", "https://arxiv.org/abs/1703.01680v1", null], ["v2", "Thu, 9 Mar 2017 19:46:50 GMT  (208kb)", "http://arxiv.org/abs/1703.01680v2", null], ["v3", "Sun, 19 Mar 2017 15:50:42 GMT  (211kb)", "http://arxiv.org/abs/1703.01680v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guy uziel", "ran el-yaniv"], "accepted": true, "id": "1703.01680"}, "pdf": {"name": "1703.01680.pdf", "metadata": {"source": "CRF", "title": "Multi-Objective Non-parametric Sequential Prediction", "authors": ["Guy Uziel"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n01 68\n0v 3\n[ cs\n.L G\nOnline-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion."}, {"heading": "1 Introduction", "text": "In the traditional online learning setting, and in particular in sequential prediction under uncertainty, the learner is evaluated by a single loss function that is not completely known at each iteration [9]. When dealing with multiple objectives, since it is impossible to simultaneously minimize all of the objectives, one objective is chosen as the main function to minimize, leaving the others to be bound by pre-defined thresholds. Methods for dealing with one objective function can be transformed to deal with several objective functions by giving each objective a pre-defined weight. The difficulty, however, lies in assigning an appropriate weight to each objective in order to keep the objectives below a given threshold. This approach is very problematic in real world applications, where the player is required to to satisfy certain constraints. For example, in online portfolio selection [16, 7], the player may want to maximize wealth while keeping the risk (i.e., variance) contained below a certain threshold. Another example is the Neyman-Pearson (NP) classification paradigm (see, e.g., [22]) (which extends the objective in classical binary classification) where the goal is to learn a classifier achieving low type II error whose type I error is kept below a given threshold.\nRecently, [19] presented an algorithm for dealing with the case of one main objective and fully-known constraints. In a subsequent work, [20] proposed a framework for\ndealing with multiple objectives in the stochastic i.i.d. case, where the learner does not have full information about the objective functions. They proved that if there exists a solution that minimizes the main objective function while keeping the other objectives below given thresholds, then their algorithm will converge to the optimal solution.\nIn this work, we study online prediction with multiple objectives but now consider the challenging general case where the unknown underlying process is stationary and ergodic, thus allowing observations to depend on each other arbitrarily. We consider a non-parametric approach, which has been applied successfully in various application domains. For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome. Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6]. A common theme to all these results is that the asymptotically optimal strategies are constructed by combining the predictions of many simple experts. The algorithm presented in this paper utilizes as a sub-routine the Weak Aggregating Algorithm of [24], and [15] to handle multiple objectives. While we discuss here the case of only two objective functions, our theorems can be extended easily to any fixed number of functions.\nOutline The paper is organized as follows: In Section 2, we define themulti-objective optimization framework under a jointly stationary and ergodic process. In Section 3, we identify an asymptotic lower-bound for any prediction strategy. In Section 4, we present Algorithm 1, which asymptotically achieves an optimal feasible solution."}, {"heading": "2 Problem Formulation", "text": "We consider the following prediction game. Let X , [\u2212D,D]d \u2282 Rd be a compact observation space where D > 0. At each round, n = 1, 2, . . ., the player is required to make a prediction yn \u2208 Y , where Y \u2282 Rm is a compact and convex set, based on past observations, Xn\u221211 , (x1, . . . , xn\u22121) and, xi \u2208 X (X01 is the empty observation). After making the prediction yn, the observation xn is revealed and the player suffers two losses, u(yn, xn) and c(yn, xn), where u and c are real-valued continuous functions and convex w.r.t. their first argument. We view the player\u2019s prediction strategy as a sequence S , {Sn}\u221en=1 of forecasting functions Sn : X (n\u22121) \u2192 Y; that is, the player\u2019s prediction at round n is given by Sn(X n\u22121 1 ). Throughout the paper we assume that x1, x2, . . . are realizations of random variables X1, X2, . . . such that the stochastic process (Xn) \u221e \u2212\u221e is jointly stationary and ergodic and P(Xi \u2208 X ) = 1. The player\u2019s goal is to play the game with a strategy that minimizes the average u-loss, 1 N \u2211N i=1 u(S(X i\u22121 1 ), xi), while keeping the average c-loss 1 N \u2211N i=1 c(S(X i\u22121 1 ), xi) bounded below a prescribed threshold \u03b3. Formally, we define the following:\nDefinition 1 (\u03b3-boundedness). A prediction strategy S will be called \u03b3-bounded if\nlim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nc(Si(X i\u22121 1 ), Xi)\n)\n\u2264 \u03b3\nalmost surely. The set of all \u03b3-bounded strategies will be denoted S\u03b3 .\nDefinition 2 (\u03b3-feasible process). We say that the stationary and ergodic process {Xi}\u221e\u2212\u221e is \u03b3-feasible w.r.t. the functions u and c, if for P\u221e, the regular conditional probability distribution of X0 given F\u221e (the \u03c3-algebra generated by the infinite past X\u22121, X\u22122, . . .), and for a threshold \u03b3 > 0, if there exists some y\n\u2032 \u2208 Y such that EP\u221e [c(y \u2032, X0)] < \u03b3.\nIf \u03b3-feasibility holds, then we will denote by y\u2217\u221e (y \u2217 \u221e is not necessarily unique) the\nsolution to the following minimization problem:\nminimize y\u2208Y EP\u221e [u(y,X0)] subject to EP\u221e [c(y,X0)] \u2264 \u03b3, (1)\n(1) and we define the \u03b3-feasible optimal value as\nV\u2217 = E [EP\u221e [u(y\u2217\u221e, X0)]] a.s.\nNote that problem (1) is a convex minimization problem over Y , which in turn is a compact and convex subset of Rm. Therefore, the problem is equivalent to finding the saddle point of the Lagrangian function [3], namely,\nmin y\u2208Y max \u03bb\u2208R+\nL(y, \u03bb),\nwhere the Lagrangian is\nL(y, \u03bb) , (EP\u221e [u(y,X0)] + \u03bb (EP\u221e [c(y,X0)]\u2212 \u03b3)) .\nWe denote the optimal dual by \u03bb\u2217\u221e and assume that \u03bb \u2217 \u221e is unique. Moreover, we set a constant 1 \u03bbmax such that \u03bbmax > \u03bb \u2217 \u221e, and set \u039b , [0, \u03bbmax]. We also define the instantaneous Lagrangian function as\nl(y, \u03bb, x) , u(y, x) + \u03bb (c(y, x)\u2212 \u03b3) . (2)\nIn Brief, we are seeking a strategy S \u2208 S\u03b3 that is as good as any other \u03b3-bounded strategy, in terms of the average u-loss, when the underlying process is \u03b3-feasible. Such a strategy will be called \u03b3-universal."}, {"heading": "3 Optimallity of V\u2217", "text": "In this section, we prove that the average u-loss of any \u03b3-bounded prediction strategy cannot be smaller than V\u2217, the \u03b3-feasible optimal value. This result is a generalization of the well-known result of [1] regarding the best possible outcome under a single objective. Before stating and proving this optimallity result, we state one known lemma and state and prove two lemmas that will be used repeatedly in this paper. The first lemma is known as Breiman\u2019s generalized ergodic theorem. The second and the third lemmas concern the continuity of the saddle point w.r.t. the probability distribution.\n1This can be done, for example, by imposing some regularity conditions on the constraint function (see,\ne.g., [20]).\nLemma 1 (Ergodicity, [8]). Let X = {Xi}\u221e\u2212\u221e be a stationary and ergodic process. For each positive integer i, let Ti denote the operator that shifts any sequence by i places to the left. Let f1, f2, . . . be a sequence of real-valued functions such that limn\u2192\u221e fn(X) = f(X) almost surely, for some function f . Assume thatE supn |fn(X)| < \u221e. Then,\nlim n\u2192\u221e\n1\nn\nn \u2211\ni=1\nfi(T i X) = Ef(X)\nalmost surely.\nLemma 2 (Continuity andMinimax). LetY,\u039b,X be compact real spaces. l : Y \u00d7\u039b\u00d7X \u2192 R be a continuous function. Denote by P(X ) the space of all probability measures on X (equipped with the topology of weak-convergence). Then the following function L\u2217 : P(X ) \u2192 R is continuous\nL\u2217(Q) = inf y\u2208Y sup \u03bb\u2208\u039b EQ [l(y, \u03bb, x)] . (3)\nMoreover, for any Q \u2208 P(X ),\ninf y\u2208Y sup \u03bb\u2208\u039b EQ [l(y, \u03bb, x)] = sup \u03bb\u2208\u039b inf y\u2208Y EQ [l(y, \u03bb, x)] .\nProof. Y,\u039b,X are compact, implying that the function l (y, \u03bb, x) is bounded. Therefore, the function L : Y \u00d7 \u039b \u00d7 P(X ) \u2192 R, defined as\nL (y, \u03bb,Q) = EQ [l (y, \u03bb, x)] , (4)\nis continuous. By applying Proposition 7.32 from [4], we have that sup\u03bb\u2208\u039b EQ [l(y, \u03bb,X)] is continuous inQ\u00d7Y . Again applying the same proposition, we get the desired result. The last part of the lemma follows directly from Fan\u2019s minimax theorem [10].\nLemma 3 (Continuity of the optimal selection). Let Y,\u039b,X be compact real spaces, and let L be as defined in Equation (4). Then, there exist two measurable selection functions hX ,h\u03bb such that\nhy(Q) \u2208 argmin y\u2208Y\n(\nmax \u03bb\u2208\u039b L(y, \u03bb,Q)\n)\n,\nh\u03bb(Q) \u2208 argmax \u03bb\u2208\u039b\n(\nmin y\u2208Y L(y, \u03bb,Q)\n)\nfor any Q \u2208 P(X ). Moreover, let L\u2217 be as defined in Equation (3). Then, the set\nGr(L\u2217) , {(u\u2217, v\u2217,Q) | u\u2217 \u2208 hy(Q), v\u2217 \u2208 h\u03bb(Q),Q \u2208 P(X )},\nis closed in Y \u00d7 \u039b\u00d7 P(X ).\nProof. The first part of the proof follows immediately from the minimax measurable theorem of [21] due to the compactness of Y,\u039b,X and the properties of the loss function L. The proof of the second part is similar to the one presented in Theorem 3 of [2]. In order to show that Gr(L\u2217) is closed, it is enough to show that if (i) Qn \u2192 Q\u221e in P(X ); (ii) un \u2192 u\u221e in Y; (iii) vn \u2192 v\u221e in \u039b and (iv) un \u2208 hy(Qn), vn \u2208 h\u03bb(Qn) for all n, then,\nu\u221e \u2208 hy(Q\u221e), v\u221e \u2208 h\u03bb(Q\u221e).\nThe function L(y, \u03bb,Q), as defined in Equation (4), is continuous. Therefore,\nlim n\u2192\u221e L(un, vn,Qn) = L(u\u221e, v\u221e,Q\u221e).\nIt remains to show that u\u221e \u2208 hy(Q\u221e) and v\u221e \u2208 h\u03bb(Q\u221e). From the optimality of un and vn, we obtain\nL(u\u221e, v\u221e,Q\u221e) = lim n\u2192\u221e L(un, vn,Qn) = lim n\u2192\u221e\nL\u2217(Qn). (5)\nFinally, from the continuity of L\u2217 (Lemma 2), we get\n(5) = L\u2217( lim n\u2192\u221e Qn) = L \u2217(Q\u221e),\nwhich gives the desired result.\nCorollary 1. Under the conditions of Lemma 3. Define Ln(y, \u03bb,Q) = L(y, \u03bb,Q) + ||y||2\u2212||\u03bb||2\nn and denote hyLn(Qn), h \u03bb Ln (Qn) to be the measurable selection functions of\nLn. If Qn \u2192 Q\u221e weakly in P(X ) and un \u2208 hyLn(Qn), vn \u2208 h\u03bbLn(Qn), then\nLn(un, vn,Qn) \u2192 L(u\u221e, v\u221e,Q\u221e)\nalmost surely for u\u221e \u2208 hy(Q\u221e) and v\u221e \u2208 h\u03bb(Q\u221e). Proof. Denote u\u0302n \u2208 hy(Q\u221e) and v\u0302n \u2208 h\u03bb(Q\u221e)\n|Ln(un, vn,Qn)\u2212 L(u\u221e, v\u221e,Q\u221e)| \u2264 |Ln(un, vn,Qn)\u2212 L(u\u0302n, v\u0302n,Qn)|+ |L(u\u0302n, v\u0302n,Qn)\u2212 L(u\u221e, v\u221e,Q\u221e)|. (6)\nNote that for every n and for constant E > 0,\nmin y\u2208Y max \u03bb\u2208\u039b\nL(y, \u03bb,Q)\u2212 ||\u03bbmax|| 2\nn \u2264 min y\u2208Y max \u03bb\u2208\u039b Ln(y, \u03bb,Q)\n= min y\u2208Y max \u03bb\u2208\u039b\n(\nEQ [l(y, \u03bb,X)] + ||y||2 \u2212 ||\u03bb||2\nn\n)\n\u2264 min y\u2208Y max \u03bb\u2208\u039b\nL(y, \u03bb,Q) + E\nn .\nThus, for some constantC, |Ln(un, vn,Qn)\u2212L(u\u221e, v\u221e,Q\u221e)| < Cn and fromLemma 3, the last summand also converges to 0 as n approaches\u221e, we get the desired result, and clearly, if hy(Q\u221e) and h\n\u03bb(Q\u221e) are singletons, then, the only accumulation point of {(vn, un)}\u221en=1 is (v\u221e, u\u221e).\nThe importance of Lemma 3 stems from the fact that it proves the continuity properties of the multi-valued correspondences Q \u2192 hy(Q) and Q \u2192 h\u03bb(Q). This leads to the knowledge that if for the limiting distribution,Q\u221e, the optimal set is a singleton, then Q \u2192 hy(Q) and Q \u2192 h\u03bb(Q) are continuous in Q\u221e. We are now ready to prove the optimality of V\u2217.\nTheorem 1 (Optimality of V\u2217). Let {Xi}\u221e\u2212\u221e be a \u03b3-feasible process. Then, for any strategy S \u2208 S\u03b3 , the following holds a.s.\nlim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nu(S(X i\u221211 ), Xi) \u2265 V\u2217.\nProof. For any given strategy S \u2208 S\u03b3 , we will look at the following sequence:\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb\u0303 \u2217 i , Xi). (7)\nwhere \u03bb\u0303\u2217i \u2208 h\u03bb(PXi|Xi\u221211 ) Observe that\n(7) = 1\nN\nN \u2211\ni=1\nE\n[\nl(S(X i\u221211 ), \u03bb\u0303 \u2217 i , Xi) | X i\u221211\n] \u2212 1 N\nN \u2211\ni=1\n(l(S(X i\u221211 ), \u03bb\u0303 \u2217 i , Xi)\n\u2212E [\nl(S(X i\u221211 ), \u03bb\u0303 \u2217 i , X) | X i\u221211\n]\n).\nSince Ai = l(S(X i\u22121 1 ), \u03bb\u0303 \u2217 i , Xi)\u2212E\n[\nl(S(X i\u221211 ), \u03bb\u0303 \u2217 i , Xi) | X i\u221211\n]\nis a martingale dif-\nference sequence, the last summand converges to 0 a.s., by the strong law of large numbers (see, e.g., [23]). Therefore,\nlim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb\u0303 \u2217 i , Xi) = lim inf\nN\u2192\u221e\n1\nN\nN \u2211\ni=1\nE\n[\nl(S(X i\u221211 ), \u03bb\u0303 \u2217 i , Xi) | X i\u221211\n]\n\u2265 lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nmin y\u2208Y() E\n[ l(y, \u03bb\u0303\u2217i , Xi) | X i\u221211 ] , (8)\nwhere the minimum is taken w.r.t. all the \u03c3(X i\u221211 )-measurable functions. Because the process is stationary, we get for \u03bb\u0302\u2217i \u2208 h\u03bb(PX0|X\u221211\u2212i),\n(8) = lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nmin y\u2208Y() E\n[ l(y, \u03bb\u0302\u2217i , X0) | X\u221211\u2212i ]\n(9)\n= lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nL\u2217(PX0|X\u221211\u2212i ). (10)\nUsing Levy\u2019s zero-one law, PX0|X\u221211\u2212i \u2192 P\u221e weakly as i approaches \u221e and from Lemma 2 we know that L\u2217 is continuous. Therefore, we can apply Lemma 1 and get\nthat a.s.\n(10) = E [L\u2217(P\u221e)] = E [EP\u221e [l (y \u2217 \u221e, \u03bb \u2217 \u221e, X0)]] = E [L (y\u2217\u221e, \u03bb\u2217\u221e, X0)] . (11)\nNote also, that due to the complementary slackness condition of the optimal solution, i.e., \u03bb\u2217\u221e(EP\u221e [c(y \u2217 \u221e, X0)]\u2212 \u03b3) = 0, we get\n(11) = E [EP\u221e [u (y \u2217 \u221e, X0)]] = V\u2217.\nFrom the uniqueness of \u03bb\u2217\u221e, and using Lemma 3 \u03bb\u0302 \u2217 i \u2192 \u03bb\u2217\u221e as i approaches\u221e. Moreover, since l is continuous on a compact set, l is also uniformly continuous. Therefore, for any given \u01eb > 0, there exists \u03b4 > 0, such that if |\u03bb\u2032 \u2212 \u03bb| < \u03b4, then\n|l(y, \u03bb\u2032, x)\u2212 l(y, \u03bb, x)| < \u01eb\nfor any y \u2208 Y and x \u2208 X . Therefore, there exists i0 such that if i > i0 then |l(y, \u03bb\u0302\u2217i , x)\u2212 l(y, \u03bb\u2217\u221e, x)| < \u01eb for any y \u2208 Y and x \u2208 X . Thus,\nlim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb \u2217 \u221e, Xi)\u2212 lim inf\nN\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb\u0302 \u2217 i , Xi)\n= lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb \u2217 \u221e, Xi) + lim sup\nN\u2192\u221e\n1\nN\nN \u2211\ni=1\n\u2212l(S(X i\u221211 ), \u03bb\u0302\u2217i , Xi)\n\u2265 lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb\u0302 \u2217 i , Xi)\u2212\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb \u2217 \u221e, Xi) \u2265 \u2212\u01eb a.s.,\nand since \u01eb is arbitrary,\nlim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb \u2217 \u221e, Xi) \u2265 lim inf\nN\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb\u0302 \u2217 i , Xi).\nTherefore we can conclude that\nlim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(S(X i\u221211 ), \u03bb \u2217 \u221e, Xi) \u2265 V\u2217 a.s.\nWe finish the proof by noticing that since S \u2208 S\u03b3 , then by definition\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\nc(S(X i\u221211 ), Xi) \u2264 \u03b3 a.s.\nand since \u03bb\u2217\u221e is non negative, we will get the desired result.\nThe above lemma also provides the motivation to find the saddle point of the Lagrangian L. Therefore, for the reminder of the paper we will use the loss function l as defined in Equation 2.\nAlgorithm 1 Minimax Histogram Based Aggregation (MHA)\nInput: Countable set of experts {Hk,h} , y0 \u2208 Y \u03bb0 \u2208 \u039b, initial probability {\u03b1k,h}, For n = 0 to\u221e Play yn, \u03bbn. Nature reveals xn Suffer loss l(yn, \u03bbn, xn). Update the cumulative loss of the experts\nlk,hy,n ,\nn \u2211\ni=0\nl(yik,h, \u03bbi, xi) l k,h \u03bb,n ,\nn \u2211\ni=0\nl(yi, \u03bb i k,h, xi)\nUpdate experts\u2019 weights\nwy,(k,h)n , \u03b1k,h exp\n(\n\u2212 1\u221a n lk,hy,n\n)\np y,(k,h) n+1 ,\nw y,(k,h) n+1\n\u2211\u221e h=1 \u2211\u221e k=1 w y,(k,h) n+1\nUpdate experts\u2019 weights w \u03bb,(k,h) n+1\nw \u03bb,(k,h) n+1 , \u03b1k,h exp\n(\n1\u221a n lk,h\u03bb,n\n)\np \u03bb,(k,h) n+1 =\nw \u03bb,(k,h) n+1\n\u2211\u221e h=1 \u2211\u221e k=1 w \u03bb,(k,h) n+1\nChoose yn+1 and \u03bbn+1 as follows\nyn+1 = \u2211\nk,h\np y,(k,h) n+1 y n+1 k,h \u03bbn+1 =\n\u2211\nk,h\np \u03bb,(k,h) n+1 \u03bb n+1 k,h\nEnd For"}, {"heading": "4 Minimax Histogram Based Aggregation", "text": "We are now ready to present our algorithmMinimax Histogram based Aggregation (MHA) and prove that its predictions are as good as the best strategy. By Theorem 1 we can restate our goal: find a prediction strategy S \u2208 S\u03b3 such that for any \u03b3-feasible process {Xi}\u221e\u2212\u221e the following holds:\nlim N\u2192\u221e\n1\nN\nN \u2211\ni=1\nu(S(X i\u221211 ), Xi) = V\u2217 a.s.\nSuch a strategy will be called \u03b3-universal. We do so by maintaining a countable set of\nexperts {Hk,h}, where an expertHk,l will output a pair (yik,l, \u03bbik,l) \u2208 Y\u00d7\u039b at round i. Our algorithm outputs at round i a pair (yi, \u03bbi) \u2208 Y \u00d7\u039b where the sequence of predictions y1, y2, . . . tries to minimize the average loss 1 N \u2211N i=1 l(y, \u03bbi, xi) and the sequence of predictions \u03bb1, \u03bb2, . . . tries to maximize the average loss 1 N \u2211N i=1 l(yi, \u03bb, xi). Each of yi and \u03bbi is the aggregation of predictions y i k,l and \u03bb i k,l, k, l = 1, 2, . . . , respectively. In order to ensure that the performance of MHA will be as good as any other expert for both the y and the \u03bb predictions, we apply the Weak Aggregating Algorithm of [24], and [15] twice simultaneously. In Theorem 2 we prove that there exists a countable set of experts whose selection of points converges to the optimal solution. Then, in Theorem 3 we prove that MHA applied on the experts defined in Theorem 2 generates a sequence of predictions that is \u03b3-bounded and as good as any other strategy w.r.t. any \u03b3-feasible process.\nTheorem 2. Assume that {Xi}\u221e\u2212\u221e is a \u03b3-feasible process. Then, it is possible to construct a countable set of experts {Hk,h} for which\nlim k\u2192\u221e lim h\u2192\u221e lim n\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yik,h, \u03bb i k,h, Xi) = V\u2217 a.s.,\nwhere (yik,h, \u03bb i k,h) are the predictions made by expert Hk,h at round i. Proof. We start by defining a countable set of experts {Hk,h} as follow: For h = 1, 2, . . ., let Ph = {Ah,j | j = 1, 2, ...,mh} be a sequence of finite partitions of X such that: (i) any cell of Ph+1 is a subset of a cell of Ph for any h. Namely, Ph+1 is a refinement of Ph; (ii) for a set A, if diam(A) = supx,y\u2208A ||x \u2212 y|| denotes the diameter of A, then for any sphere B centered at the origin,\nlim h\u2192\u221e max j:Ah,j\u2229B 6=\u2205 diam(Ah,j) = 0.\nDefine the corresponding quantizer qh(x) = j, if x \u2208 Ah,j . Thus, for any n and Xn1 , we define Qh(X n 1 ) as the sequence qh(x1), . . . , qh(xn). For expert Hk,h, we define for k > 0, a k-long string of positive integers, denoted by w, the following set,\nB w,(1,n\u22121) k,h , {xi | k < i < n, Qh(X i\u22121i\u2212k) = w}.\nWe define also\nhyk,h(X n\u22121 1 , w) , argmin\ny\u2208Y\n\n  max \u03bb\u2208\u039b\n1\n|Bw,(1,n\u22121)k,h |\n\u2211\nxi\u2208B w,(1,n\u22121) k,h\nlk,l,n(y, \u03bb, xi)\n\n \nh\u03bbk,h(X n\u22121 1 , w) , argmax\n\u03bb\u2208\u039b\n\n  min y\u2208Y\n1\n|Bw,(1,n\u22121)k,h |\n\u2211\nxi\u2208B w,(1,n\u22121) k,h\nlk,l,n(y, \u03bb, xi)\n\n \nfor\nlk,h,n(y, \u03bb, x) , l(y, \u03bb, x) + ( ||y||2 \u2212 ||\u03bb||2 )\n(\n1 n + 1 h + 1 k\n)\nand we will set hyk,h(X n\u22121 1 , w) = y0 and h \u03bb k,h(X n\u22121 1 , w) = \u03bb0 for arbitrary (y0, \u03bb0) \u2208 Y \u00d7 \u039b if Bw,(1,n\u22121)k,h is empty. Using the above, we define the predictions of Hk,h to be:\nHyk,h(X n\u22121 1 ) = h y k,h(X n\u22121 1 , Q(X n\u22121 n\u2212k)), n = 1, 2, 3.... H\u03bbk,h(X n\u22121 1 ) = h \u03bb k,h(X n\u22121 1 , Q(X n\u22121 n\u2212k)), n = 1, 2, 3....\nWewill add two experts: H0,0 whose predictions are always (y0, \u03bbmax) andH\u22121,\u22121 whose predictions are always (y0, 0).\nFixing k, h > 0 andw, we will define a (random)measureP (k.h) j,w that is the measure\nconcentrated on the set B w,(0,1\u2212j) k,h , defined by\nP (k,h) j,w (A) =\n\u2211\nXi\u2208B w,(0,1\u2212j) k,h\n1A(Xi)\n|Bw,(0,1\u2212j)k,h | ,\nwhere 1A denotes the indicator function of the set A \u2282 X . If the above set Bwk,h is empty, then let P\n(k,h) j,w (A) = \u03b4(x \u2032) be the probability measure concentrated on arbitrary vector x\u2032 \u2208 X .\nIn other words, P (k.h) j,w (A) is the relative frequency of the the vectors amongX1\u2212j+k, . . . , X0\nthat fall in the set A. Applying Lemma 1 twice, it is straightforward to prove that for all w, w.p. 1\nP (k,h) j,w \u2192\n{\nPX0|Gl(X\u22121 \u2212k\n)=w P(Gl(X \u22121 \u2212k) = w) > 0\n\u03b4(x\u2032) otherwise\nweakly as j \u2192 \u221e, where PX0|Gl(X\u22121 \u2212k )=w denotes the distribution of the vector X0 conditioned on the event Gl(X \u22121 \u2212k) = w. To see this, let f be a bounded continuous function. Then,\n\u222b\nf(x)P (k,h) j,w (dx) =\n1 |1\u2212j+k|\n\u2211\nXi\u2208B w,(0,1\u2212j) k,h\nf(Xi)\n1 |1\u2212j+k| |B w,(0,1\u2212j) k,h |\n\u2192 E\n[\nf(X0)1Gl(X\u22121 \u2212k\n)=w(X0) ]\nP(Gl(X \u22121 \u2212k) = w)\n= E [ f(X0) | Gl(X\u22121\u2212k) = w ] ,\nand in case P(||X\u22121\u2212k \u2212 s|| \u2264 c/l) = 0, then w.p. 1, P (k,h) j,w is concentrated on x \u2032 for all j. We will denote the limit distribution of P (k,h) j,w by P \u2217(k,h) w .\nBy definition, (\nhyk,h(X \u22121 1\u2212n, w), h \u03bb k,h(X \u22121 1\u2212n, w)\n)\nis the minimax of ln,k,h w.r.t.\nP (k,h) j,w . The sequence of functions ln,k,h converges uniformly as n approaches\u221e to\nlk,h(y, \u03bb, x) = l(y, \u03bb, x) + ( ||y||2 \u2212 ||\u03bb||2 )\n(\n1 h + 1 k\n)\n.\nNote also that for any fixed Q, Lk,h(y, \u03bb,Q) = EQ [lk,h(y, \u03bb,X)] is strictly convex in y and strictly concave in \u03bb, and therefore, has a unique saddle-point (see, e.g., [18]). Therefore, since w is arbitrary, and following a Corollary 1 of Lemma 3, we get that a.s.\nynk,h \u2192 y\u2217k,h \u03bbnk,h \u2192 \u03bb\u2217k,h,\nwhere (\ny\u2217k,h, \u03bb \u2217 k,h\n)\nis the minimax of Lk,h w.r.t. P \u2217(k,h)\nX \u22121 \u2212k\n. Thus, we can apply Lemma 1\nand conclude that as N approaches\u221e,\n1\nN\nN \u2211\ni=1\nl(yik,h, \u03bb i k,h, Xi) \u2192 E\n[\nl(y\u2217k,h, \u03bb \u2217 k,h, X0)\n]\n.\na.s.. We now evaluate\nlim h\u2192\u221e\nE [ l(y\u2217k,h, \u03bb \u2217 k,h, X0) ] .\nUsing the properties of the partition Ph (see, e.g., [11, 13]), we get that\nP \u2217(k,h)\nX \u22121 \u2212k \u2192 P{X0|X\u22121 \u2212k}\nweakly as h \u2192 \u221e. Moreover, the sequence of functions lk,h converges uniformly as h approaches\u221e\nlk(y, \u03bb, x) = l(y, \u03bb, x) + ||y||2 \u2212 ||\u03bb||2\nk .\nNote also, that for any fixed Q, Lk(y, \u03bb,Q) = EQ [lk(y, \u03bb,X)] is strictly convexconcave, and therefore, has a unique saddle point. Accordingly, by applying Corollary 1 again, we get that a.s.\ny\u2217k,h \u2192 y\u2217k \u03bb\u2217k,h \u2192 \u03bb\u2217k,\nwhere (y\u2217k, \u03bb \u2217 k) is the minimax of Lk w.r.t. P{X0|X\u22121 \u2212k}. Therefore, as h approaches\u221e,\nl(y\u2217k,h, \u03bb \u2217 k,h, X0) \u2192 l (y\u2217k, \u03bb\u2217k, X0)\na.s.. Thus, by Lebesgue\u2019s dominated convergence,\nlim h\u2192\u221e\nE [ l(y\u2217k,h, \u03bb \u2217 k,h, X0) ] = E [l (y\u2217k, \u03bb \u2217 k, X0)] .\nNotice that for anyQ \u2208 P(X ), the distance between the saddle point of Lk w.r.t. Q and the the saddle point of L w.r.t. Q converges to 0 as k approaches\u221e. To see this, notice that\nmin y\u2208Y max \u03bb\u2208\u039b\nL(y, \u03bb,Q)\u2212 ||\u03bbmax|| 2\nk \u2264 min y\u2208Y max \u03bb\u2208\u039b Lk(y, \u03bb,Q)\n\u2264 min y\u2208Y max \u03bb\u2208\u039b\nL(y, \u03bb,Q) + E\nk\nfor some constant E, since Y is bounded. The last part in our proof will be to show that if (y\u0302\u2217k, \u03bb\u0302 \u2217 k) is the minimax of L w.r.t. P{X0|X\u22121 \u2212k}, then as k approaches \u221e,\nE\n[ l (\ny\u0302\u2217k, \u03bb\u0302 \u2217 k, X0\n)]\nwill converge a.s. to V\u2217 and so E [l (y\u2217k, \u03bb\u2217k, X0)]. To show this, we will use the sub-martingale convergence theorem twice. First, we\ndefine Zk as\nZk , min y\u2208Y() E\n[\nmax \u03bb\u2208\u039b()\nE [ l (y, \u03bb,X0) | X\u22121\u2212\u221e ] | X\u22121\u2212k ] ,\nwhere the minimum is taken w.r.t. all \u03c3(X\u22121\u2212k)-measurable strategies and the maximum is taken w.r.t. all \u03c3(X\u22121\u2212\u221e)-measurable strategies. Notice that Zk is a super-martingale. We can see this by using the tower property of conditional expectations,\nE[Zk+1 | X\u22121\u2212k ] = E [ E [ Zk+1 | X\u22121\u2212k\u22121 ] | X\u22121\u2212k ]\nand since Zk+1 is the optimal choice in Y w.r.t. to X\u22121\u2212k\u22121,\n\u2264 E [ E[Zk | X\u22121\u2212k\u22121] | X\u22121\u2212k ] = E[Zk | X\u22121\u2212k ] = Zk.\nNote also that E[Zk] is uniformly bounded. Therefore, we can apply the supermartingale convergence theorem and get that Zk \u2192 Z\u221e a.s., where,\nZ\u221e = E [ l(y\u2217\u221e, \u03bb \u2217 \u221e, X0) | X\u22121\u2212\u221e ] = V\u2217,\nand by using Lebesgue\u2019s dominated convergence theorem, alsoE[Zk] \u2192 E[Z\u221e] = V\u2217. Using the same arguments, Z \u2032k, defined as\nZ \u2032k , max \u03bb\u2208\u039b() E\n[\nmin y\u2208Y()\nE [ l (y, \u03bb,X0) | X\u22121\u2212\u221e ] | X\u22121\u2212k ] ,\nwhere the maximum is taken w.r.t. all \u03c3(X\u22121\u2212k)-measurable strategies and the minimum is taken w.r.t. all \u03c3(X\u22121\u2212\u221e)-measurable strategies, is a sub-martingale that also converges a.s. to Z\u221e and thus E[Z \u2032 k] \u2192 E[Z\u221e] = V\u2217.\nWe conclude the proof by noticing that the following relation holds for any k,\nE[Z \u2032k] = E\n[\nmax \u03bb\u2208\u039b() E\n[\nmin y\u2208Y()\nE [ l (y, \u03bb,X0) | X\u22121\u2212\u221e ] | X\u22121\u2212k ]]\n\u2264 E [\nmax \u03bb\u2208\u039b() E\n[\nE\n[ l (\ny\u0302\u2217k, \u03bb,X0\n) | X\u22121\u2212\u221e ] | X\u22121\u2212k ]\n]\n= E\n[\nmax \u03bb\u2208\u039b() E\n[ l (\ny\u0302\u2217k, \u03bb,X0\n) | X\u22121\u2212k ]\n]\n= E [ l (\ny\u0302\u2217k, \u03bb\u0302 \u2217 k, X0\n)]\n,\nand using similar arguments we can show that also\nE\n[ l (\ny\u0302\u2217k, \u03bb\u0302 \u2217 k, X0\n)]\n\u2264 E[Zk],\nand since both E[Zk] and E[Z \u2032 k] converge to V\u2217, we get the desired result.\nBefore stating the main theorem regarding MHA, we now state and prove the fol-\nlowing lemma, which is used in the proof of the main result regarding MHA.\nLemma 4. Let {Hk,h} be a countable set of experts as defined in the proof of Theorem 2. Then, the following relation holds a.s.:\ninf k,h lim sup n\u2192\u221e\n1\nN\nN \u2211\ni=1\nl ( yik,h, \u03bbi, Xi ) \u2264 V\u2217\n\u2264 sup k,h lim inf n\u2192\u221e\n1\nN\nN \u2211\ni=1\nl ( yi, \u03bb i k,h, Xi ) ,\nwhere (yi, \u03bbi) are the predictions of MHA when applied on {Hk,h}. Proof. Set\nf(y,Q) , max \u03bb\u2208\u039b EQ [l (y, \u03bb,X0)] .\nWe will start from the LHS,\ninf k,h lim sup n\u2192\u221e\n1\nN\nN \u2211\ni=1\nl ( yik,h, \u03bbi, Xi ) , (12)\nand similarly to Lemma 1, by using the strong law of large numbers we can write\n(12) = inf k,h lim sup n\u2192\u221e\n1\nN\nN \u2211\ni=1\nE [ l ( yik,h, \u03bbi, X0 ) | X\u221211\u2212i ]\n\u2264 inf k,h lim sup n\u2192\u221e\n1\nN\nN \u2211\ni=1\nf(yik,h,PX0|X\u221211\u2212i ) a.s. (13)\nFor fixed k, h > 0, from the proof of Theorem (2), yik,h \u2192 y\u2217k,h a.s. as i approaches \u221e, and from Levy\u2019s zero-one law also PX0|X\u221211\u2212i \u2192 P\u221e weakly. From Lemma 2 we know that f is continuous, therefore, we can apply Lemma 1 and get that\n(13) = inf k,h\nE [ E [ f(y\u2217k,h,P\u221e) ]] \u2264 lim k\u2192\u221e lim l\u2192\u221e E [ f(y\u2217k,h,P\u221e) ] . (14)\nFrom the uniqueness of the saddle point and from the proof of Theorem (2), for\nfiked k > 0, lim h\u2192\u221e\ny\u2217k,h \u2192 y\u2217k a.s.. Thus, from the continuity of f we get that\nlim h\u2192\u221e\nf(y\u2217k,h,P\u221e) \u2192 f(y\u2217k,P\u221e)\nand again by Lebesgue\u2019s dominated convergence,\n(14) = lim k\u2192\u221e E [f(y\u2217k,P\u221e)] = lim k\u2192\u221e E\n[\nmax \u03bb\u2208\u039b\nEP\u221e [l (y \u2217 k, \u03bb,X0)]\n]\n. (15)\nNow, from Theorem 2 we know that every accumulation point of the sequence {y\u2217k} is in the optimal set\nargmin y\u2208Y\n(\nmax \u03bb\u2208\u039b EP\u221e [l (y, \u03bb,X0)]\n)\n.\nTherefore a.s.\nlim k\u2192\u221e max \u03bb\u2208\u039b\nEP\u221e [l (y \u2217 k, \u03bb,X0)] \u2192 EP\u221e [l (y\u2217\u221e, \u03bb\u2217\u221e, X0)] ,\nand using Lebesgue\u2019s dominated convergence,\n(15) = E [EP\u221e [l (y \u2217 \u221e, \u03bb \u2217 \u221e, X0)]] = V\u2217.\nUsing similar arguments, we can show the second part of the lemma.\nWe are now ready to state and prove the optimality of MHA.\nTheorem 3 (Optimality of MHA). Let (yi, \u03bbi) be the predictions generated by MHA when applied on {Hk,h} as defined in the proof of Theorem 2. Then, for any \u03b3-feasible process {Xi}\u221e\u2212\u221e: MHA is a \u03b3-bounded and \u03b3-universal strategy.\nProof. We first show that\nlim N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, Xi) = V\u2217 a.s. (16)\nApplying Lemma 5 in [15], we know that the x updates guarantee that for every expert Hk,h,\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi) \u2264 1\nN\nN \u2211\ni=1\nl(yik,h, \u03bbi, xi) + Ck,h\u221a N\n(17)\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi) \u2265 1\nN\nN \u2211\ni=1\nl(yi, \u03bb i k,h, xi)\u2212 C\u2032k,h\u221a N , (18)\nwhere Ck,h, C \u2032 k,h > 0 are some constants independent of N . In particular, using\nEquation (17),\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi) \u2264 inf k,h\n(\n1\nN\nN \u2211\ni=1\nl(yik,h, \u03bbi, xi) + Ck,h\u221a N\n)\n.\nTherefore, we get\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi)\n\u2264 lim sup N\u2192\u221e inf k,h\n(\n1\nN\nN \u2211\ni=1\nl(yik,h, \u03bbi, xi) + Ck,h\u221a N\n)\n\u2264 inf k,h lim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nl(yik,h, \u03bbi, xi) + Ck,h\u221a N\n)\n\u2264 inf k,h lim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nl(yik,h, \u03bbi, xi)\n)\n, (19)\nwhere in the last inequality we used the fact that lim sup is sub-additive. Using Lemma (4), we get that\n(19) \u2264 V\u2217\n\u2264 sup k,h lim inf n\u2192\u221e\n1\nN\nN \u2211\ni=1\nl ( yi, \u03bb i k,h, Xi ) . (20)\nUsing similar arguments and using Equation (18) we can show that\n(20) \u2264 lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi).\nSummarizing, we have\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi) \u2264 V\u2217 \u2264 lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi).\nTherefore, we can conclude that a.s.\nlim N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, Xi) = V\u2217.\nTo show that MHA is indeed a \u03b3-bounded strategy and to shorten the notation, we will denote g(y, \u03bb, x) , \u03bb(c(y, x)\u2212 \u03b3). First, from Equation (18) applied on the expertH0,0, we get that:\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbmax, x) \u2264 lim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbi, x). (21)\nMoreover, since l is uniformly continuous, for any given \u01eb > 0, there exists \u03b4 > 0, such that if |\u03bb\u2032 \u2212 \u03bb| < \u03b4, then\n|l(y, \u03bb\u2032, x)\u2212 l(y, \u03bb, x)| < \u01eb\nfor any y \u2208 Y and x \u2208 X . We also know that\nlim k\u2192\u221e lim h\u2192\u221e lim i\u2192\u221e\n\u03bbik,h = \u03bb \u2217 \u221e.\nTherefore, there exist k0, h0, i0 such that |\u03bbik0,h0 \u2212 \u03bb\u2217\u221e| < \u03b4 for any i > i0. Since limk\u2192\u221e \u03bb \u2217 k = \u03bb \u2217 \u221e there exists k0 such that |\u03bb\u2217k0\u2212\u03bb\u2217\u221e| < \u03b4 3 . Note that limh\u2192\u221e \u03bb \u2217 k0,h = \u03bb\u2217k0 , so there exists h0 such that |\u03bb\u2217k0,h0 \u2212 \u03bb\u2217k0 | < \u03b4 3 . Finally, since limi\u2192\u221e \u03bb i k0,l0 = \u03bb\u2217k0,l0 , there exists i0 such that if i > i0, then |\u03bbik0,l0 \u2212 \u03bb\u2217k0,l0 | < \u03b4 3 . Combining all the above, we get that for k0, h0, i0 if i > i0, then\n|\u03bbik0,h0 \u2212 \u03bb \u2217 \u221e| < |\u03bbik0,h0 \u2212 \u03bb \u2217 k0,h0 |+ |\u03bbik0,h0 \u2212 \u03bb \u2217 k0 |+ |\u03bb\u2217k0 \u2212 \u03bb \u2217 \u221e| < \u03b4.\nTherefore,\nlim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb \u2217 \u221e, xi)\u2212\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi)\n)\n\u2264\nlim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb \u2217 \u221e, xi)\u2212\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb i k0,h0 , xi)\n)\n+\nlim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb i k0,h0 , xi)\u2212 1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi)\n)\n(22)\nFrom the uniform continuity we also learn that the first summand is bounded above by \u01eb, and from Equation (18), we get that the last summand is bounded above by 0. Thus,\n(22) \u2264 \u01eb,\nand since \u01eb is arbitrary, we get that\nlim sup N\u2192\u221e\n(\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb \u2217 \u221e, xi)\u2212\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi)\n)\n\u2264 0.\nThus,\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb \u2217 \u221e, Xi) \u2264 V\u2217,\nand from Theorem 1 we can conclude that\nlim N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bb \u2217 \u221e, Xi) = V\u2217.\nTherefore, we can deduce that\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbi, xi)\u2212 lim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bb \u2217 \u221e, xi) =\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbi, xi) + lim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\n\u2212g(yi, \u03bb\u2217\u221e, xi)\n\u2264 lim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbi, xi)\u2212 1\nN\nN \u2211\ni=1\ng(yi, \u03bb \u2217 \u221e, xi)\n= lim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\nl(yi, \u03bbi, xi)\u2212 1\nN\nN \u2211\ni=1\nl(yi, \u03bb \u2217 \u221e, xi) = 0,\nwhich results in\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbi, xi) \u2264 lim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bb \u2217 \u221e, xi).\nCombining the above with Equation (21), we get that\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bbmax, xi)\n\u2264 lim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\ng(yi, \u03bb \u2217 \u221e, xi).\nSince 0 \u2264 \u03bb\u2217\u221e < \u03bbmax, we get that MHA is \u03b3-bounded. This also implies that\nlim sup N\u2192\u221e\n1\nN\nN \u2211\ni=1\n\u03bbi(c(yi, xi)\u2212 \u03b3) \u2264 0.\nNow, if we apply Equation (18) on the expertH\u22121,\u22121, we get that\nlim inf N\u2192\u221e\n1\nN\nN \u2211\ni=1\n\u03bbi(c(yi, xi)\u2212 \u03b3) \u2265 0.\nThus,\nlim N\u2192\u221e\n1\nN\nN \u2211\ni=1\n\u03bbi(c(yi, xi)\u2212 \u03b3) = 0,\nand using Equation (16), we get that MHA is also \u03b3-universal."}, {"heading": "5 Concluding Remarks", "text": "In this paper, we introduced theMinimax HistogramAggregation (MHA) algorithm for multiple-objective sequential prediction. We considered the general setting where the unknown underlying process is stationary and ergodic., and given that the underlying process is \u03b3-feasible, we extended the well-known result of [1] regarding the asymptotic lower bound of prediction with a single objective, to the case of multi-objectives. We proved that MHA is a \u03b3-bounded strategy whose predictions also converge to the optimal solution in hindsight.\nIn the proofs of the theorems and lemmas above, we used the fact that the initial weights of the experts, \u03b1k,h, are strictly positive thus implying a countably infinite expert set. In practice, however, one cannot maintain an infinite set of experts. Therefore, it is customary to apply such algorithms with a finite number of experts (see [14, 12, 13, 17]). Despite the fact that in the proof we assumed that the observation set X is known a priori, the algorithm can also be applied in the case that X is unknown by applying the doubling trick. For a further discussion on this point, see [11]. In our proofs, we relied on the compactness of the set X . It will be interesting to see whether the universality of MHA can be sustained under unbounded processes as well. A very interesting open question would be to identify conditions allowing for finite sample bounds when predicting with multiple objectives."}], "references": [{"title": "The strong law of large numbers for sequential decisions under uncertainty", "author": ["P.H. Algoet"], "venue": "IEEE Transactions on Information Theory, 40(3):609\u2013633,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Asymptotic optimality and asymptotic equipartition properties of log-optimum investment", "author": ["P.H. Algoet", "T.M. Cover"], "venue": "The Annals of Probability, pages 876\u2013898,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "Optimization iii", "author": ["A. Ben-Tal", "A. Nemirovsky"], "venue": "Lecture Notes,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic optimal control: The discrete time case, volume 23", "author": ["D. Bertsekas", "S. Shreve"], "venue": "Academic Press New York,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1978}, {"title": "Nonparametric sequential prediction of time series", "author": ["G. Biau", "K. Bleakley", "L. Gy\u00f6rfi", "G. Ottucs\u00e1k"], "venue": "Journal of Nonparametric Statistics, 22(3):297\u2013317,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequential quantile prediction of time series", "author": ["G. Biau", "B. Patra"], "venue": "IEEE Transactions on Information Theory, 57(3):1664\u20131674,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Online Computation and Competitive Analysis", "author": ["A. Borodin", "R. El-Yaniv"], "venue": "Cambridge University Press,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "The individual ergodic theorem of information theory", "author": ["L. Breiman"], "venue": "The Annals of Mathematical Statistics, 28(3):809\u2013811,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1957}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge University Press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Minimax theorems", "author": ["K. Fan"], "venue": "Proceedings of the National Academy of Sciences, 39(1):42\u201347,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1953}, {"title": "Strategies for sequential prediction of stationary time series", "author": ["L. Gy\u00f6rfi", "G. Lugosi"], "venue": "InModeling uncertainty, pages 225\u2013248. Springer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonparametric kernel-based sequential investment strategies", "author": ["L. Gy\u00f6rfi", "G. Lugosi", "F. Udina"], "venue": "Mathematical Finance, 16(2):337\u2013357,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Nonparametric prediction", "author": ["L. Gy\u00f6rfi", "D. Sch\u00e4fer"], "venue": "Advances in learning theory: methods, models and applications, 339:354,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel-based semi-log-optimal empirical portfolio selection strategies", "author": ["L. Gy\u00f6rfi", "A. Urb\u00e1n", "I. Vajda"], "venue": "International Journal of Theoretical and Applied Finance, 10(03):505\u2013516,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Y. Kalnishkan", "M. Vyugin"], "venue": "International Conference on Computational Learning Theory, pages 188\u2013203. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Online portfolio selection: A survey", "author": ["B. Li", "S.C.H. Hoi"], "venue": "ACM Computing Surveys (CSUR), 46(3):35,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Corn: Correlation-driven nonparametric learning approach for portfolio selection", "author": ["B. Li", "S.C.H Hoi", "V. Gopalkrishnan"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):21,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Nash equilibrium computation in subnetwork zero-sum games with switching communications", "author": ["Y. Lou", "Y. Hong", "L. Xie", "G. Shi", "K. Johansson"], "venue": "IEEE Transactions on Automatic Control, 61(10):2920\u20132935,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Trading regret for efficiency: online convex optimization with long term constraints", "author": ["M. Mahdavi", "R. Jin", "T. Yang"], "venue": "Journal of Machine Learning Research, 13(Sep):2503\u20132528,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Stochastic convex optimization with multiple objectives", "author": ["M. Mahdavi", "T. Yang", "R. Jin"], "venue": "Advances in Neural Information Processing Systems, pages 1115\u2013 1123,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Measurable selection theorems for minimax stochastic optimization problems", "author": ["A. Nowak"], "venue": "SIAM Journal on Control and Optimization, 23(3):466\u2013476,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1985}, {"title": "Neyman-pearson classification, convexity and stochastic constraints", "author": ["P. Rigollet", "X. Tong"], "venue": "Journal of Machine Learning Research, 12(Oct):2831\u20132855,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Almost sure convergence, vol", "author": ["W. Stout"], "venue": "24 of probability and mathematical statistics,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1974}, {"title": "Competing with stationary prediction strategies", "author": ["V. Vovk"], "venue": "International Conference on Computational Learning Theory, pages 439\u2013453. Springer,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 8, "context": "1 Introduction In the traditional online learning setting, and in particular in sequential prediction under uncertainty, the learner is evaluated by a single loss function that is not completely known at each iteration [9].", "startOffset": 219, "endOffset": 222}, {"referenceID": 15, "context": "For example, in online portfolio selection [16, 7], the player may want to maximize wealth while keeping the risk (i.", "startOffset": 43, "endOffset": 50}, {"referenceID": 6, "context": "For example, in online portfolio selection [16, 7], the player may want to maximize wealth while keeping the risk (i.", "startOffset": 43, "endOffset": 50}, {"referenceID": 21, "context": ", [22]) (which extends the objective in classical binary classification) where the goal is to learn a classifier achieving low type II error whose type I error is kept below a given threshold.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "Recently, [19] presented an algorithm for dealing with the case of one main objective and fully-known constraints.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "In a subsequent work, [20] proposed a framework for", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 44, "endOffset": 52}, {"referenceID": 12, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 54, "endOffset": 58}, {"referenceID": 16, "context": "For example, in online portfolio selection, [14, 12], [13], and [17] proposed non-parametric online strategies that guarantee, under mild conditions, the best possible outcome.", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6].", "startOffset": 84, "endOffset": 87}, {"referenceID": 10, "context": "Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "Another interesting example in this regard is the work on time-series prediction by [5], [11], and [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 23, "context": "The algorithm presented in this paper utilizes as a sub-routine the Weak Aggregating Algorithm of [24], and [15] to handle multiple objectives.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "The algorithm presented in this paper utilizes as a sub-routine the Weak Aggregating Algorithm of [24], and [15] to handle multiple objectives.", "startOffset": 108, "endOffset": 112}, {"referenceID": 2, "context": "Therefore, the problem is equivalent to finding the saddle point of the Lagrangian function [3], namely, min y\u2208Y max \u03bb\u2208R+ L(y, \u03bb), where the Lagrangian is L(y, \u03bb) , (EP\u221e [u(y,X0)] + \u03bb (EP\u221e [c(y,X0)]\u2212 \u03b3)) .", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "This result is a generalization of the well-known result of [1] regarding the best possible outcome under a single objective.", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 7, "context": "Lemma 1 (Ergodicity, [8]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "32 from [4], we have that sup\u03bb\u2208\u039b EQ [l(y, \u03bb,X)] is continuous inQ\u00d7Y .", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "The last part of the lemma follows directly from Fan\u2019s minimax theorem [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "The first part of the proof follows immediately from the minimax measurable theorem of [21] due to the compactness of Y,\u039b,X and the properties of the loss function L.", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "The proof of the second part is similar to the one presented in Theorem 3 of [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 22, "context": ", [23]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 23, "context": "In order to ensure that the performance of MHA will be as good as any other expert for both the y and the \u03bb predictions, we apply the Weak Aggregating Algorithm of [24], and [15] twice simultaneously.", "startOffset": 164, "endOffset": 168}, {"referenceID": 14, "context": "In order to ensure that the performance of MHA will be as good as any other expert for both the y and the \u03bb predictions, we apply the Weak Aggregating Algorithm of [24], and [15] twice simultaneously.", "startOffset": 174, "endOffset": 178}, {"referenceID": 17, "context": ", [18]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": ", [11, 13]), we get that P \u2217(k,h) X \u22121 \u2212k \u2192 P{X0|X\u22121 \u2212k} weakly as h \u2192 \u221e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 12, "context": ", [11, 13]), we get that P \u2217(k,h) X \u22121 \u2212k \u2192 P{X0|X\u22121 \u2212k} weakly as h \u2192 \u221e.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": "Applying Lemma 5 in [15], we know that the x updates guarantee that for every expert Hk,h,", "startOffset": 20, "endOffset": 24}], "year": 2017, "abstractText": "Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion.", "creator": "LaTeX with hyperref package"}}}