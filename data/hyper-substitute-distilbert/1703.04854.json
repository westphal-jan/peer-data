{"id": "1703.04854", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2017", "title": "Distributed-Representation Based Hybrid Recommender System with Short Item Descriptions", "abstract": "design understanding ( cf ) aims to build improved recovery from members'psychological behaviors and / or similar symptoms... whether modeling practitioners, subsequently use the paradigm to incorporate items for analysis. as of the success of contemporary item planning endeavors, they though all converge on current assumption that there do small rating scores available for building arbitrary - quality profile models. inside real world applications, however, diagnosis is indeed difficult yet yield sufficient rating averages, especially when new norms are introduced into the context, nor causes the assigned task challenging. we find therefore there differ often \" short \" texts resembling features of items, based on ways we accurately approximate the similarity of those and make predictions together with rating scores. upon this setup we \" borrow \" this idea of vector representation of words to capture the character of short documents and embed it into a complex factorization framework. we empirically report that this approach is effective by comparing it with show - of - the - art approaches.", "histories": [["v1", "Wed, 15 Mar 2017 00:47:28 GMT  (666kb)", "http://arxiv.org/abs/1703.04854v1", "10 pages, 5 figures"]], "COMMENTS": "10 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["junhua he", "hankz hankui zhuo", "jarvan law"], "accepted": false, "id": "1703.04854"}, "pdf": {"name": "1703.04854.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hejunh@mail2.sysu.edu.cn,"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n04 85\n4v 1\n[ cs\n.I R\n] 1\n5 M\nar 2\n01 7\ncisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often \u201cshort\u201d texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we \u201cborrow\u201d the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches."}, {"heading": "1 Introduction", "text": "Recommender systems are a subclass of information filtering systems that seek to predict the rating or preference that a user would give to an item (Ricci et al., 2011). Recommender systems have been applied to a variety of applications, e.g., movies, music, news, books, research articles, search queries, social tags, financial services (Felfernig et al., 2007), and Twitter followers (Gupta et al., 2013). In general there are three ways to design recommender systems (Adomavicius and Tuzhilin, 2005), i.e., collaborative filtering (Breese et al., 1998), content-based filtering (Gopalan et al., 2014), and the hybrid filtering (Burke, 2002). Our work follows the strand of hybrid filtering systems.\nThere have been works on hybrid filtering systems. For example, Saveski and Mantrach (Saveski and Mantrach, 2014) propose to exploit information from item document, i.e., each item is assumed to be associated with a document, to help with recommendation based on the word frequency (or TF-IDF) in documents. Chen et al. present a topic-model based approach to utilize the context and item information (Chen et al., 2014) to help with recommendation. McAuley and Leskovec propose to build a hybrid recommender system by integrating information from review texts with rating scores (McAuley and Leskovec, 2013). Despite the success of the previous approaches, they are based on the assumption that the text information is abundant enough for frequency mining or topic models extraction. When the item description is limited or short, e.g., only a few phrases or tags available, they will not work well since \u201csimilar\u201d items with limited descriptions can be very different based on frequency. For example, an item described by \u201ca portable device\u201d should be similar to the item described by \u201ca light-weight and small equipment\u201d, while they are very different based on frequency mining since they share very few words. There are indeed many applications, i.e., MovieLens1 as shown in Table 1, where item descriptions are often short.\nIn this paper, we aim to explore the similarity between short item descriptions by looking into the semantic relations between descriptions. Inspired by the vector representations of words (Mikolov et al., 2013c), which has been shown to be effective in capturing the semantic relations among words, we borrow the idea of vector representations to take advantage of short item descriptions to assist\n1https://movielens.org\nrecommendation. We first build a matrix based on the vector representations of words, and then integrate the matrix into the rating scores to build a matrix factorization objective function. Finally we solve the objective function using an expectation-maximization algorithm to make item recommendation. We call our algorithm RECF, which stands for hybrid RECommender system based on collaborative Filtering with short reviews."}, {"heading": "2 Related Work", "text": "Our work is related to distributed representations of words. In earlier work, many models have been proposed to learn a distributed representation of words. Collobert and Weston (Collobert and Weston, 2008) propose a single convolutional neural network called SENNA, to output a host of language processing predictions. Mnih and Hinton (Mnih and Hinton, 2008) propose a fast hierarchical language model called HLBL, based on Log-Bilinear in (Mnih and Hinton, 2007) along with a simple feature-based algorithm, which outperforms non-hierarchical neural models in their evaluations. Mikolov (Mikolov, 2012) proposes a new statistical language model, RNNLM, based on RNN in (Mikolov et al., 2010). Huang et al. (Huang et al., 2012) propose a new model which increases the global context-aware to enrich the semantic information of words. And Mikolov et al. (Mikolov et al., 2013a) proposed two new models, CBOW and Skip-gram. Both models use a simple neural network architecture that aims to predict the neighbors of a word. CBOW predicts the current words based on the context and Skipgram tries to maximize the classification accuracy of a word based on another word in the same sentence. Mikolov et al. (Mikolov et al., 2013a) also proposed a new tool for learning word vectors called word2vec (Mikolov et al., 2013c). To improve the accuracy of the word representation, Then in the following year, focusing on this technology of distributed representations. Frome et al. used it to make the language model pre-training of a new deep visual-semantic embedding model, as it has been shown to efficiently learn semantically-meaningful floating point representations of terms from unannotated text (Frome et al., 2013). Mikolov et al. (Mikolov et al., 2013b) developed a method that can automate the process of generating and extending dictionaries and phrase tables. Le and Mikolov (Le and Mikolov, 2014) proposed an unsupervised algorithm that learns fixed-length feature representations from variable-length texts. Qiu et al. (Qiu et al., 2015) explored distributed representations of words to detect analogies. In this paper, we exploit the distributed representation approach to transform item descriptions to vectors, and assist recommendation based on these vectors."}, {"heading": "3 Problem Formulation", "text": "A rating matrix is denoted by R \u2208 {1, 2, 3, 4, 5, ?}N\u00d7M , where Ruv is a rating score given by user u for item v, N is the number of users, M is the number of items, and the symbol \u201c?\u201d indicates no score is given by user u. An labeling matrix is denoted by L \u2208 {0, 1, ?}N\u00d7M , where Luv is the label given by user u for item v, with the meaning of \u201cdislike\u201d, \u201clike\u201d and \u201cunknown label\u201d for \u201c0\u201d, \u201c1\u201d and \u201c?\u201d, respectively. An item description vector is denoted by Q, where Qv is composed of a set of words describing the properties of item v. Note that Qv can be an empty set \u2205 suggesting no item description given to item v.\nOur recommender system can be defined by: given as input a rating matrix R, a labeling matrix L and an item description vector Q, it aims to estimate unknown rating scores \u201c?\u201d in R.\n4 Our RECF Algorithm\nIn this section, we present our RECF algorithm in detail. We first build distributed representations of item descriptions, and then integrate the ratings, labelings and distributed representations of item descriptions to build a bayesian model and learn parameters of the model to build the recommender system. An overview of RECF is shown in Algorithm 1. We will address each step of Algorithm 1 in detail in the subsequent sections.\nAlgorithm 1 The framework of our RECF algorithm input: ratings R, labelings L, item descriptions Q output: estimated ratings R\u0302\n1: build representations C from descriptions Q 2: build hybrid model M based on R, L and C 3: learn the parameters ofM with EM approach\n3.0: initiate U , V , BR, BL and WC while the maximal iteration is not reached do\n3.1: update V using U , BR, BL and WC 3.2: update U using V , BR, BL and WC 3.3: calculate BR, BL and WC using V and U\nend while 4: compute R\u0302 = UTBRV return R\u0302"}, {"heading": "4.1 Distributed representations of descriptions", "text": "As the first step of Algorithm 1, we aim to build the distributed representations of item descriptions with Q as input. We first learn the vector representations for words using the Skip-gram model with hierarchical softmax, which has been shown an efficient method for learning high-quality vector representations of words from unstructured corpora (Mikolov et al., 2013c). The objective of the Skip-gram model is to learn vector representations for predicting the surrounding words in a sentence or document. Given a corpus C, composed of a sequence of training words \u3008w1, w2, . . . , wT \u3009, where T = |C|, the Skip-gram model maximizes the average log probability\n1\nT\nT \u2211\nt=1\n\u2211\n\u2212c\u2264j\u2264c,j 6=0\nlog p(wt+j |wt), (1)\nwhere c is the size of the training window or context.\nThe basic probability p(wt+j |wt) is defined by the hierarchical softmax, which uses a binary tree representation of the output layer with the K words as its leaves and for each node, explicitly represents the relative probabilities of its child nodes (Mikolov et al., 2013c). For each leaf node, there is an unique path from the root to the node, and this path is used to estimate the probability of the word represented by the leaf node. There are no explicit output vector representations for words. Instead, each inner node has an output vector v\u2032 n(w,j), and the probability of a word being the output word is defined by p(wt+j |wt) = \u220fL(wt+j)\u22121\ni=1\n{ \u03c3(I(n(wt+j , i+1) = child(n(wt+j , i))) \u00b7 vn(wt+j ,i) \u00b7 vwt) } ,where \u03c3(x) =\n1/(1 + exp(\u2212x)). L(w) is the length from the root to the word w in the binary tree, e.g., L(w) = 4 if there are four nodes from the root to w. n(w, i) is the ith node from the root to w, e.g., n(w, 1) = root and n(w,L(w)) = w. child(n) is a fixed child (e.g., left child) of node n. vn is the vector representation of the inner node n. vwt is the input vector representation of word wt. The identity function I(x) is 1 if\nx is true; otherwise it is -1. We can thus build vector representations of words w, denoted by vec(w), by maximizing Equation (1) with corpora.\nWith vector representations of words, we calculate the overall representations of item descriptions by \u201csummarizing\u201d all words in each item description. There could be many different ways to \u201csummarize\u201d all words. In this paper we consider a straightforward way of computing the overall representations of item descriptions, i.e., calculating an average representation over all words in each item description. We have Cv = 1\n|Qv|\n\u2211\nw\u2208Qv vec(w), where Qv is the set of words describing the properties of item\nv. If Qv = \u2205, Cv is assigned with symbol \u201c?\u201d with the same meaning in R. Note that we assume the importance of different words in Qv is identical in describing item v. It is possible to extend it to considering different importance of words by introducing weights to words when the prior knowledge is provided. We call the resulting matrix C = [C1, C2, . . . , CM ] T description matrix."}, {"heading": "4.2 The hybrid model with item descriptions", "text": "In Step 2 of Algorithm 1, we aim to build a hybrid model M to capture the underlying relations among ratings R, labelings L, and item descriptions C . The framework of the hybrid model is shown in Figure\n1. The rationale of the hybrid model is based on the following four assumptions. Assumption 1: Each user u and item v are characterized by an unknown feature vector Uu controlled by parameter \u03b8u and Vv controlled by parameter \u03b8v, respectively. The rating Ruv, which is controlled by parameter \u03b1, is assumed to be resulted from bridging Uu and Vv with unknown matrix BR controlled by parameter \u03b2. In other words, rating Ruv should be close to UuBRV T v , i.e., Ruv \u223c UuBRV T v . The similar idea is exploited by (Pan and Yang, 2013). This idea can be formulated by maximizing the conditional distribution below, assuming it follows a Gaussian distribution: p(Ruv|Uu, BR, Vv , \u03b1) = N (Ruv|UuBRV T v , \u03b1 \u22121I) where N (x|\u00b5, \u03b1\u22121I) = \u221a \u03b1 2\u03c0 exp(\u2212\u03b1(x \u2212 \u00b5)\n2). Assumption 2: Likewise, the labeling Luv, which is controlled by parameter \u03b1, is assumed to be resulted from bridging Uu and Vv with unknown matrix BL controlled by the same parameter \u03b2 of BR, i.e., Luv \u223c UuBLV T v . We thus have p(Luv|Uu, BL, Vv , \u03b1) = N (Luv|UuBLV T v , \u03b1\n\u22121I). Assumption 3: The item description Cv, which is controlled by parameter \u03be, is assumed to be resulted from the item features Vv and unknown matrix WC controlled by parameter \u03b4, i.e., Cv \u223c V Tv WC . We thus have p(Cv|Vv ,WC , \u03be) = N (Cv|V T v WC , \u03be\n\u22121I). Assumption 4: Furthermore, we assume the distributions of Uu, Vv, BR, BL and WC are p(Uu|\u03b8u) = N (Uu|0, \u03b8 \u22121 u I), p(Vv|\u03b8v) = N (Vv |0, \u03b8 \u22121 v I), p(BR|\u03b2) = N (BR|0, (\u03b2/qR)\n\u22121I), p(BL|\u03b2) = N (BL|0, (\u03b2/qL) \u22121I), p(WC |\u03b4) = N (WC |0, (\u03b4/qC ) \u22121I), where qR, qL, and qC are numbers of not \u201c?\u201d elements in R, L and C , respectively.\nBased on the hybrid model shown in Figure 1, our objective is to maximize the function as below:\nmax U,V,BR,BL,WC , logFR + \u03bbL logFL + \u03bbC logFC (2)\nwhere \u03bbL > 0 and \u03bbC > 0 are tradeoff parameters to balance the ratings, labelings and item descriptions. U \u2208 Rn\u00d7d and V \u2208 Rm\u00d7d satisfy UTU = I and V TV = I , respectively. FR, FL and FC are defined by FR = \u220f\nu,v\n[ p(Ruv|Uu, BR, Vv, \u03b1)p(Uu|\u03b8U )p(Vv |\u03b8V )p(BR|\u03b2) ]xuv\n,FL = \u220f\nu,v\n[ p(Luv|Uu, BL, Vv, \u03b1)p(Uu|\u03b8U)p(Vv |\u03b8V )p(BL|\u03b2) ]yuv\n,and FC = \u220f\nv\n[ p(Cv|WC , Vv, \u03be)p(Vv |\u03b8V )p(WC |\u03b4) ]zv ,where xuv, yuv and zv are indicator variables for Ruv, Luv and Cv, respectively. If Ruv = \u201c?\u201d (or Luv =\u201c?\u201d or Cv = \u2205), then xuv = 0 (or yuv = 0 or zv = 0); otherwise xuv = 1 (or yuv = 1 or zv = 1).\nSpecifically, based on the Gaussian distributions given above, the log-posterior function of the ratings\nis shown below:\nlogFR = \u2212 \u2211\nu,v\nxuv[ \u03b1\n2 (Ruv \u2212 UuBRV\nT v ) 2 + \u03b8U 2 \u2016Uu\u2016 2 + \u03b8V 2 \u2016Vv\u2016 2 + \u03b2 2q \u2016BR\u2016 2 F +KR], (3)\nwhereKR = ln \u221a \u03b1 2\u03c0 + ln\n\u221a\n\u03b8U 2\u03c0 + ln\n\u221a\n\u03b8V 2\u03c0 + ln\n\u221a\n\u03b2 2qR\u03c0 is a constant. Likewise, we can compute the log-\nposterior functions of the labelings logFL and descriptions logFC . We can see the objective function Equation (2) can be reduced to a polynomial function. We will solve the optimization problem using an EM-style algorithm in the next subsection."}, {"heading": "4.3 The EM algorithm", "text": "In Step 3 of Algorithm 1 we aim to learn the parameters BR, BL,WC , U and V using the EM approach. As the beginning of the EM approach, we initialize U and V using the SVD result of labelings L, since the labeling data L describes users\u2019 \u201chigh-level\u201d or general interest in items. After that we initialize BR, BL and WC with Equations (5) and (6) using U and V , which will be introduced in Section 4.3.2."}, {"heading": "4.3.1 Learning V and U", "text": "In Steps 3.1 and 3.2 of Algorithm 1, we aim to learn V and U . Given U andBR, BL,WC , we can update V using gradient descent approach. We first simplify the optimization function from Equation (2), as shown below:\nmin U,V f = min U,V\n1 2 \u2016X \u2299 (R\u2212 UBRV T )\u20162F + \u03bbR 2 \u2016Y \u2299 (L\u2212 UBLV T )\u20162F + \u03bbC 2 \u2016Z \u2299 (C \u2212 VWC)\u2016 2 F\ns.t. UTU = I, V TV = I, (4)\nwhere X = [xuv], Y = [yuv], Z = [zv ]. We then iteratively update V and U by V = V \u2212 \u03b31 \u2202f \u2202V and U = U \u2212 \u03b32 \u2202f \u2202U , where \u03b31 and \u03b32 are two learning constants.\n4.3.2 Calculating BR, BL and WC\nIn Step 3.3 of Algorithm 1, we compute BR, BL and WC using U and V . For BR, we have the optimal function shown below,minBR 1 2\u2016X\u2299(R\u2212UBRV T )\u20162F + \u03b2 2 \u2016BR\u2016 2 F .Letting BR = vec(BR) = [BR\u00b71 \u00b7 \u00b7 \u00b7 BR \u00b7d ],mui = vec(U T u\u00b7Vi\u00b7),R = vec(R), where vec(Y ) indicates a vector built by concatenating columns of the matrix Y , we have the following equivalent problem, minBR 1 2\u2016R\u2212M \u00b7 BR\u2016 2 F + \u03b2 2 \u2016BR\u2016 2 F ,where M = [...mui...] T . Letting \u2207BR = 0, we have\nvec(BR) = BR = (M T M+ \u03b2I)\u22121MTR. (5)\nLikewise, we have vec(BL) = (M T M+\u03b2I)\u22121MTL, where L = vec(L). Finally, we can easily compute BR and BL from vec(BR) and vec(BL), respectively.\nGiven V , we can estimate the parameter WC by optimizing the subject function from Equation (2). We have minWC \u03bbC 2 \u2016Z \u2299 (C \u2212 VWC)\u2016 2 F + \u03b4 2\u2016WC\u2016 2 F .We calculate the gradient \u2207WC = \u2212V\nTC + V TV WC + \u03b2WC , and set \u2207WC = 0. As a result, we have\nWC = (V TV + \u03b4I)\u22121V TC. (6)"}, {"heading": "4.3.3 Tradeoff between \u03bbL and \u03bbC", "text": "The initial values of the tradeoff parameters \u03bbL and \u03bbC are set before running the program, which are determined through repeated experiments. During execution, \u03bbL will remain the same while C will change. The reason is that, the labeling data includes accurate information while item description matrix C is obtained based on distributed representations of descriptions. When the labeling data is sparse, the noise issue with item descriptions may be worsen. Thus, the positive influence of C only plays in a macroscopic level but not in a microcosmic one. At the later period of convergence, continuing using C may reduce the accuracy. In other words, the influence of C should be gradually decreased as running the algorithm. We thus propose three options to adjust the value of \u03bbC , as shown below.\n1. Linear decline: Linear decline is the simplest model to specify the declining, in which we compute\n\u03bbC as follows:\n\u03bbC =\n{\nm\u2212 (iter \u2212 1) \u00b7 k if iter < m k + 1 0 else (7)\nwhere m is the initial value of \u03bbC , iter is the iteration and k is the step size.\n2. Nonlinear decline: To emphasize the strong influence of C in the early period, in nonlinear decline, the decreasing speed of \u03bbC also decreases in the execution. We propose a simple model as follows:\n\u03bbC = m/iter (8)\n3. Mutation: While the two methods mentioned above are easy to implement, the problem is that it\nis difficult to determine the step size. Intuitively, if the number of iterations before convergence is large, we should adjust the value ofm to decrease the step size, thus extending the time of influence by C . Hence, we propose a method of mutating C according to the convergence situation:\n\u03bbC =\n{\nm if before the first convergence 0 else (9)\nThe advantage of this method is that we do not need to consider the convergence speed.\nFinally, in Step 4 of Algorithm 1, we estimate values of \u201c?\u201d in R for recommendations by calculating UBRV T ."}, {"heading": "5 Experiments", "text": "In this section, we evaluate our RECF algorithm using two datasets MovieLens and Douban2 by comparing it against other four algorithms, SVD (Pan and Yang, 2013), CSVD (Pan and Yang, 2013), CSVD+Binary (Saveski and Mantrach, 2014) and CSVD+TFIDF (Saveski and Mantrach, 2014). SVD is an approach that exploits just ratings for building models for item recommendations. CSVD is an approach that exploits both ratings and labelings information for building models for item recommendations. CSVD+Binary and CSVD+TFIDF are two state-of-the-art approaches that exploit item description information for improving recommendation accuracy. They convert the item descriptions to Binary matrix and tf-idf representations, respectively, and combine them with ratings together to build recommender systems (Saveski and Mantrach, 2014). To make the comparison fair, we fed the labelings to the\n2 http://www.datatang.com/data/42832 and http://www.datatang.com/data/44858\napproaches by (Saveski and Mantrach, 2014), resulting in CSVD+Binary and CSVD+TFIDF. For both datasets MovieLens and Douban, we randomly split the data into n (n=3, 5, 10, 15, 20) subsets. We randomly selected one for training, one for building labeling data L by setting Luv be 1 if Rui > 3 and Luv be 0 otherwise (as done by (Pan and Yang, 2013)). The other n\u2212 2 subsets are used for testing.\nWe exploit two metrics to measure the performance, i.e., Mean Absolute Error (MAE) and Root\nMean Square Error (RMSE), as shown below, MAE = \u2211\n(u,i,rui)\u2208TE |rui \u2212 r\u0302ui|/|TE | and RMSE =\n\u221a\n\u2211\n(u,i,rui)\u2208TE (rui \u2212 r\u0302ui)2/|TE |, where rui is the ground-truth rating, r\u0302ui is the predicted rating and\n|TE | is the number of testing ratings."}, {"heading": "5.1 Performance w.r.t. sparsity", "text": "We first would like to see the performance with respect to different sparsities, by varying the percentage of available ratings (i.e., the rating scores given by users). We ran our RECF algorithm and SVD, CSVD, CSVD+Binary, CSVD+TFIDF five times with different training and testing subsets and computed an average of accuracies. In our RECF algorithm we set \u03bbL to be 0.2 and \u03bbC to be 2.5 in Equation (2). The results are shown in Figures 2 and 3, where we varied the sparsity from 1.4% to 0.21% in dataset Douban, and from 0.16% to 0.02% in dataset MovieLens, respectively.\nFrom the figures, we can see that both MAE and RMSE become larger when the percentage of ratings decreases in both datasets. This is consistent with our intuition since the fewer the ratings are, the larger the MAE and RMSE are. Comparing different curves, CSVD+Binary, CSVD+TFIDF and RECF algorithms generally perform better than SVD and CSVD in terms of MAE and RMSE, especially when the rating message is very sparse. This indicates item descriptions can indeed help improve the recommendation accuracy. However, in Douban field, we find that CSVD-TFIDF performs almost the same as CSVDwhile CSVD-Binary even makes a negative effect on the result. The main reason is that the item descriptions we can use are only tags instead of long text descriptions. The item description information cannot be captured correctly by Binary or tf-idf matrix, which harms the recommendation accuracy. In contrast, our RECF algorithm can better leverage these item description information based on distributed representations of words.\nFurthermore, in both datasets, we can also observe that MAE (or RMSE) of SVD (or CSVD, CSVD+Binary, CSVD+TFIDF) increases faster than our RECF algorithm as the percentage of rating scores decreases, i.e., the sparsity increases, which suggests that our RECF algorithm functions even better, compared to the other four approaches, when the rating data is much sparser. This is because the impact of item descriptions relatively becomes larger when rating data decreases, resulting larger improvement of accuracies by item descriptions."}, {"heading": "5.2 Tradeoff between \u03bbC and \u03bbL", "text": "Next, we would like to see the impact of \u03bbC in Equation (2). We tuned the tradeoff between \u03bbL and \u03bbC by varying the value of \u03bbC with respect to the number of iterations in RECF, as presented by Equation (9). As we can see from Equation (9), the value of \u03bbC is fixed to be m before the first convergence and 0 once our RECF algorithm converging, where m is the preset initial value of \u03bbC . We fixed m to be 2.5 and \u03bbL to be 0.2 as done in the last subsection. We present the results in Figures 4 and 5.\nWe find that the changes of performance (i.e., curves) can be divided into two stages, which indicate two phases of convergence. The first phase is for the tradeoff parameter of description matrix C , namely \u03bbC . At the beginning of convergence, C weights more than L and dominates the convergence. In this period, the curve declines as expected. However, as we can see from the figures, the curve may prematurely converge with a relatively low accuracy. The reason is that, due to the characteristics of C \u2013 capturing the similarity information in short descriptions with noise generated by word embedding, it may have a negative effect in a microcosmic level to get more accurate results. When we change \u03bbC to 0, i.e., C no longer has any impact on the recommendation result, the curves go to another convergence stage, which verifies that C mainly help improve the accuracy in the early stage by estimating values of \u201c?\u201d in R. Once the information from item descriptions C has been encoded in R and L after the first convergence, the impact of item descriptions should be reduced (letting \u03bbC be 0) and as a result, the impact of rating scores R and labelings L is relatively magnified to improve the recommendation accuracy. The rationale is that when the number of iterations reaches a threshold reducing the impact of description matrix C could help avoiding overfitting when continuing running our RECF algorithm. Note that setting \u03bbC to be 0 indicates we do not need to update parameters WC in the objective function of Equation (2) and as a result the size of parameters to be learnt is reduced. In summary, C should be weighed larger than L in the early stage for quickly injecting it\u2019s impact on the learning process, and then reduced to zero to increase the impact of R and L."}, {"heading": "6 Conclusion", "text": "In this paper, we propose a novel algorithm RECF to explore item descriptions to help improve the recommendation accuracy using distributed representations of item descriptions. Using this vector representation, we transform the item descriptions into vector representations, and combine them with rating and labeling data to build a hybrid recommender system. We exhibit that our RECF approach is effective by comparing with the state-of-the-art approaches that exploit item descriptions. In the future, we would like to explore more information in our algorithm framework, such as user profiles or reviews, to further improve recommendation accuracies."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["Adomavicius", "Alexander Tuzhilin"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "Adomavicius et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Adomavicius et al\\.", "year": 2005}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["David Heckerman", "Carl Myers Kadie"], "venue": "InUAI \u201998: Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Breese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Breese et al\\.", "year": 1998}, {"title": "Hybrid recommender systems: Survey and experiments", "author": ["Robin D. Burke"], "venue": "User Model. UserAdapt. Interact.,", "citeRegEx": "Burke.,? \\Q2002\\E", "shortCiteRegEx": "Burke.", "year": 2002}, {"title": "Context-aware collaborative topic regression with social matrix factorization for recommender systems", "author": ["Chen et al.2014] Chaochao Chen", "Xiaolin Zheng", "Yan Wang", "Fuxing Hong", "Zhen Lin"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Ronan Collobert", "Jason Weston"], "venue": "In Machine Learning, Proceedings of the TwentyFifth International Conference (ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "The VITA financial services sales support environment", "author": ["Klaus Isak", "Kalman Szabo", "Peter Zachar"], "venue": "In Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26,", "citeRegEx": "Felfernig et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Felfernig et al\\.", "year": 2007}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Gregory S. Corrado", "Jonathon Shlens", "Samy Bengio", "Jeffrey Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Content-based recommendations with poisson factorization", "author": ["Gopalan et al.2014] Prem Gopalan", "Laurent Charlin", "David M. Blei"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Gopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gopalan et al\\.", "year": 2014}, {"title": "Wtf: The who to follow service at twitter", "author": ["Pankaj Gupta", "Ashish Goel", "Jimmy Lin", "Aneesh Sharma", "Dong Wang", "Reza Zadeh"], "venue": "In Proceedings of the 22Nd International Conference on World Wide Web,", "citeRegEx": "Gupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2013}, {"title": "Improving word representations via global context and multiple word prototypes. In The 50th Annual Meeting of the Association for Computational Linguistics", "author": ["Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "Proceedings of the Conference,", "citeRegEx": "Huang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2012}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc V. Le", "Tomas Mikolov"], "venue": "In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Hidden factors and hidden topics: understanding rating dimensions with review text", "author": ["McAuley", "Leskovec2013] Julian J. McAuley", "Jure Leskovec"], "venue": "In Proceedings of RecSys,", "citeRegEx": "McAuley et al\\.,? \\Q2013\\E", "shortCiteRegEx": "McAuley et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Mikolov et al.2010] TomasMikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u00fd", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "TomasMikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "TomasMikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. CoRR, abs/1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Exploiting similarities among languages for machine translation. CoRR, abs/1309.4168", "author": ["Quoc V. Le", "Ilya Sutskever"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": null, "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Three new graphical models for statistical language modelling", "author": ["Mnih", "Hinton2007] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "InMachine Learning, Proceedings of the Twenty-Fourth International Conference (ICML", "citeRegEx": "Mnih et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2007}, {"title": "A scalable hierarchical distributed language model", "author": ["Mnih", "Hinton2008] Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2008}, {"title": "Transfer learning in heterogeneous collaborative filtering domains", "author": ["Pan", "Yang2013] Weike Pan", "Qiang Yang"], "venue": "Artif. Intell.,", "citeRegEx": "Pan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pan et al\\.", "year": 2013}, {"title": "Syntactic dependencies and distributed word representations for analogy detection and mining", "author": ["Qiu et al.2015] Likun Qiu", "Yue Zhang", "Yanan Lu"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Qiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Qiu et al\\.", "year": 2015}, {"title": "Item cold-start recommendations: learning local collective embeddings", "author": ["Saveski", "Mantrach2014] Martin Saveski", "Amin Mantrach"], "venue": "In Proceedings of RecSys,", "citeRegEx": "Saveski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saveski et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": ", movies, music, news, books, research articles, search queries, social tags, financial services (Felfernig et al., 2007), and Twitter followers (Gupta et al.", "startOffset": 97, "endOffset": 121}, {"referenceID": 8, "context": ", 2007), and Twitter followers (Gupta et al., 2013).", "startOffset": 31, "endOffset": 51}, {"referenceID": 1, "context": ", collaborative filtering (Breese et al., 1998), content-based filtering (Gopalan et al.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": ", 1998), content-based filtering (Gopalan et al., 2014), and the hybrid filtering (Burke, 2002).", "startOffset": 33, "endOffset": 55}, {"referenceID": 2, "context": ", 2014), and the hybrid filtering (Burke, 2002).", "startOffset": 34, "endOffset": 47}, {"referenceID": 3, "context": "present a topic-model based approach to utilize the context and item information (Chen et al., 2014) to help with recommendation.", "startOffset": 81, "endOffset": 100}, {"referenceID": 16, "context": "Mikolov (Mikolov, 2012) proposes a new statistical language model, RNNLM, based on RNN in (Mikolov et al.", "startOffset": 8, "endOffset": 23}, {"referenceID": 9, "context": "(Huang et al., 2012) propose a new model which increases the global context-aware to enrich the semantic information of words.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "used it to make the language model pre-training of a new deep visual-semantic embedding model, as it has been shown to efficiently learn semantically-meaningful floating point representations of terms from unannotated text (Frome et al., 2013).", "startOffset": 223, "endOffset": 243}, {"referenceID": 20, "context": "(Qiu et al., 2015) explored distributed representations of words to detect analogies.", "startOffset": 0, "endOffset": 18}], "year": 2017, "abstractText": "Collaborative filtering (CF) aims to build a model from users\u2019 past behaviors and/or similar decisions made by other users, and use the model to recommend items for users. Despite of the success of previous collaborative filtering approaches, they are all based on the assumption that there are sufficient rating scores available for building high-quality recommendation models. In real world applications, however, it is often difficult to collect sufficient rating scores, especially when new items are introduced into the system, which makes the recommendation task challenging. We find that there are often \u201cshort\u201d texts describing features of items, based on which we can approximate the similarity of items and make recommendation together with rating scores. In this paper we \u201cborrow\u201d the idea of vector representation of words to capture the information of short texts and embed it into a matrix factorization framework. We empirically show that our approach is effective by comparing it with state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}