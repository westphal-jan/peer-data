{"id": "1703.06284", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2017", "title": "Multi-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks", "abstract": "despite introducing significant leaps made in the recent years in learning single - object speech, significant progress depicted in speaker independent multi - talker mixed tag separation and tracing, often turned to as cluster intra - party event, has passed less impressive. in earlier literature respondents propose any novel analysis utilizing a mesh problem. the core of our technique is permutation spectral estimation ( pit ), which aims nearly integrating the process stream reconstruction without no about how labels contain ordered. this is achieved through aligning speakers to the output membrane automatically during short training time. this algorithms mostly solves the mask permutation problem observed regarding explicit learning based groups exhibiting speech mapping. more appropriately, one approach practitioners offer standardized methods termed direct pit framework whereas that separation and tracing can be carried there skip one step and trained once - on - end. accuracy is achieved using recurrent neural networks ( cs ) by forcing chunk chunks belonging to identifying next speaker to be aligned for matching final output layer triggering training. furthermore, overall computational cost introduced behind pit is very small comparable to the rnn pathway during delay adding weights zero during separation. test evaluated pit on the wsj0 algorithm danish individually - and associate - talker mixed - data language components and found that mesh compares models to high - negative matrix factorization ( mla ), computational interference threshold analysis ( casa ), sparse clustering ( dpcl ) and deep context screening ( sat ), and generalizes well knowing unseen entire individual languages.", "histories": [["v1", "Sat, 18 Mar 2017 10:59:03 GMT  (2955kb,D)", "https://arxiv.org/abs/1703.06284v1", null], ["v2", "Tue, 11 Jul 2017 12:02:01 GMT  (3523kb,D)", "http://arxiv.org/abs/1703.06284v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["morten kolb{\\ae}k", "dong yu", "zheng-hua tan", "jesper jensen"], "accepted": false, "id": "1703.06284"}, "pdf": {"name": "1703.06284.pdf", "metadata": {"source": "CRF", "title": "Multi-talker Speech Separation with Utterance-level Permutation Invariant Training of Deep Recurrent Neural Networks", "authors": ["Morten Kolb\u00e6k", "Dong Yu", "Zheng-Hua Tan", "Jesper Jensen"], "emails": ["mok@es.aau.dk;", "zt@es.aau.dk).", "dongyu@ieee.org).", "jje@es.aau.dk;", "jesj@oticon.com)."], "sections": [{"heading": null, "text": "We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on Non-negative Matrix Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network (DANet). Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.\nIndex Terms\u2014Permutation Invariant Training, Speech Separation, Cocktail Party Problem, Deep Learning, DNN, CNN, LSTM.\nI. INTRODUCTION\nHAVING a conversation in a complex acoustic envi-ronment, with multiple noise sources and competing background speakers, is a task humans are remarkably good at [1], [2]. The problem that humans solve when they focus their auditory attention towards one audio signal in a complex mixture of signals is commonly known as the cocktail party problem [1], [2]. Despite intense research for more than half a century, a general machine based solution to the cocktail party problem is yet to be discovered [1]\u2013[4]. A machine solution to the cocktail party problem is highly desirable for a vast range of applications. These include automatic meeting\nM. Kolb\u00e6k and Z.-H. Tan are with the Department of Electronic Systems, Aalborg University, Aalborg 9220, Denmark (e-mail: mok@es.aau.dk; zt@es.aau.dk).\nD. Yu, corresponding author, is with Tencent AI Lab, Bellevue WA, USA. Part of work was done while he was at Microsoft Research (e-mail: dongyu@ieee.org).\nJ. Jensen is with the Department of Electronic Systems, Aalborg University, Aalborg 9220, Denmark, and also with Oticon A/S, Sm\u00f8rum 2765, Denmark (e-mail: jje@es.aau.dk; jesj@oticon.com).\ntranscription, automatic captioning for audio/video recordings (e.g., YouTube), multi-party human-machine interaction (e.g., in the world of Internet of things (IoT)), and advanced hearing aids, where overlapping speech is commonly encountered.\nSince the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10]. In CASA, different segmentation and grouping rules are used to group Time-Frequency (T-F) units that are believed to belong to the same speaker. The rules are typically hand-engineered and based on heuristics such as pitch trajectory, common onset/offset, periodicity, etc. The grouped T-F units are then used to extract a particular speaker from the mixture signal. Another popular technique for multi-talker speech separation is Non-negative Matrix Factorization (NMF) [11]\u2013[14]. The NMF technique uses non-negative dictionaries to decompose the spectrogram of the mixture signal into speaker specific activations, and from these activations an isolated target signal can be approximated using the dictionaries. For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals. Unfortunately, these models assume and only work under closed-set speaker conditions, i.e. the identity of the speakers must be known a priori.\nMore recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34]. Deep learning has also been applied in the context of multi-talker speech separation (e.g., [30]), although successful work has, similarly to NMF and CASA, mainly been reported for closed-set speaker conditions.\nThe limited success in deep learning based speaker independent multi-talker speech separation is partly due to the label permutation problem (which will be described in detail in Sec. IV). To the authors knowledge only four deep learning based works [35]\u2013[38] exist, that have tried to address and solve the harder speaker independent multi-talker speech separation task.\nIn Weng et al. [35], which proposed the best performing system in the 2006 monaural speech separation and recognition challenge [4], the instantaneous energy was used to\nar X\niv :1\n70 3.\n06 28\n4v 2\n[ cs\n.S D\n] 1\n1 Ju\nl 2 01\n7\n2 determine the training label assignment, which alleviated the label permutation problem and allowed separation of unknown speakers. Although this approach works well for two-speaker mixtures, it is hard to scale up to mixtures of three or more speakers.\nHershey et al. [36] have made significant progress with their Deep Clustering (DPCL) technique. In their work, a deep Recurrent Neural Network (RNN) is used to project the speech mixture into an embedding space, where T-F units belonging to the same speaker form a cluster. In this embedding space a clustering algorithm (e.g. K-means) is used to identify the clusters. Finally, T-F units belonging to the same clusters are grouped together and a binary mask is constructed and used to separate the speakers from the mixture signal. To further improve the model [39], another RNN is stacked on top of the first DPCL RNN to estimate continuous masks for each target speaker. Although DPCL show good performance, the technique is potentially limited because the objective function is based on the affinity between the sources in the embedding space, instead of the separated signals themselves. That is, low proximity in the embedding space does not necessarily imply perfect separation of the sources in the signal space.\nChen et al. [37], [40] proposed a related technique called Deep Attractor Network (DANet). Following DPCL, the DANet approach also learns a high-dimensional embedding of the mixture signals. Different from DPCL, however, it creates attractor points (cluster centers) in the embedding space, which attract the T-F units corresponding to each target speaker. The training is conducted in a way similar to the Expectation Maximization (EM) principle. The main disadvantage of DANet over DPCL is the added complexity associated with estimating attractor points during inference.\nRecently, we proposed the Permutation Invariant Training (PIT) technique1 [38] for attacking the speaker independent multi-talker speech separation problem and showed that PIT effectively solves the label permutation problem. However, although PIT solves the label permutation problem at training time, PIT does not effectively solve the permutation problem during inference, where the permutation of the separated signals at the frame-level is unknown. We denote the challenge of identifying this frame-level permutation, as the speaker tracing problem.\nIn this paper, we extend PIT and propose an utterancelevel Permutation Invariant Training (uPIT) technique, which is a practically applicable, end-to-end, deep learning based solution for speaker independent multi-talker speech separation. Specifically, uPIT extends the frame-level PIT technique [38] with an utterance-level training criterion that effectively eliminates the need for additional speaker tracing or very large input/output contexts, which is otherwise required by the original PIT [38]. We achieve this using deep Long Short-Term Memory (LSTM) RNNs [41] that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. This is unlike other techniques, such as DPCL\n1In [36], a related permutation free technique, which is similar to PIT for exactly two-speakers, was evaluated with negative results and conclusion.\nand DANet, that require a distinct clustering step to separate speakers during inference. Furthermore, the computational cost associated with the uPIT training criterion is negligible compared to the computations required by the RNN during training and is zero during inference. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on NMF and CASA, and compares favorably with DPCL and DANet. Furthermore, we show that models trained with uPIT generalize well to unseen speakers and languages, and finally, we found that a single model trained with uPIT can separate both two-speaker, and three-speaker speech mixtures.\nThe rest of the paper is organized as follows. In Sec. II we describe the monaural speech separation problem. In Sec. III we extend popular optimization criteria used in separating single-talker speech from noises, to multi-talker speech separation tasks. In Sec. IV we discuss the label permutation problem and present the PIT framework. In Sec. V we introduce uPIT and show how an utterance-level permutation criterion can be combined with PIT. We report series of experimental results in Sec. VI and conclude the paper in Sec. VII."}, {"heading": "II. MONAURAL SPEECH SEPARATION", "text": "The goal of monaural speech separation is to estimate the individual source signals xs[n], s = 1, \u00b7 \u00b7 \u00b7 , S in a linearly mixed single-microphone signal\ny[n] = S\u2211 s=1 xs[n], (1)\nbased on the observed signal y[n] only. In real situations, the received signals may be reverberated, i.e., the underlying clean signals are filtered before being observed in the mixture. In this condition, we aim at recovering the reverberated source signals xs[n], i.e., we are not targeting the dereverberated signals.\nThe separation is usually carried out in the T-F domain, in which the task can be cast as recovering the Short-Time discrete Fourier Transformation (STFT) of the source signals Xs(t, f) for each time frame t and frequency bin f , given the mixed speech\nY (t, f) = N\u22121\u2211 n=0 y[n+ tL]w[n] exp(\u2212j2\u03c0nf/N), (2)\nwhere w[n] is the analysis window of length N , the signal is shifted by an amount of L samples for each time frame t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1, and each frequency bin f = 0, \u00b7 \u00b7 \u00b7 , N \u2212 1 is corresponding to a frequency of (f/N)fs [Hz] when the sampling rate is fs [Hz].\nFrom the estimated STFT X\u0302s(t, f) of each source signal, an inverse Discrete Fourier Transform (DFT)\nx\u0302s,t[n] = 1\nN N\u22121\u2211 f=0 X\u0302s(t, f) exp(j2\u03c0nf/N) (3)\ncan be used to construct estimated time-domain frames, and the overlap-add operation\nx\u0302s[n] = T\u22121\u2211 t=0 v[n\u2212 tL]x\u0302s,t[n\u2212 tL] (4)\n3 can be used to reconstruct the estimate x\u0302s[n] of the original signal, where v[n] is the synthesis window.\nIn a typical setup, however, only the STFT magnitude spectrum As(t, f) , |Xs(t, f)| is estimated from the mixture during the separation process, and the phase of the mixed speech is used directly, when recovering the time domain waveforms of the separated sources. This is because phase estimation is still an open problem in the speech separation setup [42], [43]. Obviously, given only the magnitude of the mixed spectrum, R(t, f) , |Y (t, f)|, the problem of recovering As(t, f) is under-determined, as there are an infinite number of possible As(t, f), s = 1, . . . , S combinations that lead to the same R(t, f). To overcome this problem, a supervised learning system has to learn from some training set S that contains corresponding observations of R(t, f) and As(t, f), s = 1, . . . , S.\nLet as,i = [ As(i, 1), As(i, 2), . . . As(i, N 2 + 1) ]T \u2208 RN2 +1 denote the single-sided magnitude spectrum for source s at frame i. Furthermore, let As \u2208 R( N 2 +1)\u00d7T be the singlesided magnitude spectrogram for source s and all frames i = 1, . . . , T , defined as As = [as,1, as,2, . . . ,as,T ]. Similarly, let ri = [ R(i, 1), R(i, 2), . . . R(i, N2 + 1) ]T be the singlesided magnitude spectrum of the observed signal at frame i and let R = [r1, r2, . . . , rT ] \u2208 R( N 2 +1)\u00d7T be the singlesided magnitude spectrogram for all frames i = 1, . . . , T . Furthermore, let us denote a supervector zi =[\naT1,i a T 2,i . . . a T S,i ]T \u2208 RS(N2 +1), consisting of the stacked source magnitude spectra for each source s = 1, . . . , S at frame i and let Z = [z1, z2, . . . , zT ] \u2208 RS( N 2 +1)\u00d7T denote the matrix of all T supervectors. Finally, let yi = [ Y (i, 1), Y (i, 2), . . . Y (i, N2 + 1)\n]T \u2208 CN2 +1 be the single-sided STFT of the observed mixture signal at frame i and Y = [y1, y2, . . . ,yT ] \u2208 C( N 2 +1)\u00d7T be the STFT of the mixture signal for all T frames. Our objective is then to train a deep learning model g(\u00b7), parameterized by a parameter set \u03a6, such that g (d (Y) ;\u03a6) = Z, where d(Y) is some feature representation of the mixture signal: In a particularly simple situation, d(Y) = R, i.e., the feature representation is simply the magnitude spectrum of the observed mixture signal.\nIt is possible to directly estimate the magnitude spectra Z of all sources using a deep learning model. However, it is wellknown (e.g., [27], [43]), that better results can be achieved if, instead of estimating Z directly, we first estimate a set of masks Ms(t, f), s = 1, . . . , S.\nLet ms,i = [ Ms(i, 1) , Ms(i, 2) , . . . Ms(i, N 2 + 1) ]T \u2208 RN2 +1 be the ideal mask (to be defined in detail in Sec. III) for speaker s at frame i, and let Ms = [ms,1, ms,2, . . . ,ms,T ] \u2208 R( N 2 +1)\u00d7T be the ideal mask for all T frames, such that As = Ms \u25e6 R, where \u25e6 is the Hadamard product, i.e. element-wise product of two operands. Furthermore, let us introduce the mask supervector ui =[ mT1,i m T 2,i . . . m T S,i\n]T \u2208 RS(N2 +1) and the corresponding mask matrix U = [u1, u2, . . . ,uT ] \u2208 RS( N 2 +1)\u00d7T . Our goal is then to find an estimate U\u0302 of U, using a deep learning model, h (R;\u03a6) = U\u0302. Since, U\u0302 = [u\u03021, u\u03022, . . . , u\u0302T ] and\nu\u0302i = [ m\u0302T1,i m\u0302 T 2,i . . . m\u0302 T S,i ]T , the model output is easily divided into output streams corresponding to the estimated masks for each speaker m\u0302s,i, and their resulting magnitudes are estimated as a\u0302s,i = m\u0302s,i \u25e6 ri. The estimated time-domain signal for speaker s is then computed as the inverse DFT of a\u0302s,i using the phase of the mixture signal yi."}, {"heading": "III. MASKS AND TRAINING CRITERIA", "text": "Since masks are to be estimated as an intermediate step towards estimating magnitude spectra of source signals, we extend in the following three popular masks defined for separating single-talker speech from noises to the multi-talker speech separation task at hand.\nA. Ideal Ratio Mask\nThe Ideal Ratio Mask (IRM) [27] for each source is defined as\nM irms (t, f) = |Xs(t, f)|\u2211S s=1 |Xs(t, f)| . (5)\nWhen the phase of Y is used for reconstruction, the IRM achieves the highest Signal to Distortion Ratio (SDR) [44], when all sources have the same phase, (which is an invalid assumption in general). IRMs are constrained to 0 \u2264 M irms (t, f) \u2264 1 and \u2211S s=1M irm s (t, f) = 1 for all T-F units. This constraint can easily be satisfied using the softmax activation function. Since Y is the only observed signal in practice and\u2211S s=1 |Xs(t, f)| is unknown during separation, the IRM is not a desirable target for the problem at hand. Nevertheless, we report IRM results as an upper performance bound since the IRM is a commonly used training target for deep learning based monaural speech separation [31], [32].\nB. Ideal Amplitude Mask\nAnother applicable mask is the Ideal Amplitude Mask (IAM) (known as FFT-mask in [27]), or simply Amplitude Mask (AM), when estimated by a deep learning model. The IAM is defined as\nM iams (t, f) = |Xs(t, f)| |Y (t, f)| . (6)\nThrough IAMs we can construct the exact |Xs(t, f)| given the magnitude spectra of the mixed speech |Y (t, f)|. If the phase of each source equals the phase of the mixed speech, the IAM achieves the highest SDR. Unfortunately, as with the IRM, this assumption is not satisfied in most cases. IAMs satisfy the constraint that 0 \u2264 M iams (t, f) \u2264 \u221e, although we found empirically that the majority of the T-F units are in the range of 0 \u2264 M iams (t, f) \u2264 1. For this reason, softmax, sigmoid and ReLU are all possible output activation functions for estimating IAMs.\n4 C. Ideal Phase Sensitive Mask\nBoth IRM and IAM do not consider phase differences between source signals and the mixture. This leads to suboptimal results, when the phase of the mixture is used for reconstruction. The Ideal Phase Sensitive Mask (IPSM) [43], [45]\nM ipsms (t, f) = |Xs(t, f)| cos(\u03b8y(t, f)\u2212 \u03b8s(t, f))\n|Y (t, f)| , (7)\nhowever, takes phase differences into consideration, where \u03b8y and \u03b8s are the phases of mixed speech Y (t, f) and source Xs(t, f), respectively. Due to the phase-correcting term, the IPSM sums to one, i.e. \u2211S s=1M ipsm s (t, f) = 1. Note that since | cos(\u00b7)| \u2264 1 the IPSM is smaller than the IAM, especially when the phase difference between the mixed speech and the source is large.\nEven-though the IPSM in theory is unbounded, we found empirically that the majority of the IPSM is in the range of 0 \u2264 M ipsms (t, f) \u2264 1. Actually, in our study we have found that approximately 20% of IPSMs are negative. However, those negative IPSMs usually are very close to zero. To account for this observation, we propose the Ideal Nonnegative PSM (INPSM), which is defined as\nM inpsms (t, f) = max(0,M ipsm s (t, f)). (8)\nFor estimating the IPSM and INPSM, Softmax, Sigmoid, tanh, and ReLU are all possible activation functions, and similarly to the IAM, when the IPSM is estimated by a deep learning model we refer to it as PSM."}, {"heading": "D. Training Criterion", "text": "Since we first estimate masks, through which the magnitude spectrum of each source can be estimated, the model parameters can be optimized to minimize the Mean Squared Error (MSE) between the estimated mask M\u0302s and one of the target masks defined above as\nJm = 1\nB S\u2211 s=1 \u2016M\u0302s \u2212Ms\u20162F , (9)\nwhere B = T\u00d7N\u00d7S is the total number of T-F units over all sources and \u2016\u00b7\u2016F is the Frobenius norm. This approach comes with two problems. First, in silence segments, |Xs(t, f)| = 0 and |Y (t, f)| = 0, so that the target masks Ms(t, f) are not well defined. Second, what we really care about is the error between the reconstructed source signal and the true source signal.\nTo overcome these limitations, recent works [27] directly minimize the MSE\nJa = 1\nB S\u2211 s=1 \u2016A\u0302s \u2212As\u20162F\n= 1\nB S\u2211 s=1 \u2016M\u0302s \u25e6R\u2212As\u20162F\n(10)\nbetween the estimated magnitude, i.e. A\u0302s = M\u0302s \u25e6R and the true magnitude As. Note that in silence segments As(t, f) = 0\nand R(t, f) = 0, so the accuracy of mask estimation does not affect the training criterion for those segments. Furthermore, using Eq. (10) the IAM is estimated as an intermediate step.\nWhen the PSM is used, the cost function becomes\nJpsm = 1\nB S\u2211 s=1 \u2016M\u0302s \u25e6R\u2212As \u25e6 cos(\u03b8y \u2212 \u03b8s)\u20162F . (11)\nIn other words, using PSMs is as easy as replacing the original training targets with the phase discounted targets. Furthermore, when Eq. (11) is used as a cost function, the IPSM is the upper bound achievable on the task [43]."}, {"heading": "IV. PERMUTATION INVARIANT TRAINING", "text": ""}, {"heading": "A. Conventional Multi-Talker Separation", "text": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig. 1.\nFor this conventional two-talker separation model, J frames of feature vectors of the mixed signal Y are used as the input to some deep learning model e.g. a feed-forward Deep Neural Network (DNN), Convolutional Neural Network (CNN), or LSTM RNN, to generate M frames of masks for each talker. Specifically, if M = 1, the output of the model can be described by the vector u\u0302i = [ m\u0302T1,i m\u0302 T 2,i ]T and the sources are separated as a\u03021,i = m\u03021,i \u25e6 ri and a\u03022,i = m\u03022,i \u25e6 ri, for sources s = 1, 2, respectively."}, {"heading": "B. The Label Permutation Problem", "text": "During training, the error (e.g. using Eq. (11)) between the clean magnitude spectra a1,i and a2,i and their estimated counterparts a\u03021,i and a\u03022,i needs to be computed. However, since the model estimates the masks m\u03021,i and m\u03022,i simultaneously, and they depend on the same input mixture, it is unknown in advance whether the resulting output vector u\u0302i is ordered as u\u0302i = [ m\u0302T1,i m\u0302 T 2,i ]T or u\u0302i = [ m\u0302T2,i m\u0302 T 1,i ]T . That is, the permutation of the output masks is unknown. A na\u0131\u0308ve approach to train a deep learning separation model, without exact knowledge about the permutation of the output\n5 masks, is to use a constant permutation as illustrated by Fig. 1. Although such a training approach works for simple cases e.g. female speakers mixed with male speakers, in which case a priori convention can be made that e.g. the first output stream contains the female speaker, while the second output stream is paired with the male speaker, the training fails if the training set consists of many utterances spoken by many speakers of both genders.\nThis problem is referred to as the label permutation (or ambiguity) problem in [35], [36]. Due to this problem, prior arts perform poorly on speaker independent multi-talker speech separation."}, {"heading": "C. Permutation Invariant Training", "text": "Our solution to the label permutation problem is illustrated in Fig. 2 and is referred to as Permutation Invariant Training (PIT) [38].\nIn the model depicted in Fig. 2 (and unlike the conventional model in Fig. 1) the reference signals are given as a set instead of an ordered list. In other words, the same training result is obtained, no matter in which order these references are listed. This behavior is achieved with PIT highlighted inside the dashed rectangle in Fig. 2. Specifically, following the notation from Sec. II, we associate the reference signals for speaker one and two, i.e. a1,i and a2,i, to the output masks m\u03021,i and m\u03022,i, by computing the (total of S2) pairwise MSE\u2019s between each reference signal as,i and each estimated source a\u0302s,i. We then determine the (total of S!) possible permutations between the references and the estimated sources, and compute the perpermutation-loss for each permutation. That is, for the twospeaker case in Fig. 2 we compute the per-permutation-loss for the two candidate output vectors u\u0302i = [ m\u0302T1,i m\u0302 T 2,i ]T and\nu\u0302i = [ m\u0302T2,i m\u0302 T 1,i ]T . The permutation with the lowest MSE is chosen and the model is optimized to reduce this least MSE. In other words, we simultaneously conduct label assignment and error evaluation. Similarly to prior arts, we can use J , and M\nsuccessive input, and output frames, respectively, (i.e., a metaframe) to exploit the contextual information. Note that only S2 pairwise MSE\u2019s are required (and not S!) to compute the per-permutation-loss for all S! possible permutations. Since S! grows much faster than S2, with respect to S, and the computational complexity of the pairwise MSE is much larger than the per-permutation-loss (sum of pairwise MSE\u2019s), PIT can be used with a large number of speakers, i.e. S 2.\nDuring inference, the only information available is the mixed speech, but speech separation can be directly carried out for each input meta-frame, for which an output metaframe with M frames of speech is estimated. Due to the PIT training criterion, the permutation will stay the same for frames inside the same output meta-frame, but may change across output meta-frames. In the simplest setup, we can just assume that permutations do not change across output metaframes, when reconstructing the target speakers. However, this usually leads to unsatisfactory results as reported in [38]. To achieve better performance, speaker tracing algorithms, that identify the permutations of output meta-frames with respect to the speakers, need to be developed and integrated into the PIT framework or applied on top of the output of the network."}, {"heading": "V. UTTERANCE-LEVEL PIT", "text": "Several ways exist for identifying the permutation of the output meta-frames, i.e. solving the tracing problem. For example, in CASA a related problem referred to as the Sequential Organization Problem has been addressed using a model-based sequential grouping algorithm [9]. Although moderately successful for co-channel speech separation, where prior knowledge about the speakers is available, this method is not easily extended to the speaker independent case with multiple speakers. Furthermore, it is not easily integrated into a deep learning framework.\nA more straight-forward approach might be to determine a change in permutation by comparing MSEs for different permutations of output masks measured on the overlapping frames of adjacent output meta-frames. However, this approach has two major problems. First, it requires a separate tracing step, which may complicate the model. Second, since the permutation of later frames depends on that of earlier frames, one incorrect assignment at an earlier frame would completely switch the permutation for all frames after it, even if the assignment decisions for the remaining frames are all correct.\nIn this work we propose utterance-level Permutation Invariant Training (uPIT), a simpler yet more effective approach to solve the tracing problem and the label permutation problem than original PIT. Specifically, we extend the frame-level PIT technique with the following utterance-level cost function:\nJ\u03c6\u2217 = 1\nB S\u2211 s=1 \u2016M\u0302s \u25e6R\u2212A\u03c6\u2217(s) \u25e6 cos(\u03b8y \u2212 \u03b8\u03c6\u2217(s))\u20162F , (12)\nwhere \u03c6\u2217 is the permutation that minimizes the utterance-level separation error defined as\n\u03c6\u2217 = argmin \u03c6\u2208P S\u2211 s=1 \u2016M\u0302s \u25e6R\u2212A\u03c6(s) \u25e6 cos(\u03b8y\u2212 \u03b8\u03c6(s))\u20162F , (13)\n6 and P is the symmetric group of degree S, i.e. the set of all S! permutations.\nIn original PIT, the optimal permutation (in MSE sense) is computed and applied for each output meta-frame. This implies that consecutive meta-frames might be associated with different permutations, and although PIT solves the label permutation problem, it does not solve the speaker tracing problem. With uPIT, however, the permutation corresponding to the minimum utterance-level separation error is used for all frames in the utterance. In other words, the pair-wise scores in Fig. 2 are computed for the whole utterance assuming all output frames follow the same permutation. Using the same permutation for all frames in the utterance might imply that a non-MSE-optimal permutation is used for individual frames within the utterance. However, the intuition behind uPIT is that since the permutation resulting in the minimum utterance-level separation error is used, the number of non-optimal permutations is small and the model sees enough correctly permuted frames to learn an efficient separation model. For example, the output vector u\u0302i of a perfectly trained two-talker speech separation model, given an input utterance, should ideally be u\u0302i = [ m\u0302T1,i m\u0302 T 2,i ]T , or u\u0302i = [ m\u0302T2,i m\u0302 T 1,i\n]T \u2200 i = 1, . . . , T , i.e. the output masks should follow the same permutation for all T frames in the utterance. Fortunately, using Eq. (12) as a training criterion, for deep learning based speech separation models, this seems to be the case in practice (See Sec. VI for examples).\nSince utterances have variable length, and effective separation presumably requires exploitation of long-range signal dependencies, models such as DNNs and CNNs are no longer good fits. Instead, we use deep LSTM RNNs and bi-directional LSTM (BLSTM) RNNs together with uPIT to learn the masks. Different from PIT, in which the input layer and each output layer has N \u00d7T and N \u00d7M units, respectively, in uPIT, both input and output layers have N units (adding contextual frames in the input does not help for LSTMs). With deep LSTMs, the utterance is evaluated frame-by-frame exploiting the whole past history information at each layer. When BLSTMs are used, the information from the past and future (i.e., across the whole utterance) is stacked at each layer and used as the input to the subsequent layer. With uPIT, during inference we don\u2019t need to compute pairwise MSEs and errors of each possible permutation and no additional speaker tracing step is needed. We simply assume a constant permutation and treat the same output mask to be from the same speaker for all frames. This makes uPIT a simple and attractive solution."}, {"heading": "VI. EXPERIMENTAL RESULTS", "text": "We evaluated uPIT on various setups and all models were implemented using the Microsoft Cognitive Toolkit (CNTK) [47], [48]2. The models were evaluated on their potential to improve the Signal-to-Distortion Ratio (SDR) [44] and the Perceptual Evaluation of Speech Quality (PESQ) [49] score, both of which are metrics widely used to evaluate speech enhancement performance for multi-talker speech separation tasks.\n2Available at: https://www.cntk.ai/"}, {"heading": "A. Datasets", "text": "We evaluated uPIT on the WSJ0-2mix, WSj0-3mix3 and Danish-2mix datasets using 129-dimensional STFT magnitude spectra computed with a sampling frequency of 8 kHz, a frame size of 32 ms and a 16 ms frame shift.\nThe WSJ0-2mix dataset was introduced in [36] and was derived from the WSJ0 corpus [50]. The 30h training set and the 10h validation set contain two-speaker mixtures generated by randomly selecting from 49 male and 51 female speakers and utterances from the WSJ0 training set si tr s, and mixing them at various Signal-to-Noise Ratios (SNRs) uniformly chosen between 0 dB and 5 dB. The 5h test set was similarly generated using utterances from 16 speakers from the WSJ0 validation set si dt 05 and evaluation set si et 05. The WSJ0-3mix dataset was generated using a similar approach but contains mixtures of speech from three talkers.\nThe Danish-2mix dataset is based on a corpus4 with approximately 560 speakers each speaking 312 utterances with average utterance duration of approximately 5 sec. The dataset was constructed by randomly selecting a set of 45 male and 45 female speakers from the corpus, and then allocating 232 and 40 utterances from each speaker to generate mixed speech in the training, and validation set, respectively. A number of 40 utterances from each of another 45 male and 45 female speakers were randomly selected to construct the opencondition (OC) (unseen speaker) test set. Speech mixtures were constructed similarly to the WSJ0-2mix with SNRs selected uniformly between 0 dB and 5 dB. Similarly to the WSJ0-2mix dataset we constructed 20k and 5k mixtures in total in the training and validation set, respectively, and 3k mixtures for the OC test set.\nIn our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39]."}, {"heading": "B. Permutation Invariant Training", "text": "We first evaluated the original frame-level PIT on the twotalker separation dataset WSJ0-2mix, and differently from [38], we fixed the input dimension to 51 frames, to isolate the effect of a varying output dimension. In PIT, the input window and output window sizes are fixed. For this reason, we can use DNNs and CNNs. The DNN model has three hidden layers each with 1024 ReLU units. In (inChannel, outChannel)-(strideW, strideH) format, the CNN model has one (1, 64)\u2212(2, 2), four (64, 64)\u2212(1, 1), one (64, 128)\u2212(2, 2), two (128, 128) \u2212 (1, 1), one (128, 256) \u2212 (2, 2), and two (256, 256) \u2212 (1, 1) convolution layers with 3 \u00d7 3 kernels, a 7 \u00d7 17 average pooling layer and a 1024-unit ReLU layer. The input to the models is the stack (over multiple frames) of the 129-dimensional STFT spectral magnitude of the speech mixture. The output layer u\u0302i is divided into S output masks/streams for S-talker mixed speech as u\u0302i = [m\u03021,i ; m\u03022,i ; . . . ; m\u0302S,i]\nT . Each output mask vector m\u0302s,i has a dimension of 129 \u00d7 M , where M is the number of frames in the output meta-frame.\n3Available at: http://www.merl.com/demos/deep-clustering 4Available at: http://www.nb.no/sbfil/dok/nst taledat dk.pdf\n7\nIn Fig. 3 we present the DNN training progress as measured by the MSE on the training and validation set with conventional training (CONV-DNN) and PIT on the WSJ02mix datasets described in subsection VI-A. We also included the training progress for another conventionally trained model but with a slightly modified version of the WSJ0-2mix dataset, where speaker labels have been randomized (CONV-DNNRAND).\nThe WSJ0-2mix dataset, used in [36], was designed such that speaker one was always assigned the most energy, and consequently speaker two the lowest, when scaling to a given SNR. Previous work [35] has shown that such speaker energy patterns are an effective discriminative feature, which is clearly seen in Fig. 3, where the CONV-DNN model achieves considerably lower training and validation MSE than the CONVDNN-RAND model, which hardly decreases in either training or validation MSE due to the label permutation problem [35], [36]. In contrast, training converges quickly to a very low MSE when PIT is used.\nIn Table I we summarize the SDR improvement in dB from different frame-level PIT separation configurations for twotalker mixed speech in closed condition (CC) and open condition (OC). In these experiments each frame was reconstructed by averaging over all output meta-frames that contain the same frame. In the default assignment (def. assign.) setup, a constant output mask permutation is assumed across frames (which is an invalid assumption in general). This is the maximum achievable SDR improvement using PIT without the utterancelevel training criterion and without an additional tracing step.\nIn the optimal assignment (opt. assign.) setup, the output-mask permutation for each output meta-frame is determined based on the true target, i.e. oracle information. This reflects the separation performance within each segment (meta-frame) and is the improvement achievable when the speakers are correctly separated. The gap between these two values indicates the possible contribution from speaker tracing. As a reference, we also provided the IRM and IPSM results.\nFrom the table we can make several observations. First, PIT can already achieve 7.5 dB SDR improvement (def. assign.), even though the model is very simple. Second, as we reduce the output window size, we can improve the separation performance within each window and achieve better SDR improvement, if speakers are correctly traced (opt. assign.). However, when output window size is reduced, the output mask permutation changes more frequently as indicated by the poor default assignment performance. Speaker tracing thus becomes more important given the larger gap between the optimal assignment and default assignment. Third, PIT generalizes well to unseen speakers, since the performances on the open and closed conditions are very close. Fourth, powerful models such as CNNs consistently outperform DNNs, but the gain diminishes when the output window size is small."}, {"heading": "C. Utterance-level Permutation Invariant Training", "text": "As indicated by Table I, an accurate output mask permutation is critical to further improve the separation quality. In this subsection we evaluate the uPIT technique as discussed in Sec. V and the results are summarized in Table II.\nDue to the formulation of the uPIT cost function in Eq. (12) and Eq. (13), and to utilize long-range context, RNNs are the\n8 natural choice, and in this set of experiments, we used LSTM RNNs. All the uni-directional LSTMs (uPIT-LSTM) evaluated have 3 LSTM layers each with 1792 units and all the bidirectional LSTMs (uPIT-BLSTM) have 3 BLSTM layers each with 896 units, so that both models have similar number of parameters.\nAll models contain random dropouts when fed from a lower layer to a higher layer and were trained with a dropout rate of 0.5. Note that, since we used Nvidia\u2019s cuDNN implementation of LSTMs, to speed up training, we were unable to apply dropout across time steps, which was adopted by the best DPCL model [39] and is known to be more effective, both theoretically and empirically, than the simple dropout strategy used in this work [51].\nIn all the experiments reported in Table II the maximum epoch is set to 200 although we noticed that further performance improvement is possible with additional training epochs. Note that the epoch size of 200 seems to be significantly larger than that in PIT as indicated in Fig. 3. This is likely because in PIT each frame is used by T (T = 51) training samples (input meta-frames) while in uPIT each frame is used just once in each epoch.\nThe learning rates were set to 2\u00d710\u22125 per sample initially and scaled down by 0.7 when the training objective function value increases on the training set. The training was terminated when the learning rate got below 10\u221210. Each minibatch contains 8 randomly selected utterances.\nAs a related baseline, we also include PIT-BLSTM results in Table II. These models were also trained using LSTMs with whole utterances instead of meta-frames. The only difference between these models and uPIT models is that uPIT models use the utterance-level training criterion defined in Eqs. (12) and (13), instead of the meta-frame based criterion used by PIT.\n1) uPIT Training Progress: In Fig. 4 we present a representative example of the BLSTM training progress, as measured by the MSE of the two-talker mixed speech training and validation set, using Eq. (12). We see that the training and validation MSE\u2019s are both steadily decreasing as function of epochs, hence uPIT, similarly to PIT, effectively solves the label permutation problem.\n2) uPIT Performance for Different Setups: From Table II, we can notice several things. First, with uPIT, we can signifi-\ncantly improve the SDR with default assignment over original PIT. In fact, a 9.4 dB SDR improvement on both CC and OC sets can be achieved by simply assuming a constant output mask permutation (def. assign.), which compares favorably to 7.6 dB (CC) and 7.5 dB (OC) achieved with deep CNNs combined with PIT. We want to emphasize that this is achieved through Eqs. (12) and (13), and not by using BLSTMs because the corresponding PIT-BLSTM default assignment results are so much worse, even though the optimal assignment results are the best among all models. The latter may be explained from the PIT objective function that attempts to obtain a constant output mask permutation at the meta-frame-level, which for small meta-frames is assumed easier compared to the uPIT objective function, that attempts to obtain a constant output mask permutation throughout the whole utterance. Second, we can achieve better SDR improvement over the AM using PSM and NPSM training criteria. This indicates that including phase information does improve performance, eventhough it was used implicitly via the cosine term in Eq. (12). Third, with uPIT the gap between optimal assignment and default assignment is always less than 1.5 dB across different setups, hence additional improvements from speaker tracing algorithms is limited to 1.5 dB.\n3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54]. In Table III we show that a similar principle of cascading two BLSTM models into a two-stage model (-ST models in Table III) can lead to improved performance over the models presented in Table II. In Table III we also show that improved performance, with respect to the same models, can be achieved with additional training epochs combined with a reduced dropout rate (-RD models in Table III). Specifically, if we continue the training of the two best performing models from Table II (i.e. uPIT-BLSTM-PSM-ReLU and uPIT-BLSTMNPSM-Sigmoid) with 200 additional training epochs at a reduced dropout rate of 0.3, we see an improvement of 0.1 dB. Even larger improvements can be achieved with the two-stage approach, where an estimated mask is computed as the average mask from two BLSTM models as\nM\u0302s = M\u0302\n(1) s + M\u0302 (2) s\n2 . (14)\nThe mask M\u0302(1)s is from an -RD model that serves as a firststage model, and M\u0302(2)s is the output mask from a secondstage model. The second-stage model is trained using the original input features as well as the mask M\u0302(1)s from the firststage model. The intuition behind this architecture is that the second-stage model will learn to correct the errors made by the first-stage model. Table III shows that the two-stage models (-ST models) always outperform the single-stage models (- RD models) and overall, a 10 dB SDR improvement can be achieved on this task using a two-stage approach.\n4) Opposite Gender vs. Same Gender.: Table IV reports SDR (dB) improvements on test sets of WSJ0-2mix divided into opposite-gender (Opp.) and same-gender (Same). From this table we can clearly see that our approach achieves much better SDR improvements on the opposite-gender mixed\n9\nspeech than the same-gender mixed speech, although the gender information is not explicitly used in our model and training procedure. In fact, for the opposite-gender condition, the SDR improvement is already very close to the IRM result. These results agree with breakdowns from other works [36], [39] and generally indicate that same-gender mixed speech separation is a harder task.\n5) Multi-Language Models: To further understand the properties of uPIT, we evaluated the uPIT-BLSTM-PSM-ReLU model trained on WSJ0-2mix (English) on the Danish-2mix test set. The results of this is reported in Table V. An interesting observation, is that although the system has never seen Danish speech, it performs remarkably well in terms of SDR, when compared to the IRM (oracle) values. These results indicate, that the separation ability learned with uPIT generalizes well, not only across speakers, but also across languages. In terms of PESQ, we see a somewhat larger performance gap with respect to the IRM. This might be explained by the fact that SDR is a waveform matching criteria and does not necessarily reflect perceived quality as well as PESQ. Furthermore, we note that the PESQ improvements are similar to what have been reported for DNN based speech enhancement systems [32].\nWe also trained a model with the combination of English and Danish datasets and evaluated the models on both languages. The results of these experiments are summarized in Table V. Table V, indicate that by including Danish data, we can achieve better performance on the Danish dataset, at the cost of slightly worse performance on the English dataset. Note that while doubling the training set, we did not change the model size. Had we done this, performance would likely improve on both languages.\n6) Summary of Multiple 2-Speaker Separation Techniques: Table VI summarizes SDR (dB) and PESQ improvements for different separation methods on the WSJ0-2mix dataset. From the table we can observe that the models trained with PIT already achieve similar or better SDR than the original DPCL [36], respectively, with DNNs and CNNs. Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51]. Note that, both uPIT and PIT models are much simpler than DANet, DPCL, DPCL+, and DPCL++, because uPIT and PIT models do not require any clustering step during inference or estimation of attractor points, as required by DANet."}, {"heading": "D. Three-Talker Speech Separation", "text": "In Fig. 5 we present the uPIT training progress as measured by MSE on the three-talker mixed speech training and validation sets WSJ0-3mix. We observe that similar to the two-talker scenario in Fig. 4, a low training MSE is achieved, although the validation MSE is slightly higher. A better balance between the training and validation MSEs may be achieved by hyperparameter tuning. We also observe that increasing the model size decreases both training and validation MSE, which is expected due to the more variability in the dataset.\n5 [37], [39] did not use the SDR measure from [44]. Instead a related variant called scale-invariant SNR was used.\n10\nIn Table VII we summarize the SDR improvement in dB from different uPIT separation configurations for threetalker mixed speech, in closed condition (CC) and open condition (OC). We observe that the basic uPIT-BLSTM model (896 units) compares favorably with DPCL++. Furthermore, with additional units, further training and two-stage models (based on uPIT-BLSTM), uPIT achieves higher SDR than DPCL++ and similar SDR as DANet, without curriculum training, on this three-talker separation task."}, {"heading": "E. Combined Two- and Three-Talker Speech Separation", "text": "To illustrate the flexibility of uPIT, we summarize in Table VIII the performance of the three-speaker uPIT-BLSTM, and uPIT-BLSTM-ST models (from Table VII), when they are trained and tested on both the WSJ0-2mix and WSJ0-3mix datasets, i.e. on both two- and three-speaker mixtures.\nTo be able to train the three-speaker models with the twospeaker WSJ0-2mix dataset, we extended WSJ0-2mix with a third \u201dsilent\u201d channel. The silent channel consists of white Gaussian noise with an energy level 70 dB below the average energy level of the remaining two speakers in the mixture. When we evaluated the model, we identified the two speakeractive output streams as the ones corresponding to the signals with the most energy.\nWe see from Table VIII that uPIT-BLSTM achieves good, but slightly worse, performance compared to the corresponding two-speaker (Table VI) and three-speaker (Table VII)\nmodels. Surprisingly, the uPIT-BLSTM-ST model outperforms both the two-speaker (Table III) and three-speaker uPITBLSTM-ST (Table VII) models. These results indicate that a single model can handle a varying, and more importantly, unknown number of speakers, without compromising performance. This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].\nDuring evaluation of the 3000 mixtures in the WSJ0-2mix test set, output stream one and two were the output streams with the most energy, i.e. the speaker-active output streams, in 2999 cases. Furthermore, output stream one and two had, on average, an energy level approximately 33 dB higher than the silent channel, indicating that the models successfully keep a constant permutation of the output masks throughout the test utterance. As an example, Fig. 6 shows the spectrogram for a single two-speaker (male-vs-female) test case along with the spectrograms of the three output streams of the uPITBLSTM model, as well as the clean speech signals from each of the two speakers. Clearly, output streams one and two contain the most energy and output stream three consists primarily of a low energy signal without any clear structure. Furthermore, by comparing the spectrograms of the clean speech signals (\u201dSpeaker 1\u201d and \u201dSpeaker 2\u201d in Fig. 6) to the spectrogram of the corresponding output streams, it is observed that they share many similarities, which indicate that the model kept a constant output-mask permutation for the entire mixture and successfully separated the two speakers into two separate output streams. This is also supported by the SDR improvements, which for output stream one (\u201dSpeaker 1\u201d) is 13.7 dB, and for output stream two (\u201dSpeaker 2\u201d) is 12.1 dB."}, {"heading": "VII. CONCLUSION AND DISCUSSION", "text": "In this paper, we have introduced the utterance-level Permutation Invariant Training (uPIT) technique for speaker independent multi-talker speech separation. We consider uPIT an interesting step towards solving the important cocktail party problem in a real-world setup, where the set of speakers is unknown during the training time.\nOur experiments on two- and three-talker mixed speech separation tasks indicate that uPIT can indeed effectively deal with the label permutation problem. These experiments show that bi-directional Long Short-Term Memory (LSTM)\n11\nRecurrent Neural Networks (RNNs) perform better than unidirectional LSTMs and Phase Sensitive Masks (PSMs) are better training criteria than Amplitude Masks (AM). Our results also suggest that the acoustic cues learned by the model are largely speaker and language independent since the models generalize well to unseen speakers and languages. More importantly, our results indicate that uPIT trained models do not require a priori knowledge about the number of speakers in the mixture. Specifically, we show that a single model can handle\nboth two-speaker and three-speaker mixtures. This indicates that it might be possible to train a universal speech separation model using speech in various speaker, language and noise conditions.\nThe proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference. Since uPIT, as a training technique, can be easily integrated and combined with other advanced techniques such as complex-domain separation and multi-channel techniques, such as beam-forming, uPIT has great potential for further improvement."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Dr. John Hershey at MERL and Zhuo Chen at Columbia University for sharing the WSJ02mix and WSJ0-3mix datasets and for valuable discussions. We also thank Dr. Hakan Erdogan at Microsoft Research for discussions on PSM."}], "references": [{"title": "The Cocktail Party Problem", "author": ["S. Haykin", "Z. Chen"], "venue": "Neural Comput., vol. 17, no. 9, pp. 1875\u20131902, 2005.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1875}, {"title": "The Cocktail Party Phenomenon: A Review of Research on Speech Intelligibility in Multiple-Talker Conditions", "author": ["A.W. Bronkhorst"], "venue": "Acta Acust united Ac, vol. 86, no. 1, pp. 117\u2013128, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Some Experiments on the Recognition of Speech, with One and with Two Ears", "author": ["E.C. Cherry"], "venue": "J. Acoust. Soc. Am., vol. 25, no. 5, pp. 975\u2013 979, Sep. 1953.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1953}, {"title": "Monaural Speech Separation and Recognition Challenge", "author": ["M. Cooke", "J.R. Hershey", "S.J. Rennie"], "venue": "Comput. Speech Lang., vol. 24, no. 1, pp. 1\u201315, Jan. 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech Separation by Humans and Machines", "author": ["P. Divenyi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Prediction-driven computational auditory scene analysis", "author": ["D.P.W. Ellis"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 1996.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Modelling Auditory Processing and Organisation", "author": ["M. Cooke"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Computational Auditory Scene Analysis: Principles, Algorithms, and Applications", "author": ["D. Wang", "G.J. Brown"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Model-based sequential organization in cochannel speech", "author": ["Y. Shao", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 14, no. 1, pp. 289\u2013298, Jan. 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "An Unsupervised Approach to Cochannel Speech Separation", "author": ["K. Hu", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 21, no. 1, pp. 122\u2013131, Jan. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Single-Channel Speech Separation using Sparse Non-Negative Matrix Factorization", "author": ["M.N. Schmidt", "R.K. Olsson"], "venue": "Proc. INTER- SPEECH, 2006, pp. 2614\u20132617.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Convolutive Speech Bases and Their Application to Supervised Speech Separation", "author": ["P. Smaragdis"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 15, no. 1, pp. 1\u201312, Jan. 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Algorithms for Non-negative Matrix Factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS, 2000, pp. 556\u2013562.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Super-human multi-talker speech recognition: the IBM 2006 speech separation challenge system", "author": ["T.T. Kristjansson"], "venue": "Proc. INTER- SPEECH, 2006, pp. 97\u2013100.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Speech Recognition Using Factorial Hidden Markov Models for Separation in the Feature Space", "author": ["T. Virtanen"], "venue": "Proc. INTERSPEECH, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Source-Filter-Based Single- Channel Speech Separation Using Pitch Information", "author": ["M. Stark", "M. Wohlmayr", "F. Pernkopf"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 19, no. 2, pp. 242\u2013255, Feb. 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Factorial Hidden Markov Models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning, vol. 29, no. 2-3, pp. 245\u2013273, 1997.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Roles of Pre-Training and Fine- Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition", "author": ["D. Yu", "L. Deng", "G.E. Dahl"], "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-Dependent Pre- Trained Deep Neural Networks for Large-Vocabulary Speech Recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 20, no. 1, pp. 30\u201342, Jan. 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. INTERSPEECH, 2011, pp. 437\u2013440.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", "author": ["G. Hinton"], "venue": "IEEE Sig. Process. Mag., vol. 29, no. 6, pp. 82\u201397, Nov. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Achieving Human Parity in Conversational Speech Recognition", "author": ["W. Xiong"], "venue": "arXiv:1610.05256 [cs], 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "English Conversational Telephone Speech Recognition by Humans and Machines", "author": ["G. Saon"], "venue": "arXiv:1703.02136 [cs], 2017.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Towards Scaling Up Classification-Based Speech Separation", "author": ["Y. Wang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 21, no. 7, pp. 1381\u20131390, Jul. 2013.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "On Training Targets for Supervised Speech Separation", "author": ["Y. Wang", "A. Narayanan", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 22, no. 12, pp. 1849\u20131858, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1849}, {"title": "An Experimental Study on Speech Enhancement Based on Deep Neural Networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE Sig. Process. Let., vol. 21, no. 1, pp. 65\u201368, Jan. 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech Enhancement with LSTM Recurrent Neural Networks and Its Application to Noise-Robust ASR", "author": ["F. Weninger"], "venue": "LVA/ICA. Springer, 2015, pp. 91\u201399.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for Monaural Source Separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 12, pp. 2136\u20132147, 2015.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises", "author": ["J. Chen"], "venue": "J. Acoust. Soc. Am., vol. 139, no. 5, pp. 2604\u20132612, 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech Intelligibility Potential of General and Specialized Deep Neural Network Based Speech Enhancement Systems", "author": ["M. Kolb\u00e6k", "Z.H. Tan", "J. Jensen"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 25, no. 1, pp. 153\u2013167, 2017.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2017}, {"title": "Speech separation of a target speaker based on deep neural networks", "author": ["J. Du"], "venue": "ICSP, 2014, pp. 473\u2013477.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech enhancement based on neural networks improves speech intelligibility in noise for cochlear implant users", "author": ["T. Goehring"], "venue": "Hearing Research, vol. 344, pp. 183\u2013194, 2017.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep Neural Networks for Single-Channel Multi-Talker Speech Recognition", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 23, no. 10, pp. 1670\u20131679, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["J.R. Hershey", "Z. Chen", "J.L. Roux", "S. Watanabe"], "venue": "Proc. ICASSP, 2016, pp. 31\u201335.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep attractor network for singlemicrophone speaker separation", "author": ["Z. Chen", "Y. Luo", "N. Mesgarani"], "venue": "Proc. ICASSP, 2017, pp. 246\u2013250.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}, {"title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "author": ["D. Yu", "M. Kolb\u00e6k", "Z.-H. Tan", "J. Jensen"], "venue": "Proc. ICASSP, 2017, pp. 241\u2013245.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2017}, {"title": "Single-Channel Multi-Speaker Separation Using Deep Clustering", "author": ["Y. Isik"], "venue": "Proc. INTERSPEECH, 2016, pp. 545\u2013549.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Single Channel Auditory Source Separation with Neural Network", "author": ["Z. Chen"], "venue": "Ph.D., Columbia University, United States \u2013 New York, 2017.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1997}, {"title": "Complex Ratio Masking for Monaural Speech Separation", "author": ["D.S. Williamson", "Y. Wang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 24, no. 3, pp. 483\u2013492, Mar. 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Recurrent Networks for Separation and Recognition of Single Channel Speech in Non-stationary Background Audio", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "J.L. Roux"], "venue": "New Era for Robust Speech Recognition: Exploiting Deep Learning. Springer, 2017.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2017}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 14, no. 4, pp. 1462\u20131469, 2006.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2006}, {"title": "Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "J.L. Roux"], "venue": "Proc. ICASSP, 2015, pp. 708\u2013712.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network based speech separation for robust speech recognition", "author": ["Y. Tu"], "venue": "ICSP, 2014, pp. 532\u2013536.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "The Computational Network Toolkit", "author": ["D. Yu", "K. Yao", "Y. Zhang"], "venue": "IEEE Sig. Process. Mag., vol. 32, no. 6, pp. 123\u2013126, Nov. 2015.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["A. Agarwal"], "venue": "Microsoft Technical Report {MSR- TR}-2014-112, Tech. Rep., 2014.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs", "author": ["A. Rix", "J. Beerends", "M. Hollier", "A. Hekstra"], "venue": "Proc. ICASSP, vol. 2, 2001, pp. 749\u2013752.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "CSR-I (WSJ0) Complete LDC93s6a", "author": ["J. Garofolo", "D. Graff", "P. Doug", "D. Pallett"], "venue": "1993, philadelphia: Linguistic Data Consortium.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1993}, {"title": "A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", "author": ["Y. Gal", "Z. Ghahramani"], "venue": "arXiv:1512.05287, Dec. 2015.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2015}, {"title": "A Deep Ensemble Learning Method for Monaural Speech Separation", "author": ["X.L. Zhang", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 24, no. 5, pp. 967\u2013977, May 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep stacking networks with time series for speech separation", "author": ["S. Nie", "H. Zhang", "X. Zhang", "W. Liu"], "venue": "Proc. ICASSP, 2014, pp. 6667\u2013 6671.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent Deep Stacking Networks for Supervised Speech Separation", "author": ["Z.-Q. Wang", "D. Wang"], "venue": "Proc. ICASSP, 2017, pp. 71\u201375.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "HAVING a conversation in a complex acoustic environment, with multiple noise sources and competing background speakers, is a task humans are remarkably good at [1], [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "HAVING a conversation in a complex acoustic environment, with multiple noise sources and competing background speakers, is a task humans are remarkably good at [1], [2].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "The problem that humans solve when they focus their auditory attention towards one audio signal in a complex mixture of signals is commonly known as the cocktail party problem [1], [2].", "startOffset": 176, "endOffset": 179}, {"referenceID": 1, "context": "The problem that humans solve when they focus their auditory attention towards one audio signal in a complex mixture of signals is commonly known as the cocktail party problem [1], [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 0, "context": "Despite intense research for more than half a century, a general machine based solution to the cocktail party problem is yet to be discovered [1]\u2013[4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 3, "context": "Despite intense research for more than half a century, a general machine based solution to the cocktail party problem is yet to be discovered [1]\u2013[4].", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 120, "endOffset": 123}, {"referenceID": 5, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 230, "endOffset": 233}, {"referenceID": 9, "context": "Since the cocktail party problem was initially formalized [3], a large number of potential solutions have been proposed [5], and the most popular techniques originate from the field of Computational Auditory Scene Analysis (CASA) [6]\u2013[10].", "startOffset": 234, "endOffset": 238}, {"referenceID": 10, "context": "Another popular technique for multi-talker speech separation is Non-negative Matrix Factorization (NMF) [11]\u2013[14].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "Another popular technique for multi-talker speech separation is Non-negative Matrix Factorization (NMF) [11]\u2013[14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 3, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 191, "endOffset": 195}, {"referenceID": 15, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 196, "endOffset": 200}, {"referenceID": 16, "context": "For multi-talker speech separation, both CASA and NMF have led to limited success [4], [5] and the most successful techniques, before the deep learning era, are based on probabilistic models [15]\u2013[17], such as factorial GMM-HMM [18], that model the temporal dynamics and the complex interactions of the target and competing speech signals.", "startOffset": 228, "endOffset": 232}, {"referenceID": 17, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 142, "endOffset": 146}, {"referenceID": 22, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 176, "endOffset": 180}, {"referenceID": 31, "context": "More recently, a large number of techniques based on deep learning [19] have been proposed, especially for Automatic Speech Recognition (ASR) [20]\u2013[25], and speech enhancement [26]\u2013[34].", "startOffset": 181, "endOffset": 185}, {"referenceID": 27, "context": ", [30]), although successful work has, similarly to NMF and CASA, mainly", "startOffset": 2, "endOffset": 6}, {"referenceID": 32, "context": "To the authors knowledge only four deep learning based works [35]\u2013[38] exist, that have tried to address and solve the harder speaker independent multi-talker speech separation task.", "startOffset": 61, "endOffset": 65}, {"referenceID": 35, "context": "To the authors knowledge only four deep learning based works [35]\u2013[38] exist, that have tried to address and solve the harder speaker independent multi-talker speech separation task.", "startOffset": 66, "endOffset": 70}, {"referenceID": 32, "context": "[35], which proposed the best performing system in the 2006 monaural speech separation and recognition challenge [4], the instantaneous energy was used to ar X iv :1 70 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[35], which proposed the best performing system in the 2006 monaural speech separation and recognition challenge [4], the instantaneous energy was used to ar X iv :1 70 3.", "startOffset": 113, "endOffset": 116}, {"referenceID": 33, "context": "[36] have made significant progress with their Deep Clustering (DPCL) technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "To further improve the model [39], another RNN is stacked on top of the first DPCL RNN to estimate continuous masks for each target speaker.", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "[37], [40] proposed a related technique called Deep Attractor Network (DANet).", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[37], [40] proposed a related technique called Deep Attractor Network (DANet).", "startOffset": 6, "endOffset": 10}, {"referenceID": 35, "context": "Recently, we proposed the Permutation Invariant Training (PIT) technique1 [38] for attacking the speaker independent multi-talker speech separation problem and showed that PIT effectively solves the label permutation problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "Specifically, uPIT extends the frame-level PIT technique [38] with an utterance-level training criterion that effectively eliminates the need for additional speaker tracing or very large input/output contexts, which is otherwise required by the original PIT [38].", "startOffset": 57, "endOffset": 61}, {"referenceID": 35, "context": "Specifically, uPIT extends the frame-level PIT technique [38] with an utterance-level training criterion that effectively eliminates the need for additional speaker tracing or very large input/output contexts, which is otherwise required by the original PIT [38].", "startOffset": 258, "endOffset": 262}, {"referenceID": 38, "context": "We achieve this using deep Long Short-Term Memory (LSTM) RNNs [41] that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream.", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "1In [36], a related permutation free technique, which is similar to PIT for exactly two-speakers, was evaluated with negative results and conclusion.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "This is because phase estimation is still an open problem in the speech separation setup [42], [43].", "startOffset": 89, "endOffset": 93}, {"referenceID": 40, "context": "This is because phase estimation is still an open problem in the speech separation setup [42], [43].", "startOffset": 95, "endOffset": 99}, {"referenceID": 24, "context": ", [27], [43]), that better results can be achieved if, instead of estimating Z directly, we first estimate a set of masks Ms(t, f), s = 1, .", "startOffset": 2, "endOffset": 6}, {"referenceID": 40, "context": ", [27], [43]), that better results can be achieved if, instead of estimating Z directly, we first estimate a set of masks Ms(t, f), s = 1, .", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "The Ideal Ratio Mask (IRM) [27] for each source is defined as", "startOffset": 27, "endOffset": 31}, {"referenceID": 41, "context": "When the phase of Y is used for reconstruction, the IRM achieves the highest Signal to Distortion Ratio (SDR) [44], when all sources have the same phase, (which is an invalid assumption in general).", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Nevertheless, we report IRM results as an upper performance bound since the IRM is a commonly used training target for deep learning based monaural speech separation [31], [32].", "startOffset": 166, "endOffset": 170}, {"referenceID": 29, "context": "Nevertheless, we report IRM results as an upper performance bound since the IRM is a commonly used training target for deep learning based monaural speech separation [31], [32].", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "Another applicable mask is the Ideal Amplitude Mask (IAM) (known as FFT-mask in [27]), or simply Amplitude Mask (AM), when estimated by a deep learning model.", "startOffset": 80, "endOffset": 84}, {"referenceID": 40, "context": "The Ideal Phase Sensitive Mask (IPSM) [43], [45]", "startOffset": 38, "endOffset": 42}, {"referenceID": 42, "context": "The Ideal Phase Sensitive Mask (IPSM) [43], [45]", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "To overcome these limitations, recent works [27] directly minimize the MSE", "startOffset": 44, "endOffset": 48}, {"referenceID": 40, "context": "(11) is used as a cost function, the IPSM is the upper bound achievable on the task [43].", "startOffset": 84, "endOffset": 88}, {"referenceID": 27, "context": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig.", "startOffset": 121, "endOffset": 125}, {"referenceID": 32, "context": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig.", "startOffset": 127, "endOffset": 131}, {"referenceID": 43, "context": "A natural, and commonly used, approach for deep learning based speech separation is to cast the problem as a multi-class [30], [35], [46] regression problem as depicted in Fig.", "startOffset": 133, "endOffset": 137}, {"referenceID": 32, "context": "This problem is referred to as the label permutation (or ambiguity) problem in [35], [36].", "startOffset": 79, "endOffset": 83}, {"referenceID": 33, "context": "This problem is referred to as the label permutation (or ambiguity) problem in [35], [36].", "startOffset": 85, "endOffset": 89}, {"referenceID": 35, "context": "2 and is referred to as Permutation Invariant Training (PIT) [38].", "startOffset": 61, "endOffset": 65}, {"referenceID": 35, "context": "However, this usually leads to unsatisfactory results as reported in [38].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "For example, in CASA a related problem referred to as the Sequential Organization Problem has been addressed using a model-based sequential grouping algorithm [9].", "startOffset": 159, "endOffset": 162}, {"referenceID": 44, "context": "We evaluated uPIT on various setups and all models were implemented using the Microsoft Cognitive Toolkit (CNTK) [47], [48]2.", "startOffset": 113, "endOffset": 117}, {"referenceID": 45, "context": "We evaluated uPIT on various setups and all models were implemented using the Microsoft Cognitive Toolkit (CNTK) [47], [48]2.", "startOffset": 119, "endOffset": 123}, {"referenceID": 41, "context": "The models were evaluated on their potential to improve the Signal-to-Distortion Ratio (SDR) [44] and the Perceptual Evaluation of Speech Quality (PESQ) [49] score, both of which are metrics widely used to evaluate speech enhancement performance for multi-talker speech separation tasks.", "startOffset": 93, "endOffset": 97}, {"referenceID": 46, "context": "The models were evaluated on their potential to improve the Signal-to-Distortion Ratio (SDR) [44] and the Perceptual Evaluation of Speech Quality (PESQ) [49] score, both of which are metrics widely used to evaluate speech enhancement performance for multi-talker speech separation tasks.", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "The WSJ0-2mix dataset was introduced in [36] and was derived from the WSJ0 corpus [50].", "startOffset": 40, "endOffset": 44}, {"referenceID": 47, "context": "The WSJ0-2mix dataset was introduced in [36] and was derived from the WSJ0 corpus [50].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "In our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39].", "startOffset": 152, "endOffset": 156}, {"referenceID": 35, "context": "In our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39].", "startOffset": 158, "endOffset": 162}, {"referenceID": 36, "context": "In our study, the validation set is used to find initial hyperparameters and to evaluate closed-condition (CC) (seen speaker) performance, similarly to [36], [38], [39].", "startOffset": 164, "endOffset": 168}, {"referenceID": 35, "context": "We first evaluated the original frame-level PIT on the twotalker separation dataset WSJ0-2mix, and differently from [38], we fixed the input dimension to 51 frames, to isolate the effect of a varying output dimension.", "startOffset": 116, "endOffset": 120}, {"referenceID": 33, "context": "The WSJ0-2mix dataset, used in [36], was designed such that speaker one was always assigned the most energy, and consequently speaker two the lowest, when scaling to a given SNR.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "Previous work [35] has shown that such speaker energy patterns are an effective discriminative feature, which is clearly seen in Fig.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "3, where the CONV-DNN model achieves considerably lower training and validation MSE than the CONVDNN-RAND model, which hardly decreases in either training or validation MSE due to the label permutation problem [35], [36].", "startOffset": 210, "endOffset": 214}, {"referenceID": 33, "context": "3, where the CONV-DNN model achieves considerably lower training and validation MSE than the CONVDNN-RAND model, which hardly decreases in either training or validation MSE due to the label permutation problem [35], [36].", "startOffset": 216, "endOffset": 220}, {"referenceID": 36, "context": "Note that, since we used Nvidia\u2019s cuDNN implementation of LSTMs, to speed up training, we were unable to apply dropout across time steps, which was adopted by the best DPCL model [39] and is known to be more effective, both theoretically and empirically, than the simple dropout strategy used in this work [51].", "startOffset": 179, "endOffset": 183}, {"referenceID": 48, "context": "Note that, since we used Nvidia\u2019s cuDNN implementation of LSTMs, to speed up training, we were unable to apply dropout across time steps, which was adopted by the best DPCL model [39] and is known to be more effective, both theoretically and empirically, than the simple dropout strategy used in this work [51].", "startOffset": 306, "endOffset": 310}, {"referenceID": 36, "context": "3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54].", "startOffset": 152, "endOffset": 156}, {"referenceID": 49, "context": "3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54].", "startOffset": 158, "endOffset": 162}, {"referenceID": 51, "context": "3) Two-stage Models and Reduced Dropout Rate: It is well known that cascading DNNs can improve performance for certain deep learning based applications [39], [52]\u2013[54].", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "These results agree with breakdowns from other works [36], [39] and generally indicate that same-gender mixed speech separation is a harder task.", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "These results agree with breakdowns from other works [36], [39] and generally indicate that same-gender mixed speech separation is a harder task.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Furthermore, we note that the PESQ improvements are similar to what have been reported for DNN based speech enhancement systems [32].", "startOffset": 128, "endOffset": 132}, {"referenceID": 33, "context": "Oracle NMF [36] - - 5.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "1 CASA [36] - - 2.", "startOffset": 7, "endOffset": 11}, {"referenceID": 33, "context": "1 DPCL [36] - - 5.", "startOffset": 7, "endOffset": 11}, {"referenceID": 34, "context": "8 DPCL+ [37] - - - 9.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "1 DANet [37] - - - 9.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "6 DANet\u2021 [37] - - - 10.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "5 DPCL++ [39] - - - 9.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "4 DPCL++\u2021 [39] - - - 10.", "startOffset": 10, "endOffset": 14}, {"referenceID": 33, "context": "From the table we can observe that the models trained with PIT already achieve similar or better SDR than the original DPCL [36], respectively, with DNNs and CNNs.", "startOffset": 124, "endOffset": 128}, {"referenceID": 34, "context": "Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51].", "startOffset": 136, "endOffset": 140}, {"referenceID": 36, "context": "Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51].", "startOffset": 142, "endOffset": 146}, {"referenceID": 48, "context": "Using the uPIT training criteria, we improve on PIT and achieve comparable performance with DPCL+, DPCL++ and DANet models5 reported in [37], [39], which used curriculum training [55], and recurrent dropout [51].", "startOffset": 207, "endOffset": 211}, {"referenceID": 34, "context": "5 [37], [39] did not use the SDR measure from [44].", "startOffset": 2, "endOffset": 6}, {"referenceID": 36, "context": "5 [37], [39] did not use the SDR measure from [44].", "startOffset": 8, "endOffset": 12}, {"referenceID": 41, "context": "5 [37], [39] did not use the SDR measure from [44].", "startOffset": 46, "endOffset": 50}, {"referenceID": 33, "context": "Oracle NMF [36] 4.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "5 - - DPCL++\u2021 [39] - - - 7.", "startOffset": 14, "endOffset": 18}, {"referenceID": 37, "context": "1 DANet [40] - - - 7.", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "7 DANet\u2021 [37] - - - 8.", "startOffset": 9, "endOffset": 13}, {"referenceID": 36, "context": "This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].", "startOffset": 169, "endOffset": 173}, {"referenceID": 34, "context": "This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].", "startOffset": 184, "endOffset": 188}, {"referenceID": 37, "context": "This is of great practical importance, since a priori knowledge about the number of speakers is not needed at test time, as required by competing methods such as DPCL++ [39] and DANet [37], [40].", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 85, "endOffset": 89}, {"referenceID": 36, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 91, "endOffset": 95}, {"referenceID": 34, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 121, "endOffset": 125}, {"referenceID": 37, "context": "The proposed uPIT technique is algorithmically simpler yet performs on par with DPCL [36], [39] and comparable to DANets [37], [40], both of which involve separate embedding and clustering stages during inference.", "startOffset": 127, "endOffset": 131}], "year": 2017, "abstractText": "In this paper we propose the utterance-level Permutation Invariant Training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep learning based solution for speaker independent multi-talker speech separation. Specifically, uPIT extends the recently proposed Permutation Invariant Training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using Recurrent Neural Networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multi-talker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity or gender. We evaluated uPIT on the WSJ0 and Danish twoand three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on Non-negative Matrix Factorization (NMF) and Computational Auditory Scene Analysis (CASA), and compares favorably with Deep Clustering (DPCL) and the Deep Attractor Network (DANet). Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "creator": "LaTeX with hyperref package"}}}