{"id": "1703.06683", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2017", "title": "A Systematic Study of Online Class Imbalance Learning with Concept Drift", "abstract": "targeting an emerging statistical dimension, online class imbalance learning often combines the challenges around preserving underlying variability among property drift. it mixes with data streams featuring disrupted normal class distributions, where hybrid drift may occur. mathematics has accordingly encountered increased research attention ; though, very little work addresses the combined problem where both trait imbalance and concept similarity coexist. utilizing such first feasibility study describing vertical concept drift incorporates class - imbalanced data exchanges, this paper first requires a comprehensive review of current research projects in wide field, including advanced teaching funding besides textbook reviewers. then, an in - store adaptive study is recommended, with the goal of understanding how learners best overcome concept drift in online learning pricing pricing vs. based on the analysis, simple coherent transformation is proposed involving the development of an equivalent algorithm.", "histories": [["v1", "Mon, 20 Mar 2017 11:22:10 GMT  (1437kb,D)", "http://arxiv.org/abs/1703.06683v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shuo wang", "leandro l minku", "xin yao"], "accepted": false, "id": "1703.06683"}, "pdf": {"name": "1703.06683.pdf", "metadata": {"source": "CRF", "title": "A Systematic Study of Online Class Imbalance Learning with Concept Drift", "authors": ["Shuo Wang", "Leandro L. Minku", "Xin Yao"], "emails": ["X.Yao}@cs.bham.ac.uk.", "leandro.minku@leicester.ac.uk."], "sections": [{"heading": null, "text": "Index Terms\u2014Online learning, class imbalance, concept drift, resampling.\nI. INTRODUCTION\nWith the wide application of machine learning algorithms to the real world, class imbalance and concept drift have become crucial learning issues. Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both class imbalance and concept drift. Class imbalance happens when the data categories are not equally represented, i.e., at least one category is minority compared to other categories [5]. It can cause learning bias towards the majority class and poor generalization. Concept drift is a change in the underlying distribution of the problem, and is a significant issue specially when learning from data streams [6]. It requires learners to be adaptive to dynamic changes.\nClass imbalance and concept drift can significantly hinder predictive performance, and the problem becomes particularly challenging when they occur simultaneously. This challenge arises from the fact that one problem can affect the treatment of the other. For example, drift detection algorithms based on the traditional classification error may be sensitive to the imbalanced degree and become less effective; and class imbalance techniques need to be adaptive to changing imbalance rates, otherwise the class receiving the preferential treatment may not be the correct minority class at the current moment.\nAlthough there have been papers studying data streams with an imbalanced distribution and data streams with concept drift\nS. Wang and X. Yao are with the Centre of Excellence for Research in Computational Intelligence and Applications (CERCIA), School of Computer Science, The University of Birmingham, Edgbaston, Birmingham B15 2TT, UK. E-mail: {S.Wang, X.Yao}@cs.bham.ac.uk.\nL. L. Minku is with Department of Informatics, University of Leicester, Leicester LE1 7RH, UK. E-mail: leandro.minku@leicester.ac.uk.\nrespectively, very little work discusses the cases when both class imbalance and concept drift exist. This paper aims to provide a systematic study of handling concept drift in classimbalanced data streams. We focus on online (i.e. one-byone) learning, which is a more difficult case than chunk-based learning, because only a single instance is available at a time.\nWe first give a comprehensive review of current research progress in this field, including problem definitions, problem and approach categorization, performance evaluation and upto-date approaches. It reveals new challenges and research gaps. Most existing work focuses on the concept drift in posterior probabilities (i.e. real concept drift [7], changes in P (y | x)). The challenges in other types of concept drift have not been fully discussed and addressed. Especially, the change in prior probabilities P (y) is closely related to class imbalance and has been overlooked by most existing work. Most proposed concept drift detection approaches are designed for and tested on balanced data streams. Very few approaches aim to tackle class imbalance and concept drift simultaneously. Among limited solutions, it is still unclear which approach is better and when. It is also unknown whether and how applying class imbalance techniques (e.g. resampling methods) affects concept drift detection and online prediction.\nTo fill in the research gaps, we then provide an experimental insight into how to best overcome concept drift in online learning with class imbalance, by focusing on three research questions: 1) what are the challenges in detecting each type of concept drift when the data stream is imbalanced? 2) Among the proposed methods designed for online class imbalance learning with concept drift, which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.g. resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.e. changes in prior probability P (y), class-conditional probability density function (pdf) p (x | y) and posterior probability P (y | x)) in artificial data streams, as well as real-world data sets. To the best of our knowledge, they are the very few methods that are explicitly designed for online learning problems with class imbalance and concept drift so far.\nFinally, based on the review and experimental results, we provide some guidelines for developing an effective algorithm for learning from imbalanced data streams with concept drift. We stress the importance of studying the mutual effect of class imbalance and concept drift.\nar X\niv :1\n70 3.\n06 68\n3v 1\n[ cs\n.L G\n] 2\n0 M\nar 2\n01 7\nThe contributions of this paper include: this is the first comprehensive study that looks into concept drift detection in class-imbalanced data streams; data problems are categorized in different types of concept drift and class imbalance with illustrative applications; existing approaches are compared and analysed systematically in each type; pros and cons of each approach are investigated; the results provide guidance for choosing the appropriate technique and developing better algorithms for future learning tasks; this is also the first work exploring the role of class imbalance techniques in concept drift detection, which sheds light on whether and how to tackle class imbalance and concept drift simultaneously.\nThe rest of this paper is organized as follows. Section II formulate the learning problem, including a learning framework and detailed problem descriptions and introduction of class imbalance and concept drift individually. Section III reviews the combined issue of class imbalance and concept drift, including example applications and existing solutions. Section IV carries out the experimental study, aiming to find out the answers to the three research questions. Section V draws the conclusions and points out potential future directions."}, {"heading": "II. ONLINE LEARNING FRAMEWORK WITH CLASS IMBALANCE AND CONCEPT DRIFT", "text": "In data stream applications, data arrives over time in streams of examples or batches of examples. The information up to a specific time step t is used to build/update predictive models, which then predict the new example(s) arriving at time step t+1. Learning under such conditions needs chunkbased learning or online learning algorithms, depending on the number of training examples available at each time step. According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at every second in engineering systems. The term \u201cincremental learning\u201d is also frequently used under this scenario. It is usually referred to as any algorithm that can process data streams with certain criteria met [16].\nOn one hand, online learning can be viewed as a special case of chunk-based learning. Online learning algorithms can be used to deal with data coming in batches. They both build and continuously update a learning model to accommodate newly available data, and simultaneously maintain its performance on old data, giving rise to the stability-plasticity dilemma [17]. On the other hand, the way of designing online and chunkbased learning algorithms can be very different [6]. Most chunk-based learning algorithms are not suitable for online learning tasks, because batch learners process a chunk of data each time, possibly using an offline learning algorithm for each chunk. Online learning requires the model being adapted immediately upon seeing the new example, and the example is then immediately discarded, which allows to process highspeed data streams. From this point of view, designing online learning algorithm can be more challenging but so far has received much less attention than the other.\nFirst, the online learner needs to learn from a single data example, so it needs a more sophisticated training mechanism. Second, data streams are often non-stationary (concept drift). The limited availability of training examples at the current moment in online learning hinders the detection of such changes and the application of techniques to overcome the change. Third, it is often seen that data is class imbalanced in many classification tasks, such as the fault detection task in an engineering system, where the fault is always the minority. Class imbalance aggravates the learning difficulty [5] and complicates the data status [18]. However, there is a severe lack of research addressing the combined issue of class imbalance and concept drift in online learning.\nTo fill in this research gap, this paper aims at a comprehensive review of the work done to overcome class imbalance and concept drift, a systematic study of learning challenges, and an in-depth analysis of the performance of current approaches. We begin by formalizing the learning problem in this section."}, {"heading": "A. Learning Procedure", "text": "In supervised online classification, suppose a data generating process provides a sequence of examples (xt, yt) arriving one at a time from an unknown probability distribution pt (x, y). xt is the input vector belonging to an input space X , and yt is the corresponding class label belonging to the label set Y = {c1, . . . , cN}. We build an online classifier F that receives the new input xt at time step t and then makes a prediction. The predicted class label is denoted by y\u0302t. After some time, the classifier receives the true label yt, used to evaluate the predictive performance and further train the classifier. This whole process will be repeated at following time steps. It is worth pointing out that we do not assume new training examples always arrive at regular and pre-defined intervals here. In other words, the actual time interval between time step t and t + 1 may be different from the actual time interval between t+ 1 and t+ 2.\nOne challenge arises when data is class imbalanced. Class imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3]. It is the phenomenon when some classes of data are highly under-represented (i.e. minority) compared to other classes (i.e. majority). For example, if P (ci) P (cj), then cj is a majority class and ci is a minority class. The difficulty in learning from imbalanced data is that the relatively or absolutely underrepresented class cannot draw equal attention to the learning algorithm, which often leads to very specific classification rules or missing rules for this class without much generalization ability for future prediction. It has been wellstudied in offline learning [20], and has attracted growing attention in data stream learning in recent years [21].\nIn many applications, such as energy forecasting and climate data analysis [22], the data generator operates in nonstationary environments. It gives rise to another challenge, called \u201cconcept drift\u201d. It means that the probability density function (pdf) of the data generating process is changing over time. For such cases, the fundamental assumption of traditional data mining \u2013 the training and testing data are sampled from the same static\nand unknown distribution \u2013 does not hold anymore. Therefore, it is crucial to monitor the underlying changes, and adapt the model to accommodate the changes accordingly.\nWhen both issues exist, the online learner needs to be carefully designed for effectiveness, efficiency and adaptivity. An online class imbalance learning framework was proposed in [18] as a guide for algorithm design. The framework breaks down the learning procedure into three modules \u2013 a class imbalance detector, a concept drift detector and an adaptive online learner, as illustrated in Fig. 1.\nThe class imbalance detector reports the current class imbalance status of data streams. The concept drift detector captures concept drifts involving changes in classification boundaries. Based on the information provided by the first two modules, the adaptive online learner determines when and how to respond to the detected class imbalance and concept drift, in order to maintain its performance. The learning objective of an online class imbalance algorithm can be described as \u201crecognizing minority-class data effectively, adaptively and timely without sacrificing the performance on the majority class\u201d [18]."}, {"heading": "B. Problem Descriptions", "text": "A more detailed introduction about class imbalance and concept drift is given here individually, including the terminology, research focuses and state-of-the-art approaches. The purpose of this section is to understand the fundamental issues that we need to take extra care of in online class imbalance learning. We also aim at understanding whether and how the current research in class imbalance learning and concept drift detection are individually related to their combined issue elaborated later in Section III, rather than to provide an exhaustive list of approaches in the literature. Among others, we will answer the following questions: can existing class\nimbalance techniques process data streams? Would existing concept drift detectors be able to handle imbalanced data streams?\n1) Class imbalance: In class imbalance problems, the minority class is usually much more difficult or expensive to be collected than the majority class, such as the spam class in spam filtering and the fraud class in credit card application. Thus, misclassifying a minority-class example is more costly. Unfortunately, the performance of most conventional machine learning algorithms is significantly compromised by class imbalance, because they assume or expect balanced class distributions or equal misclassification costs. Their training procedure with the aim of maximizing overall accuracy often leads to a high probability of the induced classifier predicting an example as the majority class, and a low recognition rate on the minority class. In reality, it is common to see that the majority class has accuracy close to 100% and the minority class has very low accuracy between 0%-10% [23]. The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied. A classifier that provides a balanced degree of predictive performance for all classes is required. The major research questions in this area are summarized and answered as follows:\n(a) How do we define the imbalanced degree of data? It seems to be a trivial question. However, there is no consensus on the definition in the literature. To describe how imbalanced the data is, researchers choose to use the percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32]. The coefficient of variance is used in [33], which is less straightforward. The description of imbalance status may not be a crucial issue in offline learning, but becomes more important in online learning, because there is no static data set in online scenarios. It is necessary to have some measurement automatically describing the up-todate imbalanced degree and techniques monitoring the changes in class imbalance status. This will help the online learner to decide when and how to tackle class imbalance. The issue of changes in class imbalance status is relevant to concept drift, which will be further discussed in the next subsection.\nTo define the imbalanced degree suitable for online learning, a real-time indicator was proposed \u2013 time-decayed class size [18], expressing the size percentage of each class in the data stream. It is updated incrementally at each time step by using a time decay (forgetting) factor, which emphasizes the current status of data and weakens the effect of old data. Based on this, a class imbalance detector was proposed to determine which classes should be regarded as the minority/majority and how imbalanced the current data stream is, and then used for designing better online classifiers [11] [3]. The merit of this indicator is that it is suitable for data with arbitrary number of classes. (b) When does class imbalance matter?\nIt has been shown that class imbalance is not the only problem responsible for the performance reduction of classifiers. Classifiers\u2019 sensitivity to class imbalance also depends\non the complexity and overall size of the data set. Data complexity comprises issues such as overlapping [34] [35] and small disjuncts [36]. The degree of overlapping between classes and how the minority class examples distribute in data space aggravate the negative effect of class imbalance. The small disjunct problem is associated with the within-class imbalance [37]. Regarding the size of the training data, a very large domain has a good chance that the minority class is represented by a reasonable number of examples, and thus may be less affected by imbalance than a small domain containing very few minority class examples. In other words, the rarity of the minority class can be in a relative or absolute sense in terms of the number of available examples [5].\nIn particular, authors in [38] [39] distinguished and analysed four types of data distributions in the minority class \u2013 safe, borderline, outliers and rare examples. Safe examples are located in the homogenous regions populated by the examples from one class only; borderline examples are scattered in the boundary regions between classes, where the examples from both classes overlap; rare examples and outliers are singular examples located deeper in the regions dominated by the majority class. Borderline, rare and outlier data sets were found to be the real source of difficulties in learning imbalanced data sets offline, which have also been shown to be the harder cases in online applications [11]. Therefore, for any developed algorithms dealing with imbalanced data online, it is worth discussing their performance on data with different types of distributions. (c) How can we tackle class imbalance effectively (state-ofthe-art solutions)?\nA number of algorithms have been proposed to tackle class imbalance at the data and algorithm levels. Data-level algorithms include a variety of resampling techniques, manipulating training data to rectify the skewed class distributions. They oversample minority-class examples (i.e. expanding the minority class), undersample majority-class examples (i.e. shrinking the majority class), or combine both, until the data set is relatively balanced. Random oversampling and random undersampling are the simplest and most popular resampling techniques, where examples are randomly chosen to be added or removed. There are also smart resampling techniques (a.k.a guided resampling). For example, SMOTE [32] is a widely used oversampling method, which generates new minorityclass data points based on the similarities between original minority-class examples in the feature space. Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few. Smart undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc. The effectiveness of resampling techniques have been proved in real-world applications [46]. They work independently of classifiers, and are thus more versatile than algorithmlevel methods. The key is to choose an appropriate sampling rate [47], which is relatively easy for two-class data sets, but becomes more complicated for multi-class data sets [48]. Empirical studies have been carried out to compare different resampling methods [30]. Particularly, it is shown that smart resampling techniques are not necessarily superior to random\noversampling and undersampling; besides, they cannot be applied to online scenarios directly, because they work on a static data set for the relation among the training examples. Some initial effort has been made recently, to extend smart resampling techniques to online learning [49].\nAlgorithm-level methods address class imbalance by modifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52]. They require different treatments for specific kinds of learning algorithms. In other words, they are algorithmdependent, so they are not as widely used as data-level methods. Some online cost-sensitive methods have been proposed, such as CSOGD [53] and RLSACP [12]. They are restricted to the perceptron-based classifiers, and require pre-defined misclassification costs of classes that may or may not be updated during the online learning.\nFinally, ensemble learning (also known as multiple classifier systems) [54] has become a major category of approaches to handling class imbalance [55]. It combines multiple classifiers as base learners and aims to outperform every one of them. It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63]. A few ensemble methods are available for online class imbalance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.\nIt is worth pointing out that, the aforementioned online learning algorithms designed for imbalanced data are not suitable for non-stationary data streams. They do not involve any mechanism handling drifts that affect classification boundaries, although OOB and UOB can detect and react to class imbalance changes. (d) How do we evaluate the performance of class imbalance learning algorithms?\nTraditionally, overall accuracy and error rate are the most frequently used metrics of performance evaluation. However, they are strongly biased towards the majority class when data is imbalanced. Therefore, other performance measures have been adopted. Most studies concentrate on two-class problems. By convention, the minority class is treated to be the positive, and the majority class is treated to be the negative. Table I illustrates the confusion matrix of a two-class problem, producing four numbers on testing data.\nFrom the confusion matrix, we can derive the expressions for recall and precision:\nrecall = TP\nTP + FN , (1)\nprecision = TP\nTP + FP . (2)\nRecall (i.e. TP rate) is a measure of completeness \u2013 the proportion of positive class examples that are classified correctly to all positive class examples. Precision is a measure of exactness \u2013 the proportion of positive class examples that are classified correctly to the examples predicted as positive by the classifier. The learning objective of class imbalance learning is to improve recall without hurting precision. However, improving recall and precision can be conflicting. Thus, F-measure is defined to show the trade-off between them.\nFm =\n( 1 + \u03b22 ) \u00b7 recall \u00b7 precision\n\u03b22 \u00b7 precision+ recall , (3)\nwhere \u03b2 corresponds to the relative importance of recall and precision. It is usually set to 1. Kubat et al. [44] proposed to use G-mean to replace overall accuracy:\nGm =\n\u221a TP\nTP + FN \u00d7 TN TN + FP . (4)\nIt is the geometric mean of positive accuracy (i.e. TP rate) and negative accuracy (i.e. TN rate). A good classifier should have high accuracies on both classes, and thus a high G-mean.\nAccording to [5], any metric that uses values from both rows of the confusion matrix for addition (or subtraction) will be inherently sensitive to class imbalance. In other words, the performance measure will change as class distribution changes, even though the underlying performance of the classifier does not. This performance inconsistency can cause problems when we compare different algorithms over different data sets. Precision and F-measure, unfortunately, are sensitive to the class distribution. Therefore, recall and G-mean are better options.\nTo compare classifiers over a range of sample distributions, AUC (abbr. of the Area Under the ROC curve) is the best choice. A ROC curve depicts all possible trade-offs between TP rate and FP rate, where FP rate = FP/ (TN + FP ). TP rate and FP rate can be understood as the benefits and costs of classification with respect to data distributions. Each point on the curve corresponds to a single trade-off. A better classifier should produce a ROC curve closer to the top left corner. AUC represents a ROC curve as a single scalar value by estimating the area under the curve, varying in [0, 1]. It is insensitive to the class distribution, because both TP rate and FP rate use values from only one row of the confusion matrix. AUC is usually generated by varying the classification decision threshold for separating positive and negative classes in the testing data set [66] [67]. In other words, calculating AUC requires a set of confusion matrices. Therefore, unlike other measures based on a single confusion matrix, AUC cannot be used as an evaluation metric in online learning without memorizing data. Although a recent study has modified AUC for evaluating online classifiers [10], it still needs to collect recently received examples.\nThe properties of the above measures are summarized in Table II. They are defined under the two-class context. They cannot be used to evaluate multi-class data directly, except for recall. Their multi-class versions have been developed [68] [69] [70]. The \u201cmulti-class\u201d and \u201conline\u201d columns\nin the table show whether the corresponding measure can be used directly without modification in multi-class and online data scenarios.\n2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72]. The key research topics in this area include:\n(a) How many types of concept drift are there? Which type is more challenging?\nConcept drift can manifest three fundamental forms of changes corresponding to the three major variables in the Bayes\u2019 theorem [73]: 1) a change in prior probability P (y); 2) a change in class-conditional pdf p (x | y); 3) a change in posterior probability P (y | x). The three types of concept drift are illustrated in Figure 2. Comparing to the original data distribution shown in Figure 2(a),\nFig. 2(b) shows the P (y) type of concept drift without affecting p (x | y) and P (y | x). The decision boundary remains unaffected. The prior probability of the circle class is reduced in this example. Such change can lead to class imbalance. A well-learnt discrimination function may drift away from the true decision boundary, due to the imbalanced class distribution.\nFig. 2(c) shows the p (x | y) type of concept drift without affecting P (y) and P (y | x). The true decision boundary remains unaffected. Elwell and Polikar claimed that this type of drift is the result of an incomplete representation of the true distribution in current data, which simply requires providing supplemental data information to the learning model [74].\nFig. 2(d) shows the P (y | x) type of concept drift. The true boundary between classes changes after the drift, so that the previously learnt discrimination function does not apply any more. In other words, the old function becomes unsuitable or partially unsuitable, and the learning model needs to be adapted to the new knowledge.\nThe posterior distribution change clearly indicates the most fundamental change in the data generating function. This is classified as real concept drift. The other two types belong to virtual concept drift [21], which does not change the decision (class) boundaries. In practice, one type of concept drift may appear in combination with other types.\nExisting studies primarily focus on the development of drift detection methods and techniques to overcome the real drift. There is a significant lack of research on virtual drift, which can also deteriorate classification performance. As illustrated in Fig. 2(b), even though these types of drift do not affect the true decision boundaries, they can cause a well-learnt decision boundary to become unsuitable. Unfortunately, the current techniques for handling real drift may not be suitable for virtual drift, because they present very different learning difficulties and require different solutions. For instance, the methods for handling real drift often choose to reset and retrain the classifier, in order to forget the old concept and better learn the new concept. This is not an appropriate strategy for data with virtual drift, because the examples from previous time steps may still remain valid and help the current classification in virtual drift cases. It would be more effective and efficient to calibrate the existing classifier than retraining it. Besides, techniques for handling real drift typically rely on feedback about the performance of the classifier, while techniques for handling virtual drift can operate without such feedback [7]. From our point of view, all three types are equally important. Particularly, the two virtual types require more research effort than currently dedicated work by our community. A systematic study of the challenges in each type will be given in Section IV.\nConcept drift has further been characterized by its speed, severity, cyclical nature, etc. A detailed and mutually exclusive categorization can be found in [72]. For example, according to speed, concept drift can be either abrupt, when the generating function is changed suddenly (usually within one time step), or gradual, when the distribution evolves slowly over time. They are the most commonly discussed types in the literature, because the effectiveness of drift detection methods can vary with the drifting speed. While most methods are quite successful in detecting abrupt drifts, as future data is no longer related to old data [75], gradual drifts are often more difficult, because the slow change can delay or hide the hint left by the drift. We can see some drift detection methods specifically designed for gradual concept drift, such as Early Drift Detection method (EDDM) [76].\n(b) How can we tackle concept drift effectively (state-of-the-art solutions)?\nThere is a wide range of algorithms for learning in nonstationary environments. Most of them assume and specialize in some specific types of concept drift, although real-world data often contains multiple types. They are commonly categorized into two major groups: active vs. passive approaches, depending on whether an explicit drift detection mechanism is employed. Active approaches (also known as trigger-based approaches) determine whether and when a drift has occurred before taking any actions. They operate based on two mechanisms \u2013 a change detector aiming to sense the drift accurately and timely, and an adaptation mechanism aiming to maintain the performance of the classifier by reacting to the detected drift. Passive approaches (also known as adaptive classifiers) evolve the classifier continuously without an explicit trigger reporting the drift. A comprehensive review of up-to-date techniques tackling concept drift is given by Ditzler et al. [14]. They further organise these techniques based on their core mechanisms, summarized in Table III. This table will help us to understand how online class imbalance algorithms are designed, which will be introduced in details in Section III. There exist other ways to classify the proposed algorithms, such as Gama et al.\u2019s taxonomy based on the four modules of an adaptive learning system [7], and Webb et al.\u2019s quantitative characterization [77]. This paper adopts the one proposed by Ditzler et al. [14] for its simplicity.\nThe best algorithm varies with the intended applications. A general observation is that, while active approaches are quite effective in detecting abrupt drift, passive approaches are very good at overcoming gradual drift [74] [14]. It is worth noting that most algorithms do not consider class imbalance. It is unclear whether they will remain effective if data becomes imbalanced. For example, some algorithms determine concept drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80]. As we have explained in Section II-B 1), the classification error is sensitive to the imbalance degree of data, and does not reflect the performance of the classifier very well when there is class imbalance. Therefore, these algorithms may not perform well when concept drift and class imbalance occur simultaneously. Some other algorithms are specifically designed for data streams coming in batches, such as AUE [81] and the Learn++ family [74]. These algorithms cannot be applied to online cases directly. (c) How do we evaluate the performance of concept drift detectors and online classifiers?\nTo fully test the performance of drift detection approaches (especially an active detector), it is necessary to discuss both data with artificial concept drifts and real-world data with unknown drifts. Using data with artificial concept drifts allows us to easily manipulate the type and timing of concept drifts, so as to obtain an in-depth understanding of the performance of approaches under various conditions. Testing on data from real-world problems helps us to understand their effectiveness from the practical point of view, but the information about when and how concept drift occurs is unknown in most cases. The following aspects are usually considered to assess the accuracy of active drift detectors. Their measurement is based\non data with artificial concept drifts where drifts are known.\n\u2022 True detection rate: the possibility of detecting the true concept drift. It shows the accuracy of the detection approach. \u2022 False alarm rate: the possibility of reporting a concept drift that does not exist (false-positive rate). It characterizes the costs and reliability of the detection approach. \u2022 Delay of detection: an estimate of how many time steps are required on average to detect a drift after the actual occurrence. It reflects how much time would be taken before the drift is detected.\nWang and Abraham [9] use a histogram to visualize the distribution of detection points from the drift detection approach over multiple runs. It reflects all the three aspects above in one plot. It is worth nothing that there are tradeoffs between these measures. For example, an approach with a high true detection rate may produce a high false alarm rate. A very recent algorithm, Hierarchical Change-Detection Tests (HCDTs), was proposed to explicitly deal with the tradeoff [82].\nAfter the performance of drift detection approaches is better understood, we need to quantify the effect of those detections on the performance of predictive models. All the performance metrics introduced in the previous section of \u201cclass imbalance\u201d can be used. The key question here is how to calculate them in the streaming settings with evolving data. The performance of the classifier may get better or worse every now and then. There are two common ways to depict such performance over time \u2013 holdout and prequential evaluation [7].\nHoldout evaluation is mostly used when the testing data set (holdout set) is available in advance. At each time step or every few time steps, the performance measures are calculated based on the valid testing set, which must represent the same data concept as the training data at that moment. However, this is a very rigorous requirement for data from real-world applications.\nIn prequential evaluation, data received at each time step is used for testing before it is use for training. From this, the performance measures can be incrementally updated for evaluation and comparison. This strategy does not require a holdout set, and the model is always tested on unseen data.\nWhen the data stream is stationary, the prequential perfor-\nmance measures can be computed based on the accumulated sum of a loss function from the beginning of the training. However, if the data stream is evolving, the accumulated measure can mask the fluctuation in performance and the adaptation ability of the classifier. For example, consider that an online classifier correctly predicts 90 out of 100 examples received so far (90% accuracy on data with the original concept). Then, an abrupt concept drift occurs at time step 101, which makes the classifier only correctly predict 3 out of 10 examples from the new concept (30% accuracy on data with the new concept). If we use the accumulated measure based on all the historical data, the overall accuracy will be 93/110, which seems to be high but does not reflect the true performance on the new data concept. This problem can be solved by using a sliding window or a time-based fading factor that weigh observations [83]."}, {"heading": "III. OVERCOMING CLASS IMBALANCE AND CONCEPT DRIFT SIMULTANEOUSLY", "text": "Following the review of class imbalance and concept drift in Section II, this section reviews the combined issue, including example applications and existing solutions. When both exist, one problem affects the treatment of the other. For example, the drift detection algorithms based on the traditional classification error may be sensitive to imbalanced degree and become less effective; the class imbalance techniques need to be adaptive to changing P (y), otherwise the class receiving the preferential treatment may not be the correct minority class at the current moment. Therefore, their mutual effect should be considered during the algorithm design.\nA. Illustrative Applications\nThe combined problems of concept drift and class imbalance have been found in many real-world applications. Three examples are given here, to help us understand each type of concept drift.\n1) Environment monitoring with P (y) drift: Environment monitoring systems usually consist of various sensors generating streaming data in high speed. Real-time prediction is required. For example, a smart building has sensors deployed to monitor hazardous events. Any sensor fault can cause catastrophic failures. Machine learning algorithms can be used\nto build models based on the sensor information, aiming to predict faults in sensors accurately and timely [3]. First, the data is characterized by class imbalance, because obtaining a fault in such systems can be very expensive. Examples representing faults are the minority. Second, the number of faults varies with the faulty condition. If the damage gets worse over time, the faults will occur more and more frequently. It implies a prior probability change, a type of virtual concept drift.\n2) Spam filtering with p (x | y) drift: Spam filtering is a typical classification problem involving class imbalance and concept drift [84]. First of all, the spam class is the minority and suffers from a higher misclassification cost. Second, the spammers are actively working on how to break through the filter. It means that the adversary actions are adaptive. For example, one of the spamming behaviours is to change email content and presentation in disguise, implying a possible classconditional pdf (p (x | y)) change [7].\n3) Social media analysis with P (y | x) drift: Social media (e.g. twitter, facebook) is becoming a valuable source of timely information on the internet. It attracts a growing number of people, sharing, communicating, connecting and creating usergenerated data. Consider the example where a company would like to make relevant product recommendations to people who have shown some type of interest in their tweets. Machine learning algorithms can be used to discover who is interested in the product from the large amount of tweets [85]. The number of users who have shown the interest is always very small. Their information tends to be overwhelmed by other unrelated messages. Thus, it is utterly important to overcome the imbalanced distribution and discover the hidden information. Another challenge is users\u2019 interest changing from time to time. Users may lose their interest in the current trendy product very quickly, causing posterior probability (P (y | x)) changes.\nAlthough the above examples are associated with only one type of concept drift, different types often coexist in realworld problems, which are hard to know in advance. For the example of spam filtering, which email belongs to spam also depends on users\u2019 interpretation. Users may re-label a particular category of normal emails as spam, which indicates a posterior probability change."}, {"heading": "B. Approaches to Tackling Both Class Imbalance and Concept Drift", "text": "Some research efforts have been made to address the joint problem of concept drift and class imbalance, due to the rising need from practical problems [86] [1]. Uncorrelated Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89]. Selectively recursive approaches SERA [90] and REA [91] use similar ideas to Uncorrelated Bagging of building an ensemble of weighted classifiers, but with a \u201csmarter\u201d oversampling technique. Learn++.CDS and Learn++.NIE are more recent algorithms, which tackle class imbalance through the oversampling technique SMOTE [32]\nor a sub-ensemble technique, and overcome concept drift through a dynamic weighting strategy [92]. HUWRS.IP [93] improves HUWRS [94] to deal with imbalanced data streams by introducing an instance propagation scheme based on a Na\u0131\u0308ve Bayes classifier, and uses Hellinger distance as a weighting measure for concept drift detection. This method relies on finding examples that are similar to the current minority-class concept, which however may not exist. So, Hellinger Distance Decision Tree (HDDT) was proposed to use Hellinger distance as the decision tree splitting criteria that is imbalance-insensitive [95]. All these approaches belong to chunk-based learning algorithms. Their core techniques work when a batch of data is received at each time step, i.e. they are not suitable for online processing. Developing a true online algorithm for concept drift is very challenging because of the difficulties in measuring minority-class statistics using only one example at a time [14].\nTo handle class imbalance and concept drift in an online fashion, a few methods have been proposed recently. Drift Detection Method for Online Class Imbalance (DDM-OCI) [8] is one of the very first algorithms detecting concept drift actively in imbalanced data streams online. It monitors the reduction in minority-class recall (i.e. true positive rate). If there is a significant drop, a drift will be reported. It was shown to be effective in cases when minority-class recall is affected by the concept drift, but not when the majority class is mainly affected. A Linear Four Rates (LFR) approach was then proposed to improve DDM-OCI, which monitors four rates from the confusion matrix \u2013 minority-class recall and precision and majority-class recall and precision, with statistically-supported bounds for drift detection [9]. If any of the four rates exceeds the bound, a drift will be confirmed. Instead of tracking several performance rates for each class, prequential AUC (PAUC) [10] [96] was proposed as an overall performance measure for online scenarios, and was used as the concept drift indicator in Page-Hinkley (PH) test [97]. However, it needs access to historical data. DDM-OCI, LFR and PAUC-based PH test are active drift detectors designed for imbalanced data streams, and are independent of classification algorithms. They aim at concept drift with classification boundary changes by default. Therefore, if a concept drift is reported, they will reset and retrain the online model. Although these drift detectors are designed for imbalanced data, they themselves do not handle class imbalance. It is still unclear how they perform when working with class imbalance techniques.\nBesides the above active approaches, the perceptron-based algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance. RLSACP and ONN are single-model approaches with the same general idea. Their error function for updating the perceptron weights is modified, including a forgetting function for model adaptation and an error weighting strategy as the class imbalance treatment. The forgetting function has a predefined form, allowing the old data concept to be forgotten gradually. The error weights in RLSACP are incrementally updated based either on the classification performance or the imbalance rate from recently received data. It was shown that\nweight updating based on the imbalance rate leads to better performance.\nESOS-ELM is an ensemble approach, maintaining a set of online sequential extreme learning machines (OS-ELM) [99]. For tackling class imbalance, resampling is applied in a way that each OS-ELM is trained with approximately equal number of minority- and majority-class examples. For tackling concept drift, voting weights of base classifiers are updated according to their performance G-mean on a separate validation data set from the same environment as the current training data. In addition to the passive drift detection technique, ESOS-ELM includes an independent module \u2013 ELM-store, to handle recurring concept drift. ELM-store maintains a pool of weighted extreme learning machines (WELM) [65] to retain old information. It adopts a threshold-based technique and hypothesis testing to detect abrupt and gradual concept drift actively. If a concept drift is reported, a new WELM will be built and kept in ELM-store. If any stored model performs better than the current OS-ELM ensemble, indicating a possible recurring concept, it will be introduced in the ensemble. ESOS-ELM assumes the imbalance rate is known in advance and fixed. It needs a separate data set for initializing OS-ELMs and WELMs, which must include examples from all classes. It is also necessary to have validation data sets reflecting every data concept for concept drift detection, which can be a quite restrictive requirement for real-world data.\nWith a different goal of concept drift detection from the above, a class imbalance detection (CID) approach was proposed, aiming at P (y) changes [18]. It reports the current imbalance status and provides information of which classes belong to the minority and which classes belong to the majority. Particularly, a key indicator is the real-time class size w(t)k , the percentage of class ck at time step t. When a new example xt arrives, w (t) k is incrementally updated by the following equation [18]:\nw (t) k = \u03b8w (t\u22121) k + (1\u2212 \u03b8) [(xt, ck)] , (k = 1, . . . , N) (5)\nwhere [(xt, ck)] = 1 if the true class label of xt is ck, and 0 otherwise. \u03b8 (0 < \u03b8 < 1) is a pre-defined time decay (forgetting) factor, which reduces the contribution of older data to the calculation of class sizes along with time. It is independent of learning algorithms, so it can be used with any type of online classifiers. For example, it has been used in OOB and UOB [11] for deciding the resampling rate adaptively and overcoming class imbalance effectively over time. OOB and UOB integrate oversampling and undersampling respectively into ensemble algorithm Online Bagging (OB) [64].\nOversampling and undersampling are one of the simplest and most effective techniques of tackling class imbalance [30].\nThe properties of the above online approaches are summarized in Table IV, answering the following six questions in order:\n\u2022 How do they handle concept drift (the type based on the categorization in Table III)? \u2022 Do they involve any class imbalance technique to improve the predictive performance of online models, in addition to concept drift detection? \u2022 Do they need access to previously received data? \u2022 Do they need additional data sets for initialisation or\nvalidation? \u2022 Can they handle data streams with more than two classes\n(multi-class data)? \u2022 Do they involve any mechanism handling P (y) drift?"}, {"heading": "IV. PERFORMANCE ANALYSIS", "text": "With a complete review of online class imbalance learning, we aim at a deep understanding of concept drift detection in imbalanced data streams and the performance of existing approaches introduced in Section III-B. Three research questions will be looked into through experimental analysis: 1) what are the difficulties in detecting each type of concept drift? Little work has given separate discussions on the three fundamental types of concept drift, especially the P (y) drift. It is important to understand their differences, so that the most suitable approaches can be used for the best performance. 2) Among existing approaches designed for imbalanced data streams with concept drift, which approach is better and when? Although a few approaches have been proposed for the purpose of overcoming concept drift and class imbalance, it is still unclear how well they perform for each type of concept drift. 3) Whether and how do class imbalance techniques affect concept drift detection and online prediction? No study has looked into the mutual effect of applying class imbalance techniques and concept drift detection methods. Understanding the role of class imbalance techniques will help us to develop more effective concept drift detection methods for imbalanced data."}, {"heading": "A. Data Sets", "text": "For an accurate analysis and comparable results, we choose two most commonly used artificial data generators, SINE1 [79] and SEA [100], to produce imbalanced data streams containing three simulated types of concept drift. This is one of the very few studies that individually discuss P (y), p (x | y)\nand P (y | x) types of concept drift in depth. In addition, each generator produces two data streams with a different drifting speed \u2013 abrupt and gradual drifts. The drifting speed is defined as the inverse of the time taken for a new concept to completely replace the old one [72]. According to speed, drifts can be either abrupt, when the generating function is changed completely in only one time step, or gradual, otherwise. The data streams with a gradual concept drift are denoted by \u2018g\u2019 in the following experiment, i.e. SINE1g [76] and SEAg. Every data stream has 3000 time steps, with one concept drift starting at time step 1501. The new concept in SINE1 and SEA fully takes over the data stream from time step 1501; the concept drift in SINE1g and SEAg takes 500 time steps to complete, which means that the new concept fully replaces the old one from time step 2001. The detailed settings for generating each type of concept drift are included in the individual subsections.\nAfter the detailed analysis of the three types of concept drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102]. Data in PAKDD are collected from the private label credit card operation of a Brazilian retail chain. The task of this problem is to identify whether the client has a good or bad credit. The \u201cbad\u201d credit is the minority class, taking 19.75% of the provided modelling data. Because the data have been collected from a time interval in the past, gradual market change occurs. The Weather data set aims to predict whether rain precipitation was observed on each day, with inherent seasonal changes. The class of \u201crain\u201d is the minority at IR of 31%. The original Tweet data include 50 million tweets posted mainly from 2008 to 2011. The task is to predict the tweet topic. We choose a time interval, containing 8774 examples and covering seven tweet topics [103]. Then, we further reduce it to 2-class data by using only two out of seven topics for our experiment. These real-world data will help us to understand the effectiveness of existing concept drift and class imbalance approaches in practical scenarios, which usually have more complex data distributions and concept drift."}, {"heading": "B. Experimental and Evaluation Settings", "text": "The approaches listed in Table IV, which are explicitly designed for the combined problem of class imbalance and concept drift, are discussed in our experiment. For the three active drift detection methods \u2013 DDM-OCI, LFR and PAUCPH, they are used with the traditional Online Bagging (abbr. OB) [64] and OOB with CID [11] respectively for classification. Because OOB applies oversampling to overcome class imbalance and OB does not, it can help us to observe the role of class imbalance techniques (oversampling in our experiment) in concept drift detection. UOB is not chosen, for the consideration that undersampling may cause unstable performance which may indirectly affect our observation [11]. Between RLSACP and ONN, due to their similarity and the more theoretical support in RLSACP, only RLSACP is included in our experiment.\nConsidering RLSACP and ESOS-ELM are perceptronbased methods, we use the Multilayer Perceptron (MLP)\nclassifier as the base learner of OB and OOB. The number of neurons in the hidden layer of MLPs is set to the average of the number of attributes and classes in data, which is also the number of perceptrons in RLSACP and ESOS-ELM. All ensemble methods maintain 15 base learners. For ESOSELM, we disable the \u201cELM-Store\u201d, which is designed for recurring concept drift; we allow that its ensemble size can grow to 20. In addition, ESOS-ELM requires an initialisation data set to initialize ELMs, and validation data sets to adjust misclassification costs. When dealing with artificial data, we use the first 100 examples to initialize ESOS-ELM, and generate a separate validation data set for each concept stage. We track the performance of all the methods from time step 101.\nIn summary, ten algorithms join the comparison from Table IV: OB, OOB, DDM-OCI+OB/OOB, PAUCPH+OB/OOB, LFR+OB/OOB, RLSACP and ESOS-ELM. OB is the baseline without involving any class imbalance and concept drift techniques.\nTo evaluate the effectiveness of concept drift detection methods and online learners, we adopt prequential test (as described in Section II) for its simplicity and popularity. Prequential recall of each class (defined in Eq. 1) and prequential G-mean (defined in Eq. 4) are tracked over time for comparison, because they are insensitive to imbalance rates. When discussing the generated artificial data sets with ground truth known, we also compare the true detection rate (abbr. TDR), total number of false alarms (abbr. FA) and delay of detection (abbr. DoD) (as defined in Section II) among methods using any of the three active drift detectors (i.e. DDM-OCI, LFR and PAUC-PH). The calculation of TDR, FA and DoD is based on the following understanding: before a real concept drift occurs, all the reported alarms are considered as false alarms; after a real concept drift occurs, the first detection is seen as the true alarm; after that and before the next new real concept drift, the consequent detections are considered as false alarms.\nFurthermore, because we are particularly interested in how the learner performs on the new data concept in the artificial data sets, we calculate the average recall and G-mean over all the time steps before the concept drift starts and after the concept drift completely ends. It is worth noting that the recall and G-mean values are reset to 0 when the drift starts and ends for an accurate analysis. We use the Wilcoxon Sign Rank test at the confidence level of 95% as our significance test in this paper."}, {"heading": "C. Comparative Study on Artificial Data", "text": "C.1. P (y) Concept Drift\nThis section focuses on the P (y) type of concept drift, without p (x | y) and P (y | x) changes. Data streams SINE1 and SINE1g have a severe class imbalance change, in which the minority (majority) class during the first half of data streams becomes the majority (minority) during the latter half. SEA and SEAg have a less severe change, in which the data stream presented to be balanced during the first half becomes imbalanced during the latter half. The concrete setting for each data stream is summarized in Table V.\nTable VI compares the detection performance of the three active concept drift detectors, in terms of TDR, FA and DoD. The first column is the data ID number, as denoted in Table V. We can see that DDM-OCI and LFR are sensitive to class imbalance changes in data. They present very high true detection rate; especially, LFR has 100% TDR in all cases regardless of whether resampling is used to tackle class imbalance. PAUC-PH does not report any concept drift, showing 0% TDR in all cases. This is because DDM-OCI and LFR use time-decayed metrics as the indicator of concept drift, which have higher sensitivity to performance change in general than the prequential AUC used by PAUC-PH. LFR shows even higher TDR than DDM-OCI, because it tracks four rates in the confusion matrix instead of one. For the same reason, DDM-OCI and LFR have a higher chance of issuing false alarms than PAUC-PH. For DDM-OCI, oversampling in OOB increases the probability of reporting a concept drift by observing TDR in SEA and SEAg, compared to OB. This is because more examples are used for training in OOB, which improves the performance on the minority class for concept drift detection.\nTable VII compares recall and G-mean of all models over the new data concept, i.e. performance over time steps 1501- 3000 for data streams with an abrupt change and performance over time steps 2001-3000 for data streams with a gradual change, showing whether and how well the drift detector can\nhelp with learning after concept drift is completed. The first column is the data ID number, as denoted in Table V. In SINE1 and SINE1g, the negative class presents to be the minority after the change; in SEA and SEAg, the positive class presents to be the minority after the change.\nIn terms of minority-class recall, we can see that ESOSELM performs the significantly best, but ESOS-ELM sacrifices majority-class recall, especially in SINE1 and SINE1g. In terms of G-mean, OOB and OOB using PAUC-PH perform the significantly best, which shows they can best balance the performance between classes. It is worth noting that PAUC-PH is the drift detection method with 0% TDR based on Table VI. It means that OOB plays the main role in learning. It also explains that OOB and OOB using PAUC-PH have very close\nperformance. All the OB and OOB models using the other active drift detectors do not show competitive recall and Gmean. Especially for those using DDM-OCI and LFR, the high number of false alarms causes too much resetting and performance loss; OOB can increase the chance of producing a false alarm, because more minority-class examples join the training.\nTherefore, we conclude that, for P (y) type of concept drift, it is not necessary to apply any drift detection techniques that are not specifically designed for class imbalance changes; the use of these drift detectors could be even detrimental to the predictive performance due to false alarms and performance resetting; the adaptive resampling in OOB is sufficient to deal with the change and maintain the predictive performance; when using OOB with other active concept drift detectors, the number of false alarms and performance resetting need to be carefully considered.\nC.2. p (x | y) Concept Drift The data streams in this section only involve p (x | y) type of concept drift, without P (y) and P (y | x) changes. The class imbalance ratio is fixed to 1:9 and we let the positive class be the minority, so that the data stream is constantly imbalanced. The concept drift in each data stream is controlled by p (x) of the negative class, as shown in Table VIII.\nTable IX compares the detection performance of the three active concept drift detectors. Similar to our previous results, DDM-OCI and LFR are more sensitive to P (x | y) changes than PAUC-PH. When DDM-OCI and LFR work with OOB, their TDR shows 100%; and LFR has higher FA and shorter DOD than DDM-OCI, due to more indicators it monitors. PAUC-PH shows 0% TDR in most cases of working with both OB and OOB. Different from P (y) changes, when DDM-OCI and LFR work with OB, their TDR is rather low, which suggests that their sensitivity is dependent on the class imbalance techniques. Unlike the cases with class imbalance changes, where it is possible for the minorityclass examples to become more frequent, the data streams generated in this section have a fixed minority class with a constantly small prior probability. In other words, it would be more difficult to recognize examples from this minority class, which indirectly affects the detection sensitivity of DDM-OCI and LFR. When oversampling is applied, which introduces more training examples for the minority class, the performance metrics (G-mean, recall and precision) monitored by DDMOCI and LFR can be substantially improved. It also increases the possibility of reporting a concept drift. This explains the low detection rate of DDM-OCI and LFR when working with OB and their high detection rate when working with OOB.\nTable X compares recall and G-mean of all models over\nthe new data concept. As we expected, almost all OB models show significantly worse minority-class recall and G-mean. On SINE1 and SINE1g data, minority-class recall of OB models is as low as 0, which may hinder the detection of any concept drift. Among the OOB models, those using DDMOCI and LFR perform significantly worse than OOB using PAUC-PH and OOB itself, and the latter two show very close performance. This is because DDM-OCI and LFR trigger concept drift with false alarms, and cause model resetting multiple times. Along with the resetting, the useful and valid information learnt in the past is forgotten at the same time. For the two passive models, RLSACP and ESOS-ELM do not perform very well compared to OOB. Generally speaking, for imbalanced data streams with p (x | y) changes, class imbalance seems to be a more important issue than concept drift, considering that the learning model without triggering any concept drift detection achieves the best performance. Besides, while the adopted class imbalance technique can improve the final prediction, it can also improve the performance of active concept drift detection methods, depending on their working mechanism.\nC.3. P (y | x) Concept Drift The data streams in this section only involve P (y | x) type of concept drift, without P (y) and p (x | y) changes. Following the settings in Section IV-C.2, we fix the class\nimbalance ratio to 1:9 and let the positive class be the minority, so that the data stream is constantly imbalanced. As shown in Table XI, the data distribution in SINE1 and SINE1g involves a concept swap, and this change occurs probabilistically in SINE1g; the data distribution in SEA and SEAg has a concept threshold moving, and this change occurs continuously in SEAg. The change in SEA and SEAg is less severe than the change in SINE1 and SINE1g, because some of the examples from the old concept are still valid under the new concept after the threshold moves completely. The concept drift discussed in this section belongs to the real concept drift category, which affects the classification boundary and is expected to be captured by all concept drift detectors.\nAccording to Table XII, we can see that DDM-OCI and LFR have difficulty in detecting the concept drift when working with OB, because of the poor recall and G-mean produced by OB, which is also observed and explained in Section IV-C.2. When DDM-OCI and LFR work with OOB, their detection rate TDR is greatly improved (above 90% in most cases). This is because the improved performance metrics facilitate the detection. LFR is more sensitive to the change, which\nproduces higher FA and shorter DoD. Different from previous observations in terms of concept drift detection performance, PAUC-PH working with OB produces 100% TDR and low FA on data streams SINE1 and SINE1g, but PAUC-PH does not work well with OOB on the same data. It is interesting to see that oversampling does not always play a positive role in drift detection. One possible reason is that class imbalance techniques may sometimes hide the performance drop caused by the real concept drift, while it tries to maintain the overall predictive performance, especially for AUC type of metrics in our case. On data streams SEA and SEAg, PAUC-PH does not report any concept drift, probably due to the less severe concept drift.\nThe recall and G-mean over the new data concept in Table XIII further confirms the above analysis. The OB models produce very low minority-class recall and thus low G-mean. RLSACP and ESOS-ELM do not perform well on the new data concept either. By comparing the models that captures concept drifts (DDM-OCI+OOB, LFR+OOB, PAUC-PH+OB) and the models without reporting any concept drift (PAUCPH+OOB, OOB), it seems that class imbalance causes a more difficult learning issue than the real concept drift in our cases. The models solely tackling class imbalance produce the significantly best recall and G-mean. The rather low imbalance ratio (i.e. 1:9) could be a reason. It would be worth discussing various imbalance levels in data with concept drift in our future work, in order to find out when it is worthwhile considering concept drift in imbalanced data streams. By comparing the results in Table XIII, Table X and Table VII, the P (y | x) type of concept drift indeed leads to the most performance reduction. It is consistent with our understanding that the real concept drift is the most radical type of change in data. However, existing approaches do not seem to tackle it well when data streams are very imbalanced. To develop better concept drift detection methods, the key issues here include how to best have them and class imbalance techniques work together and how to tackle the performance loss brought by false alarms."}, {"heading": "D. Comparative Study on Real-World Data", "text": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A. Based on the experimental results on the artificial data, we focus on the best active (PAUC+OOB) and the best passive concept drift detection methods (ESOS-ELM) here for a clear observation, in comparison with OOB. The three methods use the same parameter settings as before. The initialisation and validation data required by ESOS-ELM is the first 2% examples of each data set.\nWithout knowing the true concept drifts in real-world data, we calculate and track the time-decayed G-mean by setting the decay factor to 0.995, which means that the old performance is forgotten at the rate of 0.5%. All the compared metrics are the average of 100 runs in the following figures.\nFig. 3 presents the time-decayed G-mean curves from OOB, PAUC-PH+OOB and ESOS-ELM on the three real-world data\nsets. The average number of reported drift by PAUC-PH is 1, 3 and 1 on Weather, PAKDD and Tweet data respectively. Compared to the artificial cases, we obtain some similar results: the passive approach ESOS-ELM does not perform as well as the other two methods; OOB and PAUC-PH show very close G-mean over time on Weather and PAKDD data, which suggests the importance of tackling class imbalance adaptively.\nIn the PAKDD plot, we can see that the G-mean level is relatively stable without significant drop; differently, G-mean in the Tweet plot is reducing. It may suggest that the concept drift in PAKDD is less significant or influential than that in Tweet. Compared to the gradual market and environment\nchange in PAKDD, the tweet topic change can be much faster and more noticeable. Therefore, although PAUC-PH detects 3 concept drifts in PAKDD, the two methods, OOB and PAUCPH+OOB, does not show much difference. In tweet, PAUCPH+OOB presents better G-mean than using OOB alone, showing the positive effect of the active concept drift detector in fast changing data streams."}, {"heading": "E. Further Discussions", "text": "In this section, we summarize and further discuss the results in the above comparative study on the artificial and real-world data. We also answer the research questions proposed at the beginning of this paper. When dealing with imbalanced data streams with concept drift, we have obtained the following:\n\u2022 When both class imbalance and concept drift exist, class imbalance status and class imbalance changes are shown to be more crucial issues than the traditional concept drift (i.e. p (x | y) and P (y | x) changes) in terms of the online prediction performance. It is necessary to adopt adaptive class imbalance techniques (e.g. OOB discussed in our experiment), in addition to using concept drift detection methods alone (e.g. DDM-OCI, LFR). Most existing papers that proposed new concept drift detection methods for imbalanced data so far did not consider the effect of class imbalance techniques on final prediction and concept drift detection. \u2022 P (y | x) concept drift (i.e. real concept drift) is the most severe type of change in data, compared to p (x | y) and P (y) concept drift. This is based on the observation on the final prediction performance. For all three types of concept drift, existing concept drift approaches do not show much benefit in performance improvement. Concept drift is hard to be detected when no class imbalance technique is applied. Their drift detection performance is affected by the class imbalance technique, depending on their detection mechanism. \u2022 For P (y) concept drift, it is not necessary to apply any concept drift detection methods that are not designed for class imbalance changes, due to their false alarms and model resetting. It is crucial to detect and handle the class imbalance change in time. \u2022 From the results on real-world data, we see that the effectiveness of traditional concept drift detectors (e.g. PAUC-PH) depends on the type of concept drift. For fast and significant concept drift, applying PAUC-PH seems to be more beneficial to the prediction performance. \u2022 Among existing methods designed for imbalanced data with concept drift (4 active methods and 2 passive methods), the passive methods (i.e. ESOS-ELM and RLSACP) do not perform well in general. Although they contain both class imbalance and concept drift techniques, firstly, their class imbalance technique is not effectively adaptive to class imbalance changes, so that wrong imbalance status might be used during learning; secondly, they are restricted to the use of certain perceptron-based classifiers, so that the disadvantages of the classifiers are also inherited by the online model. For example, the training\nof OS-ELM in ESOS-ELM requires initialisation and validation data sets reflecting the correct data concepts, and the weighted OS-ELM was found to over-emphasize the minority class and present large performance variance sometimes in earlier studies [11]. \u2022 Among the three active methods discussed in this work, which are DDM-OCI, LFR and PAUC-PH, DDM-OCI and LFR are more sensitive to concept drift than PAUCPH, with a higher detection rate but also higher false alarms. In addition, the detection performance of DDMOCI and LFR can be greatly improved by OOB. The explanation can be found in the previous analysis.\nOverall, all these results suggest us that class imbalance and concept drift need to be studied simultaneously, when we design an algorithm to deal with imbalanced data with concept drift. Their mutual effect must be taken into consideration. Hence, we propose the following key issues to be considered for an effective algorithm:\n\u2022 Is the class imbalance technique effective in predicting minority-class examples? \u2022 Is the class imbalance technique adaptive to class imbalance changes? \u2022 Is the concept drift technique effective in detecting different types of concept drift, in terms of detection rate, false alarms and detection promptness? Which type of concept drift is it designed for? Which type of concept drift does it perform better? \u2022 Is the detection performance of the concept drift technique affected by the class imbalance technique? And how? \u2022 How can we have the class imbalance technique and concept drift technique work together, to achieve better detection rate, fewer false alarms, less detection delay or better online prediction?"}, {"heading": "V. CONCLUSION", "text": "This paper gives the first systematic study of handling concept drift in class-imbalanced data streams. In the context of online learning, we provide a thorough review and an experimental insight into this problem.\nFirst, a comprehensive review is given, including the problem description and definitions, the individual learning issues and solutions in class imbalance and concept drift respectively, the combined challenges and existing solutions in online class imbalance learning with concept drift, and example applications. The review reveals research gaps in the field of online class imbalance learning with concept drift. Specifically, little work has looked into the concept drift issue in imbalanced data streams systematically, although a few methods have been proposed for this purpose; P (y) type of concept drift is closely related to the class imbalance issue, but it has not been investigated properly so far; most existing concept drift detection methods are only designed for or tested on balanced data streams.\nSecond, to fill in these research gaps, we carry out a thorough empirical study by looking into the following research questions: 1) what are the challenges in detecting each type of\nconcept drift when the data stream is imbalanced (i.e. changes in P (y), p (x | y), and P (y | x))? 2) Among the proposed methods designed for online class imbalance learning with concept drift, i.e. DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.g. resampling methods) facilitate the concept drift detection and online prediction? By generating artificial data streams with different types of class imbalance and concept drift and experimenting on real-world data, we make the following conclusions.\nFor the first research question, a P (y) change can be easily tackled by an adaptive class imbalance technique (e.g. OOB used in this work). The traditional concept drift detectors, such as LFR, DDM-OCI and PAUC-PH, do not perform well in detecting a p (x | y) change. The prediction performance on an imbalanced data stream with p (x | y) changes can be effectively improved by solely using an adaptive class imbalance technique. A P (y | x) change is the most challenging case for learning, where the traditional active and passive concept drift detection methods do not bring much performance improvement. Class imbalance is shown to be a more crucial issue in terms of final prediction performance.\nFor the second research question, the two passive methods, RLSACP and ESOS-ELM, do not perform well in general. DDM-OCI and LFR are sensitive to different types of concept drift, with a high detection rate but also high false alarms. PAUC-PH is more conservative in terms of drift detection. Based on the observation on minority-class recall and G-mean, the combination PAUC-PH and OOB was shown to be the best approach among all.\nFor the third research question, it is necessary to apply adaptive class imbalance techniques when learning from imbalanced data streams with concept drift \u2013 they bring the most prediction performance improvement. In our experiment, our class imbalance technique OOB facilitates the concept drift detection of DDM-OCI and LFR.\nThis paper also provides guidelines for future algorithm design. Several important issues are pointed out for consideration. There are still many challenges and learning issues in this field that are worth of ongoing research, such as more effective concept drift detection methods for imbalanced data streams, studying the mutual effect of class imbalance and concept drift, and more real-world applications with different types of class imbalance and concept drift."}], "references": [{"title": "A new dynamic modeling framework for credit risk assessment", "author": ["M.R. Sousa", "J. Gama", "E. Brand\u00e3o"], "venue": "Expert Systems with Applications, vol. 45, p. 341351, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Fault diagnosis using a timed discrete-event approach based on interval observers: Application to sewer networks", "author": ["J. Meseguer", "V. Puig", "T. Escobet"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans, vol. 40, no. 5, pp. 900\u2013916, 2010.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Online class imbalance learning and its applications in fault detection", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "International Journal of Computational Intelligence and Applications, vol. 12, no. 4, pp. 1 340 001(1\u2013 19), 2013.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Online ensemble learning of data streams with gradually evolved classes", "author": ["Y. Sun", "K. Tang", "L.L. Minku", "S. Wang", "X. Yao"], "venue": "IEEE Transaction on Knowledge and Data Engineering, vol. (Accepted), 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning from imbalanced data", "author": ["H. He", "E.A. Garcia"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 21, no. 9, pp. 1263\u20131284, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Online ensemble learning in the presence of concept drift", "author": ["L.L. Minku"], "venue": "Ph.D. dissertation, School of Computer Science, The University of Birmingham, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv., vol. 46, no. 4, pp. 44:1\u201344:37, Mar. 2014. [Online]. Available: http://doi.acm.org/10.1145/2523813", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Concept drift detection for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "D. Ghezzi", "D. Caltabiano", "P. Tino", "X. Yao"], "venue": "International Joint Conference on Neural Networks (IJCNN \u201913), 2013, pp. 1\u20138.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Concept drift detection for streaming data", "author": ["H. Wang", "Z. Abraham"], "venue": "International Joint Conference of Neural Networks, 2015, pp. 1\u20139.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Prequential auc for classifier evaluation and drift detection in evolving data streams", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "New Frontiers in Mining Complex Patterns, vol. 8983, pp. 87\u2013101, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Resampling-based ensemble methods for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, no. 5, pp. 1356 \u2013 1368, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Recursive least square perceptron model for non-stationary and imbalanced data stream classification", "author": ["A. Ghazikhani", "R. Monsefi", "H.S. Yazdi"], "venue": "Evolving Systems, vol. 4, no. 2, pp. 119\u2013131, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Ensemble of subset online sequential extreme learning machine for class imbalance and concept drift", "author": ["B. Mirza", "Z. Lin", "N. Liu"], "venue": "Neurocomputing, vol. 149, pp. 316\u2013329, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning in nonstationary environments: A survey", "author": ["G. Ditzler", "M. Roveri", "C. Alippi", "R. Polikar"], "venue": "IEEE Computational Intelligence Magazine, vol. 10, no. 4, pp. 12 \u2013 25, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Experimental comparisons of online and batch versions of bagging and boosting", "author": ["N.C. Oza", "S. Russell"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2001, pp. 359\u2013364.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Learn++: an incremental learning algorithm for supervised neural networks", "author": ["R. Polikar", "L. Udpa", "S.S. Udpa", "V. Honavar"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 31, no. 4, pp. 497 \u2013 508, 2001.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear neural networks: Principles, mechanisms, and architectures", "author": ["S. Grossber"], "venue": "Neural Networks, vol. 1, no. 1, p. 1761, 1988.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1988}, {"title": "A learning framework for online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "IEEE Symposium on Computational Intelligence and Ensemble Learning (CIEL), 2013, pp. 36\u201345.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Detecting sudden concept drift with knowledge of human behavior", "author": ["K. Nishida", "S. Shimada", "S. Ishikawa", "K. Yamauchi"], "venue": "IEEE International Conference on Systems, Man and Cybernetics, 2008, pp. 3261\u20133267.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "The class imbalance problem: A systematic study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis, vol. 6, no. 5, pp. 429 \u2013 449, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning from streaming data with concept drift and imbalance: an overview", "author": ["T.R. Hoens", "R. Polikar", "N.V. Chawla"], "venue": "Progress in Artificial Intelligence, vol. 1, no. 1, pp. 89\u2013101, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Wind power forecasting : state-of-the-art 2009", "author": ["C. Monteiro", "R. Bessa", "V. Miranda", "A. Botterud", "J. Wang", "G. Conzelmann"], "venue": "Technical Report (ANL/DIS-10-1), Argonne National Laboratory (ANL), 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine learning for the detection of oil spills in satellite radar images", "author": ["M. Kubat", "R.C. Holte", "S. Matwin"], "venue": "Machine Learning, vol. 30, no. 2-3, pp. 195\u2013215, 1998.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "Issues in mining imbalanced data sets a review paper", "author": ["S. Visa", "A. Ralescu"], "venue": "Proceedings of the Sixteen Midwest Artificial Intelligence and Cognitive Science Conference, 2005, pp. 67\u201373.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Addressing the curse of imbalanced training sets: One-sided selection", "author": ["M. Kubat", "S. Matwin"], "venue": "Proceedings of the 14th International Conference on Machine Learning, 1997, pp. 179\u2013186.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "A study of the behavior of several methods for balancing machine learning training data", "author": ["G.E.A.P.A. Batista", "R.C. Prati", "M.C. Monard"], "venue": "Special issue on learning from imbalanced datasets, Sigkdd Explorations, vol. 6, no. 1, pp. 20\u201329, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "knn approach to unbalanced data distributions: A case study involving information extraction", "author": ["J. Zhang", "I. Mani"], "venue": "Workshop on Learning from Imbalanced Datasets II, ICML, 2003, pp. 42\u201348.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "On predicting rare classes with svm ensembles in scene classification", "author": ["R. Yan", "Y. Liu", "R. Jin", "A. Hauptmann"], "venue": "IEEE International  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. X, FEBRUARY 2017  17 Conference on Acoustics, Speech, and Signal Processing, 2003, pp. III \u2013 21\u20134 vol.3.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Class-boundary alignment for imbalanced dataset learning", "author": ["G. Wu", "E.Y. Chang"], "venue": "Workshop on Learning from Imbalanced Datasets II, ICML, 2003, pp. 49\u201356.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Experimental perspectives on learning from imbalanced data", "author": ["J.V. Hulse", "T.M. Khoshgoftaar", "A. Napolitano"], "venue": "Proceedings of the 24th international conference on Machine learning, 2007, pp. 935\u2013942.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "An insight into classification with imbalanced data: Empirical results and current trends on using data intrinsic characteristics", "author": ["V. Lopez", "A. Fernandez", "S. Garcia", "V. Palade", "F. Herrera"], "venue": "Information Sciences, vol. 250, pp. 113\u2013141, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Smote: Synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research, vol. 16, pp. 341\u2013378, 2002.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Living in an imbalanced world", "author": ["T.R. Hoens"], "venue": "Ph.D. dissertation, Graduate School of the University of Notre Dame, 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Balancing strategies and class overlapping", "author": ["G.E. Batista", "R.C. Prati", "M.C. Monard"], "venue": "Advances in Intelligent Data Analysis, vol. 3646, pp. 24\u201335, 2005.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Class imbalances versus class overlapping: An analysis of a learning system behavior", "author": ["R.C. Prati", "G.E. Batista", "M.C. Monard"], "venue": "Lecture Notes in Computer Science, vol. 2972, pp. 312\u2013321, 2004.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2004}, {"title": "Class imbalances versus small disjuncts", "author": ["T. Jo", "N. Japkowicz"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 6, 2004, pp. 40\u201349.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2004}, {"title": "Class imbalances: are we focusing on the right issue", "author": ["N. Japkowicz"], "venue": "Workshop on Learning from Imbalanced Data Sets II, ICML, 2003, pp. 17\u201323.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2003}, {"title": "Identification of different types of minority class examples in imbalanced data", "author": ["K. Napierala", "J. Stefanowski"], "venue": "Hybrid Artificial Intelligent Systems, vol. 7209, pp. 139\u2013150, 2012.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2012}, {"title": "Types of minority class examples and their influence on learning classifiers from imbalanced data", "author": ["\u2014\u2014"], "venue": "Journal of Intelligent Information Systems, vol. 46, no. 3, p. 563597, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Borderline-smote: A new oversampling method in imbalanced data sets learning", "author": ["H. Han", "W.-Y. Wang", "B.-H. Mao"], "venue": "International Conference on Intelligent Computing (ICIC), 2005, pp. 878\u2013887.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Adasyn: Adaptive synthetic sampling approach for imbalanced learning", "author": ["H. He", "Y. Bai", "E.A. Garcia", "S. Li"], "venue": "2008, pp. 1322\u20131328.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "MWMOTE - majority weighted minority oversampling technique for imbalanced data set learning", "author": ["S. Barua", "M.M. Islam", "X. Yao", "K. Murase"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 2, pp. 405\u2013425, 2014, an oversampling method by generating new examples.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Two modifications of cnn", "author": ["I. Tomek"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 6, no. 11, pp. 769\u2013772, 1976.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1976}, {"title": "Learning when negative examples abound", "author": ["M. Kubat", "R. Holte", "S. Matwin"], "venue": "9th European Conference on Machine Learning Prague, vol. 1224, 1997, pp. 146\u2013153.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving identification of difficult small classes by balancing class distribution", "author": ["L. Jorma"], "venue": "8th Conference on Artificial Intelligence in Medicine in Europe, AIME 2001, vol. 2101, 2001, pp. 63\u201366.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "An efficient algorithm coupled with synthetic minority over-sampling technique to classify imbalanced pubchem bioassay data", "author": ["M. Hao", "Y. Wang", "S.H. Bryant"], "venue": "Analytica Chimica Acta, vol. 806, no. 2, p. 117127, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "A multiple resampling method for learning from imbalanced data sets", "author": ["A. Estabrooks", "T. Jo", "N. Japkowicz"], "venue": "Computational Intelligence, vol. 20, no. 1, pp. 18\u201336, 2004.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2004}, {"title": "Analyzing the oversampling of different classes and types of examples in multi-class imbalanced datasets", "author": ["J.A. S\u00e1ez", "B. Krawczyk", "M. Wo\u017aniak"], "venue": "Pattern Recognition, vol. (In press), 2016.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Online sequential classification of imbalanced data by combining extreme learning machine and improved smote algorithm", "author": ["W. Mao", "J. Wang", "L. Wang"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), 2015, pp. 1\u20138.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2015}, {"title": "A novelty detection approach to classification", "author": ["N. Japkowicz", "C. Myers", "M.A. Gluck"], "venue": "Proceedings of the 14th international joint conference on Artificial intelligence, 1995, pp. 518\u2013523.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1995}, {"title": "The influence of class imbalance on cost-sensitive learning: An empirical study", "author": ["X.-Y. Liu", "Z.-H. Zhou"], "venue": "Sixth International Conference on Data Mining (ICDM\u201906), 2006, pp. 970\u2013974.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning when training data are costly: the effect of class distribution on tree induction", "author": ["G.M. Weiss", "F. Provost"], "venue": "Journal of Artificial Intelligence Research, 2003, pp. 315\u2013354, 2003.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2003}, {"title": "Cost-sensitive online classification", "author": ["J. Wang", "P. Zhao", "S.C. Hoi"], "venue": "IEEE Transaction on Knowledge and Data Engineering, vol. 26, no. 10, pp. 2425 \u2013 2438, 2014.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2014}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "IEEE Circuits and Systems Magazine, vol. 6, no. 3, pp. 21\u201345, 2006.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "A review on ensembles for the class imbalance problem: Bagging- , boosting-, and hybrid-based approaches", "author": ["M. Galar", "A. Fernandez", "E. Barrenechea", "H. Bustince", "F. Herrera"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, vol. PP, pp. 1\u201322, 2011.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2011}, {"title": "Classifying imbalanced data using a bagging ensemble variation", "author": ["C. Li"], "venue": "Proceedings of the 45th annual southeast regional conference, 2007, pp. 203\u2013208.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploratory undersampling for class imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 39, no. 2, pp. 539\u2013550, 2009.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "SMOTE- Boost: Improving prediction of the minority class in boosting", "author": ["N.V. Chawla", "A. Lazarevic", "L.O. Hall", "K.W. Bowyer"], "venue": "Knowledge Discovery in Databases: PKDD 2003, vol. 2838, 2003, pp. 107\u2013119.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2003}, {"title": "Neighbourhood sampling in bagging for imbalanced data", "author": ["J. B\u0142aszczy\u0144ski", "J. Stefanowski"], "venue": "Special Issue on Information Processing and Machine Learning for Applications of Engineering, Neurocomputing, vol. 150, no. Part B, p. 529542, 2015.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2015}, {"title": "Evaluating boosting algorithms to classify rare classes: Comparison and improvements", "author": ["M.V. Joshi", "V. Kumar", "R.C. Agarwal"], "venue": "Proceedings IEEE International Conference on Data Mining, 2001, pp. 257\u2013264.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2001}, {"title": "Exploiting diversity in ensembles: Improving the performance on unbalanced datasets", "author": ["N.V. Chawla", "J. Sylvester"], "venue": "Multiple Classifier Systems, vol. 4472, pp. 397\u2013406, 2007.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning from imbalanced data sets with boosting and data generation: the databoost-im approach", "author": ["H. Guo", "H.L. Viktor"], "venue": "SIGKDD Explor. Newsl., vol. 6, no. 1, pp. 30\u201339, 2004.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2004}, {"title": "Adacost: Misclassification cost-sensitive boosting", "author": ["W. Fan", "S.J. Stolfo", "J. Zhang", "P.K. Chan"], "venue": "Proceedings of the 16th International Conference on Machine Learning, 1999, pp. 97\u2013105.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1999}, {"title": "Online bagging and boosting", "author": ["N.C. Oza"], "venue": "IEEE International Conference on Systems, Man and Cybernetics, pp. 2340\u20132345, 2005.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "Weighted online sequential extreme learning machine for class imbalance learning", "author": ["B. Mirza", "Z. Lin", "K.-A. Toh"], "venue": "Neural Processing Letters, vol. 38, no. 3, pp. 465\u2013486, 2013.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning when data sets are imbalanced and when costs are unequal and unknown", "author": ["M.A. Maloof"], "venue": "Workshop on Learning from Imbalanced Data Sets II, ICML, 2003.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2003}, {"title": "An introduction to roc analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters, vol. 27, no. 8, pp. 861\u2013874, 2006.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2006}, {"title": "A systematic analysis of performance measures for classification tasks", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing and Management, vol. 45, no. 4, p. 427437, 2009.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2009}, {"title": "Boosting for learning multiple classes with imbalanced class distribution", "author": ["Y. Sun", "M.S. Kamel", "Y. Wang"], "venue": "Sixth International Conference on Data Mining (ICDM\u201906), 2006, pp. 592\u2013602.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2006}, {"title": "A simple generalisation of the area under the roc curve for multiple class classification problems", "author": ["D.J. Hand", "R.J. Till"], "venue": "Machine Learning, vol. 45, no. 2, pp. 171\u2013186, 2001.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2001}, {"title": "DDD: A new ensemble approach for dealing with concept drift", "author": ["L.L. Minku", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 24, no. 4, pp. 619 \u2013633, 2012.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "The impact of diversity on online ensemble learning in the presence of concept drift", "author": ["L.L. Minku", "A.P. White", "X. Yao"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 5, pp. 730\u2013742, 2010.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2010}, {"title": "The impact of changing populations on classifier performance", "author": ["M.G. Kelly", "D.J. Hand", "N.M. Adams"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1999, pp. 367\u2013371.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1999}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 10, pp. 1517\u20131531, 2011.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["G. Ditzler", "R. Polikar"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 10, pp. 2283 \u2013 2301, 2013.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2013}, {"title": "Early drift detection method", "author": ["M. Baena-Garcia", "J. del Campo-Avila", "R. Fidalgo", "A. Bifet", "R. Gavalda", "R. Morales-Bueno"], "venue": "2006.  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. XX, NO. X, FEBRUARY 2017  18", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2006}, {"title": "Characterizing concept drift", "author": ["G.I. Webb", "R. Hyde", "H. Cao", "H.L. Nguyen", "F. Petitjean"], "venue": "Data Mining and Knowledge Discovery, vol. (In print), 2016.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2016}, {"title": "Real-time data mining of non-stationary data streams from sensor networks", "author": ["L. Cohen", "G. Avrahami-Bakish", "M. Last", "A. Kandel", "O. Kipersztok"], "venue": "Information Fusion, vol. 9, no. 3, pp. 344\u2013353, 2008.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with drift detection", "author": ["J. Gama", "P. Medas", "G. Castillo", "P. Rodrigues"], "venue": "Advances in Artificial Intelligence, vol. 3171, pp. 286\u2013295, 2004.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2004}, {"title": "Concept drift detection through resampling", "author": ["M. Harel", "S. Mannor", "R. El-Yaniv", "K. Crammer"], "venue": "Proceedings of The 31st International Conference on Machine Learning, 2014, p. 10091017.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2014}, {"title": "Reacting to different types of concept drift: The accuracy updated ensemble algorithm", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 1, pp. 81 \u2013 94, 2014.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical change-detection tests", "author": ["C. Alippi", "G. Boracchi", "M. Roveri"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 2, pp. 246 \u2013 258, 2017.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2017}, {"title": "On evaluating stream learning algorithms", "author": ["J. Gama", "R. Sebasti\u00e3o", "P.P. Rodrigues"], "venue": "Machine Learning, vol. 90, no. 3, pp. 317\u2013346, 2013.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2013}, {"title": "Handling concept drift in text data stream constrained by high labelling cost", "author": ["P. Lindstrom", "S.J. Delany", "B.M. Namee"], "venue": "Proceeding of the 23rd International Florida Artificial Intelligence Research Society Conference, 2010.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards social user profiling: unified and discriminative influence model for inferring home locations", "author": ["R. Li", "S. Wang", "H. Deng", "R. Wang", "K.C.-C. Chang"], "venue": "KDD, 2012, pp. 1023\u20131031.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2012}, {"title": "Graph ensemble boosting for imbalanced noisy graph stream classification", "author": ["S. Pan", "J. Wu", "X. Zhu", "C. Zhang"], "venue": "IEEE Transactions on Cybernetics, vol. 45, no. 5, pp. 954 \u2013 968, 2015.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2015}, {"title": "Classifying data streams with skewed class distributions and concept drifts", "author": ["J. Gao", "B. Ding", "J. Han", "W. Fan", "P.S. Yu"], "venue": "IEEE Internet Computing, vol. 12, no. 6, pp. 37\u201349, 2008.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2008}, {"title": "A general framework for mining concept-drifting data streams with skewed distributions", "author": ["J. Gao", "W. Fan", "J. Han", "P.S. Yu"], "venue": "Proceedings of SIAM ICDM, 2007, pp. 3\u201314.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2007}, {"title": "Classifying imbalanced data streams via dynamic feature group weighting with importance sampling", "author": ["K. Wu", "A. Edwards", "W. Fan", "J. Gao", "K. Zhang"], "venue": "Proceedings of the 2014 SIAM International Conference on Data Mining, 2014, pp. 722\u2013730.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Sera: Selectively recursive approach towards nonstationary imbalanced stream data mining", "author": ["S. Chen", "H. He"], "venue": "International Joint Conference on Neural Networks, 2009, pp. 522\u2013529.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards incremental learning of nonstationary imbalanced data stream: a multiple selectively recursive approach", "author": ["\u2014\u2014"], "venue": "Evolving Systems, vol. 2, no. 1, pp. 35\u201350, 2011.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental learning of concept drift from streaming imbalanced data", "author": ["G. Ditzler", "R. Polikar"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 10, pp. 2283 \u2013 2301, 2013.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning in non-stationary environments with class imbalance", "author": ["T.R. Hoens", "N.V. Chawla"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, 2013, pp. 168\u2013176.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2013}, {"title": "Heuristic updatable weighted random subspaces for non-stationary environments", "author": ["T.R. Hoens", "N.V. Chawla", "R. Polikar"], "venue": "IEEE 11th International Conference on Data Mining (ICDM), 2011, pp. 241\u2013 250.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}, {"title": "Using hddt to avoid instances propagation in unbalanced and evolving data streams", "author": ["A.D. Pozzolo", "R. Johnson", "O. Caelen"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2014, pp. 588 \u2013 594.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2014}, {"title": "Prequential auc: properties of the area under the roc curve for data streams with concept drift", "author": ["D. Brzezinski", "J. Stefanowski"], "venue": "Knowledge and Information Systems, pp. 1\u201332, 2017.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2017}, {"title": "Continuous inspection schemes", "author": ["E.S. Page"], "venue": "Biometrika, vol. 41, no. 1/2, pp. 100\u2013115, 1954.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1954}, {"title": "Online neural network model for non-stationary and imbalanced data stream classification", "author": ["A. Ghazikhani", "R. Monsefi", "H.S. Yazdi"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 5, no. 1, pp. 51\u201362, 2014.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N. ying Liang", "G. bin Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 6, pp. 1411 \u2013 1423, 2006.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "A streaming ensemble algorithm (sea) for large-scale classification", "author": ["W.N. Street", "Y. Kim"], "venue": "pp. 377\u2013382, 2001.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2001}, {"title": "PAKDD data mining competition 2009: New ways of using known methods", "author": ["C. Linhart", "G. Harari", "S. Abramovich", "A. Buchris"], "venue": "New Frontiers in Applied Data Mining, Lecture Notes in Computer Science, vol. 5669, pp. 99\u2013105, 2010.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards social user profiling: unified and discriminative influence model for inferring home locations", "author": ["R. Li", "S. Wang", "H. Deng", "R. Wang", "K. Chang"], "venue": "Proceedings of the 18th ACM SIGKDD, 2012, pp. 1023\u20131031.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2012}, {"title": "Dealing with multiple classes in online class imbalance learning", "author": ["S. Wang", "L.L. Minku", "X. Yao"], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence (IJCAI-16), 2016, pp. 2118\u20132124.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "Applications in various domains such as risk management [1], anomaly detection [2], software engineering [3], and social media mining [4] are affected by both", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": ", at least one category is minority compared to other categories [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "when learning from data streams [6].", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "real concept drift [7], changes in P (y | x)).", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 131, "endOffset": 135}, {"referenceID": 95, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 159, "endOffset": 163}, {"referenceID": 12, "context": "resampling methods) facilitate concept drift detection and online prediction? Six recent approaches, DDM-OCI [8], LFR [9], PAUC-PH [10] [96], OOB [11], RLSACP [12] and ESOS-ELM [13], are compared and analyzed in depth under each of the three fundamental types of concept drift (i.", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": "According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at", "startOffset": 41, "endOffset": 44}, {"referenceID": 13, "context": "According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "According to the most agreed definitions [6] [14], chunk-based learning algorithms process a batch of data examples at each time step, such as the case of daily internet usage from a set of users; online learning algorithms process examples one by one and the predictive model is updated after receiving each example [15], such as the case of sensor readings at", "startOffset": 317, "endOffset": 321}, {"referenceID": 15, "context": "It is usually referred to as any algorithm that can process data streams with certain criteria met [16].", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "They both build and continuously update a learning model to accommodate newly available data, and simultaneously maintain its performance on old data, giving rise to the stability-plasticity dilemma [17].", "startOffset": 199, "endOffset": 203}, {"referenceID": 5, "context": "On the other hand, the way of designing online and chunkbased learning algorithms can be very different [6].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "Class imbalance aggravates the learning difficulty [5] and complicates the data status [18].", "startOffset": 51, "endOffset": 54}, {"referenceID": 17, "context": "Class imbalance aggravates the learning difficulty [5] and complicates the data status [18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3].", "startOffset": 93, "endOffset": 97}, {"referenceID": 1, "context": "imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3].", "startOffset": 118, "endOffset": 121}, {"referenceID": 2, "context": "imbalance is an important data feature, commonly seen in applications such as spam filtering [19] and fault diagnosis [2] [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 19, "context": "It has been wellstudied in offline learning [20], and has attracted growing", "startOffset": 44, "endOffset": 48}, {"referenceID": 20, "context": "attention in data stream learning in recent years [21].", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "In many applications, such as energy forecasting and climate data analysis [22], the data generator operates in nonstationary environments.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "in [18] as a guide for algorithm design.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "1: Learning framework for online class imbalance learning [18].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "of an online class imbalance algorithm can be described as \u201crecognizing minority-class data effectively, adaptively and timely without sacrificing the performance on the majority class\u201d [18].", "startOffset": 186, "endOffset": 190}, {"referenceID": 22, "context": "In reality, it is common to see that the majority class has accuracy close to 100% and the minority class has very low accuracy between 0%-10% [23].", "startOffset": 143, "endOffset": 147}, {"referenceID": 19, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 100, "endOffset": 104}, {"referenceID": 24, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 132, "endOffset": 136}, {"referenceID": 25, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 137, "endOffset": 141}, {"referenceID": 26, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 142, "endOffset": 146}, {"referenceID": 27, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 155, "endOffset": 159}, {"referenceID": 28, "context": "The negative effect of class imbalance on classifiers, such as decision trees [20], neural networks [24], k-Nearest Neighbour (kNN) [25] [26] [27] and SVM [28] [29], has been studied.", "startOffset": 160, "endOffset": 164}, {"referenceID": 29, "context": "percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32].", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32].", "startOffset": 86, "endOffset": 90}, {"referenceID": 31, "context": "percentage of the minority class in the data set [30], the size ratio between classes [31], or simply a list of the number of examples in each class [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 32, "context": "The coefficient of variance is used in [33], which is less straightforward.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "To define the imbalanced degree suitable for online learning, a real-time indicator was proposed \u2013 time-decayed class size [18], expressing the size percentage of each class in the data stream.", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "Based on this, a class imbalance detector was proposed to determine which classes should be regarded as the minority/majority and how imbalanced the current data stream is, and then used for designing better online classifiers [11] [3].", "startOffset": 227, "endOffset": 231}, {"referenceID": 2, "context": "Based on this, a class imbalance detector was proposed to determine which classes should be regarded as the minority/majority and how imbalanced the current data stream is, and then used for designing better online classifiers [11] [3].", "startOffset": 232, "endOffset": 235}, {"referenceID": 33, "context": "complexity comprises issues such as overlapping [34] [35] and small disjuncts [36].", "startOffset": 48, "endOffset": 52}, {"referenceID": 34, "context": "complexity comprises issues such as overlapping [34] [35] and small disjuncts [36].", "startOffset": 53, "endOffset": 57}, {"referenceID": 35, "context": "complexity comprises issues such as overlapping [34] [35] and small disjuncts [36].", "startOffset": 78, "endOffset": 82}, {"referenceID": 36, "context": "The small disjunct problem is associated with the within-class imbalance [37].", "startOffset": 73, "endOffset": 77}, {"referenceID": 4, "context": "In other words, the rarity of the minority class can be in a relative or absolute sense in terms of the number of available examples [5].", "startOffset": 133, "endOffset": 136}, {"referenceID": 37, "context": "In particular, authors in [38] [39] distinguished and analysed four types of data distributions in the minority class \u2013 safe, borderline, outliers and rare examples.", "startOffset": 26, "endOffset": 30}, {"referenceID": 38, "context": "In particular, authors in [38] [39] distinguished and analysed four types of data distributions in the minority class \u2013 safe, borderline, outliers and rare examples.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "be the harder cases in online applications [11].", "startOffset": 43, "endOffset": 47}, {"referenceID": 31, "context": "For example, SMOTE [32] is a widely used oversampling method, which generates new minorityclass data points based on the similarities between original minority-class examples in the feature space.", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few.", "startOffset": 61, "endOffset": 65}, {"referenceID": 40, "context": "Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few.", "startOffset": 74, "endOffset": 78}, {"referenceID": 41, "context": "Other smart oversampling techniques include Borderline-SMOTE [40], ADASYN [41], MWMOTE [42], to name but a few.", "startOffset": 87, "endOffset": 91}, {"referenceID": 42, "context": "undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc.", "startOffset": 45, "endOffset": 49}, {"referenceID": 43, "context": "undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc.", "startOffset": 70, "endOffset": 74}, {"referenceID": 44, "context": "undersampling techniques include Tomek links [43], Onesided selection [44], Neighbourhood cleaning rule [45], etc.", "startOffset": 104, "endOffset": 108}, {"referenceID": 45, "context": "The effectiveness of resampling techniques have been proved in real-world applications [46].", "startOffset": 87, "endOffset": 91}, {"referenceID": 46, "context": "rate [47], which is relatively easy for two-class data sets, but becomes more complicated for multi-class data sets [48].", "startOffset": 5, "endOffset": 9}, {"referenceID": 47, "context": "rate [47], which is relatively easy for two-class data sets, but becomes more complicated for multi-class data sets [48].", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "Empirical studies have been carried out to compare different resampling methods [30].", "startOffset": 80, "endOffset": 84}, {"referenceID": 48, "context": "Some initial effort has been made recently, to extend smart resampling techniques to online learning [49].", "startOffset": 101, "endOffset": 105}, {"referenceID": 49, "context": "ifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52].", "startOffset": 124, "endOffset": 128}, {"referenceID": 50, "context": "ifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52].", "startOffset": 154, "endOffset": 158}, {"referenceID": 51, "context": "ifying their training mechanism with the direct goal of better accuracy on the minority class, including one-class learning [50], cost-sensitive learning [51] and threshold methods [52].", "startOffset": 181, "endOffset": 185}, {"referenceID": 52, "context": "Some online cost-sensitive methods have been proposed, such as CSOGD [53] and RLSACP [12].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "Some online cost-sensitive methods have been proposed, such as CSOGD [53] and RLSACP [12].", "startOffset": 85, "endOffset": 89}, {"referenceID": 53, "context": "Finally, ensemble learning (also known as multiple classifier systems) [54] has become a major category of approaches to", "startOffset": 71, "endOffset": 75}, {"referenceID": 54, "context": "handling class imbalance [55].", "startOffset": 25, "endOffset": 29}, {"referenceID": 55, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 107, "endOffset": 111}, {"referenceID": 56, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 112, "endOffset": 116}, {"referenceID": 57, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 117, "endOffset": 121}, {"referenceID": 58, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 122, "endOffset": 126}, {"referenceID": 59, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 172, "endOffset": 176}, {"referenceID": 60, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 177, "endOffset": 181}, {"referenceID": 61, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 182, "endOffset": 186}, {"referenceID": 62, "context": "It can be easily adapted for emphasizing the minority class by integrating different resampling techniques [56] [57] [58] [59] or by making base classifiers cost-sensitive [60] [61] [62] [63].", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "ance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.", "startOffset": 35, "endOffset": 39}, {"referenceID": 63, "context": "ance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.", "startOffset": 105, "endOffset": 109}, {"referenceID": 64, "context": "ance learning, such as OOB and UOB [11] applying random oversampling and undersampling in Online Bagging [64], and WOS-ELM [65] training a set of cost-sensitive online extreme learning machines.", "startOffset": 123, "endOffset": 127}, {"referenceID": 43, "context": "[44] proposed to use G-mean to replace overall accuracy:", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "According to [5], any metric that uses values from both", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "estimating the area under the curve, varying in [0, 1].", "startOffset": 48, "endOffset": 54}, {"referenceID": 65, "context": "AUC is usually generated by varying the classification decision threshold for separating positive and negative classes in the testing data set [66] [67].", "startOffset": 143, "endOffset": 147}, {"referenceID": 66, "context": "AUC is usually generated by varying the classification decision threshold for separating positive and negative classes in the testing data set [66] [67].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "Although a recent study has modified AUC for evaluating online classifiers [10], it still needs to collect recently received examples.", "startOffset": 75, "endOffset": 79}, {"referenceID": 67, "context": "Their multi-class versions have been developed [68] [69] [70].", "startOffset": 47, "endOffset": 51}, {"referenceID": 68, "context": "Their multi-class versions have been developed [68] [69] [70].", "startOffset": 52, "endOffset": 56}, {"referenceID": 69, "context": "Their multi-class versions have been developed [68] [69] [70].", "startOffset": 57, "endOffset": 61}, {"referenceID": 67, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 82, "endOffset": 86}, {"referenceID": 67, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 101, "endOffset": 105}, {"referenceID": 68, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 121, "endOffset": 125}, {"referenceID": 69, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Measures Multi-class Online Sensitive to Imbalance recall yes yes no precision no [68] yes yes Fm no [68] yes yes Gm yes [69] yes no AUC no (See MAUC [70]) no (See PAUC [10]) no", "startOffset": 169, "endOffset": 173}, {"referenceID": 6, "context": "2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72].", "startOffset": 93, "endOffset": 96}, {"referenceID": 70, "context": "2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72].", "startOffset": 97, "endOffset": 101}, {"referenceID": 71, "context": "2) Concept drift: Concept drift is said to occur when the joint probability P (x, y) changes [7] [71] [72].", "startOffset": 102, "endOffset": 106}, {"referenceID": 72, "context": "more challenging? Concept drift can manifest three fundamental forms of changes corresponding to the three major variables in the Bayes\u2019 theorem [73]: 1) a change in prior probability P (y); 2) a change in class-conditional pdf p (x | y); 3) a change in posterior probability P (y | x).", "startOffset": 145, "endOffset": 149}, {"referenceID": 73, "context": "Elwell and Polikar claimed that this type of drift is the result of an incomplete representation of the true distribution in current data, which simply requires providing supplemental data information to the learning model [74].", "startOffset": 223, "endOffset": 227}, {"referenceID": 20, "context": "The other two types belong to virtual concept drift [21], which does not change the decision (class) boundaries.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "on feedback about the performance of the classifier, while techniques for handling virtual drift can operate without such feedback [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 71, "context": "A detailed and mutually exclusive categorization can be found in [72].", "startOffset": 65, "endOffset": 69}, {"referenceID": 74, "context": "to old data [75], gradual drifts are often more difficult, because the slow change can delay or hide the hint left by the drift.", "startOffset": 12, "endOffset": 16}, {"referenceID": 75, "context": "We can see some drift detection methods specifically designed for gradual concept drift, such as Early Drift Detection method (EDDM) [76].", "startOffset": 133, "endOffset": 137}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "\u2019s taxonomy based on the four modules of an adaptive learning system [7], and Webb et al.", "startOffset": 69, "endOffset": 72}, {"referenceID": 76, "context": "\u2019s quantitative characterization [77].", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "[14] for its simplicity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "general observation is that, while active approaches are quite effective in detecting abrupt drift, passive approaches are very good at overcoming gradual drift [74] [14].", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "general observation is that, while active approaches are quite effective in detecting abrupt drift, passive approaches are very good at overcoming gradual drift [74] [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 77, "context": "drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80].", "startOffset": 70, "endOffset": 74}, {"referenceID": 78, "context": "drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80].", "startOffset": 80, "endOffset": 84}, {"referenceID": 79, "context": "drift based on the change in the classification error, including OLIN [78], DDM [79] and PERM [80].", "startOffset": 94, "endOffset": 98}, {"referenceID": 80, "context": "Some other algorithms are specifically designed for data streams coming in batches, such as AUE [81] and the Learn++ family [74].", "startOffset": 96, "endOffset": 100}, {"referenceID": 73, "context": "Some other algorithms are specifically designed for data streams coming in batches, such as AUE [81] and the Learn++ family [74].", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "See [14] for the full list of techniques under each category.", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "Wang and Abraham [9] use a histogram to visualize the distribution of detection points from the drift detection approach over multiple runs.", "startOffset": 17, "endOffset": 20}, {"referenceID": 81, "context": "A very recent algorithm, Hierarchical Change-Detection Tests (HCDTs), was proposed to explicitly deal with the tradeoff [82].", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "There are two common ways to depict such performance over time \u2013 holdout and prequential evaluation [7].", "startOffset": 100, "endOffset": 103}, {"referenceID": 82, "context": "This problem can be solved by using a sliding window or a time-based fading factor that weigh observations [83].", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "predict faults in sensors accurately and timely [3].", "startOffset": 48, "endOffset": 51}, {"referenceID": 83, "context": "2) Spam filtering with p (x | y) drift: Spam filtering is a typical classification problem involving class imbalance and concept drift [84].", "startOffset": 135, "endOffset": 139}, {"referenceID": 6, "context": "For example, one of the spamming behaviours is to change email content and presentation in disguise, implying a possible classconditional pdf (p (x | y)) change [7].", "startOffset": 161, "endOffset": 164}, {"referenceID": 84, "context": "Machine learning algorithms can be used to discover who is interested in the product from the large amount of tweets [85].", "startOffset": 117, "endOffset": 121}, {"referenceID": 85, "context": "Some research efforts have been made to address the joint problem of concept drift and class imbalance, due to the rising need from practical problems [86] [1].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "Some research efforts have been made to address the joint problem of concept drift and class imbalance, due to the rising need from practical problems [86] [1].", "startOffset": 156, "endOffset": 159}, {"referenceID": 86, "context": "Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89].", "startOffset": 247, "endOffset": 251}, {"referenceID": 87, "context": "Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89].", "startOffset": 252, "endOffset": 256}, {"referenceID": 88, "context": "Bagging is one of the earliest algorithms, which builds an ensemble of classifiers trained on a more balanced set of data through resampling and overcomes concept drift passively by weighing the base classifier based on their discriminative power [87] [88] [89].", "startOffset": 257, "endOffset": 261}, {"referenceID": 89, "context": "SERA [90] and REA [91] use similar ideas to Uncorrelated Bagging of building an ensemble of weighted classifiers, but with a \u201csmarter\u201d oversampling technique.", "startOffset": 5, "endOffset": 9}, {"referenceID": 90, "context": "SERA [90] and REA [91] use similar ideas to Uncorrelated Bagging of building an ensemble of weighted classifiers, but with a \u201csmarter\u201d oversampling technique.", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "NIE are more recent algorithms, which tackle class imbalance through the oversampling technique SMOTE [32] or a sub-ensemble technique, and overcome concept drift through a dynamic weighting strategy [92].", "startOffset": 102, "endOffset": 106}, {"referenceID": 91, "context": "NIE are more recent algorithms, which tackle class imbalance through the oversampling technique SMOTE [32] or a sub-ensemble technique, and overcome concept drift through a dynamic weighting strategy [92].", "startOffset": 200, "endOffset": 204}, {"referenceID": 92, "context": "IP [93] improves HUWRS [94] to deal with imbalanced data streams by introducing an instance propagation scheme based on a Na\u0131\u0308ve Bayes classifier, and uses Hellinger distance as a", "startOffset": 3, "endOffset": 7}, {"referenceID": 93, "context": "IP [93] improves HUWRS [94] to deal with imbalanced data streams by introducing an instance propagation scheme based on a Na\u0131\u0308ve Bayes classifier, and uses Hellinger distance as a", "startOffset": 23, "endOffset": 27}, {"referenceID": 94, "context": "So, Hellinger Distance Decision Tree (HDDT) was proposed to use Hellinger distance as the decision tree splitting criteria that is imbalance-insensitive [95].", "startOffset": 153, "endOffset": 157}, {"referenceID": 13, "context": "Developing a true online algorithm for concept drift is very challenging because of the difficulties in measuring minority-class statistics using only one example at a time [14].", "startOffset": 173, "endOffset": 177}, {"referenceID": 7, "context": "Drift Detection Method for Online Class Imbalance (DDM-OCI) [8] is one of the very first algorithms detecting concept drift actively in imbalanced data streams online.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "the confusion matrix \u2013 minority-class recall and precision and majority-class recall and precision, with statistically-supported bounds for drift detection [9].", "startOffset": 156, "endOffset": 159}, {"referenceID": 9, "context": "Instead of tracking several performance rates for each class, prequential AUC (PAUC) [10] [96] was proposed as an overall performance", "startOffset": 85, "endOffset": 89}, {"referenceID": 95, "context": "Instead of tracking several performance rates for each class, prequential AUC (PAUC) [10] [96] was proposed as an overall performance", "startOffset": 90, "endOffset": 94}, {"referenceID": 96, "context": "measure for online scenarios, and was used as the concept drift indicator in Page-Hinkley (PH) test [97].", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance.", "startOffset": 18, "endOffset": 22}, {"referenceID": 97, "context": "algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "algorithms RLSACP [12], ONN [98] and ESOS-ELM [13] adapt the classification model to non-stationary environments passively, and involve mechanisms to overcome class imbalance.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 109, "endOffset": 112}, {"referenceID": 8, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 175, "endOffset": 178}, {"referenceID": 9, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 245, "endOffset": 249}, {"referenceID": 11, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 316, "endOffset": 320}, {"referenceID": 97, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 325, "endOffset": 329}, {"referenceID": 12, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 385, "endOffset": 389}, {"referenceID": 10, "context": "Approaches Category? Class Access to Additional data? Multi-class? P (y) drift? imbalance? old data? DDM-OCI [8] Active (change detection test + windowing) No No No No No LFR [9] Active (change detection test + windowing) No No No No No PAUC-PH [10] Active (change detection test + windowing) No Yes No No No RLSACP [12]/ONN [98] Passive (single classifier) Yes Yes No No Yes ESOS-ELM [13] Passive+Active (ensemble) Yes No Yes No No OOB/UOB using CID [11] Active (weighting) Yes No No No Yes", "startOffset": 451, "endOffset": 455}, {"referenceID": 98, "context": "ESOS-ELM is an ensemble approach, maintaining a set of online sequential extreme learning machines (OS-ELM) [99].", "startOffset": 108, "endOffset": 112}, {"referenceID": 64, "context": "ELM-store maintains a pool of weighted extreme learning machines (WELM) [65] to retain old information.", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "With a different goal of concept drift detection from the above, a class imbalance detection (CID) approach was proposed, aiming at P (y) changes [18].", "startOffset": 146, "endOffset": 150}, {"referenceID": 17, "context": "When a new example xt arrives, w (t) k is incrementally updated by the following equation [18]:", "startOffset": 90, "endOffset": 94}, {"referenceID": 10, "context": "in OOB and UOB [11] for deciding the resampling rate adaptively and overcoming class imbalance effectively over time.", "startOffset": 15, "endOffset": 19}, {"referenceID": 63, "context": "OOB and UOB integrate oversampling and undersampling respectively into ensemble algorithm Online Bagging (OB) [64].", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "Oversampling and undersampling are one of the simplest and most effective techniques of tackling class imbalance [30].", "startOffset": 113, "endOffset": 117}, {"referenceID": 78, "context": "For an accurate analysis and comparable results, we choose two most commonly used artificial data generators, SINE1 [79] and SEA [100], to produce imbalanced data streams containing three simulated types of concept drift.", "startOffset": 116, "endOffset": 120}, {"referenceID": 99, "context": "For an accurate analysis and comparable results, we choose two most commonly used artificial data generators, SINE1 [79] and SEA [100], to produce imbalanced data streams containing three simulated types of concept drift.", "startOffset": 129, "endOffset": 134}, {"referenceID": 71, "context": "The drifting speed is defined as the inverse of the time taken for a new concept to completely replace the old one [72].", "startOffset": 115, "endOffset": 119}, {"referenceID": 75, "context": "SINE1g [76] and SEAg.", "startOffset": 7, "endOffset": 11}, {"referenceID": 100, "context": "drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102].", "startOffset": 139, "endOffset": 144}, {"referenceID": 74, "context": "drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102].", "startOffset": 159, "endOffset": 163}, {"referenceID": 101, "context": "drift, three real-world data sets are included in our experiment with unknown concept drift, which are PAKDD 2009 credit card data (PAKDD) [101], Weather data [75] and UDI TweeterCrawl data [102].", "startOffset": 190, "endOffset": 195}, {"referenceID": 102, "context": "We choose a time interval, containing 8774 examples and covering seven tweet topics [103].", "startOffset": 84, "endOffset": 89}, {"referenceID": 63, "context": "OB) [64] and OOB with CID [11] respectively for classification.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "OB) [64] and OOB with CID [11] respectively for classification.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "UOB is not chosen, for the consideration that undersampling may cause unstable performance which may indirectly affect our observation [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 100, "context": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A.", "startOffset": 168, "endOffset": 173}, {"referenceID": 74, "context": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A.", "startOffset": 183, "endOffset": 187}, {"referenceID": 101, "context": "After the detailed analysis of the three types of concept drift, we now look into the performance of the above learning models on the three real-world data sets (PAKDD [101], Weather [75] and Tweet [102]) described in Section IV-A.", "startOffset": 198, "endOffset": 203}, {"referenceID": 10, "context": "For example, the training of OS-ELM in ESOS-ELM requires initialisation and validation data sets reflecting the correct data concepts, and the weighted OS-ELM was found to over-emphasize the minority class and present large performance variance sometimes in earlier studies [11].", "startOffset": 274, "endOffset": 278}, {"referenceID": 7, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 8, "endOffset": 11}, {"referenceID": 8, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "DDM-OCI [8], LFR [9], PAUC-PH [10], OOB [11], RLSACP [12] and ESOS-ELM [13], which one performs better for which type of concept drift? 3) Would applying class imbalance techniques (e.", "startOffset": 71, "endOffset": 75}], "year": 2017, "abstractText": "As an emerging research topic, online class imbalance learning often combines the challenges of both class imbalance and concept drift. It deals with data streams having very skewed class distributions, where concept drift may occur. It has recently received increased research attention; however, very little work addresses the combined problem where both class imbalance and concept drift coexist. As the first systematic study of handling concept drift in class-imbalanced data streams, this paper first provides a comprehensive review of current research progress in this field, including current research focuses and open challenges. Then, an in-depth experimental study is performed, with the goal of understanding how to best overcome concept drift in online learning with class imbalance. Based on the analysis, a general guideline is proposed for the development of an effective algorithm.", "creator": "LaTeX with hyperref package"}}}