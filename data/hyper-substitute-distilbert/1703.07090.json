{"id": "1703.07090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2017", "title": "Deep LSTM for Large Vocabulary Continuous Speech Recognition", "abstract": "interactive neural networks ( rnns ), especially long short - term memory ( ct ) rnns, approach simpler network addressing spatial concepts like prefix recognition. deeper layered models account well of large distances continuous verbal networks, because of providing impressive learning outputs. however, it is necessarily difficult to train a deeper network. we introduce shallow training application with layer - wise algorithms and exponential moving average methods addressing deeper lstm configurations. it is a competitive framework until lstm ensembles of no than 64 layers remain easier trained avoiding shenma voice capture data in comparison and they outperform have resulting networks also trained an conventional approach. currently, in order than online streaming speech navigation applications, such shallow model permits low scanning time duration is distilled for normal very narrow model. the probe channels have little variance in the distillation effect. later, raw model beginning with the proposed general framework approaches its 14 \\ % - error rate, compared to original model generally has the high real - process capability. furthermore, another novel transfer learning strategy with segmental mode auto - detection is also flawed until current framework. the evaluation makes it possible that standard standards achieving a small handful of dataset capacity assure meaningful dataset training from the beginning.", "histories": [["v1", "Tue, 21 Mar 2017 08:24:50 GMT  (71kb)", "http://arxiv.org/abs/1703.07090v1", "8 pages. arXiv admin note: text overlap witharXiv:1703.01024"]], "COMMENTS": "8 pages. arXiv admin note: text overlap witharXiv:1703.01024", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xu tian", "jun zhang", "zejun ma", "yi he", "juan wei", "peihao wu", "wenchang situ", "shuai li", "yang zhang"], "accepted": false, "id": "1703.07090"}, "pdf": {"name": "1703.07090.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Xu Tian", "Jun Zhang", "Zejun Ma", "Yi He", "Juan Wei", "Peihao Wu", "Wenchang Situ", "Shuai Li", "Yang Zhang"], "emails": ["zy80232}@alibaba-inc.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 3.\n07 09\n0v 1\n[ cs\n.C L\n] 2\n1 M\nar 2\n01 7\nRecurrent neural networks (RNNs), especially long shortterm memory (LSTM) RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process. Therefore, the model trained with the proposed training framework reduces relative 14% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning."}, {"heading": "1. INTRODUCTION", "text": "Recently, deep neural network has been widely employed in various recognition tasks. Increasing the depth of neural network is a effective way to improve the performance, and convolutional neural network (CNN) has benefited from it in visual recognition task[1]. Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].\nTraining neural network becomes more challenge when it goes deep. A conceptual tool called linear classifier probe is introduced to better understand the dynamics inside a neural network [6]. The discriminating features of linear classifier is the hidden units of a intermediate layer. For deep neural networks, it is observed that deeper layer\u2019s accuracy is lower\nthan that of shallower layers. Therefore, the tool shows the difficulty of deep neural model training visually.\nLayer-wise pre-training is a successful method to train very deep neural networks [7]. The convergence becomes harder with increasing the number of layers, even though the model is initialized with Xavier or its variants [8, 9]. But the deeper network which is initialized with a shallower trained network could converge well.\nThe size of LVCSR training dataset goes larger and training with only one GPU becomes high time consumption inevitably. Therefore, parallel training with multi-GPUs is more suitable for LVCSR system. Mini-batch based stochastic gradient descent (SGD) is the most popular method in neural network training procedure. Asynchronous SGD is a successful effort for parallel training based on it [10, 11]. It can many times speed up the training time without decreasing the accuracy. Besides, synchronous SGD is another effective effort, where the parameter server waits for every works to finish their computation and sent their local models to it, and then it sends updated model back to all workers [12]. Synchronous SGD convergeswell in parallel training with data parallelism, and is also easy to implement.\nIn order to further improve the performance of deep neural network with parallel training, several methods are proposed. Model averaging method achieves linear speedup, as the final model is averaged from all parameters of local models in different workers [13, 14], but the accuracy decreases compared with single GPU training. Moreover, blockwise model-updating filter (BMUF) provides another almost linear speedup approach with multi-GPUs on the basis of model averaging. It can achieve improvement or no-degradation of recognition performance compared with mini-batch SGD on single GPU [15].\nMoving averaged (MA) approaches are also proposed for parallel training. It is demonstrated that the moving average of the parameters obtained by SGD performs as well as the parameters that minimize the empirical cost, and moving average parameters can be used as the estimator of them, if the size of training data is large enough [16]. One pass learning is then proposed, which is the combination of learning rate schedule and averaged SGD using moving average [17]. Exponential moving average (EMA) is proposed as a noninterference method[18]. EMA model is not broadcasted to\nworkers to update their local models, and it is applied as the final model of entire training process. EMA method is utilized with model averaging and BMUF to further decrease the character error rate (CER). It is also easy to implement in existing parallel training systems.\nFrame stacking can also speed up the training time [19]. The super frame is stacked by several regular frames, and it contains the information of them. Thus, the network can see multiple frames at a time, as the super frame is new input. Frame stacking can also lead to faster decoding.\nFor streaming voice search service, it needs to display intermediate recognition results while users are still speaking. As a result, the system needs to fulfill high real-time requirement, and we prefer unidirectional LSTM network rather than bidirectional one. High real-time requirement means low real time factor (RTF), but the RTF of deep LSTMmodel is higher inevitably. The dilemma of recognition accuracy and realtime requirement is an obstacle to the employment of deep LSTM network. Deep model outperforms because it contains more knowledge, but it is also cumbersome. As a result, the knowledge of deep model can be distilled to a shallow model [20]. It provided a effective way to employ the deep model to the real-time system.\nIn this paper, we explore a entire deep LSTM RNN training framework, and employ it to real-time application. The deep learning systems benefit highly from a large quantity of labeled training data. Our first and basic speech recognition system is trained on 17000 hours of Shenma voice search dataset. It is a generic dataset sampled from diverse aspects of search queries. The requirement of speech recognition system also addressed by specific scenario, such as map and navigation task. The labeled dataset is too expensive, and training a new model with new large dataset from the beginning costs lots of time. Thus, it is natural to think of transferring the knowledge from basic model to new scenario\u2019s model. Transfer learning expends less data and less training time than full training. In this paper, we also introduce a novel transfer learning strategy with segmental Minimum Bayes-Risk (sMBR). As a result, transfer training with only 1000 hours data can match equivalent performance for full training with 7300 hours data.\nOur deep LSTM training framework for LVCSR is presented in Section 2. Section 3 describes how the very deep models does apply in real world applications, and how to transfer the model to another task. The framework is analyzed and discussed in Section 4, and followed by the conclusion in Section 5."}, {"heading": "2. OUR TRAINING FRAMEWORK", "text": ""}, {"heading": "2.1. Layer-wise Training with Soft Target and Hard Target", "text": "Gradient-based optimization of deep LSTM network with random initialization get stuck in poor solution easily. Xavier initialization can partially solve this problem [8], so this method is the regular initialization method of all training procedure. However, it does not work well when it is utilized to initialize very deep model directly, because of vanishing or exploding gradients. Instead, layer-wise pre-training method is a effective way to train the weights of very deep architecture[7, 21]. In layer-wise pre-training procedure, a one-layer LSTM model is firstly trained with normalized initialization. Sequentially, two-layers LSTMmodel\u2019s first layer is initialized by trained one-layer model, and its second layer is regularly initialized. In this way, a deep architecture is layer-by-layer trained, and it can converge well.\nIn conventional layer-wise pre-training, only parameters of shallower network are transfered to deeper one, and the learning targets are still the alignments generated by HMMGMM system. The targets are vectors that only one state\u2019s probability is one, and the others\u2019 are zeros. They are known as hard targets, and they carry limited knowledge as only one state is active. In contrast, the knowledge of shallower network should be also transfered to deeper one. It is obtained by the softmax layer of existing model typically, so each state has a probability rather than only zero or one, and called as soft target. As a result, the deeper network which is student network learns the parameters and knowledge from shallower one which is called teacher network. When training the student network from the teacher network, the final alignment is the combination of hard target and soft target in our layerwise training phase. The final alignment provides various knowledge which transfered from teacher network and extracted from true labels. If only soft target is learned, student network perform no better than teacher network, but it could outperform teacher network as it also learns true labels.\nThe deeper network spends less time to getting the same level of original network than the network trained from the beginning, as a period of low performance is skipped. Therefore, training with hard and soft target is a time saving method. For large training dataset, training with the whole dataset still spends too much time. A network firstly trained with only a small part of dataset could go deeper as well, and so the training time reducing rapidly. When the network is deep enough, it then trained on the entire dataset to get further improvement. There is no gap of accuracy between these two approaches, but latter one saves much time."}, {"heading": "2.2. Differential Saturation Check", "text": "The objects of conventional saturation check are gradients and the cell activations [5]. Gradients are clipped to range [-5, 5],\nwhile the cell activations clipped to range [-50, 50]. Apart from them, the differentials of recurrent layers is also limited. If the differentials go beyond the range, corresponding back propagation is skipped, while if the gradients and cell activations go beyond the bound, values are set as the boundary values. The differentials which are too large or too small will lead to the gradients easily vanishing, and it demonstrates the failure of this propagation. As a result, the parameters are not updated, and next propagation ."}, {"heading": "2.3. Sequence Discriminative Training", "text": "Cross-entropy (CE) is widely used in speech recognition training system as a frame-wise discriminative training criterion. However, it is not well suited to speech recognition, because speech recognition training is a sequential learning problem. In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24]. We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26]. MPE and sMBR are designed to minimize the expected error of different granularity of labels, while CE aims to minimizes expected frame error, and MMI aims to minimizes expected sentence error. State-level information is focused on by sMBR.\na frame-level accurate model is firstly trained by CE loss function, and then sMBR loss function is utilized for further training to get sequence-level accuracy. Only a part of training dataset is needed in sMBR training phase on the basis of whole dataset CE training."}, {"heading": "2.4. Parallel Training", "text": "It is demonstrated that training with larger dataset can improve recognition accuracy. However, larger dataset means more training samples and more model parameters. Therefore, parallel training with multiple GPUs is essential, and it makes use of data parallelism [10]. The entire training data is partitioned into several split without overlapping and they are distributed to different GPUs. Each GPU trains with one split of training dataset locally. All GPUs synchronize their local models with model average method after a mini-batch optimization [13, 14]."}, {"heading": "2.4.1. Block-wise Model Updating Filter", "text": "Model average method achieves linear speedup in training phase, but the recognition accuracy decreases compared with single GPU training. Block-wise model updating filter (BMUF) is another successful effort in parallel training with linear speedup as well. It can achieve no-degradation of recognition accuracy with multi-GPUs [15]. In the model average method, aggregated model \u03b8\u0304(t) is computed and\nbroadcasted to GPUs. On the basis of it, BMUF proposed a novel model updating strategy:\n\u03b8\u0304(t) = 1\nN\nN\u2211\ni=1\n\u03b8i\nG(t) = \u03b8\u0304(t)\u2212 \u03b8g(t\u2212 1)\n\u2206(t) = \u03b7t\u2206(t\u2212 1) + \u03b6tG(t)\nWhere G(t) denotes model update, and \u2206(t) is the globalmodel update. There are two parameters in BMUF, block momentum \u03b7, and block learning rate \u03b6. Then, the global model is updated as\n\u03b8g(t) = \u03b8g(t\u2212 1) + \u2206(t)\nConsequently, \u03b8g(t) is broadcasted to all GPUs to initial their local models, instead of \u03b8\u0304(t) in model average method."}, {"heading": "2.4.2. Exponential Moving Average Model", "text": "Averaged SGD is proposed to further accelerate the convergence speed of SGD. Averaged SGD leverages the moving average (MA) \u03b8\u0304 as the estimator of \u03b8\u2217 [16]:\n\u03b8\u0304t = 1\nt\nt\u2211\n\u03c4=1\n\u03b8\u03c4\nWhere \u03b8\u03c4 is computed by model averaging or BMUF. It is shown that \u03b8\u0304t can well converge to \u03b8 \u2217, with the large enough training dataset in single GPU training. It can be considered as a non-interference strategy that \u03b8\u0304t does not participate the main optimization process, and only takes effect after the end of entire optimization. However, for the parallel training implementation, each \u03b8\u03c4 is computed by model averaging and BMUF with multiple models, and moving average model \u03b8\u0304t does not well converge, compared with single GPU training.\nModel averaging based methods are employed in parallel training of large scale dataset, because of their faster convergence, and especially no-degradation implementation of BMUF. But combination of model averaged based methods and moving average does not match the expectation of further enhance performance and it is presented as\n\u03b8\u0304gt = 1\nt\nt\u2211\n\u03c4=1\n\u03b8g\u03c4\nThe weight of each \u03b8gt is equal in moving average method regardless the effect of temporal order. But t closer to the end of training achieve higher accuracy in the model averaging based approach, and thus it should be with more proportion in final \u03b8\u0304g. As a result, exponential moving average(EMA) is appropriate, which the weight for each older parameters decrease exponentially, and never reaching zero. After moving\naverage based methods, the EMA parameters are updated recursively as\n\u03b8\u0302gt = \u03b1\u03b8\u0302gt\u22121 + (1\u2212 \u03b1)\u03b8gt\nHere \u03b1 represents the degree of weight decrease, and called exponential updating rate. EMA is also a non-interference training strategy that is implemented easily, as the updated model is not broadcasted. Therefore, there is no need to add extra learning rate updating approach, as it can be appended to existing training procedure directly."}, {"heading": "3. DEPLOYMENT", "text": "There is a high real time requirement in real world application, especially in online voice search system. Shenma voice search is one of the most popular mobile search engines in China, and it is a streaming service that intermediate recognition results displayed while users are still speaking. Unidirectional LSTM network is applied, rather than bidirectional one, because it is well suited to real-time streaming speech recognition."}, {"heading": "3.1. Distillation", "text": "It is demonstrated that deep neural network architecture can achieve improvement in LVCSR. However, it also leads to much more computation and higher RTF, so that the recognition result can not be displayed in real time. It should be noted that deeper neural network contains more knowledge, but it is also cumbersome. the knowledge is key to improve the performance. If it can be transfered from cumbersome model to a small model, the recognition ability can also be transfered to the small model. Knowledge transferring to small model is called distillation [20]. The small model can perform as well as cumbersome model, after distilling. It provide a way to utilize high-performance but high RTF model in real time system. The class probability produced by the cumbersome model is regarded as soft target, and the generalization ability of cumbersome model is transfered to small model with it. Distillation is model\u2019s knowledge transferring approach, so there is no need to use the hard target, which is different with the layer-wise training method."}, {"heading": "3.2. Transfer Learning with sMBR", "text": "For a certain specific scenario, the model trained with the data recorded from it has better adaptation than the model trained with generic scenario. But it spends too much time training a model from the beginning, if there is a well-trained model for generic scenarios. Moreover, labeling a large quantity of training data in new scenario is both costly and time consuming. If a model transfer trained with smaller dataset can obtained the similar recognition accuracy compared with the model directly trained with larger dataset, it is no doubt that transfer learning is more practical. Since specific scenario is\na subset of generic scenario, some knowledge can be shared between them. Besides, generic scenario consists of various conditions, so its model has greater robustness. As a result, not only shared knowledge but also robustness can be transfered from the model of generic scenario to the model of specific one.\nAs the model well trained from generic scenario achieves good performance in frame level classification, sequence discriminative training is required to adapt new model to specific scenario additionally. Moreover, it does not need alignment from HMM-GMM system, and it also saves amount of time to prepare alignment."}, {"heading": "4. EXPERIMENTS", "text": ""}, {"heading": "4.1. Training Data", "text": "A large quantity of labeled data is needed for training a more accurate acoustic model. We collect the 17000 hours labeled data from Shenma voice search, which is one of the most popular mobile search engines in China. The dataset is created from anonymous online users\u2019 search queries in Mandarin, and all audio file\u2019s sampling rate is 16kHz, recorded by mobile phones. This dataset consists of many different conditions, such as diverse noise even low signal-to-noise, babble, dialects, accents, hesitation and so on.\nIn the Amap, which is one of the most popular web mapping and navigation services in China, users can search locations and navigate to locations they want though voice search. To present the performance of transfer learning with sequence discriminative training, the model trained from Shenma voice search which is greneric scenario transfer its knowledge to the model of Amap voice search. 7300 hours labeled data is collected in the similar way of Shenma voice search data collection.\nTwo dataset is divided into training set, validation set and test set separately, and the quantity of them is shown in Table 1. The three sets are split according to speakers, in order to avoid utterances of same speaker appearing in three sets simultaneously. The test sets of Shenma and Amap voice search are called Shenma Test and Amap Test."}, {"heading": "4.2. Experimental setup", "text": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [27, 24]. Shenma and Amap voice search is a streaming service that intermediate recognition results displayed while users are still speaking. So as for online recognition in real time, we prefer unidirectional LSTM model rather than bidirectional one. Thus, the training system is unidirectional LSTM-based.\nA 26-dimensional filter bank and 2-dimensional pitch feature is extracted for each frame, and is concatenated with first and second order difference as the final input of the network. The super frame are stacked by 3 frames without overlapping. The architecture we trained consists of two LSTM layers with sigmoid activation function, followed by a fullconnection layer. The out layer is a softmax layer with 11088 hidden markov model (HMM) tied-states as output classes, the loss function is cross-entropy (CE). The performancemetric of the system in Mandarin is reported with character error rate (CER). The alignment of frame-level ground truth is obtained by GMM-HMM system. Mini-batched SGD is utilized with momentum trick and the network is trained for a total of 4 epochs. The block learning rate and block momentum of BMUF are set as 1 and 0.9. 5-gram language model is leveraged in decoder, and the vocabulary size is as large as 760000. Differentials of recurrent layers is limited to range [-10000,10000], while gradients are clipped to range [-5, 5] and cell activations clipped to range [-50, 50]. After training with CE loss, sMBR loss is employed to further improve the performance.\nIt has shown that BMUF outperforms traditional model averaging method, and it is utilized at the synchronization phase. After synchronizing with BMUF, EMA method further updates the model in non-interference way. The training system is deployed on the MPI-based HPC cluster where 8 GPUs. Each GPU processes non-overlap subset split from the entire large scale dataset in parallel.\nLocal models from distributed workers synchronize with each other in decentralized way. In the traditional model averaging and BMUF method, a parameter server waits for all workers to send their local models, aggregate them, and send the updated model to all workers. Computing resource of workers is wasted until aggregation of the parameter server done. Decentralized method makes full use of computing resource, and we employ the MPI-based Mesh AllReduce method. It is mesh topology as shown in Figure 1. There is no centralized parameter server, and peer to peer communication is used to transmit local models between workers. Local model \u03b8i of i-th worker in N workers cluster is split to N pieces \u03b8i,j j = 1 \u00b7 \u00b7 \u00b7N , and send to corresponding worker. In the aggregation phase, j-th worker computed N splits of model \u03b8i,j i = 1 \u00b7 \u00b7 \u00b7N and send updated model \u03b8\u0304gj back to\nworkers. As a result, all workers participate in aggregation and no computing resource is dissipated. It is significant to promote training efficiency, when the size of neural network model is too large. The EMA model is also updated additionally, but not broadcasting it."}, {"heading": "5. RESULTS", "text": "In order to evaluate our system, several sets of experiments are performed. The Shenma test set including about 9000 samples and Amap test set including about 7000 samples contain various real world conditions. It simulates the majority of user scenarios, and can well evaluates the performance of a trained model. Firstly, we show the results of models trained with EMA method. Secondly, for real world applications, very deep LSTM is distilled to a shallow one, so as for lower RTF. The model of Amap is also needed to train for map and navigation scenarios. The performance of transfer learning from Shenma voice search to Amap voice search is also presented."}, {"heading": "5.1. Layer-wise Training", "text": "In layer-wise training, the deeper model learns both parameters and knowledge from the shallower model. The deeper model is initialized by the shallower one, and its alignment is the combination of hard target and soft target of shallower one. Two targets have the same weights in our framework. The teacher model is trained with CE. For each layer-wise trained CE model, corresponding sMBR model is also trained, as sMBR could achieve additional improvement. In our framework, 1000 hours data is randomly selected from\nthe total dataset for sMBR training. There is no obvious performance enhancement when the size of sMBR training dataset increases.\nFor very deep unidirectional LSTM initialized with Xavier initialization algorithm, 6-layers model converges well, but there is no further improvement with increasing the number of layers. Therefore, the first 6 layers of 7-layers model is initialized by 6-layers model, and soft target is provided by 6-layers model. Consequently, deeper LSTM is also trained in the same way. It should be noticed that the teacher model of 9-layers model is the 8-layers model trained by sMBR, while the other teacher model is CE model. As shown in Table 2, the layer-wise trained models always performs better than the models with Xavier initialization, as the model is deep. Therefore, for the last layer training, we choose 8-layers sMBR model as the teacher model instead of CE model. A comparison between 6-layers and 9-layers sMBR models shows that 3 additional layers of layer-wise training brings relative 12.6% decreasing of CER. It is also significant that the averaged CER of sMBR models with different layers decreases absolute 0.73% approximately compared with CE models, so the improvement of sequence discriminative learning is promising."}, {"heading": "5.2. Distillation", "text": "9-layers unidirectional LSTM model achieves outstanding performance, but meanwhile it is too computationally expensive to allow deployment in real time recognition system. In order to ensure real-time of the system, the number of layers needs to be reduced. The shallower network can learn the knowledge of deeper network with distillation. It is found that RTF of 2-layers network is acceptable, so the knowledge is distilled from 9-layers well-trained model to 2-layers model. Table 3 shows that distillation from 9-layers to 2-layers brings RTF decrease of relative 53%, while CER only increases 5%. The knowledge of deep network is almost transfered with distillation, Distillation brings promising RTF reduction, but only little knowledge of deep network is lost. Moreover, CER of 2-layers distilled LSTM decreases relative 14%, compared with 2-layers regular-trained LSTM."}, {"heading": "5.3. Transfer Learning", "text": "2-layers distilled model of Shenma voice search has shown a impressive performance on Shenma Test, and we call it Shenma model. It is trained for generic search scenario, but it has less adaptation for specific scenario like Amap voice search. Training with very large dataset using CE loss is regarded as improvement of frame level recognition accuracy, and sMBR with less dataset further improves accuracy as sequence discriminative training. If robust model of generic scenario is trained, there is no need to train a model with very large dataset, and sequence discriminative training with less dataset is enough. Therefore, on the basis of Shenmamodel, it is sufficient to train a new Amap model with small dataset using sMBR. As shown in Table 4, Shenma model presents the worst performance among three methods, since it does not trained for Amap scenario. 2-layers Shenma model further trained with sMBR achieves about 8.1% relative reduction, compared with 2-layers regular-trained Amap model. Both training sMBR datasets contain the same 1000 hours data. As a result, with the Shenma model, only about 14% data usage achieves lower CER, and it leads to great time and cost saving with less labeled data. Besides, transfer learning with sMBR does not use the alignment from the HMM-GMM system, so it also saves huge amount of time."}, {"heading": "6. CONCLUSION", "text": "We have presented a whole deep unidirectional LSTM parallel training system for LVCSR. The recognition performance improves when the network goes deep. Distillation makes it possible that deep LSTM model transfer its knowledge to shallow model with little loss. The model could be distilled to 2-layers model with very low RTF, so that it can display the immediate recognition results. As a result, its CER decrease relatively 14%, compared with the 2-layers regular trained model. In addition, transfer learning with sMBR is also pro-\nposed. If a great model has well trained from generic scenario, only 14% of the size of training dataset is needed to train a more accuracy acoustic model for specific scenario. Our future work includes 1) finding more effective methods to reduce the CER by increasing the number of layers; 2) applying this training framework to Connectionist Temporal Classification (CTC) and attention-based neural networks."}, {"heading": "7. REFERENCES", "text": "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, \u201cDeep residual learning for image recognition,\u201d arXiv preprint arXiv:1512.03385, 2015.\n[2] Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl,\nAbdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\n[3] Alex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton, \u201cSpeech recognition with deep recurrent neural networks,\u201d in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.\n[4] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-\nhamed, \u201cHybrid speech recognition with deep bidirectional lstm,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.\n[5] Hasim Sak, AndrewW Senior, and Franc\u0327oise Beaufays,\n\u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling.,\u201d in INTERSPEECH, 2014, pp. 338\u2013342.\n[6] Guillaume Alain and Yoshua Bengio, \u201cUnderstanding\nintermediate layers using linear classifier probes,\u201d arXiv preprint arXiv:1610.01644, 2016.\n[7] Geoffrey E Hinton and Ruslan R Salakhutdinov, \u201cRe-\nducing the dimensionality of data with neural networks,\u201d Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.\n[8] Xavier Glorot and Yoshua Bengio, \u201cUnderstanding\nthe difficulty of training deep feedforward neural networks.,\u201d in Aistats, 2010, vol. 9, pp. 249\u2013256.\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun, \u201cDelving deep into rectifiers: Surpassing humanlevel performance on imagenet classification,\u201d in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.\n[10] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen,\nMatthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc V Le, et al., \u201cLarge scale distributed deep networks,\u201d in Advances in neural information processing systems, 2012, pp. 1223\u20131231.\n[11] Shanshan Zhang, Ce Zhang, Zhao You, Rong Zheng,\nand Bo Xu, \u201cAsynchronous stochastic gradient descent for dnn training,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663.\n[12] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal\nJozefowicz, \u201cRevisiting distributed synchronous sgd,\u201d arXiv preprint arXiv:1604.00981, 2016.\n[13] Ryan McDonald, Keith Hall, and Gideon Mann, \u201cDis-\ntributed training strategies for the structured perceptron,\u201d in Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.\n[14] Martin Zinkevich, Markus Weimer, Lihong Li, and\nAlex J Smola, \u201cParallelized stochastic gradient descent,\u201d in Advances in neural information processing systems, 2010, pp. 2595\u20132603.\n[15] Kai Chen and Qiang Huo, \u201cScalable training of deep\nlearning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.\n[16] Boris T Polyak and Anatoli B Juditsky, \u201cAcceleration of\nstochastic approximation by averaging,\u201d SIAM Journal on Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992.\n[17] Wei Xu, \u201cTowards optimal one pass large scale learn-\ning with averaged stochastic gradient descent,\u201d arXiv preprint arXiv:1107.2490, 2011.\n[18] Tian Xu, Zhang Jun, Ma Zejun, He Yi, and Wei Juan,\n\u201cExponential moving average model in parallel speech recognition training,\u201d arXiv preprint arXiv:1703.01024, 2017.\n[19] Has\u0327im Sak, Andrew Senior, Kanishka Rao, and\nFranc\u0327oise Beaufays, \u201cFast and accurate recurrent neural network acoustic models for speech recognition,\u201d arXiv preprint arXiv:1507.06947, 2015.\n[20] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, \u201cDistill-\ning the knowledge in a neural network,\u201d arXiv preprint arXiv:1503.02531, 2015.\n[21] Geoffrey E Hinton, Simon Osindero, and Yee-Whye\nTeh, \u201cA fast learning algorithm for deep belief nets,\u201d Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.\n[22] Brian Kingsbury, \u201cLattice-based optimization of se-\nquence classification criteria for neural-network acoustic modeling,\u201d in Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 3761\u20133764.\n[23] Has\u0327im Sak, Andrew Senior, Kanishka Rao, Ozan Irsoy,\nAlex Graves, Franc\u0327oise Beaufays, and Johan Schalkwyk, \u201cLearning acoustic frame labeling for speech recognition with recurrent neural networks,\u201d in Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.\n[24] Has\u0327im Sak, Fe\u0301lix de Chaumont Quitry, Tara Sainath,\nKanishka Rao, et al., \u201cAcoustic modelling with cd-ctcsmbr lstm rnns,\u201d in Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.\n[25] Yves Normandin, Hidden Markov models, maximum\nmutual information estimation, and the speech recognition problem, Ph.D. thesis, McGill University, Montreal, 1991.\n[26] Daniel Povey, Discriminative training for large vocab-\nulary speech recognition, Ph.D. thesis, University of Cambridge, 2005.\n[27] Michiel Hermans and Benjamin Schrauwen, \u201cTraining\nand analysing deep recurrent neural networks,\u201d in Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198."}], "references": [{"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alex Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "AndrewW Senior", "Fran\u00e7oise Beaufays"], "venue": "IN- TERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding intermediate layers using linear classifier probes", "author": ["Guillaume Alain", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1610.01644, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026\u20131034.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2012, pp. 1223\u20131231.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Asynchronous stochastic gradient descent for dnn training", "author": ["Shanshan Zhang", "Ce Zhang", "Zhao You", "Rong Zheng", "Bo Xu"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 6660\u20136663.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Revisiting distributed synchronous sgd", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed training strategies for the structured perceptron", "author": ["Ryan McDonald", "Keith Hall", "Gideon Mann"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010, pp. 456\u2013464.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J Smola"], "venue": "Advances in neural information processing systems, 2010, pp. 2595\u20132603.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise modelupdate filtering", "author": ["Kai Chen", "Qiang Huo"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5880\u20135884.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["Boris T Polyak", "Anatoli B Juditsky"], "venue": "SIAM Journal on Control and Optimization, vol. 30, no. 4, pp. 838\u2013 855, 1992.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1992}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["Wei Xu"], "venue": "arXiv preprint arXiv:1107.2490, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Exponential moving average model in parallel speech recognition training", "author": ["Tian Xu", "Zhang Jun", "Ma Zejun", "He Yi", "Wei Juan"], "venue": "arXiv preprint arXiv:1703.01024, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["Brian Kingsbury"], "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, 2009, pp. 3761\u20133764.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning acoustic frame labeling for speech recognition with recurrent neural networks", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Ozan Irsoy", "Alex Graves", "Fran\u00e7oise Beaufays", "Johan Schalkwyk"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015, pp. 4280\u20134284.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modelling with cd-ctcsmbr lstm rnns", "author": ["Ha\u015fim Sak", "F\u00e9lix de Chaumont Quitry", "Tara Sainath", "Kanishka Rao"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2015 IEEE Workshop on. IEEE, 2015, pp. 604\u2013609.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Hidden Markov models, maximum mutual information estimation, and the speech recognition", "author": ["Yves Normandin"], "venue": "problem, Ph.D. thesis,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1991}, {"title": "Discriminative training for large vocabulary speech recognition", "author": ["Daniel Povey"], "venue": "Ph.D. thesis, University of Cambridge,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Increasing the depth of neural network is a effective way to improve the performance, and convolutional neural network (CNN) has benefited from it in visual recognition task[1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 2, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 3, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 4, "context": "Deeper long short-term memory (LSTM) recurrent neural networks (RNNs) are also applied in large vocabulary continuous speech recognition (LVCSR) task, because LSTM networks have shown better performance than Fully-connected feed-forward deep neural network[2, 3, 4, 5].", "startOffset": 256, "endOffset": 268}, {"referenceID": 5, "context": "A conceptual tool called linear classifier probe is introduced to better understand the dynamics inside a neural network [6].", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "Layer-wise pre-training is a successful method to train very deep neural networks [7].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "The convergence becomes harder with increasing the number of layers, even though the model is initialized with Xavier or its variants [8, 9].", "startOffset": 134, "endOffset": 140}, {"referenceID": 8, "context": "The convergence becomes harder with increasing the number of layers, even though the model is initialized with Xavier or its variants [8, 9].", "startOffset": 134, "endOffset": 140}, {"referenceID": 9, "context": "Asynchronous SGD is a successful effort for parallel training based on it [10, 11].", "startOffset": 74, "endOffset": 82}, {"referenceID": 10, "context": "Asynchronous SGD is a successful effort for parallel training based on it [10, 11].", "startOffset": 74, "endOffset": 82}, {"referenceID": 11, "context": "Besides, synchronous SGD is another effective effort, where the parameter server waits for every works to finish their computation and sent their local models to it, and then it sends updated model back to all workers [12].", "startOffset": 218, "endOffset": 222}, {"referenceID": 12, "context": "Model averaging method achieves linear speedup, as the final model is averaged from all parameters of local models in different workers [13, 14], but the accuracy decreases compared with single GPU training.", "startOffset": 136, "endOffset": 144}, {"referenceID": 13, "context": "Model averaging method achieves linear speedup, as the final model is averaged from all parameters of local models in different workers [13, 14], but the accuracy decreases compared with single GPU training.", "startOffset": 136, "endOffset": 144}, {"referenceID": 14, "context": "It can achieve improvement or no-degradation of recognition performance compared with mini-batch SGD on single GPU [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "It is demonstrated that the moving average of the parameters obtained by SGD performs as well as the parameters that minimize the empirical cost, and moving average parameters can be used as the estimator of them, if the size of training data is large enough [16].", "startOffset": 259, "endOffset": 263}, {"referenceID": 16, "context": "One pass learning is then proposed, which is the combination of learning rate schedule and averaged SGD using moving average [17].", "startOffset": 125, "endOffset": 129}, {"referenceID": 17, "context": "Exponential moving average (EMA) is proposed as a noninterference method[18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 18, "context": "Frame stacking can also speed up the training time [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "As a result, the knowledge of deep model can be distilled to a shallow model [20].", "startOffset": 77, "endOffset": 81}, {"referenceID": 7, "context": "Xavier initialization can partially solve this problem [8], so this method is the regular initialization method of all training procedure.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Instead, layer-wise pre-training method is a effective way to train the weights of very deep architecture[7, 21].", "startOffset": 105, "endOffset": 112}, {"referenceID": 20, "context": "Instead, layer-wise pre-training method is a effective way to train the weights of very deep architecture[7, 21].", "startOffset": 105, "endOffset": 112}, {"referenceID": 4, "context": "The objects of conventional saturation check are gradients and the cell activations [5].", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "Gradients are clipped to range [-5, 5],", "startOffset": 31, "endOffset": 38}, {"referenceID": 21, "context": "In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24].", "startOffset": 148, "endOffset": 160}, {"referenceID": 22, "context": "In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24].", "startOffset": 148, "endOffset": 160}, {"referenceID": 23, "context": "In contrast, sequence discriminative training criterion has shown to further improve performance of neural network first trained with cross-entropy [22, 23, 24].", "startOffset": 148, "endOffset": 160}, {"referenceID": 21, "context": "We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26].", "startOffset": 47, "endOffset": 51}, {"referenceID": 24, "context": "We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26].", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "We choose state-level minimum bayes risk (sMBR)[22] among a number of sequence discriminative criterion is proposed, such as maximum mutual information (MMI) [25] and minimum phone error (MPE) [26].", "startOffset": 193, "endOffset": 197}, {"referenceID": 9, "context": "Therefore, parallel training with multiple GPUs is essential, and it makes use of data parallelism [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "All GPUs synchronize their local models with model average method after a mini-batch optimization [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 13, "context": "All GPUs synchronize their local models with model average method after a mini-batch optimization [13, 14].", "startOffset": 98, "endOffset": 106}, {"referenceID": 14, "context": "It can achieve no-degradation of recognition accuracy with multi-GPUs [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "Averaged SGD leverages the moving average (MA) \u03b8\u0304 as the estimator of \u03b8\u2217 [16]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 19, "context": "Knowledge transferring to small model is called distillation [20].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [27, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 23, "context": "LSTM RNNs outperform conventional RNNs for speech recognition system, especially deep LSTM RNNs, because of its long-range dependencies more accurately for temporal sequence conditions [27, 24].", "startOffset": 185, "endOffset": 193}, {"referenceID": 4, "context": "Differentials of recurrent layers is limited to range [-10000,10000], while gradients are clipped to range [-5, 5] and cell activations clipped to range [-50, 50].", "startOffset": 107, "endOffset": 114}], "year": 2017, "abstractText": "Recurrent neural networks (RNNs), especially long shortterm memory (LSTM) RNNs, are effective network for sequential task like speech recognition. Deeper LSTM models perform well on large vocabulary continuous speech recognition, because of their impressive learning ability. However, it is more difficult to train a deeper network. We introduce a training framework with layer-wise training and exponential moving average methods for deeper LSTM models. It is a competitive framework that LSTM models of more than 7 layers are successfully trained on Shenma voice search data in Mandarin and they outperform the deep LSTM models trained by conventional approach. Moreover, in order for online streaming speech recognition applications, the shallow model with low real time factor is distilled from the very deep model. The recognition accuracy have little loss in the distillation process. Therefore, the model trained with the proposed training framework reduces relative 14% character error rate, compared to original model which has the similar real-time capability. Furthermore, the novel transfer learning strategy with segmental Minimum Bayes-Risk is also introduced in the framework. The strategy makes it possible that training with only a small part of dataset could outperform full dataset training from the beginning.", "creator": "LaTeX with hyperref package"}}}