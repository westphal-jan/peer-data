{"id": "1703.07713", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Hierarchical RNN with Static Sentence-Level Attention for Text-Based Speaker Change Detection", "abstract": "robust speaker personality detection recognition dialogues introduces uncertainty based among audio input. upon some tasks, consequently, researchers can rapidly improve clarity, and do nonetheless determine access toward raw cue signals. eventually, utilizing the increasingly density of communication processing processing, text - based dialogue understanding is attracting more problems in the community. challenges raise the problem preventing dialect - based user change detection. presenting this paper, psychologists formulate the task language auditory matching problem interpreting utterances before and after a certain input point ; several propose a spoken structure framing capacity ( acc ) with static sentence - level attention. our model comprises three input components : a sentence encoder with a specialized short term memory ( pg ) - context hierarchy, a retrieval encoder around binary lstm - rnn, and a parallel sentence - structure context meter, which blocks rich information interaction. detailed results show that neural networks consistently assure better performance than feature - fill circuits, and that our attention - based system significantly outperforms self - attention finding networks.", "histories": [["v1", "Wed, 22 Mar 2017 15:42:28 GMT  (948kb,D)", "http://arxiv.org/abs/1703.07713v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhao meng", "lili mou", "zhi jin"], "accepted": false, "id": "1703.07713"}, "pdf": {"name": "1703.07713.pdf", "metadata": {"source": "CRF", "title": "Hierarchical RNN with Static Sentence-Level Attention for Text-Based Speaker Change Detection", "authors": ["Zhao Meng", "Lili Mou", "Zhi Jin"], "emails": ["zhaomeng.pku@outlook.com,", "doublepower.mou@gmail.com,", "zhijin@sei.pku.edu.cn"], "sections": [{"heading": null, "text": "Index Terms: speaker change detection, recurrent neural network, sentence-level attention"}, {"heading": "1. Introduction", "text": "Speaker change detection, or sometimes known as speaker segmentation, aims to find changing points of speakers in a dialogue. Specifically, a speaker change occurs when the current and the next sentences are not uttered by the same speaker [1]. Detecting speaker changes plays an important role in dialogue processing, and is a premise of speaker clustering [2], dialogue understanding [3], etc.\nTraditional speaker change detection typically uses audio input [4, 5]. However, researchers may not always have access to these raw audio signals. Further, the fast development of dialogue analysis puts high demands on understanding textual data [6, 7], as human-computer conversation involves deep semantics, requiring complicated natural language processing (NLP). Vinyals et al. [6] and Li et al. [7], for example, train sequence-to-sequence neural networks to automatically generate replies in an open-domain dialogue system. They use OpenSubtitle [8] as the corpus, but assume every two consecutive sentences are uttered by different speakers (which brings much noise to their training data). Therefore, text-based speaker change detection is important to dialogue analysis, being the main focus of this paper.\nUsing only text to detect speaker changes raises new challenges. Previous audio-based studies depend largely on acoustic features, e.g., pitch [9] and silence points [10], which provide\n1We release our code (including both data pre-processing and neural models) at https://sites.google.com/site/scdrnn/\nmuch information indicating the change of speakers. With textual features alone, we need deeper semantic understanding of natural language utterances.\nIn this paper, we formulate text-based speaker change detection as a binary sentence-pair classification problem, that is, we would like to judge whether the speaker is changing between each consecutive sentence pair (which we call a decision point). We also take into consideration previous and future sentences around the current decision point as context (Figure 1), serving as additional evidence.2\nWe propose a hierarchical RNN with static sentence-level attention for text-based speaker change detection. First, we use a long short term memory (LSTM)-based recurrent neural network (RNN) to capture the meaning of each sentence. Another LSTM-RNN integrates sentence information into a vector, before and after the decision point, respectively; the two vectors are combined for prediction. To better explore the context, we further apply an attention mechanism [11] over sentences to focus on relevant information during context integration. Our attention is static in that only the nearest two sentences around the decision point search for relevant information; it differs from previous research that uses dynamic attention [11, 12], which buries more important sentences under less important context. Compared with word-level attention [12, 13], our sentence-level attention is more efficient because there could be hundreds of words in the context within only a few sentences.\nOur model was evaluated on transcripts of nearly 3,000 episodes of TV talk shows. We crawled data from the Cable News Network website, and extracted dialogue utterances from original html files; speaker identities are also tagged in the corpus, serving as labels. In our experiments, modern neural networks consistently outperform traditional methods that use handcrafted features. Ablation tests confirm the effectiveness of context; the proposed hierarchical RNN with sentence-level static attention can better utilize such contextual information, and significantly outperforms non-attention neural networks. The results show that our tailored model is especially suited to the task of text-based speaker change detection."}, {"heading": "2. Related Work", "text": "Traditional speaker change detection deals with audio input and is a key step for speaker diarization (determining \u201cwho spoke when?\u201d) [1]. A typical approach is to compare consecutive sliding windows of input\u2014with spectral features [14], say\u2014 using metrics including the Bayesian information criterion [15], generalized likelihood ratio [16], and Kullback-Leibler diver-\n2Because our task is based on text (and is not online speaker change detection), we actually have access to the \u201cfuture context\u201d after the decision point.\nar X\niv :1\n70 3.\n07 71\n3v 1\n[ cs\n.C L\n] 2\n2 M\nar 2\n01 7\ngence [17]. Our paper differs from the above work and focuses on textual input, which is useful in various scenarios, for example, processing dialogue transcripts where speaker identities are missing (e.g., OpenSubtitle) [7], and enhancing audio speaker change detection with textual information [18].\nNowadays, text-based dialogue analysis has been increasingly important, as surface acoustic features are insufficient for semantic understanding in conversations. Previous research has addressed a variety of tasks, ranging from topic tracking [19], dialogue act classification [20], slot filling [21], to user intent modeling [22]. In our previous study, we address the problem of session segmentation in text-based human-computer conversations [23]. Without enough annotated data, we apply a heuristic matching approach; thus the task is unsupervised. Likewise, Li et al. [18] enhance audio-based speaker change detection with transcribed text, and they are also in the unsupervised regime. By contrast, this paper adopts a supervised setting as we have obtained massive, high-quality labels of speaker identities from the Cable News Network website.\nAs described in Section 1, we formulate our task as a sentence-pair classification problem. Previous studies utilize convolutional/recurrent neural networks (CNNs/RNNs) to detect the relationship (e.g., paraphrase and logical entailment) between two sentences [24, 25, 26]; Rockta\u0308schel et al. [27] equip RNN with attention mechanisms. These studies do not consider contextual information. In a dialogue system, Yan et al. [28] rerank candidate replies based on a user-issued query; they enhance sentence-pair modeling with previous utterances as context. In our scenario, the context appears on both sides of the decision point, and we carefully design the neural architecture to better use such contextual information."}, {"heading": "3. Approach", "text": "In this section, we describe the proposed approach in detail. Figure 1 illustrates our neural architecture, which consists of three parts. \u2022 First, an LSTM-based RNN encodes each individual sen-\ntence, around the decision point, into a real-valued vector (Figure 1a and Subsection 3.1). \u2022 Then another LSTM-based RNN integrates contextual in-\nformation by going through the sentence vectors. This applies to both sides of the current decision point (Figure 1b and Subsection 3.2). \u2022 We also add a static sentence-level attention mechanism to better utilize context by focusing on more relevant information (Figure 1c and Subsection 3.3)."}, {"heading": "3.1. Sentence Encoder", "text": "We use a recurrent neural network (RNN) with long short term memory (LSTM) units to encode a sentence as a vector (also known as a sentence embedding), shown in Figure 1a.\nAn RNN is suited for processing sequential data (e.g., a sentence consisting of several words), as it keeps a hidden state, changing at each time step based on its previous state and the current input. But vanilla RNNs with perceptron-like hidden states suffer from the problem of vanishing or exploding gradients [29, 30], being less effective to model long dependencies. LSTM units [31] alleviate the problem by better balancing input and its previous state with gating mechanisms.\nFormally, let x(t) be the embedding of the t-th word in a sentence, and h(t\u22121) be the last step\u2019s hidden state. We have\ni(t) = \u03c3(Wix (t) + Uih (t\u22121) + bi) (1)\nf (t) = \u03c3(Wfx (t) + Ufh (t\u22121) + bf ) (2)\no(t) = \u03c3(Wox (t) + Uoh (t\u22121) + bo) (3)\ng(t) = tanh(Wgx (t) + Ugh (t\u22121) + bg) (4)\nc(t) = i(t) \u2297 g(t) + c(t\u22121) \u2297 f (t) (5)\nh(t) = o(t) \u2297 tanh(c(t)) (6)\nwhere W \u2019s and U \u2019s are weights, and b\u2019s are bias terms. \u2297 denotes element-wise product, \u03c3 the sigmoid function. i(t), f (t), and o(t) are known as gates, andh(t) is the current step\u2019s hidden state. For convenience, we use LSTM\u2019s final state (corresponding to the last word in a sentence) as the sentence embedding."}, {"heading": "3.2. Context Encoder", "text": "Another LSTM-RNN encodes contextual information over sentence vectors, shown in Figure 1b. Since we model our task as\na matching problem, we apply the RNN to the decision point\u2019s both sides separately, the resulting vectors of which are concatenated as an input of prediction.\nIt should be noticed that our LSTM-RNN goes from faraway sentences to the nearest ones (called critical sentences) on both sides of the current decision point. We observe that nearer sentences play a more important role for prediction; that an RNN is better at keeping recent input information by its nature. Hence, our treatment is appropriate.\nBesides, our neural network is hierarchical in that it composites sentences with words, and discourses3 with sentences. It is similar to the hierarchical autoencoder in [32]. Other studies apply a single RNN over a discourse, also achieving high performance in tasks like sentiment analysis [33] and machine comprehension [34]. In our scenario, however, the sentences (either context or critical ones) are not necessarily uttered by a single speaker. Experiments in Section 4.3 show that hierarchical models are more suitable for speaker change detection."}, {"heading": "3.3. Our Attention Mechanism", "text": "We use an attention mechanism to better utilize contextual information. Attention-based neural networks are first proposed to dynamically focus on relevant words of the source sentence in machine translation [11]. In our scenario, we would like to match a critical sentence with all utterances on the other side of the decision point (Figure 1c). That is to say, the attention mechanism is applied to the sentence level, different from other work that uses word-level attention [12, 13]. Our method is substantially more efficient because a context of several utterances could contain hundreds of words.\nConsidering t sentences (context size being t \u2212 1) before and after the decision point, respectively, we have 2t sentences in total, namely\ns(1)p \u2192s(2)p \u2192\u00b7 \u00b7 \u00b7\u2192s(t)p s (t) f \u2190\u00b7 \u00b7 \u00b7\u2190s (2) f \u2190s (1) f (7) where subscripts p and f refer to previous and future utterances around the current decision point; the arrows indicate RNN\u2019s directions, as has been emphasized in Subsection 3.2.\nIn our attention mechanism, a critical sentence, e.g., s(t)p , focuses on all sentences on the other side of the decision point s (1) f , \u00b7 \u00b7 \u00b7 , s (t) f , and aggregates information weighted by a probabilistic distribution \u03b1p \u2208 Rt, i.e.,\n\u03b1\u0303(i)p = u T tanh ( W [s(t)p ; s (i) f ] )\n(8)\n\u03b1(i)p = softmax ( \u03b1\u0303(i)p ) =\nexp{\u03b1\u0303(i)p }\u2211t j=1 exp{\u03b1\u0303 (j) p }\n(9)\nHere, s(t)p is concatenated with s (i) f , processed by a two-layer perceptron (with parametersW andu). \u03b1\u0303i is a real-valued measure, normalized by the softmax activation function to give the probability \u03b1i. The aggregated information, known as an attention vector, is\nmp = t\u2211\ni=1\n\u03b1(i)p \u00b7 s (i) f (10)\nLikewise, the other critical sentence s(t)f yields an attention vector mf . They are concatenated along with LSTM\u2019s output of the critical sentences for prediction. In other words, the input of softmax is [s(t)p ; s (t) f ;mp;mf ].\n3A discourse can be viewed as a sequence of sentences.\nIt should be pointed out that, our attention is static, as only critical sentences search for relevant information using Equations (8)\u2013(10). It resembles a variant in [27], but differs from common attention where two LSTMs interact dynamically along their propagations (Figure 1d). Such approach, however, may bury the critical sentences; it leads to slight performance degradation in our experiment, as we shall see in Section 4.3. By contrast, our model puts more emphasis on the critical sentences."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Dataset Collection", "text": "In our experiments, we crawled transcripts of nearly 3,000 TV talk shows from the Cable News Network website4 (example shown in Figure 2), and extracted main content from the original html files. We used the Natural Language Toolkit5 to segment sentences and words. The transcripts contain speaker identities (highlighted by red rectangles), with which we induced speaker changes in the dialogue. Preprocessing code is released on our website (Footnote 1).\nThe crawled dataset comprises 1.5M utterances. We split training, validation, and test sets by episodes (TV shows) at a ratio of 8:1:1. In other words, each episode appears in either the training set, or the val/test sets. This prevents utterance overlapping between training and prediction, and thus is a more realistic setting than splitting by utterances. Table 1 presents relevant statistics.\nWe notice that, our corpus is larger than previous ones by magnitudes: the 1997 HUB4 dataset, for example, is 97 hours long, whereas our TV shows are estimated to be 3,000 hours,6 which are more suitable for training deep neural networks. In the dataset, speaker changes count to 25%. We thus used F1-measure in addition to accuracy as metrics. Here, F1 = 2P \u00b7 R/(P + R), where P = #correctly detected changes#detected changes is the precision, and R = #correctly detected changes#all changes is the recall."}, {"heading": "4.2. Settings", "text": "We set all neural layers, including word embeddings, to 200 dimensional. Since our dataset is large, we randomly initialized word embeddings, which were tuned during training. We used the Adam optimizer [35] with mini-batch update (batch size being 100). Other hyperparameters were chosen by validation: dropout rate from {0.1, 0.3} and initial learning rate from\n4http://transcripts.cnn.com 5http://www.nltk.org 6Each episode roughly lasts for an hour.\n{3\u00d7 10\u22124, 9\u00d7 10\u22124}. We also had several baselines with handcrafted features: we extracted unigram and bigram features of the critical sentences as two vectors, which are concatenated for prediction. We applied logistic regression and a 3-layer deep neural network (DNN) as the classifier; the former is a linear model whereas the latter is nonlinear. DNN\u2019s hidden dimension was set to 200, which is the same as our neural network.\nA convolutional neural network (CNN) is also included for comparison. It adopts a window size of 3, and a max-pooling layer aggregates extracted features. All competing neural models (including DNN and CNN) were tuned in the same manner, so our comparison is fair."}, {"heading": "4.3. Performance", "text": "Table 2 presents the performance of our model as well as baselines. As shown, all modern neural networks (CNNs/RNNs with word embeddings) are consistently better than methods using handcrafted features of unigrams and bigrams. Because we have applied a 3-layer DNN to these features, we believe the performance improvement is not merely caused by using a better classifier, but the automatic feature/representation learning nature of modern neural networks.\nFor neural network-based sentence encoders, we compared LSTM-RNN with CNN.7 Results show that LSTM-RNN outperforms CNN by 6% in terms of both accuracy and F1measure.\nTo cope with context, the simplest approach, perhaps, is to use a word-level RNN as above to go through surrounding utterances of the critical sentences, denoted as non-hierarchical in Table 2. Using contextual information yields an F1 improvement of 2%. This controlled experiment validates the usefulness of context in speaker change detection. The hierarchical RNN introduced in Sections 3.1 and 3.2 further improves the F1 measure by 3%. With sentence-level static attention, our model achieves the highest performance of 89.2% accuracy and 78.4% F1-measure.\nWe would like to have in-depth analysis regarding how the context size and different attention mechanisms affect our model. The context size was chosen by validation from {1, 2, 4, 8}.8 As shown in Figure 3, even a single context sentence (on each side of the decision point) improves the performance by 2%; with more surrounding utterances, the performance grows gradually. Moreover, attention-based neural networks significantly outperform non-attention models by a margin of 10%. We also tried a dynamic sentence-by-sentence attention mechanism, similar to most existing work [11, 12, 13].\n7CNN here should not be confused with the Cable News Network. 8Due to efficiency concerns, we did not try larger context sizes.\nAs analyzed in Section 3.3, such model buries critical sentences and thus slightly hurts the performance by 1\u20132% F1-measure (green dashed line in Figure 3). The experiments verify the effectiveness of our hierarchical RNN with sentence-level, static attention. Case study. Table 3 showcases a dialogue snippet. In the example, our model makes an error as it fails to detect the change between Sentences 4\u20135. However, it is even hard for humans to judge this particular change, because the word absolutely also goes fluently into the next speaker. For other utterances with more substance, the neural network correctly joins Sentences 1\u2013 2 and 5\u20136, as well as segments Sentences 2\u20133 and 3\u20134, showing that our proposed model can effectively capture the semantics of these sentences."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a static sentence-level attention LSTM-RNN for text-based speaker change detection. Our model uses an LSTM-RNN to encode each utterance into a vector, based on which another LSTM-RNN integrates contextual information, before and after a particular decision point, respectively. A static sentence-level attention mechanism is also applied to enhance information interaction.\nWe crawled dialogue transcripts from Cable News Network TV talk shows for evaluation. Experimental results demonstrate the effectiveness of our approach. In particular, in-depth analysis validates that contextual information is indeed helpful for speaker change detection, and that our tailored model can make better use of context than other neural networks."}, {"heading": "6. References", "text": "[1] X. Anguera Miro, S. Bozonnet, N. Evans, C. Fredouille, G. Fried-\nland, and O. Vinyals, \u201cSpeaker diarization: A review of recent research,\u201d IEEE Transactions on Audio Speech and Language Processing, vol. 20, no. 2, pp. 356\u2013370, 2012.\n[2] R. Sinha, S. E. Tranter, M. J. Gales, and P. C. Woodland, \u201cThe Cambridge University March 2005 speaker diarisation system,\u201d in Proceedings of INTERSPEECH, 2005, pp. 2437\u20132440.\n[3] N. Mrks\u030cic\u0301, D. O\u0301 Se\u0301aghdha, B. Thomson, M. Gasic, P.-H. Su, D. Vandyke, T.-H. Wen, and S. Young, \u201cMulti-domain dialog state tracking using recurrent neural networks,\u201d in Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 2, 2015, pp. 794\u2013799.\n[4] S. Chen and P. Gopalakrishnan, \u201cSpeaker, environment and channel change detection and clustering via the Bayesian information criterion,\u201d in Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, 1998, pp. 127\u2013132.\n[5] S. Kwon and S. S. Narayanan, \u201cSpeaker change detection using a new weighted distance measure,\u201d in Proceedings of INTERSPEECH, 2002, pp. 2537\u20132540.\n[6] O. Vinyals and Q. Le, \u201cA neural conversational model,\u201d in ICML Workshop, 2015.\n[7] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, \u201cA diversitypromoting objective function for neural conversation models,\u201d in Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 110\u2013119.\n[8] J. Tiedemann, \u201cNews from OPUS-A collection of multilingual parallel corpora with tools and interfaces,\u201d in Proceedings of Recent Advances in Natural Language Processing, 2009, pp. 237\u2013 248.\n[9] L. Lu and H.-J. Zhang, \u201cUnsupervised speaker segmentation and tracking in real-time audio content analysis,\u201d Multimedia Systems, vol. 10, no. 4, pp. 332\u2013343, 2005.\n[10] T. Kemp, M. Schmidt, M. Westphal, and A. Waibel, \u201cStrategies for automatic segmentation of audio data,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2000, pp. 1423\u20131426.\n[11] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine translation by jointly learning to align and translate,\u201d in Proceedings of the International Conference on Learning Representations, 2015.\n[12] T. Luong, H. Pham, and C. D. Manning, \u201cEffective approaches to attention-based neural machine translation,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1412\u20131421.\n[13] B. Liu and I. Lane, \u201cAttention-based recurrent neural network models for joint intent detection and slot filling,\u201d in Proceedings of INTERSPEECH, 2016, pp. 685\u2013689.\n[14] L. Lu and H.-J. Zhang, \u201cSpeaker change detection and tracking in real-time news broadcasting analysis,\u201d in Proceedings of the Tenth ACM International Conference on Multimedia, 2002, pp. 602\u2013610.\n[15] K. Mori and S. Nakagawa, \u201cSpeaker change detection and speaker clustering using VQ distortion for broadcast news speech recognition,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2001, pp. 413\u2013416.\n[16] H. Gish, M.-H. Siu, and R. Rohlicek, \u201cSegregation of speakers for speech recognition and speaker identification,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1991, pp. 873\u2013876.\n[17] M. A. Siegler, U. Jain, B. Raj, and R. M. Stern, \u201cAutomatic segmentation, classification and clustering of broadcast news audio,\u201d in Proceedings of DARPA Speech Recognition Workshop, 1997.\n[18] R. Li, T. Schultz, and Q. Jin, \u201cImproving speaker segmentation via speaker identification and text segmentation.\u201d in Proceedings of INTERSPEECH, 2009, pp. 904\u2013907.\n[19] S. Kim, R. E. Banchs, and H. Li, \u201cWikipedia-based kernels for dialogue topic tracking,\u201d in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, 2014, pp. 131\u2013135.\n[20] S.-S. Shen and H.-Y. Lee, \u201cNeural attention models for sequence classification: Analysis and application to key term extraction and dialogue act detection,\u201d in Proceedings of INTERSPEECH, 2016, pp. 2716\u20132720.\n[21] N. T. Vu, \u201cSequential convolutional neural networks for slot filling in spoken language understanding,\u201d in Proceedings of INTERSPEECH, 2016, pp. 3250\u20133254.\n[22] Y.-N. Chen, M. Sun, A. I. Rudnicky, and A. Gershman, \u201cUnsupervised user intent modeling by feature-enriched matrix factorization,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2016, pp. 6150\u20136154.\n[23] Y. Song, L. Mou, R. Yan, L. Yi, Z. Zhu, X. Hu, and M. Zhang, \u201cDialogue session segmentation by embedding-enhanced texttiling,\u201d in Proceedings of INTERSPEECH, 2016, pp. 2706\u20132710.\n[24] B. Hu, Z. Lu, H. Li, and Q. Chen, \u201cConvolutional neural network architectures for matching natural language sentences,\u201d in Advances in Neural Information Processing Systems, 2014, pp. 2042\u20132050.\n[25] L. Mou, R. Men, G. Li, Y. Xu, L. Zhang, R. Yan, and Z. Jin, \u201cNatural language inference by tree-based convolution and heuristic matching,\u201d in Proceedings of the Annual Meeting of the Association for Computational Linguistics, vol. 2, 2016, pp. 130\u2013136.\n[26] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, \u201cA large annotated corpus for learning natural language inference,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2015, pp. 632\u2013642.\n[27] T. Rockta\u0308schel, E. Grefenstette, K. M. Hermann, T. Koc\u030cisky\u0300, and P. Blunsom, \u201cReasoning about entailment with neural attention,\u201d in Proceedings of the International Conference on Learning Representations, 2016.\n[28] R. Yan, Y. Song, and H. Wu, \u201cLearning to respond with deep neural networks for retrieval-based human-computer conversation system,\u201d in Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2016, pp. 55\u201364.\n[29] Y. Bengio, P. Simard, and P. Frasconi, \u201cLearning long-term dependencies with gradient descent is difficult,\u201d IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.\n[30] S. Hochreiter, \u201cThe vanishing gradient problem during learning recurrent neural nets and problem solutions,\u201d International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 02, pp. 107\u2013116, 1998.\n[31] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\n[32] J. Li, T. Luong, and D. Jurafsky, \u201cA hierarchical neural autoencoder for paragraphs and documents,\u201d in Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 2015, pp. 1106\u20131115.\n[33] J. Xu, D. Chen, X. Qiu, and X. Huang, \u201cCached long short-term memory neural networks for document-level sentiment classification,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2016, pp. 1660\u20131669.\n[34] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, \u201cBidirectional attention flow for machine comprehension,\u201d arXiv preprint arXiv:1611.01603, 2016.\n[35] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proceedings of the International Conference for Learning Representations, 2014."}], "references": [{"title": "Speaker diarization: A review of recent research", "author": ["X. Anguera Miro", "S. Bozonnet", "N. Evans", "C. Fredouille", "G. Friedland", "O. Vinyals"], "venue": "IEEE Transactions on Audio Speech and Language Processing, vol. 20, no. 2, pp. 356\u2013370, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "The Cambridge University March 2005 speaker diarisation system", "author": ["R. Sinha", "S.E. Tranter", "M.J. Gales", "P.C. Woodland"], "venue": "Proceedings of INTERSPEECH, 2005, pp. 2437\u20132440.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Multi-domain dialog state tracking using recurrent neural networks", "author": ["N. Mrk\u0161i\u0107", "D. \u00d3 S\u00e9aghdha", "B. Thomson", "M. Gasic", "P.-H. Su", "D. Vandyke", "T.-H. Wen", "S. Young"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 2, 2015, pp. 794\u2013799.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker, environment and channel change detection and clustering via the Bayesian information criterion", "author": ["S. Chen", "P. Gopalakrishnan"], "venue": "Proceedings of DARPA Broadcast News Transcription and Understanding Workshop, 1998, pp. 127\u2013132.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Speaker change detection using a new weighted distance measure", "author": ["S. Kwon", "S.S. Narayanan"], "venue": "Proceedings of INTER- SPEECH, 2002, pp. 2537\u20132540.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "A neural conversational model", "author": ["O. Vinyals", "Q. Le"], "venue": "ICML Workshop, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A diversitypromoting objective function for neural conversation models", "author": ["J. Li", "M. Galley", "C. Brockett", "J. Gao", "B. Dolan"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 110\u2013119.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "News from OPUS-A collection of multilingual parallel corpora with tools and interfaces", "author": ["J. Tiedemann"], "venue": "Proceedings of Recent Advances in Natural Language Processing, 2009, pp. 237\u2013 248.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Unsupervised speaker segmentation and tracking in real-time audio content analysis", "author": ["L. Lu", "H.-J. Zhang"], "venue": "Multimedia Systems, vol. 10, no. 4, pp. 332\u2013343, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Strategies for automatic segmentation of audio data", "author": ["T. Kemp", "M. Schmidt", "M. Westphal", "A. Waibel"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2000, pp. 1423\u20131426.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "Proceedings of the International Conference on Learning Representations, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["T. Luong", "H. Pham", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1412\u20131421.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Attention-based recurrent neural network models for joint intent detection and slot filling", "author": ["B. Liu", "I. Lane"], "venue": "Proceedings of INTERSPEECH, 2016, pp. 685\u2013689.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Speaker change detection and tracking in real-time news broadcasting analysis", "author": ["L. Lu", "H.-J. Zhang"], "venue": "Proceedings of the Tenth ACM International Conference on Multimedia, 2002, pp. 602\u2013610.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Speaker change detection and speaker clustering using VQ distortion for broadcast news speech recognition", "author": ["K. Mori", "S. Nakagawa"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2001, pp. 413\u2013416.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Segregation of speakers for speech recognition and speaker identification", "author": ["H. Gish", "M.-H. Siu", "R. Rohlicek"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 1991, pp. 873\u2013876.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Automatic segmentation, classification and clustering of broadcast news audio", "author": ["M.A. Siegler", "U. Jain", "B. Raj", "R.M. Stern"], "venue": "Proceedings of DARPA Speech Recognition Workshop, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving speaker segmentation via speaker identification and text segmentation.", "author": ["R. Li", "T. Schultz", "Q. Jin"], "venue": "Proceedings of INTERSPEECH,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Wikipedia-based kernels for dialogue topic tracking", "author": ["S. Kim", "R.E. Banchs", "H. Li"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, 2014, pp. 131\u2013135.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural attention models for sequence classification: Analysis and application to key term extraction and dialogue act detection", "author": ["S.-S. Shen", "H.-Y. Lee"], "venue": "Proceedings of INTERSPEECH, 2016, pp. 2716\u20132720.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequential convolutional neural networks for slot filling in spoken language understanding", "author": ["N.T. Vu"], "venue": "Proceedings of INTER- SPEECH, 2016, pp. 3250\u20133254.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised user intent modeling by feature-enriched matrix factorization", "author": ["Y.-N. Chen", "M. Sun", "A.I. Rudnicky", "A. Gershman"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, 2016, pp. 6150\u20136154.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Dialogue session segmentation by embedding-enhanced texttiling", "author": ["Y. Song", "L. Mou", "R. Yan", "L. Yi", "Z. Zhu", "X. Hu", "M. Zhang"], "venue": "Proceedings of INTERSPEECH, 2016, pp. 2706\u20132710.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2042\u20132050.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language inference by tree-based convolution and heuristic matching", "author": ["L. Mou", "R. Men", "G. Li", "Y. Xu", "L. Zhang", "R. Yan", "Z. Jin"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, vol. 2, 2016, pp. 130\u2013136.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["S.R. Bowman", "G. Angeli", "C. Potts", "C.D. Manning"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2015, pp. 632\u2013642.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Reasoning about entailment with neural attention", "author": ["T. Rockt\u00e4schel", "E. Grefenstette", "K.M. Hermann", "T. Ko\u010disk\u1ef3", "P. Blunsom"], "venue": "Proceedings of the International Conference on Learning Representations, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to respond with deep neural networks for retrieval-based human-computer conversation system", "author": ["R. Yan", "Y. Song", "H. Wu"], "venue": "Proceedings of the 39th International ACM SI- GIR Conference on Research and Development in Information Retrieval, 2016, pp. 55\u201364.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013166, 1994.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["S. Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, vol. 6, no. 02, pp. 107\u2013116, 1998.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1998}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1997}, {"title": "A hierarchical neural autoencoder for paragraphs and documents", "author": ["J. Li", "T. Luong", "D. Jurafsky"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, 2015, pp. 1106\u20131115.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Cached long short-term memory neural networks for document-level sentiment classification", "author": ["J. Xu", "D. Chen", "X. Qiu", "X. Huang"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2016, pp. 1660\u20131669.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": "arXiv preprint arXiv:1611.01603, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "Proceedings of the International Conference for Learning Representations, 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Specifically, a speaker change occurs when the current and the next sentences are not uttered by the same speaker [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "Detecting speaker changes plays an important role in dialogue processing, and is a premise of speaker clustering [2], dialogue understanding [3], etc.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Detecting speaker changes plays an important role in dialogue processing, and is a premise of speaker clustering [2], dialogue understanding [3], etc.", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "Traditional speaker change detection typically uses audio input [4, 5].", "startOffset": 64, "endOffset": 70}, {"referenceID": 4, "context": "Traditional speaker change detection typically uses audio input [4, 5].", "startOffset": 64, "endOffset": 70}, {"referenceID": 5, "context": "Further, the fast development of dialogue analysis puts high demands on understanding textual data [6, 7], as human-computer conversation involves deep semantics, requiring complicated natural language processing (NLP).", "startOffset": 99, "endOffset": 105}, {"referenceID": 6, "context": "Further, the fast development of dialogue analysis puts high demands on understanding textual data [6, 7], as human-computer conversation involves deep semantics, requiring complicated natural language processing (NLP).", "startOffset": 99, "endOffset": 105}, {"referenceID": 5, "context": "[6] and Li et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7], for example, train sequence-to-sequence neural networks to automatically generate replies in an open-domain dialogue system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "They use OpenSubtitle [8] as the corpus, but assume every two consecutive sentences are uttered by different speakers (which brings much noise to their training data).", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": ", pitch [9] and silence points [10], which provide", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": ", pitch [9] and silence points [10], which provide", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "To better explore the context, we further apply an attention mechanism [11] over sentences to focus on relevant information during context integration.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Our attention is static in that only the nearest two sentences around the decision point search for relevant information; it differs from previous research that uses dynamic attention [11, 12], which buries more important sentences under less important context.", "startOffset": 184, "endOffset": 192}, {"referenceID": 11, "context": "Our attention is static in that only the nearest two sentences around the decision point search for relevant information; it differs from previous research that uses dynamic attention [11, 12], which buries more important sentences under less important context.", "startOffset": 184, "endOffset": 192}, {"referenceID": 11, "context": "Compared with word-level attention [12, 13], our sentence-level attention is more efficient because there could be hundreds of words in the context within only a few sentences.", "startOffset": 35, "endOffset": 43}, {"referenceID": 12, "context": "Compared with word-level attention [12, 13], our sentence-level attention is more efficient because there could be hundreds of words in the context within only a few sentences.", "startOffset": 35, "endOffset": 43}, {"referenceID": 0, "context": "Traditional speaker change detection deals with audio input and is a key step for speaker diarization (determining \u201cwho spoke when?\u201d) [1].", "startOffset": 134, "endOffset": 137}, {"referenceID": 13, "context": "A typical approach is to compare consecutive sliding windows of input\u2014with spectral features [14], say\u2014 using metrics including the Bayesian information criterion [15], generalized likelihood ratio [16], and Kullback-Leibler diver-", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "A typical approach is to compare consecutive sliding windows of input\u2014with spectral features [14], say\u2014 using metrics including the Bayesian information criterion [15], generalized likelihood ratio [16], and Kullback-Leibler diver-", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "A typical approach is to compare consecutive sliding windows of input\u2014with spectral features [14], say\u2014 using metrics including the Bayesian information criterion [15], generalized likelihood ratio [16], and Kullback-Leibler diver-", "startOffset": 198, "endOffset": 202}, {"referenceID": 16, "context": "gence [17].", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": ", OpenSubtitle) [7], and enhancing audio speaker change detection with textual information [18].", "startOffset": 16, "endOffset": 19}, {"referenceID": 17, "context": ", OpenSubtitle) [7], and enhancing audio speaker change detection with textual information [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "Previous research has addressed a variety of tasks, ranging from topic tracking [19], dialogue act classification [20], slot filling [21], to user intent modeling [22].", "startOffset": 80, "endOffset": 84}, {"referenceID": 19, "context": "Previous research has addressed a variety of tasks, ranging from topic tracking [19], dialogue act classification [20], slot filling [21], to user intent modeling [22].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "Previous research has addressed a variety of tasks, ranging from topic tracking [19], dialogue act classification [20], slot filling [21], to user intent modeling [22].", "startOffset": 133, "endOffset": 137}, {"referenceID": 21, "context": "Previous research has addressed a variety of tasks, ranging from topic tracking [19], dialogue act classification [20], slot filling [21], to user intent modeling [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 22, "context": "In our previous study, we address the problem of session segmentation in text-based human-computer conversations [23].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "[18] enhance audio-based speaker change detection with transcribed text, and they are also in the unsupervised regime.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": ", paraphrase and logical entailment) between two sentences [24, 25, 26]; Rockt\u00e4schel et al.", "startOffset": 59, "endOffset": 71}, {"referenceID": 24, "context": ", paraphrase and logical entailment) between two sentences [24, 25, 26]; Rockt\u00e4schel et al.", "startOffset": 59, "endOffset": 71}, {"referenceID": 25, "context": ", paraphrase and logical entailment) between two sentences [24, 25, 26]; Rockt\u00e4schel et al.", "startOffset": 59, "endOffset": 71}, {"referenceID": 26, "context": "[27] equip RNN with attention mechanisms.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] rerank candidate replies based on a user-issued query; they enhance sentence-pair modeling with previous utterances as context.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "But vanilla RNNs with perceptron-like hidden states suffer from the problem of vanishing or exploding gradients [29, 30], being less effective to model long dependencies.", "startOffset": 112, "endOffset": 120}, {"referenceID": 29, "context": "But vanilla RNNs with perceptron-like hidden states suffer from the problem of vanishing or exploding gradients [29, 30], being less effective to model long dependencies.", "startOffset": 112, "endOffset": 120}, {"referenceID": 30, "context": "LSTM units [31] alleviate the problem by better balancing input and its previous state with gating mechanisms.", "startOffset": 11, "endOffset": 15}, {"referenceID": 31, "context": "It is similar to the hierarchical autoencoder in [32].", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "Other studies apply a single RNN over a discourse, also achieving high performance in tasks like sentiment analysis [33] and machine comprehension [34].", "startOffset": 116, "endOffset": 120}, {"referenceID": 33, "context": "Other studies apply a single RNN over a discourse, also achieving high performance in tasks like sentiment analysis [33] and machine comprehension [34].", "startOffset": 147, "endOffset": 151}, {"referenceID": 10, "context": "Attention-based neural networks are first proposed to dynamically focus on relevant words of the source sentence in machine translation [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 11, "context": "That is to say, the attention mechanism is applied to the sentence level, different from other work that uses word-level attention [12, 13].", "startOffset": 131, "endOffset": 139}, {"referenceID": 12, "context": "That is to say, the attention mechanism is applied to the sentence level, different from other work that uses word-level attention [12, 13].", "startOffset": 131, "endOffset": 139}, {"referenceID": 26, "context": "It resembles a variant in [27], but differs from common attention where two LSTMs interact dynamically along their propagations (Figure 1d).", "startOffset": 26, "endOffset": 30}, {"referenceID": 34, "context": "We used the Adam optimizer [35] with mini-batch update (batch size being 100).", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "We also tried a dynamic sentence-by-sentence attention mechanism, similar to most existing work [11, 12, 13].", "startOffset": 96, "endOffset": 108}, {"referenceID": 11, "context": "We also tried a dynamic sentence-by-sentence attention mechanism, similar to most existing work [11, 12, 13].", "startOffset": 96, "endOffset": 108}, {"referenceID": 12, "context": "We also tried a dynamic sentence-by-sentence attention mechanism, similar to most existing work [11, 12, 13].", "startOffset": 96, "endOffset": 108}], "year": 2017, "abstractText": "Traditional speaker change detection in dialogues is typically based on audio input. In some scenarios, however, researchers can only obtain text, and do not have access to raw audio signals. Moreover, with the increasing need of deep semantic processing, text-based dialogue understanding is attracting more attention in the community. These raise the problem of textbased speaker change detection. In this paper, we formulate the task as a matching problem of utterances before and after a certain decision point; we propose a hierarchical recurrent neural network (RNN) with static sentence-level attention. Our model comprises three main components: a sentence encoder with a long short term memory (LSTM)-based RNN, a context encoder with another LSTM-RNN, and a static sentence-level attention mechanism, which allows rich information interaction. Experimental results show that neural networks consistently achieve better performance than feature-based approaches, and that our attention-based model significantly outperforms nonattention neural networks.", "creator": "LaTeX with hyperref package"}}}