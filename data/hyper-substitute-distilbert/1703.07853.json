{"id": "1703.07853", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2017", "title": "Faster Reinforcement Learning Using Active Simulators", "abstract": "an evolutionary concept, we offer several simulated methods to build some \\ scenario { learning block } from a given set to agency - subject - specific training packages in order primarily speed up reinforcement simulation ( rl ). simulation methods can decrease the total response time gained by eliminating rl agent subject to training on the target method - working. unlike traditional team learning, and consider creating a portfolio from several training options in order to provide the most benefit in favour of reducing projected total fitness to train.", "histories": [["v1", "Wed, 22 Mar 2017 21:07:35 GMT  (352kb,D)", "http://arxiv.org/abs/1703.07853v1", "12 pages and 4 figures"]], "COMMENTS": "12 pages and 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vikas jain", "theja tulabandhula"], "accepted": false, "id": "1703.07853"}, "pdf": {"name": "1703.07853.pdf", "metadata": {"source": "CRF", "title": "Faster Reinforcement Learning Using Active Simulators", "authors": ["Vikas Jain"], "emails": ["vksjn18@gmail.com", "tt@theja.org"], "sections": [{"heading": null, "text": "Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up the overall training time on two different domains."}, {"heading": "1 Introduction", "text": "In transfer reinforcement learning (RL), the knowledge obtained from training on a source task can be leveraged to learn a target task more efficiently [17]. A source task is typically similar to and sometimes lesser complex than the target task. When such a source task is not readily available, one typically has to select or build a few candidate tasks from scratch and hope that the total time spent to train on (a subset of) these training tasks and then the target is less than the time needed to learn a policy from the target task directly.\nNarvekar et al. [10] present preliminary attempts in direction. They define the problem of curriculum learning for RL as follows: Design a sequence of tasks (i.e., a curriculum) on which a learning agent learns sequentially by transferring knowledge across the stages of the curriculum, ultimately leading to reduced learning time on the target task.\nThere are two parts to any solution for curriculum learning: the first is to create a set of source tasks, and the second is to determine a curriculum from these. Though in [10], the authors showed that a learning curriculum can speed up the learning on the target task, they stop short of addressing the process of finding a curriculum from the source tasks. Instead, they focus on creating agent-specific training tasks from the target task. In this work, we address this latter key part of the curriculum learning problem, viz., given a set of training tasks, how can one order them in an online fashion to form a learning curriculum that speeds up the overall training for a target task.\nWe propose methods for online learning of a curriculum in two scenarios: first, when there is no domain knowledge about the tasks at hand, and second, where there is additional knowledge about\nar X\niv :1\n70 3.\n07 85\n3v 1\n[ cs\n.L G\n] 2\n2 M\nar 2\nthe tasks. In the latter case, such domain information can be encoded in various ways (for instance, in [10], the authors are able to design a parametric model for tasks using domain knowledge). We use the available domain information as features to extrapolate learnability across training tasks, which consequently is used to further speed up curriculum learning. In both settings, our proposed methods are independent of the choice of RL algorithm employed by the agent as well as the choice of the transfer technique used (the latter can depend on the representation maintained by the RL agent, viz., Q-values, policies or the model estimates). We only assume that the RL algorithm and the transfer technique used are compatible with each other. Thus our methods can work with existing RL algorithms and transfer methods introduced in the literature [4, 7]."}, {"heading": "2 Related Work", "text": "Offline curriculum learning in the context of supervised learning has been explored before. In [3], the authors showed that learning using a curriculum has an effect on the rate of convergence of the prediction model. In [12], the authors proposed an approach that processes multiple supervised learning tasks in a sequence and finds the best curriculum of tasks to be learned. This is analogous to our reinforcement learning setting, although the emphasis in [12] is on empirical risk minimization rather than training time. Active learning methods have also been used to find a curriculum of supervised learning tasks in the lifelong machine learning setting [13, 11]. However, as mentioned before, all these methods apply to the supervised learning setting and typically need to exploit domain knowledge. In contrast, for reinforcement learning, as we show in our experiments, domain knowledge is not necessary for online curriculum learning and can still lead to faster training of an agent on the target RL task.\nAnalogous to our work is that of [9], wherein the authors define a student-teacher setting for supervised learning. There, the teacher\u2019s task is to provide a sequence of training data to the supervised learning algorithm such that the latter can quickly learn the correct hypothesis. As we show later, our methods follow the template of presenting a task to the RL agent for a number of steps and then changing the task (for instance, picking a slightly more difficult but transfer-learnable task) such that the agent is steered towards the target task.\nTransfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15]. However, the focus here is typically on efficient transfer (which in itself is a hard problem) and not on the selection of source tasks. Certain recent works in multi-task reinforcement learning look at the problem of task selection where the agent is presented with a set of tasks to learn from [20]. Here, the goal is to make the agent learn policies that are simultaneously good for all the tasks. As such, this is very different from our setting where we want our agent to learn a target task successfully in a time efficient manner.\nIn [14], the authors presented an offline method to find a curriculum by computing transfer measure for each pair of training tasks to form an inter-task transferability model. They then use this model to find a curriculum in a recursive fashion offline. Not only this is computationally intensive, such an approach is bound to fail in our online setting where we measure the success of learning a curriculum based on the total time to train (i.e., the time to learn the curriculum and the time to train on the target task). The computation of the inter-task transferability model requires a significant amount of time and domain knowledge. In that time, one could easily train an RL agent on the target task directly from scratch. Nonetheless, when there is domain information available in the form of features, we are able to build on this approach by proposing an active learning method. This method of ours builds a model similar to that in [14] while benefiting from reduced\ncomputation time. One of the directions for future exploration would be to decide in an online manner, when domain knowledge is helpful and when it is not. We currently do not address this choice. It is intuitive that when the number of training tasks is large compared to the complexity of the target task, it may be helpful to use the domain aware online curriculum learning methods compare to the domain-agnostic ones. To the best of our knowledge, we are the first to present general methods for creating a curriculum of training tasks online in both the domain agnostic and domain aware scenarios.\nFinally, note that in theory, designing a curriculum while designing the training tasks in a coupled manner can further improve the total time needed for an agent to learn a target task. Unfortunately, the current state of the art is not yet amenable to a coupled approach. Our work complements the work in [10], which present methods to create a set of training tasks which are related to a target task. A future extension could look into the more complex coupled task design and sequencing. In this work, we reasonably assume a set of training tasks similar to the target task are already present and we only need to create a curriculum and learn on the target task faster than learning on the target directly from scratch."}, {"heading": "3 Preliminaries", "text": "We introduce a couple of key ingredients that will be useful in defining our problem and solutions. Transfer in RL: In transfer reinforcement learning, instead of learning on a task M directly, an agent first learns on a suitable source task Msub. The learned knowledge (for instance, Q-values) from Msub is then reused to warm-start learning on the target task M making learning the latter task faster. Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16]. In our work, we demonstrate the performance of our online curriculum learning methods using Q-function transfer. This entails initializing Q-function for the agent working with M using those obtained for Msub. In the literature, there are ways to facilitate such a transfer even if the state and action spaces of the two tasks are distinct, as long as there is a cross-task mapping between them [19].\nActive learning: We discuss active learning in the well known setting of ordinary least squares (with additive Gaussian noise assumption). In this problem, the objective is to find a model that minimizes the Mean Squared Error (MSE) between the true values and the predicted values. That is, we minimize over \u03b8, the objective \u2211n t=1(yt \u2212 \u03b8T ( xt 1 )) 2, where (xt, yt) n 1 is the training data. An analytical solution to this problem (under certain conditions) is given by \u03b8\u0302 = (XTX)\u22121XT y, where X is a matrix whose rows are indexed by training examples (xTt 1) (1 \u2264 t \u2264 n) and y is a column vector indexed by training example ground truth values (yt) . Further, let \u03b8\u2217 be the true parameter that linearly relates yt and xt. Then, the expected Euclidean distance between the true and the estimated parameters is given by E(\u2016\u03b8\u0302 \u2212 \u03b8\u2217\u20162) = \u03c3\u22172Tr[(XTX)\u22121] where \u03c3\u22172 is the noise variance and Tr[\u00b7] is the trace operator.\nIn the active learning variant of this problem [6], we sequentially select training examples to minimize the estimation error. Let\u2019s assume that we already have some training points (so X is well defined) and let A = (XTX)\u22121. If we now choose a training example (xT 1) and append it to X,\nthen A matrix gets updated as: (\nX xT 1 )T ( X xT 1 ) = (XTX)+( x1 ) ( x 1 ) T = A+vvT where v = (xT 1) T .\nSince we want to choose the training example such that the estimation error is minimized, we can\nwrite estimation error in terms of the new example:\nTr[(A+ vvT )\u22121] = Tr[A]\u2212 v TAAv\n1 + vTAv .\nAnd observe that we only need to choose a v that maximizes v TAAv\n1+vTAv . If we assume that our data\nis normialized (and hence \u2016v\u2016 = 1), then the maximizing v, v\u0302, turn out to be the normalized eigenvector of A with the largest eigenvalue. We can thus choose the next training point that is most similar to v\u0302."}, {"heading": "4 Problem Setup", "text": "Let the MDP corresponding to the target task be Mtarget. Let the set of training tasks be Tsub = {Msubk : 1 \u2264 k \u2264 K}, where each Msubk is an MDP similar to Mtarget with possibly different state and action spaces. Let the total number of steps (each corresponding to a state transition) allowed for training the RL agent be T .\nThe objective of curriculum learning is to design an algorithm that, when given an RL agent as an input, constructs a (sub)sequence of tasks from Tsub and trains the agent on them first and then on theMtarget to achieve a desired level of performance. And in particular, the total number of steps it takes for designing the curriculum, training the agent on the curriculum and then on the target Mtarget is lesser than the steps needed to train the agent directly on Mtarget for the same level of performance. In other words, the algorithm (active simulator) actively chooses and switches the tasks which the RL agent is training on. It is assumed that the RL agent is able to transfer knowledge from previous tasks while starting a new task. The algorithm itself can be agnostic to the exact transfer technique. Let the (sub)sequence of tasks designed by the algorithm be M\u22171 \u2192M\u22172 \u2192 \u00b7 \u00b7 \u00b7 \u2192 M\u2217i \u2192Mtarget where M\u2217j \u2208 Tsub 1 \u2264 j \u2264 i. Further let the ordered set of curriculum tasks be denoted by Im = (M\u22171,M\u22172, \u00b7 \u00b7 \u00b7 ,M\u2217i ).\nWe hypothesize that learning the target task after transfer from a learned curriculum is faster than learning on the target task alone or by using single stage transfer methods. Intuitively, the algorithms that address online curriculum learning have to find a learning curriculum (a sequence) from these training tasks and make the agent learn on the sequence by transferring knowledge from one task to another, with the ultimate aim of learning the target task in the lowest number of simulation steps. Because these algorithms change the task (i.e. the simulation environment) seen by the agent over time, we call them active simulators.\nThe algorithms are given a budget of total T steps within which they need to train the RL agent on the target task, where each step signifies one state\u2013action\u2013reward\u2013state cycle. All our algorithms operate in two phases: curriculum selection and training on training tasks, and training on the target task. The first phase can have a budget of g(T ) < T steps that can optionally be provided as an input. The algorithms also have access to an oracle TLearn(L,M, \u03c4), where L is an RL agent,M is an RL task and \u03c4 is the upper bound on number of steps available for learning. This oracle is an abstraction to the process where the RL agent trains on a task for a given number of steps. At the end of training, the oracle returns the reward accumulated by L on the task M that it trained on.\nFurther, the algorithms have no control over the RL algorithm (say a tabular method or a function approximation based method) that the agent L employs and treat it as a black box. One of the consequences of this is that we cannot reset the learned knowledge of the agent. To get around this, we assume that our algorithms can clone the agent at any given step to freeze the learned knowledge (more details below). We also assume that the agent L can first transfer\npreviously learned knowledge (say in the form of a policy or Q-values or the dynamics) using a transfer method and then learn a given task M."}, {"heading": "5 Active Simulators for Online Curriculum Learning", "text": "We now present several active simulator variants under two scenarios, viz., the domain agnostic and the domain aware settings. In the domain agnostic setting, online curriculum building is based on the task specific rewards accumulated and their transformations. In the domain aware setting, curricula are obtained using features that describe each of the tasks. The active simulator methods are: (a) Binary Curriculum Selector, Reward Maximizing Greedy Selector, Local Transfer Maximizing Selector, (d) Active Reward Maximizing Greedy Selector, and (e) Active Local Transfer Maximizing Selector. Among them, the first three are domain agnostic and the latter two are domain aware methods.\n5.0.1 Binary Curriculum Selector\nIn this method (see Algorithm 1), the agent is assigned tasks on which the rewards it has accumulated in the past is less. The intuition is that tasks in which the agent has relatively collected more rewards, are less likely to help with good transfer of knowledge in the future compared to tasks where it has not. In essence, the rewards collected are a proxy for the difficulty levels of the tasks and an implicit assumption that more complex tasks are closer to the target task is being made. The method works as follows. First, the agent is trained sequentially on each task in Tsub for a fixed number of steps \u03c4 and rewards accumulated for each task (Mi.r) are logged. Then, Tsub is modified to include those tasks on which the accumulated reward is lesser than the remaining half. This shrinks the size of Tsub by 2. And the process is repeated till Tsub is empty. After this, the agent learns on the target task Mtarget for remaining number of steps. Note that transfer of knowledge is assumed every time there is a switch in the task assigned. The number of steps spent in training with the source task can be a proportion of the total time (say 0.6T ) and automatically determines \u03c4 . Finally, note that because of eliminating half the tasks after each stage, the total number of times the training tasks are switched is K logK.\nAlgorithm 1: Binary Curriculum Selector\nInput: L, \u03c4 , Tsub = {Msubk }Kk=1, Mtarget, T Output: L\n1 g(T ) = 0 2 while Tsub 6= \u03c6 do 3 L,Mi.r \u2190 TLearn(L,Mi, \u03c4) \u2200Mi \u2208 Tsub 4 g(T )\u2190 g(T ) + \u03c4 |Tsub| 5 Sort Tsub in ascending order of Mi.r 6 Tsub \u2190 Tsub[1 : 12 |Tsub|] 7 end 8 L \u2190 TLearn(L,Mtarget, T \u2212 g(T ))\n5.0.2 Reward Maximizing Greedy Selector\nIn this method (see Algorithm 2), the next task in the curriculum is selected greedily based on the performance of the agent on each task in the set of remaining tasks to choose from. In order to\nachieve this, the method needs access to multiple copies (clones) of the RL agent L so that it can accurately measure the transfer effect. This is because, when the agent trains on a task Mu and then transfers to train on another task Mv (for instance, to measure the transferability between the two tasks), the information state of the agent at the end of training onMv is different from the information state at the beginning of training. For a third task Mw, one cannot now compute its transferability withMu unless the state of the agent after training onMu is preserved. The use of copies helps the method replicate an agent\u2019s knowledge level for computing inter-task transferability reliably. Note that making a copy is a mild assumption compared to having direct access to the internal state of the learning agent.\nThus there are multiple learners: (a) the original agent L, and (b) virtual learners {Lc} that copy agent L\u2019s state. A copy agent Lc is used to assess inter-task transferability effect starting from the current state of L. The method then selects the task where the copy agent gets the maximum reward and adds it to the curriculum. The original agent L then learns from this new task that was added to the curriculum. This process is repeated till all the subtasks are added sequentially into the curriculum. After training on all tasks in the curriculum, the method trains the agent L on the target task Mtarget. One key aspect of this method is that this selects the next task based on the sequence of tasks already in the curriculum at every decision epoch.\nIn Algorithm 2, \u03c41 and \u03c42 are the number of steps used for evaluating inter-task transferability and learning a task respectively. Procedure Eval(Lc,Mi, \u03c41) outputs the reward accumulated on task Mi by Lc in \u03c41 number of steps. Note that, this procedure does not alter the original RL agent L. The length of the curriculum obtained in this method is equal to the number of training tasks K.\nAlgorithm 2: Reward Maximizing Greedy Selector\nInput: L, \u03c41, \u03c42, Tsub = {Msubk }Kk=1, Mtarget, T Output: L\n1 g(T )\u2190 0 2 while Tsub 6= \u03c6 do 3 Lc \u2190 copy(L) 4 Mi.r \u2190 Eval(Lc,Mi, \u03c41) \u2200Mi \u2208 Tsub 5 M\u2217 \u2190 arg maxMi\u2208TsubMi.r 6 L \u2190 Tlearn(L,M\u2217, \u03c42) 7 g(T )\u2190 g(T ) + \u03c41|Tsub|+ \u03c42 8 Tsub \u2190 Tsub \\M\u2217 9 end\n10 L \u2190 TLearn(L,Mtarget, T \u2212 g(T ))\n5.0.3 Local Transfer Maximizing Selector\nIn this method (see Algorithm 3), the effect of transfer from one task to another task is calculated for each task pair in Tsub as well as for pair of tasks where the first task is in Tsub and the second task is Mtarget. Based on the K \u00d7 (K + 1) numbers computed, the methods designs a curriculum that maximizes local transfer. That is, the curriculum is obtained by starting from Mtarget and traversing in the reverse order. In particular, a predecessor task with which the transfer effect to Mtarget is the highest is first selected. Then, the process is repeated recursively and a sequence of tasks M\u22171 \u2192 M\u22172 \u2192 . . . \u2192 Mtarget is obtained. Then the method assigns this curriculum to the\nRL agent L. As in the previous method, copies of the agent L are used to accurately capture the transfer effect.\nIn Algorithm 3, the quantity F is the inter-task transferability matrix where each entry F [i][j] = TransferMeasure(Lc,Mi,Mj , \u03c41) stores the reward accumulated while training on task Mj after transferring knowledge (a learned policy or Q-values etc.) from experience with task Mi and starting with taskMj . Note that the copy agent Lc also follows L\u2019s transfer method while capturing transferred knowledge. The method spends \u03c41 steps for computing the transfer effect for each pair of tasks. The length of the curriculum obtained in this method is K, the size of the training task set Tsub. This method is in principle an offline method similar to [14] but without taking domain knowledge in consideration.\nAlgorithm 3: Local Transfer Maximizing Selector\nInput: L, \u03c41, \u03c42, Tsub = {Msubk}Kk=1, Mtarget, T Output: L\n1 F [K][K + 1]\u2190 0, g(T ) = 0 2 forMi \u2208 Tsub do 3 forMj \u2208 Tsub \u222a Ttarget do 4 Lc \u2190 copy(L) 5 F [i][j]\u2190 TransferMeasure(Lc,Mi,Mj , \u03c41) 6 g(T )\u2190 g(T ) + \u03c41 7 end\n8 end 9 C \u2190 {Mtarget}, NextTask \u2190Mtarget\n10 while Tsub 6= \u03c6 do 11 M\u2217 \u2190 arg maxMi\u2208Tsub F [Mi][NextTask] 12 C \u2190 C.add(M\u2217) 13 Tsub \u2190 Tsub \\M\u2217 14 NextTask \u2190M\u2217 15 end 16 C \u2190 Reverse(C) 17 forMi \u2208 C \\ {Mtarget} do 18 L \u2190 TLearn(L,Mi, \u03c42) 19 g(T )\u2190 g(T ) + \u03c42 20 end 21 L \u2190 TLearn(L,Mtarget, T \u2212 g(T ))\n5.0.4 Active Reward Maximizing Greedy Selector\nThis is the first of the two domain aware methods we propose. In this setting, we are also given feature vectors fi = [fi1, fi2, . . . , fin] for each task Mi \u2208 Tsub.\nWe will adapt the Reward Maximizing Greedy Selector method described before (see Algorithm 2) to this information rich setting in the following way. Consider the partial list of tasks (M\u2217) at a particular iteration m in the while loop and denote it by Im. When we need to decide on the next task in the curriculum at step m + 1, we will avoid evaluating the reward collected (see line 4 in Algorithm 2) for every possible task under consideration.\nIn other words, instead of finding the transferability measure for each sequence-task pair (the\nsequence being Im and the task being one of the remaining tasks in Tsub at step m + 1), the method will now use the task specific features to create feature vectors for every pair of tasks and build a transferability prediction model. In particular, the method will apply the active learning technique described in Section 3 to build a regression model and predict the transferability measure for sequence-task pairs. Further, it will relearn the model at each step m.\nLet the curriculum sequence at step m be Im = {M\u22171,M\u22172, . . . ,M\u2217m\u22121}. Feature vectors for the sequence as well as the tasks that are not yet in the curriculum are designed as follows. The feature vector fIm for sequence Im is computed as: fImk = 1 m\u22121 \u2211m\u22121 i=0 f \u2217 ik, where f \u2217 i is feature of task M\u2217i . For each remaining task in Tsub (say Mj), the sequence-task pair feature vector fImj is consequently defined as: fImjk = fImk\u2212fjk max(fImk, ) , where > 0 is a small positive constant.\nTo find a new task to be added to the curriculum, a regression model is learned at each step. At step m, the method learns a regression model from Im and the remaining tasks in Tsub. Then, it picks that task in Tsub which has maximum value of transferability measure. The length of the curriculum obtained in this method is \u2264 n, because one may not include a particular task based on the regression predictions.\n5.0.5 Active Local Transfer Maximizing Selector\nThe second domain aware method extends Local Transfer Maximizing Selector (Algorithm 3) as follows. Instead of finding transferability measure F [i][j] for every pair of tasks \u2200i, j, it uses the features of the tasks to form pair wise feature vectors and learn a regression model F\u0302 that can predict the entries of matrix F . Again the active learning technique can be used to build such a model using limited number of task pairs (i, j), speeding up the curriculum design in the process.\nThe feature vector f ij of a task pair is computed accounting for the similarity of the two tasks\u2019\nfeatures. The k-th element f ijk of the vector f ij is defined as: f ijk = fik\u2212fjk max(fik, ) , where > 0 is a small constant. The regression model F\u0302 is used instead of inter-task transferability matrix F and the algorithm function very similarly to Algorithm 3. The length of the curriculum obtained in this method can also be \u2264 n.\nWe conclude by summarizing the differences between the two settings. Firstly, since there is no task representation available to domain agnostic methods, ideally the transfer performance needs to be measured for all task pairs (or sequence-task pairs) and based on this information, the next task of the curriculum should be chosen. On the other hand, in the domain aware scenarios, one can utilize the task features to model and predict the task pair (or sequence-task pair) transfer effects and avoid computing them for all pairs. Thus, tools such as active learning can be used (as shown above) to reduce the number of pairs required to learn a transfer effect model, leading to potential gains in the number of steps needed while training on the curriculum of training tasks.\nSecondly, in the case of redundant tasks in Tsub, domain independent methods cannot easily prune the tasks. In domain aware scenario, where task features are available, diversity scores d(M, Im) which is the measure of diversity(inverse of similarity) between taskM and the tasks in task-sequence Im can be theoretically computed to remove the redundancy."}, {"heading": "6 Experiments", "text": "We verified the effectiveness of our methods on two domains: (a) a maze environment [1], and (b) a grid world environment (see Figure 1). We found that the curricula selected by our approaches indeed outperform the alternative (training on the target) in all cases. In the domain aware setting, we also demonstrate that active learning methods can speed up training on the target task.\nBoth domains require an agent (employing the Q-learning algorithm) to find an optimal policy for reaching a goal state. The reward value for reaching the goal state is +1. We set the discount factor (\u03b3) to be 0.9 and learning rate (\u03b1) to be 0.6. In the maze task, the start position of the agent is sampled uniformly at random from the set of feasible states while in the grid world, the start position is fixed. The episode ends when the agent reaches the goal position.\nTraining Tasks: We construct training tasks for the above two target tasks using methods similar to [10] (see Figure 2). In the maze task, since the start position of the agent is not fixed, the source tasks are created by reducing the state space of the target task while keeping the goal position same. In the grid world, since the start position of the agent is also given, the source tasks are created by changing the start position of the agent while keeping the rest of the task fixed.\nWe now present both quantitative and qualitative results of performance. In particular, we report the convergence time of our methods as well as the reward accumulated. And we also comment on the curricula selected and the final policy learned. Quantitative Results: As a baseline, we consider the agent learning the target task from scratch. We plot the total reward accumulated by our methods as well as the baseline until the Q-function converges on the target task (with Monte Carlo averaging over 30 runs). Figure 3(Left) shows the number of steps required for the convergence as well as the reward accumulated on both the domains. Qualitative Results: For both environments, we consider 4 source tasks leading to 24 possible curricula of full length. We calculate the number of steps required by each curriculum for convergence on the target task. Figure 4 shows the convergence time of each curriculum as compared to the baseline and the curriculum selected by our methods for both the domains. The final policies learned for both the domains using our methods are shown in the Figure 3(Right). Domain Aware: It can be seen in Figure 3 that the Local Transfer Maximizing Selector [14] is taking larger number of steps for training. We utilize the active learning setting described in section 3 to compute the inter-task transferability matrix using lesser number of steps. In the grid\nworld, the features fi is defined as the distance between the goal and the start position and f ij is defined as f i\u2212f j . In the maze world, we directly define inter-task features f ij to be the number of overlapping states between the two tasks divided by the number of states in the task Mi. Figure 5 shows that the number of steps reduce significantly while the performance and the curriculum selected remain the same."}, {"heading": "7 Conclusion", "text": "In this paper, we have defined the online curriculum learning problem as a way to speed up the learning of reinforcement learning agents on a target task. We designed several algorithms for two broad settings. In the first, no task information is available and in the second, task dependent features are available. Our algorithms were able to choose curricula and train reinforcement learning agents more efficiently compared to training the agents directly on the target tasks for two different\ndomains. Statistical and computational guarantees for the proposed algorithms are left for future work."}], "references": [{"title": "Vision-based behavior acquisition for a shooting robot by using a reinforcement learning", "author": ["M. Asada", "S. Noda", "S. Tawaratsumida", "K. Hosoda"], "venue": "Proc. of IAPR/IEEE Workshop on Visual Behaviors, pages 112\u2013118. Citeseer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Successor features for transfer in reinforcement learning", "author": ["A. Barreto", "R. Munos", "T. Schaul", "D. Silver"], "venue": "arXiv preprint arXiv:1606.05312", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert", "J. Weston"], "venue": "Proceedings of the 26th annual international conference on machine learning, pages 41\u201348. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Transferring task models in reinforcement learning agents", "author": ["A. Fachantidis", "I. Partalas", "G. Tsoumakas", "I. Vlahavas"], "venue": "Neurocomputing, 107:23\u201332", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Probabilistic policy reuse in a reinforcement learning agent", "author": ["F. Fern\u00e1ndez", "M. Veloso"], "venue": "Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems, pages 720\u2013727. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "course materials for 6.867 machine learning, fall 2006. mit opencourseware", "author": ["T. Jaakkola"], "venue": "(http://ocw.mit.edu/) Massachusetts Institute of Technology. Downloaded on", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Transfer in reinforcement learning: a framework and a survey", "author": ["A. Lazaric"], "venue": "Reinforcement Learning, pages 143\u2013173. Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer of samples in batch reinforcement learning", "author": ["A. Lazaric", "M. Restelli", "A. Bonarini"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 544\u2013551. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "The teaching dimension of linear learners", "author": ["J. Liu", "X. Zhu"], "venue": "Journal of Machine Learning Research, 17(162):1\u201325", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Source task creation for curriculum learning", "author": ["S. Narvekar", "J. Sinapov", "M. Leonetti", "P. Stone"], "venue": "Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems, pages 566\u2013574. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Smart task orderings for active online multitask learning", "author": ["P. Pang", "J. An", "J. Zhao", "X. Li", "T. Ban", "D. Inoue", "H. Sarrafzadeh"], "venue": "Proceedings of SIAM International Conference on Data Mining", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Curriculum learning of multiple tasks", "author": ["A. Pentina", "V. Sharmanska", "C.H. Lampert"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5492\u2013 5500", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Active task selection for lifelong machine learning", "author": ["P. Ruvolo", "E. Eaton"], "venue": "AAAI", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning inter-task transferability in the absence of target task samples", "author": ["J. Sinapov", "S. Narvekar", "M. Leonetti", "P. Stone"], "venue": "Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, pages 725\u2013733. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous transfer for reinforcement learning", "author": ["M.E. Taylor", "G. Kuhlmann", "P. Stone"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 1, pages 283\u2013290. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Behavior transfer for value-function-based reinforcement learning", "author": ["M.E. Taylor", "P. Stone"], "venue": "Proceedings of the fourth international joint conference on Autonomous agents and multiagent systems, pages 53\u201359. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["M.E. Taylor", "P. Stone"], "venue": "Journal of Machine Learning Research, 10(Jul):1633\u20131685", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "An introduction to intertask transfer for reinforcement learning", "author": ["M.E. Taylor", "P. Stone"], "venue": "AI Magazine, 32(1):15", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Transfer via inter-task mappings in policy search reinforcement learning", "author": ["M.E. Taylor", "S. Whiteson", "P. Stone"], "venue": "Proceedings of the 6th international joint conference on Autonomous agents and multiagent systems, page 37. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-task reinforcement learning: a hierarchical bayesian approach", "author": ["A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli"], "venue": "Proceedings of the 24th international conference on Machine learning, pages 1015\u20131022. ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 16, "context": "In transfer reinforcement learning (RL), the knowledge obtained from training on a source task can be leveraged to learn a target task more efficiently [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 9, "context": "[10] present preliminary attempts in direction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Though in [10], the authors", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "In the latter case, such domain information can be encoded in various ways (for instance, in [10], the authors are able to design a parametric model for tasks using domain knowledge).", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Thus our methods can work with existing RL algorithms and transfer methods introduced in the literature [4, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 6, "context": "Thus our methods can work with existing RL algorithms and transfer methods introduced in the literature [4, 7].", "startOffset": 104, "endOffset": 110}, {"referenceID": 2, "context": "In [3], the authors showed that learning using a curriculum has an effect on the rate of convergence of the prediction model.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "In [12], the authors proposed an approach that processes multiple supervised learning tasks in a sequence and finds the best curriculum of tasks to be learned.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "This is analogous to our reinforcement learning setting, although the emphasis in [12] is on empirical risk minimization rather than training time.", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Active learning methods have also been used to find a curriculum of supervised learning tasks in the lifelong machine learning setting [13, 11].", "startOffset": 135, "endOffset": 143}, {"referenceID": 10, "context": "Active learning methods have also been used to find a curriculum of supervised learning tasks in the lifelong machine learning setting [13, 11].", "startOffset": 135, "endOffset": 143}, {"referenceID": 8, "context": "Analogous to our work is that of [9], wherein the authors define a student-teacher setting for supervised learning.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Transfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15].", "startOffset": 159, "endOffset": 170}, {"referenceID": 17, "context": "Transfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15].", "startOffset": 159, "endOffset": 170}, {"referenceID": 14, "context": "Transfer in reinforcement learning addresses the problem of designing schemes for efficient knowledge transfer from a given source task to a given target task [2, 18, 15].", "startOffset": 159, "endOffset": 170}, {"referenceID": 19, "context": "Certain recent works in multi-task reinforcement learning look at the problem of task selection where the agent is presented with a set of tasks to learn from [20].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "In [14], the authors presented an offline method to find a curriculum by computing transfer measure for each pair of training tasks to form an inter-task transferability model.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "This method of ours builds a model similar to that in [14] while benefiting from reduced", "startOffset": 54, "endOffset": 58}, {"referenceID": 9, "context": "Our work complements the work in [10], which present methods to create a set of training tasks which are related to a target task.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 170, "endOffset": 173}, {"referenceID": 15, "context": "Different works propose different ways of transferring the knowledge from the source taskMsub toM including samples [8], the learned policy [5], model of the environment [4] and the value function [16].", "startOffset": 197, "endOffset": 201}, {"referenceID": 18, "context": "In the literature, there are ways to facilitate such a transfer even if the state and action spaces of the two tasks are distinct, as long as there is a cross-task mapping between them [19].", "startOffset": 185, "endOffset": 189}, {"referenceID": 5, "context": "In the active learning variant of this problem [6], we sequentially select training examples to minimize the estimation error.", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "This method is in principle an offline method similar to [14] but without taking domain knowledge in consideration.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "We verified the effectiveness of our methods on two domains: (a) a maze environment [1], and (b) a grid world environment (see Figure 1).", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "Training Tasks: We construct training tasks for the above two target tasks using methods similar to [10] (see Figure 2).", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "Domain Aware: It can be seen in Figure 3 that the Local Transfer Maximizing Selector [14]", "startOffset": 85, "endOffset": 89}], "year": 2017, "abstractText": "In this work, we propose several online methods to build a learning curriculum from a given set of target-task-specific training tasks in order to speed up reinforcement learning (RL). These methods can decrease the total training time needed by an RL agent compared to training on the target task from scratch. Unlike traditional transfer learning, we consider creating a sequence from several training tasks in order to provide the most benefit in terms of reducing the total time to train. Our methods utilize the learning trajectory of the agent on the curriculum tasks seen so far to decide which tasks to train on next. An attractive feature of our methods is that they are weakly coupled to the choice of the RL algorithm as well as the transfer learning method. Further, when there is domain information available, our methods can incorporate such knowledge to further speed up the learning. We experimentally show that these methods can be used to obtain suitable learning curricula that speed up the overall training time on two different domains.", "creator": "LaTeX with hyperref package"}}}