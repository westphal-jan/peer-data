{"id": "1703.09146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Mar-2017", "title": "GPU Activity Prediction using Representation Learning", "abstract": "gpu activity prediction is strategically important and practical paradigm. this is due to sufficiently high level of contention among thousands of parallel threads. solution problem misses actually modeling involving heuristics. simulations propose a representation learning tree to address static complaint. software model interactive performance network with a temporal aggregate of the executed instructions with the premise that static flow of orders lacks be identified or distinct flows of the interaction. lab experiments show high accuracy and non - trivial predictive variation of representation analysis as a benchmark.", "histories": [["v1", "Mon, 27 Mar 2017 15:31:02 GMT  (675kb,D)", "http://arxiv.org/abs/1703.09146v1", "Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s)"]], "COMMENTS": "Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aswin raghavan", "mohamed amer", "timothy shields", "david zhang", "sek chai"], "accepted": false, "id": "1703.09146"}, "pdf": {"name": "1703.09146.pdf", "metadata": {"source": "META", "title": "GPU Activity Prediction using Representation Learning", "authors": ["Aswin Raghavan", "Mohamed Amer", "Timothy Shields", "David Zhang", "Sek Chai"], "emails": ["FIRSTNAME.LASTNAME@SRI.COM"], "sections": [{"heading": "1. Introduction", "text": "The performance of a computing system relies on the sustained operational throughput. Sustained operation is becoming harder to achieve as computation workloads become more complex. At the same time, with the end of Dennard scaling (Esmaeilzadeh & et. al., 2011), and the increasing abundance of Big Data, it is imperative to minimize wasted processor effort in order to achieve processor reliability and scalability (Wulf & McKee, 1995; Patterson, 2006; Bergman et al., 2008).\nThe goal of this paper is to demonstrate the efficacy of machine learning to designing computing systems. We anticipate that classical problems such as branch predictions and cache management can be re-evaluated such that heuristically based approaches (Yeh & Patt, 1991; Fung et al., 2007; Li et al., 2015) can be replaced with a machine learning approach (Leng et al., 2015). It is well understood in the computer architecture community that processor behavior is highly complex and data dependent. Processor data is widely available in the form of benchmarks (Che et al., 2010), and algorithms are extensively compared using these benchmarks (Blem et al., 2011).\nWe choose a well-understood and well-defined problem\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nof predicting GPU Cache Misses (Li et al., 2015; Chen et al., 2014) for this paper. General Purpose GPU (GPGPU) achieve high throughput execution via a high level of parallelism. Predicting GPU Cache Misses is complex due to the high level of contention among thousands of threads. Cache contention is a bottleneck for parallel execution when many threads are waiting for cache operation, causing the addition of more threads (or cores) to be detrimental. Predicting whether a cache miss is about to occur is useful for better cache management such as cache bypassing (Chen et al., 2014), pre-fetching (Lee et al., 2010), prioritized allocation (Li et al., 2015) etc. Further, cache misses indirectly cause increased energy and power usage (Leng et al., 2015) because of second order effects beyond memory latency. In principle, our approach is amenable to predict these higher order events (such as voltage scaling (Leng et al., 2015) and faults (S. Chai, 2014)) either directly (Tiwari et al., 1994) or via hierarchical modeling.\nWe propose a new model that can predict key processor events that limit processor throughput. We propose a new variant of the Conditional Restricted Boltzmann Machines (CRBMs) (Taylor et al., 2011) to directly address system performance and reliability. CRBMs efficiently model short-term temporal phenomenon. Prior work used a perceptron to predict cache misses (Leng et al., 2013). Unlike their approach, our model accounts for time-series and count data.\nOur approach assumes the availability of a simulator for CUDA (Bakhoda et al., 2009) that can generate a dataset for training our model. In principle, this approach can be used in real-time by incrementally augmenting the dataset. Multiple repeated executions can even lead to increased predictive power because more data is available for machine learning. Our predictor is naturally agnostic to the hardware and architecture as it relies on execution traces.\nOur contributions:\n\u2022 Prediction of processor events as temporally-extended activities in a stream of instructions.\n\u2022 Using Discriminative Conditional Restrictive Boltzmann Machines (DCRBM) to learn processor states.\nar X\niv :1\n70 3.\n09 14\n6v 1\n[ cs\n.L G\n] 2\n7 M\nar 2\n01 7"}, {"heading": "2. Literature Review", "text": "Cache Miss Prediction: There is a large body of research on branch prediction to improve cache performance. Simple static solutions can achieve 80% correct prediction by analyzing control-flow and static heuristics (Ball & Larus., 1993). Dynamic solutions (Yeh & Patt, 1991) are more complicated as they are implemented with counters and tables to store branch history based on branch memory address. Other approaches that are data-driven use perceptrons (Jime\u0301nez & Lin, 2001) and feed-forward neural networks (Calder & et al., 1997).\nRepresentation Learning: Restricted Boltzmann Machines (RBMs) form the building blocks in energy based deep networks (Hinton et al., 2006; Salakhutdinov & Hinton, 2006). Recently, temporal models based on deep networks have been proposed, capable of modeling a more temporally rich set of problems. These include Conditional RBMs (CRBMs) (Taylor et al., 2011) and Temporal RBMs (TRBMs) (Sutskever & Hinton, 2007). CRBMs have been used in both visual (Taylor et al., 2011) and audio (Mohamed & Hinton, 2009). In addition to efficiently modeling time-series data, RBMs were formulated to be trained discriminatively for classification (Larochelle & Bengio, 2008), and model word-count vectors from a large set of documents (Salakhutdinov & Hinton, 2009)."}, {"heading": "3. Model", "text": "The input to our model (called visible units) is an instruction mix per time step, ie. the histogram of counts of instructions being excuted, obtained from the GPU simulator. The labels are any chosen performance metric also output by the simulator.\nWe discuss a sequence of models, gradually increasing in complexity, so that the different components of our model can be understood in isolation. We start with the basic CRBM model, then we extend to the discriminative DCRBM, and finally CountDCRBM.\nConditional Restricted Boltzmann Machines: CRBMs (Taylor et al., 2011), are a natural extension of RBMs for modeling short term temporal dependencies. A CRBM is an RBM which takes into account history from the previous time instances t \u2212 N, . . . , t \u2212 1 at time t. This is done by treating the previous time instances as additional inputs. Doing so does not complicate inference. v is a vector of visible nodes, h is a vector of hidden nodes, and v<t is the visible vectors from the previous N time instances, which influences the current visible and hidden vectors. EC is the energy function, and Z is the partition function. The parameters \u03b8 to be learned are a and b the biases for v and h respectively and the weights W . A and B are matrices of concatenated vectors of previous time instances of a and\nb. The CRBM is fully connected between layers, with no lateral connections. This architecture implies that v and h are factorial given one of the two vectors. This allows for the exact computation of pC(v|h,v<t) and pR(h|v,v<t). Some approximations have been made to facilitate efficient training and inference, more details are available in (Taylor et al., 2011). A CRBM defines a probability distribution pC as a Gibbs distribution (1).\npC(vt,ht|v<t) = exp[\u2212EC(vt,ht|v<t)]/Z(\u03b8). (1)\nThe energy function EC(vt,ht|v<t) in (2) is defined in a manner similar to that of the RBM.\nEC-Real(vt,ht|v<t) = \u2212 \u2211 i(ci \u2212 vi,t)2/2\u2212 \u2211\nj djhj,t \u2212 \u2211\ni,j vi,twi,jhj,t, EC-Binary(vt,ht|v<t) = \u2212 \u2211 i civi,t \u2212 \u2211\nj djhj,t \u2212 \u2211\ni,j vi,twi,jhj,t, EC-Count(vt,ht|v<t) = \u2212 \u2211\ni(civi,t \u2212 log(vi,t!)) \u2212 \u2211 j djhj,t \u2212 \u2211 i,j vi,twi,jhj,t,\n(2)\nThe probability distributions for the visible nodes are defined in (3),\npC-Real(vi,t|ht,v<t) = N (ci + \u2211 j hj,twi,j , 1),\npC-Binary(vi,t = 1|ht,v<t) = \u03c3(ci + \u2211 j hj,twi,j),\npC-Count(vi,t|ht,v<t) = P(m, exp(ci + \u2211 j hjwij)), (3)\nwhere, N is a normal distribution, \u03c3 is a sigmoid distribution, and P is a Poisson distribution. The hidden nodes is defined in (4),\npC(hj,t = 1|vt,v<t) = \u03c3(dj + \u2211 i vi,twi,j). (4)\nci = ai + \u2211 p Ap,ivp,<t, dj = bj + \u2211 p Bp,jvp,<t. (5)\nDiscriminative CRBMs: DCRBMs are based on the model in (Larochelle & Bengio, 2008), generalized to account for temporal phenomenon using CRBMs. DCRBMs are a simpler version of the Factored Conditional Restricted Boltzmann Machines (Taylor et al., 2011) and Gated Restricted Boltzmann Machines (Memisevic & Hinton, 2007). Both these models incorporate labels in learning representations, however, they use a more complicated\npotential which involves three way connections into factors. DCRBMs define the probability distribution pDC as a Gibbs distribution (6).\npDC(yt,vt,ht|v<t;\u03b8) = 1Z(\u03b8) exp[\u2212EDC(yt,vt,ht|v<t)] (6)\nThe hidden layer h is defined as a function of the labels y and the visible nodes v. A new probability distribution for the classifier is defined to relate the label y to the hidden nodes h as in (7), as well as relate h to y as in (8). The new energy function EDC is shown in (9).\npDC(hj,t = 1|yt,vt,v<t) = \u03c3(dj + uj,k + \u2211 i vi,twij), (7)\npDC(yl,t|ht) = exp[sl + \u2211 j uj,lhj,t]\u2211\nl\u2217 exp[sl\u2217 + \u2211 j uj,l\u2217hj,t] (8)\nEDC(yt,vt,ht|v<t) = EC(vt,ht|v<t)\ufe38 \ufe37\ufe37 \ufe38 Generative \u2212 \u2211 j,l hj,tujlyl,t \u2212 \u2211 l\nslyl,t\ufe38 \ufe37\ufe37 \ufe38 Discriminative\n(9)\nCount-DCRBMs: We extend the DCRBM to CountDCRBM Figure 1. Count-DCRBMs are based on the model in (Salakhutdinov & Hinton, 2009), generalized to account for temporal phenomenon using CRBMs, and discriminative classification. Count-DCRBMs are used to model time varying histograms of counts. The probability distribution over the visible layer will follow a constrained Poisson distribution, pC-Count(vi,t|ht,v<t) defined in (3), the hidden layer follows (7) and the label layer follows (8) and the energy function EC-Count(vt,ht|v<t) defined in (9)."}, {"heading": "4. Inference and Learning", "text": "Inference: to perform classification at time t in the CountDCRBM given v<t and vt we use a bottom-up approach, computing a cost for each possible label yt then choosing the label with least cost. We compute the cost for label yt to be the free energy \u2212 log pDC(yt,vt|v<t) computed by marginalizing over h<t and ht. Then, the cost associated with the candidate label is the free energy in the CountDCRBM, namely\u2212 log pDC(yt,ht|h<t) is tractable, because the sum over exponentially many terms can be algebraically eliminated.\nLearning: the parameters our model could be learned using Contrastive Divergence (CD) (Hinton, 2002), where \u3008\u00b7\u3009data is the expectation with respect to the data distribution and \u3008\u00b7\u3009recon is the expectation with respect to the reconstructed data. The learning is done using two steps a bottom-up pass and a top-down pass using sampling equations from (3), (7), and (8). Bottom-up: the reconstruction is generated by first sampling the hidden layer p(ht,j = 1|vt,v<t, yl) for all the hidden nodes in parallel. Top-down: This is followed by sampling the visible nodes p(vi,t|ht,v<t) and p(yl,t|ht,h<t) for all the visible nodes in parallel."}, {"heading": "5. Experiments", "text": "We used the open-source simulator GPGPU-Sim (Bakhoda et al., 2009) to generate data to validate our approach. The simulator has been verified rigorously for accuracy against on a suite of 80 microbenchmarks (Leng et al., 2013). We used the BACKProp problem from the RODINIA benchmark (Che et al., 2010), and simulate a NVIDIA GTX480 GPU with the default configurations for GPGPU-Sim. This benchmark CUDA program trains a feedforward neural network with one hidden layer consisting of 4096 units.\nTo generate our dataset, we modified GPGPU-Sim to retrieve the time-indexed list of instruction mix, ie. for each time cycle the number of different instruction types based on opcode. These are the visible units in our model. We tested our approach on three different caches (Instruction (IC), Data Read (D R), Data Write (D W)) localized within one core of the GPU. For each cache, GPGPU-Sim outputs a list of time-indexed binary labels corresponding to whether a cache miss occured. Since we want to predict a cache miss ahead of time, we aggregated the labels over 128 cycles so a label of y(t) = 1 means that a cache miss occurred in cycles [t, t+ 128].\nThe Count-DCRBM was trained on a Tesla K20C GPU using Contrastive Divergence with a constant learning rate of 10\u22125. Table 1 shows the final accuracies of a model with 15 hidden nodes and varying temporal history available for DCRBM. The second and third columns are metrics that describe predictive power, taking into account false positives and negatives. We observe high accuracy and predictive power of the model for all three caches. We also observe that increased history generally leads to better performance despite the increased model complexity. Our baseline is an SVM that uses the raw instruction mix as features without any temporal history.\nModel accuracy can be misleading because cache miss events are rare (e.g. about 10% for IC). Figure 2 (Top) shows these metrics over training epochs for data write cache. Note that the initial model accuracy is already about 70% where the model predicts that cache miss never occurs, with a corresponding metric F1 and Mathew Correlation Coefficient (MCC) value of zero. As training epochs increase, we note a sharp increase in predictive power around 5000 epochs. We also show the reconstruction error in Figure 2 (Middle), the objective value for training, over epochs for the data write cache. We observe that the reconstruction error significantly drops in the first 20k epochs. Figure 2 (Bottom) shows a measure of the classification error, measured in terms of the binary cross entropy between the true and predicted labels. We also observe that the classification error continues to drop steadily even though the reconstruction error has converged, showing that the model accounts for label information. Figure 3 shows the pre-\ndiction using a history of 10 cycles, in comparison with the ground truth. Future work includes validating our approach across microbenchmarks."}, {"heading": "6. Conclusions", "text": "Our approach has significant implications for the GPU revolution of computing. A data driven approach can potentially identify mix of instructions that cause performance bottlenecks. Although we focused on cache misses, any statistic of interest to the computer architecture community such as power consumption and voltage can potentially be predicted. The extension to an online embedded setting is straightforward and could potentially save computation time. Prediction of performance bottlenecks is a step towards a cognitive processor architecture."}, {"heading": "Acknowledgments", "text": "This research is partially funded under NSF #1526399, the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL). The views, opinions and/or findings expressed are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government."}], "references": [{"title": "Analyzing Cuda Workloads Using a Detailed GPU Simulator", "author": ["Bakhoda", "Ali", "Yuan", "George L", "Fung", "Wilson WL", "Wong", "Henry", "Aamodt", "Tor M"], "venue": "In IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),", "citeRegEx": "Bakhoda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bakhoda et al\\.", "year": 2009}, {"title": "Branch Prediction for Free", "author": ["Ball", "Thomas", "Larus", "James R"], "venue": "In ACM,", "citeRegEx": "Ball et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Ball et al\\.", "year": 1993}, {"title": "ExaScale Computing Study: Technology Challenges in Achieving Exascale Systems. volume", "author": ["Bergman", "Keren"], "venue": null, "citeRegEx": "Bergman and Keren,? \\Q2008\\E", "shortCiteRegEx": "Bergman and Keren", "year": 2008}, {"title": "Challenge Benchmarks that must be Conquered to Sustain the GPU Revolution", "author": ["Blem", "Emily", "Sinclair", "Matthew", "Sankaralingam", "Karthikeyan"], "venue": "CELL,", "citeRegEx": "Blem et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Blem et al\\.", "year": 2011}, {"title": "Evidence-based Static Branch Prediction using Machine Learning", "author": ["Calder", "Brad"], "venue": "In ACM Transactions on Programming Languages and Systems (TOPLAS),", "citeRegEx": "Calder and Brad,? \\Q1997\\E", "shortCiteRegEx": "Calder and Brad", "year": 1997}, {"title": "A Characterization of the Rodinia Benchmark Suite with Comparison to Contemporary CMP Workloads", "author": ["Che", "Shuai", "Sheaffer", "Jeremy W", "Boyer", "Michael", "Szafaryn", "Lukasz G", "Wang", "Liang", "Skadron", "Kevin"], "venue": "IISWC,", "citeRegEx": "Che et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Che et al\\.", "year": 2010}, {"title": "Adaptive Cache Management for Energy-efficient GPU Computing", "author": ["Chen", "Xuhao", "Chang", "Li-Wen", "Rodrigues", "Christopher I", "Lv", "Jie", "Wang", "Zhiying", "Hwu", "Wen-Mei"], "venue": "In Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Dark Silicon and the End of Multicore Scaling", "author": ["Esmaeilzadeh", "Hadi", "et. al"], "venue": "In Proceedings of the International Symposium on Computer Architecture (ISCA),", "citeRegEx": "Esmaeilzadeh et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Esmaeilzadeh et al\\.", "year": 2011}, {"title": "Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow", "author": ["Fung", "Wilson WL", "Sham", "Ivan", "Yuan", "George", "Aamodt", "Tor M"], "venue": "In Proceedings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Fung et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2007}, {"title": "Training Products of Experts by Minimizing Contrastive Divergence", "author": ["Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Hinton and E.,? \\Q2002\\E", "shortCiteRegEx": "Hinton and E.", "year": 2002}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["Hinton", "Geoffrey E", "Osindero", "Simon", "Teh", "Yee-Whye"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Dynamic Branch Prediction with Perceptrons", "author": ["Jim\u00e9nez", "Daniel A", "Lin", "Calvin"], "venue": "In High-Performance Computer Architecture,", "citeRegEx": "Jim\u00e9nez et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Jim\u00e9nez et al\\.", "year": 2001}, {"title": "Classification using Discriminative Restricted Boltzmann Machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "In ICML,", "citeRegEx": "Larochelle and Bengio,? \\Q2008\\E", "shortCiteRegEx": "Larochelle and Bengio", "year": 2008}, {"title": "Many-thread Aware Prefetching Mechanisms for GPGPU Applications", "author": ["Lee", "Jaekyu", "Lakshminarayana", "Nagesh B", "Kim", "Hyesoon", "Vuduc", "Richard"], "venue": "In Microarchitecture,", "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "GPUWattch:Enabling Energy Optimizations in GPGPUs", "author": ["Leng", "Jingwen", "Hetherington", "Tayler", "El Tantawy", "Ahmed", "Gilani", "Syed", "Kim", "Nam Sung", "Aamodt", "Tor M", "Reddi", "Vijay Janapa"], "venue": "ACM SIGARCH,", "citeRegEx": "Leng et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2013}, {"title": "Safe Limits on Voltage Reduction Efficiency in GPUs: A Direct Measurement Approach", "author": ["Leng", "Jingwen", "Buyuktosunoglu", "Alper", "Bertran", "Ramon", "Bose", "Pradip", "Reddi", "Vijay Janapa"], "venue": "In Microarchitecture. ACM,", "citeRegEx": "Leng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leng et al\\.", "year": 2015}, {"title": "Priority-based Cache Allocation in Throughput Processors", "author": ["Li", "Dong", "Rhu", "Minsoo", "Johnson", "Daniel R", "O\u2019Connor", "Mike", "Erez", "Mattan", "Burger", "Doug", "Fussell", "Donald S", "Redder", "Stephen W"], "venue": "In HPCA,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Unsupervised Learning of Image Transformations", "author": ["R. Memisevic", "G.E. Hinton"], "venue": "In CVPR,", "citeRegEx": "Memisevic and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Memisevic and Hinton", "year": 2007}, {"title": "Phone Recognition using Restricted Boltzmann Machines", "author": ["A.R. Mohamed", "G.E. Hinton"], "venue": "In ICASSP,", "citeRegEx": "Mohamed and Hinton,? \\Q2009\\E", "shortCiteRegEx": "Mohamed and Hinton", "year": 2009}, {"title": "Future of Computer Architecture", "author": ["Patterson", "David"], "venue": "In Berkeley EECS Annual Research Symposium,", "citeRegEx": "Patterson and David.,? \\Q2006\\E", "shortCiteRegEx": "Patterson and David.", "year": 2006}, {"title": "Lightweight Detection and Recovery Mechanisms to Extend Algorithm Resiliency in Noisy Computation", "author": ["S. Chai"], "venue": "In WNTC,", "citeRegEx": "Chai,? \\Q2014\\E", "shortCiteRegEx": "Chai", "year": 2014}, {"title": "Reducing the Dimensionality of Data with Neural Networks", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "In Science,", "citeRegEx": "Salakhutdinov and Hinton,? \\Q2006\\E", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2006}, {"title": "Semantic Hashing", "author": ["Salakhutdinov", "Ruslan", "Hinton", "Geoffrey"], "venue": "In International Journal of Approximate Reasoning,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2009}, {"title": "Learning Multilevel Distributed Representations for High-Dimensional Sequences", "author": ["I. Sutskever", "G.E. Hinton"], "venue": "In AISTATS,", "citeRegEx": "Sutskever and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Sutskever and Hinton", "year": 2007}, {"title": "Two Distributed-State Models For Generating High-Dimensional Time Series", "author": ["Taylor", "Graham W", "Hinton", "Geoffrey E", "Roweis", "Sam T"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Taylor et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taylor et al\\.", "year": 2011}, {"title": "Power Analysis of Embedded Software: A First Step Towards Software Power Minimization", "author": ["Tiwari", "Vivek", "Malik", "Sharad", "Wolfe", "Andrew"], "venue": "VLSI Systems,", "citeRegEx": "Tiwari et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Tiwari et al\\.", "year": 1994}, {"title": "Hitting the Memory Wall: Implications of the Obvious", "author": ["Wulf", "Wm A", "McKee", "Sally A"], "venue": "In ACM SIGARCH computer architecture news,", "citeRegEx": "Wulf et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Wulf et al\\.", "year": 1995}, {"title": "Two-level Adaptive Training Branch Prediction", "author": ["Yeh", "Tse-Yu", "Patt", "Yale N"], "venue": "In ACM ISM,", "citeRegEx": "Yeh et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Yeh et al\\.", "year": 1991}], "referenceMentions": [{"referenceID": 8, "context": "We anticipate that classical problems such as branch predictions and cache management can be re-evaluated such that heuristically based approaches (Yeh & Patt, 1991; Fung et al., 2007; Li et al., 2015) can be replaced with a machine learning approach (Leng et al.", "startOffset": 147, "endOffset": 201}, {"referenceID": 16, "context": "We anticipate that classical problems such as branch predictions and cache management can be re-evaluated such that heuristically based approaches (Yeh & Patt, 1991; Fung et al., 2007; Li et al., 2015) can be replaced with a machine learning approach (Leng et al.", "startOffset": 147, "endOffset": 201}, {"referenceID": 15, "context": ", 2015) can be replaced with a machine learning approach (Leng et al., 2015).", "startOffset": 57, "endOffset": 76}, {"referenceID": 5, "context": "Processor data is widely available in the form of benchmarks (Che et al., 2010), and algorithms are extensively compared using these benchmarks (Blem et al.", "startOffset": 61, "endOffset": 79}, {"referenceID": 3, "context": ", 2010), and algorithms are extensively compared using these benchmarks (Blem et al., 2011).", "startOffset": 72, "endOffset": 91}, {"referenceID": 16, "context": "of predicting GPU Cache Misses (Li et al., 2015; Chen et al., 2014) for this paper.", "startOffset": 31, "endOffset": 67}, {"referenceID": 6, "context": "of predicting GPU Cache Misses (Li et al., 2015; Chen et al., 2014) for this paper.", "startOffset": 31, "endOffset": 67}, {"referenceID": 6, "context": "Predicting whether a cache miss is about to occur is useful for better cache management such as cache bypassing (Chen et al., 2014), pre-fetching (Lee et al.", "startOffset": 112, "endOffset": 131}, {"referenceID": 13, "context": ", 2014), pre-fetching (Lee et al., 2010), prioritized allocation (Li et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 16, "context": ", 2010), prioritized allocation (Li et al., 2015) etc.", "startOffset": 32, "endOffset": 49}, {"referenceID": 15, "context": "Further, cache misses indirectly cause increased energy and power usage (Leng et al., 2015) because of second order effects beyond memory latency.", "startOffset": 72, "endOffset": 91}, {"referenceID": 15, "context": "In principle, our approach is amenable to predict these higher order events (such as voltage scaling (Leng et al., 2015) and faults (S.", "startOffset": 101, "endOffset": 120}, {"referenceID": 25, "context": "Chai, 2014)) either directly (Tiwari et al., 1994) or via hierarchical modeling.", "startOffset": 29, "endOffset": 50}, {"referenceID": 24, "context": "We propose a new variant of the Conditional Restricted Boltzmann Machines (CRBMs) (Taylor et al., 2011) to directly address system performance and reliability.", "startOffset": 82, "endOffset": 103}, {"referenceID": 14, "context": "Prior work used a perceptron to predict cache misses (Leng et al., 2013).", "startOffset": 53, "endOffset": 72}, {"referenceID": 0, "context": "Our approach assumes the availability of a simulator for CUDA (Bakhoda et al., 2009) that can generate a dataset for training our model.", "startOffset": 62, "endOffset": 84}, {"referenceID": 10, "context": "Representation Learning: Restricted Boltzmann Machines (RBMs) form the building blocks in energy based deep networks (Hinton et al., 2006; Salakhutdinov & Hinton, 2006).", "startOffset": 117, "endOffset": 168}, {"referenceID": 24, "context": "These include Conditional RBMs (CRBMs) (Taylor et al., 2011) and Temporal RBMs (TRBMs) (Sutskever & Hinton, 2007).", "startOffset": 39, "endOffset": 60}, {"referenceID": 24, "context": "CRBMs have been used in both visual (Taylor et al., 2011) and audio (Mohamed & Hinton, 2009).", "startOffset": 36, "endOffset": 57}, {"referenceID": 24, "context": "Conditional Restricted Boltzmann Machines: CRBMs (Taylor et al., 2011), are a natural extension of RBMs for modeling short term temporal dependencies.", "startOffset": 49, "endOffset": 70}, {"referenceID": 24, "context": "Some approximations have been made to facilitate efficient training and inference, more details are available in (Taylor et al., 2011).", "startOffset": 113, "endOffset": 134}, {"referenceID": 24, "context": "DCRBMs are a simpler version of the Factored Conditional Restricted Boltzmann Machines (Taylor et al., 2011) and Gated Restricted Boltzmann Machines (Memisevic & Hinton, 2007).", "startOffset": 87, "endOffset": 108}, {"referenceID": 0, "context": "We used the open-source simulator GPGPU-Sim (Bakhoda et al., 2009) to generate data to validate our approach.", "startOffset": 44, "endOffset": 66}, {"referenceID": 14, "context": "The simulator has been verified rigorously for accuracy against on a suite of 80 microbenchmarks (Leng et al., 2013).", "startOffset": 97, "endOffset": 116}, {"referenceID": 5, "context": "We used the BACKProp problem from the RODINIA benchmark (Che et al., 2010), and simulate a NVIDIA GTX480 GPU with the default configurations for GPGPU-Sim.", "startOffset": 56, "endOffset": 74}], "year": 2017, "abstractText": "GPU activity prediction is an important and complex problem. This is due to the high level of contention among thousands of parallel threads. This problem was mostly addressed using heuristics. We propose a representation learning approach to address this problem. We model any performance metric as a temporal function of the executed instructions with the intuition that the flow of instructions can be identified as distinct activities of the code. Our experiments show high accuracy and non-trivial predictive power of representation learning on a benchmark.", "creator": "LaTeX with hyperref package"}}}