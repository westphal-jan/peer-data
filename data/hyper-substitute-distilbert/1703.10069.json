{"id": "1703.10069", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games", "abstract": "real - world artificial intelligence ( ai ) applications often allow multiple agents / maneuver in a collaborative effort. efficient explanations for co - agent problems and presents such an indispensable proposition as improving research. ignoring that paper, we take enterprise combat game as the test platform, where the task begins to blend multiple agents in a team to defeat their enemies. to maintain universally scalable yet effective communication program, we introduce a multiagent bidirectionally - coordinated network ( ie ['biknet ] ) showing a vectorised choice of trait - critic formulation. we show that bicnet operations handle different types of activities with diverse systems while absolute numbers employing concurrent analysts guarding both sides. our analysis differs : without grammatical supervisions such as personal questions or scientific questions, linguistic scholars learn various types of interactive processes consequently is problematic when these various experienced behavior advocates. moreover, logic is easily adaptable as the tasks directing heterogeneous agents. if our experiments, you include his approach maintaining multiple baselines under different problems ;, delivers state - of - mass - market performance, therefore displays accurate values for large - valued critical - interaction applications.", "histories": [["v1", "Wed, 29 Mar 2017 14:37:25 GMT  (8892kb,D)", "http://arxiv.org/abs/1703.10069v1", "13 pages, 10 figures"], ["v2", "Fri, 16 Jun 2017 07:01:31 GMT  (9211kb,D)", "http://arxiv.org/abs/1703.10069v2", "15 pages, 11 figures"], ["v3", "Tue, 20 Jun 2017 10:08:42 GMT  (9508kb,D)", "http://arxiv.org/abs/1703.10069v3", "15 pages, 11 figures"], ["v4", "Thu, 14 Sep 2017 12:45:46 GMT  (3356kb,D)", "http://arxiv.org/abs/1703.10069v4", "10 pages, 10 figures. Previously as title: \"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\", Mar 2017"]], "COMMENTS": "13 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["peng peng", "ying wen", "yaodong yang", "quan yuan", "zhenkun tang", "haitao long", "jun wang"], "accepted": false, "id": "1703.10069"}, "pdf": {"name": "1703.10069.pdf", "metadata": {"source": "CRF", "title": "Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games", "authors": ["Peng Peng", "Quan Yuan", "Ying Wen", "Yaodong Yang", "Zhenkun Tang", "Haitao Long", "Jun Wang"], "emails": ["jun.wang@cs.ucl.ac.uk;", "yuanquan.yq@alibaba-inc.com."], "sections": [{"heading": "1 Introduction", "text": "The last decade has witnessed massive progresses in the field of Artificial Intelligence (AI) [1]. With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].\nYet, true human intelligence embraces social and collective wisdom [9], which lays an essential foundation for reaching the grand goal of Artificial General Intelligence (AGI) [10]. As demonstrated by crowd sourcing [11], aggregating efforts collectively from the public would solve the problem which is otherwise unthinkable by a single person. Even social animals like a brood of well-organised ants could accomplish challenging tasks such as hunting, building a kingdom, and even waging a war [12], although each ant by itself is weak and limited. Interestingly, in the coming era of algorithmic economy, AI agents with a certain rudimentary level of artificial collective intelligence start to emerge from multiple domains. Typical examples include the trading robots gaming on the stock markets [13], ad bidding agents competing with each other over online advertising exchanges [14], and e-commerce collaborative filtering recommenders [15] predicting user interests through the wisdom of the crowd [16].\nA next grand challenge of AGI is to answer how large-scale multiple AI agents could learn humanlevel collaborations, or competitions, from their experiences with the environment where both of their incentives and economic constraints co-exist. As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.\n\u2217Correspondence to Jun Wang jun.wang@cs.ucl.ac.uk; Quan Yuan yuanquan.yq@alibaba-inc.com.\nar X\niv :1\n70 3.\n10 06\n9v 1\n[ cs\n.A I]\n2 9\nM ar\n2 01\nIn this paper, we leverage a real-time strategy game, StarCraft2, as the use case to explore the learning of intelligent collaborative behaviours among multiple agents. Particularly, we focus on StarCraft micromanagement tasks [23], where each player controls their own units (with different functions to collaborate) to destroy the opponent\u2019s army in the combats under different terrain conditions. Such game is considered as one of the most difficult games for computers with more possible states than Go game [23]. The learning of this large-scale multiagent system faces a major challenge that the parameters space grows exponentially with the increasing number of agents involved. As such, the behaviours of the agents can become so sophisticated that any joint learner method [20] would be inefficient and unable to deal with the dynamically changeable number of agents in the game.\nWe formulate multiagent learning for StarCraft combat tasks as a zero-sum Stochastic Game. Agents are communicated by our proposed bidirectionally-coordinated net (BiCNet), while the learning is done using a multiagent actor-critic framework. In addition, we also introduce the concept of dynamic grouping and parameter sharing to solve the scalability issue. Our analysis shows that BiCNet can automatically learn various optimal strategies to coordinate agents, similar to what experienced human players would adopt in playing the StarCraft game, ranging from trivial move without collision to a basic tactic hit and run to sophisticated cover attack and focus fire without overkill. We have conducted our experiments by testing over a set of combat tasks with different level of difficulties. Our method outperforms state-of-the-art methods and shows its potential usage in a wide range of multiagent tasks in the real-world applications."}, {"heading": "2 Related Work", "text": "The studies on interaction and collaboration in multiagent settings have a long history [24, 25, 18, 26]. Although limited to toy examples in the beginning, reinforcement learning, as a means, has long been applied to multiagent systems in order to learn optimal collaboration policies [27]. Typically, they are formalised as a stochastic game [28], and solved by minimax Q-learning [25]. As function approximators, neural networks have also been adopted and proven to be effective and flexible [18]. Nonetheless, with the increased complexity of the environment, these traditional methods no longer work well. For solving StarCraft combats, researchers resort to deep reinforcement learning (DRL) [5, 17, 6] due to the complexity of the environment and action space. For the analysis of complexity of StarCraft combat games, we refer to [29, 30]. One of the key components in using DRL is to learn a communication protocol among agents. Representative solutions include the differentiable inter-agent learning (DIAL) [19] and the CommNet [20], both of which are end-to-end trainable by back-propagation.\nDIAL [19] was introduced in partially observable settings where messages passing between agents are allowed. The agent is also named as a independent learner. The idea of learning independent agents can also be found [31\u201333, 19]. In DIAL, each agent consists of a recurrent neural network that outputs individual agent\u2019s Q-value and a message to transfer for each time-step. The generated messages is then to be transferred to other agents as inputs for the next time-step. When an agent receives the messages from others, it will embed the messages together with its current observations and last action in order to take into account the overall information. However, since DIAL is designed for independent learners, it inevitably faces the challenge of not being able to tackle the non-stationary environments; in other words, the environment will keep changing for each agent. Such non-stationary problem is even more severe in real-time strategy games such as StarCraft.\nBy contrast, CommNet [20] is designed for joint action learners in fully observable settings. Unlike DIAL, CommNet proposes a single network in the multiagent setting, passing the averaged message over the agent modules between layers. However, as the communication network is fully symmetric and embedded in the original network, it lacks the ability of handle heterogeneous agent types. Also it is a single network for all agents, and therefore its scalability is unclear. In this paper, we solve these issues by creating a dedicated bi-directional communication channel using recurrent neural networks [34]. As such, heterogeneous agents can be created with a different set of parameters and output actions. The bi-directional nature means that the communication is not entirely symmetric, and the different priority among agents would help solving any possible tie between multiple optimal joint actions [35, 36].\n2Trademark of Blizzard EntertainmentTM.\nAttention Neron Bi-directional RNN Policy Action Value Function Reward Shaping Agent\na1a2a3\n(a) Multiagent policy networks with grouping (b) Multiagent Q networks with reward shaping\nFigure 1: Bidirectionally-Coordinated Net (BiCNet).\nMultiagent systems have also been explored in more complex cases including StarCraft combat games [30]. Recent work in [37] applies the DIAL model [19] assuming agents in StarCraft are fully decentralised. The studies from [23, 30] focus on a greedy MDP approach, i.e., the action of an agent is dependent explicitly on the action of another agent that has has been generated. In this paper, the dependency of agents is, rather, modelled over hidden layers by making use of bi-directional recurrent neural networks (RNN) [34]. A significant benefit over the greedy solution is that, while keeping simple, its gradient update from all the actions is efficiently propagated through the entire networks."}, {"heading": "3 Multiagent Bidirectionally-Coordinated Nets (BiCNet)", "text": ""}, {"heading": "3.1 Preliminaries: Combat as Stochastic Games with Global Reward", "text": "The StarCraft combat, aka the micromanagement task, refers to the low-level, short-term control of the army members during a combat with enemy members [30]. We formulate it as a zero-sum Stochastic Game (SG) [28, 38, 39], i.e., a dynamic game in a multiple state situation played by multiple agents. A SG of N agents and M opponents (enemies in a combat) can be described by a tuple (S, {Ai}Ni=1, {Bi}Mi=1, T , {Ri} N+M i=1 }.\n\u2022 S denotes the state space of the current game, shared among all the agents; \u2022 Ai = A is the action space of the controlled agent i for i \u2208 [1, N ]; \u2022 Bi = B is the action space of the enemy i for i \u2208 [1,M ]; \u2022 T : S \u00d7AN \u00d7 BM \u2192 S is the deterministic transition function of the environment. \u2022 Ri : S \u00d7AN \u00d7 BM \u2192 R is the reward function of agent/enemy i for i \u2208 [1, N +M ].\nIn order to maintain a flexible framework and allow an arbitrary number of agents, we consider that all the agents, both the controlled and the enemies, share the same state space S of the current game; and within each camp, either the controlled\u2019s or the enemy\u2019s, agents are homogeneous3 and have the same action spaces A and B respectively. Also we consider the continuous action space instead of the discrete one in this work. It significantly reduces the redundancy in modelling the large discrete action space.\nIn the combat, the agents in our side or the other side are fully cooperative within their own teams, but compete between teams. Thus, each combat is a zero-sum competitive game between N agents and M enemies. In this paper, we propose to use the deterministic policy a\u03b8 : S \u2192 AN of the controlled agents and the deterministic policy b\u03c6 : S \u2192 BM of the enemies. The next is to define reward function. Reward shaping is of great importance in learning the collaborative or competing behaviours among agents [40]. In this section, as a starting point, we first consider a basic formulation with global reward (each agent in the same team shares the same reward), while leaving the definition of individual rewards and the resulting more general formulation in the next section. Specifically, we introduce a time-variant global reward based on the difference of the heath level between two consecutive time steps:\nr(s,a,b) \u2261 1 M M\u2211 j=N+1 \u2206Rtj(s,a,b)\u2212 1 N N\u2211 i=1 \u2206Rti(s,a,b), (1)\nwhere for simplicity, we drop the subscript t in global reward r(s,a,b). For given time step t with state s, the controlled agents take actions a \u2208 AN , the opponents take actions b \u2208 BM , and \u2206Rtj(\u00b7) \u2261 R t\u22121 j (s,a,b) \u2212 Rtj(s,a,b) represents the reduced health level for agent j. Note that Eq.(1) is presented from the aspect of controlled agents; the enemy\u2019s global reward is the exact opposite, making the sum of rewards from both camps equalling to zero. As the health level is non-decreasing over time, Eq. (1) gives a positive reward at time step t if the increase of enemies\u2019 health levels exceeds that of ours.\nWith the defined global reward r(s,a,b), the controlled agents jointly take actions a in state s when the enemies take joint actions b. The objective of the controlled agents is to learn a policy that maximises the expected sum of discounted rewards, i.e., E[ \u2211+\u221e k=0 \u03bb\nkrt+k], where 0 \u2264 \u03bb < 1 is discount factor. Conversely, the enemies\u2019 joint policy is to minimise the expected sum. Correspondingly, we have the following Minimax game:\nQ\u2217SG(s,a,b) =r(s,a,b) + \u03bbmax \u03b8 min \u03c6 Q\u2217SG(s \u2032,a\u03b8(s \u2032),b\u03c6(s \u2032)), (2)\nwhere s\u2032 \u2261 st+1 is determined by T (s,a,b). Q\u2217SG(s,a,b) is the optimal action-state value function, which follows the Bellman Optimal Equation.\nIn small-scale MARL problems, a common solution is to employ Minimax Q-learning [25]. However, the micromanagement combat in StarCraft is computationally expensive [30], and therefore make minimax Q-learning intractable to apply. In addition, solving the adversarial Q-function of Eq.(2) also requires an opponent/enemy modelling [35, 41]. We employ fictitious play [28] and deep neural nets to learn the enemies policy b\u03c6 in two situations, respectively. When the enemies are AI agents, we consider fictitious play, where both the controlled agents and the enemies apply empirical strategies learned from the other players so far, and then iteratively optimise the above Q-function. Alternatively, when dealing with the Game\u2019s own control of the enemies, we modelled the deterministic policy b\u03c6 through DNN in a supervised learning setting. Specifically, the policy network is trained together with sampled historical state-action pairs (s,b) of the enemies:\n4\u03c6 \u221d\u2202b\u03c6(s) \u2202\u03c6 . (3)\nBy learning the policy of enemies and fixing them, SG defined in Eq. (2) effectively turns into a simper MDP problem [41]:\nQ\u03b8(s,a) =r(s,a) + \u03bbQ\u03b8(s\u2032,a\u03b8(s \u2032)), (4)\nwhere we drop notation b\u03c6 for brevity.\n3We, however, demonstrate from latter section that with our framework heterogeneous agents can be also trained using different parameters and action space."}, {"heading": "3.2 Proposed Multiagent Actor-Critic with Individual Reward", "text": "A potential drawback of the global reward in Eq. (1) and its resulting zero-sum game is that it ignores the fact that a team collaboration, typically consisting of local collaborations and reward function, would normally include certain internal structure. Moreover, in practice, each agent tends to have its own objective which drives the collaboration. To model this, we extend the formulation in the previous section by replacing Eq. (1) with each agent\u2019s local reward and including the evaluation of their attribution to other agents that have been interacting with (either completing or collaborating), i.e.,\nri(s,a,b) \u2261 1\n|j| M\u2211 j=N+1\u2229top-K(i) \u2206Rj(s,a,b)\u2212 1 |i\u2032| N\u2211 i\u2032=1\u2229top-K(i) \u2206Ri\u2032(s,a,b) (5)\nwhere each agent i maintaining top-K(i), a top-K list of other agents that are currently being interacted. Replacing it with Eq. (1), we have N number of Bellman equations for agent i, where i \u2208 {1, ..., N}, for the same parameter \u03b8 of the policy function:\nQ\u03b8i (s,a) =ri(s,a) + \u03bbQ \u03b8 i (s \u2032,a\u03b8(s \u2032)). (6)\nWe can then write the objective as an expectation:\nJ(\u03b8) =Es\u223cp1 [ N\u2211 i=1 ri(s,a\u03b8(s))] = \u222b S p1(s) N\u2211 i=1 Q\u03b8i (s,a)|a=a\u03b8(s)ds. (7)\nIn continuous action spaces, the classical model-free policy iteration methods become intractable, as the greedy policy search requires a global maximisation at every step. Here we develop a vectorised version of deterministic policy gradient (DPG) on the basis of [17, 42, 43] for searching the policy in the direction of the gradient of the sum of Qi instead of maximising it. Taking the derivative of Eq. (7) gives:\n\u2207\u03b8J(\u03b8) = \u2202\n\u2202\u03b8 \u222b S p1(s) N\u2211 i=1 Qa\u03b8i (s,a)ds = Es\u223c\u03c1Ta\u03b8 (s)  N\u2211 j ( N\u2211 i=1 \u2202Qa\u03b8i (s,a)|a=a\u03b8(s) \u2202aj )\u2202aj,\u03b8(s) \u2202\u03b8  , (8)\nwhere the gradients pass to both individual Qi function and the policy function. They are aggregated from all the agents and their actions. In other words, the gradients from all agents rewards are first propagated to influence each of agent\u2019s actions, and the resulting gradients are further propagated back to updating the parameters.\nTo ensure adequate exploration, we considered the off-policy deterministic actor-critic algorithms [17, 42, 43]. Particularly, we employ a critic to estimate the action-value function where off-policy explorations can also be conducted. We thus have the following gradient for learning the parameters of critic Q\u03be(s,a) aggregated from multiple agents:\n\u2207\u03beL(\u03be) = Es\u223c\u03c1Ta\u03b8 (s) [ N\u2211 i=1 ( ri(s,a\u03b8(s)) + \u03bbQ \u03be(s\u2032,a\u03b8(s \u2032))\u2212Q\u03be(s,a\u03b8(s)) ) \u2202Q\u03bei (s,a\u03b8(s)) \u2202\u03be ] . (9)\nWith Eqs. (8) and (9), we are ready to use Stochastic Gradient Descent (SGD) to compute the updates for both the actor and the critic networks.\nWe next introduce our design for the two networks. For each agent, we share their parameters so that the number of parameters is independent of the number of agents. The resulting more compact model would be able to learn over various situations experienced from multiple agents, and thus speed up the learning process. Additionally, as later demonstrated in our experiment, such shared parameter space would allow us to train using only a smaller number of of agents (typically three), while freely scaling up to any larger number of agents during the test.\nTo make effective communication between agents, we introduce dedicated bi-directional connections in the internal layers by employing bi-directional recurrent neural networks (RNN) [34]. As we discussed, it is different with the greedy approach from [23, 30] in that the dependency of our\nagents are built upon the internal layers, rather than directly from the actions. While simple, our approach allow full dependency among agents because the gradients from all the actions in Eq. (9) are efficiently propagated through the entire networks. Yet, unlike CommNet [20], our communication is not fully symmetric, and we maintain certain social conventions and roles by fixing the order of the agents that join the RNN. This would help solving any possible tie between multiple optimal joint actions [35, 36].\nThe structure of our bidirectionally-coordinated net (BiCNet) is illustrated in Fig. 1. It consists of the policy networks (actor) and the Q-networks (critic), both of which are based on bi-directional RNN. The policy networks, taking a shared observation together with a local view, return actions for individual agents. Therefore, individual agents are able to maintain their own internal states, while being able to share the information with other collaborators. The bi-directional recurrent mechanism acts not only as a communication means between agents but also as a local memory state. We further intensify the actor with the concept of grouping [44], which plays an important role in influencing social behaviours. As an input we allow a small number of agents to build a local correlation before fed into the recurrent layers. As such, the model scales much better. In the experiments, we found that the group-based actor can help learning the group activities such as focusing fire. The Q-value networks, taking the state and the actions from the policy networks as inputs, return the estimated local Q-value for each individual agent. The local Q-value is then combined to provide the estimation of the global reward."}, {"heading": "4 Analysis and Discussion on Learned Coordination Strategies", "text": "With adequate trainings from scratch, BiCNet would be able to discover several effective collaboration strategies. In this section, we conduct a qualitative analysis on the collaboration policies that BiCNet has learned. For the detailed experiment configurations (as well as the parameter tuning and the performance comparisons), we refer to Section 5.\nCoordinated moves without collision. We observe that, in the initial stages of learning (as illustrated in Fig. 2 (a) and (b)), the agents move in a rather uncoordinated way, particularly, when two agents are close to each other, i.e., one agent may unintentionally block the other\u2019s path. With the increasing rounds of training (typically over 40k steps in around 50 episodes in the \u201c3 Marines vs. 1 Super Zergling\u201d combat setting), the number of collisions reduces dramatically. Finally, when the training bocame stable, the coordinated moves emerge, as illustrated in the right two scenes in Fig. 2. Such coordinated moves are also demonstrated in Fig. 6 (a) and (b).\nHit and Run tactics. For human players, a common tactic of controlling agents in StarCraft combat is Hit and Run, i.e., move agents away if under attack, and fight back when feel safe again. We find that BiCNet can rapidly grasp the tactic of Hit and Run, either in the case of single agent or multiple agents settings. This is illustrated in Fig. 3. Despite the simplicity of Hit and Run, it serves as the basis for more advanced and sophisticated collaboration tactics to be explained next.\nCoordinated cover attack. Cover attack is a high-level collaborative strategy that is often used on the real battlefield. The essence of cover attack is to let one agent draw fire or attention from the enemies. At the meantime, other agents can take advantage of the time or distance gap to cause more harms. The difficulty of conducting cover attack lies in how to turn the moves of multiple agents into a coordinated sequential hit and run moves. As shown in Figs. 4 and 5, BiCNet can master it well.\nIn Fig. 4 time step 1, BiCNet controls the bottom two Dragoons to run away from the enemy Ultralisk, while the one in the upper-right corner immediately starts to attack the enemy Ultralisk (e.g., cover them up). As a response, the enemy starts to attack the top one in time step 2. The bottom two Dragoons fight back and form another cover-up, consequently. By continuously looping this over, the team of Dragoons guarantees consecutive attack outputs to the enemy while minimising the team-level damages (because the enemy wastes time in targeting different Dragoons) until the enemy is killed.\nInterestingly, we also find that the coordinated cover attack strategy appears only for combats with a certain level of difficulty. In StarCraft combat, the difficulty of a combat can be adjusted by changing the number of enemies and the hit points and the damage of each enemy. The hit points of a unit is referred to the amount of damage this unit may take before it is destroyed; the damage of a unit is the loss of hit points made by this unit when attacking others. Table 1 gives the winning rates under the different configurations of the hit points and damage.\nIn combat \u201c3 Marines vs. 1 Super Zergling\u201d (shown in Figure 5), we fix the number of enemies (i.e., 1) and adjust the difficulty by changing the hit points and the damage of Zergling. In this case, BiCNet only masters the policy of cover attack when the hit points of the Zergling are higher than 210 and the damage is set to 4 \u2014 the original hit points and damage of a Zergling are 35 and 5, respectively.\nFocus fire without overkill. As the number of agents increases, how to efficiently allocate the attacking resources becomes important. Neither scattering over all enemies nor focusing on one enemy (wasting attacking fires is also called overkill) are desired. The grouping design in the policy network serves as the key factor for BiCNet to learn \u201cfocus fire without overkill\u201d. In our experiments, we dynamically group the agents based on agents\u2019 geometric locations. Based on the grouping inputs, BiCNet manages to capture the intra-group behaviour and the inter-group behaviour among the agents. For the agents from the same group, their behaviours are generally consistent and are expected to concentrate their fires on one or two enemies. For the agents from different groups, they are expected to focus fire on different enemies. In Fig. 6, we set up group size as 6 in the \u201c15 Marines vs. 16 Marines\u201d battle; our units were roughly assigned to 3 groups. We found that the agents learn to focus their fires on attacking 2-3 enemies, and different groups of agents are able to learn to move to the sides to spread the fire. Even with the decreasing of our unit number, each group can be dynamically resigned to make sure that the 3-5 units focus on attacking one same enemy.\nCollaborations between heterogeneous agents. In StarCraft, there are tens of types of agent units, each with unique functionalities, action space, strength, and weakness. For a combat with different types of units involved, we still expect the collaborations can be done based on their attributes. In fact, heterogeneous collaborations can be easily implemented in our framework by limiting the parameter sharing only to the same types of the units. In this paper, we study a simple case where two Dropships and two tanks collaborate to fight against an Ultralisk. A Dropship does not have the function of attack, but it can carry maximally two ground units in the air. As shown in Fig. 7, when the Ultralisk is attacking one of the tanks, the Dropship escorts the tank to escape from the attack. At the same time, the other Dropship unloads his tank to the ground so as to attack the Ultralisk. At each side, the collaboration between the Dropship and the tank keeps iterating until the Ultralisk has been destroyed."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "To understand the properties of our proposed BiCNet and its performance, we conducted experiments over different settings of the StarCraft combats. Following similar experiment set-up as [20], BiCNet\ncontrols a group of agents trying to defeat the enemy units controlled by the built-in AI. The level of combat difficulties can be adjusted by varying the unit types and the number of units in both sides. We measured the winning rates, and compared it with the state-of-the-art approaches. The comparative baselines consist of both the rule-based approaches and deep reinforcement learning approaches. Our setting is summarised as follows where BiCNet controls the former units and the built-in AI controls the latter:\n\u2022 Easy combats: 3 Marines vs. 1 Super Zergling, and 3 Wraiths vs. 3 Mutalisks. \u2022 Difficult combats: 5 Marines vs. 5 Marines, 15 Marines vs. 16 Marines, 20 Marines vs. 30\nZerglings, 10 Marines vs. 13 Zerglings, and 15 Wraiths vs. 17 Wraiths.\n\u2022 Heterogeneous combats: 2 Dropships and 2 Tanks vs. 1 Ultralisk.\nDespite the fact that rule-based approaches are usually trivial, they allow us to have a reference point that we could make sense of. Here we adopted three rule-based baselines: StarCraft built-in AI, Attack the Weakest, Attack the Closest.\nFor the deep reinforcement learning approaches, we considered the following the baselines:\nIndependent controller (IND): We trained the model for single agent and control each agent individually in the combats. Note that there is no information sharing among different agents even though such method is easily adaptable to all kinds of multiagent combats.\nFully-connected (FC): We trained the model for all agents in a multiagent setting and control them collectively in the combats. The communication between agents are fully-connected. We need to re-train a different model when the number of units at either side changes.\nCommNet: CommNet [20] is a multiagent network designed to learning to communicate among multiple agents. To make a fair comparison, we implement bot the CommNet and the BiCNet on the same (state, action) spaces and follow the same training processes.\nGreedyMDP with Episodic Zero-Order Optimisation(GMEZO): GMEZO [30] was proposed particularly to solve StarCraft micromanagement tasks. Apart from traditional DRL approaches, two novel ideas are introduced: conducting collaborations through a greedy update over MDP agents, as well as adding episodic noises in the parameter space for explorations. To focus on the comparison with these two ideas, we replaced our bi-directional formulation with the greedy MDP approach, and employed episodic zero-order optimisation with noise over the parameter space in the last layer of Q networks in our BiCNet."}, {"heading": "5.2 Parameter Tuning", "text": "In our training, Nadam [45] is set as the optimiser with learning rate equal to 0.002 and the other arguments set by default values. We set the maximum steps of each episode as 800.\nWe first study the impact of the batch size and skip frame (how many frames we should skip for controlling the agents actions). Figure 8 gives the results of the parameter tuning in the \u201c2 Marines vs. 1 Super Zergling\u201d combat. The two metrics, the winning rate and the Q value, are given. We fine-tune the batch_size and the skip_frame by selecting the best BiCNet model which are trained on 800 episodes (more than 700k steps) and then tested on 100 independent games. As suggested in the\nfigures, when batch_size is 32 and skip_frame is 2, the model has the highest winning rates. Also, the model with batch_size 32 achieves the highest mean Q-value after 600k training steps, while the model with skip frame 2 has the highest mean Q-value between 300k and 600k training steps. We fix both parameters with the learned optimal values in the remaining of our test.\nNext, we analyse the impact of the dynamic group size on the winning rate. Empirical results in Figure 9 show that setting 4-5 as group size would help achieve best performance. This is consistent with the discussion about the focus fire strategy in Section 4, where we discover that letting 4-6 agents work together as a group can efficiently control individual agents while maximising the damage output."}, {"heading": "5.3 Performance Comparison", "text": "Table 2 compares our proposed BiCNet model against the baselines under multiple combat scenarios. In each scenario, BiCNet is trained over 100k steps, and we measure the performance as the average winning rate on 100 test games. The winning rate of the built-in AI is also provided as an indicator of the level of difficulty of the combats.\nAs illustrated in Table 2, in 4/5 of the scenarios, BiCNet outperforms the other baseline models. In particular, when the number of agents goes beyond 10, where cohesive collaborations are required, the margin of performance between BiCNet and the second best starts to increase. Apart from the winning rate, in Fig. 10, we also compare the convergence speed by plotting the winning rate against the number of training episodes. It shows that the proposed BiCNet model has a much quicker convergence than the rest.\nIn combat \u201c5 M vs. 5 M\u201d, where the key factor to win is to \u201cfocus fire\u201d on the weak, the IND and the FC models have relatively poorer performance. We believe it is because both of the models do not come with an explicit collaboration mechanism between agents in the training stage; Coordinating the attacks towards one single enemy is even challenging. On the contrary, GMEZO, CommNet, and BiCNet, which are designed for the multiagent collaboration, can grasp and master this simple strategy, thus enjoying better performances. As BiCNet has built-in design for dynamic grouping, small number of agents (such as the case \u201c5 M vs. 5 M\u201d ) does not suffice to show the advantages of BiCNet on large-scale collaborations. Furthermore, it is worth mentioning that despite the second best performance on \u201c5 Marines vs. 5 Marines\u201d, our BiCNet only needs 10 combats before learning the idea of \u201cfocus fire\u201d, and achieves over 85% win rate, whereas CommNet needs more than 50 episodes to grasp the skill of \u201cfocus fire\u201d with a much lower winning rate.\n1 2 3 4 5 6 7 8 9 10 Group Size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nW in\nin g\nR at\ne\nFigure 9: Winning rate vs. group size in combat \u201c10 Marines vs. 13 Zerglings. 20 40 60 80 100 120 140 160 Num. Episodes\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nW in\nin g\nR at\ne\nBiCNet CommNet GMEZO\nFigure 10: Learning Curves in Combat \u201c10 Marines vs. 13 Zerglings\u201d"}, {"heading": "6 Conclusions", "text": "In this paper, we have introduced a new deep multiagent reinforcement learning framework, making use of bi-directional neural networks. The collaboration is learned by constructing a vectorised actor-critic framework, where each dimension corresponds to an agent. The coordination is done by bi-directional communications in the internal layers. Through end-to-end learning, our BiCNet would be able to successfully learn several effective coordination strategies. Our experiments have demonstrated its ability to collaborate and master diverse combats in StarCraft combat games.\nIn our experiments, we found that there was a strong correlation between the specified rewards and the learned policies. We plan to further investigate the relationships and study how the policies are communicated over the networks among agents, and whether there is a specific language that may emerged [21, 22]. In addition, it is of interest to explore Nash equilibrium when both sides are played by deep multiagent models."}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Heads-up limit hold\u2019em poker is solved", "author": ["Michael Bowling", "Neil Burch", "Michael Johanson", "Oskari Tammelin"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Safe and nested endgame solving for imperfect-information games", "author": ["Noam Brown", "Tuomas Sandholm"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2017}, {"title": "Labeling images with a computer game", "author": ["Luis Von Ahn", "Laura Dabbish"], "venue": "In Proceedings of the SIGCHI conference on Human factors in computing systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Social evolution in ants", "author": ["Andrew FG Bourke", "Nigel R Franks"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Trading on the edge: neural, genetic, and fuzzy systems for chaotic financial markets, volume 39", "author": ["Guido Deboeck"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "Display advertising with real-time bidding (RTB) and behavioural targeting. Foundations and Trends R  \u00a9 in Information", "author": ["Jun Wang", "Weinan Zhang", "Shuai Yuan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Recommender systems in e-commerce", "author": ["J Ben Schafer", "Joseph Konstan", "John Riedl"], "venue": "In Proceedings of the 1st ACM conference on Electronic commerce,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Unifying user-based and item-based collaborative filtering approaches by similarity fusion", "author": ["Jun Wang", "Arjen P De Vries", "Marcel JT Reinders"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Continuous control with deep reinforcement learning", "author": ["Timothy P Lillicrap", "Jonathan J Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "A general method for multi-agent reinforcement learning in unrestricted environments. In Adaptation, Coevolution and Learning in Multiagent Systems: Papers", "author": ["Jurgen Schmidhuber"], "venue": "AAAI Spring Symposium,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["Jakob Foerster", "Yannis M Assael", "Nando de Freitas", "Shimon Whiteson"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Learning multiagent communication with backpropagation", "author": ["Sainbayar Sukhbaatar", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Multi-agent cooperation and the emergence of (natural) language", "author": ["Angeliki Lazaridou", "Alexander Peysakhovich", "Marco Baroni"], "venue": "arXiv preprint arXiv:1612.07182,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Emergence of grounded compositional language in multi-agent populations", "author": ["Igor Mordatch", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1703.04908,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2017}, {"title": "Torchcraft: a library for machine learning research on real-time strategy games", "author": ["Gabriel Synnaeve", "Nantas Nardelli", "Alex Auvolat", "Soumith Chintala", "Timoth\u00e9e Lacroix", "Zeming Lin", "Florian Richoux", "Nicolas Usunier"], "venue": "arXiv preprint arXiv:1611.00625,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["Ming Tan"], "venue": "In Proceedings of the tenth international conference on machine learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1993}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["Michael L Littman"], "venue": "In Proceedings of the eleventh international conference on machine learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1994}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["Caroline Claus", "Craig Boutilier"], "venue": "AAAI/IAAI, 1998:746\u2013752,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "Multi-agent reinforcement learning: A modular approach", "author": ["Norihiko Ono", "Kenji Fukumoto"], "venue": "In Second International Conference on Multiagent Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1996}, {"title": "Game theory", "author": ["Guillermo Owen"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1995}, {"title": "A survey of real-time strategy game ai research and competition in starcraft", "author": ["Santiago Ontan\u00f3n", "Gabriel Synnaeve", "Alberto Uriarte", "Florian Richoux", "David Churchill", "Mike Preuss"], "venue": "IEEE Transactions on Computational Intelligence and AI in games,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks", "author": ["Nicolas Usunier", "Gabriel Synnaeve", "Zeming Lin", "Soumith Chintala"], "venue": "arXiv preprint arXiv:1609.02993,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "An algorithm for distributed reinforcement learning in cooperative multi-agent systems", "author": ["Martin Lauer", "Martin Riedmiller"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning. Citeseer,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2000}, {"title": "Reinforcement learning of coordination in cooperative multiagent systems", "author": ["Spiros Kapetanakis", "Daniel Kudenko"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2002}, {"title": "Reinforcement learning for stochastic cooperative multi-agent systems", "author": ["Martin Lauer", "Martin Riedmiller"], "venue": "In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Bidirectional recurrent neural networks", "author": ["Mike Schuster", "Kuldip K Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Lucian Busoniu", "Robert Babuska", "Bart De Schutter"], "venue": "IEEE Transactions on Systems Man and Cybernetics Part C Applications and Reviews,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "High level coordination of agents based on multiagent markov decision processes with roles", "author": ["Matthijs TJ Spaan", "Nikos Vlassis", "Frans CA Groen"], "venue": "In IROS,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Stabilising experience replay for deep multi-agent reinforcement learning", "author": ["Jakob Foerster", "Nantas Nardelli", "Gregory Farquhar", "Philip Torr", "Pushmeet Kohli", "Shimon Whiteson"], "venue": "arXiv preprint arXiv:1702.08887,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Stochastic games", "author": ["Lloyd S Shapley"], "venue": "Proceedings of the national academy of sciences,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1953}, {"title": "Analyzing and visualizing multiagent rewards in dynamic and stochastic domains", "author": ["Adrian K Agogino", "Kagan Tumer"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2008}, {"title": "Opponent modeling in deep reinforcement learning", "author": ["He He", "Jordan Boyd-Graber", "Kevin Kwok", "Hal Daum\u00e9 III"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2016}, {"title": "Deterministic policy gradient algorithms", "author": ["David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller"], "venue": "In ICML,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "The evolution of social behavior", "author": ["Richard D Alexander"], "venue": "Annual review of ecology and systematics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1974}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E Dahl", "Geoffrey E Hinton"], "venue": "ICML (3),", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The last decade has witnessed massive progresses in the field of Artificial Intelligence (AI) [1].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].", "startOffset": 123, "endOffset": 129}, {"referenceID": 2, "context": "With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].", "startOffset": 154, "endOffset": 157}, {"referenceID": 3, "context": "With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].", "startOffset": 279, "endOffset": 282}, {"referenceID": 4, "context": "With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].", "startOffset": 292, "endOffset": 295}, {"referenceID": 5, "context": "With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].", "startOffset": 311, "endOffset": 317}, {"referenceID": 6, "context": "With supervision from labelled data, machines have, to some extent, exceeded human-level perception on visual recognitions [2, 3] and speech recognitions [4], while fed with feedback reward, single AI units (aka agents) defeat humans in various games including Atari video games [5], Go game [6], and card game [7, 8].", "startOffset": 311, "endOffset": 317}, {"referenceID": 7, "context": "As demonstrated by crowd sourcing [11], aggregating efforts collectively from the public would solve the problem which is otherwise unthinkable by a single person.", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "Even social animals like a brood of well-organised ants could accomplish challenging tasks such as hunting, building a kingdom, and even waging a war [12], although each ant by itself is weak and limited.", "startOffset": 150, "endOffset": 154}, {"referenceID": 9, "context": "Typical examples include the trading robots gaming on the stock markets [13], ad bidding agents competing with each other over online advertising exchanges [14], and e-commerce collaborative filtering recommenders [15] predicting user interests through the wisdom of the crowd [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "Typical examples include the trading robots gaming on the stock markets [13], ad bidding agents competing with each other over online advertising exchanges [14], and e-commerce collaborative filtering recommenders [15] predicting user interests through the wisdom of the crowd [16].", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "Typical examples include the trading robots gaming on the stock markets [13], ad bidding agents competing with each other over online advertising exchanges [14], and e-commerce collaborative filtering recommenders [15] predicting user interests through the wisdom of the crowd [16].", "startOffset": 214, "endOffset": 218}, {"referenceID": 12, "context": "Typical examples include the trading robots gaming on the stock markets [13], ad bidding agents competing with each other over online advertising exchanges [14], and e-commerce collaborative filtering recommenders [15] predicting user interests through the wisdom of the crowd [16].", "startOffset": 277, "endOffset": 281}, {"referenceID": 3, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 55, "endOffset": 65}, {"referenceID": 13, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 55, "endOffset": 65}, {"referenceID": 4, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 55, "endOffset": 65}, {"referenceID": 14, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 131, "endOffset": 138}, {"referenceID": 15, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 131, "endOffset": 138}, {"referenceID": 16, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 131, "endOffset": 138}, {"referenceID": 17, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 131, "endOffset": 138}, {"referenceID": 18, "context": "As the flourishes of deep reinforcement learning (DRL) [5, 17, 6], researchers start to shed light on tackling multiagent problems [18\u201322] with the enhanced learning capabilities.", "startOffset": 131, "endOffset": 138}, {"referenceID": 19, "context": "Particularly, we focus on StarCraft micromanagement tasks [23], where each player controls their own units (with different functions to collaborate) to destroy the opponent\u2019s army in the combats under different terrain conditions.", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "Such game is considered as one of the most difficult games for computers with more possible states than Go game [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "As such, the behaviours of the agents can become so sophisticated that any joint learner method [20] would be inefficient and unable to deal with the dynamically changeable number of agents in the game.", "startOffset": 96, "endOffset": 100}, {"referenceID": 20, "context": "The studies on interaction and collaboration in multiagent settings have a long history [24, 25, 18, 26].", "startOffset": 88, "endOffset": 104}, {"referenceID": 21, "context": "The studies on interaction and collaboration in multiagent settings have a long history [24, 25, 18, 26].", "startOffset": 88, "endOffset": 104}, {"referenceID": 14, "context": "The studies on interaction and collaboration in multiagent settings have a long history [24, 25, 18, 26].", "startOffset": 88, "endOffset": 104}, {"referenceID": 22, "context": "The studies on interaction and collaboration in multiagent settings have a long history [24, 25, 18, 26].", "startOffset": 88, "endOffset": 104}, {"referenceID": 23, "context": "Although limited to toy examples in the beginning, reinforcement learning, as a means, has long been applied to multiagent systems in order to learn optimal collaboration policies [27].", "startOffset": 180, "endOffset": 184}, {"referenceID": 24, "context": "Typically, they are formalised as a stochastic game [28], and solved by minimax Q-learning [25].", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Typically, they are formalised as a stochastic game [28], and solved by minimax Q-learning [25].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "As function approximators, neural networks have also been adopted and proven to be effective and flexible [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 3, "context": "For solving StarCraft combats, researchers resort to deep reinforcement learning (DRL) [5, 17, 6] due to the complexity of the environment and action space.", "startOffset": 87, "endOffset": 97}, {"referenceID": 13, "context": "For solving StarCraft combats, researchers resort to deep reinforcement learning (DRL) [5, 17, 6] due to the complexity of the environment and action space.", "startOffset": 87, "endOffset": 97}, {"referenceID": 4, "context": "For solving StarCraft combats, researchers resort to deep reinforcement learning (DRL) [5, 17, 6] due to the complexity of the environment and action space.", "startOffset": 87, "endOffset": 97}, {"referenceID": 25, "context": "For the analysis of complexity of StarCraft combat games, we refer to [29, 30].", "startOffset": 70, "endOffset": 78}, {"referenceID": 26, "context": "For the analysis of complexity of StarCraft combat games, we refer to [29, 30].", "startOffset": 70, "endOffset": 78}, {"referenceID": 15, "context": "Representative solutions include the differentiable inter-agent learning (DIAL) [19] and the CommNet [20], both of which are end-to-end trainable by back-propagation.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "Representative solutions include the differentiable inter-agent learning (DIAL) [19] and the CommNet [20], both of which are end-to-end trainable by back-propagation.", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "DIAL [19] was introduced in partially observable settings where messages passing between agents are allowed.", "startOffset": 5, "endOffset": 9}, {"referenceID": 27, "context": "The idea of learning independent agents can also be found [31\u201333, 19].", "startOffset": 58, "endOffset": 69}, {"referenceID": 28, "context": "The idea of learning independent agents can also be found [31\u201333, 19].", "startOffset": 58, "endOffset": 69}, {"referenceID": 29, "context": "The idea of learning independent agents can also be found [31\u201333, 19].", "startOffset": 58, "endOffset": 69}, {"referenceID": 15, "context": "The idea of learning independent agents can also be found [31\u201333, 19].", "startOffset": 58, "endOffset": 69}, {"referenceID": 16, "context": "By contrast, CommNet [20] is designed for joint action learners in fully observable settings.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "In this paper, we solve these issues by creating a dedicated bi-directional communication channel using recurrent neural networks [34].", "startOffset": 130, "endOffset": 134}, {"referenceID": 31, "context": "The bi-directional nature means that the communication is not entirely symmetric, and the different priority among agents would help solving any possible tie between multiple optimal joint actions [35, 36].", "startOffset": 197, "endOffset": 205}, {"referenceID": 32, "context": "The bi-directional nature means that the communication is not entirely symmetric, and the different priority among agents would help solving any possible tie between multiple optimal joint actions [35, 36].", "startOffset": 197, "endOffset": 205}, {"referenceID": 26, "context": "Multiagent systems have also been explored in more complex cases including StarCraft combat games [30].", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "Recent work in [37] applies the DIAL model [19] assuming agents in StarCraft are fully decentralised.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "Recent work in [37] applies the DIAL model [19] assuming agents in StarCraft are fully decentralised.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "The studies from [23, 30] focus on a greedy MDP approach, i.", "startOffset": 17, "endOffset": 25}, {"referenceID": 26, "context": "The studies from [23, 30] focus on a greedy MDP approach, i.", "startOffset": 17, "endOffset": 25}, {"referenceID": 30, "context": "In this paper, the dependency of agents is, rather, modelled over hidden layers by making use of bi-directional recurrent neural networks (RNN) [34].", "startOffset": 144, "endOffset": 148}, {"referenceID": 26, "context": "The StarCraft combat, aka the micromanagement task, refers to the low-level, short-term control of the army members during a combat with enemy members [30].", "startOffset": 151, "endOffset": 155}, {"referenceID": 24, "context": "We formulate it as a zero-sum Stochastic Game (SG) [28, 38, 39], i.", "startOffset": 51, "endOffset": 63}, {"referenceID": 34, "context": "We formulate it as a zero-sum Stochastic Game (SG) [28, 38, 39], i.", "startOffset": 51, "endOffset": 63}, {"referenceID": 35, "context": "Reward shaping is of great importance in learning the collaborative or competing behaviours among agents [40].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "In small-scale MARL problems, a common solution is to employ Minimax Q-learning [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 26, "context": "However, the micromanagement combat in StarCraft is computationally expensive [30], and therefore make minimax Q-learning intractable to apply.", "startOffset": 78, "endOffset": 82}, {"referenceID": 31, "context": "(2) also requires an opponent/enemy modelling [35, 41].", "startOffset": 46, "endOffset": 54}, {"referenceID": 36, "context": "(2) also requires an opponent/enemy modelling [35, 41].", "startOffset": 46, "endOffset": 54}, {"referenceID": 24, "context": "We employ fictitious play [28] and deep neural nets to learn the enemies policy b\u03c6 in two situations, respectively.", "startOffset": 26, "endOffset": 30}, {"referenceID": 36, "context": "(2) effectively turns into a simper MDP problem [41]: Q(s,a) =r(s,a) + \u03bbQ(s,a\u03b8(s \u2032)), (4) where we drop notation b\u03c6 for brevity.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "Here we develop a vectorised version of deterministic policy gradient (DPG) on the basis of [17, 42, 43] for searching the policy in the direction of the gradient of the sum of Qi instead of maximising it.", "startOffset": 92, "endOffset": 104}, {"referenceID": 37, "context": "Here we develop a vectorised version of deterministic policy gradient (DPG) on the basis of [17, 42, 43] for searching the policy in the direction of the gradient of the sum of Qi instead of maximising it.", "startOffset": 92, "endOffset": 104}, {"referenceID": 38, "context": "Here we develop a vectorised version of deterministic policy gradient (DPG) on the basis of [17, 42, 43] for searching the policy in the direction of the gradient of the sum of Qi instead of maximising it.", "startOffset": 92, "endOffset": 104}, {"referenceID": 13, "context": "To ensure adequate exploration, we considered the off-policy deterministic actor-critic algorithms [17, 42, 43].", "startOffset": 99, "endOffset": 111}, {"referenceID": 37, "context": "To ensure adequate exploration, we considered the off-policy deterministic actor-critic algorithms [17, 42, 43].", "startOffset": 99, "endOffset": 111}, {"referenceID": 38, "context": "To ensure adequate exploration, we considered the off-policy deterministic actor-critic algorithms [17, 42, 43].", "startOffset": 99, "endOffset": 111}, {"referenceID": 30, "context": "To make effective communication between agents, we introduce dedicated bi-directional connections in the internal layers by employing bi-directional recurrent neural networks (RNN) [34].", "startOffset": 181, "endOffset": 185}, {"referenceID": 19, "context": "As we discussed, it is different with the greedy approach from [23, 30] in that the dependency of our", "startOffset": 63, "endOffset": 71}, {"referenceID": 26, "context": "As we discussed, it is different with the greedy approach from [23, 30] in that the dependency of our", "startOffset": 63, "endOffset": 71}, {"referenceID": 16, "context": "Yet, unlike CommNet [20], our communication is not fully symmetric, and we maintain certain social conventions and roles by fixing the order of the agents that join the RNN.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "This would help solving any possible tie between multiple optimal joint actions [35, 36].", "startOffset": 80, "endOffset": 88}, {"referenceID": 32, "context": "This would help solving any possible tie between multiple optimal joint actions [35, 36].", "startOffset": 80, "endOffset": 88}, {"referenceID": 39, "context": "We further intensify the actor with the concept of grouping [44], which plays an important role in influencing social behaviours.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Following similar experiment set-up as [20], BiCNet", "startOffset": 39, "endOffset": 43}, {"referenceID": 16, "context": "CommNet: CommNet [20] is a multiagent network designed to learning to communicate among multiple agents.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "GreedyMDP with Episodic Zero-Order Optimisation(GMEZO): GMEZO [30] was proposed particularly to solve StarCraft micromanagement tasks.", "startOffset": 62, "endOffset": 66}, {"referenceID": 40, "context": "In our training, Nadam [45] is set as the optimiser with learning rate equal to 0.", "startOffset": 23, "endOffset": 27}], "year": 2017, "abstractText": "Real-world artificial intelligence (AI) applications often require multiple agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as the test scenario, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a multiagent bidirectionally-coordinated network (BiCNet [\u2019bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats under diverse terrains with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of coordination strategies that is similar to these of experienced game players. Moreover, BiCNet is easily adaptable to the tasks with heterogeneous agents. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.", "creator": "LaTeX with hyperref package"}}}