{"id": "1703.10186", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Mar-2017", "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding", "abstract": "we present a hierarchy of pragmatic operating system interpretation in a grounded communication task ( creating colors upon information ) science draws upon predictions examining various recurrent knowledge network classifiers, a speaker helping a robot, unified. a recursive pragmatic reasoning step. experiments show that dynamic combined mechanism model interprets make sense more accurately as the web beside which it is built. investigators observe as pragmatic reasoning questions primarily verify the hardest reasoning : when listener host must distinguish very similar text, or when few utterances participants do correctly target color. preliminary findings demonstrates use of emotionally re - collected corpus without human utterances demonstrating color reference games, still exhibit a variety of pragmatic expressions. problems fully show whether the embedded speaker model recognizes one of numerous useful behaviors.", "histories": [["v1", "Wed, 29 Mar 2017 18:15:06 GMT  (213kb,D)", "https://arxiv.org/abs/1703.10186v1", "12 pages, 3 tables, 5 figures. To appear in TACL (pre-camera-ready draft)"], ["v2", "Tue, 16 May 2017 04:03:10 GMT  (251kb,D)", "http://arxiv.org/abs/1703.10186v2", "14 pages, 3 tables, 6 figures. TACL"]], "COMMENTS": "12 pages, 3 tables, 5 figures. To appear in TACL (pre-camera-ready draft)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["will monroe", "robert x d hawkins", "noah d goodman", "christopher potts"], "accepted": true, "id": "1703.10186"}, "pdf": {"name": "1703.10186.pdf", "metadata": {"source": "CRF", "title": "Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding", "authors": ["Will Monroe", "Robert X.D. Hawkins", "Noah D. Goodman", "Christopher Potts"], "emails": ["wmonroe4@cs.stanford.edu,", "cgpotts}@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Human communication is situated. In using language, we are sensitive to context and our interlocutors\u2019 expectations, both when choosing our utterances (as speakers) and when interpreting the utterances we hear (as listeners). Visual referring tasks exercise this complex process of grounding, in the environment and in our mental models of each other, and thus provide a valuable test-bed for computational models of production and comprehension.\nTable 1 illustrates the situated nature of reference understanding with descriptions of colors from a\ntask-oriented dialogue corpus we introduce in this paper. In these dialogues, the speaker is trying to identify their (privately assigned) target color for the listener. In context 1, the comparative darker implicitly refers to both the target (boxed) and one of the other colors. In contexts 2 and 3, the target color is the same, but the distractors led the speaker to choose different basic color terms. In context 4, blue is a pragmatic choice even though two colors are shades of blue, because the interlocutors assume about each other that they find the target color a more prototypical representative of blue and would prefer other descriptions (teal, cyan) for the middle color. The fact that blue appears in three of these four cases highlights the flexibility and context dependence of color descriptions.\nIn this paper, we present a scalable, learned model of pragmatic language understanding. The model is built around a version of the Rational Speech Acts (RSA) model (Frank and Goodman, 2012; Goodman and Frank, 2016), in which agents reason recur-\nar X\niv :1\n70 3.\n10 18\n6v 2\n[ cs\n.C L\n] 1\n6 M\nay 2\nsively about each other\u2019s expectations and intentions to communicate more effectively than literal semantic agents could. In most work on RSA, the literal semantic agents use fixed message sets and stipulated grammars, which is a barrier to experiments in linguistically complex domains. In our formulation, the literal semantic agents are recurrent neural networks (RNNs) that produce and interpret color descriptions in context. These models are learned from data and scale easily to large datasets containing diverse utterances. The RSA recursion is then defined in terms of these base agents: the pragmatic speaker produces utterances based on a literal RNN listener (Andreas and Klein, 2016), and the pragmatic listener interprets utterances based on the pragmatic speaker\u2019s behavior.\nWe focus on accuracy in a listener task (i.e., at language understanding). However, our most successful model integrates speaker and listener perspectives, combining predictions made by a system trained to understand color descriptions and one trained to produce them.\nWe evaluate this model with a new, psycholinguistically motivated corpus of real-time, dyadic reference games in which the referents are patches of color. Our task is fundamentally the same as that of Baumgaertner et al. (2012), but the corpus we release is larger by several orders of magnitude, consisting of 948 complete games with 53,365 utterances produced by human participants paired into dyads on the web. The linguistic behavior of the players exhibits many of the intricacies of language in general, including not just the context dependence and cognitive complexity discussed above, but also compositionality, vagueness, and ambiguity. While many previous data sets feature descriptions of individual colors (Cook et al., 2005; Munroe, 2010; Kawakami et al., 2016), situating colors in a communicative context elicits greater variety in language use, including negations, comparatives, superlatives, metaphor, and shared associations.\nExperiments on the data in our corpus show that this combined pragmatic model improves accuracy in interpreting human-produced descriptions over the basic RNN listener alone. We find that the largest improvement over the single RNN comes from blending it with an RNN trained to perform the speaker task, despite the fact that a model based\nonly on this speaker RNN performs poorly on its own. Pragmatic reasoning on top of the listener RNN alone also yields improvements, which moreover come primarily in the hardest cases: 1) contexts with colors that are very similar, thus requiring the interpretation of descriptions that convey fine distinctions; and 2) target colors that most referring expressions fail to identify, whether due to a lack of adequate descriptive terms or a consistent bias against the color in the RNN listener."}, {"heading": "2 Task and data collection", "text": "We evaluate our agents on a task of language understanding in a dyadic reference game (Rosenberg and Cohen, 1964; Krauss and Weinheimer, 1964; Paetzel et al., 2014). Unlike traditional natural language processing tasks, in which participants provide impartial judgements of language in isolation, reference games embed language use in a goal-oriented communicative context (Clark, 1996; Tanenhaus and Brown-Schmidt, 2008). Since they offer the simplest experimental setup where many pragmatic and discourse-level phenomena emerge, these games have been used widely in cognitive science to study topics like common ground and conventionalization (Clark and Wilkes-Gibbs, 1986), referential domains (Brown-Schmidt and Tanenhaus, 2008), perspective-taking (Hanna et al., 2003), and overinformativeness (Koolen et al., 2011).\nTo obtain a corpus of natural color reference data across varying contexts, we recruited 967 unique participants from Amazon Mechanical Turk to play 1,059 games of 50 rounds each, using the open-\nsource framework of Hawkins (2015). Participants were sorted into dyads, randomly assigned the role of speaker or listener, and placed in a game environment containing a chat box and an array of three color patches (Figure 1). On each round, one of the three colors was chosen to be the target and highlighted for the speaker. They were instructed to communicate this information to the listener, who could then click on one of the colors to advance to the next trial. Both participants were free to use the chat box at any point.\nTo ensure a range of difficulty, we randomly interspersed an equal number of trials from three different conditions: 1) close, where colors were all within a distance of \u03b8 from one another but still perceptible,1 2) split, where one distractor was within a distance of \u03b8 of the target, but the other distractor was farther than \u03b8, and 3) far, where all colors were farther than \u03b8 from one another. Colors were rejection sampled uniformly from RGB (red, green, blue) space to meet these constraints.\nAfter excluding extremely long messages,2 incomplete games, and games whose participants selfreported confusion about the instructions or nonnative English proficiency, we were left with a corpus of 53,365 speaker utterances across 46,994 rounds in 948 games. The three conditions are equally represented, with 15,519 close trials, 15,693 split trials, and 15,782 far trials. Participants were allowed to play more than once, but the modal number of games played per participant was one (75%). The modal number of messages sent per round was also one (90%). We release the filtered corpus we used throughout our analyses alongside the raw, pre-filter data collected from these experiments (see Footnote 11)."}, {"heading": "3 Behavioral results", "text": "Our corpus was developed not only to facilitate the development of models for grounded language un-\n1We used the most recent CIEDE standard to measure color differences, which is calibrated to human vision (Sharma et al., 2005). All distances were constrained to be larger than a lower bound of = 5 to ensure perceptible differences, and we used a threshold value of \u03b8 = 20 to create conditions.\n2Specifically, we set a length criterion at 4\u03c3 of the mean number of words per message (about 14 words, in our case), excluding 627 utterances. These often included meta-commentary about the game rather than color terms.\nderstanding, but also to provide a richer picture of human pragmatic communication. The collection effort was thus structured like a large-scale behavioral experiment, closely following experimental designs like those of Clark and Wilkes-Gibbs (1986). This paves the way to assessing our model not solely based on the listener\u2019s classification accuracy, but also in terms of how qualitative features of the speaker\u2019s production compare to that of our human participants. Thus, the current section briefly reviews some novel findings from the human corpus that we use to inform our model assessment."}, {"heading": "3.1 Listener behavior", "text": "Since color reference is a difficult task even for humans, we compared listener accuracy across conditions to calibrate our expectations about model performance. While participants\u2019 accuracy was close to ceiling (97%) on the far condition, they made significantly more errors on the split (90%) and close (83%) conditions (see Figure 4)."}, {"heading": "3.2 Speaker behavior", "text": "For ease of comparison to computational results, we focus on five metrics capturing different aspects of pragmatic behavior displayed by both human and artificial speakers in our task (Table 2). In all cases, we report test statistics from a mixed-effects regression including condition as a fixed effect and game ID as a random effect; except where noted, all test statistics reported correspond to p-values < 10\u22124 and have been omitted for readability.\nWords and characters We expect human speakers to be more verbose in split and close contexts than far contexts; the shortest, simplest color terms for the target may also apply to one or both distractors, thus incentivizing the speaker to use more lengthy descriptions to fully distinguish it. Indeed, even if they know enough simple color terms to distinguish all the colors lexically, they might be unsure their listeners will and so resort to modifiers anyway. To assess this hypothesis, we counted the average number of words and characters per message. Compared to the baseline far context, participants used significantly more words both in the split context (t = 45.85) and the close context (t = 73.06). Similar results hold for the character metric.\nComparatives and superlatives As noted in Section 1, comparative morphology implicitly encodes a dependence on the context; a speaker who refers to the target color as the darker blue is presupposing that there is another (lighter) blue in the context. Similarly, superlatives like the bluest one or the lightest one presuppose that all the colors can be compared along a specific semantic dimension. We thus expect to see this morphology more often where two or more of the colors are comparable in this way. To test this, we used the Stanford CoreNLP part-ofspeech tagger (Toutanova et al., 2003) to mark the presence or absence of comparatives (JJR or RBR) and superlatives (JJS or RBS) for each message.\nWe found two related patterns across conditions. First, participants were significantly more likely to use both comparatives (z = 37.39) and superlatives (z = 31.32) when one or more distractors were close to the target. Second, we found evidence of an asymmetry in the use of these constructions across the split and close contexts. Comparatives were used significantly more often in the split context (z = 4.4), where only one distractor was close to the target, while superlatives were much more likely to be used in the close condition (z = 32.72).3\nNegatives In our referential contexts, negation is likely to play a role similar to that of comparatives: a phrase like not the red or blue one singles out the third color, and blue but not bright blue achieves a more nuanced kind of comparison. Thus, as with\n3We used Helmert coding to test these specific patterns: the first regression coefficient compares the \u2018far\u2019 condition to the mean of the other two conditions, and the second regression coefficient compares the \u2018split\u2019 condition to the \u2018close\u2019 condition.\ncomparatives, we expect negation to be more likely where one or more distractors are close to the target. To test this, we counted occurrences of the string \u2018not\u2019 (by far the most frequent negation in the corpus). Compared to the baseline far context, we found that participants were more likely to use negative constructions when one (z = 27.36) or both (z = 34.32) distractors were close to the target.\nWordNet specificity We expect speakers to prefer basic color terms wherever they suffice to achieve the communicative goal, since such terms are most likely to succeed with the widest range of listeners. Thus, a speaker might choose blue even for a clear periwinkle color. However, as the colors get closer together, the basic terms become too ambiguous, and thus the risk of specific terms becomes worthwhile (though lengthy descriptions might be a safer strategy, as discussed above). To evaluate this idea, we use WordNet (Fellbaum, 1998) to derive a specificity hierarchy for color terms, and we hypothesized that split or close conditions will tend to lead speakers to go lower in this hierarchy.\nFor each message, we transformed adjectives into their closest noun forms (e.g. \u2018reddish\u2019 \u2192 \u2018red\u2019), filtered to include only nouns with \u2018color\u2019 in their hypernym paths, calculated the depth of the hypernym path of each color word, and took the maximum depth occurring in a message. For instance, the message \u201cdeep magenta, purple with some pink\u201d received a score of 9. It has three color terms: \u201cpurple\u201d and \u201cpink,\u201d which have the basic-level depth of 7, and \u201cmagenta,\u201d which is a highly specific color term with a depth of 9. Finally, because there weren\u2019t meaningful differences between words at depths of\n8 (\u201crose\u201d, \u201cteal\u201d) and 9 (\u201ctan,\u201d \u201ctaupe\u201d), we conducted our analyses on a binary variable thresholded to distinguish \u201chigh specificity\u201d messages with a depth greater than 7. We found a small but reliable increase in the likelihood of \u201chigh specificity\u201d messages from human speakers in the split (z = 2.84, p = 0.005) and close (z = 2.33, p = 0.02) contexts, compared to the baseline far context."}, {"heading": "4 Models", "text": "We first define the basic RSA model as applied to the color reference games introduced in Section 2; a worked example is shown in Figure 2.\nListener-based listener The starting point of RSA is a model of a literal listener:\nl0(t | u,L) \u221d L(u, t)P (t) (1)\nwhere t is a color in the context setC, u is a message drawn from a set of possible utterances U , P is a prior over colors, and L(u, t) is a semantic interpretation function that takes the value 1 if u is true of t, else 0. Figure 2a shows the values of L defined for a very simple context in which U = {blue, teal, dull}, and C = { xx , xx , xx }; Figure 2b shows the corresponding literal listener l0 if the prior P over colors is flat. (In our scalable extension, we will substitute a neural network model for l0, bypassing L and allowing for non-binary semantic judgments.)\nRSA postulates a model of a pragmatic speaker (Figure 2c) that behaves according to a distribution that soft-maximizes a utility function rewarding informativity and penalizing cost:\ns1(u | t,L) \u221d e\u03b1 log(l0(t|u,L))\u2212\u03ba(u) (2)\nHere, \u03ba is a real-valued cost function on utterances, and \u03b1 \u2208 [0,\u221e) is an inverse temperature parameter governing the \u201crationality\u201d of the speaker model. A large \u03b1 means the pragmatic speaker is expected to choose the most informative utterance (minus cost) consistently; a small\u03b1means the speaker is modeled as choosing suboptimal utterances frequently.\nFinally, a pragmatic listener (Figure 2d) interprets utterances by reasoning about the behavior of the pragmatic speaker:\nl2(t | u,L) \u221d s1(u | t,L)P (t) (3)\nThe \u03b1 parameter of the speaker indirectly affects the listener\u2019s interpretations: the more reliably the speaker chooses the optimal utterance for a referent, the more the listener will take deviations from the optimum as a signal to choose a different referent.\nThe most important feature of this model is that the pragmatic listener l2 reasons not about the semantic interpretation function L directly, but rather about a speaker who reasons about a listener who reasons about L directly. The back-and-forth nature of this interpretive process mirrors that of conversational implicature (Grice, 1975) and reflects more general ideas from Bayesian cognitive modeling (Tenenbaum et al., 2011). The model and its variants have been shown to capture a wide range of pragmatic phenomena in a cognitively realistic manner (Goodman and Stuhlmu\u0308ller, 2013; Smith et al., 2013; Kao et al., 2014; Bergen et al., 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al., 2014; Vogel et al., 2013).\nu1 u2 u3\n(\u00b5,\u03a3) c1 c2 c3\n\u2022 \u2022 \u2022\nc3\nEmbedding\nLSTM\nSoftmax\n(a) The L0 agent processes tokens ui of a color description u sequentially. The final representation is transformed into a Gaussian distribution in color space, which is used to score the context colors c1 . . . c3.\nc1 c2 ct\nh h; \u3008s\u3009 h;u1 h;u2\nu1 u2 \u3008/s\u3009\nLSTM\nFully connected\nSoftmax\n(b) The S0 agent processes the target color ct in context and produces tokens ui of a color description sequentially. Each step in production is conditioned by the context representation h and the previous word produced.\nFigure 3: The neural base speaker and listener agents.\nSpeaker-based listener The definitions of s1 (2) and l2 (3) give a general method of deriving a speaker from a listener and vice versa. This suggests an alternative formulation of a pragmatic listener, starting from a literal speaker:\ns0(u | t,L) \u221d L(u, t)e\u2212\u03ba(u) (4) l1(t | u,L) \u221d s0(u | t,L)P (t) (5)\nHere, it is the speaker that reasons about the semantics, while the listener reasons about this speaker.\nBoth of these versions of RSA pose problems with scalability, stemming from the set of messages U and the interpretation function L. In most versions of RSA, these are specified by hand (but see Monroe and Potts 2015). This presents a serious practical obstacle to applying RSA to large data sets containing realistic utterances. The set U also raises a more fundamental issue: if this set is not finite (as one would expect from a compositional grammar), then in general there is no exact way to normalize the s1 scores, since the denominator must sum over all messages. The same problem applies to s0, unless L factorizes in an unrealistically clean way.\nOver the next few subsections, we overcome these obstacles by replacing l0 and s0 with RNN-based listener agents, denoted with capital letters: L0, S0. We use the S0 agent both as a base model for a pragmatic listener analogous to l1 in (5) and to acquire\nsample utterances for approximating the normalization required in defining the s1 agent in (2)."}, {"heading": "4.1 Base listener", "text": "Our base listener agent L0 (Figure 3a) is an LSTM encoder model that predicts a Gaussian distribution over colors in a transformed representation space. The input words are embedded in a 100-dimensional vector space. Word embeddings are initialized to random normally-distributed vectors (\u00b5 = 0, \u03c3 = 0.01) and trained. The sequence of word vectors is used as input to an LSTM with 100-dimensional hidden state, and a linear transformation is applied to the output representation to produce the parameters \u00b5 and \u03a3 of a quadratic form4\nscore(f) = \u2212(f \u2212 \u00b5)T\u03a3(f \u2212 \u00b5)\nwhere f is a vector representation of a color. Each color is represented in its simplest form as a threedimensional vector in RGB space. These RGB vectors are then Fourier-transformed as in Monroe et al. (2016) to obtain the representation f .\nThe values of score(f) for each of the K context colors are normalized in log space to produce a probability distribution over the context colors. We denote this distribution by L0(t | u,C; \u03b8), where \u03b8\n4The quadratic form is not guaranteed to be negative definite and thus define a Gaussian; however, it is for > 95% of inputs. The distribution over context colors is well-defined regardless.\nrepresents the vector of parameters that define the trained model."}, {"heading": "4.2 Base speaker", "text": "We also employ an LSTM-based speaker model S0(u | t, C;\u03c6). This speaker serves two purposes: 1) it is used to define a pragmatic listener akin to l1 in (5), and 2) it provides samples of alternative utterances for each context, to avoid enumerating the intractably large space of possible utterances.\nThe speaker model consists of an LSTM context encoder and an LSTM description decoder (Figure 3b). In this model, the colors of the context ci \u2208 C are transformed into Fourier representation space, and the sequence of color representations is passed through an LSTM with 100-dimensional hidden state. The context is reordered to place the target color last, minimizing the length of dependence between the most important input color and the output (Sutskever et al., 2014) and eliminating the need to represent the index of the target separately. The final cell state of this recurrent neural network is concatenated with a 100-dimensional embedding for the previous token output at each step of decoding. The resulting vector is input along with the previous cell state to the LSTM cell, and an affine transformation and softmax function are applied to the output to produce a probability distribution predicting the following token of the description. The model is substantively similar to well-known models for image caption generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), which use the output of a convolutional neural network as the representation of an input image and provide this representation to the RNN as an initial state or first word (we represent the context using a second RNN and concatenate the context representation onto each input word vector)."}, {"heading": "4.3 Pragmatic agents", "text": "Using the above base agents, we define a pragmatic speaker S1 and a pragmatic listener L2:\nS1(u | t, C; \u03b8) = L0(t | u,C; \u03b8)\u03b1\u2211 u\u2032 L0(t | u\u2032, C; \u03b8)\u03b1\n(6)\nL2(t | u,C; \u03b8) = S1(u | t, C; \u03b8)\u2211 t\u2032 S1(u | t\u2032, C; \u03b8)\n(7)\nThese definitions mirror those in (2) and (3) above, with L replaced by the learned weights \u03b8.\nJust as in (2), the denominator in (6) should consist of a sum over the entire set of potential utterances, which is exponentially large in the maximum utterance length and might not even be finite. As mentioned in Section 4.2, we limit this search by taking m samples from S0(u | i, C;\u03c6) for each target index i, adding the actual utterance from the testing example, and taking the resulting multiset as the universe of possible utterances, weighted towards frequently-sampled utterances.5 Taking a number of samples from S0 for each referent in the context gives the pragmatic listener a variety of informative alternative utterances to consider when interpreting the true input description. We have found thatm can be small; in our experiments, it is set to 8.\nTo reduce the noise resulting from the stochastically chosen alternative utterance sets, we also perform this alternative-set sampling n times and average the resulting probabilities in the final L2 output. We again choose n = 8 as a satisfactory compromise between effectiveness and computation time.\nBlending with a speaker-based agent A second pragmatic listener L1 can be formed in a similar way, analogous to l1 in (5):\nL1(t | u,C;\u03c6) = S0(u | t, C;\u03c6)\u2211 t\u2032 S0(u | t\u2032, C;\u03c6)\n(8)\nWe expect L1 to be less accurate than L0 or L2, because it is performing a listener task using only the outputs of a model trained for a speaker task. However, this difference in training objective can also give the model strengths that complement those of the two listener-based agents. One might also expect a realistic model of human language interpretation to lie somewhere between the \u201creflex\u201d interpretations of the neural base listener and the \u201creasoned\u201d interpretations of one of the pragmatic models. This has an intuitive justification in people\u2019s uncertainty about whether their interlocutors are speaking pragmatically: \u201cshould I read more into that statement, or take it at face value?\u201d We therefore also evaluate models defined as a weighted average of L0\n5An alternative would be to enforce uniqueness within the alternative set, keeping it a true set as in the basic RSA formulation; this could be done with rejection sampling or beam search for the highest-scoring speaker utterances. We found that doing so with rejection sampling hurt model performance somewhat, so we did not pursue the more complex beam search approach.\nand each of L1 and L2, as well as an \u201censemble\u201d model that combines all of these agents. Specifically, we consider the following blends of neural base models and pragmatic models, with Li abbreviating Li(t | u,C; \u03b8, \u03c6) for convenience:\nLa \u221d L0\u03b2a \u00b7 L1\u2212\u03b2a1 (9) Lb \u221d L0\u03b2b \u00b7 L1\u2212\u03b2b2 (10) Le \u221d La\u03b3 \u00b7 L1\u2212\u03b3b (11)\nThe hyperparameters in the exponents allow tuning the blend of each pair of models\u2014e.g., overriding the neural model with the pragmatic reasoning in Lb. The value of the weights \u03b2a, \u03b2b, and \u03b3 can be any real number; however, we find that good values of these weights lie in the range [\u22121, 1]. As an example, setting \u03b2b = 0 makes the blended model Lb equivalent to the pragmatic model L2; \u03b2b = 1 ignores the pragmatic reasoning and uses the base model L0\u2019s outputs; and \u03b2b = \u22121 \u201csubtracts\u201d the base model from the pragmatic model (in log probability space) to yield a \u201chyperpragmatic\u201d model."}, {"heading": "4.4 Training", "text": "We split our corpus into approximately equal train/dev/test sets (15,665 train trials, 15,670 dev, 15,659 test), ensuring that trials from the same dyad are present in only one split. We preprocess the data by 1) lowercasing; 2) tokenizing by splitting off punctuation as well as the endings -er, -est, and -ish;6 and 3) replacing tokens that appear once or not at all in the training split7 with <unk>. We also remove listener utterances and concatenate speaker utterances on the same context. We leave handling of interactive dialogue to future work (Section 8).\nWe use ADADELTA (Zeiler, 2012) and Adam (Kingma and Ba, 2014), adaptive variants of stochastic gradient descent (SGD), to train listener and speaker models. The choice of optimization algorithm and learning rate for each model were tuned with grid search on a held-out tuning set consisting of 3,500 contexts.8 We also use a fine-grained\n6We only apply this heuristic ending segmentation for the listener; the speaker is trained to produce words with these endings unsegmented, to avoid segmentation inconsistencies when passing speaker samples as alternative utterances to the listener.\n71.13% of training tokens, 1.99% of dev/test. 8ForL0: ADADELTA, learning rate \u03b7 = 0.2; for S0: Adam,\nlearning rate \u03b1 = 0.004.\ngrid search on this tuning set to determine the values of the pragmatic reasoning parameters \u03b1, \u03b2, and \u03b3. In our final ensemble Le, we use \u03b1 = 0.544, base weights \u03b2a = 0.492 and \u03b2b = \u22120.15, and a final blending weight \u03b3 = 0.491. It is noteworthy that the optimal value of \u03b2b from grid search is negative. The effect of this is to amplify the difference between L0 and L2: the listener-based pragmatic model, evidently, is not quite pragmatic enough."}, {"heading": "5 Model results", "text": ""}, {"heading": "5.1 Speaker behavior", "text": "To compare human behavior with the behavior of our embedded speaker models, we performed the same behavorial analysis done in Section 3.2. Results from this analysis are included alongside the human results in Table 2. Our pragmatic speaker model S1 did not differ qualitatively from our base speaker S0 on any of the metrics, so we only summarize results for humans and the pragmatic model.\nWords and characters We found human speakers to be more verbose when colors were closer together, in both number of words and number of characters. As Table 2 shows, our S1 agent shows the same increase in utterance length in the split (t = 18.07) and close (t = 35.77) contexts compared to the far contexts.\nComparatives and superlatives Humans used more comparatives and superlatives when colors were closer together; however, comparatives were preferred in the split contexts, superlatives in the close contexts. Our pragmatic speaker shows the first of these two patterns, producing more comparatives (z = 14.45) and superlatives (z = 16) in the split or close conditions than in the baseline far condition. It does not, however, capture the peak in comparative use in the split condition. This suggests that our model is simulating the human strategy at some level, but that more subtle patterns require further attention.\nNegations Humans used more negations when the colors were closer together. Our pragmatic speaker\u2019s use of negation shows the same relationship to the context (z = 8.55 and z = 16.61, respectively).\nWordNet specificity Humans used more \u201chigh specificity\u201d words (by WordNet hypernymy depth) when the colors were closer together. Our pragmatic speaker showed a similar effect (z = 2.65, p = 0.008 and z = 2.1, p = 0.036, respectively)."}, {"heading": "5.2 Listener accuracy", "text": "Table 3 shows the accuracy and perplexity of the base listener L0, the pragmatic listeners L1 and L2, and the blended models La, Lb, and Le at resolving the human-written color references. Accuracy differences are significant9 for all pairs except L2/Lb and La/Le. As we expected, the speaker-based L1 alone performs the worst of all the models. However, blending it with L0 doesn\u2019t drag down L0\u2019s performance but rather produces a considerable improvement compared to both of the original models, consistent with our expectation that the listenerbased and speaker-based models have complementary strengths.\nWe observe that L2 significantly outperforms its own base model L0, showing that pragmatic reasoning on its own contributes positively. Blending the pragmatic models with the base listener also improves over both individually, although not significantly in the case of Lb over L2. Finally, the most effective listener combines both pragmatic models with the base listener. Plotting the number of ex-\n9p < 0.012, approximate permutation test (Pado\u0301, 2006) with Bonferroni correction, 10,000 samples.\namples changed by condition on the dev set (Figure 4) reveals that the primary gain from including the pragmatic models is in the close and split conditions, when the model has to distinguish highly similar colors and often cannot rely only on basic color terms. On the test set, the final ensemble improves significantly10 over the base model on both metrics."}, {"heading": "6 Model analysis", "text": "Examining the full probability tables for various dev set examples offers insight into the value of each model in isolation and how they complement each other when blended together. In particular, we see that the listener-based (L2) and speaker-based (L1) pragmatic listeners each overcome a different kind of \u201cblind spot\u201d in the neural base listener\u2019s understanding ability.\nFirst, we inspect examples in which L2 is superior to L0. In most of these examples, the alternative utterances sampled from S0 for one of the referents i fail to identify their intended referent to L0. The pragmatic listener interprets this to mean that referent i is inherently difficult to refer to, and it compensates by increasing referent i\u2019s probability.\nThis is beneficial when i is the true target. The\n10p < 0.001, approximate permutation test, 10,000 samples.\nleft column of Figure 5 shows one such example: a context consisting of a somewhat prototypical blue, a bright cyan, and a purple-tinged brown, with the utterance blue. The base listener interprets this as referring to the cyan with 91% probability, perhaps due to the extreme saturation of the cyan maximally activating certain parts of the neural network. However, when the pragmatic model takes samples from S0 to probe the space of alternative utterances, it becomes apparent that indicating the more ordinary blue to the listener is difficult: for the utterances chosen by S0 intending this referent (true blue, light blue), the listener also chooses the cyan with >89% confidence. Pragmatic reasoning overcomes this difficulty. Only two utterances in the alternative set (the actual utterance blue and the sampled alternative true blue) result in any appreciable probability mass on the true target, so the pragmatic listener\u2019s model of the speaker predicts that the speaker would usually choose one of these two utterances for the prototypical blue. However, if the target were the cyan, the speaker would have many good options. Therefore, the fact that the speaker chose blue is interpreted as evidence for the true target. This mirrors the backand-forth reasoning behind the definition of conver-\nsational implicature (Grice, 1975). This reasoning can be harmful when i is one of the distractors: the pragmatic listener is then in danger of overweighting the distractor and incorrectly choosing it. This is a likely reason for the small performance difference between L0 and L2. Still, the fact that L2 is more accurate overall, in addition to the negative value of \u03b2b discovered in grid search, suggests that the pragmatic reasoning provides value on its own.\nHowever, the final performance improves greatly when we incorporate both listener-based and speaker-based agents. To explain this improvement, we examine examples in which both listener-based agents L0 and L2 give the wrong answer but are overridden by the speaker-based L1 to produce the correct referent. The discrepancy between the two kinds of models in many of these examples can be explained by the fact that the speaker takes the context as input, while the listener does not. The listener is thus asked to predict a region of color space from the utterance a priori, while the speaker can take into account relationships between the context colors in scoring utterances.\nThe right column of Figure 5 shows an example of this. The context contains a grayish green (the target), a grayish blue-green (\u201cdistractor 1\u201d), and a yellowish green (\u201cdistractor 2\u201d). The utterance from the human speaker is drab green not the bluer one, presumably intending drab to exclude the brighter yellowish green. However, the L0 listener must choose a region of color space to predict based on the utter-\nance alone, without seeing the other context colors. Figure 6 shows a visualization of the listener\u2019s prediction. The figure is a heatmap of the probability density output by the listener, as a function of hue and saturation in HSV (hue, saturation, value) space. We use HSV here, rather than the RGB coordinate system used by the model, because the semantic constraints are more clearly expressed in terms of hue and saturation components: the color should be drab (low-saturation) and green (near 120 on the hue spectrum) but not blue (near 240 in hue). The utterance does not constrain the value (roughly, brightness\u2013darkness) component, so we sum over this component to summarize the 3-dimensional distribution in 2 dimensions.\nThe L0 model correctly interprets all of these constraints: it gives higher probability to lowsaturation colors and greens, while avoiding bluer colors. However, the result is a probability distribution nearly centered at distractor 2, the brighter green. In fact, if we were not comparing it to the other colors in the context, distractor 2 would be a very good example of a drab green that is not bluish.\nThe speaker S0, however, produces utterances conditioned on the context; it has successfully learned that drab would be more likely as a description of the grayish green than as a description of the yellowish one in this context. The speaker-based listener L1 therefore predicts the true target, with greater confidence than L0 or L2. This prediction results in the blends La and Le preferring the true target, allowing the speaker\u2019s perspective to override the listener\u2019s."}, {"heading": "7 Related work", "text": "Prior work combining machine learning with probabilistic pragmatic reasoning models has largely focused on the speaker side, i.e., generation. Golland et al. (2010) develop a pragmatic speaker model, S(L0), that reasons about log-linear listeners trained on human utterances containing spatial references in virtual-world environments. Tellex et al. (2014) apply a similar technique, under the name inverse semantics, to create a robot that can informatively ask humans for assistance in accomplishing tasks. Meo et al. (2014) evaluate a model of color description generation (McMahan and Stone, 2015) on the\ncolor reference data of Baumgaertner et al. (2012) by creating an L(S0) listener. Monroe and Potts (2015) implement an end-to-end trained S(L(S0)) model for referring expression generation in a reference game task. Many of these models require enumerating the set of possible utterances for each context, which is infeasible when utterances are as varied as those in our dataset.\nThe closest work to ours that we are aware of is that of Andreas and Klein (2016), who also combine neural speaker and listener models in a reference game setting. They propose a pragmatic speaker, S(L0), sampling from a neural S0 model to limit the search space and regularize the model toward human-like utterances. We show these techniques help in listener (understanding) tasks as well. Approaching pragmatics from the listener side requires either inverting the pragmatic reasoning (i.e., deriving a listener from a speaker), or adding another step of recursive reasoning, yielding a two-level derived pragmatic model L(S(L0)). We show both approaches contribute to an effective listener."}, {"heading": "8 Conclusion", "text": "In this paper, we present a newly-collected corpus of color descriptions from reference games, and we show that a pragmatic reasoning agent incorporating neural listener and speaker models interprets color descriptions in context better than the listener alone.\nThe separation of referent and utterance representation in our base speaker and listener models in principle allows easy substitution of referents other than colors (for example, images), although the performance of the listener agents could be limited by the representation of utterance semantics as a Gaussian distribution in referent representation space. Our pragmatic agents also rely on the ability to enumerate the set of possible referents. Avoiding this enumeration, as would be necessary in tasks with intractably large referent spaces, is a challenging theoretical problem for RSA-like models.\nAnother important next step is to pursue multiturn dialogue. As noted in Section 2, both participants in our reference game task could use the chat window at any point, and more than half of dyads had at least one two-way interaction. Dialogue agents are more challenging to model than\nisolated speakers and listeners, requiring long-term planning, remembering previous utterances, and (for the listener) deciding when to ask for clarification or commit to a referent (Lewis, 1979; Brown and Yule, 1983; Clark, 1996; Roberts, 1996). We release our dataset11 with the expectation that others may find interest in these challenges as well."}, {"heading": "Acknowledgments", "text": "We thank Kai Sheng Tai and Ashwin Paranjape for helpful feedback. This material is based in part upon work supported by the Stanford Data Science Initiative and by the NSF under Grant No. BCS-1456077. RXDH was supported by the Stanford Graduate Fellowship and the NSF Graduate Research Fellowship under Grant No. DGE-114747. NDG was supported by the Alfred P. Sloan Foundation Fellowship and DARPA under Agreement No. FA8750-14-2-0009. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF, DARPA, or the Sloan Foundation."}], "references": [{"title": "Reasoning about pragmatics with neural listeners and speakers", "author": ["Jacob Andreas", "Dan Klein."], "venue": "EMNLP, pages 1173\u20131182.", "citeRegEx": "Andreas and Klein.,? 2016", "shortCiteRegEx": "Andreas and Klein.", "year": 2016}, {"title": "Towards a flexible semantics: Colour terms in collaborative reference tasks", "author": ["Bert Baumgaertner", "Raquel Fernandez", "Matthew Stone."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM 2012), pages 80\u201384.", "citeRegEx": "Baumgaertner et al\\.,? 2012", "shortCiteRegEx": "Baumgaertner et al\\.", "year": 2012}, {"title": "Pragmatic reasoning through semantic inference", "author": ["Leon Bergen", "Roger Levy", "Noah D. Goodman."], "venue": "Semantics and Pragmatics, 9(20).", "citeRegEx": "Bergen et al\\.,? 2016", "shortCiteRegEx": "Bergen et al\\.", "year": 2016}, {"title": "Discourse analysis", "author": ["Gillian Brown", "George Yule."], "venue": "Cambridge University Press.", "citeRegEx": "Brown and Yule.,? 1983", "shortCiteRegEx": "Brown and Yule.", "year": 1983}, {"title": "Real-time investigation of referential domains in unscripted conversation: A targeted language game approach", "author": ["Sarah Brown-Schmidt", "Michael K. Tanenhaus."], "venue": "Cognitive science, 32(4):643\u2013684.", "citeRegEx": "Brown.Schmidt and Tanenhaus.,? 2008", "shortCiteRegEx": "Brown.Schmidt and Tanenhaus.", "year": 2008}, {"title": "Referring as a collaborative process", "author": ["Herbert H. Clark", "Deanna Wilkes-Gibbs."], "venue": "Cognition, 22(1):1\u2013", "citeRegEx": "Clark and Wilkes.Gibbs.,? 1986", "shortCiteRegEx": "Clark and Wilkes.Gibbs.", "year": 1986}, {"title": "Using Language", "author": ["Herbert H. Clark."], "venue": "Cambridge University Press.", "citeRegEx": "Clark.,? 1996", "shortCiteRegEx": "Clark.", "year": 1996}, {"title": "The World Color Survey database", "author": ["Richard S. Cook", "Paul Kay", "Terry Regier."], "venue": "Handbook of Categorization in Cognitive Science, pages 223\u2013241.", "citeRegEx": "Cook et al\\.,? 2005", "shortCiteRegEx": "Cook et al\\.", "year": 2005}, {"title": "WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum."], "venue": "MIT Press.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "Predicting pragmatic reasoning in language games", "author": ["Michael C. Frank", "Noah D. Goodman."], "venue": "Science, 336(6084):998.", "citeRegEx": "Frank and Goodman.,? 2012", "shortCiteRegEx": "Frank and Goodman.", "year": 2012}, {"title": "A game-theoretic approach to generating spatial descriptions", "author": ["Dave Golland", "Percy Liang", "Dan Klein."], "venue": "EMNLP, pages 410\u2013419.", "citeRegEx": "Golland et al\\.,? 2010", "shortCiteRegEx": "Golland et al\\.", "year": 2010}, {"title": "Pragmatic language interpretation as probabilistic inference", "author": ["Noah D. Goodman", "Michael C. Frank."], "venue": "Trends in Cognitive Sciences, 20(11):818\u2013829.", "citeRegEx": "Goodman and Frank.,? 2016", "shortCiteRegEx": "Goodman and Frank.", "year": 2016}, {"title": "Knowledge and implicature: Modeling language understanding as social cognition", "author": ["Noah D. Goodman", "Andreas Stuhlm\u00fcller."], "venue": "Topics in Cognitive Science, 5(1):173\u2013184.", "citeRegEx": "Goodman and Stuhlm\u00fcller.,? 2013", "shortCiteRegEx": "Goodman and Stuhlm\u00fcller.", "year": 2013}, {"title": "Logic and conversation", "author": ["H. Paul Grice."], "venue": "Peter Cole and Jerry Morgan, editors, Syntax and Semantics, volume 3: Speech Acts, pages 43\u201358. Academic Press.", "citeRegEx": "Grice.,? 1975", "shortCiteRegEx": "Grice.", "year": 1975}, {"title": "The effects of common ground and perspective on domains of referential interpretation", "author": ["Joy E. Hanna", "Michael K. Tanenhaus", "John C. Trueswell."], "venue": "Journal of Memory and Language, 49(1):43\u201361.", "citeRegEx": "Hanna et al\\.,? 2003", "shortCiteRegEx": "Hanna et al\\.", "year": 2003}, {"title": "Conducting real-time multiplayer experiments on the web", "author": ["Robert X.D. Hawkins."], "venue": "Behavior Research Methods, 47(4):966\u2013976.", "citeRegEx": "Hawkins.,? 2015", "shortCiteRegEx": "Hawkins.", "year": 2015}, {"title": "Nonliteral understanding of number words", "author": ["Justine T. Kao", "Jean Y. Wu", "Leon Bergen", "Noah D. Goodman."], "venue": "Proceedings of the National Academy of Sciences, 111(33):12002\u201312007.", "citeRegEx": "Kao et al\\.,? 2014", "shortCiteRegEx": "Kao et al\\.", "year": 2014}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "CVPR, pages 3128\u20133137.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Character sequence models for colorful words", "author": ["Kazuya Kawakami", "Chris Dyer", "Bryan Routledge", "Noah A. Smith."], "venue": "EMNLP, pages 1949\u20131954.", "citeRegEx": "Kawakami et al\\.,? 2016", "shortCiteRegEx": "Kawakami et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Factors causing overspecification in definite descriptions", "author": ["Ruud Koolen", "Albert Gatt", "Martijn Goudbeek", "Emiel Krahmer."], "venue": "Journal of Pragmatics, 43(13):3231\u20133250.", "citeRegEx": "Koolen et al\\.,? 2011", "shortCiteRegEx": "Koolen et al\\.", "year": 2011}, {"title": "Changes in reference phrases as a function of frequency of usage in social interaction: A preliminary study", "author": ["Robert M. Krauss", "Sidney Weinheimer."], "venue": "Psychonomic Science, 1(1\u201312):113\u2013114.", "citeRegEx": "Krauss and Weinheimer.,? 1964", "shortCiteRegEx": "Krauss and Weinheimer.", "year": 1964}, {"title": "Scorekeeping in a language game", "author": ["David Lewis."], "venue": "Journal of philosophical logic, 8(1):339\u2013359.", "citeRegEx": "Lewis.,? 1979", "shortCiteRegEx": "Lewis.", "year": 1979}, {"title": "A Bayesian model of grounded color semantics", "author": ["Brian McMahan", "Matthew Stone."], "venue": "Transactions of", "citeRegEx": "McMahan and Stone.,? 2015", "shortCiteRegEx": "McMahan and Stone.", "year": 2015}, {"title": "Generating and resolving vague color references", "author": ["Timothy Meo", "Brian McMahan", "Matthew Stone."], "venue": "Proceedings of the 18th Workshop on the Semantics and Pragmatics of Dialogue (SemDial), pages 107\u2013115.", "citeRegEx": "Meo et al\\.,? 2014", "shortCiteRegEx": "Meo et al\\.", "year": 2014}, {"title": "Learning in the Rational Speech Acts model", "author": ["Will Monroe", "Christopher Potts."], "venue": "Proceedings of the 20th Amsterdam Colloquium, pages 1\u201312.", "citeRegEx": "Monroe and Potts.,? 2015", "shortCiteRegEx": "Monroe and Potts.", "year": 2015}, {"title": "Learning to generate compositional color descriptions", "author": ["Will Monroe", "Noah D. Goodman", "Christopher Potts."], "venue": "EMNLP, pages 2243\u20132248.", "citeRegEx": "Monroe et al\\.,? 2016", "shortCiteRegEx": "Monroe et al\\.", "year": 2016}, {"title": "Color survey results", "author": ["Randall Munroe."], "venue": "Online at http://blog.xkcd.com/2010/05/03/color-survey-results.", "citeRegEx": "Munroe.,? 2010", "shortCiteRegEx": "Munroe.", "year": 2010}, {"title": "User\u2019s guide to sigf: Significance testing by approximate randomisation. http://www.nlpado.de/ \u0303sebastian/ software/sigf.shtml", "author": ["Sebastian Pad\u00f3"], "venue": null, "citeRegEx": "Pad\u00f3,? \\Q2006\\E", "shortCiteRegEx": "Pad\u00f3", "year": 2006}, {"title": "A multimodal corpus of rapid dialogue games", "author": ["Maike Paetzel", "David Nicolas Racca", "David DeVault."], "venue": "LREC, pages 4189\u20134195.", "citeRegEx": "Paetzel et al\\.,? 2014", "shortCiteRegEx": "Paetzel et al\\.", "year": 2014}, {"title": "Information structure in discourse: Towards an integrated formal theory of pragmatics", "author": ["Craige Roberts."], "venue": "Working Papers in Linguistics-Ohio State University Department of Linguistics, pages 91\u2013136.", "citeRegEx": "Roberts.,? 1996", "shortCiteRegEx": "Roberts.", "year": 1996}, {"title": "Speakers\u2019 and listeners\u2019 processes in a word communication task", "author": ["Seymour Rosenberg", "Bertram D. Cohen."], "venue": "Science, 145(3637):1201\u20131203.", "citeRegEx": "Rosenberg and Cohen.,? 1964", "shortCiteRegEx": "Rosenberg and Cohen.", "year": 1964}, {"title": "The CIEDE2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations", "author": ["Gaurav Sharma", "Wencheng Wu", "Edul N. Dalal."], "venue": "Color Research & Application, 30(1):21\u201330.", "citeRegEx": "Sharma et al\\.,? 2005", "shortCiteRegEx": "Sharma et al\\.", "year": 2005}, {"title": "Learning and using language via recursive pragmatic reasoning about other agents", "author": ["Nathaniel J. Smith", "Noah D. Goodman", "Michael C. Frank."], "venue": "NIPS, pages 3039\u20133047.", "citeRegEx": "Smith et al\\.,? 2013", "shortCiteRegEx": "Smith et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Language processing in the natural world", "author": ["Michael K. Tanenhaus", "Sarah Brown-Schmidt."], "venue": "Philosophical Transactions of the Royal Society of London B: Biological Sciences, 363(1493):1105\u20131122.", "citeRegEx": "Tanenhaus and Brown.Schmidt.,? 2008", "shortCiteRegEx": "Tanenhaus and Brown.Schmidt.", "year": 2008}, {"title": "Asking for help using inverse semantics", "author": ["Stefanie Tellex", "Ross A. Knepper", "Adrian Li", "Daniela Rus", "Nicholas Roy."], "venue": "Robotics: Science and Systems.", "citeRegEx": "Tellex et al\\.,? 2014", "shortCiteRegEx": "Tellex et al\\.", "year": 2014}, {"title": "How to grow a mind: Statistics, structure, and abstraction", "author": ["Joshua B. Tenenbaum", "Charles Kemp", "Thomas L. Griffiths", "Noah D. Goodman."], "venue": "Science, 331(6022):1279\u20131285.", "citeRegEx": "Tenenbaum et al\\.,? 2011", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "NAACLHLT, pages 173\u2013180.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "CVPR, pages 3156\u20133164.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Implicatures and nested beliefs in approximate Decentralized-POMDPs", "author": ["Adam Vogel", "Christopher Potts", "Dan Jurafsky."], "venue": "ACL, pages 74\u201380.", "citeRegEx": "Vogel et al\\.,? 2013", "shortCiteRegEx": "Vogel et al\\.", "year": 2013}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "The model is built around a version of the Rational Speech Acts (RSA) model (Frank and Goodman, 2012; Goodman and Frank, 2016), in which agents reason recurar X iv :1 70 3.", "startOffset": 76, "endOffset": 126}, {"referenceID": 11, "context": "The model is built around a version of the Rational Speech Acts (RSA) model (Frank and Goodman, 2012; Goodman and Frank, 2016), in which agents reason recurar X iv :1 70 3.", "startOffset": 76, "endOffset": 126}, {"referenceID": 0, "context": "The RSA recursion is then defined in terms of these base agents: the pragmatic speaker produces utterances based on a literal RNN listener (Andreas and Klein, 2016), and the pragmatic listener interprets utterances based on the pragmatic speaker\u2019s behavior.", "startOffset": 139, "endOffset": 164}, {"referenceID": 7, "context": "While many previous data sets feature descriptions of individual colors (Cook et al., 2005; Munroe, 2010; Kawakami et al., 2016), situating colors in a communicative context elicits greater variety in language use, including negations, comparatives, superlatives, metaphor, and shared associations.", "startOffset": 72, "endOffset": 128}, {"referenceID": 27, "context": "While many previous data sets feature descriptions of individual colors (Cook et al., 2005; Munroe, 2010; Kawakami et al., 2016), situating colors in a communicative context elicits greater variety in language use, including negations, comparatives, superlatives, metaphor, and shared associations.", "startOffset": 72, "endOffset": 128}, {"referenceID": 18, "context": "While many previous data sets feature descriptions of individual colors (Cook et al., 2005; Munroe, 2010; Kawakami et al., 2016), situating colors in a communicative context elicits greater variety in language use, including negations, comparatives, superlatives, metaphor, and shared associations.", "startOffset": 72, "endOffset": 128}, {"referenceID": 1, "context": "Our task is fundamentally the same as that of Baumgaertner et al. (2012), but the corpus we release is larger by several orders of magnitude, consisting of 948 complete games with 53,365 utterances produced by human participants paired into dyads on the web.", "startOffset": 46, "endOffset": 73}, {"referenceID": 31, "context": "We evaluate our agents on a task of language understanding in a dyadic reference game (Rosenberg and Cohen, 1964; Krauss and Weinheimer, 1964; Paetzel et al., 2014).", "startOffset": 86, "endOffset": 164}, {"referenceID": 21, "context": "We evaluate our agents on a task of language understanding in a dyadic reference game (Rosenberg and Cohen, 1964; Krauss and Weinheimer, 1964; Paetzel et al., 2014).", "startOffset": 86, "endOffset": 164}, {"referenceID": 29, "context": "We evaluate our agents on a task of language understanding in a dyadic reference game (Rosenberg and Cohen, 1964; Krauss and Weinheimer, 1964; Paetzel et al., 2014).", "startOffset": 86, "endOffset": 164}, {"referenceID": 6, "context": "Unlike traditional natural language processing tasks, in which participants provide impartial judgements of language in isolation, reference games embed language use in a goal-oriented communicative context (Clark, 1996; Tanenhaus and Brown-Schmidt, 2008).", "startOffset": 207, "endOffset": 255}, {"referenceID": 35, "context": "Unlike traditional natural language processing tasks, in which participants provide impartial judgements of language in isolation, reference games embed language use in a goal-oriented communicative context (Clark, 1996; Tanenhaus and Brown-Schmidt, 2008).", "startOffset": 207, "endOffset": 255}, {"referenceID": 5, "context": "Since they offer the simplest experimental setup where many pragmatic and discourse-level phenomena emerge, these games have been used widely in cognitive science to study topics like common ground and conventionalization (Clark and Wilkes-Gibbs, 1986), referential domains (Brown-Schmidt and Tanenhaus, 2008), perspective-taking (Hanna et al.", "startOffset": 222, "endOffset": 252}, {"referenceID": 4, "context": "Since they offer the simplest experimental setup where many pragmatic and discourse-level phenomena emerge, these games have been used widely in cognitive science to study topics like common ground and conventionalization (Clark and Wilkes-Gibbs, 1986), referential domains (Brown-Schmidt and Tanenhaus, 2008), perspective-taking (Hanna et al.", "startOffset": 274, "endOffset": 309}, {"referenceID": 14, "context": "Since they offer the simplest experimental setup where many pragmatic and discourse-level phenomena emerge, these games have been used widely in cognitive science to study topics like common ground and conventionalization (Clark and Wilkes-Gibbs, 1986), referential domains (Brown-Schmidt and Tanenhaus, 2008), perspective-taking (Hanna et al., 2003), and overinformativeness (Koolen et al.", "startOffset": 330, "endOffset": 350}, {"referenceID": 20, "context": ", 2003), and overinformativeness (Koolen et al., 2011).", "startOffset": 33, "endOffset": 54}, {"referenceID": 15, "context": "source framework of Hawkins (2015). Participants were sorted into dyads, randomly assigned the role of speaker or listener, and placed in a game environment containing a chat box and an array of three color patches (Figure 1).", "startOffset": 20, "endOffset": 35}, {"referenceID": 32, "context": "We used the most recent CIEDE standard to measure color differences, which is calibrated to human vision (Sharma et al., 2005).", "startOffset": 105, "endOffset": 126}, {"referenceID": 5, "context": "The collection effort was thus structured like a large-scale behavioral experiment, closely following experimental designs like those of Clark and Wilkes-Gibbs (1986). This paves the way to assessing our model not solely based on the listener\u2019s classification accuracy, but also in terms of how qualitative features of the speaker\u2019s production compare to that of our human participants.", "startOffset": 137, "endOffset": 167}, {"referenceID": 38, "context": "To test this, we used the Stanford CoreNLP part-ofspeech tagger (Toutanova et al., 2003) to mark the presence or absence of comparatives (JJR or RBR) and superlatives (JJS or RBS) for each message.", "startOffset": 64, "endOffset": 88}, {"referenceID": 8, "context": "To evaluate this idea, we use WordNet (Fellbaum, 1998) to derive a specificity hierarchy for color terms, and we hypothesized that split or close conditions will tend to lead speakers to go lower in this hierarchy.", "startOffset": 38, "endOffset": 54}, {"referenceID": 13, "context": "The back-and-forth nature of this interpretive process mirrors that of conversational implicature (Grice, 1975) and reflects more general ideas from Bayesian cognitive modeling (Tenenbaum et al.", "startOffset": 98, "endOffset": 111}, {"referenceID": 37, "context": "The back-and-forth nature of this interpretive process mirrors that of conversational implicature (Grice, 1975) and reflects more general ideas from Bayesian cognitive modeling (Tenenbaum et al., 2011).", "startOffset": 177, "endOffset": 201}, {"referenceID": 12, "context": "The model and its variants have been shown to capture a wide range of pragmatic phenomena in a cognitively realistic manner (Goodman and Stuhlm\u00fcller, 2013; Smith et al., 2013; Kao et al., 2014; Bergen et al., 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 33, "context": "The model and its variants have been shown to capture a wide range of pragmatic phenomena in a cognitively realistic manner (Goodman and Stuhlm\u00fcller, 2013; Smith et al., 2013; Kao et al., 2014; Bergen et al., 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 16, "context": "The model and its variants have been shown to capture a wide range of pragmatic phenomena in a cognitively realistic manner (Goodman and Stuhlm\u00fcller, 2013; Smith et al., 2013; Kao et al., 2014; Bergen et al., 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 2, "context": "The model and its variants have been shown to capture a wide range of pragmatic phenomena in a cognitively realistic manner (Goodman and Stuhlm\u00fcller, 2013; Smith et al., 2013; Kao et al., 2014; Bergen et al., 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al.", "startOffset": 124, "endOffset": 214}, {"referenceID": 36, "context": ", 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al., 2014; Vogel et al., 2013).", "startOffset": 102, "endOffset": 143}, {"referenceID": 40, "context": ", 2016), and the central Bayesian calculation has proven useful in a variety of communicative domains (Tellex et al., 2014; Vogel et al., 2013).", "startOffset": 102, "endOffset": 143}, {"referenceID": 26, "context": "These RGB vectors are then Fourier-transformed as in Monroe et al. (2016) to obtain the representation f .", "startOffset": 53, "endOffset": 74}, {"referenceID": 34, "context": "The context is reordered to place the target color last, minimizing the length of dependence between the most important input color and the output (Sutskever et al., 2014) and eliminating the need to represent the index of the target separately.", "startOffset": 147, "endOffset": 171}, {"referenceID": 17, "context": "The model is substantively similar to well-known models for image caption generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), which use the output of a convolutional neural network as the representation of an input image and provide this representation to the RNN as an initial state or first word (we represent the context using a second RNN and concatenate the context representation onto each input word vector).", "startOffset": 85, "endOffset": 135}, {"referenceID": 39, "context": "The model is substantively similar to well-known models for image caption generation (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), which use the output of a convolutional neural network as the representation of an input image and provide this representation to the RNN as an initial state or first word (we represent the context using a second RNN and concatenate the context representation onto each input word vector).", "startOffset": 85, "endOffset": 135}, {"referenceID": 41, "context": "We use ADADELTA (Zeiler, 2012) and Adam (Kingma and Ba, 2014), adaptive variants of stochastic gradient descent (SGD), to train listener and speaker models.", "startOffset": 16, "endOffset": 30}, {"referenceID": 19, "context": "We use ADADELTA (Zeiler, 2012) and Adam (Kingma and Ba, 2014), adaptive variants of stochastic gradient descent (SGD), to train listener and speaker models.", "startOffset": 40, "endOffset": 61}, {"referenceID": 28, "context": "012, approximate permutation test (Pad\u00f3, 2006) with Bonferroni correction, 10,000 samples.", "startOffset": 34, "endOffset": 46}, {"referenceID": 13, "context": "sational implicature (Grice, 1975).", "startOffset": 21, "endOffset": 34}, {"referenceID": 23, "context": "(2014) evaluate a model of color description generation (McMahan and Stone, 2015) on the", "startOffset": 56, "endOffset": 81}, {"referenceID": 10, "context": "Golland et al. (2010) develop a pragmatic speaker model, S(L0), that reasons about log-linear listeners trained on human utterances containing spatial references in virtual-world environments.", "startOffset": 0, "endOffset": 22}, {"referenceID": 10, "context": "Golland et al. (2010) develop a pragmatic speaker model, S(L0), that reasons about log-linear listeners trained on human utterances containing spatial references in virtual-world environments. Tellex et al. (2014) apply a similar technique, under the name inverse semantics, to create a robot that can informatively ask humans for assistance in accomplishing tasks.", "startOffset": 0, "endOffset": 214}, {"referenceID": 10, "context": "Golland et al. (2010) develop a pragmatic speaker model, S(L0), that reasons about log-linear listeners trained on human utterances containing spatial references in virtual-world environments. Tellex et al. (2014) apply a similar technique, under the name inverse semantics, to create a robot that can informatively ask humans for assistance in accomplishing tasks. Meo et al. (2014) evaluate a model of color description generation (McMahan and Stone, 2015) on the", "startOffset": 0, "endOffset": 384}, {"referenceID": 1, "context": "color reference data of Baumgaertner et al. (2012) by creating an L(S0) listener.", "startOffset": 24, "endOffset": 51}, {"referenceID": 1, "context": "color reference data of Baumgaertner et al. (2012) by creating an L(S0) listener. Monroe and Potts (2015) implement an end-to-end trained S(L(S0)) model for referring expression generation in a reference game task.", "startOffset": 24, "endOffset": 106}, {"referenceID": 0, "context": "The closest work to ours that we are aware of is that of Andreas and Klein (2016), who also combine neural speaker and listener models in a reference game setting.", "startOffset": 57, "endOffset": 82}, {"referenceID": 22, "context": "Dialogue agents are more challenging to model than isolated speakers and listeners, requiring long-term planning, remembering previous utterances, and (for the listener) deciding when to ask for clarification or commit to a referent (Lewis, 1979; Brown and Yule, 1983; Clark, 1996; Roberts, 1996).", "startOffset": 233, "endOffset": 296}, {"referenceID": 3, "context": "Dialogue agents are more challenging to model than isolated speakers and listeners, requiring long-term planning, remembering previous utterances, and (for the listener) deciding when to ask for clarification or commit to a referent (Lewis, 1979; Brown and Yule, 1983; Clark, 1996; Roberts, 1996).", "startOffset": 233, "endOffset": 296}, {"referenceID": 6, "context": "Dialogue agents are more challenging to model than isolated speakers and listeners, requiring long-term planning, remembering previous utterances, and (for the listener) deciding when to ask for clarification or commit to a referent (Lewis, 1979; Brown and Yule, 1983; Clark, 1996; Roberts, 1996).", "startOffset": 233, "endOffset": 296}, {"referenceID": 30, "context": "Dialogue agents are more challenging to model than isolated speakers and listeners, requiring long-term planning, remembering previous utterances, and (for the listener) deciding when to ask for clarification or commit to a referent (Lewis, 1979; Brown and Yule, 1983; Clark, 1996; Roberts, 1996).", "startOffset": 233, "endOffset": 296}], "year": 2017, "abstractText": "We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.", "creator": "LaTeX with hyperref package"}}}