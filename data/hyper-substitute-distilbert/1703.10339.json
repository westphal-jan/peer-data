{"id": "1703.10339", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Mar-2017", "title": "Finding News Citations for Wikipedia", "abstract": "an example scholarly target in wikipedia is to provide citations / added statements in dictionary pages, stating statements can be arbitrary symbol, text, possibly from their blog to a paragraph. in many cases citations are either outdated simply missing easily.", "histories": [["v1", "Thu, 30 Mar 2017 07:48:31 GMT  (982kb,D)", "https://arxiv.org/abs/1703.10339v1", null], ["v2", "Mon, 24 Apr 2017 18:28:09 GMT  (983kb,D)", "http://arxiv.org/abs/1703.10339v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.SI", "authors": ["besnik fetahu", "katja markert", "wolfgang nejdl", "avishek anand"], "accepted": false, "id": "1703.10339"}, "pdf": {"name": "1703.10339.pdf", "metadata": {"source": "CRF", "title": "Finding News Citations for Wikipedia", "authors": ["Besnik Fetahu", "Katja Markert", "Wolfgang Nejdl", "Avishek Anand"], "emails": ["fetahu@L3S.de", "nejdl@L3S.de", "anand@L3S.de", "markert@cl.uni-heidelberg.de", "Permissions@acm.org."], "sections": [{"heading": null, "text": "In this work we address the problem of finding and updating news citations for statements in entity pages. We propose a two-stage supervised approach for this problem. In the first step, we construct a classifier to find out whether statements need a news citation or other kinds of citations (web, book, journal, etc.). In the second step, we develop a news citation algorithm for Wikipedia statements, which recommends appropriate citations from a given news collection. Apart from IR techniques that use the statement to query the news collection, we also formalize three properties of an appropriate citation, namely: (i) the citation should entail the Wikipedia statement, (ii) the statement should be central to the citation, and (iii) the citation should be from an authoritative source.\nWe perform an extensive evaluation of both steps, using 20 million articles from a real-world news collection. Our results are quite promising, and show that we can perform this task with high precision and at scale."}, {"heading": "1. INTRODUCTION", "text": "Wikipedia has become the most used Internet encyclopedia and, indeed, one of the most popular websites overall.1 In addition, due to Wikipedia\u2019s inclusion into widely used applications such as Google KnowledgeGraph or Apple\u2019s Siri system, its content will influence the knowledge and, potentially, the behavior of millions of users, even if they do not visit the Wikipedia site directly. Therefore, it is essential that its content is accurate and reliable.\nIn contrast to traditional encyclopedias, Wikipedia is not authored mainly by experts. Also, the articles are authored\n1In 2015 it was in the top 10 most visited Internet sites according to the Alexis Internet ranking www.alexa.com).\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.\nCIKM\u201916 , October 24-28, 2016, Indianapolis, IN, USA c\u00a9 2016 ACM. ISBN 978-1-4503-4073-1/16/10. . . $15.00 DOI: http://dx.doi.org/10.1145/2983323.2983808\ncollaboratively by more than just a small number of contributors and the identity and expertise of authors is hard to verify. This leaves Wikipedia articles open to addition of inaccurate content, spamming or vandalism, and calls into question its reliability. A substantial number of reliability studies have compared Wikipedia against other reference works (such as the Encyclopedia Britannica or drug package information) or subjected them to expert review: The exhaustive survey in [16] concludes that the results of these studies have overall been favourable to Wikipedia when it comes to accuracy of facts, although some works (especially on medical articles) found errors of omission.2\nThese surprisingly favorable results on the reliability of Wikipedia can in all probability be traced to a small number of Wikipedia editorial policies, one of which we are concerned with in this paper. The Verifiability policy requires Wikipedia contributors to support their additions with citations from authoritative external sources. In particular, Wikipedia policy states that \u201carticles should be based on reliable, third-party, published sources with a reputation for fact-checking and accuracy.\u201d3 This policy, on the one hand, guides contributors towards both neutrality and the importance of authoritative assessment and, on the other hand, allows Wikipedia core editors to identify unreliable articles more easily via a lack of such citations. Citations therefore play a crucial role in ensuring and upholding Wikipedia reliability.\nFor current and recent events, news citations are one of the most-used sources [9]. Again, Wikipedia encourages the use of news outlets as citations: \u201cnews reporting from wellestablished news outlets is generally considered to be reliable for statements of fact\u201d3. As we show in Section 3, news are indeed the second-most widely used citation category in Wikipedia (with 1.88 million citations in our English Wikipedia snapshot) \u2013 however, around 26% of these are no longer available due to dead or redirected links. In addition, new information is added all the time and will need verification. For both these purposes, an automatic way of finding an authoritative news citation for any fact(s) one might wish to update, locate again or add would greatly facilitate Wikipedia editing and improve its reliability. Moreover, if no such citation can be found, it can guide contributors or core editors towards questioning their edits.\n2The standard for medical information should be higher for obvious reasons and omitted information for side effects or risks can be crucial. 3https://en.wikipedia.org/wiki/Wikipedia:Identifying reliable sources\nar X\niv :1\n70 3.\n10 33\n9v 2\n[ cs\n.I R\n] 2\n4 A\npr 2\n01 7\nIn this paper, we suggest such a method for automatic news citation discovery for Wikipedia. In particular, we make the following contributions: (i) We analyze for which type of Wikipedia statements a news citation is appropriate (in contrast to, for example, a scientific journal citation), taking into account the type and structure of entity the statement is about, as well as the language the statement is written in. We provide a supervised learning algorithm for statement classification into citation categories. (ii) We then develop a citation discovery algorithm which formalizes three properties of a good citation, namely that it entails the statement it supports, that it is from an authoritative source and that the statement it supports is central to it. (iii) We establish a large-scale evaluation framework for citation discovery which uses crowdsourcing for measuring our approach\u2019s precision.\nTo the best of our knowledge, this is the first work that automatically discovers citations for fine-grained Wikipedia statements. We show that news citations can be discovered with high precision, in large contemporary news collections. In particular, we with high accuracy recover the same or very similar citations as the ones originally given by Wikipedia contributors in the presence of numerous strong distractors or even find citations which are preferable to the original ones (as established via crowdsourcing)."}, {"heading": "2. PROBLEM DEFINITION AND APPROACH OUTLINE", "text": "In this section, we describe the terminology and problem definition for finding news citations for Wikipedia."}, {"heading": "2.1 Terminology and Problem Definition", "text": "We operate on a specific snapshot of Wikipedia W where the text in each Wikipedia page e \u2208 W is organized into sections denoted by \u03a8(e). Additionally, entity pages are organized into a type structure, which is a directed-acyclicgraph (DAG) induced by the Wikipedia categories. This is routinely exploited by knowledge bases like YAGO (e.g. Barack Obama isA Person) and we leverage this type structure where each page e belongs to a set of types T (e). We, however, modify the original YAGO type structure to make it depth consistent as explained in Section 4.3.\n2.1.1 Citations and Wikipedia Statements Citation: In Wikipedia pages, any piece of text can be\nsupported by a citation. The citation points to an external information source, such as a news article, blog, book or journal, that is considered as evidence for the fact mentioned in the text. Citations in Wikipedia are categorized into a predefined set of 16 citation categories viz. c = {web, news, books, journal, map, comic, court, press release, . . .}. The distribution of the citation types is given in Figure 2.\nStatement: We will refer to the piece of text from a Wikipedia page that has or needs a citation as a Wikipedia statement or simply a statement. In this work, we restrict statements to a single sentence or a sequence of sentences that occur between two consecutive citation markers or a citation marker and paragraph beginning/end. A citation marker is either an actual citation or a placeholder citation needed4. We therefore leave the identification of statements to future work. We also do not consider finding evidence\n4https://en.wikipedia.org/wiki/Template:Citation needed\nfor partial sentences or clauses. Each statement s in a page e belongs to a section \u03c8 \u2208 \u03a8(e), and the set of statements extracted from a section \u03c8 of e is represented as S(e, \u03c8).\nAnchors and Entities: Typically words or phrases in statements link to other Wikipedia pages which represent entities through anchors. We denote these links to other pages or entities starting from a statement s as \u03b3(s), and T (s) = {T (e) | e \u2208 \u03b3(s)} the corresponding entity types.\n2.1.2 Citation Finding Tasks We posit that the following two tasks are integral to find-\ning a citation for a Wikipedia statement. Statement Categorization. For a statement s from a page e of an unknown citation category, the task aims to determine the correct citation category for s.\nSC : f(s, e)\u2192 c,where c \u2208 {web, news,. . .} (1)\nWe want to categorize s as a news statement if it requires a news citation. This is based on the hypothesis that each statement typically has a preferred citation category, which we need to determine before making a high precision citation recommendation.\nCitation Discovery. Given a (i) statement s found in page e and of category c =news, and (ii) an external news collection N , we define the citation discovery task as finding articles n \u2208 N that serve as evidence for s. We define the function FC which for s outputs the subset of articles that can be suggested for citation.\nFC : f(s, e,N )\u2192 \u3008s, n\u3009 \u2208 {\u2018correct\u2032, \u2018incorrect\u2032} (2)"}, {"heading": "2.2 Approach Overview", "text": "Figure 1 shows an overview of our approach. For an entity, we extract entity and type structure, and its statements and finally run the steps of statement categorization and citation discovery.\nStatement Categorization\u2013SC. In the first step, we predict the citation category of a Wikipedia statement s via supervised machine learning. We train a multi-class classification model, where the classes correspond to the citation categories c.\nCitation Discovery\u2013FC. In the second step, for all news statements we find evidence for them via news articles. We retrieve candidate news articles from a news collection N through standard information retrieval methods with s serving as our query, and classify each candidate as either an appropriate citation for s or not."}, {"heading": "3. WIKIPEDIA GROUND-TRUTH", "text": ""}, {"heading": "3.1 Ground-Truth: Wikpedia News Statements", "text": "From a Wikipedia snapshot W (2015-07-01) we extract all statements and all citations associated with that statement.5 We extract 6.9 million statements with 8.8 million citations, from 1.65 million entities and 668k section types.\nCitations are categorized into one of the categories c by the Wikipedia editors. However, sometimes the editors do\n5As a statement can have different clauses, sometimes extracted citations only serve as evidence for part of the statement. We, however, do not distinguish at this level of granularity but assume that all associated citations support the whole statement.\nnot categorize a citation as news although they should do so.\nFor example, in W, its top\u20133 news domains BBC, NYTimes, Guardian, are often cited in categories other than news.6 Most of such violations by the editors occur when citing news under the category web, which often is a catch-all for almost any type of resource (news, book, etc.).\nIn most cases such violations can be accurately corrected by applying two simple heuristics:\nMajority Voting. Citations from the same domain URL are tagged with different categories. We resolve such cases based on majority voting. In case a domain is cited more often under the news category, then all citations to the same domain are changed to news.\nURL Patterns. In this heuristic we look for patterns in the URL, specifically for \u2018/news/ \u2019 and \u2018http://news.\u2019. This rule is applied to web statements, and in case the URL matches one of the patterns, we change its category to news.\nTable 1 shows the top\u20134 most frequent citation categories and the impact of our ground-truth curation rules. Rule application changes the citation category for 1,652,619 citations, approximately 18% of all citations in W. The cells in the table show the number of statements that are changed from the category in the row to the category in the column table.\nWe say that a statement is a news statement if it contains at least one news citation (after ground-truth curation). Figure 2 shows the statement distribution across the categories. It is evident that web and news are the two most popular categories, with 5.3 and 1.88 million citations, coming from 1.2 million and 436k entities, respectively."}, {"heading": "3.2 Wikipedia News Collection", "text": "From the news statements, we extract the cited news articles and construct the Wikipedia news collectionNW , which 6Thus, the citation http://news.bbc.co.uk/1/hi/uk politics/ 7433479.s\\tm from the entity Liam Byrne has been categorized as web, although the more specific news category would have been appropriate.\nserves as our ground-truth for the citation discovery task. We define Nt \u2286 NW as the set of articles cited from statements s which come from entities of type t. With Ns we denote the set of articles cited by s.\nFrom the collection of news statements, we have 1.88 million citations to news articles (see above). We successfully crawled 1.5 million articles.The remaining 19% of citations point to non-existent articles (dead links, moved content etc.). Furthermore, some of the successfully crawled URLs point to the index pages. This can be noticed when we consider the article length (in terms of characters) in Figure 3. Filtering out articles that are below 200 characters, we are left with with 1.39 million articles, a decrease of 26% from the original 1.88 million news citations.\nAn additional issue we notice in NW are citations to nonEnglish news articles. We find that 23% of articles in NW are in languages other than English, using Apache Tika7 for language detection."}, {"heading": "4. STATEMENT CATEGORIZATION", "text": "7http://tika.apache.org\nIn the statement categorization task, we are given a statement s and the entity e from which it is extracted. We compute features that exploit the language style of s and the type and section structure of e to categorize s into one of the citation categories c. We learn a multi-class classifier (Section 4.3) with classes corresponding to citation categories c and optimize for predicting news statements. Table 2 shows an overview of the feature list."}, {"heading": "4.1 Statement Language-Style", "text": "We hypothesize that Wikipedia statements with news citations are similar to the language style of news, as they often paraphrase cited news articles. Different genres (such as news, recipes, sermons, FAQs, fiction . . . ) differ in their linguistic properties as the different functions they fulfill influence linguistic form [5]. For example, we expect news reports (which center mostly on past events) to contain more past tense verbs than a recipe which gives instructions via verbs in the imperative. We use features that were successful in automatic genre classification including structural features via parts-of-speech as well as lexical surface cues [18].\nPart of Speech Density. Frequency of part-of-speech (POS) tags, determined via the Stanford tagger, allows us to capture some of the structural properties of text. For example, news statements can be characterized by a high number of past tense verbs as well as proper names. We normalize the POS tag frequency w.r.t the sum of all tags in a statement, to account for varying statement length.\nVerbs of Attribution and Quotation Marks. News articles often report statements by persons of repute, witnesses or other sources. We approximate this by two features: Firstly, we count verbs of attribution in s, via a list of 92 such verbs (claim, tell etc) with POS tag VB* and normalize w.r.t the total number of VB*. Secondly, we use quotation marks as a potential indicator of paraphrasing. The feature simply counts the number of quotation marks in s, normalized w.r.t the statement length.\nTemporal Proximity \u03bb(s). Most Wikipedia statements with news citations refer to relatively recent events, i.e. events close to the time of the Wikipedia snapshot. Therefore, we use temporal expressions such as dates and years as distinguishing features for news statements. We use a set of hand-crafted regular expression rules to extract tempo-\nral expressions.8. We use the following rules: (1) DD Month YYYY, (2) DD MM YYYY, (3) MM DD YY(YY), (4) YYYY, with different delimiters (whitespace, \u2018-\u2019, \u2018.\u2019). We then compute \u03bb(s) = |Y ear(W)\u2212 Y ear(s)|.\nDiscourse Analysis. We use discourse connectives to annotate the statements s with explicit discourse relations based on an approach proposed by Pitler and Church [19]. The annotations belong to the categories {temporal, contingency, comparison, expansion}, following the Penn Discourse Treebank annotation [20]. Some of the explicit discourse relations are particularly interesting (i.e., temporal) as they represent a common language construct used in news articles that report event sequences. The features are boolean indicators on whether s contains a specific explicit discourse relation.\nLanguage Model and Topic Model Scoring. As surface lexical features have been shown to be efficient in genre recognition [22], we compute n\u2013gram (up to n=3) language models with Kneser-Ney smoothing (LM) from news articles Nt and compute the score \u03b8(s,Nt). This score shows how likely s can be constructed from the LM. Similarly, we compute topic models using the LDA framework [6], where the score is the jaccard similarity between s and the topic terms from Nt."}, {"heading": "4.2 Entity-Structure Based Features", "text": "Determining if a statement requires a news citation solely on language style is not always feasible. We exploit the entity structure of e and compute the probability of statements having a news citation given its types T (e) and sections \u03a8(e). Section-Type Probability. A good indicator of the likelihood that a statement s requires a news citation is the entity type it belongs to and the section type that it appears in. For instance, for type Politician, news statements have higher density in section \u2018Early Life and Career\u2019 as these tend to be more reflected in news. To avoid over-fitting we filter out entity types with fewer than 10 statements. Similarly, we filter out section with fewer than 10 statements, and in which they belong to the same citation category.\nWe compute the conditional probability of having a news citation for s for an entity type t \u2208 T (e) given a section type \u03c8.\np(s = news|t, \u03c8) = \u2211 e\u2208W\u2227t\u2208T (e) \u2211 s\u2208S(e,\u03c8) 1s typeOf news\u2211\ne\u2208W\u2227t\u2208T (e) |S(e, \u03c8)|\nThe p(s = news|t, \u03c8) probability is likely to be a sparse feature, so we compute type and section news-priors. We compute section p(s = news|\u03c8) and type news-priors p(s = news|t) based on the news statement ratio that belong to a section or type, respectively.\nSince s is associated with an entity e, which has a set of types T (e), we aggregate the computed type news-priors and the section-type joint probability into their min, max and avg probabilities.\nType Co-Occurrence. From the entity types T (s) and T (e) we measure the likelihood of type co-occurrence in news. The probability simply counts the co-occurrence between t and t\u2032 in news statements with respect to their to-\n8This proved to be more scalable than state-of-the-art extractors like HeidelTime [23] and Stanford\u2019s CoreNLP [11] module\ntal co-occurrence. Examples of highly co-occurring types in news are Politician and Organization. p(s = news|t\u2032, t) = \u2211 e\u2208W\u2227t\u2208T (e) \u2211 s\u2208S(e)\u2227t\u2032\u2208T (s) 1s typeOf news\u2211\ne\u2208W\u2227t\u2208T (e) \u2211 s\u2208S(e) 1t\u2032\u2208T (s)"}, {"heading": "4.3 Learning Framework", "text": "Learning Setup. Wikipedia consists of a highly diverse set of entities. A model trained on all entities is unlikely to work. For example, the types Location and Politician represent two highly divergent groups with regard to entity page structure, the statements they contain and the way they are reported in news.\nWe therefore learn SC for individual types in the YAGO type taxonomy. The advantages of type specific functions SC is that they are trained on homogeneous entities, which helps the models predict with greater accuracy. We take only types that have more than 1000 entity instances, resulting in 672 types. The types are organized from very broad types such as (owl:Thing) to very specific types like Serie_A_Players.\nTo utilize the specialization and generalization in a principled manner we transform the YAGO type taxonomy (DAG) into a hierarchical DAG. This is utilized later on in order to find the right level of type granularity for learning SC.\nWe assume that the hierarchy is rooted at owl:Thing and all internal nodes are depth-consistent, i.e. all paths from the root to the node are of the same length. We obtain this by a simple heuristic whereby for every child type \u2192 parent type we remove edges where the parent\u2019s depth level in the taxonomy is higher than the minimum level from other parent nodes.\nWith this hierarchical type-taxonomy, we can determine the optimal level of type granularity such that we have optimal performance in categorizing statements. For learning the type specific SC, we keep 10% of entity instances for evaluation and the remainder for training. It is important to note that when we learn SC for a given type, the training instances are sampled through stratified sampling from all its children types.\nLearning Model. The functions SC represent multiclass classifiers with classes corresponding to the citation categories. Since we want to predict the news category c =\u2018news\u2019 with high accuracy, one question is why we do not pose this as a binary classification problem, where a statement is categorized as news or not. We used the multi-class classifiers because they give us a more balanced distribution when compared to merging all non-news statements into a single category.\nFinally, we opt for Random Forests (RF) [7] as our supervised machine learning model. We experimented with other models, but the differences in performance are marginal, and RF have superior learning time. We train the models on the full feature set in Table 2."}, {"heading": "5. CITATION DISCOVERY", "text": "For the citation discovery task, we follow the citation policy9 guidelines in Wikipedia and single out three key properties on what makes a good citation.\n9https://en.wikipedia.org/wiki/Wikipedia:Citing sources\n1. the statement should be entailed by the cited news article\n2. the statement should be central in the cited news article\n3. the cited news article should be from an authoritative source\nWe approach the citation discovery for news statements as follows. We use statement s as a query (see Section 5.1) to retrieve the top\u2013k news articles from N as citation candidates for s. We then classify the candidate citations as either \u2018correct \u2019 or \u2018incorrect \u2019, depending on whether they meet the above criteria of a good citation.\nIn order to do so, we compute features for each pair \u3008s, ni\u3009, w.r.t the individual sentences of a news article ni. The feature vectors become the following \u3008s, [\u03c31i , \u03c32i , . . . , \u03c3ji ]\u3009, where \u03c3ji represents the j-th sentence from ni.\nSince the number of sentences \u03c3i varies across news articles, we aggregate the individually computed features at sentence level into the corresponding min, max, average, weighted average, and exponential decay function scores as shown below.\n\u3008s,min j F (\u03c3ji ),max j F (\u03c3ji ), Avg(F (\u03c3i)), \u2211 \u03c3 j i 1 j \u2217F, \u2211 \u03c3 j i F 1 j , . . .\u3009\nwhere F is a feature from the complete feature list in Table 3."}, {"heading": "5.1 Query Construction", "text": "We use the statement text as query which can vary from a sentence to a paragraph. One way to improve the likelihood of obtaining good citation candidates from top\u2013k articles is through query construction approaches (QC). It has been shown that in similar cases where the query corresponds to a sentence or paragraph, QC approaches are necessary to increase the accuracy of IR models. Henzinger et al. [13] propose several QC approaches that weigh query terms based on the tf\u2013idf score.\nWe experimented with different QC approaches from [13] and their impact on finding news articles in NW . We found that QCA1Base performed best and use it in the remainder of the paper. In QCA1Base, the terms extracted from the statements are weighted based on tf\u2013idf, with tf and idf are computed w.r.t the other statements under consideration.\nIn principle, one should consider all retrieved articles from the result set. However, this is not only computationally expensive for our subsequent learning step but also unbalances our training set. To determine a reasonable retrieval depth, we experimented with 1000 randomly chosen statement queries with QC and determined the hit-rate at retrieval depth k , i.e. whether the cited article is retrieved in the top\u2014k articles.\nFigure 4 shows the hit-rate in top\u20131000 with top 50 ranked query terms and with divergence from randomness query similarity measure [1] for our random sample of 1000 news statements.\nWe focus on the top\u2013100 retrieved news articles as potential citations for s, as the achieved hit-rate beyond the top\u2013100 shows only minor improvement. In Figure 4, we also note that the hit-rate does not go beyond 50%. We found that most of the news articles that are not retrieved are either missing or non-English articles in NW ."}, {"heading": "5.2 Textual Entailment Features", "text": "As the citation is supposed to give close evidence for the statement\u2019s content, in the ideal case the cited news article should fully entail the statement, i.e. the statement should be derivable from the news article. The recognition of textual entailment has been the study of extensive research in the last 10 years; cf [8] for an overview. A full treatment of entailment needs extensive world knowledge and inference rules; we here restrict ourselves to much simpler lexical and syntactic similarity methods used in baseline entailment systems and leave the extensions to future work.10\nIR Baseline Features. We use the retrieval model as a pre-filter to find candidate news articles as citations for s. The retrieval model also provides us with two possible features for the learning model: firstly, a matching score of ni for query s, where the score corresponds to the divergence from randomness query similarity measure [1]. Secondly, the retrieval rank of ni. We use the IR model as our baseline and hence refer to them as baselines features.\nTree Kernel Similarity. Lexical similarity measures in many cases fail to capture the joint semantic and syntactic similarity. For this purpose, we consider the tree kernel similarity measure proposed in [14]. We first compute the dependency parse trees of s and \u03c3ji using the Stanford tagger [24], and then compute the tree kernel, K(s, \u03c3ji ). Tree kernel similarity through the dependency parse tree measures the maximum matching subtrees between s and \u03c3ji , where the matching subtrees have the same syntactic and semantic meaning. We refer the reader to [14] for details.\n10Off-the-shelf entailment systems exist but are too slow to use at scale.\nLM & Topic Model Scoring. From an article ni we compute a unigram LM and compute \u03b8(s, ni) as the likelihood of s being generated from the computed LM. In addition, we compute n\u2013gram LM (with n up to 3) from articles in Nt, and compute the score \u03b8\nn(s,Nt) accordingly. Similarly, we compute LDA topic models [6] for entity types, specifically from articles in Nt. This follows the intuition that content usually is clustered around specific topics, i.e. for type Politician most discussions are centered around politics, career, etc. The topic score is the Jaccard similarity between ni and the topic terms."}, {"heading": "5.3 Centrality Features", "text": "Similarity to most central news sentence. As described above we compute similarity features between s and sentences in ni. However, some sentences in ni are more central than others. Hence, the computed features between the pairs \u3008s, [\u03c31i , \u03c32i , . . . , \u03c3ji ]\u3009, do not have uniform weight. Therefore, we find the most central sentence \u03c3ci in ni and distinguish the computed entailment/similarity features between s and \u03c3ci .\nWe compute centrality of a sentence in ni through the TextRank approach introduced in [17]. We first construct a graph G = (V,E) from ni, where V corresponds to the sentences of ni, with edges in E weighted with the Jaccard similarity between any two sentences, in this case \u03c3ji \u2208 V . Computation of centrality for any vertex \u03c3ji is similar to that of PageRank, with slight changes accounting for the weighted edges between vertices.\n\u0393(\u03c3i) = (1\u2212 d) + d \u2217 \u2211\n\u03c3j\u2208In(\u03c3i) J(\u03c3i, \u03c3j)\u2211 \u03c3k\u2208Out(\u03c3j) J(\u03c3j , \u03c3k) \u0393(\u03c3j) (3)\nwhere d is the damping factor (d = 0.85), a common value in PageRank computation. The computation converges if the difference in the score of \u0393(\u03c3j) in two consecutive iterations is small.\nRelative Entity Frequency. The importance of e in ni is crucial when finding citations for s. This importance is partially mirrored simply in how often e is mentioned in ni. However, another genre-typical property of news is its inverted pyramid structure, i.e. the most important information is mentioned at the beginning of the article. We therefore measure relative entity frequency of e in ni based on an approach described in [10]. It attributes higher weight\nto entities appearing in the top paragraphs of ni, where the weight follows an exponential decay function.\n\u03c6(e, n) = |\u03c1(e, n)| |\u03c1(n)| \u2211 \u03c1\u2208\u03c1(n)  tf(e, \u03c1)\u2211 e\u2032 6=e tf(e\u2032, \u03c1)  1 \u03c1\n(4)\nwhere \u03c1 represents a news paragraph from n and \u03c1(n) indicates the set of all paragraphs. tf(e, \u03c1) indicates the frequency of e in \u03c1. With |\u03c1(e, n)| and |\u03c1(n)| we indicate the number of paragraphs in which entity e occurs and the total number of paragraphs.\nAdditionally we consider the relative entity frequency for entities in e \u2208 \u03b3(s) and measure the minimum, maximum and average relative entity frequency scores."}, {"heading": "5.4 News-Domain Authority Features", "text": "Wikipedia\u2019s editing policy distinguishes clearly between more and less-established news outlets and prefers the former (see the Introduction). We therefore compute the authority of news domains w.r.t entity types and sections.\nWe will denote the domain of the news article referred from s as D[s], and with D any arbitrary domain.\nType-Domain Authority. Authority of news domains is non-uniformly distributed across types. For types such as Politician the authority of domains like BBC is higher than for types such as Athletes, where a domain specialized in sports news is more likely to be authoritative. We capture the type-domain authority as follows:\np(D|t) = \u2211 e\u2208W\u2227t\u2208T (e) \u2211 s\u2208S(e) 1D=D[s]\u2211\ne\u2208W\u2227t\u2208T (e) \u2211 s\u2208S(e)D[s]\nSection-Domain Authority. We measure the authority of domains associated to certain entity sections. The density of news references across sections varies heavily. Therefore, it is natural to consider the authority of news domains for a given section.\np(D|\u03c8) = \u2211 e\u2208W \u2211 s\u2208S(e,\u03c8) 1D=D[s]\u2211\ne\u2208W \u2211 s\u2208S(e,\u03c8)D[s]\nNote that these features compute news outlet authority with regard to current Wikipedia usage, which we seek to re-create. An alternative we intend to look at in future work is to measure authoritativeness via Wikipedia-external measures of news outlets, such as page visits or interlinkage."}, {"heading": "6. STATEMENT CATEGORIZATION EVALUATION", "text": "Here we describe the evaluation of our approach for SC. Since we consider a type taxonomy, we have a hierarchy of models. Each statement belongs to an entity, which in turn is a child to a type (node) in the hierarchy. Consequently, we construct each model from training instances (statements) that are its children. We focus on two aspects of our approach (i) performance of models at varying depths, and (ii) performance of various feature classes.\nWe provide the detailed results for the statement categorization task and the corresponding ground-truth data at the paper URL11.\n11http://l3s.de/\u02dcfetahu/cikm2016/"}, {"heading": "6.1 Experimental Setup", "text": "Setup. We consider 672 entity types from our Yago taxonomy, for which we learn individual SC models. We consider types that have more than 1000 entity instances. The level of granularity in the YAGO taxonomy has a maximum depth of 20, while the root type is owl:Thing containing all possible entities.\nTrain/Test. We learn the SC models using up to 90% of the entity instances of a type t as training set, and the remainder of 10% for evaluation. We use stratified sampling to pick entities of type t and its subtypes for the train and test set. We train and test SC models over 6 million statements coming from 1.3 million entities.\nMetrics. We evaluate the performance of SC with precision P , recall R and F1. A statement is categorized correctly if the predicted category corresponds to the groundtruth."}, {"heading": "6.2 Results and Discussion", "text": "The following discussion focuses on the results for the statement categorization task for the news category. Due to space constraints we report the first three type levels in the Yago taxonomy, specifically the immediate child Legal Actor Geo of owl:Thing.12 The results for the remainder of the types are accessible at the URL11.\nTable 4 shows the results for SC models evaluated over 61k entities and trained with up to 550k entities, depending on the training sample size \u03c4 \u2208 [1%, 90%]. The results for this type represent more than 47% of the total set of entities in our evaluation dataset.\nThe overall performance of SC for all types for \u03c4 = 90% measured through micro-average precision is 0.57. Since a statement belongs to multiple types T (s), we decide the category of s based on majority as categorized from the individual SC models.\n6.2.1 Level of Type Granularity As expected, we observe that model performance depends\non the type level (cf. Table 4). A unified model from heterogeneous training instances performs poorly: the SC model for the main type Legal Actor Geo achieves a precision P=0.527 with high variance across its subtypes. Comparing the types at depth level 3, the difference in terms of precision can go as high as 15% between Legal Actor Geo and the best performing subtype preserver.\nAt higher depths, performance of the SC models often improves significantly as the instances belonging to a given type become more homogeneous. For example, the fine grained entity type wcat Italian footballers has a precision of P=0.87 and recall of R=0.58, which constitutes a 50% precision and a 26% recall improvement over its parent type Person. However, the performance improvement is not monotonically increasing. In some fine-grained types, there is in fact a performance reduction which can be attributed to over-fitting. This suggests that there is indeed a sweet spot in terms of choice of the best performing model for an instance. We observed that the instances that are children of person showed best performances between levels 5 and 8.\nOur models perform poorly for types such as location since location pages have a lower news density. We again\n12For readability we remove the wordnet prefix from the types and their numerical ID values.\nobserve that news articles are usually centered around people and its instances benefit the most from our approach. We also observe that the performance of our approach is sensitive to the type hierarchy. The choice of YAGO as a taxonomy is due its fine-grained types. However, there exist many long-tail entities that are direct descendants from the higher levels and fail to leverage the homogeneity of finegrained types. We also perform poorly on such instances.\nIn the YAGO taxonomy, the entities are distributed normally with a mean at depth level 8, which contains around 36% of entities. The long tail with types lower than depth level 8 accounts for 28% of entities in the YAGO taxonomy.\nWe focussed on the category news in our discussion and in Table 4. Performance of SC models for the categories c = {web, book, journal} and type person is P=0.62 and R=0.59, P=0.29 and R=0.69, and P=0.25 and R=0.26, respectively. The relatively high score for the web category can be attributed to the high density of statements of category web, accounting for more than 54% of the total statements. Hence, by always choosing web as the category of a statement we get an average precision of 0.54.\n6.2.2 Convergence and Feature Ablation Convergence. We measure the amount of training data\nrequired for the models to converge to optimal performance. Figure 5 shows the learning curve for some of the types reported in Table 4. We see that SC models converge and achieve optimal performance early on with a sample around 7% to 10%. Ablation. We apply a feature ablation test for the different different features groups from Table 2. Figure 6 shows the results for the feature groups language style, and entity structure. The highest gain is achieved with the feature group entity structure, which reveals the challenging nature of the task where language style features cannot be applied alone."}, {"heading": "7. CITATION DISCOVERY EVALUATION", "text": "In this section, we evaluate the citation discovery task for news statements. We perform an extensive evaluation for approximately 22k news statements and discover citations from a real-word news collection with 20 million articles in a timespan of two years.\n7.1 Statement and News Collection\nWe limit ourselves to the subset of news statements with citations to news articles in NW from 2013 to 2015. The resulting set contains 22k news statements with 27k news article citations in NW .13 We denote this temporal slice of news articles in NW by NW13\u221215.\nAs finding the right citation from this preselected collection is easier than the realistic scenario of finding a citation among all possible news, we also collected all English news articles from the period [2013-08, 2015-08] from the GDelt project14. We call the resulting high-coverage dataset NG.\nWe merge NG with NW13\u221215 and call the resulting dataset N = NW13\u221215 \u222a NG. The set N contains around 20 million news articles. NW13\u221215 accounts for less than 1% in N , making the correct articles hard to find.\n13A statement can have more than one citation. 14http://gdeltproject.org/"}, {"heading": "7.2 Evaluation Strategies", "text": "Evaluation Strategy E1: In this scenario, we, for each news statement s, only consider the pairs \u3008s, n\u3009, where n \u2208 Ns as correct and all other possible citations as incorrect. This allows for fully automatic evaluation but is only a lower bound for FC, as there can be additional articles that are relevant for s but do not exist in Ns. We therefore also consider a variant E1+FP, where we consider n\u2032 /\u2208 Ns as additional correct citations if the similarity (based on the jaccard similarity) to one of the articles in Ns is above 0.8.\nEvaluation Strategy E2: E2 assesses the true performance of FC. In this case, apart from already existing citations for s from Ns, we assess through crowd-sourcing the appropriateness as citations of articles n \u2208 N \u2227 n /\u2208 Ns.\nWe set up the crowd-sourcing experiment for E2 as follows. For a statement s and an article ni /\u2208 Ns marked as correct by FC, we ask the crowd to compare ni with the groundtruth article n \u2208 Ns and answer the question \u2018Which of the two shown news articles is an appropriate citation for the statement?\u2019. The workers are shown s as well as ni and the ground truth article in random order without an indication which one is the ground truth. We provide the following response options: (i) first, (ii) second, (iii) both, (iv) none, and (v) insufficient info. We deployed the experiment in CrowdFlower15 and chose only high quality workers to ensure the reliability of our experiments16. Furthermore, we removed workers who did not spend the minimum amount of two minutes to assess the appropriateness of a citation17.\nWe collect three judgments per question. We count citations as correct which are ground-truth articles or articles which the majority of workers judge as appropriate citations."}, {"heading": "7.3 Experimental Setup", "text": "Retrieval model. We use the retrieval model in [1] via the implementation provided by Solr18. We use the top-100 retrieved news articles for a statement as candidate citations, from which we perform feature extraction and learn our SC models.\nLearning Setup. We learn classifiers specific to entity types for a total of 83 types. We limit ourselves to types that have news statements in the date range 2013-2015 and with at least 100 entity instances. From our set of 22k statements, we randomly sample statements from each entity type if they have more than 1000 instances, otherwise we take all statements. Training and testing data consist of the pairs \u3008s, ni\u3009, where s is a news statement, and ni is one of the top\u2013100 citation candidates which we retrieve from N . We split training and testing data per statement s, where each s and all its candidates are included completely either in the training or test set. Learning Approach. We learn the FC models as supervised binary classification models using random forests RF[7]. We predict \u3008s, n\u3009 \u2208\u2018correct\u2019, \u2018incorrect\u2019, i.e. if a candidate news article is an appropriate citation for s or not. We optimize for the \u2018correct \u2019 class. The correct labels in training and automatic evaluation E1 are all part of\n15https://www.crowdflower.com 16We select workers with the highest quality as provided by the CrowdFlower platform. 17The amount of two minutes was decided based on the number of citations the workers had to assess per page (consisting of 5 citations to assess). 18http://lucene.apache.org/solr/\nNW13\u221215, which makes up less than 1% of our news collection N . Therefore, we learn FC as a cost-sensitive classifier. Metrics. We evaluate performance of FC models via precision P , recall R, and F1 score. Baselines. We consider two baselines (B1 and B2) for this task. For B1, we use the divergence from randomness model [1] to retrieve news articles from N for s and simply suggest the top\u20131 article as citation. In B2 we learn a supervised model based on the IR baseline features (see Table 3)."}, {"heading": "7.4 Results and Discussion", "text": "Table 5 shows the results for all evaluation strategies for the citation discovery task. We only display detailed results for the top\u201310 best performing entity types out of the 83 types in our evaluation. The results in each row in Table 5 show the best performance we achieve for the individual types, while varying the variables such as the training sample size and feature number. We show results with a maximum of 60% training sample size.\nWe report additionally the overall performance of FC models across all 83 types through micro-average in the last row in Table 5. The detailed results are accessible at the paper URL11.\n7.4.1 E1: Automated Evaluation In Table 5, in the third column, we show the evaluation\nresults for the strategy E1. Results for E1 are encouraging given the fact that in top\u2013 100 news candidates retrieved from N only 1% of the news are \u2018correct \u2019 (on average one relevant citation in NW13\u221215 per statement). Furthermore, as shown in Figure 4 the highest recall we get at top\u2013100 is on average around 45%.\nWe achieve the best performance in terms of precision for the entity type football player, with precision P=0.80 and a recall of R=0.30. For F1 the best performing type in this setup is the entity type player with F1=0.57.\nUsing the evaluation strategy E1+FP, we consider as relevant all false positive (FP) articles which are highly similar to the ground-truth articles Ns (above 0.8 similarity). Even though the FP articles do not exist in our ground-truth, the high similarity to the ground-truth article is a strong indicator for them being relevant citations. Using this strategy, the results improve for some of the types with up to 8% in terms of precision. For type entertainer we have an increase of 11%. In absolute numbers, by considering the highly-similar FP articles as relevant we gain an additional 757 news articles out of 12,877, i.e. an additional 6% news citations.\nBaselines B1 and B2 show the difficulty of the citation discovery task. In particular, we show that standard IR models struggle with this task. Choosing only the top\u20131 article for citation (B1) achieves only up to P=0.37. On the other hand, for B2, we see that we cannot learn well using only the IR baseline features, and perform even worse than using B1.\n7.4.2 E2: Automated+Crowdsourced Evaluation For E2, we report results after re-evaluating performance\nof FC models via gathering judgements for false positive (FP) news articles suggested as citations for s. We evaluate 11,803 false positive news article citation candidates for the top\u201310 entity types in Table 5, from 6.9k news state-\nments. As reported above, crowd-workers could choose between both ground truth and our suggestion being correct, one of them or neither. The inter-rater agreement between workers was 64%. Table 6 shows how these false positives were assessed.\nWe see that in many cases our suggestion was equal to (38.2%) or even preferred (19.4%) over the ground-truth suggestion. Hence, our method can even improve citation quality in Wikipedia.\nIn the E2 column in Table 5 we show the updated results for FC after collecting judgments for false positive news articles. We see that for most of the types we have an average gain of 18% in terms of precision. We achieve the biggest gain of 28% for the entity type location. For the types football player, creator, entertainer, we can suggest news citations with 90-91% precision. Please note that we do not report the recall score for E2, since assessing the appropriateness of every article in N as a citation for s is not feasible. The recall score is only reported w.r.t the groundtruth articles in NW13\u221215."}, {"heading": "8. PIPELINE EVALUATION", "text": "For the evaluation of both tasks in a pipeline scenario, we randomly sample 1000 statements from all categories and ran the process of citation discovery through both steps. Each statement is associated with multiple entity types, as they are extracted from e where T (e) is a set of types. For the statement categorization task we perform the evaluation based on our ground-truth; for the citation discovery we evaluate the suggested citations as in evaluation strategy E2. Note, that here in the evaluation pair we have a news article (that we suggest) and a resource that can be of any type including book, web, journal.\nStatement Categorization. We set up statement categorization as a majority voting categorization. For each statement and the type specific classifiers SC we predict the category and pick the category that has the majority of\nvotes. In contrast to the statement categorization in Section 4, where the original task aimed at showing for which types this task can be performed accurately, we now aim to set up citation discovery in an automated manner.\nBased on the ground-truth, 340 out of the 1000 statements were news statements. We categorize 368 as news statements, out of which 263 are correct, i.e. P=0.72 and R=0.77. It is interesting to see that we can leverage additional information through majority voting, where for the same statement and its associated types we can predict with high accuracy the citation category label of s.\nCitation Discovery. For the citation discovery task we ran it based on the generic FC model trained on statements belonging to all types, namely owl:Thing. We could use the type specific FC, with additional costs for computing type specific features.\nIn the second task, from the 368 statements classified as news statements, we ran the citation discovery model FC. We are able to suggest 549 news citations for 78 statements. Based on crowd-sourcing evaluation, we suggest 346 relevant citations, i.e. a precision of P=0.63, out of which 200 citations are citations that were preferred over existing ones in the ground-truth. For 146 cases the citations we suggest are considered to be equally appropriate as the existing ones in the ground-truth, for 116 citations the ground-truth ones were preferred over the ones we suggested. Note that our FC models suggest citations for s only in case they fulfill the criteria in Section 5, thus, enforcing high accuracy."}, {"heading": "9. RELATED WORK", "text": "Citation Sources. Ford et al. [12] analyze the citation behavior of Wikipedia editors with respect to their adherence to the citation guidelines. They investigate what types of sources are most often cited, i.e. primary, secondary and tertiary as defined in Wikipedia3. They conclude that news are one of the top cited source in the secondary type, while they see a growing trend of primary sources due to their persistence on the web, contrary to the policies of preferring secondary sources. Luyt and Tan [15] analyze a subset of history entity pages and show that citations are biased towards a specific group of sources. [12, 15] emphasize the importance of citations in Wikipedia as a means to ensure the quality of entity pages.\nWikipedia Quality. Anderka et al. [2] propose an approach to predict quality flaws in Wikipedia pages. A quality flaw in Wikipedia is usually annotated with specific cleanup\ntags. They train a model to predict quality flaws, where among the top\u201310 quality flaws they identify unreferenced, refimprove, primary sources as some of the most serious flaws. Our work is complementary to theirs since we aim at finding appropriate citations for Wikipedia statements, thereby improving the quality of Wikipedia pages.\nWikipedia Enrichment. Sauper and Barzilay [21] propose an approach to automatically generate complete entity pages for a specific entity type. The approach is trained on already-populated entity pages of a specific type by learning templates about the section structure at the type level. For a new entity page, they extract documents through Web search (with entity and section title as a query) and identify the most relevant paragraphs to add in a section. Fetahu et al. in [10] proposed an approach for suggesting news articles for a Wikipedia entity and entity section. They first identify news articles that are important to an entity and in which the entity is salient, and further identify the most appropriate section to suggest the article. In case of a missing section, a new section is added by exploiting the section structure from the entity type.\nThis work differs from [21, 10] as we do not add content or suggest news articles to a complete section in an entity page, but rather provide citations to already existing statements.\nCumulative Citation Recommendation (CCR). TREC introduced the CCR track in the Knowledge base acceleration track in 2012. For a stream of news and social media content and a target entity from a knowledge base (Wikipedia), the goal of the task is to generate a score for each document based on how pertinent it is to the input entity. Balog et al. [4, 3] propose approaches that find entity mentions in the document collection and rank them according to how central the entity is in the respective documents. This however is a filtering task for documents towards checking if they are relevant for a pre-defined set of entities. In contrast, in our task we aim at finding news citations as evidence for Wikipedia statements."}, {"heading": "10. CONCLUSIONS", "text": "In this work we define and attempt to solve the automatic news citation discovery problem for Wikipedia. We define two tasks \u2013 sentence categorization and the citation discovery \u2013 towards finding the correct news citation for a given Wikipedia statement. For the sentence categorization task, we learn a multi-class classifier to predict if a statement requires a news statement. For the news citation discovery problem, we first find the likely candidates by a retrieval model over a real-world news collection followed by a binary classification for the top-ranked candidates.\nWe find that statement categorization is a hard problem due to lack of context for the NLP-based features to perform well. However, the Wikipedia page and its type structure provide important cues towards accurate classification. On the other hand, we perform well on the citation discovery task with 67% precision (for top-categories) using the automated evaluation, which further improves to over 80% when crowd-sourced. This shows that we not only identify the correct ground truth articles present in Wikipedia, but in some cases our suggestions are a better fit compared to the sources in Wikipedia.\nAcknowledgments. This work is funded by the ERC Advanced Grant ALEXANDRIA (grant no. 339233)."}, {"heading": "11. REFERENCES", "text": "[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20.\n[2] M. Anderka, B. Stein, and N. Lipka. Predicting quality flaws in user-generated content: the case of wikipedia. In The 35th ACM SIGIR, Portland, USA, 2012.\n[3] K. Balog and H. Ramampiaro. Cumulative citation recommendation: classification vs. ranking. In 36th ACM SIGIR, Dublin, Ireland, 2013.\n[4] K. Balog, H. Ramampiaro, N. Takhirov, and K. N\u00f8rv\u030aag. Multi-step classification approaches to cumulative citation recommendation. In OAIR, Lisbon, Portugal, 2013.\n[5] D. Biber. Variation across speech and writing. Cambridge University Press, 1991.\n[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3, 2003.\n[7] L. Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001.\n[8] I. Dagan, D. Roth, M. Sammons, and F. M. Zanzotto. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1\u2013220, 2013.\n[9] B. Fetahu, A. Anand, and A. Anand. How much is wikipedia lagging behind news? In Proceedings of the ACM Web Science Conference, WebSci 2015, Oxford, United Kingdom, June 28 - July 1, 2015.\n[10] B. Fetahu, K. Markert, and A. Anand. Automated news suggestions for populating wikipedia entity pages. In 24th CIKM, Melbourne, Australia, 2015.\n[11] J. R. Finkel, T. Grenager, and C. D. Manning. Incorporating non-local information into information extraction systems by gibbs sampling. In 43rd ACL, 2005, USA.\n[12] H. Ford, S. Sen, D. R. Musicant, and N. Miller. Getting to the source: where does wikipedia get its information from? In 9th WikiSym, Hong Kong, China, 2013.\n[13] M. R. Henzinger, B. Chang, B. Milch, and S. Brin. Query-free news search. In 12th WWW, Budapest, Hungary, 2003.\n[14] R. J. Kate. A dependency-based word subsequence kernel. In 2008 EMNLP, Honolulu.\n[15] B. Luyt and D. Tan. Improving wikipedia\u2019s credibility: References and citations in a sample of history articles. JASIST, 61(4), 2010.\n[16] M. Mesgari, C. Okoli, M. Mehdi, F. A\u030a. Nielsen, and A. Lanama\u0308ki. \u201dthe sum of all human knowledge\u201d: A systematic review of scholarly research on the content of wikipedia. JASIST, 66(2), 2015.\n[17] R. Mihalcea and P. Tarau. Textrank: Bringing order into text. In 2004 EMNLP, Barcelona, Spain.\n[18] P. Petrenz and B. Webber. Stable classification of text genres. Computational Linguistics, 37(2):385\u2013393, 2011.\n[19] E. Pitler and K. W. Church. Using word-sense disambiguation methods to classify web queries by intent. In 2009 EMNLP, Singapore.\n[20] R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. K. Joshi, and B. L. Webber. The penn discourse treebank 2.0. In LREC. Citeseer, 2008.\n[21] C. Sauper and R. Barzilay. Automatically generating wikipedia articles: A structure-aware approach. In 47th ACL, 2009, Singapore.\n[22] S. Sharoff, Z. Wu, and K. Markert. The web library of babel: evaluating genre collections. In LREC. Citeseer, 2010.\n[23] J. Stro\u0308tgen and M. Gertz. Heideltime: High quality rule-based extraction and normalization of temporal expressions. In 5th SemEval, Stroudsburg, PA, USA, 2010.\n[24] K. Toutanova and C. D. Manning. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In SIGDAT, pages 63\u201370. ACL, 2000."}], "references": [{"title": "Predicting quality flaws in user-generated content: the case of wikipedia", "author": ["M. Anderka", "B. Stein", "N. Lipka"], "venue": "The 35th ACM SIGIR, Portland, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Cumulative citation recommendation: classification vs", "author": ["K. Balog", "H. Ramampiaro"], "venue": "ranking. In 36th ACM SIGIR, Dublin, Ireland", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "and K", "author": ["K. Balog", "H. Ramampiaro", "N. Takhirov"], "venue": "N\u00f8rv\u030aag. Multi-step classification approaches to cumulative citation recommendation. In OAIR, Lisbon, Portugal", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Variation across speech and writing", "author": ["D. Biber"], "venue": "Cambridge University Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 3", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Recognizing textual entailment: Models and applications", "author": ["I. Dagan", "D. Roth", "M. Sammons", "F.M. Zanzotto"], "venue": "Synthesis Lectures on Human Language Technologies, 6(4):1\u2013220", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "How much is wikipedia lagging behind news? In Proceedings of the ACM Web Science Conference", "author": ["B. Fetahu", "A. Anand", "A. Anand"], "venue": "WebSci 2015, Oxford, United Kingdom, June 28 - July 1", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Automated news suggestions for populating wikipedia entity pages", "author": ["B. Fetahu", "K. Markert", "A. Anand"], "venue": "24th CIKM, Melbourne, Australia", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C.D. Manning"], "venue": "43rd ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Getting to the source: where does wikipedia get its information from? In 9th WikiSym", "author": ["H. Ford", "S. Sen", "D.R. Musicant", "N. Miller"], "venue": "Hong Kong, China", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Query-free news search", "author": ["M.R. Henzinger", "B. Chang", "B. Milch", "S. Brin"], "venue": "12th WWW, Budapest, Hungary", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "A dependency-based word subsequence kernel", "author": ["R.J. Kate"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Improving wikipedia\u2019s credibility: References and citations in a sample of history articles", "author": ["B. Luyt", "D. Tan"], "venue": "JASIST, 61(4)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "the sum of all human knowledge\u201d: A systematic review of scholarly research on the content of wikipedia", "author": ["M. Mesgari", "C. Okoli", "M. Mehdi", "F.\u00c5. Nielsen", "A. Lanam\u00e4ki"], "venue": "JASIST, 66(2)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Textrank: Bringing order into text", "author": ["R. Mihalcea", "P. Tarau"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Stable classification of text genres", "author": ["P. Petrenz", "B. Webber"], "venue": "Computational Linguistics, 37(2):385\u2013393", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Using word-sense disambiguation methods to classify web queries by intent", "author": ["E. Pitler", "K.W. Church"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "and B", "author": ["R. Prasad", "N. Dinesh", "A. Lee", "E. Miltsakaki", "L. Robaldo", "A.K. Joshi"], "venue": "L. Webber. The penn discourse treebank 2.0. In LREC. Citeseer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatically generating wikipedia articles: A structure-aware approach", "author": ["C. Sauper", "R. Barzilay"], "venue": "47th ACL", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "The web library of babel: evaluating genre collections", "author": ["S. Sharoff", "Z. Wu", "K. Markert"], "venue": "LREC. Citeseer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Heideltime: High quality rule-based extraction and normalization of temporal expressions", "author": ["J. Str\u00f6tgen", "M. Gertz"], "venue": "5th SemEval, Stroudsburg, PA, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "SIGDAT, pages 63\u201370. ACL", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}], "referenceMentions": [{"referenceID": 14, "context": "A substantial number of reliability studies have compared Wikipedia against other reference works (such as the Encyclopedia Britannica or drug package information) or subjected them to expert review: The exhaustive survey in [16] concludes that the results of these studies have overall been favourable to Wikipedia when it comes to accuracy of facts, although some works (especially on medical articles) found errors of omission.", "startOffset": 225, "endOffset": 229}, {"referenceID": 7, "context": "For current and recent events, news citations are one of the most-used sources [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "\u2022 Sections \u2022 Anchors \u2022 Text \u2022 Categories typeOf\u2028 Politician Obama was born on August 4, 1961,[4] .", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": ") differ in their linguistic properties as the different functions they fulfill influence linguistic form [5].", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "We use features that were successful in automatic genre classification including structural features via parts-of-speech as well as lexical surface cues [18].", "startOffset": 153, "endOffset": 157}, {"referenceID": 17, "context": "We use discourse connectives to annotate the statements s with explicit discourse relations based on an approach proposed by Pitler and Church [19].", "startOffset": 143, "endOffset": 147}, {"referenceID": 18, "context": "The annotations belong to the categories {temporal, contingency, comparison, expansion}, following the Penn Discourse Treebank annotation [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "As surface lexical features have been shown to be efficient in genre recognition [22], we compute n\u2013gram (up to n=3) language models with Kneser-Ney smoothing (LM) from news articles Nt and compute the score \u03b8(s,Nt).", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "Similarly, we compute topic models using the LDA framework [6], where the score is the jaccard similarity between s and the topic terms from Nt.", "startOffset": 59, "endOffset": 62}, {"referenceID": 21, "context": "This proved to be more scalable than state-of-the-art extractors like HeidelTime [23] and Stanford\u2019s CoreNLP [11] module", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "This proved to be more scalable than state-of-the-art extractors like HeidelTime [23] and Stanford\u2019s CoreNLP [11] module", "startOffset": 109, "endOffset": 113}, {"referenceID": 5, "context": "Finally, we opt for Random Forests (RF) [7] as our supervised machine learning model.", "startOffset": 40, "endOffset": 43}, {"referenceID": 11, "context": "[13] propose several QC approaches that weigh query terms based on the tf\u2013idf score.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "We experimented with different QC approaches from [13] and their impact on finding news articles in N .", "startOffset": 50, "endOffset": 54}, {"referenceID": 6, "context": "The recognition of textual entailment has been the study of extensive research in the last 10 years; cf [8] for an overview.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "For this purpose, we consider the tree kernel similarity measure proposed in [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "We first compute the dependency parse trees of s and \u03c3 i using the Stanford tagger [24], and then compute the tree kernel, K(s, \u03c3 i ).", "startOffset": 83, "endOffset": 87}, {"referenceID": 12, "context": "We refer the reader to [14] for details.", "startOffset": 23, "endOffset": 27}, {"referenceID": 4, "context": "Similarly, we compute LDA topic models [6] for entity types, specifically from articles in Nt.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "We compute centrality of a sentence in ni through the TextRank approach introduced in [17].", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "We therefore measure relative entity frequency of e in ni based on an approach described in [10].", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": "We learn the FC models as supervised binary classification models using random forests RF[7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 10, "context": "[12] analyze the citation behavior of Wikipedia editors with respect to their adherence to the citation guidelines.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Luyt and Tan [15] analyze a subset of history entity pages and show that citations are biased towards a specific group of sources.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "[12, 15] emphasize the importance of citations in Wikipedia as a means to ensure the quality of entity pages.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[12, 15] emphasize the importance of citations in Wikipedia as a means to ensure the quality of entity pages.", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "[2] propose an approach to predict quality flaws in Wikipedia pages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Sauper and Barzilay [21] propose an approach to automatically generate complete entity pages for a specific entity type.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "in [10] proposed an approach for suggesting news articles for a Wikipedia entity and entity section.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "This work differs from [21, 10] as we do not add content or suggest news articles to a complete section in an entity page, but rather provide citations to already existing statements.", "startOffset": 23, "endOffset": 31}, {"referenceID": 8, "context": "This work differs from [21, 10] as we do not add content or suggest news articles to a complete section in an entity page, but rather provide citations to already existing statements.", "startOffset": 23, "endOffset": 31}, {"referenceID": 2, "context": "[4, 3] propose approaches that find entity mentions in the document collection and rank them according to how central the entity is in the respective documents.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[4, 3] propose approaches that find entity mentions in the document collection and rank them according to how central the entity is in the respective documents.", "startOffset": 0, "endOffset": 6}], "year": 2017, "abstractText": "An important editing policy in Wikipedia is to provide citations for added statements in Wikipedia pages, where statements can be arbitrary pieces of text, ranging from a sentence to a paragraph. In many cases citations are either outdated or missing altogether. In this work we address the problem of finding and updating news citations for statements in entity pages. We propose a two-stage supervised approach for this problem. In the first step, we construct a classifier to find out whether statements need a news citation or other kinds of citations (web, book, journal, etc.). In the second step, we develop a news citation algorithm for Wikipedia statements, which recommends appropriate citations from a given news collection. Apart from IR techniques that use the statement to query the news collection, we also formalize three properties of an appropriate citation, namely: (i) the citation should entail the Wikipedia statement, (ii) the statement should be central to the citation, and (iii) the citation should be from an authoritative source. We perform an extensive evaluation of both steps, using 20 million articles from a real-world news collection. Our results are quite promising, and show that we can perform this task with high precision and at scale.", "creator": "LaTeX with hyperref package"}}}