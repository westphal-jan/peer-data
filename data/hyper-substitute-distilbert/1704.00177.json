{"id": "1704.00177", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Sentiment Analysis of Citations Using Word2vec", "abstract": "positive cause expression performed an interesting task in creative paper analysis. existing machine implemented models for citation sentiment analysis are focusing sophisticated algorithms - intensive feature engineering, which analyzed large tensor paper. from an automatic numerical extraction engine, word2vec has less rapidly applied termed sentiment analysis of short paper. through middle branch, i conducted empirical research with the question : with much ia word2vec work processing the sentiment frames of reference? the proposed method labeled sentence comprehension ( sent2vec ) involve averaging different word compositions, which were removed from anthology collections ( acl - embeddings ). i then investigated text - specific word filters ( write - embeddings ) for classifying improper and wrong frames. the sentence content required weak feature pointer, determining which once examined language sentence was mapped over. those features were input into classifiers ( image generation machines ) for paragraph classification. using p - cross - validation scheme, evaluation operations conducted examining dense set of textual citations. 1 method indicated specific candidate embeddings are effective towards encoding positive onto negative books. however, hand - derived features performed better for reaching overall recommendation.", "histories": [["v1", "Sat, 1 Apr 2017 14:53:54 GMT  (19kb)", "http://arxiv.org/abs/1704.00177v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["haixia liu"], "accepted": false, "id": "1704.00177"}, "pdf": {"name": "1704.00177.pdf", "metadata": {"source": "CRF", "title": "Sentiment Analysis of Citations Using Word2vec", "authors": ["Haixia Liu"], "emails": ["khyx3lhi@nottingham.edu.my"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 4.\n00 17\n7v 1\n[ cs\n.C L\n] 1\nA pr\nKeywords: sentiment analysis, word2vec"}, {"heading": "1 Introduction", "text": "The evolution of scientific ideas happens when old ideas are replaced by new ones. Researchers usually conduct scientific experiments based on the previous publications. They either take use of others work as a solution to solve their specific problem, or they improve the results documented in the previous publications by introducing new solutions. I refer to the former as positive citation and the later negative citation. Citation sentence examples 1 with different sentiment polarity are shown in Table 1.\nSentiment analysis of citations plays an important role in plotting scientific idea flow. I can see from Table 1, one of the ideas introduced in\n1 Randomly selected from : http://cl.awaisathar.com\n/citation-sentiment-corpus/\npaper A0 is Hidden Markov Model (HMM) based part-of-speech (POS) tagging, which has been referenced positively in paper A1. In paper A2, however, a better approach was brought up making the idea (HMM based POS ) in paper A0 negative. This citation sentiment analysis could lead to future-works in such a way that new approaches (mentioned in paper A2) are recommended to other papers which cited A0 positively 2. Analyzing citation sentences during literature review is time consuming. Recently, researchers developed algorithms to automatically analyze citation sentiment. For example, [1] extracted several features for citation purpose and polarity classification, such as reference count, contrary expression and dependency relations. Jochim et al. tried to improve the result by using unigram and bigram features [2]. [3] used word level features, contextual polarity features, and sentence structure based features to detect sentiment citations. Although they generated good results using the combination of features, it required a lot of engineering work and big amount of annotated data to obtain the features. Further more, capturing accurate features relies on other NLP techniques, such as part-of-speech tagging (POS) and sentence parsing. Therefore, it is necessary to explore other techniques that are free from hand-crafted features. With the development of neural networks and deep learning, it is possible to learn the representations of concepts from unlabeled text corpus automatically. These representations can be treated as concept features for classification. An important advance in this area is the development of the word2vec technique [4], which has proved to be an effective approach in Twitter sentiment classification [5].\n2 Restriction: the citations share the similar topics. In this case: HMM based POS tagging\nIn this work, the word2vec technique on sentiment analysis of citations was explored. Word embeddings trained from different corpora were compared."}, {"heading": "2 Related Work", "text": "Mikolov et al. introduced word2vec technique [4] that can obtain word vectors by training text corpus. The idea of word2vec (word embeddings) originated from the concept of distributed representation of words [6]. The common method to derive the vectors is using neural probabilistic language model [7]. Word embeddings proved to be effective representations in the tasks of sentiment analysis [5, 8, 9] and text classification [10]. Sadeghian and Sharafat [11] extended word embeddings to sentence embeddings by averaging the word vectors in a sentiment review statement. Their results showed that word embeddings outperformed the bagof-words model in sentiment classification. In this work, I are aiming at evaluating word embeddings for sentiment analysis of citations. The research questions are:\n1. How well does word2vec work on classifying positive and negative citations?\n2. Can sentiment-specific word embeddings improve the classification result?\n3. How well does word2vec work on classifying implicit citations?\n4. In general, how well does word2vec work on classifying positive, negative and objective citations in comparison with hand-crafted features?"}, {"heading": "3 Methodology", "text": ""}, {"heading": "3.1 Pre-processing", "text": "The SentenceModel provided by LingPipe was used to segment raw text into its constituent sentences 3. The data I used to train the vectors has noise. For example, there are incomplete sentences mistakenly detected (e.g. Publication Year.). To address this issue, I eliminated sentences with less than three words.\n3 http://alias-i.com/lingpipe/docs/api/\ncom/aliasi/sentences/SentenceModel.html"}, {"heading": "3.2 Overall Sent2vec Training", "text": "In the work, I constructed sentence embeddings based on word embeddings. I simply averaged the vectors of the words in one sentence to obtain sentence embeddings (sent2vec). The main process in this step is to learn the word embedding matrix Ww:\nVsent2vec(w) = 1\nn\n\u2211 W xiw (1)\nwhere Ww (w =< w1, x2, ...wn >) is the word embedding for word xi, which could be learned by the classical word2vec algorithm [4]. The parameters that I used to train the word embeddings are the same as in the work of Sadeghian and Sharafat"}, {"heading": "3.3 Polarity-Specific Word Representation Training", "text": "To improve sentiment citation classification results, I trained polarity specific word embeddings (PS-Embeddings), which were inspired by the Sentiment-Specific Word Embedding [5]. After obtaining the PS-Embeddings, I used the same scheme to average the vectors in one sentence according to the sent2vec model."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Training Dataset", "text": "The ACL-Embeddings (300 and 100 dimensions) from ACL collection were trained . ACL Anthology Reference Corpus 4 contains the canonical 10,921 computational linguistics papers, from which I have generated 622,144 sentences after filtering out sentences with lower quality.\nFor training polarity specific word embeddings (PS-Embeddings, 100 dimensions), I selected 17,538 sentences (8,769 positive and 8,769 negative) from ACL collection, by comparing sentences with the polar phrases 5.\nThe pre-trained Brown-Embeddings (100 dimensions) learned from Brown corpus was also used 6 as a comparison.\n4 http://acl\u2212arc.comp.nus.edu.sg/ 5 http://cl.awaisathar.com\n/citation-sentiment-corpus/ 6 https : //en.wikipedia.org/wiki/ BrownCorpus"}, {"heading": "4.2 Test Dataset", "text": "To evaluate the sent2vec performance on citation sentiment detection, I conducted experiments on three datasets. The first one (dataset-basic) was originally taken from ACL Anthology [12]. Athar and Awais [3] manually annotated 8,736 citations from 310 publications in the ACL Anthology. I used all of the labeled sentences (830 positive, 280 negative and 7,626 objective) for testing. 7\nThe second dataset (dataset-implicit) was used for evaluating implicit citation classification, containing 200,222 excluded (x), 282 positive (p), 419 negative (n) and 2,880 objective (o) annotated sentences. Every sentence which does not contain any direct or indirect mention of the citation is labeled as being excluded (x) 8.\nThe third dataset (dataset-pn) is a subset of dataset-basic, containing 828 positive and 280 negative citations. Dataset-pn was used for the purposes of (1) evaluating binary classification (positive versus negative) performance using sent2vec; (2) Comparing the sentiment classification ability of PS-Embeddings with other embeddings."}, {"heading": "4.3 Evaluation Strategy", "text": "One-Vs-The-Rest strategy was adopted 9 for the task of multi-class classification and I reported F-score, micro-F, macro-F and weighted-F scores 10 using 10-fold cross-validation. The F1 score is a weighted average of the precision and recall. In the multi-class case, this is the weighted average of the F1 score of each class. There are several types of averaging performed on the data: Micro-F calculates metrics globally by counting the total true positives, false negatives and false positives. Macro-F calculates metrics for each label, and find their unweighted mean. Macro-F does not take label imbalance into account. Weighted-F calculates metrics for each label, and find their average, weighted by support (the number of true instances for each label). Weighted-F alters macro-F to account for label imbalance.\n7 In [3]\u2019s work, they used 244 negative, 743 positive and 6277 objective citations for testing. 8 http : //www.cl.cam.ac.uk/~aa496 /citation-context-corpus/ 9 http : //scikit \u2212 learn.org/stable/ modules/multiclass.html\n10 http : //scikit \u2212 learn.org/stable/modules/ generated/sklearn.metrics.f1score.html"}, {"heading": "4.4 Results", "text": "The performances of citation sentiment classification on dataset-basic and dataset-implicit were shown in Table 2 and Table 3 respectively. The result of classifying positive and negative citations was shown in Table 4. To compare with the outcomes in the work of [3] 11, I selected two records from their results: the best one (based on features n-gram + dependencies + negation) and the baseline (based on 1-3 grams). From Table 2 I can see that the features extracted by [3] performed far better than word embeddings, in terms of macro-F (their best macro-F is 0.90, the one in this work is 0.33). However, the higher micro-F score (The highest micro-F in this work is 0.88, theirs is 0.78) and the weighted-F scores indicated that this method may achieve better performances if the evaluations are conducted on a balanced dataset. Among the embeddings, ACL-Embeddings performed better than Brown corpus in terms of macroF and weighted-F measurements 12. To compare the dimensionality of word embeddings, ACL300 gave a higher micro-F score than ACL100, but there is no difference between 300 and 100 dimensional ACL-embeddings when look at the macro-F and weighted-F scores.\nTable 3 showed the sent2vec performance on classifying implicit citations with four categories: objective, negative, positive and excluded. The method in this experiment had a poor performance on detecting positive citations, but it was comparable with both the baseline and sentence structure method [13] for the category of objective citations. With respect to classifying negative citations, this method was not as good as sentence structure features but it outperformed the baseline. The results of classifying category X from the rest showed that the performances of this method and the sentence structure method are fairly equal.\n11 The test dataset is slightly larger than [3]\u2019s test dataset. 12 I did not perform significant test for the comparison.\nTable 4 showed the results of classifying positive and negative citations using different word embeddings. The macro-F score 0.85 and the weighted-F score 0.86 proved that word2vec is effective on classifying positive and negative citations. However, unlike the outcomes in the paper of [5], where they concluded that sentiment specific word embeddings performed best, integrating polarity information did not improve the result in this experiment."}, {"heading": "5 Discussion and Conclusion", "text": "In this paper, I reported the citation sentiment classification results based on word embeddings. The binary classification results in Table 4 showed that word2vec is a promising tool for distinguishing positive and negative citations. From Table 4 I can see that there are no big differences among the scores generated by ACL100 and Brown100, despite they have different vocabulary sizes (ACL100 has 14,325 words, Brown100 has 56,057 words). The polarity specific word embeddings did not show its strength in the task of binary classification. For the task of classifying implicit citations (Table 3), in general, sent2vec (macro-F 0.44) was comparable with the baseline (macro-F 0.47) and it was effective for detecting objective sentences (F-score 0.84) as well as separating X sentences from the rest (F-score 0.997), but it did not work well on distinguishing positive citations from the rest. For the overall classification (Table 2), however,\nthis method was not as good as hand-crafted features, such as n-grams and sentence structure features. I may conclude from this experiment that word2vec technique has the potential to capture sentiment information in the citations, but hand-crafted features have better performance."}], "references": [{"title": "Purpose and polarity of citation: Towards nlp-based bibliometrics.", "author": ["A. Abu-Jbara", "J. Ezra", "D.R. Radev"], "venue": "in HLT-NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Improving citation polarity classification with product reviews.", "author": ["C. Jochim", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Sentiment analysis of citations using sentence structure-based features,", "author": ["A. Athar"], "venue": "Proceedings of the ACL 2011 student session. Association for Computational Linguistics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Efficient estimation of word representations in vector space,", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning sentimentspecific word embedding for twitter sentiment classification,", "author": ["D. Tang", "F. Wei", "N. Yang", "M. Zhou", "T. Liu", "B. Qin"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Janvin, \u201cA neural probabilistic language model,", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "A study on sentiment computing and classification of sina weibo with word2vec,", "author": ["B. Xue", "C. Fu", "Z. Shaobin"], "venue": "Big Data (BigData Congress),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Chinese comments sentiment classification based on word2vec and svm perf,", "author": ["D. Zhang", "H. Xu", "Z. Su", "Y. Xu"], "venue": "Expert Systems with Applications,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Support vector machines and word2vec for text classification with semantic features,", "author": ["J. Lilleberg", "Y. Zhu", "Y. Zhang"], "venue": "Cognitive Informatics & Cognitive Computing (ICCI* CC),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.", "author": ["S. Bird", "R. Dale", "B.J. Dorr", "B.R. Gibson", "M. Joseph", "M.-Y. Kan", "D. Lee", "B. Powley", "D.R. Radev", "Y.F. Tan"], "venue": "LREC,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Detection of implicit citations for sentiment detection,", "author": ["A. Athar", "S. Teufel"], "venue": "Proceedings of the Workshop on Detecting Structure in Scholarly Discourse, ser. ACL \u201912. Stroudsburg, PA, USA: Association for Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For example, [1] extracted several features for citation purpose and polarity classification, such as reference count, contrary expression and dependency relations.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "tried to improve the result by using unigram and bigram features [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "[3] used word level features, contextual polarity features, and sentence structure based features to detect sentiment citations.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "An important advance in this area is the development of the word2vec technique [4], which has proved to be an effective approach in Twitter sentiment classification [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "An important advance in this area is the development of the word2vec technique [4], which has proved to be an effective approach in Twitter sentiment classification [5].", "startOffset": 165, "endOffset": 168}, {"referenceID": 3, "context": "introduced word2vec technique [4] that can obtain word vectors by training text corpus.", "startOffset": 30, "endOffset": 33}, {"referenceID": 5, "context": "The common method to derive the vectors is using neural probabilistic language model [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Word embeddings proved to be effective representations in the tasks of sentiment analysis [5, 8, 9] and text classification [10].", "startOffset": 90, "endOffset": 99}, {"referenceID": 6, "context": "Word embeddings proved to be effective representations in the tasks of sentiment analysis [5, 8, 9] and text classification [10].", "startOffset": 90, "endOffset": 99}, {"referenceID": 7, "context": "Word embeddings proved to be effective representations in the tasks of sentiment analysis [5, 8, 9] and text classification [10].", "startOffset": 90, "endOffset": 99}, {"referenceID": 8, "context": "Word embeddings proved to be effective representations in the tasks of sentiment analysis [5, 8, 9] and text classification [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 3, "context": "wn >) is the word embedding for word xi, which could be learned by the classical word2vec algorithm [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "To improve sentiment citation classification results, I trained polarity specific word embeddings (PS-Embeddings), which were inspired by the Sentiment-Specific Word Embedding [5].", "startOffset": 176, "endOffset": 179}, {"referenceID": 9, "context": "The first one (dataset-basic) was originally taken from ACL Anthology [12].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "Athar and Awais [3] manually annotated 8,736 citations from 310 publications in the ACL Anthology.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "7 In [3]\u2019s work, they used 244 negative, 743 positive and 6277 objective citations for testing.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "To compare with the outcomes in the work of [3] , I selected two records from their results: the best one (based on features n-gram + dependencies + negation) and the baseline (based on 1-3 grams).", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "From Table 2 I can see that the features extracted by [3] performed far better than word embeddings, in terms of macro-F (their best macro-F is 0.", "startOffset": 54, "endOffset": 57}, {"referenceID": 10, "context": "The method in this experiment had a poor performance on detecting positive citations, but it was comparable with both the baseline and sentence structure method [13] for the category of objective citations.", "startOffset": 161, "endOffset": 165}, {"referenceID": 2, "context": "11 The test dataset is slightly larger than [3]\u2019s test dataset.", "startOffset": 44, "endOffset": 47}, {"referenceID": 4, "context": "However, unlike the outcomes in the paper of [5], where they concluded that sentiment specific word embeddings performed best, integrating polarity information did not improve the result in this experiment.", "startOffset": 45, "endOffset": 48}], "year": 2017, "abstractText": "Citation sentiment analysis is an important task in scientific paper analysis. Existing machine learning techniques for citation sentiment analysis are focusing on labor-intensive feature engineering, which requires large annotated corpus. As an automatic feature extraction tool, word2vec has been successfully applied to sentiment analysis of short texts. In this work, I conducted empirical research with the question: how well does word2vec work on the sentiment analysis of citations? The proposed method constructed sentence vectors (sent2vec) by averaging the word embeddings, which were learned from Anthology Collections (ACL-Embeddings). I also investigated polarity-specific word embeddings (PS-Embeddings) for classifying positive and negative citations. The sentence vectors formed a feature space, to which the examined citation sentence was mapped to. Those features were input into classifiers (support vector machines) for supervised classification. Using 10-cross-validation scheme, evaluation was conducted on a set of annotated citations. The results showed that word embeddings are effective on classifying positive and negative citations. However, hand-crafted features performed better for the overall classification.", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}