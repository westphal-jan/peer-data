{"id": "1704.00200", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Apr-2017", "title": "Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal domain-aware conversations", "abstract": "owing to the necessity achieving deep interactive processes for objects such as q / hmm and language - based dialog, there are successively increasing array potential research agents in varied domains providing cross retail, travel, entertainment, intelligence. technologies can appear on multimodal experimenting with processors employing both 3d visual vision composing a dialog seamlessly. however, deep learning research is under strategy has rendered limited primarily due to current lack of availability like per - parameter, open conversation computing. to overcome knowledge bottleneck, extending this order we attempt the creation of split - modal, image - aware conversations, collectively propose the mmd benchmark dataset to - self response task. this dataset automatically gathered by working in close coordination with large array of domain experts in the print arena whereas consists entirely over 150k digital presentations linking shoppers and sales editors. with editorial help, we propose 5 new ms - tasks conducting competitive conversations along with their evaluation skills. we simultaneously propose 24 successful multi - modal interactive learning models in industry encode - attend - generation steps at demonstrate their implementation on two of s sub - tasks, namely text response generation involves best image encoding selection. these experiments aiming to establish competitive input numbers through reflect new directions of practice for each of ourselves sub - tasks.", "histories": [["v1", "Sat, 1 Apr 2017 17:05:35 GMT  (9353kb,D)", "http://arxiv.org/abs/1704.00200v1", null], ["v2", "Tue, 9 May 2017 07:50:08 GMT  (9353kb,D)", "http://arxiv.org/abs/1704.00200v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amrita saha", "mitesh khapra", "karthik sankaranarayanan"], "accepted": false, "id": "1704.00200"}, "pdf": {"name": "1704.00200.pdf", "metadata": {"source": "CRF", "title": "Multimodal Dialogs (MMD): A large-scale dataset for studying multimodal domain-aware conversations", "authors": ["Amrita Saha", "Mitesh Khapra", "Karthik Sankaranarayanan"], "emails": ["amrsaha4@in.ibm.com", "miteshk@cse.iitm.ac.in", "kartsank@in.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "In the area of conversation systems, the demand for autonomous AI agents that can assist humans with complex activities requiring a long series of interactions (such as shopping on the web or mobile) has grown by leaps and bounds in several domains\nsuch as retail, travel, entertainment, etc. which have witnessed an explosion of multimodal content on the internet. Such AI agents need to demonstrate both of the following capabilities: (i) the ability to converse with humans seamlessly using multimodal content, (ii) ability to exploit domainspecific knowledge.\nThe recent progress with deep learning techniques for other problems at the intersection of NLP and Computer Vision such as image captioning (Vinyals et al., 2015; Xu et al., 2015), video description (Yu et al., 2016), image question answering (Antol et al., 2015), video question answering (Zeng et al., 2016; Maharaj et al., 2016), is owed in large part to the availability of large-scale open datasets for their respective tasks. However, the primary hindrance for deep learning research towards multimodal conversation systems has been the lack of large-scale open datasets that exemplify both of the challenges mentioned above. Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research. While such multimodal, humanto-human conversation transcripts (for example, between shoppers and retail salespersons) might be available in industry settings, they are both limited in scale and are proprietary, hindering open deep learning research.\nIn this paper, we attempt to alleviate both of these challenges by developing a large-scale multimodal conversational dataset in the retail domain that embodies the required generic capabilities for such autonomous agents. Since the actual transcripts are both limited and proprietary, we conducted a series of interviews with a large number of retail salespersons in the fashion domain and devel-\nar X\niv :1\n70 4.\n00 20\n0v 1\n[ cs\n.C L\n] 1\nA pr\n2 01\n7\noped the dataset interactively with them in a semiautomatic manner at a large scale as follows. The domain experts described in detail various phases of the sales process which were materialized into over 80 states for a conversational agent. Each such state had a specific intent (such as a shopper wanting to see more items similar to a specific one identified, shopper asking for a style tip or about the latest trends being endorsed by celebrities, etc). Corresponding to each such intent, a collection of representative utterance patterns involving both text and images were designed by us along with the experts. Each such state exhibited multi-modality (i.e. involving text and images) in both the utterance/response as well as the dialog context. The domain experts described various possible sales flows where customer went from one state to another, which were captured by transitions of an expert model automata between these states. The experts then inspected the outputs of controlled runs of the automata, provided feedback, which was thereafter employed to further refine the expert automata. Proceeding in this manually-intensive and iterative manner under the supervision of domain experts, we produced a large-scale dataset of over 150K multimodal dialogs capturing a wide variety of conversational sessions exhibiting the challenges described earlier. Note that such a data collection could not have been achieved with typical crowdsourcing platforms such as Amazon Mechanical Turk using ordinary crowd workers since it was necessary to be performed under the supervision of fashion sales experts exhibiting domain-specific understanding.\nAn example of a typical multimodal conversation involving both text and images, and exhibiting domain knowledge is shown in Figure 16. Notice that the response generated by the system can either be text (for example, see the 3rd turn inside the highlighted portion in red) or a set of images (as seen in 1st turn there). Similarly, at every point, the context available to the system is multimodal which the system has to process to generate a coherent response. For example, to give a relevant response in Turn 2 inside the box the agent needs to first pay attention to the words \u201cespadrille\u201d, \u201csilver\u201d and the sentiment which are mentioned in the text and then understand the style (\u201cstrapped ankle\u201d) from the image to fetch images for \u201csilver espadrille with strapped ankle\u201d in response to the given context.\nThe body of work most relevant to ours is Visual QA(Antol et al., 2015) involving a single question\nand response, and the work of (de Vries et al., 2016; Mostafazadeh et al., 2017; Das et al., 2016) involving a sequence of QA pairs with an image forming a dialog. There are a few key differences between these datasets and our work as demonstrated by the example above. First, in their case, all questions in a sequence pertain only to a single image. Secondly, their responses are always textual. However, as is the case with natural conversations amongst humans, in our work, (i) there could be multiple images providing context, (ii) these context images could change across turns during the course of the conversation, and (iii) the response at each step can be text or image(s) or both.\nFinally, in this paper, we propose a baseline framework to model the agent\u2019s responses in such multimodal conversations. In particular, we propose hierarchical attention-based models for the textual and image response as two separate tasks and empirically estimate the feasibility of these tasks. We also discuss limitations that open new directions for research for these and multiple other tasks enabled by this new dataset. The main contributions of this work can be summarized as follows:\n\u2022 We introduce the task of Multimodal Conversation which is significantly distinct from the sequential Visual QA driven dialog tasks mentioned before. \u2022 We introduce a large dataset for this task and de-\nfine several research tasks that can be evaluated using this dataset \u2022 We propose baseline multimodal encoder de-\ncoder models for two such tasks and define appropriate metrics for evaluating these tasks"}, {"heading": "2 The Multimodal Dialogs (MMD) Dataset", "text": "As mentioned in the previous section, a key contribution of this paper is a large-scale dataset of 2-party dialogs that seamlessly employ multimodal data in their utterances and context and also demonstrate domain-specific knowledge in their series of interactions. Toward this goal, in this section, we first describe the methodology employed for collecting this dataset and then explain in detail the various sub-tasks exhibited by the dataset that open up new research problems."}, {"heading": "2.1 Data Collection Methodology", "text": "The data collection process was performed by us in close coordination with a team of 20 fashion experts and primarily consisted of two steps, (i)\ncuration and representation of a large-scale domain knowledge, and (ii) developing a large collection of multimodal conversations, each consisting of a series of interactions employing this knowledge. We next proceed to describe these 2 steps in detail."}, {"heading": "2.1.1 Domain Knowledge Curation", "text": "Through our series of interviews with the domain experts, we observed that a lot of the complexity in a natural conversation in this domain comes from the background knowledge that both the expert agent and the shopper employ in their conversation. The expert\u2019s domain knowledge is multitude in nature, varying from knowledge about which attire goes well with which accessory, to which celebrity is presently endorsing which kind of fashion items, or what kind of look is better suited for which occasion. Therefore, the first step in our data collection process was to curate this domain knowledge from unstructured multimodal content on the web at scale and represent them in a machine consumable manner. This process involved a series of steps as enumerated below: 1. Crawling over 1 Million fashion items from the web along with their available semi/un-structured information and associated image(s) 2. Parsing different types of domain knowledge from the free-text information, and curating them in a structured form after a round of manual inspection by domain experts. 2a. Creating a hand-crafted taxonomy of the different types of fashion items. For example, man > apparel > layer-2-lower-body > trouser > formaltrousers, dressed pants, suit pants i.e. formaltrousers is synonymous to dressed pants or suit pants and is a type of trouser which is again a type of layer-2-lower body apparel. Each entry in the taxonomy has a synonym-set (called \u201csynset\u201d). 282 such fashion \u201csynsets\u201d were collected from domain experts for men and 434 for women. 2b. Identifying the set of fashion attributes relevant (especially for the purpose of shopping) to each of the fashion synsets. Overall 52 such attributes (like color, pattern, style, price, wash-care information) were identified by domain experts, where 45 of them are visual attributes and remaining are metadata attributed about the synset (e.g. wash-care information, price, seller ranking). 2c. Seeding the lexicon for each of these attributes with a set of realistic values provided by the domain experts\n3. Parsing the semi-structured catalog data into a single unified structured form of the tuple <fashion synset, {fashion attribute:{attribute values}}> where {} denotes a set. 4. Constructing a distribution of attributes and their values for each of the fashion synsets, from the structured catalog data curated in step 3 and filtering them through a close manual inspection by the domain experts. 5. From the unstructured product description in the catalog, spotting and extracting style-tip information (e.g. black trousers go well with white shirt) 6. Creating fashion profiles for celebrities based on the type of clothes and accessories worn or endorsed by them. Since the profile information for real celebrities was proprietary, we generated profiles of imaginary celebrities by simulating a distribution of fashion synsets that each of these celebrities endorse, and a further distribution of fashion attributes preferred by these celebrities for each of these synsets. Note that doing so does not affect the generality of the dataset technically. Statistics about the final domain knowledge curated using this semi-automated methodology are tabulated in Table 1"}, {"heading": "2.1.2 Gathering multimodal dialogs", "text": "During the interviews, the domain experts described in detail various phases of the sales process. For example, a dialog between the sales agent and a shopper who visits an e-commerce website with the objective of either buying or browsing one or more fashion items begins by the shopper providing their shopping requirements to the agent. The agent then browses the corpus and comes back with a mul-\ntimodal response (i.e. with a set of images that satisfy the shopper\u2019s constraints and/or some associated text). Now, using this response the shopper provides feedback or modifies their requirements. Through this iterative response and feedback loop the shopper continues to explore their items of interest, adding chosen items to their shopping cart. The session continues until they either choose to exit without a purchase or culminates with the shopper buying one or more items. Note that at various steps during such a conversation, the response at the current step of the dialog is based on inference drawn from an aggregate of images and text in the unstructured dialog context as well as a structured background domain knowledge (which is again multimodal in nature).\nThe domain experts described each of these various possible states involved in the conversations, a subset of which are shown in Table 2. This was mapped to an expert model automata which consisted of a total of 21 state types for the shopper covering 83 states. Each such state had a specific intent and corresponding to them, a collection of representative utterance patterns involving both text and images were designed by us along with the experts. Each such state would exhibit the following 3 features: (a) multimodality of utterance/response: shopper\u2019s utterance and the agent\u2019s response could involve either text or image or both, (b) multimodality of context: shopper\u2019s utterance would employ the context of the conversation which would include both the text history and a number of images and (c) combination of the structured domain knowledge and the unstructured dialog context, both being multimodal in nature\nThe domain experts then provided a large number of possible flows for a sales process in terms of customers proceeding from one state to another. These transitions were captured by the automata between these states with expert designed transition probabilities. The domain experts then inspected the outputs of small runs of the automata and provided feedback. This feedback was then incorporated to further refine the expert automata whose runs were again inspected by the experts. This iterative process was manually-intensive, and required closed coordination with the domain experts. Following this process, we produced a large-scale dataset of over 150K multimodal dialogs."}, {"heading": "2.1.3 Qualitative Survey", "text": "In order to ensure that the dataset is representative and not biased by the specific fashion experts\ninterviewed, we conducted a survey of the dataset by involving a different set of 16 expert participants from the fashion domain. These experts were asked to evaluate both whether the text portions of the dialog are natural sounding and meaningful and whether the images in the dialog are appropriate. The survey was conducted with a randomly sampled set of 760 dialog sessions and the experts were asked to provide an overall rating between 1 to 5 (with 5 being most realistic).\nTwo types of errors were documented: (i) minor error being conversational mistakes (e.g. grammatical and phrasing error), (ii) severe error being logical mistakes (e.g. deductions errors in generating the image or text response, incorrect understanding of the shopper\u2019s question, wrong fashion recommendation, etc.). As the survey results in Table 5 show, the average rating obtained was 4 out of 5, thereby implying that on average there were only a few conversational errors in a typical sized dialog session consisting of about 40 utterances."}, {"heading": "2.1.4 Discussion", "text": "Note that the first version of the dataset being published will not contain the background knowledge mined from this domain as it requires some\nadditional standardization, but this will be included in a subsequent release.\nAlso note that for evaluation purposes of the image response in selection/ranking mode (as described in the next section), a system would also need negative training examples. To support this, we also create a negative automata, which simulates a dialog between a shopper and an agent who always provides incorrect image responses in a dialog context. These incorrect examples alongside the correct image examples can be employed for training models for such evaluation settings."}, {"heading": "2.2 Tasks", "text": "The proposed MMD dataset consists of multimodal, domain-aware conversations between 2 agents, and hence can be used for evaluating a wide variety of tasks. We describe each of these tasks and explain the technical challenges involved: 1. Text Response: Given a context of k turns the task here is to generate the next text response. 2. Image Response: Given a context of k turns the task here is to output the most relevant image(s). There are 2 typical approaches to achieve this: 2.1 Image Retrieval: Given a context of k turns and a database of images, retrieve and rank n images based on their relevance to the given context. 2.2 Image Generation: Given a context of k turns, generate the most relevant image (typically performed using generative models e.g. contextual GANs(Reed et al., 2016; Goodfellow et al., 2014)). We propose both tasks since the evaluation criteria for each approach is quite different. 3. Employing Domain-knowledge: This is essentially performing tasks (1) and (2) of text and image response generation using both the unstructured dialog context along with the structured domain knowledge. We propose this as a separate task to evaluate the impact of domain-knowledge. 4. User Modeling: Another important aspect of conversations is to study user behavior. Since different users exhibit significantly varying shopping behaviors such as buying preferences, speed of decision making, etc. we propose a task that explicitly models the shopper since it impacts the agent\u2019s most appropriate response at each step. Setup: In this work, we focus on tasks (1) and (2.1). More specifically, the conversation system should be able to perform the following: (i) by utilizing the context, decide the modality of the response, i.e., whether the response should be a text or an image (ii) generate a text response if\nrequired, or (iii) if an image response is required then rank all the images in the catalog based on their contextual relevance as opposed to ranking a given set of m (m < catalog size) images.\nIn this work, we make two simplifications to this setup: (a) We evaluate the text response and image response task separately, which means that the system does not need to decide the modality of the response, and (b) instead of retrieving and ranking all of the images in the catalog/database, the system needs to rank only a given smaller subset of m images, which contain the correct image(s) in addition to a few incorrect ones. This simplified evaluation protocol of \u201cselecting/ranking\u201d the best response was proposed for the Ubuntu Dialog Corpus (Lowe et al., 2015) and helps provide more control on the experimental setup and evaluate individual parts of the system in a more thorough manner.\nGiven this setup, in the next section we propose baseline models for these tasks based on the encode-attend-decode paradigm."}, {"heading": "3 Models", "text": "To empirically estimate the feasibility of the two tasks described earlier we implement one baseline method (and some variations thereof) for each task. These methods are based on the popular hierarchical encode-attend-decode paradigm (Serban et al., 2016a) typically used for (text) conversation systems. We split the description below into two parts (i) Multimodal encoder which is common for the\ntwo tasks (ii) Multimodal decoder which is different depending on whether we need to generate a text response or predict an image response."}, {"heading": "3.1 Mutimodal encoder", "text": "As mentioned earlier, for both the tasks, the context contains k utterances. Each utterance in the context could either be (i) a text only utterance or (ii) an image only utterance or (iii) a multimodal utterance containing both text and images (as shown in Figure 2). We use a multimodal hierarchical encoder for computing the encoder representation in each of these cases as described below. (a) Text only utterance: Every text utterance in the context is encoded using a recurrent neural network with GRU (Chung et al., 2014) cells in a process similar to the utterance level encoder described in (Serban et al., 2016a). This is the level 1 encoder in the hierarchical encoder. (b) Image only utterance: In the current setup, each image or multimodal utterance can contain nmax number of images (in the example shown in Figure 2 nmax is 3). We encode each image using a 4096 dimensional representation obtained from the FC7 layer of a VGGNet-16 (Simonyan and Zisserman, 2014) convolutional neural network. The representation of the utterance is simply the concatenation of the individual images. This is also a part of the first level in the hierarchical encoder. (c) Multimodal utterance: The text portion of the multimodal utterance is encoded using the same GRU cells as used for encoding the text only utterance. Similarly, the images in the multimodal utterance are encoded using the same VGGNet-16 as used for the image only utterance. The final representation of the multimodal utterance is simply the concatenation of the individual utterances.\nNote that we expect a fixed number of images\nfor each utterance and if there are fewer images we pad the utterance with empty images (similar to what is typically done for text inputs).\nThe multimodal utterance representation is then fed to a level two encoder which is again a GRU. This second level encoder (or context-level encoder) essentially encodes the sequence of utterances where the representation of each utterance in the sequence is computed and projected as described above. Figure 2 and Figure 3 show this process of computing the encoder representation for a given multimodal context."}, {"heading": "3.2 Decoder for generating text responses", "text": "As shown in Fig. 2, we use a standard recurrent neural network based decoder with GRU cells. Such a decoder has been used successfully for various natural language generation tasks including text conversation systems (Serban et al., 2016b). We also implemented a version where we couple the decoder with an attention model. The attention model learns to pay attention to different time-steps of the second level encoder (again this has been tried successfully in the context of text conversation systems (Yao et al., 2016))."}, {"heading": "3.3 Layer for ranking image responses:", "text": "The task here is to rank a given set of images depending on their relevance to the context. While training we are given a set of m images for each context of which only npos max are relevant for the context. The remaining m\u2212 npos max are picked from the corresponding false image responses in the dataset. We train the model using a max margin loss. Specifically, we compute the cosine similarity between the learnt image embedding and the encoded representation of the multimodal context.\nThe model is then trained to maximize the margin between the cosine similarity for the correct image and the incorrect images. Fig. 3 depicts this for the case when m = 2 and npos max = 1.\nNote that due to space constraints we only provide pictorial representations of these models."}, {"heading": "4 Experiments", "text": "In this section we describe the experimental setup used to evaluate the following models on the two tasks: \u2022 Hierarchical Encoder Decoder (ignoring\nimage context), whose architecture is similar to that proposed in (Serban et al., 2016a) \u2022 The proposed Multimodal Hierarchical En-\ncoder Decoder, where we varied the size of the dialog context. \u2022 The proposed Multimodal Hierarchical En-\ncoder Decoder with attention over the multimodal utterance representation at each time step"}, {"heading": "4.1 Evaluating the Text Response Task", "text": "For this task we only considered those dialogs which end with a text response. The sizes of the training, validation and test sets are reported in the 6th row of Table 3. We tuned the following hyperparameters using the validation set; learning rate \u2208 {1e-2, 1e-3, 4e-4}, RNN hidden unit size \u2208 {256, 512}, text embedding size \u2208 {256, 512}, image embedding size \u2208 {256, 512}, batch size \u2208 {32, 64}. The numbers in brackets indicate the distinct values of each hyperparameter we considered. We used Adam (Kingma and Ba, 2014) as the optimization algorithm. The results in Table 6 summarize the BLEU and NIST scores used as the evaluation metric for this task."}, {"heading": "4.2 Evaluating Image Response Task", "text": "Here we only consider those dialogs that end in an image response from the system. The sizes of the training, validation and test sets are reported in the 5th row of Table 3. Both during training and testing, the model is provided with m=5 target images out of which only npos max=1 is relevant and at test time the model has to rank the images in order of their relevance as a response to the given context. The hyperparameters of the model were tuned in the same way as mentioned above.\nWe use Recall@top-m as the evaluation metrics where top-m is varied from 1 to 3, and the model prediction is considered to be correct only if the true response is among the top-m entries in the ranked list. The results are summarized in Table 7. In the supplementary material we provide examples of the model\u2019s outputs for the two tasks."}, {"heading": "4.3 Discussions", "text": "We make a few observations from the results. \u2022 For both tasks, the basic model which does not\nuse any image information performs almost at par with the models which use this information.\nThis suggests that we need better models for capturing the interactions between text and images. In particular, we need some common representation learning for the two modalities. \u2022 While BLEU gives more importance to word\nordering in calculating similarity, NIST emphasizes more on informative words. The low NIST scores suggest that the models need some external knowledge to understand the informative words in the domain. \u2022 Adding attention degrades the performance.\nThough counter intuitive, this suggests we need better models for multimodal attention. \u2022 Overall, we feel there is a lot of scope for im-\nprovement and the current models only establish the feasibility of the two tasks."}, {"heading": "5 Conclusion", "text": "In this paper, we introduced the Multimodal Dialogs (MMD) dataset that is well suited to the task of studying multimodal domain-aware conversations in a wide variety of settings. This dataset was gathered by working closely with a group of 20 experts in the retail domain and consists of over .15 Million conversation sessions between shoppers and sales agents involving both text and images. Driven by the MMD dataset, we proposed 5 new sub-tasks along with their evaluation methodologies. We also proposed 2 multimodal deep learning models using the encode-attend-decode paradigm and demonstrated their performance on both text response generation and best image response selection. The performance numbers obtained demonstrate the feasibility of the involved sub-tasks and highlight the challenges involved."}, {"heading": "6 Supplementary Materials", "text": ""}, {"heading": "6.1 Examples of Text Responses generated by the proposed Multimodal HRED model", "text": "This section enumerates some of the examples taken from the test set, of the model generating the agent\u2019s text responses. In each figure, the last row (in red) is the predicted output of the system and directly above it is the true response from the original dialog in the dataset. The multimodal content appearing above is the context fed into the model to generate this response."}, {"heading": "6.2 Examples of Target Images Ranked by the proposed Multimodal HRED model", "text": "Here, we similarly enumerate some of the example outputs for the Image Response Selection task taken from the test set. The model is given a multimodal context of an ongoing dialog and a set of target images, which it has to rank in order of their relevance as a response, given the context. The target images are shown in the bottom row for every example, and the true positive response and the true negative response (according to the dataset) is demarcated by a green or a red box respectively. The model internally scores each of these images based on the likelihood of the image being a response, given the context. In the\nfigures, the target images have been sorted by the cosine similarity score given out by the model, where a higher value of similarity implies a higher confidence of the model in that image being a relevant response."}, {"heading": "6.3 Examples of dialog sessions from MMD dataset", "text": "In this section we show some of the sample dialog sessions between a shopper and an agent, where each dialog culminates in either a successful purchase or with the shopper exiting without a purchase."}], "references": [{"title": "VQA: visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh."], "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago,", "citeRegEx": "Antol et al\\.,? 2015", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Visual dialog", "author": ["Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "Jos\u00e9 M.F. Moura", "Devi Parikh", "Dhruv Batra."], "venue": "CoRR abs/1611.08669. http://arxiv.org/abs/1611.08669.", "citeRegEx": "Das et al\\.,? 2016", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Guesswhat?! visual object discovery through multi-modal dialogue", "author": ["Harm de Vries", "Florian Strub", "Sarath Chandar", "Olivier Pietquin", "Hugo Larochelle", "Aaron C. Courville."], "venue": "CoRR abs/1611.08481. http://arxiv.org/abs/1611.08481.", "citeRegEx": "Vries et al\\.,? 2016", "shortCiteRegEx": "Vries et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron C. Courville", "Yoshua Bengio."], "venue": "Advances in Neural Information Processing Systems 27: Annual Conference", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "Proceedings of the SIGDIAL 2015 Conference, The", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering", "author": ["Tegan Maharaj", "Nicolas Ballas", "Aaron C. Courville", "Christopher Joseph Pal."], "venue": "CoRR abs/1611.07810. http://arxiv.org/abs/1611.07810.", "citeRegEx": "Maharaj et al\\.,? 2016", "shortCiteRegEx": "Maharaj et al\\.", "year": 2016}, {"title": "Imagegrounded conversations: Multimodal context for natural question and response generation", "author": ["Nasrin Mostafazadeh", "Chris Brockett", "Bill Dolan", "Michel Galley", "Jianfeng Gao", "Georgios P. Spithourakis", "Lucy Vanderwende."], "venue": "CoRR", "citeRegEx": "Mostafazadeh et al\\.,? 2017", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2017}, {"title": "Generative adversarial text-to-image synthesis", "author": ["Scott Reed", "Zeynep Akata", "Xinchen Yan", "Lajanugen Logeswaran", "Bernt Schiele", "Honglak Lee."], "venue": "Proceedings of The 33rd International Conference on Machine Learning.", "citeRegEx": "Reed et al\\.,? 2016", "shortCiteRegEx": "Reed et al\\.", "year": 2016}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Alan Ritter", "Colin Cherry", "Bill Dolan."], "venue": "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 2-4,", "citeRegEx": "Ritter et al\\.,? 2010", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V. Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intel-", "citeRegEx": "Serban et al\\.,? 2016a", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron C. Courville", "Yoshua Bengio."], "venue": "CoRR abs/1605.06069.", "citeRegEx": "Serban et al\\.,? 2016b", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "CoRR abs/1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR abs/1506.05869. http://arxiv.org/abs/1506.05869.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston,", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio."], "venue": "In", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "An attentional neural conversation model with improved specificity", "author": ["Kaisheng Yao", "Baolin Peng", "Geoffrey Zweig", "Kam-Fai Wong."], "venue": "CoRR abs/1606.01292. http://arxiv.org/abs/1606.01292.", "citeRegEx": "Yao et al\\.,? 2016", "shortCiteRegEx": "Yao et al\\.", "year": 2016}, {"title": "Video paragraph captioning using hierarchical recurrent neural networks", "author": ["Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu."], "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas,", "citeRegEx": "Yu et al\\.,? 2016", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Leveraging video descriptions to learn video question answering", "author": ["Kuo-Hao Zeng", "Tseng-Hung Chen", "Ching-Yao Chuang", "Yuan-Hong Liao", "Juan Carlos Niebles", "Min Sun."], "venue": "CoRR abs/1611.04021. http://arxiv.org/abs/1611.04021.", "citeRegEx": "Zeng et al\\.,? 2016", "shortCiteRegEx": "Zeng et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "The recent progress with deep learning techniques for other problems at the intersection of NLP and Computer Vision such as image captioning (Vinyals et al., 2015; Xu et al., 2015), video description (Yu et al.", "startOffset": 141, "endOffset": 180}, {"referenceID": 16, "context": "The recent progress with deep learning techniques for other problems at the intersection of NLP and Computer Vision such as image captioning (Vinyals et al., 2015; Xu et al., 2015), video description (Yu et al.", "startOffset": 141, "endOffset": 180}, {"referenceID": 18, "context": ", 2015), video description (Yu et al., 2016), image question an-", "startOffset": 27, "endOffset": 44}, {"referenceID": 0, "context": "swering (Antol et al., 2015), video question answering (Zeng et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 19, "context": ", 2015), video question answering (Zeng et al., 2016; Maharaj et al., 2016), is owed in large part to the availability of large-scale open datasets for their respective tasks.", "startOffset": 34, "endOffset": 75}, {"referenceID": 7, "context": ", 2015), video question answering (Zeng et al., 2016; Maharaj et al., 2016), is owed in large part to the availability of large-scale open datasets for their respective tasks.", "startOffset": 34, "endOffset": 75}, {"referenceID": 12, "context": "Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different", "startOffset": 33, "endOffset": 95}, {"referenceID": 17, "context": "Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different", "startOffset": 33, "endOffset": 95}, {"referenceID": 11, "context": "Though there has been recent work(Serban et al., 2016b; Yao et al., 2016; Serban et al., 2016a) with different", "startOffset": 33, "endOffset": 95}, {"referenceID": 6, "context": "conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research.", "startOffset": 22, "endOffset": 84}, {"referenceID": 14, "context": "conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research.", "startOffset": 22, "endOffset": 84}, {"referenceID": 10, "context": "conversation datasets (Lowe et al., 2015; Vinyals and Le, 2015; Ritter et al., 2010), the mode of interaction there is limited to text conversations only, rendering them inadequate for multimodal conversation research.", "startOffset": 22, "endOffset": 84}, {"referenceID": 0, "context": "The body of work most relevant to ours is Visual QA(Antol et al., 2015) involving a single question and response, and the work of (de Vries et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 9, "context": "contextual GANs(Reed et al., 2016; Goodfellow et al., 2014)).", "startOffset": 15, "endOffset": 59}, {"referenceID": 4, "context": "contextual GANs(Reed et al., 2016; Goodfellow et al., 2014)).", "startOffset": 15, "endOffset": 59}, {"referenceID": 6, "context": "This simplified evaluation protocol of \u201cselecting/ranking\u201d the best response was proposed for the Ubuntu Dialog Corpus (Lowe et al., 2015) and helps provide more control on the experimental setup and evaluate individual parts of", "startOffset": 119, "endOffset": 138}, {"referenceID": 11, "context": "These methods are based on the popular hierarchical encode-attend-decode paradigm (Serban et al., 2016a) typically used for (text) conversation sys-", "startOffset": 82, "endOffset": 104}, {"referenceID": 1, "context": "(a) Text only utterance: Every text utterance in the context is encoded using a recurrent neural network with GRU (Chung et al., 2014) cells in", "startOffset": 114, "endOffset": 134}, {"referenceID": 11, "context": "a process similar to the utterance level encoder described in (Serban et al., 2016a).", "startOffset": 62, "endOffset": 84}, {"referenceID": 13, "context": "We encode each image using a 4096 dimensional representation obtained from the FC7 layer of a VGGNet-16 (Simonyan and Zisserman, 2014) convolutional neural network.", "startOffset": 104, "endOffset": 134}, {"referenceID": 12, "context": "Such a decoder has been used successfully for various natural language generation tasks including text conversation systems (Serban et al., 2016b).", "startOffset": 124, "endOffset": 146}, {"referenceID": 17, "context": "tion systems (Yao et al., 2016)).", "startOffset": 13, "endOffset": 31}, {"referenceID": 11, "context": "In this section we describe the experimental setup used to evaluate the following models on the two tasks: \u2022 Hierarchical Encoder Decoder (ignoring image context), whose architecture is similar to that proposed in (Serban et al., 2016a) \u2022 The proposed Multimodal Hierarchical Encoder Decoder, where we varied the size of the", "startOffset": 214, "endOffset": 236}, {"referenceID": 5, "context": "We used Adam (Kingma and Ba, 2014) as the optimization algorithm.", "startOffset": 13, "endOffset": 34}], "year": 2017, "abstractText": "Owing to the success of deep learning techniques for tasks such as Q/A and text-based dialog, there is an increasing demand for AI agents in several domains such as retail, travel, entertainment, etc. that can carry on multimodal conversations with humans employing both text and images within a dialog seamlessly. However, deep learning research is this area has been limited primarily due to the lack of availability of large-scale, open conversation datasets. To overcome this bottleneck, in this paper we introduce the task of multimodal, domain-aware conversations, and propose the MMD benchmark dataset towards this task. This dataset was gathered by working in close coordination with large number of domain experts in the retail domain and consists of over 150K conversation sessions between shoppers and sales agents. With this dataset, we propose 5 new sub-tasks for multimodal conversations along with their evaluation methodology. We also propose two novel multimodal deep learning models in the encodeattend-decode paradigm and demonstrate their performance on two of the sub-tasks, namely text response generation and best image response selection. These experiments serve to establish baseline performance numbers and open new directions of research for each of these sub-tasks.", "creator": "LaTeX with hyperref package"}}}