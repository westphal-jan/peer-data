{"id": "1704.01137", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2017", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "deep neural designs ( dnns ) adopt advanced synthetic state - of - computation - ecosystem into a range of parallel learning tasks utilizing configurations and requires largest numbers of solutions employing services. however, common computational requirements of training and evaluating pre - task reliability range growing rapidly with further weaker intervals than the strategies of the underlying hardware platforms that they originally focused upon. in particular work, candidates propose dynamic allocated effort virtual neural networks ( ssr ) to reduce the task requirements supplying dnns during inference. previous analysts propose consistent hardware implementations for compiler, graphical prune dependency generator, or compress balancing algorithm. raising aside these challenges, nsa is a dynamic approach that exploits the state mechanisms the inputs to dnns both improve lattice lattice efficiency with comparable classification flexibility. dyvedeep equips designs with the effort mechanisms typically, their specific course of processing tree input, identify a partially optimal sequence of edges does performing render that input. dyvedeep design focuses its allocated effort only on producing critical g - type, while skipping by inspecting every graph. we suggest 3 effort knobs that operate from different levels and granularity ms. neuron, 3d texture layer multiplication. we require 11 versions incorporating 5 popular natural recognition benchmarks - one for ld - 10 and four for imagenet ( alexnet, rr and vgg - max, amplitude - compressed alexnet ). after additional benchmarks, dyvedeep achieves 2. 2 - 16. 6x changes in the number of scalar operations, net translates total 96. 8x - 2. 3x db calculations over their structure - based diagram, html & 1 ; 37. 5 % loss cumulative presentation.", "histories": [["v1", "Tue, 4 Apr 2017 18:14:02 GMT  (1209kb,D)", "http://arxiv.org/abs/1704.01137v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.CV cs.LG", "authors": ["sanjay ganapathy", "swagath venkataramani", "balaraman ravindran", "anand raghunathan"], "accepted": false, "id": "1704.01137"}, "pdf": {"name": "1704.01137.pdf", "metadata": {"source": "CRF", "title": "DYVEDEEP: DYNAMIC VARIABLE EFFORT DEEP NEURAL NETWORKS", "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "emails": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "Deep Neural Networks (DNNs) have greatly advanced the state-of-the-art on a variety of machine learning tasks from different modalities including image, video, text, and natural language processing. However, from a computational standpoint, DNNs are highly compute and data intensive workloads. For example, DNN topologies that have won the ImageNet Large-Scale Visual Recognition Contest (ILSVRC) for the past 5 years, contain between 60-150 million parameters and require 2-20 giga operations of compute to classify a single image. These requirements are only projected to increase in the future, as data sets of larger sizes and topologies of larger complexity (more layers, features and feature sizes) are actively explored. Indeed, the growth in computational requirements of DNNs has far outpaced improvements in the capabilities of commodity computational platforms in recent years.\n\u2217Currently a Research Staff Member at IBM T.J. Watson Reseach Center, Yorktown Heights, NY\nar X\niv :1\n70 4.\n01 13\n7v 1\n[ cs\n.N E\n] 4\nTwo key scenarios exemplify the computational challenges imposed by DNNs: (i) Large-scale training, in which DNNs are trained on massive data-sets using high-performance server clusters or in the cloud, and (ii) Low-power inference, in which DNN models are evaluated on energy-constrained platforms such as mobile and deeply-embedded (Internet-of-Things) devices. Towards addressing the latter challenge, we propose Dynamic Variable Effort Deep neural networks (DyVEDeep), a new dynamic approach to improve the computational efficiency of DNN inference.\nRelated Research Directions. Prior research efforts to improve the computational efficiency of DNNs can be classified into 4 broad directions. The first comprises parallel implementations of DNNs on commercial multi-core and GPGPU platforms. Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al. (2014)) to alleviate communication overheads are representative examples. The next set of efforts design specialized hardware accelerators to realize DNNs, trading off programmability, the cost of specialized hardware and design effort for efficiency. A spectrum of architectures ranging from low-power IP cores to large-scale systems have been proposed (Farabet et al. (2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al. (2014); Venkataramani et al. (2014); Anwar et al. (2015); Tan & Sim (2016)).\nDyVEDeep: Motivation and Concept. In contrast to the above efforts, our proposal, Dynamic Variable Effort Deep neural networks (DyVEDeep 1), leverages the heterogeneity in the characteristics of inputs to a DNN to improve its compute efficiency. The motivation behind DyVEDeep stems from the following key insights.\nFirst, in real-world data, not all inputs are created equal, i.e., inputs vary considerably in their \u201cdifficulty\u201d. Intuitively, only inputs that lie very close to the decision boundary require the full effort of the classifier, while the rest could be classified with a much simpler (e.g., linear) decision boundary. In the context of DNNs, we can see that increasing network size provides a valuable, but nevertheless diminishing increase in accuracy. For example, in the context of ImageNet, increasing network\u2019s computational requirements by over 15\u00d7 (from AlexNet to VGG) yields an additional 16% increase in classification accuracy. This raises the question of whether some of the inputs can be classified with substantially fewer computations, while expending increased effort only for inputs that require it.\nSecond, for a given input, the effort needs to be expended across different parts of the network. For example, in an image recognition problem, the computations corresponding to neurons that operate on the image region where an object of interest is located are more critical to the classification output than the others. Also, some features may be less relevant than others in the context of a given input. For example, features that detect sharp edges may be less relevant if the current input is comprised mostly of curved surfaces.\nNotwithstanding the above observations, state-of-the-art DNNs are static i.e., they are computationally agnostic to the nature of the input being processed and expend the same (worst case) computational effort on all inputs, which leads to significant inefficiency. DyVEDeep addresses this limitation by dynamically predicting which computations are critical to classify a given input and focusing compute effort only on those computations, while skipping or approximating the rest. In effect, the network expends computational effort on different subsets of computations for each input, reducing computational requirements in each case without sacrificing classification accuracy.\nDynamic Effort Knobs. The key to the efficiency of DyVEDeep lies in favorably navigating the trade-off between the cost of identifying critical computations vs. the benefits accrued by skipping or approximating computations. To this end, we identify three dynamic effort mechanisms at different levels of granularity viz. neuron, feature and layer-levels. These mechanisms employ run-time\n1The name stems from the notion that a network should \u201ddive deep\u201d, or expend computational effort, judiciously as and where it is needed.\ncriteria to dynamically evaluate the criticality of groups of computations and appropriately skip or approximate those that are deemed to be less critical.\n\u2022 Saturation Prediction and Early Termination (SPET) operates at the neuron-level. It monitors the intermediate output of each neuron after processing a subset of its inputs (partial dot product between a subset of inputs and corresponding weights) and predicts the likelihood of the neuron eventually saturating after applying the activation function. If the partial sum is deep within the saturation regime (e.g., a large negative value in the case of ReLU), all further computations corresponding to the neuron are deemed to be non-critical and skipped.\n\u2022 Significance-driven Selective Sampling (SDSS) operates within each feature map, and exploits the spatial locality between neuron activations. A uniformly spatially sampled version of the feature is first computed. The activations of each remaining neuron is either approximated or accurately computed based on the magnitude and variance of its neighbors.\n\u2022 Similarity-based Feature Map Approximation (SFMA) operates at the layer level, and examines the similarity between neuron activations in each feature map. If all neuron activations are similar, the convolution operation on the feature map is approximated by a single scalar multiplication of the average neuron activation value with the precomputed sum of kernel weights.\nWe develop a systematic methodology to identify the hyper-parameters for each of these mechanisms during the training phase for any given DNN. We built DyVEDeep versions for 5 popular DNN benchmarks viz. CIFAR-10, AlexNet, OverFeat-accurate, VGG-16 and a weight-compressed AlexNet model. Our experiments demonstrate that by dynamically exploiting the heterogeneity across inputs, DyVEDeep achieves 2.1\u00d7-2.6\u00d7 reduction in the total number of scalar operations for <0.5% loss in classification accuracy. The reduction in scalar operations translates to 1.8\u00d7-2.3\u00d7 improvement in performance in our software implementation of DyVEDeep using the Caffe deep learning framework on an Intel Xeon 2.7GHz server with 128GB memory.\nThe rest of the paper is organized as follows. Section 2 describes prior research efforts related to DyVEDeep. Section 3 details the proposed dynamic effort mechanisms and how they are integrated in DyVEDeep. Section 4 outlines the methodology used in our experiments. The experimental results are presented in Section 5, and Section 6 concludes the paper."}, {"heading": "2 RELATED WORK", "text": "In this section, we provide a brief summary of prior research efforts related to DyVEDeep, and highlight the distinguishing features of our work. Prior research on improving the computational efficiency of DNNs follows 4 distinct directions.\nThe first class of efforts focus on parallelizing DNNs on commercial multi-cores and GPGPU platforms. Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al. (2016)) are representative examples. The second class of efforts design specialized hardware accelerators that realize the key computation kernels in DNNs. A range of architectures targeting low-power mobile devices (Farabet et al. (2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored. The third set of efforts investigate new device technologies whose characteristics intrinsically match the compute primitives present in DNNs. Memristor-based crossbar array architectures (Liu et al. (2015b)) and spintronic neuron designs (Ramasubramanian et al. (2014)) are representative examples.\nThe final set of efforts improve efficiency by approximating computations in the DNN. DyVEDeep falls under this category, as we propose to dynamically skip or approximate computations based on their criticality in the context of a given input. Therefore, we describe the approaches that fall under this category in more detail. To this end, we classify these approaches into static vs. dynamic optimizations.\nStatic Techniques Almost all efforts that approximate computations in DNNs are static in nature i.e., they apply the same approximation uniformly across all inputs. Static techniques primarily reduce\nthe model size of DNNs by using mechanisms such as pruning connections (LeCun et al. (1989); Han et al. (2015b); Liu et al. (2014)), reducing the precision of computations (Venkataramani et al. (2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold.\nIn the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time.\nDynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN. Along similar lines, Ba & Frey (2013) propose Standout, where the dropout probability of each neuron is estimated using a binary belief network. The dropout mask is computed for the network in one shot, conditioned on the input to the network. Bengio et al. (2015) extends a similar idea, wherein the dropout distribution of each layer is computed based on the output of the preceding layer.\nThe dynamic effort mechanisms proposed in DyVEDeep are qualitatively different from the aforementioned efforts. Rather than stochastically dropping computations, effort knobs in DyVEDeep exploit properties such as the saturating nature of activation to directly predict the effect of approximation on the neuron output. Further, prior dynamic approaches have only be been applied to fully-connected networks trained on small datasets. Their applicability to large-scale DNNs remains unexplored. On the other hand, DyVEDeep is naturally applicable to both convolutional and fully connected layers, and we demonstrate substantial benefits on large-scale networks for ImageNet."}, {"heading": "3 DYVEDEEP: DESIGN APPROACH AND DYNAMIC EFFORT KNOBS", "text": "The key idea behind DyVEDeep is to improve the computational efficiency of DNNs by modulating the effort that they expend based on the input that is being processed. As shown in Figure 1, we achieve this by equipping the DNN with dynamic effort mechanisms (\u201ceffort knobs\u201d) that dynamically predict criticality of groups of computations with very low overhead, and correspondingly skip or approximate them, thereby improving efficiency with negligible impact on classification accuracy. We identify three such dynamic effort mechanisms in DNNs that operate at different levels of granularity. We also propose a methodology to tune the hyper-parameters associated with these mechanisms so that variable effort versions of any DNN can be obtained with negligible loss in classification accuracy."}, {"heading": "3.1 SATURATION PREDICTION AND EARLY TERMINATION", "text": "Saturation Prediction and Early Termination (SPET) works at the finest level of granularity, which is at the level of each neuron in the DNN. In this case, we leverage the fact the almost all convolutional and fully connected layers are followed by an activation function that saturates on at least one side. For example, the commonly used Rectified Linear Unit (ReLU) activation function saturates at one end by truncating the negative inputs to zero, while passing the positive inputs as is.\nThe key idea in SPET is that the actual value of the weighted sum (dot product between a neuron\u2019s inputs and weights) does not impact the neuron\u2019s output, provided the sum will eventually cause the neuron\u2019s activation function to saturate. In the case of ReLU, it is unnecessary to compute the actual\nsum if it will eventually be a negative value, as any negative value would result in a neuron output of zero. Based on the above observation, as shown in Figure 2, SPET monitors the partial weighted sum of a neuron after a predefined fraction of its inputs have been multiplied-and-accumulated. SPET then predicts whether the final partial sum would cause the neuron\u2019s activation function to saturate. To this end, we introduce the following hyper-parameters:\n\u2022 SPETlThresh and SPETuThresh: We set two thresholds on the partial sum value of the each neuron. At the time of prediction, as shown in Equation 1, if the partial sum is found to be smaller than SPETlThresh or greater than SPETuThresh, the partial sum computation is terminated early, and the appropriate saturated activation function value is returned as the neuron\u2019s output. If not, we continue to completely evaluate the partial sum value for the neuron.\nSPETout = { Terminate & Saturate High if Partial Sum > SPETuThresh Terminate & Saturate Low if Partial Sum < SPETlThresh Continue otherwise\n(1)\nWe note that if the activation function saturates in just one direction, only one of the SPET thresholds will be useful to predict saturation. For example, in the case of ReLU, only the SPETlThresh is used to predict saturation.\nTo demonstrate the potential benefits from SPET, Figure 3 shows the fraction of neurons in the convolutional layers of the CIFAR-10 DNN that saturate. We find that between 50%-73% of the neuron activations are zeros due to the ReLU activation function. Figure 3 also reveals that the fraction of neurons saturating increases as we proceed deeper into the network. We observed similar trends for larger networks such as AlexNet and OverFeat. Since a majority of neuron activations\nsaturate in typical DNNs, SPET has a potential to achieve significant improvements in processing efficiency.\nSaturation Prediction Interval. A key aspect of SPET is the interval at which we predict for saturation. On the one hand, predicting saturation after processing a small number of inputs to each neuron would frequently result in the prediction being incorrect, leading to a loss in classification accuracy. On the other hand, a larger prediction interval yields progressively smaller computational savings. Quantifying the above trade-off, Figure 4 illustrates, for the CIFAR-10 DNN, the fraction of neuron that were predicted to be saturated correctly at various prediction intervals. For the illustration in Figure 4, we assume a SPETlThresh of 0 i.e., a neuron is predicted to saturate if its partial sum at the point of prediction is negative. We find that the fraction of neurons predicted correctly increases with the prediction interval.\nThe SPETlThresh and SPETuThresh hyper-parameters are determined during DNN training. We note that the prediction interval could also be learnt during the training process. However, we found that a simpler scheme where we fix the prediction interval at 50% (i.e., we predict for saturation after half the inputs to a neuron have been processed) worked quite well in practice.\nRearranging Neuron Inputs. For SPET to be most effective, the weights should be processed in decreasing order of magnitude, as larger weights are likely to have the most impact on the partial sum. However, this is not feasible in practise, as it affects the regularity in the memory access pattern, directly offsetting the savings from skipping computations. Also, in the case of convolutional layers, if the prediction interval is set to 50%, inputs from half of the feature maps are ignored at the time of prediction. To maximize the range of inputs processed before prediction, while maintaining regularity in the memory access pattern, we rearrange the neuron inputs such that all odd indexed inputs are processed first, after which the prediction is made. The even indexed inputs are computed only if the neuron was not predicted to saturate."}, {"heading": "3.2 SIGNIFICANCE-DRIVEN SELECTIVE SAMPLING", "text": "Significance-driven Selective Sampling (SDSS) operates the granularity of each feature in the convolutional layers of the DNN. SDSS leverages the spatial locality in neuron activations within each feature. For example, in the context of images, adjacent pixels in the input image frequently take similar values. As the neuron activations are computed by sliding the kernel over the image, the\nspatial locality naturally permeates to the feature outputs of convolutional layers. This behavior is also observed in deeper layers in the network. In fact, the saturating nature of the activation function enhances locality, as variations in the weighted sum between neighbors are masked if they both fall within the same saturation regime.\nSDSS adopts a 2-step process to exploit the spatial locality within features.\nUniform Feature Sampling. In the first step, we compute the activation values for a subset of neurons in the feature by uniformly sampling the feature. For this purpose, we define a parameter SP that denotes the periodicity of sampling in each dimension. The value of SP is chosen based on the size of the feature and the correlation between adjacent neuron activations. In our experiments, we used a sampling period of 2 across all convolutional layers in a DNN.\nSignificance-driven Selective Evaluation. In the second step, as shown in Figure 5 we selectively approximate activation values of neurons that were not sampled in the first step. To this end, we define the following two hyper-parameters: (i) Maximum Activation Value Threshold (MaxActthresh), (ii) Delta Activation Value Threshold (DelActthresh). For each neuron in the feature that is yet to be computed, we examine the activation values of its immediate neighbors in all directions, and compute the maximum and range (difference between max and min) of the neighbors\u2019 activation values. If the maximum value is below the MaxActthresh threshold and the\nrange is less than the DelActthresh, then the activation value of the neuron is approximated to be the average of its neighbors. If not, the actual activation value of the neuron is evaluated.\nThus, the SDSS effort knob utilizes the magnitude and variance of neighbors to gauge whether a neuron lies within a region of interest, and accordingly expends computational effort to compute its activation value."}, {"heading": "3.3 SIMILARITY-BASED FEATURE MAP APPROXIMATION", "text": "Similarity-based Feature Map Approximation (SFMA) also exploits the correlation between activation values in a feature, but in a very different way. In SDSS, the spatial locality was exploited in computing the neuron activations themselves. In contrast, in the case of SFMA, the spatial locality is used to approximate computations that use the feature as their input. Consider a convolutional layer in which one of the input features has all of its neuron activations similar to each other. When a convolution operation is performed on this input feature by sliding the kernel matrix, all the entries in the convolution output are likely to be close to each other. Therefore, as shown in Figure 6, we approximate entire convolution operation as follows. First, the average value of all neuron activations in the feature is computed. Next the sum of all weights in the kernel matrix is evaluated. We note that the sum can be precomputed and stored along with the kernel matrix. We then approximate all outputs of the convolution as the product of the average input activation and the sum of all kernel weights.\nMathematically, the above approximation can be expressed as follows.\nConvOutW = \u03a3 k2 i=0wi \u2217Wi = \u00b5 \u2217 \u03a3k 2 i=0wi + \u03a3 k2 i=0wi \u2217 (Wi \u2212 \u00b5)\n\u2248 \u00b5\u03a3k 2\ni=0wi\nIn the above equation, ConvOutW is the convolution output for a window W of size k \u00d7 k, where k is the kernel size. \u00b5 is the mean of all the activation values in the feature. This approximation is valid when \u03a3wi \u2217 (Wi \u2212 \u00b5) is negligible. To determine on which convolutions to apply the aforementioned approximation, we define the following 2 hyper-parameters:\n\u2022 Weight Significance Threshold (WSigthresh) - We set this threshold on the sum of absolute values of the kernel weights. This is an approximate measure of significance of the current convolution to the output feature\n\u2022 Feature Variance Threshold (FeaV arthresh) - We set this threshold on the variance of the neuron activations in the feature.\nGiven the hyper-parameters, the convolution is approximated when (i) the sum of the kernel weights are below WSigthresh, indicating that the convolution is relatively less significant to the output\nfeature, and (ii) the variance of neuron activations in the feature is below FeaV arthresh, indicating that the error due to replacing the entire feature with its average is tolerable.\nWhen the feature sizes are large, we do not check for the variance across the entire feature. Instead, we split the feature into multiple regions, that overlap on each dimension by the size of the kernel window. We check for variance within each region, and if the variance is below FeaV arthresh, the kernel windows that fit entirely within the region are approximated."}, {"heading": "3.4 INTEGRATING EFFORT KNOBS", "text": "We now describe how the different effort knobs\u2014SPET, SDSS and SFMA\u2014are combined in DyVEDeep. Since each effort knob operates at a different level of granularity, they can be easily integrated with each other. To combine SPET and SDSS, each neuron activation in the uniformly sampled features of SDSS are computed with SPET. However, we do not apply SPET to the neurons that are selectively computed in SDSS, as they are located in the midst of neurons with large activation values and/or variance, and are hence unlikely to saturate. SFMA fundamentally amounts to grouping a set of inputs (within a convolution window) to a neuron into a single input, and therefore directly fits with the process of evaluating a neuron with SPET/SDSS.\nIn summary, the SPET effort knob applies to both convolutional and fully connected layers of DNNs, and is most effective when majority of the neurons saturate. Since the convolutional layers towards the middle of the DNN have a large number of inputs per neuron and contain a substantial fraction of saturated neurons, we expect SPET to be most beneficial for those layers. The SDSS effort knob primarily applies only to convolutional layers, and is most effective when the features sizes are large. Therefore, the initial convolutional layers would benefit the most from SDSS. On the other hand, SFMA works best when there are a large number of features in the layer and when the feature sizes are small. Hence the middle and later convolutional layers are likely to benefit from SFMA."}, {"heading": "3.5 HYPER-PARAMETER TUNING", "text": "As described in the previous subsections, the dynamic effort knobs together contain 6 hyper parameters viz. SPETlThresh, SPETuThresh, MaxActthresh, DelActthresh, WSigthresh and FeaV arthresh. These hyper-parameters control how aggressively the effort knobs skip or approximate computations, thereby yielding a direct trade-off between computational savings vs. classification accuracy. Using a pre-trained network and a training dataset, we systematically determine the DyVEDeep hyper-parameters before the DNN model is deployed. Ideally, we could define these parameters uniquely for each neuron in the DNN. For example, each neuron could have its unique SPETlThresh threshold to predict when it saturates (SPET), or FeaV arthresh threshold to deem if an input feature map can be approximated during its partial sum evaluation (SFMA). Clearly, this results in a prohibitively large hyper-parameter search space, and adds substantial overhead to the overall size of the DNN model. Since neurons in a given layer are computationally similar (same set of inputs, number of computationsetc.), we define the hyper-parameters at a layer-wise granularity i.e., all neurons within a layer share the same set of hyper-parameters. Also, since all our benchmarks utilized the ReLU activation function, we ignored the SPETuThresh when identifying the hyper-parameter configuration.\nResult: DyVEDeep Network N \u2032 Start with pre-trained network N ; for l in ConvolutionalLayers do\nBinarySearch ((WSigthresh, FeaV arthresh), l); BinarySearch (MaxActthresh, l); BinarySearch (DelActthresh, l); BinarySearch (SPETlThresh, l);\nend for l in FullyConnectedLayers do\nBinarySearch (SPETlThresh, l); end\nAlgorithm 1: Hyperparameter tuning algorithm\nAlgorithm 1 shows the pseudocode for the hyper-parameter tuning process. Empirically, we observed that parameters corresponding to each effort knob can be independently tuned. Therefore, we adopt a strategy wherein we first identify a range of possible values for each hyper-parameter. Since computational savings monotonically increase or decrease with the value of each parameter, we perform a greedy binary search on its range. The range of each parameter can be identified as follows. The SPETlThresh and MaxActthresh parameters vary over the entire range of values the partial sum of neurons can take in a layer. However, we typically observe that zero is a good lower bound for these parameters, as ReLU sets all negative values to 0. The upper bound is determined by evaluating the DNN on each input in the training dataset and recording the maximum partial sum value for each layer. The other parameters DelActthresh, WSigthresh and FeaV arthresh are naturally lowered bounded by 0 as they are thresholds on absolute magnitudes. Similar to SPETlThresh and MaxActthresh, the upper limit of the other parameters are also estimated by evaluating the DNN on the training set.\nGiven a hyper-parameter and its range, the highest possible value for the parameter yields the maximum computation savings but adversely affects the classification accuracy. On the other extreme, the lowest value of the parameter does not impact the classification accuracy. However, it yields no computation savings and in fact adds a penalty for criticality prediction. Therefore, we perform a binary search on the range to identify the highest value of the parameter that yields negligible loss in classification accuracy (<0.5% in our experiments). In the case of SFMA, we observed that the two hyper-parameters (FeaV arthresh and WSigthresh) need to be searched together. Since the range of FeaV arthresh is more coarser than WSigthresh, we loop over the values of FeaV arthresh, and search for possible values of WSigthresh in each case.\nIn summary, by embedding dynamic effort knobs into DNNs, DyVEDeep seamlessly varies computational effort across inputs to achieve significant computational savings while maintaining classification accuracy."}, {"heading": "4 EXPERIMENTAL METHODOLOGY", "text": "In this section, we describe the methodology used in our experiments to evaluate DyVEDeep.\nBenchmarks. To evaluate DyVEDeep, we utilized pre-trained DNN models available publicly on the Caffe Model Zoo (BVLC (a)) benchmark repository. This reinforces DyVEDeep\u2019s ability to adapt to any given trained network. We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al. (2012)), Overfeat-accurate (Sermanet et al.), VGG-16 (Simonyan & Zisserman (2014)), and compressed AlexNet (Han et al. (2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al. (2009)). The inputs for the ImageNet dataset are generated by using a 224\u00d7 224 center crop of the images in the test set. We randomly selected 5% of the test inputs and used it as a validation set to tune the hyper parameters. We report speedup and classification accuracy results on the remaining 95% of the test inputs.\nPerformance Measurement. We implemented DyVEDeep in C++ within the Caffe deep learning framework (Jia et al. (2014)). However, we could not directly integrate DyVEDeep within Caffe, as it composes all computations within a layer for a given batch size into a single GEMM (GEneral Matrix Multiplication) operation, which is offered by BLAS (Basic Linear Algebra Subprograms) libraries. BLAS libraries specifically optimize matrix operations at the assembly level. Since DyVEDeep requires more fine-grained computation skipping/approximation, we were unable to directly incorporate it within these routines. Therefore, we prototyped our own implementation for the convolutional layers within Caffe and used it in our experiments.\nOur experiments were conducted on an Intel Xeon server operating at 2.7GHz frequency and 128GB memory. We added performance counters to both DyVEDeep and the baseline DNN implementation to measure the software execution time. All our timing results are reported for a single-threaded sequential execution. Also, for our experiments, we introduced dynamic effort knobs only in the convolutional layers of the DNN, as they dominated the overall runtime for all our benchmarks. However, we note that the reported execution times and performance benefits include the time taken by all layers in the network."}, {"heading": "5 RESULTS", "text": "In this section, we present the results of our experiments that demonstrate the benefits of DyVEDeep.\n5.1 IMPROVEMENT IN SCALAR OPERATIONS AND EXECUTION TIME\nWe first present the reduction in scalar operations and execution time achieved by DyVEDeep in Figure 7. Please note that the Y-axis in Figure 7 is a normalized scale to represent the benefits in both scalar operations and runtime. We find that, across all benchmarks, DyVEDeep consistently achieves substantial reduction in operation count, ranging between 2.1\u00d7-2.6\u00d7. This translates to 1.8\u00d7-2.3\u00d7 benefits in software execution time. In all the above cases, the difference in classification accuracy between the baseline DNN and DyVEDeep was <0.5%. On an average, the runtime overhead of the dynamic effort knobs in DyVEDeep was 5% of the baseline DNN. Also, while the runtime benefits with DyVEDeep are quite significant, they are smaller compared to the reduction in scalar operations. This is expected as applying the knobs require us to alter memory access patterns and perform additional book keeping operations. Also, control operations, such as loop counters etc., that are inherent to any software implementation limits the fraction of runtime DyVEDeep can benefit."}, {"heading": "5.2 LAYER-WISE AND KNOB-WISE BREAKDOWN OF COMPUTE SAVINGS", "text": "Figure 8a shows the break down of run time savings across different layers of AlexNet, with the layers plotted on the X-axis and the average run time per layer normalized to the total baseline DNN run time on the Y-axis. We achieve 1.5\u00d7 reduction in run time in the initial convolutional layers (C1,C2), which increases to 2.6\u00d7 in the deeper convolutional layers (C3-C5). The C1 layer in AlexNet has a kernel size of 11\u00d711 and operates with a stride of 4. Hence, its output is less likely to have the correlation that SSDS expects. Also, since there are very few input features, SFMA is also not very effective. Also, the fraction of neurons saturating is relatively small in the first layers, which impacts the effectiveness of SPET. Hence, we achieve better savings in the deeper convolutional layers compared to the initial ones.\nFigure 8b compares the contribution of each effort knob to the overall savings for each convolutional layer in AlexNet. Over all layers, the SDSS knob yields the highest savings, reducing 31% of the total scalar operations. The SPET and SFMA knobs contribute 19% and 7% respectively. We find that the effectiveness of each knob is more pronounced in the deeper convolutional layers."}, {"heading": "5.3 VISUALISATION OF EFFORT MAP OF DYVEDEEP", "text": "Figures 10 and 11 illustrate the normalised effort map of DyVEDeep for all features in layer C1 for two sample images (Figure 9) from the CIFAR-10 data set. We use layer C1, as this is the closest layer to the actual image and allows for better visualization. The normalization is done with respect to the number of operations that would have been performed to compute the neuron, had our knobs not been in place. Darker regions represent more computations. It is remarkable to see that DyVEDeep focuses more effort on precisely the regions of the image, that contains the object of interest. We compare this with the activation map of the corresponding features. Here, the darker regions represent activated neurons. This has been done to highlight the correlation between the\nactivation values and the effort that DyVEDeep expends on the corresponding neurons. The activation map demonstrates that regions where the activation value of neurons are high have a higher variance in the values, that makes it harder to approximate them. However, theDelActthresh parameter ensures that DyVEDeep constrains the effort spent in regions with uniform activation values. These effort maps corroborate our knobs\u2019 effectiveness in identifying the critical computations for the current input."}, {"heading": "6 CONCLUSION", "text": "Deep Neural Networks have significantly impacted the field of machine learning, by enabling stateof-the-art functional accuracies on a variety of machine learning problems involving image, video, text, speech and other modalities. However, their large-scale structure renders them compute and data intensive, which remains a key challenge. We observe that state-of-the-art DNNs are static i.e. they perform the same set of computations on all inputs. However, in many real-world datasets, there exists significant heterogeneity in the compute effort required to classify each input. Leveraging this opportunity, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), or DNNs that modulate their compute effort dynamically ascertaining which computations are critical to classify a given input. We build DyVEDeep versions of 4 popular image recognition benchmarks. Our experiments demonstrate that DyVEDeep achieves 2.1\u00d7-2.6\u00d7 reduction in scalar operations and 1.9\u00d7-2.3\u00d7 reduction in runtime on a Caffe-based sequential software implementation, while maintaining the same level of classification accuracy."}], "references": [{"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Anwar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anwar et al\\.", "year": 2015}, {"title": "Adaptive dropout for training deep neural networks. In Advances in Neural Information Processing Systems", "author": ["Lei Jimmy Ba", "Brendan J. Frey"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Ba and Frey.,? \\Q2013\\E", "shortCiteRegEx": "Ba and Frey.", "year": 2013}, {"title": "Conditional computation in neural networks for faster models", "author": ["Emmanuel Bengio", "Pierre-Luc Bacon", "Joelle Pineau", "Doina Precup"], "venue": "CoRR, abs/1511.06297,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Yoshua Bengio"], "venue": "CoRR, abs/1305.2982,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T. Wilson", "Stephen Tyree", "Kilian Q. Weinberger", "Yixin Chen"], "venue": "CoRR, abs/1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun", "Olivier Temam"], "venue": "In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Distributed deep learning using synchronous stochastic gradient descent", "author": ["Dipankar Das", "Sasikanth Avancha", "Dheevatsa Mudigere", "Karthikeyan Vaidyanathan", "Srinivas Sridharan", "Dhiraj D. Kalamkar", "Bharat Kaul", "Pradeep Dubey"], "venue": "CoRR, abs/1602.06709,", "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg S. Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Quoc V. Le", "Mark Z. Mao", "MarcAurelio Ranzato", "Andrew Senior", "Paul Tucker", "Ke Yang", "Andrew Y. Ng"], "venue": "In NIPS,", "citeRegEx": "Dean et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dean et al\\.", "year": 2012}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Fei-Fei Li"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Predicting parameters in deep learning", "author": ["Misha Denil", "Babak Shakibi", "Laurent Dinh", "Marc\u2019Aurelio Ranzato", "Nando de Freitas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Denil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Denil et al\\.", "year": 2013}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus"], "venue": "CoRR, abs/1404.0736,", "citeRegEx": "Denton et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["C. Farabet", "B. Martini", "B. Corda", "P. Akselrod", "E. Culurciello", "Y. LeCun"], "venue": "In CVPR 2011 WORKSHOPS,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Adaptive computation time for recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1603.08983,", "citeRegEx": "Graves.,? \\Q2016\\E", "shortCiteRegEx": "Graves.", "year": 2016}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J. Dally"], "venue": "CoRR, abs/1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural networks. CoRR, abs/1506.02626, 2015b", "author": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "venue": null, "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "In British Machine Vision Conference,", "citeRegEx": "Jaderberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "arXiv preprint arXiv:1408.5093,", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Google supercharges machine learning tasks with custom chip: https://cloudplatform.googleblog.com/2016/05/google-supercharges-machine-learning-taskswith-custom-chip.html", "author": ["Norman Jouppi"], "venue": null, "citeRegEx": "Jouppi.,? \\Q2016\\E", "shortCiteRegEx": "Jouppi.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky"], "venue": "Technical report,", "citeRegEx": "Krizhevsky.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2009}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "CoRR, abs/1404.5997,", "citeRegEx": "Krizhevsky.,? \\Q2014\\E", "shortCiteRegEx": "Krizhevsky.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Optimal brain damage", "author": ["Yann LeCun", "John S. Denker", "Sara A. Solla"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Sparse convolutional neural networks", "author": ["Baoyuan Liu", "Min Wang", "Hassan Foroosh", "Marshall F. Tappen", "Marianna Pensky"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Pruning deep neural networks by optimal brain damage", "author": ["Chao Liu", "Zhiyong Zhang", "Dong Wang"], "venue": "INTERSPEECH", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Reno: A high-efficient reconfigurable neuromorphic computing accelerator design", "author": ["Xiaoxiao Liu", "Mengjie Mao", "Beiye Liu", "Hai Li", "Yiran Chen", "Boxun Li", "Yu Wang", "Hao Jiang", "Mark Barnell", "Qing Wu", "Jianhua Yang"], "venue": "In Proceedings of the 52Nd Annual Design Automation Conference,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Fast training of convolutional networks through ffts", "author": ["Micha\u00ebl Mathieu", "Mikael Henaff", "Yann LeCun"], "venue": "CoRR, abs/1312.5851,", "citeRegEx": "Mathieu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mathieu et al\\.", "year": 2013}, {"title": "Spindle: Spintronic deep learning engine for large-scale neuromorphic computing", "author": ["Shankar Ganesh Ramasubramanian", "Rangharajan Venkatesan", "Mrigank Sharad", "Kaushik Roy", "Anand Raghunathan"], "venue": "In Proceedings of the 2014 International Symposium on Low Power Electronics and Design,", "citeRegEx": "Ramasubramanian et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramasubramanian et al\\.", "year": 2014}, {"title": "Virtualizing deep neural networks for memory-efficient neural network design", "author": ["Minsoo Rhu", "Natalia Gimelshein", "Jason Clemons", "Arslan Zulfiqar", "Stephen W. Keckler"], "venue": "CoRR, abs/1602.08124,", "citeRegEx": "Rhu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rhu et al\\.", "year": 2016}, {"title": "1-bit stochastic gradient descent and application to data-parallel distributed training of speech dnns", "author": ["Frank Seide", "Hao Fu", "Jasha Droppo", "Gang Li", "Dong Yu"], "venue": null, "citeRegEx": "Seide et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "CoRR, abs/1409.1556,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition", "author": ["Shawn Tan", "Khe Chai Sim"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Tan and Sim.,? \\Q2016\\E", "shortCiteRegEx": "Tan and Sim.", "year": 2016}, {"title": "Axnn: Energyefficient neuromorphic systems using approximate computing", "author": ["Swagath Venkataramani", "Ashish Ranjan", "Kaushik Roy", "Anand Raghunathan"], "venue": "In Proceedings of the 2014 International Symposium on Low Power Electronics and Design,", "citeRegEx": "Venkataramani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venkataramani et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 8, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al.", "startOffset": 71, "endOffset": 89}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al.", "startOffset": 90, "endOffset": 108}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al.", "startOffset": 90, "endOffset": 166}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al. (2014)) to alleviate communication overheads are representative examples.", "startOffset": 90, "endOffset": 202}, {"referenceID": 3, "context": "Parallelization strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), techniques such as asynchronous SGD (Dean et al. (2012)) and 1-bit SGD (Seide et al. (2014)) to alleviate communication overheads are representative examples. The next set of efforts design specialized hardware accelerators to realize DNNs, trading off programmability, the cost of specialized hardware and design effort for efficiency. A spectrum of architectures ranging from low-power IP cores to large-scale systems have been proposed (Farabet et al. (2011); Chen et al.", "startOffset": 90, "endOffset": 572}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi).", "startOffset": 8, "endOffset": 27}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al.", "startOffset": 8, "endOffset": 262}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)).", "startOffset": 8, "endOffset": 293}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al.", "startOffset": 8, "endOffset": 400}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)).", "startOffset": 8, "endOffset": 478}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al.", "startOffset": 8, "endOffset": 742}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al.", "startOffset": 8, "endOffset": 762}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al. (2014); Venkataramani et al.", "startOffset": 8, "endOffset": 781}, {"referenceID": 3, "context": "(2011); Chen et al. (2014); Jouppi). The third set of efforts focus on developing new device technologies whose characteristics intrinsically match the computational primitives in neural networks, leading to improvements in energy efficiency (Liu et al. (2015b); Ramasubramanian et al. (2014)). The final set of efforts exploit the fact that DNNs are typically over-parametrized (Denil et al. (2013)) due to the non-convex nature of the optimization space (Hinton et al. (2012)). Therefore, they approximate DNNs by statically pruning network connections, representing weights with reduced bit precision and/or in a compressed format, thereby improving compute efficiency for a negligible loss in classification accuracy (LeCun et al. (1989); Han et al. (2015b); Liu et al. (2014); Venkataramani et al. (2014); Anwar et al.", "startOffset": 8, "endOffset": 810}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015); Tan & Sim (2016)).", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015); Tan & Sim (2016)).", "startOffset": 8, "endOffset": 46}, {"referenceID": 14, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 4, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al.", "startOffset": 102, "endOffset": 120}, {"referenceID": 4, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al. (2016)) are representative examples.", "startOffset": 102, "endOffset": 236}, {"referenceID": 4, "context": "Different work distribution strategies such as model, data and hybrid parallelism (Krizhevsky (2014); Das et al. (2016)), and hardware transparent on-chip memory allocation/management schemes such as virtualized DNNs (Rhu et al. (2016)) are representative examples. The second class of efforts design specialized hardware accelerators that realize the key computation kernels in DNNs. A range of architectures targeting low-power mobile devices (Farabet et al. (2011)) to high-performance server clusters (Chen et al.", "startOffset": 102, "endOffset": 468}, {"referenceID": 4, "context": "(2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored.", "startOffset": 45, "endOffset": 64}, {"referenceID": 4, "context": "(2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored. The third set of efforts investigate new device technologies whose characteristics intrinsically match the compute primitives present in DNNs. Memristor-based crossbar array architectures (Liu et al. (2015b)) and spintronic neuron designs (Ramasubramanian et al.", "startOffset": 45, "endOffset": 301}, {"referenceID": 4, "context": "(2011)) to high-performance server clusters (Chen et al. (2014); Jouppi) have been explored. The third set of efforts investigate new device technologies whose characteristics intrinsically match the compute primitives present in DNNs. Memristor-based crossbar array architectures (Liu et al. (2015b)) and spintronic neuron designs (Ramasubramanian et al. (2014)) are representative examples.", "startOffset": 45, "endOffset": 363}, {"referenceID": 13, "context": "the model size of DNNs by using mechanisms such as pruning connections (LeCun et al. (1989); Han et al.", "startOffset": 72, "endOffset": 92}, {"referenceID": 7, "context": "(1989); Han et al. (2015b); Liu et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 7, "context": "(1989); Han et al. (2015b); Liu et al. (2014)), reducing the precision of computations (Venkataramani et al.", "startOffset": 8, "endOffset": 46}, {"referenceID": 7, "context": "(1989); Han et al. (2015b); Liu et al. (2014)), reducing the precision of computations (Venkataramani et al. (2014); Anwar et al.", "startOffset": 8, "endOffset": 116}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)).", "startOffset": 8, "endOffset": 93}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network.", "startOffset": 8, "endOffset": 180}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold.", "startOffset": 8, "endOffset": 388}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al.", "startOffset": 8, "endOffset": 607}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation.", "startOffset": 8, "endOffset": 632}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function.", "startOffset": 8, "endOffset": 757}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency.", "startOffset": 8, "endOffset": 954}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time. Dynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN.", "startOffset": 8, "endOffset": 1762}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time. Dynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN. Along similar lines, Ba & Frey (2013) propose Standout, where the dropout probability of each neuron is estimated using a binary belief network.", "startOffset": 8, "endOffset": 1862}, {"referenceID": 0, "context": "(2014); Anwar et al. (2015)), and storing weights in a compressed format (Han et al. (2015a)). For example, in the context of fully connected layers, HashNets ( Chen et al. (2015)) use a hash function to randomly group weights into bins, which share a common parameter value, thereby reducing the number of parameters needed to represent the network. Deep compression (Han et al. (2015a)) attempts to prune connections in the network by adding a regularization term during training, and removing connections with weights below a certain threshold. In the context of convolution layers, Denton et al. (2014); Jaderberg et al. (2014) exploit the linear structure of the network to find a suitable low rank approximation. On the other hand, Liu et al. (2015a) propose sparse convolutional DNNs, wherein almost 90% of the parameters in the kernels are zeroed out by adding a weight sparsity term to the objective function. In contrast, Mathieu et al. (2013) demonstrate that performing convolution in the Fourier domain can yield substantial improvement in efficiency. Finally, /citeDBLP:journals/corr/FigurnovVK15 propose perforated CNNs, in which only a subset of the neurons in a feature are evaluated. The neurons to be evaluated for each feature are determined statically at training time. Dynamic Techniques. Dynamic optimizations adapt the computations that are approximated based on the input currently being processed. Dynamic techniques are more powerful than statically optimised DNNs, as they can capture additional input-dependent opportunities for efficiency that static methods lack. Notwithstanding this, very little focus has been devoted to developing dynamic DNN approximation techniques. One of the first efforts in this direction (Bengio (2013)), utilizes stochastic neurons to gate regions within the DNN. Along similar lines, Ba & Frey (2013) propose Standout, where the dropout probability of each neuron is estimated using a binary belief network. The dropout mask is computed for the network in one shot, conditioned on the input to the network. Bengio et al. (2015) extends a similar idea, wherein the dropout distribution of each layer is computed based on the output of the preceding layer.", "startOffset": 8, "endOffset": 2089}, {"referenceID": 15, "context": "We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al.", "startOffset": 119, "endOffset": 137}, {"referenceID": 15, "context": "We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al. (2012)), Overfeat-accurate (Sermanet et al.", "startOffset": 119, "endOffset": 177}, {"referenceID": 15, "context": "We used the following 5 DNN benchmarks in our experiments: CIFAR-10 Caffe network (BVLC (b)) for the CIFAR-10 dataset (Krizhevsky (2009)), and AlexNet (Krizhevsky et al. (2012)), Overfeat-accurate (Sermanet et al.), VGG-16 (Simonyan & Zisserman (2014)), and compressed AlexNet (Han et al.", "startOffset": 119, "endOffset": 252}, {"referenceID": 12, "context": "), VGG-16 (Simonyan & Zisserman (2014)), and compressed AlexNet (Han et al. (2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al.", "startOffset": 65, "endOffset": 84}, {"referenceID": 8, "context": "(2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al. (2009)).", "startOffset": 48, "endOffset": 67}, {"referenceID": 8, "context": "(2015a)) for the ImageNet ILSVRC 2012 data set (Deng et al. (2009)). The inputs for the ImageNet dataset are generated by using a 224\u00d7 224 center crop of the images in the test set. We randomly selected 5% of the test inputs and used it as a validation set to tune the hyper parameters. We report speedup and classification accuracy results on the remaining 95% of the test inputs. Performance Measurement. We implemented DyVEDeep in C++ within the Caffe deep learning framework (Jia et al. (2014)).", "startOffset": 48, "endOffset": 498}], "year": 2017, "abstractText": "Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks \u2014 one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weightcompressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1\u00d7-2.6\u00d7 reduction in the number of scalar operations, which translates to 1.8\u00d7-2.3\u00d7 performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.", "creator": "LaTeX with hyperref package"}}}