{"id": "1704.03952", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Apr-2017", "title": "Virtual to Real Reinforcement Learning for Autonomous Driving", "abstract": "reinforcement learning is often as well promising direction for driving policy learning. however, apt test model effective with reinforcement learning in computational environment involves context - adaptive trial - self - error. it is more desirable where first train in designing social interaction and then efficiently involving the real users. in laboratory example, respondents propose having robust realistic smart network to make model trained in fantasy environment modeling workable in real world. hence proposed network can contain non - realistic virtual image input into perfect reality hybrid with similar scene structure. given realistic autonomous technology options, driving policy assessment and reinforcement learning can nicely adapt further modelling terrain driving. reports show that understanding intuitive virtual to real ( vr ) reinforcement system ( ev ) works pretty well. to our believe, above is the first successful case of driving policy trained by reinforcement learning that people introduce to familiar world driving data.", "histories": [["v1", "Thu, 13 Apr 2017 00:03:40 GMT  (1543kb,D)", "http://arxiv.org/abs/1704.03952v1", null], ["v2", "Tue, 9 May 2017 08:09:40 GMT  (4942kb,D)", "http://arxiv.org/abs/1704.03952v2", null], ["v3", "Thu, 11 May 2017 16:56:54 GMT  (2131kb,D)", "http://arxiv.org/abs/1704.03952v3", null], ["v4", "Tue, 26 Sep 2017 17:22:04 GMT  (8581kb,D)", "http://arxiv.org/abs/1704.03952v4", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["yurong you", "xinlei pan", "ziyan wang", "cewu lu"], "accepted": false, "id": "1704.03952"}, "pdf": {"name": "1704.03952.pdf", "metadata": {"source": "CRF", "title": "Virtual to Real Reinforcement Learning for Autonomous Driving", "authors": ["Yurong You", "Xinlei Pan", "Ziyan Wang", "Cewu Lu"], "emails": ["yurongyou@sjtu.edu.cn", "xinleipan@berkeley.edu", "zy-wang13@mails.tsinghua.edu.cn", "lu-cw@cs.sjtu.edu.cn"], "sections": [{"heading": "1. Introduction", "text": "Autonomous driving aims to make a vehicle sense its environment and navigate without human input. To achieve this goal, the most important task is to learn the driving policy that automatically outputs control signals for steering wheel, throttle, brake, etc, based on observed surroundings. The straight-forward idea is end-to-end supervised learning [3, 4], which trains a neural network model mapping visual input directly to action output, and the training data is labeled image-action pairs. However, supervised approach learns driving skills in a short-sighted manner, which means action learning is solely based on current observation. By\n\u2217Indicates equal contribution\ncontrast, human drivers attempt to predict what\u2019s happening in the near future and plan their actions. Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16]. In other words, reinforcement learning is smarter than supervised approach, in that it acquires the driving skill by maximizing long-term reward instead of focusing on the short-term benefits.\nHowever, reinforcement learning learns in a trial-anderror fashion and undesirable driving actions would happen at early stage. Training autonomous cars in real world will cause damages to vehicles and the surroundings. Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.\nIn this paper, we propose a realistic translation network to tackle this problem. Our proposed network (shown as Figure 1) converts virtual image rendered by simulator to a realistic one. Though virtual and realistic images have different characteristics, they share a common scene parsing representation (segmentation map of road, vehicle etc.). Therefore, our realistic translation network makes use of scene parsing representation as the interim to achieve translation from virtual to realistic image. This insight is similar to natural language translation, where semantic meaning is the interim between different languages. Specifically, our realistic translation network includes two modules. The first one is a virtue-to-parsing or virtual-to-segmentation module that pursues a scene parsing representation of input virtual image. The second one is a parsing-to-real network to translate scene parsing representations into realistic images. With realistic translation network, reinforcement learning learnt on the realistic driving video can nicely apply to real world driving.\nar X\niv :1\n70 4.\n03 95\n2v 1\n[ cs\n.A I]\n1 3\nA pr\nTo demonstrate the effectiveness of our method, we trained our reinforcement learning model by using the realistic translation network to filter virtual images to synthetic real images and feed these real images as state inputs. We further trained another two models for comparison. The first model is also a reinforcement learning model, however, with only virtual image input, and has never seen real images before. The second model is a simple supervised learning model that maps real images to their corresponding actions. Our experiments illustrate that reinforcement learning model trained with translated real images has significantly better performance than reinforcement learning model trained with only virtual input, and has similar or close performance compared with supervised learning model which requires lots of labeled data samples for training.\nOur method has several contributions: Novel Virtual to Real Image Translation Network. We propose a novel virtual to real image translation network that first segment images into their semantic labels and then translate segmented images to real images using network trained on segmentation-real image pairs.\nTrain Autonomous Vehicle with Real Data as Prior Information. Instead of solely relying on simulation data, we propose to use real image-segmentation pairs to train an image-to-image translation network that can translate segmented images into real images. Besides, the real data are not annotated with corresponding actions which means our method does not require real world interactions or annotations which are often expensive to acquire."}, {"heading": "2. Related Work", "text": "Supervised Learning for Autonomous Driving. Supervised learning methods are obviously straightforward way to train autonomous vehicles. ALVINN [20] provides an early example of using neural network for autonomous driving. Their model is simple and direct, which maps image inputs to action predictions with a shallow network. Pow-\nered by deep learning especially convolutional neural network, NVIDIA [3] recently provides an attempt to leverage driving video data for simple lane following task. Another work by [4] learns a mapping between input images to a number of key perception indicators, which are closely related to the affordance of a driving state. However, the learned affordance must be associated with actions through hand-engineered rules. These supervised methods works relatively well in simple tasks such as lane-following and driving on highway. On the other hand, imitation learning can also be regarded as supervised learning approach [33], where the agent observes the demonstrations performed by some expert and learns to imitate the action of the expert. However, an intrinsic shortcoming of the imitation learning algorithm is that it can not generalize very well. There is also the covariant shift problem in imitation learning [21].\nReinforcement Learning for Autonomous Driving. Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26]. One of the challenges in practical real-world applications of reinforcement learning is the high-dimensionality of state space as well as the non-trivial large action range. Developing an optimal policy over such high-complexity space is time consuming. Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18]. However, both deep Q-learning method [19] and policy gradient method [16] require the agent to interact with the environment to get reward and feedback. It is unrealistic to train autonomous vehicle with reinforcement learning in a real environment because of the potential traffic accidents risk.\nReinforcement Learning in the Wild. Performing reinforcement learning with a car driving simulator and transfer learned models to real environment could enable faster, lower-cost training, and it is much safer than training with a real car. However, real-world driving challenge usually\nspans a diverse range, and it is often significantly different from the training environment in a car driving simulator in terms of their visual appearance. Models trained purely on virtual data do not generalize well to real images [7, 28]. Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27]. These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27]. The work of [22] proposes to use progressive network to transfer network weights from model trained on virtual data to the real environment and then fine-tune the model in a real setting. The training time in real environment has been greatly reduced by first training in virtual environment. However, it is still necessary to train the agent in the real environment, thus it does not solve the critical problem of avoiding risky trial-and-error in real world. Methods that try to learn an alignment between virtual image and real image could fail to generalize to more complex scenarios, especially when it is hard to find a good alignment between virtual image and real image. As a more recent work, [23] proposed a new framework for training a reinforcement learning agent with only virtual environment. Their work proved the possibility of performing collisionfree flight in real world with training in 3D CAD model simulator. However, as mentioned in the conclusion of their paper [23], the manual engineering work to design suitable training environments is nontrivial, and it is more reasonable to attain better results by combining simulated training with some real data, though it is unclear from their paper how to combine real data with simulated training.\nImage Synthesis and Image Translation. Image translation aims to predict image in some specific modality, given an image from another modality. This can be treated as a generic method as it predicts pixel from pixel. Recently, the community has made significant progress in generative approaches, mostly based on generative adversarial networks [10]. To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style. More recently, the work of [12] developed a general and simple framework for image-to-image translation which can handle various pixel level generative tasks like semantic segmentation, colorization, rendering edge maps, etc.\nScene Parsing. One part of our network is the semantic image segmentation network. There are already many great work in the field of semantic image segmentation. Many of them are based on deep convolutional neural network or fully convolutional neural network [17]. In order to get bet-\nter quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance. Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5]. In this paper, we use the SegNet for image segmentation, the structure of the network is reveal in [2], which is composed of two main parts. The first part is an encoder, which consists of Convolutional, Batch Normalization, ReLU and max pooling layers. The second part is a decoder, which replace the pooling layers with upsampling layers.\nIn our work, we do not use a single image translation network nor a single image segmentation network directly as it is hard to get a large amount of virtual-real image pairs, especially given the infinite possibility of real world images. Instead, we train two networks both end-to-end and then connect them together as a virtual-real translator. The image segmentation network will be used to transform virtual world images into their segmentations, and the image translation network will be used to translate segmented images into real world images."}, {"heading": "3. Reinforcement Learning on Realistic", "text": "Frames\nWe aim to successfully apply a trained agent in virtual environment into real-world driving. One of major gaps is that what RL observed is frames rendered by simulator. These frames is different with real frame captured by camera in terms of appearance. Therefore, we proposed a realistic translation network to convert virtual frames to realistic ones. Inspired by the work of pix2pix network [12], our network includes two modules, namely virtual-to-parsing and parsing-to-realistic network. The first one maps virtual frame to scene parsing image. The second one translates scene parsing to realistic frame with similar scene structure as input virtual frame. These two modules achieve realistic frames and maintain the scene structure of input virtual frames. The architecture of realistic translation network is illustrated on Figure 1.\nFinally, we train a self-driving agent using reinforcement learning method on realistic frames obtained by realistic translation network. The approach we adopted is developed by [18], where they use the asynchronous actor-critic reinforcement learning algorithm to train a self-driving vehicle in the car racing simulator TORCS [31]. In this section, we will first present proposed realistic translation network and then discuss how to train driving agent under a reinforcement learning framework."}, {"heading": "3.1. Realistic Translation Network", "text": "As there is no direct connection between virtual world images such as the images we see in the TORCS environment, and the real world images such as the data in [3], a\ndirect mapping from virtual world image to real world image would be awkward. However, these two type of images both express driving scene. We can translate them by scene parsing representation. Therefore, our proposed realistic translation network attempts to build the connection by scene parsing on virtual image first and then translate scene parsing representation into realistic image."}, {"heading": "3.2. Training Framework", "text": "We present how to train two main modules as follows. Virtual-to-Parsing Network. This network aims to convert the virtual frames rendered by TORCS into scene\nparsing result. We use the network of [2] to parse the image into different semantic regions. The network structure of SegNet is a deep fully convolutional neural network architecture that follows a encoder-decoder fashion. To use the SegNet to segment images in virtual world, the network will be trained on CityScape driving scene segmentation dataset [8]. Then the trained model is used to parse virtual scenes in TORCS [31].\nParsing-to-Realistic Network. To produce realistic images, we learn how to translate the scene parsing result into realistic images with a unchanged scene structure. To this end, architecture of [12] is adopted. We train the model on\nthe dataset of [6] with scene segmentation and natural image pairs. The scene segmentations are also obtained using [2].\n3.3. Reinforcement Learning for Training a SelfDriving Vehicle\nWe use a conventional RL solver Asynchronous Advantage Actor-Critic (A3C)[18] to train the self driving vehicle, which has performed well on various machine learning tasks. A3C algorithm is a fundamental Actor-Critic algorithm that combines several classic reinforcement learning algorithms with the idea of asynchronous parallel threads. Multiple threads run at the same time with unrelated copies of the environment, generating their own sequences of training samples. Those actors-learners proceed as though they are exploring different parts of the unknown space. For one thread, parameters are synchronized before an iteration of learning and updated after finishing it. When the algorithm is applied to our experiment, we define a reward function proportional to the agent\u2019s velocity along the center of the track at the agent\u2019s current position [18]."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Experiments Setting", "text": "To demonstrate the performance of our method, we trained three models. The first one is our proposed reinforcement learning model with realistic translation network. The second one is a reinforcement learning model with virtual input as state representation. The third one is a supervised learning end-to-end model trained on real data with action labels. We further evaluate these models on a held out real driving data with action labels."}, {"heading": "4.2. Dataset", "text": "The virtual image data are collected in the aalborg environment in TORCS [31], and a total 1673 images are collected which covers the entire driving cycle of aalborg environment. The real driving video data with action labels are from [6], which is collected in a sunny day on highway with detailed steering angle annotations per frame."}, {"heading": "4.3. Training Details", "text": "Scene Segmentation. We adopt the image semantic segmentation network design of [2] and their trained segmentation network on the CityScape image segmentation dataset [8] to segment both virtual images rendered by TORCS and real images from [6]. The network was trained on the CityScape dataset with 11 classes and was trained with 30000 iterations. We used the Aalborg environment in TORCS [31], and collected 1673 images from this environment as well as their segmentations. We further segmented all 45569 images in [6].\nImage Translation Network Training. We trained both virtual-to-parsing and parsing-to-real network using the segmented virtual-segmentation image pairs and segmentation-real image pairs. The translation network are of a encoder-decoder fashion as shown in figure 1. In the image translation network, we used U-Net architecture with skip connection to connect two separate layers from encoder and decoder respectively, which have the same output feature map shape. The input size of the generator is 256 \u00d7 256. Each convolutional layer has a kernel size of 4 \u00d7 4 and striding size of 2. LeakyReLU is applied after every convolutional layer with a slope of 0.2 and ReLU is applied after every deconvolutional layer. In addition, batch normalization layer is applied after every convolutional and deconvolutional layer. The final output of the encoder is connected with a convolutional layer which yields output of shape 3 \u00d7 256 \u00d7 256 followed by Tanh. We used all 1673 virtual-segmentation image pairs to train a virtual to segmentation network. As there are redundancies in the 45k real images, we select 1762 images from the 45k images to train a parsing-to-real image translation network. To train the image translation models, we used the Adam optimizer with an initial learning rate of 0.0002, momentum of 0.5, batchsize of 16, and 200 iterations until convergence.\nReinforcement Training. The network structure used in our training is similar to that of [18] where the actor network is a 4-layer convolutional network with ReLU activation functions in-between. The network takes in 4 consecutive RGB frames as state input and output 9 discrete actions which corresponds to \u201cgo straight with acceleration\u201d, \u201cgo left with acceleration\u201d, \u201cgo right with acceleration\u201d, \u201cgo straight and brake\u201d, \u201cgo left and brake\u201d, \u201cgo right and brake\u201d, \u201cgo straight\u201d, \u201cgo left\u201d, and \u201cgo right\u201d. We trained the reinforcement learning agent with 12 asynchronous threads, and with the RMSProp optimizer at an initial learning rate of 0.01, \u03b3 = 0.9, and = 0.1.\nBaseline Reinforcement Learning. To make a fair comparison between our method and reinforcement learning with only virtual input, we trained the vehicle in the virtual car racing simulator TORCS [31] with virtual image as input using the same reinforcement learning framework as in our method. We later show that performance improves to some extent by training with translated realistic images.\nSupervised Learning Method. We further trained a neural network mapping real images to learn driving actions under supervised learning manner. The network architecture is the same as the the policy network in our proposed method. The input of the network is a sequence of four consecutive images, the output of the network is the action probability vector, and the elements in the vector represent the probability of going straight, turning left and turning right.\nEvaluation. The original dataset [6] provides the steer-\ning angle annotations per frame. However, the actions performed in the TORCS virtual environment only contain \u201dgoing left\u201d, \u201dgoing right\u201d, and \u201dgoing straight\u201d or their combinations with \u201dacceleration\u201d or \u201dbrake\u201d. Therefore, we define a label mapping strategy to translate steering angle labels to action labels in the virtual simulator. We relate steering angle in (\u221210, 10) to action \u201dgoing straight\u201d (since small steering angle is not able to result in a distinct turning\nin a short time), steering angle less than \u221210 to action \u201dgoing left\u201d and steering angle more than 10 to action \u201dgoing right\u201d."}, {"heading": "5. Results", "text": ""}, {"heading": "5.1. Image Segmentation Results", "text": "We used image segmentation model trained on the cityscape [8] dataset to segment both virtual and real images. Examples are shown in figure 2. As shown in the figure, although the original virtual image and real image look quite different, their scene parsing results are very similar. Therefore, it is reasonable to use scene parsing as the interim to connect virtual image and real image."}, {"heading": "5.2. Qualitative Result of Realistic Translation Network", "text": "Figure 3 shows some representative results of our image translation network. The odd columns are virtual image in TORCS, and the even columns are translated images in the real world. The images in the virtual environment appears to be darker than the translated images, as the real images used to train the translation network is captured in a sunny day. Therefore, our model succeed to synthesize realistic images with similar appearance with the original ground truth real images."}, {"heading": "5.3. Reinforcement Training Results", "text": "We first trained the self driving vehicle with our model and got the reward per iteration curve shown in figure 4. We further trained the self driving vehicle in the virtual environment with the baseline method and got the reward per iteration curve shown in figure 4. The two curves show that by interacting with translated realistic images, our model can achieve similar reward level as model trained solely in virtual environment.\nWe further provide the evaluation results of our proposed method (Ours), the baseline method (BS), and the supervised learning method (SV). The results contain action prediction accuracy shown in table 1. Results show that our proposed method has a better overall performance than the baseline method, where the reinforcement training agent is trained in a virtual environment without seeing any real data. The supervised method has the best overall performance, however, was trained with supervised labeled data."}, {"heading": "6. Conclusion", "text": "We proved that by using synthetic real images as training data in reinforcement learning, the agent generalize better in a real environment than pure training with virtual data. The\nnext step would be to design a better image-to-image translation network and a better reinforcement learning framework to surpass the performance of supervised learning.\nThanks to the bridge of scene parsing, virtual images can be translated into realistic images which maintains its scene structure. The learnt RL model on realistic frames can be easily applied to real-world environment. We also notice that the translation results of a segmentation map is not unique. For example, segmentation map indicates a car, but it does not assign which color of that car should be. Therefore, one of our future work is to make parsing-torealistic network output various possible appearances (e.g. color, texture). In this way, bias in RL training would be largely reduced.\nWe provide the first example of training a self-driving vehicle using reinforcement learning algorithm by interacting with a synthesized real environment with our proposed image-to-segmentation -to-image framework. We show that by training in this environment, it is possible to train a self driving vehicle that can be placed in the real world."}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems, 19:1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba"], "venue": "CoRR, abs/1604.07316", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepdriving: Learning affordance for direct perception in autonomous driving", "author": ["C. Chen", "A. Seff", "A.L. Kornhauser", "J. Xiao"], "venue": "CoRR, abs/1505.00256", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "atrous convolution, and fully connected crfs. arXiv:1606.00915", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer from simulation to real world through learning deep inverse dynamics model", "author": ["P. Christiano", "Z. Shah", "I. Mordatch", "J. Schneider", "T. Blackwell", "J. Tobin", "P. Abbeel", "W. Zaremba"], "venue": "arXiv preprint arXiv:1610.03518", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CoRR, abs/1604.01685", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning cpg-based biped locomotion with a policy gradient method: Application to a humanoid robot", "author": ["G. Endo", "J. Morimoto", "T. Matsubara", "J. Nakanishi", "G. Cheng"], "venue": "The International Journal of Robotics Research, 27(2):213\u2013228", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672\u20132680. Curran Associates, Inc.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning invariant feature spaces to transfer skills with reinforcement learning", "author": ["A. Gupta", "C. Devin", "Y. Liu", "P. Abbeel", "S. Levine"], "venue": "arXiv preprint arXiv:1703.02949", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2017}, {"title": "Image-to-image translation with conditional adversarial networks", "author": ["P. Isola", "J. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CoRR, abs/1611.07004", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy gradient reinforcement learning for fast quadrupedal locomotion", "author": ["N. Kohl", "P. Stone"], "venue": "Robotics and Automation, 2004. Proceedings. ICRA\u201904. 2004 IEEE International Conference on, volume 3, pages 2619\u20132624. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "Evolving large-scale neural networks for vision-based reinforcement learning", "author": ["J. Koutn\u0131\u0301k", "G. Cuccu", "J. Schmidhuber", "F. Gomez"], "venue": "In Proceedings of the 15th annual conference on Genetic and evolutionary computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["A.B.L. Larsen", "S.K. S\u00f8nderby", "O. Winther"], "venue": "CoRR, abs/1512.09300", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "CoRR, abs/1602.01783", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Alvinn", "author": ["D.A. Pomerleau"], "venue": "an autonomous land vehicle in a neural network. Technical report, Carnegie Mellon University, Computer Science Department", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1989}, {"title": "Efficient reductions for imitation learning", "author": ["S. Ross", "D. Bagnell"], "venue": "AISTATS, volume 3, pages 3\u20135", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Sim-to-real robot learning from pixels with progressive nets", "author": ["A.A. Rusu", "M. Vecerik", "T. Roth\u00f6rl", "N. Heess", "R. Pascanu", "R. Hadsell"], "venue": "CoRR, abs/1610.04286", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "cad)$\u02c62$rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "CoRR, abs/1611.04201", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Endto-end deep reinforcement learning for lane keeping assist", "author": ["A.E. Sallab", "M. Abdou", "E. Perot", "S. Yogamani"], "venue": "arXiv preprint arXiv:1612.04340", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Abbeel", "M.I. Jordan", "P. Moritz"], "venue": "ICML, pages 1889\u20131897", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Safe", "author": ["S. Shalev-Shwartz", "S. Shammah", "A. Shashua"], "venue": "multi-agent, reinforcement learning for autonomous driving. CoRR, abs/1610.03295", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Domain randomization for transferring deep neural networks from simulation to the real world", "author": ["J. Tobin", "R. Fong", "A. Ray", "J. Schneider", "W. Zaremba", "P. Abbeel"], "venue": "arXiv preprint arXiv:1703.06907", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2017}, {"title": "Adapting deep visuomotor representations with weak pairwise constraints", "author": ["E. Tzeng", "C. Devin", "J. Hoffman", "C. Finn", "P. Abbeel", "S. Levine", "K. Saenko", "T. Darrell"], "venue": "Workshop on the Algorithmic Foundations of Robotics (WAFR)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative image modeling using style and structure adversarial networks", "author": ["X. Wang", "A. Gupta"], "venue": "ECCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "author": ["J. Wu", "C. Zhang", "T. Xue", "W.T. Freeman", "J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, pages 82\u201390", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Torcs", "author": ["B. Wymann", "E. Espi\u00e9", "C. Guionneau", "C. Dimitrakakis", "R. Coulom", "A. Sumner"], "venue": "the open racing car simulator. Software available at http://torcs. sourceforge. net", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2000}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Query-efficient imitation learning for end-to-end autonomous driving", "author": ["J. Zhang", "K. Cho"], "venue": "arXiv preprint arXiv:1605.06450", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "The straight-forward idea is end-to-end supervised learning [3, 4], which trains a neural network model mapping visual input directly to action output, and the training data is labeled image-action pairs.", "startOffset": 60, "endOffset": 66}, {"referenceID": 3, "context": "The straight-forward idea is end-to-end supervised learning [3, 4], which trains a neural network model mapping visual input directly to action output, and the training data is labeled image-action pairs.", "startOffset": 60, "endOffset": 66}, {"referenceID": 22, "context": "Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16].", "startOffset": 141, "endOffset": 153}, {"referenceID": 24, "context": "Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16].", "startOffset": 141, "endOffset": 153}, {"referenceID": 14, "context": "Recently, reinforcement learning has been considered as a promising technique to learn driving policy due to its expertise in action planing [24, 26, 16].", "startOffset": 141, "endOffset": 153}, {"referenceID": 16, "context": "Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.", "startOffset": 65, "endOffset": 77}, {"referenceID": 24, "context": "Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.", "startOffset": 65, "endOffset": 77}, {"referenceID": 14, "context": "Therefore, most of research is still at the stage of simulations [18, 26, 16], which fails to meet the ultimate expectation of driving in real world, since the appearance of virtual environment is different from real world scene.", "startOffset": 65, "endOffset": 77}, {"referenceID": 18, "context": "ALVINN [20] provides an early example of using neural network for autonomous driving.", "startOffset": 7, "endOffset": 11}, {"referenceID": 2, "context": "Powered by deep learning especially convolutional neural network, NVIDIA [3] recently provides an attempt to leverage driving video data for simple lane following task.", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "Another work by [4] learns a mapping between input images to a number of key perception indicators, which are closely related to the affordance of a driving state.", "startOffset": 16, "endOffset": 19}, {"referenceID": 31, "context": "On the other hand, imitation learning can also be regarded as supervised learning approach [33], where the agent observes the demonstrations performed by some expert and learns to imitate the action of the expert.", "startOffset": 91, "endOffset": 95}, {"referenceID": 19, "context": "There is also the covariant shift problem in imitation learning [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 131, "endOffset": 138}, {"referenceID": 7, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 131, "endOffset": 138}, {"referenceID": 0, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 163, "endOffset": 170}, {"referenceID": 24, "context": "Reinforcement learning has been applied to a wide variety of robotics related tasks, such as computer games [19], robot locomotion [13, 9], and autonomous driving [1, 26].", "startOffset": 163, "endOffset": 170}, {"referenceID": 12, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 23, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 14, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 16, "context": "Recent work in deep reinforcement learning has made great progress in learning in a high dimensional space with the power of deep neural networks [14, 19, 25, 16, 18].", "startOffset": 146, "endOffset": 166}, {"referenceID": 17, "context": "However, both deep Q-learning method [19] and policy gradient method [16] require the agent to interact with the environment to get reward and feedback.", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "However, both deep Q-learning method [19] and policy gradient method [16] require the agent to interact with the environment to get reward and feedback.", "startOffset": 69, "endOffset": 73}, {"referenceID": 5, "context": "Models trained purely on virtual data do not generalize well to real images [7, 28].", "startOffset": 76, "endOffset": 83}, {"referenceID": 26, "context": "Models trained purely on virtual data do not generalize well to real images [7, 28].", "startOffset": 76, "endOffset": 83}, {"referenceID": 20, "context": "Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27].", "startOffset": 137, "endOffset": 149}, {"referenceID": 9, "context": "Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27].", "startOffset": 137, "endOffset": 149}, {"referenceID": 25, "context": "Recent progress of transfer and domain adaptation learning in robotics has provide examples of simulation-to-real reinforcement training [22, 11, 27].", "startOffset": 137, "endOffset": 149}, {"referenceID": 20, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 238, "endOffset": 242}, {"referenceID": 21, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 335, "endOffset": 343}, {"referenceID": 25, "context": "These models either first train a model in virtual environment and then fine-tune in the real environment [22], or learn an alignment between virtual image and real image by finding representations that are shared between the two domains [28], or use randomized rendered virtual environments to train and then test in real environment [23, 27].", "startOffset": 335, "endOffset": 343}, {"referenceID": 20, "context": "The work of [22] proposes to use progressive network to transfer network weights from model trained on virtual data to the real environment and then fine-tune the model in a real setting.", "startOffset": 12, "endOffset": 16}, {"referenceID": 21, "context": "As a more recent work, [23] proposed a new framework for training a reinforcement learning agent with only virtual environment.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "However, as mentioned in the conclusion of their paper [23], the manual engineering work to design suitable training environments is nontrivial, and it is more reasonable to attain better results by combining simulated training with some real data, though it is unclear from their paper how to combine real data with simulated training.", "startOffset": 55, "endOffset": 59}, {"referenceID": 8, "context": "Recently, the community has made significant progress in generative approaches, mostly based on generative adversarial networks [10].", "startOffset": 128, "endOffset": 132}, {"referenceID": 28, "context": "To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "To name a few, the work of [30] explored the use of VAE-GAN [15] in generating 3D voxel models, and the work of [29] proposed a cascade GAN to generate natural image by structure and style.", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "More recently, the work of [12] developed a general and simple framework for image-to-image translation which can handle various pixel level generative tasks like semantic segmentation, colorization, rendering edge maps, etc.", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Many of them are based on deep convolutional neural network or fully convolutional neural network [17].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "In order to get better quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance.", "startOffset": 61, "endOffset": 64}, {"referenceID": 30, "context": "In order to get better quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance.", "startOffset": 145, "endOffset": 152}, {"referenceID": 4, "context": "In order to get better quality segmented images, the work of [2] cut down the downsampling layers to avoid resolution reduction, and the work of [32, 5] uses dilated convolution to improve performance.", "startOffset": 145, "endOffset": 152}, {"referenceID": 1, "context": "Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 15, "context": "Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 4, "context": "Bi-linear interpolation and deconvolutional methods are also very popular these days such as [2, 17, 5].", "startOffset": 93, "endOffset": 103}, {"referenceID": 1, "context": "In this paper, we use the SegNet for image segmentation, the structure of the network is reveal in [2], which is composed of two main parts.", "startOffset": 99, "endOffset": 102}, {"referenceID": 10, "context": "Inspired by the work of pix2pix network [12], our network includes two modules, namely virtual-to-parsing and parsing-to-realistic network.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "The approach we adopted is developed by [18], where they use the asynchronous actor-critic reinforcement learning algorithm to train a self-driving vehicle in the car racing simulator TORCS [31].", "startOffset": 40, "endOffset": 44}, {"referenceID": 29, "context": "The approach we adopted is developed by [18], where they use the asynchronous actor-critic reinforcement learning algorithm to train a self-driving vehicle in the car racing simulator TORCS [31].", "startOffset": 190, "endOffset": 194}, {"referenceID": 2, "context": "As there is no direct connection between virtual world images such as the images we see in the TORCS environment, and the real world images such as the data in [3], a", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "We use the network of [2] to parse the image into different semantic regions.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "To use the SegNet to segment images in virtual world, the network will be trained on CityScape driving scene segmentation dataset [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 29, "context": "Then the trained model is used to parse virtual scenes in TORCS [31].", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "To this end, architecture of [12] is adopted.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "The scene segmentations are also obtained using [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 16, "context": "We use a conventional RL solver Asynchronous Advantage Actor-Critic (A3C)[18] to train the self driving vehicle, which has performed well on various machine learning tasks.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "When the algorithm is applied to our experiment, we define a reward function proportional to the agent\u2019s velocity along the center of the track at the agent\u2019s current position [18].", "startOffset": 176, "endOffset": 180}, {"referenceID": 29, "context": "The virtual image data are collected in the aalborg environment in TORCS [31], and a total 1673 images are collected which covers the entire driving cycle of aalborg environment.", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "We adopt the image semantic segmentation network design of [2] and their trained segmentation network on the CityScape image segmentation dataset [8] to segment both virtual images rendered by TORCS and real images from [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "We adopt the image semantic segmentation network design of [2] and their trained segmentation network on the CityScape image segmentation dataset [8] to segment both virtual images rendered by TORCS and real images from [6].", "startOffset": 146, "endOffset": 149}, {"referenceID": 29, "context": "We used the Aalborg environment in TORCS [31], and collected 1673 images from this environment as well as their segmentations.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "The network structure used in our training is similar to that of [18] where the actor network is a 4-layer convolutional network with ReLU activation functions in-between.", "startOffset": 65, "endOffset": 69}, {"referenceID": 29, "context": "To make a fair comparison between our method and reinforcement learning with only virtual input, we trained the vehicle in the virtual car racing simulator TORCS [31] with virtual image as input using the same reinforcement learning framework as in our method.", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "We used image segmentation model trained on the cityscape [8] dataset to segment both virtual and real images.", "startOffset": 58, "endOffset": 61}], "year": 2017, "abstractText": "Reinforcement learning is considered as a promising direction for driving policy learning. However, training autonomous driving vehicle with reinforcement learning in real environment involves non-affordable trial-and-error. It is more desirable to first train in a virtual environment and then transfer to the real environment. In this paper, we propose a novel realistic translation network to make model trained in virtual environment be workable in real world. The proposed network can convert non-realistic virtual image input into a realistic one with similar scene structure. Given realistic frames as input, driving policy trained by reinforcement learning can nicely adapt to real world driving. Experiments show that our proposed virtual to real (VR) reinforcement learning (RL) works pretty well. To our knowledge, this is the first successful case of driving policy trained by reinforcement learning that can adapt to real world driving data.", "creator": "LaTeX with hyperref package"}}}