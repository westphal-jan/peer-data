{"id": "1704.04327", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "Deep API Programmer: Learning to Program with APIs", "abstract": "on present mapping, a font - like - example function that learns to mix with apis to perform data transformation tasks. we project unified compiler - composition language ( dsl ) that allows generates arbitrary descriptions indicating unique numbers indicating constant matching. the dsl consists of different family diagram apis : regular expression - grammar apis, lookup modeling, and transformation apis. we then present a novel synthetic optimization suite to generate for programs in the dsl however are consistent with a random order of examples. past project application is recently accumulated neural architectures to encode input - output calculations as further modify prior program search in another computation. we announce that synthesis algorithm outperforms baseline methods since synthesizing programs, future synthetic and real - estate benchmarks.", "histories": [["v1", "Fri, 14 Apr 2017 02:04:06 GMT  (92kb,D)", "http://arxiv.org/abs/1704.04327v1", "8 pages + 4 pages of supplementary material. Submitted to IJCAI 2017"]], "COMMENTS": "8 pages + 4 pages of supplementary material. Submitted to IJCAI 2017", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["surya bhupatiraju", "rishabh singh", "abdel-rahman mohamed", "pushmeet kohli"], "accepted": false, "id": "1704.04327"}, "pdf": {"name": "1704.04327.pdf", "metadata": {"source": "CRF", "title": "Deep API Programmer: Learning to Program with APIs", "authors": ["Surya Bhupatiraju", "Rishabh Singh", "Pushmeet Kohli"], "emails": ["surya@mit.edu", "risin@microsoft.com", "asamir@microsoft.com", "pkohli@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The ability to discover a program consistent with a given user intent (specification) is considered as one of the central problems in artificial intelligence [Green, 1969]. While significant progress has been made in synthesizing programs in different domains [Alur et al., 2013], current synthesis techniques do not scale to larger and more complex programs. Moreover, the state-of-the-art synthesis techniques [Gulwani et al., 2012] require a great deal of domain expertise with manually designed heuristics and rules to develop an efficient search procedure. In this paper, we present DAPIP, or Deep API Programmer, a system that aims to overcome some of these shortcomings by automatically learning a synthesis algorithm in the domain of data transformation tasks.\nThe process of transforming data from raw data into usable formats (also known as data wrangling) is a key problem faced by data scientists for any data analysis task. Some studies have reported that this data wrangling process can sometimes take up to 80% of the total data analysis time [Dasu and Johnson, 2003; Kandel et al., 2011]. Recently, ProgrammingBy-Example (PBE) techniques such as FlashFill [Gulwani, 2011; Gulwani et al., 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs. These techniques encode the space of programs using\na domain-specific language (DSL), and then develop algorithms based on version-space algebra (VSA) [Polozov and Gulwani, 2015; Lau et al., 2003] to efficiently search the space of programs. There are two key shortcomings of these approaches. First, the DSL is limited to only certain lowlevel syntactic regular expression-based operators that allow for an efficient structuring of search space. This limits the expressiveness of the PBE systems; for example, they do not allow semantic data transformations using arbitrary transformation functions such as obtaining month names from a date or abbreviating the state name in an input address. Second, building an efficient synthesizer using VSA requires a large engineering effort with manually designed heuristic rules.\nWe tackle the first shortcoming by designing DAPIP\u2019s DSL to have function APIs as the core element, which allows for composition of APIs with constant strings. The DSL consists of three kinds of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. The regular expression-based APIs perform a regular expression-based transformation on the input strings, which are needed for syntactic data transformations. The lookup APIs search for a particular string in the input data based on a dictionary of strings, and the transformation APIs perform some transformation on top of a lookup operation based on a predefined mapping between two sets of strings. The lookup and transformation APIs allow for semantic data transformations.\nThe second shortcoming is handled by learning the synthesis algorithm in DAPIP automatically from data using two recently introduced neural modules [Parisotto et al., 2016]. The first module called the cross-correlational encoder computes a fixed-dimension vector representation of the inputoutput examples by using tensor representations obtained by running two bi-directional LSTMs [Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005] on the input and output strings and computing their cross correlation. The second module, the recursive-reverse-recursive neural network, or R3NN, encodes a partial derivation in the DSL and given the example encoding vector, returns a distribution over the space of possible expansions to the partial derivation. The R3NN incrementally builds a program in the DSL that is consistent with the input-output examples. The input-output encoder and the R3NN modules are trained end-to-end using thousands of programs and corresponding input-output examples, which are automatically sampled from the DSL. ar X\niv :1\n70 4.\n04 32\n7v 1\n[ cs\n.A I]\n1 4\nA pr\n2 01\n7\nWe evaluate DAPIP on a set of synthetic and 238 realworld FlashFill benchmarks. Our experiments indicate that our deep learning based approach is able to effectively model and predict the presence of different types of APIs. It is able to solve 45% of the FlashFill benchmarks and significantly outperforms the enumerative search based baseline.\nTo summarize, the key contributions of this paper are: \u2022 We design an expressive DSL with APIs that can encode\nboth syntactic and semantic data transformation tasks. \u2022 We automatically learn a synthesis algorithm for synthe-\nsizing programs in the DSL using neural architectures. \u2022 We evaluate our system DAPIP on 238 real-world Flash-\nFill benchmarks and thousands of synthetic benchmarks."}, {"heading": "2 Motivating Examples", "text": "We present a few real-world examples to motivate the DSL. Example 1. An Excel user wanted to transform names to first initial followed by last name as shown in Figure 1. Since some input examples had optional middle names, the user was struggling to find a macro to perform the desired task.\nDAPIP learns the following program for this task: Concat(GetFirstChar(v),ConstStr(\u201c.\u2032\u2032),GetLastWord(v)). The learned program uses the GetFirstChar and GetLastWord APIs that belong to the class of regex APIs, which extract substrings from the input string based on regular expressions. Example 2. An Excel user had a list of addresses and wanted to extract the city and state values as shown in Figure 2.\nThis is an example of a very common task that can not be performed by systems such as FlashFill. Since the data is in many different formats, there is no consistent regular expression that can be used to extract the city names. Moreover, to obtain the state name, the system needs to use a transform API GetStateFromCity. DAPIP learns the following program:Concat(GetCity(v),ConstStr(\u201c,\u2032\u2032 ), GetStateFromCity(GetCity(v))).\nMore examples of real-world FlashFill tasks can be found in Appendices C and D."}, {"heading": "3 Overview of Approach", "text": "We now present an overview of our end-to-end system that learns to synthesize programs in a DSL that are consistent with a set of examples. The training phase of our system is shown in Figure 4 and the test phase is shown in Figure 3. We first design a DSL that allows for composition of nested API calls with constant strings. We designed this DSL after studying a large family of real-world string transformation tasks so that it is expressive enough to encode these tasks. During the training phase, we use a program sampler to uniformly sample a large number of programs from this DSL. For each program, we use a rule-based approach to construct 5 input strings for the program such that the prerequisites of the program are met. We obtain the output strings by executing the program on the input strings.\nDuring training, each sampled program together with the corresponding input-output examples is used to train the R3NN model, a neural architecture that learns distributions over the expansions in the DSL conditioned on the examples. The examples are encoded using a second neural architecture called the cross-correlational encoder, which produces a fixed-dimensional vector. The R3NN system takes as input the input-output conditioning vector, the DSL, and the training program, and is trained to predict a conditional distribution over the set of DSL expansions. The next expansion is sampled from this conditional distribution, leading to the partial tree, and the procedure repeats; one can observe a potential order of the nodes growing in the respective figures.\nThe trained R3NN model can then be used to synthesize programs in the DSL given a set of examples. The trained model takes the input-output conditioning vector as input, and generates a distribution over the set of DSL expansions that are likely to be the expansions required to construct the desired program. The distribution is then sampled to derive programs in the DSL, where the order of expansions is specified by the distribution, as shown in the respective figure, and the system returns the first program that is consistent with the input-output examples."}, {"heading": "4 Domain-Specific Language", "text": "The syntax of the domain-specific language for API-based string transformations is shown in Figure 5. The top-level construct of the language is the Concat function that returns the concatenation of its argument substrings fi. A substring expression f can either be a constant string s, the input string v, or the result of an API function with f as its argument. The Concat operator allows for composition of API calls with constant strings. The DSL consists of 3 types of APIs: regex APIsR, lookup APIs L, and transformation APIs T .\nRegex API R: The regex APIs search for certain regular expression-based patterns in the input string and return the matched string. Some examples of regex APIs are GetFirstNum, GetBetFirstAndSecondCommas, etc. Our DSL consists of 104 such regex APIs.\nLookup API L: The lookup APIs look for presence of certain strings in the input string and return the lookup string. Each lookup API consists of a dictionary of a finite collection of strings, which are used for searching input substrings. Some examples of lookup APIs are GetCity,GetState, GetStockSymbol etc. For example, the GetState API contains a dictionary of 50 US state names, whereas the GetCity API contains a dictionary of 18, 200 US cities. Our DSL consists of 18 such lookup APIs.\nTransformation API T : The transformation APIs consists of a dictionary D : S1 \u2192 S2, which maps a finite collection of strings S1 to another finite collection of strings S2. These APIs search for a string s \u2208 S1 in the input string and return the corresponding output string D[s] \u2208 S2. Some examples of such APIs include GetStateFromCity, GetFirstNameInitial, etc. For example, the transformation API GetStateFromCity consists of a dictionary mapping a collection of 18, 200 US cities to the corresponding US states. Our DSL consists of 13 such functions.\nThe full list of all functions is provided in Appendix A."}, {"heading": "5 Neural Architecture for Search", "text": "The neural search over the programs in the DSL conditioned on the input-output examples is performed using the model outlined in [Parisotto et al., 2016]. First, the input-output examples are encoded into a fixed length feature vector that\naims to capturing shared patterns between the input and output strings. This example representation is then passed to a neural tree-based generative model over program trees, called R3NN, to generate the desired hidden program. We provide a high level overview of the both the architectures."}, {"heading": "5.1 Neural Input-Output Encoder", "text": "The cross-correlational encoder generates a fixeddimensional vector representation of a set of input-output (I/O) examples. Intuitively, the encoder needs to capture three key information: parts of the output strings that are likely to be constant strings, parts of the output strings that can be computed from input strings, and some characteristics of the example strings that will help the program generator module identify the set of useful APIs for the given task. To simplify the DSL, we assume a fixed universe of possible constant strings so that we can focus on training the encoder to produce the likely set of APIs.\nThe I/O encoder first runs two bidirectional LSTM networks separately on the input and output strings in each example pair, which produces two matrices of size 2\u00d7H \u00d7 T , where H is the LSTM hidden dimension and T is the maximum length of the I/O string. The encoder then slides the output matrix over the input matrix for each time step and computes the outer product between respective matrix columns. There are in total 2 \u00b7 (T \u22121) alignments as we slide the matrices and we obtain 2\u00b7(T\u22121) vectors in total after the dot product. Finally, the encoder concatenates the values for overlapping time steps to obtain a 2\u00d7T\u00d7(T\u22121)-dimensional vector encoding for each example pair."}, {"heading": "5.2 Tree-Structured Generation Model", "text": "The tree generation model incrementally constructs a program tree starting from the start symbol of the DSL grammar and expanding the tree with one derivation at a time until obtaining a tree with that consists only non-terminal nodes. The R3NN network assigns posterior probabilities to every valid expansion of a partial tree to guide the search algorithm. In other words, given a partial program tree, the R3NN network decides which non-terminal node to expand in the tree and with which expansion rule in the grammar.\nThe R3NN is defined by the following parameters: i) an M -dimensional representation \u03c6(s) \u2208 RM for every symbol s in the grammar, ii) an M -dimensional representation \u03c9(r) \u2208 RM for each grammar rule r, iii) a deep neural network fr for each grammar rule r that takes as input a vector x \u2208 RQ\u00b7M (where Q is the number of RHS symbols of r) and outputs a vector y \u2208 RM , and iv) a deep neural network gr (reverse of fr) which takes as input a vector x\u2032 \u2208 RQ\u00b7M and outputs a vector y\u2032 \u2208 RQ\u00b7M .\nGiven a partial program tree, R3NN first assigns the representation \u03c6(S(l)) to each leaf node l, where S(l) denotes the grammar symbol of node l. It then performs a standard recursive pass over the tree from bottom-to-top, by recursively applying fR(n) for every non-leaf node n on its RHS node representations to compute the representation of n, where R(n) denotes the rule associated with node n. This pass continues until we reach the root node. The \u03c6(root) represents information about all tree nodes, but does not encode any notion of the node positions in the tree. To solve this issue, R3NN performs a reverse-recursive pass starting from the root node to compute updated representations of all child nodes using the reverse deep network gR(n). After performing the reverserecursive pass, each leaf node l is assigned a new distributed representation \u03c6\u2032(l), which intuitively captures the global information about every other node in the tree.\nThe scores for each expansion e \u2208 E can now be obtained from the global leaf representations \u03c6\u2032(l). Let er be the expansion type (production rule r \u2208 R that e applies) and let el be the leaf node l that er is applied to for an expansion e. The score of an expansion is calculated using ze = \u03c6\u2032(el) \u00b7 \u03c9(er) and the probability of the expansion is obtained by exponentiated normalized sum over the scores: \u03c0(e) = e\nze\u2211 e\u2032\u2208E e z e\u2032 ."}, {"heading": "6 Evaluation", "text": "We now present results from two major sets of experiments and analyze the model in more detail with the goal of assessing its expressiveness. We demonstrate that our model is capable of learning to synthesize simple programs when provided with a library of over 100 API functions. We also show that the model is capable of strong generalization, where it can not only generalize across different I/O examples for a given program, but also across new, unseen programs."}, {"heading": "6.1 Experimental Setup and Training Details", "text": "We use both synthetic benchmarks and real-world FlashFill benchmarks for evaluation. The synthetic benchmarks are obtained by sampling the programs in the DSL uniformly, and then using a rule-based approach to generate corresponding\ninput-output examples. For example, if we sample a program consisting of GetThirdNum and GetState APIs, the rule-based approach would ensure that the input strings in the example consist of at least three numbers and one state strings. For each benchmark, we sample five input strings and the corresponding output strings are obtained by executing the sampled program on the input strings. Several examples of training data are shown in Appendix B.\nWe first train the R3NN on a DSL consisting of only one family of APIs to evaluate its effectiveness on learning individual API family. We call the models trained on only the regex APIs (and constant strings) as the FF models and call the corresponding DSL as the regex-only DSL. We then train the R3NN with all APIs to evaluate the effectiveness of learning programs in the DSL consisting of different APIs and their composition with constant strings; we call this DSL the full DSL. The models trained on the full DSL are called the FF++ models. Since the FlashFill benchmarks can be solved using only the regex APIs and the set of constant strings, we also evaluate the FF model on the FlashFill benchmarks.\nWe train the cross-correlation encoder and R3NN jointly with the principle of maximum likelihood; the model produces posterior probabilities over possible expansions and we backpropagate an error signal based on the ground truth programs. We use the Adam optimizer [Kingma and Ba, 2014], with an initial learning rate of 0.001 and clipping gradients at 10 for both modules. We found that small learning rates are crucial for R3NN to prevent unstable learning. Every epoch consists of 1000 training batches of 10 instances, where each instance contains a ground truth program and 5 input-output pairs. The evaluation on synthetic data is performed on programs that are not seen during training. We report results when evaluating with both 1-best inference and with stochastic search (10, 50, or 100 samples), where we resample a program conditioned on the same input-output examples multiple times. This way, we allow the model to have small errors in its final posterior probabilities for selecting an expansion."}, {"heading": "6.2 Learning API Types", "text": "Each of the three classes of API functions, while much more interpretable, still pose nontrivial challenges for the model to learn to compose. The lookup API functions contain large dictionaries and the model must learn when to call such APIs given the input-output examples. For example, while the difference between names and cities may seem trivial to human practitioners, the model must learn to disambiguate each of these entities. The transformation API functions pose an additional challenge; with programs that require these types of API calls, not only does the model need to learn some encoding of the hidden dictionary, but the output string may not contain any obvious matching substring in the input string because of the nature of the API function. As a result, a simple string matching algorithm between the inputs and outputs will not work to solve this problem, and the input-output encoder must learn useful representations of pairs of them, and be expressive enough to capture the implicit string transformation. Lastly, the regex API functions do not encode dictionaries but represent syntactic substring operations, and the model must learn to recognize which API functions to call based on which\nparts of the output are present in the input. We first present an ablative study of what class of APIs are the easiest to learn in isolation, and which one is the most challenging in the full DSL.\nRegex APIs In Table 1, we report the training and validation set accuracies of different models trained on the regexonly DSL (FF model). The length column denotes the maximum length of programs that each model was trained on. The length 7 model was trained with 9000 programs, length 8 with 16000, length 9 with 616510, and length 10 with 1263000 programs. For validation, we select 1000 randomly chosen held-out programs from this set and generate new I/O examples to test the generalization power of the trained model."}, {"heading": "7 80% 67%", "text": ""}, {"heading": "8 91% 85%", "text": ""}, {"heading": "9 22% 20%", "text": ""}, {"heading": "10 64% 63%", "text": "Of particular note is the performance on programs of length 10. At this length, the DSL can generate programs with API nesting, API composition, and concatenation with a constant string; this represents all possible constructs in our DSL.\nLookup and Transform APIs In this experiment, we fix the maximum size of the programs in the training and validation set to size 10 and only include the lookup and transform APIs in the DSL. The results are shown in Table 2. We find that when the DSL is restricted to these APIs, the trained models achieve a very high accuracy and are able to identify composition of APIs with very high precision.\nAll APIs: Regex + Transform + Lookup We now present the model evaluation that was trained on the full DSL. Recall that because we\u2019ve trained on the full DSL, these models are referred to as the FF++ models.\nThe performance of the FF++ models is shown in Table 3. We observe that both training and validation accuracies decreased as compared to the FF models, which is expected since we now have an increased set of APIs that also include more complex APIs encoding large dictionaries. However, the length 10 model is still able to get 44% accuracy.\nWe analyze these results further to understand the learnability of different APIs when trained together as shown in Table 4. The regex APIs seem to be the easiest to learn for"}, {"heading": "7 54% 46%", "text": ""}, {"heading": "8 75% 64%", "text": ""}, {"heading": "9 46% 37%", "text": ""}, {"heading": "10 50% 44%", "text": "the network, which may be accredited to the specific nature of the IO encoder, as it was designed to detect patterns in substrings between the input and output examples. Interestingly, the lookup APIs are harder to learn than the transformation APIs, which can be attributed to the fact that they encode larger dictionaries as compared to the dictionaries of transform APIs."}, {"heading": "10 32% 50% 72%", "text": ""}, {"heading": "50 37% 52% 89%", "text": ""}, {"heading": "6.3 FlashFill using API Compositions", "text": "We now present the results of the best FF and FF++ models on the FlashFill benchmarks obtained from the authors of FlashFill [Gulwani et al., 2012]. These benchmarks correspond to real-world string transformation tasks in Excel, where each benchmark comprises of 5 input-output string examples."}, {"heading": "FF models", "text": "Baseline performance with uniform search We first present the results we obtain with a baseline uniform search model on the FlashFill benchmarks in Table 5. The baseline model performs a uniform search over the DSL expansions and is biased towards small programs. We also present stochastic sampling results for a fair comparison with the performance of the FF models.\nThe uniform search does surprisingly well considering the large space of all possible programs because the DSL we designed with APIs allows many of the benchmarks to be solved with a single call, e.g. GetFirstWord, and the uniform search sampler is biased towards shorter programs.\nFF Model performance on FlashFill Benchmarks We now evaluate the trained models whose accuracies on synthetic data are reported in Table 1. Note that unlike in Table\n1, each of the model is evaluated on the same dataset and so the results are comparable across rows. In this case, we not only report the results with stochastic sampling, but also report the 1-best programs under the 1 column in Table 6.\nIn this case, we observe that with 100 samples, the length10 model is able to solve 45% of the benchmarks. It surpasses the performance of Neural FlashFill [Parisotto et al., 2016], which achieves an accuracy of 23% with 100 samples and 34% with 1000 samples. On further inspection of the benchmarks, we find that only 50% of the benchmarks can be solved with programs of length \u2264 10 in our DSL. If we normalize across this, we see that we can solve 90% of all solvable benchmarks. This indicates that our model is capable of learning to synthesize realistic programs.\nFF++ Model Baseline Performance with uniform search We first present the baseline results of uniform search. Since the DSL has expanded, the uniform search performs slightly worse and can only achieve an accuracy of about 11% with 100 samples.\nFlashFill benchmark performance The results for evaluating the FF++ model on the FlashFill benchmarks is shown in Table 8. The length 10 models can still remarkably solve 37% of the benchmarks even with the extended DSL."}, {"heading": "7 Related Work", "text": "We describe the related work from the domains of VSA-based programming by example systems and neural program induction and synthesis systems.\nProgramming By Example for String Manipulations There has been much recent work on designing version space algebra-based PBE systems for performing data transformation and extraction. FlashFill [Gulwani, 2011; Gulwani et al., 2012] is a PBE system that performs regular expression based string transformations using examples. Given an inputoutput example string, FlashFill first searches over all possible ways to decompose the output string and represent the set of those sub-programs concisely using a DAG data structure. This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al., 2015]. While these methods are interpretable and tractable, they are unscalable to any additions of new functionality. DAPIP, unlike the VSA-based PBE systems, is trained automatically using the R3NN network by sampling several thousands of programs from arbitrary DSLs.\nNeural Program Induction and Synthesis There has been a plethora of recent work in both neural program induction and neural program synthesis. The goal in neural program induction is to teach neural networks the functional behavior of a program by augmenting the neural networks with additional computational modules such as Neural GPU [Kaiser and Sutskever, 2015], Neural Turing Machine [Graves et al., 2014], and stacks-augmented RNNs [Joulin and Mikolov, 2015]. One limitation of these architectures is that although they are able to learn the functional behavior, they do not expose an interpretable program back to the user. In addition, they need to be trained per task separately, representing a lack of strong generality. More recent work, such as Terpret [Gaunt et al., 2016] and Neural-RAM [Kurach et al., 2015] seek to mitigate the interpretability issue but they need to be trained for each individual benchmark problem, which is prohibitively expensive.\nA recent approach was proposed to use the R3NN-based neural architectures to synthesize programs in a DSL similar to that of FlashFill [Parisotto et al., 2016]. We employ the same architecture but in a different DSL consisting of APIs at the core level of expressions. The APIs allows the program depth to be shallower than programs in a DSL with more primitives, and we investigate if that can make the task of automatically learning a search strategy easier for the R3NN. We argue that imposing higher-order functions is much more extensible and more akin to human-like programming."}, {"heading": "8 Future Work", "text": "There are a number of ways that we can extend the results and techniques presented in this paper to yield both improvements in the current numbers as well as allow us to scale to larger programs."}, {"heading": "8.1 Function embeddings", "text": "We rely on the R3NN and the input-output encoder to implicitly encode the semantics of each function, and we\u2019ve shown through a number of experiments that the tree model is capable of doing so. This is impressive in its own right, but in\norder to improve the performance further, we should extend the model to support explicit, continuous representations of each function. This can be achieved in a number of ways - the simplest of which involves encoding each function as a randomly initialized vector and allowing the model to attend to API functions that may be relevant to the input-output examples. We can freeze the embeddings, or we can elect to backpropagate errors through both the attention mechanism and the embeddings, and jointly learn these representations. This represents a principled approach to adding new functions and method is easy to extend to additional API functions that we may choose to add."}, {"heading": "8.2 Divide and conquer", "text": "Function embeddings allow us to perform better on existing problems by giving the model more information as to what choices to make when generating the tree. However, this does not resolve the issue of scalability. Even with function embeddings, as the inputs and outputs grow in size and complexity, we have no scalable method of performing inference over which programs to synthesize. However, instead of viewing the problem as a whole, we can break up the problem into smaller pieces and try to solve each subpiece and concatenate the answers together. This divide-and-conquer approach allows us to treat larger problems as conglomerations of a number of smaller problems. This procedure requires two general mechanisms: one module will need to predict how to split the output string into smaller, meaningful chunks, and the second module will consume each input-output piece, synthesize the correct program, and each piece will eventually be concatenated together. This is especially convenient in this problem setting because the FlashFill language is one that is focused on concatenations, so we lose no generality in being able to solve the problem."}, {"heading": "8.3 Extending the DSL", "text": "An interesting extension of the DSL is to add multi-argument API function calls. This could yield more general API functions, such as GetNthObj(n, o), and could replace functions like GetFirstWord, GetSecondNumber. In addition, we can also add multi-argument Concat functions; this idea goes neatly with the divide-and-conquer approach and can be used to help scale the model to synthesize larger programs."}, {"heading": "8.4 Batching Trees", "text": "While the divide-and-conquer approach is an algorithmic improvement to speed up the process of training the model, we can also take advantage of the model to incorporate faster batching proocols. Using a tree-based generative models allows us to batch operations together that occur at the same depth in the tree because each operation is indepenedent of all of its siblings. Moreover, we can also batch multiple trees together for increased performance."}, {"heading": "9 Conclusion", "text": "In this paper, we presented DAPIP, a system that tries to automatically learn a synthesis algorithm given a DSL. In particular, we designed a DSL consisting of APIs as first class\nconstructs that allows the system to perform richer tasks using small sized programs. We used the recently introduced R3NN neural architecture to automatically learn a synthesis algorithm for our DSL. The preliminary results suggest that the system is able to efficiently learn programs up to size 10 with about 45% accuracy on real-world benchmarks. We believe this direction of using neural architectures to automatically develop synthesis algorithms for PBE systems can lead to big advancements in program synthesis techniques and make it more generally applicable to many new domains."}, {"heading": "B Samples of Training Data", "text": ""}, {"heading": "C Samples of Solved FlashFill Benchmarks", "text": ""}, {"heading": "D Samples of Unsolved FlashFill Benchmarks", "text": ""}], "references": [{"title": "Syntax-guided synthesis", "author": ["Alur et al", "2013] Rajeev Alur", "Rastislav Bodik", "Garvit Juniwal", "Milo MK Martin", "Mukund Raghothaman", "Sanjit A Seshia", "Rishabh Singh", "Armando Solar-Lezama", "Emina Torlak", "Abhishek Udupa"], "venue": "In Formal Methods in Computer-Aided Design (FMCAD),", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Flashrelate: extracting relational data from semi-structured spreadsheets using examples", "author": ["Daniel W. Barowy", "Sumit Gulwani", "Ted Hart", "Benjamin G. Zorn"], "venue": "PLDI, pages 218\u2013228,", "citeRegEx": "Barowy et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "volume 479", "author": ["Tamraparni Dasu", "Theodore Johnson. Exploratory data mining", "data cleaning"], "venue": "John Wiley & Sons,", "citeRegEx": "Dasu and Johnson. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Terpret: A probabilistic programming language for program induction", "author": ["Alexander L Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "venue": "arXiv preprint arXiv:1608.04428,", "citeRegEx": "Gaunt et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural Networks", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber. Framewise phoneme classification with bidirectional lstm", "other neural network architectures"], "venue": "18(5):602\u2013610,", "citeRegEx": "Graves and Schmidhuber. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "Graves et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 1st International Joint Conference on Artificial Intelligence", "author": ["Cordell Green. Application of theorem proving to problem solving"], "venue": "IJCAI\u201969, pages 219\u2013239,", "citeRegEx": "Green. 1969", "shortCiteRegEx": null, "year": 1969}, {"title": "ACM", "author": ["Sumit Gulwani", "William R. Harris", "Rishabh Singh. Spreadsheet data manipulation using examples. Commun"], "venue": "55(8):97\u2013105,", "citeRegEx": "Gulwani et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In POPL", "author": ["Sumit Gulwani. Automating string processing in spreadsheets using input-output examples"], "venue": "pages 317\u2013330,", "citeRegEx": "Gulwani. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neural computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber. Long short-term memory"], "venue": "9(8):1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "In Advances in Neural Information Processing Systems", "author": ["Armand Joulin", "Tomas Mikolov. Inferring algorithmic patterns with stackaugmented recurrent nets"], "venue": "pages 190\u2013198,", "citeRegEx": "Joulin and Mikolov. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural gpus learn algorithms", "author": ["\u0141ukasz Kaiser", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.08228,", "citeRegEx": "Kaiser and Sutskever. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Wrangler: interactive visual specification of data transformation scripts", "author": ["Kandel et al", "2011] Sean Kandel", "Andreas Paepcke", "Joseph M. Hellerstein", "Jeffrey Heer"], "venue": "In Proceedings of the International Conference on Human Factors in Computing Systems,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "arXiv preprint arXiv:1511.06392,", "citeRegEx": "Kurach et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine Learning", "author": ["Tessa A. Lau", "Steven A. Wolfman", "Pedro M. Domingos", "Daniel S. Weld. Programming by demonstration using version space algebra"], "venue": "53(1-2):111\u2013156,", "citeRegEx": "Lau et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Flashextract: a framework for data extraction by examples", "author": ["Vu Le", "Sumit Gulwani"], "venue": "PLDI, page 55,", "citeRegEx": "Le and Gulwani. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuro-symbolic program synthesis", "author": ["Emilio Parisotto", "Abdel-rahman Mohamed", "Rishabh Singh", "Lihong Li", "Dengyong Zhou", "Pushmeet Kohli"], "venue": "arXiv preprint arXiv:1611.01855,", "citeRegEx": "Parisotto et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Flashmeta: a framework for inductive program synthesis", "author": ["Oleksandr Polozov", "Sumit Gulwani"], "venue": "OOPSLA, pages 107\u2013126,", "citeRegEx": "Polozov and Gulwani. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "PVLDB", "author": ["Rishabh Singh", "Sumit Gulwani. Learning semantic string transformations from examples"], "venue": "5(8):740\u2013751,", "citeRegEx": "Singh and Gulwani. 2012a", "shortCiteRegEx": null, "year": 2012}, {"title": "In CAV", "author": ["Rishabh Singh", "Sumit Gulwani. Synthesizing number transformations from inputoutput examples"], "venue": "pages 634\u2013651,", "citeRegEx": "Singh and Gulwani. 2012b", "shortCiteRegEx": null, "year": 2012}, {"title": "Blinkfill: Semi-supervised programming by example for syntactic string transformations", "author": ["Rishabh Singh"], "venue": "PVLDB, 9(10):816\u2013827,", "citeRegEx": "Singh. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "The ability to discover a program consistent with a given user intent (specification) is considered as one of the central problems in artificial intelligence [Green, 1969].", "startOffset": 158, "endOffset": 171}, {"referenceID": 7, "context": "Moreover, the state-of-the-art synthesis techniques [Gulwani et al., 2012] require a great deal of domain expertise with manually designed heuristics and rules to develop an efficient search procedure.", "startOffset": 52, "endOffset": 74}, {"referenceID": 2, "context": "Some studies have reported that this data wrangling process can sometimes take up to 80% of the total data analysis time [Dasu and Johnson, 2003; Kandel et al., 2011].", "startOffset": 121, "endOffset": 166}, {"referenceID": 8, "context": "Recently, ProgrammingBy-Example (PBE) techniques such as FlashFill [Gulwani, 2011; Gulwani et al., 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs.", "startOffset": 67, "endOffset": 104}, {"referenceID": 7, "context": "Recently, ProgrammingBy-Example (PBE) techniques such as FlashFill [Gulwani, 2011; Gulwani et al., 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs.", "startOffset": 67, "endOffset": 104}, {"referenceID": 21, "context": ", 2012] and BlinkFill [Singh, 2016] were developed to help users perform data transformation tasks using examples instead of having to write complex programs.", "startOffset": 22, "endOffset": 35}, {"referenceID": 18, "context": "These techniques encode the space of programs using a domain-specific language (DSL), and then develop algorithms based on version-space algebra (VSA) [Polozov and Gulwani, 2015; Lau et al., 2003] to efficiently search the space of programs.", "startOffset": 151, "endOffset": 196}, {"referenceID": 15, "context": "These techniques encode the space of programs using a domain-specific language (DSL), and then develop algorithms based on version-space algebra (VSA) [Polozov and Gulwani, 2015; Lau et al., 2003] to efficiently search the space of programs.", "startOffset": 151, "endOffset": 196}, {"referenceID": 17, "context": "The second shortcoming is handled by learning the synthesis algorithm in DAPIP automatically from data using two recently introduced neural modules [Parisotto et al., 2016].", "startOffset": 148, "endOffset": 172}, {"referenceID": 9, "context": "The first module called the cross-correlational encoder computes a fixed-dimension vector representation of the inputoutput examples by using tensor representations obtained by running two bi-directional LSTMs [Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005] on the input and output strings and computing their cross correlation.", "startOffset": 210, "endOffset": 274}, {"referenceID": 4, "context": "The first module called the cross-correlational encoder computes a fixed-dimension vector representation of the inputoutput examples by using tensor representations obtained by running two bi-directional LSTMs [Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005] on the input and output strings and computing their cross correlation.", "startOffset": 210, "endOffset": 274}, {"referenceID": 17, "context": "The neural search over the programs in the DSL conditioned on the input-output examples is performed using the model outlined in [Parisotto et al., 2016].", "startOffset": 129, "endOffset": 153}, {"referenceID": 13, "context": "We use the Adam optimizer [Kingma and Ba, 2014], with an initial learning rate of 0.", "startOffset": 26, "endOffset": 47}, {"referenceID": 7, "context": "3 FlashFill using API Compositions We now present the results of the best FF and FF++ models on the FlashFill benchmarks obtained from the authors of FlashFill [Gulwani et al., 2012].", "startOffset": 160, "endOffset": 182}, {"referenceID": 17, "context": "It surpasses the performance of Neural FlashFill [Parisotto et al., 2016], which achieves an accuracy of 23% with 100 samples and 34% with 1000 samples.", "startOffset": 49, "endOffset": 73}, {"referenceID": 8, "context": "FlashFill [Gulwani, 2011; Gulwani et al., 2012] is a PBE system that performs regular expression based string transformations using examples.", "startOffset": 10, "endOffset": 47}, {"referenceID": 7, "context": "FlashFill [Gulwani, 2011; Gulwani et al., 2012] is a PBE system that performs regular expression based string transformations using examples.", "startOffset": 10, "endOffset": 47}, {"referenceID": 20, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al.", "startOffset": 100, "endOffset": 126}, {"referenceID": 19, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al.", "startOffset": 140, "endOffset": 166}, {"referenceID": 16, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al.", "startOffset": 184, "endOffset": 206}, {"referenceID": 1, "context": "This VSA-based approach has then been extended to also build PBE systems for number transformations [Singh and Gulwani, 2012b], table joins [Singh and Gulwani, 2012a], data extraction [Le and Gulwani, 2014], and data reshaping [Barowy et al., 2015].", "startOffset": 227, "endOffset": 248}, {"referenceID": 11, "context": "The goal in neural program induction is to teach neural networks the functional behavior of a program by augmenting the neural networks with additional computational modules such as Neural GPU [Kaiser and Sutskever, 2015], Neural Turing Machine [Graves et al.", "startOffset": 193, "endOffset": 221}, {"referenceID": 5, "context": "The goal in neural program induction is to teach neural networks the functional behavior of a program by augmenting the neural networks with additional computational modules such as Neural GPU [Kaiser and Sutskever, 2015], Neural Turing Machine [Graves et al., 2014], and stacks-augmented RNNs [Joulin and Mikolov, 2015].", "startOffset": 245, "endOffset": 266}, {"referenceID": 10, "context": ", 2014], and stacks-augmented RNNs [Joulin and Mikolov, 2015].", "startOffset": 35, "endOffset": 61}, {"referenceID": 3, "context": "More recent work, such as Terpret [Gaunt et al., 2016] and Neural-RAM [Kurach et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 14, "context": ", 2016] and Neural-RAM [Kurach et al., 2015] seek to mitigate the interpretability issue but they need to be trained for each individual benchmark problem, which is prohibitively expensive.", "startOffset": 23, "endOffset": 44}, {"referenceID": 17, "context": "A recent approach was proposed to use the R3NN-based neural architectures to synthesize programs in a DSL similar to that of FlashFill [Parisotto et al., 2016].", "startOffset": 135, "endOffset": 159}], "year": 2017, "abstractText": "We present DAPIP, a Programming-By-Example system that learns to program with APIs to perform data transformation tasks. We design a domainspecific language (DSL) that allows for arbitrary concatenations of API outputs and constant strings. The DSL consists of three family of APIs: regular expression-based APIs, lookup APIs, and transformation APIs. We then present a novel neural synthesis algorithm to search for programs in the DSL that are consistent with a given set of examples. The search algorithm uses recently introduced neural architectures to encode input-output examples and to model the program search in the DSL. We show that synthesis algorithm outperforms baseline methods for synthesizing programs on both synthetic and real-world benchmarks.", "creator": "LaTeX with hyperref package"}}}