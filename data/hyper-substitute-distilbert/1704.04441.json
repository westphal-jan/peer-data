{"id": "1704.04441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Apr-2017", "title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?", "abstract": "this structure investigates performance robustness of nlp against typical word forms. most neural approaches actually match ( reliable ) human - like accuracy representing certain results involving conditions, conclusions consequently are closed to small issues in explicit query language - non - canonical input ( e. s., typos ). seeing both stability and robustness areas included here in applications regulating shareholder - owned content, and functional possibility efficiently humans easily cope with such noisy or trivial processes. in this paper, we research their impact : discrete input. just consider weak noise distributions ( one type computational noise, combination of interaction types ) and mismatched noise effects for training and testing. lately, research empirically review the robustness of functional programs ( individual interactive platforms, specific functional networks, non - spatial models ), different basic units ( characters, byte pair activation units ), and their cognitive tasks ( phrase tagging, machine operators ).", "histories": [["v1", "Fri, 14 Apr 2017 14:43:44 GMT  (260kb,D)", "http://arxiv.org/abs/1704.04441v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["georg heigold", "g\\\"unter neumann", "josef van genabith"], "accepted": false, "id": "1704.04441"}, "pdf": {"name": "1704.04441.pdf", "metadata": {"source": "CRF", "title": "How Robust Are Character-Based Word Embeddings in Tagging and MT Against Wrod Scramlbing or Randdm Nouse?", "authors": ["Georg Heigold", "G\u00fcnter Neumann", "Josef van Genabith"], "emails": ["georg.heigold@dfki.de", "neumann@dfki.de", "genabith@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "In this paper, we study the effect of non-normalized text on natural language processing (NLP). Nonnormalized text includes non-canonical word forms, noisy word forms, and word forms with \u201dsmall\u201d perturbations, such as informal spellings, typos, scrambled words. Compared to normalized text, the variability of non-normalized text is much greater and aggravates the problem of data sparsity.\nNon-normalized text dominates in many real world applications. Similar to humans, ideally NLP should perform reliably and robustly also under suboptimal or even adversarial conditions, without a significant degradation in performance. Web-\nbased content and social media are a rich source for noisy and informal text. Noise can also be introduced in a downstream NLP application where errors are propagated from one module to the next. For example, speech translation where the machine translation (MT) module needs to be robust against errors introduced by the automatic speech recognition (ASR) module. Moreover, NLP should not be vulnerable to adversarial examples. While all these examples do not pose a real challenge to an experienced human reader, even \u201dsmall\u201d perturbations from the canonical form can make a state-of-the-art NLP system fail.\nTo illustrate the typical behavior of state-of-theart NLP on normalized and non-normalized text, we discuss an example in the context of neural MT (NMT). Different research groups have shown that NMT can generate natural and fluent translations (Bentivogli et al., 2016), achieving human-like performance in certain settings (Wu et al., 2016). The state-of-the-art NMT engine Google Translate1, for example, perfectly translates the English sentence I used my card to purchase a meal on the menu and the total\non my receipt was $ 8.95 but when I went on line to check my\ntransaction it show $ 10.74 .\ninto the German sentence Ich benutzte meine Karte , um eine Mahlzeit auf der\nSpeisekarte zu kaufen und die Gesamtsumme auf meiner\nQuittung war $ 8,95 , aber als ich online ging , um meine\nTransaktion zu u\u0308berpru\u0308fen , zeigt es $ 10,74 .\nAdding some noise to the source sentence by swapping a few neighboring characters, e.g., I used my card ot purchase a meal no the mneu and the total\nno my receipt was $ 8.95 but whne I went on line to check ym\ntransaction it show $ 1.074 .\nconfuses the same NMT engine considerably: Ich benutzte meine Karte ot Kauf eine Mahlzeit nicht die\nMneu und die insgesamt nicht meine Quittung war $ 8,95 aber\n1https://translate.google.com/, February 2017\nar X\niv :1\n70 4.\n04 44\n1v 1\n[ cs\n.C L\n] 1\n4 A\npr 2\n01 7\nwhne ging ich auf Linie zu u\u0308berpru\u0308fen ym Transaktion es $\n1.074 .\nBy contrast, an experienced human reader can still understand and correctly translate the noisy sentence and compensate for some information loss (including real word errors such as \u201dno\u201d vs. \u201don\u201d, but rather not \u201d10.74\u201d vs. \u201d1.074\u201d), with little additional effort and often not even noticing \u201dsmall\u201d perturbations.\nOne might argue that a good translation should in fact translate corrupted language into corrupted language. Here, we rather adopt the position that the objective is to preserve the intended content and meaning of a sentence regardless of noise.\nIt should be noted that neural networks with sufficient capacity, in particular recurrent neural networks, are universal function approximators (Scha\u0308fer and Zimmermann, 2006). Hence, the performance degradation on non-normalized text is not so much a question whether the model can capture the variability but rather how to train a robust model. In particular, it can be expected that training on noisy data will make NLP more robust, as it was successfully demonstrated for other application domains including vision (Cui et al., 2015) and speech recognition (Doulaty et al., 2016).\nIn this paper, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, NMT). Due to easy availability and to have more control on the experimental setup with respect to error type and error density, we use synthetic data generated from existing clean corpora by perturbing the word forms. The perturbations include character flips and swaps of neighboring characters to imitate typos, and word scrambling.\nThe contributions of this paper are the following. Our experiments confirm that (i) noisy input substantially degrades the output of models trained on clean data. The experiments show that (ii) training on noisy data can help models achieve performance on noisy data similar to that of models trained on clean data tested on clean data, that (iii) models trained noisy data can achieve good results on noisy data almost without performance loss on clean data, that (iv) error type mismatches between training and test data can have a greater impact than error density mismatches, that (v) character based approaches are almost always better than byte pair\nencoding (BPE) approaches with noisy data, that (vi) the choice of neural models (recurrent, convolutional) is not as significant, and that (vii) for morphological tagging, under the same data conditions, the neural models outperform a conditional random field (CRF) based model.\nThe remainder of the paper is organized as follows. Section 2 discusses related work. Section 3 describes the noise type and Section 4 briefly summarizes the modeling approaches used in this paper. Experimental results are shown and discussed in Section 5. The paper is concluded in Section 6."}, {"heading": "2 Related Work", "text": "A large body of work on regularization techniques to learn robust representations and models exists. Examples include `2-regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising. Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al., 1987; Tu\u0308ske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al., 2017), do not have a long and extensive history in NLP.\nWhile invariance transformations such as rotation, translation in vision or vocal tract length, reverberation, and noise in speech have all been harnessed, we do not have a good intuition on useful perturbations for written language yet. Label dropout and flip (cf. typos) have been proposed both on the byte-level (Gillick et al., 2015) and the word-level (Xie et al., 2017). Syntactic and semantic noise for semantic analysis was studied in (Yitong et al., 2017). From a human perception perspective, word scrambling may be of interest (Rawlinson, 1976; Rayner et al., 2006).\nThe arbitrary relationship between the orthography of a word and its meaning in general is a well known assumption in linguistics (de Saussure, 1916). However, the word form often carries additional important information. This is, for example, the case in morphologically rich languages or in non-normalized text where small perturbations result in similar word forms. Recently, subword units have attracted some attention in NLP to handle rarely and unseen words and to reduce the computational complexity in neural network approaches (Ling et al., 2015; Gillick et al., 2015;\nSennrich et al., 2015; Chung et al., 2016; Heigold et al., 2017). Examples for sub-word units include BPE based units (Sennrich et al., 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al., 2015). A comparison of BPE and characters for machine translation regarding grammaticality can be found in (Sennrich, 2016)."}, {"heading": "3 Noise Types", "text": "In this work, we experiment with three different noise types: character swaps, character flips, and word scrambling. Character flips and swaps are rough approximations to typos. Word scrambling is motivated from psycholinguistic studies (Rawlinson, 1976). This choice of noise types allows us to automatically generate noisy text with different type and density distributions from existing properly edited \u201dclean\u201d corpora. Using synthetic data is clearly suboptimal, but we use synthetic data because of their easy availability and because it gives us better control on the experimental setup.\nCharacter swaps This type of perturbation randomly swaps two neighboring characters in a word. The words are processed from left to right. A swap is performed at each position with a pre-defined probability. Hence, movements from the left to the right beyond neighboring characters are possible. A character-swapped version (10% swapping probability) of the clean example sentence in the introduction may look like this: I used my card ot purchase a meal no the mneu and the total\nno my receipt was $ 8.95 but whne I went on line to check ym\ntransaction it show $ 1.074 .\nWord scrambling Humans appear to be good at reading scrambled text2. In a word scramble, the characters can be in an arbitrary order. The only constraint is that the first and last character be at the right place. In particular, all word scrambles are assumed to be equally likely. A scrambled version of the clean example sentence in the introduction may look like this: I uesd my card to pchasure a mael on the mneu and the ttaol\non my repciet was $ 89.5 but wehn I went on line to chcek my\ntanrsactoin it sohw $ 1.074 . Clearly, some word scrambles are easier than others. Word scrambling approximately includes char-\n2 http://www.mrc-cbu.cam.ac.uk/people/ matt-davis/cmabridge/, note the word scramble in the URL!\nacter swaps.\nCharacter flips This type of perturbation randomly replaces a character with another character at a pre-specified rate. Characters are drawn uniformly, but special symbols (e.g., end of stream) are excluded. We do not assume any correlation across characters. A character-flipped version (10% flipping probability) of the clean example sentence in the introduction may look like this: I used my car\u00bf to purch.s\u2019 a meal on the menu and the total on\nmy receipv tas $ 8.95 but whe3 = wen+ on lin4 to chece my\ntran&awtion it shzw $ 10.74 .\nCharacter flips preserve the order of characters but replace some information with random information, whereas character swaps and word scrambling relax the order of characters but do not add random information.\nOther simple perturbations include randomly removing or adding (in particular, repeating) characters.\nIn the experimental section, we will consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions for training and testing.\nA word of length n with at most one character flip can have up to nC different word forms, where C denotes the number of characters in the vocabulary. Word scrambling multiplies the number of word forms by a factor of (n \u2212 2)!. In general, perturbing word forms introduces a great deal of variability and data becomes much more sparse, implying that efficient handling of rare and unseen words will be crucial."}, {"heading": "4 Modeling", "text": "This section briefly summarizes the modeling approaches used in this work.\nFirst, we address the choice of unit. As illustrated in Table 1 on an example from the UD English corpus3, a word-based unit does not seem to be an appropriate unit in the presence of perturbations. Any change of the word form implies a different, independent word index. Even worse, most perturbed word forms do not represent valid words and are mapped to the <unk>-token and no word-specific information is preserved. This suggests the use of sub-word units. Here, we use BPE units (Sennrich et al., 2015) and characters as the basic units. BPE is based on char-\n3http://dependencies.org/\nacter co-occurrence frequency distributions and has the effect of representing frequent words as whole words and splitting rare words into subword units (e.g., \u201dused\u201d as \u201dused\u201d, \u201dpurchase\u201d as \u201dpurcha@@se\u201d). BPE provides a good tradeoff between modeling efficiency (i.e., the model does not need to learn for the frequent words how to assemble them) and handling unknown words. However, BPE may not be efficient at representing noisy word forms as small perturbations can lead to a different representation using different BPE units (e.g., \u201dused\u201d vs. \u201du@@es@@d\u201d, \u201dpurcha@@se\u201d vs. \u201dp@@cha@@sure\u201d). As the example illustrates (Table 1), perturbations tend to break longer units into smaller units, which makes the use of whole word units less useful. Finally, characters as the basic units have similar representations for similar word forms, but result in longer sequences, which makes the modeling of long-range dependencies harder and increases the computational complexity.\nNoise modeling for a word-level system is straightforward as perturbed word forms are mapped to <unk>, i.e., noise modeling reduces to word-level label dropout (and rarely word-level label flips) (Xie et al., 2017). This is not true for sub-word level representations, for which more detailed noise modeling will be important.\nWe use model architectures based on recurrent and convolutional neural networks in this work. Assuming that a word segmentation is given, we first map the sub-word units of a word to a word vector and then continue as for word-based approaches. Deep neural networks are universal function approximators (Scha\u0308fer and Zimmermann, 2006). Hence, a neural network with sufficient capacity is expected to learn the variability induced by perturbations. We compare the neural networks with a conditional random field (Lafferty et al., 2001)."}, {"heading": "5 Experiments", "text": "In this section, we empirically evaluate the robustness against perturbed word forms (Section 3) for the two common NLP tasks morphological tagging and machine translation."}, {"heading": "5.1 Morphological Tagging", "text": "We used the model configurations and setups from (Heigold et al., 2017) for the morphological tagging experiments in this paper. Training and testing was\nperformed on the UD English data set4. Figure 1 summarizes the results. We explored the three main dimensions of noise type and distribution, choice of unit, and type of model. Noise-adaptive training means standard training on noisy input sentences (but with correct labels: rich morphological tags or target language translation). We distinguish the noise type and distribution used for training (\u201dtraining noise type\u201d) and testing (\u201dtest noise type\u201d).\nWe start our discussion with the upper left histogram in Figure 1 for the character-based LSTMBLSTM architecture. It shows a clear performance degradation from around 95% to around 80% tag accuracy across all noise types compared to when trained on clean data (\u201dclean\u201d). Here, we consider the noise types word scrambling (\u201dscramble\u201d, note that all words are scrambled), character swaps with probability 10% (\u201dswap@10%), and character flips with probability 10% (\u201dflip@10%\u201d). Bar groups 2, 3 and 4 in the upper left histogram in Figure 1 show that noise-adaptive training helps in all cases, bringing the tag accuracy back to above 90% and without greatly affecting the accuracy on clean data. As expected, the accuracy under matched training and test conditions is highest in all cases. The transferability from a noise type to another depends on the noise types. For example, noise-adaptive training for \u201dswap@10%\u201d improves the accuracy on the \u201dscramble\u201d test condition by approximately 10%. On the other hand, the \u201dflip@10%\u201d test condition gets slightly worse. This outcome may be expected because characters swaps are more closely related with word scrambling than character flips. The transferability does not need to be symmetric. An example is \u201dflip@10%\u201d-adaptive training which improves on the \u201dswap@10%\u201d and \u201dscramble\u201d test conditions, whereas we observe slight degradation in the opposite direction. Finally, can we train a model that performs well across all these noise types as well as on clean data? For this, we mixed different noise types at the sentence level for training (\u201dcombined\u201d), i.e., a clean sentence, followed by a sentence with scrambling inside words, followed by a sentence with swapped characters inside words, followed by a sentence with flipped characters inside words, and so forth in the training data. The test data, by contrast, was pure clean (\u201dclean\u201d), scrambled (\u201dscramble\u201d), swapped (\u201dswap@10%\u201d), or flipped (\u201dflip@10%\u201d) data. According to the results summarized in the final group of bars in\n4http://dependencies.org/\nTable 1: Clean (left) vs. scrambled (right) example sentence using a word-based (top), a BPE-based (middle), and a characterbased (bottom) representation I used my card to purchase a meal on the menu and the total on my receipt was $ 8.95 but when i went on line to check my transaction it show $ 10.74 . I <unk> my card to <unk> a <unk> on the <unk> and the <unk> on my <unk> was $ 89.5 but <unk> i went on line to <unk> my <unk> it <unk> $ 1.074 .\nI used my c@@ ard to purcha@@ se a me@@ al on the men@@ u and the to@@ tal on my recei@@ pt was $ 8@@ .@@ 9@@ 5 but when I went on line to check my trans@@ action it show $ 10@@ .@@ 7@@ 4 . I u@@ es@@ d my c@@ ard to p@@ cha@@ sure a ma@@ el on the m@@ ne@@ u and the t@@ ta@@ ol on my rep@@ ci@@ et was $ 8@@ 9@@ .@@ 5 but we@@ h@@ n I went on line to ch@@ c@@e@@ k my t@@ on@@ tri@@ as@@ ac@@ n it so@@ h@@ w $ 1@@ .@@ 0@@ 7@@ 4 . I used my card to purchase a meal on the menu and the total on my receipt was $ 8.95 but when i went on line to check my transaction it show $ 10.74 ."}, {"heading": "I uesd my card to pchasure a mael on the mneu and the ttaol on my repciet was $", "text": ""}, {"heading": "89.5 but wehn I went on line to chcek my", "text": "tanrsactoin it sohw $ 1.074 .\nthe upper left histogram in Figure1, this is approximately possible. This result again suggests that noise strongly impacts on models trained on clean data (curve for 0% character flips), and that injecting noise at training time is critical but the exact noise distribution is not so important in this case.\nThe upper left and lower left histograms in Figure 1 differ in the choice of unit on the input text side, \u201dchar-LSTM-BLSTM\u201d uses characters and \u201dbpe-LSTM-BLSTM\u201d 2,000 BPE units5. The overall behavior is similar, but characters seem to degrade more gracefully than BPE units for mismatched noise conditions (compare bar columns 2, 3 and 4 between the upper left and lower left histograms in Figure 1).\nFinally, we explore how different models behave on noisy input (compare bar columns 2, 3 and 4 between the upper left and lower left histograms in Figure 1). For this, we compare a char-LSTMBLSTM, a char-CNNHighway-BLSTM (same as char-LSTM-BLSTM but uses a convolutional neural network to compute the word vectors) (Heigold et al., 2017), and a conditional random field (Mu\u0308ller and Schu\u0308tze, 2015) including word-based features and prefix/suffix features up to length 10 for rare words (we used MarMoT6 for the experiments). The upper left, upper right and lower right histograms in Figure1 show that the qualitative behavior of the three models is very similar. char-LSTMBLSTM and char-CNNHighway-BLSTM achieve similar performance. One might speculate if charLSTM-BLSTM is slightly better at flip@10% and char-CNNHighway-BLSTM at swap@10% and word scrambling, but the differences are most likely not significant. MarMoT\u2019s tag accuracies for all noise conditions is worse by 5-10%.\nAs indicated above, Table 1 shows results on English morphological tagging. In a suite of experiments (not shown here in full detail for reasons of space) we have confirmed similar overall results for morphologically-richer languages such as German. Morphological tagging for German is much harder than for English: while the English UD training data exhibit 119 distinct types of sequences of POS tags followed by morphological feature descriptions, the TIGER training data for German\n5 In neural MT, BPE size is usually around 50,000. For morphological tagging we adjust the number of BPE units according to the amount of data: the UD English training data roughly includes 2,000 unique words with at least 10 occurrences. For our NMT based experiments in Section 5.2, we use the customary BPE setting in NMT.\n6http://cistern.cis.lmu.de/marmot/\nexhibit 681 distinct types of such sequences. To give one result from our German experiments, Figure 2 shows the dependency of the test accuracy on the amount of character flips in the test data, for various amounts of character flips in training. Assuming an average word length of 6 characters, 10% character flips correspond with one typo in every second word, 20% character flips with one typo per word, and 30% character flips with two typos per word. This result suggests that injecting noise at training time is critical, whereas the test accuracy does not depend so much on the exact amount of training noise (curves for 10%, 20% and 30% character flips) and that models trained on noise injected data are still able to tag clean data with almost no loss in performance compared to a model trained on clean data only.\nMorphological tagging is a sequence-tosequence labelling task, where (to a first approximation) the number and order of elements in the two sequences is the same (each word/token is paired with a POS tag plus morphological description). Translation is arguably a much harder task as it often relates sequences of different lengths with possibly substantial changes in the order of corresponding words/tokens between source and target and, compared to morphological tagging, much larger sizes of output vocabularies. In a second set of experiments, we explore the impact and handling of noise in the input to machine translation."}, {"heading": "5.2 Machine Translation", "text": "Our NMT setup is based on the setup in (Heigold and van Genabith, 2016). We use BPE units or characters as the basic units at the source side and always BPE units at the target side (following com-\nmon practice in our experiments we use a BPE size of 50,000), resulting in the two model configurations \u201dBPE-BPE\u201d and \u201dchar-BPE\u201d. For the character-based encoder, we assume the word segmentation and map the word string consisting of characters or BPE units to a word vector by a twolayer unidirectional LSTM. The baseline model (\u201dclean\u201d) is trained on the German-English (DEEN) parallel corpora provided by WMT\u2019167. Results for the newstest2016-deen data set are shown in Table 2. For noise-adaptive training, we perform\ntransfer learning on the perturbed source sentencetarget sentence pairs (\u201dnoise-adapted\u201d). For training, we choose the following sentence-level noise distribution: 50% clean sentences, 20% sentences with character swaps (5% swap probability), 10% sentences with word scrambles, and 20% sentences with character flips (5% flip probability). We refer to this noise distribution to as \u201dnoisy.\u201d Beside this \u201dnoisy\u201d noise distribution, we also use mismatched noise conditions at test time, consisting of a single noise type only, referred to as \u201dclean\u201d, \u201dswap@5%\u201d, \u201dscramble\u201d, and \u201dflip@5%\u201d.\nThe baseline\u2019s performance drop for noisy test data is drastic and clearly depends on the noise type. Word scrambling seems to be the hardest noise type, for which BLEU goes down from around 30 to around 5 for BPE-BPE and char-BPE. Overall, however, the results suggest that the char-BPE baseline degrades much more gracefully than the BPE-BPE baseline.\nThe results in Table 2 show that noise-adaptive training can considerably improve the performance on noisy data and the gap between clean and noisy conditions can be almost closed for the \u201deasy\u201d noise conditions. Similar to the baseline, char-BPE tends to be less sensitive to mismatched noise con-\n7http://www.statmt.org/wmt16/ translation-task.html\nditions. This may be best seen from the fact that char-BPE performs better or no worse than BPEBPE for all noise conditions. Moreover, noiseadaptive training does not affect BLEU for charBPE (30.7 vs. 30.6) but there is a small performance penalty for BPE-BPE (31.6 vs. 30.4). Furthermore, the \u201dnoisy\u201d BLEU is the highest among the noisy conditions for BPE-BPE while the \u201dswap@5%\u201d BLEU is the best for char-BPE.\nWe show an example for the different noise types and source representations in Table 3. The example reflects the general findings based on BLEU scores (Table 2). The example also highlights the potential difficulty of correctly translating proper names in noisy conditions."}, {"heading": "6 Conclusion", "text": "In this paper, we presented an empirical study on morphological tagging and machine translation for noisy input. Mostly as expected from other application domains such as vision and speech, we found that state-of-the-art systems are very sensitive to slightly perturbed word forms that do not pose a challenge to humans and that injecting noise at training time can improve the robustness of such systems considerably. The best results were observed for matched training and test noise conditions but generalization across certain noise types and noise distributions is possible. Character-based approaches seem to degrade more gracefully compared with BPE-based approaches. We observe similar overall trends across tasks (morphological tagging and machine translation) and languages (English and German for morphological tagging). The results in this paper are promising but should be taken with a grain of salt as we used augmented data based on a limited number of idealized perturbation types. Future work will aim at a better comprehension of relevant and hard or even adversarial perturbations and noise types (including noisy sentence structure) in language and testing on real noisy user input. Moreover, the observation that the lower the BPE size is, the closer BPE is to character based encoding and the higher the BPE size is, the closer BPE is to word based approaches, will allow us to tune the system for the optimal granularity. A reasonable assumption is that the denoising is task-independent and could be trained independently of the actual NLP task, or shared across NLP tasks and jointly optimized.\nTable 3: Example sentence for different noise types (clean, character swaps, word scrambling, character flips) and NMT configurations (BPE/characters and standard training/noise-adaptive training)\nsource Herr Modi befindet sich auf einer fu\u0308nfta\u0308gigen Reise nach Japan , um die wirtschaftlichen Beziehungen mit (clean) der drittgro\u0308\u00dften Wirtschaftsnation der Welt zu festigen . unadapted Mr Modi is on a five-day trip to Japan to consolidate economic relations with the world \u2019s third largest (BPE-BPE) economies . noise-adapted Mr Modi is on a five-day trip to Japan to consolidate economic relations with the third largest economic (BPE-BPE) nation in the world . unadapted Mr Modi is on a five-day trip to Japan to consolidate economic relations with the world \u2019s third largest (char-BPE) economy . noise-adapted Mr Prodi is on a five-day trip to Japan in order to consolidate economic relations with the world \u2019s third (char-BPE) largest economy. source Herr Modi befindet sich auf einer fu\u0308nfta\u0308gigen Reise nach Japan, um die wirtschaftlichen Beziehungen mit (swap@5%) der rdtitgro\u0308\u00dften Wirtschaftsnation der Welt zu festiegn. unadapted Mr Modi is on a five-day trip to Japan to entrench economic relations with the world \u2019s most basic economic nation . noise-adapted Mr Modi is on a five-day trip to Japan to establish economic relations with the world \u2019s largest economic (BPE-BPE) nation . unadapted Mr Modi is on a five-day trip to Japan to establish economic relations with the world\u2019s largest economy.\nnoise-adapted Mr Prodi is on a five-day trip to Japan in order to consolidate economic relations with the world\u2019s third (char-BPE) largest economy. source Hrer Modi bfdneeit scih auf eienr fggnefu\u0308ia\u0308tn Reise ncah Jpaan , um die wctathhilsfecirn Buzegehnein mit (scramble) der drtio\u0308\u00dfettrgn Wsfactohtairsntin der Welt zu fgteesin . unadapted Hrer modes Bfdneeit scih on eienr fggnefu\u0308n journey ncah Jpaan to get the wctathsusfecirn Buzehno with the drone Wsfactohtairsntin in the world . noise-adapted Mr Modi is looking forward to a successful trip to Jpaan in order to find the scientific evidence with the (BPE-BPE) world \u2019s largest economy in the world . unadapted Hear is a member of the United States of America and the United States of America .\nnoise-adapted Mr Prodi is working on a fictitious journey to Japan in order to address economic relations with the world \u2019s (char-BPE) third largest economy . source Herr Modi befindet sicC 0uf einer fu\u0308nfta\u0308gigen Reise nach Japan , u\u201d die wirtsch a\u0301tlichen Beziehungen mi4 (flip@5%) dLr drittgro\u0308\u00dften Wirtschaftsn,tion der Welt zu f?stigen . unadapted Mr. Modi is located at sicC 0uf a five-day trip to Japan , u\u201d the wiring relations mi4 dLr third-largest economy of the world . noise-adapted Mr Modi is on a five-day trip to Japan to promote economic relations with the world \u2019s third largest (BPE-BPE) economy . unadapted Mr Modi is going to Japan on a five-day trip to Japan to fudge economic relations with the world\u2019s third largest economy . noise-adapted Mr Prodi is on a five-day trip to Japan to consolidate economic relations with the world \u2019s third largest (char-BPE) economy ."}], "references": [{"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico."], "venue": "CoRR abs/1608.04631.", "citeRegEx": "Bentivogli et al\\.,? 2016", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1603.06147. http://arxiv.org/abs/1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Data augmentation for deep neural network acoustic modeling", "author": ["Xiaodong Cui", "Vaibhava Goel", "Brian Kingsbury."], "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing 23(9):1469\u20131477.", "citeRegEx": "Cui et al\\.,? 2015", "shortCiteRegEx": "Cui et al\\.", "year": 2015}, {"title": "Course in general linguistics", "author": ["Ferdinand de Saussure"], "venue": null, "citeRegEx": "Saussure.,? \\Q1916\\E", "shortCiteRegEx": "Saussure.", "year": 1916}, {"title": "Twitter part-of-speech tagging", "author": ["Leon Derczynski", "Alan Ritter", "Sam Clark", "Kalina Bontcheva"], "venue": null, "citeRegEx": "Derczynski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Derczynski et al\\.", "year": 2013}, {"title": "Automatic optimization of data perturbation distributions for multi-style training in speech recognition", "author": ["Mortaza Doulaty", "Richard Rose", "Olivier Siohan."], "venue": "Proceedings of the IEEE 2016 Workshop on Spoken Language Technology (SLT2016).", "citeRegEx": "Doulaty et al\\.,? 2016", "shortCiteRegEx": "Doulaty et al\\.", "year": 2016}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "CoRR abs/1512.00103.", "citeRegEx": "Gillick et al\\.,? 2015", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["Kevin Gimpel", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": null, "citeRegEx": "Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2011}, {"title": "Explaining and harnessing adversarial examples", "author": ["Ian J. Goodfellow", "Jonathan Shlens", "Christian Szegedy."], "venue": "CoRR abs/1412.6572.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "An extensive empirical evaluation of character-based morphological tagging for 14 languages", "author": ["Georg Heigold", "G\u00fcnter Neumann", "Josef van Genabith."], "venue": "EACL.", "citeRegEx": "Heigold et al\\.,? 2017", "shortCiteRegEx": "Heigold et al\\.", "year": 2017}, {"title": "Character-based neural machine translation", "author": ["Georg Heigold", "Josef van Genabith."], "venue": "Technical report, DFKI GmbH.", "citeRegEx": "Heigold and Genabith.,? 2016", "shortCiteRegEx": "Heigold and Genabith.", "year": 2016}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "CoRR abs/1207.0580. http://arxiv.org/abs/1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning. Morgan", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Learning robust representations of text", "author": ["Yitong Li", "Trevor Cohn", "Timothy Baldwin."], "venue": "CoRR abs/1609.06082. http://arxiv.org/abs/1609.06082.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the 2015", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multi-style training for robust isolated-word speech recognition", "author": ["Richard Lippmann", "Edward Martin", "D. Paul."], "venue": "ICASSP. volume 12, pages 705\u2013708.", "citeRegEx": "Lippmann et al\\.,? 1987", "shortCiteRegEx": "Lippmann et al\\.", "year": 1987}, {"title": "Robust morphological tagging with word representations", "author": ["Thomas M\u00fcller", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "M\u00fcller and Sch\u00fctze.,? 2015", "shortCiteRegEx": "M\u00fcller and Sch\u00fctze.", "year": 2015}, {"title": "What to do about nonstandard (or non-canonical) language in NLP", "author": ["Barbara Plank."], "venue": "CoRR abs/1608.07836. http://arxiv.org/abs/1608.07836.", "citeRegEx": "Plank.,? 2016", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "The significance of letter position in word recognition", "author": ["G.E. Rawlinson."], "venue": "Ph.D. thesis, Psychology Department, University of Nottingham, Nottingham UK. Unpublished Ph.D. Thesis.", "citeRegEx": "Rawlinson.,? 1976", "shortCiteRegEx": "Rawlinson.", "year": 1976}, {"title": "Raeding wrods with jubmled lettres there is a cost", "author": ["K. Rayner", "S.J. White", "R.L. Johnson", "S.P. Liversedge."], "venue": "Psychological Science 17(3):192\u2013 193.", "citeRegEx": "Rayner et al\\.,? 2006", "shortCiteRegEx": "Rayner et al\\.", "year": 2006}, {"title": "The manifold tangent classifier", "author": ["Salah Rifai", "Yann Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller."], "venue": "NIPS\u20192011. Student paper award.", "citeRegEx": "Rifai et al\\.,? 2011", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Recurrent neural networks are universal approximators", "author": ["Anton Maximilian Sch\u00e4fer", "Hans Georg Zimmermann."], "venue": "Proceedings of the 16th International Conference on Artificial Neural Networks Volume Part I. Springer-Verlag, Berlin, Heidelberg,", "citeRegEx": "Sch\u00e4fer and Zimmermann.,? 2006", "shortCiteRegEx": "Sch\u00e4fer and Zimmermann.", "year": 2006}, {"title": "How grammatical is characterlevel neural machine translation? Assessing MT quality with contrastive translation pairs", "author": ["Rico Sennrich."], "venue": "CoRR abs/1612.04629.", "citeRegEx": "Sennrich.,? 2016", "shortCiteRegEx": "Sennrich.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR abs/1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Data augmentation, feature combination, and multilingual neural networks to improve asr and kws performance for low-resource languages", "author": ["Zolt\u00e1n T\u00fcske", "Pavel Golik", "David Nolden", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH. pages", "citeRegEx": "T\u00fcske et al\\.,? 2014", "shortCiteRegEx": "T\u00fcske et al\\.", "year": 2014}, {"title": "Data Noising as Smoothing in Neural Network Language Models", "author": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng."], "venue": "ArXiv:1703.02573v1.", "citeRegEx": "Xie et al\\.,? 2017", "shortCiteRegEx": "Xie et al\\.", "year": 2017}, {"title": "Robust training under linguistic adversity", "author": ["Li Yitong", "Trevor Cohn", "Timothy Baldwin."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017). Valencia, Spain, pages 21\u201327.", "citeRegEx": "Yitong et al\\.,? 2017", "shortCiteRegEx": "Yitong et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "Different research groups have shown that NMT can generate natural and fluent translations (Bentivogli et al., 2016), achieving human-like performance in certain settings (Wu et al.", "startOffset": 91, "endOffset": 116}, {"referenceID": 22, "context": "It should be noted that neural networks with sufficient capacity, in particular recurrent neural networks, are universal function approximators (Sch\u00e4fer and Zimmermann, 2006).", "startOffset": 144, "endOffset": 174}, {"referenceID": 2, "context": "In particular, it can be expected that training on noisy data will make NLP more robust, as it was successfully demonstrated for other application domains including vision (Cui et al., 2015) and speech recognition (Doulaty et al.", "startOffset": 172, "endOffset": 190}, {"referenceID": 5, "context": ", 2015) and speech recognition (Doulaty et al., 2016).", "startOffset": 31, "endOffset": 53}, {"referenceID": 11, "context": "Examples include `2-regularization, dropout (Hinton et al., 2012), Jacobian-based sensitivity penalty (Rifai et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 21, "context": ", 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising.", "startOffset": 44, "endOffset": 81}, {"referenceID": 14, "context": ", 2012), Jacobian-based sensitivity penalty (Rifai et al., 2011; Li et al., 2016), and data noising.", "startOffset": 44, "endOffset": 81}, {"referenceID": 13, "context": "Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al.", "startOffset": 53, "endOffset": 98}, {"referenceID": 8, "context": "Compared to other application domains such as vision (LeCun et al., 1998; Goodfellow et al., 2014) and speech (Lippmann et al.", "startOffset": 53, "endOffset": 98}, {"referenceID": 16, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 25, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 2, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 5, "context": ", 2014) and speech (Lippmann et al., 1987; T\u00fcske et al., 2014; Cui et al., 2015; Doulaty et al., 2016), working on noisy data (Gimpel et al.", "startOffset": 19, "endOffset": 102}, {"referenceID": 7, "context": ", 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al.", "startOffset": 31, "endOffset": 90}, {"referenceID": 4, "context": ", 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al.", "startOffset": 31, "endOffset": 90}, {"referenceID": 18, "context": ", 2016), working on noisy data (Gimpel et al., 2011; Derczynski et al., 2013; Plank, 2016) and in particular data noising (Yitong et al.", "startOffset": 31, "endOffset": 90}, {"referenceID": 27, "context": ", 2013; Plank, 2016) and in particular data noising (Yitong et al., 2017), do not have a long and extensive history in NLP.", "startOffset": 52, "endOffset": 73}, {"referenceID": 6, "context": "typos) have been proposed both on the byte-level (Gillick et al., 2015) and the word-level (Xie et al.", "startOffset": 49, "endOffset": 71}, {"referenceID": 26, "context": ", 2015) and the word-level (Xie et al., 2017).", "startOffset": 27, "endOffset": 45}, {"referenceID": 27, "context": "Syntactic and semantic noise for semantic analysis was studied in (Yitong et al., 2017).", "startOffset": 66, "endOffset": 87}, {"referenceID": 19, "context": "From a human perception perspective, word scrambling may be of interest (Rawlinson, 1976; Rayner et al., 2006).", "startOffset": 72, "endOffset": 110}, {"referenceID": 20, "context": "From a human perception perspective, word scrambling may be of interest (Rawlinson, 1976; Rayner et al., 2006).", "startOffset": 72, "endOffset": 110}, {"referenceID": 24, "context": "Examples for sub-word units include BPE based units (Sennrich et al., 2015), characters (Ling et al.", "startOffset": 52, "endOffset": 75}, {"referenceID": 15, "context": ", 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 1, "context": ", 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 9, "context": ", 2015), characters (Ling et al., 2015; Chung et al., 2016; Heigold et al., 2017) or even bytes (Gillick et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 6, "context": ", 2017) or even bytes (Gillick et al., 2015).", "startOffset": 22, "endOffset": 44}, {"referenceID": 23, "context": "A comparison of BPE and characters for machine translation regarding grammaticality can be found in (Sennrich, 2016).", "startOffset": 100, "endOffset": 116}, {"referenceID": 19, "context": "Word scrambling is motivated from psycholinguistic studies (Rawlinson, 1976).", "startOffset": 59, "endOffset": 76}, {"referenceID": 24, "context": "Here, we use BPE units (Sennrich et al., 2015) and characters as the basic units.", "startOffset": 23, "endOffset": 46}, {"referenceID": 26, "context": ", noise modeling reduces to word-level label dropout (and rarely word-level label flips) (Xie et al., 2017).", "startOffset": 89, "endOffset": 107}, {"referenceID": 22, "context": "Deep neural networks are universal function approximators (Sch\u00e4fer and Zimmermann, 2006).", "startOffset": 58, "endOffset": 88}, {"referenceID": 12, "context": "We compare the neural networks with a conditional random field (Lafferty et al., 2001).", "startOffset": 63, "endOffset": 86}, {"referenceID": 9, "context": "We used the model configurations and setups from (Heigold et al., 2017) for the morphological tagging experiments in this paper.", "startOffset": 49, "endOffset": 71}, {"referenceID": 9, "context": "For this, we compare a char-LSTMBLSTM, a char-CNNHighway-BLSTM (same as char-LSTM-BLSTM but uses a convolutional neural network to compute the word vectors) (Heigold et al., 2017), and a conditional random field (M\u00fcller and Sch\u00fctze, 2015) including word-based features and prefix/suffix features up to length 10 for rare words (we used MarMoT6 for the experiments).", "startOffset": 157, "endOffset": 179}, {"referenceID": 17, "context": ", 2017), and a conditional random field (M\u00fcller and Sch\u00fctze, 2015) including word-based features and prefix/suffix features up to length 10 for rare words (we used MarMoT6 for the experiments).", "startOffset": 40, "endOffset": 66}], "year": 2017, "abstractText": "This paper investigates the robustness of NLP against perturbed word forms. While neural approaches can achieve (almost) human-like accuracy for certain tasks and conditions, they often are sensitive to small changes in the input such as non-canonical input (e.g., typos). Yet both stability and robustness are desired properties in applications involving user-generated content, and the more as humans easily cope with such noisy or adversary conditions. In this paper, we study the impact of noisy input. We consider different noise distributions (one type of noise, combination of noise types) and mismatched noise distributions for training and testing. Moreover, we empirically evaluate the robustness of different models (convolutional neural networks, recurrent neural networks, non-neural models), different basic units (characters, byte pair encoding units), and different NLP tasks (morphological tagging, machine translation).", "creator": "TeX"}}}