{"id": "1704.04920", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Apr-2017", "title": "Deep Joint Entity Disambiguation with Local Neural Attention", "abstract": "we identify a novel target recognition model for joint function - level entity disambiguation, which leverages learned neural representations. supported components are entity modelling, eliminating neural attention mechanism over local context control, significantly simplified differentiable neural inference stage for context. external evaluation thereby combines experiences of software training permitting a general expressions such enhanced graphical models showing probabilistic absolute - confidence maps. extensive experiments propose improvements we better able to obtain competitive or dynamic - all - the - system accuracy at moderate computational lengths.", "histories": [["v1", "Mon, 17 Apr 2017 10:18:32 GMT  (409kb,D)", "http://arxiv.org/abs/1704.04920v1", null], ["v2", "Fri, 21 Jul 2017 23:47:24 GMT  (401kb,D)", "http://arxiv.org/abs/1704.04920v2", "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 long paper"], ["v3", "Mon, 31 Jul 2017 18:25:57 GMT  (389kb,D)", "http://arxiv.org/abs/1704.04920v3", "Conference on Empirical Methods in Natural Language Processing (EMNLP) 2017 long paper"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["octavian-eugen ganea", "thomas hofmann"], "accepted": true, "id": "1704.04920"}, "pdf": {"name": "1704.04920.pdf", "metadata": {"source": "CRF", "title": "Deep Joint Entity Disambiguation with Local Neural Attention", "authors": ["Octavian-Eugen Ganea"], "emails": ["octavian.ganea@inf.ethz.ch", "thomas.hofmann@inf.ethz.ch"], "sections": [{"heading": null, "text": "We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or stateof-the-art accuracy at moderate computational costs."}, {"heading": "1 Introduction", "text": "Entity disambiguation (ED) is an important stage in text understanding which automatically resolves references to entities in a given knowledge base (KB). This task is challenging due to the inherent ambiguity between surface form mentions such as names and the entities they refer to. This many-to-many ambiguity can often be captured partially by name-entity co-occurrence counts extracted from entity-linked corpora.\nED research has largely focused on two types of contextual information for disambiguation: local information based on words that occur in a context window around an entity mention, and, global information, exploiting document-level coherence of the referenced entities. Many stateof-the-art methods aim to combine the benefits of both, which is also the philosophy we follow in this paper. What is specific to our approach is that we use embeddings of entities as a common representation to assess local as well as global evidence.\nIn recent years, many text and language understanding tasks have been advanced by neural network architectures. However, despite recent work, competitive ED systems still largely employ manually designed features. Such features often rely on domain knowledge and may fail to capture all relevant statistical dependencies and interactions. The explicit goal of our work is to use deep learning in order to learn basic features and their combinations from scratch. To the best of our knowledge, our approach is the first to carry out this program with full rigor."}, {"heading": "2 Contributions and Related Work", "text": "There is a vast prior research on entity disambiguation, highlighted by (Ji, 2016). We will focus here on a discussion of our main contributions in relation to prior work.\nEntity Embeddings. We have developed a simple, yet effective method to embed entities and words in a common vector space. This follows the popular line of work on word embeddings, e.g. (Mikolov et al., 2013; Pennington et al., 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015). In contrast to the above methods that require data about entity-entity co-occurrences, we avoid the use of such additional data and rather bootstrap entity embeddings from their canonical entity pages and local context of their hyperlink annotations. This allows for more efficient training and alleviates the need to compile co-linking statistics. Also, we avoid hand-engineered features, multiple disambiguation steps, or the need for additional ad hoc heuristics.\nContext Attention. We present a novel attention mechanism for local ED. Inspired by mem-\nar X\niv :1\n70 4.\n04 92\n0v 1\n[ cs\n.C L\n] 1\n7 A\npr 2\n01 7\nory networks (Sukhbaatar et al., 2015) and insights of (Lazic et al., 2015), our model deploys attention to select words that are informative for the disambiguation decision. A learned combination of the resulting context-based entity scores and a mention\u2013entity prior yields the final local scores. Our local model achieves better accuracy than the local probabilistic model of (Ganea et al., 2016), as well as the feature-engineered local model of (Globerson et al., 2016). As an added benefit, our model also has a smaller memory footprint. There have been other deep learning approaches to define better context models for ED. For instance (Francis-Landau et al., 2016; He et al., 2013) use convolutional neural networks (CNNs) and stacked denoising auto-encoders, respectively, to learn representations of textual documents and canonical entity pages. Entities for each mention are locally scored based on cosine similarity with the respective document embedding. In a similar local setting (Sun et al., 2015) embed mentions, their immediate contexts and their candidate entities using word embeddings and CNNs. However, their entity representations are built from entity titles and entity categories only.\nCollective Disambiguation. Third, a novel deep learning architecture for global ED is proposed. Mentions in a document are resolved jointly, using a conditional random field (Lafferty et al., 2001) with parametrized potentials. We suggest to learn the latter by casting loopy belief propagation (LBP) (Murphy et al., 1999) as a rolled-out deep network. This is inspired by similar approaches in computer vision, e.g. (Domke, 2013), and allows us to backpropagate through the (truncated) message passing, thereby optimizing the CRF potentials to work well in conjunction with the inference scheme. Our model is thus trained end-to-end with the exception of the pre-trained word and entity embeddings. Previous work has investigated different approximation techniques, including: random graph walks (Guo and Barbosa, 2016), personalized PageRank (Pershina et al., 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al., 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al., 2011). Mostly connected to our approach is (Ganea et al., 2016) where LBP is used for inference (but not learning) in a probabilistic graphical model and (Globerson et al., 2016) where a single round of message passing\nwith attention is performed. To our knowledge, we are one of the first to investigate differentiable message passing for NLP problems."}, {"heading": "3 Learning Entity Embeddings", "text": "In a first step, we propose to train entity vectors that can be used for the ED task (and potentially for other tasks). These embeddings compress the semantic meaning of entities and drastically reduce the need for manually designed features or co-occurrence statistics.\nEntity embeddings are bootstrapped from word embeddings and are trained independently for each entity. A few arguments motivate this decision: (i) there is no need for entity co-occurrence statistics that suffer from sparsity issues and/or large memory footprints; (ii) vectors of entities in a subset domain of interest can be trained separately, obtaining potentially significant speed-ups and memory savings that would otherwise be prohibitive for large entity KBs;1 (iii) entities can be easily added in an incremental manner, which is important in practice; (iv) the approach extends well into the tail of rare entities with few linked occurrences; (v) empirically, we achieve better quality compared to methods that use entity cooccurrence statistics.\nOur model embeds words and entities in the same low-dimensional vector space in order to exploit geometric similarity between them. We start with a pre-trained word embedding map x :W \u2192 Rd that is known to encode semantic meaning of words w \u2208 W; specifically we use word2vec pretrained vectors (Mikolov et al., 2013). We extend this map to entities E , i.e. x : E \u2192 Rd, as described below.\nSuppose that words that co-occur with the mention of an entity e are governed by a conditional distribution p(w|e). Empirically, we assume that words with high probability p(w|e) appear (i) on canonical KB description pages of the entity, and (ii) within windows of fixed size surrounding mentions of an entity. The above distribution is then approximated from word-entity co-occurrence counts from these two sources: p\u0302(w|e) \u221d #(w, e). Next, let q(w) be a modified unigram word distribution p\u0302(w) which we use for sampling \u201dnegative\u201d words, i.e. ones unrelated to a specific entity. As in (Mikolov et al., 2013), we choose a modified unigram distribution\n1Notably useful with (limited memory) GPU hardware.\nq(w) = p\u0302(w)\u03b1 for \u03b1 \u2208 (0, 1). We suggest to use a max-margin objective to infer the optimal entity embeddings. Let w+ \u223c p\u0302(w|e) and w\u2212 \u223c q(w), then:\nJ(z; e) = Ew+|e Ew\u2212 [ h ( z;w+, w\u2212 )] h(z;w, v) = [\u03b3 \u2212 \u3008z,xw \u2212 xv\u3009]+ (1) xe = arg min\nz:\u2016z\u2016=1 J(z; e)\nwhere \u03b3 > 0 is a margin parameter and [\u00b7]+ is the soft-plus function. The above loss is optimized using stochastic gradient descent with projection over sampled pairs (w+, w\u2212).\nWe empirically assess the quality of our entity embeddings on entity similarity and ED tasks as detailed in Section 7.2, Table 1 and Appendix A. The technique described in this section can also be applied, in principle, for computing embeddings of general text documents, but a comparison with such methods is left as future work."}, {"heading": "4 Local Model with Neural Attention", "text": "We now explain our local ED approach that uses embeddings to steer a neural attention mechanism. We build on the insight that only a few context words are informative for resolving an ambiguous mention, something that has been exploited before in (Lazic et al., 2015). Focusing only on those words helps reducing noise and improves disambiguation. (Yamada et al., 2016) observe the same problem and adopt the restrictive strategy of removing all non-nouns. Here, we assume that a context word may be relevant, if it is related to at least one of the entity candidates of a given mention.\nContext Scores. Let us assume that we have a mention\u2013entity prior p\u0302(e|m) available and that for each mention m, a pruned candidate set of entities \u0393(m), |\u0393(m)|\u2264 S has been identified. Our model, depicted in Figure 1, computes a score for each e \u2208 \u0393(m) based on the local context c = w1, . . . , wK surrounding m as well as the prior. It is a composition of differentiable functions, thus it is smooth from input to output, allowing us to easily compute gradients and backpropagate through it.\nEach word w \u2208 c and entity e \u2208 \u0393(m) is mapped to its embedding via the pre-trained map x (cf. Section 3). We then compute an unnormalized support score for each word in the context as\nfollows:\nu(w) = max e\u2208\u0393(m)\nx>e Axw (2)\nwhere A is a parameterized diagonal matrix. The score is high, if a word is strongly related to at least one candidate entity. We then apply a softmax function, (hard) pruned to the R \u2264 K words which receive the highest score2 Denote the reduced context by c\u0304. Then we get explicitly\n\u03b2(w) =\n{ exp[u(w)]\u2211 v\u2208c\u0304 exp[u(v)]\n. if w \u2208 c\u0304 0 otherwise.\n(3)\nWe then define a \u03b2-weighted context-based entitymention score via\n\u03a8(e, c) = \u2211 w\u2208c\u0304 \u03b2(w) x>e Bxw (4)\nwhere B is another trainable diagonal matrix. We will later use the same architecture for unary scores of our global ED model.\nLocal Score Combination. We integrate these context scores with the context-independent scores encoded in p\u0302(e|m). Our final (unnormalized) local model is a combination of both \u03a8(e, c) and log p\u0302(e|m):\n\u03a8(e,m, c) = f(\u03a8(e, c), log p\u0302(e|m)) (5)\nHere, we found a flexible choice for f to be important and to be superior to any na\u0131\u0308ve combination model. We therefore used a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities, which we regularized as suggested in (Denton et al., 2015) by constraining the sum of squares of all weights in the linear layer. We use standard projected SGD for training. The same network is also used in Section 5.\nLearning the Local Model. Entity and word embeddings are pre-trained as discussed in Section 3. Thus, the only learnable parameters are the diagonal matrices A and B, plus the parameters of f . Having few parameters helps to avoid overfitting and to be able to train with little annotated data. We assume that a set of known mentionentity pairs {(m, e\u2217)}with their respective context\n2We implement this using the layers Threshold and TemporalDynamicKMaxPooling from Torch nn package, which allows to compute a subgradient. Without the pruning stage, we get slightly worse results (see Section 7.4).\nwindows have been extracted from a corpus. For model fitting, we then utilize a max-margin loss that ranks ground truth entities higher than other candidate entities. This leads us to the objective:\n\u03b8\u2217 = arg min \u03b8 \u2211 m \u2211 e\u2208\u0393(m) g(e,m), (6) g(e,m) := [\u03b3 \u2212\u03a8(e\u2217,m, c) + \u03a8(e,m, c)]+\nwhere \u03b3 > 0 is a margin parameter. So we aim to find a \u03a8 (i.e. parameterized by \u03b8) such that the score of the correct entity e\u2217 referenced by m is at least \u03b3 higher than that of any other candidate entity e. Whenever this is not the case, the margin violation becomes the experienced loss."}, {"heading": "5 Document-Level Deep Model", "text": "Next, we address global ED assuming document coherence among entities. We therefore introduce the notion of a document as consisting of a sequence of mentions m = m1, . . . ,mn, along with their context windows c = c1, . . . cn. Our goal is to define a joint probability distribution over \u0393(m1) \u00d7 . . . \u00d7 \u0393(mn) 3 e. Each such e selects one candidate entity to each mention in the document. Obviously, the state space of e grows exponentially in the number of mentions n.\nCRF Model Our model is a fully-connected pairwise conditional random field, defined on the\nlog scale as\ng(e,m, c)= n\u2211 i=1 \u03a8(ei,mi, ci) + \u2211 i<j \u03a6(ei, ej) (7)\nThe unary factors are the local scores described in Eq. (4). The pairwise factors are bilinear forms of the entity embeddings\n\u03a6(e, e\u2032) = 2\nn\u2212 1 x>e Cxe\u2032 , (8)\nwhere C is again a diagonal matrix. Similar to (Ganea et al., 2016), the above normalization helps balancing the unary and pairwise terms across documents with different numbers of mentions.\nDifferentiable Inference Training and prediction in binary CRF models as above is NP-hard. Therefore, in learning one usually maximizes a likelihood approximation and during operations (i.e. in prediction) one may use an approximate inference procedure, often based on messagepassing. Among many challenges of these approaches, it is worth pointing out that weaknesses of the approximate inference procedure are generally not captured during learning. Inspired by (Domke, 2011, 2013), we use truncated fitting of LBP to a fixed number of message passing iterations. Our model directly optimizes the marginal\nlikelihoods, using the same networks for learning and prediction. As noted by (Domke, 2013), this method is robust to model mis-specification, avoids inherent difficulties of partition functions and is faster compared to double-loop likelihood training (where, for each stochastic update, inference is run until convergence is achieved).\nOur architecture is shown in Figure 2. A neural network with T layers encodes T message passing iterations of synchronous max-product LBP3 which is designed to find the most likely (MAP) entity assignments. We also use message damping, which is known to speed-up and stabilize convergence of message passing. Formally, in iteration t, mention mi votes for entity candidate e \u2208 \u0393(mj) of mention mj using the normalized log-message mti\u2192j(e) computed as:\nmt+1i\u2192j(e \u2032) = max\ne\u2208\u0393(mi)\n{ \u03a8(e,mi, ci) + \u03a6(e, e \u2032)\n+ \u2211 k 6=j mtk\u2192i(e)} . (9)\nHerein the first part just reflects the CRF potential, whereas the second part is defined as\nmti\u2192j(e) = log[\u03b4 \u00b7 softmax(mti\u2192j(e)) (10) + (1\u2212 \u03b4) \u00b7 exp(mt\u22121i\u2192j(e))]\nwhere \u03b4 \u2208 (0, 1] is a damping factor. Note that, without loss of generality, we simplified the LBP procedure by dropping the factor nodes.\nAfter T iterations, the beliefs (marginals) are computed as:\n\u00b5i(e) = \u03a8(e,mi, ci) + \u2211 k 6=i mTk\u2192i(e) (11) \u00b5i(e) = exp[\u00b5i(e)]\u2211\ne\u2032\u2208\u0393(mi) exp[\u00b5i(e \u2032)]\n(12)\n3Sum-product and mean-field performed worse in our experiments.\nSimilar to the local case, we obtain accuracy improvement when combining the mention-entity prior p\u0302(e|m) with marginal \u00b5i(e) using the nonlinear combination function f described in Section 4. The learned function f for global ED is non-trivial (see Figure 3), showing that the influence of the prior tends to weaken for larger \u00b5(e), whereas it has a dominating influence, whenever the document-level evidence is weak.\nParameters of our global model are the diagonal matrices A,B,C and the weights of the f network. As before we found a margin based objective to be most effective and we suggest to fit parameters by minimizing a ranking loss4 defined over the marginals\n\u03c1i(e) := f(\u00b5i(e), p\u0302(e|mi)) (13)\nas follows: L(\u03b8) = \u2211 D \u2211 mi \u2211 e\u2208\u0393(mi) e 6=e\u2217\ni\nh(mi, e) (14)\nh(mi, e) = [\u03b3 \u2212 \u03c1i(e\u2217i ) + \u03c1i(e)]+ (15) 4Optimizing a marginal log-likelihood loss function per-\nformed worse.\nComputing this objective is trivial by running T times the steps described by Eqs. (9), (10), followed in the end by step in Eq. (12). Each step is differentiable and the gradient over model parameters can be computed on the resulting marginals and back-propagated over messages using the chain rule.\nIn operations, prediction is done independently for each mention mi by based on maximizing the \u03c1i(e) score."}, {"heading": "6 Candidate Selection", "text": "We make use of a mention-entity prior p\u0302(e|m) both as a feature and for entity candidate selection. It is computed by averaging probabilities from two indexes build from mention entity hyperlink statistics from Wikipedia and a large Web corpus (Spitkovsky and Chang, 2012), plus the YAGO index of (Hoffart et al., 2011) (with uniform prior).\nCandidate selection, i.e. building of \u0393(e), is done for each input mention as follows: first, the top 30 candidates are selected based on p\u0302(e|m). Then, in order to optimize for memory and run time (LBP has complexity quadratic in S), we keep only 7 of these entities based on the following heuristic: (i) the top 4 entities based on p\u0302(e|m) are selected, (ii) the top 3 entities based on the local context-entity similarity measured as in Eq. 4 are selected.5. We refrain from annotating mentions without any candidate, implying that precision and recall can be different in our case.\nIn a few cases, generic mentions of persons (e.g. \u201dPeter\u201d) are coreferences of more specific mentions (e.g. Peter Such\u201d) from the same document. We employ a simple heuristic to address this issue: for each mention m, if there exist mentions\n5We have used a simpler context vector here and simply used the average of all its constituent word vectors\nof persons that contain m as a continuous subsequence of words, then we consider the merged candidate set of these specific mentions for the mention m. We assume that a mention refers to a person if its most probable candidate by p\u0302(e|m) is a person."}, {"heading": "7 Experiments", "text": ""}, {"heading": "7.1 ED Datasets", "text": "We validate our ED models on some of the most popular available datasets used by our predecessors6. We provide statistics in Table 2.\n\u2022 AIDA-CoNLL dataset (Hoffart et al., 2011) is one of the biggest manually annotated ED datasets. It contains training (AIDA-train), validation (AIDA-A) and test (AIDA-B) sets.\n\u2022 MSNBC (MSB), AQUAINT (AQ) and ACE2004 (ACE) datasets cleaned and updated by (Guo and Barbosa, 2016)7\n\u2022 WNED-WIKI (WW) and WNED-CWEB (CWEB): larger, but less reliable, automatically annotated datasets built from ClueWeb and Wikipedia by (Guo and Barbosa, 2016)."}, {"heading": "7.2 Training Details and (Hyper)Parameters", "text": "We explain training details of our approach. All models are implemented in the Torch framework.\nEntity Vectors Training & Relatedness Evaluation For entity embeddings only, we use Wikipedia (Feb 2014) corpus for training. Entity vectors are initialized randomly from a 0-mean normal distribution with standard deviation 1. We\n6TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.\n7Available at: bit.ly/2gnSBLg\nfirst train each entity vector on the entity canonical description page (title words included) for 400 iterations. Subsequently, Wikipedia hyperlinks of the respective entities are used for learning until validation score (described below) stops improving. In each iteration, 20 positive words, each with 5 negative words, are sampled and used for optimization as explained in Section 3. We use Adagrad (Duchi et al., 2011) with a learning rate of 0.3. We choose embedding size d = 300, pretrained (fixed) Word2Vec word vectors8, \u03b1 = 0.6, \u03b3 = 0.1 and window size of 20 for the hyperlinks. We remove stop words before training. Learning of vectors for all candidate entities in all datasets (270000 entities) takes 20 hours on a single TitanX GPU with 12GB of memory.\nWe test and validate our entity embeddings on the respective parts of the entity relatedness dataset of (Ceccarelli et al., 2013). It contains 3319 and 3673 queries for the test and validation sets. Each query consist of one target entity and up to 100 candidate entities with gold standard binary\n8By Word2Vec authors: http://bit.ly/1R9Wsqr\nlabels indicating if the two entities are related. The associated task requires ranking of related candidate entities. Following previous work, we use different evaluation metrics: normalized discounted cumulative gain (NDCG) and mean average precision (MAP). The validation score used during learning is then the sum of the four metrics showed in Table 1. We perform candidate ranking based on cosine similarity of entity pairs.\nLocal and Global Model Training Our local and global ED models are trained on AIDAtrain (multiple epochs), validated on AIDA-A and tested on AIDA-B and other datasets mentioned in Section 7.1. We use Adam (Kingma and Ba, 2014) with learning rate of 1e-4 until validation accuracy exceeds 90%, afterwards setting it to 1e5. Variable size mini-batches consisting of all mentions in a document are used during training. We remove stop words. Hyper-parameters of the best validated global model are: \u03b3 = 0.01,K = 100, R = 25, S = 7, \u03b4 = 0.5, T = 10. Validation accuracy is computed after each 5 epochs. To regularize, we keep the number of parameters low (approx. 1.2K parameters) and use early stopping, i.e. we stop if the validation accuracy does not increase after 500 epochs. Training on a single GPU takes, on average, 2ms per mention, or 16 hours\nfor 1250 epochs over AIDA-train."}, {"heading": "7.3 Baselines & Results", "text": "We compare with systems that report state-of-theart results on the datasets. Some baseline scores from Table 4 are taken from (Guo and Barbosa, 2016). The best results for the AIDA datasets are reported by (Yamada et al., 2016) and (Globerson et al., 2016). We do not compare against (Pershina et al., 2015) since, as noted also by (Globerson et al., 2016), their mention index artificially includes the gold entity (guaranteed gold recall).\nFor a fair comparison with prior work, we use in-KB accuracy and micro F1 (averaged per mention) metrics to evaluate our approach. Results are shown in Tables 3 and 4. We run our system 5 times, each time we pick the best model on the validation set, and report results on the test set for these models. We obtain state of the art accuracy on AIDA which is the largest and hardest (by the accuracy of the p\u0302(e|m) baseline) manually created ED dataset . We are also competitive on the other datasets. It should be noted that all the other methods use, at least partially, engineered features. The merit of our proposed method is to show that, with the exception of the p\u0302(e|m) feature, a neural network is able to learn the best features for ED without requiring expert input.\nTo gain further insight, we analyzed the accuracy on the AIDA-B dataset for situations where gold entities have low frequency or mention prior. Table 6 shows that our method performs well in these harder cases."}, {"heading": "7.4 Hyperparameter Studies", "text": "In Figure 5, we analyze the effect of two hyperparameters. First, we see that hard attention (i.e. R < K) helps reducing the noise from uninformative context words. Second, we see that a small number of LBP iterations (hard-coded in our network) is enough to obtain good accuracy. This speeds up training and testing compared to traditional methods that run LBP until convergence."}, {"heading": "7.5 Qualitative Analysis of Local Model", "text": "In Table 7 we show some examples of context words attended by our local model for hard cases (where the mention prior of the correct entity is low)."}, {"heading": "8 Conclusion", "text": "We have proposed a novel deep learning architecture for entity disambiguation that combines entity embeddings, a contextual attention mechanism, an adaptive local score combination, as well as unrolled differentiable message passing for global inference9. Compared to many other methods, we do not rely on hand-engineered features, nor on an extensive corpus for entity co-occurrences or relatedness. Our system is fully differentiable, although we chose to pre-train word and entity embeddings. Extensive experiments show the competitiveness of our approach across a wide range of corpora. In the future, we would like to extend\n9Code available: github.com/dalab/deep-ed\nthis system to perform nil detection, coreference resolution and mention detection."}, {"heading": "Acknowledgments", "text": "We thank Aurelien Lucchi, Marina Ganea, Jason Lee, Florian Schmidt and Hadi Daneshmand for their comments and suggestions."}], "references": [{"title": "Learning relatedness measures for entity linking", "author": ["Diego Ceccarelli", "Claudio Lucchese", "Salvatore Orlando", "Raffaele Perego", "Salvatore Trani."], "venue": "Proceedings of the 22nd ACM international conference on Information & Knowledge Management.", "citeRegEx": "Ceccarelli et al\\.,? 2013", "shortCiteRegEx": "Ceccarelli et al\\.", "year": 2013}, {"title": "Relational inference for wikification", "author": ["Xiao Cheng", "Dan Roth."], "venue": "Urbana 51(61801):16\u201358.", "citeRegEx": "Cheng and Roth.,? 2013", "shortCiteRegEx": "Cheng and Roth.", "year": 2013}, {"title": "Entity disambiguation with web links", "author": ["Andrew Chisholm", "Ben Hachey."], "venue": "Transactions of the Association for Computational Linguistics 3:145\u2013156.", "citeRegEx": "Chisholm and Hachey.,? 2015", "shortCiteRegEx": "Chisholm and Hachey.", "year": 2015}, {"title": "User conditional hashtag prediction for images", "author": ["Emily Denton", "Jason Weston", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus."], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.", "citeRegEx": "Denton et al\\.,? 2015", "shortCiteRegEx": "Denton et al\\.", "year": 2015}, {"title": "Parameter learning with truncated message-passing", "author": ["Justin Domke."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, pages 2937\u20132943.", "citeRegEx": "Domke.,? 2011", "shortCiteRegEx": "Domke.", "year": 2011}, {"title": "Learning graphical model parameters with approximate marginal inference", "author": ["Justin Domke."], "venue": "IEEE transactions on pattern analysis and machine intelligence 35(10):2454\u20132467.", "citeRegEx": "Domke.,? 2013", "shortCiteRegEx": "Domke.", "year": 2013}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12(Jul):2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Entity disambiguation by knowledge and text jointly embedding", "author": ["Wei Fang", "Jianwen Zhang", "Dilin Wang", "Zheng Chen", "Ming Li."], "venue": "CoNLL 2016 page 260.", "citeRegEx": "Fang et al\\.,? 2016", "shortCiteRegEx": "Fang et al\\.", "year": 2016}, {"title": "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["Paolo Ferragina", "Ugo Scaiella."], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management. ACM, pages 1625\u20131628.", "citeRegEx": "Ferragina and Scaiella.,? 2010", "shortCiteRegEx": "Ferragina and Scaiella.", "year": 2010}, {"title": "Capturing semantic similarity for entity linking with convolutional neural networks", "author": ["Matthew Francis-Landau", "Greg Durrett", "Dan Klein."], "venue": "arXiv preprint arXiv:1604.00734 .", "citeRegEx": "Francis.Landau et al\\.,? 2016", "shortCiteRegEx": "Francis.Landau et al\\.", "year": 2016}, {"title": "Probabilistic bag-of-hyperlinks model for entity linking", "author": ["Octavian-Eugen Ganea", "Marina Ganea", "Aurelien Lucchi", "Carsten Eickhoff", "Thomas Hofmann."], "venue": "Proceedings of the 25th International Conference on World Wide Web. International World", "citeRegEx": "Ganea et al\\.,? 2016", "shortCiteRegEx": "Ganea et al\\.", "year": 2016}, {"title": "Collective entity resolution with multi-focal attention", "author": ["Amir Globerson", "Nevena Lazic", "Soumen Chakrabarti", "Amarnag Subramanya", "Michael Ringgaard", "Fernando Pereira"], "venue": null, "citeRegEx": "Globerson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2016}, {"title": "Robust named entity disambiguation with random walks", "author": ["Zhaochen Guo", "Denilson Barbosa"], "venue": null, "citeRegEx": "Guo and Barbosa.,? \\Q2016\\E", "shortCiteRegEx": "Guo and Barbosa.", "year": 2016}, {"title": "Learning entity representation for entity disambiguation", "author": ["Zhengyan He", "Shujie Liu", "Mu Li", "Ming Zhou", "Longkai Zhang", "Houfeng Wang."], "venue": "ACL (2). pages 30\u201334.", "citeRegEx": "He et al\\.,? 2013", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "Robust disambiguation of named entities in text", "author": ["Johannes Hoffart", "Mohamed Amir Yosef", "Ilaria Bordino", "Hagen F\u00fcrstenau", "Manfred Pinkal", "Marc Spaniol", "Bilyana Taneva", "Stefan Thater", "Gerhard Weikum."], "venue": "Proceedings of the Conference", "citeRegEx": "Hoffart et al\\.,? 2011", "shortCiteRegEx": "Hoffart et al\\.", "year": 2011}, {"title": "Leveraging deep neural networks and knowledge graphs for entity disambiguation", "author": ["Hongzhao Huang", "Larry Heck", "Heng Ji."], "venue": "arXiv preprint arXiv:1504.07678 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Entity discovery and linking reading list http://nlp.cs.rpi.edu/kbp/2014/elreading.html", "author": ["Heng Ji"], "venue": null, "citeRegEx": "Ji.,? \\Q2016\\E", "shortCiteRegEx": "Ji.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John Lafferty", "Andrew McCallum", "Fernando Pereira"], "venue": "In Proceedings of the eighteenth international conference on machine learning, ICML", "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Plato: A selective context model for entity resolution", "author": ["Nevena Lazic", "Amarnag Subramanya", "Michael Ringgaard", "Fernando Pereira."], "venue": "Transactions of the Association for Computational Linguistics 3:503\u2013515.", "citeRegEx": "Lazic et al\\.,? 2015", "shortCiteRegEx": "Lazic et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning to link with wikipedia", "author": ["David Milne", "Ian H Witten."], "venue": "Proceedings of the 17th ACM conference on Information and knowledge management. ACM, pages 509\u2013518.", "citeRegEx": "Milne and Witten.,? 2008", "shortCiteRegEx": "Milne and Witten.", "year": 2008}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["Kevin P Murphy", "Yair Weiss", "Michael I Jordan."], "venue": "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., pages", "citeRegEx": "Murphy et al\\.,? 1999", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP. volume 14, pages 1532\u2013", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Personalized page rank for named entity disambiguation", "author": ["Maria Pershina", "Yifan He", "Ralph Grishman"], "venue": null, "citeRegEx": "Pershina et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pershina et al\\.", "year": 2015}, {"title": "Local and global algorithms for disambiguation to wikipedia", "author": ["Lev Ratinov", "Dan Roth", "Doug Downey", "Mike Anderson."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Ratinov et al\\.,? 2011", "shortCiteRegEx": "Ratinov et al\\.", "year": 2011}, {"title": "A cross-lingual dictionary for english wikipedia", "author": ["Valentin I Spitkovsky", "Angel X Chang"], "venue": null, "citeRegEx": "Spitkovsky and Chang.,? \\Q2012\\E", "shortCiteRegEx": "Spitkovsky and Chang.", "year": 2012}, {"title": "End-to-end memory networks. In Advances in neural information processing systems", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Modeling mention, context and entity with neural networks for entity disambiguation", "author": ["Yaming Sun", "Lei Lin", "Duyu Tang", "Nan Yang", "Zhenzhou Ji", "Xiaolong Wang."], "venue": "IJCAI. pages 1333\u20131339.", "citeRegEx": "Sun et al\\.,? 2015", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "Joint learning of the embedding of words and entities for named entity disambiguation", "author": ["Ikuya Yamada", "Hiroyuki Shindo", "Hideaki Takeda", "Yoshiyasu Takefuji."], "venue": "CoNLL 2016 page 250.", "citeRegEx": "Yamada et al\\.,? 2016", "shortCiteRegEx": "Yamada et al\\.", "year": 2016}, {"title": "Robust and collective entity disambiguation through semantic embeddings", "author": ["Stefan Zwicklbauer", "Christin Seifert", "Michael Granitzer."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Informa-", "citeRegEx": "Zwicklbauer et al\\.,? 2016", "shortCiteRegEx": "Zwicklbauer et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "There is a vast prior research on entity disambiguation, highlighted by (Ji, 2016).", "startOffset": 72, "endOffset": 82}, {"referenceID": 20, "context": "(Mikolov et al., 2013; Pennington et al., 2014), which was recently extended to entities and ED by (Yamada et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 23, "context": "(Mikolov et al., 2013; Pennington et al., 2014), which was recently extended to entities and ED by (Yamada et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 29, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 7, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 30, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 15, "context": ", 2014), which was recently extended to entities and ED by (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).", "startOffset": 59, "endOffset": 145}, {"referenceID": 27, "context": "ory networks (Sukhbaatar et al., 2015) and insights of (Lazic et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 19, "context": ", 2015) and insights of (Lazic et al., 2015), our model deploys attention to select words that are informative for the disambiguation decision.", "startOffset": 24, "endOffset": 44}, {"referenceID": 10, "context": "Our local model achieves better accuracy than the local probabilistic model of (Ganea et al., 2016), as well as the feature-engineered local model of (Globerson et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 11, "context": ", 2016), as well as the feature-engineered local model of (Globerson et al., 2016).", "startOffset": 58, "endOffset": 82}, {"referenceID": 9, "context": "For instance (Francis-Landau et al., 2016; He et al., 2013) use convolutional neural networks (CNNs) and stacked denoising auto-encoders, respectively, to learn representations of textual documents and canonical entity pages.", "startOffset": 13, "endOffset": 59}, {"referenceID": 13, "context": "For instance (Francis-Landau et al., 2016; He et al., 2013) use convolutional neural networks (CNNs) and stacked denoising auto-encoders, respectively, to learn representations of textual documents and canonical entity pages.", "startOffset": 13, "endOffset": 59}, {"referenceID": 28, "context": "In a similar local setting (Sun et al., 2015) embed mentions, their immediate contexts and their candidate entities using word embeddings and CNNs.", "startOffset": 27, "endOffset": 45}, {"referenceID": 18, "context": "Mentions in a document are resolved jointly, using a conditional random field (Lafferty et al., 2001) with parametrized potentials.", "startOffset": 78, "endOffset": 101}, {"referenceID": 22, "context": "We suggest to learn the latter by casting loopy belief propagation (LBP) (Murphy et al., 1999) as a rolled-out deep network.", "startOffset": 73, "endOffset": 94}, {"referenceID": 5, "context": "(Domke, 2013), and allows us to backpropagate through the (truncated) message passing, thereby optimizing the CRF potentials to work well in conjunction with the inference scheme.", "startOffset": 0, "endOffset": 13}, {"referenceID": 12, "context": "Previous work has investigated different approximation techniques, including: random graph walks (Guo and Barbosa, 2016), personalized PageRank (Pershina et al.", "startOffset": 97, "endOffset": 120}, {"referenceID": 24, "context": "Previous work has investigated different approximation techniques, including: random graph walks (Guo and Barbosa, 2016), personalized PageRank (Pershina et al., 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al.", "startOffset": 144, "endOffset": 167}, {"referenceID": 8, "context": ", 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al.", "startOffset": 29, "endOffset": 59}, {"referenceID": 14, "context": ", 2015), intermention voting (Ferragina and Scaiella, 2010), graph pruning (Hoffart et al., 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al.", "startOffset": 75, "endOffset": 97}, {"referenceID": 1, "context": ", 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al.", "startOffset": 36, "endOffset": 58}, {"referenceID": 25, "context": ", 2011), integer linear programming (Cheng and Roth, 2013), or ranking SVMs (Ratinov et al., 2011).", "startOffset": 76, "endOffset": 98}, {"referenceID": 10, "context": "Mostly connected to our approach is (Ganea et al., 2016) where LBP is used for inference (but not learning) in a probabilistic graphical model and (Globerson et al.", "startOffset": 36, "endOffset": 56}, {"referenceID": 11, "context": ", 2016) where LBP is used for inference (but not learning) in a probabilistic graphical model and (Globerson et al., 2016) where a single round of message passing with attention is performed.", "startOffset": 98, "endOffset": 122}, {"referenceID": 20, "context": "We start with a pre-trained word embedding map x :W \u2192 Rd that is known to encode semantic meaning of words w \u2208 W; specifically we use word2vec pretrained vectors (Mikolov et al., 2013).", "startOffset": 162, "endOffset": 184}, {"referenceID": 20, "context": "As in (Mikolov et al., 2013), we choose a modified unigram distribution", "startOffset": 6, "endOffset": 28}, {"referenceID": 19, "context": "We build on the insight that only a few context words are informative for resolving an ambiguous mention, something that has been exploited before in (Lazic et al., 2015).", "startOffset": 150, "endOffset": 170}, {"referenceID": 29, "context": "(Yamada et al., 2016) observe the same problem and adopt the restrictive strategy of removing all non-nouns.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "We therefore used a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities, which we regularized as suggested in (Denton et al., 2015) by constraining the sum of squares of all weights in the linear layer.", "startOffset": 150, "endOffset": 171}, {"referenceID": 10, "context": "Similar to (Ganea et al., 2016), the above normalization helps balancing the unary and pairwise terms across documents with different numbers of mentions.", "startOffset": 11, "endOffset": 31}, {"referenceID": 5, "context": "As noted by (Domke, 2013), this method is robust to model mis-specification, avoids inherent difficulties of partition functions and is faster compared to double-loop likelihood training (where, for each stochastic update, inference is run until convergence is achieved).", "startOffset": 12, "endOffset": 25}, {"referenceID": 29, "context": "48 (Yamada et al., 2016) d = 500 0.", "startOffset": 3, "endOffset": 24}, {"referenceID": 0, "context": "Table 1: Entity relatedness results on the test set of (Ceccarelli et al., 2013).", "startOffset": 55, "endOffset": 80}, {"referenceID": 21, "context": "WLM is a well-known similarity method of (Milne and Witten, 2008).", "startOffset": 41, "endOffset": 65}, {"referenceID": 26, "context": "It is computed by averaging probabilities from two indexes build from mention entity hyperlink statistics from Wikipedia and a large Web corpus (Spitkovsky and Chang, 2012), plus the YAGO index of (Hoffart et al.", "startOffset": 144, "endOffset": 172}, {"referenceID": 14, "context": "It is computed by averaging probabilities from two indexes build from mention entity hyperlink statistics from Wikipedia and a large Web corpus (Spitkovsky and Chang, 2012), plus the YAGO index of (Hoffart et al., 2011) (with uniform prior).", "startOffset": 197, "endOffset": 219}, {"referenceID": 14, "context": "\u2022 AIDA-CoNLL dataset (Hoffart et al., 2011) is one of the biggest manually annotated ED datasets.", "startOffset": 21, "endOffset": 43}, {"referenceID": 12, "context": "\u2022 MSNBC (MSB), AQUAINT (AQ) and ACE2004 (ACE) datasets cleaned and updated by (Guo and Barbosa, 2016)7", "startOffset": 78, "endOffset": 101}, {"referenceID": 12, "context": "\u2022 WNED-WIKI (WW) and WNED-CWEB (CWEB): larger, but less reliable, automatically annotated datasets built from ClueWeb and Wikipedia by (Guo and Barbosa, 2016).", "startOffset": 135, "endOffset": 158}, {"referenceID": 29, "context": "TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.", "startOffset": 25, "endOffset": 88}, {"referenceID": 11, "context": "TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.", "startOffset": 25, "endOffset": 88}, {"referenceID": 28, "context": "TAC-KBP datasets used by (Yamada et al., 2016; Globerson et al., 2016; Sun et al., 2015) are no longer available.", "startOffset": 25, "endOffset": 88}, {"referenceID": 19, "context": "9 (Lazic et al., 2015) 86.", "startOffset": 2, "endOffset": 22}, {"referenceID": 11, "context": "4 (Globerson et al., 2016) 87.", "startOffset": 2, "endOffset": 26}, {"referenceID": 29, "context": "9 (Yamada et al., 2016) 87.", "startOffset": 2, "endOffset": 23}, {"referenceID": 15, "context": "Global models (Huang et al., 2015) 86.", "startOffset": 14, "endOffset": 34}, {"referenceID": 10, "context": "6 (Ganea et al., 2016) 87.", "startOffset": 2, "endOffset": 22}, {"referenceID": 2, "context": "6 (Chisholm and Hachey, 2015) 88.", "startOffset": 2, "endOffset": 29}, {"referenceID": 12, "context": "7 (Guo and Barbosa, 2016) 89.", "startOffset": 2, "endOffset": 25}, {"referenceID": 11, "context": "0 (Globerson et al., 2016) 91.", "startOffset": 2, "endOffset": 26}, {"referenceID": 29, "context": "0 (Yamada et al., 2016) 91.", "startOffset": 2, "endOffset": 23}, {"referenceID": 7, "context": "2 (Fang et al., 2016) 81.", "startOffset": 2, "endOffset": 21}, {"referenceID": 10, "context": "3 - (Ganea et al., 2016) 91 89.", "startOffset": 4, "endOffset": 24}, {"referenceID": 21, "context": "7 - (Milne and Witten, 2008) 78 85 81 64.", "startOffset": 4, "endOffset": 28}, {"referenceID": 14, "context": "7 (Hoffart et al., 2011) 79 56 80 58.", "startOffset": 2, "endOffset": 24}, {"referenceID": 25, "context": "6 63 (Ratinov et al., 2011) 75 83 82 56.", "startOffset": 5, "endOffset": 27}, {"referenceID": 1, "context": "2 (Cheng and Roth, 2013) 90 90 86 67.", "startOffset": 2, "endOffset": 24}, {"referenceID": 12, "context": "4 (Guo and Barbosa, 2016) 92 87 88 77 84.", "startOffset": 2, "endOffset": 25}, {"referenceID": 6, "context": "We use Adagrad (Duchi et al., 2011) with a learning rate of 0.", "startOffset": 15, "endOffset": 35}, {"referenceID": 0, "context": "We test and validate our entity embeddings on the respective parts of the entity relatedness dataset of (Ceccarelli et al., 2013).", "startOffset": 104, "endOffset": 129}, {"referenceID": 17, "context": "We use Adam (Kingma and Ba, 2014) with learning rate of 1e-4 until validation accuracy exceeds 90%, afterwards setting it to 1e5.", "startOffset": 12, "endOffset": 33}, {"referenceID": 12, "context": "Some baseline scores from Table 4 are taken from (Guo and Barbosa, 2016).", "startOffset": 49, "endOffset": 72}, {"referenceID": 29, "context": "The best results for the AIDA datasets are reported by (Yamada et al., 2016) and (Globerson et al.", "startOffset": 55, "endOffset": 76}, {"referenceID": 11, "context": ", 2016) and (Globerson et al., 2016).", "startOffset": 12, "endOffset": 36}, {"referenceID": 24, "context": "We do not compare against (Pershina et al., 2015) since, as noted also by (Globerson et al.", "startOffset": 26, "endOffset": 49}, {"referenceID": 11, "context": ", 2015) since, as noted also by (Globerson et al., 2016), their mention index artificially includes the gold entity (guaranteed gold recall).", "startOffset": 32, "endOffset": 56}], "year": 2017, "abstractText": "We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations. Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation. Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps. Extensive experiments show that we are able to obtain competitive or stateof-the-art accuracy at moderate computational costs.", "creator": "LaTeX with hyperref package"}}}