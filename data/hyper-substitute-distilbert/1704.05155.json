{"id": "1704.05155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Stein Variational Autoencoder", "abstract": "a corresponding setting for the normal autoencoders is described, modeling on an application of stein'ring operator. the framework represents the encoder as a nonlinear nonlinear function cone. samples from single sampled distribution should recovered. one wants manually make parametric assumptions supporting the filter satisfying the encoder integral, and performance is further described by integrating the global encoder interval importance criteria. improves results are demonstrated of both unsupervised variable semi - supervised problems, indicating semi - directed inspection of the imagenet data, demonstrating improving scalability of improved results under varying datasets.", "histories": [["v1", "Tue, 18 Apr 2017 00:08:34 GMT  (7065kb,D)", "http://arxiv.org/abs/1704.05155v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yunchen pu", "zhe gan", "ricardo henao", "chunyuan li", "shaobo han", "lawrence carin"], "accepted": false, "id": "1704.05155"}, "pdf": {"name": "1704.05155.pdf", "metadata": {"source": "META", "title": "Stein Variational Autoencoder", "authors": ["Yunchen Pu", "Zhe Gan", "Ricardo Henao", "Chunyuan Li", "Shaobo Han", "Lawrence Carin"], "emails": ["<yunchen.pu@duke.edu>."], "sections": [{"heading": "1. Introduction", "text": "The autoencoder (Vincent et al., 2010) is a widely employed unsupervised framework to learn (typically) lowdimensional features from complex data. There has been significant recent interests in the variational autoencoder (VAE) (Kingma & Welling, 2014), which generalizes the original autoencoder in several ways. The VAE encodes input data to a distribution of codes (latent features). Further, the VAE decoder is a generative model, specifying a probabilistic representation of the data via a likelihood function. Another advantage of the VAE is that it yields efficient estimation of the often intractable latent-feature posterior via an approximate model, i.e., the recognition network (Kingma & Welling, 2014; Mnih & Gregor, 2014). As a result, the encoder yields efficient inference of the latent features of the generative model (decoder), which is critical for fast computation at test time. The VAE may also be scaled to handle massive amounts of data, such as ImageNet (Pu et al., 2016).\nThe VAE is a powerful framework for unsupervised learning. Additionally, when given labels on a subset of data, a classifier may be associated with the latent features, allowing for semi-supervised learning (Kingma et al., 2014; Pu\n1Duke University. Correspondence to: Yunchen Pu <yunchen.pu@duke.edu>.\net al., 2016). Further, the latent features (codes) may be associated with state-of-the-art natural-language-processing models, such as the Long Short-Term Memory (LSTM) network, for semi-supervised learning of text captions from an image (Pu et al., 2016).\nVAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016). This lower bound is maximized by alternating between optimizing the parameters of the recognition model (encoder) and the parameters of the generative model (decoder). For evaluation of the variational expression, being able to sample efficiently from the encoder is not sufficient; one must be able to explicitly evaluate the associated distribution of latent features. This requirement has motivated design of encoders in which a neural network maps input data to the parameters of a distribution in the exponential family (Kingma & Welling, 2014; Rezende et al., 2014), which serves as the latent-features distribution. For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).\nThe Gaussian assumption may be too restrictive in some cases (Rezende & Mohamed, 2015). Consequently, recent work has considered normalizing flows (Rezende & Mohamed, 2015), in which random variables from (for example) a Gaussian distribution are fed through a series of nonlinear functions to increase the complexity and representational power of the distribution over latent features. However, because of the need to explicitly evaluate the distribution within the variational expression, these nonlinear functions must be relatively simple, e.g., planar flows. In this framework, a sequence of relatively simple nonlinear functions are \u201cstacked\u201d atop a probabilistic (Gaussian) encoder. However, because of the simple nature of these functions, one may require many layers to achieve the desired representational power.\nResearchers have recently considered the idea of viewing the encoder as a proposal distribution for the latent features, thus using importance weighting when sampling from this distribution (Burda et al., 2016). This idea has been demonstrated to significantly improve the variational lower bound. Although importance sampling is useful to\nar X\niv :1\n70 4.\n05 15\n5v 1\n[ cs\n.L G\n] 1\n8 A\npr 2\n01 7\nStein Variational Autoencoder\nimprove performance, it does not address the fundamental limitation associated with the simple form of the encoder distribution (required for explicit evaluation of the variational bound).\nFrom a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016). Motivated by these ideas, we present a new approach for learning the distribution of latent features within a VAE framework. We recognize that the need for an explicit form for the encoder distribution is only a consequence of the fact that learning is performed based on the variational lower bound. For inference (e.g., at test time), we do not need an explicit form for the distribution of latent features, we only require fast sampling from the encoder. Consequently, in the proposed approach, we no longer explicitly use the variational lower bound to train the encoder. Rather, we seek an encoder that minimizes the Kullback-Leibler (KL) distance between the distribution of codes and the true (posterior) distribution on the latent features. To minimize this KL distance, we generalize use of Stein\u2019s identity, and employ ideas associated with a reproducing kernel Hilbert space (RKHS). Analogous work was introduced by Liu & Wang (2016); Wang & Liu (2016); however, they considered sampling from a general unnormalized distribution. They did not consider an autoencoder, and they didn\u2019t use Stein\u2019s identity to design a nonparametric recognition model (encoder).\nA key contribution of this paper concerns integrating Steinbased sampling with importance sampling, for learning VAE parameters. An advantage of using the Stein formulation with importance sampling is that we need not assume a form (e.g., Gaussian) for the distribution of the encoder, and hence we yield an improved proposal distribution.\nThe concepts developed here are demonstrated on a wide range of unsupervised and semi-supervised learning problems, including a large-scale semi-supervised analysis of the ImageNet dataset. These experimental results illustrate the advantage of a Stein VAE with nonparametric recognition model, relative to the traditional VAE that assumes a Gaussian form of encoder. Moreover, the results demonstrate further improvements realized by integrating the Stein VAE with importance sampling."}, {"heading": "2. Stein Variational Autoencoder (Stein VAE)", "text": "Consider data D = {xn}Nn=1, where xn are modeled as xn|zn \u223c p(x|zn;\u03b8). Distribution p(x|zn;\u03b8) may be viewed as a probabilistic decoder of latent code zn, and \u03b8 represents the decoder parameters. The set of codes associated with all xn \u2208 D is represented Z = {zn}Nn=1. In Bayesian statistics, one sets a prior on {\u03b8,Z}, here represented p(\u03b8,Z) = p(\u03b8)\u220fNn=1 p(zn). We desire the pos-\nAlgorithm 1 Stein Variational Autoencoder. Require: Input data D and number of samples M .\n1: Initialize samples {\u03b80j}Mj=1 from p(\u03b8) and \u03b70. 2: for t = 1 to Maximum Iterations do 3: Sample {\u03bej}Mj=1 from q0(\u03be). 4: Draw minibatch from D. 5: Evaluate samples z(t)jn = f\u03b7(xn, \u03bej). 6: Update samples \u03b8(t+1)j \u2190 \u03b8 (t) j according to (3) and\nz\u0302 (t) jn \u2190 z (t) jn according to (4).\n7: for k = 1 to K do 8: Update \u03b7(t,k) \u2190 \u03b7(t,k\u22121) according to (7). 9: end for\n10: end for\nterior p(\u03b8,Z|D), which we approximate here via samples, without imposing an explicit form for the posterior distribution (as is common in existing VAE work (Kingma & Welling, 2014)). We generalize concepts in Liu & Wang (2016) to manifest the approximate posterior samples."}, {"heading": "2.1. Stein Variational Gradient Descent (SVGD)", "text": "Assume we have samples {\u03b8j}Mj=1 drawn from distribution q(\u03b8), and samples {zjn}Mj=1 drawn from distribution q(Z). The distribution q(\u03b8)q(Z) is some KL distance from the true posterior p(\u03b8,Z|D). We wish to transform {\u03b8j}Mj=1 by pushing them through a function, and the corresponding transformed distribution from which they are drawn is denoted qT (\u03b8). It is desired that, in a KL sense, qT (\u03b8)q(Z) is closer to p(\u03b8,Z|D) than was q(\u03b8)q(Z). The following theorem is useful for defining how best to update {\u03b8j}Mj=1.\nTheorem 1 Assume \u03b8 and Z are Random Variables (RVs) drawn from distributions q(\u03b8) and q(Z), respectively. Consider the transformation T (\u03b8) = \u03b8 + \u03c8(\u03b8;D) and let qT (\u03b8) represent the distribution of \u03b8 \u2032 = T (\u03b8). We have\n\u2207 ( KL(qT \u2016p) ) | =0 = \u2212E\u03b8\u223cq(\u03b8) ( trace(Ap(\u03b8;D)) ) , (1)\nwhere qT = qT (\u03b8)q(Z), p = p(\u03b8,Z|D), and\nAp(\u03b8;D) =\u2207\u03b8 log p\u0303(\u03b8;D)\u03c8(\u03b8;D)T +\u2207\u03b8\u03c8(\u03b8;D) log p\u0303(\u03b8;D) = EZ\u223cq(Z)[log p(D,Z,\u03b8)] .\nThe proof is provided in Appendix A. Following Liu & Wang (2016), we assume \u03c8(\u03b8;D) lives in a reproducing kernel Hilbert space (RKHS) with kernel k(\u00b7, \u00b7). Under this assumption, the solution for \u03c8(\u03b8;D) that maximizes the decrease in the KL distance (1) is\n\u03c8\u2217(\u00b7;D) = Eq(\u03b8)[k(\u03b8, \u00b7)\u2207\u03b8 log p\u0303(\u03b8;D) +\u2207\u03b8k(\u03b8, \u00b7)] . (2)\nStein Variational Autoencoder\nTheorem 1 concerns updating samples from q(\u03b8) assuming fixed q(Z). To similarly update q(Z) with q(\u03b8) fixed, we employ a complementary form of Theorem 1 (omitted for brevity); in that case we consider transformation T (Z) = Z + \u03c8(Z;D), with Z \u223c q(Z). The function \u03c8(Z;D) is also assumed to be in a RKHS.\nThe expectations in (1) and (2) are approximated by samples, and we have\n\u03b8 (t+1) j = \u03b8 (t) j + \u2206\u03b8 (t) j , (3)\nwith\n\u2206\u03b8 (t) j \u2248\n1\nM M\u2211 j\u2032=1 [k\u03b8(\u03b8 (t) j\u2032 ,\u03b8 (t) j )\u2207\u03b8(t) j\u2032 log p\u0303(\u03b8 (t) j\u2032 ;D)\n+\u2207 \u03b8 (t) j\u2032 k\u03b8(\u03b8\n(t) j\u2032 ,\u03b8 (t) j ))].\n\u2207\u03b8 log p\u0303(\u03b8;D) \u2248 1\nM\n\u2211N n=1 \u2211M j=1\u2207\u03b8 log p(xn|zjn,\u03b8)p(\u03b8) ,\nSimilarly, when updating samples of the latent variables, we have\nz (t+1) jn = z (t) jn + \u2206z (t) jn , (4)\nwith\n\u2206z (t) jn =\n1\nM M\u2211 j\u2032=1 [kz(z (t) j\u2032n,z (t) jn )\u2207z(t) j\u2032n log p\u0303(z (t) j\u2032n;D)\n+\u2207 z (t) j\u2032n kz(z\n(t) j\u2032n,z (t) jn )] (5)\n\u2207zn log p\u0303(zn;D) \u2248 1M \u2211M j=1\u2207zn log p(xn|zn,\u03b8 \u2032 j)p(zn) ,\nThe kernels used to update samples of \u03b8 and zn are in general different, denoted respectively k\u03b8(\u00b7, \u00b7) and kz(\u00b7, \u00b7). is a small step size."}, {"heading": "2.2. Stein Recognition Model", "text": "At iteration t of the above learning procedure, we realize a set of latent-variable (code) samples {z(t)jn}Mj=1 for each xn \u2208 D under analysis. For large N this training may be computationally expensive. Further, the need to evolve (learn) samples {zj\u2217}Mj=1 for each new test sample x\u2217 is undesirable. We therefore develop a recognition model that efficiently computes samples of codes for a data sample of interest. The recognition model draws samples via zjn = f\u03b7(xn, \u03bejn) with \u03bejn \u223c q0(\u03be). Distribution q0(\u03be) is selected such that it may be sampled easily, e.g., isotropic Gaussian.\nAfter each iteration of Algorithm 1, we refine recognition model f\u03b7(x, \u03be) to mimic the Stein sample dynamics. Assume recognition-model parameters \u03b7(t) have been learned thus far. Using \u03b7(t), latent codes for iteration t are constituted as z(t)jn = f\u03b7(t)(xn, \u03bejn), with \u03bejn \u223c q0(\u03be). These\ncodes are computed for all data xn \u2208 Bt, where Bt \u2282 D is the minibatch of data at iteration t. The change in the codes is \u2206z(t)jn , as defined (5). We then update \u03b7 to match the refined codes, as\n\u03b7(t+1) = arg min \u03b7 \u2211 xn\u2208Bt M\u2211 j=1 \u2016f\u03b7(xn, \u03bejn)\u2212 z (t+1) jn \u2016 2 . (6) The analytic solution of (6) is intractable. We update \u03b7 with K steps of gradient descent as\n\u03b7(t,k) = \u03b7(t,k\u22121) \u2212 \u03b4 \u2211 xn\u2208Bt \u2211M j=1 \u2206\u03b7 (t,k\u22121) jn (7)\n\u2206\u03b7 (t,k\u22121) jn = \u2202\u03b7f\u03b7(xn, \u03bejn)(f\u03b7(xn, \u03bejn)\u2212 z (t+1) jn )|\u03b7=\u03b7(t,k\u22121)\nwhere \u03b4 is a small step size, \u03b7(t) = \u03b7(t,0) and \u03b7(t+1) = \u03b7(t,K), and \u2202\u03b7f\u03b7(xn, \u03bejn) is the transpose of the Jacobian of f\u03b7(xn, \u03bejn) w.r.t. \u03b7. Note that the use of minibatches mitigates challenges of training with large training sets, D. A similar concept to the above was developed in Wang & Liu (2016), although in that work neither the VAE nor recognition model were considered."}, {"heading": "3. Stein Variational Importance Weighted Autoencoder (Stein VIWAE)", "text": ""}, {"heading": "3.1. Multi-sample importance-weighted KL divergence", "text": "Consider a known joint distribution p(x, z), where x is observed and z is latent (e.g., corresponding to the code discussed above). The marginal log-likelihood log p(x) = log \u222b p(x, z)dz is typically intractable. One often seeks to bound log p(x) via an approximation q(z|x) to the posterior p(z|x):\nL(x) = Eq(z|x){log[p(x, z)/q(z|x)]} \u2264 log p(x) .\nThis lower bound plays a pivotal in the VAE (Kingma & Welling, 2014), and q(z|x) is analogous to the encoder discussed above. Recently, Burda et al. (2016); Mnih & Rezende (2016) showed that the multi-sample (k samples) importance-weighted estimator\nLk(x) = Ez1,...,zk\u223cq(z|x) [ log 1k \u2211k i=1 p(x,zi) q(zi|x) ] , (8)\nprovides a tighter lower bound and a better proxy for the log-likelihood, where z1, . . . ,zk are random variables sampled independently from q(z|x). Recall from (1) that the KL divergence played a key role in the Stein-based learning of Sec. 2. Equation (8) motivates replacement of the KL objective function with the multisample importance-weighted KL divergence\nKLkq,p(\u0398;D) , \u2212E\u03981:k\u223cq(\u0398) [ log 1 k \u2211k i=1 p(\u0398i|D) q(\u0398i) ] , (9)\nStein Variational Autoencoder\nwhere \u0398 = (\u03b8,Z) and \u03981:k = \u03981, . . . ,\u0398k are independent samples from q(\u03b8,Z). Note that the special case of k = 1 recovers the standard KL divergence.\nInspired by Burda et al. (2016), the following theorem (proved in Appendix A) shows that increasing the number of samples k is guaranteed to reduce the KL divergence and provide a better approximation of target distribution.\nTheorem 2 For any natural number k, we have\nKLkq,p(\u0398;D) \u2265 KLk+1q,p (\u0398;D) \u2265 0 ,\nand if q(\u0398)/p(\u0398|D) is bounded\nlimk\u2192\u221e KLkq,p(\u0398;D) = 0 .\nSimliar to the Stein autoencoder, we minimize (9) with a sample transformation based on Stein\u2019s operator (detailed in Sec. 3.2) and the recognition model is trained in the same way as Sec. 2.2. Specifically, we first draw samples {\u03b81:kj }Mj=1 and {z1:kjn }Mj=1 from a simple distribution q0(\u00b7), and convert these to approximate draws from p(\u03b81:k,Z1:k|D) by minimizing the multi-sample importance weighted KL divergence via nonlinear functional transformation."}, {"heading": "3.2. Importance-weighted learning procedure", "text": "Theorem 3 Let \u03981:k be RVs drawn independently from distribution q(\u0398) and KLkq,p(\u0398,D) is the multi-sample importance weighted KL divergence in (9). Let T (\u0398) = \u0398 + \u03c8(\u0398;D) and qT (\u0398) represent the distribution of \u0398\u2032 = T (\u0398). We have\n\u2207 (\nKLkq,p(\u0398 \u2032;D) ) | =0 = \u2212E\u03981:k\u223cq(\u0398)(Akp(\u03981:k;D)) ,\nwhere\nAkp(\u03981:k;D) = 1\u03c9\u0303 \u2211k i=1 \u03c9i ( trace ( Ap(\u0398i;D) ))\n\u03c9i = p(\u0398 i;D)/q(\u0398i), \u03c9\u0303 = \u2211ki=1 \u03c9i\nAp(\u0398;D) = \u2207\u0398 log p\u0303(\u0398;D)\u03c8(\u0398;D)T +\u2207\u0398\u03c8(\u0398;D) .\nThe proof is provided in Appendix A.\nThe following corollary generalizes Theorem 1 via use of importance sampling.\nCorollary 3.1 \u03b81:k andZ1:k are RVs drawn independently from distributions q(\u03b8) and q(Z), respectively. Let T (\u03b8) = \u03b8 + \u03c8(\u03b8;D), qT (\u03b8) represent the distribution of \u03b8\u2032 = T (\u03b8), and \u0398\u2032 = (\u03b8\u2032,Z) . We have\n\u2207 (\nKLkqT ,p(\u0398 \u2032;D) ) | =0 = \u2212E\u03b81:k\u223cq(\u03b8)(A k p(\u03b8 1:k;D)) (10)\nwhere\nqT = qT (\u03b8)q(Z), p = p(\u03b8,Z|D) Akp(\u03b81:k;D) = 1\u03c9\u0303 \u2211k i=1 \u03c9iAp(\u03b8i;D) \u03c9i = EZi\u223cq(Z) [ p(\u03b8i,Zi,D) q(\u03b8i)q(Zi) ] , \u03c9\u0303 = \u2211k i=1 \u03c9i\nAp(\u03b8;D) = \u2207\u03b8 log p\u0303(\u03b8;D)\u03c8(\u03b8;D)T +\u2207\u03b8\u03c8(\u03b8;D) log p\u0303(\u03b8;D) = EZ\u223cq(Z)[log p(D,Z,\u03b8)] .\nThe following corollary generalizes (2).\nCorollary 3.2 Assume \u03c8(\u03b8;D) lives in a reproducing kernel Hilbert space (RKHS) with kernel k\u03b8(\u00b7, \u00b7). The solution for \u03c8(\u03b8;D) that maximizes the decrease in the KL distance (10) is\n\u03c8\u2217(\u00b7;D) = E\u03b81:k\u223cq(\u03b8) [ 1 \u03c9\u0303 \u2211k i=1 \u03c9i ( \u2207\u03b8ik\u03b8(\u03b8i, \u00b7)\n+ k\u03b8(\u03b8 i, \u00b7)\u2207\u03b8i log p\u0303(\u03b8i;D)\n)] . (11)\nCorollary 3.1 and Corollary 3.2 provide a means of updating multiple samples {\u03b81:kj }Mj=1 from q(\u03b8) via T (\u03b8i) = \u03b8i + \u03c8(\u03b8i;D). The expectation wrt q(Z) is approximated via samples drawn from q(Z). Similarly, we can employ a complementary form of Corollary 3.1 and Corollary 3.2 to update multiple samples {Z1:kj }Mj=1 from q(Z). This suggests an importance-weighted learning procedure that alternates between update of particles {\u03b81:kj }Mj=1 and {Z1:kj }Mj=1, which is similiar as Sec. 2.1.\nSpecifically, let {\u03b81:k,tj }Mj=1 and {z1:k,tjn }Mj=1 denote the samples acquired at iteration t of the learning procedure. To update samples of \u03b81:k, we apply the transformation \u03b8 (i,t+1) j = T (\u03b8 (i,t) j ;D) = \u03b8 (i,t) j + \u03c8(\u03b8 (i,t) j ;D), for i = 1, . . . , k, by approximating the expectation by samples {z1:kjn }Mj=1, and we have\n\u03b8 (i,t+1) j = \u03b8 (i,t) j + \u2206\u03b8 (i,t) j , for i = 1, . . . , k, (12)\nwith\n\u2206\u03b8 (i,t) j \u2248 1M \u2211M j\u2032=1 [ 1 \u03c9\u0303 \u2211k i\u2032=1 \u03c9i ( \u2207 \u03b8 (i\u2032,t) j\u2032 k\u03b8(\u03b8 (i\u2032,t) j\u2032 ,\u03b8 (i,t) j ))\n+ k(\u03b8 (i\u2032,t) j\u2032 ,\u03b8 (i,t) j )\u2207\u03b8(i\u2032,t)\nj\u2032 log p\u0303(\u03b8\n(i\u2032,t) j\u2032 ;D) ] \u03c9i \u2248 1M \u2211N n=1 \u2211M j=1 p(\u03b8i,zijn,xn)\nq(\u03b8i)q(zijn) , \u03c9\u0303 =\n\u2211k i=1 \u03c9i\n\u2207\u03b8 log p\u0303(\u03b8;D) \u2248 1M \u2211N n=1 \u2211M j=1\u2207\u03b8 log p(xn|zjn,\u03b8)p(\u03b8) .\nSimilarly, when updating samples of the latent variables, we have\nz (i,t+1) jn = z (i,t) jn + \u2206z (i,t) jn , for i = 1, . . . , k, (13)\nStein Variational Autoencoder\nwith\n\u2206z (i,t) jn \u2248 1M \u2211M j\u2032=1 [ 1 \u03c9\u0303n \u2211k i\u2032=1 \u03c9in ( \u2207 z (i\u2032,t) j\u2032n kz(z (i\u2032,t) j\u2032n ,z (i,t) jn ))\n+ kz(z (i\u2032,t) j\u2032n ,z (i,t) jn )\u2207z(i\u2032,t)\nj\u2032n log p\u0303(z\n(i\u2032,t) j\u2032n ;D) ] \u03c9in \u2248 1M \u2211M j=1 p(\u03b8i,zijn,xn)\nq(\u03b8i)q(zijn) , \u03c9\u0303n =\n\u2211k i=1 \u03c9in\n\u2207zn log p\u0303(zn;D) \u2248 1M \u2211M j=1\u2207zn log p(xn|zn,\u03b8 \u2032 j)p(zn)\nOne may worry about the variance of this gradient. However, Burda et al. (2016) has showed that the estimator based on log of importance weighted average will not lead to high variance."}, {"heading": "4. Semi-supervised Learning with Stein VAE and Stein VIWAE", "text": "We extend our model to semi-supervised learning. Consider labeled data as pairs Dl = {xn,yn}Nln=1, where the label yn \u2208 {1, . . . , C} and the decoder is modeled as (xn,yn|zn) \u223c p(x,y|zn;\u03b8, \u03b8\u0303) = p(x|zn;\u03b8)p(y|zn; \u03b8\u0303), where \u03b8\u0303 represents the parameters of the decoder for labels. The set of codes associated with all labeled data is represented Zl = {zn}Nln=1. We desire to approximate the posterior distribution on the entire dataset p(\u03b8, \u03b8\u0303,Z,Zl|D,Dl) via samples, where D represents the unlabeled data, and Z is the set of codes associated with D. In the following, we will only discuss how to update the samples of \u03b8, \u03b8\u0303 and Zl. Updating samples Z is the same as (4) and (13) for Stein VAE and Stein VIWAE, respectively.\nTo make the following discussion concrete, we describe learning within the context of Stein VAE, generalizing the models in Sec. 2. This setup is also applied to Stein VIWAE, generalizing the models in Sec. 3. Assume {\u03b8j}Mj=1 drawn from distribution q(\u03b8), {\u03b8\u0303j}Mj=1 drawn from distribution q(\u03b8\u0303), and samples {zjn}Mj=1 drawn from (distinct) distribution q(Zl). The following corollary generalizes Theorem 1 and (2), which is useful for defining how best to update {\u03b8j}Mj=1.\nCorollary 3.3 Assume \u03b8, \u03b8\u0303, Z and Zl are RVs drawn from distributions q(\u03b8), q(\u03b8\u0303), q(Z) and q(Zl), respectively. Consider the transformation T (\u03b8) = \u03b8 + \u03c8(\u03b8;D,Dl) where \u03c8(\u03b8;D,Dl) lives in a RKHS with kernel k\u03b8(\u00b7, \u00b7). Let qT (\u03b8) represent the distribution of \u03b8 \u2032 = T (\u03b8). We have\n\u2207 ( KL(qT \u2016p) ) | =0 = \u2212E\u03b8\u223cq(\u03b8)(Ap(\u03b8;D,Dl)) , (14)\nand the solution for \u03c8(\u03b8;D,Dl) that maximizes the change in the KL distance (14) is\n\u03c8\u2217(\u00b7; D\u0302) = Eq(\u03b8)[k(\u03b8, \u00b7)\u2207\u03b8 log p\u0303(\u03b8; D\u0302) +\u2207\u03b8k(\u03b8, \u00b7)] , (15)\nwhere D\u0302 = (D,Dl), and qT = qT (\u03b8)q(Z)q(\u03b8\u0303), p = p(\u03b8, \u03b8\u0303,Z|D,Dl) Ap(\u03b8;D,Dl) = \u2207\u03b8 log p\u0303(\u03b8;D,Dl)\u03c8(\u03b8;D,Dl)T\n+\u2207\u03b8\u03c8(\u03b8;D,Dl) log p\u0303(\u03b8;D,Dl) = EZ\u223cq(Z)[log p(D|Z,\u03b8)]\n+ EZl\u223cq(Zl)[log p(Dl|Zl,\u03b8)] .\nThe expectations in (14) and (15) are approximated by samples. Updating samples \u03b8 and \u03b8\u0303 is similar to (3), with details provided in Appendix B. Samples of zn \u2208 Zl are updated\nz (t+1) jn = z (t) jn + \u2206z (t) jn ,\nwith\n\u2206z (t) jn = 1 M \u2211M j\u2032=1[kz(z (t) j\u2032n, z (t) jn)\u2207z(t)\nj\u2032n log p\u0303(z\n(t) j\u2032n;Dl)\n+\u2207 z (t)\nj\u2032n kz(z\n(t) j\u2032n, z (t) jn))]\n\u2207zn log p\u0303(zn;Dl) \u2248 1M \u2211M j=1\u2207znp(zn) { log p(xn|zn,\u03b8\u2032j)\n+ \u03b6 log p(yn|zn, \u03b8\u0303 \u2032 j) } ,\nwhere \u03b6 is a tuning parameter that balances the two components. Motivated by assigning the same weight to every data point (Pu et al., 2016), we set \u03b6 = NX/(C\u03c1) in the experiments, where NX is the dimension of xn, C is the number of categories for the corresponding label and \u03c1 is the proportion of labeled data in the mini-batch."}, {"heading": "5. Experiments", "text": "For all experiments, we use a radial basis-function (RBF) kernel as in Liu & Wang (2016), i.e., k(x,x\u2032) = exp(\u2212 1h\u2016x \u2212 x\u2032\u201622), where the bandwidth, h, is the median of pairwise distances between current samples. q0(\u03b8) and q0(\u03be) are set to isotropic Gaussian distributions. We share the samples of \u03be across data points, i.e., \u03bejn = \u03bej , for n = 1, . . . , N (this is not necessary, but it saves computation). The samples of \u03b8 and z, and parameters of the recognition model, \u03b7, are optimized via Adam (Kingma & Ba, 2015) with learning rate 0.0002, and with the gradient approximations in (3), (4), (7), (12) and (13). We do not perform any dataset-specific tuning or regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets. We set M = 100 and k = 50, and use minibatches of size 64 for all experiments, unless otherwise specified."}, {"heading": "5.1. Expressive power of Stein recognition model", "text": "We first evaluate the expressive power of non-Gaussian posterior approximation based on the Stein recognition\nStein Variational Autoencoder\nmodel developed in Section 2.2. This is done generating samples of z using standard VAE (Kingma & Welling, 2014) and via the proposed Stein VAE, with each compared to ground truth on testing data (considering a non-trivial example for which the exact posterior is available). We fix the decoder (generative model) parameters \u03b8, and only generate samples of z and update encoder parameters \u03b7 during training, for both standard VAE and Stein VAE.\nGaussian Mixture Model We synthesize data by (i) drawing zn \u223c 12N (\u00b51, I) + 12N (\u00b52, I), where \u00b51 = [5, 5]T , \u00b52 = [\u22125,\u22125]T ; (ii) drawing xn \u223c N (\u03b8zn, \u03c32I), where \u03b8 = ( 2 \u22121 1 \u22122 ) and \u03c3 = 0.1. The recognition model f\u03b7(xn, \u03bej) is specified as a multi-layer perceptron (MLP) with 100 hidden units, by first concatenating \u03bej and xn into a long vector. The dimension of \u03bej is set to 2. The recognition model for standard VAE is also an MLP with 100 hidden units, and with the assumption of a Gaussian distribution for the latent codes (Kingma & Welling, 2014).\nWe generate N = 10, 000 data points for training and 10 data points for testing. The analytic form of true posterior distribution is provided in Appendix C. Figure 1 shows the performance of Stein VAE approximations for the true posterior using M = 10, 20, 50 and 100 samples on one test data; other similar examples are provided in Appendix D. Our Stein recognition model is able to capture the multimodal posterior and produce accurate density approximation. As the number of samplesM is increased, we observe a substantial improvement in the approximation quality. By contrast, the Gaussian form of the standard VAE posterior is incapable of capturing the multimodal form of the true posterior.\nPoisson Factor Analysis Given a discrete vector xn \u2208 ZP+, Poisson factor analysis (Zhou et al., 2012) assumes xn is a weighted combination of V latent factors xn \u223c Pois(\u03b8zn), where \u03b8 \u2208 RP\u00d7V+ is the factor loadings matrix and zn \u2208 RV+ is the vector of factor scores. We consider topic modeling with Dirichlet priors on \u03b8v (v-th column of \u03b8) and gamma priors on each component of zn.\nWe evaluate our model on the 20 Newsgroups dataset containing N = 18, 845 documents with a vocabulary of P = 2, 000. The data are partitioned into 10,314 training, 1,000 validation and 7,531 test documents. The number of factors (topics) is set to V = 128. \u03b8 is first learned by Markov chain Monte Carlo (MCMC) (Gan et al., 2015). We then fix \u03b8 at its MAP value, and only learn the recognition model \u03b7 using standard VAE and Stein VAE; this is done, as in the previous example, to examine the accuracy of the recognition model to estimate the posterior of the latent codes (factor scores), isolated from estimation of \u03b8. The recognition model is an MLP with 100 hidden units.\nAn analytic form of the true posterior distribution p(zn|xn) is intractable for this problem. Consequently, we employ samples collected from MCMC as ground truth. Specifically, with \u03b8 fixed, we sample zn via Gibbs sampling, using 2,000 burn-in iterations followed by 2,500 collection draws, retaining every 10th collection sample. We show the marginal and pairwise posterior of one test data point in Fig. 2. Additional results are provided in Appendix D. As observed, Stein VAE leads to a more accurate approximation than standard VAE, compared to the MCMC samples.\nConsidering Fig. 2, note that VAE significantly underestimates the variance of the posterior (examining the marginals), a well-known problem of variational Bayesian\nStein Variational Autoencoder\nanalysis (Han et al., 2016). This is manifested here despite the fact that the marginals are not significantly different in form from the Gaussian assumption of the standard VAE. By contrast, the proposed Stein VAE does not assume a parametric form for the posterior, and therefore it does not seek to directly estimate the mean or variance of the posterior. We observe that Stein VAE yields highly accurate approximations to the true posterior, with this performed efficiently at test time via the recognition model."}, {"heading": "5.2. Density estimation", "text": "Data We consider five benchmark datasets: MNIST and four text corpora: 20 Newsgroups (20News), New York Times (NYT), Science and RCV1-v2 (RCV2). For MNIST, we used the standard split of 50K training, 10K validation and 10K test examples. The latter three text corpora consist of 133K, 166K and 794K documents. These three datasets are split into 1K validation, 10K testing and the rest for training.\nEvaluation Given new data x\u2217 (testing data), the marginal log-likelihood/perplexity values are estimated by the variational lower bound while integrating the decoder parameters \u03b8 out\nlog p(x\u2217) \u2265 Eq(z\u2217)[log p(x\u2217, z\u2217)] +H(q(z\u2217)) , (16)\nwhere p(x\u2217, z\u2217) = Eq(\u03b8)[log p(x\u2217,\u03b8, z\u2217)] and H(q(\u00b7)) = \u2212Eq(log q(\u00b7)) is the entropy. The expectation is approximated with samples {\u03b8j}Mj=1 and {z\u2217j}Mj=1 with z\u2217j = f\u03b7(x\u2217, \u03bej), \u03bej \u223c q0(\u03be). Directly evaluating q(z\u2217) is intractable. We alternatively estimate it via density transformation q(z) = q0(\u03be) \u2223\u2223\u2223det\u2202f\u03b7(x,\u03be)\u2202\u03be \u2223\u2223\u2223 \u22121\n. Note that we only need to compute the Jacobian determinant one time during testing, which will not lead to very high computational complexity.\nWe further estimate the marginal log-likelihood/perplexity values via the stochastic variational lower bound, as the mean of 5K-sample importance weighting estimate (Burda et al., 2016). Therefore, for each dataset, we report four results: (i) Stein VAE + ELBO, (ii) Stein VAE + S-ELBO, (iii)\nStein VIWAE + ELBO and (iv) Stein VIWAE + S-ELBO; the first term denotes the training procedure is employed as Stein VAE in Sec. 2 or Stein VIWAE in Section 3; the second term denotes the testing log-likelihood/perplexity is estimated by the ELBO in (16) or the stochastic variational lower bound, S-ELBO (Burda et al., 2016).\nModel For MNIST, we train the model with one stochastic layer, zn, with 50 hidden units and two deterministic layers, each with 200 units. The nonlinearity is set as tanh. The visible layer, xn, follows a Bernoulli distribution. For the text corpora, we build a three-layer deep Poisson network (Ranganath et al., 2015). The sizes of hidden units are 200, 200 and 50 for the first, second and third layer, respectively (see Ranganath et al. (2015) for detailed architectures).\nResults The log-likelihood/perplexity results are summarized in Tables 1 and 2. On MNIST, our Stein VAE achieves a variational lower bound of -85.21 nats, which outperforms standard VAE with the same model architecture. Our Stein VIWAE achieves a log-likelihood of -82.88 nats, exceeding normalizing flow (-85.1 nats) and importance weighted autoencoder (-84.78 nats), which is the best prior result obtained by feedforward neural network(FNN). Gregor et al. (2015) and Oord et al. (2016), which exploit spatial structure, achieved log-likelihoods of around -80 nats. Our model can also be applied on these models, but this is left as interesting future work. For the text corpora, we observe our Stein VAEs outperform other models and Stein VIWAE further improves the performance on all datasets. These results demonstrate that the proposed models are able to provide a better approximation of the posterior distribution."}, {"heading": "5.3. Semi-supervised Classification", "text": "We consider semi-supervised classification on MNIST and ImageNet (Russakovsky et al., 2014) data. For each dataset, we report the results obtained by (i) VAE, (ii) Stein VAE, and (iii) Stein VIWAE.\nStein Variational Autoencoder\nMNIST We randomly split the training set into a labeled and unlabeled set, and the number of labeled samples in each category varies from 10 to 300. We perform testing on the standard test set with 20 different training-set splits.\nThe decoder for labels is implemented as p(yn|zn, \u03b8\u0303) = softmax(\u03b8\u0303zn). We consider two types of decoders for images p(xn|zn,\u03b8) and encoder f\u03b7(x, \u03be): (i) FNN: Following Kingma et al. (2014), we use a 50-dimensional latent variables zn and two hidden layers, each with 600 hidden units, for both encoder and decoder; softplus log(1 + ex) is employed as the nonlinear activation function. (ii) All convolutional nets (CNN): Inspired by Springenberg et al. (2015), we replace the two hidden layers with 32 and 64 kernels of size 5 \u00d7 5 and a stride of 2. A fully connected layer is stacked on the CNN to produce a 50-dimensional latent variables zn. We use the leaky rectified activation (Maas et al., 2013). This architecture is employed for both the encoder and decoder. The input of the encoder is formed by spatially aligning and stacking xn and \u03be, while the output of decoder is the image itself.\nTable 3 shows the classification results. Our Stein VAE and Stein VIWAE consistently achieve better performance than the VAE, demonstrating the effectiveness of our model in providing good representations of images. We further observe that the variance of Stein VIWAE results is much lower than that of Stein VAE results on small labeled data, indicating the former produces more robust parameter estimates. State-of-the-art results (Rasmus et al., 2015) are achieved by the Ladder network, which can be employed with our Stein-based approach, however, we will consider this extension as future work.\nImageNet 2012 ImageNet 2012 is used to assess the scalability of our model to large datasets. We split the 1.3M training images into an unlabeled and labeled set, and vary the proportion of labeled images from 1% to 40%. The classes are balanced to ensure that no particular class is over-represented, i.e., the ratio of labeled and unlabeled images is the same for each class. We repeat the training process 10 times for the training setting with labeled images ranging from 1% to 10% , and 5 times for the the training setting with labeled images ranging from 20% to 40%. Each time we utilize different sets of images as the unlabeled ones.\nWe employ all convolutional net (Springenberg et al., 2015) for both the encoder and decoder, which replaces deterministic pooling (e.g., max-pooling) with stridden convolutions. Such a model has been shown to be effective for training higher resolution and deeper generative models (Radford et al., 2016). The decoder for labels is employed as global average pooling with softmax, which has been utilized in state-of-the-art image classification models (He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used to stabilize learning by normalizing the activations throughout the network and preventing relatively small parameter changes from being amplified into larger but suboptimal activation changes in other layers. We use the leaky rectified activation (Maas et al., 2013). Residual connections (He et al., 2016) are incorporated to encourage gradient flow. The model architecture is detailed in Appendix E. Following Krizhevsky et al. (2012), images are resized to 256\u00d7 256. A 224\u00d7 224 crop is randomly sampled from the images or its horizontal flip with the mean subtracted (Krizhevsky et al., 2012). We set the M = 20 and k = 10.\nTable 4 shows classification results indicating that Stein VAE and Stein IVWAE outperform VAE in all the experiments, demonstrating the effectiveness of our approach for semi-supervised classification. When the proportion of labeled examples is too small (< 10%), DGDN outperforms all the VAE-based models, which is not surprising provided that our models are deeper, thus have considerably more parameters than DGDN (Pu et al., 2016).\nStein Variational Autoencoder"}, {"heading": "6. Conclusion", "text": "We have employed the Stein operator to develop a new method for designing the encoder in a variational autoencoder, and for learning an approximate distribution (samples) for the parameters of the decoder. The distributions for the codes and for the decoder parameters are represented non-parametrically in terms of samples, inferred by minimizing a KL distance and via use of a RKHS. Fast inference is manifested by learning a recognition model that mimics the manner in which the inferred code samples are manifested. The method is further generalized and improved by performing importance sampling. An extensive set of results, for unsupervised and semi-supervised learning, demonstrate excellent performance and scaling to large datasets."}], "references": [{"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "In ICLR,", "citeRegEx": "Burda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Burda et al\\.", "year": 2016}, {"title": "A kernel test of goodness of fit", "author": ["K. Chwialkowski", "H. Strathmann", "A. Gretton"], "venue": "In ICML,", "citeRegEx": "Chwialkowski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chwialkowski et al\\.", "year": 2016}, {"title": "Scalable deep poisson factor analysis for topic modeling", "author": ["Z. Gan", "C. Chen", "R. Henao", "D. Carlson", "L. Carin"], "venue": "In ICML,", "citeRegEx": "Gan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["K. Gregor", "I. Danihelka", "A. Graves", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Gregor et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gregor et al\\.", "year": 2015}, {"title": "Variational gaussian copula inference", "author": ["S. Han", "X. Liao", "D.B. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Han et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Han et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML,", "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Auto-encoding variational Bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "citeRegEx": "Kingma and Welling,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Welling", "year": 2014}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["D.P. Kingma", "T. Salimans", "R. Jozefowicz", "Chen", "X.i", "I. Sutskever", "M. Welling"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2016}, {"title": "Semi-supervised learning with deep generative models", "author": ["D.P. Kingma", "D.J. Rezende", "S. Mohamed", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Laulyi"], "venue": "In NIPS,", "citeRegEx": "Larochelle and Laulyi,? \\Q2012\\E", "shortCiteRegEx": "Larochelle and Laulyi", "year": 2012}, {"title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "author": ["Q. Liu", "D. Wang"], "venue": "In NIPS,", "citeRegEx": "Liu and Wang,? \\Q2016\\E", "shortCiteRegEx": "Liu and Wang", "year": 2016}, {"title": "A kernelized stein discrepancy for goodness-of-fit tests", "author": ["Q. Liu", "J.D. Lee", "M. Jordan"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Maas et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Maas et al\\.", "year": 2013}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "Blunsomi", "Phil"], "venue": "In ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["A. Mnih", "K. Gregor"], "venue": "In ICML,", "citeRegEx": "Mnih and Gregor,? \\Q2014\\E", "shortCiteRegEx": "Mnih and Gregor", "year": 2014}, {"title": "Variational inference for monte carlo objectives", "author": ["A. Mnih", "D.J. Rezende"], "venue": "In ICML,", "citeRegEx": "Mnih and Rezende,? \\Q2016\\E", "shortCiteRegEx": "Mnih and Rezende", "year": 2016}, {"title": "Pixel recurrent neural network", "author": ["A. Oord", "N. Kalchbrenner", "K. Kavukcuoglu"], "venue": "In ICML,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Variational autoencoder for deep learning of images, labels and captions", "author": ["Y. Pu", "Z. Gan", "R. Henao", "X. Yuan", "C. Li", "A. Stevens", "L. Carin"], "venue": null, "citeRegEx": "Pu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pu et al\\.", "year": 2016}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "In ICLR,", "citeRegEx": "Radford et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2016}, {"title": "Deep exponential families", "author": ["R. Ranganath", "L. Tang", "L. Charlin", "D.M.Blei"], "venue": "In AISTATS,", "citeRegEx": "Ranganath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2015}, {"title": "Hierarchical variational models", "author": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "In ICML,", "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ranganath et al\\.", "year": 2016}, {"title": "Semi-supervised learning with ladder networks", "author": ["A. Rasmus", "M. Berglund", "M. Honkala", "H. Valpola", "T. Raiko"], "venue": "In NIPS,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["D.J. Rezende", "S. Mohamed", "D. Wierstra"], "venue": "In ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Variational inference with normalizing flows", "author": ["D.J. Rezende", "S. Mohamed"], "venue": "In ICML,", "citeRegEx": "Rezende and Mohamed,? \\Q2015\\E", "shortCiteRegEx": "Rezende and Mohamed", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-fei"], "venue": null, "citeRegEx": "Russakovsky et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2014}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR workshop,", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": null, "citeRegEx": "Vincent et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Learning to samples: With application to amoritized mle for generalized adversarial learning", "author": ["D. Wang", "Q. Liu"], "venue": "In arXiv:1611.01722v2,", "citeRegEx": "Wang and Liu,? \\Q2016\\E", "shortCiteRegEx": "Wang and Liu", "year": 2016}, {"title": "Betanegative binomial process and Poisson factor analysis", "author": ["M. Zhou", "L. Hannah", "D. Dunson", "L. Carin"], "venue": "In AISTATS,", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 30, "context": "The autoencoder (Vincent et al., 2010) is a widely employed unsupervised framework to learn (typically) lowdimensional features from complex data.", "startOffset": 16, "endOffset": 38}, {"referenceID": 20, "context": "The VAE may also be scaled to handle massive amounts of data, such as ImageNet (Pu et al., 2016).", "startOffset": 79, "endOffset": 96}, {"referenceID": 10, "context": "Additionally, when given labels on a subset of data, a classifier may be associated with the latent features, allowing for semi-supervised learning (Kingma et al., 2014; Pu Duke University. Correspondence to: Yunchen Pu <yunchen.pu@duke.edu>. et al., 2016).", "startOffset": 148, "endOffset": 256}, {"referenceID": 20, "context": "Further, the latent features (codes) may be associated with state-of-the-art natural-language-processing models, such as the Long Short-Term Memory (LSTM) network, for semi-supervised learning of text captions from an image (Pu et al., 2016).", "startOffset": 224, "endOffset": 241}, {"referenceID": 25, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 10, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 23, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 20, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 9, "context": "VAEs are typically trained by maximizing a variational lower bound of the data log-likelihood (Kingma & Welling, 2014; Mnih & Gregor, 2014; Rezende et al., 2014; Kingma et al., 2014; Ranganath et al., 2016; Pu et al., 2016; Kingma et al., 2016).", "startOffset": 94, "endOffset": 244}, {"referenceID": 25, "context": "This requirement has motivated design of encoders in which a neural network maps input data to the parameters of a distribution in the exponential family (Kingma & Welling, 2014; Rezende et al., 2014), which serves as the latent-features distribution.", "startOffset": 154, "endOffset": 200}, {"referenceID": 25, "context": "For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).", "startOffset": 62, "endOffset": 152}, {"referenceID": 23, "context": "For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).", "startOffset": 62, "endOffset": 152}, {"referenceID": 0, "context": "For example, Gaussian distributions have been widely utilized (Kingma & Welling, 2014; Rezende et al., 2014; Ranganath et al., 2016; Burda et al., 2016).", "startOffset": 62, "endOffset": 152}, {"referenceID": 0, "context": "Researchers have recently considered the idea of viewing the encoder as a proposal distribution for the latent features, thus using importance weighting when sampling from this distribution (Burda et al., 2016).", "startOffset": 190, "endOffset": 210}, {"referenceID": 1, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016).", "startOffset": 112, "endOffset": 157}, {"referenceID": 14, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016).", "startOffset": 112, "endOffset": 157}, {"referenceID": 1, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016). Motivated by these ideas, we present a new approach for learning the distribution of latent features within a VAE framework. We recognize that the need for an explicit form for the encoder distribution is only a consequence of the fact that learning is performed based on the variational lower bound. For inference (e.g., at test time), we do not need an explicit form for the distribution of latent features, we only require fast sampling from the encoder. Consequently, in the proposed approach, we no longer explicitly use the variational lower bound to train the encoder. Rather, we seek an encoder that minimizes the Kullback-Leibler (KL) distance between the distribution of codes and the true (posterior) distribution on the latent features. To minimize this KL distance, we generalize use of Stein\u2019s identity, and employ ideas associated with a reproducing kernel Hilbert space (RKHS). Analogous work was introduced by Liu & Wang (2016); Wang & Liu (2016); however, they considered sampling from a general unnormalized distribution.", "startOffset": 113, "endOffset": 1104}, {"referenceID": 1, "context": "From a different perspective, recent work extended ideas from Stein\u2019s identity and operator to machine learning (Chwialkowski et al., 2016; Liu et al., 2016). Motivated by these ideas, we present a new approach for learning the distribution of latent features within a VAE framework. We recognize that the need for an explicit form for the encoder distribution is only a consequence of the fact that learning is performed based on the variational lower bound. For inference (e.g., at test time), we do not need an explicit form for the distribution of latent features, we only require fast sampling from the encoder. Consequently, in the proposed approach, we no longer explicitly use the variational lower bound to train the encoder. Rather, we seek an encoder that minimizes the Kullback-Leibler (KL) distance between the distribution of codes and the true (posterior) distribution on the latent features. To minimize this KL distance, we generalize use of Stein\u2019s identity, and employ ideas associated with a reproducing kernel Hilbert space (RKHS). Analogous work was introduced by Liu & Wang (2016); Wang & Liu (2016); however, they considered sampling from a general unnormalized distribution.", "startOffset": 113, "endOffset": 1123}, {"referenceID": 0, "context": "Recently, Burda et al. (2016); Mnih & Rezende (2016) showed that the multi-sample (k samples) importance-weighted estimator", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Recently, Burda et al. (2016); Mnih & Rezende (2016) showed that the multi-sample (k samples) importance-weighted estimator", "startOffset": 10, "endOffset": 53}, {"referenceID": 0, "context": "Inspired by Burda et al. (2016), the following theorem (proved in Appendix A) shows that increasing the number of samples k is guaranteed to reduce the KL divergence and provide a better approximation of target distribution.", "startOffset": 12, "endOffset": 32}, {"referenceID": 0, "context": "However, Burda et al. (2016) has showed that the estimator based on log of importance weighted average will not lead to high variance.", "startOffset": 9, "endOffset": 29}, {"referenceID": 20, "context": "Motivated by assigning the same weight to every data point (Pu et al., 2016), we set \u03b6 = NX/(C\u03c1) in the experiments, where NX is the dimension of xn, C is the number of categories for the corresponding label and \u03c1 is the proportion of labeled data in the mini-batch.", "startOffset": 59, "endOffset": 76}, {"referenceID": 29, "context": "We do not perform any dataset-specific tuning or regularization other than dropout (Srivastava et al., 2014) and early stopping on validation sets.", "startOffset": 83, "endOffset": 108}, {"referenceID": 32, "context": "Poisson Factor Analysis Given a discrete vector xn \u2208 Z+, Poisson factor analysis (Zhou et al., 2012) assumes xn is a weighted combination of V latent factors xn \u223c Pois(\u03b8zn), where \u03b8 \u2208 RP\u00d7V + is the factor loadings matrix and zn \u2208 R+ is the vector of factor scores.", "startOffset": 81, "endOffset": 100}, {"referenceID": 2, "context": "\u03b8 is first learned by Markov chain Monte Carlo (MCMC) (Gan et al., 2015).", "startOffset": 54, "endOffset": 72}, {"referenceID": 25, "context": "Method NLL DGLM (Rezende et al., 2014) 89.", "startOffset": 16, "endOffset": 38}, {"referenceID": 0, "context": "10 VAE + IWAE (Burda et al., 2016)\u2020 86.", "startOffset": 14, "endOffset": 34}, {"referenceID": 0, "context": "76 IWAE + IWAE (Burda et al., 2016)\u2021 84.", "startOffset": 15, "endOffset": 35}, {"referenceID": 4, "context": "analysis (Han et al., 2016).", "startOffset": 9, "endOffset": 27}, {"referenceID": 0, "context": "We further estimate the marginal log-likelihood/perplexity values via the stochastic variational lower bound, as the mean of 5K-sample importance weighting estimate (Burda et al., 2016).", "startOffset": 165, "endOffset": 185}, {"referenceID": 22, "context": "\u00a7(Larochelle & Laulyi, 2012); \u2020(Ranganath et al., 2015); \u2021(Miao et al.", "startOffset": 31, "endOffset": 55}, {"referenceID": 16, "context": ", 2015); \u2021(Miao et al., 2016).", "startOffset": 10, "endOffset": 29}, {"referenceID": 0, "context": "2 or Stein VIWAE in Section 3; the second term denotes the testing log-likelihood/perplexity is estimated by the ELBO in (16) or the stochastic variational lower bound, S-ELBO (Burda et al., 2016).", "startOffset": 176, "endOffset": 196}, {"referenceID": 22, "context": "For the text corpora, we build a three-layer deep Poisson network (Ranganath et al., 2015).", "startOffset": 66, "endOffset": 90}, {"referenceID": 22, "context": "For the text corpora, we build a three-layer deep Poisson network (Ranganath et al., 2015). The sizes of hidden units are 200, 200 and 50 for the first, second and third layer, respectively (see Ranganath et al. (2015) for detailed architectures).", "startOffset": 67, "endOffset": 219}, {"referenceID": 3, "context": "Gregor et al. (2015) and Oord et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Gregor et al. (2015) and Oord et al. (2016), which exploit spatial structure, achieved log-likelihoods of around -80 nats.", "startOffset": 0, "endOffset": 44}, {"referenceID": 27, "context": "We consider semi-supervised classification on MNIST and ImageNet (Russakovsky et al., 2014) data.", "startOffset": 65, "endOffset": 91}, {"referenceID": 10, "context": "\u00a7(Kingma et al., 2014); \u2020our implementation.", "startOffset": 1, "endOffset": 22}, {"referenceID": 15, "context": "We use the leaky rectified activation (Maas et al., 2013).", "startOffset": 38, "endOffset": 57}, {"referenceID": 24, "context": "State-of-the-art results (Rasmus et al., 2015) are achieved by the Ladder network, which can be employed with our Stein-based approach, however, we will consider this extension as future work.", "startOffset": 25, "endOffset": 46}, {"referenceID": 9, "context": "We consider two types of decoders for images p(xn|zn,\u03b8) and encoder f\u03b7(x, \u03be): (i) FNN: Following Kingma et al. (2014), we use a 50-dimensional latent variables zn and two hidden layers, each with 600 hidden units, for both encoder and decoder; softplus log(1 + e) is employed as the nonlinear activation function.", "startOffset": 97, "endOffset": 118}, {"referenceID": 9, "context": "We consider two types of decoders for images p(xn|zn,\u03b8) and encoder f\u03b7(x, \u03be): (i) FNN: Following Kingma et al. (2014), we use a 50-dimensional latent variables zn and two hidden layers, each with 600 hidden units, for both encoder and decoder; softplus log(1 + e) is employed as the nonlinear activation function. (ii) All convolutional nets (CNN): Inspired by Springenberg et al. (2015), we replace the two hidden layers with 32 and 64 kernels of size 5 \u00d7 5 and a stride of 2.", "startOffset": 97, "endOffset": 388}, {"referenceID": 20, "context": "\u2020(Pu et al., 2016) VAE Stein VAE Stein VIWAE DGDN\u2020 1 % 35.", "startOffset": 1, "endOffset": 18}, {"referenceID": 28, "context": "We employ all convolutional net (Springenberg et al., 2015) for both the encoder and decoder, which replaces deterministic pooling (e.", "startOffset": 32, "endOffset": 59}, {"referenceID": 21, "context": "Such a model has been shown to be effective for training higher resolution and deeper generative models (Radford et al., 2016).", "startOffset": 104, "endOffset": 126}, {"referenceID": 5, "context": "The decoder for labels is employed as global average pooling with softmax, which has been utilized in state-of-the-art image classification models (He et al., 2016).", "startOffset": 147, "endOffset": 164}, {"referenceID": 15, "context": "We use the leaky rectified activation (Maas et al., 2013).", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "Residual connections (He et al., 2016) are incorporated to encourage gradient flow.", "startOffset": 21, "endOffset": 38}, {"referenceID": 11, "context": "A 224\u00d7 224 crop is randomly sampled from the images or its horizontal flip with the mean subtracted (Krizhevsky et al., 2012).", "startOffset": 100, "endOffset": 125}, {"referenceID": 20, "context": "When the proportion of labeled examples is too small (< 10%), DGDN outperforms all the VAE-based models, which is not surprising provided that our models are deeper, thus have considerably more parameters than DGDN (Pu et al., 2016).", "startOffset": 215, "endOffset": 232}, {"referenceID": 5, "context": "The decoder for labels is employed as global average pooling with softmax, which has been utilized in state-of-the-art image classification models (He et al., 2016). Batch normalization (Ioffe & Szegedy, 2015) is used to stabilize learning by normalizing the activations throughout the network and preventing relatively small parameter changes from being amplified into larger but suboptimal activation changes in other layers. We use the leaky rectified activation (Maas et al., 2013). Residual connections (He et al., 2016) are incorporated to encourage gradient flow. The model architecture is detailed in Appendix E. Following Krizhevsky et al. (2012), images are resized to 256\u00d7 256.", "startOffset": 148, "endOffset": 656}], "year": 2017, "abstractText": "A new method for learning variational autoencoders is developed, based on an application of Stein\u2019s operator. The framework represents the encoder as a deep nonlinear function through which samples from a simple distribution are fed. One need not make parametric assumptions about the form of the encoder distribution, and performance is further enhanced by integrating the proposed encoder with importance sampling. Example results are demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.", "creator": "LaTeX with hyperref package"}}}