{"id": "1704.05415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "An Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence Identification", "abstract": "end - to - end linguistics machine translation uses disadvantage statistical machine translation in economies of translation complexity because some language translators, demanding tasks making a large the more parallel data stored. over this palpable improvement, programming tasks embrace generally versatile aims. a single system can be trained to translate between many languages towards almost certain additional cost hence cost training time. furthermore, internal representations learned by the dual register as a new semantic function of consonants - or sentences - generally, unlike standard word embeddings, are crafted out an essentially bilingual or even unified dialect. during view representing these properties, the contribution of the present encoding is backward - legged. together, we readily estimate the component levels, or. e. output alongside the encoder, and represent capacities as an information tree of a sentence. their quality and flexibility are characterized by common measures across translations, semantically related, partially semantically unrelated sentence pairs. lastly, over whatever linguistic evaluation completes the first point, will identify parallel sentences related input values, obtaining r f1 = 52. 2 % on statements from a spoken dialogue when using only context vectors. f1 reaches 46. 9 % when complementary semantic programs are used.", "histories": [["v1", "Tue, 18 Apr 2017 16:38:01 GMT  (294kb,D)", "http://arxiv.org/abs/1704.05415v1", "15 pages, 3 figures"]], "COMMENTS": "15 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cristina espa\\~na-bonet", "\\'ad\\'am csaba varga", "alberto barr\\'on-cede\\~no", "josef van genabith"], "accepted": false, "id": "1704.05415"}, "pdf": {"name": "1704.05415.pdf", "metadata": {"source": "CRF", "title": "An Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence Identification", "authors": ["Cristina Espa\u00f1a-Bonet", "\u00c1d\u00e1m Csaba Varga", "Alberto Barr\u00f3n-Cede\u00f1o", "Josef van Genabith"], "emails": ["cristinae@dfki.de,", "adamcs.varga@gmail.com,", "albarron@hbku.edu.qa,", "Josef.Van_Genabith@dfki.de"], "sections": [{"heading": "1 Introduction", "text": "End-to-end neural machine translation systems (NMT) emerged in 2013 (Kalchbrenner\nand Blunsom, 2013) as a promising alternative to statistical and rule-based systems. Nowadays, they are more than a promise. NMT systems are the state-of-the-art for language pairs with a large amount of parallel data (Bojar et al., 2016) and have nice properties that other paradigms lack. Just to point out three of them, being a deep learning architecture NMT does not require manually predefined features, it allows for the simultaneous training of systems across multiple languages, and it can provide zero-shot translations, i.e. translations for language pairs not directly seen in the training data (Ha et al., 2016; Johnson et al., 2016).\nMultilingual neural machine translation systems (ML-NMT) have particularly interesting features. For performing multilingual translation, the network must project all the languages into the same common embedding space. In principle this space is multilingual, but the network is doing more than simply locating words according to their language and meaning independently. Previous analyses suggest that the network locates words according to their semantics, irrespective of their language (Sutskever et al., 2014; Ha et al., 2016; Johnson et al., 2016). That is somehow reinforced by the fact that zero-shot translation is possible (even if with low quality). If that is confirmed, ML-NMT systems are learning a representation akin to an interligua for a source text and such interlingual embeddings could be used to assess cross-language similarity, among other applications.\nIn the past, the analysis of internal embeddings in NMT systems has been limited to visual studies; e.g., showing the proximity between semantically-similar representations. In the first part of this paper, we go beyond such graphical analyses and search for empir-\nar X\niv :1\n70 4.\n05 41\n5v 1\n[ cs\n.C L\n] 1\n8 A\npr 2\n01 7\nical evidence of the interlinguality. We address four specific research questions: (i) we investigate whether the embedding learned by the network for a source text also depends on the target language, (ii) how close representations of sentence pairs of a same language and across languages are, (iii) how distinguishable representations of semantically-similar and semantically-distant sentence pairs are, and (iv) how representations evolve throughout the training. These questions are addressed by means of statistics on cosine similarities between pairs of sentences both in a monolingual and a crosslingual setting. In order to do that, we perform a large number of experiments using parallel and comparable data in Arabic, English, French, German, and Spanish (ar, en, fr, de, and es from here onwards).\nThe second part of the paper is devoted to an application of the findings gathered in the first part. We explore the use of the \u201cinterlingua\u201d representations to extract parallel sentences from comparable corpora. In this context, comparable corpora are text data on the same topic that are not direct translations of each other but that may contain fragments that are translation equivalents. Examples include Wikipedia or news articles on the same subject in different languages. We evaluate the performance of supervised classification algorithms based upon our best contextual representations when discriminating between parallel and non-parallel sentences in them.\nThe remaining of the article is organised as follows. Section 2 gives some background about NMT systems and describes related work. Section 3 describes the ML-NMT engines used in our analysis. Section 4 reports the study on the NMT-derived interlingual embeddings and discusses the results obtained. Section 5 presents a use case where we use the embeddings for identifying parallel sentences. The conclusions are drawn in Section 6."}, {"heading": "2 Background", "text": "State-of-the-art NMT systems utilise a twostage encoder\u2013decoder architecture with recurrent neural networks (RNN) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). The purpose of the encoder is to project source\nsentences into an embedding space. The purpose of the decoder is to generate target sentences from the encoder embeddings.\nLet s = (x1, . . . , xn) be a source sentence of length n. The encoder encodes s as a set of context vectors1, one per word:\nc = {h1,h2, . . . ,hn} . (1)\nEach component of this vector is obtained by concatenating the forward ( \u2212\u2192 h i) and backward ( \u2190\u2212 h i) encoder RNN hidden states:\nhi = [\u2190\u2212 h i, \u2212\u2192 h i ] (2)\n= [ f( \u2190\u2212 h i\u22121, r), f( \u2212\u2192 h i+1, r) ] , (3)\nwhere f is a recurrent unit (Gated Recurrent Units (GRU) (Cho et al., 2014) in our experiments) and r is a representation of the source sentence given by the product of the word embeddings matrix and the one-hot vector representation of xi: r = Wx \u00b7 xi.\nThe decoder generates the output sentence (y1, . . . , ym) of length m on a word-by-word basis. The recurrent hidden state of the decoder zj is computed using its previous hidden state zj\u22121, as well as the previous continuous representation of the target word tj\u22121 and the weighted context vector qj at time step j:\nzj = g(zj\u22121, tj\u22121,qj) (4) tj\u22121 = Wy \u00b7 yj\u22121, (5)\nwhere g is the recurrent unit of the decoder and Wy is the matrix of the target embeddings. The weighted context vector qj is calculated by the attention mechanism as described in Bahdanau et al. (2014). Its function is to assign weights to the context vectors in order to selectively focus on different source words at different time steps of the translation. To this end, a single-hidden-layer feedforward neural network is utilised that assigns relevance scores (a, as they can be interpreted as alignment scores) to the context vectors, which are then normalised into probabilities by the softmax\n1Called \u201cannotation vectors\u201d by Bahdanau et al. (2014), who use \u201ccontext vectors\u201d to designate the vectors after the attention mechanism.\nfunction:\na(zj\u22121,hi) = va \u00b7 tanh(Wa \u00b7 zj\u22121 + Ua \u00b7 hi) \u03b1ij = exp (a(zj\u22121,hi))\u2211\nk\nexp (a(zj\u22121,hk)) , qj = \u2211 i \u03b1ijhi\nThe attention mechanism takes the decoder\u2019s previous hidden state zj\u22121 and the context vector hi as inputs and weighs them up with the trainable weight matrices Wa and Ua, respectively.\nA number of papers extend this architecture to deal with multilingual translation by using multiple encoders and/or decoders either with multiple or shared attention mechanisms (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016). A simpler approximation (Ha et al., 2016; Johnson et al., 2016) considers exactly the same architecture as the one-toone NMT for many-to-many NMT using multilingual data with some additional labelling. Johnson et al. (2016) append the tag of the target language to the source-side sentences, forcing the decoder to translate to the appropriate language. Ha et al. (2016) also include tags specifying the language of every source word. Both papers show how these ML-NMT architectures can improve translation quality between under-resourced language pairs and how they can even be used for zero-shot translation. Given the premise that the encoder of an NMT system projects sentences into an embedding space, we can expect the encoder of ML-NMT systems to project sentences in different languages into a common (interlingual) embedding space. One of our aims is to study the characteristics of the internal representations of the encoder module in a ML-NMT system, and validate this assumption (see Section 4).\nThere is some relevant previous research on qualitative studies of NMT embedding space. Sutskever et al. (2014) show how a monolingual NMT encoder represents sentences with similar meaning close in embedding space. They show graphically \u2014with two instance sentences\u2014 that clustering by meaning goes beyond a bag-of-words understanding, and that differences caused by the order of words are reflected in the representation. Ha et al.\n(2016) go a step further and visualise the internal space in a many-to-one language NMT system. A 2D-representation of some multilingual word embeddings from the encoder after training displays translations and related words close together. Finally, Johnson et al. (2016) provide a visual evidence for a shared space for the attention vectors in an ML-NMT setup. Sentences with the same meaning but in different languages group together, except in the case of zero-shot translations. When a language pair has not been seen during training, the embeddings lie in a different region of the space. Notice that in the latter case (Johnson et al., 2016) the authors study the representation generated by the attention vectors, that is, the vectors showing the activations in the layer between encoder and decoder. The activations indicate which part of the source sentence is important during decoding to produce a particular chunk of the translation. Although the attention mechanism is shared across all the languages, the relevant chunks in the source sentence can vary depending on the target language.\nIn contrast to previous relevant research, in our work here we focus on the context vectors: the concatenation of the hidden states from the forward and the backward network in the encoding module \u2014right before applying the attention mechanism. Our goal goes beyond understanding the internal representations that the network learns. We aim at finding an appropriate representation to assess multilingual similarity. With this objective in mind, we are looking for a representation that is as target-independent as possible. Similarity assessment is at the core of many natural language processing and information retrieval tasks. Paraphrase identification, which has been also applied to machine translation (Shimohata, 2004), is essentially similarity assessment, and so is the task of plagiarism detection (Potthast et al., 2010). In multi-document summarisation (Goldstein et al., 2000) finding two highly-similar pieces of information from two texts may imply it is worth adding it into a good summary. In information retrieval, and in particular in question answering (Hirschman and Gaizauskas, 2001), a high similarity between a document and an\ninformation request is a key factor of relevance. As expected, similarity estimation also plays an important role in machine translation. It is essential in MT evaluation and, in the current cross-language setting, to identify parallel corpora to feed a (neural) machine translation model with (Munteanu and Marcu, 2005). Efforts have been carried out to approach crosslanguage versions of these tasks without translating all the texts into one common language (e.g., (Bouma et al., 2008; Mun\u0303oz Terol et al., 2008; Potthast et al., 2011)), but using interlingua or multilingual representations instead. Still, such representations are usually hard to design and this is precisely when our neural context vector NMT embedding representation comes into play. A multilingual encoder offers an environment where interlingua representations are learnt in a multilingual context. To some extent, it can be thought as a generalisation of methods that project monolingual embeddings in two different languages into a common space to obtain bilingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Madhyastha and Espan\u0303a-Bonet, 2016).\nLarge amounts of parallel corpora are needed for training high-performance NMT systems and these are not always readily available. Since the NMT embeddings are learnt in a translation task, it seems natural to use them not only to generate translations but also to detect them in the first place, and in particular in comparable (rather than parallel) corpora. For low-resourced language pairs parallel data are scarce and, even for well-resourced language pairs, data on specific domains are often hard to obtain. Automatically extracting parallel corpora is then an important issue and a necessary step in many data settings."}, {"heading": "3 NMT Systems Description", "text": "We carry out our experiments with two multilingual many-to-many NMT systems trained with Nematus (Sennrich et al., 2017). As done in Johnson et al. (2016) and similarly to Ha et al. (2016), our systems are trained on parallel corpora for several language pairs simultaneously, with the only addition of a tag in the source sentence to account for the target language \u201c<2L2>\u201d (e.g., <2ar> when the target language is Arabic). Table 1 summarises the key parameters of the engines.\nSince our aim is to study the capability of NMT representations to characterise similar sentences within and across languages, we selected languages for which text similarity test sets and/or translation test sets are available. First we build a ML-NMT engine for ar, en, and es. We trained the multilingual system for the 6 language pair directions on 56M parallel sentences in total with less than 50 tokens (\u223c 10M parallel sentences per language pair). We used 1024 hidden units, which correspond to 2048-dimensional context vectors. The parallel corpus includes data from United Nations (Rafalovitch and Dale, 2009), Common Crawl2, News Commentary3 and IWSLT4. We train system S1-w after cleaning and tokenising the texts. We used MADAMIRA (Pasha et al., 2014) for Arabic and the Moses tokeniser for the other languages. A second system called S1-l is trained on lemmatised sentences using the MADAMIRA lemmatiser for Arabic, and the IXA pipeline (Agerri et al., 2014) for\n2http://commoncrawl.org 3http://www.casmacat.eu/corpus/\nnews-commentary.html 4https://sites.google.com/site/ iwsltevaluation2016/mt-track\nEnglish and Spanish. In both cases we employ a vocabulary of 60K tokens plus 2K for subword units, segmented using Byte Pair Encoding (BPE) (Sennrich et al., 2016). For validation, we use sentences shorter than 50 tokens from different corpora (1500 sentences in es\u2013en from newstest20125, 1000 sentences in ar\u2013en from eTIRR Arabic English News Text6, and a 1000 sentences partition in ar\u2013es from News Commentary).\nWe build a second ML-NMT engine for de, fr, en, and es. We train the system with data on 4 language pairs: de\u2013en, fr\u2013en, es\u2013 en and es\u2013fr. Although some corpora exist for the remaining two (es\u2013de and fr\u2013de), we exclude them to study these pairs as instances of zero-shot translation. The parallel corpus includes data from United Nations (Chen and Eisele, 2012), Common Crawl, Europarl (Koehn, 2005), EMEA (Tiedemann, 2009) and Scielo7. We obtain about 15M parallel sentences per language pair \u2014notice that for de\u2013en, we needed oversampling in order to reach the same amount of data and we triple the original sentences. The 22K sentences shorter than 50 tokens in the newstest2012 for the 4 main language pairs are used for validation purposes. For this engine, we use a larger vocabulary size, 80K type tokens plus 2K for BPE, given that, compared to our first set of experiments, one language more is involved. Regarding the number of hidden units, we experiment with three configurations: S2w-d512, S2-w-d1024, and S2-w-d2048.\n3.1 Test Sets and Evaluation\nIn order to assess the degree of similarity between sentences, we consider three types of test sets. The source side is always the same and it is aligned to a target set that contains either: (i) exact translations of the source, (ii) highly-similar sentences (both mono- and cross-language), and (iii) unrelated sentences (both mono- and cross-language). For Arabic, English and Spanish, we build the three kinds of pairs out of the recently-held \u201cSemantic Textual Similarity Task at SemEval\n5http://www.statmt.org/wmt14/translation-task. html\n6LDC2004E72 available from the Linguistic Data Consortium\n7http://www.scielo.org\n2017\u201d (Agirre et al., 2017 (to appear)8). The task asks to assess the similarity between two texts within the range [0, 5], where 5 stands for semantic equivalence. We extract the subset of sentences with the highest similarity, 4 and 5, and use 140 sentences originally derived from the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) (MSR), and 203 sentences from WMT20089 to build our final test set with 343 sentences (subSTS2017). These data were available for ar and en but not for es, so we manually translated the MSR part of the corpus into es, and gathered the Spanish counterparts of the WMT2008 from the official set. With this process, we generated the test with translations (trad) and highsimilarity sentence pairs (semrel). We shuffled one of the sides of the test set to generate the unrelated pairs (unrel).\nIn order to simultaneously evaluate the German, French, English and Spanish experiments, we used the test set from WMT2013 (newstest2013); the last edition that includes these four languages. The test set contains 3000 sentences translated into the four languages. As before, we shuffle one of the sides to obtain the test set with unrelated sentence pairs, but we could not generate the equivalent set with high-similarity pairs."}, {"heading": "4 Context Vectors in Multilingual NMT Systems", "text": "The NMT architecture used for the experiments is the encoder\u2013decoder model with recurrent neural networks and attention mechanism described in Section 2, as implemented in Nematus.\nWe use the sum of the context vector associated to every word (Eq. 1) at a specific point of the training as the representation of a source sentence s:\nC = n\u2211 i=1 ci. (6)\nThis representation depends on the length of the sentence. However, we stick to this definition rather than using a mean over words because the length of the sentences is an indica-\n8http://alt.qcri.org/semeval2017/task1 9http://www.statmt.org/wmt08/\nshared-evaluation-task.html\ntor of their similarity. That is, sentences with similar meaning tend to have similar lengths.10\nGiven a sentence s1 represented by Cs1 and a sentence s2 represented by Cs2 , we can estimate their similarity by means of the cosine measure:\nsim(Cs1 ,Cs2) = Cs1 \u00b7Cs2 \u2016Cs1\u2016 \u2016Cs2\u2016 . (7)\n4.1 Graphical Analysis\nContext vectors are high-dimensional structures: for the standard 1024-dimensional hidden layers one has 2048-dimensional context vectors. In order to get a first impression on the behaviour of the embeddings, we project the vectors for a set of sentences into a 2D space using t-Distributed Stochastic Neighbour Embedding (t-SNE) (Van Der Maaten, 2014).\nWe consider 21 sentences extracted from the trial set of the Semantic Textual Similarity Task at SemEval 2017 for this purpose. Figure 1 shows the set of sentences and the relations between triplets. Sentences are divided into 7 triplets with 3 sentences each. Each sentence is an exact translation in ar, en and es. Some triplets are related semantically. For instance, a triplet with the element \u201cMandela\u2019s condition has improved\u201d is semantically related to the triplet with the element \u201cMandela\u2019s condition has worsened over past 48 hours\u201d. In a real multilingual space, one would expect sentences within a triplet lo lie together and sentences within related triplets to be close but, as Figure 2 shows, the range of behaviours may be diverse. The plot shows the evolution of the context vectors for these 21 sentences throughout the training (central panel), paying special attention to an early (left panel) and a late stage (right panel).\nAt the beginning of the training, English and Spanish sentences in a same triplet (same colour) lie close together and, for some triplets, they even overlap; see t4 and t7. This is an effect of having a representation that depends on the length of the sentence: the elements in t4 and t7 not only share some vocabulary but also have very similar lengths. Arabic sentences remain together, almost irrespective of their\n10We explored alternative combinations and this one resulted in the best performance.\nmeaning. One has to take into account that en and es are closer between them than to ar \u2014 they share a subject-verb-object structure and have many cognates. Meanwhile, ar is closer to es than to en \u2014Arabic influenced the Spanish language during 800 years. At this early stage in training, the closer languages have already been unified (en and es) and sentences can be grouped according to their semantics, but the most distant language (ar) is not in the same stage yet. For Arabic sentences, the language is more important than the semantics: sentence s9 is closer to s14 (another sentence in Arabic with similar length) than to s7 (a strict and longer translation of s9 into Spanish).\nAs training continues, Arabic sentences spread through the space and slowly tend to join their counterparts in the other languages. English and Spanish sentences also move apart towards a more general interlingua position. That is, there is a flow from near to overlapping locations for translations of the same sentence towards locations grouped by topic, irrespective of the language (look at the evolution of the related triplets t6 and t7 for example). This evolution must be considered if one wants to use context vectors as a semantic representation of a sentence: representations at different points of the training process might be useful for different tasks.\nThe expected behaviour, however, is not observed for all the triplets. It is the case for t1 or t5, where representations of the individual sentences get closer at every iteration. Still, it is not the case for other triplets, such as t6, in which sentences are farther away from each other at each iteration (notice that this triplet has the longest sentences and the highest length variation).\nA more systematic study is necessary in order to be able to draw strong conclusions. In the following sections we conduct such a study and draw conclusions quantitatively, rather than only qualitatively. We also determine at which point in the training process the internal representation of a sentence is optimal for our aim: parallel sentence selection from comparable data.\n4.2 Source vs. Source\u2013Target Semantic Representations\nThe training of the ML-NMT systems involves one-to-many instances, that is, for the same source language L1 one has different examples of translations into L2, L3 or L4. A first question one can address given this set up is whether the interpretation of a source sentence learnt by the network depends on the language it is going to be translated into or not. In a truly interlingual space, such a representation should be the same, or at least very close.\nIn order to test this, we computed the cosine similarity between the representation of a source sentence s when it is translated with the same engine into two different languages Li and Lj:\n< 2Li\u2212 2Lj >\u2261 sim(s<2Li>, s<2Lj>) . (8)\nSentence representations are extracted with engine S1-w for {ar, en, es} on subSTS2017 data and with engine S2-w-d1024 for {de, en, es, fr} on newstest2013. Afterwards, we compute the mean over all the sentences in a test set. Table 2 shows the results for this analysis.\nSimilarities are close to one in all cases; a number that would indicate that representations are fully equivalent, and are compatible with one within a 2\u03c3 interval. Although differences among languages and test sets are not statistically significant at that level, some general trends can be observed. Despite the fact that the similarity between instances of the same sentence is not one, it is larger than the similarity between closely related sentences when translated into the same language (see Section 4.3). That is, we can identify a sentence by a unique representation. Also notice that there is no difference either when we translate into a language without any direct parallel data (zero-shot translation): for es\u2013de and fr\u2013de, system S2-w-d1024 had no data, but the similarities involving these pairs (marked with an asterisk in Table 2) are not statistically-significantly different to those involving es\u2013fr and es\u2013en, for example.\nFinally, we can strengthen the correlation between the relatedness between languages and the closeness of the internal representations observed also via the first graphical analysis. The representation of an Arabic sentence\nwhen translated into en or es is almost the same (sim = 0.97 \u00b1 0.05), but the difference in the representation of a Spanish sentence when translated into ar or en is the largest one (sim = 0.91 \u00b1 0.05) due to the disparity between ar and en. The same effect, but at a lower degree, is observed in the {de, fr, en, es} case when making the distinction between {fr, es} and {de, en} as two groups of \u201cclose\u201d languages.\n4.3 Representations throughout Training\nDuring training, the network learns the most appropriate representation of words/sentences in order to be translated, so the embeddings themselves evolve over time. As we have seen with the graphical analysis (Section 4.1), it is interesting to follow this evolution and examine how sentences are grouped together depending on their language and semantics. For this, we analyse in parallel an engine trained on lemmatised sentences (S1-l) and one trained on tokenised sentences (S1-w). The rationale behind this is that the vocabulary in the lemmatised system is smaller and therefore can be better covered during training. Still, the ambiguity becomes higher, which could damage the quality of the representations.\nTable 3 shows the results. At the beginning of the training process, after having seen 4 \u00b7 106 sentences only, the results are still very much dependent on the language. Exact trans-\nlations in ar\u2013es have a similarity of 0.81\u00b10.04, whereas exact translations in ar\u2013en have a similarity of 0.44 \u00b1 0.07 (see the first row for system S1-lemmas). Perhaps for this reason monolingual pairs show higher similarity values than cross-language pairs, even for unrelated sentences (sim = 0.70 \u00b1 0.09 for ar and sim = 0.73 \u00b1 0.09 for en). Nevertheless, within a language pair the system has already learned the meaning of the sentences: cosine similarities are the highest for exact translations (trad), slightly lower for semantically related sentences (semrel) and significantly lower for unrelated sentences (unrel). The difference between the mean similarities\nobtained for translations and unrelated sentences,\n\u2206tr\u2212ur \u2261 \u2206(sim(trad)\u2212 sim(unrel)),\nshows how, already at this point, parallel sentences can be identified and located in the multilingual space, even though the similarity for translations is in general far from one and the similarity for unrelated sentences is far from zero.\nAt this starting point, sentences lie closer together irrespective of their meaning in the lemmatised system than in the tokenised one. Similarities are always higher for S1-l than for\nits counterpart in S1-w. The separation between translations and unrelated sentences is always more important in the S1-w (\u2206tr\u2212ur is higher). This is true all along the training process, confirming that the ambiguity introduced by the lemmatisation damages the representativity of the embeddings.\nWhen the training process has covered 28 \u00b7 106 sentences, half an epoch for this system, the difference among languages has diminished. Now sentences lie closer together in the tokenised system than in the lemmatised one, irrespective of their meaning. From this point onwards, this trait is maintained. Although all similarities keep going down throughout the training, even for translations, \u2206tr\u2212ur remains almost constant. The maximum value for this difference is found after one epoch (\u223c 56 \u00b7 106 sentences) for all the cross-language pairs in the tokenised system. In this case, \u2206tr\u2212ur is 0.34\u00b10.12 for ar\u2013en, 0.33\u00b10.13 for ar\u2013es and 0.43\u00b1 0.12 for en\u2013es. Again, the distinction is the clearest for the closest language pair and diminishes when Arabic is involved, mainly because translations involving Arabic are more difficult to detect (the mean similarity between en\u2013es translations is 0.74 \u00b1 0.06; 0.61 \u00b1 0.08 for ar\u2013en).\nAnalogous conclusions can be drawn from the {de, fr, en, es} engine. Table 4 includes the results. The maximum distinction between related and unrelated sentences, \u2206tr\u2212ur, is found after \u223c 56 \u00b7 106 sentences, half an\nepoch in this case, even though the difference was well established at a third of an epoch. \u2206tr\u2212ur is 0.3 \u00b1 0.1 when German is involved (de\u2013en, de\u2013es, de\u2013fr) and 0.4\u00b1 0.1 when it is not (en\u2013es, en\u2013fr, es\u2013fr). The difference is mostly given by the similarity between translations, which is higher when German is not concerned.\nNotice that this optimal point does not correspond to the optimal point regarding translation quality. Figure 3 displays the progression of the BLEU score along training for the English-to-Spanish translation. The dashed vertical line indicates the iteration where \u2206tr\u2212ur is maximum. At this time, the engine is still learning, as seen by the fact that the translation quality is clearly increasing.\nAnother interesting observation is that the expressivity of the embeddings does not depend on their dimensionality. Context vectors with 1024 dimensions (S2-w-d512), 2048 dimensions (S2-w-d1024) and 4096 dimensions (S2-w-d2048), lead to similar figures for similarity values between pairs of sentences. At the beginning of the training, S2-w-d1024 gives slightly better representations than the other two systems, but this difference is narrowed when the training advances. The training time almost doubles when doubling the dimensionality of the hidden layer, but this higher capacity does not result in a better description of the data. Actually, 4096-dimensional vectors perform worse than the 1024-dimensional ones at\nall the training stages. However, translation quality does depend on the size of the hidden layer and, in our experiments, S2-w-d2048 performs better than the lower-dimensional systems (see Figure 3 to observe the variation for the English-to-Spanish translation)."}, {"heading": "5 Use Case: Parallel Sentence Extraction", "text": "The previous section shows howML-NMT context vectors can be used as a representation that allows to calculate sensitive similarities between sentences with the potential to distinguish translations from non-translations and even translations from pairs with similar meaning. As a first application, we can use these representations learned when mapping parallel sentences \u2014the NMT system training\u2014 in order to detect new parallel pairs. In particular, we use a semantic similarity measure based on the context vectors obtained with the NMT system of Section 4 to extract parallel sentences and study its performance as compared to other measures.\nOur translation engine is the ML-NMT system for de, fr, en, and es, described in Section 3. According to the conclusions gathered in Section 4, we use system S2-w-d512 after half an epoch of training for extracting the context vectors. This system gives us the best trade-off between speed (low-dimensional vectors are extracted faster) and dissociation between translations and unrelated sentences, as this is the training point where the difference\n\u2206tr\u2212ur is maximum.\n5.1 Parallel Sentence Identification\nIn order to perform a complete analysis, we consider five additional/complementary measures to context vectors and test different scenarios. We borrow two well known representations from cross-language information retrieval to account for syntactic features by means of cosine similarities: (i) character n-grams (McNamee and Mayfield, 2004), considering n = [2, 5] and (ii) pseudo-cognates. From a natural language point of view, cognates are \u201cwords that are similar across languages\u201d (Manning and Schu\u0308tze, 1999). We relax the concept of cognate and consider as pseudo-cognates any words in two languages that share prefixes. To do so, tokens shorter than four characters are discarded, unless they contain nonalphabetical characters. The resulting tokens are cut off to four characters (Simard et al., 1993). The necessary preprocessing consists of casefolding, punctuation marks removal, and diacritics removal only. For the character ngram measure, we also remove spaces to better account for compounds in German. Besides, we include general features at sentence level such as (iii) token and (iv) character counts, and (v) the length factor measure (Pouliquen et al., 2003).\nWe test three different scenarios to observe the effect of context vectors when extracting sentence pairs and compare them against the other standard characterisations:\nctx: only context vectors are used,\ncomp: the set of five complementary measures is used,\nall: a combination of ctx and comp is used.\nFor each of these scenarios, we learn a binary classifier on annotated data. We use the de\u2013en and fr\u2013en training corpora provided for the shared task on identifying parallel sentences in comparable corpora at the \u201c10th Workshop on Building and Using Comparable Corpora\u201d11 (BUCC 2017). This set contains 1,454,890 sentences from Wikipedia\n11https://comparable.limsi.fr/bucc2017/ bucc2017-task.html\nand News Commentary from which approximately 20,000 are aligned sentence pairs. Negative indexes are manually added by randomly pairing up the same amount of non-matching pairs. From the full set, we use 35,000 instances for training and evaluating classifiers with 10-fold cross-validation, 4,000 instances for training an ensemble of the best classifiers and 1,000 instances for held-out testing purposes.\nFor ctx, where only the context vector similarities are considered, the problem can be reduced to finding a suitable decision threshold. To this end, similarity values between the lowest value among positive examples and the highest value among negative samples are incrementally increased by a step size of 0.005 and the threshold giving the highest accuracy on the training set is selected. With this methodology, we obtain a threshold t = 0.43 for de\u2013en leading to an accuracy of 97.2%, and 0.41 for fr\u2013en with an accuracy of 97.4%. These values are slightly lower than the ones reported in Table 4, but consistent with them. The thresholds in both cases depend on the language pair, but the fact that we are working with an interlingua representation makes the differences minimal. This allows us to estimate a joint threshold for the full training set in de\u2013en and fr\u2013en and later use this decision boundary for other language pairs. If we do the search on the joint datasets the best threshold is t = 0.43 leading to an accuracy of 97.2% in the training set.\nIn comp and all we have 7 and 8 features respectively, and we employ supervised classi-\nfiers rather than a simple threshold estimation. We train support vector machines (SVM) using the radial basis function kernel and gradient boosting (GB) on the deviance objective function with 10-fold cross-validation. A soft voting ensemble (Ens.) of these two algorithms is trained in order to obtain the final model. In all these scenarios, we use the implementations of Python\u2019s scikit-learn12 package.\nTable 5 shows Precision (P), Recall (R) and F1 scores for the three scenarios. Notice that a simple greedy threshold search is better than any of the machine learning counterparts when only context vectors are used, but differences are not significant. The greedy search on the context vector similarities gives a better F1 score on the held-out test set than an ensemble of SVM and GB operating only the set of additional features with almost no knowledge of semantics. As we have argued in the previous section, translations and non-translations are clearly differentiated by a cosine similarity of the context vectors for these pairs of languages, as the difference between the mean similarities of translations and unrelated texts is much higher than its uncertainty (\u2206tr\u2212ur= 0.36\u00b1 0.14 for de\u2013en, and 0.41\u00b1 0.14 for fr\u2013 en). This clear distinction in the similarities is translated into a F1 = 98.2% in the task of parallel sentence identification.\nDue to its interlingual nature, our feature behaves equally well in both language pairs and improves in the multilingual setting (joint columns in Table 5). On the contrary, the set\n12http://scikit-learn.org/stable/index.html\nof complementary features depends on the language pair and has a drop in performance for de\u2013en. For this reason, the results in the multilingual setting are always worse than in the bilingual one. This fact is inherited in the all scenario, where the classification for the joint corpus has F1 = 98.9%, which is lower than the one obtained for fr\u2013en alone (F1 = 99.3%). Nevertheless, semantic and syntactic similarity features are complementary and the combination of all similarity measures slightly improves precision, recall and F1 in the multilingual setting. Finally, it is worth noting the high recall derived from the context vectors, which reaches 100% for fr\u2013en and falls to 98.1% for the joint data, being still 6.5 points higher than for the comp features."}, {"heading": "6 Conclusions", "text": "In this article we provide evidence of the interlingual nature of the context vectors generated by an end-to-end multilingual neural machine translation system and study their power in the assessment of monolingual and crosslanguage similarity.\nWe investigate how the representation of a sentence varies in order to be accommodated to a particular target language and observe that the difference is negligible, even though it grows when we consider distant target languages such Arabic and English. Even in these cases, the representation of a sentence is unique enough as closely related sentences have a smaller similarity than different instances of a same sentence.\nResults also show that the contextual interlanguage vectors are able to differentiate among sentences with identical, similar, and different meaning across different languages \u2014 including Arabic, English, French, German, and Spanish. Our training-evolution experiments reveal that vectors at early training are the best ones for similarity assessment, whereas the optimal ones for translation require further training. Besides, whereas for reaching a good translation quality the dimensionality of the vectors is important, we show that the expressivity of the embeddings as regards semantic similarity within and across languages does not depend on their dimension.\nAs a direct application of our similarity fea-\nture, we identify parallel sentences in comparable corpora achieving a performance of F1 = 98.2% on data of the shared task on identifying parallel sentences in comparable corpora at BUCC 2017. The fact that this is an interlingual feature allows to use data on different languages simultaneously for setting a threshold or a classification model that can be later used on other languages without a loss of performance."}, {"heading": "Acknowledgments", "text": "Part of the research in this work has been funded by the Leibniz Gemeinschaft via the SAW-2016-ZPID-2 project. The research work of A. Barro\u0301n-Ceden\u0303o is carried out in the framework of the Interactive sYstems for Answer Search project (IYAS), at the Qatar Computing Research Institute, HBKU."}], "references": [{"title": "IXA pipeline: Efficient and Ready to Use Multilingual NLP Tools", "author": ["Rodrigo Agerri", "Josu Bermudez", "German Rigau."], "venue": "(Calzolari et al., 2014).", "citeRegEx": "Agerri et al\\.,? 2014", "shortCiteRegEx": "Agerri et al\\.", "year": 2014}, {"title": "Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation", "author": ["Eneko Agirre", "Daniel Cer", "Mona Diab", "I\u00f1igo Lopez-Gazpioa", "Lucia Specia"], "venue": null, "citeRegEx": "Agirre et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2017}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Findings of the 2016 Conference on Machine Translation", "author": ["Martin Popel", "Matt Post", "Raphael Rubino", "Carolina Scarton", "Lucia Specia", "Marco Turchi", "Karin Verspoor", "Marcos Zampieri."], "venue": "Proceedings of the First Con-", "citeRegEx": "Popel et al\\.,? 2016", "shortCiteRegEx": "Popel et al\\.", "year": 2016}, {"title": "Question Answering with Joost at CLEF 2008", "author": ["Gosse Bouma", "Jori Mur", "Gertjan van Noord."], "venue": "(Peters et al., 2008).", "citeRegEx": "Bouma et al\\.,? 2008", "shortCiteRegEx": "Bouma et al\\.", "year": 2008}, {"title": "MultiUN v2: UN Documents with Multilingual Alignments", "author": ["Yu Chen", "Andreas Eisele."], "venue": "Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet U\u011fur Do\u011fan, Bente Maegaard, Joseph Mariani, Asuncion Moreno,", "citeRegEx": "Chen and Eisele.,? 2012", "shortCiteRegEx": "Chen and Eisele.", "year": 2012}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Automatically constructing a corpus of sentential paraphrases", "author": ["Bill Dolan", "Chris Brockett."], "venue": "Third International Workshop on Paraphrasing (IWP2005). Asia Federation of Natural Language Processing.", "citeRegEx": "Dolan and Brockett.,? 2005", "shortCiteRegEx": "Dolan and Brockett.", "year": 2005}, {"title": "Multi-Task Learning for Multiple Language Translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Manaal Faruqui", "Chris Dyer."], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics. Asso-", "citeRegEx": "Faruqui and Dyer.,? 2014", "shortCiteRegEx": "Faruqui and Dyer.", "year": 2014}, {"title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "(Knight et al., 2016), pages 866\u2013875.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Multi-Document Summarization By Sentence Extraction", "author": ["Jade Goldstein", "Vibhu Mittal", "Jaime Carbonell", "Mark Kantrowitz."], "venue": "NAACL-ANLP 2000 Workshop on Automatic Summarization. Association for Computational", "citeRegEx": "Goldstein et al\\.,? 2000", "shortCiteRegEx": "Goldstein et al\\.", "year": 2000}, {"title": "Natural language question answering: the view from here", "author": ["L. Hirschman", "R. Gaizauskas."], "venue": "Natural Language Engineering 7(4):275\u2013300. https://doi.org/10.1017/S1351324901002807.", "citeRegEx": "Hirschman and Gaizauskas.,? 2001", "shortCiteRegEx": "Hirschman and Gaizauskas.", "year": 2001}, {"title": "Recurrent Continuous Translation Models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle,", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Conference Proceedings: the Tenth Machine Translation Summit. AAMT, AAMT, Phuket, Thailand, pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Fully Character-Level Neural Machine Translation without Explicit Segmentation", "author": ["Jason Lee", "Kyunghyun Cho", "Thomas Hofmann."], "venue": "CoRR abs/1610.03017.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Multitask Sequence to Sequence Learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Resolving Out-ofVocabulary Words with Bilingual Embeddings in Machine Translation", "author": ["Pranava Swaroop Madhyastha", "Cristina Espa\u00f1a-Bonet."], "venue": "CoRR abs/1608.01910.", "citeRegEx": "Madhyastha and Espa\u00f1a.Bonet.,? 2016", "shortCiteRegEx": "Madhyastha and Espa\u00f1a.Bonet.", "year": 2016}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Christopher D. Manning", "Hinrich Sch\u00fctze."], "venue": "The MIT Press.", "citeRegEx": "Manning and Sch\u00fctze.,? 1999", "shortCiteRegEx": "Manning and Sch\u00fctze.", "year": 1999}, {"title": "Character n-gram tokenization for european language text retrieval", "author": ["Paul McNamee", "James Mayfield."], "venue": "Information retrieval 7(1-2):73\u2013", "citeRegEx": "McNamee and Mayfield.,? 2004", "shortCiteRegEx": "McNamee and Mayfield.", "year": 2004}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever."], "venue": "CoRR abs/1309.4168.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "AliQAn, Spanish QA System at multilingual QA@CLEF2008", "author": ["R. Mu\u00f1oz Terol", "M. Puchol-Blasco", "M. Pardi\u00f1o", "J.M. G\u00f3mez", "S. Roger", "K. Vila", "A. Ferr\u00e1ndez", "J. Peral", "P. Mart\u0301inez-Barco"], "venue": "In (Peters et al.,", "citeRegEx": "Terol et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Terol et al\\.", "year": 2008}, {"title": "Improving Machine Translation Performance by Exploiting Non-Parallel Corpora", "author": ["Dragos Stefan Munteanu", "Daniel Marcu."], "venue": "Computational Linguistics 31(4):477\u2013504. https://doi.org/10.1162/089120105775299168.", "citeRegEx": "Munteanu and Marcu.,? 2005", "shortCiteRegEx": "Munteanu and Marcu.", "year": 2005}, {"title": "MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and", "author": ["Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth"], "venue": null, "citeRegEx": "Pasha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pasha et al\\.", "year": 2014}, {"title": "Cross-language plagiarism detection", "author": ["Martin Potthast", "Alberto Barr\u00f3n-Cede\u00f1o", "Benno Stein", "Paolo Rosso."], "venue": "Language Resources and Evaluation (LRE), Special Issue on Plagiarism and Authorship Analysis 45(1):1\u201318.", "citeRegEx": "Potthast et al\\.,? 2011", "shortCiteRegEx": "Potthast et al\\.", "year": 2011}, {"title": "An Evaluation Framework for Plagiarism Detection", "author": ["Martin Potthast", "Benno Stein", "Alberto Barr\u00f3nCede\u00f1o", "Paolo Rosso."], "venue": "Chu-Ren Huang and Dan Jurafsky, editors, Proceedings of the 23rd International Conference", "citeRegEx": "Potthast et al\\.,? 2010", "shortCiteRegEx": "Potthast et al\\.", "year": 2010}, {"title": "Automatic Identification of Document Translations in Large Multilingual Document Collections", "author": ["Bruno Pouliquen", "Ralf Steinberger", "Camelia Ignat."], "venue": "Proceedings of the International Conference on Recent Advances in", "citeRegEx": "Pouliquen et al\\.,? 2003", "shortCiteRegEx": "Pouliquen et al\\.", "year": 2003}, {"title": "United Nations General Assembly Resolutions: A Six-Language Parallel Corpus", "author": ["Alexandre Rafalovitch", "Robert Dale."], "venue": "Proceedings of the Machine Translation Summit XII. International Association of Machine Translation,", "citeRegEx": "Rafalovitch and Dale.,? 2009", "shortCiteRegEx": "Rafalovitch and Dale.", "year": 2009}, {"title": "Nematus: a Toolkit for Neural Machine Translation", "author": ["Hitschler", "Marcin Junczys-Dowmunt", "Samuel L\u00e4ubli", "Antonio Valerio Miceli Barone", "Jozef Mokry", "Maria Nadejde."], "venue": "Proceedings of the Demonstrations at the 15th", "citeRegEx": "Hitschler et al\\.,? 2017", "shortCiteRegEx": "Hitschler et al\\.", "year": 2017}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016,", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Acquiring Paraphrases from Corpora and Its Application to Machine Translation", "author": ["Mitsuo Shimohata."], "venue": "Ph.D. thesis, Nara Institute of Science and Technology, Nara, Japan.", "citeRegEx": "Shimohata.,? 2004", "shortCiteRegEx": "Shimohata.", "year": 2004}, {"title": "Using Cognates to Align Sentences in Bilingual Corpora", "author": ["Michel Simard", "George F Foster", "Pierre Isabelle."], "venue": "Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed", "citeRegEx": "Simard et al\\.,? 1993", "shortCiteRegEx": "Simard et al\\.", "year": 1993}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "News from OPUS A Collection of Multilingual Parallel Corpora with Tools and Interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Lan-", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Accelerating tSNE Using Tree-based Algorithms", "author": ["Laurens Van Der Maaten."], "venue": "Journal of Machine Learning Research 15(1):3221\u20133245.", "citeRegEx": "Maaten.,? 2014", "shortCiteRegEx": "Maaten.", "year": 2014}, {"title": "MultiSource Neural Translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "(Knight et al.,", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "End-to-end neural machine translation systems (NMT) emerged in 2013 (Kalchbrenner and Blunsom, 2013) as a promising alternative to statistical and rule-based systems.", "startOffset": 68, "endOffset": 100}, {"referenceID": 32, "context": "Previous analyses suggest that the network locates words according to their semantics, irrespective of their language (Sutskever et al., 2014; Ha et al., 2016; Johnson et al., 2016).", "startOffset": 118, "endOffset": 181}, {"referenceID": 6, "context": "State-of-the-art NMT systems utilise a twostage encoder\u2013decoder architecture with recurrent neural networks (RNN) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 114, "endOffset": 179}, {"referenceID": 32, "context": "State-of-the-art NMT systems utilise a twostage encoder\u2013decoder architecture with recurrent neural networks (RNN) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 114, "endOffset": 179}, {"referenceID": 2, "context": "State-of-the-art NMT systems utilise a twostage encoder\u2013decoder architecture with recurrent neural networks (RNN) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 114, "endOffset": 179}, {"referenceID": 6, "context": "where f is a recurrent unit (Gated Recurrent Units (GRU) (Cho et al., 2014) in our experiments) and r is a representation of the source sentence given by the product of the word embeddings matrix and the one-hot vector representation of xi: r = Wx \u00b7 xi.", "startOffset": 57, "endOffset": 75}, {"referenceID": 2, "context": "The weighted context vector qj is calculated by the attention mechanism as described in Bahdanau et al. (2014). Its function is to assign weights to the context vectors in order to selectively focus on different source words at different time steps of the translation.", "startOffset": 88, "endOffset": 111}, {"referenceID": 2, "context": "Called \u201cannotation vectors\u201d by Bahdanau et al. (2014), who use \u201ccontext vectors\u201d to designate the vectors after the attention mechanism.", "startOffset": 31, "endOffset": 54}, {"referenceID": 16, "context": "A number of papers extend this architecture to deal with multilingual translation by using multiple encoders and/or decoders either with multiple or shared attention mechanisms (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016).", "startOffset": 177, "endOffset": 277}, {"referenceID": 8, "context": "A number of papers extend this architecture to deal with multilingual translation by using multiple encoders and/or decoders either with multiple or shared attention mechanisms (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016).", "startOffset": 177, "endOffset": 277}, {"referenceID": 10, "context": "A number of papers extend this architecture to deal with multilingual translation by using multiple encoders and/or decoders either with multiple or shared attention mechanisms (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016).", "startOffset": 177, "endOffset": 277}, {"referenceID": 35, "context": "A number of papers extend this architecture to deal with multilingual translation by using multiple encoders and/or decoders either with multiple or shared attention mechanisms (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016).", "startOffset": 177, "endOffset": 277}, {"referenceID": 15, "context": "A number of papers extend this architecture to deal with multilingual translation by using multiple encoders and/or decoders either with multiple or shared attention mechanisms (Luong et al., 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016).", "startOffset": 177, "endOffset": 277}, {"referenceID": 8, "context": ", 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016). A simpler approximation (Ha et al., 2016; Johnson et al., 2016) considers exactly the same architecture as the one-toone NMT for many-to-many NMT using multilingual data with some additional labelling. Johnson et al. (2016) append the tag of the target language to the source-side sentences, forcing the decoder to translate to the appropriate language.", "startOffset": 8, "endOffset": 313}, {"referenceID": 8, "context": ", 2015; Dong et al., 2015; Firat et al., 2016; Zoph and Knight, 2016; Lee et al., 2016). A simpler approximation (Ha et al., 2016; Johnson et al., 2016) considers exactly the same architecture as the one-toone NMT for many-to-many NMT using multilingual data with some additional labelling. Johnson et al. (2016) append the tag of the target language to the source-side sentences, forcing the decoder to translate to the appropriate language. Ha et al. (2016) also include tags specifying the language of every source word.", "startOffset": 8, "endOffset": 460}, {"referenceID": 32, "context": "Sutskever et al. (2014) show how a monolingual NMT encoder represents sentences with", "startOffset": 0, "endOffset": 24}, {"referenceID": 30, "context": "chine translation (Shimohata, 2004), is essentially similarity assessment, and so is the task of plagiarism detection (Potthast et al.", "startOffset": 18, "endOffset": 35}, {"referenceID": 25, "context": "chine translation (Shimohata, 2004), is essentially similarity assessment, and so is the task of plagiarism detection (Potthast et al., 2010).", "startOffset": 118, "endOffset": 141}, {"referenceID": 11, "context": "In multi-document summarisation (Goldstein et al., 2000) finding two highly-similar pieces of information from two texts may imply it is worth adding it into a good summary.", "startOffset": 32, "endOffset": 56}, {"referenceID": 12, "context": "In information retrieval, and in particular in question answering (Hirschman and Gaizauskas, 2001), a high similarity between a document and an", "startOffset": 66, "endOffset": 98}, {"referenceID": 22, "context": "It is essential in MT evaluation and, in the current cross-language setting, to identify parallel corpora to feed a (neural) machine translation model with (Munteanu and Marcu, 2005).", "startOffset": 156, "endOffset": 182}, {"referenceID": 4, "context": ", (Bouma et al., 2008; Mu\u00f1oz Terol et al., 2008; Potthast et al., 2011)), but using interlingua or multilingual representations instead.", "startOffset": 2, "endOffset": 71}, {"referenceID": 24, "context": ", (Bouma et al., 2008; Mu\u00f1oz Terol et al., 2008; Potthast et al., 2011)), but using interlingua or multilingual representations instead.", "startOffset": 2, "endOffset": 71}, {"referenceID": 20, "context": "To some extent, it can be thought as a generalisation of methods that project monolingual embeddings in two different languages into a common space to obtain bilingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Madhyastha and Espa\u00f1a-Bonet, 2016).", "startOffset": 184, "endOffset": 265}, {"referenceID": 9, "context": "To some extent, it can be thought as a generalisation of methods that project monolingual embeddings in two different languages into a common space to obtain bilingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Madhyastha and Espa\u00f1a-Bonet, 2016).", "startOffset": 184, "endOffset": 265}, {"referenceID": 17, "context": "To some extent, it can be thought as a generalisation of methods that project monolingual embeddings in two different languages into a common space to obtain bilingual word embeddings (Mikolov et al., 2013; Faruqui and Dyer, 2014; Madhyastha and Espa\u00f1a-Bonet, 2016).", "startOffset": 184, "endOffset": 265}, {"referenceID": 29, "context": "We carry out our experiments with two multilingual many-to-many NMT systems trained with Nematus (Sennrich et al., 2017). As done in Johnson et al. (2016) and similarly to Ha et al.", "startOffset": 98, "endOffset": 155}, {"referenceID": 29, "context": "We carry out our experiments with two multilingual many-to-many NMT systems trained with Nematus (Sennrich et al., 2017). As done in Johnson et al. (2016) and similarly to Ha et al. (2016), our systems are trained on parallel corpora for several language pairs simultaneously, with the only addition of a tag in the source sentence to account for the target language \u201c<2L2>\u201d (e.", "startOffset": 98, "endOffset": 189}, {"referenceID": 27, "context": "The parallel corpus includes data from United Nations (Rafalovitch and Dale, 2009), Common Crawl2, News Commentary3 and IWSLT4.", "startOffset": 54, "endOffset": 82}, {"referenceID": 23, "context": "We used MADAMIRA (Pasha et al., 2014) for Arabic and the Moses tokeniser for the other languages.", "startOffset": 17, "endOffset": 37}, {"referenceID": 0, "context": "A second system called S1-l is trained on lemmatised sentences using the MADAMIRA lemmatiser for Arabic, and the IXA pipeline (Agerri et al., 2014) for", "startOffset": 126, "endOffset": 147}, {"referenceID": 29, "context": "In both cases we employ a vocabulary of 60K tokens plus 2K for subword units, segmented using Byte Pair Encoding (BPE) (Sennrich et al., 2016).", "startOffset": 119, "endOffset": 142}, {"referenceID": 5, "context": "The parallel corpus includes data from United Nations (Chen and Eisele, 2012), Common Crawl, Europarl (Koehn, 2005), EMEA (Tiedemann, 2009) and Scielo7.", "startOffset": 54, "endOffset": 77}, {"referenceID": 14, "context": "The parallel corpus includes data from United Nations (Chen and Eisele, 2012), Common Crawl, Europarl (Koehn, 2005), EMEA (Tiedemann, 2009) and Scielo7.", "startOffset": 102, "endOffset": 115}, {"referenceID": 33, "context": "The parallel corpus includes data from United Nations (Chen and Eisele, 2012), Common Crawl, Europarl (Koehn, 2005), EMEA (Tiedemann, 2009) and Scielo7.", "startOffset": 122, "endOffset": 139}, {"referenceID": 7, "context": "We extract the subset of sentences with the highest similarity, 4 and 5, and use 140 sentences originally derived from the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) (MSR), and 203 sentences from WMT20089 to build our final test set with 343 sentences (subSTS2017).", "startOffset": 160, "endOffset": 186}, {"referenceID": 19, "context": "We borrow two well known representations from cross-language information retrieval to account for syntactic features by means of cosine similarities: (i) character n-grams (McNamee and Mayfield, 2004), considering n = [2, 5] and (ii) pseudo-cognates.", "startOffset": 172, "endOffset": 200}, {"referenceID": 18, "context": "From a natural language point of view, cognates are \u201cwords that are similar across languages\u201d (Manning and Sch\u00fctze, 1999).", "startOffset": 94, "endOffset": 121}, {"referenceID": 31, "context": "The resulting tokens are cut off to four characters (Simard et al., 1993).", "startOffset": 52, "endOffset": 73}, {"referenceID": 26, "context": "Besides, we include general features at sentence level such as (iii) token and (iv) character counts, and (v) the length factor measure (Pouliquen et al., 2003).", "startOffset": 136, "endOffset": 160}], "year": 2017, "abstractText": "End-to-end neural machine translation has overtaken statistical machine translation in terms of translation quality for some language pairs, specially those with a large amount of parallel data available. Beside this palpable improvement, neural networks embrace several new properties. A single system can be trained to translate between many languages at almost no additional cost other than training time. Furthermore, internal representations learned by the network serve as a new semantic representation of words \u2014or sentences\u2014 which, unlike standard word embeddings, are learned in an essentially bilingual or even multilingual context. In view of these properties, the contribution of the present work is twofold. First, we systematically study the context vectors, i.e. output of the encoder, and their prowess as an interlingua representation of a sentence. Their quality and effectiveness are assessed by similarity measures across translations, semantically related, and semantically unrelated sentence pairs. Second, and as extrinsic evaluation of the first point, we identify parallel sentences in comparable corpora, obtaining an F1 = 98.2% on data from a shared task when using only context vectors. F1 reaches 98.9% when complementary similarity measures are used.", "creator": "LaTeX with hyperref package"}}}