{"id": "1704.05495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2017", "title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "abstract": "eligibility bias in reinforcement learning ( used as a pro - variance trade - off and can often speed up training times by propagating knowledge back up input - flows in a single occurrence. we define meaningful use of eligibility traces in combination with voting networks in the analytical domain. we assessed improvement effectiveness by both recurrent nets \u2013 eligibility currents in verbal guessing games, and specify virtually the nature of the optimization used facing the treatment.", "histories": [["v1", "Tue, 18 Apr 2017 18:46:12 GMT  (400kb,D)", "http://arxiv.org/abs/1704.05495v1", "8 pages, 3 figures, NIPS 2016 Deep Reinforcement Learning Workshop"]], "COMMENTS": "8 pages, 3 figures, NIPS 2016 Deep Reinforcement Learning Workshop", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["jean harb", "doina precup"], "accepted": false, "id": "1704.05495"}, "pdf": {"name": "1704.05495.pdf", "metadata": {"source": "CRF", "title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "authors": ["Jean Harb"], "emails": ["jharb@cs.mcgill.ca", "dprecup@cs.mcgill.ca"], "sections": [{"heading": "1 Introduction", "text": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)). We provide experiments in the Atari domain showing that eligibility traces boost the performance of Deep RL. We also demonstrate a surprisingly strong effect of the optimization method on the performance of the recurrent networks.\nThe paper is structured as follows. In Sec. 2 we provide background and notation needed for the paper. Sec. 3 describes the algorithms we use. In sec. 4 we present and discuss our experimental results. In Sec. 5 we conclude and present avenues for future work."}, {"heading": "2 Background", "text": "A Markov Decision Process (MDP) consists of a tuple \u3008S,A, r,P, \u03b3\u3009, where S is the set of states, A is the set of actions, r : S \u00d7 A 7\u2192 R is the reward function, P(s\u2032|s, a) is the transition function (giving the next state distribution, conditioned on the current state and action), and \u03b3 \u2208 [0, 1) is the discount factor. Reinforcement learning (RL) (Sutton & Barto, 1998) is a framework for solving unknown MDPs, which means finding a good (or optimal) way of behaving, also called a policy. RL works by obtaining transitions from the environment and using them, in order to compute a policy that maximizes the expected return, given by E [\u2211\u221e t=0 \u03b3 trt ] .\nThe state-value function for a policy \u03c0 : S \u00d7 A \u2192 [0, 1], V \u03c0(s), is defined as the expected return obtained by starting at state s and picking actions according to \u03c0. State-action values Q(s, a) are\nNIPS 2016, Deep Reinforcement Learning Workshop, Barcelona, Spain.\nar X\niv :1\n70 4.\n05 49\n5v 1\n[ cs\n.A I]\n1 8\nA pr\nsimilar to state values, but conditioned also on the initial action a. A policy can be derived from the Q values by picking always the action with the best estimated value at any state.\nMonte Carlo (MC) and Temporal Difference (TD) are two standard methods for updating the value function from data. In MC, an entire trajectory\u2019s return is used as the target value of the current state.\nMC error = \u221e\u2211 i=0 \u03b3irt+i \u2212 V (st) (1)\nIn TD, the estimate of the next state\u2019s value is used to correct the current state\u2019s estimate:\nTD error = rt + \u03b3V (st+1)\u2212 V (st) (2)\nQ-learning is an RL algorithm that allows an agent to learn by imagining that it will take the best possible action in the following step:\nTD error = rt + \u03b3max a\u2032\nQ(st+1, a \u2032)\u2212Q(st, at) (3)\nThis is an instance of off-policy learning, in which the agent gathers data with an exploratory policy, which randomizes the choice of action, but updates its estimates by constructing targets according to a differnet policy (in this case, the policy that is greedy with respect to the current value estimates."}, {"heading": "2.1 Eligibility Traces", "text": "Eligibility traces are a fundamental reinforcement learning mechanism which allows a trade-off between TD and MC. MC methods suffer from high variance, as many trajectories can be taken from any given state and stochasticity is often present in the MDP. TD suffers from high bias, as it updates values based on its own estimates.\nUsing eligibility traces allows one to design algorithms that cover the middle-ground between MC and TD. The central notion for these are n-step returns, which provide a way of calculating the target by using the value estimate for the state which occurs n steps in the future (compared to the current state):\nR (n) t = n\u22121\u2211 i=0 \u03b3irt+i + \u03b3 nV (st+n). (4)\nWhen n is 1, the results is the TD target, and taking n\u2192\u221e yields the MC target. Eligibility traces use a geometric weighting of these n-step returns, where the weight of the k-step return is \u03bb times the weight of the k \u2212 1-step return. Using a \u03bb = 0 reduces to using TD, as all n-steps for n > 1 have a weight of 0. One of the appealing effects of using eligibility traces is that a single update allows states many steps behind a reward signal to receive credit. This propagates knowledge back at a faster rate, allowing for accelerated learning. Especially in environments where rewards are sparse and/or delayed, eligibility traces can help assign credit to past states and actions. Without traces, seeing a sparse reward will only propagate the value back by one step, which in turn needs to be sampled to send the value back a second step, and so on.\nR\u03bbt = (1\u2212 \u03bb) \u221e\u2211 i=0 \u03bbiR (i) t = (1\u2212 \u03bb) \u221e\u2211 i=1 \u03bbi\u22121 i\u22121\u2211 j=0 \u03b3jrj + \u03b3 i+1V (st+i) (5)\nThis way of viewing eligibility traces is called the forward view, as states are looking ahead at the rewards received in the future. The forward view is rarely used, as it requires a state to wait for the future to unfold before calculating an update, and requires memory to store the experience. There is an equivalent view called the backward view, which allows us to calculate updates for every previous state as we take a single action. This requires no memory and lets us perform updates without having to wait for the future. However, this view has had limited success in the neural network setting as it requires using a trace on each neuron of the network, which tend to be dense and heavily used at each step resulting in noisy signals. For this reason, eligibility traces aren\u2019t heavily used when using deep learning, despite their potential benefits."}, {"heading": "2.1.1 Q(\u03bb)", "text": "Q(\u03bb) is a variant of Q-learning where eligibility traces are used to calculate the TD error. As mentioned previously, the backwards view of traces is traditionally used.\nA few versions of Q(\u03bb) exist, but the most used one is Watkins\u2019s Q(\u03bb). As Q-learning is off-policy, the sequence of actions used in the past trajectory used to calculate the trace might be different from the actions that the current policy might take. In that case, one should not be using the trajectory past the point where actions differ. To handle such a case, Watkins\u2019s Q(\u03bb) sets the trace to 0 if the action that the current policy would select is different from the one used in the past."}, {"heading": "2.2 Deep Q-Networks", "text": "Mnih et al. (2015) introduced deep Q-networks (DQN), one of the first successful reinforcement learning algorithms that use deep learning for function approximation in a way general enough which is applicable to a variety of environments. Applying it to a set of Atari games, they used a convolutional neural network (CNN) which took as input the last four frames of the game, and output Q-values for each possible action.\nEquation 6 shows the DQN cost function, where we are optimizing the \u03b8 parameters. The \u03b8\u2212 parameters represent frozen Q-value weights which are update at a chosen frequency.\nL(st, at|\u03b8) = (rt + \u03b3max a\u2032 Q(st+1, a \u2032|\u03b8\u2212)\u2212Q(st, at|\u03b8))2 (6)"}, {"heading": "2.2.1 Deep Recurrent Q-Networks", "text": "As introduced in Hausknecht & Stone (2015), deep recurrent Q-networks (DRQN) are a modification on DQN, where single frames are passed through a CNN, which generates a feature vector that is then fed to an RNN which finally outputs Q-values. This architecture gives the agent a memory, allowing it to learn long-term temporal effects and handle partial observability, which is the case in many environments. The authors showed that randomly blanking out frames was difficult to overcome for DQN, but that DRQN learned to handle without issue.\nTo train DRQN, they proposed two variants of experience replay. The first was to sample entire trajectories and run the RNN from end to end. However this is very computationally demanding as some trajectories can be over 10000 steps long. The second alternative was to sample sub-trajectories instead of single transitions. This is required as the RNN needs to fill its hidden state and to allow it to understand the temporal aspect of the data."}, {"heading": "2.3 Optimizers", "text": "Stochastic gradient descent (SGD) is generally the algorithm used to optimize neural networks. However, some information is lost during the process as past gradients might signal that a weight drastically needs to change, or that it is oscillating, requiring a decrease in learning rate. Adaptive SGD algorithms have been built to use this information.\nRMSprop (Tieleman & Hinton (2012)), uses a geometric averaging over gradients squared, and divides the current gradient by its square root. To perform RMSprop, first we calculate the averaging as g = \u03b2g + (1\u2212 \u03b2)\u2207\u03b82 and then update the parameters \u03b8 \u2190 \u03b8 + \u03b1 \u2207\u03b8\u221a\ng+ .\nDQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average. First we calculate the running averages m = \u03b2m+(1\u2212\u03b2)\u2207\u03b8 and g = \u03b2g+(1\u2212\u03b2)\u2207\u03b82, and then update the parameters using \u03b8 \u2190 \u03b8+\u03b1 \u2207\u03b8\u221a\ng\u2212m2+ .\nIn the rest of the paper, when mentioning RMSprop, we\u2019ll be referring to this version.\nFinally, Kingma & Ba (2014) introduced Adam, which is essentially RMSprop coupled with Nesterov momentum, along with the running averages being corrected for bias. We have a term for the rate of momentum of each of the running averages. To calculate the update with Adam, we start with the updating the averages m = \u03b21m + (1 \u2212 \u03b21)\u2207\u03b8, v = \u03b22v + (1 \u2212 \u03b22)\u2207\u03b82, the correct their biases m\u0302 = m/(1\u2212 \u03b2t1), v\u0302 = v/(1\u2212 \u03b2t2) and finally calculate the gradient update \u03b8 \u2190 \u03b8 + \u03b1 m\u0302\u221av\u0302+ ."}, {"heading": "3 Experimental Setup", "text": "As explained, the forward view of eligibility traces can be useful, but is computationally demanding in terms of memory and time. One must store all transitions and apply the neural network to each state in the trajectory. By using DRQN, experience replay is already part of the algorithm, which removes the memory requirement of the traces. Then, by training on sub-trajectories of data, the states must be run through the RNN with all state values as the output, which eliminates the computational cost. Finally, all that\u2019s left to use eligibility traces is simply to calculate the weighted sum of the targets, which is very cheap to do.\nIn this section we analyze the use of eligibility traces when training DRQN and try both RMSprop and Adam as optimizers. We only tested the algorithms on fully observable games as to compare the learning capacities without the unfair advantage of having a memory, as would be the case on partially observable environments."}, {"heading": "3.1 Architecture", "text": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells. We then have a last linear layer that takes the output of the recurrent layer to output the Q-values. All layers before the LSTM are activated using rectified linear units (ReLU).\nAs mentioned in subsection 2.2.1, we also altered experience replay to sample sub-trajectories. We use backprop through time (BPTT) to train the RNN, but only train on a sub-trajectory of experience. In runtime, the RNN will have had a large sequence of inputs in its hidden state, which can be problematic if always trained with an empty hidden state. Like in Lample & Singh Chaplot (2016), we therefore sample a slightly longer length of trajectory and use the first m states to fill the hidden state. In our experiments, we selected trajectory lengths of 32, where the first 10 states are used as filler and the remaining 22 are used for the traces and TD costs. We used a batch size of 4.\nAll experiments using eligibility traces use \u03bb = 0.8. Furthermore, we use Watkins\u2019s Q(\u03bb). To limit computation costs of using traces, we cut the trace off once it becomes too small. In our experiments, we choose the limit of 0.01, which allows the traces to affect 21 states ahead (when \u03bb = 0.8). We\ncalculate the trace for every state in the trajectory, except for a few in the beginning, use to fill in the hidden state of the RNN.\nWhen using RMSprop, we used a momentum of 0.95, an epsilon of 0.01 and a learning rate of 0.00025. When using Adam, we used a momentum of gradients of 0.9, a momentum of squared gradients of 0.999, an epsilon of 0.001 and a learning rate of 0.00025.\nTesting phases are consistent across all models, with the score being the average over each game played during 125000 frames. We also use an of 0.05 for action selection.\nChoose k as number of trace steps and m as RNN-filler steps Initialize weights \u03b8, experience replay D \u03b8\u2212 \u2190 \u03b8 s\u2190 s0 repeat\nInitialize RNN hidden state to 0. repeat\nChoose a according to \u2212greedy policy on Q(s, a|\u03b8) Take action a in s, observe s\u2032, r Store s, a, r, s\u2032 in Experience Replay Sample 4 sub-trajectories of m+ k sequential transitions (s, a, r, s\u2032) from D\ny\u0302 = { r s\u2019 is terminal, r + \u03b3max\na\u0304 Q(s\u2032, a\u0304|\u03b8\u2212) otherwise\nforeach transition sampled do\n\u03bbt = { \u03bb at = arg maxa\u0304(st, a\u0304|\u03b8), 0 otherwise\nend for l from 0 to k \u2212 1 do\nR\u0302\u03bbt+l = [\u2211k s=l (\u220fs i=l \u03bbt+i ) R (s\u2212l+1) t+s ] / [\u2211k s=l (\u220fs i=l \u03bbt+i )] end Perform gradient descent on\u2202(R\u0302\n\u03bb\u2212Q(s,a|\u03b8))2 \u2202\u03b8\nEvery 10000 steps \u03b8\u2212 \u2190 \u03b8 s\u2190 s\u2032\nuntil s\u2032 is terminal until training complete Algorithm 1: Deep Recurrent Q-Networks with forward view eligibility traces on Atari. The eligibility traces are calculated using the n-step return function R(n)t for time-step t was described in section 2.1."}, {"heading": "4 Experimental Results", "text": "We describe experiments in two Atari games: Pong and Tennis. We chose Pong because it permits quick experimentation, and Tennis because it is one of the games that has proven difficult in all published results on Atari."}, {"heading": "4.1 Pong", "text": "First, we tested an RNN model both with \u03bb = 0 and \u03bb = 0.8, trained with RMSprop. Figure 2 shows that the model without a trace (\u03bb = 0) learned at the same rate as DQN, while the model with traces (\u03bb = 0.8) learned substantially faster and with more stability, without exhibiting any epochs with depressed performance. This is probably due to the eligibility traces propagating rewards back by many steps in a single update. In Pong, when the agent hits the ball, it must wait several time-steps before the ball gets either to or past the opponent. Once this happens, the agent must assign the credit of the event back to the time when it hit the ball, and not to the actions performed after the ball had already left. The traces clearly help send this signal back faster.\nWe then tested the same models but using Adam as the optimizer instead of RMSprop. All models learn much faster with this setting. However, the model with no trace gains significantly more than the model with the trace. Our current intuition is that some hyper-parameters, such as the frozen network\u2019s update frequency, are limiting the rate at which the model can learn. Note also that the DQN model also learns faster with Adam as the optimizer, but remains quite unstable, in comparison with the recurrent net models.\nFinally, the results in Table 1 show that both using eligibility traces and Adam provide performance improvements. While training with RMSProp, the model with traces gets to near optimal performance more than twice as quickly as the other models. With Adam, the model learns to be optimal in just 6 epochs."}, {"heading": "4.2 Tennis", "text": "The second Atari 2600 game we tested was Tennis. A match consists of only one set, which is won by the player who is the first to win 6 \"games\" (as in regular tennis). The score ranges from 24 to -24, given as the difference between the number of balls won by the two players.\nAs in Pong, we first tried an RNN trained with RMSprop and the standard learning rate of 0.00025, both with and without eligibility traces (using again \u03bb = 0.8 and \u03bb = 0). Figure 3 shows that both RNN models learned to get optimal scores after about 50 epochs. This is in contrast with DQN, which never seems to be able to pass the 0 threshold, with large fluctuations ranging from -24 to 0. After visually inspecting the games played in the testing phase, we noticed that the DQN agent gets stuck in a loop, where it exchanges the ball with the opponent until the timer runs out. In such a case, the agent minimizes the number of points scored against, but never learns to beat the opponent. The score fluctuations depend on how few points the agent allows before entering the loop. We suspect that the agent gets stuck in this policy because the reward for trying to defeat the opponent is delayed, waiting for the ball to reach the opponent and get past it. Furthermore, the experiences of getting a point are relatively sparse. Together, it makes it difficult to propagate the reward back to the action of hitting the ball correctly.\nWe also notice that both the RNN with and without eligibility traces manage to learn a near-optimal policy without getting stuck in the bad policy. The RNN has the capacity of sending the signal back to past states with BPTT, allowing it to do credit assignment implicitly, which might explain their ability to escape the bad policy. Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al. (2015)), which tend to get stuck in the policy of delaying. The model without traces learned at a faster pace than the one with traces, arriving to a score of 20 in 45 epochs as opposed to 62 for its counterpart. It\u2019s possible that the updates for model with traces were smaller, due to the weighting of target values, indirectly leading to a lower learning rate. We also trained the models with RMSprop and a higher learning rate of 0.001. This led to the model with traces getting to 20 points in just 27 epochs, while the model without traces lost its ability to get optimal scores and never passed the 0 threshold.\nWe then tried using Adam as the optimizer, with the original learning rate of 0.00025. Both RNN models learned substantially faster than with RMSprop, with the RNN with traces getting to nearoptimal performance in just 13 epochs. With Adam, the gradient for the positive TD is stored in the momentum part of the equation for quite some time. Once in momentum, the gradient is part of many updates, which makes it enough to overtake the safe strategy. We also notice that the model with traces was much more stable than its counterpart. The model without traces fell back to the policy of delaying the game on two occasions, after having learned to beat the opponent. Finally, we trained DQN with Adam, but the model acted the same way as DQN trained with RMSprop."}, {"heading": "5 Discussion and Conclusion", "text": "In this paper, we analyzed the effects of using eligibility traces and different optimization functions. We showed that eligibility traces can improve and stabilize learning and using Adam can strongly accelerate learning.\nAs shown in the Pong results, the model using eligibility traces didn\u2019t gain much performance from using Adam. One possible cause is the frozen network. While it has a stabilizing effect in DQN, by stopping policies from drastically changing from a single update, it also stops newly learned values from being propagated back. Double DQN seems to partially go around this issue, allowing the\npolicy of the next state to change, while keeping the values frozen. In future experiments, we must consider eliminating or increasing the frozen network\u2019s update frequency. It would also be interesting to reduce the size of experience replay, as with increased learning speed, old observations can become too off-policy and barely be used in eligibility traces."}], "references": [{"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Marc G Bellemare", "Yavar Naddaf", "Joel Veness", "Michael Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "arXiv preprint arXiv:1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["Matthew Hausknecht", "Peter Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "citeRegEx": "Hausknecht and Stone.,? \\Q2015\\E", "shortCiteRegEx": "Hausknecht and Stone.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Playing fps games with deep reinforcement learning", "author": ["Guillaume Lample", "Devendra Singh Chaplot"], "venue": "arXiv preprint arXiv:1609.05521,", "citeRegEx": "Lample and Chaplot.,? \\Q2016\\E", "shortCiteRegEx": "Lample and Chaplot.", "year": 2016}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["Sergey Levine", "Pieter Abbeel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Levine and Abbeel.,? \\Q2014\\E", "shortCiteRegEx": "Levine and Abbeel.", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adria Puigdomenech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1602.01783,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Safe and efficient off-policy reinforcement learning", "author": ["R\u00e9mi Munos", "Tom Stepleton", "Anna Harutyunyan", "Marc G Bellemare"], "venue": "arXiv preprint arXiv:1606.02647,", "citeRegEx": "Munos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2016}, {"title": "Prioritized experience replay", "author": ["Tom Schaul", "John Quan", "Ioannis Antonoglou", "David Silver"], "venue": "arXiv preprint arXiv:1511.05952,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Reinforcement learning: An introduction, volume 1", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Reinforcement learning: An introduction, In Preparation", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 2017}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "True online td (lambda)", "author": ["Harm van Seijen", "Rich Sutton"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "Seijen and Sutton.,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton.", "year": 2014}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Ziyu Wang", "Nando de Freitas", "Marc Lanctot"], "venue": "arXiv preprint arXiv:1511.06581,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al.", "startOffset": 78, "endOffset": 97}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)).", "startOffset": 78, "endOffset": 118}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)).", "startOffset": 78, "endOffset": 156}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)).", "startOffset": 78, "endOffset": 649}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)).", "startOffset": 78, "endOffset": 745}, {"referenceID": 7, "context": "Deep reinforcement learning has had many practical successes in game playing (Mnih et al. (2015),Silver et al. (2016)) and robotics (Levine & Abbeel (2014)). Our interest is in further exploring these algorithms in the context of environments with sparse rewards and partial observability. To this end, we investigate the use of two methods that are known to mitigate these problems: recurrent networks, which provide a form of memory summarizing past experiences, and eligibility traces, which allow information to propagate over multiple time steps. Eligibility traces have been shown empirically to provide faster learning (Sutton & Barto (2017), in preparation) but their use with deep RL has been limited so far (van Seijen & Sutton (2014), Hausknecht & Stone (2015)).", "startOffset": 78, "endOffset": 772}, {"referenceID": 1, "context": "DQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average.", "startOffset": 5, "endOffset": 19}, {"referenceID": 1, "context": "DQN (Graves (2013)) introduced a variant of RMSprop where the gradient is instead divided by the standard deviation of the running average. First we calculate the running averages m = \u03b2m+(1\u2212\u03b2)\u2207\u03b8 and g = \u03b2g+(1\u2212\u03b2)\u2207\u03b8, and then update the parameters using \u03b8 \u2190 \u03b8+\u03b1 \u2207\u03b8 \u221a g\u2212m2+ . In the rest of the paper, when mentioning RMSprop, we\u2019ll be referring to this version. Finally, Kingma & Ba (2014) introduced Adam, which is essentially RMSprop coupled with Nesterov momentum, along with the running averages being corrected for bias.", "startOffset": 5, "endOffset": 388}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis.", "startOffset": 91, "endOffset": 115}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84.", "startOffset": 91, "endOffset": 212}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells.", "startOffset": 91, "endOffset": 623}, {"referenceID": 0, "context": "We tested the algorithms on two Atari 2600 games, part of the Arcade Learning Environment (Bellemare et al. (2012)), Pong and Tennis. The architecture used is similar to the one used in Hausknecht & Stone (2015). The frames are converted to gray-scale and re-sized to 84x84. These are then fed to a CNN with the first layer being 32 8x8 filters and a stride of 4, followed by 64 4x4 filters with a stride of 2, then by 64 3x3 filters with a stride of 1. The output of the CNN is then flattened before being fed to a single dense layer of 512 output neurons, which is finally fed to an LSTM (Hochreiter & Schmidhuber (1997)) with 512 cells. We then have a last linear layer that takes the output of the recurrent layer to output the Q-values. All layers before the LSTM are activated using rectified linear units (ReLU). As mentioned in subsection 2.2.1, we also altered experience replay to sample sub-trajectories. We use backprop through time (BPTT) to train the RNN, but only train on a sub-trajectory of experience. In runtime, the RNN will have had a large sequence of inputs in its hidden state, which can be problematic if always trained with an empty hidden state. Like in Lample & Singh Chaplot (2016), we therefore sample a slightly longer length of trajectory and use the first m states to fill the hidden state.", "startOffset": 91, "endOffset": 1211}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al.", "startOffset": 123, "endOffset": 142}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al.", "startOffset": 123, "endOffset": 163}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al.", "startOffset": 123, "endOffset": 183}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al.", "startOffset": 123, "endOffset": 203}, {"referenceID": 7, "context": "Remarkably, this is the only algorithm that succeeds in getting near-optimal scores on Tennis, out of all variants of DQN (Mnih et al. (2015), Munos et al. (2016), Wang et al. (2015), Mnih et al. (2016), Schaul et al. (2015)), which tend to get stuck in the policy of delaying.", "startOffset": 123, "endOffset": 225}], "year": 2017, "abstractText": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "creator": "LaTeX with hyperref package"}}}