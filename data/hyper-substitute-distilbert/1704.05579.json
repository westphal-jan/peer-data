{"id": "1704.05579", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "A Large Self-Annotated Corpus for Sarcasm", "abstract": "we expect the q - annotated question analysis ( index ), an large corpus continually investigating ambiguity and for training and evaluating systems for speech detection. the corpus has 39. 3 million sarcastic statements - - 10 times more \" her previous library - - and many times corpus full of non - sarcastic discourse, allowing artificial learning in regimes of general objective and useful labels. each fragment is initially pseudo - annotated - - besides ambiguity labeled by the author and not perfectly adequate annotator - - intentionally provided similar factual, topic, interactive citation context. students evaluate ironic corpus comparative accuracy, compare authenticity to previous ambiguous clauses, and incorporate validation for the task of mock detection.", "histories": [["v1", "Wed, 19 Apr 2017 02:01:39 GMT  (226kb,D)", "http://arxiv.org/abs/1704.05579v1", "5 pages, 5 figures. In submission"], ["v2", "Fri, 21 Apr 2017 01:25:08 GMT  (226kb,D)", "http://arxiv.org/abs/1704.05579v2", "5 pages, 5 figures. In submission"], ["v3", "Fri, 6 Oct 2017 03:09:01 GMT  (175kb,D)", "http://arxiv.org/abs/1704.05579v3", "5 pages, 2 Figures. In submission"]], "COMMENTS": "5 pages, 5 figures. In submission", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["mikhail khodak", "nikunj saunshi", "kiran vodrahalli"], "accepted": false, "id": "1704.05579"}, "pdf": {"name": "1704.05579.pdf", "metadata": {"source": "CRF", "title": "A Large Self-Annotated Corpus for Sarcasm", "authors": ["Mikhail Khodak", "Kiran Vodrahalli"], "emails": ["mkhodak@cs.princeton.edu", "nsaunshi@cs.princeton.edu", "knv@cs.princeton.edu"], "sections": [{"heading": "1 Introduction", "text": "Sarcasm detection is an important component of many natural language processing (NLP) systems, with direct relevance to natural language understanding, dialogue systems, and text mining. However, detecting sarcasm is difficult because it occurs infrequently and is difficult for even human annotators to discern (Wallace et al., 2014). Despite these properties, existing datasets either have balanced labels \u2014 data with approximately the same number of examples for each label (Gonza\u0301lez-Iba\u0301nez et al., 2011; Bamman and Smith, 2015; Joshi et al., 2015; Amir et al., 2016; Oraby et al., 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al., 2013; Swanson et al., 2014; Wallace et al., 2015).\nIn this work, we make available the first corpus for sarcasm detection that has both unbalanced and\n1https://nlp.cs.princeton.edu/SARC/\nself-annotated labels and does not consist of lowquality text snippets from Twitter2. With more than a million examples of sarcastic statements, each provided with author, topic, and contex information, the dataset also exceeds all previous sarcasm corpora by an order of magnitude. This dataset is possible due to the comment structure of the social media site Reddit3 as well its frequentlyused and standardized annotation for sarcasm.\nFollowing a discussion of corpus construction and relevant statistics, in Section 4 we present results of a manual evaluation on a subsample of the data as well as a direct comparison with alternative sources. Then in Section 5 we examine simple methods of detecting sarcasm on both a balanced and unbalanced version of our dataset."}, {"heading": "2 Related Work", "text": "Since our main contribution is a corpus and not a method for sarcasm detection, we point the reader to a recent survey by Joshi et al. (2016) that discusses many interesting efforts in this area. Note that many of the works the authors mention will be discussed by us in this section, with many papers using their own datasets; this illustrates the need for common baselines for evaluation.\nSarcasm datasets can largely be distinguished by the sources used to get sarcastic and nonsarcastic statements, the amount of human annotation, and whether the dataset is balanced or unbalanced. Reddit has been used before, notably by Wallace et al. (2015); while the authors allow unabalanced labeling, they do not exploit the possibility of using self-annotation and generate around 10,000 human-labeled sentences. Twitter is a frequent source due to the self-annotation provided by hashtags such as #sarcasm, #notsarcasm, and\n2https://www.twitter.com 3https://www.reddit.com\nar X\niv :1\n70 4.\n05 57\n9v 1\n[ cs\n.C L\n] 1\n9 A\npr 2\n01 7\n#irony (Reyes et al., 2013; Bamman and Smith, 2015; Joshi et al., 2015). As discussed in Section 4.2, its low quality language and other properties make Twitter a less attractive source for annotated comments. However, it is by far the largest raw source of data for this purpose and has led to some large unbalanced corpora in previous efforts (Riloff et al., 2013; Pta\u0301c\u0306ek et al., 2014). A further source of comments is the Internet Argument Corpus (IAC) (Walker et al., 2012), a scraped corpus of Internet discussions that can be further annotated for sarcasm by humans or by machine learning; this is done by Lukin and Walker (2013) and Oraby et al. (2016), in both cases resulting in around 10,000 labeled statements."}, {"heading": "3 Corpus Details", "text": ""}, {"heading": "3.1 Reddit Structure and Annotation", "text": "Reddit is a social media site in which users communicate by commenting on submissions, which are titled posts consisting of embedded media, external links, and/or text, that are posted on topicspecific forums known as subreddits; examples of subreddits include funny, pics, and science. Users comment on submissions and on other comments, resulting in tree-like conversation structure such that each comment has a parent comment. We refer to elements as any nodes in the tree of a Reddit link (i.e., comments or submissions).\nUsers on Reddit have adopted a common method for sarcasm annotation consisting of adding the marker \u201c/s\u201d to the end of sarcastic statements; this originates from the HTML text delineation <sarcasm>...</sarcasm> As with Twitter hashtags, using these markers as indicators of sarcasm is noisy (Bamman and Smith, 2015), especially since many users do not make use of the marker, do not know about it, or only use it where sarcastic intent is not otherwise obvious. We dicuss the extent of this noise in Section 4.1."}, {"heading": "3.2 Constructing SARC", "text": "Reddit comments from December 2005 have been made available due to web-scraping 4; we construct our dataset as a subset of comments from 2009-2016, comprising the vast majority of comments and excluding noisy data from earlier years. For each comment we provide a sarcasm label, author, the subreddit it appeared in, the comment\n4http://files.pushshift.io/reddit\nscore as voted on by users, the date of the comment, and the parent comment or submission.\nTo reduce noise, we use several filters to remove noisy and uninformative comments. Many of these are standard preprocessing steps such as excluding URLs and limiting characters to be ASCII. To handle Reddit data, we also perform the following two filtering steps:\n\u2022 Only use comments from an author starting from the first month in which that author used the standard sarcasm annotation. This ensures that the author knows the annotation and makes unlabeled sarcasm less likely.\n\u2022 Exclude comments that are descendants of sarcastic comments in the conversation tree, as annotation in such cases is extremely noisy, with authors agreeing or disagreeing with the previously expressed sarcasm with their own sarcasm but often not marking.\nWe collect a very large corpus, SARC-raw, with around 500-600 million total comments, of which 1.3 million are sarcastic. To obtain a smaller, clear corpus with better-quality comments, we take a subset consisting of single sentences having between 2 and 50 tokens. This yields a smaller corpus of around 200 million comments and 600 thousand sarcastic comments, SARC-main. Finally, we take a subset of the latter corpus corresponding to comments in the subreddit politics, which is a very large and sarcastic subreddit; we call this last segment SARC-pol."}, {"heading": "4 Corpus Evaluation", "text": "There are three major metrics of interest for evaluating our corpora: (1) size, (2) the proportion of sarcastic to non-sarcastic comments, and (3) the rate of false positives and false negatives. Of interest is also the quality of the text in the corpus and\nits applicability to other NLP tasks. Thus in this section we consider evaluate error in the SARCmain subset and provide comparison with other corpora used to construct sarcasm datasets."}, {"heading": "4.1 Manual Evaluation", "text": "To investigate the noisiness of using Reddit as a source of self-annotated sarcasm we estimate the proportion of false positives and false negatives induced by our filtering. This is done by having three human evaluators manually check a random subset of 500 comments from SARC-main tagged as sarcastic and 500 tagged as non-sarcastic, with full access to the comment\u2019s context. A comment was labeled a false positive if a majority determined that the \u201c/s\u201d tag was not an annotation but part of the sentence and a false negative if a majority determined that the comment author was clearly being sarcastic. After evaluation, the false positive rate was determined to be 2.0% and the false negative rate 3.0%. Although the false positive rate is reasonable, the false negative rate is significant compared to the sarcasm proportion, indicating large variation in the working definition of sarcasm and the need for methods that can handle noisy data in the unbalanced setting."}, {"heading": "4.2 Comparison with other Sources", "text": "As noted before, Twitter has been the most common source for sarcasm in previous corpora; this is likely due to the explicit annotation provided by its hashtags. However, using Reddit as a source of sarcastic comments holds many research advantages. Unlike Reddit comments, which are not\nconstrained by length and contain fewer hashtags, tweets are not written in true English. Hashtagged tokens are also frequently used as a part of the statement itself (e.g. \u201cthat was #sarcasm\u201d), blurring the line between text and annotation; on Reddit \u201c/s\u201d is generally only used as something other than annotation when its use as an annotation is being referred to (e.g. \u201cyou forgot the /s\u201d).\nFurthermore, from a subsample of Twitter and Reddit data from July 2014 we determined that a vastly smaller percentage (.002% vs. .927%) of Twitter authors make use of sarcasm annotation (#sarcasm, #sarcastic, or #sarcastictweet). We hypothesize that Reddit users require sarcastic annotation more frequently and in a more standardized form because they are largely anonymous and so cannot rely on a shared context to communicate sarcasm. Finally, Reddit also benefits from having subreddits, which enable featurization and data exploration based on an explicit topic assignment.\nThe Internet Argument Corpus (IAC) has also been used as a source of sarcastic comments (Walker et al., 2012). The corpus developers found 12% of examples in the IAC to be sarcastic, which is a much nicer class proportion for sarcasm detection than ours. As the Reddit data consists of arbitrary conversations, not just arguments, it is not surprising that our sarcasm percentage is much smaller, even when accounting for false negatives; this property also makes our dataset more realistic. Unlike Reddit and Twitter, the IAC also requires manual annotation of sarcasm."}, {"heading": "5 Baselines for Sarcasm Detection", "text": "A direct application of our corpus is for training and evaluating sarcasm detection systems; we thus provide baselines for the task of classifying a given statement as sarcastic or not-sarcastic. We consider non-contextual representations of the comments as feature vectors to be classified by a regularized linear support vector machine (SVM) trained by stochastic gradient descent (SGD). In the unbalanced regime we use validation to tune the class-weight, assigning more weight to the sarcasm class to force its consideration in the loss."}, {"heading": "5.1 Bag-of-NGrams", "text": "The Bag-of-NGrams representation consists of using a document\u2019s n-gram counts as features in a vector. For the SARC-main subset we use all unigrams, bigrams, and trigrams that appear at least 5 times in the sarcastic training comments. For the SARC-raw subset we use all unigrams, bigrams, and trigrams that appear at least 5 times in all training comments. The feature vectors are normalized before classification."}, {"heading": "5.2 Word Embeddings", "text": "Given a document, taking the elementwise average of word embeddings of its words provides a simple low-dimensional document representation. For word vectors we use normalized 300-dimensional GloVe representations trained on the Common Crawl corpus (Pennington et al., 2014). Since we are establishing baselines and SGD yields inconsistent results on word embedding representations in the unbalanced regime, we do report not any word embedding results for that task.\nSmooth Inverse Frequency (SIF) embedding, in which a document is represented as a weighted average of the embeddings of its words, has been show to be an effective baseline representation\nmethod (Arora et al., 2017). Given a word w\u2019s frequency fw, SIF-weights assign a weight aa+fw , where a is a hyperparameter often set to a = 10\u22124. As in TF-IDF, this assigns a low weight to high frequency words; however, it performs surprisingly poorly compared to average embedding.\nHypothesizing that, unlike in regular text classification, a document\u2019s sarcasm depends on words with fairly high frequencies such as sure, totally, and wow, we instead use Smooth Negative Inverse Frequency (SNIF) weights, assigning 1\u2212 aa+fw to each word before also representing the document as a weighted average of word embeddings. We find that such a weighting does in fact improve performance over vector averaging."}, {"heading": "6 Conclusion", "text": "We introduce a large sarcasm dataset based on self-annotated Reddit comments. The datasets are provided in three versions, allowing for diverse applications: SARC-raw, SARC-main, and SARCpol. SARC-raw has over 1 million sarcastic sentences, larger than any existing dataset. We evaluate the baseline performance of simple machine learning methods and compare them with human performance, with SVM over Bag-of-NGram representations outperforming humans on the task of sarcasm detection. We hope that future users of this dataset will improve upon these benchmarks."}], "references": [{"title": "Modelling context with user embeddings for sarcasm detection in social media", "author": ["Silvio Amir", "Byron C. Wallace", "Hao Lyu", "Paula Carvalho", "M\u00e1rio J. Silva."], "venue": "https://arxiv.org/pdf/1607.00976.pdf.", "citeRegEx": "Amir et al\\.,? 2016", "shortCiteRegEx": "Amir et al\\.", "year": 2016}, {"title": "A simple but tough-to-beat baseline for sentence embeddings", "author": ["Sanjeev Arora", "Yingyu Liang", "Tengyu Ma."], "venue": "Conference Paper at ICLR 2017. https://openreview.net/pdf?id=SyK00v5xx.", "citeRegEx": "Arora et al\\.,? 2017", "shortCiteRegEx": "Arora et al\\.", "year": 2017}, {"title": "Contextualized sarcasm detection on twitter", "author": ["David Bamman", "Noah A. Smith."], "venue": "Association for the Advancement of Artificial Intelligence.", "citeRegEx": "Bamman and Smith.,? 2015", "shortCiteRegEx": "Bamman and Smith.", "year": 2015}, {"title": "Identifying sarcasm in twitter: A closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguis-", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Automatic sarcasm detection: A survey", "author": ["Aditya Joshi", "Pushbak Bhattacharyya", "Mark J. Carman."], "venue": "https://arxiv.org/pdf/1602.03426.pdf.", "citeRegEx": "Joshi et al\\.,? 2016", "shortCiteRegEx": "Joshi et al\\.", "year": 2016}, {"title": "Harnessing context incongruity for sarcasm detection", "author": ["Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics. Association", "citeRegEx": "Joshi et al\\.,? 2015", "shortCiteRegEx": "Joshi et al\\.", "year": 2015}, {"title": "Really? well", "author": ["Stephanie Lukin", "Marilyn Walker."], "venue": "apparently bootstrapping improves the performance of sarcasm and nastiness classifiers for online dialogue. In Proceedings of the Workshop on Language in Social Media. Association for Compu-", "citeRegEx": "Lukin and Walker.,? 2013", "shortCiteRegEx": "Lukin and Walker.", "year": 2013}, {"title": "Creating and characterizing a diverse corpus of sarcasm in dialogue", "author": ["Shereen Oraby", "Vrindavan Harrison", "Lena Reed", "Ernesto Hernandez", "Ellen Riloff", "Marilyn Walker."], "venue": "Proceedings of the SIGDIAL 2016 Conference. Association for Compu-", "citeRegEx": "Oraby et al\\.,? 2016", "shortCiteRegEx": "Oraby et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Associa-", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Sarcasm detection on czech and english twitter", "author": ["Tom\u00e1s Pt\u00e1c\u0306ek", "Ivan Habernal", "Jun Hong"], "venue": "In 25th International Conference on Computational Linguistics: Technical Papers", "citeRegEx": "Pt\u00e1c\u0306ek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pt\u00e1c\u0306ek et al\\.", "year": 2014}, {"title": "A multidimensional approach for detecting irony in twitter", "author": ["Antonio Reyes", "Paolo Rosso", "Tony Veale."], "venue": "Data Knowledge Engineering 47(1).", "citeRegEx": "Reyes et al\\.,? 2013", "shortCiteRegEx": "Reyes et al\\.", "year": 2013}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "Proceedings of the 2013 Conference on Empirical Meth-", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": "Getting reliable annotations for sarcasm in online dialogues", "author": ["Reid Swanson", "Stephanie Lukin", "Luke Eisenberg", "Thomas Chase Corcoran", "Marilyn A. Walker."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Swanson et al\\.,? 2014", "shortCiteRegEx": "Swanson et al\\.", "year": 2014}, {"title": "A corpus for research on deliberation and debate", "author": ["Marilyn A. Walker", "Pranav Anand", "Jean E. Fox Tree", "Rob Abbott", "Joseph King."], "venue": "Language Resources and Evaluation Conference.", "citeRegEx": "Walker et al\\.,? 2012", "shortCiteRegEx": "Walker et al\\.", "year": 2012}, {"title": "Sparse, contextually informed models for irony detection: Exploiting user communities, entities, and sentiment", "author": ["Byron C. Wallace", "Do Kook Choe", "Eugene Charniak."], "venue": "Proceedings of the 53rd Annual Meeting of the Associ-", "citeRegEx": "Wallace et al\\.,? 2015", "shortCiteRegEx": "Wallace et al\\.", "year": 2015}, {"title": "Humans require context to infer ironic intent (so computers probably do, too", "author": ["Byron C. Wallace", "Do Kook Choe", "Laura Kertz", "Eugene Charniak."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Associ-", "citeRegEx": "Wallace et al\\.,? 2014", "shortCiteRegEx": "Wallace et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 15, "context": "However, detecting sarcasm is difficult because it occurs infrequently and is difficult for even human annotators to discern (Wallace et al., 2014).", "startOffset": 125, "endOffset": 147}, {"referenceID": 3, "context": "Despite these properties, existing datasets either have balanced labels \u2014 data with approximately the same number of examples for each label (Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Bamman and Smith, 2015; Joshi et al., 2015; Amir et al., 2016; Oraby et al., 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al.", "startOffset": 141, "endOffset": 254}, {"referenceID": 2, "context": "Despite these properties, existing datasets either have balanced labels \u2014 data with approximately the same number of examples for each label (Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Bamman and Smith, 2015; Joshi et al., 2015; Amir et al., 2016; Oraby et al., 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al.", "startOffset": 141, "endOffset": 254}, {"referenceID": 5, "context": "Despite these properties, existing datasets either have balanced labels \u2014 data with approximately the same number of examples for each label (Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Bamman and Smith, 2015; Joshi et al., 2015; Amir et al., 2016; Oraby et al., 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al.", "startOffset": 141, "endOffset": 254}, {"referenceID": 0, "context": "Despite these properties, existing datasets either have balanced labels \u2014 data with approximately the same number of examples for each label (Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Bamman and Smith, 2015; Joshi et al., 2015; Amir et al., 2016; Oraby et al., 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al.", "startOffset": 141, "endOffset": 254}, {"referenceID": 7, "context": "Despite these properties, existing datasets either have balanced labels \u2014 data with approximately the same number of examples for each label (Gonz\u00e1lez-Ib\u00e1nez et al., 2011; Bamman and Smith, 2015; Joshi et al., 2015; Amir et al., 2016; Oraby et al., 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al.", "startOffset": 141, "endOffset": 254}, {"referenceID": 11, "context": ", 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al., 2013; Swanson et al., 2014; Wallace et al., 2015).", "startOffset": 64, "endOffset": 129}, {"referenceID": 12, "context": ", 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al., 2013; Swanson et al., 2014; Wallace et al., 2015).", "startOffset": 64, "endOffset": 129}, {"referenceID": 14, "context": ", 2016) \u2014 or use human annotators to label sarcastic statements (Riloff et al., 2013; Swanson et al., 2014; Wallace et al., 2015).", "startOffset": 64, "endOffset": 129}, {"referenceID": 4, "context": "Since our main contribution is a corpus and not a method for sarcasm detection, we point the reader to a recent survey by Joshi et al. (2016) that discusses many interesting efforts in this area.", "startOffset": 122, "endOffset": 142}, {"referenceID": 14, "context": "Reddit has been used before, notably by Wallace et al. (2015); while the authors allow unabalanced labeling, they do not exploit the possibility of using self-annotation and generate around 10,000 human-labeled sentences.", "startOffset": 40, "endOffset": 62}, {"referenceID": 10, "context": "#irony (Reyes et al., 2013; Bamman and Smith, 2015; Joshi et al., 2015).", "startOffset": 7, "endOffset": 71}, {"referenceID": 2, "context": "#irony (Reyes et al., 2013; Bamman and Smith, 2015; Joshi et al., 2015).", "startOffset": 7, "endOffset": 71}, {"referenceID": 5, "context": "#irony (Reyes et al., 2013; Bamman and Smith, 2015; Joshi et al., 2015).", "startOffset": 7, "endOffset": 71}, {"referenceID": 11, "context": "However, it is by far the largest raw source of data for this purpose and has led to some large unbalanced corpora in previous efforts (Riloff et al., 2013; Pt\u00e1c\u0306ek et al., 2014).", "startOffset": 135, "endOffset": 178}, {"referenceID": 9, "context": "However, it is by far the largest raw source of data for this purpose and has led to some large unbalanced corpora in previous efforts (Riloff et al., 2013; Pt\u00e1c\u0306ek et al., 2014).", "startOffset": 135, "endOffset": 178}, {"referenceID": 13, "context": "A further source of comments is the Internet Argument Corpus (IAC) (Walker et al., 2012), a scraped corpus of Internet discussions that can be further annotated for sarcasm by humans or by machine learning; this is done by Lukin and Walker (2013) and Oraby et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 2, "context": ", 2013; Bamman and Smith, 2015; Joshi et al., 2015). As discussed in Section 4.2, its low quality language and other properties make Twitter a less attractive source for annotated comments. However, it is by far the largest raw source of data for this purpose and has led to some large unbalanced corpora in previous efforts (Riloff et al., 2013; Pt\u00e1c\u0306ek et al., 2014). A further source of comments is the Internet Argument Corpus (IAC) (Walker et al., 2012), a scraped corpus of Internet discussions that can be further annotated for sarcasm by humans or by machine learning; this is done by Lukin and Walker (2013) and Oraby et al.", "startOffset": 8, "endOffset": 617}, {"referenceID": 2, "context": ", 2013; Bamman and Smith, 2015; Joshi et al., 2015). As discussed in Section 4.2, its low quality language and other properties make Twitter a less attractive source for annotated comments. However, it is by far the largest raw source of data for this purpose and has led to some large unbalanced corpora in previous efforts (Riloff et al., 2013; Pt\u00e1c\u0306ek et al., 2014). A further source of comments is the Internet Argument Corpus (IAC) (Walker et al., 2012), a scraped corpus of Internet discussions that can be further annotated for sarcasm by humans or by machine learning; this is done by Lukin and Walker (2013) and Oraby et al. (2016), in both cases resulting in around 10,000 labeled statements.", "startOffset": 8, "endOffset": 641}, {"referenceID": 2, "context": "</sarcasm> As with Twitter hashtags, using these markers as indicators of sarcasm is noisy (Bamman and Smith, 2015), especially since many users do not make use of the marker, do not know about it, or only use it where sarcastic intent is not otherwise obvious.", "startOffset": 91, "endOffset": 115}, {"referenceID": 13, "context": "The Internet Argument Corpus (IAC) has also been used as a source of sarcastic comments (Walker et al., 2012).", "startOffset": 88, "endOffset": 109}, {"referenceID": 8, "context": "For word vectors we use normalized 300-dimensional GloVe representations trained on the Common Crawl corpus (Pennington et al., 2014).", "startOffset": 108, "endOffset": 133}, {"referenceID": 1, "context": "method (Arora et al., 2017).", "startOffset": 7, "endOffset": 27}], "year": 2017, "abstractText": "We introduce the Self-Annotated Reddit Corpus (SARC)1, a large corpus for sarcasm research and for training and evaluating systems for sarcasm detection. The corpus has 1.3 million sarcastic statements \u2014 10 times more than any previous dataset \u2014 and many times more instances of non-sarcastic statements, allowing for learning in regimes of both balanced and unbalanced labels. Each statement is furthermore self-annotated \u2014 sarcasm is labeled by the author and not an independent annotator \u2014 and provided with user, topic, and conversation context. We evaluate the corpus for accuracy, compare it to previous related corpora, and provide baselines for the task of sarcasm detection.", "creator": "TeX"}}}