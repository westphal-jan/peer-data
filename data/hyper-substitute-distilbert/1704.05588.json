{"id": "1704.05588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Learning to Fly by Crashing", "abstract": "how then you determine ways navigate an unmanned utility vehicle ( sas ). explain obstacles? this science is getting showcase its technical dataset collected the optimization experts : well, high neural learning systems tend too feature methods performing with little data. those adults prefer to use simulation. but the gap link scientists in real world remains fixed especially for structural problems. the book most research stops using large - scale laboratory knowledge is applied theory of crashes! in this paper, we propose flying scale the surface and collect a dataset of crashes again! we build a functional program predominant purpose is to crash into objects : it samples naive trajectories and crashes into natural objects. we crash our drones 11, 500 centimeters to create one of 450 known intelligent crash puzzles. typical dataset captures 180 different ways fly which a fighter can crash. we randomly check this predicted parameter data in integrating with positive data sampled from theoretical modeling environment to learn a simple yet powerful policy for uav navigation. we show that making simple self - supervised model is quite suitable in navigating the uav handler in familiar quiet environments with dynamic paths behind humans. for supplementary video monitoring :", "histories": [["v1", "Wed, 19 Apr 2017 02:20:20 GMT  (2824kb,D)", "http://arxiv.org/abs/1704.05588v1", null], ["v2", "Thu, 27 Apr 2017 00:13:19 GMT  (2824kb,D)", "http://arxiv.org/abs/1704.05588v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["dhiraj gandhi", "lerrel pinto", "abhinav gupta"], "accepted": false, "id": "1704.05588"}, "pdf": {"name": "1704.05588.pdf", "metadata": {"source": "CRF", "title": "Learning to Fly by Crashing", "authors": ["Dhiraj Gandhi", "Lerrel Pinto"], "emails": ["gabhinav}@andrew.cmu.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nHow do you navigate an autonomous system and avoid obstacles? What is the right approach and data to learn how to navigate? Should we use an end-to-end approach or should there be intermediate representations such as 3D? These are some of the fundamental questions that needs to be answered for solving the indoor navigation of unmanned air vehicle (UAV) and other autonomous systems. Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands to travel in freespace. While the two step-approach seems reasonable, the cost of sensors and unrecoverable errors from perception make it infeasible.\nAnother alternative is to use a monocular camera and learn to predict the motor commands. But how should we learn the mapping from input images to the motor commands? What should be the right data to learn this mapping? One possibility is to use imitation learning [4]. In imitation learning setting, we have a user who provides trajectories to train the flying policy. In order to visit the states not sampled by expert trajectories, they use the learned policy with human corrective actions to train the policy in iterative manner. But learning in such scenarios is restricted to small datasets since human experts are the bottlenecks in providing the training data. Therefore, such approaches cannot exploit high-capacity learning algorithms to train their policies.\nRecently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7]. Can we use selfsupervised learning to remove the labeling bottleneck of imitation learning? But how do we collect data for selfsupervised learning? In contemporary work, Sadeghi and Levine [8] use Reinforcement Learning (RL) in simulation to train the navigation policy of the drone. They focus on using simulations to avoid collisions that are inevitable since RLtechniques involve a trial-and-error component. They also demonstrate how a policy learned in simulation can transfer to real world without any retraining. But is it really true that simulation-based training can work out of box in real world? Most approaches in computer vision suggest otherwise and require small amounts of real-world data for adaptation. We note that the testing scenarios in [8] consist mostly of empty corridors where perspective cues are sufficient for navigation [3]. These perspective cues are captured in simulation as well. However, when it comes to navigation in cluttered environment (such as one shown in figure I), the gap between real and simulation widens dramatically. So, how can we collect data for self-supervised learning in realworld itself? Most approaches avoid self-supervised learning in real-world due to fear of collisions. Instead of finding ways to avoid collisions and crashes, we propose to bite the bullet and collect a dataset of crashes itself! We build a ar X iv :1 70 4. 05 58\n8v 1\n[ cs\n.R O\n] 1\n9 A\npr 2\n01 7\ndrone whose goal is to crash into objects: it samples random trajectories to crash into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This negative dataset captures the different ways a UAV can crash. It also represents the policy of how UAV should NOT fly. We use all this negative data in conjunction with positive data sampled from the same trajectories to learn a simple yet surprisingly powerful policy for UAV navigation. We show that this simple self-supervised paradigm is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles like humans."}, {"heading": "II. RELATED WORK", "text": "This work, which combines self supervised learning with flying a drone in an indoor environment, touches upon the broad fields of robot learning and drone control. We briefly describe these works and their connections to our method."}, {"heading": "A. Drone control", "text": "Controlling drones has been a widely studied area motivated by applications in surveillance and transportation. The most prevalent approach is that of SLAM. Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map. However these SLAM based methods are computationally expensive due to explicit 3D reconstruction, which greatly reduces the ability to have realtime navigation on an inexpensive platform.\nAnother method, which is closer in real-time applicability to our method, is that of depth estimation methods. One can use onboard range sensors to fly autonomously while avoiding obstacles [14], [15]. This is however not practical for publically available drones that often have low battery life and low load carrying capacity. One can also use stereo vision based estimation [16], [17] with light and cheap cameras. However stereo matching often fails on plain surfaces like the white walls of an office.\nMost of the methods described so far use multiple sensors to decide control parameters for the drone. This often leads to higher cost, poor realtime response and bulkier systems. Monocular camera based methods [3] use vanishing points as a guidance for drone flying, but still rely on range sensors for collision avoidance. A more recent trend of approaches have been in using learning based methods to infer flying control for the drones. Researchers [4] have used imitation learning strategies to transfer human demonstrations to autonomous navigation. This however fails to collect any negative examples, since humans never crash drones into avoidable obstacles. Because of this data bias, these methods fail to generalize to trajectories outside the training demonstrations from human controllers.\nAnother recent idea is to use simulators to generate this drone data [8], [18]. However transferring simulator learned policies to the real world works well only in simplistic scenarios and often require additional training on real-world data."}, {"heading": "B. Deep learning for robots", "text": "Learning from trial and error has regained focus in robotics. Self supervised methods [6], [7], [19], [20], show how large scale data collection in the real world can be used to learn tasks like grasping and pushing objects in a tabletop environment. Our work extends this idea of self supervision to flying a drone in an indoor environment. Deep reinforcement learning methods [21] have shown impressive results, however they are too data intensive (order of million examples) for our task of drone flying.\nA key component of deep learning, is the high amount of data required to train these generalizable models [22], [23]. This is where self-supervised learning comes into the picture by allowing the collection of high amounts of data with minimal human supervision. To the best of our knowledge, this is the first large scale effort in collecting more than 40 hours of real drone flight time data which we show is crucial in learning to fly."}, {"heading": "III. APPROACH", "text": "We now describe details of our data driven flying approach with discussions on methodologies for data collection and learning. We further describe our hardware setup and implementation for reproducibility."}, {"heading": "A. Hardware Specifications:", "text": "A important goal of our method is to demonstrate the effectiveness of low cost systems for the complex task of flying in an indoor environment. For this purpose, we use the Parrot Ar-Drone 2.0 which, due to its inaccuracies, is often run in outdoor environments as a hobby drone. We attach no additional sensors/cameras in the flying space in the data collection process. The key required components for the drone is it\u2019s inbuilt camera which broadcast 720p resolution images at 30 hz, it\u2019s inbuilt accelerometer and a safety hull to collide with objects without damaging the rotors. We externally attach a small camera of the same specification as of drones inbuilt camera to aid in localization during data collection."}, {"heading": "B. Data Collection:", "text": "We use a two-step procedure for data collection. First, we sample naive trajectories that lead to collisions with different kind of objects. Based on these sampled we then learn a policy for navigation. These initial naive trajectories and collisions provide a good initialization for reward leading to sampling more complex trajectories. This policy is then used to collect more example akin to hard example mining approach. At this stage, we can obtain much better trajectories that only collide when the learned strategy fails. This sampling of hard negatives has been shown to improve performance [6], [24].\n1) Collecting collision data: Most methods for learning with drones [3], [4] often have very few examples of colliding with objects. Our focus is however to collect as much of collision information as possible. This large amount of\ncrashing data should teach our learning model how not to crash/collide with objects.\nBut how should we collect this collisions data? One way would be to manually control the drone and crash into objects. But this would severely bias the dataset collected due to human intuition. Another way to collect this data is by commanding the drone to autonomously navigate the environment by SLAM based approaches [25], [26] and collect failure cases. But we again have dataset bias issues due to failure modes of SLAM along with sparse collision data. What we need is a method that has low bias to objects it collides with and can also generate lots of collision data. Our proposed method for data collection involves naive random straight line trajectories described in Figure 2.\nAlgorithm 1 Data Collection 1: Init: Track position error using PTAM from intial Take\nOff location 2: while No Crash do 3: Choose a random direction 4: while No Collision do 5: Continue in the same direction 6: while Position error > do 7: calculate control command based on position\nerror\nAlgorithm 1 describes our method of data collection in more detail. The drone is first placed randomly in the environment we desire to collect collision data. The drone then takes off, randomly decides a direction of motion and is commanded to follow a straight line path until collision. After a collision, the drone is commanded to go back to its original position followed by choosing another random direction of motion. This process is continued until the drone cannot recover itself from a collision. After Nenv trial\ncrashes, the drone collision data collection is restarted in a new environment. For each trajectory, we store time stamped images from the camera, estimated trajectories from IMU readings and accelerometer data. The accelerometer data is used to identify the exact moments of collision. Using this, we create our dataset D = {di} where di = {Iit} Ni t=0. For the ith trajectory di, image Ii0 is the image at the beginning of the trajectory far away from the collision object, while image IiNi is the image at the end of the trajectory after Ni timesteps when the drone hits an object.\nA key component of this data collection strategy is the ability of the drone to come back to its initial position to collect the next datapoint. However due to cheap low accuracy IMU, it isn\u2019t possible to do accurate backtracking using these naive sensors. For this purpose, we use PTAM [27] module that localizes the robot and helps it backtrack.\nWe collect 11,500 trajectories of collisions in 20 diverse indoor environments (Figure 3). This data is collected over 40 drone flying hours. Note that since the hulls of the drone are cheap and easy to replace, the cost of catastrophic failure is negligible.\n2) Data processing: We now describe the annotation procedure for the collected trajectories. The trajectories are first segmented automatically using the accelerometer data. This step restricts each trajectory upto the time of collision. As a next step, we further need to segment the trajectory into positive and negative data i.e. di = d+i \u22c3 d\u2212i . Here d + i is the part of the trajectory far away from the collision object while d\u2212i is the part of the trajectory close to the colliding object. Note the positive part of the dataset correspond to images where the control signal should be to continue forward. This segmentation is done heuristically by splitting the first N+ timesteps of the trajectory as positive and the last N\u2212 timesteps as negative trajectories. We ignore the images in the middle part of the trajectory. We now have a dataset with\nbinary classification labels."}, {"heading": "C. Learning Methodology", "text": "We now describe our learning methodology given the binary classification dataset we have collected. We will first describe our learning architecture and follow it with description of test time execution.\n1) Network architecture: Given the recent successes of deep networks in learning from visual inputs, we employ them to learn controls to fly. We use the AlexNet architecture [22]. We use ImageNet-pretrained weights as initial-\nization for our network [23]. This network architecture is represented in Figure 4. Note that the weights for last fully connected layer is initialized from a gaussian distribution. We learn a simple classification network which given an input image predicts if the drone should move forward in straight line or not. Therefore, the final layer is a binary softmax and the loss is negative log-likelihood.\n2) Test time execution: Our model essentially learns if going straight in a specific direction is good or not. But how can we use this model to fly autonomously in environments with multiple obstacles, narrow corridors and turns? We\nemploy a simple approach to use our binary classification network to fly long distances indoors. Algorithm 2 succinctly describes this strategy which evaluates the learned network on cropped segments of the drone\u2019s image. Based on the right cropped image, complete image and left cropped image network predicts the probability to move in right, straight and left direction. If the straight prediction (P(S)) is greater than \u03b1, drone moves forward with the yaw proportional to the difference between the right prediction (P(R)) and left prediction (P(L)). Intuitively, based on the confidence predictions of left and right, we decide to turn the robot left and right while moving forward. If the prediction for moving straight is below \u03b1 (going to hit the obstacle soon), we turn the drone left or right depending on which crop of the image predicts move forward. Intuitively, if the network is fed with only the left part of the image, and the network believes that it is good to go straight there, an optimal strategy would be take a left. In this way we can use a binary classification network to choose more complex directional movements given the image of the scene. This cropping based strategy can also be extended to non planar flying by cropping vertical patches instead of horizontal ones.\nAlgorithm 2 Policy for flying indoor 1: while No Crash do 2: Input : Real time image 3: P(L), P(S), P(R) = Network{image, left & right crop} 4: if P(S) > \u03b1 then 5: Linear Velocity = \u03b2 6: Angular Velocity \u221d P(R) - P(L) 7: else 8: Linear Velocity = 0 9: if P(R) > P(L) then 10: while P(S) < \u03b1 do 11: P(S) = Network{image} 12: Take Right turn 13: else 14: while P(S) < \u03b1 do 15: P(S) = Network{image} 16: Take Left turn"}, {"heading": "IV. EXPERIMENTAL EVALUATION", "text": "We now describe the evaluation procedure of our method on indoor environments and compare with strong depth driven baselines."}, {"heading": "A. Baselines", "text": "To evaluate our model, we compare the performance with a Straight line policy, a Depth prediction based policy and a human controlled policy.\n1) Straight line policy: A weak baseline for indoor navigation is to take an open-loop straight line path. For environments that contain narrow corridors, this will fail since errors compound which results in curved paths that will hit the walls. To strengthen this baseline, we choose the best (oracle) direction for the straight line policy.\n2) Depth prediction based policy: Recent advances in depth estimation from monocular cameras [28] have shown impressive results. These models for depth estimation are often trained over around 220k indoor depth data. A strong baseline is to use this depth prediction network to generate a\ndepth maps given a monocular image and make an inference on the direction of movement based on this depth map.\n3) Human policy: One of the strongest baseline is to use a human operator to fly the drone. In this case, we ask participants to control the drone only given the monocular image seen by the drone. The participants then use a joystick to give the commanded direction of motion to the drone. We also allow the participants to fly the drone in a test trial so that they get familiar with the response of the drone."}, {"heading": "B. Testing Environments", "text": "To show the generalizaility of our method, we test it on 6 complex indoor environments: \u2018Glass Door\u2019, \u2018NSH 4th Floor\u2019, \u2018NSH Entrance\u2019, \u2018Wean Hall\u2019, \u2018Hallway\u2019 and \u2018Hallway with Chairs\u2019. Floor plans for these environments can be seen in Figure 5. These environments have unique challenges that encompas most of the challenges faced in general purpose indoor navigation. For each of these environments, our method along with all the baselines are run 5 times with different start orientations and positions. This is done to ensure that the comparisons are robust to initializations.\n1) Glass Door: This environment is corridor with a corner that has transparent doors. The challenge for the drone is to take a turn at this corner without crashing into the transparent door. Most depth based estimation techniques are prone to fail in this scenario (depth sensing is poor with transparent objects). However data driven visual techniques have been shown to perform reasonably with these tough transparent obstacles.\n2) NSH 4th Floor: The second environment we test on is the office space of the 4th floor of the Newell Simon Hall at Carnegie Mellon University. In this environment, the drone has the capacity to navigate through a larger environment with several turns in narrow corridors.\n3) NSH Entrance: In this environment, the drone is initialized at the entrance of the Newell Simon Hall at Carnegie Mellon University. Here the challenge is manoeuvre through a hall with glass walls and into an open space (atrium) that is cluttered with dining tables and chairs.\n4) Wean Hall: This environment has small stretches of straight path which are connected at 90 degree to each other.\nDrone needs to take required turn to avoid collision at the intersection.\n5) Hallway: This is a narrow (2m) dead end straight corridor where drone has keep in the center to avoid the collision with the wall. At the dead end, the drone needs to take a turn and fly back. While flying back, the drone will again meet the dead end and can turn back again. This environment tests the long term flight capacity of the drone.\n6) Hallway With chairs: To the explicitly test the robustness of the controller to clutter and obstacles, we modify \u2018Smith Hallway\u2019 with chairs. The controller has to identify the narrow gaps between the chairs and the walls to avoid collisions. This gap can be less than 1m at some points.\nWe would like to point out that 2 out of 6 environments were also seen during training. These correspond to the NSH 4th Floor and NSH Entrance. The remaining 4 environments are completely novel."}, {"heading": "C. Results", "text": "To evaluate the performance of different baselines, we used average distance and average time of flight without collisions as the metric of evaluation. This metric also terminates flight runs when they take small loops (spinning on spot). Quantitative results are presented in Table I. We also qualitatively show in Figure 6 the comparison of trajectories generated by our method vs the depth prediction based baseline.\nOn every environment/setting we test on, we see that our method performs much better than the depth baseline. The best straight baseline provides an estimate of how difficult the environments are. The human controlled baselines are higher than our method for most environments. However for some environments like \u2018Hallway with Chairs\u2019, the presence of cluttered objects makes it difficult for the participants to navigate through narrow spaces which allows our method to surpass human level control in this environment.\nA key observation is the failure of depth based methods to (a) glass walls and doors, and (b) untextured flat walls. Glass walls give the depth based models the impression that the closest obstacle is farther away. However our method, since it has been seen examples of collisions with glass windows may have latched onto features that help it avoid glass walls or doors. Another difficulty depth based models face are in\nuntextured environments like corridors. Since our model has already seen untextured environments during training, it has learned to identify corners and realise that moving towards these corridor corners isn\u2019t desirable.\nThe results on the \u2018Hallway\u2019 environments further cements our method\u2019s claim by autonomously navigating for more than 3 minutes (battery life of drone in flight use is 5 minutes)."}, {"heading": "V. CONCLUSION", "text": "We propose a data driven approach to learn to fly by crashing more than 11,500 times in a multitude of diverse training environments. These crashing trajectories generate the biggest (to our knowledge) UAV crashing dataset and demonstrates the importance of negative data in learning. A standard deep network architecture is trained on this indoor crashing data, with the task of binary classification. By learning how to NOT fly, we show that even simple strategies easily outperforms depth prediction based methods on a variety of testing environments and is comparable to human control on some environments. This work demonstrates: (a) it is possible to collect self-supervised data for navigation at large-scale; (b) such data is crucial for learning how to navigate."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by ONR MURI N000141612007, NSF IIS-1320083 and Google Focused Award. AG was supported in part by Sloan Research Fellowship."}], "references": [{"title": "Autonomous flight in unstructured and unknown indoor environments", "author": ["A.G. Bachrach"], "venue": "Ph.D. dissertation, Massachusetts Institute of Technology, 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning deep control policies for autonomous aerial vehicles with mpc-guided policy search", "author": ["T. Zhang", "G. Kahn", "S. Levine", "P. Abbeel"], "venue": "Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEE, 2016, pp. 528\u2013535.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Autonomous mav flight in indoor environments using single image perspective cues", "author": ["C. Bills", "J. Chen", "A. Saxena"], "venue": "Robotics and automation (ICRA), 2011 IEEE international conference on. IEEE, 2011, pp. 5776\u20135783.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning monocular reactive uav control in cluttered natural environments", "author": ["S. Ross", "N. Melik-Barkhudarov", "K.S. Shankar", "A. Wendel", "D. Dey", "J.A. Bagnell", "M. Hebert"], "venue": "Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013, pp. 1765\u20131772.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "author": ["Y. Zhu", "R. Mottaghi", "E. Kolve", "J.J. Lim", "A. Gupta", "L. Fei-Fei", "A. Farhadi"], "venue": "arXiv preprint arXiv:1609.05143, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours", "author": ["L. Pinto", "A. Gupta"], "venue": "ICRA, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to poke by poking: Experiential learning of intuitive physics", "author": ["P. Agrawal", "A. Nair", "P. Abbeel", "J. Malik", "S. Levine"], "venue": "arXiv preprint arXiv:1606.07419, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "cad)$\u02c62$rl: Real single-image flight without a single real image", "author": ["F. Sadeghi", "S. Levine"], "venue": "CoRR, vol. abs/1611.04201, 2016. [Online]. Available: http://arxiv.org/abs/1611.04201", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Lsd-slam: Large-scale direct monocular slam", "author": ["J. Engel", "T. Sch\u00f6ps", "D. Cremers"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 834\u2013849.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Rslam: A system for large-scale mapping in constant-time using stereo", "author": ["C. Mei", "G. Sibley", "M. Cummins", "P. Newman", "I. Reid"], "venue": "International journal of computer vision, vol. 94, no. 2, pp. 198\u2013214, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft kinect sensor and its effect", "author": ["Z. Zhang"], "venue": "IEEE multimedia, vol. 19, no. 2, pp. 4\u201310, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Rgb-d mapping: Using kinect-style depth cameras for dense 3d modeling of indoor environments", "author": ["P. Henry", "M. Krainin", "E. Herbst", "X. Ren", "D. Fox"], "venue": "The International Journal of Robotics Research, vol. 31, no. 5, pp. 647\u2013663, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Stereo vision based indoor/outdoor navigation for flying robots", "author": ["K. Schmid", "T. Tomic", "F. Ruess", "H. Hirschm\u00fcller", "M. Suppa"], "venue": "Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEE, 2013, pp. 3955\u20133962.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Quadrotor using minimal sensing for autonomous indoor flight", "author": ["J.F. Roberts", "T. Stirling", "J.-C. Zufferey", "D. Floreano"], "venue": "European Micro Air Vehicle Conference and Flight Competition (EMAV2007), no. LIS-CONF-2007-006, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "State estimation for aggressive flight in gps-denied environments using onboard sensing", "author": ["A. Bry", "A. Bachrach", "N. Roy"], "venue": "Robotics and Automation (ICRA), 2012 IEEE International Conference on. IEEE, 2012, pp. 1\u20138.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Stereo vision and laser odometry for autonomous helicopters in gps-denied indoor environments", "author": ["M. Achtelik", "A. Bachrach", "R. He", "S. Prentice", "N. Roy"], "venue": "SPIE Defense, security, and sensing. International Society for Optics and Photonics, 2009, pp. 733 219\u2013733 219.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Vision-based autonomous mapping and exploration using a quadrotor mav", "author": ["F. Fraundorfer", "L. Heng", "D. Honegger", "G.H. Lee", "L. Meier", "P. Tanskanen", "M. Pollefeys"], "venue": "Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE, 2012, pp. 4557\u20134564.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Aerial Informatics and Robotics platform", "author": ["S. Shah", "D. Dey", "C. Lovett", "A. Kapoor"], "venue": "Microsoft Research, Tech. Rep. MSR-TR-2017-9, 2017.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2017}, {"title": "Learning handeye coordination for robotic grasping with deep learning and largescale data collection", "author": ["S. Levine", "P. Pastor", "A. Krizhevsky", "D. Quillen"], "venue": "ISER, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to push by grasping: Using multiple tasks for effective learning", "author": ["L. Pinto", "A. Gupta"], "venue": "CoRR, vol. abs/1609.09025, 2016. [Online]. Available: http://arxiv.org/abs/1609.09025", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 580\u2013587.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Training region-based object detectors with online hard example mining", "author": ["A. Shrivastava", "A. Gupta", "R.B. Girshick"], "venue": "CoRR, vol. abs/1604.03540, 2016. [Online]. Available: http://arxiv.org/abs/1604. 03540", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Fastslam: A factored solution to the simultaneous localization and mapping problem", "author": ["M. Montemerlo", "S. Thrun", "D. Koller", "B. Wegbreit"], "venue": "2002.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2002}, {"title": "Simultaneous localization and mapping with sparse extended information filters", "author": ["S. Thrun", "Y. Liu", "D. Koller", "A.Y. Ng", "Z. Ghahramani", "H. Durrant- Whyte"], "venue": "The International Journal of Robotics Research, vol. 23, no. 7-8, pp. 693\u2013716, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Parallel tracking and mapping for small AR workspaces", "author": ["G. Klein", "D. Murray"], "venue": "Proc. Sixth IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR\u201907), Nara, Japan, November 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Depth map prediction from a single image using a multi-scale deep network", "author": ["D. Eigen", "C. Puhrsch", "R. Fergus"], "venue": "Advances in neural information processing systems, 2014, pp. 2366\u20132374.  NSH 4th Floor Wean Hall  Glass Door D  ep th O  ur D  ep th O  ur D  ep th O  ur NSH Entrance D  ep th O  ur Hallway D  ep th O  ur Hallway with Chairs D  ep th O ur Fig. 6. Here we show the comparisons of the trajectories of our method vs the strong baseline of depth based prediction on our testing environments. The arrows denote the action taken by corresponding method.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "Most early research focused on a two-step approach: the first step being perception where either SLAM [1] or sensors [2] are used to estimate the underlying map and/or 3D [3]; the second step is to use the predicted depth or map to issue motor commands", "startOffset": 171, "endOffset": 174}, {"referenceID": 3, "context": "possibility is to use imitation learning [4].", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "Recently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "Recently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 6, "context": "Recently, there has been growing interest in using selfsupervised learning for variety of tasks like navigation [5], grasping [6] and pushing/poking [7].", "startOffset": 149, "endOffset": 152}, {"referenceID": 7, "context": "Can we use selfsupervised learning to remove the labeling bottleneck of imitation learning? But how do we collect data for selfsupervised learning? In contemporary work, Sadeghi and Levine [8] use Reinforcement Learning (RL) in simulation to train the navigation policy of the drone.", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "We note that the testing scenarios in [8] consist mostly of empty corridors where perspective cues are sufficient", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "for navigation [3].", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 26, "endOffset": 29}, {"referenceID": 12, "context": "Several methods [1], [3], [9]\u2013[13] use range or visual sensors to infer maps of the environment while simultaneously estimating its position in the map.", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "One can use onboard range sensors to fly autonomously while avoiding obstacles [14], [15].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "One can use onboard range sensors to fly autonomously while avoiding obstacles [14], [15].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "One can also use stereo vision based estimation [16], [17] with light and cheap cameras.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "One can also use stereo vision based estimation [16], [17] with light and cheap cameras.", "startOffset": 54, "endOffset": 58}, {"referenceID": 2, "context": "Monocular camera based methods [3] use vanishing points as a guidance for drone flying, but still rely on range sensors for collision avoidance.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "Researchers [4] have used imitation learning strategies to transfer human demonstrations to autonomous navigation.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "drone data [8], [18].", "startOffset": 11, "endOffset": 14}, {"referenceID": 17, "context": "drone data [8], [18].", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 29, "endOffset": 32}, {"referenceID": 18, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Self supervised methods [6], [7], [19], [20], show", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "Deep reinforcement learning methods [21] have shown impressive results, however they are too data intensive (order of million examples) for our task of drone flying.", "startOffset": 36, "endOffset": 40}, {"referenceID": 21, "context": "A key component of deep learning, is the high amount of data required to train these generalizable models [22], [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "A key component of deep learning, is the high amount of data required to train these generalizable models [22], [23].", "startOffset": 112, "endOffset": 116}, {"referenceID": 5, "context": "This sampling of hard negatives has been shown to improve performance [6], [24].", "startOffset": 70, "endOffset": 73}, {"referenceID": 23, "context": "This sampling of hard negatives has been shown to improve performance [6], [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "1) Collecting collision data: Most methods for learning with drones [3], [4] often have very few examples of colliding with objects.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "1) Collecting collision data: Most methods for learning with drones [3], [4] often have very few examples of colliding with objects.", "startOffset": 73, "endOffset": 76}, {"referenceID": 24, "context": "Another way to collect this data is by commanding the drone to autonomously navigate the environment by SLAM based approaches [25], [26] and collect failure cases.", "startOffset": 126, "endOffset": 130}, {"referenceID": 25, "context": "Another way to collect this data is by commanding the drone to autonomously navigate the environment by SLAM based approaches [25], [26] and collect failure cases.", "startOffset": 132, "endOffset": 136}, {"referenceID": 26, "context": "For this purpose, we use PTAM [27] module that localizes the robot and helps it backtrack.", "startOffset": 30, "endOffset": 34}, {"referenceID": 21, "context": "We use the AlexNet architecture [22].", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "We use ImageNet-pretrained weights as initialization for our network [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "The convolutional weights of our network (in grey) are pretrained from ImageNet classification [22], while the fully connected weights (in orange) is initialized randomly and learnt entirely from the collision data.", "startOffset": 95, "endOffset": 99}, {"referenceID": 27, "context": "2) Depth prediction based policy: Recent advances in depth estimation from monocular cameras [28] have shown impressive results.", "startOffset": 93, "endOffset": 97}], "year": 2017, "abstractText": "How do you learn to navigate an Unmanned Aerial Vehicle (UAV) and avoid obstacles? One approach is to use a small dataset collected by human experts: however, high capacity learning algorithms tend to overfit when trained with little data. An alternative is to use simulation. But the gap between simulation and real world remains large especially for perception problems. The reason most research avoids using large-scale real data is the fear of crashes! In this paper, we propose to bite the bullet and collect a dataset of crashes itself! We build a drone whose sole purpose is to crash into objects: it samples naive trajectories and crashes into random objects. We crash our drone 11,500 times to create one of the biggest UAV crash dataset. This dataset captures the different ways in which a UAV can crash. We use all this negative flying data in conjunction with positive data sampled from the same trajectories to learn a simple yet powerful policy for UAV navigation. We show that this simple self-supervised model is quite effective in navigating the UAV even in extremely cluttered environments with dynamic obstacles including humans. For supplementary video see: https://youtu.be/HbHqC8HimoI", "creator": "LaTeX with hyperref package"}}}