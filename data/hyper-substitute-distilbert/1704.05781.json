{"id": "1704.05781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2017", "title": "Redefining Context Windows for Word Embedding Models: An Experimental Study", "abstract": "conceptual semantic models yielding vector representations of words through dependency database will report here. although the role of mapping ( which often drives the paradigm of a function regression ) has left direct influence on the resulting embeddings, hence behavioral orientation of this model component is still not fully understood. this mechanism yields a conceptual analysis of context artifacts based on a set recognizing four distinct hyper - parameters. we train relevant four - gram approaches including using english - language terms for various values of these global - parameters, and evaluate them on both inference tests and analogy criteria. other numerical discoveries are : positive impact into cross - sentential correlation and the surprisingly good performance of right - context markers.", "histories": [["v1", "Wed, 19 Apr 2017 15:41:34 GMT  (106kb,D)", "http://arxiv.org/abs/1704.05781v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["pierre lison", "rey kutuzov"], "accepted": false, "id": "1704.05781"}, "pdf": {"name": "1704.05781.pdf", "metadata": {"source": "CRF", "title": "Redefining Context Windows for Word Embedding Models: An Experimental Study", "authors": ["Pierre Lison", "Andrei Kutuzov"], "emails": ["plison@nr.no", "andreku@ifi.uio.no"], "sections": [{"heading": "1 Introduction", "text": "Distributional semantic models represent words through real-valued vectors of fixed dimensions, based on the distributional properties of these words observed in large corpora. Recent approaches such as prediction-based models (Mikolov et al., 2013b) and GloVe (Pennington et al., 2014) have shown that it is possible to estimate dense, low-dimensional vectors (often called embeddings) able to capture various functional or topical relations between words. These embeddings are used in a wide range of NLP tasks, including part-of-speech tagging, syntactic parsing, named entity recognition and semantic role labelling; see (Collobert et al., 2011; Lin et al., 2015; Zhou and Xu, 2015; Lample et al., 2016), among others.\nAs recently shown by (Levy et al., 2015), the empirical variations between embedding models\nare largely due to differences in hyper-parameters (many of which are tied to the underlying definition of context) rather than differences in the embedding algorithms themselves. In this paper, we further develop their findings with a comprehensive analysis of the role played by context window parameters when learning word embeddings. Four specific aspects are investigated:\n1. The maximum size of the context window;\n2. The weighting scheme of context words according to their distance to the focus word;\n3. The relative position of the context window (symmetric, left or right side);\n4. The treatment of linguistic boundaries such as end-of-sentence markers.\nThe next section 2 provides a brief overview on word embeddings and context windows. Section 3 describes the experimental setup used to evaluate the influence of these four aspects. Finally, Section 4 presents and discusses the results."}, {"heading": "2 Background", "text": "The works of (Bengio et al., 2003) and (Mikolov et al., 2013b) introduced a paradigm-shift for distributional semantic models with new predictionbased algorithms outperforming the existing count-based approaches (Baroni et al., 2014). The word2vec models from (Mikolov et al., 2013b), comprising the Continuous Skip-gram and the Continuous Bag-of-Words algorithms, are now a standard part of many NLP pipelines.\nDespite their differences, all types of distributional semantic models require the definition of a context for each word observed in a given corpus. Given a set of (word, context) pairs extracted from the corpus, vector representations of words can be derived through various estimation methods, such as predicting words given their contexts\nar X\niv :1\n70 4.\n05 78\n1v 1\n[ cs\n.C L\n] 1\n9 A\npr 2\n01 7\n(CBOW), predicting the contexts from the words (Skip-Gram), or factorizing the log of their cooccurrence matrix (GloVe). In all of these approaches, the choice of context is a crucial factor that directly affects the resulting vector representations. The most common method for defining this context is to rely on a window centered around the word to estimate (often called the focus word)1. The context window thus determines which contextual neighbours are taken into account when estimating the vector representations.\nThe most prominent hyper-parameter associated to the context window is the maximum window size (i.e. the maximum distance between the focus word and its contextual neighbours). This parameter is the easiest one to adjust using existing software, which is why it is comparatively well studied. Larger windows are known to induce embeddings that are more \u2018topical\u2019 or \u2018associative\u2019, improving their performance on analogy test sets, while smaller windows induce more \u2018functional\u2019 and \u2018synonymic\u2019 models, leading to better performance on similarity test sets (Goldberg, 2016).\nHowever, the context window is also affected by other, less obvious hyper-parameters. Inside a given window, words that are closer to the focus word should be given more weights than more distant ones. To this end, CBOW and Continuous Skip-gram rely on a dynamic window mechanism where the actual size of the context window is sampled uniformly from 1 to L, where L is the maximum window size. This mechanism is equivalent to sampling each context word w j with a probability that decreases linearly with the distance | j\u2212 i| to the focus word wi:\nP(w j|wi) = L\n\u2211 window=1 P(w j|wi,window)P(window)\n= 1 L (L\u2212| j\u2212 i|+1)\nwhere window is the actual window size (from 1 to L) sampled by the algorithm. Similarly, the cooccurrence statistics used by GloVe rely on harmonic series where words at distance d from the focus word are assigned a weight 1d . For example, with the window size 3, the context word at the position 2 will be sampled with the probability of 2/3 in word2vec and the probability of 1/2 in GloVe.\n1Other types of context have been proposed, such as dependency-based contexts (Levy and Goldberg, 2014) or multilingual contexts (Upadhyay et al., 2016), but these are outside the scope of the present paper.\nAnother implicit hyper-parameter is the symmetric nature of the context window. The word2vec and GloVe models pay equivalent attention to the words to the left and to the right of the focus word. However, the relative importance of left or right contexts may in principle depend on the linguistic properties of the corpus language, in particular its word ordering constraints.\nFinally, although distributional semantic models do not themselves enforce any theoretical limit on the boundaries of context windows, word embeddings are in practice often estimated on a sentence by sentence basis, thus constraining the context windows to stop at sentence boundaries. However, to the best of our knowledge, there is no systematic evaluation of how this sentence-boundary constraint affects the resulting embeddings."}, {"heading": "3 Experimental setup", "text": "To evaluate how context windows affect the embeddings, we trained Continuous Skip-gram with Negative Sampling (SGNS) embeddings for various configurations of hyper-parameters, whose values are detailed in Table 1. In particular, the \u201cweighting scheme\u201d encodes how the context words should be weighted according to their distance with the focus word. This hyperparameter is given two possible values: a linear weighting scheme corresponding to the default word2vec weights, or an alternative scheme using the squared root of the distance.\nThe embeddings were trained on two Englishlanguage corpora: Gigaword v5 (Parker et al., 2011), a large newswire corpus of approx. 4 billion word tokens, and the English version of OpenSubtitles (Lison and Tiedemann, 2016), a large repository of movie and TV subtitles, of approx. 700 million word tokens. The two corpora correspond to distinct linguistic genres, Gigaword being a corpus of news documents (average sentence length\n21.7 tokens) while OpenSubtitles is a conversational corpus (average sentence length 7.3 tokens). OpenSubtitles notably contains a large number of non-sentential utterances, which are utterances lacking an overt predicate and depend on the surrounding dialogue context for their interpretation (Ferna\u0301ndez, 2006). The corpora were lemmatized and POS-tagged with the Stanford CoreNLP (Manning et al., 2014) and each token was replaced with its lemma and POS tag. Two versions of the corpora were used for the evaluation: one raw version with all tokens, and one filtered version after removal of stop words and punctuation.\nThe word embeddings were trained with 300- dimensional vectors, 10 negative samples per word and 5 iterations. Very rare words (less than 100 occurrences in Gigaword, less than 10 in OpenSubtitles) were filtered out. The models were then evaluated using two standard test workflows: Spearman correlation against SimLex-999 semantic similarity dataset (Hill et al., 2015) and accuracy on the semantic sections of the Google Analogies Dataset (Mikolov et al., 2013a)."}, {"heading": "4 Results", "text": "All in all, we trained 96 models on Gigaword (GW) and 96 models on OpenSubtitles (OS)2. Figure 1 illustrates the results for the SGNS embeddings on lexical similarity and analogy tasks using various types of context windows. The main findings from the experiments are as follows."}, {"heading": "Window size", "text": "As expected for a lexical similarity task (Schu\u0308tze and Pedersen, 1993), narrow context windows perform best with the SimLex999 dataset, which contains pairs of semantically similar words (not just related). For the analogy task, larger context windows are usually beneficial, but not always: the word embeddings trained on OpenSubtitles perform best with the window size of 10, while the best results on the analogy task for Gigaword are obtained with the window size of 2."}, {"heading": "Window position", "text": "Table 2 shows how the position of the context window influences the average model performance. Note that symmetric windows of, for instance, 10 are in fact 2 times larger than the \u2018left\u2019 or \u2018right\u2019\n2Encompassing different values of window size, weighting scheme, window position, cross-sentential boundaries and stop-words removal (4\u00d72\u00d73\u00d72\u00d72 = 96).\nwindows of the same size, as they consider 10 words both to the left and to the right of the focus word. This is most likely why symmetric windows consistently outperform \u2018single-sided\u2019 ones on the analogy task, as they are able to include twice as much contextual input.\nHowever, the average performance on the semantic similarity task (as indicated by the Spearman correlation with the SimLex999 test set) does not exhibit the same trend. \u2018Left\u2019 windows are indeed worse than symmetric ones, but \u2018right\u2019 windows are on par with the symmetric windows for OpenSubtitles and only one percent point behind them for Gigaword. It means that in many cases (at least with English texts) taking into account only n context words to the right of the focus word is sufficient to achieve the same performance with SimLex999 as by using a model which additionally considers n words to the left, and thus requires significantly more training time."}, {"heading": "Cross-sentential contexts", "text": "The utility of cross-sentential contexts depends on several covariates, most importantly the type of corpus and the nature of the evaluation task. For similarity tasks, cross-sentential contexts do not seem useful, and can even be detrimental for large window sizes. However, for analogy tasks, crosssentential contexts lead to improved results thanks to the increased window it provides. This is especially pronounced for corpora with short sentences such as OpenSubtitles (see Table 3)."}, {"heading": "Weighting scheme", "text": "Our experimental results show that none of the two evaluated weighting schemes (with weights that decrease respectively linearly or with the squareroot of the distance) gives a consistent advantage averaged across all models. However, the squared weighting scheme is substantially slower (as it\nincreases the number of context words to consider for each focus word), decreasing the training speed about 25% with window size 5. Thus, the original linear weighting scheme proposed in (Mikolov et al., 2013b) should be preferred."}, {"heading": "Stop words removal", "text": "As shown in Table 4, the removal of stop words does not really influence the average model performance for the semantic similarity task. The analogy task, however, benefits substantially from this filtering, for both corpora. Although not shown in the table, filtering stop words also significantly decreases the size of the corpus, thereby reducing the total time needed to train the word embeddings."}, {"heading": "5 Conclusion", "text": "Our experiments demonstrate the importance of choosing the right type of context window when learning word embedding models. The two most prominent findings are (1) the positive role of cross-sentential contexts and (2) the fact that, at least for English corpora, right-side contexts seem to be more important than left-side contexts for similarity tasks, and achieve a performance comparable to that of symmetric windows.\nIn the future, we wish to extend this study to the CBOW algorithm, to other weighting schemes (such as the harmonic series employed by GloVe), and to non-English corpora."}], "references": [{"title": "Georgiana Dinu", "author": ["Marco Baroni"], "venue": "and Germ\u00e1n Kruszewski.", "citeRegEx": "Baroni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Rejean Ducharme", "author": ["Yoshua Bengio"], "venue": "and Pascal Vincent.", "citeRegEx": "Bengio et al.2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Koray Kavukcuoglu", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen"], "venue": "and Pavel Kuksa.", "citeRegEx": "Collobert et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "NonSentential Utterances in Dialogue: Classification, Resolution and Use", "author": ["Raquel Fern\u00e1ndez"], "venue": "Ph.D. thesis, King\u2019s College London", "citeRegEx": "Fern\u00e1ndez.,? \\Q2006\\E", "shortCiteRegEx": "Fern\u00e1ndez.", "year": 2006}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Goldberg.", "year": 2016}, {"title": "Roi Reichart", "author": ["Felix Hill"], "venue": "and Anna Korhonen.", "citeRegEx": "Hill et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Kazuya Kawakami", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian"], "venue": "and Chris Dyer.", "citeRegEx": "Lample et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014] Omer Levy", "Yoav Goldberg"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Yoav Goldberg", "author": ["Omer Levy"], "venue": "and Ido Dagan.", "citeRegEx": "Levy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Chris Dyer", "author": ["Chu-Cheng Lin", "Waleed Ammar"], "venue": "and Lori Levin.", "citeRegEx": "Lin et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Opensubtitles2016: Extracting large parallel corpora from movie and TV subtitles", "author": ["Lison", "Tiedemann2016] P. Lison", "J. Tiedemann"], "venue": "In Proceedings of the 10th International Conference on Language Resources and Evaluation", "citeRegEx": "Lison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lison et al\\.", "year": 2016}, {"title": "Bethard", "author": ["Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J"], "venue": "and David McClosky.", "citeRegEx": "Manning et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Ke Chen", "author": ["Robert Parker", "David Graff", "Junbo Kong"], "venue": "and Kazuaki Maeda.", "citeRegEx": "Parker et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Christopher D", "author": ["Jeffrey Pennington", "Richard Socher"], "venue": "Manning.", "citeRegEx": "Pennington et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A vector model for syntagmatic and paradigmatic relatedness", "author": ["Sch\u00fctze", "Pedersen1993] Hinrich Sch\u00fctze", "Jan Pedersen"], "venue": "In Proceedings of the 9th Annual Conference of the UW Centre for the New OED and Text Research,", "citeRegEx": "Sch\u00fctze et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Sch\u00fctze et al\\.", "year": 1993}, {"title": "Chris Dyer", "author": ["Shyam Upadhyay", "Manaal Faruqui"], "venue": "and Dan Roth.", "citeRegEx": "Upadhyay et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Endto-end learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Distributional semantic models learn vector representations of words through the contexts they occur in. Although the choice of context (which often takes the form of a sliding window) has a direct influence on the resulting embeddings, the exact role of this model component is still not fully understood. This paper presents a systematic analysis of context windows based on a set of four distinct hyperparameters. We train continuous SkipGram models on two English-language corpora for various combinations of these hyper-parameters, and evaluate them on both lexical similarity and analogy tasks. Notable experimental results are the positive impact of cross-sentential contexts and the surprisingly good performance of right-context windows.", "creator": "LaTeX with hyperref package"}}}