{"id": "1704.06380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Improving Context Aware Language Models", "abstract": "increased implementation of synthetic tool sets present multiple promising predictions that plagued many applications. instead, current methods ought not take full toll along ordinary xml structure. players check that the most worst - encountered responding to adaptation ( concatenating cipher node with potential underlying embedding at the input to corresponding recurrent layer ) operates outperformed by a model that presents some low - yielding improvements : adaptation connecting both the hidden and output layers. and integrating feature hashing grammar term to capture context idiosyncrasies. experiments on synthetic modeling are conditioning mechanisms using three word corpora demonstrate the advantages above multiple proposed techniques.", "histories": [["v1", "Fri, 21 Apr 2017 02:27:26 GMT  (92kb,D)", "http://arxiv.org/abs/1704.06380v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaron jaech", "mari ostendorf"], "accepted": false, "id": "1704.06380"}, "pdf": {"name": "1704.06380.pdf", "metadata": {"source": "CRF", "title": "Improving Context Aware Language Models", "authors": ["Aaron Jaech", "Mari Ostendorf"], "emails": ["ostendor}@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "The dominant paradigm for language model adaptation relies on the notion of a domain. Domains are in many ways inadequate representations of context due to being ill-defined, discrete and incomparable, and not reflective of the diversity of human language (Ruder et al., 2016). In context aware language models, the notion of a domain is replaced with a set of context variables that each describe some aspect of the associated language such as the topic, time, or language. These variables can be dynamically combined to create a continuous representation of context as a lowdimensional embedding (Tang et al., 2016). The context variables and context embedding can then be used to adapt a recurrent neural network language model (RNNLM).\nThe standard approach for using a context embedding to adapt an RNNLM is to simply concatenate the context representation with the word embedding at the input to the RNN (Mikolov and\nZweig, 2012). Optionally, the context embedding is also concatenated with the output from the recurrent layer so that the output layer can be adapted as well. This basic strategy has been adopted for various types of adaptation such as for LM personalization (Wen et al., 2013; Li et al., 2016), adapting an LM to different genres of television shows (Chen et al., 2015), adapting to long range dependencies in a document (Ji et al., 2015), sharing information in generative text classifiers (Yogatama et al., 2017), and in other cases as well.\nIn this paper, we study methods of improving the mechanism for using context variables for adapting an RNNLM. The standard approach of adapting the hidden layer is equivalent to an additive transformation of the hidden state. We propose complimenting this with a multiplicative rescaling at the hidden layer and show that it consistently helps when the language model is used as a generative text classifier and can sometimes improve perplexity.\nUsing context dependent bias vectors is one way to adapt the output layer but it becomes infeasible when both the vocabulary size and the number of contexts are large. The method from Mikolov and Zweig (2012) of using the lowdimensional context embedding to adapt the output layer avoids the excessive memory issue of context-dependent bias vectors but our experiments show that it does not capture isolated but important details. We propose a hashing technique to simultaneously benefit from context-dependent weights and avoid the high memory cost. The combination of the low-rank and hashing techniques for adapting the output layer shows a consistent improvement across our experiments on three different corpora. ar X iv :1\n70 4.\n06 38\n0v 1\n[ cs\n.C L\n] 2\n1 A\npr 2\n01 7"}, {"heading": "2 Model", "text": "Our model is built on top of a standard RNN language model. There are three key parts which we discuss below: how we represent context using a low-dimensional embedding, the mechanism for using the context embedding for adapting the recurrent layer, and the mechanisms for adapting the output layer."}, {"heading": "2.1 Representing outside context", "text": "We assume access to one or more indicator variables, c1:n = c1, c2, . . . cn, that hold information about the outside context for each sentence. These can be indicators for topic, geographic region, time period, or other meta-data. In (Mikolov and Zweig, 2012) LDA topic vectors are used for the outside context. In (Tang et al., 2016) the outside context is a sentiment score and a product id for a product review dataset. We adopt their method of combining information from multiple context variables using a simple neural network. This strategy is well-suited for the types of context variables that we will see in our experiments, such as speaker identity. In other cases, it may be more appropriate to use topic models (Chen et al., 2015; Ghosh et al., 2016) or an RNN (Hoang et al., 2016) to build the context representation.\nFor each context variable ci, we learn an associated embedding matrix Ei, i = 1, . . . , n. If n = 1 then the embedding can directly be used as the context representation. Otherwise, a single layer neural network is used to combine the embeddings from the individual variables.\n~c = tanh( \u2211 i MiEici + b0)\nMi and b0 are parameters learned by the model. The context embedding, ~c, is used for adapting both the hidden and the output layer of the RNN."}, {"heading": "2.2 Adapting the hidden layer", "text": "The equation for the hidden layer of an RNN is\nst = \u03c3(U~wt + Sst\u22121 + b1)\nwhere ~wt is the word embedding of the t-th word, st\u22121 is the hidden state from the previous time step and \u03c3 is the activation function. To make use of the context embedding, ~c, for adapting the hidden layer the term F~c is inserted resulting in\nst = \u03c3(U~wt + Sst\u22121 + F~c+ b1)\nWe refer to the insertion of the F~c term as an additive adaptation of the hidden layer. It is equivalent to the unadapted version except with an adapted bias term. It can be implemented by simply concatenating the context vector ~c with the word embedding ~wt at each timestep at the input to the recurrent layer.\nTo increase the adaptability of the hidden layer we use a context-dependent multiplicative rescaling of the hidden layer weights. The method is borrowed from Ha et al. (2016) where it is used for dynamically adjusting the parameters of a language model in response to the previous words in the sentence. Using this row rescaling technique on top of the additive adaptation from above, the equation becomes\nst = \u03c3(Cu~c U~wt + Cw~c Sst\u22121 + F~c+ b1)\nwhere Cu and Cw are parameters of the model and is the elementwise multiplication operator. The element-wise multiplication is a low-cost operation and can even be pre-calculated so that model evaluation can happen with no extra computation compared to a vanilla RNN."}, {"heading": "2.3 Adapting the output layer", "text": "The output probabilities of an RNN are given by yt = softmax(Vst + b2). In our case, we tie the weights between the word embeddings in the input and output layer: WT = V (Press and Wolf, 2016; Inan et al., 2016).\nOne way of adapting the output layer is to let each context have its own bias vector. This requires the use of a matrix of size |V | \u00d7 |C|, which may be intractable when both |V | and |C| are large. Here, |V | is the size of the vocabulary and |C| is the total number of possible contexts. Mikolov and Zweig (2012) use a low-rank factorization of of the adaptation matrix, replacing the |V |\u00d7|C|matrix with the product of a matrix G of size |V | \u00d7 k and a context embedding ~c of size k.\nyt = softmax(Vst + G~c+ b2)\nThe total number of parameters is now a much more manageable O(|V | + \u2211 i |Ci|) instead of\nO( \u2211\ni |V ||Ci|). The advantage of a low-rank adaptation is that it forces the model to share information between similar contexts. The disadvantage, is that important differences between similar contexts can be lost.\nWe employ feature hashing to reduce the memory requirements but retain some of the benefits of having an individual bias term for each contextword pair. The context-word pairs are hashed into buckets and individual bias terms are learned for each bucket. The hashing technique relies on having direct access to the context variables c1:n. Representing context as a latent topic distribution precludes the use of this hashing adaptation.\nThe choice of hashing function is motivated by what is easy and fast to perform inside the Tensorflow computation graph framework. If w is a word id and c1n are context variable ids then the hash table index is computed as\nhi(w, ci) = wr0 + ciri mod l\nwhere l is the size of the hash table and r0 and the ri\u2019s are all fixed random integers. The value of l is usually set to a large prime number. The function H : Z \u2192 R maps hash indices to hash values and is implemented as a simple array.\nSince l is much smaller than the total number of inputs, there will be many hash collusions. Hash collusions are known to negatively effect the perplexity (Mikolov et al., 2011). To deal with this issue, we restrict the hash table to context-word pairs that are observed in the training data. A Bloom filter data structure records which context-word pairs are eligible to have entries in the hash table. The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011). A small amount of false positives is relatively harmless in this application because they do not impair the ability of the Bloom filter to eliminate almost all of the hash collusions.\nThe function \u03b2 : Z \u2192 [0, 1] is used by the Bloom filter to map hash indices to binary values.\nB(w, ci) = 16\u220f j=1 \u03b2(hi,j(w, ci))\nThe hash functions hi,j are defined in the same way as the hi\u2019s above except that they use distinct random integers and the size of the table, l, can be different. Because \u03b2 is a binary function, the productB(w, ci) will always be zero or one. Thus, any word-context pairs not found in the Bloom filter will have their hash values set to zero.\nThe final expression for the hashed adaptation term is given by\nHash(w, c1:n) = n\u2211 i=1 H(hi(w, ci))B(w, ci)\nyt = softmax(Vst+G~c+b2+Hash(wt, c1:n))"}, {"heading": "3 Data", "text": "The experiments make use of three corpora chosen to give a diverse prospective on adaptation in language modeling. Summary information on the datasets (Reddit, Twitter, and SCOTUS) is provided in Table 1 and each source is discussed individually below. The Reddit and SCOTUS data are tokenized and lower-cased using the standard NLTK tokenizer (Bird et al., 2009).\nReddit Reddit is the world\u2019s largest online discussion forum and is comprised of thousands of active subcommunities dedicated to a wide variety of themes. Our training data is 8 million sentences from Reddit comments during the month of April 2015. The 68,000 word vocabulary is selected by taking all tokens that occur at least 20 times in the training data. The remaining tokens are mapped to a special UNK token leaving us with an OOV rate of 2.3%.\nThe context variable is the identity of the subreddit, i.e. community, that the comment came from. There are 5,800 subreddits with at least 50 training sentences. The remaining ones are grouped together in an UNK category. The largest subreddit occupies just 4.5% of the data and the perplexity of the subreddit distribution is 742. By using a large number of subreddits, we highlight an advantage of model adaptation which is to be able to use a single unified model instead of training thousands of separate models for each individual community. Similarly, using context dependent bias vectors for this data instead of the hash adaptation would require learning 400 million additional parameters.\nTwitter The Twitter training data has 77,000 Tweets each annotated with one of nine languages: English, German, Italian, Spanish, Portuguese, Basque, Catalan, Galician, and French. The corpus was collected by combining resources from published data for language identification tasks during the past few years. Sentences labeled as unknown, ambiguous, or containing code-switching were not included. The data is unbalanced across languages with more than 32% of the Tweets being Spanish and the smallest four languages (Italian, German, Basque, and Galician) all individually less than 1.5% of the total. There are 194 unique character tokens in the vocabulary. Graphemes that are surrogate-pairs in the UTF-16 encoding, such as emoji, are split into multiple vocabulary tokens. No preprocessing or tokenization is performed on this data except that newlines were replaced with spaces for convenience.\nSCOTUS Approximately 864,000 sentences of training data spanning arguments from 1990- 2011. These are speech transcripts from the United States Supreme Court. Utterances are labeled with the case being argued (n=1,765), the speaker id (n=2,276), and the speaker role (justice, advocate, or unidentified). These three context variables are defined in the same way as in Hutchinson et al. (2013), where a small portion of this data was used in language modeling experiments. The vocabulary size is around 18,000 words. Utterances longer than 45 words were split into smaller utterances."}, {"heading": "4 Experiments", "text": "In these experiments we fix the size of the word embedding dimensions and recurrent layers so as not to exhaust our computational resources and then vary the different mechanisms for adapting the model. We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016). Dropout was used as a regularizer on the input and outputs of the recurrent layer as described in Zaremba et al. (2014). When the vocabulary is large, computing the full cross-entropy loss can be prohibitively expensive. For the large vocabulary experiments, we used a sampled softmax strategy with a unigram distribution to speed up training (Jean et al., 2015).\nA summary of the key hyperparameters for each class of experiments if given in Table 2. The total parameter column in this table is based on the un-\nadapted model. Adapted models will have more parameters depending on the type of adaptation. When using hash adaptation of the output layer, the size of the Bloom filter is 100 million and the size of the hash table is 80 million. The model is implemented using the Tensorflow library.1 Optimization is done using Adam with a learning rate of 0.001. Each model trained in under three days using 8 CPU threads.\nAlthough the model is trained as a language model, it can be used as a generative text classifier. When there are multiple context variables, we treat all but one of them as known values and attempt to identify the unknown one. It is not necessary to compute the probabilities over the full vocabulary. The sampled softmax criteria can be used to greatly speed up evaluation of the classifier."}, {"heading": "4.1 Reddit Experiments", "text": "The size of the subreddit embeddings was set to 25. Table 3 gives the perplexities and average AUCs for subreddit detection for different adapted models.The evaluation data contains 60,000 sentences. For comparison, an unadapted 4-gram Kneser-Ney model trained on the same data has a perplexity of 119. The models with the best perplexity do not use multiplicative adaptation of the hidden layer, but it is useful in the detection experiments.\nWe can inspect the context embeddings learned by the model to see if it is exploiting similarities between subreddits in the way that we expect. Table 4 lists the nearest neighbors by Euclidean distance to three selected subreddits. We can see that the nearest neighbors match our intuitions. The closest subreddits to Pittsburgh are communities created for other big cities and states. The Python subreddit is close to other programming languages\u2019 communities, and the NBA subreddit\n1See https://github.com/ajaech/calm for code.\nis close to the communities for individual NBA teams.\nThe subreddit detection involves predicting the subreddit a given comment came from with eight subreddits to choose from (AskMen, AskScience, AskWomen, Atheism, ChangeMyView, Fitness, Politics, and Worldnews) and nine distractors (Books, Chicago, NYC, Seattle, ExplainLikeImFive, Science, Running, NFL, and TodayILearned).2 To make a classification decision we evaluate the perplexity of each comment under the assumption that it belongs to each of the eight subreddits. We use z-score normalization across the eight perplexities to create a score for each class. The predictions are evaluated by averaging the AUC of the eight individual ROC curves. The best model for the classification task uses all four types of adaptation. Interestingly, the multiplicative adaptation of the hidden layer is clearly useful for classification even though it does not help with perplexity.\nThe perplexities for selected large subreddits are listed in Table 5. It can be seen that the relative gain from adaptation is largest when the topic of the subreddit is more narrowly focused. The biggest gains were achieved for subreddits\n2These are the same subreddit used in Tran and Ostendorf (2016) for a related but not comparable classification task.\ndedicated to specific sports, tv shows, or video games. Whereas, the gains were smallest for subreddits like Videos or Funny whose content tends to be more diverse. The knowledge that a sentence came from a pro-wrestling subreddit effectively provides more information about the text than the analogous piece of knowledge for the Pics or Videos subreddit. This would seem to indicate that further gains could be possible if additional contextual information could be provided. An alternative explanation, that subreddits with less sentences in the training data receive more benefit from adaptation, is not supported by the data."}, {"heading": "4.2 Twitter experiments", "text": "The Twitter evaluation was done on a set of 14,960 Tweets. The language context embedding vector dimensionality was set to 8. When both the vocabulary and the number of contexts are small, as in this case, there is no danger of hash collusions. We disable the bloom filter making the hash adaptation essentially equivalent to having context dependent bias vectors.\nTable 6 reports the results of the experiments on the Twitter corpus. We compute both the perplexity and measure the performance of the models on a language identification task. In terms of perplexity, the best models do not make use of the multiplicative hidden layer adaptation, consistent with the results from the Reddit corpus. In general, the improvement in perplexity from adaptation is small (less than 5%) on this corpus compared to our other experiments where we saw relative improvements two to four times as big. This is likely because the LSTM can figure out by itself which language it is modeling early on in the sequence and adjust its predictions accordingly.\nTo investigate this further, we trained a logistic regression classifier to predict the language using the state from the LSTM at the last time step on the unadapted model as a feature vector. Using just 30 labeled examples per class it is possible to get 74.6% accuracy and a 49.3 F1 score. Furthermore, we find that a single dimension in the hidden state of the unadapted model is often enough to distinguish between different languages even though the model was not given any supervision signal (Karpathy et al., 2015; Radford et al., 2017). Figure 1 visualizes the value of the dimension of the hidden layer that is the strongest indicator of Spanish on three different code-swtiched tweets.\nCode-switching is not a part of the training data for the model but it provides a compelling visualization of the ability of the unsupervised model to quickly recognize the language. The fact that it is so easy for the unadapted model to pick-up on the identity of the contextual variable fits with our explanation for the small relative gain in perplexity from the adapted models.\nOur best model, using multiplicative adaptation of the hidden layer, achieves an accuracy of 94.2% on this task. That is a 19% relative reduction in the error rate from the best model without multiplicative adaptation."}, {"heading": "4.3 SCOTUS experiments", "text": "Table 7 lists the results for the experiments on the SCOTUS corpus. The size of the context embeddings are 9, 15, and 8 for the case, speaker, and role variables respectively. For calculating perplexity we use 60,000 sentence evaluation set. For the classification experiment we selected 4,000 sentences from the test data from eleven different justices and attempted to classify the identity of the justice. The perplexity of the distribution of judges over those sentences is 8.9 (11.0 would be uniform). So, the data is roughly balanced. When classifying justices, the model is given the case context variable, but we do not make any special\neffort to filter candidates based on who was serving on the court during that time, i.e. all eleven justices are considered for every case.\nFor both the perplexity and classification metrics, the hash adaptation makes a big difference. The model that uses only hash adaptation and no hidden layer adaptation has a better perplexity than any of the model variants that use both hidden adaptation and low-rank adaptation of the output layer.\nTo ascertain which of the context variables have the most impact, we trained additional models with using different combinations of context variables. The model architecture is the one that uses all four forms of adaptation. Results are listed in Table 8. The most useful variable is the indicator for the case. The role variable is highly redundant\u2014 almost every speaker only appears in a single role. The experiments indicate that the role variable provides useful information to the model, and the knowledge of the speaker identity seems to not convey much useful information beyond what is provided by the role.\nIn Table 9 we list sentences generated from the fully adapted model (same one as the last line in Table 7) using beam search. The value of the context variable for the Case is held fixed while we explore different values for the Speaker and Role variables. Anecdotally, we see that the model captures some information about John Roberts role as chief justice. The model learns that Justice Breyer tends to start his questions with the phrase \u201cI mean\u201d while Justice Kagan tends to start with \u201cWell\u201d. Roberts and Kagan appear in our data both as justices and earlier as advocates."}, {"heading": "5 Related Work", "text": "Multiple survey papers cover the early history of language model adaptation (DeMori and Federico, 1999; Bellegarda, 2004). We mention just the most recent closely related work here.\nThe multiplicative rescaling of the recurrent layer weights is used in the Hypernetwork model (Ha et al., 2016). The focus of this model is to allow the LSTM to adjust automatically depending on the context of the previous words. This is different from our work in that we are adapting based on contextual information external to the word sequence. Gangireddy et al. (2016) also use a rescaling of the hidden layer for adaptation but it is done as a fine-tuning step and not during training like our model.\nThe RNNME model from Mikolov et al. (2011) uses feature hashing to train a maximum entropy model alongside an RNN language model. The setup is similar to our method of using hashing to learn context-dependent biases. However, there are a number of differences. The motivation for the RNNME model was to speed-up training of the RNN not to compensate for the inadequacy of low-rank output layer adaptation, which had yet to be invented. Furthermore, Mikolov et al. (2011) do not use context dependent features in the maxent component of the RNNME model nor do they have a method for dealing with hash collusions such as our use of Bloom filters.\nThe idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work (Eisenstein et al., 2011; Hutchinson et al., 2013). In both of these cases, the correction term is encouraged to be sparse by including an L1 penalty. Our implementation did not promote sparsity in the hash adaptation\nfeatures but this idea is worth further consideration. The hybrid LSTM and count based language model is an alternative way of correcting for a lowrank approximation (Neubig and Dyer, 2016).\nHoang et al. (2016) studies how to incorporate side information into an RNN language model. For their data, they claim a bigger win by adapting at the output layer rather than the hidden layer. (This matches our own observations on the Reddit and SCOTUS data.) Their work did not address adapting at both the hidden and output layers simultaneously. Most work on adaptation does not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016)."}, {"heading": "6 Conclusions & Future Work", "text": "While our results suggest that there is not a onesize-fits-all approach to language model adaptation, it is clear that we improve over the standard adaptation approach. The model from Mikolov and Zweig (2012), equivalent to using just additive adaptation on the hidden layer and low-rank adaptation of the output layer, is outperformed for all three datasets at both the language modeling and classification tasks. For language modeling, the multiplicative hidden layer adaptation was only helpful for the SCTOUS dataset. However, the combined low-rank and hash adaptation of the output layer consistently gave the best perplexity. For the classification tasks, the multiplicative hidden layer adaptation is clearly useful, as is the combined low-rank and hash adaptation of the output layer.\nImportantly, there is not always a strong relationship between perplexity and classification scores. Our results may have implications for work on text generation where it can be more desirable to have more control over the generation rather than the lowest perplexity model. More studies are needed to get intuition about what\ntypes of context variables will provide the most benefit. Our investigation of the language context in the Twitter experiments gives a useful takeaway: context variables that are easily predictable from the text alone are unlikely to be helpful.\nIn future work, we would like to consider additional mechanisms for using the context embedding ~c to adapt the LSTM parameters. We also plan to extend our hash adaptation to incorporate longer word histories, rather than just unigrams combined with context."}], "references": [{"title": "Statistical language model adaptation: review and perspectives", "author": ["Jerome R Bellegarda."], "venue": "Speech Communication 42(1):93\u2013108.", "citeRegEx": "Bellegarda.,? 2004", "shortCiteRegEx": "Bellegarda.", "year": 2004}, {"title": "Natural Language Processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper."], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Space/time trade-offs in hash coding with allowable errors", "author": ["Burton H Bloom."], "venue": "Communications of the ACM 13(7):422\u2013426.", "citeRegEx": "Bloom.,? 1970", "shortCiteRegEx": "Bloom.", "year": 1970}, {"title": "Recurrent neural network language model adaptation for multi-genre broadcast speech recognition", "author": ["Xie Chen", "Tian Tan", "Xunying Liu", "Pierre Lanchantin", "Moquan Wan", "Mark JF Gales", "Philip C Woodland."], "venue": "Proceedings of InterSpeech.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Language model adaptation", "author": ["Renato DeMori", "Marcello Federico."], "venue": "Computational models of speech pattern processing, Springer, pages 280\u2013 303.", "citeRegEx": "DeMori and Federico.,? 1999", "shortCiteRegEx": "DeMori and Federico.", "year": 1999}, {"title": "Sparse additive generative models of text", "author": ["Jacob Eisenstein", "Amr Ahmed", "Eric P Xing"], "venue": null, "citeRegEx": "Eisenstein et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Eisenstein et al\\.", "year": 2011}, {"title": "Unsupervised adaptation of recurrent neural network language models", "author": ["Siva Reddy Gangireddy", "Pawel Swietojanski", "Peter Bell", "Steve Renals."], "venue": "Interspeech 2016 pages 2333\u20132337.", "citeRegEx": "Gangireddy et al\\.,? 2016", "shortCiteRegEx": "Gangireddy et al\\.", "year": 2016}, {"title": "Contextual LSTM (CLSTM) models for large scale NLP tasks", "author": ["Shalini Ghosh", "Oriol Vinyals", "Brian Strope", "Scott Roy", "Tom Dean", "Larry Heck."], "venue": "arXiv preprint arXiv:1602.06291 .", "citeRegEx": "Ghosh et al\\.,? 2016", "shortCiteRegEx": "Ghosh et al\\.", "year": 2016}, {"title": "LSTM: A search space odyssey. IEEE transactions on neural networks and learning systems", "author": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Hypernetworks", "author": ["David Ha", "Andrew Dai", "Quoc V Le."], "venue": "arXiv preprint arXiv:1609.09106 .", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Incorporating side information into recurrent neural network language models", "author": ["Cong Duy Vu Hoang", "Trevor Cohn", "Gholamreza Haffari."], "venue": "HLTNAACL.", "citeRegEx": "Hoang et al\\.,? 2016", "shortCiteRegEx": "Hoang et al\\.", "year": 2016}, {"title": "Exceptions in language as learned by the multi-factor sparse plus low-rank language model", "author": ["Brian Hutchinson", "Mari Ostendorf", "Maryam Fazel."], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, pages", "citeRegEx": "Hutchinson et al\\.,? 2013", "shortCiteRegEx": "Hutchinson et al\\.", "year": 2013}, {"title": "Tying word vectors and word classifiers: A loss framework for language modeling", "author": ["Hakan Inan", "Khashayar Khosravi", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01462 .", "citeRegEx": "Inan et al\\.,? 2016", "shortCiteRegEx": "Inan et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Document context language models", "author": ["Yangfeng Ji", "Trevor Cohn", "Lingpeng Kong", "Chris Dyer", "Jacob Eisenstein."], "venue": "CoRR abs/1511.03962.", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei."], "venue": "arXiv preprint arXiv:1506.02078 .", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "A persona-based neural conversation model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "preprint arXiv:1603.06155 .", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Strategies for training large scale neural network language models", "author": ["Tom\u00e1\u0161 Mikolov", "Anoop Deoras", "Daniel Povey", "Luk\u00e1\u0161 Burget", "Jan \u010cernock\u1ef3."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE,", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig."], "venue": "SLT . pages 234\u2013239.", "citeRegEx": "Mikolov and Zweig.,? 2012", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Generalizing and hybridizing count-based and neural language models", "author": ["Graham Neubig", "Chris Dyer."], "venue": "arXiv preprint arXiv:1606.00499 .", "citeRegEx": "Neubig and Dyer.,? 2016", "shortCiteRegEx": "Neubig and Dyer.", "year": 2016}, {"title": "Using the output embedding to improve language models", "author": ["Ofir Press", "Lior Wolf."], "venue": "arXiv preprint arXiv:1608.05859 .", "citeRegEx": "Press and Wolf.,? 2016", "shortCiteRegEx": "Press and Wolf.", "year": 2016}, {"title": "Learning to generate reviews and discovering sentiment", "author": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1704.01444 .", "citeRegEx": "Radford et al\\.,? 2017", "shortCiteRegEx": "Radford et al\\.", "year": 2017}, {"title": "Towards a continuous modeling of natural language domains", "author": ["Sebastian Ruder", "Parsa Ghaffari", "John G Breslin."], "venue": "arXiv preprint arXiv:1610.09158 .", "citeRegEx": "Ruder et al\\.,? 2016", "shortCiteRegEx": "Ruder et al\\.", "year": 2016}, {"title": "Randomized language models via perfect hash functions", "author": ["David Talbot", "Thorsten Brants."], "venue": "ACL. volume 8, pages 505\u2013513.", "citeRegEx": "Talbot and Brants.,? 2008", "shortCiteRegEx": "Talbot and Brants.", "year": 2008}, {"title": "Context-aware natural language generation with recurrent neural networks", "author": ["Jian Tang", "Yifan Yang", "Sam Carton", "Ming Zhang", "Qiaozhu Mei."], "venue": "arXiv preprint arXiv:1611.09900 .", "citeRegEx": "Tang et al\\.,? 2016", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Characterizing the language of online communities and its relation to community reception", "author": ["Trang Tran", "Mari Ostendorf."], "venue": "arXiv preprint arXiv:1609.04779 .", "citeRegEx": "Tran and Ostendorf.,? 2016", "shortCiteRegEx": "Tran and Ostendorf.", "year": 2016}, {"title": "Recurrent neural network based language model personalization by social network crowdsourcing", "author": ["Tsung-Hsien Wen", "Aaron Heidel", "Hung-yi Lee", "Yu Tsao", "Lin-Shan Lee."], "venue": "INTERSPEECH. pages 2703\u20132707.", "citeRegEx": "Wen et al\\.,? 2013", "shortCiteRegEx": "Wen et al\\.", "year": 2013}, {"title": "Randomized maximum entropy language models", "author": ["Puyang Xu", "Sanjeev Khudanpur", "Asela Gunawardana."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, pages 226\u2013230.", "citeRegEx": "Xu et al\\.,? 2011", "shortCiteRegEx": "Xu et al\\.", "year": 2011}, {"title": "Generative and discriminative text classification with recurrent neural networks", "author": ["Dani Yogatama", "Chris Dyer", "Wang Ling", "Phil Blunsom"], "venue": null, "citeRegEx": "Yogatama et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yogatama et al\\.", "year": 2017}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329 .", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 22, "context": "Domains are in many ways inadequate representations of context due to being ill-defined, discrete and incomparable, and not reflective of the diversity of human language (Ruder et al., 2016).", "startOffset": 170, "endOffset": 190}, {"referenceID": 24, "context": "These variables can be dynamically combined to create a continuous representation of context as a lowdimensional embedding (Tang et al., 2016).", "startOffset": 123, "endOffset": 142}, {"referenceID": 18, "context": "The standard approach for using a context embedding to adapt an RNNLM is to simply concatenate the context representation with the word embedding at the input to the RNN (Mikolov and Zweig, 2012).", "startOffset": 170, "endOffset": 195}, {"referenceID": 3, "context": "2016), adapting an LM to different genres of television shows (Chen et al., 2015), adapting to long range dependencies in a document (Ji et al.", "startOffset": 62, "endOffset": 81}, {"referenceID": 14, "context": ", 2015), adapting to long range dependencies in a document (Ji et al., 2015), sharing information in generative text classifiers (Yogatama et al.", "startOffset": 59, "endOffset": 76}, {"referenceID": 28, "context": ", 2015), sharing information in generative text classifiers (Yogatama et al., 2017), and in other cases as well.", "startOffset": 60, "endOffset": 83}, {"referenceID": 18, "context": "The method from Mikolov and Zweig (2012) of using the lowdimensional context embedding to adapt the output layer avoids the excessive memory issue of context-dependent bias vectors but our experiments show that it does not capture isolated but important details.", "startOffset": 16, "endOffset": 41}, {"referenceID": 18, "context": "In (Mikolov and Zweig, 2012) LDA topic vectors are used for the outside context.", "startOffset": 3, "endOffset": 28}, {"referenceID": 24, "context": "In (Tang et al., 2016) the outside context is a sentiment score and a product id for a product review dataset.", "startOffset": 3, "endOffset": 22}, {"referenceID": 3, "context": "more appropriate to use topic models (Chen et al., 2015; Ghosh et al., 2016) or an RNN (Hoang et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 7, "context": "more appropriate to use topic models (Chen et al., 2015; Ghosh et al., 2016) or an RNN (Hoang et al.", "startOffset": 37, "endOffset": 76}, {"referenceID": 10, "context": ", 2016) or an RNN (Hoang et al., 2016) to build the context representation.", "startOffset": 18, "endOffset": 38}, {"referenceID": 9, "context": "The method is borrowed from Ha et al. (2016) where it is used for dynamically adjusting the parameters of a language model in response to the previous words in the sentence.", "startOffset": 28, "endOffset": 45}, {"referenceID": 20, "context": "In our case, we tie the weights between the word embeddings in the input and output layer: WT = V (Press and Wolf, 2016; Inan et al., 2016).", "startOffset": 98, "endOffset": 139}, {"referenceID": 12, "context": "In our case, we tie the weights between the word embeddings in the input and output layer: WT = V (Press and Wolf, 2016; Inan et al., 2016).", "startOffset": 98, "endOffset": 139}, {"referenceID": 18, "context": "Mikolov and Zweig (2012) use a low-rank factorization of of the adaptation matrix, replacing the |V |\u00d7|C|matrix with the product of a matrix G of size |V | \u00d7 k and a context embedding ~c of size k.", "startOffset": 0, "endOffset": 25}, {"referenceID": 17, "context": "Hash collusions are known to negatively effect the perplexity (Mikolov et al., 2011).", "startOffset": 62, "endOffset": 84}, {"referenceID": 2, "context": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).", "startOffset": 135, "endOffset": 190}, {"referenceID": 23, "context": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).", "startOffset": 135, "endOffset": 190}, {"referenceID": 27, "context": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).", "startOffset": 135, "endOffset": 190}, {"referenceID": 1, "context": "The Reddit and SCOTUS data are tokenized and lower-cased using the standard NLTK tokenizer (Bird et al., 2009).", "startOffset": 91, "endOffset": 110}, {"referenceID": 8, "context": "We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016).", "startOffset": 92, "endOffset": 112}, {"referenceID": 13, "context": "For the large vocabulary experiments, we used a sampled softmax strategy with a unigram distribution to speed up training (Jean et al., 2015).", "startOffset": 122, "endOffset": 141}, {"referenceID": 8, "context": "We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016). Dropout was used as a regularizer on the input and outputs of the recurrent layer as described in Zaremba et al. (2014). When the vocabulary is large, computing the full cross-entropy loss can be prohibitively expensive.", "startOffset": 93, "endOffset": 234}, {"referenceID": 25, "context": "These are the same subreddit used in Tran and Ostendorf (2016) for a related but not comparable classification task.", "startOffset": 37, "endOffset": 63}, {"referenceID": 15, "context": "Furthermore, we find that a single dimension in the hidden state of the unadapted model is often enough to distinguish between different languages even though the model was not given any supervision signal (Karpathy et al., 2015; Radford et al., 2017).", "startOffset": 206, "endOffset": 251}, {"referenceID": 21, "context": "Furthermore, we find that a single dimension in the hidden state of the unadapted model is often enough to distinguish between different languages even though the model was not given any supervision signal (Karpathy et al., 2015; Radford et al., 2017).", "startOffset": 206, "endOffset": 251}, {"referenceID": 4, "context": "Multiple survey papers cover the early history of language model adaptation (DeMori and Federico, 1999; Bellegarda, 2004).", "startOffset": 76, "endOffset": 121}, {"referenceID": 0, "context": "Multiple survey papers cover the early history of language model adaptation (DeMori and Federico, 1999; Bellegarda, 2004).", "startOffset": 76, "endOffset": 121}, {"referenceID": 9, "context": "layer weights is used in the Hypernetwork model (Ha et al., 2016).", "startOffset": 48, "endOffset": 65}, {"referenceID": 6, "context": "Gangireddy et al. (2016) also use a rescaling of the hidden layer for adaptation but it is done as a fine-tuning step and not during training like our model.", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "The RNNME model from Mikolov et al. (2011) uses feature hashing to train a maximum entropy model alongside an RNN language model.", "startOffset": 21, "endOffset": 43}, {"referenceID": 16, "context": "The RNNME model from Mikolov et al. (2011) uses feature hashing to train a maximum entropy model alongside an RNN language model. The setup is similar to our method of using hashing to learn context-dependent biases. However, there are a number of differences. The motivation for the RNNME model was to speed-up training of the RNN not to compensate for the inadequacy of low-rank output layer adaptation, which had yet to be invented. Furthermore, Mikolov et al. (2011) do not use context dependent features in the maxent component of the RNNME model nor do they have a method for dealing with hash collusions such as our use of Bloom filters.", "startOffset": 21, "endOffset": 471}, {"referenceID": 5, "context": "The idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work (Eisenstein et al., 2011; Hutchinson et al., 2013).", "startOffset": 164, "endOffset": 214}, {"referenceID": 11, "context": "The idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work (Eisenstein et al., 2011; Hutchinson et al., 2013).", "startOffset": 164, "endOffset": 214}, {"referenceID": 19, "context": "The hybrid LSTM and count based language model is an alternative way of correcting for a lowrank approximation (Neubig and Dyer, 2016).", "startOffset": 111, "endOffset": 134}, {"referenceID": 11, "context": "not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 24, "context": "not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 10, "context": "not consider combining multiple context factors but there are some exceptions (Hutchinson et al., 2013; Tang et al., 2016; Hoang et al., 2016).", "startOffset": 78, "endOffset": 142}, {"referenceID": 18, "context": "The model from Mikolov and Zweig (2012), equivalent to using just additive", "startOffset": 15, "endOffset": 40}], "year": 2017, "abstractText": "Increased adaptability of RNN language models leads to improved predictions that benefit many applications. However, current methods do not take full advantage of the RNN structure. We show that the most widely-used approach to adaptation (concatenating the context with the word embedding at the input to the recurrent layer) is outperformed by a model that has some low-cost improvements: adaptation of both the hidden and output layers. and a feature hashing bias term to capture context idiosyncrasies. Experiments on language modeling and classification tasks using three different corpora demonstrate the advantages of the proposed techniques.", "creator": "LaTeX with hyperref package"}}}