{"id": "1704.06497", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2017", "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "abstract": "simplified structured prediction describes an mathematical plot maneuver utilizing learning is performed after automated feedback. this ensemble is received into the form of corresponding task item evaluation to a predicted output structure, without having connection to gold standard structures. advocates maximize this methodology by lifting simplified bandit learning to neural trend - reward - query learning problems using attention - based recurrent neural networks. furthermore, lets show aim to incorporate control traits into our learning ( such variance reduction ; statistical generalization. nsa present random expression on a simulated machine feedback model that shows confidence of up to vs. 89 bleu points for regression adaptation reducing simulated bandit feedback.", "histories": [["v1", "Fri, 21 Apr 2017 11:56:00 GMT  (34kb,D)", "http://arxiv.org/abs/1704.06497v1", "To appear in ACL 2017"]], "COMMENTS": "To appear in ACL 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["julia kreutzer", "artem sokolov", "stefan riezler"], "accepted": true, "id": "1704.06497"}, "pdf": {"name": "1704.06497.pdf", "metadata": {"source": "CRF", "title": "Bandit Structured Prediction for Neural Sequence-to-Sequence Learning", "authors": ["Julia Kreutzer", "Artem Sokolov", "Stefan Riezler"], "emails": ["kreutzer@cl.uni-heidelberg.de", "sokolov@cl.uni-heidelberg.de", "riezler@cl.uni-heidelberg.de"], "sections": [{"heading": "1 Introduction", "text": "Many NLP tasks involve learning to predict a structured output such as a sequence, a tree or a graph. Sequence-to-sequence learning with neural networks has recently become a popular approach that allows tackling structured prediction as a mapping problem between variable-length sequences, e.g., from foreign language sentences into target-language sentences (Sutskever et al., 2014), or from natural language input sentences into linearized versions of syntactic (Vinyals et al., 2015) or semantic parses (Jia and Liang, 2016). A known bottleneck in structured prediction is the requirement of large amounts of gold-standard structures for supervised learning of model parameters, especially for data-hungry neural network models. Sokolov et al. (2016a,b) presented a framework for stochastic structured prediction under bandit feedback that alleviates the need for\nlabeled output structures in learning: Following an online learning protocol, on each iteration the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure.1 They \u201cbanditize\u201d several objective functions for linear structured predictions, and evaluate the resulting algorithms with simulated bandit feedback on various NLP tasks.\nWe show how to lift linear structured prediction under bandit feedback to non-linear models for sequence-to-sequence learning with attentionbased recurrent neural networks (Bahdanau et al., 2015). Our framework is applicable to sequenceto-sequence learning from various types of weak feedback. For example, extracting learning signals from the execution of structured outputs against databases has been established in the communities of semantic parsing and grounded language learning since more than a decade (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011). Our work can build the basis for neural semantic parsing from weak feedback.\nIn this paper, we focus on the application of machine translation via neural sequence-to-sequence learning. The standard procedure of training neural machine translation (NMT) models is to compare their output to human-generated translations and to infer model updates from this comparison. However, the creation of reference translations or post-edits requires professional expertise of users. Our framework allows NMT models to learn from feedback that is weaker than human references or post-edits. One could imagine a scenario of personalized machine translation where translations have to be adapted to the user\u2019s specific purpose and domain. The feedback required by our methods can be provided by laymen users or can even\n1The name \u201cbandit feedback\u201d is inherited from the problem of maximizing the reward for a sequence of pulls of arms of so-called \u201cone-armed bandit\u201d slot machines.\nar X\niv :1\n70 4.\n06 49\n7v 1\n[ st\nat .M\nL ]\n2 1\nA pr\n2 01\n7\nbe implicit, e.g., inferred from user interactions with the translated content on a web page.\nStarting from the work of Sokolov et al. (2016a,b), we lift their objectives to neural sequence-to-sequence learning. We evaluate the resulting algorithms on the task of French-toEnglish translation domain adaptation where a seed model trained on Europarl data is adapted to the NewsCommentary and the TED talks domain with simulated weak feedback. By learning from this feedback, we find 4.08 BLEU points improvements on NewsCommentary, and 5.89 BLEU points improvement on TED. Furthermore, we show how control variates can be integrated in our algorithms, yielding faster learning and improved generalization in our experiments."}, {"heading": "2 Related Work", "text": "NMT models are most commonly trained under a word-level maximum likelihood objective. Even though this objective has successfully been applied to many sequence-to-sequence learning tasks, the resulting models suffer from exposure bias, since they learn to generate output words based on the history of given reference words, not on their own predictions. Ranzato et al. (2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daume\u0301 et al., 2009) to learn from feedback to the model\u2019s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards. In our bandit setting, feedback to only a single sample per sentence is available, making the learning\nproblem much harder. The approach by Ranzato et al. (2016) approximates the expectation with single samples, but still requires reference translations which are unavailable in the bandit setting.\nTo our knowledge, the only work on training NMT from weak feedback is the work by He et al. (2016). They propose a dual-learning mechanism where two translation models are jointly trained on monolingual data. The feedback in this case is a reward signal from language models and a reconstruction error. This is attractive because the feedback can automatically be generated from monolingual data and does not require any human references. However, we are interested in using estimates of human feedback on translation quality to directly adapt the model to the users\u2019 needs.\nOur approach follows most closely the work of Sokolov et al. (2016a,b). They introduce bandit learning objectives for structured prediction and apply them to various NLP tasks, including machine translation with linear models. Their approach can be seen as an instantiation of reinforcement learning to one-state Markov decision processes under linear policy models. In this paper, we transfer their algorithms to nonlinear sequence-to-sequence learning. Sokolov et al. (2016a) showed applications of linear bandit learning to tasks such as multiclass-classification, OCR, and chunking, where learning can be done from scratch. We focus on lifting their linear machine translation experiments to the more complex NMT that requires a warm start for training. This is done by training a seed model on one domain and adapting it to a new domain based on bandit feedback only. For this task we build on the work of Freitag and Al-Onaizan (2016), who investigate strategies to find the best of both worlds: models that adapt well to the new domain without deteriorating on the old domain. In contrast to previous approaches to domain adaptation for NMT, we do not require in-domain parallel data, but consult direct feedback to the translations generated for the new domain."}, {"heading": "3 Neural Machine Translation", "text": "Neural models for machine translation are based on a sequence-to-sequence learning architecture consisting of an encoder and a decoder (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015). An encoder Recurrent Neural Network (RNN) reads in the source sentence and a decoder\nRNN generates the target sentence conditioned on the encoded source.\nThe input to the encoder is a sequence of vectors x = (x1, . . . , xTx) representing a sequence of source words of length Tx. In the approach of Sutskever et al. (2014), they are encoded into a single vector c = q({h1, . . . , hTx}), where ht = f(xt, ht\u22121) is the hidden state of the RNN at time t. Several choices are possible for the non-linear functions f and q: Here we are using a Gated Recurrent Unit (GRU) (Chung et al., 2014) for f , and for q an attention mechanism that defines the context vector as a weighted sum over encoder hidden states (Bahdanau et al., 2015; Luong et al., 2015a).\nThe decoder RNN predicts the next target word yt at time t given the context vector c and the previous target words y<t = {y1, . . . , yt\u22121} from a probability distribution over the target vocabulary V . This distribution is the result of a softmax transformation of the decoder outputs o = {o1, . . . , oTy}, such that\np\u03b8(yt = wi|y<t, c) = exp(owi)\u2211V v=1 exp(owv) .\nThe probability of a full sequence of outputs y = (y1, . . . , yTy) of length Ty is defined as the product of the conditional word probabilities:\np\u03b8(y|x) = Ty\u220f t=1 p\u03b8(yt|y<t, c).\nSince this encoder-decoder architecture is fully differentiable, it can be trained with gradient descent methods. Given a parallel training set of S source sentences and their reference translations D = {(x(s),y(s))}Ss=1, we can define a wordlevel Maximum Likelihood Estimation (MLE) objective, which aims to find the parameters\n\u03b8\u0302MLE = arg max \u03b8 LMLE(\u03b8)\nof the following loss function:\nLMLE(\u03b8) = S\u2211 s=1 log p\u03b8(y (s)|x(s))\n= S\u2211 s=1 Ty\u2211 t=1 log p\u03b8(yt|x(s),y (s) <t ).\nThis loss function is non-convex for the case of neural networks. Clever initialization strategies,\nAlgorithm 1 Neural Bandit Structured Prediction Input: Sequence of learning rates \u03b3k Output: Optimal parameters \u03b8\u0302\n1: Initialize \u03b80 2: for k = 0, . . . ,K do 3: Observe xk 4: Sample y\u0303k \u223c p\u03b8(y|xk) 5: Obtain feedback \u2206(y\u0303k) 6: \u03b8k+1 = \u03b8k \u2212 \u03b3k sk 7: Choose a solution \u03b8\u0302 from the list {\u03b80, . . . , \u03b8K}\nadaptive learning rates and momentum techniques are required to find good local maxima and to speed up convergence (Sutskever et al., 2013). Another trick of the trade is to ensemble several models with different random initializations to improve over single models (Luong et al., 2015a).\nAt test time, we face a search problem to find the sequence of target words with the highest probability. Beam search reduces the search error in comparison to greedy search, but also exponentially increases decoding time."}, {"heading": "4 Neural Bandit Structured Prediction", "text": "Algorithm 1 is an adaptation of the Bandit Structured Prediction algorithm of Sokolov et al. (2016b) to neural models: For K rounds, a model with parameters \u03b8 receives an input, samples an output structure, and receives user feedback. Based on this feedback, a stochastic gradient sk is computed and the model parameters are updated. As a post-optimization step, a solution \u03b8\u0302 is selected from the iterates. This is done with onlineto-batch conversion by choosing the model with optimal performance on held-out data.\nThe core of the algorithm is the sampling: if the model distribution is very peaked, the model exploits, i.e., it presents the most probable outputs to the user. If the distribution is close to uniform, the model explores, i.e., it presents random outputs to the user. The balance between exploitation and exploration is crucial to the learning process: in the beginning the model is rather uninformed and needs to explore in order to find outputs with high reward, while in the end it ideally converges towards a peaked distribution that exactly fits the user\u2019s needs. Pre-training the model, i.e. setting \u03b80 wisely, ensures a reasonable exploitationexploration trade-off.\nThis online learning algorithm can be applied\nto any objective L provided the stochastic gradients sk are unbiased estimators of the true gradient of the objective, i.e., we require \u2207L = E[sk]. In the following, we will present objectives from Sokolov et al. (2016b) transferred to neural models, and explain how they can be enhanced by control variates."}, {"heading": "4.1 Expected Loss (EL) Minimization", "text": "The first objective is defined as the expectation of a task loss \u2206(y\u0303), e.g. \u2212BLEU(y\u0303), over all input and output structures:\nLEL(\u03b8) =Ep(x) p\u03b8(y\u0303|x) [\u2206(y\u0303)] . (1)\nIn the case of full-information learning where reference outputs are available, we could evaluate all possible outputs against the reference to obtain an exact estimation of the loss function. However, this is not feasible in our setting since we only receive partial feedback for a single output structure per input. Instead, we use stochastic approximation to optimize this loss. The stochastic gradient for this objective is computed as follows:\nsELk =\u2206(y\u0303) \u2202 log p\u03b8(y\u0303|xk)\n\u2202\u03b8 . (2)\nObjective (1) is known from minimum risk training (Och, 2003) and has been lifted to NMT by Shen et al. (2016) \u2013 but not for learning from weak feedback. Equation (2) is an instance of the score function gradient estimator (Fu, 2006) where\n\u2207 log p\u03b8(y\u0303|xk) (3)\ndenotes the score function. We give an algorithm to sample structures from an encoder-decoder model in Algorithm 2. It corresponds to the algorithm presented by Shen et al. (2016) with the difference that it samples single structures, does not assume a reference structure, and additionally returns the sample probabilities. A similar objective has also been used in the REINFORCE algorithm (Williams, 1992) which has been adapted to NMT by Ranzato et al. (2016)."}, {"heading": "4.2 Pairwise Preference Ranking (PR)", "text": "The previous objective requires numerical feedback as an estimate of translation quality. Alternatively, we can learn from pairwise preference judgments that are formalized in preference ranking objectives. Let P(x) = {\u3008yi,yj\u3009 |yi,yj \u2208 Y(x)} denote the set of output pairs for an input x, and\nlet \u2206(\u3008yi,yj\u3009) : P(x) \u2192 [0, 1] denote a task loss function that specifies a dispreference of yi over yj . In our experimental simulations we use two types of pairwise feedback. Firstly, continuous pairwise feedback2 is computed as\n\u2206(\u3008yi,yj\u3009) = \u2206(yj)\u2212\u2206(yi),\nand secondly, binary feedback is computed as\n\u2206(\u3008yi,yj\u3009) = { 1 if \u2206(yj) > \u2206(yi), 0 otherwise.\nAnalogously to the sequence-level sampling for linear models (Sokolov et al., 2016b), we define the following probabilities for word-level sampling:\np+\u03b8 (y\u0303t = wi|x, y\u0302<t) = exp(owi)\u2211V v=1 exp(owv) , p\u2212\u03b8 (y\u0303t = wj |x, y\u0302<t) = exp(\u2212owj )\u2211V v=1 exp(\u2212owv) .\nThe effect of the negation within the softmax is that the two distributions p+\u03b8 and p \u2212 \u03b8 rank the next candidate target words y\u0303t (given the same history, here the greedy output y\u0302<t) in opposite order. Globally normalized models as in the linear case, or LSTM-CRFs (Huang et al., 2015) for the non-linear case would allow sampling full structures such that the ranking over full structures is reversed. But in the case of locally normalized RNNs we retrieve only locally reversed-rank samples. Since we want the model to learn to rank y\u0303i over y\u0303j , we would have to sample y\u0303i word-byword from p+\u03b8 and y\u0303j from p \u2212 \u03b8 . However, sampling all words of y\u0303j from p\u2212\u03b8 leads to translations that are neither fluent nor source-related, so we propose to randomly choose one position of y\u0303j where the next word is sampled from p\u2212\u03b8 and sample the remaining words from p+\u03b8 . We found that this method produces suitable negative samples, which are only slightly perturbed and still relatively fluent and source-related. A detailed algorithm is given in Algorithm 3.\nIn the same manner as for linear models, we define the probability of a pair of sequences as\np\u03b8(\u3008y\u0303i, y\u0303j\u3009 |x) = p+\u03b8 (y\u0303i|x)\u00d7 p \u2212 \u03b8 (y\u0303j |x).\n2Note that our definition of continuous feedback is slightly different from the one proposed in Sokolov et al. (2016b) where updates are only made for misrankings.\nAlgorithm 2 Sampling Structures Input: Model \u03b8, target sequence length limit Ty Output: Sequence of words w = (w1, . . . , wT y)\nand log-probability p 1: w0 = START, p0 = 0 2: w = (w0) 3: for t\u2190 1 . . . Ty do 4: wt \u223c p\u03b8(w|x,w<t) 5: pt = pt\u22121 + log p\u03b8(w|x,w<t) 6: w = (w1, . . . , wt\u22121, wt) 7: end for 8: Return w and pT\nNote that with the word-based sampling scheme described above, the sequence y\u0303j also includes words sampled from p+\u03b8 . The pairwise preference ranking objective expresses an expectation over losses over these pairs:\nLPR(\u03b8) =Ep(x) p\u03b8(\u3008y\u0303i,y\u0303j\u3009|x) [\u2206(\u3008y\u0303i, y\u0303j\u3009)] . (4)\nThe stochastic gradient for this objective is\nsPRk =\u2206(\u3008y\u0303i, y\u0303j\u3009) (5) \u00d7 ( \u2202 log p+\u03b8 (y\u0303i|xk)\n\u2202\u03b8 + \u2202 log p\u2212\u03b8 (y\u0303j |xk) \u2202\u03b8\n) .\nThis training procedure resembles well-known approaches for noise contrastive estimation (Gutmann and Hyva\u0308rinen, 2010) with negative sampling that are commonly used for neural language modeling (Collobert et al., 2011; Mnih and Teh, 2012; Mikolov et al., 2013). In these approaches, negative samples are drawn from a non-parametric noise distribution, whereas we draw them from the perturbed model distribution."}, {"heading": "4.3 Control Variates", "text": "The stochastic gradients defined in equations (2) and (5) can be used in stochastic gradient descent optimization (Bottou et al., 2016) where the full gradient is approximated using a minibatch or a single example in each update. The stochastic choice, in our case on inputs and outputs, introduces noise that leads to slower convergence and degrades performance. In the following, we explain how antithetic and additive control variate techniques from Monte Carlo simulation (Ross, 2013) can be used to remedy these problems.\nThe idea of additive control variates is to augment a random variable X whose expectation is\nAlgorithm 3 Sampling Pairs of Structures Input: Model \u03b8, target sequence length limit Ty Output: Pair of sequences \u3008w,w\u2032\u3009 and their log-\nprobability p 1: p0 = 0 2: w,w\u2032, w\u0302 = (START) 3: i \u223c U(1, T ) 4: for t\u2190 1 . . . Ty do 5: w\u0302t = arg maxw\u2208V p + \u03b8 (w|x, w\u0302<t) 6: wt \u223c p+\u03b8 (w|x, w\u0302<t) 7: pt = pt\u22121 + log p + \u03b8 (wt|x, w\u0302<t) 8: if i = t then 9: w\u2032t \u223c p\u2212\u03b8 (w|x, w\u0302<t)\n10: pt = pt + log p \u2212 \u03b8 (w \u2032 t|x, w\u0302<t) 11: else 12: w\u2032t \u223c p+\u03b8 (w|x, w\u0302<t) 13: pt = pt + log p + \u03b8 (w \u2032 t|x, w\u0302<t) 14: end if 15: w = (w1, . . . , wt\u22121, wt) 16: w\u2032 = (w\u20321, . . . , w \u2032 t\u22121, w \u2032 t) 17: w\u0302 = (w\u03021, . . . , w\u0302t\u22121, w\u0302t) 18: end for 19: Return \u3008w,w\u2032\u3009 and pT\nsought, by another random variable Y to which X is highly correlated. Y is then called the control variate. Let Y\u0304 furthermore denote its expectation. Then the following quantityX\u2212c\u0302 Y+c\u0302 Y\u0304 is an unbiased estimator of E[X]. In our case, the random variable of interest is the noisy gradient X = sk from Equation (2). The variance reduction effect of control variates can be seen by computing the variance of this quantity:\nVar(X \u2212 c\u0302 Y ) = Var(X) + c\u03022Var(Y ) (6) \u2212 2c\u0302Cov(X,Y ).\nChoosing a control variate such that Cov(X,Y ) is positive and high enough, the variance of the gradient estimate will be reduced.\nAn example is the average reward baseline known from reinforcement learning (Williams, 1992), yielding\nYk = \u2207 log p\u03b8(y\u0303|xk) 1\nk k\u2211 j=1 \u2206(y\u0303j). (7)\nThe optimal scalar c\u0302 can be derived easily by taking the derivative of (6), leading to c\u0302 = Cov(X,Y )Var(X) . This technique has been applied to using the score\nfunction (Equation (3)) as control variate in Ranganath et al. (2014), yielding the following control variate:\nY k = \u2207 log p\u03b8(y\u0303|xk). (8)\nNote that for both types of control variates, (7) and (8), the expectation Y\u0304 is zero, simplifying the implementation. However, the optimal scalar c\u0302 has to be estimated for every entry of the gradient separately for the score function control variate. We will explore both types of control variates for the stochastic gradient (2) in our experiments.\nA further effect of control variates is to reduce the magnitude of the gradient, the more so the more the stochastic gradient and the control variate covary. For L-Lipschitz continuous functions, a reduced gradient norm directly leads to a bound on L which appears in the algorithmic stability bounds of Hardt et al. (2016). This effect of improved generalization by control variates is empirically validated in our experiments.\nA similar variance reduction effect can be obtained by antithetic control variates. Here E[X] is approximated by the estimator X1+X22 whose variance is\nVar ( X1 +X2\n2\n) = 1\n4\n( Var(X1) (9)\n+ Var(X2) + 2Cov(X1, X2) ) .\nChoosing the variates X1 and X2 such that Cov(X1, X2) is negative will reduce the variance of the gradient estimate. Under certain assumptions, the stochastic gradient (5) of the pairwise preference objective can be interpreted as an antithetic estimator of the score function (3). The antithetic variates in this case would be\nX1 = \u2207 log p+\u03b8 (y\u0303i|xk), (10) X2 = \u2207 log p\u2212\u03b8 (y\u0303j |xk),\nwhere an antithetic dependence of X2 on X1 can be achieved by construction of p+\u03b8 and p \u2212 \u03b8 (see Capriotti (2008) which is loosely related to our approach). Similar to control variates, antithetic variates have the effect of shrinking the gradient norm, the more so the more the variates are antithetically correlated, leading to possible improvements in algorithmic stability (Hardt et al., 2016)."}, {"heading": "5 Experiments", "text": "In the following, we present an experimental evaluation of the learning objectives presented above\non machine translation domain adaptation. We compare how the presented neural bandit learning objectives perform in comparison to linear models, then discuss the handling of unknown words and eventually investigate the impact of techniques for variance reduction."}, {"heading": "5.1 Setup", "text": "Data. We perform domain adaptation from Europarl (EP) to News Commentary (NC) and TED talks (TED) for translations from French to English. Table 1 provides details about the datasets. For data pre-processing we follow the procedure of Sokolov et al. (2016a,b) using cdec tools for filtering, lowercasing and tokenization. The challenge for the bandit learner is to adapt from the EP domain to NC or TED with weak feedback only.\nNMT Models. We choose a standard encoderdecoder architecture with single-layer GRU RNNs with 800 hidden units, a word embedding size of 300 and tanh activations. The encoder consists of a bidirectional RNN, where the hidden states of backward and forward RNN are concatenated. The decoder uses the attention mechanism proposed by Bahdanau et al. (2015).3 Source and target vocabularies contain the 30k most frequent words of the respective parts of the training corpus. We limit the maximum sentence length to 50. Dropout (Srivastava et al., 2014) with a probability of 0.5 is applied to the network in several places: on the embedded inputs, before the output layer, and on the initial state of the decoder RNN. The gradient is clipped when its norms exceeds 1.0 to prevent exploding gradients and stabilize learning (Pascanu et al., 2013). All models are implemented and trained with the sequence learning framework Neural Monkey (Libovicky\u0300 et al.,\n3We do not use beam search nor ensembling, although we are aware that higher performance is almost guaranteed with these techniques. Our goal is to show relative differences between different models, so a simple setup is sufficient for the purpose of our experiments.\n2016; Bojar et al., 2016). 4 They are trained with a minibatch size of 20, fitting onto single 8GB GPU machines. The training dataset is shuffled before each epoch.\nBaselines. The out-of-domain baseline is trained on the EP training set with standard MLE. For both NC and TED domains, we train two fullinformation in-domain baselines: The first indomain baseline is trained on the relatively small in-domain training data. The second in-domain baseline starts from the out-of-domain model and is further trained on the in-domain data. All baselines are trained with MLE and Adam (Kingma and Ba, 2014) (\u03b1 = 1\u00d7 10\u22124, \u03b21 = 0.9, \u03b22 = 0.999) until their performance stops increasing on respective held-out validation sets. The gap between the performance of the out-ofdomain model and the in-domain models defines the range of possible improvements for bandit learning. All models are evaluated with Neural Monkey\u2019s mteval. For statistical significance tests we used Approximate Randomization testing (Noreen, 1989).\nBandit Learning. Bandit learning starts with the parameters of the out-of-domain baseline. The bandit models are expected to improve over the out-of-domain baseline by receiving feedback from the new domain, but at most to reach the indomain baseline since the feedback is weak. The models are trained with Adam on in-domain data for at most 20 epochs. Adam\u2019s step-size parameter \u03b1 was tuned on the validation set and was found to perform best when set to 1\u00d7 10\u22125 for non-pairwise, 1\u00d7 10\u22126 for pairwise objectives on NC, 1\u00d7 10\u22127 for pairwise objectives on TED. The best model parameters, selected with early stopping on the in-domain validation set, are evaluated on the held-out in-domain test set. In the spirit of Freitag and Al-Onaizan (2016) they are additionally evaluated on the out-of-domain test set to investigate how much knowledge of the old domain the models lose while adapting to the new domain. Bandit learning experiments are repeated two times, with different random seeds, and mean BLEU scores with standard deviation are reported.\n4The Neural Monkey fork https://github. com/juliakreutzer/bandit-neuralmonkey contains bandit learning objectives and the configuration files for our experiments.\nFeedback Simulation. Weak feedback is simulated from the target side of the parallel corpus, but references are never revealed to the learner. Sokolov et al. (2016a,b) used a smoothed version of per-sentence BLEU for simulating the weak feedback for generated translations from the comparison with reference translations. Here, we use gGLEU instead, which Wu et al. (2016) recently introduced for learning from sentence-level reward signals correlating well with corpus BLEU. This metric is closely related to BLEU, but does not have a brevity penalty and considers the recall of matching n-grams. It is defined as the minimum of recall and precision over the total n-grams up to a certain n. Hence, for our experiments \u2206(y\u0303) = \u2212gGLEU(y\u0303,y), where y\u0303 is a sample translation and y is the reference translation.\nUnknown words. One drawback of NMT models is their limitation to a fixed source- and target vocabulary. In a domain adaptation setting, this limitation has a critical impact to the translation quality. The larger the distance between old and new domain, the more words in the new domain are unknown to the models trained on the old domain (represented with a special UNK token). We consider two strategies for this problem for our experiments:\n1. UNK-Replace: Jean et al. (2015) and Luong et al. (2015b) replace generated UNK tokens with aligned source words or their lexical translations in a post-processing step. Freitag and Al-Onaizan (2016) and Hashimoto et al. (2016) demonstrated that this technique is beneficial for NMT domain adaptation.\n2. BPE: Sennrich et al. (2016) introduce byte pair encoding (BPE) for word segmentation to build translation models on sub-word units. Rare words are decomposed into subword units, while the most frequent words remain single vocabulary items.\nFor UNK-Replace we use fast align to generate lexical translations on the EP training data. When an UNK token is generated, we look up the attention weights and find the source token that receives most attention in this step. If possible, we replace the UNK token by its lexical translation. If it is not included in the lexical translations, it is replaced by the source token. The main benefit of this technique is that it deals well\nwith unknown named entities that are just passed through from source to target. However, since it is a non-differentiable post-processing step, the NMT model cannot directly be trained for this behavior. Therefore we also train sub-word level NMT with BPE. We apply 29,800 merge operations to obtain a vocabulary of 29,908 sub-words. The procedure for training these models is exactly the same as for the word-based models. The advantage of this method is that the model is in principle able to generate any word composing it from sub-word units. However, training sequences become longer and candidate translations are sampled on a sub-word level, which introduces the risk of sampling nonsense words.\nControl variates. We implement the average baseline control variate as defined in Equation 7, which results in keeping an running average over previous losses. Intuitively, absolute gGLEU feedback is turned into relative feedback that reflects the current state of the model. The sign of the update is switched when the gGLEU for the current sample is worse than the average gGLEU, so the model makes a step away from it, while in the case of absolute feedback it would still make a small step towards it. In addition, we implement the score function control variate with a running estimate c\u0302k = 1k \u2211k j=1 Cov(sj ,\u2207 log p\u03b8(y\u0303j |xj)) Var(sj) ."}, {"heading": "5.2 Results", "text": "In the following, we discuss the results of the experimental evaluation of the models described above. The out-of-domain baseline results are given in Table 2, those for the in-domain baselines in 3. The results for bandit learning on NC and TED are reported in Table 4. For bandit learning we give mean improvements over the respective out-of-domain baselines in the Diff.-columns.\nBaselines. The NMT out-of-domain baselines, reported in Table 2, perform comparable to the linear baseline from Sokolov et al. (2016a,b) on\nNC, but the in-domain EP\u2192NC (Table 3) baselines outperform the linear baseline by more than 3 BLEU points. Continuing training of a pre-trained out-of-domain model on a small amount of in domain data is very hence effective, whilst the performance of the models solely trained on small indomain data is highly dependent on the size of this training data set. For TED, the in-domain dataset is almost four times as big as the NC training set, so the in-domain baselines perform better. This effect was previously observed by Luong and Manning (2015) and Freitag and Al-Onaizan (2016).\nBandit Learning. The NMT bandit models that optimize the EL objective yield generally a much higher improvement over the out-of-domain models than the corresponding linear models: As listed in Table 4, we find improvements of between 2.33 and 2.89 BLEU points on the NC domain, and between 4.18 and 5.18 BLEU points on the TED domain. In contrast, the linear models with sparse features and hypergraph re-decoding achieved a maximum improvement of 0.82 BLEU points on NC.\nOptimization of the PR objective shows improvements of up to 1.79 BLEU points on NC (compared to 0.6 BLEU points for linear models), but no significant improvement on TED. The biggest impact of this variance reduction tech-\nnique is a considerable speedup of training speed of 1 to 2 orders of magnitude compared to EL.\nA beneficial side-effect of NMT learning from weak feedback is that the knowledge from the out-domain training is not simply \u201coverwritten\u201d. This happens to full-information in-domain tuning where more than 4 BLEU points are lost in an evaluation on the out-domain data. On the contrary, the bandit learning models still achieve high results on the original domain. This is useful for conservative domain adaptation, where the performance of the models in the old domain is still relevant.\nUnknown words. By handling unknown words with UNK-Replace or BPEs, we find consistent improvements over the plain word-based models for all baselines and bandit learning models. We observe that the models with UNK replacement essentially benefit from passing through source tokens, and only marginally from lexical translations. Bandit learning models take particular advantage of UNK replacement when it is included already during training. The sub-word models achieve the overall highest improvement over the baselines, although sometimes generating nonsense words.\nControl variates. Applying the score function control variate to EL optimization does not largely change learning speed or BLEU results. However, the average reward control variate leads to\nimprovements of around 1 BLEU over the EL optimization without variance reduction on both domains."}, {"heading": "6 Conclusion", "text": "In this paper, we showed how to lift structured prediction under bandit feedback from linear models to non-linear sequence-to-sequence learning using recurrent neural networks with attention. We introduced algorithms to train these models under numerical feedback to single output structures or under preference rankings over pairs of structures. In our experimental evaluation on the task of neural machine translation domain adaptation, we found relative improvements of up to 5.89 BLEU points over out-of-domain seed models, outperforming also linear bandit models. Furthermore, we argued that pairwise ranking under bandit feedback can be interpreted as a use of antithetic variates, and we showed how to include average reward and score function baselines as control variates for improved training speed and generalization. In future work, we would like to apply the presented non-linear bandit learners to other structured prediction tasks."}, {"heading": "Acknowledgments", "text": "This research was supported in part by the German research foundation (DFG), and in part by a research cooperation grant with the Amazon Development Center Germany."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR. San Diego, CA.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "UFAL submissions to the IWSLT 2016 MT track", "author": ["Ond\u0159ej Bojar", "Roman Sudarikov", "Tom Kocmi", "Jind\u0159ich Helcl", "Ond\u0159ej C\u0131fka."], "venue": "IWSLT . Seattle, WA.", "citeRegEx": "Bojar et al\\.,? 2016", "shortCiteRegEx": "Bojar et al\\.", "year": 2016}, {"title": "Optimization methods for large-scale machine learning", "author": ["Leon Bottou", "Frank E. Curtis", "Jorge Nocedal."], "venue": "eprint arXiv:1606.04838v1 .", "citeRegEx": "Bottou et al\\.,? 2016", "shortCiteRegEx": "Bottou et al\\.", "year": 2016}, {"title": "Reducing the variance of likelihood ratio greeks in Monte Carlo", "author": ["Luca Capriotti."], "venue": "WCS. Miami, FL.", "citeRegEx": "Capriotti.,? 2008", "shortCiteRegEx": "Capriotti.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "\u00c7alar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "eprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Driving semantic parsing from the world\u2019s response", "author": ["James Clarke", "Dan Goldwasser", "Wing-Wei Chang", "Dan Roth."], "venue": "CoNLL. Portland, OR.", "citeRegEx": "Clarke et al\\.,? 2010", "shortCiteRegEx": "Clarke et al\\.", "year": 2010}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "JMLR 12:2461\u20132505.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Search-based structured prediction", "author": ["Hal Daum\u00e9", "John Langford", "Daniel Marcu."], "venue": "Machine learning 75(3):297\u2013325.", "citeRegEx": "Daum\u00e9 et al\\.,? 2009", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "Fast domain adaptation for neural machine translation", "author": ["Markus Freitag", "Yaser Al-Onaizan."], "venue": "eprint arXiv:1612.06897 .", "citeRegEx": "Freitag and Al.Onaizan.,? 2016", "shortCiteRegEx": "Freitag and Al.Onaizan.", "year": 2016}, {"title": "Gradient estimation", "author": ["Michael C. Fu."], "venue": "S.G. Henderson and B.L. Nelson, editors, Handbook in Operations Research and Management Science, volume 13, pages 575\u2013616.", "citeRegEx": "Fu.,? 2006", "shortCiteRegEx": "Fu.", "year": 2006}, {"title": "Softmaxmargin training for structured log-linear models", "author": ["Kevin Gimpel", "Noah A. Smith."], "venue": "Technical Report CMU-LTI-10-008, Carnegie Mellon University.", "citeRegEx": "Gimpel and Smith.,? 2010", "shortCiteRegEx": "Gimpel and Smith.", "year": 2010}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "AISTATS. Sardinia, Italy.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "author": ["Moritz Hardt", "Ben Recht", "Yoram Singer."], "venue": "ICML. New York, NY.", "citeRegEx": "Hardt et al\\.,? 2016", "shortCiteRegEx": "Hardt et al\\.", "year": 2016}, {"title": "Domain adaptation and attentionbased unknown word replacement in chinese-tojapanese neural machine translation", "author": ["Kazuma Hashimoto", "Akiko Eriguchi", "Yoshimasa Tsuruoka."], "venue": "COLING Workshop on Asian Translation. Osaka, Japan.", "citeRegEx": "Hashimoto et al\\.,? 2016", "shortCiteRegEx": "Hashimoto et al\\.", "year": 2016}, {"title": "Dual learning for machine translation", "author": ["Di He", "Yingce Xia", "Tao Qin", "Liwei Wang", "Nenghai Yu", "Tieyan Liu", "Wei-Ying Ma."], "venue": "NIPS. Barcelona, Spain.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Maximum expected BLEU training of phrase and lexicon translation models", "author": ["Xiaodong He", "Li Deng."], "venue": "ACL. Jeju Island, Korea.", "citeRegEx": "He and Deng.,? 2012", "shortCiteRegEx": "He and Deng.", "year": 2012}, {"title": "Bidirectional LSTM-CRF models for sequence tagging", "author": ["Zhiheng Huang", "Wei Xu", "Kai Yu."], "venue": "eprint arXiv:1508.01991 .", "citeRegEx": "Huang et al\\.,? 2015", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Montreal neural machine translation systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "WMT . Lisbon, Portugal.", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "ACL. Berlin, Germany.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "eprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Learning dependency-based compositional semantics", "author": ["Percy Liang", "Michael I. Jordan", "Dan Klein."], "venue": "ACL-HLT . Portland, OR.", "citeRegEx": "Liang et al\\.,? 2011", "shortCiteRegEx": "Liang et al\\.", "year": 2011}, {"title": "CUNI system for WMT16 automatic post-editing and multimodal translation tasks", "author": ["Jind\u0159ich Libovick\u1ef3", "Jind\u0159ich Helcl", "Marek Tlust\u1ef3", "Pavel Pecina", "Ond\u0159ej Bojar."], "venue": "WMT . Berlin, Germany.", "citeRegEx": "Libovick\u1ef3 et al\\.,? 2016", "shortCiteRegEx": "Libovick\u1ef3 et al\\.", "year": 2016}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "IWSLT . Da Nang, Vietnam.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "EMNLP. Lisbon, Portugal.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL. Beijing, China.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean."], "venue": "NIPS. Lake Tahoe, CA.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "ICML. Edinburgh, Scotland.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Computer Intensive Methods for Testing Hypotheses", "author": ["Eric W. Noreen."], "venue": "An Introduction. Wiley.", "citeRegEx": "Noreen.,? 1989", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz J. Och."], "venue": "HLT-NAACL. Edmonton, Canada.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "ICML. Atlanta, GA.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Black box variational inference", "author": ["Rajesh Ranganath", "Sean Gerrish", "David M. Blei."], "venue": "AISTATS. Reykjavik, Iceland.", "citeRegEx": "Ranganath et al\\.,? 2014", "shortCiteRegEx": "Ranganath et al\\.", "year": 2014}, {"title": "Sequence level training with recurrent neural networks", "author": ["MarcAurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba."], "venue": "ICLR. San Juan, Puerto Rico.", "citeRegEx": "Ranzato et al\\.,? 2016", "shortCiteRegEx": "Ranzato et al\\.", "year": 2016}, {"title": "Simulation", "author": ["Sheldon M. Ross."], "venue": "Elsevier, fifth edition.", "citeRegEx": "Ross.,? 2013", "shortCiteRegEx": "Ross.", "year": 2013}, {"title": "A reduction of imitation learning and structured prediction to no-regret online learning", "author": ["St\u00e9phane Ross", "Geoffrey J Gordon", "Drew Bagnell."], "venue": "AISTATS. Ft. Lauderdale, FL.", "citeRegEx": "Ross et al\\.,? 2011", "shortCiteRegEx": "Ross et al\\.", "year": 2011}, {"title": "Is imitation learning the route to humanoid robots? Trends in Cognitive Sciences 3(6):233\u2013242", "author": ["Stefan Schaal"], "venue": null, "citeRegEx": "Schaal.,? \\Q1999\\E", "shortCiteRegEx": "Schaal.", "year": 1999}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["John Schulman", "Nicolas Heess", "Theophane Weber", "Pieter Abbeel."], "venue": "NIPS. Montreal, Canada.", "citeRegEx": "Schulman et al\\.,? 2015", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL. Berlin, Germany.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "ACL. Berlin, Germany.", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Minimum risk annealing for training log-linear models", "author": ["David A. Smith", "Jason Eisner."], "venue": "COLING-ACL. Sydney, Australia.", "citeRegEx": "Smith and Eisner.,? 2006", "shortCiteRegEx": "Smith and Eisner.", "year": 2006}, {"title": "Learning structured predictors from bandit feedback for interactive NLP", "author": ["Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler."], "venue": "ACL. Berlin, Germany.", "citeRegEx": "Sokolov et al\\.,? 2016a", "shortCiteRegEx": "Sokolov et al\\.", "year": 2016}, {"title": "Stochastic structured prediction under bandit feedback", "author": ["Artem Sokolov", "Julia Kreutzer", "Christopher Lo", "Stefan Riezler."], "venue": "NIPS. Barcelona, Spain.", "citeRegEx": "Sokolov et al\\.,? 2016b", "shortCiteRegEx": "Sokolov et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton."], "venue": "ICML. Atlanta, GA.", "citeRegEx": "Sutskever et al\\.,? 2013", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS. Montreal, Canada.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Reinforcement Learning", "author": ["Richard S. Sutton", "Andrew G. Barto."], "venue": "An Introduction. The MIT Press.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["Richard S. Sutton", "David McAllester", "Satinder Singh", "Yishay Mansour."], "venue": "NIPS. Vancouver, Canada.", "citeRegEx": "Sutton et al\\.,? 2000", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "NIPS. Montreal, Canada.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams."], "venue": "Machine Learning 20:229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human", "author": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey"], "venue": null, "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Probabilistic models of vision and max-margin methods", "author": ["Alan Yuille", "Xuming He."], "venue": "Frontiers of Electrical and Electronic Engineering 7(1):94\u2013106.", "citeRegEx": "Yuille and He.,? 2012", "shortCiteRegEx": "Yuille and He.", "year": 2012}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "UAI. Edinburgh, Scotland.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 44, "context": ", from foreign language sentences into target-language sentences (Sutskever et al., 2014), or from natural language input sentences into linearized versions of syntactic (Vinyals et al.", "startOffset": 65, "endOffset": 89}, {"referenceID": 47, "context": ", 2014), or from natural language input sentences into linearized versions of syntactic (Vinyals et al., 2015) or semantic parses (Jia and Liang, 2016).", "startOffset": 88, "endOffset": 110}, {"referenceID": 19, "context": ", 2015) or semantic parses (Jia and Liang, 2016).", "startOffset": 27, "endOffset": 48}, {"referenceID": 0, "context": "We show how to lift linear structured prediction under bandit feedback to non-linear models for sequence-to-sequence learning with attentionbased recurrent neural networks (Bahdanau et al., 2015).", "startOffset": 172, "endOffset": 195}, {"referenceID": 51, "context": "For example, extracting learning signals from the execution of structured outputs against databases has been established in the communities of semantic parsing and grounded language learning since more than a decade (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011).", "startOffset": 216, "endOffset": 288}, {"referenceID": 6, "context": "For example, extracting learning signals from the execution of structured outputs against databases has been established in the communities of semantic parsing and grounded language learning since more than a decade (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011).", "startOffset": 216, "endOffset": 288}, {"referenceID": 21, "context": "For example, extracting learning signals from the execution of structured outputs against databases has been established in the communities of semantic parsing and grounded language learning since more than a decade (Zettlemoyer and Collins, 2005; Clarke et al., 2010; Liang et al., 2011).", "startOffset": 216, "endOffset": 288}, {"referenceID": 45, "context": "(2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al.", "startOffset": 52, "endOffset": 97}, {"referenceID": 46, "context": "(2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al., 2000) and imitation learning (Schaal, 1999; Ross et al.", "startOffset": 52, "endOffset": 97}, {"referenceID": 35, "context": ", 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum\u00e9 et al., 2009) to learn from feedback to the model\u2019s own predictions.", "startOffset": 31, "endOffset": 84}, {"referenceID": 34, "context": ", 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum\u00e9 et al., 2009) to learn from feedback to the model\u2019s own predictions.", "startOffset": 31, "endOffset": 84}, {"referenceID": 8, "context": ", 2000) and imitation learning (Schaal, 1999; Ross et al., 2011; Daum\u00e9 et al., 2009) to learn from feedback to the model\u2019s own predictions.", "startOffset": 31, "endOffset": 84}, {"referenceID": 48, "context": "Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss.", "startOffset": 136, "endOffset": 152}, {"referenceID": 29, "context": "(2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT.", "startOffset": 34, "endOffset": 133}, {"referenceID": 39, "context": "(2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT.", "startOffset": 34, "endOffset": 133}, {"referenceID": 11, "context": "(2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT.", "startOffset": 34, "endOffset": 133}, {"referenceID": 50, "context": "(2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT.", "startOffset": 34, "endOffset": 133}, {"referenceID": 16, "context": "(2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT.", "startOffset": 34, "endOffset": 133}, {"referenceID": 10, "context": "These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning.", "startOffset": 109, "endOffset": 142}, {"referenceID": 36, "context": "These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning.", "startOffset": 109, "endOffset": 142}, {"referenceID": 9, "context": "Even though this objective has successfully been applied to many sequence-to-sequence learning tasks, the resulting models suffer from exposure bias, since they learn to generate output words based on the history of given reference words, not on their own predictions. Ranzato et al. (2016) apply techniques from reinforcement learning (Sutton and Barto, 1998; Sutton et al.", "startOffset": 38, "endOffset": 291}, {"referenceID": 8, "context": ", 2011; Daum\u00e9 et al., 2009) to learn from feedback to the model\u2019s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT.", "startOffset": 8, "endOffset": 365}, {"referenceID": 8, "context": ", 2011; Daum\u00e9 et al., 2009) to learn from feedback to the model\u2019s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards.", "startOffset": 8, "endOffset": 767}, {"referenceID": 8, "context": ", 2011; Daum\u00e9 et al., 2009) to learn from feedback to the model\u2019s own predictions. Furthermore, they address the mismatch between word-level loss and sequence-level evaluation metric by using a mixture of the REINFORCE (Williams, 1992) algorithm and the standard maximum likelihood training to directly optimize a sequence-level loss. Similarly, Shen et al. (2016) lift minimum risk training (Och, 2003; Smith and Eisner, 2006; Gimpel and Smith, 2010; Yuille and He, 2012; He and Deng, 2012) from linear models for machine translation to NMT. These works are closely related to ours in that they use the technique of score function gradient estimators (Fu, 2006; Schulman et al., 2015) for stochastic learning. However, the learning environment of Shen et al. (2016) is different from ours in that they approximate the true gradient of the risk objective in a full information setting by sampling a subset of translations and computing the expectation over their rewards. In our bandit setting, feedback to only a single sample per sentence is available, making the learning problem much harder. The approach by Ranzato et al. (2016) approximates the expectation with single samples, but still requires reference translations which are unavailable in the bandit setting.", "startOffset": 8, "endOffset": 1134}, {"referenceID": 15, "context": "To our knowledge, the only work on training NMT from weak feedback is the work by He et al. (2016). They propose a dual-learning mechanism where two translation models are jointly trained on monolingual data.", "startOffset": 82, "endOffset": 99}, {"referenceID": 39, "context": "Our approach follows most closely the work of Sokolov et al. (2016a,b). They introduce bandit learning objectives for structured prediction and apply them to various NLP tasks, including machine translation with linear models. Their approach can be seen as an instantiation of reinforcement learning to one-state Markov decision processes under linear policy models. In this paper, we transfer their algorithms to nonlinear sequence-to-sequence learning. Sokolov et al. (2016a) showed applications of linear bandit learning to tasks such as multiclass-classification, OCR, and chunking, where learning can be done from scratch.", "startOffset": 46, "endOffset": 478}, {"referenceID": 9, "context": "For this task we build on the work of Freitag and Al-Onaizan (2016), who investigate strategies to find the best of both worlds: models that adapt well to the new domain without deteriorating on the old domain.", "startOffset": 38, "endOffset": 68}, {"referenceID": 4, "context": "Neural models for machine translation are based on a sequence-to-sequence learning architecture consisting of an encoder and a decoder (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 135, "endOffset": 200}, {"referenceID": 44, "context": "Neural models for machine translation are based on a sequence-to-sequence learning architecture consisting of an encoder and a decoder (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 135, "endOffset": 200}, {"referenceID": 0, "context": "Neural models for machine translation are based on a sequence-to-sequence learning architecture consisting of an encoder and a decoder (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 135, "endOffset": 200}, {"referenceID": 5, "context": "Several choices are possible for the non-linear functions f and q: Here we are using a Gated Recurrent Unit (GRU) (Chung et al., 2014) for f , and for q an attention mechanism that defines the context vector as a weighted sum over encoder hidden states (Bahdanau et al.", "startOffset": 114, "endOffset": 134}, {"referenceID": 0, "context": ", 2014) for f , and for q an attention mechanism that defines the context vector as a weighted sum over encoder hidden states (Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 126, "endOffset": 170}, {"referenceID": 24, "context": ", 2014) for f , and for q an attention mechanism that defines the context vector as a weighted sum over encoder hidden states (Bahdanau et al., 2015; Luong et al., 2015a).", "startOffset": 126, "endOffset": 170}, {"referenceID": 38, "context": "In the approach of Sutskever et al. (2014), they are encoded into a single vector c = q({h1, .", "startOffset": 19, "endOffset": 43}, {"referenceID": 43, "context": "adaptive learning rates and momentum techniques are required to find good local maxima and to speed up convergence (Sutskever et al., 2013).", "startOffset": 115, "endOffset": 139}, {"referenceID": 24, "context": "Another trick of the trade is to ensemble several models with different random initializations to improve over single models (Luong et al., 2015a).", "startOffset": 125, "endOffset": 146}, {"referenceID": 39, "context": "Algorithm 1 is an adaptation of the Bandit Structured Prediction algorithm of Sokolov et al. (2016b) to neural models: For K rounds, a model with parameters \u03b8 receives an input, samples an output structure, and receives user feedback.", "startOffset": 78, "endOffset": 101}, {"referenceID": 29, "context": "to any objective L provided the stochastic gradients sk are unbiased estimators of the true gradient of the objective, i.e., we require \u2207L = E[sk]. In the following, we will present objectives from Sokolov et al. (2016b) transferred to neural models, and explain how they can be enhanced by control variates.", "startOffset": 34, "endOffset": 221}, {"referenceID": 29, "context": "Objective (1) is known from minimum risk training (Och, 2003) and has been lifted to NMT by Shen et al.", "startOffset": 50, "endOffset": 61}, {"referenceID": 10, "context": "Equation (2) is an instance of the score function gradient estimator (Fu, 2006) where", "startOffset": 69, "endOffset": 79}, {"referenceID": 28, "context": "Objective (1) is known from minimum risk training (Och, 2003) and has been lifted to NMT by Shen et al. (2016) \u2013 but not for learning from weak feedback.", "startOffset": 51, "endOffset": 111}, {"referenceID": 48, "context": "A similar objective has also been used in the REINFORCE algorithm (Williams, 1992) which has been adapted to NMT by Ranzato et al.", "startOffset": 66, "endOffset": 82}, {"referenceID": 10, "context": "denotes the score function. We give an algorithm to sample structures from an encoder-decoder model in Algorithm 2. It corresponds to the algorithm presented by Shen et al. (2016) with the difference that it samples single structures, does not assume a reference structure, and additionally returns the sample probabilities.", "startOffset": 18, "endOffset": 180}, {"referenceID": 10, "context": "denotes the score function. We give an algorithm to sample structures from an encoder-decoder model in Algorithm 2. It corresponds to the algorithm presented by Shen et al. (2016) with the difference that it samples single structures, does not assume a reference structure, and additionally returns the sample probabilities. A similar objective has also been used in the REINFORCE algorithm (Williams, 1992) which has been adapted to NMT by Ranzato et al. (2016).", "startOffset": 18, "endOffset": 463}, {"referenceID": 41, "context": "Analogously to the sequence-level sampling for linear models (Sokolov et al., 2016b), we define the following probabilities for word-level sampling:", "startOffset": 61, "endOffset": 84}, {"referenceID": 17, "context": "Globally normalized models as in the linear case, or LSTM-CRFs (Huang et al., 2015) for the non-linear case would allow sampling full structures such that the ranking over full structures is reversed.", "startOffset": 63, "endOffset": 83}, {"referenceID": 40, "context": "Note that our definition of continuous feedback is slightly different from the one proposed in Sokolov et al. (2016b) where updates are only made for misrankings.", "startOffset": 95, "endOffset": 118}, {"referenceID": 12, "context": "This training procedure resembles well-known approaches for noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) with negative sampling that are commonly used for neural language modeling (Collobert et al.", "startOffset": 89, "endOffset": 118}, {"referenceID": 7, "context": "This training procedure resembles well-known approaches for noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) with negative sampling that are commonly used for neural language modeling (Collobert et al., 2011; Mnih and Teh, 2012; Mikolov et al., 2013).", "startOffset": 194, "endOffset": 260}, {"referenceID": 27, "context": "This training procedure resembles well-known approaches for noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) with negative sampling that are commonly used for neural language modeling (Collobert et al., 2011; Mnih and Teh, 2012; Mikolov et al., 2013).", "startOffset": 194, "endOffset": 260}, {"referenceID": 26, "context": "This training procedure resembles well-known approaches for noise contrastive estimation (Gutmann and Hyv\u00e4rinen, 2010) with negative sampling that are commonly used for neural language modeling (Collobert et al., 2011; Mnih and Teh, 2012; Mikolov et al., 2013).", "startOffset": 194, "endOffset": 260}, {"referenceID": 2, "context": "The stochastic gradients defined in equations (2) and (5) can be used in stochastic gradient descent optimization (Bottou et al., 2016) where the full gradient is approximated using a minibatch or a single example in each update.", "startOffset": 114, "endOffset": 135}, {"referenceID": 33, "context": "In the following, we explain how antithetic and additive control variate techniques from Monte Carlo simulation (Ross, 2013) can be used to remedy these problems.", "startOffset": 112, "endOffset": 124}, {"referenceID": 48, "context": "An example is the average reward baseline known from reinforcement learning (Williams, 1992), yielding", "startOffset": 76, "endOffset": 92}, {"referenceID": 10, "context": "However, the optimal scalar \u0109 has to be estimated for every entry of the gradient separately for the score function control variate. We will explore both types of control variates for the stochastic gradient (2) in our experiments. A further effect of control variates is to reduce the magnitude of the gradient, the more so the more the stochastic gradient and the control variate covary. For L-Lipschitz continuous functions, a reduced gradient norm directly leads to a bound on L which appears in the algorithmic stability bounds of Hardt et al. (2016). This effect of improved generalization by control variates is empirically validated in our experiments.", "startOffset": 107, "endOffset": 556}, {"referenceID": 13, "context": "Similar to control variates, antithetic variates have the effect of shrinking the gradient norm, the more so the more the variates are antithetically correlated, leading to possible improvements in algorithmic stability (Hardt et al., 2016).", "startOffset": 220, "endOffset": 240}, {"referenceID": 3, "context": "where an antithetic dependence of X2 on X1 can be achieved by construction of p+\u03b8 and p \u2212 \u03b8 (see Capriotti (2008) which is loosely related to our approach).", "startOffset": 97, "endOffset": 114}, {"referenceID": 42, "context": "Dropout (Srivastava et al., 2014) with a probability of 0.", "startOffset": 8, "endOffset": 33}, {"referenceID": 30, "context": "0 to prevent exploding gradients and stabilize learning (Pascanu et al., 2013).", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "The decoder uses the attention mechanism proposed by Bahdanau et al. (2015).3 Source and target vocabularies contain the 30k most frequent words of the respective parts of the training corpus.", "startOffset": 53, "endOffset": 76}, {"referenceID": 20, "context": "All baselines are trained with MLE and Adam (Kingma and Ba, 2014) (\u03b1 = 1\u00d7 10\u22124, \u03b21 = 0.", "startOffset": 44, "endOffset": 65}, {"referenceID": 28, "context": "For statistical significance tests we used Approximate Randomization testing (Noreen, 1989).", "startOffset": 77, "endOffset": 91}, {"referenceID": 9, "context": "In the spirit of Freitag and Al-Onaizan (2016) they are additionally evaluated on the out-of-domain test set to investigate how much knowledge of the old domain the models lose while adapting to the new domain.", "startOffset": 17, "endOffset": 47}, {"referenceID": 40, "context": "Sokolov et al. (2016a,b) used a smoothed version of per-sentence BLEU for simulating the weak feedback for generated translations from the comparison with reference translations. Here, we use gGLEU instead, which Wu et al. (2016) recently introduced for learning from sentence-level reward signals correlating well with corpus BLEU.", "startOffset": 0, "endOffset": 230}, {"referenceID": 16, "context": "UNK-Replace: Jean et al. (2015) and Luong et al.", "startOffset": 13, "endOffset": 32}, {"referenceID": 16, "context": "UNK-Replace: Jean et al. (2015) and Luong et al. (2015b) replace generated UNK tokens with aligned source words or their lexical translations in a post-processing step.", "startOffset": 13, "endOffset": 57}, {"referenceID": 9, "context": "Freitag and Al-Onaizan (2016) and Hashimoto et al.", "startOffset": 0, "endOffset": 30}, {"referenceID": 9, "context": "Freitag and Al-Onaizan (2016) and Hashimoto et al. (2016) demonstrated that this technique is beneficial for NMT domain adaptation.", "startOffset": 0, "endOffset": 58}, {"referenceID": 37, "context": "BPE: Sennrich et al. (2016) introduce byte pair encoding (BPE) for word segmentation to build translation models on sub-word units.", "startOffset": 5, "endOffset": 28}, {"referenceID": 22, "context": "This effect was previously observed by Luong and Manning (2015) and Freitag and Al-Onaizan (2016).", "startOffset": 39, "endOffset": 64}, {"referenceID": 9, "context": "This effect was previously observed by Luong and Manning (2015) and Freitag and Al-Onaizan (2016).", "startOffset": 68, "endOffset": 98}], "year": 2017, "abstractText": "Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.", "creator": "LaTeX with hyperref package"}}}