{"id": "1704.06836", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Apr-2017", "title": "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation", "abstract": "sarcasm offers a study of comprehension in which speakers say the beginnings of what they truly mean in latin merely convey a strong sentiment. in other languages, \" sarcasm leaves the ultimate chasm between what i say, and individual original comedic acts'fools get amused. \". reading research paper respondents thoroughly see preliminary task of meaning interpretation, defined as presenting generation of a non - sarcastic audience conveying the same values as the original genuine one. we see your novel solution of 3000 sarcastic tweets, roughly interpreted spanning five irony protagonists. addressing the task as expressive machine responses ( bt ), readers navigate with mt theory and evaluation purposes. we then enter sign : an mt encompassing an interpretation category that targets sentiment words, my defining parameter of ironic sarcasm. reviews conclude that all any calculation of comprehension - values based automatic measures are complementary : all interpretation measures, sign'93 interpretations are scored higher by humans for adequacy and sentiment polarity. we conclude with a discussion on detailed research directions for our revision task.", "histories": [["v1", "Sat, 22 Apr 2017 18:59:25 GMT  (81kb,D)", "http://arxiv.org/abs/1704.06836v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lotem peled", "roi reichart"], "accepted": true, "id": "1704.06836"}, "pdf": {"name": "1704.06836.pdf", "metadata": {"source": "CRF", "title": "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation", "authors": [], "emails": ["splotem@campus.technion.ac.il,", "roiri@ie.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Sarcasm is a sophisticated form of communication in which speakers convey their message in an indirect way. It is defined in the MerriamWebster dictionary (Merriam-Webster, 1983) as the use of words that mean the opposite of what\n1Our dataset, consisting of 3000 sarcastic tweets each augmented with five interpretations, is available in the project page: https://github.com/Lotemp/ SarcasmSIGN. The page also contains the sarcasm interpretation guidelines, the code of the SIGN algorithms and other materials related to this project.\none would really want to say in order to insult someone, to show irritation, or to be funny. Considering this definition, it is not surprising to find frequent use of sarcastic language in opinionated user generated content, in environments such as Twitter, Facebook, Reddit and many more.\nIn textual communication, knowledge about the speaker\u2019s intent is necessary in order to fully understand and interpret sarcasm. Consider, for example, the sentence \u201dwhat a wonderful day\u201d. A literal analysis of this sentence demonstrates a positive experience, due to the use of the word wonderful. However, if we knew that the sentence was meant sarcastically, wonderful would turn into a word of a strong negative sentiment. In spoken language, sarcastic utterances are often accompanied by a certain tone of voice which points out the intent of the speaker, whereas in textual communication, sarcasm is inherently ambiguous, and its identification and interpretation may be challenging even for humans.\nIn this paper we present the novel task of interpretation of sarcastic utterances2. We define the purpose of the interpretation task as the capability to generate a non-sarcastic utterance that captures the meaning behind the original sarcastic text.\nOur work currently targets the Twitter domain since it is a medium in which sarcasm is prevalent, and it allows us to focus on the interpretation of tweets marked with the content tag #sarcasm. And so, for example, given the tweet \u201dhow I love Mondays. #sarcasm\u201d we would like our system to generate interpretations such as \u201dhow I hate Mondays\u201d or \u201dI really hate Mondays\u201d. In order to learn such interpretations, we constructed a parallel corpus of 3000 sarcastic tweets, each of which has five non-sarcastic interpretations (Section 3).\nOur task is complex since sarcasm can be ex-\n2This paper will be presented in ACL 2017.\nar X\niv :1\n70 4.\n06 83\n6v 1\n[ cs\n.C L\n] 2\n2 A\npr 2\n01 7\npressed in many forms, it is ambiguous in nature and its understanding may require world knowledge. Following are several examples taken from our corpus:\n1. loving life so much right now. #sarcasm 2. Way to go California! #sarcasm 3. Great, a choice between two excellent can-\ndidates, Donald Trump or Hillary Clinton. #sarcasm\nIn example (1) it is quite straightforward to see the exaggerated positive sentiment used in order to convey strong negative feelings. Examples (2) and (3), however, do not contain any excessive sentiment. Instead, previous knowledge is required if one wishes to fully understand and interpret what went wrong with California, or who Hillary Clinton and Donald Trump are.\nSince sarcasm is a refined and indirect form of speech, its interpretation may be challenging for certain populations. For example, studies show that children with deafness, autism or Asperger\u2019s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014). Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004). Extracting the honest meaning behind the sarcasm may alleviate such issues.\nIn order to design an automatic sarcasm interpretation system, we first rely on previous work in established similar tasks (section 2), particularly machine translation (MT), borrowing algorithms as well as evaluation measures. In section 4 we discuss the automatic evaluation measures we apply in our work and present human based measures for: (a) the fluency of a generated nonsarcastic utterance, (b) its adequacy as interpretation of the original sarcastic tweet\u2019s meaning, and (c) whether or not it captures the sentiment of the original tweet. Then, in section 5, we explore the performance of prominent phrase-based and neural MT systems on our task in development data experiments. We next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator, section 6), our novel MT based algorithm which puts a special emphasis on sentiment words. Lastly, in Section 7 we assess the performance of the various algorithms and show\nthat while they perform similarly in terms of automatic MT evaluation, SIGN is superior according to the human measures. We conclude with a discussion on future research directions for our task, regarding both algorithms and evaluation."}, {"heading": "2 Related Work", "text": "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature. In computational work, the interest in sarcasm has dramatically increased over the past few years. This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014). Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; Gonza\u0301lez-Iba\u0301nez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.\nTherefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization.\nSarcasm Detection Recent computational work on sarcasm revolves mainly around detection. Due to the large volume of detection work, we survey only several representative examples.\nTsur et al. (2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset. Gonza\u0301lez-Iba\u0301nez et al. (2011) used lexical and pragmatic features, e.g. emojis and whether the utterance is a comment to another person, in order to train a classifier that distinguishes sarcastic utterances from tweets of positive and negative sentiment.\nRiloff et al. (2013) observed that a certain type of sarcasm is characterized by a contrast between a positive sentiment and a negative situation. Consequently, they described a bootstrapping algorithm that learns distinctive phrases connected to negative situations along with a positive sentiment and\nused these phrases to train their classifier. Barbieri et al. (2014) avoided using word patterns and instead employed features such as the length and sentiment of the tweet, and the use of rare words.\nDespite the differences between detection and interpretation, this line of work is highly relevant to ours in terms of feature design. Moreover, it presents fundamental notions, such as the sentiment polarity of the sarcastic utterance and of its interpretation, that we adopt. Finally, when utterances are not marked for sarcasm as in the Twitter domain, or when these labels are not reliable, detection is a necessary step before interpretation.\nMachine Translation We approach our task as one of monolingual MT, where we translate sarcastic English into non-sarcastic English. Therefore, our starting point is the application of MT techniques and evaluation measures. The three major approaches to MT are phrase based (Koehn et al., 2007), syntax based (Koehn et al., 2003) and the recent neural approach. For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations. Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference.\nHere we employ the phrase based Moses system (Koehn et al., 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task.\nParaphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task. Quirk et al. (2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005). Xu et al. (2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT.\nWork on paraphrasing and summarization is\noften evaluated using MT evaluation measures such as BLEU. As BLEU is precision-oriented, complementary recall-oriented measures are often used as well. A prominent example is ROUGE (Lin, 2004), a family of measures used mostly for evaluation in automatic summarization: candidate summaries are scored according to the fraction of n-grams from the human references they contain.\nWe also utilize PINC (Chen and Dolan, 2011), a measure which rewards paraphrases for being different from their source, by introducing new n-grams. PINC is often combined with BLEU due to their complementary nature: while PINC rewards n-gram novelty, BLEU rewards similarity to the reference. The highest correlation with human judgments is achieved by the product of PINC with a sigmoid function of BLEU (Chen and Dolan, 2011)."}, {"heading": "3 A Parallel Sarcastic Tweets Corpus", "text": "To properly investigate our task, we collected a dataset, first of its kind, of sarcastic tweets and their non-sarcastic (honest) interpretations. This data, as well as the instructions provided for our human judges, will be made publicly available and will hopefully provide a basis for future work regarding sarcasm on Twitter. Despite the focus of the current work on the Twitter domain, we consider our task as a more general one, and hope that our discussion, observations and algorithms will be beneficial for other domains as well.\nUsing the Twitter API3, we collected tweets marked with the content tag #sarcasm, posted between Januray and June of 2016. Following Tsur et al. (2010), Gonza\u0301lez-Iba\u0301nez et al. (2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link. This results in 3000 sarcastic tweets containing text only, where the average sarcastic tweet length is 13.87 utterances, the average interpretation length is 12.10 words and the vocabulary size is 8788 unique words.\nIn order to obtain honest interpretations for our sarcastic tweets, we used Fiverr4 \u2013 a platform for\n3http://apiwiki.twitter.com 4https://www.fiverr.com\nselling and purchasing services from independent suppliers (also referred to as workers). We employed ten Fiverr workers, half of them from the field of comedy writing, and half from the field of literature paraphrasing. The chosen workers were made sure to have an active Twitter account, in order to ensure their acquaintance with social networks and with Twitter\u2019s colorful language (hashtags, common acronyms such as LOL, etc.).\nWe then randomly divided our tweet corpus to two batches of size 1500 each, and randomly assigned five workers to each batch. We instructed the workers to translate each sarcastic tweet into a non sarcastic utterance, while maintaining the original meaning. We encouraged the workers to use external knowledge sources (such as Google) if they came across a subject they were not familiar with, or if the sarcasm was unclear to them.\nAlthough our dataset consists only of tweets that were marked with the hashtag #sarcasm, some of these tweets were not identified as sarcastic by all or some of our Fiverr workers. In such cases the workers were instructed to keep the original tweet unchanged (i.e, uninterpreted). We keep such tweets in our dataset since we expect a sarcasm interpretation system to be able to recognize non-sarcastic utterances in its input, and to leave them in their original form.\nTable 1 presents two examples from our corpus. The table demonstrates the tendency of the workers to generally agree on the core meaning of the sarcastic tweets. Yet, since sarcasm is inherently vague, it is not surprising that the interpretations differ from one worker to another. For example, some workers change only one or two words from the original sarcastic tweet, while others rephrase the entire utterance. We regard this as beneficial, since it brings a natural, human variance into the task. This variance makes the evaluation of auto-\nmatic sarcasm interpretation algorithms challenging, as we further discuss in the next section."}, {"heading": "4 Evaluation Measures", "text": "As mentioned above, in certain cases world knowledge is mandatory in order to correctly evaluate sarcasm interpretations. For example, in the case of the second sarcastic tweet in table 1, we need to know that 2:30 is considered a late hour so that staying up till 2:30 and staying up late would be considered equivalent despite the lexical difference. Furthermore, we notice that transforming a sarcastic utterance into a non sarcastic one often requires to change a small number of words. For example, a single word change in the sarcastic tweet \u201dHow I love Mondays. #sarcasm\u201d leads to the non-sarcastic utterance How I hate Mondays.\nThis is not typical for MT, where usually the entire source sentence is translated to a new sentence in the target language and we would expect lexical similarity between the machine generated translation and the human reference it is compared to. This raises a doubt as to whether n-gram based MT evaluation measures such as the aforementioned are suitable for our task. We hence asses the quality of an interpretation using automatic evaluation measures from the tasks of MT, paraphrasing, and summarization (Section 2), and compare these measures to human-based measures.\nAutomatic Measures We use BLEU and ROUGE as measures of n-gram precision and recall, respectively. We report scores of ROUGE-1, ROUGE-2 and ROUGE-L (recall based on unigrams, bigrams and longest common subsequence between candidate and reference, respectively). In order to asses the n-gram novelty of interpretations (i.e, difference from the source), we report PINC and PINC\u2217sigmoid(BLEU) (see Section 2).\nHuman judgments We employed an additional group of five Fiverr workers and asked them to score each generated interpretations with two scores on a 1-7 scale, 7 being the best. The scores are: adequacy: the degree to which the interpretation captures the meaning of the original tweet; and fluency: how readable the interpretation is. In addition, reasoning that a high quality interpretation is one that captures the true intent of the sarcastic utterance by using words suitable to its sentiment, we ask the workers to assign the interpretation with a binary score indicating whether the sentiment presented in the interpretation agrees with the sentiment of the original sarcastic tweet.5\nThe human measures enjoy high agreement levels between the human judges. The averaged root mean squared error calculated on the test set across all pairs of judges and across the various algorithms we experiment with are: 1.44 for fluency and 1.15 for adequacy. For sentiment scores the averaged agreement at the same setup is 93.2%."}, {"heading": "5 Sarcasm Interpretations as MT", "text": "As our task is about the generation of one English sentence given another, a natural starting point is treating it as monolingual MT. We hence begin with utilizing two widely used MT systems, representing two different approaches: Phrase Based\n5For example, we consider \u201dBest day ever #sarcasm\u201d and its interpretation \u201dWorst day ever\u201d to agree on the sentiment, despite the use of opposite sentiment words.\nMT vs. Neural MT. We then analyze the performance of these two systems, and based on our conclusions we design our SIGN model. Phrase Based MT We employ Moses6, using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy. We use phrases of up to 8 words to build our phrase table, and do not filter sentences according to length since tweets contain at most 140 characters. We employ the KenLM algorithm (Heafield, 2011) for language modeling, and train it on the non-sarcastic tweet interpretations (the target side of the parallel corpus).\nNeural Machine Translation We use GroundHog, a publicly available implementation of an RNN encoder-decoder, with LSTM hidden states.7 Our encoder and decoder contain 250 hidden units each. We use the minibatch stochastic gradient descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model, where each SGD update is computed using a minibatch of 16 utterances. Following Sutskever et al. (2014), we use beam search for test time decoding. Henceforth we refer to this system as RNN. Performance Analysis We divide our corpus into training, development and test sets of sizes 2400, 300 and 300 respectively. We train Moses and the RNN on the training set and tune their parameters on the development set. Table 3 presents development data results, as these are preliminary experiments that aim to asses the compatibility of MT algorithms to our task.\nMoses scores much higher in terms of BLEU and ROUGE, meaning that compared to the RNN its interpretations capture more n-grams appearing in the human references while maintaining high precision. The RNN outscores Moses in terms of PINC and PINC\u2217sigmoid(BLEU), meaning that its interpretations are more novel, in terms of ngrams. This alone might not be a negative trait; However, according to human judgments Moses\n6http://www.statmt.org/moses 7https://github.com/lisa-groundhog/\nGroundHog\nperforms better in terms of fluency, adequacy and sentiment, and so the novelty of the RNN\u2019s interpretations does not necessarily contribute to their quality, and even possibly reduces it.\nTable 2 illustrates several examples of the interpretations generated by both Moses and the RNN. While the interpretations generated by the RNN are readable, they generally do not maintain the meaning of the original tweet. We believe that this is the result of the neural network overfitting the training set, despite regularization and dropout layers, probably due to the relatively small training set size. In light of these results when we experiment with the SIGN algorithm (Section 7), we employ Moses as its MT component.\nThe final example of Table 2 is representative of cases where both Moses and the RNN fail to capture the sarcastic sense of the tweet, incorrectly interpreting it or leaving it unchanged. In order to deal with such cases, we wish to utilize a property typical of sarcastic language. Sarcasm is mostly used to convey a certain emotion by using strong sentiment words that express the exact opposite of their literal meaning. Hence, many sarcastic utterances can be correctly interpreted by keeping most of their words, replacing only sentiment words with expressions of the opposite sentiment. For example, the sarcasm in the utterance \u201dYou\u2019re the best. #sarcasm\u201d is hidden in best, a word of a strong positive sentiment. If we transform this word into a word of the opposite sentiment, such as worst, then we get a non-sarcastic utterance with the correct sentiment.\nWe next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator), an algorithm which capitalizes on sentiment words in order to produce accurate interpretations."}, {"heading": "6 The Sarcasm SIGN Algorithm", "text": "SIGN (Figure 1) targets sentiment words in sarcastic utterances. First, it clusters sentiment words ac-\ncording to semantic relatedness. Then, each sentiment word is replaced with its cluster 8 and the transformed data is fed into an MT system (Moses in this work), at both its training and test phases. Consequently, at test time the MT system outputs non-sarcastic utterances with clusters replacing sentiment words. Finally, SIGN performs a declustering process on these MT outputs, replacing sentiment clusters with suitable words.\nIn order to detect the sentiment of words, we turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990). Using SentiWordNet\u2019s positivity and negativity scores, we collect from our training data a set of distinctly positive words (\u223c 70) and a set of distinctly negative words (\u223c 160).9 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)10 and cluster each set using the k-means algorithm with L2 distance. We aim to have ten words on average in each cluster, and so the positive set is clustered into 7 clusters, and the negative set into 16 clusters. Table 4 presents examples from our clusters.\nUpon receiving a sarcastic tweet, at both training and test, SIGN searches it for sentiment words\n8This means that we replace a word with cluster-j where j is the number of the cluster to which the word belongs.\n9The scores are in the [0,1] range. We set the threshold of 0.6 for both distinctly positive and distinctly negative words.\n10https://levyomer.wordpress.com/2014/ 04/25/dependency-based-word-embeddings/. We choose these embeddings since they are believed to better capture the relations between a word and its context, having been trained on dependency-parsed sentences.\naccording to the positive and negative sets. If such a word is found, it is replaced with its cluster. For example, given the sentence \u201dHow I love Mondays. #sarcasm\u201d, love will be recognized as a positive sentiment word, and the sarcastic tweet will become: \u201dHow I cluster-i Mondays. #sarcasm\u201d where i is the cluster number of the word love.\nDuring training, this process is also applied to the non-sarcastic references. And so, if one such reference is \u201dI dislike Mondays.\u201d, then dislike will be identified and the reference will become \u201dI cluster-j Mondays.\u201d, where j is the cluster number of the word dislike. Moses is then trained on these new representations of the corpus, using the exact same setup as before. This training process produces a mapping between positive and negative clusters, and outputs sarcastic interpretations with clustered sentiment words (e.g, \u201dI cluster-j Mondays.\u201d). At test time, after Moses generates an utterance containing clusters, a de-clustering process takes place: the clusters are replaced with the appropriate sentiment words.\nWe experiment with several de-clustering approaches: (1) SIGN-centroid: the chosen sentiment word will be the one closest to the centroid of cluster j. For example in the tweet \u201dI cluster-j Mondays.\u201d, the sentiment word closest to the centroid of cluster j will be chosen; (2) SIGNcontext: the cluster is replaced with its word that has the highest average Pointwise Mutual Information (PMI) with the words in a symmetric context window of size 3 around the cluster\u2019s location in the output. For example, for \u201dI cluster-j Mondays.\u201d, the sentiment word from cluster j which has the highest average PMI with the words in {\u2019I\u2019,\u2019Mondays\u2019} will be chosen. The PMI values are computed on the training data; and (3) SIGNOracle: an upper bound where a person manually chooses the most suitable word from the cluster.\nWe expect this process to improve the quality of sarcasm interpretations in two aspects. First, as mentioned earlier, sarcastic tweets often differ\nfrom their non sarcastic interpretations in a small number of sentiment words (sometimes even in a single word). SIGN should help highlight the sentiment words most in need of interpretation. Second, under the pre-processing SIGN performs to the input examples of Moses, the latter is inclined to learn a mapping from positive to negative clusters, and vice versa. This is likely to encourage the Moses output to generate outputs of the same sentiment as the original sarcastic tweet, but with honest sentiment words. For example, if the sarcastic tweet expresses a negative sentiment with strong positive words, the non-sarcastic interpretation will express this negative sentiment with negative words, thus stripping away the sarcasm."}, {"heading": "7 Experiments and Results", "text": "We experiment with SIGN and the Moses and RNN baselines at the same setup of section 5. We report test set results for automatic and human measures, in Tables 5 and 6 respectively. As in the development data experiments (Table 3), the RNN presents critically low adequacy scores of 2.11 across the entire test set and of 1.89 in cases where the interpretation and the tweet differ. This,\nalong with its low fluency scores (5.74 and 5.43 respectively) and its very low BLEU and ROUGE scores make us deem this model immature for our task and dataset, hence we exclude it from this section\u2019s tables and do not discuss it further.\nIn terms of automatic evaluation (Table 5), SIGN and Moses do not perform significantly different. When it comes to human evaluation (Table 6) however, SIGN-context presents substantial gains. While for fluency Moses and SIGN-context perform similarly, SIGN-context performs much better in terms of adequacy and the percentage of tweets with the correct sentiment. The differences are substantial as well as statistically significant: adequacy of 3.61 for SIGN-context compared to 2.55 of Moses, and correct sentiment for 46.2% of the SIGN interpretations, compared to only 25.7% of the Moses interpretations.\nTable 6 further provides an initial explanation to the improvement of SIGN over Moses: Moses tends to keep interpretations identical to the original sarcastic tweet, altering them in only 42.3% of the cases, 11 while SIGN-context\u2019s interpretations differ from the original sarcastic tweet in 68.5% of the cases, which comes closer to the 73.8% in the gold standard human interpretations. If for each of the algorithms we only regard to interpretations that differ from the original sarcastic tweet, the differences between the models are less substantial. Nonetheless, SIGN-context still presents improvement by correctly changing sentiment in 67.5% of the cases compared to 60.8% for Moses.\nBoth tables consistently show that the contextbased selection strategy of SIGN outperforms the centroid alternative. This makes sense as, being context-ignorant, SIGN-centroid might produce non-fluent or inadequate interpretations for a given context. For example, the tweet \u201dAlso gotta move a piano as well. joy #sarcasm\u201d is changed to \u201dAlso gotta move a piano as well. bummer\u201d by SIGN-context, while SIGN-centroid changes it to the less appropriate \u201dAlso gotta move a piano as well. boring\u201d. Nonetheless, even this naive de-clustering approach substantially improves adequacy and sentiment accuracy over Moses.\nFinally, comparison to SIGN-oracle reveals that the context selection strategy is not far from human performance with respect to both automatic and human evaluation measures. Still, some gain can be achieved, especially for the human mea-\n11We elaborate on this in section 8.\nsures on tweets that were changed at interpretation. This indicates that SIGN can improve mostly through a better clustering of sentiment words, rather than through a better selection strategy."}, {"heading": "8 Discussion and Future Work", "text": "Automatic vs. Human Measures The performance gap between Moses and SIGN may stem from the difference in their optimization criteria. Moses aims to optimize the BLEU score and given the overall lexical similarity between the original tweets and their interpretations, it therefore tends to keep them identical. SIGN, in contrast, targets sentiment words and changes them frequently. Consequently, we do not observe substantial differences between the algorithms in the automatic measures that are mostly based on ngram differences between the source and the interpretation. Likewise, the human fluency measure that accounts for the readability of the interpretation is not seriously affected by the translation process. When it comes to the human adequacy and sentiment measures, which account for the understanding of the tweet\u2019s meaning, SIGN reveals its power and demonstrates much better performance compared to Moses.\nTo further understand the relationship between the automatic and the human based measures we computed the Pearson correlations for each pair of (automatic, human) measures. We observe that all correlation values are low (up to 0.12 for fluency, 0.13-0.18 for sentiment and 0.19-0.24 for adequacy). Moreover, for fluency the correlation values are insignificant (using a correlation significance t-test with p = 0.05). We believe this indicates that these automatic measures do not provide appropriate evaluation for our task. Designing automatic measures is hence left for future research.\nSarcasm Interpretation as Sentiment Based Monolingual MT: Strengths and Weaknesses The SIGN models\u2019 strength is revealed when interpreting sarcastic tweets with strong sentiment words, transforming expressions such as \u201dAudits are a blast to do #sarcasm\u201d and \u201dBeing stuck in an airport is fun #sarcasm\u201d into \u201dAudits are a bummer to do\u201d and \u201dBeing stuck in an airport is boring\u201d, respectively. Even when there are no words of strong sentiment, the MT component of SIGN still performs well, interpreting tweets such as \u201dthe Cavs aren\u2019t getting any calls, this is new #sarcasm\u201d into \u201dthe Cavs aren\u2019t getting any\ncalls, as usuall\u201d. The SIGN models perform well even in cases where there are several sentiment words but not all of them require change. For example, for the sarcastic tweet \u201dConstantly being irritated, anxious and depressed is a great feeling! #sarcasm\u201d, SIGN-context produces the adequate interpretation: \u201dConstantly being irritated, anxious and depressed is a terrible feeling\u201d.\nFuture research directions rise from cases in which the SIGN models left the tweet unchanged. One prominent set of examples consists of tweets that require world knowledge for correct interpretation. Consider the tweet \u201dCan you imagine if Lebron had help? #sarcasm\u201d. The model requires knowledge of who Lebron is and what kind of help he needs in order to fully understand and interpret the sarcasm. In practice the SIGN models leave this tweet untouched.\nAnother set of examples consists of tweets that lack an explicit sentiment word, for example, the tweet \u201dClear example they made of Sharapova then, ey? #sarcasm\u201d. While for a human reader it is apparent that the author means a clear example was not made of Sharapova, the lack of strong sentiment words results in all SIGN models leaving this tweet uninterpreted.\nFinally, tweets that present sentiment in phrases or slang words are particularly challenging for our approach which relies on the identification and clustering of sentiment words. Consider, for example, the following two cases: (a) the sarcastic tweet \u201dCan\u2019t wait until tomorrow #sarcasm\u201d, where the positive sentiment is expressed in the phrase can\u2019t wait; and (b) the sarcastic tweet \u201danother shooting? yeah we totally need to make guns easier for people to get #sarcasm\u201d, where the word totally receives a strong sentiment despite its normal use in language. While we believe that identifying the role of can\u2019t wait and of totally in the sentiment of the above tweets can be a key to properly interpreting them, our approach that relies on a sentiment word lexicon is challenged by such cases.\nSummary We presented a first attempt to approach the problem of sarcasm interpretation. Our major contributions are:\n\u2022 Construction of a dataset, first of its kind, that consists of 3000 tweets each augmented with five non-sarcastic interpretations generated by human experts.\n\u2022 Discussion of the proper evaluation in our task. We proposed a battery of human measures and compared their performance to the accepted measures in related fields such as machine translation.\n\u2022 An algorithmic approach: sentiment based monolingual machine translation. We demonstrated the strength of our approach and pointed on cases that are currently beyond its reach.\nSeveral challenges are still to be addressed in future research so that sarcasm interpretation can be performed in a fully automatic manner. These include the design of appropriate automatic evaluation measures as well as improving the algorithmic approach so that it can take world knowledge into account and deal with cases where the sentiment of the input tweet is not expressed with a clear sentiment words.\nWe are releasing our dataset with its sarcasm interpretation guidelines, the code of the SIGN algorithms, and the output of the various algorithms considered in this paper (https://github. com/Lotemp/SarcasmSIGN). We hope this new resource will help researchers make further progress on this new task."}], "references": [{"title": "Contextualized sarcasm detection on twitter", "author": ["David Bamman", "Noah A Smith."], "venue": "Ninth International AAAI Conference on Web and Social Media. http://dblp.unitrier.de/rec/bib/conf/icwsm/BammanS15.", "citeRegEx": "Bamman and Smith.,? 2015", "shortCiteRegEx": "Bamman and Smith.", "year": 2015}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["Colin Bannard", "Chris Callison-Burch."], "venue": "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 597\u2013604.", "citeRegEx": "Bannard and Callison.Burch.,? 2005", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Proceedings of the 5th workshop on computational approaches to subjectivity, sentiment and social media analysis", "author": ["Francesco Barbieri", "Horacio Saggion", "Francesco Ronzano."], "venue": "pages 50\u201358. https://doi.org/10.3115/v1/W14-2609.", "citeRegEx": "Barbieri et al\\.,? 2014", "shortCiteRegEx": "Barbieri et al\\.", "year": 2014}, {"title": "Collecting highly parallel data for paraphrase evaluation", "author": ["David L Chen", "William B Dolan."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Associa-", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proc. of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Dmitry Davidov", "Oren Tsur", "Ari Rappoport."], "venue": "Proceedings of the fourteenth conference on computational natural language learning. Association", "citeRegEx": "Davidov et al\\.,? 2010", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "In Proceedings of the Sixth Workshop on Statistical Machine Translation", "citeRegEx": "Denkowski and Lavie.,? \\Q2011\\E", "shortCiteRegEx": "Denkowski and Lavie.", "year": 2011}, {"title": "Automatic evaluation of machine translation quality using ngram co-occurrence statistics", "author": ["George Doddington."], "venue": "Proceedings of the second international conference on Human Language Technology Research. Mor-", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Sentiwordnet: A publicly available lexical resource for opinion mining", "author": ["Andrea Esuli", "Fabrizio Sebastiani."], "venue": "Proceedings of LREC. http://aclweb.org/anthology/L06-1225.", "citeRegEx": "Esuli and Sebastiani.,? 2006", "shortCiteRegEx": "Esuli and Sebastiani.", "year": 2006}, {"title": "Irony in language and thought: A cognitive science reader", "author": ["Raymond W Gibbs", "Herbert L Colston."], "venue": "Psychology Press.", "citeRegEx": "Gibbs and Colston.,? 2007", "shortCiteRegEx": "Gibbs and Colston.", "year": 2007}, {"title": "Some statistical issues in the comparison of speech recognition algorithms", "author": ["Laurence Gillick", "Stephen J Cox."], "venue": "Proc. of ICASSP. IEEE. https://doi.org/10.1109/ICASSP.1989.266481.", "citeRegEx": "Gillick and Cox.,? 1989", "shortCiteRegEx": "Gillick and Cox.", "year": 1989}, {"title": "Identifying sarcasm in twitter: a closer look", "author": ["Roberto Gonz\u00e1lez-Ib\u00e1nez", "Smaranda Muresan", "Nina Wacholder."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.,? 2011", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1nez et al\\.", "year": 2011}, {"title": "Kenlm: Faster and smaller language model queries", "author": ["Kenneth Heafield."], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. Association for Computational Linguistics, pages 187\u2013 197. www.aclweb.org/anthology/W11-2123.", "citeRegEx": "Heafield.,? 2011", "shortCiteRegEx": "Heafield.", "year": 2011}, {"title": "Two hearts in three-quarter time: How to waltz the social media/viral marketing dance", "author": ["Andreas M Kaplan", "Michael Haenlein."], "venue": "Business Horizons 54(3):253\u2013263. https://doi.org/10.1016/j.bushor.2011.01.006.", "citeRegEx": "Kaplan and Haenlein.,? 2011", "shortCiteRegEx": "Kaplan and Haenlein.", "year": 2011}, {"title": "Theory of mind abilities and", "author": ["Yael Kimhi"], "venue": null, "citeRegEx": "Kimhi.,? \\Q2014\\E", "shortCiteRegEx": "Kimhi.", "year": 2014}, {"title": "Moses: Open source", "author": ["Richard Zens"], "venue": null, "citeRegEx": "Zens,? \\Q2007\\E", "shortCiteRegEx": "Zens", "year": 2007}, {"title": "Challenges in developing opinion mining tools for social media", "author": ["Diana Maynard", "Kalina Bontcheva", "Dominic Rout."], "venue": "Proceedings of the@ NLP can u tag# usergeneratedcontent (LREC-12 workshop) pages 15\u201322.", "citeRegEx": "Maynard et al\\.,? 2012", "shortCiteRegEx": "Maynard et al\\.", "year": 2012}, {"title": "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis", "author": ["Diana Maynard", "Mark A Greenwood."], "venue": "LREC. pages 4238\u20134243. http://dblp.unitrier.de/rec/bib/conf/lrec/MaynardG14.", "citeRegEx": "Maynard and Greenwood.,? 2014", "shortCiteRegEx": "Maynard and Greenwood.", "year": 2014}, {"title": "Webster\u2019s ninth new collegiate dictionary", "author": ["Inc Merriam-Webster."], "venue": "Merriam-Webster. https://doi.org/10.1353/dic.1984.0017.", "citeRegEx": "Merriam.Webster.,? 1983", "shortCiteRegEx": "Merriam.Webster.", "year": 1983}, {"title": "Introduction to wordnet: An on-line lexical database", "author": ["George A Miller", "Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "Katherine J Miller."], "venue": "International journal of lexicography 3(4):235\u2013244. https://doi.org/10.1093/ijl/3.4.235.", "citeRegEx": "Miller et al\\.,? 1990", "shortCiteRegEx": "Miller et al\\.", "year": 1990}, {"title": "Irony and the Ironic", "author": ["Douglas Colin Muecke."], "venue": "Methuen.", "citeRegEx": "Muecke.,? 1982", "shortCiteRegEx": "Muecke.", "year": 1982}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational linguistics 29(1):19\u201351. http://aclweb.org/anthology/J03-1002.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Opinion mining and sentiment analysis", "author": ["Bo Pang", "Lillian Lee."], "venue": "Foundations and trends in information retrieval 2(1-2):1\u2013135. http://dblp.unitrier.de/rec/bib/journals/ftir/PangL07.", "citeRegEx": "Pang and Lee.,? 2008", "shortCiteRegEx": "Pang and Lee.", "year": 2008}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics. Associa-", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "The mind behind the message: Advancing theory-of-mind scales for typically developing children, and those with deafness, autism, or asperger syndrome", "author": ["Candida C Peterson", "Henry M Wellman", "Virginia Slaughter."], "venue": "Child development", "citeRegEx": "Peterson et al\\.,? 2012", "shortCiteRegEx": "Peterson et al\\.", "year": 2012}, {"title": "Opine: Extracting product features and opinions from reviews", "author": ["Ana-Maria Popescu", "Bao Nguyen", "Oren Etzioni."], "venue": "Proceedings of HLT/EMNLP interactive demonstrations. Association for Computational Linguistics, pages 32\u201333.", "citeRegEx": "Popescu et al\\.,? 2005", "shortCiteRegEx": "Popescu et al\\.", "year": 2005}, {"title": "Proceedings of the 2004 conference on empirical methods in natural language processing", "author": ["Chris Quirk", "Chris Brockett", "William B Dolan."], "venue": "pages 142\u2013149. http://aclweb.org/anthology/W04-3219.", "citeRegEx": "Quirk et al\\.,? 2004", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Sarcasm as contrast between a positive sentiment and negative situation", "author": ["Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang."], "venue": "Proceedings of the Conference on Empirical Methods", "citeRegEx": "Riloff et al\\.,? 2013", "shortCiteRegEx": "Riloff et al\\.", "year": 2013}, {"title": "The neuroanatomical basis of understanding sarcasm and its relationship to social cognition", "author": ["SG Shamay-Tsoory", "Rachel Tomer", "Judith Aharon-Peretz."], "venue": "Neuropsychology 19(3):288. https://doi.org/10.1037/0894-4105.19.3.288.", "citeRegEx": "Shamay.Tsoory et al\\.,? 2005", "shortCiteRegEx": "Shamay.Tsoory et al\\.", "year": 2005}, {"title": "The Meaning of Irony", "author": ["FJ Stingfellow."], "venue": "New York: State University of NY.", "citeRegEx": "Stingfellow.,? 1994", "shortCiteRegEx": "Stingfellow.", "year": 1994}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112. http://papers.nips.cc/paper/5346-sequence-to-", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Icwsm-a great catchy name: Semisupervised recognition of sarcastic sentences in online product reviews", "author": ["Oren Tsur", "Dmitry Davidov", "Ari Rappoport."], "venue": "ICWSM. http://dblp.unitrier.de/rec/bib/conf/icwsm/TsurDR10.", "citeRegEx": "Tsur et al\\.,? 2010", "shortCiteRegEx": "Tsur et al\\.", "year": 2010}, {"title": " irony or# sarcasma quantitative and qualitative study based on twitter", "author": ["Po-Ya Angela Wang."], "venue": "Proceedings of the 27th Pacific Asia Conference on", "citeRegEx": "Wang.,? 2013", "shortCiteRegEx": "Wang.", "year": 2013}, {"title": "Learning subjective language", "author": ["Janyce Wiebe", "Theresa Wilson", "Rebecca Bruce", "Matthew Bell", "Melanie Martin."], "venue": "Computational linguistics 30(3):277\u2013308. http://aclweb.org/anthology/J043002.", "citeRegEx": "Wiebe et al\\.,? 2004", "shortCiteRegEx": "Wiebe et al\\.", "year": 2004}, {"title": "Paraphrase generation as monolingual translation: Data and evaluation", "author": ["Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer."], "venue": "Proceedings of the 6th International Natural Language Generation Conference. Association for Computa-", "citeRegEx": "Wubben et al\\.,? 2010", "shortCiteRegEx": "Wubben et al\\.", "year": 2010}, {"title": "Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)", "author": ["Wei Xu", "Chris Callison-Burch", "William B Dolan."], "venue": "Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) https://doi.org/10.18653/v1/s15-", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701 http://dblp2.uni-trier.de/rec/bib/journals/corr/abs1212-5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "It is defined in the MerriamWebster dictionary (Merriam-Webster, 1983) as the use of words that mean the opposite of what", "startOffset": 47, "endOffset": 70}, {"referenceID": 24, "context": "For example, studies show that children with deafness, autism or Asperger\u2019s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014).", "startOffset": 152, "endOffset": 188}, {"referenceID": 14, "context": "For example, studies show that children with deafness, autism or Asperger\u2019s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014).", "startOffset": 152, "endOffset": 188}, {"referenceID": 20, "context": "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al.", "startOffset": 70, "endOffset": 128}, {"referenceID": 29, "context": "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al.", "startOffset": 70, "endOffset": 128}, {"referenceID": 9, "context": "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al.", "startOffset": 70, "endOffset": 128}, {"referenceID": 28, "context": "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature.", "startOffset": 148, "endOffset": 199}, {"referenceID": 24, "context": "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature.", "startOffset": 148, "endOffset": 199}, {"referenceID": 16, "context": "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the", "startOffset": 132, "endOffset": 217}, {"referenceID": 13, "context": "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the", "startOffset": 132, "endOffset": 217}, {"referenceID": 0, "context": "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the", "startOffset": 132, "endOffset": 217}, {"referenceID": 32, "context": "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the", "startOffset": 132, "endOffset": 217}, {"referenceID": 22, "context": "challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).", "startOffset": 79, "endOffset": 128}, {"referenceID": 17, "context": "challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).", "startOffset": 79, "endOffset": 128}, {"referenceID": 5, "context": "(2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset.", "startOffset": 11, "endOffset": 33}, {"referenceID": 5, "context": "(2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset. Gonz\u00e1lez-Ib\u00e1nez et al. (2011) used lexical and pragmatic features, e.", "startOffset": 11, "endOffset": 248}, {"referenceID": 2, "context": "Barbieri et al. (2014) avoided using word patterns and instead employed features such as the length and sentiment of the tweet, and the use of rare words.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al.", "startOffset": 26, "endOffset": 44}, {"referenceID": 6, "context": "Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al.", "startOffset": 53, "endOffset": 80}, {"referenceID": 4, "context": ", 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task.", "startOffset": 58, "endOffset": 76}, {"referenceID": 34, "context": "(2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005).", "startOffset": 132, "endOffset": 207}, {"referenceID": 1, "context": "(2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005).", "startOffset": 132, "endOffset": 207}, {"referenceID": 25, "context": "Quirk et al. (2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": ", 2010; Bannard and Callison-Burch, 2005). Xu et al. (2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT.", "startOffset": 8, "endOffset": 60}, {"referenceID": 3, "context": "We also utilize PINC (Chen and Dolan, 2011), a measure which rewards paraphrases for being different from their source, by introducing new n-grams.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "human judgments is achieved by the product of PINC with a sigmoid function of BLEU (Chen and Dolan, 2011).", "startOffset": 83, "endOffset": 105}, {"referenceID": 29, "context": "Following Tsur et al. (2010), Gonz\u00e1lez-Ib\u00e1nez et al.", "startOffset": 10, "endOffset": 29}, {"referenceID": 10, "context": "(2010), Gonz\u00e1lez-Ib\u00e1nez et al. (2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link.", "startOffset": 8, "endOffset": 38}, {"referenceID": 0, "context": "(2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link.", "startOffset": 11, "endOffset": 35}, {"referenceID": 21, "context": "Phrase Based MT We employ Moses6, using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy.", "startOffset": 76, "endOffset": 95}, {"referenceID": 12, "context": "We employ the KenLM algorithm (Heafield, 2011) for language modeling, and train it on the non-sarcastic tweet interpretations (the target side of the parallel corpus).", "startOffset": 30, "endOffset": 46}, {"referenceID": 36, "context": "(Zeiler, 2012) to train each model, where each SGD update is computed using a minibatch of 16 utterances.", "startOffset": 0, "endOffset": 14}, {"referenceID": 30, "context": "Following Sutskever et al. (2014), we use beam search for test time decoding.", "startOffset": 10, "endOffset": 34}, {"referenceID": 8, "context": "In order to detect the sentiment of words, we turn to SentiWordNet (Esuli and Sebastiani, 2006),", "startOffset": 67, "endOffset": 95}, {"referenceID": 19, "context": "a lexical resource based on WordNet (Miller et al., 1990).", "startOffset": 36, "endOffset": 57}, {"referenceID": 19, "context": "a lexical resource based on WordNet (Miller et al., 1990). Using SentiWordNet\u2019s positivity and negativity scores, we collect from our training data a set of distinctly positive words (\u223c 70) and a set of distinctly negative words (\u223c 160).9 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)10 and cluster each set using the k-means algorithm with L2 distance.", "startOffset": 37, "endOffset": 332}, {"referenceID": 10, "context": "Statistical significance is tested with the paired t-test for fluency and adequacy, and with the McNemar paired test for labeling disagreements (Gillick and Cox, 1989) for % correct sentiment, in both cases with p < 0.", "startOffset": 144, "endOffset": 167}], "year": 2017, "abstractText": "Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, \u201dSarcasm is the giant chasm between what I say, and the person who doesn\u2019t get it.\u201d. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN\u2019s interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.1", "creator": "LaTeX with hyperref package"}}}