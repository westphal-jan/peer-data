{"id": "1704.08384", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks", "abstract": "transformation tables answering problems infer answers either toward relevant knowledge base or from raw insights. while constraint base ( kb ) could conduct good ways answering compositional questions, their specification extends somewhat robust around neighboring context matrix resource kb. au contraire, global framework contains millions of facts that are absent in the kb, not in an unstructured domain. { \\ it universal semantics } means support reasoning on the assumption of other semantic elements and unstructured text by aligning them in specified common search space. in this paper we extend universal schema targeting natural noun question answering, employing \\ emph { sensor networks } how attend to other large influx of facts in particular combination _ text spaces kb. 2d models can enable trained applying an now - to - end fashion on question - answer analytics. with support on \\ toe ^ - bin - cube - norm question answering dataset mathematical algorithm exploiting universal item _ question answering is significantly suited possessing either a kb or context alone. this knowledge clearly offers underlying current swing - that - the - art kernel 8. 5 $ f _ 1 $ points. \\ dot { code and syntax unknown in \\ url {", "histories": [["v1", "Thu, 27 Apr 2017 00:03:02 GMT  (157kb,D)", "http://arxiv.org/abs/1704.08384v1", "ACL 2017 (short)"]], "COMMENTS": "ACL 2017 (short)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rajarshi das", "manzil zaheer", "siva reddy", "andrew mccallum"], "accepted": true, "id": "1704.08384"}, "pdf": {"name": "1704.08384.pdf", "metadata": {"source": "CRF", "title": "Question Answering on Knowledge Bases and Text using Universal Schema and Memory Networks", "authors": ["Rajarshi Das", "Manzil Zaheer", "Siva Reddy", "Andrew McCallum"], "emails": ["mccallum}@cs.umass.edu,", "manzilz@cs.cmu.edu", "siva.reddy@ed.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Question Answering (QA) has been a longstanding goal of natural language processing. Two main paradigms evolved in solving this problem: 1) answering questions on a knowledge base; and 2) answering questions using text.\nKnowledge bases (KB) contains facts expressed in a fixed schema, facilitating compositional reasoning. These attracted research ever since the early days of computer science, e.g., BASEBALL (Green Jr et al., 1961). This problem has matured\n1Code and data available in https://rajarshd.github. io/TextKBQA\ninto learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), to recent scaling of methods to work on very large KBs like Freebase using question and answer pairs (Berant et al., 2013). However, a major drawback of this paradigm is that KBs are highly incomplete (Dong et al., 2014). It is also an open question whether KB relational structure is expressive enough to represent world knowledge (Stanovsky et al., 2014; Gardner and Krishnamurthy, 2017)\nThe paradigm of exploiting text for questions started in the early 1990s (Kupiec, 1993). With the advent of web, access to text resources became abundant and cheap. Initiatives like TREC QA competitions helped popularizing this paradigm (Voorhees et al., 1999). With the recent advances in deep learning and availability of large public datasets, there has been an explosion of research in a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016). Still, text representation is unstructured and does not allow the compositional reasoning which structured KB supports.\nAn important but under-explored QA paradigm is where KB and text are exploited together (Ferrucci et al., 2010). Such combination is attractive because text contains millions of facts not present in KB, and a KB\u2019s generative capacity represents infinite number of facts that are never seen in text. However QA inference on this combination is challenging due to the structural non-uniformity of KB and text. Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB. But the rich and ambiguous nature of language allows a fact to be expressed in many different forms which these models fail to capture. ar X\niv :1\n70 4.\n08 38\n4v 1\n[ cs\n.C L\n] 2\n7 A\npr 2\n01 7\nUniversal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts and text into a uniform structured representation, allowing interleaved propagation of information. Figure 1 shows a universal schema matrix which has pairs of entities as rows, and Freebase and textual relations in columns. Although universal schema has been extensively used for relation extraction, this paper shows its applicability to QA. Consider the question USA has elected blank , our first african-american president with its answer Barack Obama. While Freebase has a predicate for representing presidents of USA, it does not have one for \u2018african-american\u2019 presidents. Whereas in text, we find many sentences describing the presidency of Barack Obama and his ethnicity at the same time. Exploiting both KB and text makes it relatively easy to answer this question than relying on only one of these sources.\nMemory networks (MemNN; Weston et al. 2015) are a class of neural models which have an external memory component for encoding short and long term context. In this work, we define the memory components as observed cells of the universal schema matrix, and train an end-to-end QA model on question-answer pairs.\nThe contributions of the paper are as follows (a) We show that universal schema representation is a better knowledge source for QA than either KB or text alone, (b) On the SPADES dataset (Bisk et al., 2016), containing real world fill-in-the-blank questions, we outperform state-of-the-art semantic parsing baseline, with 8.5 F1 points. (c) Our analysis shows how individual data sources help fill the\nweakness of the other, thereby improving overall performance."}, {"heading": "2 Background", "text": "Problem Definition Given a question q with words w1,w2, . . . ,wn, where these words contain one blank and at least one entity, our goal is to fill in this blank with an answer entity qa using a knowledge base K and text T . Few example question answer pairs are shown in Table 2.\nUniversal Schema Traditionally universal schema is used for relation extraction in the context of knowledge base population. Rows in the schema are formed by entity pairs (e.g. USA, NYC), and columns represent the relation between them. A relation can either be a KB relation, or it could be a pattern of text that exist between these two entities in a large corpus. The embeddings of entities and relation types are learned by low-rank matrix factorization techniques. Riedel et al. (2013) treat textual patterns as static symbols, whereas recent work by Verga et al. (2016) replaces them with distributed representation of sentences obtained by a RNN. Using distributed representation allows reasoning on sentences that are similar in meaning but different on the surface form. We too use this variant to encode our textual relations.\nMemory Networks MemNNs are neural attention models with external and differentiable memory. MemNNs decouple the memory component from the network thereby allowing it store external information. Previously, these have been successfully applied to question answering on KB where\nthe memory is filled with distributed representation of KB triples (Bordes et al., 2015), or for reading comprehension (Sukhbaatar et al., 2015; Hill et al., 2016), where the memory consists of distributed representation of sentences in the comprehension. Recently, key-value MemNN are introduced (Miller et al., 2016) where each memory slot consists of a key and value. The attention weight is computed only by comparing the question with the key memory, whereas the value is used to compute the contextual representation to predict the answer. We use this variant of MemNN for our model. Miller et al. (2016), in their experiments, store either KB triples or sentences as memories but they do not explicitly model multiple memories containing distinct data sources like we do."}, {"heading": "3 Model", "text": "Our model is a MemNN with universal schema as its memory. Figure 1 shows the model architecture.\nMemory: Our memory M comprise of both KB and textual triples from universal schema. Each memory cell is in the form of key-value pair. Let (s, r,o) \u2208 K represent a KB triple. We represent this fact with distributed key k \u2208 R2d formed by concatenating the embeddings s \u2208 Rd and r \u2208 Rd of subject entity s and relation r respectively. The embedding o \u2208 Rd of object entity o is treated as its value v.\nLet (s, [w1, . . . ,arg1, . . . ,arg2,wn], o) \u2208 T represent a textual fact, where arg1 and arg2 correspond to the positions of the entities \u2018s\u2019 and \u2018o\u2019. We represent the key as the sequence formed by replacing arg1 with \u2018s\u2019 and arg2 with a special \u2018 blank \u2019 token, i.e., k = [w1, . . . ,s, . . . , blank , wn] and value as just the entity \u2018o\u2019. We convert k to a distributed representation using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005), where k \u2208 R2d is formed by concatenating the last states of forward and backward LSTM, i.e., k = [\u2212\u2212\u2212\u2212\u2192 LSTM(k); \u2190\u2212\u2212\u2212\u2212 LSTM(k) ] .\nThe value v is the embedding of the object entity o. Projecting both KB and textual facts to R2d offers a unified view of the knowledge to reason upon. In Figure 1, each cell in the matrix represents a memory containing the distributed representation of its key and value.\nQuestion Encoder: A bidirectional LSTM is also used to encode the input question q to a distributed representation q \u2208 R2d similar to the key encoding step above.\nAttention over cells: We compute attention weight of a memory cell by taking the dot product of its key k with a contextual vector c which encodes most important context in the current iteration. In the first iteration, the contextual vector is the question itself. We only consider the memory cells that contain at least one entity in the question. For example, for the input question in Figure 1, we only consider memory cells containing USA. Using the attention weights and values of memory cells, we compute the context vector ct for the next iteration t as follows:\nct = Wt ( ct\u22121 +Wp \u2211\n(k,v)\u2208M (ct\u22121 \u00b7k)v ) where c0 is initialized with question embedding q, Wp is a projection matrix, and Wt represents the weight matrix which considers the context in previous hop and the values in the current iteration based on their importance (attention weight). This multi-iterative context selection allows multi-hop reasoning without explicitly requiring a symbolic query representation.\nAnswer Entity Selection: The final contextual vector ct is used to select the answer entity qa (among all 1.8M entities in the dataset) which has the highest inner product with it."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Evaluation Dataset", "text": "We use Freebase (Bollacker et al., 2008) as our KB, and ClueWeb (Gabrilovich et al., 2013) as our text source to build universal schema. For evaluation, literature offers two options: 1) datasets for text-based question answering tasks such as answer sentence selection and reading comprehension; and 2) datasets for KB question answering.\nAlthough the text-based question answering datasets are large in size, e.g., SQuAD (Rajpurkar et al., 2016) has over 100k questions, answers to these are often not entities but rather sentences which are not the focus of our work. Moreover these texts may not contain Freebase entities at all, making these skewed heavily towards text. Coming to the alternative option, WebQuestions (Berant et al., 2013) is widely used for QA on Freebase. This dataset is curated such that all questions can be answered on Freebase alone. But since our goal is to explore the impact of universal schema, testing on a dataset completely answerable on a KB is not ideal. WikiMovies dataset (Miller et al., 2016) also has similar properties. Gardner and Krishnamurthy\n(2017) created a dataset with motivations similar to ours, however this is not publicly released during the submission time.\nInstead, we use SPADES (Bisk et al., 2016) as our evaluation data which contains fill-in-the-blank cloze-styled questions created from ClueWeb. This dataset is ideal to test our hypothesis for following reasons: 1) it is large with 93K sentences and 1.8M entities; and 2) since these are collected from Web, most sentences are natural. A limitation of this dataset is that it contains only the sentences that have entities connected by at least one relation in Freebase, making it skewed towards Freebase as we will see (\u00a7 4.4). We use the standard train, dev and test splits for our experiments. For text part of universal schema, we use the sentences present in the training set."}, {"heading": "4.2 Models", "text": "We evaluate the following models to measure the impact of different knowledge sources for QA.\nONLYKB: In this model, MemNN memory contains only the facts from KB. For each KB triple (e1,r,e2), we have two memory slots, one for (e1,r,e2) and the other for its inverse (e2,ri,e1).\nONLYTEXT: SPADES contains sentences with blanks. We replace the blank tokens with the answer entities to create textual facts from the training set. Using every pair of entities, we create a memory cell similar to as in universal schema.\nENSEMBLE This is an ensemble of the above two models. We use a linear model that combines the scores from, and use an ensemble to combine the evidences from individual models.\nUNISCHEMA This is our main model with universal schema as its memory, i.e., it contains memory slots corresponding to both KB and textual facts."}, {"heading": "4.3 Implementation Details", "text": "The dimensions of word, entity and relation embeddings, and LSTM states were set to d =50. The word and entity embeddings were initialized with\nword2vec (Mikolov et al., 2013) trained on 7.5 million ClueWeb sentences containing entities in Freebase subset of SPADES. The network weights were initialized using Xavier initialization (Glorot and Bengio, 2010). We considered up to a maximum of 5k KB facts and 2.5k textual facts for a question. We used Adam (Kingma and Ba, 2015) with the default hyperparameters (learning rate=1e3, \u03b21=0.9, \u03b22=0.999, \u03b5=1e-8) for optimization. To overcome exploding gradients, we restricted the magnitude of the `2 norm of the gradient to 5. The batch size during training was set to 32.\nTo train the UNISCHEMA model, we initialized the parameters from a trained ONLYKB model. We found that this is crucial in making the UNISCHEMA to work. Another caveat is the need to employ a trick similar to batch normalization (Ioffe and Szegedy, 2015). For each minibatch, we normalize the mean and variance of the textual facts and then scale and shift to match the mean and variance of the KB memory facts. Empirically, this stabilized the training and gave a boost in the final performance."}, {"heading": "4.4 Results and Discussions", "text": "Table 1 shows the main results on SPADES. UNISCHEMA outperforms all our models validating our hypothesis that exploiting universal schema for QA is better than using either KB or text alone. Despite SPADES creation process being friendly to Freebase, exploiting text still provides a significant improvement. Table 2 shows some of the questions which UNISCHEMA answered but ONLYKB failed. These can be broadly classified into (a) relations that are not expressed in Freebase (e.g., african-american presidents in sentence 1); (b) intentional facts since curated databases only represent concrete facts rather than intentions (e.g., threating to leave in sentence 2); (c) comparative predicates like first, second, largest, smallest\n(e.g., sentences 3 and 4); and (d) providing additional type constraints (e.g., in sentence 5, Freebase does not have a special relation for father. It can be expressed using the relation parent along with the type constraint that the answer is of gender male).\nWe have also anlalyzed the nature of UNISCHEMA attention. In 58.7% of the cases the attention tends to prefer KB facts over text. This is as expected since KBs facts are concrete and accurate than text. In 34.8% of cases, the memory prefers to attend text even if the fact is already present in the KB. For the rest (6.5%), the memory distributes attention weight evenly, indicating for some questions, part of the evidence comes from text and part of it from KB. Table 3 gives a more detailed quantitative analysis of the three models in comparison with each other.\nTo see how reliable is UNISCHEMA, we gradually increased the coverage of KB by allowing only a fixed number of randomly chosen KB facts for each entity. As Figure 2 shows, when the KB coverage is less than 16 facts per entity, UNISCHEMA outperforms ONLYKB by a wide-margin indicating UNISCHEMA is robust even in resource-scarce scenario, whereas ONLYKB is very sensitive to the coverage. UNISCHEMA also outperforms ENSEMBLE showing joint modeling is superior to ensemble on the individual models. We also achieve the state-of-the-art with 8.5 F1 points difference. Bisk et al. use graph matching techniques to convert natural language to Freebase queries whereas even without an explicit query representation, we outperform them."}, {"heading": "5 Related Work", "text": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015).\nLimited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text. Our models are trained on a weaker supervision signal without requiring the annotation of the logical forms.\nA few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu et al., 2016a). Our work differs from them in two ways: 1) we do not need an explicit database query to retrieve the answers (Neelakantan et al., 2015a; Andreas et al., 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al., 2007; Carlson et al., 2010)."}, {"heading": "6 Conclusions", "text": "In this work, we showed universal schema is a promising knowledge source for QA than using KB or text alone. Our results conclude though KB is preferred over text when the KB contains the fact of interest, a large portion of queries still attend to text indicating the amalgam of both text and KB is\nsuperior than KB alone."}, {"heading": "Acknowledgments", "text": "We sincerely thank Luke Vilnis for helpful insights. This work was supported in part by the Center for Intelligent Information Retrieval and in part by DARPA under agreement number FA8750-132-0020. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "Learning to Compose Neural Networks for Question Answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "NAACL.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "Open Information Extraction from the Web", "author": ["Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni."], "venue": "IJCAI.", "citeRegEx": "Banko et al\\.,? 2007", "shortCiteRegEx": "Banko et al\\.", "year": 2007}, {"title": "Semantic Parsing on Freebase from Question-Answer Pairs", "author": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang."], "venue": "EMNLP.", "citeRegEx": "Berant et al\\.,? 2013", "shortCiteRegEx": "Berant et al\\.", "year": 2013}, {"title": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing", "author": ["Yonatan Bisk", "Siva Reddy", "John Blitzer", "Julia Hockenmaier", "Mark Steedman."], "venue": "EMNLP.", "citeRegEx": "Bisk et al\\.,? 2016", "shortCiteRegEx": "Bisk et al\\.", "year": 2016}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "ICDM.", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston."], "venue": "CoRR .", "citeRegEx": "Bordes et al\\.,? 2015", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Razvan C. Bunescu", "Raymond J. Mooney."], "venue": "ACL.", "citeRegEx": "Bunescu and Mooney.,? 2007", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2007}, {"title": "Toward an Architecture for Neverending Language Learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Jr. Estevam R. Hruschka", "Tom M. Mitchell."], "venue": "AAAI.", "citeRegEx": "Carlson et al\\.,? 2010", "shortCiteRegEx": "Carlson et al\\.", "year": 2010}, {"title": "Hierarchical question answering for long documents", "author": ["Eunsol Choi", "Daniel Hewlett", "Alexandre Lacoste", "Illia Polosukhin", "Jakob Uszkoreit", "Jonathan Berant."], "venue": "arXiv preprint arXiv:1611.01839 .", "citeRegEx": "Choi et al\\.,? 2016", "shortCiteRegEx": "Choi et al\\.", "year": 2016}, {"title": "Scalable Semantic Parsing with Partial Ontologies", "author": ["Eunsol Choi", "Tom Kwiatkowski", "Luke Zettlemoyer."], "venue": "ACL.", "citeRegEx": "Choi et al\\.,? 2015", "shortCiteRegEx": "Choi et al\\.", "year": 2015}, {"title": "Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang."], "venue": "New York, NY, USA, KDD \u201914.", "citeRegEx": "Dong et al\\.,? 2014", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "KDD. ACM, pages 1156\u20131165.", "citeRegEx": "Fader et al\\.,? 2014", "shortCiteRegEx": "Fader et al\\.", "year": 2014}, {"title": "Facc1: Freebase annotation of clueweb corpora", "author": ["Evgeniy Gabrilovich", "Michael Ringgaard", "Amarnag Subramanya."], "venue": "(http://lemurproject. org/clueweb09/.", "citeRegEx": "Gabrilovich et al\\.,? 2013", "shortCiteRegEx": "Gabrilovich et al\\.", "year": 2013}, {"title": "OpenVocabulary Semantic Parsing with both Distributional Statistics and Formal Knowledge", "author": ["Matt Gardner", "Jayant Krishnamurthy."], "venue": "AAAI.", "citeRegEx": "Gardner and Krishnamurthy.,? 2017", "shortCiteRegEx": "Gardner and Krishnamurthy.", "year": 2017}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "AISTATS.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks .", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Baseball: an automatic question-answerer", "author": ["Bert F Green Jr", "Alice K Wolf", "Carol Chomsky", "Kenneth Laughery."], "venue": "Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference. ACM, pages 219\u2013224.", "citeRegEx": "Jr et al\\.,? 1961", "shortCiteRegEx": "Jr et al\\.", "year": 1961}, {"title": "Traversing knowledge graphs in vector space", "author": ["K. Guu", "J. Miller", "P. Liang."], "venue": "EMNLP.", "citeRegEx": "Guu et al\\.,? 2015", "shortCiteRegEx": "Guu et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston."], "venue": "ICLR .", "citeRegEx": "Hill et al\\.,? 2016", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation .", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "ICML. JMLR Workshop and Conference Proceedings.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Knowledge Graph and Corpus Driven Segmentation and Answer Inference for Telegraphic Entity-seeking Queries", "author": ["Mandar Joshi", "Uma Sawant", "Soumen Chakrabarti."], "venue": "EMNLP.", "citeRegEx": "Joshi et al\\.,? 2014", "shortCiteRegEx": "Joshi et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "ICLR .", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Weakly Supervised Training of Semantic Parsers", "author": ["Jayant Krishnamurthy", "Tom Mitchell."], "venue": "EMNLP.", "citeRegEx": "Krishnamurthy and Mitchell.,? 2012", "shortCiteRegEx": "Krishnamurthy and Mitchell.", "year": 2012}, {"title": "MURAX: A robust linguistic approach for question answering using an on-line encyclopedia", "author": ["Julian Kupiec."], "venue": "SIGIR. ACM.", "citeRegEx": "Kupiec.,? 1993", "shortCiteRegEx": "Kupiec.", "year": 1993}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das."], "venue": "arXiv preprint arXiv:1611.01436 .", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Key-value memory networks for directly reading documents", "author": ["Alexander H. Miller", "Adam Fisch", "Jesse Dodge", "AmirHossein Karimi", "Antoine Bordes", "Jason Weston."], "venue": "EMNLP .", "citeRegEx": "Miller et al\\.,? 2016", "shortCiteRegEx": "Miller et al\\.", "year": 2016}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "ACL.", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1511.04834 .", "citeRegEx": "Neelakantan et al\\.,? 2015a", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum."], "venue": "ACL.", "citeRegEx": "Neelakantan et al\\.,? 2015b", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "CoRR abs/1611.09268.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "EMNLP. Austin, Texas.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Large-scale semantic parsing without questionanswer pairs", "author": ["Siva Reddy", "Mirella Lapata", "Mark Steedman."], "venue": "TACL 2.", "citeRegEx": "Reddy et al\\.,? 2014", "shortCiteRegEx": "Reddy et al\\.", "year": 2014}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "ECML PKDD.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin."], "venue": "NAACL.", "citeRegEx": "Riedel et al\\.,? 2013", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "When a knowledge base is not enough: Question answering over knowledge bases with external text data", "author": ["Denis Savenkov", "Eugene Agichtein."], "venue": "SIGIR. ACM.", "citeRegEx": "Savenkov and Agichtein.,? 2016", "shortCiteRegEx": "Savenkov and Agichtein.", "year": 2016}, {"title": "Query-reduction networks for question answering", "author": ["Minjoon Seo", "Sewon Min", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1606.04582 .", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Proposition Knowledge Graphs", "author": ["Gabriel Stanovsky", "Omer Levy", "Ido Dagan."], "venue": "COLING 2014 .", "citeRegEx": "Stanovsky et al\\.,? 2014", "shortCiteRegEx": "Stanovsky et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "NIPS.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Open domain question answering via semantic enrichment", "author": ["Huan Sun", "Hao Ma", "Wen-tau Yih", "Chen-Tse Tsai", "Jingjing Liu", "Ming-Wei Chang."], "venue": "WWW. ACM.", "citeRegEx": "Sun et al\\.,? 2015", "shortCiteRegEx": "Sun et al\\.", "year": 2015}, {"title": "NewsQA: A Machine Comprehension Dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "CoRR abs/1611.09830.", "citeRegEx": "Trischler et al\\.,? 2016", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Multilingual relation extraction using compositional universal schema", "author": ["Patrick Verga", "David Belanger", "Emma Strubell", "Benjamin Roth", "Andrew McCallum"], "venue": null, "citeRegEx": "Verga et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Verga et al\\.", "year": 2016}, {"title": "The trec-8 question answering track report", "author": ["Ellen M Voorhees"], "venue": "Trec. volume 99, pages 77\u2013", "citeRegEx": "Voorhees,? 1999", "shortCiteRegEx": "Voorhees", "year": 1999}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905 .", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordes."], "venue": "ICLR .", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Dynamic Coattention Networks For Question Answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604 .", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Hybrid Question Answering over Knowledge Base and Free Text", "author": ["Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "COLING.", "citeRegEx": "Xu et al\\.,? 2016a", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Question Answering on Freebase via Relation Extraction and Textual Evidence", "author": ["Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao."], "venue": "ACL.", "citeRegEx": "Xu et al\\.,? 2016b", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Relationship queries on extended knowledge graphs", "author": ["Mohamed Yahya", "Denilson Barbosa", "Klaus Berberich", "Qiuyue Wang", "Gerhard Weikum."], "venue": "Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, pages", "citeRegEx": "Yahya et al\\.,? 2016", "shortCiteRegEx": "Yahya et al\\.", "year": 2016}, {"title": "Collective cross-document relation extraction without labelled data", "author": ["Limin Yao", "Sebastian Riedel", "Andrew McCallum."], "venue": "EMNLP.", "citeRegEx": "Yao et al\\.,? 2010", "shortCiteRegEx": "Yao et al\\.", "year": 2010}, {"title": "Information Extraction over Structured Data: Question Answering with Freebase", "author": ["Xuchen Yao", "Benjamin Van Durme."], "venue": "ACL.", "citeRegEx": "Yao and Durme.,? 2014", "shortCiteRegEx": "Yao and Durme.", "year": 2014}, {"title": "Semantic Parsing via Staged Query Graph Generation: Question Answering with Knowledge Base", "author": ["Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao."], "venue": "ACL.", "citeRegEx": "Yih et al\\.,? 2015", "shortCiteRegEx": "Yih et al\\.", "year": 2015}, {"title": "Learning to parse database queries using inductive logic programming", "author": ["John M Zelle", "Raymond J Mooney."], "venue": "AAAI. Portland, Oregon.", "citeRegEx": "Zelle and Mooney.,? 1996", "shortCiteRegEx": "Zelle and Mooney.", "year": 1996}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao."], "venue": "EMNLP.", "citeRegEx": "Zeng et al\\.,? 2015", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars", "author": ["Luke S. Zettlemoyer", "Michael Collins."], "venue": "UAI. Edinburgh, Scotland.", "citeRegEx": "Zettlemoyer and Collins.,? 2005", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 53, "context": "io/TextKBQA into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), to recent scaling of methods to work on very large KBs like Freebase using question and answer pairs (Berant et al.", "startOffset": 89, "endOffset": 144}, {"referenceID": 55, "context": "io/TextKBQA into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), to recent scaling of methods to work on very large KBs like Freebase using question and answer pairs (Berant et al.", "startOffset": 89, "endOffset": 144}, {"referenceID": 2, "context": "io/TextKBQA into learning semantic parsers from parallel question and logical form pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), to recent scaling of methods to work on very large KBs like Freebase using question and answer pairs (Berant et al., 2013).", "startOffset": 247, "endOffset": 268}, {"referenceID": 10, "context": "However, a major drawback of this paradigm is that KBs are highly incomplete (Dong et al., 2014).", "startOffset": 77, "endOffset": 96}, {"referenceID": 38, "context": "It is also an open question whether KB relational structure is expressive enough to represent world knowledge (Stanovsky et al., 2014; Gardner and Krishnamurthy, 2017)", "startOffset": 110, "endOffset": 167}, {"referenceID": 13, "context": "It is also an open question whether KB relational structure is expressive enough to represent world knowledge (Stanovsky et al., 2014; Gardner and Krishnamurthy, 2017)", "startOffset": 110, "endOffset": 167}, {"referenceID": 24, "context": "The paradigm of exploiting text for questions started in the early 1990s (Kupiec, 1993).", "startOffset": 73, "endOffset": 87}, {"referenceID": 32, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 41, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 31, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 44, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 25, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 46, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 37, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 8, "context": "a very short time (Rajpurkar et al., 2016; Trischler et al., 2016; Nguyen et al., 2016; Wang and Jiang, 2016; Lee et al., 2016; Xiong et al., 2016; Seo et al., 2016; Choi et al., 2016).", "startOffset": 18, "endOffset": 184}, {"referenceID": 6, "context": "Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB.", "startOffset": 28, "endOffset": 132}, {"referenceID": 28, "context": "Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB.", "startOffset": 28, "endOffset": 132}, {"referenceID": 34, "context": "Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB.", "startOffset": 28, "endOffset": 132}, {"referenceID": 50, "context": "Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB.", "startOffset": 28, "endOffset": 132}, {"referenceID": 54, "context": "Distant supervision methods (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Yao et al., 2010; Zeng et al., 2015) address this problem partially by means of aligning text patterns with KB.", "startOffset": 28, "endOffset": 132}, {"referenceID": 35, "context": "Universal schema (Riedel et al., 2013) avoids the alignment problem by jointly embedding KB facts and text into a uniform structured representation, allowing interleaved propagation of information.", "startOffset": 17, "endOffset": 38}, {"referenceID": 45, "context": "Memory networks (MemNN; Weston et al. 2015) are a class of neural models which have an external memory component for encoding short and long term context.", "startOffset": 16, "endOffset": 43}, {"referenceID": 3, "context": "The contributions of the paper are as follows (a) We show that universal schema representation is a better knowledge source for QA than either KB or text alone, (b) On the SPADES dataset (Bisk et al., 2016), containing real world fill-in-the-blank questions, we outperform state-of-the-art semantic parsing baseline, with 8.", "startOffset": 187, "endOffset": 206}, {"referenceID": 34, "context": "Riedel et al. (2013) treat textual patterns as static symbols, whereas recent work by Verga et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 34, "context": "Riedel et al. (2013) treat textual patterns as static symbols, whereas recent work by Verga et al. (2016) replaces them with distributed representation of sentences obtained by a RNN.", "startOffset": 0, "endOffset": 106}, {"referenceID": 5, "context": "the memory is filled with distributed representation of KB triples (Bordes et al., 2015), or for reading comprehension (Sukhbaatar et al.", "startOffset": 67, "endOffset": 88}, {"referenceID": 39, "context": ", 2015), or for reading comprehension (Sukhbaatar et al., 2015; Hill et al., 2016), where the memory consists of distributed representation of sentences in the comprehension.", "startOffset": 38, "endOffset": 82}, {"referenceID": 18, "context": ", 2015), or for reading comprehension (Sukhbaatar et al., 2015; Hill et al., 2016), where the memory consists of distributed representation of sentences in the comprehension.", "startOffset": 38, "endOffset": 82}, {"referenceID": 27, "context": "Recently, key-value MemNN are introduced (Miller et al., 2016) where each memory slot consists of a key and value.", "startOffset": 41, "endOffset": 62}, {"referenceID": 27, "context": "Miller et al. (2016), in their experiments, store either KB triples or sentences as memories but they do not explicitly model multiple memories containing distinct data sources like we do.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "We convert k to a distributed representation using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005), where k \u2208 R2d is formed by concatenating the last states of forward and back-", "startOffset": 72, "endOffset": 136}, {"referenceID": 15, "context": "We convert k to a distributed representation using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005), where k \u2208 R2d is formed by concatenating the last states of forward and back-", "startOffset": 72, "endOffset": 136}, {"referenceID": 4, "context": "We use Freebase (Bollacker et al., 2008) as our KB, and ClueWeb (Gabrilovich et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 12, "context": ", 2008) as our KB, and ClueWeb (Gabrilovich et al., 2013) as our text source to build universal schema.", "startOffset": 31, "endOffset": 57}, {"referenceID": 32, "context": ", SQuAD (Rajpurkar et al., 2016) has over 100k questions, answers to these are often not entities but rather sentences which are not the focus of our work.", "startOffset": 8, "endOffset": 32}, {"referenceID": 2, "context": "Coming to the alternative option, WebQuestions (Berant et al., 2013) is widely used for QA on Freebase.", "startOffset": 47, "endOffset": 68}, {"referenceID": 27, "context": "WikiMovies dataset (Miller et al., 2016) also has similar properties.", "startOffset": 19, "endOffset": 40}, {"referenceID": 3, "context": "Instead, we use SPADES (Bisk et al., 2016) as our evaluation data which contains fill-in-the-blank cloze-styled questions created from ClueWeb.", "startOffset": 23, "endOffset": 42}, {"referenceID": 26, "context": "word2vec (Mikolov et al., 2013) trained on 7.", "startOffset": 9, "endOffset": 31}, {"referenceID": 14, "context": "The network weights were initialized using Xavier initialization (Glorot and Bengio, 2010).", "startOffset": 65, "endOffset": 90}, {"referenceID": 22, "context": "We used Adam (Kingma and Ba, 2015) with the default hyperparameters (learning rate=1e3, \u03b21=0.", "startOffset": 13, "endOffset": 34}, {"referenceID": 20, "context": "Another caveat is the need to employ a trick similar to batch normalization (Ioffe and Szegedy, 2015).", "startOffset": 76, "endOffset": 101}, {"referenceID": 23, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 33, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 21, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 52, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 30, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 17, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 48, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 9, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 36, "context": "A majority of the QA literature that focused on exploiting KB and text either improves the inference on the KB using text based features (Krishnamurthy and Mitchell, 2012; Reddy et al., 2014; Joshi et al., 2014; Yao and Van Durme, 2014; Yih et al., 2015; Neelakantan et al., 2015b; Guu et al., 2015; Xu et al., 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al.", "startOffset": 137, "endOffset": 366}, {"referenceID": 40, "context": ", 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015).", "startOffset": 81, "endOffset": 99}, {"referenceID": 11, "context": "A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu et al., 2016a).", "startOffset": 73, "endOffset": 131}, {"referenceID": 49, "context": "A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu et al., 2016a).", "startOffset": 73, "endOffset": 131}, {"referenceID": 47, "context": "A few QA methods infer on curated databases combined with OpenIE triples (Fader et al., 2014; Yahya et al., 2016; Xu et al., 2016a).", "startOffset": 73, "endOffset": 131}, {"referenceID": 29, "context": "Our work differs from them in two ways: 1) we do not need an explicit database query to retrieve the answers (Neelakantan et al., 2015a; Andreas et al., 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al.", "startOffset": 109, "endOffset": 158}, {"referenceID": 0, "context": "Our work differs from them in two ways: 1) we do not need an explicit database query to retrieve the answers (Neelakantan et al., 2015a; Andreas et al., 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al.", "startOffset": 109, "endOffset": 158}, {"referenceID": 1, "context": ", 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al., 2007; Carlson et al., 2010).", "startOffset": 98, "endOffset": 140}, {"referenceID": 7, "context": ", 2016); and 2) our text-based facts retain complete sentential context unlike the OpenIE triples (Banko et al., 2007; Carlson et al., 2010).", "startOffset": 98, "endOffset": 140}, {"referenceID": 5, "context": ", 2016b; Choi et al., 2015; Savenkov and Agichtein, 2016) or improves the inference on text using KB (Sun et al., 2015). Limited work exists on exploiting text and KB jointly for question answering. Gardner and Krishnamurthy (2017) is the closest to ours who generate a open-vocabulary logical form and rank candidate answers by how likely they occur with this logical form both in Freebase and text.", "startOffset": 9, "endOffset": 232}], "year": 2017, "abstractText": "Existing question answering methods infer answers either from a knowledge base or from raw text. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, web text contains millions of facts that are absent in the KB, however in an unstructured form. Universal schema can support reasoning on the union of both structured KBs and unstructured text by aligning them in a common embedded space. In this paper we extend universal schema to natural language question answering, employing memory networks to attend to the large body of facts in the combination of text and KB. Our models can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on SPADES fill-in-the-blank question answering dataset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the current state-of-the-art by 8.5 F1 points.1", "creator": "LaTeX with hyperref package"}}}