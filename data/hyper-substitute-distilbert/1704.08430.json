{"id": "1704.08430", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "A GRU-Gated Attention Model for Neural Machine Translation", "abstract": "neural machine matching ( nmt ) heavily relies on an attention network so produce its context vector encoding each probe word exactly. in practice, we note that context vectors connecting different target words respond quite similar like one another and participants are equally constantly discriminatively predicting target errors. third reason was this might be that bias vectors developed by the compound attention network integrate using a weighted matrices matching these representations that already used for equilibrium states. with written paper, we note two novel software - modelled inverse model ( rd ) for nmt which enhances approximate degree of discrimination of parallel vectors whereby enabling source representations upon acts sensitive to the partial input generated by the decoder. gatt illustrates a weighted intensity unit ( gru ) for induce two types of outcomes : treating static source annotation capability originally produced mirrors the bidirectional encoder as the index drawn from the concurrent response measurement program as the input faces the gru. the constant - time problem forms a new source annotation vector. in this technique, we can obtain language - adapted source representations which define formally called generalized matrix global ensemble to reveal discriminative source vectors. we finally calculate a variant system regards presenting particular memory vector only the actual input serving the previous decoder function as the history. experiments on nist chinese - originated translation tasks show that both gatt - based models achieve significant improvements over generalized vanilla attentionbased nmt. extensive analyses on attention weights utilizing context functions demonstrate the effectiveness of interpreting processes improving the discrimination power of representations and handling the challenging approach of target - repetition.", "histories": [["v1", "Thu, 27 Apr 2017 04:25:41 GMT  (374kb,D)", "http://arxiv.org/abs/1704.08430v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["biao zhang", "deyi xiong", "jinsong su"], "accepted": false, "id": "1704.08430"}, "pdf": {"name": "1704.08430.pdf", "metadata": {"source": "CRF", "title": "A GRU-Gated Attention Model for Neural Machine Translation", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su"], "emails": ["zb@stu.xmu.edu.cn,", "jssu@xmu.edu.cn", "dyxiong@suda.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016]. Currently, most NMT systems use an encoder to\nread a source sentence into a vector and a decoder to map the vector into the corresponding target sentence. What makes NMT outperform conventional statistical machine translation (SMT) is the attention mechanism [Bahdanau et al., 2014], an information bridge between the encoder and the decoder that produces context vectors by dynamically detecting relevant source words for predicting the next target word.\nIntuitively, different target words would be aligned to different source words so that the generated context vectors differ significantly from one another across different decoding steps. In other words, these context vectors should be discriminative enough for target word prediction otherwise the same target words might be generated repeatedly (a well-known issue of NMT: over-translation, see Section 5.6). However, this is often not true in practice, even when \u201cattended\u201d source words are rather relevant. We observe that (see Section 5.5) the context vectors are very similar to each other, and that the variance in each dimension of these vectors across different decoding steps is very small. These indicate that the vanilla attention mechanism suffers from its inadequacy in distinguishing different translation predictions. The reason behind, we conjecture, lies in the architecture of the attention mechanism which simply calculates a linearly weighted sum of source representations that are invariar X\niv :1\n70 4.\n08 43\n0v 1\n[ cs\n.C L\n] 2\n7 A\npr 2\n01 7\nant across decoding steps. Such invariance in source representations may lead to the undesirable small variance of context vectors.\nIn order to handle this issue, in this paper, we propose a novel GRU-gated attention model (GAtt) for NMT. The key is that we can increase the degree of variance in context vectors by refining source representations according to the partial translation generated by the decoder. The refined source representations are composed of the original source representations and the previous decoder state at each decoding step. We show the overall framework of our model and highlight the difference between GAtt and the vanilla attention in Figure 1. GAtt significantly extends the vanilla attention by inserting a gating layer between the encoder and the vanilla attention network. Specifically, we model this gating layer with a GRU unit [Chung et al., 2014], which takes the original source representations as its history and the corresponding previous decoder state as its current input. In this way, GAtt can produce translation-sensitive source representations so as to improve the variance in context vectors and therefore its discrimination ability in target word prediction.\nAs GRU is able to control the information flow between the history and current input through its reset and update gate, we further propose a variant of GAtt that, instead, regards the previous decoder state as the history while the original source representations as the current inputs. Both models are simple yet efficient in training and decoding.\nWe testify GAtt on Chinese-English translation tasks. Experimental results show that both GAtt-based models significantly outperform the vanilla attention-based NMT. We further analyze the generated attention weights and context vectors, showing that the attention weights are more accurate and the context vectors are more discriminative for target word prediction."}, {"heading": "2 Related Work", "text": "Our work contributes to the development of attention mechanism in NMT. Originally, NMT does not have the attention mechanism and mainly relies on the encoder to summarize all source-side semantic details into a fixed-length vector [Sutskever et al., 2014; Cho et al., 2014]. Bahdanau et al. [2014] find that using a fixed-length vector, however, is not adequate to represent a source sentence and propose the popular attention mechanism, enabling the model to automatically search for parts of a source sentence that are relevant to the next target word. From then on, the attention mechanism has gained extensive concern. Luong et al. [2015a] explore several effective approaches to the attention network, introducing the local and global attention model. Tu et al. [2016] introduce a coverage vector to keep track of the attention history such that the attention network can pay more attention to untranslated source words. Mi et al. [2016] leverage welltrained word alignments to directly supervise the attention weights in NMT. Yang et al. [2016] bring a recurrence along the context vector to help adjust the future attention. Cohn et al. [2016] incorporate several structural bias, such as position bias, markov condition and fertilities, into the attention-based neural translation model. However, all these models mainly\nfocus on how to make the attention weights more accurate. As we mentioned, even with well-designed attention models, context vectors may be lack of discrimination ability for target word prediction.\nAnother closely related work is the interactive attention model [Meng et al., 2016] which treats source representations as a memory and models the interaction between the decoder and this memory during translation via reading and writing operations. To some extent, our model can also be regarded as a memory network, which only includes the reading operation. However, our reading operation differs significantly from that in the interactive attention, where we employ the GRU unit for composition while they merely use the contentbased addressing. Compared with the interactive attention, our GAtt, without the writing operation, is more efficient in both training and decoding.\nThe gate mechanism in our GAtt is built on the GRU unit. GRU usually acts as a recurrent unit that leverages a reset gate and an update gate to control how much information flow from the history state and the current input respectively [Chung et al., 2014]. It is an extension of the vanilla recurrent neural network (RNN) unit with the advantage of alleviating the vanishing and exploding gradient problems during training [Bengio et al., 1994], and also a simplification of the LSTM model [Hochreiter and Schmidhuber, 1997] with the advantage of efficient computation. The idea of using GRU as a gate mechanism, to the best of our knowledge, has never been investigated before.\nAdditionally, our model is also related with the treestructured LSTM [Tai et al., 2015], where LSTM is adapted to compose vary-sized children nodes and current input node in a dependency tree into the current hidden state. GAtt differs significantly from the tree-structured LSTM in that the latter employs the sum operation to deal with the varysized representations, while our model leverages the attention mechanism."}, {"heading": "3 Background", "text": "In this section, we briefly review the vanilla attention-based NMT [Bahdanau et al., 2014]. Unlike conventional SMT, NMT directly maps a source sentence x = {x1, . . . , xn} to its target translation y = {y1, . . . , ym} using an encoderdecoder framework. The encoder reads the source sentence x, and encodes the representation of each word hi by summarizing the information of neighboring words. As shown by the blue color in Figure 1, this is achieved by a bidirectional RNN, specifically the bidirectional GRU model.\nThe decoder is a conditional language model which generates the target sentence word by word using the following conditional probability (see the yellow lines in Figure 1 (a)):\np(yj |x,y<j) = softmax ( g(Eyj\u22121 , sj , cj) ) (1)\nwhere y<j = {y1, \u00b7 \u00b7 \u00b7 , yj\u22121} is a partial translation,Eyj\u22121 \u2208 Rdw is the embedding of previously generated target word yj\u22121, sj \u2208 Rdh is the j-th target-side decoder state and g(\u00b7) is a highly non-linear function. Please refer to [Bahdanau et al., 2014] for more details. What we concern in this paper is cj \u2208 R2dh , which is the translation-sensitive context vector produced by the attention mechanism.\nAttention Mechanism acts as a bridge between the encoder and the decoder, which makes them tightly coupled. The attention network aims at recognizing which source words are relevant to the next target word and giving high attention weights to these words in computing the context vector cj . This is based on the encoded source representations H = {h1, \u00b7 \u00b7 \u00b7 ,hn} and the previous decoder state sj\u22121 (see the purple color in Figure 1 (a)). Formally,\ncj = Att(H, sj\u22121) (2)\nAtt(\u00b7) denotes the whole process. It first computes an attention weight \u03b1ji to measure the degree of relevance of a source word xi for predicting the target word yj via a feed-forward neural network:\n\u03b1ji = exp (eji)\u2211 k exp (ejk))\n(3)\nThe relevance score eji is estimated via an alignment model as in [Bahdanau et al., 2014]: eji = vTa tanh(Wasj\u22121 + Uahi). Intuitively, the higher attention weight \u03b1ji is, the more important word xi is for the next word prediction. Therefore, Att(\u00b7) generates cj by directly weighting the source representations H with their corresponding attention weights {\u03b1ji}ni=1:\ncj = \u2211\ni\n\u03b1jihi (4)\nAlthough this vanilla attention model is very successful, we find that, in practice, the resulted context vectors {cj}mj=1 are very similar to one another. In other words, these context vectors are not discriminative enough. This is undesirable because it makes the decoder (Eq. (1)) hesitate in deciding which target word should be predicted. We attempt to solve this problem in the next section."}, {"heading": "4 GRU-Gated Attention for NMT", "text": "The problem mentioned above reveals some shortcomings of the vanilla attention mechanism. Let\u2019s revisit the generation of cj in Eq. (4). As different target words might be aligned to different source words, the attention weights of source words vary across different decoding steps. However, no matter how the attention weights of source words vary, the source representations H remain the same, i.e. they are decodinginvariant. And this invariance would limit the discrimination power of the generated context vectors.\nAccordingly, we attempt to break up this invariance by refining the source representations before they are input to the vanilla attention network at each decoding step. To this end, we propose the GRU-gated attention (GAtt), which, similar to the vanilla attention, can be formulated into the following form:\ncj = GAtt(H, sj\u22121) (5)\nThe gray color in Figure 1 (b) highlights the major difference between GAtt and the vanilla attention. Specifically, GAtt consists of two layers: a gating layer and an attention layer.\nGating Layer. This layer aims at refining the source representations according to the previous decoder state sj\u22121 so as\nto compute translation-relevant source representations. Formally, Hgj = Gate(H, sj\u22121) (6) The Gate(\u00b7) should be capable of dealing with the complex interactions between the source sentence and the partial translation, and freely controlling the semantic match and information flow between them. Instead of using conventional gating mechanism [Chung et al., 2014], we directly choose the whole GRU unit to perform this task. For a source representation hi, GRU treats it as the history representation and refines it using the current input, i.e. the previous decoder state sj\u22121:\nzji = \u03c3(Wzsj\u22121 + Uzhi + bz)\nrji = \u03c3(Wrsj\u22121 + Urhi + br)\nhji = tanh(W sj\u22121 + U [rji hi] + b) hgji = (1\u2212 zji) hi + zji hji\n(7)\nwhere \u03c3(\u00b7) is the sigmoid function, and denotes the element-wise multiplication. Intuitively, the reset gate rji and update gate zji measure the degree of the semantic match between the source sentence and partial translation. The former determines how much the original source information could be used to combine the partial translation, while the latter defines how much the original source information can be kept around. As a result, hgji becomes translation-sensitive, rather than decoding-invariant, which is desired to strengthen the discrimination power of cj .\nAttention Layer. This layer is the same as the vanilla attention mechanism:\ncj = Att(H g j , sj\u22121) (8)\nThe Att(\u00b7) in Eq. (8) denotes the same procedure as that in Eq. (2). However, instead of paying attention to the original source representations H, this layer relies on the gate-refined source representations Hgj . Notice that H g j is adaptive during decoding, indicated with the subscript j. Ideally, we expect Hgj is decoding-specific enough such that cj can vary significantly across different target words.\nNotice that Gate(\u00b7) is not a multi-stepped RNN. It is simply a composition function, or only one-stepped RNN. Therefore, it is computationally efficient. To train our model, we employ the standard training objective, i.e. maximizing the log-likelihood of the training data, and optimize the model parameters using the standard stochastic gradient algorithm.\nModel Variant We refer to the above model as GAtt, which regards the source representations as the history and the previous decoder state as the current input. Which information should be treated as input or history does not matter, especially for the GRU unit since GRU is able to control the information flow freely. We can also use the previous decoder state as the history and the source representations as the current input. We refer to this model as GAtt-Inv. Formally,\ncj = GAtt-Inv(H, sj\u22121) (9) with,\ncj = Att(H g j \u2032 , sj\u22121) H g j \u2032 = Gate(sj\u22121,H)\nThe major difference lies at the order of the inputs in Gate(\u00b7), since the inputs to GRU are directional. We verify both model variants through the following experiments."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Setup", "text": "We evaluated the effectiveness of our model on ChineseEnglish translation tasks. Our training data consists of 1.25M sentence pairs, with 27.9M Chinese words and 34.5M English words respectively1. We chose the NIST 2005 dataset as the development set to perform model selection, and the NIST 2002, 2003, 2004, 2006 and 2008 datasets as our test sets. There are 878, 919, 1788, 1082 and 1664 sentences in NIST 2002, 2003, 2004, 2005, 2006, 2008 dataset respectively. We evaluated the translation quality using the caseinsensitive BLEU-4 metric [Papineni et al., 2002]2 and TER metric [Snover et al., 2006]3. We performed paired bootstrap sampling [Koehn, 2004] for statistical significance test using the script in Moses4."}, {"heading": "5.2 Baselines", "text": "We compared our proposed model against the following two state-of-the-art SMT and NMT systems:\n\u2022 Moses [Koehn et al., 2007]: an open source state-of-theart phrase-based SMT system. \u2022 RNNSearch [Bahdanau et al., 2014]: a state-of-the-art\nattention-based NMT system using the vanilla attention mechanism. We further feed the information of yj\u22121 to the attention, and implemented the decoder with two GRU layers, following the suggestions in dl4mt5.\n1This data is a combination of LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.\n2https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ generic/multi-bleu.perl\n3http://www.cs.umd.edu/ snover/tercom/ 4https://github.com/moses-smt/mosesdecoder/blob/master/scripts/\nanalysis/bootstrap-hypothesis-difference-significance.pl 5https://github.com/nyu-dl/dl4mt-tutorial/tree/master/session3\nFor Moses, we trained a 4-gram language model on the target portion of training data using the SRILM6 toolkit with modified Kneser-Ney smoothing. The word alignments were obtained with GIZA++ [Och and Ney, 2003] on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d strategy [Koehn et al., 2003]. All other parameters were kept as the default settings.\nFor RNNSearch, we limit the vocabulary of both source and target languages to be the most frequent 30K words, covering approximately 97.7% and 99.3% of the two corpora respectively. The words that do not appear in the vocabulary were mapped to a special token \u201cUNK\u201d. We trained our model with the sentences of length up to 50 words in the training data. Following the settings in [Bahdanau et al., 2014], we set dw = 620, dh = 1000. We initialized all parameters randomly according to a normal distribution (\u00b5 = 0, \u03c3 = 0.01) except the square matrices which are initialized with random orthogonal matrices. We used the Adadelta algorithm [Zeiler, 2012] for optimization, with a batch size of 80 and gradient norm as 5. The model parameters were selected according to the maximum BLEU points on the development set. Additionally, during decoding, we used the beam-search algorithm, and set the beam size to 10.\nFor GAtt, we randomly initialized its parameters as what we do in RNNSearch. All the other settings are the same as RNNSearch. All NMT systems were trained on a GeForce GTX 1080 using the computational framework Theano. In one hour, the RNNSearch system processes about 2769 batches while GAtt processes 1549 batches."}, {"heading": "5.3 Translation Results", "text": "The results are summarized in Table 1. Both GAtt and GAtt-Inv outperform both Moses and RNNSearch. Specially, GAtt yields 35.70 BLEU and 56.06 TER scores on\n6http://www.speech.sri.com/projects/srilm/download.html\naverage, with improvements of 4.59 BLEU and 1.61 TER points over Moses, and 1.66 BLEU and 2.12 TER points over RNNSearch; GAtt-Inv achieves 35.70 BLEU and 55.99 TER scores on average, with gains of 4.59 BLEU and 1.68 TER points over Moses, and 1.66 BLEU and 2.19 TER points over RNNSearch. All improvements are statistically significant.\nIt seems that GAtt-Inv obtains very slightly better performance than GAtt in terms of TER on average. However, these improvements are neither significant nor consistent. In other words, GAtt is as efficient as GAtt-Inv. This is reasonable, since the difference of GAtt and GAtt-Inv lies at the order of inputs to GRU, and GRU is able to control its information flow from each input through its reset and update gate."}, {"heading": "5.4 Effects of Model Ensemble", "text": "We further testify whether the ensemble of our models and RNNSearch can generate better performance against any single system. We ensemble different systems by simply averaging their predicted target word probabilities at each decoding step, as suggested in [Luong et al., 2015b]. We show the results in Table 2. Not surprisingly, all the ensemble systems achieves significant improvements over the best single system. And the ensemble of \u201cRNNSearch+GAtt+GAtt-Inv\u201d produces the best results, 38.64 BLEU and 54.15 TER scores on average. This demonstrates that these neural models are complementary and beneficial to each other."}, {"heading": "5.5 Translation Analysis", "text": "In order to have a deep understanding of how the proposed models work, we dug into the translated sentences of different neural systems. Table 3 shows an example. All the neural models generate very fluent translations. However, RNNSearch only translates the rough meaning of the source sentence, ignoring important sub-phrases \u201c\u91cd\u65b0\u878d\u5165\u793e\u4f1a\u201d and \u201c \u4e34\u65f6\u201d. These missing translations resonate with the\nfinding of Tu et al. [2016]. sIn contrast, GAtt and GAtt-Inv are able to capture these two sub-phrases, generating the key translations \u201cintegration\u201d and \u201cinterim\u201d.\nTo find the underlying reason, we investigated their generated attention weights. Rather than using the generated target sentences, we feed the same reference translations into RNNSearch and GAtt for making a fair comparison7. Figure 2 visualizes the attention weights. Both RNNSearch and GAtt have very intuitive attentions, e.g. \u201crefugees\u201d is aligned to \u201c \u96be\u6c11\u201d, \u201cgovernment\u201d is aligned to \u201c \u653f\u5e9c\u201d. However, compared against those of RNNSearch, the attentions learned by GAtt are more focused and accurate. In other words, the refined source representations in GAtt help the attention mechanism concentrate its weights on translation-related words.\nTo verify this point, we evaluated the quality of word alignments induced from different neural systems in terms of alignment error rate (AER) [Och and Ney, 2003] and the soft version (SAER) of AER, following Tu et al. [Tu et al., 2016].8 Table 4 display the evaluation results of word alignments. We find that both GAtt and GAtt-Inv significantly outperform RNNSearch in terms of both AER and SAER. Specifically, GAtt obtains a gain of 7.91 SAER and 7.3 AER points over\n7We do not analyze GAtt-Inv because it is very similar to GAtt. 8Notice that we used the same dataset and evaluation script as Tu et al. [Tu et al., 2016]. We refer the readers to [Tu et al., 2016] for more details.\nRNNSearch. As we obtain word alignments by connecting target words to source words with the highest alignment probabilities computed according to their attention weights, the consistent improvements of our model over RNNSearch on AER score indicate that our model indeed learns more accurate attentions.\nAnother very important question is whether GAtt enhances the discrimination of the context vectors. We answer this question by visualizing these vectors, as shown in Figure 3. We can observe that the heatmap of RNNSearch is very smooth, which varies very slightly across different decoding steps (the horizontal axis). This means that these context vectors are very similar to one another, thus lacking of discrimination. In contrast, there are obvious variations in GAtt. Statistically, the mean variance of the context vectors across different dimensions in RNNSearch is 0.0057, while it is 0.0365 in GAtt, 6 times larger than that of RNNSearch. Additionally, across different decoding steps, the mean variance is 0.0088 in RNNSearch, while it is 0.0465 in GAtt. All these strongly suggest that our model makes the context vectors more discriminative across different target words."}, {"heading": "5.6 Over-Translation Evaluation", "text": "Over-translation or repeatedly predicting the same target words [Tu et al., 2016] is a challenging problem for NMT. We conjecture that the reason behind the over-translation issue is partially due to the small differences in context vectors learned by the vanilla attention mechanism. As the proposed GAtt can improve the discrimination power of context vectors, we hypothesize that our model can deal better with the over-translation issue than the vanilla attention network. To testify this hypothesis, we introduce a metric called N-Gram Repetition Rate (N-GRR) that calculates the portion of repeated n-grams in a sentence:\nN-GRR = 1\nCR\nC\u2211\nc=1\nR\u2211\nr=1\n|N-gramsc,r| \u2212 |u(N-gramsc,r)| |N-gramsc,r|\n(10) where |N-gramsc,r| denotes the number of total n-grams in the r-th translation of the c-th sentence in the testing corpus and u(N-gramsc,r) the number of n-grams after duplicate ngrams are removed. In our test sets, there are C = 6606 sentences with r = 4 and r = 1 translations for the Reference and NMT systems respectively. If we compare N-GRR scores of machine-generated translation against those of reference translations, we can roughly know how serious the over-translation problem is.\nWe show N-GRR results in Table 5. Compared with reference translations (Reference), RNNSearch yields significant high scores, indicating that RNNSearch generates redundant repeated n-grams in translations, and therefore the over-translation problem in RNNSearch is serious. In contrast, both GAtt and GAtt-Inv achieve considerable improvements over RNNSearch in terms of N-GRR. Especially, we find that GAtt-Inv performs better than GAtt on all n-grams, which is in accordance with the translation results in Table 1. These N-GRR results strongly suggest that the proposed models are able to handle the over-translation issue and that generating more discriminative context vectors makes NMT suffer less from the over-translation issue."}, {"heading": "6 Conclusion", "text": "In this paper, we have presented a novel GRU-gated attention model (GAtt) for NMT. Instead of using decoding-invariant source representations, GAtt produces new source representations that vary across different decoding steps according to the partial translation so as to improve the discrimination of context vectors for translation. This is achieved by a gating layer that regards the source representations and previous decoder state as the history and input to a gated recurrent unit. Experiments on Chinese-English translation tasks demonstrate the effectiveness of our model. In-depth analysis further reveals that our model is able to significantly reduce repeated redundant translations (over-translations).\nIn the future, we would like to apply our model to other sequence learning tasks as our model is easily to be adapted to any other sequence-to-sequence tasks (e.g. document summarization, neural conversion model, speech recognition, .etc). Additionally, except for the GRU unit, we will explore more different end-to-end neural architectures, such as convolutional neural network, LSTM unit as the gate mechanism plays a very important role in our model. Finally, we are interested in adapting our GAtt model as a tree-structured unit to compose different nodes in a dependency tree."}], "references": [{"title": "In Proc", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio. Neural machine translation by jointly learning to align", "translate"], "venue": "of ICLR,", "citeRegEx": "Bahdanau et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "IEEE Transactions on Neural Networks", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult"], "venue": "5:157\u2013166,", "citeRegEx": "Bengio et al.. 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Cho et al", "2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proc. of EMNLP,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR,", "citeRegEx": "Chung et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "CoRR", "author": ["Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari. Incorporating structural alignment biases into an attentional neural translation model"], "venue": "abs/1601.01085,", "citeRegEx": "Cohn et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput., 9:1735\u20131780,", "citeRegEx": "Hochreiter and Schmidhuber. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "pages 1\u201310", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio. On using very large target vocabulary for neural machine translation. In Proc. of ACL-IJCNLP"], "venue": "July", "citeRegEx": "Jean et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "NAACL \u201903", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu. Statistical phrase-based translation. In Proc. of NAACL"], "venue": "pages 48\u201354,", "citeRegEx": "Koehn et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Open source toolkit", "author": ["Koehn et al", "2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "In Proc", "author": ["Philipp Koehn. Statistical significance tests for machine translation evaluation"], "venue": "of EMNLP,", "citeRegEx": "Koehn. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "pages 1412\u20131421", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning. Effective approaches to attention-based neural machine translation. In Proc. of EMNLP"], "venue": "September", "citeRegEx": "Luong et al.. 2015a", "shortCiteRegEx": null, "year": 2015}, {"title": "pages 11\u201319", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba. Addressing the rare word problem in neural machine translation. In Proc. of ACL-IJCNLP"], "venue": "July", "citeRegEx": "Luong et al.. 2015b", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive attention for neural machine", "author": ["Meng et al", "2016] Fandong Meng", "Zhengdong Lu", "Hang Li", "Qun Liu"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "pages 2283\u20132288", "author": ["Haitao Mi", "Zhiguo Wang", "Abe Ittycheriah. Supervised attentions for neural machine translation. In Proc. of EMNLP"], "venue": "Austin, Texas, November", "citeRegEx": "Mi et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "29(1):19\u201351", "author": ["Franz Josef Och", "Hermann Ney. A systematic comparison of various statistical alignment models. Comput. Linguist."], "venue": "March", "citeRegEx": "Och and Ney. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Proc. of ACL, pages 311\u2013318,", "citeRegEx": "Papineni et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "CoRR", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu. Minimum risk training for neural machine translation"], "venue": "abs/1512.02433,", "citeRegEx": "Shen et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In In Proceedings of Association for Machine Translation in the Americas", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul. A study of translation edit rate with targeted human annotation"], "venue": "pages 223\u2013231,", "citeRegEx": "Snover et al.. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "CoRR", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le. Sequence to sequence learning with neural networks"], "venue": "abs/1409.3215,", "citeRegEx": "Sutskever et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "pages 1556\u20131566", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D. Manning. Improved semantic representations from tree-structured long short-term memory networks. In Proc. of ACL"], "venue": "Beijing, China, July", "citeRegEx": "Tai et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "CoRR", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li. Coverage-based neural machine translation"], "venue": "abs/1601.04811,", "citeRegEx": "Tu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "CoRR", "author": ["Xing Wang", "Zhengdong Lu", "Zhaopeng Tu", "Hang Li", "Deyi Xiong", "Min Zhang. Neural machine translation advised by statistical machine translation"], "venue": "abs/1610.05150,", "citeRegEx": "Wang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ArXiv e-prints", "author": ["Z. Yang", "Z. Hu", "Y. Deng", "C. Dyer", "A. Smola. Neural Machine Translation with Recurrent Attention Modeling"], "venue": "July", "citeRegEx": "Yang et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler. 2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 0, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 16, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 6, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 11, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 21, "context": "Neural machine translation (NMT), as a large, single and endto-end trainable neural network, has attracted wide attention in recent years [Sutskever et al., 2014; Bahdanau et al., 2014; Shen et al., 2015; Jean et al., 2015; Luong et al., 2015b; Wang et al., 2016].", "startOffset": 138, "endOffset": 263}, {"referenceID": 0, "context": "What makes NMT outperform conventional statistical machine translation (SMT) is the attention mechanism [Bahdanau et al., 2014], an information bridge between the encoder and the decoder that produces context vectors by dynamically detecting relevant source words for predicting the next target word.", "startOffset": 104, "endOffset": 127}, {"referenceID": 3, "context": "Specifically, we model this gating layer with a GRU unit [Chung et al., 2014], which takes the original source representations as its history and the corresponding previous decoder state as its current input.", "startOffset": 57, "endOffset": 77}, {"referenceID": 18, "context": "Originally, NMT does not have the attention mechanism and mainly relies on the encoder to summarize all source-side semantic details into a fixed-length vector [Sutskever et al., 2014; Cho et al., 2014].", "startOffset": 160, "endOffset": 202}, {"referenceID": 3, "context": "GRU usually acts as a recurrent unit that leverages a reset gate and an update gate to control how much information flow from the history state and the current input respectively [Chung et al., 2014].", "startOffset": 179, "endOffset": 199}, {"referenceID": 1, "context": "It is an extension of the vanilla recurrent neural network (RNN) unit with the advantage of alleviating the vanishing and exploding gradient problems during training [Bengio et al., 1994], and also a simplification of the LSTM model [Hochreiter and Schmidhuber, 1997] with the advantage of efficient computation.", "startOffset": 166, "endOffset": 187}, {"referenceID": 5, "context": ", 1994], and also a simplification of the LSTM model [Hochreiter and Schmidhuber, 1997] with the advantage of efficient computation.", "startOffset": 53, "endOffset": 87}, {"referenceID": 19, "context": "Additionally, our model is also related with the treestructured LSTM [Tai et al., 2015], where LSTM is adapted to compose vary-sized children nodes and current input node in a dependency tree into the current hidden state.", "startOffset": 69, "endOffset": 87}, {"referenceID": 0, "context": "In this section, we briefly review the vanilla attention-based NMT [Bahdanau et al., 2014].", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "Please refer to [Bahdanau et al., 2014] for more details.", "startOffset": 16, "endOffset": 39}, {"referenceID": 0, "context": "The relevance score eji is estimated via an alignment model as in [Bahdanau et al., 2014]: eji = v a tanh(Wasj\u22121 + Uahi).", "startOffset": 66, "endOffset": 89}, {"referenceID": 3, "context": "Instead of using conventional gating mechanism [Chung et al., 2014], we directly choose the whole GRU unit to perform this task.", "startOffset": 47, "endOffset": 67}, {"referenceID": 15, "context": "We evaluated the translation quality using the caseinsensitive BLEU-4 metric [Papineni et al., 2002]2 and TER metric [Snover et al.", "startOffset": 77, "endOffset": 100}, {"referenceID": 17, "context": ", 2002]2 and TER metric [Snover et al., 2006]3.", "startOffset": 24, "endOffset": 45}, {"referenceID": 9, "context": "We performed paired bootstrap sampling [Koehn, 2004] for statistical significance test using the script in Moses4.", "startOffset": 39, "endOffset": 52}, {"referenceID": 0, "context": "\u2022 RNNSearch [Bahdanau et al., 2014]: a state-of-the-art attention-based NMT system using the vanilla attention mechanism.", "startOffset": 12, "endOffset": 35}, {"referenceID": 14, "context": "The word alignments were obtained with GIZA++ [Och and Ney, 2003] on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d strategy [Koehn et al.", "startOffset": 46, "endOffset": 65}, {"referenceID": 7, "context": "The word alignments were obtained with GIZA++ [Och and Ney, 2003] on the training corpora in both directions, using the \u201cgrow-diag-final-and\u201d strategy [Koehn et al., 2003].", "startOffset": 151, "endOffset": 171}, {"referenceID": 0, "context": "Following the settings in [Bahdanau et al., 2014], we set dw = 620, dh = 1000.", "startOffset": 26, "endOffset": 49}, {"referenceID": 23, "context": "We used the Adadelta algorithm [Zeiler, 2012] for optimization, with a batch size of 80 and gradient norm as 5.", "startOffset": 31, "endOffset": 45}, {"referenceID": 11, "context": "We ensemble different systems by simply averaging their predicted target word probabilities at each decoding step, as suggested in [Luong et al., 2015b].", "startOffset": 131, "endOffset": 152}, {"referenceID": 14, "context": "To verify this point, we evaluated the quality of word alignments induced from different neural systems in terms of alignment error rate (AER) [Och and Ney, 2003] and the soft version (SAER) of AER, following Tu et al.", "startOffset": 143, "endOffset": 162}, {"referenceID": 20, "context": "[Tu et al., 2016].", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "[Tu et al., 2016].", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "We refer the readers to [Tu et al., 2016] for more details.", "startOffset": 24, "endOffset": 41}, {"referenceID": 20, "context": "Over-translation or repeatedly predicting the same target words [Tu et al., 2016] is a challenging problem for NMT.", "startOffset": 64, "endOffset": 81}], "year": 2017, "abstractText": "Neural machine translation (NMT) heavily relies on an attention network to produce a context vector for each target word prediction. In practice, we find that context vectors for different target words are quite similar to one another and therefore are insufficient in discriminatively predicting target words. The reason for this might be that context vectors produced by the vanilla attention network are just a weighted sum of source representations that are invariant to decoder states. In this paper, we propose a novel GRU-gated attention model (GAtt) for NMT which enhances the degree of discrimination of context vectors by enabling source representations to be sensitive to the partial translation generated by the decoder. GAtt uses a gated recurrent unit (GRU) to combine two types of information: treating a source annotation vector originally produced by the bidirectional encoder as the history state while the corresponding previous decoder state as the input to the GRU. The GRU-combined information forms a new source annotation vector. In this way, we can obtain translation-sensitive source representations which are then feed into the attention network to generate discriminative context vectors. We further propose a variant that regards a source annotation vector as the current input while the previous decoder state as the history. Experiments on NIST Chinese-English translation tasks show that both GAtt-based models achieve significant improvements over the vanilla attentionbased NMT. Further analyses on attention weights and context vectors demonstrate the effectiveness of GAtt in improving the discrimination power of representations and handling the challenging issue of over-translation.", "creator": "LaTeX with hyperref package"}}}