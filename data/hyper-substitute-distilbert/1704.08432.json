{"id": "1704.08432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction Prediction", "abstract": "chemical - chemical affinity ( cci ) plays a main role thereby managing candidate drugs, toxicity, medical effects, and reproductive functions. cci was created amidst text mining, experiments, approaches, and databases ; though date, no learning - drug treatment project currently survives. requiring chemical analyses, consistent approaches are required. the potent synthetic growth and repeated performance of mixed learning also provided tremendous media attention. however, on lacking sample - from - the - hundreds structural analyses, deep learning continues to be problematic only assuming a classifier. furthermore, its potential includes not only simple definitions, but automatic automated target extraction. in this paper, scientist propose proposed first end - reader - consumer validation matrix for cci, named deepcci. mask features are known from weak semantic global input line entry system ( smiles ), which is a string operator representing a chemical structure, proof of learning from identifying features. to discover hidden representations for all smiles strings, we build vector neural networks ( cnns ). to explore numerical commutative property of homogeneous interaction, analysts apply model dynamics with hidden attribute search models. improves performance of scaling estimates compared atop typical plain symmetric thread and conventional machine learning methods. the proposed deepcci showed this estimated efficiency after every seven different methods used. in 1978, generic fatigue measure was experimentally validated. the automatically extracted features through \u03b4 - square - tail smiles learning alleviates the significant efforts required undertaking manual feature elimination. it presents expected to improve prediction methods, notably drug analyses.", "histories": [["v1", "Thu, 27 Apr 2017 05:03:08 GMT  (1106kb,D)", "http://arxiv.org/abs/1704.08432v1", null], ["v2", "Tue, 27 Jun 2017 04:28:17 GMT  (1275kb,D)", "http://arxiv.org/abs/1704.08432v2", "ACM-BCB 2017"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sunyoung kwon", "sungroh yoon"], "accepted": false, "id": "1704.08432"}, "pdf": {"name": "1704.08432.pdf", "metadata": {"source": "CRF", "title": "DeepCCI: End-to-end Deep Learning for Chemical-Chemical Interaction Prediction", "authors": ["Sunyoung Kwon", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr"], "sections": [{"heading": null, "text": "KEYWORDS chemical-chemical interaction, deep learning, neural networks, convolutional neural networks, commutative property"}, {"heading": "1 INTRODUCTION", "text": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52]. Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].\nCCI is an interaction that occurs between chemicals. e following CCI-related studies have been conducted: a study on compound synergism that predicts the therapeutic e cacy of molecule combinations, based on interactive chemicals [67]; toxicity related studies\n\u2217To whom correspondence should be addressed.\non predicting chemical toxicities and side e ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22]. As evidenced by the range of studies mentioned above, CCI has been used for many purposes. However, no method exists to learn CCI. Instead, CCI is created from text mining, biological experiments, chemical-chemical similarities, and other information from chemical databases. We therefore propose a learning-based CCI prediction method.\ne conventional learning-based chemical (drug) analyses can be divided into two stages, feature engineering and predicting activity, as shown in Figure 1. In drug analyses, features are also called descriptors, ngerprints, or molecular representations.\nar X\niv :1\n70 4.\n08 43\n2v 1\n[ cs\n.L G\n] 2\n7 A\npr 2\n01 7\ne feature engineering stage extracts informative features from chemicals. A signi cant amount of information has been manually designed by domain experts, ranging from simple counts (e.g., the number of atoms, rings, and bonds) to topological properties (e.g., atom pairs, shapes, and connectivities) [60]. Extracted features have been used in subsequent analyses and signi cantly a ected prediction performance. Various works have been proposed to capture the essential properties of chemicals, and more than 5,000 diverse features have been identi ed [60] (e.g., PubChem, MACCS, and CDK ngerprint).\ne predicting activity stage uncovers the relationship between the extracted features and biological activities, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) [58, 61]. To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.\nRecently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46]. In addition, successful results of deep learning in drug elds [11, 40, 47, 59] have been achieved and a racted further a ention. Deep learning approaches to drug analyses can be categorized in the following three cases (see Figure 1):\n(A) Deep classi er: Deep learning is used as a back-end classi er for manually cra ed chemical features. e deep learning based classi er has a racted considerable a ention through the Kaggle \u2018Merck Molecular Activity Challenge\u2019 and its subsequent quantitative structure activity relationship (QSAR) predictions [11, 47]. e Kaggle competition and its subsequent predictions showed signi cant improvements in performance, by using the multi-task technique based on deep learning from the provided features. In addition, prediction of chemical-protein interaction [59] (for drug discovery, network pharmacology, and drug/protein target identi cation) showed boosted results by using deep learning from manually cra ed chemical features. Although deep learning improves prediction performance, it has been used only as a classi er based on the well-cra ed chemical features. (B) Deep generator: Deep learning is used as a front-end generator for drug generation. Drug generation extracts the hidden features from the chemicals and then regenerates the chemicals from the extracted features. Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51]. Both models use the simpli ed molecular input line entry system (SMILES) [58] as input to extract the features and regenerate the SMILES string (or functionally similar SMILES string). Deep learning is only used as a deep generator between chemical strings and extracted features. (C) End-to-end deep learning: Deep learning is used as an end-to-end learner which involves the entire process from feature engineering to predicting biological activity. It\nshould extract hidden features from the original chemical inputs and then predict the biological activity by using the extracted features. However, owing to the considerable performance of using the cra ed features and the di culties of handling the original chemical input, the end-to-end learning framework has not been actively pursued.\nDeep learning in drug (chemical) analyses is mainly used as a classi er; however, the original purpose of deep learning is not only for classi cation, but also for extracting hidden representations [41]. e hidden representations, extracted by deep learning, may have potentially important features that are unknown to domain experts. erefore, we expect that end-to-end deep learning can improve prediction performance. For this reason, we propose an end-to-end deep learning framework, DeepCCI, for CCI prediction."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 Representation of chemical compound", "text": "For end-to-end learning, it is important that the input contains all the latent feature information of the chemical compound. A considerable amount of chemical information exists, such as weight, molecular formula, rings, atoms, SMILES [66], and InChI [19]. Among the many pieces of information, SMILES represents the chemical structure as a line of ASCII characters. As shown in Figure 2, cyclohexane and acetaminophen are expressed as C1CCCC1 and CC(=O)NC1=CC=C(O)C=C1 respectively (C for carbon, O for oxygen, = for double bonds and 1 for the rst ring). Atoms (e.g., carbon, nitrogen, and oxygen), bonds (e.g., single, double, and triple bonds), rings (e.g., open ring, close ring, and ring number), aromaticitiy, and branching can be represented with SMILES. As a one-dimensional representation for the chemical structure, SMILES can be converted into a two- or three-dimensional chemical structure, which means that it contains su cient structure information to derive higher dimensions. SMILES is also used for drug generation [17, 51] and for compound similarity [45].\nIt is generally known that structure is closely related to function [48, 54]. QSAR prediction, which exploits the relationship between the chemical structure and the biological activity, for identifying \u2018druglikeness\u2019 1 and establishing metabolic pathways [28, 56]. As an expression of structure information, SMILES can be a means of clarifying the function of a chemical compound.\nFor these reasons, we used SMILES as a front-end input format for representing a chemical compound.\n1Druglikeness is a qualitative indicator to evaluate a drug-like molecule in terms of its bioavailability."}, {"heading": "2.2 Convolutional neural networks", "text": "e convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].\nDesigned to analyze shi invariant spatial information, CNNs consist of convolution layers and pooling layers. At each convolution layer, learnable feature maps called lters scan the sub-regions across the sequence. Filters enable CNNs to discover locally correlated motifs regardless of their locations through local connectivity and parameter sharing. en, each pooling layer summarizes nonoverlapping sub-regions, and aggregates local features into more global features. We applied CNNs to capture the SMILES string speci city."}, {"heading": "2.3 Commutative property", "text": "In mathematics, a commutative property means that changing the order does not a ect the result (for example, 2+5=5+2).\nAs in indirect symmetric problems, such as distance or similarity between objects, the interaction between A and B should be the same regardless of the order I(A,B) = I(B,A).\nHowever, if the values xA and xB for objects A and B are simply concatenated, and then used for learning, the result can be di erent according to the input order\nI(xA, xB ) = w1xA1 + \u00b7 \u00b7 \u00b7 +wnxA +wn+1xB1 + \u00b7 \u00b7 \u00b7 +w2nxBn , w1xB1 + \u00b7 \u00b7 \u00b7 +wnxB +wn+1xA1 + \u00b7 \u00b7 \u00b7 +w2nxAn = I(xB , xA).\n(1) With this di erence in mind, we strived to build a model that produces the same interaction probability regardless of the input order."}, {"heading": "3 PROPOSED METHODOLOGY", "text": "Figure 3 shows the overview of the proposed DeepCCI. Our method presents end-to-end SMILES learning for CCI. It takes SMILES strings as inputs xA and xB for objects A and B, and then produces an interaction probability y\u0302. e structure is divided into three stages. e rst stage is for preprocessing SMILES inputs, the second stage is for learning latent hidden representations through 1D-CNNs, and the third stage is for interaction learning through mergeing and fully-connected layers. Algorithm 1 shows the overall procedure of DeepCCI."}, {"heading": "3.1 Notations", "text": "e notations used in our paper are outlined below.\n\u2022 I(A,B): interaction between objects A and B \u2022 X: SMILES character set, X = {C, =, (, ), O, F, 1, 2, \u00b7 \u00b7 \u00b7 } \u2022 x: SMILES string, xA and xB for objects A and B \u2022 xi : i-th character of x, x = (x1,x2, \u00b7 \u00b7 \u00b7 ,xn ), xi \u2208 X \u2022 h: hidden representations, hA and hB for objects A and B \u2022 hi : i-th element of h, h = (h1,h2, \u00b7 \u00b7 \u00b7 ,hn ) \u2022 \u03bb: maximum length of SMILES \u2022 F: number of lters \u2022 k : length of lter (kernel size) \u2022 W: learning weights\nC C ( = O ) N C 1 = C C\nconvolution\nmax-pooling\n|F| number of filters\n|F| number of filters\npr ep\nro ce\nss in\ng\none-hot encoding\nSMILES object B SMILES object A\npadding\none-hot encoding\npadding\nxA xB\nsh ar\ned C\nN N\ns\nobject A representations\nobject B representations\noutput\nhBhA\nmerged (hA+hB) representations\nfu lly\nc on\nne ct\ned\n1\nfully-connected\nfully-connected\nm er\nge\nFigure 3: Overview of the proposed DeepCCI end-to-end methodology. Our method extracts hidden representations from SMILES learning and predicts the interaction through the extracted representations, using CNNs and fully connected layers. Inputs are one-hot encoded and zero padded (or truncated) to the maximum length. e preprocessed inputs are fed into shared 1D-CNNs for transforming into hidden representations. e transformed hidden representations for objects A and B are merged and then fed into two fully-connected layers to predict the interaction. e output is the predicted probability of the interaction between objects A and B.\nAlgorithm 1 Pseudo-code of proposed DeepCCI\n1: Input: xA = ( xA1 ,x A 2 , \u00b7 \u00b7 \u00b7 ,xA\u03bb ) and xB = ( xB1 ,x B 2 , \u00b7 \u00b7 \u00b7 ,xB\u03bb ) . xA, xB : the encoded and padded/truncated inputs for objects A and B 2: Output: y\u0302 . y\u0302: predicted probability of interaction 3: repeat 4: h = Pool(\u03c3r (Conv(x))) . shared CNNs described in Section 3.3 5: y\u0302 = \u03c3s (Fnn(\u03c3r (Fnn(hA + hB )))) . interaction prediction described in Section 3.4 . hA, hB : the outputs of shared CNNs from inputs xA,xB 6: L(y, y\u0302) = \u2212y log y\u0302 \u2212 (1 \u2212 y) log(1 \u2212 y\u0302) . learning objective to minimize the loss L(y, y\u0302) 7: W =W \u2212 \u2206W . weights update \u2206W based on averaged L from mini-batch 8: until number of epoch reaches nepoch\n\u2022 y\u0302: predicted probability of interaction \u2022 \u03c3 : activation function, \u03c3s for the sigmoid, \u03c3r for the recti-\ned linear unit \u2022 N : sample size, Nt for total sample, Nb for mini-batch size"}, {"heading": "3.2 Input Preprocessing", "text": "We used input x = (x1,x2, \u00b7 \u00b7 \u00b7 ,xn ) which is represented by 65- character xi \u2208 X where X = {C, =, (, ), O, F, 1, 2, \u00b7 \u00b7 \u00b7 }, |X| = 65 in the SMILES format. Input x should be converted into a numeric expression for learning.\nFor the numerical expression, we employ a widely used one-hot encoding scheme, which sets the corresponding single character to \u20181\u2019 and all the others to \u20180\u2019, thereby converting each character xi into an |X|-dimensional binary vector. An example of cyclohexane (C1CCCCC1) encoded x is as follows:\n{ C, =, ), (, O, N, 1, 2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 }\nx =  x1 x2 x3 x4 x5 x6 x7 x8  =  C 1 C C C C C 1  =  [1, 0, 0, 0, 0, 0, 0, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [1, 0, 0, 0, 0, 0, 0, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0] [0, 0, 0, 0, 0, 0, 1, 0, \u00b7 \u00b7 \u00b7 , 0, 0, 0]  In general, SMILES strings are variable lengths depending on the complexity of the chemical structure. Chemical compounds of a complex structure have relatively longer SMILES strings compared to compounds of a simple structure. To e ectively learn the hidden representations for SMILES, we limit the inputs to a certain maximum length, \u03bb. If a sequence is shorter than the maximum length, zero values are pre-padded; otherwise, the sequence is truncated to the maximum length. A er the padded or truncated processes, all input lengths are xed to the maximum length, \u03bb. According to the maximum length, the prediction performance is a ected (see Figure 6A).\nAccording to the encoding scheme and maximum length, input x is encoded into a \u03bb \u00d7 |X|-dimensional binary matrix. e input collection is represented by a Nt \u00d7 \u03bb \u00d7 |X|-dimensional binary tensor."}, {"heading": "3.3 Modeling of Hidden Representations", "text": "To learn hidden representations h from preprocessed input x of SMILES strings, we used CNNs. e CNN architecture, used in our\nmethod, consists of a convolution layer and a pooling layer.\nh = Pool(\u03c3r (Conv(x))) (2)\ne convolution layer plays a \u2018motif detector\u2019 role across the sequence. e preprocessed input x is convoluted with a set of learnable feature maps called lters (or kernels). Di erent lters may detect motifs of di erent chemical properties in the SMILES string. Scanning the local region across the input string with a parameter-shared lter may enable recognition of the shi -invariant local pa ern.\ne output shape of convoluted input x becomes an F\u00d7(\u03bb\u2212k+1)dimensional matrix, where F represents the number of lters, and k represents the lter length.\nConv(x) = (cf ,i ) \u2208 RF\u00d7(\u03bb\u2212k+1)\ncf ,i = k\u2211 j=1 |X |\u2211 s=1 Wf , j,sxi+j,s (3)\nwhere f and i are 1 \u2264 f \u2264 F and 1 \u2264 i \u2264 (\u03bb \u2212 k + 1), respectively. e parameters F and k of the CNN lter are decided through experiments (see Figure 6D and 6E).\nA er the convolution layer, each convolued element is processed with the recti ed linear unit (ReLU) [42]\n\u03c3r (t) =max(0, t) (4)\nwhich is an activation function of non-saturating for positives and clamping to zero for negatives.\ne pooling layer partitions the recti ed responses with nonoverlapping sub-regions and then outputs the maximum if it is max-pooling. It summarizes within the sub-regions and reduces the the spatial size. e F dimension remains unchanged. e maxpooling length of 6 showed the best performance in our experiments (see Figure 6C)\nTo prevent over ing and to produce generalization e ects, we apply dropout [53] for the last CNN units.\nFor a symmetric problem, such as a tweets comparison, Keras use shared layers, which reuse the weights [9]. Likewise, we use shared CNNs to extract the hidden representations hA and hB for chemicals A and B from inputs xA and xB ."}, {"heading": "3.4 Modeling of Interaction Between Chemicals", "text": "Unlike heterogeneous interactions, such as protein-chemical and miRNA-mRNA interaction, CCI is a homogeneous interaction between two chemicals. Homogeneous interaction between objects A and B should be the same regardless of their input order, I(A,B) = I(B,A), thereby guaranteeing the commutative property described in Section 2.3. However, simple concatenating of the values of objects A and B cannot guarantee the property according to Equation 1.\nTo equally recognize I(A,B) and I(B,A), we strive to give the same weights W to both hidden representations hA and hB for objects A and B\nI(A,B) \u2248WhA +WhB\n= w1h A 1 + \u00b7 \u00b7 \u00b7 +wnhAn +w1hB1 + \u00b7 \u00b7 \u00b7 +wnhBn = w1h B 1 + \u00b7 \u00b7 \u00b7 +wnhBn +w1hA1 + \u00b7 \u00b7 \u00b7 +wnhAn =WhB +WhA \u2248 I(B,A).\n(5)\nrough the weight-sharing, we can guarantee the commutative property. e weight-sharing can be implemented by merging both hidden representations through summing process\nWhA +WhB = w1hA1 + \u00b7 \u00b7 \u00b7 +wnhAn +w1hB1 + \u00b7 \u00b7 \u00b7 +wnhBn = w1(hA1 + hB1 ) + \u00b7 \u00b7 \u00b7 +wn (hAn + hBn ) =W ( hA + hB ) . (6)\ne interaction between objects A and B, is learned with two fully-connected layers (Fcl) from summed representations.\ny\u0302 = \u03c3s (Fcl(\u03c3r (Fcl(hA + hB )))) (7)\nwhere y\u0302 is the predicted probability that an interaction will occur, \u03c3r is the ReLU in Equation (4) and \u03c3s is the sigmoid activation function\n\u03c3s (t) = 1/(1 + exp(\u2212t)) (8)\nwhich make the output probability occur from zero to one. e learning processes proceed to minimize the loss between the true label and the prediction L(y, y\u0302), where y is the true label and y\u0302 is the predicted probability from Equation (7). For the loss function as the learning objective, we use the binary cross entropy\nL(y, y\u0302) = \u2212y log y\u0302 \u2212 (1 \u2212 y) log(1 \u2212 y\u0302). (9)\nWeights are updated in mini-batch units; the loss for the batch unit is averaged by the mini-batch size.\nL(y, y\u0302) = \u2212 1 Nb Nb\u2211 i=1 L(yi , y\u0302i ) (10)\nwhere Nb is the mini-batch size, and i represents the sample index. In the fully-connected layers, we use 128 hidden nodes, batch normalization [23], and a dropout [53] rate of 0.25 for regularization. e entire learning is conducted with optimization algorithm Adam [29] with an epoch of 100 and mini-batch size of 256."}, {"heading": "4 EXPERIMENTAL RESULTS", "text": ""}, {"heading": "4.1 Experimental Setup", "text": "4.1.1 Dataset. In our experiment, we used the CCI data (version 5.0) downloaded from the STITCH [32] database. Each interaction pair had a con dence score ranging from 0 to 999, which was derived from text-mining, experiments, similarity, and database. e higher the con dence score was, the greater the interaction probability was. e positive-900 (9,812 samples), positive-800 (75,908 samples), and positive-700 (171,665 samples) were generated from the samples with con dence scores more than 900, 800, and 700, respectively. e negative-0 (212,741 samples) were generated from the samples with con dence scores of zero.\nAccording to the con dence score, three datasets were prepared: chemical-chemical interaction over 900 (CCI900), 800 (CCI800), and 700 (CCI700) as listed in Table 1. e total number of the positive900, 800, and 700 samples were included in the CCI900, CCI800, and CCI700, respectively. e negatives were randomly selected from negative-zero (212,741) to balance them with the number of positives.\nAll three datasets were asymmetric, which means that only one each of the A-B pair and B-A pair was included; its opposite pair was not included in the dataset. Since methods of supporting the commutative property are advantageous to the symmetric dataset, we prepared the asymmetric dataset for a fair comparison.\n4.1.2 Preparation of Input Representation. e representations of chemical compounds were prepared as two types: the SMILES string, and the other was PubChem ngerprint [65]. e PubChem ngerprint is widely used descriptor. It has showed the best performance for certain tasks [50, 64], although the performance of the molecular descriptor depends on the target problem and classi er.\nStrings represented in SMILES were used as inputs for the proposed end-to-end deep learning. Binary feature vectors represented in the PubChem ngerprint were used as inputs for the other learning methods. We prepared the PubChem ngerprints accordingly because it is di cult to handle the string in conventional machine learning methods, and the end-to-end learning performance had to be compared with a plain deep classi er (see Figure 1A).\ne PubChem ngerprint represents the chemical properties as a 881-dimensional binary vector that has feature information, such as the presence or count of atoms, rings, and nearest neighbor pa erns. For learning, the ngerprints for objects A and B were concatenated as a (2\u00d7881)-dimensional binary vector and then used as inputs. From the chemical IDs listed in the STITCH database,"}, {"heading": "91% of SMILES strings is less than 100 characters in length.", "text": "we retrieved the SMILES string and PubChem ngerprint by using PubChemPy [57].\nA representative example of cyclohexane is as follows: \u2022 PubChem chemical ID: 8078 \u2022 SMILES string: C1CCCCC1 \u2022 Fingerprint: 1100000001100000000000000000000000000 \u00b7 \u00b7 \u00b7\n(the 9-th bit indicates \u2265 C and the 10-th bit indicates \u2265 C) 4.1.3 Experimental Configuration. Our dataset consisted of training, validation, and test samples, as in the general learning experiments [2]. Figure 4 details the dataset. e full dataset was divided into two parts, one for model selection and the other for the independent test.\ne goal of model selection was to nd the optimal hyperparameters. Models with various sets of hyperparameters were trained and then iteratively validated through k-fold cross validation. Optimal hyperparameters, which showed the best average performance in terms of AUC, were selected. In our experiments, we used 5-fold cross validation.\ne remaining test set was used to assess the generalization of the selected model, and compare its performance with other learning methods. Since the parameters were determined through training\nand validation independently of the test set, it could guarantee the fairness of the independent test.\nOur experiments were run on Ubuntu 14.04 (3.5GHz Intel i75930K and GTX Titan X Maxwell (12GB)). For the implementation of deep learning, we used the Keras library package (version 1.1.1) [9].\n4.1.4 Evaluation Metrics. For the performance comparison, we used seven di erent evaluation metrics:\n\u2022 AUC: the area under the ROC curve, which is widely used in case of binary classi cation to show the overall performance of a binary classi er 2 \u2022 ACC: accuracy, which is intuitively used metric to assess the performance 3 \u2022 TPR: true positive rate, sensitivity or recall 4 \u2022 TNR: true negative rate or speci city 5 \u2022 PPV: positive predictive value or precision 6 \u2022 NPV: negative predictive value 7 \u2022 F1: F1 score 8"}, {"heading": "4.2 E ects of Hyperparameter Variation", "text": "Five di erent hyperparameter variation experiments were performed to nd the empirical optimal parameters.\nAs shown in Figure 5, the lengths of SMILES strings vary from 1 to over 200. eir median length is 45 and the average length is 53.5. e 84%, 91%, and 94% of SMILES strings are less than 80, 100, and 120 characters in length, respectively. e proposed method DeepCCI limits the SMILES strings to a certain maximum length, \u03bb. e loss of information and dummy zeros according to the maximum length might a ect the overall performance. Figure 6A depicts the performance of DeepCCI by varying the maximum SMILES string length to 50, 80, 100 and 120. Among them, a maximum length of 100 shows the best performance in terms of AUC. In our experiments, the maximum length of 100 could mean that learning was achieved by reducing the computational burden while covering as much information as possible.\nFigure 6B illustrates the performance as varying the CNN dropout rate from 0.2 to 0.8. Dropout as a regularization technique reduces over ing by dropping out units in neural networks. In our validation experiments, a dropout rate of 0.6, meaning that 60% of units dropped out, showed the best performance and the lowest standard deviation.\ne pooling layer summarized the adjacent features resulting abstract representations, and inputs were down-sampled, resulting in a smaller number of model parameters to learn [2]. A larger pooling length means a greater summary e ect. In our experiments, pooling lengths of 2 and 6 showed similar performances in terms of AUC, as shown in Figure 6C. To achieve be er summarization e ects, we used the max-pooling with a length of 6 as the baseline parameter.\n2 e area under the ROC curve which plots T PR(T ) versus F PR(T ) with varying threshold T , where F PR = 1 \u2212T PR 3ACC = (T P +T N )/(T P +T N + F P + FN ), where TP: true positive, FP: false positive, FN: false negative, TN: true negative 4T PR = T P/(T P + FN ) 5T NR = T N /(T N + F P ) 6PPV = T P/(T P + F P ) 7NPV = T N /(T N + FN ) 8F 1 = 2T P/(2T P + F P + FN )\ne key features of CNNs are a parameter sharing architecture and translation invariance characteristics, which are due to the use of lters (or kernels). e lter length depends on the characteristic of pa erns to discover. e number of lters controls the capacity, depending on the complexity of the task. As shown in Figure 6D and 6E, we used the lter length of 22 and 256 lters as the baseline parameters."}, {"heading": "4.3 Performance Comparison with Other Methods", "text": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16]. SVM, RF, and AdaBoost were tested with default parameters. FFNN, which is a deep neural networks of three fully-connected layers (with 1,024 and 128 hidden units) was tested with ReLU for the hidden activation function and sigmoid for the output activation function, and the dropout rate of 0.25.\nIn drug development, a high FP 9 means that there are more candidate chemicals to be screened, and a high FN 10 means that more chemicals, which should not be omi ed in the screening list,\n9FP: mis-predicted that there is interaction despite of no actual interaction 10FN: mis-predicted that there is no interaction despite of actual interaction\nare omi ed. erefore, both FP and FN should be controlled in low values. We used seven evaluation metrics related to FP and FN.\nTable 2 shows the overall detailed experimental results in terms of seven evaluation metrics, Figure 7 shows the visualized performance in terms of AUC and ACC with bar plots. All the values were averaged through ten repeated experiments and showed to be stable with a standard deviation of less than 0.004 in terms of AUC for all datasets. In all the metrics and datasets, DeepCCI showed the best performance, followed by FFNN and RF. AdaBoost and SVM showed unsatisfactory results, giving the accuracy of under 0.8 for all datasets. In terms of AUC, DeepCCI, FFNN, and RF were greater than 0.9 for all datasets, and DeepCCI outperformed the FFNN and RF by 1.82% (statistically signi cantly di erent, p-value 11 of 2.3E-09) and 5.17% (p-value of 1.7E-11), respectively for ICC900 dataset. e overall performance gradually increased as the dataset size increased.\ne di erences in performance between DeepCCI and FFNN could be interpreted as a result of the di erence between the end-toend SMILES and the cra ed PubChem ngerprint. e end-to-end feature learning compared to the ngerprint showed be er performance. We can surmise that potential key features are automatically extracted, which may be unknown to domain experts.\nFigure 8 shows the experimental validation of the commutative property trained with CCI900 training data (order of A followed by B (A,B)) and tested with the switched order CCI900 training data 11p-value from t-test for ten repeated experimental results in terms of AUC of DeepCCI and the other method\n(B followed by A (B,A)). DeepCCI showed the same training and\ntest accuracy, on account of the e ect of guaranteeing the commutative property. However, the test accuracy of FFNN, SVM, RF, and AdaBoost showed performance degradation by 18%, 17%, 21%, and 10% compared to the training accuracy. ese results mean that the four methods recognized I(A,B) and I(B,A) independently and produced di erent prediction probabilities for them, whereas DeepCCI recognized them as the same."}, {"heading": "5 DISCUSSION", "text": "In this paper, we proposed the rst end-to-end deep learning method for CCI. It showed the best performance compared to a plain deep classi er and conventional machine learning methods.\nDeep learning has shown remarkable success in chemical analyses and a racted considerable research a ention. Despite the success and interest, the cra ed features designed by domain experts continue to be used, even in state-of-the-art deep learning approaches. e objective of deep learning is not merely classi - cation or regression from the cra ed features, but automatically\nextracting meaningful features from the original input and analyzing them. For this reason, we have strived to perform end-to-end deep learning based on SMILES. Our approach showed be er results than using the cra ed PubChem ngerprint. ere are a variety of well-re ned chemical features, and PubChem is one of the competent and widely used descriptors. rough a comparison with the PubChem, we con rmed the possibility of end-to-end SMILES learning, although we did not compare it with all descriptors.\nAutomatic feature learning alleviates the sophisticated e orts required for feature engineering of domain experts and it can enable discovery of meaningful features that are possibly unknown to domain experts. We expect that the proposed end-to-end SMILES learning is applied to other drug (chemical) analyses, such as QSAR prediction, chemical toxicology, and drug target prediction, and it will improve prediction performance.\nIn addition to presenting end-to-end SMILES learning framework, we guaranteed the commutative property in the prediction of CCI. Since CCI is a homogeneous interaction problem, it should produce the same interaction probability for I(A,B) and its converse, I(B,A). To guarantee the commutative property we used model sharing and merging hidden representations techniques and con rmed the property with experimental veri cation. We expect this technique can be applied to other homogeneous symmetric problems, such as protein-protein and gene-gene interactions, as well as other homogeneous comparisons."}, {"heading": "ACKNOWLEDGMENTS", "text": "is work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (Ministry of Science, ICT and Future Planning) [No. 2012M3A9D1054622 and No. 2014M3C9A3063541], in part by a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare [HI14C3405030014] and in part by the Brain Korea 21 Plus Project in 2017."}], "references": [{"title": "Predicting the sequence speci\u0080cities of DNA-and RNA-binding proteins by deep learning", "author": ["Babak Alipanahi", "Andrew Delong", "Ma\u008ahew T Weirauch", "Brendan J Frey"], "venue": "Nature biotechnology 33,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep learning for computational biology", "author": ["Christof Angermueller", "Tanel P\u00e4rnamaa", "Leopold Parts", "Oliver Stegle"], "venue": "Molecular systems biology 12,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Molecular function prediction using neighborhood features", "author": ["Petko Bogdanov", "Ambuj K Singh"], "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB) 7,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Predicting drugs side e\u0082ects based on chemicalchemical interactions and protein-chemical interactions", "author": ["Lei Chen", "Tao Huang", "Jian Zhang", "Ming-Yue Zheng", "Kai-Yan Feng", "Yu-Dong Cai", "Kuo-Chen Chou"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Predicting chemical toxicity e\u0082ects based on chemical-chemical interactions", "author": ["Lei Chen", "Jing Lu", "Jian Zhang", "Kai-Rui Feng", "Ming-Yue Zheng", "Yu-Dong Cai"], "venue": "PLoS One 8,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "\u008ce use of chemical-chemical interaction and chemical structure to identify new candidate chemicals related to lung cancer", "author": ["Lei Chen", "Jing Yang", "Mingyue Zheng", "Xiangyin Kong", "Tao Huang", "Yu-Dong Cai"], "venue": "PloS one 10,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Keras: Deep Learning library for \u008ceano and TensorFlow. h\u008aps://github.com/fchollet/keras", "author": ["Fran\u00e7ois Chollet"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Use of support vector machine in pa\u008aern classi\u0080cation: Application to QSAR studies", "author": ["Ryszard Czermi\u0144ski", "Abdelaziz Yasri", "David Hartsough"], "venue": "Molecular  Informatics 20,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Multi-task neural networks for QSAR predictions", "author": ["George E Dahl", "Navdeep Jaitly", "Ruslan Salakhutdinov"], "venue": "arXiv preprint arXiv:1406.1231", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Deep dynamic models for learning hidden representations of speech features", "author": ["Li Deng", "Roberto Togneri"], "venue": "In Speech and Audio Processing for Coding, Enhancement and Recognition", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Nonlinear SVM approaches to QSPR/QSAR studies and drug design", "author": ["Jean-Pierre Doucet", "Florent Barbault", "Hairong Xia", "Annick Panaye", "Botao Fan"], "venue": "Current Computer-Aided Drug Design 3,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "DNdisorder: predicting protein disorder using boosting and deep networks", "author": ["Jesse Eickholt", "Jianlin Cheng"], "venue": "BMC bioinformatics 14,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E Schapire"], "venue": "In European conference on computational learning theory", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Automatic chemical design using a data-driven continuous representation of molecules", "author": ["Rafael G\u00f3mez-Bombarelli", "David Duvenaud", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Jorge Aguilera-Iparraguirre", "Timothy D Hirzel", "Ryan P Adams", "Al\u00e1n Aspuru- Guzik"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Speech recognition with deep recurrent neural networks. In Acoustics, speech and signal processing", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geo\u0082rey Hinton"], "venue": "(icassp), 2013 ieee international conference on", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "InChI-the worldwide chemical structure identi\u0080er standard", "author": ["Stephen Heller", "Alan McNaught", "Stephen Stein", "Dmitrii Tchekhovskoi", "Igor Pletnev"], "venue": "Journal of cheminformatics", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Predicting biological functions of compounds based on chemical-chemical interactions", "author": ["Le-Le Hu", "Chen Chen", "Tao Huang", "Yu-Dong Cai", "Kuo-Chen Chou"], "venue": "PLoS One 6,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shi\u0089", "author": ["Sergey Io\u0082e", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Lethality and centrality in protein networks", "author": ["Hawoong Jeong", "Sean P Mason", "A-L Barab\u00e1si", "Zoltan N Oltvai"], "venue": "Nature 411,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenste\u008ae", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "A robust peak detection method for RNA structure inference by high-throughput contact mapping", "author": ["Jinkyu Kim", "Seunghak Yu", "Byonghyo Shim", "Hanjoo Kim", "Hyeyoung Min", "Eui- Young Chung", "Rhiju Das", "Sungroh Yoon"], "venue": "Bioinformatics 25,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Convolutional neural networks for sentence classi\u0080cation", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Structure-based strategies for drug design and discovery", "author": ["T Kindt", "S Morse", "E Gotschlich", "K Lyons"], "venue": "Nature", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1991}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Imagenet classi\u0080cation with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geo\u0082rey E Hinton"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "STITCH: interaction networks of chemicals and proteins", "author": ["Michael Kuhn", "Christian von Mering", "Monica Campillos", "Lars Juhl Jensen", "Peer Bork"], "venue": "Nucleic acids research 36,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Face recognition: A convolutional neural-network approach", "author": ["Steve Lawrence", "C Lee Giles", "Ah Chung Tsoi", "Andrew D Back"], "venue": "IEEE transactions on neural networks", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1997}, {"title": "deepTarget: end-to-end learning framework for microRNA target prediction using deep recurrent neural networks", "author": ["Byunghan Lee", "Junghwan Baek", "Seunghyun Park", "Sungroh Yoon"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "DNA-level splice junction prediction using deep recurrent neural networks", "author": ["Byunghan Lee", "Taehoon Lee", "Byunggook Na", "Sungroh Yoon"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "FingerNet: Deep learning-based robust \u0080nger joint detection from radiographs", "author": ["Sungmin Lee", "Minsuk Choi", "Hyun-soo Choi", "Moon Seok Park", "Sungroh Yoon"], "venue": "In Biomedical Circuits and Systems Conference (BioCAS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions", "author": ["Taehoon Lee", "Sungroh Yoon"], "venue": "In ICML", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Deep learning of the tissue-regulated splicing code", "author": ["Michael KK Leung", "Hui Yuan Xiong", "Leo J Lee", "Brendan J Frey"], "venue": "Bioinformatics 30,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "A critical review of recurrent neural networks for sequence learning", "author": ["Zachary C Lipton", "John Berkowitz", "Charles Elkan"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules", "author": ["Alessandro Lusci", "Gianluca Pollastri", "Pierre Baldi"], "venue": "Journal of chemical information and modeling 53,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Deep learning in bioinformatics", "author": ["Seonwoo Min", "Byunghan Lee", "Sungroh Yoon"], "venue": "Brie\u0080ngs in Bioinformatics (2016),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "Recti\u0080ed linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geo\u0082rey E Hinton"], "venue": "In Proceedings of the 27th international conference on machine learning", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Prediction of protein functions based on function\u2013function correlation relations", "author": ["Ka-Lok Ng", "Jin-Shuei Ciou", "Chien-Hung Huang"], "venue": "Computers in Biology and Medicine 40,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["Maxime Oquab", "Leon Bo\u008aou", "Ivan Laptev", "Josef Sivic"], "venue": "In Proceedings of the IEEE conference on computer vision and pa\u0088ern recognition", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "A comparative study of SMILES-based compound similarity functions for drug-target interaction prediction", "author": ["Hakime \u00d6zt\u00fcrk", "Elif Ozkirimli", "Arzucan \u00d6zg\u00fcr"], "venue": "BMC bioinformatics 17,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2016}, {"title": "deep- MiRGene: deep neural network based precursor microRNA prediction", "author": ["Seunghyun Park", "Seonwoo Min", "Hyunsoo Choi", "Sungroh Yoon"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2016}, {"title": "Massively multitask networks for drug discovery", "author": ["Bharath Ramsundar", "Steven Kearnes", "Patrick Riley", "Dale Webster", "David Konerding", "Vijay Pande"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "I-TASSER: a uni\u0080ed platform for automated protein structure and function prediction", "author": ["Ambrish Roy", "Alper Kucukural", "Yang Zhang"], "venue": "Nature protocols 5,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Comparing chemical \u0080ngerprints for ecotoxicology", "author": ["Leander Schietgat", "Bertrand Cuissart", "Alban Lepailleur", "Kurt De Grave", "Bruno Cr\u00e9milleux", "Ronan Bureau", "Jan Ramon"], "venue": "In 6e\u0300mes journe\u0301es de la Socie\u0301te\u0301 Franc\u0327aise de Che\u0301moinformatique", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2013}, {"title": "Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural Networks", "author": ["Marwin HS Segler", "\u008cierry Kogej", "Christian Tyrchan", "Mark P Waller"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2017}, {"title": "Network-based prediction of protein function", "author": ["Roded Sharan", "Igor Ulitsky", "Ron Shamir"], "venue": "Molecular systems biology 3,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2007}, {"title": "Dropout: a simple way to prevent neural networks from  over\u0080\u008aing", "author": ["Nitish Srivastava", "Geo\u0082rey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research 15,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2014}, {"title": "Computer assisted studies of chemical structure and biological function", "author": ["Andrew J Stuper", "William E Br\u00fcgger", "Peter C Jurs"], "venue": null, "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1979}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing", "author": ["Ilya Sutskever", "Oriol Vinyals", "\u008boc V Le"], "venue": null, "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "Random forest: a classi\u0080cation and regression tool for compound classi\u0080cation and QSAR modeling", "author": ["Vladimir Svetnik", "Andy Liaw", "Christopher Tong", "J Christopher Culberson", "Robert P Sheridan", "Bradley P Feuston"], "venue": "Journal of chemical information and computer sciences 43,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2003}, {"title": "PubChemPy: a way to interact with PubChem in Python. h\u008ap://pubchempy.readthedocs.io", "author": ["Ma\u008a Swain"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2014}, {"title": "Predictions of the ADMET properties of candidate drug molecules utilizing di\u0082erent QSAR/QSPR modelling approaches", "author": ["Mahmud Tareq Hassan Khan"], "venue": "Current Drug Metabolism 11,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2010}, {"title": "Boosting compound-protein interaction prediction by deep learning", "author": ["Kai Tian", "Mingyu Shao", "Yang Wang", "Jihong Guan", "Shuigeng Zhou"], "venue": "Methods", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Molecular descriptors for chemoinformatics, volume 41 (2 volume set)", "author": ["Roberto Todeschini", "Viviana Consonni"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2009}, {"title": "ADMET in silico modelling: towards prediction paradise? Nature reviews Drug discovery", "author": ["Han Van De Waterbeemd", "Eric Gi\u0082ord"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2003}, {"title": "\u008ae nature of statistical learning theory. Springer science & business media", "author": ["Vladimir Vapnik"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2013}, {"title": "In silico prediction of serious eye irritation or corrosion potential of chemicals", "author": ["Qin Wang", "Xiao Li", "Hongbin Yang", "Yingchun Cai", "Yinyin Wang", "Zhuang Wang", "Weihua Li", "Yun Tang", "Guixia Liu"], "venue": "RSC Advances 7,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2017}, {"title": "PubChem: a public information system for analyzing bioactivities of small molecules", "author": ["Yanli Wang", "Jewen Xiao", "Tugba O Suzek", "Jian Zhang", "Jiyao Wang", "Stephen H Bryant"], "venue": "Nucleic acids research 37,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2009}, {"title": "SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules", "author": ["David Weininger"], "venue": "In Proc. Edinburgh Math. SOC,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 1970}, {"title": "Systematic chemical-genetic and chemical-chemical interaction datasets for prediction of compound synergism", "author": ["Jan Wildenhain", "Michaela Spitzer", "Sonam Dolma", "Nick Jarvik", "Rachel White", "Marcia Roy", "Emma Gri\u0081ths", "David S Bellows", "Gerard D Wright", "Mike Tyers"], "venue": "Scienti\u0080c Data", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2016}, {"title": "Prediction of regulatory modules comprising microRNAs and target genes", "author": ["Sungroh Yoon", "Giovanni De Micheli"], "venue": "Bioinformatics 21,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2005}, {"title": "Ensemble learning can signi\u0080cantly improve human microRNA target prediction", "author": ["Seunghak Yu", "Juho Kim", "Hyeyoung Min", "Sungroh Yoon"], "venue": "Methods 69,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["Ma\u008ahew D Zeiler", "Rob Fergus"], "venue": "In European conference on computer vision", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2014}, {"title": "Convolutional neural network architectures for predicting DNA\u2013protein binding", "author": ["Haoyang Zeng", "Ma\u008ahew D Edwards", "Ge Liu", "David K Gi\u0082ord"], "venue": "Bioinformatics 32,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52].", "startOffset": 113, "endOffset": 124}, {"referenceID": 39, "context": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52].", "startOffset": 113, "endOffset": 124}, {"referenceID": 47, "context": "Interaction is an action that occurs between two entities that may share similar functions or metabolic pathways [4, 43, 52].", "startOffset": 113, "endOffset": 124}, {"referenceID": 20, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 52, "endOffset": 64}, {"referenceID": 54, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 82, "endOffset": 86}, {"referenceID": 22, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 30, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 62, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 63, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 117, "endOffset": 133}, {"referenceID": 28, "context": "Various interactions exist, such as protein-protein [15, 24, 49], compoundprotein [59], RNA-RNA including miRNA-mRNA [26, 34, 68, 69], and chemical-chemical interactions (CCI) [32].", "startOffset": 176, "endOffset": 180}, {"referenceID": 61, "context": "\u008ce following CCI-related studies have been conducted: a study on compound synergism that predicts the therapeutic e\u0081cacy of molecule combinations, based on interactive chemicals [67]; toxicity related studies", "startOffset": 178, "endOffset": 182}, {"referenceID": 4, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 145, "endOffset": 151}, {"referenceID": 5, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 145, "endOffset": 151}, {"referenceID": 6, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 352, "endOffset": 355}, {"referenceID": 18, "context": "on predicting chemical toxicities and side e\u0082ects based on the assumption that interacting chemicals are more likely to share a similar toxicity [6, 7]; targeted candidate drug discovery studies through which the new closest candidate drug to the commercialized target drug was proposed by connecting the interacting chemicals in the graphical aspects [8]; and a study in which a novel approach was developed for identifying biological functions of chemicals, based on the assumption that interactive chemicals participate in the same metabolic pathways [22].", "startOffset": 554, "endOffset": 558}, {"referenceID": 55, "context": ", atom pairs, shapes, and connectivities) [60].", "startOffset": 42, "endOffset": 46}, {"referenceID": 55, "context": "Various works have been proposed to capture the essential properties of chemicals, and more than 5,000 diverse features have been identi\u0080ed [60] (e.", "startOffset": 140, "endOffset": 144}, {"referenceID": 53, "context": "\u008ce predicting activity stage uncovers the relationship between the extracted features and biological activities, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) [58, 61].", "startOffset": 191, "endOffset": 199}, {"referenceID": 56, "context": "\u008ce predicting activity stage uncovers the relationship between the extracted features and biological activities, such as absorption, distribution, metabolism, excretion, and toxicity (ADMET) [58, 61].", "startOffset": 191, "endOffset": 199}, {"referenceID": 8, "context": "To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.", "startOffset": 170, "endOffset": 178}, {"referenceID": 11, "context": "To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.", "startOffset": 170, "endOffset": 178}, {"referenceID": 51, "context": "To robustly learn relationships between chemicals and activities, and to handle a large amount of data, machine learning approaches (such as support vector machine (SVM) [10, 13] and random forest (RF) [56]) have been successfully applied.", "startOffset": 202, "endOffset": 206}, {"referenceID": 2, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 125, "endOffset": 136}, {"referenceID": 35, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 125, "endOffset": 136}, {"referenceID": 50, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 125, "endOffset": 136}, {"referenceID": 10, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 15, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 27, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 64, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 167, "endOffset": 187}, {"referenceID": 12, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 31, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 32, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 33, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 34, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 42, "context": "Recently, the deep learning method has notably advanced to address challenging problems, such as natural language processing [3, 39, 55], speech and image recognition [12, 18, 20, 31, 70], and computational biology [14, 35\u201338, 46].", "startOffset": 215, "endOffset": 230}, {"referenceID": 9, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 36, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 43, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 54, "context": "In addition, successful results of deep learning in drug \u0080elds [11, 40, 47, 59] have been achieved and a\u008aracted further a\u008aention.", "startOffset": 63, "endOffset": 79}, {"referenceID": 9, "context": "\u008ce deep learning based classi\u0080er has a\u008aracted considerable a\u008aention through the Kaggle \u2018Merck Molecular Activity Challenge\u2019 and its subsequent quantitative structure activity relationship (QSAR) predictions [11, 47].", "startOffset": 207, "endOffset": 215}, {"referenceID": 43, "context": "\u008ce deep learning based classi\u0080er has a\u008aracted considerable a\u008aention through the Kaggle \u2018Merck Molecular Activity Challenge\u2019 and its subsequent quantitative structure activity relationship (QSAR) predictions [11, 47].", "startOffset": 207, "endOffset": 215}, {"referenceID": 54, "context": "In addition, prediction of chemical-protein interaction [59] (for drug discovery, network pharmacology, and drug/protein target identi\u0080cation) showed boosted results by using deep learning from manually cra\u0089ed chemical features.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 58, "endOffset": 62}, {"referenceID": 17, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 135, "endOffset": 143}, {"referenceID": 46, "context": "Generative deep models, such as a variational autoencoder [30] and a recurrent neural network [21], have been used for drug generation [17, 51].", "startOffset": 135, "endOffset": 143}, {"referenceID": 53, "context": "Both models use the simpli\u0080ed molecular input line entry system (SMILES) [58] as input to extract the features and regenerate the SMILES string (or functionally similar SMILES string).", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": "Deep learning in drug (chemical) analyses is mainly used as a classi\u0080er; however, the original purpose of deep learning is not only for classi\u0080cation, but also for extracting hidden representations [41].", "startOffset": 198, "endOffset": 202}, {"referenceID": 60, "context": "A considerable amount of chemical information exists, such as weight, molecular formula, rings, atoms, SMILES [66], and InChI [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "A considerable amount of chemical information exists, such as weight, molecular formula, rings, atoms, SMILES [66], and InChI [19].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "SMILES is also used for drug generation [17, 51] and for compound similarity [45].", "startOffset": 40, "endOffset": 48}, {"referenceID": 46, "context": "SMILES is also used for drug generation [17, 51] and for compound similarity [45].", "startOffset": 40, "endOffset": 48}, {"referenceID": 41, "context": "SMILES is also used for drug generation [17, 51] and for compound similarity [45].", "startOffset": 77, "endOffset": 81}, {"referenceID": 44, "context": "It is generally known that structure is closely related to function [48, 54].", "startOffset": 68, "endOffset": 76}, {"referenceID": 49, "context": "It is generally known that structure is closely related to function [48, 54].", "startOffset": 68, "endOffset": 76}, {"referenceID": 24, "context": "QSAR prediction, which exploits the relationship between the chemical structure and the biological activity, for identifying \u2018druglikeness\u2019 1 and establishing metabolic pathways [28, 56].", "startOffset": 178, "endOffset": 186}, {"referenceID": 51, "context": "QSAR prediction, which exploits the relationship between the chemical structure and the biological activity, for identifying \u2018druglikeness\u2019 1 and establishing metabolic pathways [28, 56].", "startOffset": 178, "endOffset": 186}, {"referenceID": 27, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 174, "endOffset": 181}, {"referenceID": 65, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 174, "endOffset": 181}, {"referenceID": 21, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 207, "endOffset": 215}, {"referenceID": 23, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 207, "endOffset": 215}, {"referenceID": 27, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 264, "endOffset": 276}, {"referenceID": 29, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 264, "endOffset": 276}, {"referenceID": 40, "context": "\u008ce convolutional neural network (CNN) [31], one of the most widely used deep learning architecture, has shown outstanding performance in one-dimensional biological sequences [1, 71] and linguistic sentences [25, 27], as well as in two-dimensional image processing [31, 33, 44].", "startOffset": 264, "endOffset": 276}, {"referenceID": 38, "context": "A\u0089er the convolution layer, each convolued element is processed with the recti\u0080ed linear unit (ReLU) [42]", "startOffset": 101, "endOffset": 105}, {"referenceID": 48, "context": "\u008ce maxpooling length of 6 showed the best performance in our experiments (see Figure 6C) To prevent over\u0080\u008aing and to produce generalization e\u0082ects, we apply dropout [53] for the last CNN units.", "startOffset": 165, "endOffset": 169}, {"referenceID": 7, "context": "For a symmetric problem, such as a tweets comparison, Keras use shared layers, which reuse the weights [9].", "startOffset": 103, "endOffset": 106}, {"referenceID": 19, "context": "In the fully-connected layers, we use 128 hidden nodes, batch normalization [23], and a dropout [53] rate of 0.", "startOffset": 76, "endOffset": 80}, {"referenceID": 48, "context": "In the fully-connected layers, we use 128 hidden nodes, batch normalization [23], and a dropout [53] rate of 0.", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "\u008ce entire learning is conducted with optimization algorithm Adam [29] with an epoch of 100 and mini-batch size of 256.", "startOffset": 65, "endOffset": 69}, {"referenceID": 28, "context": "0) downloaded from the STITCH [32] database.", "startOffset": 30, "endOffset": 34}, {"referenceID": 59, "context": "\u008ce representations of chemical compounds were prepared as two types: the SMILES string, and the other was PubChem \u0080ngerprint [65].", "startOffset": 125, "endOffset": 129}, {"referenceID": 45, "context": "It has showed the best performance for certain tasks [50, 64], although the performance of the molecular descriptor depends on the target problem and classi\u0080er.", "startOffset": 53, "endOffset": 61}, {"referenceID": 58, "context": "It has showed the best performance for certain tasks [50, 64], although the performance of the molecular descriptor depends on the target problem and classi\u0080er.", "startOffset": 53, "endOffset": 61}, {"referenceID": 52, "context": "we retrieved the SMILES string and PubChem \u0080ngerprint by using PubChemPy [57].", "startOffset": 73, "endOffset": 77}, {"referenceID": 1, "context": "Our dataset consisted of training, validation, and test samples, as in the general learning experiments [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 7, "context": "1) [9].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "\u008ce pooling layer summarized the adjacent features resulting abstract representations, and inputs were down-sampled, resulting in a smaller number of model parameters to learn [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 57, "context": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi\u0080er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16].", "startOffset": 218, "endOffset": 226}, {"referenceID": 51, "context": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi\u0080er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16].", "startOffset": 247, "endOffset": 254}, {"referenceID": 13, "context": "We compared our proposed method, DeepCCI, with a feedforward neural network (FFNN), which is a plain deep classi\u0080er described in Figure 1A, and the conventional machine learning methods of support vector machine (SVM) [62, 63], random forest (RF) [5, 56], and adaptive boosting (AdaBoost) [16].", "startOffset": 289, "endOffset": 293}], "year": 2017, "abstractText": "Chemical-chemical interaction (CCI) plays a key role in predicting candidate drugs, toxicity, therapeutic e\u0082ects, and biological functions. CCI was created from text mining, experiments, similarities, and databases; to date, no learning-based CCI prediction method exist. In chemical analyses, computational approaches are required. \u008ce recent remarkable growth and outstanding performance of deep learning have a\u008aracted considerable research a\u008aention. However, even in state-of-the-art drug analyses, deep learning continues to be used only as a classi\u0080er. Nevertheless, its purpose includes not only simple classi\u0080cation, but also automated feature extraction. In this paper, we propose the \u0080rst end-to-end learning method for CCI, named DeepCCI. Hidden features are derived from a simpli\u0080ed molecular input line entry system (SMILES), which is a string notation representing the chemical structure, instead of learning from cra\u0089ed features. To discover hidden representations for the SMILES strings, we use convolutional neural networks (CNNs). To guarantee the commutative property for homogeneous interaction, we apply model sharing and hidden representation merging techniques. \u008ce performance of DeepCCI was compared with a plain deep classi\u0080er and conventional machine learning methods. \u008ce proposed DeepCCI showed the best performance in all seven evaluation metrics used. In addition, the commutative property was experimentally validated. \u008ce automatically extracted features through end-to-end SMILES learning alleviates the signi\u0080cant efforts required for manual feature engineering. It is expected to improve prediction performance, in drug analyses.", "creator": "LaTeX with hyperref package"}}}