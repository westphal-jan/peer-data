{"id": "1704.08509", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "No More Discrimination: Cross City Adaptation of Road Scene Segmenters", "abstract": "despite the recent success of deep - thinking called semantic segmentation, deploying a pre - trained road scene segmenter at a city data recommendations when explicitly presented in regular map manual would not achieve appropriate performance due ina regional fragmentation. instead of taking approximately large diameter previously annotated images throughout geographic city of interest to train or refine target knowledge, we propose your unsupervised query template to adapt how scene segmenters radically different ways. by utilizing dynamic street reader and toyota time - division feature, it can generate unannotated approaches performing each road scene at inconsistent times, checking that with respective static - computed profiles'continue extracted accordingly. by advancing a joint uniform transit district - specific geographical adversarial learning framework, adaptation producing pre - trained segmenters to distant city can directly achieved providing unnecessary need of any user annotation or improvement. we continue : spatial method improves the performance of semantic mapping in multiple cities across time, rather it performs mainly employing state - of - the - word approaches requiring annotated curated data.", "histories": [["v1", "Thu, 27 Apr 2017 11:14:21 GMT  (5266kb,D)", "http://arxiv.org/abs/1704.08509v1", "13 pages, 10 figures"]], "COMMENTS": "13 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yi-hsin chen", "wei-yu chen", "yu-ting chen", "bo-cheng tsai", "yu-chiang frank wang", "min sun"], "accepted": false, "id": "1704.08509"}, "pdf": {"name": "1704.08509.pdf", "metadata": {"source": "CRF", "title": "No More Discrimination: Cross City Adaptation of Road Scene Segmenters", "authors": ["Yi-Hsin Chen", "Wei-Yu Chen", "Yu-Ting Chen", "Bo-Cheng Tsai", "Yu-Chiang Frank Wang", "Min Sun"], "emails": ["vigorous0503}@gmail.com", "ycwang@citi.sinica.edu.tw", "sunmin@ee.nthu.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "Recent developments of technologies in computer vision, deep learning, and more broadly artificial intelligence, have led to the race of building advanced driver assistance systems (ADAS). From recognizing particular objects of interest toward understanding the corresponding driving environments, road scene segmentation is among the key components for a successful ADAS. With a sufficient amount of annotated training image data, existing computer vision algorithms already exhibit promising performances on the above task. However, when one applies pre-trained seg-\n\u2217indicates equal contribution\nmenters to a scene or city which is previously not seen, the resulting performance would be degraded due to dataset (domain) biases.\nWe conduct a pilot experiment to illustrate how severe a state-of-the-art semantic segmenter would be affected by the above dataset bias problem. We consider the segmenter of [2] which is trained on Cityscapes [5], and apply for segmenting about 400 annotated road scene images of different cities across countries: Rome, Rio, Taipei, and Tokyo. A drop in mean of intersection over union (mIoU) of 25- 30% was observed (see later experiments for more details). Thus, how to suppress the dataset bias would be critical when there is a need to deploy road scene segmenters to different cities.\nIt is not surprising that, collecting a large number of an-\n1\nar X\niv :1\n70 4.\n08 50\n9v 1\n[ cs\n.C V\n] 2\n7 A\nnotated training image data for each city of interest would be time-consuming and expensive. For instance, pixel labeling of one Cityscapes image takes 90 minutes on average [5]. To alleviate this problem, a number of methods have been proposed to reduce human efforts in pixellevel semantic labeling. For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling. However, these existing techniques still require human annotation during data collection, and thus might not be easily scaled up to larger image datasets.\nInspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation. Our proposed model is able to adapt a pre-trained segmentation model to a new city of interest, while only the collection of unlabeled road scene images of that city is required. To avoid any human interaction or annotation during data collection, we utilize Google Street View with its time-machine1 feature to harvest road scene images taken at the same (or nearby) locations but across different times. As detailed later in Sec. 4, this allows us to extract static-object priors from the city of interest. By integrating such priors with the proposed global and class-specific domain adversarial learning framework, refining/adapting the pre-trained segmenter can be easily realized.\nThe main contributions of this paper can be summarized as follows:\n\u2022 We propose an unsupervised learning approach, which performs global and class-wise adaptation for deploying pre-trained road scene segmenters across cities.\n\u2022 We utilize Google Street View images with timemachine features to extract static-object priors from the collected image data, without the need of user annotation or interaction.\n\u2022 Along with the static-object priors, we advance adversarial learning for assigning pseudo labels to cross-city images, so that joint global and class-wise adaptation of segmenters can be achieved."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. CNN-based Semantic Segmentation", "text": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4]. For example, Long\n1https://maps.googleblog.com/2014/04/go-back-in-time-with-streetview.html\net al. [18] utilize CNN for performing pixel-level classification, which is able to produce pixel-wise outputs of arbitrary sizes. In order to achieve high resolution prediction, [22, 2] further adapt deconvolution layers into CNN with promising performances. On the other hand, Chen et al. [4] choose to add a fully-connected CRF layer at their CNN output, which refines the pixel labels with context information properly preserved. We note that, since the goal of this paper is to adapt pre-trained segmenters across cities, we do not limit the use of particular CNN-based segmentation solvers in our proposed framework."}, {"heading": "2.2. Segmentation of Road Scene Images", "text": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31]. For example, Cordts et al. [5] release a natural road scene segmentation dataset, which consists of over 5000 annotated images. Xie et al. [37] annotate 3D semantic labels in a scene, followed by transferring the 3D labels into the associated 2D video frames. [30, 31] collect semantic labels from Computer Graphic (CG) images at a large scale; however, building CG worlds for practical uses might still be computationally expensive.\nOn the other hand, [3] choose to relax the supervision during the data collection process, and simply require a number of point-labels per image. Moreover, [24, 26, 27] only require image-level labels during data collection and training. In addition to image-level labels, Pathak et al. [25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset. Alternatively, [15, 38] apply free-form squiggles to provide partial pixel labels for data collection. Finally, [10] utilize image-level labels with cosegmentation techniques to infer semantic segmentation of foreground objects in the images of ImageNet."}, {"heading": "2.3. DNN-based Domain Adaptation", "text": "Since the goal of our work is to adapt CNN-based segmenters across datasets (or cities to be more precise), we now review recent deep neural networks (DNN) based approaches for domain adaptation [23]. Based on Maximum Mean Discrepancy (MMD), Long et al. [19] minimize the mean distance between data domains, and later they incorporate the concept of residual learning [21] for further improvements. Zellinger et al. [40] consider Central Moment Discrepancy (CMD) instead of MMD, while Sener et al. [33] enforce cyclic consistency on adaptation and structured consistency on transduction in their framework.\nRecently, Generative Adversarial Network (GAN) [9] has raised great attention in the fields of computer vision and machine learning. While most existing architectures are applied for synthesizing images with particular\nstyles [9, 29, 41]. Some further extend such frameworks for domain adaptation. In Coupled GAN [16], domain adaptation is achieved by first generating corresponded instances across domains, followed by performing classification.\nIn parallel with the appearance of GAN [9], Ganin et al. propose Domain Adversarial Neural Networks (DANN) [7, 8], which consider adversarial training for suppressing domain biases. For further extension, Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) [28] utilizes Variational Auto Encoder (VAE) and RNN for timeseries adaptation. Sharing a similar goal as ours, Hoffman et al. [11] extend such frameworks for semantic segmentation."}, {"heading": "3. Dataset", "text": "We now detail how we collect our road scene image dataset, and explain its unique properties.\nDiverse locations and appearances. Using Google Street View, road scene images at a global scale can be accessed across a large number of cities in the world. To address the issue of geo-location discrimination of a road scene segmenter, we download the road scene images of four cities at diverse locations, Rome, Rio, Tokyo, and Taipei, which are expected to have significant appearance differences. To ensure that we cover sufficient variations in visual appearances from each city, we randomly sample the locations in each city for image collection.\nTemporal information. With the time-machine features of Google Street View, image pairs of the same location yet\nacross different times can be further obtained. As detailed later in the Sec. 4.2, this property particularly allows us to observe prior information from static objects, so that improved adaptation without any annotation can be achieved. In our work, we have collected 1600 image pairs (3200 images in total) at 1600 different locations per city with high image quality (647\u00d7 1280 pixels).\nFor evaluation purposes, we select 100 image pairs from each city as the testing set, with pixel-level ground truth labels annotated by 15 image processing experts. We define 13 major classes for annotation: road, sidewalk, building, traffic light, traffic sign, vegetation, sky, person, rider, car, bus, motorcycle, and bicycle, as defined in Cityscapes [5]. Fig. 2 shows example images of our dataset. The dataset will be publicly available later for academic uses. To see more details and examples of our dataset, please refer to Appendix B or visit our website: https://yihsinchen.github.io/segmentation adaptation/.\nWe now summarize the uniqueness of our dataset below:\n\u2022 Unlike existing datasets which typically collect images in nearby locations (e.g., road scenes of the same city), our dataset includes over 400 road scene images from four different cities around the world, with highquality pixel-level annotations (for evaluation only).\n\u2022 Our dataset include image pairs at the same location but across different times, which provide additional temporal information for further processing and learning purposes."}, {"heading": "4. Our Method", "text": "In this section, we present the details of our proposed unsupervised domain adaptation framework, which is able to adapt pre-trained segmenters across different cities without using any user annotated data. In other words, while both images IS and labels YS are available from the source domain S, only images IT for the target domain T can be observed.\nDomain shift. When adapting image segmenters across cities, two different types of domain shifts (or dataset biases) can be expected: global and class-wise domain shift. The former comes from the overall differences in appearances between the cities, while the latter is due to distinct compositions of road scene components in each city.\nTo minimize the global domain shift, we follow [11] and apply the technique of adversarial learning, which introduces a domain discriminator with a loss LG. This is to distinguish the difference between source and target-domain images, with the goal to produce a common feature space for images across domains. To perform class-wise alignment, we extend the above idea and utilize multiple classwise domain discriminators (one for each class) with the corresponding adversarial loss Lclass. Unlike the discriminator for global alignment, these class-wise discriminators are trained to suppress the difference between cross-domain images but of the same class. Since we do not have any annotation for the city of interest (i.e., target-domain images), later we will explain how our method performs unsuper-\nvised learning to jointly solve the above adaptation tasks. With the above loss terms defined, the overall loss of our approach can be written as:\nLtotal = Ltask + \u03bbGLG + \u03bbclassLclass , (1)\nwhere \u03bbG and \u03bbclass are weights for the global and class-wise domain adversarial loss, respectively. Note that Ltask denotes the prediction loss of source-domain images, which can be viewed as a regularization term when adapting the learned model across domains.\nOur proposed framework. Fig. 3 illustrates our framework. Let C be the set of classes, and an input image denoted as x. Our proposed architecture can be decoupled into four major components: feature extractor MF (x, \u03b8F ) that transforms the input image to a highlevel, semantic feature space (the gray part), label predictor MY (MF (x, \u03b8F ), \u03b8Y ) that maps feature space to task label space (the orange part), and domain discriminator for global MG(MF (x, \u03b8F ), \u03b8G) (the green part) and class-wise M cclass(MF (x, \u03b8F ), \u03b8 c class), c \u2208 C alignments (the yellow part). The feature extractor and task label predictor are initialized from a pre-trained segmenter, while the domain discriminators are randomly initialized. While we utilize the front-end dilated-FCN [39] as the pre-trained segmenter in our work, it is worth noting that our framework can be generally applied to other semantic segmenters.\nIn Sec. 4.1 and Sec. 4.2, we will detail our unsupervised learning for global alignment and class-wise alignment, respectively. In particular, how we extract and integrate static-\nobject priors for the target domain images without any human annotation will be introduced in Sec. 4.3."}, {"heading": "4.1. Global Domain Alignment", "text": "Previously, domain adversarial learning frameworks have been applied for solving cross-domain image classification tasks [7]. However, for cross-domain image segmentation, each image consists of multiple pixels, which can be viewed as multiple instances per observation. Thus, how to extend the idea of domain adversarial learning for adapting segmenters across image domains would be our focus.\nInspire by [11], we take each grid in the fc7 feature map of the FCN-based segmenter as an instance. Let the feature maps of source and target domain images as MF (IS , \u03b8F ) and MF (IT , \u03b8F ), each map consists of N grids. Let pn(x) = \u03c3(MG(MF (x, \u03b8F )n, \u03b8G)) be the probability that the grid n of image x belongs to the source domain, where \u03c3 is the sigmoid function. We note that, for cross-domain classification, Ganin et al. [7] use the same loss function plus a gradient reversal layer to update the feature extractor and domain discriminator simultaneously. If directly applying their loss function for cross-domain segmentation, we would observe:\nmax \u03b8F min \u03b8G LG =\u2212 \u2211 IS\u2208S \u2211 n\u2208N log(pn(IS))\n\u2212 \u2211 IT \u2208T \u2211 n\u2208N log(1\u2212 pn(IT )) . (2)\nUnfortunately, this loss function will result in gradient vanishing as the discriminator converges to its local minimum. To alleviate the above issue, we follow [9] and decompose the above problem into two subtasks. More specifically, we have a domain discriminator \u03b8G trained with LDG for classifying these two distributions into two groups, and a feature extractor \u03b8F updated by its inverse loss LDinvG which minimizes the associated distribution differences. In summary, our objective is to minimize LG = LDG + LDinvG by iteratively update \u03b8G and \u03b8F :\nmin \u03b8G LDG , min \u03b8F LDinvG , (3)\nwhere LDG and LDinvG are defined as: LDG =\u2212 \u2211 IS\u2208S \u2211 n\u2208N log(pn(IS))\n\u2212 \u2211 IT \u2208T \u2211 n\u2208N log(1\u2212 pn(IT )) , (4)\nLDinvG =\u2212 \u2211 IS\u2208S \u2211 n\u2208N log(1\u2212 pn(IS))\n\u2212 \u2211 IT \u2208T \u2211 n\u2208N log(pn(IT )) . (5)"}, {"heading": "4.2. Class-wise Domain Alignment", "text": "In addition to suppressing the global misalignment between image domains, we propose to advance the same adversarial learning architecture to perform class-wise domain adaptation.\nWhile the idea of regularizing class-wise information during segmenter adaptation has been seen in [11], its classwise alignment is performed based on the composition of the class components in cross-city road scene images. To be more precise, it assumes that the composition/proportion of object classes across cities would be similar. Thus, such a regularization essentially performs global instead of classspecific adaptation.\nRecall that, when adapting our segmenters across cities, we only observe road scene images of the target city of interest without any label annotation. Under such unsupervised settings, we extend the idea in [20] and assign pseudo labels to pixels/grids in the images of the target domain. That is, after the global adaptation in Fig. 3, the predicted probability distribution maps \u03c6(IT ) = softmax(MY (MF (IT , \u03b8F ), \u03b8Y )) of target domain images can be produced. Thus, \u03c6(IT ) can be viewed as the \u201csoft\u201d pseudo label map for the target domain images. As a result, class-wise association across data domains can be initially estimated by relating the ground truth label in the source domain and the soft pseudo label in the target domain.\nFrom pixel to grid-level pseudo label assignment. In Sec. 4.1, to train the domain discriminator, we define each grid n in the feature space as one instance, which corresponds to multiple pixels in the image space. If the (pseudo) labels of these grids can be produced, adapting class-wise information using the same adversarial learning framework can be achieved.\nTo propagate and to determine the pseudo labels from pixels to each grid for the above adaptation purposes, we simply calculate the proportion of each class in each grid as the soft (pseudo) label. That is, let i be the pixel index in image space, n be the grid index in feature space, andR(n) be the set of pixels that correspond to grid n. If yi(IS) denote the ground truth label of pixel i for source domain images, we then calculate source-domain grid-wise soft-label \u03a6cn(IS) as the probability of grid n belonging to class c:\n\u03a6cn(IS) = \u2211\ni\u2208R(n)\nyi(IS) == c\n| R(n) | . (6)\nOn the other hand, due to the lack of annotated targetdomain data, it is not as straightforward to assign grid-level soft pseudo labels to images in that domain. To solve this problem, we utilize \u03c6(IT ) derived above. Let \u03c6ci (IT ) be the pixel-wise soft pseudo label of pixel i corresponding to\nclass c for target-domain images, we have target grid-wise soft pseudo label \u03a6cn(IT ) of grid n:\n\u03a6cn(IT ) = \u2211\ni\u2208R(n)\n\u03c6ci (IT )\n| R(n) | . (7)\nIntuitively, grid-wise soft (pseudo) labels \u03a6cn(IS) and \u03a6cn(IT ) are estimations of the probabilities that each grid n in source and target domain images belongs to object class c. To balance the appearance frequency of different classes, we normalize the estimated outputs in (6) and (7) as follows:\n\u03a6\u0303cn(IS) = \u03a6cn(IS)\u2211\nn\u2208N \u03a6cn(IS)\n\u03a6\u0303cn(IT ) = \u03a6cn(IT )\u2211\nn\u2208N \u03a6cn(IT )\n. (8)\nClass-wise adversarial learning. With the soft labels assigned to the source-domain images and the soft pseudo labels predicted for the target-domain ones, we now explain our adversarial learning for class-wise domain adaptation.\nAs depicted in Fig. 3, we deploy multiple class-wise domain discriminators \u03b8cclass, c \u2208 C in our proposed architecture, and each discriminator is specially trained for differentiating objects of the corresponding class c across domains. Similar to pn(x), given that each object class c has a corresponded domain discriminatorM cclass, we define pcn(x) = \u03c3(M c class(MF (x, \u03b8F )n, \u03b8 c class)) as the probability predicted by M cclass that the grid n of image x is from the\nsource domain. Combining the definition in (8), we define a pair of class-wise adversarial loss LDclass and LDinvclass to guide the optimization for class-wise alignment:\nLDclass =\u2212 \u2211 IS\u2208S \u2211 c\u2208C \u2211 n\u2208N \u03a6\u0303cn(IS)log(p c n(IS))\n\u2212 \u2211 IT \u2208T \u2211 c\u2208C \u2211 n\u2208N \u03a6\u0303cn(IT )log(1\u2212 pcn(IT )) , (9)\nLDinvclass =\u2212 \u2211 IS\u2208S \u2211 c\u2208C \u2211 n\u2208N \u03a6\u0303cn(IS)log(1\u2212 pcn(IS))\n\u2212 \u2211 IT \u2208S \u2211 c\u2208C \u2211 n\u2208N \u03a6\u0303cn(IT )log(p c n(IT )) . (10)\nFinally, similar to (3), the class-wise alignment process is to iteratively solve the following optimization problem:\nmin\u22c3 c\u2208C \u03b8cclass LDclass , min \u03b8F LDinvclass , (11)\nwhich minimizes the overall loss Lclass = LDclass+LDinvclass ."}, {"heading": "4.3. Harvesting Static-Object Prior", "text": "While jointly performing global and class-wise alignment between source and target-domain images would produce promising adaptation performance, the pseudo labels are initialized by pre-trained segmenter. Under the unsupervised domain adaptation setting, since no annotation of target-domain data can be obtained, fine-tuning the segmenter by such information is not possible.\nHowever, with the use of time-machine features from Google Street View images, we are able to leverage the temporal information for extracting the static-object priors from images in the target domain. As illustrated in Fig. 4, given an image pair of the same location but across different times, we first apply DeepMatching [36] to relate pixels within each image pair. For the regions with matched pixels across images, it implies such regions are related to static objects (e.g., building, road, etc.). Then, we additionally perform superpixel segmentation on the image pair using Entropy Rate Superpixel [17], which would group the nearby pixels into regions while the boundaries of the objects can be properly preserved. With the above derivation, we view the matched superpixels containing more than k matched pixels (we fix k = 3 in this work) as the staticobject prior Pstatic(IT ). Please refer to Appendix A for typical examples of mining static-object prior.\nLet Cstatic be the set of static-object classes. For the pixels that belong to Pstatic(IT ), we then refine their soft pseudo labels by suppressing its probabilities of being non-\nstatic objects:\n\u2200 i \u2208 Pstatic(IT )\n\u03c6\u0303ci (IT ) =  \u03c6 c i (IT ) / \u2211 c\u0302\u2208Cstatic \u03c6c\u0302i (IT ) if c \u2208 Cstatic\n0 else (12)"}, {"heading": "5. Experiments", "text": "We first conduct experiments to demonstrate the issue of cross-city discrimination even using a state-of-the-art semantic segmenter. Then, we will verify the effectiveness of our proposed unsupervised learning method on the Cityscapes to Our Dataset domain adaptation task. By comparing it with a fully-supervised baseline (i.e., finetuning by fully annotated training data), we show that our unsupervised method would achieve comparable performances as the fully-supervised methods in most cases. Finally, we perform an extra experiment, SYNTHIA to Cityscapes, to prove that our method could be generally applied to different datasets."}, {"heading": "5.1. Implementation Details", "text": "In this work, all the implementations are produced utilizing the open source TensorFlow [1] framework, and the codes will be released upon acceptance. In the following experiments, we use mini-batch size 16 and the Adam optimizer [13] with learning rate of 5\u00d7 10\u22126, beta1 = 0.9, and beta2 = 0.999 to optimize the network. Moreover, we set the hyper-parameters in (1): \u03bbG and \u03bbclass, to be numbers gradually changing from 0 to 0.1 and 0 to 0.5, respectively. In addition, for the experiments using static-object priors, we use {road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, sky} as the set of static-object classes Cstatic defined in Sec. 4.3."}, {"heading": "5.2. Cross-City Discrimination", "text": "We apply the segmenter pre-trained on Cityscapes to images of different cities in Our Dataset. As shown in Table 1, there is a severe performance drop in the four cities\ncompared to its original performance on Cityscapes. Interestingly, we observe a trend that the farther the geo-distance between the target city and the pre-trained city (Frankfurt), the severer the performance degradation. This implies that different visual appearances across cities due to cultural differences would dramatically impact the accuracy of the segmenter. For example, in Taipei, as shown in Fig. 2, there are many signboards and shop signs attached to the buildings, and many scooters on the road, which are uncommon in Frankfurt. It also justifies the necessity of an effective domain adaptation method for the road scene segmenter to alleviate the discrimination."}, {"heading": "5.3. Cross-City Adaptation", "text": "Baseline. We use a fully-supervised method to establish a strong baseline as the upper bound of adaptation improvement. We divide our 100 images with fine annotations to 10 subsets for each city. Each time we select one subset as the testing set, and the other 90 images as the training set and fine-tune the segmenter for 2000 steps. We repeat the procedure for 10 times and average the testing results as the baseline performance. Our method. Now we apply our domain adversarial learning method to adapt the pre-segmenter in an unsupervised fashion. Meanwhile, we do the ablation study to demonstrate the contribution from each component: global alignment, class-wise alignment, and static-object prior. We summarize the experimental results in Table 2, where \u201dPretrained\u201d denotes the pre-trained model, \u201dUB\u201d denotes the fully-supervised upper bound, \u201dGA\u201d denotes the global alignment part of our method, \u201dGA+CA\u201d denotes the combination of global alignment and class-wise alignment, and finally, \u201dFull Method\u201d denotes our overall method that utilizes the static-object priors. On average over four cities, our global alignment method contributes 2.6% mIoU gain, our class-wise alignment method also contributes 0.9% mIoU gain, and finally, the static-object priors contributes another 0.6% mIOU improvement. Furthermore, the t-SNE visualization results in Appendix A also show that the domain shift keeps decreasing from \u201dPre-trained\u201d to \u201dGA\u201d to \u201dGA+CA\u201d. These results demonstrate the effectiveness of each component of our method. In Fig. 5, we show some typical examples."}, {"heading": "5.4. Synthetic to Real Adaptation", "text": "We additionally apply our method to another adaptation task with a different type of domain shift: SYNTHIA to Cityscapes. In this experiment, we take SYNTHIARAND-CITYSCAPES [31] as the source domain, which contains 9400 synthetic road scene images with Cityscapescompatible annotations. For the unlabeled target domain, we use the training set of Cityscapes. During evaluation, we test our adapted segmenter on the validation set of\nCityscapes. We note that, since there are no paired images with temporal information in Cityscapes (as those in our dataset), we cannot extract static-object priors in this ex-\nperiment. Nevertheless, from the results shown in Table 3, performing global and class-wise alignment using our proposed method still achieves 3.1% and 1.9% mIOU gain, re-\nspectively. These results again demonstrate the robustness of our proposed method. For typical examples of this adaptation task, please refer to Appendix C."}, {"heading": "6. Conclusion", "text": "In this paper, we present an unsupervised domain adaptation method for semantic segmentation, which alleviates cross-domain discrimination on road scene images across different cities. We propose a unified framework utilizing domain adversarial learning, which performs joint global and class-wise alignment by leveraging soft labels from source and target-domain data. In addition, our method uniquely identifies and introduce static-object priors to our method, which are retrieved from images via natural synchronization of static objects over time. Finally, we provide a new dataset containing road scene images of four cities across countries, good-quality annotations and paired images with temporal information are also included. We demonstrate the effectiveness of each component of our method on tasks with different levels of domain shift."}, {"heading": "B. Dataset", "text": "To demonstrate the uniqueness of our dataset for road scene semantic segmenter adaptation, here we show more examples of it.\nUnlabeled Image Pairs There are more examples collected at different cities with diverse appearances in Fig. 8. Valuable temporal information which facilitates unsupervised adaptation is contained in these image pairs.\nLabeled Image We also show more annotated images in Fig. 9 to demonstrate the label-quality of our dataset."}, {"heading": "C. Synthetic to Real Adaptation", "text": "In Sec. 5.4 of the main paper, we have shown the quantitative results of this adaptation task in Table 3. We conclude that our method could perform well even under this challenging setting. To better support our conclusion, here we show some typical examples of this task in Fig. 10."}], "references": [{"title": "et al", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin"], "venue": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Whats the point: Semantic segmentation with point supervision", "author": ["A. Bearman", "O. Russakovsky", "V. Ferrari", "L. Fei-Fei"], "venue": "ECCV. Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR. IEEE", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1915\u20131929", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Domainadversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "Journal of Machine Learning Research, 17(59):1\u201335", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet auto-annotation with segmentation propagation", "author": ["M. Guillaumin", "D. K\u00fcttel", "V. Ferrari"], "venue": "IJCV. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "FCNs in the wild: Pixel-level adversarial and constraint-based adaptation", "author": ["J. Hoffman", "D. Wang", "F. Yu", "T. Darrell"], "venue": "arXiv preprint arXiv:1612.02649", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Undoing the damage of dataset bias", "author": ["A. Khosla", "T. Zhou", "T. Malisiewicz", "A.A. Efros", "A. Torralba"], "venue": "ECCV. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "ICLR", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Seed", "author": ["A. Kolesnikov", "C.H. Lampert"], "venue": "expand and constrain: Three principles for weakly-supervised image segmentation. In ECCV. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation", "author": ["D. Lin", "J. Dai", "J. Jia", "K. He", "J. Sun"], "venue": "CVPR. IEEE", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Coupled generative adversarial networks", "author": ["M.-Y. Liu", "O. Tuzel"], "venue": "NIPS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Entropy rate superpixel segmentation", "author": ["M.-Y. Liu", "O. Tuzel", "S. Ramalingam", "R. Chellappa"], "venue": "CVPR. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning transferable features with deep adaptation networks", "author": ["M. Long", "Y. Cao", "J. Wang", "M.I. Jordan"], "venue": "ICML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer feature learning with joint distribution adaptation", "author": ["M. Long", "J. Wang", "G. Ding", "J. Sun", "P.S. Yu"], "venue": "ICCV", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised domain adaptation with residual transfer networks", "author": ["M. Long", "H. Zhu", "J. Wang", "M.I. Jordan"], "venue": "NIPS", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV. IEEE", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation", "author": ["G. Papandreou", "L.-C. Chen", "K.P. Murphy", "A.L. Yuille"], "venue": "ICCV. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Constrained convolutional neural networks for weakly supervised segmentation", "author": ["D. Pathak", "P. Krahenbuhl", "T. Darrell"], "venue": "ICCV. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional multi-class multiple instance learning", "author": ["D. Pathak", "E. Shelhamer", "J. Long", "T. Darrell"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "From image-level to pixellevel labeling with convolutional networks", "author": ["P.O. Pinheiro", "R. Collobert"], "venue": "CVPR. IEEE", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Variational recurrent adversarial deep domain adaptation", "author": ["S. Purushotham", "W. Carvalho", "T. Nilanon", "Y. Liu"], "venue": "ICLR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2017}, {"title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "author": ["A. Radford", "L. Metz", "S. Chintala"], "venue": "ICLR", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Playing for data: Ground truth from computer games", "author": ["S.R. Richter", "V. Vineet", "S. Roth", "V. Koltun"], "venue": "ECCV. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes", "author": ["G. Ros", "L. Sellart", "J. Materzynska", "D. Vazquez", "A.M. Lopez"], "venue": "CVPR. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Built-in foreground/background prior for weakly-supervised semantic segmentation", "author": ["F. Saleh", "M.S.A. Akbarian", "M. Salzmann", "L. Petersson", "S. Gould", "J.M. Alvarez"], "venue": "ECCV. Springer", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning transferrable representations for unsupervised domain adaptation", "author": ["O. Sener", "H.O. Song", "A. Saxena", "S. Savarese"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Distinct class-specific saliency maps for weakly supervised semantic segmentation", "author": ["W. Shimoda", "K. Yanai"], "venue": "ECCV. Springer", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "CVPR. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Deepflow: Large displacement optical flow with deep matching", "author": ["P. Weinzaepfel", "J. Revaud", "Z. Harchaoui", "C. Schmid"], "venue": "ICCV. IEEE", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Semantic instance annotation of street scenes by 3d to 2d label transfer", "author": ["J. Xie", "M. Kiefel", "M.-T. Sun", "A. Geiger"], "venue": "CVPR. IEEE", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to segment under various forms of weak supervision", "author": ["J. Xu", "A.G. Schwing", "R. Urtasun"], "venue": "CVPR. IEEE", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "ICLR", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Central moment discrepancy (CMD) for domain-invariant representation learning", "author": ["W. Zellinger", "T. Grubinger", "E. Lughofer", "T. Natschl\u00e4ger", "S. Saminger-Platz"], "venue": "ICLR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2017}, {"title": "Generative visual manipulation on the natural image manifold", "author": ["J.-Y. Zhu", "P. Kr\u00e4henb\u00fchl", "E. Shechtman", "A.A. Efros"], "venue": "ECCV. Springer", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "We consider the segmenter of [2] which is trained on Cityscapes [5], and apply for segmenting about 400 annotated road scene images of different cities across countries: Rome, Rio, Taipei, and Tokyo.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "We consider the segmenter of [2] which is trained on Cityscapes [5], and apply for segmenting about 400 annotated road scene images of different cities across countries: Rome, Rio, Taipei, and Tokyo.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": "For instance, pixel labeling of one Cityscapes image takes 90 minutes on average [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 36, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 58, "endOffset": 62}, {"referenceID": 29, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 80, "endOffset": 88}, {"referenceID": 30, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 80, "endOffset": 88}, {"referenceID": 31, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 118, "endOffset": 129}, {"referenceID": 33, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 118, "endOffset": 129}, {"referenceID": 2, "context": "For example, researchers choose to utilize 3D information [37], rendered images [30, 31], or weakly supervised labels [32, 34, 3] for labeling.", "startOffset": 118, "endOffset": 129}, {"referenceID": 22, "context": "Inspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation.", "startOffset": 53, "endOffset": 65}, {"referenceID": 34, "context": "Inspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation.", "startOffset": 53, "endOffset": 65}, {"referenceID": 11, "context": "Inspired by the recent advances in domain adaptation [23, 35, 12], we propose an unsupervised learning framework for performing cross-city semantic segmentation.", "startOffset": 53, "endOffset": 65}, {"referenceID": 5, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 17, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 21, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 1, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 3, "context": "Semantic segmentation is among the recent breakthrough in computer vision due to the development and prevalence of Convolutional Neural Networks (CNN), which has been successfully applied to predict dense pixelwise semantic labels [6, 18, 22, 2, 4].", "startOffset": 231, "endOffset": 248}, {"referenceID": 17, "context": "[18] utilize CNN for performing pixel-level classification, which is able to produce pixel-wise outputs of arbitrary sizes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In order to achieve high resolution prediction, [22, 2] further adapt deconvolution layers into CNN with promising performances.", "startOffset": 48, "endOffset": 55}, {"referenceID": 1, "context": "In order to achieve high resolution prediction, [22, 2] further adapt deconvolution layers into CNN with promising performances.", "startOffset": 48, "endOffset": 55}, {"referenceID": 3, "context": "[4] choose to add a fully-connected CRF layer at their CNN output, which refines the pixel labels with context information properly preserved.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 36, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 29, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 30, "context": "To apply CNN-based segmenters to road scene images, there are several attempts to train segmenters on large-scale image datasets [5, 37, 30, 31].", "startOffset": 129, "endOffset": 144}, {"referenceID": 4, "context": "[5] release a natural road scene segmentation dataset, which consists of over 5000 annotated images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[37] annotate 3D semantic labels in a scene, followed by transferring the 3D labels into the associated 2D video frames.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30, 31] collect semantic labels from Computer Graphic (CG) images at a large scale; however, building CG worlds for practical uses might still be computationally expensive.", "startOffset": 0, "endOffset": 8}, {"referenceID": 30, "context": "[30, 31] collect semantic labels from Computer Graphic (CG) images at a large scale; however, building CG worlds for practical uses might still be computationally expensive.", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "On the other hand, [3] choose to relax the supervision during the data collection process, and simply require a number of point-labels per image.", "startOffset": 19, "endOffset": 22}, {"referenceID": 23, "context": "Moreover, [24, 26, 27] only require image-level labels during data collection and training.", "startOffset": 10, "endOffset": 22}, {"referenceID": 25, "context": "Moreover, [24, 26, 27] only require image-level labels during data collection and training.", "startOffset": 10, "endOffset": 22}, {"referenceID": 26, "context": "Moreover, [24, 26, 27] only require image-level labels during data collection and training.", "startOffset": 10, "endOffset": 22}, {"referenceID": 24, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 46, "endOffset": 58}, {"referenceID": 33, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 46, "endOffset": 58}, {"referenceID": 31, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 46, "endOffset": 58}, {"referenceID": 13, "context": "[25] incorporate constraints on object sizes, [14, 34, 32] utilize weak object location knowledge, and [14] exploit object boundaries for constrained segmentation without using a large annotated dataset.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Alternatively, [15, 38] apply free-form squiggles to provide partial pixel labels for data collection.", "startOffset": 15, "endOffset": 23}, {"referenceID": 37, "context": "Alternatively, [15, 38] apply free-form squiggles to provide partial pixel labels for data collection.", "startOffset": 15, "endOffset": 23}, {"referenceID": 9, "context": "Finally, [10] utilize image-level labels with cosegmentation techniques to infer semantic segmentation of foreground objects in the images of ImageNet.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "Since the goal of our work is to adapt CNN-based segmenters across datasets (or cities to be more precise), we now review recent deep neural networks (DNN) based approaches for domain adaptation [23].", "startOffset": 195, "endOffset": 199}, {"referenceID": 18, "context": "[19] minimize the mean distance between data domains, and later they incorporate the concept of residual learning [21] for further improvements.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[19] minimize the mean distance between data domains, and later they incorporate the concept of residual learning [21] for further improvements.", "startOffset": 114, "endOffset": 118}, {"referenceID": 39, "context": "[40] consider Central Moment Discrepancy (CMD) instead of MMD, while Sener et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] enforce cyclic consistency on adaptation and structured consistency on transduction in their framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Recently, Generative Adversarial Network (GAN) [9] has raised great attention in the fields of computer vision and machine learning.", "startOffset": 47, "endOffset": 50}, {"referenceID": 8, "context": "styles [9, 29, 41].", "startOffset": 7, "endOffset": 18}, {"referenceID": 28, "context": "styles [9, 29, 41].", "startOffset": 7, "endOffset": 18}, {"referenceID": 40, "context": "styles [9, 29, 41].", "startOffset": 7, "endOffset": 18}, {"referenceID": 15, "context": "In Coupled GAN [16], domain adaptation is achieved by first generating corresponded instances across domains, followed by performing classification.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "In parallel with the appearance of GAN [9], Ganin et al.", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "propose Domain Adversarial Neural Networks (DANN) [7, 8], which consider adversarial training for suppressing domain biases.", "startOffset": 50, "endOffset": 56}, {"referenceID": 7, "context": "propose Domain Adversarial Neural Networks (DANN) [7, 8], which consider adversarial training for suppressing domain biases.", "startOffset": 50, "endOffset": 56}, {"referenceID": 27, "context": "For further extension, Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) [28] utilizes Variational Auto Encoder (VAE) and RNN for timeseries adaptation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "[11] extend such frameworks for semantic segmentation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "We define 13 major classes for annotation: road, sidewalk, building, traffic light, traffic sign, vegetation, sky, person, rider, car, bus, motorcycle, and bicycle, as defined in Cityscapes [5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 10, "context": "To minimize the global domain shift, we follow [11] and apply the technique of adversarial learning, which introduces a domain discriminator with a loss LG.", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "While we utilize the front-end dilated-FCN [39] as the pre-trained segmenter in our work, it is worth noting that our framework can be generally applied to other semantic segmenters.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Previously, domain adversarial learning frameworks have been applied for solving cross-domain image classification tasks [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Inspire by [11], we take each grid in the fc7 feature map of the FCN-based segmenter as an instance.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "[7] use the same loss function plus a gradient reversal layer to update the feature extractor and domain discriminator simultaneously.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "To alleviate the above issue, we follow [9] and decompose the above problem into two subtasks.", "startOffset": 40, "endOffset": 43}, {"referenceID": 10, "context": "While the idea of regularizing class-wise information during segmenter adaptation has been seen in [11], its classwise alignment is performed based on the composition of the class components in cross-city road scene images.", "startOffset": 99, "endOffset": 103}, {"referenceID": 19, "context": "Under such unsupervised settings, we extend the idea in [20] and assign pseudo labels to pixels/grids in the images of the target domain.", "startOffset": 56, "endOffset": 60}, {"referenceID": 35, "context": "4, given an image pair of the same location but across different times, we first apply DeepMatching [36] to relate pixels within each image pair.", "startOffset": 100, "endOffset": 104}, {"referenceID": 16, "context": "Then, we additionally perform superpixel segmentation on the image pair using Entropy Rate Superpixel [17], which would group the nearby pixels into regions while the boundaries of the objects can be properly preserved.", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "In this work, all the implementations are produced utilizing the open source TensorFlow [1] framework, and the codes will be released upon acceptance.", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "In the following experiments, we use mini-batch size 16 and the Adam optimizer [13] with learning rate of 5\u00d7 10\u22126, beta1 = 0.", "startOffset": 79, "endOffset": 83}, {"referenceID": 30, "context": "In this experiment, we take SYNTHIARAND-CITYSCAPES [31] as the source domain, which contains 9400 synthetic road scene images with Cityscapescompatible annotations.", "startOffset": 51, "endOffset": 55}], "year": 2017, "abstractText": "Despite the recent success of deep-learning based semantic segmentation, deploying a pre-trained road scene segmenter to a city whose images are not presented in the training set would not achieve satisfactory performance due to dataset biases. Instead of collecting a large number of annotated images of each city of interest to train or refine the segmenter, we propose an unsupervised learning approach to adapt road scene segmenters across different cities. By utilizing Google Street View and its timemachine feature, we can collect unannotated images for each road scene at different times, so that the associated static-object priors can be extracted accordingly. By advancing a joint global and class-specific domain adversarial learning framework, adaptation of pre-trained segmenters to that city can be achieved without the need of any user annotation or interaction. We show that our method improves the performance of semantic segmentation in multiple cities across continents, while it performs favorably against state-of-the-art approaches requiring annotated training data.", "creator": "LaTeX with hyperref package"}}}