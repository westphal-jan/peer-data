{"id": "1704.08772", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2017", "title": "Deep Face Deblurring", "abstract": "even though the aim of distance illumination is arguably long studied vision task, the usage of generic methods are not effective in real world problem images. dual domain knowledge favors specific object motifs, e. g. reflections or faces, possibly caused here an estimated amount of attention. shadows are among as numerous severely resolved objects in computer vision ( observing the further progress made, the optimization of faces does now contain less satisfying understanding via yet arbitrary image. we begin the problem reduction problem deblurring by inserting weak symbols in compressed form flow nets in wireless deep network. we introduce locally efficient framework, used might exploit a dataset of seventy two million frames. the dataset, which commonly call rabbit, was used around the network's execution process. players notice problems with real world blurred facial forms and acknowledge that targeting designer cuts a pair significantly to the sharp natural latent targets.", "histories": [["v1", "Thu, 27 Apr 2017 23:01:45 GMT  (9763kb,D)", "http://arxiv.org/abs/1704.08772v1", null], ["v2", "Thu, 25 May 2017 07:45:36 GMT  (6693kb,D)", "http://arxiv.org/abs/1704.08772v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["grigorios g chrysos", "stefanos zafeiriou"], "accepted": false, "id": "1704.08772"}, "pdf": {"name": "1704.08772.pdf", "metadata": {"source": "CRF", "title": "Deep Face Deblurring", "authors": ["Grigorios Chrysos", "Stefanos Zafeiriou"], "emails": ["g.chrysos@imperial.ac.uk", "s.zafeiriou@imperial.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Blind deblurring is the task of acquiring an estimate of the sharp latent image given a blurry image as input. There exists no single generic algorithm for deblurring all objects; the task is notoriously ill-posed. Hence, in this work we focus in human face deblurring and we argue that exploiting domain-specific knowledge can lead to superior deblurring results. Even though the human face is among the most studied objects in computer vision due to the significant applications in various domains including face recognition, computer graphics, surveillance, face deblurring has not received much attention yet.\nDeblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.g. state-of-the-art results in real world blurred images in Fig. 1. The difficulty in real world blurred images can be attributed to the non-linear functions involved in the imaging process, like lens saturation, depth variation, lossy compression.\nNevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of pri-\nors along with some optimisation restrictions ([28, 35]). Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.g. text or face priors ([33, 32]). Domain-specific methods frequently outperform the generic deblurring methods precisely because they insert a stronger form of supervision. The method of [32] is currently the method that explicitly models the blurring for the human face. A contour matching process with faces from an exemplar dataset is initially performed and then the exemplar image is utilised to guide the optimisation method. The contour matching restricts the usage of the method since a) it is computationally demanding to compare each image against a dataset, b) the matching is inaccurate for poses that do not exist in the dataset. Most of the generic methods include either a prior based on the gradient or the contour/edge detection, which yields suboptimal results for the face. This can be attributed to the highly structured for of it along with the little texture.\nWe capitalise on the recent developments on Convolutional Neural Networks (CNN) and object alignment to insert weak supervision to our method. The last few years the introduction of elaborate benchmarks [36] allowed CNN methods to surpass the performance of the hand-crafted linear optimisation techniques, e.g. in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20]. Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46]. In our work, the landmarks obtained through a localisation technique are used to pre-process the image (e.g. remove global transformations). There is no strict alignment in our method, since the blurring process might lead to an ambiguity in the exact positioning of the landmarks. However, in our experimentation the localisation works sufficiently well for our purpose of selecting and pre-processing the region to be fed in the network.\nAdditionally, we have devised an automatic framework that allows the creation of a large dataset with human faces. This is a crucial factor for training multi-layer networks. We have utilised this framework to create 2MF 2, a dataset that consists of more than 2 million frames, each of which con-\n1\nar X\niv :1\n70 4.\n08 77\n2v 1\n[ cs\n.C V\n] 2\n7 A\npr 2\n01 7\ntains a human face. The intervention of the user is required only in verifying visually for a single frame per video that a human face is indeed included in the scene, making the collection of such large dataset time-efficient. We utilised 2MF 2 to train our network. Sequentially we conducted several experiments with different types of blur, including low resolution real world blurred images to verify our claim.\nOur contributions in this work can be summarised as:\n\u2022 We introduce a network architecture that can perform deblurring in human faces. As demonstrated in the experimental Section, the method performs quite well in a wide variety of cases.\n\u2022 We introduce an automatic framework that allows the collection of large dataset in a time-efficient manner. We have utilised this framework to create the 2MF 2\ndataset, which consists of more than 2 million frames.\nIn the following Sections we summarise the related methods to our work in Sec. 2; develop our method in Sec. 3; describe the framework we have devised for obtaining the thousands of images required for training the network in Sec. 4 and finally experimentaly validate our method in Sec. 5."}, {"heading": "2. Related Work", "text": "Blur is typically modelled as the convolution of a blur kernelKg with a (latent) sharp image I , i.e.\nIbl = \u03c8(I \u2217Kg + ) (1)\nwith I \u2208 Rh1\u00d7w1 , Kg \u2208 Rh2\u00d7w2 (h2 h1, w2 w1), while Ibl \u2208 Ra\u00d7b denotes the blurry image with a = (h1 \u2212 h2 + dh), b = (w1 \u2212 w2 + dw). The symbol of \u2018\u2217\u2019 denotes the convolution, while dh, dw are defined based\non the type of the convolution. The symbol of stands for the noise term, \u03c8 for a function that models additional nonlinear artifacts, e.g. lossy compression, clipping for out of range intensity values, non-linear sensor response. Retrieving the sharp image is a well-known ill-posed problem, thus some strong assumptions/priors are required. A simple illustration of this is that for any fixed solution I\u0303, K\u0303g of Eq. 1, the family of I\u0303 \u00b7 \u03bb, K\u0303g\u03bb is also a valid solution, which is referred to as the scaling ambiguity.\nBased on the solution to the aforementioned optimisation problem, blind deblurring methods can be separated into three categories, each with an extensive literature, hence only the most closely related to our work are summarised below. For a more thorough study, the interested reader is redirected to [26].\nSynthesis-based: Instead of solving the optimisation, these methods typically include a heuristic for \u2019guessing\u2019 the blurry parts and then apply a synthesis-based replacement in the blurry regions. The majority of those works ([9, 41]) implicitly assume that there are multiple frames with approximately the same content and that there exists a sharp patch that matches in content the respective blurry one. These two strong assumptions combined with the little texture in a face (which weakens the heuristic to detect sharp patches), result in not using this type of deblurring for human faces or highly structured objects with poor texture.\nOptimisation-based: Based on Eq. (1) and assuming \u03c8 is the identity function, this class of methods formulates the problem as the minimisation of a cost function of the format\nI\u0303 = argmin I\n(||Ibl \u2212 I \u2217Kg||22 + f(Ibl)). (2)\nwith f(I) a prior based on generic image statistics or domain-specific prior. These methods are applied in a\ncoarse-to-fine manner, while they typically estimate the (dense) kernel and then perform a non-blind deconvolution. Apart from the I in Eq. 2, they frequently minimise with respect to the blur kernel Kg , which might lead to a blurry I\u0303 if a joint MAP (Maximum a posteriori) optimisation is followed ([28]). Levin et al. suggest instead to solve a MAP on the kernel with a gradient-based prior based natural image statistics. More recently, Pan et al. in [33] apply an `0 norm as a sparse prior on both the intensity values and the image gradient for deblurring text. HaCohen et al. [18] support that the gradient prior alone is not sufficient, and introduce a prior that locates dense correspondences of the blurry image with a similar sharp image, while they iteratively optimise over the correspondence, the kernel and the sharp image estimation. A strong requirement of their algorithm is the similar reference image, which is not always available. A generalisation of [18] is the work of [32], which also requires an exemplar dataset to locate an image with a similar contour. They perform face deblurring, where the contour is used to guide the matching with the exemplar image, the gradient of which is used in the initial blind estimation iterations. The authors indicate that this leads to an improved performance, however, both the matching of the images based on a contour and the obligatory presence of a similar contour in the dataset limit the applications of this work. Even though those methods have proven to work well with synthetic blurs, they do not generalise well in realworld blurred images ([26]) due to the strong assumptions of invariance and the simplified format of \u03c8. Another common attribute of these methods is the iterative optimisation procedure; they are executed in a loop hundreds or even thousands of times to return a deblurred image, which classifies these methods as computationally intensive; some of them require hours for deblurring a single image ([5]).\nLearning-based: With the resurrection of neural networks, few methods have appeared for learning a network to perform deblurring. These networks can model nonlinear functions \u03c8 or spatially varying blur kernels. Obtaining the sharp image in this case is defined as a function I\u0303 = \u03c6(Ibl,p), with p denoting the parameters of the method. Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image. In [21], the regularised `2 loss of an up to 15-layer CNN is minimised for text deblurring. Even though they report nice results, the text deblurring domain is a structured but limited class (the sharp text can be represented as a sequence of binary intensity values). They argue that the performance can be mainly attributed to the network that modelled well the text prior, hence it is questionable whether this would work in more complex object types. Sun et al. in [40], learn a CNN to recognise few discretised motion kernels and then perform a non-blind deconvolution in a dense\nmotion field estimate. Our method belongs in the last category of deblurring, i.e. the learning-based methods, specifically the methods that learn \u03c6 from the data. The combination of such a learning method with weak supervision through landmark localisation has not been performed before for deblurring."}, {"heading": "3. Method", "text": "In this Section, we develop our method for deblurring an image. This includes the deep network that we utilise, which is a modified version of the powerful ResNet architecture. We insert a weak supervision through landmark localisation as a preprocessing step. We explain the creation of the pairs of images for training the network, the architecture and then the inference steps for an unseen image."}, {"heading": "3.1. Notation", "text": "A sparse shape of n fiducial (landmark) points is denoted as l for the image I with l = [[`1]T , [`2]T , ..., [`n]T ]T , with `j = [xj , yj ]\nT , j \u2208 [1, n], xj , yj \u2208 R the Cartesian coordinates of the jth point. When referring to a random image I , we will implicitly hypothesise that I contains a human face, of which the facial sparse shape l is available."}, {"heading": "3.2. Training pair creation", "text": "Apart from the blurry input, the corresponding sharp image is required to train the network. Unfortunately, obtaining real world blurred images with a dense correspondence with a similar sharp image is not a trivial task, especially if thousands such images are required to train a deep network. Hence, following similar methods ([40, 5]) we resort to simulating the blur from sharp images.\nA synthetically blurred image Ibl is generated by convolving the original sharp image I with a blur kernel (simulating Eq. 1). A unique blur kernel is created for every input image to allow for the maximum variation in the number of blur kernels that have emerged during the training. The blur kernel is chosen arbitrarily in each step between a Gaussian blur kernel and a motion blur kernel, both with varying deviation and spatial support."}, {"heading": "3.3. Network", "text": "We employ the residual network (ResNet) architecture of [20] as the learning component in our method. ResNet consists of a number of \u2018blocks\u2019; each \u2018block\u2019 is a sequence of convolutional layers, followed by Rectified Linear Units, with identity connections connecting the blocks. This simple architecture has demonstrated state-of-the-art performance in several tasks.\nWe modify the original ResNet by disabling all the max pooling operation, while skip connections ([19]) are added in the 2nd and 3rd ResNet blocks. A batch normalisation is\nadded in every skip connection to ensure a common scale; a linear mapping is learnt from the high-dimensional space of the connections to the low dimensional space of the output image shape. The huber loss of [22] is utilised for our loss function. This is a continuous and differentiable function with\nLh(x) = { ||x||1 \u2212 0.5 ||x||1 > 1 0.5||x||22 otherwise\n(3)\nNamely, the loss function of our network is:\nL = Lh(\u03c6(Ibl)\u2212 I) (4)"}, {"heading": "3.4. Inference", "text": "During inference, we do not require a latent image, only a blurry image Ibl suffices. An off-the-shelf face detector is employed to obtain the bounding box; the landmarks are localised through a localisation technique. The image is rescaled based on the size of the landmarks, while a rectangular area around the face (landmarks) is cropped, this rctangular area is fed in the aforementioned network (only the feed-forward part is required).\nAmong the most successful face detectors is the deformable part models (DPM) detector [16, 30]. DPM learn a mixture of models which aim to detect faces in different poses. Each model implicitly considers some parts which are allowed to deform with a quadratic cost. The cost function of DPM contains an appearance (unary) term along with a pairwise (deformation) term plus a bias, all of which are learned with a discriminative training procedure. The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10]. Both techniques [46, 23] belong to the regression based discriminative methods for landmark localisation. These methods learn to regress from the pixel intensities (with the former extracting hand-crafted SIFT features, while the latter of Kazemi et al. rely on data driven learned features) to the sparse shape coordinates. Both methods have proven very accurate in a number of benchmarks [37, 10], hence we adopt the method of Kazemi et al. due to a publicly available fast implementation [24]."}, {"heading": "4. Data mining", "text": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]). Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7]. Similarly, to train our network thousands of real world images are required, therefore we describe here our method for mining frames from videos in a semi-supervised manner.\nA number of videos are crawled using the API\u2019s of web sources, e.g. Youtube; each video consists of few thousand frames and is analysed independently to determine the frames, if any, that are appropriate for the task. In our case, we aim at utilising the videos with dynamically moving faces. We defined the following three requirements for a video to be included in the training:\n1. a face is present in each selected frame,\n2. the face is not completely static throughout the video,\n3. the video includes real world images, not synthetically generated ones.\nTo that end, we have devised an efficient, automatic framework to perform this task; the steps are summarised in Alg. 1.\nThe face detector of [16, 30] is applied to the first frame of the video. If there is no detection, the video is discarded, otherwise the bounding box obtained initialises a modelfree tracker. Given the state of the first frame, a modelfree tracker determines the state of the subsequent frames, while no prior information about the object is provided. The tracker should adapt to any appearance, deformation changes, which constitutes a very challenging task, thus an immense amount of diverse techniques has been proposed. In our work, we utilise the SRDCF [12], which provides a decent trade-off of accurate deformable tracking quality and computation complexity [10].\nEven though SRDCF is robust to a wide range of variations, an additional criterion of overlap per frame with the bounding box of the DPM detector is performed. Specifically, we require the bounding boxes of the tracker and the\ndetector to have at least a 50% overlap (intersection over union overlap) in half of the frames, otherwise the clip is discarded. Subsequently, the landmark localisation technique of [46] is employed to obtain the sparse shapes for each face. Due to the object-agnostic nature of the modelfree tracker, we eliminate the few erroneous fittings by learning a statistical function fcl. We utilise a linear patchbased SVM [11] as the classifier fcl(I, l) which accepts a frame I along with the respective fitting l and returns a binary decision on whether this is an acceptable fitting. The classifier fulfils the first requirement for every frame, i.e. that a face is present.\nThe requirement of non-static faces is fulfilled by computing the optical flow [15] in the accepted frames and requiring that there is at least a pixel movement from frame to frame. If the average movement per pixel is above a threshold, the video is discarded.\nThis framework can be adapted for different type of objects with two minor modifications. The modifications are: (i) the face detection module, which can be trivially replaced by a generic detector like [17], (ii) the classifier module for the removal of erroneous fittings, which should be trained for the task, e.g. to accept the whole bounding box instead of the patch-based SVM utilising the landmarks.\nThe aforementioned framework was utilised to create 2MF 2 (2 million frames of faces). 2MF 2 consists of 1150 videos, with 2,1 million accepted frames that contain a human face. Exemplar frames of few videos are visualised in Fig. 2, while an accompanying video depicting accepted frames along with the derived sparse shape can be found in https://youtu.be/Mz0918XdDew."}, {"heading": "5. Experiments", "text": "In this section, the technical implementation details along with the conducted experiments are developed. In the following paragraphs we explain few implementation details, summarise a validation experiment for our method with a simple Gaussian blur, compare with the state-of-theart methods for deblurring in two different scenarios, which include motion blur and real world blurred images."}, {"heading": "5.1. Implementation details", "text": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].\nThe shapes of the public datasets with 68 facial points mark-up annotation, i.e. IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.4, b) additional input to the network for training. Few images with severe distortions were ex-\nAlgorithm 1: The automatic framework as introduced in Sec. 4 to create the 2MF 2 dataset.\nInput : Video frames V = [I(1), I(2), . . . , I(M)] Output : Accepted frames F , Landmarks L Initialize: F = [],L = [], cnt over = 0 /* detection in the first frame. */ 1 faces = face-detection(I(1)) 2 if length(faces) == 0 then 3 return F ,L 4 end /* bb: tracker\u2019s bounding box. */ 5 bb = faces[0] /* main tracking loop. */ 6 for idx = 1 to M do 7 faces = face-detection(I(idx)) 8 bb = track(I(idx), bb) 9 if length(faces) > 0 and\ncompute overlap(faces[0], bb) > 0.5 then 10 cnt over+ = 1 11 end 12 l(idx) = landmark localisation(I(idx), bb)\n/* fcl: classifier to reject the erroneous fittings. */\n13 accept fitting = fcl(I(idx), l(idx)) 14 if accept fitting then 15 append(F , I(idx)) 16 append(L, l(idx))"}, {"heading": "17 end", "text": ""}, {"heading": "18 end", "text": "19 if cnt over < M/2 then 20 return [], [] 21 end 22 return F ,L\ncluded from the training set; the frames of 2MF 2 were subsampled and one every 2nd frame was used for the training.\nThe training steps of the classifier were the following: (a) The positive training samples were extracted from the 300W trainset; perturbed versions of the annotations of those images along with selected images of Pascal dataset [14] were used for mining the negative samples. (b) A fixed size patch was extracted from each positive sample around each of the n landmark points; SIFT [29] were computed per patch. For each negative sample a random perturbation of the ground truth points was performed to create an erroneous fitting prior to extracting the patches. (c) A linear SVM was trained, with its hyper-parameters cross-validated in withheld validation frames.\nFor training our network, we used a mini-batch size of 16; SGD with an exponentially decreasing learning rate (initial value of 0.0003), and decreasing by a factor of 0.5\nevery 15k iterations. The final training consisted of 70k iterations and was completed in a single-core GPU machine. It should be noted that each frame was loaded only once in the network, to avoid over-fitting the training data. Our method functions at 6 fps in a GPU Titan X machine."}, {"heading": "5.2. Self evaluation", "text": "Seventy images of AFLW [25] were used to validate the outcome of the network. The images were synthetically blurred with Gaussian noise, while the standard visual quality metrics of PSNR and SSIM [43] were employed to compare the blurred images with the outputs of our network. The quality metrics are reported in Tab. 1 and few indicative images are visualised in Fig. 3. Both the qualitative and quantiative metrics indicate that the method indeed works well under Gaussian blur."}, {"heading": "5.3. Simulated motion blur", "text": "To extend the simple Gaussian blur, an experiment that simulates the real world blurred images was conducted. A set of sequential frames of a high frame rate video are averaged and simulate the movement of the person. The averaging creates the effect of a dynamic movement, while the middle frame of the averaging can be considered as the ground-truth frame. The edge cases of this simulation con-\nsist of (a) no movement case, (b) extreme movement case. The former case was avoided by considering the optical flow of each two sequential frames and ensuring there is at least some movement in the scene from frame to frame. For the latter case, the PSNR of the averaged frame was compared against the middle frame (ground-truth) and the frames below a threshold were discarded as too noisy.\nIn our experiment, four videos of the 300VW dataset [39] were utilised. All the videos of 300VW include a single person per video, while they are all over 25 fps. For each one of the videos employed, a different number of frames were averaged, ranging from 7 to 11 sequential frames. Also, the recent deblurring methods of Babacan et al. [2], Zhang et al. [44], Pan et al. [32], Pan et al. [33], Pan et al. [34] and Chakrabarti [5] were also included in the experiment. In Fig. 4, the qualitative results of frames with simulated blur are visualised, while in Tab. 2 the quantitative metrics are reported."}, {"heading": "5.4. Real world blurred images", "text": "Providing a method that works for real world blurred images consists a strong motivation for our work. Unfortunately, comparing with real world blurred images comes at the cost of not having any ground-truth image1. Therefore, we opted to report the visual comparisons here.\nIn Fig. 5, the comparisons among different methods are provided for the facial images of Lai et al. [26]. Additionally, to further emphasise the merits of the proposed method, we have gathered few images from internet sources in both indoors and outdoors scenes. The faces in those frames are of quite low-resolution, while there is rapid movement in the scne. The qualitative results are visualised in Fig. 6.\n1Capturing a real world blurry image and a sharp one with an exact correspondence requires an elaborate hardware/software setup, which constitutes the creation of such a dataset challenging. An approximation can be considered by a high frame rate camera and using the middle frame as the ground-truth, however this still does not guarantee the simulation to real world blurred image."}, {"heading": "6. Discussion and conclusions", "text": "In this work, we introduced a new method for deblurring facial images through inserting a weak supervision in the system, but not explicitly enforcing a strict alignment. The architecture that we have implemented is a modified version of the strong performing ResNet. We have also developed a robust framework that allows the creation of large datasets by utilising off-the-shelf tools from the literature. Utilising this framework, we have created 2MF 2, a dataset that in-\ncludes over two million frames of faces. The dataset was utilised to perform the training of our network. A number of experiments are conducted to validate the performance of our method and compare against the state-of-the-art deblurring methods."}], "references": [{"title": "Menpo: A comprehensive platform for parametric image alignment and visual deformable mod-  Figure 6: (Preferably viewed in colour) Qualitative results in real world blurred images from arbitrary videos", "author": ["J. Alabort-i-Medina", "E. Antonakos", "J. Booth", "P. Snape", "S. Zafeiriou"], "venue": "On the top row, the original frame (there is no ground-truth available); on the second row the output of our method. els. In Proceedings of ACM International Conference on Multimedia (ACM\u2019MM), pages 679\u2013682. ACM,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian blind deconvolution with general sparse image priors", "author": ["S.D. Babacan", "R. Molina", "M.N. Do", "A.K. Katsaggelos"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 341\u2013355. Springer,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Localizing parts of faces using a consensus of exemplars", "author": ["P.N. Belhumeur", "D.W. Jacobs", "D.J. Kriegman", "N. Kumar"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 35(12):2930\u20132940,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "C.L. Zitnick", "K. Bala", "R. Girshick"], "venue": "arXiv preprint arXiv:1512.04143,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural approach to blind motion deblurring", "author": ["A. Chakrabarti"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 221\u2013235. Springer,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs", "author": ["S. Chandra", "I. Kokkinos"], "venue": "Proceedings of European Conference on Computer Vision (ECCV). Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic and efficient human pose estimation for sign language videos", "author": ["J. Charles", "T. Pfister", "M. Everingham", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV), 110(1):70\u201390,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Blur identification and image restoration using a multilayer neural network", "author": ["C.-M. Cho", "H.-S. Don"], "venue": "Neural Networks, 1991. 1991 IEEE International Joint Conference on, pages 2558\u20132563. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1991}, {"title": "Video deblurring for handheld cameras using patch-based synthesis", "author": ["S. Cho", "J. Wang", "S. Lee"], "venue": "ACM Trans. Gr., 31(4):64,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A comprehensive performance evaluation of deformable face tracking \u201cin-the-wild", "author": ["G.G. Chrysos", "E. Antonakos", "P. Snape", "A. Asthana", "S. Zafeiriou"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Machine learning, 20(3):273\u2013297,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning spatially regularized correlation filters for visual tracking", "author": ["M. Danelljan", "G. H\u00e4ger", "F. Shahbaz Khan", "M. Felsberg"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 4310\u20134318,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. [Code: http: //tensorflow.org/, Status: Online; accessed 9- November-2016", "author": ["A M"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision (IJCV), 88(2):303\u2013338,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Two-frame motion estimation based on polynomial expansion", "author": ["G. Farneb\u00e4ck"], "venue": "Image analysis, pages 363\u2013370,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Object detection with discriminatively trained partbased models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 32(9):1627\u20131645,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast r-cnn", "author": ["R. Girshick"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 1440\u2013 1448,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deblurring by example using dense correspondence", "author": ["Y. Hacohen", "E. Shechtman", "D. Lischinski"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV), pages 2384\u20132391,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 447\u2013456,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural networks for direct text deblurring", "author": ["M. Hradi\u0161", "J. Kotera", "P. Zemc\u0131\u0301k", "F. \u0160roubek"], "venue": "In Proceedings of British Machine Vision Conference (BMVC),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Robust regression: asymptotics, conjectures and monte carlo", "author": ["P.J. Huber"], "venue": "The Annals of Statistics, pages 799\u2013821,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1973}, {"title": "One millisecond face alignment with an ensemble of regression trees", "author": ["V. Kazemi", "J. Sullivan"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1867\u20131874,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Dlib-ml: A machine learning toolkit", "author": ["D.E. King"], "venue": "The Journal of Machine Learning Research, 10:1755\u20131758,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization", "author": ["M. K\u00f6stinger", "P. Wohlhart", "P.M. Roth", "H. Bischof"], "venue": "Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, pages 2144\u20132151. IEEE,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "A comparative study for single image blind deblurring", "author": ["W.-S. Lai", "J.-B. Huang", "Z. Hu", "N. Ahuja", "M.-H. Yang"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Interactive facial feature localization", "author": ["V. Le", "J. Brandt", "Z. Lin", "L. Bourdev", "T.S. Huang"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 679\u2013 692. Springer,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding and evaluating blind deconvolution algorithms", "author": ["A. Levin", "Y. Weiss", "F. Durand", "W.T. Freeman"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1964\u20131971. IEEE,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision (IJCV), 60(2):91\u2013110,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Face detection without bells and whistles", "author": ["M. Mathias", "R. Benenson", "M. Pedersoli", "L. Van Gool"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 720\u2013735. Springer,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning multi-domain convolutional neural networks for visual tracking", "author": ["H. Nam", "B. Han"], "venue": "IEEE Proceedings  of International Conference on Computer Vision and Pattern Recognition (CVPR). IEEE,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "Deblurring face images with exemplars", "author": ["J. Pan", "Z. Hu", "Z. Su", "M.-H. Yang"], "venue": "Proceedings of European Conference on Computer Vision (ECCV), pages 47\u201362. Springer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Deblurring text images via l0-regularized intensity and gradient prior", "author": ["J. Pan", "Z. Hu", "Z. Su", "M.-H. Yang"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2901\u20132908,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2014}, {"title": "Blind image deblurring using dark channel", "author": ["J. Pan", "D. Sun", "H. Pfister", "M.-H. Yang"], "venue": "prior. 2016", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Total variation blind deconvolution: The devil is in the details", "author": ["D. Perrone", "P. Favaro"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2909\u20132916. IEEE,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "300 faces in-the-wild challenge: Database and results", "author": ["C. Sagonas", "E. Antonakos", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "Image and Vision Computing,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "300 faces in-the-wild challenge: The first facial landmark localization challenge", "author": ["C. Sagonas", "G. Tzimiropoulos", "S. Zafeiriou", "M. Pantic"], "venue": "IEEE Proceedings of International Conference on Computer Vision (ICCV-W), 300 Faces Inthe-Wild Challenge (300-W), pages 397\u2013403,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}, {"title": "The first facial landmark tracking in-the-wild challenge: Benchmark and results", "author": ["J. Shen", "S. Zafeiriou", "G. Chrysos", "J. Kossaifi", "G. Tzimiropoulos", "M. Pantic"], "venue": "IEEE Proceedings of International Conference on Computer Vision, 300 Videos in the Wild (300-VW): Facial Landmark Tracking in-the-Wild Challenge & Workshop (ICCV- W), December", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a convolutional neural network for non-uniform motion blur removal", "author": ["J. Sun", "W. Cao", "Z. Xu", "J. Ponce"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 769\u2013 777. IEEE,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Kernel-free video deblurring via synthesis", "author": ["F. Tan", "S. Liu", "L. Zeng", "B. Zeng"], "venue": "IEEE Proceedings of International Conference on Image Processing (ICIP), pages 2683\u20132687. IEEE,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Identification of image and blur parameters for the restoration of noncausal blurs", "author": ["A. Tekalp", "H. Kaufman", "J. Woods"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, 34(4):963\u2013972,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1986}, {"title": "Image quality assessment: from error visibility to structural similarity", "author": ["Z. Wang", "A.C. Bovik", "H.R. Sheikh", "E.P. Simoncelli"], "venue": "IEEE Transactions in Image Processing (TIP), 13(4):600\u2013612,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-image blind deblurring using a coupled adaptive sparse prior", "author": ["H. Zhang", "D. Wipf", "Y. Zhang"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 1051\u20131058. IEEE,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2013}, {"title": "From actemes to action: A strongly-supervised representation for detailed action understanding", "author": ["W. Zhang", "M. Zhu", "K. Derpanis"], "venue": "IEEE Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR), pages 2248\u20132255,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Face alignment by coarse-to-fine shape searching", "author": ["S. Zhu", "C. Li", "C. Change Loy", "X. Tang"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4998\u20135006,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 41, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 7, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 27, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 31, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 33, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 34, "endOffset": 53}, {"referenceID": 25, "context": "Deblurring has long been studied ([42, 8, 28, 32, 34]), however the results are far from satisfactory ([26]), e.", "startOffset": 103, "endOffset": 107}, {"referenceID": 32, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 27, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 17, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 33, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 96, "endOffset": 112}, {"referenceID": 27, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 218, "endOffset": 226}, {"referenceID": 34, "context": "Nevertheless, rapid progress has been observed in the optimisation-based deblurring techniques ([33, 28, 18, 34]) recently, creditted to a carefully designed choice of priors along with some optimisation restrictions ([28, 35]).", "startOffset": 218, "endOffset": 226}, {"referenceID": 27, "context": "Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.", "startOffset": 75, "endOffset": 87}, {"referenceID": 17, "context": "Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.", "startOffset": 75, "endOffset": 87}, {"referenceID": 34, "context": "Apart from the generic deblurring method which are applied to all objects ([28, 18, 35]), there are also methods that utilise domain-specific knowledge, e.", "startOffset": 75, "endOffset": 87}, {"referenceID": 32, "context": "text or face priors ([33, 32]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 31, "context": "text or face priors ([33, 32]).", "startOffset": 21, "endOffset": 29}, {"referenceID": 31, "context": "The method of [32] is currently the method that explicitly models the blurring for the human face.", "startOffset": 14, "endOffset": 18}, {"referenceID": 35, "context": "The last few years the introduction of elaborate benchmarks [36] allowed CNN methods to surpass the performance of the hand-crafted linear optimisation techniques, e.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 3, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 13, "endOffset": 20}, {"referenceID": 30, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 41, "endOffset": 45}, {"referenceID": 5, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": "in detection [17, 4], modelfree tracking [31], segmentation [6], classification [20].", "startOffset": 80, "endOffset": 84}, {"referenceID": 36, "context": "Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46].", "startOffset": 71, "endOffset": 75}, {"referenceID": 22, "context": "Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46].", "startOffset": 129, "endOffset": 137}, {"referenceID": 45, "context": "Similarly in object alignment the introduction of elaborate benchmarks [37] led to significant progress in landmark localisation [23, 46].", "startOffset": 129, "endOffset": 137}, {"referenceID": 1, "context": "[2] (c) Zhang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[44] (d) Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] (e) Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] (f) Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] (g) Original image", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The difference between deblurring results depending on the type of blur as emphasized in [26] can be visually confirmed.", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "For a more thorough study, the interested reader is redirected to [26].", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "The majority of those works ([9, 41]) implicitly assume that there are multiple frames with approximately the same content and that there exists a sharp patch that matches in content the respective blurry one.", "startOffset": 29, "endOffset": 36}, {"referenceID": 40, "context": "The majority of those works ([9, 41]) implicitly assume that there are multiple frames with approximately the same content and that there exists a sharp patch that matches in content the respective blurry one.", "startOffset": 29, "endOffset": 36}, {"referenceID": 27, "context": "2, they frequently minimise with respect to the blur kernel Kg , which might lead to a blurry \u0128 if a joint MAP (Maximum a posteriori) optimisation is followed ([28]).", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "in [33] apply an `0 norm as a sparse prior on both the intensity values and the image gradient for deblurring text.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "[18] support that the gradient prior alone is not sufficient, and introduce a prior that locates dense correspondences of the blurry image with a similar sharp image, while they iteratively optimise over the correspondence, the kernel and the sharp image estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A generalisation of [18] is the work of [32], which also requires an exemplar dataset to locate an image with a similar contour.", "startOffset": 20, "endOffset": 24}, {"referenceID": 31, "context": "A generalisation of [18] is the work of [32], which also requires an exemplar dataset to locate an image with a similar contour.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "Even though those methods have proven to work well with synthetic blurs, they do not generalise well in realworld blurred images ([26]) due to the strong assumptions of invariance and the simplified format of \u03c8.", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": "Another common attribute of these methods is the iterative optimisation procedure; they are executed in a loop hundreds or even thousands of times to return a deblurred image, which classifies these methods as computationally intensive; some of them require hours for deblurring a single image ([5]).", "startOffset": 295, "endOffset": 298}, {"referenceID": 20, "context": "Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image.", "startOffset": 14, "endOffset": 18}, {"referenceID": 39, "context": "Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image.", "startOffset": 84, "endOffset": 91}, {"referenceID": 4, "context": "Some methods ([21]) learn straight away the function \u03c6 from the data, while others ([40, 5]) learn an estimate and perform non-blind de-blurring/refinement of the sharp image.", "startOffset": 84, "endOffset": 91}, {"referenceID": 20, "context": "In [21], the regularised `2 loss of an up to 15-layer CNN is minimised for text deblurring.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "in [40], learn a CNN to recognise few discretised motion kernels and then perform a non-blind deconvolution in a dense motion field estimate.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "Hence, following similar methods ([40, 5]) we resort to simulating the blur from sharp images.", "startOffset": 34, "endOffset": 41}, {"referenceID": 4, "context": "Hence, following similar methods ([40, 5]) we resort to simulating the blur from sharp images.", "startOffset": 34, "endOffset": 41}, {"referenceID": 19, "context": "We employ the residual network (ResNet) architecture of [20] as the learning component in our method.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "We modify the original ResNet by disabling all the max pooling operation, while skip connections ([19]) are added in the 2 and 3 ResNet blocks.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "The huber loss of [22] is utilised for our loss function.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Among the most successful face detectors is the deformable part models (DPM) detector [16, 30].", "startOffset": 86, "endOffset": 94}, {"referenceID": 29, "context": "Among the most successful face detectors is the deformable part models (DPM) detector [16, 30].", "startOffset": 86, "endOffset": 94}, {"referenceID": 45, "context": "The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 22, "context": "The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 9, "context": "The crude bounding box of the DPM consists the initialisation of a landmark localisation technique [46, 23, 10].", "startOffset": 99, "endOffset": 111}, {"referenceID": 45, "context": "Both techniques [46, 23] belong to the regression based discriminative methods for landmark localisation.", "startOffset": 16, "endOffset": 24}, {"referenceID": 22, "context": "Both techniques [46, 23] belong to the regression based discriminative methods for landmark localisation.", "startOffset": 16, "endOffset": 24}, {"referenceID": 36, "context": "Both methods have proven very accurate in a number of benchmarks [37, 10], hence we adopt the method of Kazemi et al.", "startOffset": 65, "endOffset": 73}, {"referenceID": 9, "context": "Both methods have proven very accurate in a number of benchmarks [37, 10], hence we adopt the method of Kazemi et al.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "due to a publicly available fast implementation [24].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 6, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 44, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 38, "context": "The recent progress in learning tasks depends highly on the elaborate datasets collected for different tasks ([36, 7, 45, 39]).", "startOffset": 110, "endOffset": 125}, {"referenceID": 38, "context": "Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7].", "startOffset": 138, "endOffset": 146}, {"referenceID": 36, "context": "Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7].", "startOffset": 138, "endOffset": 146}, {"referenceID": 6, "context": "Collecting such datasets is an expensive and laborious process, hence there is an increasing effort to create datasets semi-automatically [39, 37] or almost in an unsupervised manner [7].", "startOffset": 183, "endOffset": 186}, {"referenceID": 15, "context": "The face detector of [16, 30] is applied to the first frame of the video.", "startOffset": 21, "endOffset": 29}, {"referenceID": 29, "context": "The face detector of [16, 30] is applied to the first frame of the video.", "startOffset": 21, "endOffset": 29}, {"referenceID": 11, "context": "In our work, we utilise the SRDCF [12], which provides a decent trade-off of accurate deformable tracking quality and computation complexity [10].", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "In our work, we utilise the SRDCF [12], which provides a decent trade-off of accurate deformable tracking quality and computation complexity [10].", "startOffset": 141, "endOffset": 145}, {"referenceID": 45, "context": "Subsequently, the landmark localisation technique of [46] is employed to obtain the sparse shapes for each face.", "startOffset": 53, "endOffset": 57}, {"referenceID": 10, "context": "We utilise a linear patchbased SVM [11] as the classifier fcl(I, l) which accepts a frame I along with the respective fitting l and returns a binary decision on whether this is an acceptable fitting.", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "The requirement of non-static faces is fulfilled by computing the optical flow [15] in the accepted frames and requiring that there is at least a pixel movement from frame to frame.", "startOffset": 79, "endOffset": 83}, {"referenceID": 16, "context": "The modifications are: (i) the face detection module, which can be trivially replaced by a generic detector like [17], (ii) the classifier module for the removal of erroneous fittings, which should be trained for the task, e.", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 19, "context": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "The network was implemented in Tensorflow [13] using the Python API; the pre-trained weights of the network were obtained from the original ResNet paper [20], while the majority of the rest functionality was provided by the Menpo project [1].", "startOffset": 238, "endOffset": 241}, {"referenceID": 37, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 17, "endOffset": 21}, {"referenceID": 2, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 28, "endOffset": 31}, {"referenceID": 36, "context": "IBUG [38], HELEN [27], LFPW [3] and the 300W [37] were utilised for a) training the classifier of Sec.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "The training steps of the classifier were the following: (a) The positive training samples were extracted from the 300W trainset; perturbed versions of the annotations of those images along with selected images of Pascal dataset [14] were used for mining the negative samples.", "startOffset": 229, "endOffset": 233}, {"referenceID": 28, "context": "(b) A fixed size patch was extracted from each positive sample around each of the n landmark points; SIFT [29] were computed per patch.", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "Seventy images of AFLW [25] were used to validate the outcome of the network.", "startOffset": 23, "endOffset": 27}, {"referenceID": 42, "context": "The images were synthetically blurred with Gaussian noise, while the standard visual quality metrics of PSNR and SSIM [43] were employed to compare the blurred images with the outputs of our network.", "startOffset": 118, "endOffset": 122}, {"referenceID": 1, "context": "[2] 25.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[44] 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32] 21.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] 22.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] 23.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "512 Chakrabarti [5] 23.", "startOffset": 16, "endOffset": 19}, {"referenceID": 38, "context": "In our experiment, four videos of the 300VW dataset [39] were utilised.", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "[2], Zhang et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 43, "context": "[44], Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33], Pan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34] and Chakrabarti [5] were also included in the experiment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[34] and Chakrabarti [5] were also included in the experiment.", "startOffset": 21, "endOffset": 24}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 23, "endOffset": 26}, {"referenceID": 43, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 31, "endOffset": 35}, {"referenceID": 31, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 40, "endOffset": 44}, {"referenceID": 32, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 49, "endOffset": 53}, {"referenceID": 33, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 58, "endOffset": 62}, {"referenceID": 4, "context": "(a) GT (b) Blurred (c) [2] (d) [44] (e) [32] (f) [33] (g) [34] (h) [5] (i) Proposed", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 22, "endOffset": 25}, {"referenceID": 43, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 39, "endOffset": 43}, {"referenceID": 31, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 48, "endOffset": 52}, {"referenceID": 33, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 57, "endOffset": 61}, {"referenceID": 4, "context": "(a) Blurred image (b) [2] (c) [44] (d) [33] (e) [32] (f) [34] (g) [5] (h) Proposed", "startOffset": 66, "endOffset": 69}, {"referenceID": 25, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Even though the problem of blind deblurring is a long studied vision task, the outcomes of generic methods are not effective in real world blurred images. Exploiting domain knowledge for specific object deblurring, e.g. text or faces, has attracted recently an increasing amount of attention. Faces are among the most well studied objects in computer vision and despite the significant progress made, the deblurring of faces does not yield yet satisfying results in an arbitrary image. We study the problem of face deblurring by inserting weak supervision in the form of alignment in a deep network. We introduce an efficient framework, used to generate a dataset of over two million frames. The dataset, which we call 2MF , was used during the networks training process. We conduct experiments with real world blurred facial images and report that our method returns a result close to the sharp natural latent image.", "creator": "LaTeX with hyperref package"}}}