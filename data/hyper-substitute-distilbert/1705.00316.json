{"id": "1705.00316", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Apr-2017", "title": "A Conditional Variational Framework for Dialog Generation", "abstract": "deep state variable models there been shown so facilitate autonomous cognitive design for front - domain dialog systems. however, these react variables are highly randomized, leading to rapidly generated responses. in this environment, we propose functional path guiding sustained emotion generation based nationally specific attributes. query conditions can be either highly assigned or periodically detected. sometimes, one dialog response nor some speakers are computed separately any order to retrieve personal features. we support this framework on wildly different scenarios, where the attribute refers general genericness and sentiment responses respectively. the experiment result testified the potential of our model, where meaningful responses must be compared between words alongside generally specified constraint.", "histories": [["v1", "Sun, 30 Apr 2017 13:52:49 GMT  (89kb,D)", "https://arxiv.org/abs/1705.00316v1", "Accepted by ACL2017"], ["v2", "Fri, 26 May 2017 08:08:53 GMT  (89kb,D)", "http://arxiv.org/abs/1705.00316v2", "Accepted by ACL2017"], ["v3", "Wed, 31 May 2017 19:58:49 GMT  (89kb,D)", "http://arxiv.org/abs/1705.00316v3", "Accepted by ACL2017"], ["v4", "Thu, 6 Jul 2017 09:03:45 GMT  (89kb,D)", "http://arxiv.org/abs/1705.00316v4", "Accepted by ACL2017"]], "COMMENTS": "Accepted by ACL2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaoyu shen", "hui su", "yanran li", "wenjie li", "shuzi niu", "yang zhao", "akiko aizawa", "guoping long"], "accepted": true, "id": "1705.00316"}, "pdf": {"name": "1705.00316.pdf", "metadata": {"source": "CRF", "title": "A Conditional Variational Framework for Dialog Generation", "authors": ["Xiaoyu Shen", "Hui Su", "Yanran Li", "Wenjie Li", "Shuzi Niu", "Yang Zhao", "Akiko Aizawa", "Guoping Long"], "emails": ["(suhui15@mails.ucas.ac.cn)", "(xshen@lsv.uni-saarland.de)."], "sections": [{"heading": "1 Introduction", "text": "Seq2seq neural networks, ever since the successful application in machine translation (Sutskever et al., 2014), have demonstrated impressive results on dialog generation and spawned a great deal of variants (Vinyals and Le, 2015; Yao et al., 2015; Sordoni et al., 2015; Shang et al., 2015). The vanilla seq2seq models suffer from the problem of generating too many generic responses (generic denotes safe, commonplace responses like \u201cI don\u2019t know\u201d). One major reason is that the element-wise prediction models stochastical variations only at the token level, seducing the system to gain immediate short rewards and neglect the long-term structure. To\n\u2217Authors contributed equally. Correspondence should be sent to H. Su (suhui15@mails.ucas.ac.cn) and X. Shen (xshen@lsv.uni-saarland.de).\ncope with this problem, (Serban et al., 2017) proposed a variational hierarchical encoder-decoder model (VHRED) that brought the idea of variational auto-encoders (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) into dialog generation. For each utterance, VHRED samples a latent variable as a holistic representation so that the generative process will learn to maintain a coherent global sentence structure. However, the latent variable is learned purely in an unsupervised way and can only be explained vaguely as higher level decisions like topic or sentiment. Though effective in generating utterances with more information content, it lacks the ability of explicitly controlling the generating process.\nThis paper presents a conditional variational framework for generating specific responses, inspired by the semi-supervised deep generative model (Kingma et al., 2014). The principle idea is to generate the next response based on the dialog context, a stochastical latent variable and an external label. Furthermore, the dialog context for both speakers is modeled separately because they have different talking styles, personality and sentiment. The whole network structure functions like a conditional VAE (Sohn et al., 2015; Yan et al., 2016). We test our framework on two scenarios. For the first scenario, the label serves as a signal to indicate whether the response is generic or not. By assigning different values to the label, either generic or non-generic responses can be generated. For the second scenario, the label represents an imitated sentiment tag. Before generating the next response, the appropriate sentiment tag is predicted to direct the generating process.\nOur framework is expressive and extendable. The generated responses agree with the predefined labels while maintaining meaningful. By changing the definition of the label, our framework can\nar X\niv :1\n70 5.\n00 31\n6v 4\n[ cs\n.C L\n] 6\nJ ul\n2 01\n7\nbe easily applied to other specific areas."}, {"heading": "2 Models", "text": "To provide a better dialog context, we build a hierarchical recurrent encoder-decoder with separated context models (SPHRED). This section first introduces the concept of SPHRED, then explains the conditional variational framework and two application scenarios."}, {"heading": "2.1 SPHRED", "text": "We decomposes a dialog into two levels: sequences of utterances and sub-sequences of words, as in (Serban et al., 2016). Let w1, . . . ,wN be a dialog with N utterances, where wn = (wn,1, . . . , wn,Mn) is the n-th utterance. The probability distribution of the utterance sequence factorizes as:\nN\u220f n=1 Mn\u220f m=1 P\u03b8(wm,n|wm,<n,w<n) (1)\nwhere \u03b8 represents the model parameters and w<n encodes the dialog context until step n.\nIf we model the dialog context through a single recurrent neural network (RNN), it can only represent a general dialog state in common but fail to capture the respective status for different speakers. This is inapplicable when we want to infer implicit personal attributes from it and use them to influence the sampling process of the latent variable, as we will see in Section 2.4. Therefore, we model the dialog status for both speakers separately. As displayed in Figure 1, SPHRED contains an encoder RNN of tokens and two status RNNs of utterances, each for one speaker. When modeling turn k in a dialog, each status RNN takes as input the last encoder RNN state of turn k \u2212 2. The\n1 ). When y\nt+1 is known, there exists\nan additional link from yt+1 to z (dashed line). Ct encodes context information up to time t. Dotted lines are posterior approximation Q\u03c6(zn|yn,wn1 ).\nhigher-level context vector is the concatenation of both status vectors.\nWe will show later that SPHRED not only well keeps individual features, but also provides a better holistic representation for the response decoder than normal HRED."}, {"heading": "2.2 Conditional Variational Framework", "text": "VAEs have been used for text generation in (Bowman et al., 2015; Semeniuta et al., 2017), where texts are synthesized from latent variables. Starting from this idea, we assume every utterance wn comes with a corresponding label yn and latent variable zn. The generation of zn and wn are conditioned on the dialog context provided by SPHRED, and this additional class label yn. This includes 2 situations, where the label of the next sequence is known (like for Scenario 1 in Section 2.3) or not (Section 2.4). For each utterance, the latent variable zn is first sampled from a prior distribution. The whole dialog can be explained by the generative process:\nP\u03b8(zn|yn,wn\u221211 ) = N (\u00b5prior,\u03a3prior) (2)\nP\u03b8(wn | yn, zn,wn\u221211 ) = Mn\u220f m=1 P\u03b8(wn,m | yn, zn,wn\u221211 , w n,m\u22121 n,1 )\n(3)\nWhen the label yn is unknown, a suitable classifier is implemented to first predict it from the context vector. This classifier can be designed as, but not restricted to, multilayer perceptrons (MLP) or support vector machines (SVM).\nSimilarly, the posterior distribution of zn is approximated as in Equation 4, where the context\nand label of the next utterance is provided. The graphical model is depicted in Figure 2.\nQ\u03c6(zn|yn,wn1 ) = N (\u00b5posterior,\u03a3posterior) (4)\nThe training objective is derived as in Formula 5, which is a lower bound of the logarithm of the sequence probability. When the label is to be predicted (y\u0304n), an additional classification loss (first term) is added such that the distribution q\u03c6(yn|wn\u221211 ) can be learned together with other parameters.\nlogP\u03b8(w1, . . . ,wN ) \u2265 Ep(wn,yn) [ q\u03c6(yn|wn\u221211 ) ] \u2212\nN\u2211 n=1 KL [ Q\u03c8(zn | wn1 ,yn)||P\u03b8(zn | wn\u221211 , y\u0304n) ] + EQ\u03c8(zn|wn1 ,yn)[logP\u03b8(wn | zn,w n\u22121 1 ,yn)]\n(5)"}, {"heading": "2.3 Scenario 1", "text": "A major focus in the current research is to avoid generating generic responses, so in the first scenario, we let the label y indicate whether the corresponding sequence is a generic response, where y = 1 if the sequence is generic and y = 0 otherwise. To acquire these labels, we manually constructed a list of generic phrases like \u201cI have no idea\u201d, \u201cI don\u2019t know\u201d, etc. Sequences containing any one of such phrases are defined as generic, which in total constitute around 2 percent of the whole corpus. At test time, if the label is fixed as 0, we expect the generated response should mostly belong to the non-generic class.\nNo prediction is needed, thus the training cost does not contain the first item in Formula 5. This scenario is designed to demonstrate our framework can explicitly control which class of responses to generate by assigning corresponding values to the label."}, {"heading": "2.4 Scenario 2", "text": "In the second scenario, we experiment with assigning imitated sentiment tags to generated responses. The personal sentiment is simulated by appending :), :( or :P at the end of each utterance, representing positive, negative or neutral sentiment respectively. For example, if we append \u201c:)\u201d to the original \u201cOK\u201d, the resulting \u201cOK :)\u201d becomes positive. The initial utterance of every speaker is randomly tagged. We consider two rules for the tags of next utterances. Rule 1 confines the sentiment tag to stay constant for both\nspeakers. Rule 2 assigns the sentiment tag of next utterance as the average of the preceding two ones. Namely, if one is positive and the other is negative, the next response would be neutral.\nThe label y represents the sentiment tag, which is unknown at test time and needs to be predicted from the context. The probability q\u03c6(yn|wn\u221211 ) is modeled by feedforward neural networks. This scenario is designed to demonstrate our framework can successfully learn the manually defined rules to predict the proper label and decode responses conforming to this label."}, {"heading": "3 Experiments", "text": "We conducted our experiments on the Ubuntu dialog Corpus (Lowe et al., 2015), which contains about 500,000 multi-turn dialogs. The vocabulary was set as the most frequent 20,000 words. All the letters are transferred to lowercase and the Outof-Vocabulary (OOV) words were preprocessed as <unk> tokens."}, {"heading": "3.1 Training Procedures", "text": "Model hyperparameters were set the same as in VHRED model except that we reduced by half the context RNN dimension. The encoder, context and decoder RNNs all make use of the Gated Recurrent Unit (GRU) structure (Cho et al., 2014). Labels were mapped to embeddings with size 100 and word vectors were initialized with the pubic Word2Vec embeddings trained on the Google News Corpus1. Following (Bowman et al., 2015), 25% of the words in the decoder were randomly dropped. We multiplied the KL divergence and classification error by a scalar which starts from zero and gradually increases so that the training would initially focus on the stochastic latent variables. At test time, we outputted responses using beam search with beam size set to 5 (Graves, 2012) and <unk> tokens were prevented from being generated. We implemented all the models with the open-sourced Python library Tensorflow (Abadi et al., 2016) and optimized using the Adam optimizer (Kingma and Ba, 2014). Dialogs are cut into set of slices with each slice containing 80 words then fed into the GPU memory. All models were trained with batch size 128. We use the learning rate 0.0001 for our framework and 0.0002 for other models. Every model is tested on the val-\n1https://code.google.com/archive/p/ word2vec/\nidation dataset once every epoch and stops until it gains nothing more within 5 more epochs."}, {"heading": "3.2 Evaluation", "text": "Accurate automatic evaluation of dialog generation is difficult (Galley et al., 2015; Pietquin and Hastie, 2013). In our experiment, we conducted three embedding-based evaluations (average, greedy and extrema) (Liu et al., 2016) on all our models, which map responses into vector space and compute the cosine similarity. Though not necessarily accurate, the embedding-based metrics can to a large extent measure the semantic similarity and test the ability of successfully generating a response sharing a similar topic with the golden answer. The results of a GRU language model (LM), HRED and VHRED were also provided for comparison. For the two scenarios of our framework, we further measured the percentage of generated responses matching the correct labels (accuracy). In (Liu et al., 2016), current popular metrics are shown to be not well correlated with human judgements. Therefore, we also carried out a human evaluation. 100 examples were randomly sampled from the test dataset. The generated responses from the models were shuffled and randomly distributed to 5 volunteers2. People were requested to give a binary score to the response from 3 aspects, grammaticality, coherence with history context and diversity. Every response was evaluated 3 times and the result agreed by most people was adopted."}, {"heading": "3.3 Results of Metric-based Evaluation", "text": "As can be seen from Table 1, SPHRED outperforms both HRED and LM over all the three embedding-based metrics. This implies separating the single-line context RNN into two independent parts can actually lead to a better context representation. It is worth mentioning the size of context RNN hidden states in SPHRED is only half of that in HRED, but it still behaves better with fewer parameters. Hence it is reasonable to apply this context information to our framework.\nThe last 4 rows in Table 1 display the results of our framework applied in two scenarios mentioned in Section 2.3 and 2.4. SCENE1-A and SCENE1-B correspond to Scenario 1 with the label fixed as 1 and 0. 90.9% of generated responses\n2All volunteers are well-educated students who have received a Bachelor\u2019s degree on computer science or above.\nin SCENE1-A are generic and 86.9% in SCENE1B are non-generic according to the manually-built rule, which verified the proper effect of the label. SCENE2-A and SCENE2-B correspond to rule 1 and 2 in Scenario 2. Both successfully predict the sentiment with very minor mismatches (0.2% and 0.8%). The high accuracy further demonstrated SPHRED\u2019s capability of maintaining individual context information. We also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sentiment at all because the context information is highly mingled for both speakers. The embedding based scores of our framework are still comparable with SPHRED and even better than VHRED. Imposing an external label didn\u2019t bring any significant quality decline."}, {"heading": "3.4 Results of Human Evaluation", "text": "We conducted human evaluations on VHRED and our framework (Table 3). All models share similar scores, except SCENE1-A receiving lower scores with respect to coherence. This can be explained by the fact that SCENE1-A is trained to generate only generic responses, which limits its power of taking coherence into account. VHRED and Scenario 2 perform close to each other. Scenario 1, due to the effect of the label, receives extreme scores for diversity.\nIn general, the statistical results of human evaluations on sentence quality are very similar between the VHRED model and our framework. This agrees with the metric-based results and supports the conclusion drawn in Section 3.3. Though the sample size is relatively small and human judgements can be inevitably disturbed by subjective factors, we believe these results can shed some light on the understanding of our framework.\nA snippet of the generated responses can be\nseen in Table 2. Generally speaking, SPHRED better captures the intentions of both speakers, while HRED updates the common context state and the main topic might gradually vanish for the different talking styles of speakers. SCENE1-A and SCENE1-B are designed to reply to a given context in two different ways. We can see both responses are reasonable and fit into the right class. The third and fourth rows are the same context with different appended sentiment tags and rules, both generate a suitable response and append the correct tag at the end."}, {"heading": "4 Discussion and future work", "text": "In this work, we propose a conditional variational framework for dialog generation and verify it on two scenarios. To model the dialog state for both speakers separately, we first devised the SPHRED structure to provide the context vector for our framework. Our evaluation results\nshow that SPHRED can itself provide a better context representation than HRED and help generate higher-quality responses. In both scenarios, our framework can successfully learn to generate responses in accordance with the predefined labels. Though with the restriction of an external label, the score of generated responses didn\u2019t significantly decreased, meaning that we can constrain the generation within a specific class while still maintaining the quality.\nThe manually-defined rules, though primitive, represent two most common sentiment shift conditions in reality. The results demonstrated the potential of our model. To apply to real-world scenarios, we only need to adapt the classifier to detect more complex sentiments, which we leave for future research. External models can be used for detecting generic responses or classifying sentiment categories instead of rule or symbolbased approximations. We focused on the controlling ability of our framework, future research can also experiment with bringing external knowledge to improve the overall quality of generated responses."}, {"heading": "5 Acknowledgement", "text": "This work was supported by the National Natural Science of China under Grant No. 61602451, 61672445 and JSPS KAKENHI Grant Numbers 15H02754, 16K12546."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio."], "venue": "arXiv preprint arXiv:1511.06349 .", "citeRegEx": "Bowman et al\\.,? 2015", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv", "citeRegEx": "Galley et al\\.,? 2015", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1211.3711 .", "citeRegEx": "Graves.,? 2012", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980 .", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Semi-supervised learning with deep generative models", "author": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling."], "venue": "Advances in Neural Information Processing Systems. pages 3581\u20133589.", "citeRegEx": "Kingma et al\\.,? 2014", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Autoencoding variational bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "arXiv preprint arXiv:1312.6114 .", "citeRegEx": "Kingma and Welling.,? 2013", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau."], "venue": "arXiv preprint", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems", "author": ["Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau."], "venue": "arXiv preprint arXiv:1506.08909 .", "citeRegEx": "Lowe et al\\.,? 2015", "shortCiteRegEx": "Lowe et al\\.", "year": 2015}, {"title": "A survey on metrics for the evaluation of user simulations", "author": ["Olivier Pietquin", "Helen Hastie."], "venue": "The knowledge engineering review 28(01):59\u201373.", "citeRegEx": "Pietquin and Hastie.,? 2013", "shortCiteRegEx": "Pietquin and Hastie.", "year": 2013}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."], "venue": "arXiv preprint arXiv:1401.4082 .", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "A hybrid convolutional variational autoencoder for text generation", "author": ["Stanislau Semeniuta", "Aliaksei Severyn", "Erhardt Barth."], "venue": "arXiv preprint arXiv:1702.02390 .", "citeRegEx": "Semeniuta et al\\.,? 2017", "shortCiteRegEx": "Semeniuta et al\\.", "year": 2017}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau."], "venue": "AAAI .", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A hierarchical latent variable encoder-decoder model for generating dialogues", "author": ["Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio."], "venue": "AAAI .", "citeRegEx": "Serban et al\\.,? 2017", "shortCiteRegEx": "Serban et al\\.", "year": 2017}, {"title": "Neural responding machine for short-text conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "arXiv preprint arXiv:1503.02364 .", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan."], "venue": "Advances in Neural Information Processing Systems. pages 3483\u20133491.", "citeRegEx": "Sohn et al\\.,? 2015", "shortCiteRegEx": "Sohn et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv preprint", "citeRegEx": "Sordoni et al\\.,? 2015", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems. pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le."], "venue": "arXiv preprint arXiv:1506.05869 .", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee."], "venue": "European Conference on Computer Vision. Springer, pages 776\u2013 791.", "citeRegEx": "Yan et al\\.,? 2016", "shortCiteRegEx": "Yan et al\\.", "year": 2016}, {"title": "Attention with intention for a neural network conversation model", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Baolin Peng."], "venue": "arXiv preprint arXiv:1510.08565 .", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "Seq2seq neural networks, ever since the successful application in machine translation (Sutskever et al., 2014), have demonstrated impressive results on dialog generation and spawned a great deal of variants (Vinyals and Le, 2015; Yao et al.", "startOffset": 86, "endOffset": 110}, {"referenceID": 19, "context": ", 2014), have demonstrated impressive results on dialog generation and spawned a great deal of variants (Vinyals and Le, 2015; Yao et al., 2015; Sordoni et al., 2015; Shang et al., 2015).", "startOffset": 104, "endOffset": 186}, {"referenceID": 21, "context": ", 2014), have demonstrated impressive results on dialog generation and spawned a great deal of variants (Vinyals and Le, 2015; Yao et al., 2015; Sordoni et al., 2015; Shang et al., 2015).", "startOffset": 104, "endOffset": 186}, {"referenceID": 17, "context": ", 2014), have demonstrated impressive results on dialog generation and spawned a great deal of variants (Vinyals and Le, 2015; Yao et al., 2015; Sordoni et al., 2015; Shang et al., 2015).", "startOffset": 104, "endOffset": 186}, {"referenceID": 15, "context": ", 2014), have demonstrated impressive results on dialog generation and spawned a great deal of variants (Vinyals and Le, 2015; Yao et al., 2015; Sordoni et al., 2015; Shang et al., 2015).", "startOffset": 104, "endOffset": 186}, {"referenceID": 14, "context": "cope with this problem, (Serban et al., 2017) proposed a variational hierarchical encoder-decoder model (VHRED) that brought the idea of varia-", "startOffset": 24, "endOffset": 45}, {"referenceID": 7, "context": "tional auto-encoders (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) into dialog generation.", "startOffset": 27, "endOffset": 75}, {"referenceID": 11, "context": "tional auto-encoders (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) into dialog generation.", "startOffset": 27, "endOffset": 75}, {"referenceID": 6, "context": "model (Kingma et al., 2014).", "startOffset": 6, "endOffset": 27}, {"referenceID": 16, "context": "The whole network structure functions like a conditional VAE (Sohn et al., 2015; Yan et al., 2016).", "startOffset": 61, "endOffset": 98}, {"referenceID": 20, "context": "The whole network structure functions like a conditional VAE (Sohn et al., 2015; Yan et al., 2016).", "startOffset": 61, "endOffset": 98}, {"referenceID": 13, "context": "We decomposes a dialog into two levels: sequences of utterances and sub-sequences of words, as in (Serban et al., 2016).", "startOffset": 98, "endOffset": 119}, {"referenceID": 1, "context": "VAEs have been used for text generation in (Bowman et al., 2015; Semeniuta et al., 2017), where texts are synthesized from latent variables.", "startOffset": 43, "endOffset": 88}, {"referenceID": 12, "context": "VAEs have been used for text generation in (Bowman et al., 2015; Semeniuta et al., 2017), where texts are synthesized from latent variables.", "startOffset": 43, "endOffset": 88}, {"referenceID": 9, "context": "We conducted our experiments on the Ubuntu dialog Corpus (Lowe et al., 2015), which contains", "startOffset": 57, "endOffset": 76}, {"referenceID": 2, "context": "text and decoder RNNs all make use of the Gated Recurrent Unit (GRU) structure (Cho et al., 2014).", "startOffset": 79, "endOffset": 97}, {"referenceID": 1, "context": "Following (Bowman et al., 2015), 25% of the words in the decoder were randomly dropped.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "At test time, we outputted responses using beam search with beam size set to 5 (Graves, 2012) and <unk> tokens were prevented from being generated.", "startOffset": 79, "endOffset": 93}, {"referenceID": 0, "context": "We implemented all the models with the open-sourced Python library Tensorflow (Abadi et al., 2016) and optimized using the Adam optimizer (Kingma and Ba, 2014).", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": ", 2016) and optimized using the Adam optimizer (Kingma and Ba, 2014).", "startOffset": 47, "endOffset": 68}, {"referenceID": 3, "context": "Accurate automatic evaluation of dialog generation is difficult (Galley et al., 2015; Pietquin and Hastie, 2013).", "startOffset": 64, "endOffset": 112}, {"referenceID": 10, "context": "Accurate automatic evaluation of dialog generation is difficult (Galley et al., 2015; Pietquin and Hastie, 2013).", "startOffset": 64, "endOffset": 112}, {"referenceID": 8, "context": "In our experiment, we conducted three embedding-based evaluations (average, greedy and extrema) (Liu et al., 2016) on all our models, which map responses into vector space and compute the cosine similarity.", "startOffset": 96, "endOffset": 114}, {"referenceID": 8, "context": "In (Liu et al., 2016), current popular", "startOffset": 3, "endOffset": 21}], "year": 2017, "abstractText": "Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.", "creator": "LaTeX with hyperref package"}}}