{"id": "1705.00464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Speech-Based Visual Question Answering", "abstract": "preceding year recommended that specialty \" speech - based visual problems answering ( vqa ), he is, to interpret an answers given an image called an individual responses question. our work is : first study of display - based vqa with visual intention of offering insights serving businesses such as speech - aware virtual assistants. current step has underway : an easy \u2010 end, deep translation approach that aggressively analyses audio waveforms as input versus a matching approach that performs asr ( automatic speech recognition ) asks the question, followed without error - based visual question assessment. neither respective findings affect this ) communication - based vqa achieves slightly worse results than those extensively - studied vqa with load - free memory and translation ) the end - rear - end model is competitive however though it has a better architecture. furthermore, mathematicians investigate innate robustness of adaptive methods by blending various stimuli into noise into the spoken question and understanding speech - specific stimuli cannot be tolerant / discomfort at reasonable levels. the speech evaluation, recommendations, and supplementary material will be released thank the viewer.", "histories": [["v1", "Mon, 1 May 2017 10:43:28 GMT  (2640kb,D)", "http://arxiv.org/abs/1705.00464v1", "In review"], ["v2", "Sat, 16 Sep 2017 03:43:20 GMT  (2690kb,D)", "http://arxiv.org/abs/1705.00464v2", null]], "COMMENTS": "In review", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["ted zhang", "dengxin dai", "tinne tuytelaars", "marie-francine moens", "luc van gool"], "accepted": false, "id": "1705.00464"}, "pdf": {"name": "1705.00464.pdf", "metadata": {"source": "META", "title": "Speech-Based Visual Question Answering", "authors": ["Ted Zhang", "Dengxin Dai", "Tinne Tuytelaars", "Luc Van Gool"], "emails": ["tedz.cs@gmail.com", "dai@vision.ee.ethz.ch", "tinne.tuytelaars@esat.kuleuven.be", "sien.moens@cs.kuleuven.be", "vangool@vision.ee.ethz.ch"], "sections": [{"heading": "1 INTRODUCTION", "text": "The recent years have witnessed great advances in computer vision, natural language processing, and speech recognition thanks to the advances in deep learning [17] and abundance of data [3, 24]. This is evidenced not only by the surge of academic papers, but also by the world-wide industry interests. The convincing successes in these individual fields naturally raise the potentials of further integration towards solutions to more general AI problems. Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3]. However, the importance of integrating vision and speech has remained relatively unexplored.\nPertaining to practical applications, voice-user interface (VUI) has become more commonplace, and people are increasingly taking advantage of its characteristics; it is natural, hands-free, eyes-free, far more mobile and even faster than typing on certain devices [23]. As many of our daily tasks are relevant to visual scenes, there is a strong need to have a VUI to talk to pictures or videos directly, be it for communication, cooperation, or guidance. Speech-based VQA can be used to assist blind people (c.f. Figure 1) in performing ordinary tasks, and to dictate robotics in real visual scenes in a hand-free manner such as clinical robotic surgery.\n1Coming soon!\nThis work investigates the potential of integrating vision and speech in the context of VQA. We synthetic generate a spoken version of the VQA 1.0 dataset to study two different methods of speech-based question answering. One method is an end-to-end approach based on a deep neural network architecture, and the other uses an ASR to first transcribe the text from the spoken question, as shown in Figure 1. The former approach is particularly useful for languages that are not serviced by popular ASR systems, i.e. minor languages that have scarce text-speech aligned training data.\nThe main contributions of this paper are three-fold: 1) We introduce an end-to-end model that directly produces answers from auditory input, without transformations into intermediate pre-learned representations, and compare this with a pipelined approach that first converts speech to text. 2) We inspect the performance impact of having different levels of background noise mixed with the original utterances. 3) We release the speech dataset, roughly 200 hours of synthetic audio data and 1 hour of real speech data, to the public.\nar X\niv :1\n70 5.\n00 46\n4v 1\n[ cs\n.C L\n] 1\nM ay\n2 01\n7\nTo be certain, readers will notice that our models are simple. The emphasis of this paper is not on achieving the best performance for either the end-to-end nor the pipelined approach. Rather, it is to provide a baseline, to induct speech-based visual question answering into the multimedia research domain, and to provide some insights in this topic.\nThe paper is structured as follows: Section 2 introduces the related work. Section 3 is devoted to our methods, which is followed by Section 4 for our data. Section 5 then presents our experimental settings, followed by Section 6 for discussions. Finally, Section 7 concludes the paper."}, {"heading": "2 RELATED WORKS", "text": "Our work is generally relevant to visual question answering, the integration of vision and speech, and end-to-end speech recognition."}, {"heading": "2.1 Visual Question Answering", "text": "The initial introduction of VQA into the AI community [6, 19] was motivated by a desire to build intelligent systems that can understand the world more holistically. In order to complete the task of VQA, it was both necessary to understand a textual question and a visual scene. However, it was not until the introduction of VQA 1.0 [3] that the application took mainstream in the computer vision and natural language processing (NLP) communities.\nRecently, popular topics of exploration have been on the development of attention models. Attention models were popularized by its success with the NLP community in machine translation [5]. [Elaborate if needed on attention] They eventually found a place in the computer vision community in works such as [20, 28, 32]. Within the context of visual question answering, attention mechanisms \u2018show\u2019 a model where to look when answering a question. Stacked Attention Network [33] learns an attention mechanism based on the of the question\u2019s encoding to determine the salient regions in an image. More sophisticated attention-centric models such as [18, 21, 30] were since then developed.\nOther points of research are based on the pooling mechanism that combines the language component with the vision components. Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al. [18] and Fukui et al. [9] have shown much success in using more complex methods. Our work differs from theirs in that we do not try to improve the performance of the VQA model by adding attention mechanisms or augmenting the pooling techniques."}, {"heading": "2.2 Integration of Speech and Vision", "text": "The works also relevant to ours are those integrating speech and vision. Pixeltone [16] and Image spirit [7] are examples that use voice commands to guide image processing and semantic segmentation. There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions. Their tasks and algorithms are both different from ours. We study the potential of integrating speech and vision in the context of VQA and aim to learn a joint understanding of speech and vision. Those approaches, however, use speech recognition for data collection or result refinement.\nOur work also shares similarity with visual-grounded speech understanding or recognition. The closest one in this vein is [12], in\nwhich a deep model is learned with speeches about image captions for speech-based image retrieval. In a broader context of integration of sound and vision, Soundnet [4] transfers visual information into sound representations, but this differs from our work because their end goal is to label a sound."}, {"heading": "2.3 End-To-End Speech Recognition", "text": "In the past decade, deep learning has allowed many fields in artificial intelligence to replace traditional hand-crafted features and pipeline systems with end-to-end models. Since speech recognition is typically thought of as a sequence to sequence transduction problem, i.e. given an input sequence, predict an output sequence, the application of LSTM and the CTC [8, 11] promptly showed the sucess needed to justify its superiority over traditional methods. Current state of the art ASR systems such as DeepSpeech2 [2] uses stacked Bi-directional Recurrent Neural Networks (RNNs) in conjunction with Convolutional Neural networks (CNNs) because these deep neural architectures all share the property of being able to perform back propagation. While our work does not attempt sequence to sequence prediction nor make use of CTCs, our approach and the integration of speech with vision is nevertheless made possible by the same underlying principle of the works listed above."}, {"heading": "3 MODEL", "text": "Two models are employed in this work, they will be referred to henceforth as TextMod and SpeechMod. TextMod and SpeechMod only differ in their language components, keeping rest of the architecture the same. On the language side, TextMod takes as input a series of one hot encodings, followed by an embedding layer that is learned from scratch, followed by an LSTM encoder, and finished with a dense layer. Architecturally, it is similar to [3] with some minor adjustments.\nThe language side of SpeechMod takes as input the raw waveform, and pushes it through a series of 1D convolutions. The main consideration taken in choosing the convolution layer parameters is that the last convolution should output dimensions of (x, 512), when\nx must be a positive number but not too large, because we treat this as a sequence to be fed into a LSTM. Sequences which are too long generally make it difficult for LSTMs to extract meaningful patterns. The exact dimensions of the convolution components are shown in Table 1, using an example waveform of an audio clip spanning two seconds, sampled at 16 kHz. The maximum length of a sample in our dataset is 107360 (6.7 seconds), and minimum is 10080 (0.63 seconds), which corresponds to the final conv layer outputting (13, 512) and (1, 512) respectively. Both of these can be reasonably handled by an LSTM. The LSTM serves to squash the variable lengthed outputs to a single (512) dimensional output, allowing our model to accept variable lengths of waveform inputs.\nOn the visual side, both models start by taking as input the 4096 dimensional vector of the last layer of VGG19 [26] followed by a single dense layer. After obtaining both visual and language side, they are merged using element-wise multiplication, a dense layer, and an output layer. The full architecture of both these models are seen in Figure 2, where is the symbol for element-wise multiplication."}, {"heading": "4 DATA", "text": "We chose to use VQA 1.0 Open-Ended dataset, for its numerous training examples and familiarity to those working in question answering. To avoid ambiguity, from here onwards VQA 1.0 refers to the dataset, and VQA refers to the task of visual question answering. The dataset contains 248,349 questions in the training set, 121,512 in validation set, and 60,864 in the test-dev set. The complete test set contains 244,302 question, but because the evaluation server allows for only one submission, we instead evaluate on test-dev, which has no such limit. Both train + val are used during the training but questions which do not contain the 1000 most common answers are filtered out. This leaves 320,029 questions for training.\nAmazon Polly API 2 is used to generate audio files for each question. The generated speech is in mp3 format, then sampled into waveform format at 16 kHz. 16 kHz was chosen due to its common usage among the speech community, but also because the model used to transcribe speech was trained on 16 kHz audio waveforms. It is worthwhile to note that the audio generator uses a female voice, thus the training and testing data are all with the same voice, except for the examples weve recorded, which is covered below. The full Amazon Polly speech dataset will be made available to the public.\nThe noise we mixed with the original speech files is selected randomly from the Urban8K dataset [25]. This dataset contains 10 categories: air conditioner, car horn, children playing, dog bark, drilling, enging idling, gun shot, jackhammer, siren, and street music. Some clips are soft enough in volume and thus considered background noise, others are loud enough to be considered foreground noise. For each original audio file, a random noise file is selected, and combined to produce a corrupted question file according to the weighting scheme:\nWcorrupted = (1 \u2212 NL) \u2217Wor i\u0434inal + NL \u2217Wnoise where NL is the noise level. The noise audio files are subsampled to 16 kHz in order to match that of the original audio file, and is clipped to also match the spoken question length. When the spoken question is longer than the noise file, the noise file is repeated until\n2The voice of Joanna is used: https://aws.amazon.com/polly/\nits duration exceeds that of the spoken question. Both files are normalized before being combined so that contributions are strictly proportional to the noise level chosen. We choose 5 noise levels to mix together: 10%-50%, at 10% intervals. Anything beyond 50% is unrealistic. A visualization of different noise levels can be seen in Figure 3 and its corresponding audio clips can be found online.3\nWe also make an additional, supplementary study of the practicality of speech-based VQA with real data. 1000 questions from the val set were randomly selected and recorded with human speakers. Two speakers (one male and one female) participated the recording task. In total, 1/3 of the data is from a male speaker, the rest is from a female speaker. Both speakers are graduate students who are not native anglophones. The data was recorded in an office environment, and there are various background noises in the audio clips as they naturally occurred."}, {"heading": "5 EXPERIMENTS", "text": "For SpeechMod, the only preprocessing step is to scale each waveform to a range of [-256, 256] as done by [4]. There was no need to center each example around 0, as they are already centered.\n3https://soundcloud.com/ted-zhang-443415306/sets/speechvqa\nFor TextMod, the standard preprocessing steps from [3] were followed. This tokenizes each word, and replaces each word with a number that corresponds to the words index. These number indices are used as input, since the question will be fed to the model as a sequence of one hot encodings. Because questions have different lengths, an encoding mask of 0 is used as padding for sequences that are too short. The 0 encoding essentially causes the model to skip that position. 0 is also used for unseen tokens, which is especially when there are out of vocabulary words during evaluation.\nWe use Kaldi [22] for ASR, due to its open-source codebase and popularity with the speech research community. There are many pretrained models available, but the one used in this work is a chain model, released by API.ai 4. Conveniently, it is trained on assistant.ai logs (essentially short commands) making it suitable to predict short utterances such as the questions in the VQA 1.0.\nWord error rate (WER) is used to measure the accuracy of speech to text transcriptions. WER is defined as follows:\nWER = (S + D + I )/N\nWhere S is the number of substitutions, D is the number of deletions, and I is the number of insertions. N is the total number of words in the sentence being translated. Each transcribed question is compared with the original; the results are shown in Table 2. WER is not expected to be a perfect measure of transcription accuracy, since some words are more essential to the meaning of a sentence than other words. E.g. missing the word dog in the sentence what is the dog eating is more detrimental than missing the word the, but we nevertheless employ it to convey a general notion of how many words are understood by the ASR. Naturally the more noise there is, the higher the word error rate becomes. Due to transcription errors, there are resulting questions that contain words not seen in the original datasets. These words, as mentioned above, are indexed as 0 and are masked when fed into the system.\nKeras was used to run all experiments, with the adam optimizer for both architectures. No parameter tuning was done; the default\n4https://github.com/api-ai/api-ai-english-asr-model\nadam parameters are as follows: learning rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08, learning rate decay=0.0. Training TextMod for 10 epochs on train + val takes roughly an hour on a Nvidia Titan X, and our best model was taken at 30 epochs. Training SpeechMod for 10 epochs takes roughly 7 hours. The reported model is taken at 30 epochs. The code is made available to the public.5\nResults are reported on test-dev and are trained on train + val, shown in Table 3. The standard format of reporting VQA 1.0 are followed: All is the overall accuracy, Y/N is for questions with yes or no as answers, Number is for questions that are answered by counting, and Other covers the rest.\nTextMod is trained on the original questions, and the best performing model on OQ (original question) is selected. ASR is used on the 0-50% variants to convert the audio question to text, then the selected model from OQ is used to evaluate based on the transcribed text. Likewise, SpeechMod is first trained on audio data with 0% noise, and the strongest model on 0% test-dev is selected. The selected model is used to evaluate on the 10-50% variants.\nBlind denotes no visual information, meaning it removes the visual components while rest of the model stays the same. TextMod Blind is trained and evaluated on the original questions. SpeechMod Blind is trained and evaluated on the 0% noise audio. Conversely, I Only denotes using visual information only, with no linguistic components of the model.\nA graphical version of the table is shown in Figure 4. The constant values of SpeechMod Blind and TextMod Blind are included to show the noise level at which they perform better than their SpeechMod and TextMod counterparts. Visual examples of the two models can be seen in Figure 5.\nFinally, we ran a small, supplementary test on non-synthetic, human generated questions to see if the models would perform differently on real-world audio inputs. The results are shown in Table 4. The 1000 samples were randomly selected from the val set, since ground truth from the test-dev and test are withheld and cannot be evaluated partially on the server. The models were trained on train, and the highest performing on val is selected to use as evaluation on the 1000 samples. The word error rate when transcribing this subset is 38.26%."}, {"heading": "6 DISCUSSION", "text": ""}, {"heading": "6.1 Results", "text": "To logically reason about the results, we make the assumption that spoken language as a modality contains more information than written language. When going from a mode of high information to one\n5https://github.com/zted/sbvqa\nof lesser, i.e. high dimensional spaces to lower dimensional spaces, information must be loss, or be preserved at best. Conversely, when transforming from a mode to another richer in information, information can be preserved easily, but cannot be gained. Thus, running TextMod with OQ can be understood as the upper bound of all the models. Any transformations from the original, clean text to speech can only result in information loss. When the original textual inputs are replaced with transcribed inputs from a recognition system, the performance worsens as adding more noise obfuscates the system\u2019s linguistic understanding.\nMore interesting is to compare the performance of TextMod and SpeechMod while treating audio as their starting point, instead of the original text. They both falter at similar rates with added noise, though TextMod beats SpeechMod by a seemingly consistent margin of 6%-7%. One might imagine SpeechMod to perform better due to its direct optimization and end-to-end training solely for the task, yet this hypothesis does not hold.\nIn the process of reducing the high dimensional audio inputs to the low dimensional class output label, i.e. the answer, the best performing system must be the one that extracts patterns most effectively (provided there are such patterns in the data). TextMod relies heavily on the intermediate ASR system, which typically is magnitudes more complicated than the entire architecture of SpeechMod, and the number of parameters one needs to learn for a speech recognition is also much greater. Many ASRs include feature extractors at the encoding phase, wide and deep central architectures, acoustic models, language models in the decoding phase, and have been trained on thousands of times more data than contained in VQA 1.0. The ASR serves to filter out noise in high dimensions and extract meaningful patterns in the form of text. From the results we must conclude that indirect as it may be, using ASR to obtain text first is more useful and informative than the end-to-end approach.\nIt is important to note the reasoning above and the conclusion does not discourage nor invalidate the end-to-end method. In contrast to the characteristics of the pipelined approach, SpeechMod does\nnot explicitly include mechanisms that learn semantics in language, and the only data it learns from is the given dataset. Bearing in mind its simple architecture and limited amount of data, we find the performance gap of 7% between SpeechMod and TextMod to be well within acceptable limits and to merit further study into the end-to-end methods.\nNext, we direct the reader\u2019s attention to comparing TextMod and SpeechMod against their respective Blind models. It is documented in [3, 10] the type of bias in VQA 1.0. Namely, if the question is understood, there is a good chance of answering correctly without looking at the image. For example, Y/N questions have the answer yes more commonly than no, so the system should guess yes if a question is identified to be a Y/N type. Therefore, Blind tells us how many questions are understood by these two modes of linguistic inputs. When comparing the linguistic only models with their complementary TextMod and SpeechMod, one can be certain that performances falling below the linguistic signifies that the model no longer understands the questions. Furthermore, perceiving the image and a noisy question becomes less informative than perceiving a clean question by itself.\nLastly, we analyze the results from the 1000 recorded samples. Although small in sample size, the human-recorded dataset provides a few insights. For SpeechMod, the performance on synthetic data serves as the upper bound and for TextMod, the performance on the original text also serves the upper bound. Although it is clear that both models have difficulties handling non-synthetic audio inputs,\nSpeechMod performs especially poorly. The synthetic audio sounds monotonous, disinterested, with little silence in between words while the human recorded audio has inflections, emphasis, accents, and pauses. An inspection of the spectrograms 3 confirms this, as the synthetic waveforms contain less variance. Because SpeechMod uses the waveforms as input directly and it has no training data similar to the human recorded samples, during prediction time it is uanble to find salient patterns in the audio clips. By comparison, the pipelined approach has an ASR that is already trained with data containing lots of variance, so it is able to normalize the waveform into a compact, salient textual representation before feeding it into TextMod. From the perspective of TextMod, its linguistic input is only slightly different than the original text it was trained on.\nThere are weaknesses and strengths associated with both approaches. Data is easier to find for the pipelined approach. There are plenty of speech data paired with transcripts, with different speech patterns and levels of noise but it is difficult to find speech data specific to an end application such as question answering. We saw that SpeechMod is sensitive to slight changes in the audio input, and one may expect that it finds patterns that cannot be extracted from textual input. Should the task be sentiment analysis or disambiguation, the end-to-end approach may very well be the better choice. However, VQA 1.0 is an objective dataset; the questions are objective and direct. There is no more information to be gained in the modality of speech that isn\u2019t contained in text, and thus the pipelined approach has the advantage. TextMod is also faster to train and run by many factors, as textual inputs exist in lower dimensional spaces than auditory inputs, and the model contains fewer parameters to learn."}, {"heading": "6.2 Future Work", "text": "One straightforward approach to improving the end-to-end model is by data augmentation. It is widely accepted that effectiveness of neural architectures is data driven, so training with noisy data and different speakers will make the model more robust to inputs during run time. Just as many possibilities exist in improving the architecture. One can add feature extractors, attention mechanisms, or any amalgamation of the techniques in the deep learning mainstream."}, {"heading": "7 CONCLUSION", "text": "We propose speech-based visual question answering and introduce two approaches that tackle this problem, one of which can be trained end-to-end on audio inputs. Despite its simple architecture, the endto-end method works well when the test data has audio signatures comparable to its training data. Both methods suffer performance decreases at similar rates when noise is introduced. A pipelined method using an ASR tolerates varied inputs much better because it normalizes the input variance into text before running the VQA module. We release the speech dataset and invite the multimedia research community to explore the intersection of speech and vision."}], "references": [{"title": "VQA: Visual Question Answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In International Conference on Computer Vision (ICCV)", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "SoundNet: Learning Sound Representations from Unlabeled Video", "author": ["Yusuf Aytar", "Carl Vondrick", "Antonio Torralba"], "venue": "CoRR abs/1610.09001", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "ImageSpirit: Verbal Guided Image Parsing", "author": ["Ming-Ming Cheng", "Shuai Zheng", "Wen-Yan Lin", "Vibhav Vineet", "Paul Sturgess", "Nigel Crook", "Niloy J. Mitra", "Philip Torr"], "venue": "ACM Trans. Graph. 34,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "An Application of Recurrent Neural Networks to Discriminative Keyword Spotting", "author": ["Santiago Fern\u00e1ndez", "Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering", "author": ["Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Connectionist temporal classification: labelling unsegmented sequence  data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 23rd international conference on Machine learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Unsupervised Learning of Spoken Language with Visual Context", "author": ["David Harwath", "Antonio Torralba", "James Glass"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Speech-based annotation and retrieval of digital photographs", "author": ["Timothy J. Hazen", "Brennan Sherry", "Mark Adler"], "venue": "In INTERSPEECH", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Video content annotation using visual analysis and a large semantic knowledgebase", "author": ["A. Hoogs", "J. Rittscher", "G. Stein", "J. Schmiederer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "A Semantics-Based Approach for Speech Annotation of Images", "author": ["D.V. Kalashnikov", "S. Mehrotra", "Jie Xu", "N. Venkatasubramanian"], "venue": "IEEE Trans. Knowl. Data Eng. 23,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "PixelTone: a multimodal interface for image editing", "author": ["Gierad P Laput", "Mira Dontcheva", "Gregg Wilensky", "Walter Chang", "Aseem Agarwala", "Jason Linder", "Eytan Adar"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Mateusz Malinowski", "Mario Fritz"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Dual Attention Networks for Multimodal Reasoning and Matching", "author": ["Hyeonseob Nam", "Jung-Woo Ha", "Jeonghee Kim"], "venue": "CoRR abs/1611.00471", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "The Kaldi Speech Recognition Toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society. IEEE Catalog No.: CFP11SRW- USB", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Speech Is 3x Faster than Typing for English and Mandarin Text Entry on Mobile Devices", "author": ["Sherry Ruan", "Jacob O Wobbrock", "Kenny Liou", "Andrew Ng", "James Landay"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "A Dataset and Taxonomy for Urban Sound Research", "author": ["Justin Salamon", "Christopher Jacoby", "Juan Pablo Bello"], "venue": "In Proceedings of the ACM International Conference on Multimedia, MM \u201914,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In ICLR", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Show&Tell: a semi-automated image annotation system", "author": ["R.K. Srihari", "Zhongfei Zhang"], "venue": "MultiMedia, IEEE 7,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2000}, {"title": "Deep networks with internal selective attention through feedback connections", "author": ["Marijn F Stollenga", "Jonathan Masci", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Show and Tell: A Neural Image Caption Generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Ask, attend and answer: Exploring questionguided spatial attention for visual question answering", "author": ["Huijuan Xu", "Kate Saenko"], "venue": "In European Conference on Computer", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Yukun Zhu", "Ryan Kiros", "Rich Zemel", "Ruslan Salakhutdinov", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "The recent years have witnessed great advances in computer vision, natural language processing, and speech recognition thanks to the advances in deep learning [17] and abundance of data [3, 24].", "startOffset": 186, "endOffset": 193}, {"referenceID": 22, "context": "Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3].", "startOffset": 147, "endOffset": 151}, {"referenceID": 28, "context": "Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "Much work has been done to integrate vision and language, resulting in a wide collection of successful applications such as image/video captioning [29], movie-to-book alignment [35], and visual question answering (VQA) [3].", "startOffset": 219, "endOffset": 222}, {"referenceID": 17, "context": "Pertaining to practical applications, voice-user interface (VUI) has become more commonplace, and people are increasingly taking advantage of its characteristics; it is natural, hands-free, eyes-free, far more mobile and even faster than typing on certain devices [23].", "startOffset": 264, "endOffset": 268}, {"referenceID": 14, "context": "The initial introduction of VQA into the AI community [6, 19] was motivated by a desire to build intelligent systems that can understand the world more holistically.", "startOffset": 54, "endOffset": 61}, {"referenceID": 0, "context": "0 [3] that the application took mainstream in the computer vision and natural language processing (NLP) communities.", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "Attention models were popularized by its success with the NLP community in machine translation [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 21, "context": "[Elaborate if needed on attention] They eventually found a place in the computer vision community in works such as [20, 28, 32].", "startOffset": 115, "endOffset": 127}, {"referenceID": 25, "context": "[Elaborate if needed on attention] They eventually found a place in the computer vision community in works such as [20, 28, 32].", "startOffset": 115, "endOffset": 127}, {"referenceID": 26, "context": "Stacked Attention Network [33] learns an attention mechanism based on the of the question\u2019s encoding to determine the salient regions in an image.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "More sophisticated attention-centric models such as [18, 21, 30] were since then developed.", "startOffset": 52, "endOffset": 64}, {"referenceID": 15, "context": "More sophisticated attention-centric models such as [18, 21, 30] were since then developed.", "startOffset": 52, "endOffset": 64}, {"referenceID": 23, "context": "More sophisticated attention-centric models such as [18, 21, 30] were since then developed.", "startOffset": 52, "endOffset": 64}, {"referenceID": 24, "context": "Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 26, "context": "Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 27, "context": "Many models [31, 33, 34] use an element-wise multiplication to pool these modalities, but Lu et Al.", "startOffset": 12, "endOffset": 24}, {"referenceID": 13, "context": "[18] and Fukui et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[9] have shown much success in using more complex methods.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Pixeltone [16] and Image spirit [7] are examples that use voice commands to guide image processing and semantic segmentation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "Pixeltone [16] and Image spirit [7] are examples that use voice commands to guide image processing and semantic segmentation.", "startOffset": 32, "endOffset": 35}, {"referenceID": 9, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 10, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 11, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 20, "context": "There is also academic work [13\u201315, 27] and an app [1] that use speech to provide image descriptions.", "startOffset": 28, "endOffset": 39}, {"referenceID": 8, "context": "The closest one in this vein is [12], in .", "startOffset": 32, "endOffset": 36}, {"referenceID": 1, "context": "In a broader context of integration of sound and vision, Soundnet [4] transfers visual information into sound representations, but this differs from our work because their end goal is to label a sound.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "given an input sequence, predict an output sequence, the application of LSTM and the CTC [8, 11] promptly showed the sucess needed to justify its superiority over traditional methods.", "startOffset": 89, "endOffset": 96}, {"referenceID": 7, "context": "given an input sequence, predict an output sequence, the application of LSTM and the CTC [8, 11] promptly showed the sucess needed to justify its superiority over traditional methods.", "startOffset": 89, "endOffset": 96}, {"referenceID": 0, "context": "Architecturally, it is similar to [3] with some minor adjustments.", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "On the visual side, both models start by taking as input the 4096 dimensional vector of the last layer of VGG19 [26] followed by a sin-", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": "The noise we mixed with the original speech files is selected randomly from the Urban8K dataset [25].", "startOffset": 96, "endOffset": 100}, {"referenceID": 1, "context": "For SpeechMod, the only preprocessing step is to scale each waveform to a range of [-256, 256] as done by [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "For TextMod, the standard preprocessing steps from [3] were followed.", "startOffset": 51, "endOffset": 54}, {"referenceID": 16, "context": "We use Kaldi [22] for ASR, due to its open-source codebase and popularity with the speech research community.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "I Only [3] 28.", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "It is documented in [3, 10] the type of bias in VQA 1.", "startOffset": 20, "endOffset": 27}, {"referenceID": 6, "context": "It is documented in [3, 10] the type of bias in VQA 1.", "startOffset": 20, "endOffset": 27}], "year": 2017, "abstractText": "This paper introduces the task of speech-based visual question answering (VQA), that is, to generate an answer given an image and an associated spoken question. Our work is the first study of speechbased VQA with the intention of providing insights for applications such as speech-based virtual assistants. Two methods are studied: an end to end, deep neural network that directly uses audio waveforms as input versus a pipelined approach that performs ASR (Automatic Speech Recognition) on the question, followed by text-based visual question answering. Our main findings are 1) speech-based VQA achieves slightly worse results than the extensively-studied VQA with noise-free text and 2) the end-to-end model is competitive even though it has a simple architecture. Furthermore, we investigate the robustness of both methods by injecting various levels of noise into the spoken question and find speech-based VQA to be tolerant of noise at reasonable levels. The speech dataset, code, and supplementary material will be released to the public. 1", "creator": "LaTeX with hyperref package"}}}