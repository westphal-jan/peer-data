{"id": "1705.00597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2017", "title": "Towards well-specified semi-supervised model-based classifiers via structural adaptation", "abstract": "semi - supervised learning occupied an impressive role in large - scale machine discovery. actually preparing matrix data materials ( still available images ) often can improve the modeling learning yield. however, if the machine learning design is needed for their underlying true theoretical structure, consistent model performance could got seriously jeopardized. this issue develops known as performance misspecification. to address this issue, applications focus on generative explanations having propose adequate criterion for detect acceptable reliability of model misspecification problems measuring the performance difference combining models performed using modeling algorithm semi - free learning. then, we propose another routinely learn the generative models before model training to verify sometimes inaccurate generative inference. rigorous phenomena were studied out onto evaluate thus proposed method using two image generated resource sets pascal voc'07 { mir ). our proposed objective should been demonstrated via outperform a variation of state - of - the - hundreds semi - direct learning schemes for using latter task.", "histories": [["v1", "Mon, 1 May 2017 17:26:53 GMT  (63kb)", "http://arxiv.org/abs/1705.00597v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["zhaocai sun", "william k cheung", "xiaofeng zhang", "jun yang"], "accepted": false, "id": "1705.00597"}, "pdf": {"name": "1705.00597.pdf", "metadata": {"source": "CRF", "title": "Towards well-specified semi-supervised model-based classifiers via structural adaptation", "authors": ["Zhaocai Sun", "William K. Cheung", "Xiaofeng Zhang"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n00 59\n7v 1\n[ cs\n.L G\n] 1\nM ay\n2 01"}, {"heading": "1 Introduction", "text": "Semi-supervised learning (SSL) plays an important role in many real world machine learning applications, such as image classification [Papandreou et al., 2015], speech recognition [Cui et al., 2012], and text categorization [Liu et al., 2016; Sun et al., 2012], where the cost of annotating unlabeled data is generally considered too high to afford. SSL tries to make use of additional unlabeled data together with a limited amount of labeled data to enhance model learning accuracy [Zhu et al., 2003], and thus has gained a lot of researchers\u2019 attention [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014]. However, it is generally believed that using additional unlabeled data for SSL does not always guarantee increase in model learning accuracy. Sometimes, it may end up with performance degradation. This phenomenon is known as model misspecification [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014; Bach, 2006] or safe semisupervised learning [Zhou and Li, 2010]. Yet in essence,\nmodel misspecification and safe semi-supervised learning are two concepts proposed independently, despite their common goal of designing SSL approach so that its performance, even in the worst case, is still better than that of the simple supervised learning approach [Li et al., 2016; Li et al., 2017]. For safe semi-supervised learning, most of the existing approaches were proposed for non-parametric models [Li et al., 2016; Li and Zhou, 2011]. For instance, in [Li et al., 2016], they first set a baseline classifier corresponding to the supervised learners in the worst cases, and then further optimize the performance of the classifier using the proposed SSL method. The performance difference between the classifiers learned in supervised and semi-supervised manners is crucial in designing such SSL algorithms.\nApproaches proposed from the perspective of model misspecification are mostly for parametric models [Yang and Priebe, 2011; Loog and Jensen, 2015; Loog, 2016] where the mismatch between the unlabeled data and the employed generative model are often used to guide the model learning. For instance, the mixture model is adopted in [Yang and Priebe, 2011] to represent both labeled and unlabeled data, which then formulates the corresponding Bayes plug-in classifier based on the mixture density functions. The performance of this Bayes plug-in classifier seriously relies on how far the learnt parameters are from the true ones. The performance degradation mainly comes from the model bias and estimation error. While how the unlabeled data could affect the SSL model performance is theoretically analysed in [Yang and Priebe, 2011], how to detect the onset of the model misspecification and how to solve the challenge have not been addressed yet. Along this line, the lower bound and upper bound of the performance of semi-supervised generative models are analyzed in [Fox-Roberts and Rosten, 2014], and then the authors propose to use the ratio of unlabeled data to labeled data to control the model performance. Inspired by these two works, we propose a generative model based approach for addressing the model misspecification issue. Different from the two aforementioned methods, we not only explore how to detect whether the model misspecification occurs, but also propose a model modification method instead of controlling the unlabeled data to be utilized.\nThe remaining of this paper is organized as follows. We first present related work in Section 2 and formulate the\nmodel misspecification problem using generative models in Section 3. The proposed SSL learning approach is described in Section 4. Experimental results based on two image classification datasets are reported in Section 5. We conclude the paper in Section 6."}, {"heading": "2 Related Work", "text": "The difficulty to achieve a reliable semi-supervised learning method has been reported in some earlier works (e.g., [Cozman et al., 2002]). The general believe is that a better SSL model is not always guaranteed even though additional unlabeled data are used for the model learning. There exist a number of factors that may affect the SSL performance such as the quality of the training data as well as the classifier itself [Yang and Priebe, 2011]. Some researchers considered that to be the consequence of a wrong model assumption [Wang et al., 2012]. Practically, it is hard to assume a perfect generative model without any prior knowledge about the unknown data set. In [Zhou and Li, 2010; Li et al., 2017], the underlying challenge is viewed as having some unlabeled data assigned with incorrect labels which are then used to augment the labeled training data set.\nSome recent research works proposed safety-aware mechanisms to restrict the SSL from using risky unlabeled data [Wang and Chen, 2013; Li et al., 2017; Li et al., 2016]. Other than the earlier disagreement based SSL [Bennett et al., 1998], several safe SSL approaches have been proposed, such as S3VMs [Bennett et al., 1998], S4VM [Li and Zhou, 2011] and UMVP [Li et al., 2016] with promising experimental results. However, most of them fall into the non-parametric category. To leverage on their good generalization capabilities, generative models based on SSL are common for many applications such as image classification [Yang et al., 2017]. The two most representative works addressing the model misspecification issue include [Yang and Priebe, 2011] and [Fox-Roberts and Rosten, 2014]. In [Yang and Priebe, 2011], they started with the asymptotic optimal parameters of two generative models obtained using fully supervised learning and fully unsupervised learning, respectively. Then, they proved that if the KL divergence between the distributions generated by the two generative models is small, the SSL performance is less likely to be affected by the addition of unlabeled data. More theoretical analysis was provided in [Fox-Roberts and Rosten, 2014] where the local and global bounds on the divergence of SSL are formulated. Then, they proposed an unbiased generative SSL which is defined based on an unbiased likelihood estimator exponentially controlled by the ratio of the number of labeled data over the total number of data.\nOur work is different from [Yang and Priebe, 2011] and [Fox-Roberts and Rosten, 2014] as follows. We focus more on adaptively modifying the structure of the generative models instead of controlling risky data to be used. Furthermore, we propose a criterion to determinewhether a model misspecification occurs or not. To the best of our knowledge, neither approaches have been considered in the literature."}, {"heading": "3 Model Misspecification Problem", "text": "Given a finite number of labeled data and an infinite number of unlabeled data, it is impossible to directly detect whether the model misspecification occurs or not as the true data distribution is generally unknown. Figure 1 gives an illustrating example of the proposed approach. Assume that the model estimation error is ignored. In Figure 1 (a), the assumed semisupervised generative model (SEM) is misspecified, and its performance, in the worst case, would converge to the classification loss bound L\u2217unsup of SEM learnt in an unsupervised manner if infinite unlabeled data were used. And the unbiased SEM [Fox-Roberts and Rosten, 2014] would converge to the classification loss bound L\u2217sup of SEM learnt in the supervised manner. However, these classification loss bounds are higher than the best classification loss L\u2217opt obtained by a well-specified model. Therefore, the minimum model bias from the learnt SEMs to the true data model can be approximated by L\u2217opt \u2212 L \u2217\nsup which is indicated by the loss difference, i.e., L\u2217sup \u2212 L \u2217\nunsup. Such loss difference can be approximated by the KL distance between two SEMs. If the assumed SEM is well-specified as plotted in Figure 1 (b), the classification loss bound of the SEM, unbiased SEM and the well-specified SEM will be the same, i.e., L\u2217opt = L \u2217 sup = L \u2217\nunsup, in the ideal case. Consequently, the model difference (KL) of two SEMs should be also small. Inspired by this observation, we conjecture that there must exist the model misspecification if the KL between two SEMs is getting large and therefore we can use this value to determine whether a model misspecification occurs or not. The problem is formulated in the following paragraphs.\nLetX := (x1, x2, \u00b7 \u00b7 \u00b7 , xN ) T and Y := (y1, y2, \u00b7 \u00b7 \u00b7 , yN) T denote the training data and the corresponding label, respectively. The set of labeled data and unlabeled data are denoted as Sl := (X,Y ) and Su := (x U 1 , x U 2 , \u00b7 \u00b7 \u00b7 )\nT , respectively. The mixed set of labeled and unlabeled data is denoted as D = Sl \u22c3\nSu. Suppose P (X,Y ) is the true data distribution for D and f(X,Y |\u0398) denote the assumed generative model with its parameter set as \u0398. A supervised generative model is obtained by learning the model parameters that best fit P (X,Y ), written as\nmin \u0398 KL(P (X,Y )\u2016f(X,Y |\u0398)) (1)\nwhereKL(\u00b7) is the Kullback-Leibler divergence, defined as\nKL(P (X,Y )\u2016f(X, Y |\u0398)) = \u222b p(X,Y ) log p(X,Y )\nf(X, Y |\u0398) dx\n(2)\nGiven a generative model f (e.g. GMM) with its parameter set \u0398, a popular semi-supervised learning objective function is given as,\nmax{ \u2211\nxi\u2208Sl\nlog(f(xi, yi|\u0398)) + \u2211\nxj\u2208Su\nlog(f(xj |\u0398))} (3)\nTheorem 1. If |Sl| \u2192 \u221e, |Su| \u2192 \u221e, SSL problem in Eq.3 is equivalent to\nmin \u0398\n{KL(P (X,Y )\u2016f(X, Y |\u0398))+ |Su|\n|Sl| KL(P (X,Y )\u2016f(X|\u0398))}\n(4)\nProof. For brevity, let Nl = |Sl|,Nu = |Su|, \u03bb = Nu/Nl, following the Theorem proposed in [Cozman et al., 2003], the MLE problem in Eq.3 is stated as:\nmax \u0398\n{ 1\n1 + \u03bb E[log f(X, Y |\u0398)] +\n\u03bb\n1 + \u03bb E[log f(X|\u0398)]} (5)\nUsing Eq.2, Eq.5 is equivalent to Eq.4.\nCorollary 1. If \u03bb \u2192 \u221e, SSL problem as Eq. 4 degenerates back to an unsupervised learning problem.\nmin \u0398 KL(P (X,Y )\u2016f(X|\u0398)) (6)\nFor this reason, [Fox-Roberts and Rosten, 2014] proposed a weighting strategy to avoid the model misspecification issue, given as\nmin \u0398 {KL(P (X, Y )\u2016f(X,Y |\u0398)) +KL(P (X,Y )\u2016f(X|\u0398))}\n(7)\nThe effectiveness of this strategy is quite tricky as it seriously relies on whether there are enough labeled data or not. It becomes risky when the labeled data are few. Therefore, we propose a safer strategy as follows. For a generative model f , let \u0398sup denote the parameter set learnt in a supervised manner using Eq. 1. Similarly, \u0398smsup, \u0398usmsup, and\u0398unsup are the solutions to Eqs. 4, 7, and 6, respectively, denoting original SSL models, unbiased SSL models, and unsupervised models. As the data set only contains a finite number of data points, the best estimations\n\u0398\u2217sup,\u0398 \u2217 smsup,\u0398 \u2217 usmsup, and \u0398 \u2217 unsup are only theoretical values. On the finite data set D, \u0398\u0302Sl,Susmsup, \u0398\u0302 Sl,Su usmsup are the solutions to Eqs. 4 and 7 respectively. Let Lf (\u0398) denote the loss of the Bayes plug-in classifier.\nCorollary 2. When Nl \u2192 \u221e, Nu \u2192 \u221e and Nl/Nu \u2192 0, for the ideal solutions \u0398\u2217sup,\u0398 \u2217 smsup,\u0398 \u2217 usmsup, and \u0398 \u2217\nunsup, there is,\nLf (\u0398 \u2217 unsup) = Lf (\u0398 \u2217 smsup) \u2265 Lf (\u0398 \u2217 usmsup) \u2265 Lf (\u0398 \u2217 sup) (8)\nThus, if f is incorrect, the difference between \u0398\u0302usmsup and \u0398\u0302smsup will become larger and larger when more unlabeled data are used for model learning.\nDefinition 1. With Corollay 2, if Lf (\u0398 \u2217 unsup) > Lf (\u0398 \u2217 sup), then f is misspecified.\nTheorem 2. if Lf (\u0398 \u2217 unsup) > Lf (\u0398 \u2217 sup), then \u2203 Sl, s.t.\nlim |Su|\u2192\u221e\nP (Lf (\u0398\u0302 Sl,Su smsup) > Lf (\u0398\u0302 Sl sup)) > 0 (9)\nThis theorem shows that the semi-supervised learning yields degradation with a positive probability when more unlabeled data are introduced. The proof is straightforward and is not given due to the page limitation. By optimiz-\ning Eqs 3 and 7, two distributions f(X,Y |\u0398\u0302smsup) and f(X,Y |\u0398\u0302usmsup) can be acquired. Then, the difference between the original and the unbiased semi-supervised learning can be defined as\nKL(f(X, Y |\u0398\u0302smsup)||f(X, Y |\u0398\u0302usmsup)) (10)\nTheorem 3. If the model f with \u0398 is not misspecified,\nlim |Su|\u2192\u221e KL(f(X,Y |\u0398\u0302smsup)||f(X, Y |\u0398\u0302usmsup)) = 0 (11)\nProof. By Definition 1, if f is not misspecified, there is Lf (\u0398 \u2217 unsup) = Lf (\u0398 \u2217\nsup). Considering Lf (\u0398 \u2217 unsup) \u2265 Lf (\u0398 \u2217 smsup) \u2265 Lf (\u0398 \u2217\nsup) and Lf (\u0398 \u2217 unsup) \u2265 Lf (\u0398 \u2217 usmsup) \u2265 Lf (\u0398 \u2217 sup), there is,\n\u0398\u2217smsup = \u0398 \u2217 usmsup = \u0398 \u2217 sup = \u0398 \u2217 unsup (12)\nSo,\nlim |Su|\u2192\u221e KL(f(X,Y |\u0398\u0302smsup)||f(X, Y |\u0398\u0302usmsup))\n=KL(f(X,Y |\u0398\u2217smsup)||f(X, Y |\u0398 \u2217 usmsup))\n= 0\n(13)\nWith the proposed theorems and corollaries, we have theoretically proved that the correctness of the previous conjectures illustrated in Figure 1 and a well-specified semisupervised model-based classifier is proposed in the next Section."}, {"heading": "4 The Proposed ASKKM Approach", "text": "To alleviate the model misspecification problem, we propose to adapt the model structure. In particular, we focus on kernel k-means model which can be considered as a special case of Gaussian mixture models (GMM), and explore mechanisms\nto learn the model structure and the model parameters to ensure the model to be well-specified. Although differentmodel complexity measures (such as BIC [Watanabe, 2013]) have been proposed to determine the optimal generative models like GMM, these measures were designed primarily for density estimation, and thus cannot be directly applied in the semi-supervised setting to address the misspecification problem. As discussed in Section 3, we adopt the KL divergence between the original and unbiased semi-supervised learning to guide the adaptive model structure learning of a kernel kmeans model. This section presents the proposed adaptive model modification based semi-supervised kernel k-means model (ASKKM for short)."}, {"heading": "4.1 Model Misspecification Criterion", "text": "As illustrated in Figure 1, if the assumed model is misspecified, the original SEM and unbiased SEM converge to different classification loss bound. The difference between classification loss bounds is approximated by the KL divergence between two SEMs. Practically, the discrete KL divergence might be problematic when calculated on the limited number od data points. Therefore, the aggregated classification disagreement is adopted to approximate the bound difference. If the aggregated classification disagreement is large enough, then their KL divergence must be greater than 0 and thus exists model misspecification according to Theorem 3. Denote B\u0398\u0302usmsup (x) and B\u0398\u0302smsup(x) as the Bayes plug-in classifiers for original SEM and unbiased SEM, respectively. The criterion for model misspecification is defined as\nCriterion = \u2211\nxi\u2208Sl\nI(B\u0398\u0302usmsup(xi) 6= B\u0398\u0302smsup(xi)) (14)\nwhere I(\u00b7) is the indicator function. If Criterion is greater than a predefined threshold \u01eb, the corresponding assumed model is determined as misspecified.\nIf model misspecification occurs, we gradually increase the model complexity of the employed semi-supervised generative model by modifying K, i.e., the number of components. Specially, for each labeled training data xi \u2208 Sl, a new label ci is assigned to it if Criterion > \u01eb. The size of new label set C is then larger than that of the given class label set Y , i.e., |C| > |Y |. For the classification task, a mapping function from new label set to the given label set is defined as,\ng(ci) =\n{\nyi if B\u0398\u0302usmsup(xi) = B\u0398\u0302smsup(xi) B\u0398\u0302usmsup\n(xi) if B\u0398\u0302usmsup(xi) 6= B\u0398\u0302smsup(xi) (15)\nWith this function, a new cluster is introduced for the new class label and thus the model structure is adaptively modified."}, {"heading": "4.2 Adaptive Semi-supervised Kernel K-means Model", "text": "For the classification task, a kernel k-means is adopted in the paper, and accordingly Eqs. 4 (original SEM) and Eq.7 (un-\nAlgorithm 1 Adaptive Semi Kernel k-means Model\n1: initializeK with number of the natural classes, K = |Y |; 2: initialize the assignment of unlabeled samples Su based on the\nkernel distance; 3: while no convergence do 4: update Kernel maps and the centroids for original SSL and unbiased SSL respectively; 5: update cluster assignments Z1, Z2; 6: if Z1l 6= Z 2\nl then 7: modify the misspecified model and update K; 8: goto 2; 9: end if 10: end while\nbiased SEM) can be rewritten respectively as\n{\u03d5\u03021, Z\u0302 1} =argmin\n\u03d51,Z 1\n{ \u2211\nxi\u2208Sl\n\u2016 \u03d51(xi)\u2212 \u03d51(\u00b5 k 1) \u2016 2 +\n\u2211 xi\u2208Su z 1 i,k \u2016 \u03d51(xi)\u2212 \u03d51(\u00b5 k 1) \u2016 2} (16)\n{\u03d5\u03022, Z\u0302 2} =argmin\n\u03d52,Z 2\n{ \u2211\nxi\u2208Sl\n\u2016 \u03d52(xi)\u2212 \u03d52(\u00b5 k 2) \u2016 2 +\nNl\nNu +Nl \u2211 xi\u2208Su z 2 i,k \u2016 \u03d52(xi)\u2212 \u03d52(\u00b5 k 2) \u2016 2} (17)\nwhere \u03d51, \u00b5 k 1 and \u03d52, \u00b5 k 2 are respectively the kernel maps and the centroids for the original and weighted semisupervised kernel k-means, and Z1 and Z2 are the cluster assignments with Z1 = Z1l \u222aZ 1 u and Z 2 = Z2l \u222aZ 2 u. That is, if xi is assigned to the k-th cluster according to Eq. 16 or 17, z1i,k = 1 or z 2 i,k = 1. Intuitively speaking, the proposed approach tracks the difference betweenZ1l andZ 2 u. If Z 1 l 6= Z 2 u, Eq. 14 is used to check whether model misspecification occurs or not. Details of the proposed ASKKM is illustrated in Algorithm 1."}, {"heading": "5 Experimental Results", "text": "For experimental evaluation, we evaluate the proposed ASKKM using two image classification data sets, i.e.,\nPASCAL VOC\u201907 [Everingham, 2010] and MIR Flickr [Huiskes and Lew, 2008]. PASCAL VOC\u201907 consists of 9,963 images from 20 classes and 804 annotated tags. Among them, 5,011 images are selected as the training set and the rest form the test set. MIR Flikr contains 25,000 images and 457 tags from 38 classes collected from the Flickr website. Among them, 12,500 images are randomly selected for training and the rest form the test set. For image feature representations, two local features (SIFT, Hue), three global histogram features (RGB, Hsv and Lab) as well as GIST are used to represent each image. We adopted different distance metrics for features of different types. In particular, the Manhattan distance, Euclidean distance and Chisquare distance are used respectively for the histogram features, the GIST features and the local features. The state-ofthe-art SSL algorithms as well as semi-supervised generative models are chosen for model comparison, including S4VM [Li and Zhou, 2011], co-training [Zhou and Li, 2005], semisupervised EM (SEM) [Fox-Roberts and Rosten, 2014] and MKL [Guillaumin et al., 2010]. In addition to utilizing the labels of images, the original MKL also utilizes the tags of images. Therefore, we extend the proposed ASKKM in the same way as what MKL does to utilize the tag information. For performance evaluation metric, we adopt average precision (AP) which is the evaluation criterion used in PASCAL VOC competition, written as\nAP = 1\n11 \u2211 r\u22080,0.1,...,1 Pinterp(r)\nPinterp(r) = max r\u0303:r\u0303\u2265r pr\u0303\nwhere this criterion requires the recall r to take value from 0 to 1 with the step as 0.1, and then sums up the precision over all r and takes the average value. The performance compari-\nson results are reported in the following sections."}, {"heading": "5.1 Verification of Model Structure Modification", "text": "To evaluate how the adaptive modification on model structure could affect the model performance, experiments are performed on \u201ccar\u201d data set of PASCAL VOC\u201907. For this binary classification task, K is fixed to 2 for the original SEM and ubiased SEM model [Fox-Roberts and Rosten, 2014]. However, the true data distributions for image data may contain more than two clusters (components). Therefore, the proposed ASKKM adaptively modifies K once the model misspecification is detected during the experiments. The comparison results are then plotted in Figure 2.\nFrom this figure, it is noticed that the model performance of the original SEM gradually degrades after Nu > 100. The unbiased SEM is better than the original SEM as its AP value, although slightly fluctuates, almost keeps around 0.4 which does not degrade. For the performance of the ASKKM, it slowly increases at the beginning part of the curve. After Nu > 100, its AP value dramatically increases where model misspecification is detected and the model structure is accordingly modified. Then, the ASKKM gradually converge with the addition of unlabeled data. The converged model performance of ASKKM is much better than that of the compared semi-supervised generative models. This verifies that a well-specified semi-supervised models could acquire a superior model performance."}, {"heading": "5.2 Model Performance Comparison", "text": "The model performance evaluation is performed on aforementioned two image data sets. For each class in the data set, we only choose few labeled data, i.e., Nl = 20 for PASCAL VOC\u201907 data set and Nl = 50 for MIR Flikr data\nset. We not only compare our approach with two semisupervised generative models but also with the most representative semi-supervised learning algorithms such as S4VM and co-training. Experimental results are reported in Table 1 and Table 2.\nFrom Table 1, the model performance of the ASKKM is the best on 17 classes out of 20 classes when tag information is not considered. The S4VM achieves the best AP value in class \u201caeroplane\u201d and \u201cbottle\u201d. The MAP value of the ASKKM is 17.2% higher than the second best model S4VM. It is also noticed that semi-supervised SVM based algorithms performs better than the generative model based ones. The superior performance of the proposed approach indicates the superiority of the ASKKM over the rest approaches. If tag information is considered, it is observed that MKL+tag is better than these approaches without the integration of tag informa-\ntion, and this is consistent with our intuition. However, the ASKKM+tag is better than MKL+tag in 16 classes and the overall MAP of the ASKKM is 10.6% higher than that of MKL+tag. This further verify the effectiveness of the proposed approach. Similar observations could be found in the evaluation results on MIR Flickr data set reported in Table 2. From these rigorous experimental results, we can conclude the proposed ASKKM is superior to the state-of-the-art semisupervised learning approaches in terms of average precision and mean average precision."}, {"heading": "6 Conclusion", "text": "To learn a reliable semi-supervised models is of utmost importance. Most of existing works are non-parametric based ones and the generativemodel based approach is seldom studied. This paper first proposes a criterion to judge whether a\nmodel misspecificatoin occurs or not. Then an adaptive semisupervised kernel K-means model (ASKKM) is proposed for the model misspecified problem. At last, we rigorously evaluate the proposed ASKKM on two image classification data sets, i.e., PASCAL VOC\u201907 and MIR Flickr. Promising results demonstrate the efficacy of the proposed approach."}], "references": [{"title": "In Proceedings of the 19th International Conference on Neural Information Processing Systems", "author": ["Francis R. Bach. Active learning for misspecified generalized linear models"], "venue": "pages 65\u201372,", "citeRegEx": "Bach. 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "et al", "author": ["Kristin Bennett", "Ayhan Demiriz"], "venue": "Semi-supervised support vector machines. In Proceedings of NIPS, volume 11, pages 368\u2013374,", "citeRegEx": "Bennett et al.. 1998", "shortCiteRegEx": null, "year": 1998}, {"title": "In Proceedings of the 15th International Florida Artificial Intelligence Research Society Conference", "author": ["Fabio Gagliardi Cozman", "Ira Cohen", "M Cirelo. Unlabeled data can degrade classification performance of generative classifiers"], "venue": "pages 327\u2013331,", "citeRegEx": "Cozman et al.. 2002", "shortCiteRegEx": null, "year": 2002}, {"title": "In Proceedings of Intl Conf on Machine Learning", "author": ["Fabio Gagliardi Cozman", "Ira Cohen", "Marcelo Cesar Cirelo. Semi-supervised learning of mixture models"], "venue": "pages 41\u201365,", "citeRegEx": "Cozman et al.. 2003", "shortCiteRegEx": null, "year": 2003}, {"title": "IEEE Transactions on Audio Speech& Language Processing", "author": ["Xiaodong Cui", "Jing Huang", "Jen Tzung Chien. Multi-view", "multi-objective semi-supervised learning for hmm-based automatic speech recognition"], "venue": "20(7):1923\u20131935,", "citeRegEx": "Cui et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "The pascal visual object classes (voc) challenge", "author": ["Mark Everingham"], "venue": "International Journal of Computer Vision, 88(2):303\u2013338,", "citeRegEx": "Everingham. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "15(1):367\u2013443", "author": ["Patrick Fox-Roberts", "Edward Rosten. Unbiased generative semi-supervised learning. The Journal of Machine Learning Research"], "venue": "January", "citeRegEx": "Fox.Roberts and Rosten. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal semi-supervised learning for image classification", "author": ["M Guillaumin", "J Verbeek", "C Schmid"], "venue": "Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pages 902\u2013909", "citeRegEx": "Guillaumin et al.. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of ACM Sigmm International Conference on Multimedia Information Retrieval", "author": ["Mark J Huiskes", "Michael S Lew. The mir flickr retrieval evaluation"], "venue": "pages 39\u201343,", "citeRegEx": "Huiskes and Lew. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "In Proceedings of International Conference on Machine Learning", "author": ["Yu Feng Li", "Zhi Hua Zhou. Towards making unlabeled data never hurt"], "venue": "pages 1081\u2013 1088,", "citeRegEx": "Li and Zhou. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "author": ["Yu-Feng Li", "James T. Kwok", "Zhi-Hua Zhou. Towards safe semi-supervised learning for multivariate performance measures"], "venue": "pages 1816\u20131822,", "citeRegEx": "Li et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning safe prediction for semi-supervised regression", "author": ["Yu-Feng Li", "Han-Wen Zha", "Zhi-Hua Zhou"], "venue": "Proceedings of the Thirty First AAAI Conference on Artificial Intelligence,", "citeRegEx": "Li et al.. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "IEEE transactions on cybernetics", "author": ["Chien-Liang Liu", "Wen-Hoar Hsaio", "ChiaHoang Lee", "Tao-Hsing Chang", "Tsung-Hsun Kuo. Semi-supervised text classification with universum learning"], "venue": "46(2):462\u2013473,", "citeRegEx": "Liu et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "IEEE transactions on neural networks and learning systems", "author": ["Marco Loog", "Are Charles Jensen. Semi-supervised nearest mean classification through a constrained log-likelihood"], "venue": "26(5):995\u2013 1006,", "citeRegEx": "Loog and Jensen. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "IEEE transactions on pattern analysis and machine intelligence", "author": ["Marco Loog. Contrastive pessimistic likelihood estimation for semi-supervised classification"], "venue": "38(3):462\u2013475,", "citeRegEx": "Loog. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of IEEE International Conference on Computer Vision", "author": ["George Papandreou", "Liang Chieh Chen", "Kevin Murphy", "Alan L. Yuille. Weakly-", "semi-supervised learning of a dcnn for semantic image segmentation"], "venue": "pages 1742\u20131750,", "citeRegEx": "Papandreou et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch-mode active learning with semi-supervised cluster tree for text classification", "author": ["Sun et al", "2012] Zhaocai Sun", "Yunming Ye", "Xiaofeng Zhang", "Zhexue Huang", "Shudong Chen", "Zhi Liu"], "venue": "In Proceddings of International Conferences on Web Intelligence and Intelligent Agent", "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "24(11):1763\u20131772", "author": ["Y. Wang", "S. Chen. Safety-aware semi-supervised classification. IEEE Transactions on Neural Networks", "Learning Systems"], "venue": "Nov", "citeRegEx": "Wang and Chen. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IEEE Transactions on Neural Networks and Learning Systems", "author": ["Y. Wang", "S. Chen", "Z.H. Zhou. New semi-supervised classification method based on modified cluster assumption"], "venue": "23(5):689\u2013702,May", "citeRegEx": "Wang et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Journal of Machine Learning Research", "author": ["Sumio Watanabe. A widely applicable bayesian information criterion"], "venue": "14(Mar):867\u2013897,", "citeRegEx": "Watanabe. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "author": ["Ting Yang", "C.E. Priebe. The effect of model misspecification on semi-supervised classification"], "venue": "33(10):2093\u20132103,", "citeRegEx": "Yang and Priebe. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Neurocomputing", "author": ["Yun Yang", "Zongze Li", "Wei Wang", "Dapeng Tao. An adaptive semi-supervised clustering approach via multiple density-based information"], "venue": "pages \u2013,", "citeRegEx": "Yang et al.. 2017", "shortCiteRegEx": null, "year": 2017}, {"title": "In Proceedings of International Joint Conference on Artificial Intelligence", "author": ["Zhi Hua Zhou", "Ming Li. Semisupervised regression with co-training"], "venue": "pages 908\u2013913,", "citeRegEx": "Zhou and Li. 2005", "shortCiteRegEx": null, "year": 2005}, {"title": "Knowledge and Information Systems", "author": ["Zhi-Hua Zhou", "Ming Li. Semisupervised learning by disagreement"], "venue": "24(3):415\u2013439,", "citeRegEx": "Zhou and Li. 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the International Conference on Machine Learning", "author": ["Xiaojin Zhu", "Zoubin Ghahramani", "John Lafferty. Semi-supervised learning using gaussian fields", "harmonic functions"], "venue": "pages 912\u2013 919,", "citeRegEx": "Zhu et al.. 2003", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 15, "context": "Semi-supervised learning (SSL) plays an important role in many real world machine learning applications, such as image classification [Papandreou et al., 2015], speech recognition [Cui et al.", "startOffset": 134, "endOffset": 159}, {"referenceID": 4, "context": ", 2015], speech recognition [Cui et al., 2012], and text categorization [Liu et al.", "startOffset": 28, "endOffset": 46}, {"referenceID": 12, "context": ", 2012], and text categorization [Liu et al., 2016; Sun et al., 2012], where the cost of annotating unlabeled data is generally considered too high to afford.", "startOffset": 33, "endOffset": 69}, {"referenceID": 24, "context": "SSL tries to make use of additional unlabeled data together with a limited amount of labeled data to enhance model learning accuracy [Zhu et al., 2003], and thus has gained a lot of researchers\u2019 attention [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014].", "startOffset": 133, "endOffset": 151}, {"referenceID": 20, "context": ", 2003], and thus has gained a lot of researchers\u2019 attention [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014].", "startOffset": 61, "endOffset": 114}, {"referenceID": 6, "context": ", 2003], and thus has gained a lot of researchers\u2019 attention [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014].", "startOffset": 61, "endOffset": 114}, {"referenceID": 20, "context": "This phenomenon is known as model misspecification [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014; Bach, 2006] or safe semisupervised learning [Zhou and Li, 2010].", "startOffset": 51, "endOffset": 116}, {"referenceID": 6, "context": "This phenomenon is known as model misspecification [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014; Bach, 2006] or safe semisupervised learning [Zhou and Li, 2010].", "startOffset": 51, "endOffset": 116}, {"referenceID": 0, "context": "This phenomenon is known as model misspecification [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014; Bach, 2006] or safe semisupervised learning [Zhou and Li, 2010].", "startOffset": 51, "endOffset": 116}, {"referenceID": 23, "context": "This phenomenon is known as model misspecification [Yang and Priebe, 2011; Fox-Roberts and Rosten, 2014; Bach, 2006] or safe semisupervised learning [Zhou and Li, 2010].", "startOffset": 149, "endOffset": 168}, {"referenceID": 10, "context": "Yet in essence, model misspecification and safe semi-supervised learning are two concepts proposed independently, despite their common goal of designing SSL approach so that its performance, even in the worst case, is still better than that of the simple supervised learning approach [Li et al., 2016; Li et al., 2017].", "startOffset": 284, "endOffset": 318}, {"referenceID": 11, "context": "Yet in essence, model misspecification and safe semi-supervised learning are two concepts proposed independently, despite their common goal of designing SSL approach so that its performance, even in the worst case, is still better than that of the simple supervised learning approach [Li et al., 2016; Li et al., 2017].", "startOffset": 284, "endOffset": 318}, {"referenceID": 10, "context": "For safe semi-supervised learning, most of the existing approaches were proposed for non-parametric models [Li et al., 2016; Li and Zhou, 2011].", "startOffset": 107, "endOffset": 143}, {"referenceID": 9, "context": "For safe semi-supervised learning, most of the existing approaches were proposed for non-parametric models [Li et al., 2016; Li and Zhou, 2011].", "startOffset": 107, "endOffset": 143}, {"referenceID": 10, "context": "For instance, in [Li et al., 2016], they first set a baseline classifier corresponding to the supervised learners in the worst cases, and then further optimize the performance of the classifier using the proposed SSL method.", "startOffset": 17, "endOffset": 34}, {"referenceID": 20, "context": "Approaches proposed from the perspective of model misspecification are mostly for parametric models [Yang and Priebe, 2011; Loog and Jensen, 2015; Loog, 2016] where the mismatch between the unlabeled data and the employed generative model are often used to guide the model learning.", "startOffset": 100, "endOffset": 158}, {"referenceID": 13, "context": "Approaches proposed from the perspective of model misspecification are mostly for parametric models [Yang and Priebe, 2011; Loog and Jensen, 2015; Loog, 2016] where the mismatch between the unlabeled data and the employed generative model are often used to guide the model learning.", "startOffset": 100, "endOffset": 158}, {"referenceID": 14, "context": "Approaches proposed from the perspective of model misspecification are mostly for parametric models [Yang and Priebe, 2011; Loog and Jensen, 2015; Loog, 2016] where the mismatch between the unlabeled data and the employed generative model are often used to guide the model learning.", "startOffset": 100, "endOffset": 158}, {"referenceID": 20, "context": "For instance, the mixture model is adopted in [Yang and Priebe, 2011] to represent both labeled and unlabeled data, which then formulates the corresponding Bayes plug-in classifier based on the mixture density functions.", "startOffset": 46, "endOffset": 69}, {"referenceID": 20, "context": "While how the unlabeled data could affect the SSL model performance is theoretically analysed in [Yang and Priebe, 2011], how to detect the onset of the model misspecification and how to solve the challenge have not been addressed yet.", "startOffset": 97, "endOffset": 120}, {"referenceID": 6, "context": "Along this line, the lower bound and upper bound of the performance of semi-supervised generative models are analyzed in [Fox-Roberts and Rosten, 2014], and then the authors propose to use the ratio of unlabeled data to labeled data to control the model performance.", "startOffset": 121, "endOffset": 151}, {"referenceID": 2, "context": ", [Cozman et al., 2002]).", "startOffset": 2, "endOffset": 23}, {"referenceID": 20, "context": "There exist a number of factors that may affect the SSL performance such as the quality of the training data as well as the classifier itself [Yang and Priebe, 2011].", "startOffset": 142, "endOffset": 165}, {"referenceID": 18, "context": "Some researchers considered that to be the consequence of a wrong model assumption [Wang et al., 2012].", "startOffset": 83, "endOffset": 102}, {"referenceID": 23, "context": "In [Zhou and Li, 2010; Li et al., 2017], the underlying challenge is viewed as having some unlabeled data assigned with incorrect labels which are then used to augment the labeled training data set.", "startOffset": 3, "endOffset": 39}, {"referenceID": 11, "context": "In [Zhou and Li, 2010; Li et al., 2017], the underlying challenge is viewed as having some unlabeled data assigned with incorrect labels which are then used to augment the labeled training data set.", "startOffset": 3, "endOffset": 39}, {"referenceID": 17, "context": "Some recent research works proposed safety-aware mechanisms to restrict the SSL from using risky unlabeled data [Wang and Chen, 2013; Li et al., 2017; Li et al., 2016].", "startOffset": 112, "endOffset": 167}, {"referenceID": 11, "context": "Some recent research works proposed safety-aware mechanisms to restrict the SSL from using risky unlabeled data [Wang and Chen, 2013; Li et al., 2017; Li et al., 2016].", "startOffset": 112, "endOffset": 167}, {"referenceID": 10, "context": "Some recent research works proposed safety-aware mechanisms to restrict the SSL from using risky unlabeled data [Wang and Chen, 2013; Li et al., 2017; Li et al., 2016].", "startOffset": 112, "endOffset": 167}, {"referenceID": 1, "context": "Other than the earlier disagreement based SSL [Bennett et al., 1998], several safe SSL approaches have been proposed, such as S3VMs [Bennett et al.", "startOffset": 46, "endOffset": 68}, {"referenceID": 1, "context": ", 1998], several safe SSL approaches have been proposed, such as S3VMs [Bennett et al., 1998], S4VM [Li and Zhou, 2011] and UMVP [Li et al.", "startOffset": 71, "endOffset": 93}, {"referenceID": 9, "context": ", 1998], S4VM [Li and Zhou, 2011] and UMVP [Li et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 10, "context": ", 1998], S4VM [Li and Zhou, 2011] and UMVP [Li et al., 2016] with promising experimental results.", "startOffset": 43, "endOffset": 60}, {"referenceID": 21, "context": "To leverage on their good generalization capabilities, generative models based on SSL are common for many applications such as image classification [Yang et al., 2017].", "startOffset": 148, "endOffset": 167}, {"referenceID": 20, "context": "The two most representative works addressing the model misspecification issue include [Yang and Priebe, 2011] and [Fox-Roberts and Rosten, 2014].", "startOffset": 86, "endOffset": 109}, {"referenceID": 6, "context": "The two most representative works addressing the model misspecification issue include [Yang and Priebe, 2011] and [Fox-Roberts and Rosten, 2014].", "startOffset": 114, "endOffset": 144}, {"referenceID": 20, "context": "In [Yang and Priebe, 2011], they started with the asymptotic optimal parameters of two generative models obtained using fully supervised learning and fully unsupervised learning, respectively.", "startOffset": 3, "endOffset": 26}, {"referenceID": 6, "context": "More theoretical analysis was provided in [Fox-Roberts and Rosten, 2014] where the local and global bounds on the divergence of SSL are formulated.", "startOffset": 42, "endOffset": 72}, {"referenceID": 20, "context": "Our work is different from [Yang and Priebe, 2011] and [Fox-Roberts and Rosten, 2014] as follows.", "startOffset": 27, "endOffset": 50}, {"referenceID": 6, "context": "Our work is different from [Yang and Priebe, 2011] and [Fox-Roberts and Rosten, 2014] as follows.", "startOffset": 55, "endOffset": 85}, {"referenceID": 6, "context": "And the unbiased SEM [Fox-Roberts and Rosten, 2014] would converge to the classification loss bound L\u2217sup of SEM learnt in the supervised manner.", "startOffset": 21, "endOffset": 51}, {"referenceID": 3, "context": "For brevity, let Nl = |Sl|,Nu = |Su|, \u03bb = Nu/Nl, following the Theorem proposed in [Cozman et al., 2003], the MLE problem in Eq.", "startOffset": 83, "endOffset": 104}, {"referenceID": 6, "context": "For this reason, [Fox-Roberts and Rosten, 2014] proposed a weighting strategy to avoid the model misspecification issue, given as", "startOffset": 17, "endOffset": 47}, {"referenceID": 19, "context": "Although differentmodel complexity measures (such as BIC [Watanabe, 2013]) have been proposed to determine the optimal generative models like GMM, these measures were designed primarily for density estimation, and thus cannot be directly applied in the semi-supervised setting to address the misspecification problem.", "startOffset": 57, "endOffset": 73}, {"referenceID": 5, "context": "PASCAL VOC\u201907 [Everingham, 2010] and MIR Flickr [Huiskes and Lew, 2008].", "startOffset": 14, "endOffset": 32}, {"referenceID": 8, "context": "PASCAL VOC\u201907 [Everingham, 2010] and MIR Flickr [Huiskes and Lew, 2008].", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "The state-ofthe-art SSL algorithms as well as semi-supervised generative models are chosen for model comparison, including S4VM [Li and Zhou, 2011], co-training [Zhou and Li, 2005], semisupervised EM (SEM) [Fox-Roberts and Rosten, 2014] and MKL [Guillaumin et al.", "startOffset": 128, "endOffset": 147}, {"referenceID": 22, "context": "The state-ofthe-art SSL algorithms as well as semi-supervised generative models are chosen for model comparison, including S4VM [Li and Zhou, 2011], co-training [Zhou and Li, 2005], semisupervised EM (SEM) [Fox-Roberts and Rosten, 2014] and MKL [Guillaumin et al.", "startOffset": 161, "endOffset": 180}, {"referenceID": 6, "context": "The state-ofthe-art SSL algorithms as well as semi-supervised generative models are chosen for model comparison, including S4VM [Li and Zhou, 2011], co-training [Zhou and Li, 2005], semisupervised EM (SEM) [Fox-Roberts and Rosten, 2014] and MKL [Guillaumin et al.", "startOffset": 206, "endOffset": 236}, {"referenceID": 7, "context": "The state-ofthe-art SSL algorithms as well as semi-supervised generative models are chosen for model comparison, including S4VM [Li and Zhou, 2011], co-training [Zhou and Li, 2005], semisupervised EM (SEM) [Fox-Roberts and Rosten, 2014] and MKL [Guillaumin et al., 2010].", "startOffset": 245, "endOffset": 270}, {"referenceID": 6, "context": "For this binary classification task, K is fixed to 2 for the original SEM and ubiased SEM model [Fox-Roberts and Rosten, 2014].", "startOffset": 96, "endOffset": 126}], "year": 2017, "abstractText": "Semi-supervised learning plays an important role in large-scale machine learning. Properly using additional unlabeled data (largely available nowadays) often can improve the machine learning accuracy. However, if the machine learningmodel is misspecified for the underlying true data distribution, the model performance could be seriously jeopardized. This issue is known as model misspecification. To address this issue, we focus on generative models and propose a criterion to detect the onset of model misspecification by measuring the performance difference between models obtained using supervised and semi-supervised learning. Then, we propose to automatically modify the generative models during model training to achieve an unbiased generative model. Rigorous experiments were carried out to evaluate the proposed method using two image classification data sets PASCAL VOC\u201907 and MIR Flickr. Our proposed method has been demonstrated to outperform a number of state-of-the-art semi-supervised learning approaches for the classification task.", "creator": "LaTeX with hyperref package"}}}