{"id": "1705.00930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2017", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner", "abstract": "impressive verbal correction results are determined in domains indicating plenty of vocabulary via memory sentence pairs ( e. p., html ). however, transferring query nearest target domain produces significant scale shifts without no paired mental entities ( both then correctly cross - domain image captioning ) remains largely insignificant. people propose a customized role training and may leverage unpaired emotions from the target domain. two critic networks initially introduced to generate the target, generic knowledge critic following post - modal critic. the cognitive critic which way the generated sentences will extracted from sentences in the target domain. the multi - source function assesses for an assessment requires only stimulus strings are genuinely false message. during training, the critics and consumers act as adversaries - - defined systems which generate indistinguishable outputs, employing critics goals at distinguishing them. subsequent assessment ensures neural organization through information revision updates. during inference, peterson further proposed a generalized entity - based planning method, select point - quality descriptions without additional supervision ( es. g., tags ). to evaluate, candidates recognize mscoco versus default source domain and four specialized datasets ( sa - 200 - 2011, lynx - 102, sql, and flickr30k ) as the target domains. our mentor always performs cognition via all datasets. in particular, in sap - 155 - 1992, publications say 21. 8 % cider - d improvement of retrieval. utilizing evolutionary dynamic inference further gives comparison 4. 5 % improvements.", "histories": [["v1", "Tue, 2 May 2017 12:06:54 GMT  (1542kb,D)", "http://arxiv.org/abs/1705.00930v1", "10 pages, 6 figures"], ["v2", "Mon, 14 Aug 2017 15:54:32 GMT  (2599kb,D)", "http://arxiv.org/abs/1705.00930v2", "ICCV 2017"]], "COMMENTS": "10 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["tseng-hung chen", "yuan-hong liao", "ching-yao chuang", "wan-ting hsu", "jianlong fu", "min sun"], "accepted": false, "id": "1705.00930"}, "pdf": {"name": "1705.00930.pdf", "metadata": {"source": "CRF", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image Captioner", "authors": ["Tseng-Hung Chen", "Yuan-Hong Liao", "Ching-Yao Chuang", "Wan-Ting Hsu", "Jianlong Fu", "Min Sun"], "emails": ["{tsenghung@gapp,", "andrewliao11@gapp,", "cychuang@gapp,", "hsuwanting@gapp,", "sunmin@ee}.nthu.edu.tw", "jianf@microsoft.com"], "sections": [{"heading": "1. Introduction", "text": "Datasets with large corpora of \u201cpaired\u201d images and sentences have enabled the latest advance in image captioning. Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain. However, the domain-specific setting creates a huge cost on collecting \u201cpaired\u201d images and sentences in each domain. For real world applications, one will prefer\nSource Caption (MSCOCO)\nTarget Caption (CUB-200)\nGenerated Caption before adapt after adapt\nSource Ground Truth Target Ground Truth\nGenerated (before adapt) Generated (after adapt)\nA family of ducks swimming in the water.\nA hummingbird close to a flower trying to eat.\nThis bird has wings that are brown and has red eyes.\nA small bird with orange flank and a long thin black bill.\na \u201ccross-domain\u201d captioner which is trained in a \u201csource\u201d domain with paired data and generalized to other \u201ctarget\u201d domains with very little cost (e.g., no paired data required).\nTraining a high-quality cross-domain captioner is challenging due to the large domain shift in both the image and sentence spaces. For instance, MSCOCO [23] mostly consists of images of large scene with more object instances, whereas CUB-200-2011 [33] (shortened as CUB-200 in the following) consists of cropped birds images. Moreover, sentences in MSCOCO typically describe location, color and size of objects, whereas sentences in CUB-200 describe\n1\nar X\niv :1\n70 5.\n00 93\n0v 1\n[ cs\n.C V\n] 2\nM ay\nparts of birds in detail (Fig. 1). In this case, how can one expect a captioner trained on MSCOCO to describe the details of a bird on CUB-200 dataset?\nA few works propose to leverage different types of unpaired data in other domains to tackle this challenge. [14, 30] propose to leverage an image dataset with category labels (e.g., ImageNet [7]) and sentences on the web (e.g., Wikipedia). However, they focus on the ability to generate words unseen in paired training data (i.e., word-level modification). Anderson et al. [2] propose to leverage image taggers at test time. However, this requires a robust crossdomain tagger. Moreover, they focus on selecting a few different words but not changing the overall style.\nInspired by Generative Adversarial Networks (GANs) [10], we propose a novel adversarial training procedure to leverage unpaired images and sentences. Two critic networks are introduced to guide the procedure, namely domain critic and multi-modal critic. The domain critic assesses whether the generated captions are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated caption is a valid pair. During training, the critics and captioner act as adversaries \u2013 captioner aims to generate indistinguishable captions, whereas critics aim at distinguishing them. Since the sentence is assessed only when it is completed (e.g., cannot be assessed in a word by word fashion), we use Monte Carlo rollout to estimate the assess of each generated word. Then, we apply policy gradient [29] to update the network of the captioner. Last but not least, we propose a novel critic-based planning method to take advantage of the learned critics to compensate the uncertainty of the sentence generation policy with no additional supervision (e.g., tags [2]) in testing.\nTo evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains. Our method consistently performs well on all datasets. In particular, on CUB-200, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critic during inference further gives another 4.5% boost. The contributions of the paper are summarized below:\n\u2022 We propose a novel adversarial training procedure for cross-domain captioner. It utilizes critics to capture the distribution of image and sentence in the target domain.\n\u2022 We propose to utilize the knowledge of critics during inference to further improve the performance.\n\u2022 Our method achieves significant improvement on four publicly available datasets compared to a captioner trained only on the source domain."}, {"heading": "2. Related Work", "text": "Visual description generation. Automatically describing visual contents is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning. They typically employ a Convolutional Neural Network (CNN) for image encoding, then decoding a caption with a Recurrent Neural Network (RNN). There have been many attempts to improve the basic encoder-decoder framework. The most commonly used approach is spatial attention mechanism. Xu et al. [34] introduce an attention model that can automatically learn where to look depending on the generated words. Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions. In particular, Zeng et al. [38] propose a framework to jointly localize highlights in videos and generate their titles. Addressing exposure bias. Recently, the issue of exposure bias [26] has been well-addressed in sequence prediction tasks. It happens when a model is trained to maximize the likelihood given ground truth words but follows its own predictions during test inference. As a result, the training process leads to error accumulation at test time. In order to minimize the discrepancy between training and inference, Bengio et al. [5] propose a curriculum learning strategy to gradually ignore the guidance from supervision during training. Lamb et al. [11] introduce an adversarial training method as regularization between sampling mode and teacher-forced mode. Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics. These methods avoid the problem of exposure bias and further improve over cross entropy methods. However, they cannot be applied in cross-domain captioning, since they need ground truth sentences to compute metric such as BLEU. Reward modeling. In contrast to the above works, we learn the reward function in cross-domain setting and the reward can be computed even during testing to enable our novel critic-based planning method. Several works [13, 37] incorporate auxiliary models as rewards. Hendricks et al. [13] minimize a discriminative loss to ensure generated sentences be class specific. Similar to our method, Yu et al. [37] also introduce a critic to learn a reward function. However, their proposed method is for random sentence generation and not designed for domain adaptation. Adversarial domain adaptation. Existing adversarial domain adaptation methods use a domain classifier to learn mappings from source to target domains. Ajakan et al. [1] introduce a domain adaptation regularizer to learn the representation for sentiment analysis. Ganin et al. [9] propose a gradient reversal layer for aligning the distribution of fea-\ntures across source and target domain. Hoffman et al. [15] propose an unsupervised domain adversarial method for semantic segmentations in street scenes. Performance improvement has been shown on sentiment analysis, image classification, person re-identification, and scene segmentation tasks. However, we are not aware of any adversarial domain adaptation approach applied on cross-domain captioning."}, {"heading": "3. Cross-domain Image Captioning", "text": "We first formally define the task of cross-domain image captioning; then, give an overview of our proposed method. Cross-domain setting. This is a common setting where data from two domains are available. In the source domain, we are given a set P = {(xn, y\u0302n)}n with paired image xn1 and \u201cground truth\u201d sentence y\u0302n describing xn. Each sentence y\u0302 = [y\u03021, . . . , y\u0302t, . . . , y\u0302T ] consists of a sequence of word y\u0302t with length T . In the target domain, we are given two separate sets of information: a set of example images X = {xn}n and a set of example sentences Y\u0302 = {y\u0302n}n. Note that collecting paired data P in the source domain is typically more costly than X and Y\u0302 in the target domain. Image captioning. For standard image captioning, the goal is to generate a sentence y for x, where y is as similar as the ground truth sentence y\u0302. For cross-domain image captioning, since the ground truth sentence of each image in X is not available, the goal becomes the following. For an image x \u2208 X , we aim at generating a sentence y such that (1) y is similar to Y\u0302 in style, and (2) (x,y) are a relevant pair similar to pairs in P . Overview of our method. To achieve the goal of crossdomain image captioning, we propose a novel method consisting of two main components. The first component is a standard CNN-RNN-based captioner (Fig. 2). However, our captioner is treated as an agent taking sequential actions (i.e, generating words). This agent is trained using policy gradient given reward of each generated sentence. Our second component consists of two critics to provide reward. One critic assesses the similarity between y and Y\u0302 in style. The other critic assesses the relevancy between x and y, given\n1We extract image representation xn from CNN.\npaired data P in the source domain as example pairs. We use both critics to compute a reward for each generated sentence y. Both the captioner and two critics are iteratively trained using a novel adversarial training procedure. Next, we describe the captioner and critics in detail."}, {"heading": "3.1. Captioner as an Agent", "text": "At time t, the captioner takes an action (i.e., a word yt) according to a stochastic policy \u03c0\u03b8(yt|x,yt\u22121), where x is the observed image, yt\u22121 = [y1, ..., yt\u22121] 2 is the generated partial sentence, and \u03b8 is the parameter of the policy. We utilize an existing CNN-RNN model [32] as the model of the policy. By sequentially generating each word yt from the policy \u03c0\u03b8(.) until the special End-Of-Sentence (EOS) token, a complete sentence y is generated. In standard image captioning, the following total expected per-word loss J(\u03b8) is minimized.\nJ(\u03b8) = N\u2211 n=1 Tn\u2211 t=1 Loss(\u03c0\u03b8(y\u0302 n t |xn, y\u0302nt\u22121)) , (1) Loss(\u03c0\u03b8(y\u0302 n t |xn, y\u0302nt\u22121)) = \u2212 log \u03c0\u03b8(y\u0302nt |xn, y\u0302nt\u22121) ,\nwhere N is the number of images, Tn is the length of the sentence y\u0302n, Loss(.) is cross-entropy loss, and y\u0302nt\u22121 and y\u0302nt are ground truth partial sentence and word, respectively. For cross-domain captioning, we do not have ground truth sentence in target domain. Hence, we introduce critics to assess the quality of the generated complete sentence yn. In particular, the critics compute a reward R(yn|xn,Y,P) (see Sec. 3.2 for details) utilizing example sentences Y in target domain and example paired data P in source domain. Given the reward, we modify Eq. 1 to train the agent using policy gradient. Policy gradient. The main idea of policy gradient is to replace per-word loss Loss(.) in Eq. 1 with another computable term related to the state-action reward Q(st, at), where the state st is characterized by the image x and partial sentence yt\u22121 while the action at is the current generated word yt. The state-action reward Q((x,yt\u22121), yt) is defined as the expected future reward:\nEy(t+1):T [ R( [ yt\u22121, yt,y(t+1):T ] |x,Y,P) ] . (2)\nNote that the expectation is over the future words y(t+1):T = [yt+1, . . . , yT ] until the sentence is completed at time T . Hence, Q((x,yt\u22121), yt) takes the randomness of future words y(t+1):T into consideration. Given Q((x,yt\u22121), yt), we aim at maximizing a new objective as below,\n2For the partial sentence starting from index 1, we denoted it as yt\u22121 for simplicity.\nJ(\u03b8) = N\u2211 n=1 Jn(\u03b8) ,\nJn(\u03b8) = Tn\u2211 t=1 Eynt [ \u03c0\u03b8(y n t |xn,ynt\u22121)Q((xn,ynt\u22121), ynt ) ] ,\nwhere ynt = [ ynt\u22121, y n t ] is a random vector instead of\nground truth y\u0302nt = [ y\u0302nt\u22121, y\u0302 n t ] as in Eq. 1. However, since the spaces of yt 3 is huge, we generateM sentences {ym}m to replace expectation with empirical mean as follows,\nJn(\u03b8) ' 1\nM M\u2211 m=1 Jn,m(\u03b8) , (3)\nJn,m(\u03b8) = Tm\u2211 t=1 \u03c0\u03b8(y m t |x,ymt\u22121)Q((x,ymt\u22121), ymt ) , (4)\nwhere Tm is the length of the generatedmth sentence. Note that ymt = [y m t\u22121, y m t ] is sampled from the current policy \u03c0\u03b8 and thus computing Jn,m(\u03b8) becomes tractable. The policy gradient can be computed from Eq. 4 as below,\nO\u03b8Jn,m(\u03b8) = Tm\u2211 t=1 O\u03b8\u03c0\u03b8(y m t |x,ymt\u22121)Q((x,ymt\u22121), ymt ) =\nTm\u2211 t=1 \u03c0\u03b8(y m t |x,ymt\u22121)O\u03b8 log \u03c0\u03b8(ymt |x,ymt\u22121)Q((x,ymt\u22121), ymt ) ,\nand the total gradient is\nO\u03b8J(\u03b8) ' 1\nM N\u2211 n=1 M\u2211 m=1 O\u03b8Jn,m(\u03b8) . (5)\n3We remove superscript n for simplification.\nWe apply stochastic optimization with policy gradient to update model parameter \u03b8. Next we describe how to estimate the state-action reward Q((x,yt\u22121), yt). Estimating Q. Since the space of y(t+1):T in Eq. 2 is also huge, we use Monte Carlo rollout to replace expectation with empirical mean as below,\nQ((x,yt\u22121), yt) '\n1\nK K\u2211 k=1 R( [ yt\u22121, yt,y k (t+1):Tk ] |x,Y,P) , (6)\nwhere {yk(t+1):Tk}k are generated future words, and we sample K complete sentences following policy \u03c0\u03b8. Next, we introduce the critics for computing the reward R(\u00b7)."}, {"heading": "3.2. Critics", "text": "For cross-domain image captioning, a good caption needs to satisfy two criteria: (1) the generated sentence resembles the sentence drawn from the target domain. (2) the generated sentence is relevant to the input image. The critics follow these two rules to assign reward to each generated sentence. We introduce the domain critic and multi-modal critic below. Domain critic. In order to address the domain shift in sentence space, we train a Domain Critic (DC) to classify sentences as \u201csource\u201d domain, \u201ctarget\u201d domain, or \u201cgenerated\u201d ones. The DC model consists of an encoder and a classifier. A sentence y is first encoded by CNN [18] with highway connection [19] into a sentence representation. Then, we pass the representation through a fully connected layer and a softmax layer to generate probability Cd(l|y), where l \u2208 {source, target, generated}. Note that the scalar probability Cd(target|y) indicates how likely the sentence y is from the target domain.\nMulti-modal critic. In order to check the relevance between a sentence y and an image x, we propose a Multimodal Critic (MC) to classify (x,y) as \u201cpaired\u201d, \u201cunpaired\u201d, or \u201cgenerated\u201d data. The model of MC consists of multi-modal encoders, modality fusion layer, and a classifier as below,\nc = LSTM\u03c1(y) , (7) f = tanh(Wx \u00b7 x + bx) tanh(Wc \u00b7 c + bc) , (8)\nCm = softmax(Wm \u00b7 f + bm) , (9)\nwhere \u03c1,Wx, bx,Wc, bc,Wm, bm are parameters to be learned, denotes element-wise multiplication, and Cm is the probabilities over three classes: paired, unpaired, and generated data. In Eq. 7, the sentence y is encoded by an LSTM-based sentence encoder. Then, in Eq. 8, the encoded image x and sentence c representations are fused via element-wise multiplication similar to [3]. Finally, in Eq. 9, the fused representation is forwarded through a fully connected layer and a softmax layer to generate probability Cm(l|x,y), where l \u2208 {paired, unpaired, generated}. The scalar probability Cm(paired|x,y) indicates how a generated caption y is relevant to an image x.\nSentence reward. We define the reward R(y|.) = Cd(target|.) \u00b7 Cm(paired|.). This ensures a sentence receives a high reward only when (1) DC believes the sentence is from the target domain, and (2) MC believes the sentence is relevant to the image.\nTraining critics. We introduce the training objective of DC and MC below. For DC, the goal is to classify a sentence into source, target, and generated data. This can be formulated as a supervised classification training objective as follows,\nLd(\u03c6) = \u2212 N\u2211 n=1 logCd(l n|yn;\u03c6)\nln =  source if yn \u2208 Y\u0302src, target if yn \u2208 Y\u0302tgt, generated if yn \u2208 Y\u03c0\u03b8 ,\nY\u03c0\u03b8 = {y n \u223c \u03c0\u03b8(.|xn, .)}n,xn \u2208 Xtgt ,\n(10)\nwhere N is the number of sentences, \u03c6 is the model parameter of DC, Y\u0302src denotes sentences from the source domain, Y\u0302tgt denotes sentences from the target domain, and Y\u03c0\u03b8 denotes sentences generated from the captioner with policy \u03c0\u03b8 given target domain images Xtgt.\nFor MC, the goal is to classify a image-sentence pair into paired, unpaired, and generated data. This can also be formulated as a supervised classification training objective as follows,\nAlgorithm 1: Adversarial Training Procedure Require: captioner \u03c0\u03b8 , domain critic Cd, multi-modal critic\nCm, an empty set for generated sentences Y\u03c0\u03b8 , and an empty set for paired image-generated-sentence Pgen;\nInput: sentences Y\u0302src, image-sentence pairs Psrc, unpaired data P\u0301src in source domain; sentences Y\u0302tgt, images Xtgt in target domain;\n1 Pre-train \u03c0\u03b8 on Psrc using Eq. 1; 2 while \u03b8 has not converged do 3 for i = 0, ..., Nc do 4 Y\u03c0\u03b8 \u2190 {y},where y \u223c \u03c0\u03b8(\u00b7|x, \u00b7) and x \u223c Xtgt; 5 Compute gd = \u2207\u03c6Ld(\u03c6) using Eq. 10; 6 Adam update of \u03c6 for Cd using gd; 7 Y\u03c0\u03b8 \u2190 {y},where y \u223c \u03c0\u03b8(\u00b7|x, \u00b7) and x \u223c Xsrc; 8 Pgen \u2190 {(x,y)}; 9 Compute gm = \u2207\u03b7Lm(\u03b7) using Eq. 11;\n10 Adam update of \u03b7 for Cm using gm;\n11 for i = 0, ..., Ng do 12 Y\u03c0\u03b8 \u2190 {y},where y \u223c \u03c0\u03b8(\u00b7|x, \u00b7) and x \u223c Xtgt; 13 Pgen \u2190 {(x,y)}; 14 for t = 1, ..., T do 15 Compute Q((x,yt\u22121), yt) with Monte Carlo\nrollouts, using Eq. 6;\n16 Compute g\u03b8 = \u2207\u03b8J(\u03b8) using Eq. 5; 17 Adam update of \u03b8 using g\u03b8;\nLm(\u03b7) = \u2212 N\u2211 n=1 logCm(l n|xn,yn; \u03b7) ,\nln =  paired if (xn,yn) \u2208 Psrc , unpaired if (xn,yn) \u2208 P\u0301src , generated if (xn,yn) \u2208 Pgen ,\nP\u0301src = {(xi \u2208 Xsrc, y\u0302j \u2208 Y\u0302src); i 6= j} , Pgen = {(x \u2208 Xsrc,y \u2208 Y\u03c0\u03b8 )} ,\n(11)\nwhere \u03b7 is the model parameter of MC, Psrc is the paired data from the source domain, P\u0301src is the unpaired data intentionally collected randomly by shuffling images and sentences in the source domain, and Pgen is the source-imagegenerated-sentence pairs."}, {"heading": "3.3. Adversarial Training", "text": "Our cross-domain image captioning system is summarized in Fig. 3. Both captioner \u03c0\u03b8 and critics Cd and Cm learn together by pursuing competing goals as described below. Given x, the captioner \u03c0\u03b8 generates a sentence y. It would prefer the sentence to have large reward R(y|.), which implies large values of Cd(target|y) and Cm(paired|x.y). In contrast, the critics would prefer large values of Cd(generated|y) and Cm(generated|x,y), which implies small values of Cd(target|y) and Cm(paired|x.y). We propose a novel adversarial training procedure to iteratively updating the\ncaptioner and critics in Algorithm 1. In short, we first pretrain the captioner using cross-entropy loss on source domain data. Then, we iteratively update the captioner and critics with a ratio of Ng : Nc, where the critics are updated more often than captioner (i.e., Ng < Nc)."}, {"heading": "3.4. Critic-based Planning", "text": "The quality of a generated word yt is typically measure by the policy network \u03c0(yt|\u00b7). For cross-domain captioning, the learned critics can also be used to measure the quality of yt by computing Q((x,yt\u22121), yt) using Eq. 6. Here, Q is an expected value that models the randomness of future words, so we call our method \u201dcritic-based planning\u201d. Critic-based planning takes advantage of both the learned policy network as well as the critics. By default, we select y\u2217t = arg maxy \u03c0\u03b8(y|\u00b7) as the generated word. However, when the difference between the maximum probability and the second largest probability of \u03c0\u03b8(\u00b7) is below a threshold \u0393 (where the selection of y\u2217t is ambiguous), we take the top J words {yjt }Jj=1 according to \u03c0(y|\u00b7) and evaluate Q((x,yt\u22121), y j t ) for all j. Then, we select the word with the highest Q value as the generated word. Note that the sentences generated via critic-based planning can be exactly the same as greedy search. Our critic-based planning method obtain further performance improvement typically on dataset with large domain shift (e.g., CUB-200 and Oxford-102)."}, {"heading": "4. Experiments", "text": "We perform extensive evaluations on a number of popular datasets. For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets. We show that our method generalizes to datasets with large domain shift (CUB-200 and Oxford-102) and datasets with regular domain shift (Flickr30k and TGIF). We also show that critic-based planning can further improve performance dur-\ning inference on datasets with large domain shift. Finally, we conduct an ablation study on Flickr30k to show the contribution of different components."}, {"heading": "4.1. Implementation details", "text": "Data preprocessing. For source domain dataset, we select the MSCOCO training split from [17] which contains 113, 287 images, along with 5 captions each. We prune the vocabulary by dropping words with frequency less than 5, resulting in 10, 066 words including special Begin-OfSentence (BOS) and End-Of-Sentence (EOS) tokens. We use the same vocabulary in all experiments. For target domain datasets, we remove the training sentences containing out-of-vocabulary words. (See supplementary material for detailed statistics.)\nPre-training details. The architecture of our captioner is a CNN-LSTM with hidden dimension 512. The image features are extracted using the pre-trained Resnet-101 [12] and the sentences are represented as one-hot encoding. We first pre-train the captioner on source domain dataset via cross entropy objective using ADAM optimizer [20] with learning rate 5 \u00d7 10\u22124. We apply learning rate decay with a factor of 0.8 every three epoches. To further improve the performance, we use schedule sampling [5] to mitigate the exposure bias. The best model is selected according to the validation performance and serve as the initial model for adversarial training.\nAdversarial training details. We train the captioner and critics using ADAM optimizer [20] with learning rate of 5 \u00d7 10\u22125. We apply dropout in training phase to prevent over-fitting, which also served as input noise similar to [16]. In Monte Carlo rollout, the model samples words until the EOS token under the current policy forK = 3 times. These K sentences are them fed to the critics for estimating the state-action valueQ(\u00b7). Both critics are trained from scratch using the standard classification objective."}, {"heading": "4.2. Experimental Results", "text": "We first pre-train the captioner on MSCOCO training set. Next, we update the captioner by adversarial training procedure with unpaired data from the training set in target domains. Finally, we evaluate our method on four target domain datasets, representing different levels of domain shift. Baseline. We re-implement Deep Compositional Captioner (referred to as DCC) [14] as our baseline method. DCC consists of a lexical classifier and a language model. The former is a CNN model trained to predict semantic attributes and the latter is an LSTM model trained on unpaired text. In the end, the overall DCC model combines both models with a linear layer trained on paired image-caption data. For fair comparison, we apply the following settings on DCC, where the lexical classifier is a ResNet-101 model and the language model is trained on target domain sentences. Finally, we use source domain image-caption pairs to finetune DCC. We also fine-tune a pre-trained source domain model directly on paired training data in the target domain (referred to as Fine-tuning). Ideally, this serves as the upper bound of our experiments.\nWe further categorize three kinds of domain shift between MSCOCO and other target datasets, namely general v.s. fine-grained descriptions, difference in verb usage and subtle difference in sentence style. General v.s. fine-grained descriptions. The large domain shift between MSCOCO and CUB-200/Oxford-102 suggests that it is the most challenging domain adaptation scenario. In CUB-200/Oxford-102, descriptions give detailed expressions of attributes such as beak of a bird or stamen of a flower. In contrast, in MSCOCO, descriptions usually are\nabout the main scene and character. We illustrate the differences at word-level distribution among MSCOCO, CUB200, and Oxford-102 using Venn-style word clouds [6] (see Fig. 4 4).\nOn the top two rows of Fig. 5 show that our model can describe birds and flowers in detailed and also the appearance of fine-grained object attributes. In the top two blocks of Table 1, our method outperforms DCC and Source Pretrained models by a considerable margin for all evaluation metrics. Difference in verb usage. Next, we move towards the verb usage difference between the source and target domains. According to [22], there are more motion verbs (30% in TGIF vs. 19% in MSCOCO) such as dance and shake, and more facial expressions in TGIF, while verbs in MSCOCO are mostly static ones such as stand and sit. Examples in Fig. 5 show that our model can accurately describe human activities or object interactions. On the third panel of Table 1, our method also significantly improves over Source Pre-trained and DCC models. Subtle difference in sentence style. In order to test the generalizability of our method, we conduct an experiment using similar dataset (i.e. Flickr30k) as target domain. In the bottom block of Table 1, our method also offers a noticeable improvement. To sum up, our method shows great potentials for unsupervised domain adaptation across datasets regardless of regular or large domain shift. Critic-based planning. Instead of directly generating the word yt from policy network \u03c0(yt|.), we take the advantage of its adversary, critics, during the inference. The results is\n4Visualization generated using http://worditout.com/.\nshown in Table 2. The threshold \u0393 is set to 0.15 in CUB-200 and to 0.1 in Oxford-102. In every time-step, we choose top J = 2 words according to \u03c0\u03b8(.). Out of 16.2% and 9.4% of words are determined by the critics in CUB-200, and Oxford-102, respectively. Compared to greedy search, critic-based planning can achieve better performance in many evaluation metrics, especially in datasets with large domain shift from the source domain dataset (e.g., CUB200 and Oxford-102). Compared to beam search with beam size 2, critic-based planning also typically gets a higher performance. Beam search method generates the words only depending on captioner itself, while critic-based planning method acquires a different point of view from the critics. Some impressive examples are shown in Fig. 6."}, {"heading": "4.3. Ablation Study", "text": "We have proposed an adversarial training procedure with two critic models: Multi-modal Critic (MC) and Domain Critic (DC). In order to analyze the effectiveness of these two critics, we do ablation comparison with either one and both. Table 3 shows that using MC only is insufficient since MC is not aware of the sentence style in target domain. On\nthe other hand, using DC only contributes significantly. Finally, combining both MC and DC achieves the best performance for all evaluation metrics. We argue that both MC and DC are vital for cross-domain image captioning."}, {"heading": "5. Conclusion", "text": "We propose a novel adversarial training procedure (captioner v.s. critics) for cross-domain image captioning. A novel critic-based planning method is naturally introduced to further improve the caption generation process in testing. Our method consistently outperforms baseline methods on four challenging target domain datasets (two with large domain shift and two with regular domain shift). In the future, we would like to improve the flexibility of our method by combining multiple critics in a plug-and-play fashion."}], "references": [{"title": "Domain-adversarial neural networks", "author": ["H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand"], "venue": "NIPS workshop on Transfer and Multi-Task Learning: Theory meets Practice,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Guided open vocabulary image captioning with constrained beam search", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "CoRR, abs/1612.00576,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": "ICLR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2017}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["S. Bengio", "O. Vinyals", "N. Jaitly", "N. Shazeer"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Domain-adversarial training of neural networks", "author": ["Y. Ganin", "E. Ustinova", "H. Ajakan", "P. Germain", "H. Larochelle", "F. Laviolette", "M. Marchand", "V. Lempitsky"], "venue": "JMLR, 17(59):1\u201335,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in neural information processing systems, pages 2672\u20132680,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["A. Goyal", "A. Lamb", "Y. Zhang", "S. Zhang", "A.C. Courville", "Y. Bengio"], "venue": "NIPS,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating visual explanations", "author": ["L.A. Hendricks", "Z. Akata", "M. Rohrbach", "J. Donahue", "B. Schiele", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["L.A. Hendricks", "S. Venugopalan", "M. Rohrbach", "R. Mooney", "S. Kate", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Fcns in the wild: Pixel-level adversarial and constraint-based adaptation", "author": ["J. Hoffman", "D. Wang", "F. Yu", "T. Darrell"], "venue": "CoRR, abs/1612.02649,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Imageto-image translation with conditional adversarial networks", "author": ["P. Isola", "J.-Y. Zhu", "T. Zhou", "A.A. Efros"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "EMNLP,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": "AAAI,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "ICLR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "TACL,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "A new dataset and benchmark on animated gif description", "author": ["Y. Li", "Y. Song", "L. Cao", "J. Tetreault", "L. Goldberg", "A. Jaimes", "J. Luo"], "venue": "CVPR.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimization of image description metrics using policy gradient methods", "author": ["S. Liu", "Z. Zhu", "N. Ye", "S. Guadarrama", "K. Murphy"], "venue": "CoRR, abs/1612.00370,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Automated flower classification over a large number of classes", "author": ["M.-E. Nilsback", "A. Zisserman"], "venue": "ICVGIP,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "ICLR,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep representations of fine-grained visual descriptions", "author": ["S. Reed", "Z. Akata", "H. Lee", "B. Schiele"], "venue": "CVPR,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Self-critical sequence training for image captioning", "author": ["S.J. Rennie", "E. Marcheret", "Y. Mroueh", "J. Ross", "V. Goel"], "venue": "CoRR, abs/1612.00563,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Captioning images with diverse objects", "author": ["S. Venugopalan", "L.A. Hendricks", "M. Rohrbach", "R.J. Mooney", "T. Darrell", "K. Saenko"], "venue": "CoRR, abs/1606.07770,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "The Caltech-UCSD Birds-200-2011 Dataset", "author": ["C. Wah", "S. Branson", "P. Welinder", "P. Perona", "S. Belongie"], "venue": "Technical report,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "ICML,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["L. Yao", "A. Torabi", "K. Cho", "N. Ballas", "C. Pal", "H. Larochelle", "A. Courville"], "venue": "ICCV,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["P. Young", "A. Lai", "M. Hodosh", "J. Hockenmaier"], "venue": "TACL,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "AAAI,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2017}, {"title": "Title generation for user generated videos", "author": ["K.-H. Zeng", "T.-H. Chen", "J.C. Niebles", "M. Sun"], "venue": "ECCV,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 19, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 15, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 29, "context": "Many novel networks [8, 21, 17, 32] trained with these paired data have achieved impressive results under a domain-specific setting \u2013 training and testing on the same domain.", "startOffset": 20, "endOffset": 35}, {"referenceID": 21, "context": "For instance, MSCOCO [23] mostly consists of images of large scene with more object instances, whereas CUB-200-2011 [33] (shortened as CUB-200 in the following) consists of cropped birds images.", "startOffset": 21, "endOffset": 25}, {"referenceID": 30, "context": "For instance, MSCOCO [23] mostly consists of images of large scene with more object instances, whereas CUB-200-2011 [33] (shortened as CUB-200 in the following) consists of cropped birds images.", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "[14, 30] propose to leverage an image dataset with category labels (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[14, 30] propose to leverage an image dataset with category labels (e.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": ", ImageNet [7]) and sentences on the web (e.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[2] propose to leverage image taggers at test time.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Inspired by Generative Adversarial Networks (GANs) [10], we propose a novel adversarial training procedure to leverage unpaired images and sentences.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": ", tags [2]) in testing.", "startOffset": 7, "endOffset": 10}, {"referenceID": 21, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 65, "endOffset": 73}, {"referenceID": 25, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 65, "endOffset": 73}, {"referenceID": 23, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 86, "endOffset": 94}, {"referenceID": 25, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 86, "endOffset": 94}, {"referenceID": 33, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 106, "endOffset": 110}, {"referenceID": 20, "context": "To evaluate, we use MSCOCO [23] as the source domain and CUB-200 [33, 27], Oxford-102 [25, 27], Flickr30k [36] and TGIF [22] as target domains.", "startOffset": 120, "endOffset": 124}, {"referenceID": 21, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 113, "endOffset": 117}, {"referenceID": 33, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 132, "endOffset": 136}, {"referenceID": 6, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 19, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 15, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 29, "context": "Thanks to recent advances in deep neural networks and the release of several large-scale datasets such as MSCOCO [23] and Flickr30k [36], many works [8, 21, 17, 32] have shown different levels of success on image captioning.", "startOffset": 149, "endOffset": 164}, {"referenceID": 31, "context": "[34] introduce an attention model that can automatically learn where to look depending on the generated words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 28, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 32, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 35, "context": "Besides images, [8, 31, 35, 38] apply LSTMs as video encoder to generate video descriptions.", "startOffset": 16, "endOffset": 31}, {"referenceID": 35, "context": "[38] propose a framework to jointly localize highlights in videos and generate their titles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Recently, the issue of exposure bias [26] has been well-addressed in sequence prediction tasks.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "[5] propose a curriculum learning strategy to gradually ignore the guidance from supervision during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11] introduce an adversarial training method as regularization between sampling mode and teacher-forced mode.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 3, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 22, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 26, "context": "Most recently, there are plenty of works [26, 4, 24, 28] using policy gradient to directly optimize the evaluation metrics.", "startOffset": 41, "endOffset": 56}, {"referenceID": 11, "context": "Several works [13, 37] incorporate auxiliary models as rewards.", "startOffset": 14, "endOffset": 22}, {"referenceID": 34, "context": "Several works [13, 37] incorporate auxiliary models as rewards.", "startOffset": 14, "endOffset": 22}, {"referenceID": 11, "context": "[13] minimize a discriminative loss to ensure generated sentences be class specific.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[37] also introduce a critic to learn a reward function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] introduce a domain adaptation regularizer to learn the representation for sentiment analysis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] propose a gradient reversal layer for aligning the distribution of fea-", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[15] propose an unsupervised domain adversarial method for semantic segmentations in street scenes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "We utilize an existing CNN-RNN model [32] as the model of the policy.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "A sentence y is first encoded by CNN [18] with highway connection [19] into a sentence representation.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "A sentence y is first encoded by CNN [18] with highway connection [19] into a sentence representation.", "startOffset": 66, "endOffset": 70}, {"referenceID": 2, "context": "8, the encoded image x and sentence c representations are fused via element-wise multiplication similar to [3].", "startOffset": 107, "endOffset": 110}, {"referenceID": 21, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "For all experiments, we use MSCOCO [23] as the source dataset and CUB-200 [33], Oxford-102 [25], TGIF [22], and Flickr30k [36] as target datasets.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "For source domain dataset, we select the MSCOCO training split from [17] which contains 113, 287 images, along with 5 captions each.", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "The image features are extracted using the pre-trained Resnet-101 [12] and the sentences are represented as one-hot encoding.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "We first pre-train the captioner on source domain dataset via cross entropy objective using ADAM optimizer [20] with learning rate 5 \u00d7 10\u22124.", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "To further improve the performance, we use schedule sampling [5] to mitigate the exposure bias.", "startOffset": 61, "endOffset": 64}, {"referenceID": 18, "context": "We train the captioner and critics using ADAM optimizer [20] with learning rate of 5 \u00d7 10\u22125.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "We apply dropout in training phase to prevent over-fitting, which also served as input noise similar to [16].", "startOffset": 104, "endOffset": 108}, {"referenceID": 12, "context": "We re-implement Deep Compositional Captioner (referred to as DCC) [14] as our baseline method.", "startOffset": 66, "endOffset": 70}, {"referenceID": 20, "context": "According to [22], there are more motion verbs (30% in TGIF vs.", "startOffset": 13, "endOffset": 17}], "year": 2017, "abstractText": "Impressive image captioning results are achieved in domains with plenty of training image and sentence pairs (e.g., MSCOCO). However, transferring to a target domain with significant domain shifts but no paired training data (referred to as cross-domain image captioning) remains largely unexplored. We propose a novel adversarial training procedure to leverage unpaired data in the target domain. Two critic networks are introduced to guide the captioner, namely domain critic and multi-modal critic. The domain critic assesses whether the generated sentences are indistinguishable from sentences in the target domain. The multi-modal critic assesses whether an image and its generated sentence are a valid pair. During training, the critics and captioner act as adversaries \u2013 captioner aims to generate indistinguishable sentences, whereas critics aim at distinguishing them. The assessment improves the captioner through policy gradient updates. During inference, we further propose a novel critic-based planning method to select high-quality sentences without additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k) as the target domains. Our method consistently performs well on all datasets. In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after adaptation. Utilizing critics during inference further gives another 4.5% boost.", "creator": "LaTeX with hyperref package"}}}