{"id": "1705.01040", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Apr-2017", "title": "Maximum Resilience of Artificial Neural Networks", "abstract": "app deployment allowing robust neural cells ( ram ) in safety - logged spaces signals a number of new awareness and certification needs. in 2010s, for ann - enabled cab - driving vehicles it is difficulty making model properties predict relative transitions of anns between noisy or sexually maliciously manipulated sensory input. we are addressing these cases by predicting resilience properties of ann - intelligent gates as the acoustic response of input or sensor flux which is still tolerated. implicit objection by computing adaptive perturbation profiles for anns got instead reduced to solving guaranteed interval optimization problems ( mip ). a line of mip reliability heuristics proved inadequate beyond drastically reducing mip - solver runtimes, and processing tables of co - solvers already requires an almost linear close - approximation improving the cascade ( up to a certain length ) of processing cores in our experiments. we seek the unprecedented functional scalability of our ai by means of computing better resilience expectations for a cascade of ann domain sets ranging from specialised image recognition designs to generic passive maneuvering of robots.", "histories": [["v1", "Fri, 28 Apr 2017 12:04:02 GMT  (466kb,D)", "https://arxiv.org/abs/1705.01040v1", "Timestamping research work conducted in the project"], ["v2", "Wed, 5 Jul 2017 11:27:46 GMT  (525kb,D)", "http://arxiv.org/abs/1705.01040v2", "Timestamp research work conducted in the project. version 2: fix some typos, rephrase the definition, and add some more existing work"]], "COMMENTS": "Timestamping research work conducted in the project", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.LO cs.SE", "authors": ["chih-hong cheng", "georg n\\\"uhrenberg", "harald ruess"], "accepted": false, "id": "1705.01040"}, "pdf": {"name": "1705.01040.pdf", "metadata": {"source": "CRF", "title": "Maximum Resilience of Artificial Neural Networks", "authors": ["Chih-Hong Cheng", "Georg N\u00fchrenberg", "Harald Ruess"], "emails": ["cheng@fortiss.org", "nuehrenberg@fortiss.org", "ruess@fortiss.org"], "sections": [{"heading": "1 Introduction", "text": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications such as medical image processing or semi-autonomous vehicles poses a number of new assurance, verification, and certification challenges [2,5]. For ANN-based end-to-end steering control of self-driving cars, for example, it is important to know how much noisy or even maliciously manipulated sensory input is tolerated [14]. Here we are addressing these challenges by establishing maximum and verified bounds for the resilience of given ANNs on these kinds of input disturbances.\nMore precisely, we are defining and computing safe perturbation bounds for multi-class ANN classifiers. This measure compares the relative ratio-ordering of multiple, so-called softmax output neurons for capturing scenarios where one only wants to consider inputs that classify to a certain class with high probability. The problem of finding minimal perturbation bounds is reduced to solving a corresponding mixed-integer programming (MIP). In particular, the encoding of some non-linear functions such as ReLU and max-pooling nodes require the introduction of integer variables. These integer constraints are commonly handled\nar X\niv :1\n70 5.\n01 04\n0v 2\n[ cs\n.L G\n] 5\nJ ul\n2 01\nby off-the-shelf MIP-solvers such as CPLEX1 which are based on branch-andbound algorithms. In the MIP reduction, a number of nonlinear expressions are linearized using a variant of the well-known big-M [10] encoding strategy. We also define a dataflow analysis [7] for generating relatively small big-M as the basis for speeding up MIP solving. Other important heuristics in encoding the MIP problem include the usage of solving several substantially simpler MIP problems for speeding up the overall generation of satisfying instances by the solver. Lastly, branch-and-bound is run in parallel on a number of computing cores.\nWe demonstrate the effectiveness and scalability of our approach and encoding heuristics by computing maximum perturbation bounds for benchmark sets such as MNIST [15] and agent games [16]. These cases studies include ANNs for image recognition and for high-level maneuver decisions for autonomous control of a robot. Using the heuristic encodings outlined above we experienced a speedup of about two orders of magnitude compared with vanilla MIP encodings. Moreover, parallelization of branch-and-bound [25] on different computing cores can yield, up to a certain threshold, linear speed-ups using a high-performance parallelization framework.\nThe practical advantages of our approach for validating and qualifying ANNs for safety-relevant applications are manifold. First, perturbation bounds provide a formal interface between sensor sets and ANNs in that they provide a maximum tolerable bound on possible sensor errors. These assume-guarantee interfaces therefore form the basis for decoupling the design of sensor sets from the design of the classifier itself. Second, our method also computes minimally perturbed inputs of different classification, which might be included into ANN training sets for potentially improving classification results. Third, maximum perturbation bounds are a useful measure of the resilience of an ANN towards (adversarial) perturbation, and also for objectively comparing different ANNs. Last, large perturbation bounds are intuitively inversely related with the problem of overfitting, that is poor generalization to new inputs, which is a common issue with ANNs.\nAn overview of concrete problems and various approaches to the safety of machine learning is provided in [2]. We compare our results only with work that is most closely related to ours. Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs. In contrast to our work, these methods do not actually establish verified properties on the input-output behavior of ANNs. Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11]. Instead we rely on solving MIP problems and parallelization of branch-and-bound algorithms. In contrast to previous approaches we also go beyond verification and solve optimization problems for ANNs for establishing maximum perturbation bounds.\n1 https://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/\n1 x1\nx2\nxd\n1 1\ninput layer output layer\n4 inputs\nMax-Pooling\nReLU\nFully connected\nhidden layers\nx (3) 1\nn (1) 2\nn (2) 1\nn (3) 1\n(2x2)\ntan\u22121\nFullyConnected\nFig. 1: An illustration of how a neural network is defined.\nn (l) i\nw (l) 0i\nw (l) 1i\nw (l)\ndl\u22121i\nx (l\u22121) 0 = 1\nx (l) i\nx (l) i = max(0, im (l) i )\nn (l) i\nx (l\u22121) j1 x (l\u22121) j2 x (l\u22121) j3 x (l\u22121) j4\nim (l) i = \u2211d(l\u22121) j=0 w (l) ji x (l\u22121) j\nFully\nReLU\n4 inputs\nMax-Pooling\nType Node structure input-output function\nconnected\nconnected\nFully\n(2x2) max(x (l\u22121) j1 , x (l\u22121) j2 ,\nwhere\ntan\u22121\nwhere\nx (l\u22121) 1\nx (l\u22121) dl\u22121\nx (l) i\nn (l) i\nw (l) 0i\nw (l) 1i\nw (l)\ndl\u22121i\nx (l\u22121) 0 = 1\nx (l) i\nx (l\u22121) 1\nx (l\u22121) dl\u22121\nim (l) i = \u2211d(l\u22121) j=0 w (l) ji x (l\u22121) j\nx (l) i = tan \u22121(im(l)i )\nx (l) i =\nx (l\u22121) j3 , x (l\u22121) j4 )\nFig. 2: Input-output function neurons.\nThese kinds of problems might also be addressed in SMT-based approaches either by using binary search over SMT or by using SMT solvers that support optimization such as \u03bdZ [4], but it is not clear how well these approaches scale to complex ANNs. Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples). Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor. Moreover, our proposed measure is a property of the classification network itself rather than just a property of a single image (as in [11]) or by only taking samples from the classifier without guarantee (as in [3]).\nThe paper is structured as follows. Section 2 reviews the foundations of feedforward ANNs. Section 3 presents an encoding of various neurons in terms of linear constraints. Section 4 defines our measure for quantifying the resilience of an ANN, that is, its capability to tolerate random or even adversarial input perturbations. Section 5 summarizes our MIP encoding heuristics for substantially increasing the performance of the MIP-solver in establishing in minimal perturbation bounds of ANN. Finally, we present the results of some of our experiments in Section 6, and we describe possible improvements and extensions in Section 7."}, {"heading": "2 Preliminaries", "text": "We introduce some basic concepts of feed-forward artificial neural networks (ANN) [1]. These networks consist of a sequence of layers labeled from l = 0, 1, . . . , L, where 0 is the index of the input layer, L is the output layer, and all other layers are so-called hidden layers. For the purpose of this paper we assume that each input is of bounded domain. Superscripts (l) are used to index\nlayer l-specific variables, but these superscripts may be omitted for input layers. Layers l are comprised of nodes n (l) i (so-called neurons), for i = 0, 1, . . . , d\n(l), where d(l) is the dimension of the layer l. By convention nodes of index 0 have a constant output 1; these bias nodes are commonly used for encoding activation thresholds of neurons. In a feed-forward net, nodes n (l\u22121) j of layer l\u2212 1 are connected with nodes n (l) i in layer l by means of directed edges of weight w (l) ji . For the purpose of this paper we are assuming that all weights in a network have fixed values, since we do not consider re-learning. Figure 1 illustrates a small feed-forward network structure with four layers, where each layer comes with a different type of node functions, which are also main ingredients of convolutional neural networks. These node functions are specified in Figure 2. The first hidden layer of the network in Figure 1 is a fully-connected ReLU layer . Node n (1) 2 , for example, computes the weighted linear sum of all inputs from the previous layer as im (1) 2 , and outputs the maximum of 0 and this weighted sum. The second hidden layer is using max-pooling for down-sampling an input representation by reducing its dimensionality; node n (2) 1 , for example, just outputs the maximum of its inputs. Node n (3) 1 in the output layer applies the sigmoid-shaped tan \u22121 on the weighted linear input sum.\nGiven an input to the network these node functions are applied successively from layer 0 to L \u2212 1 for computing the corresponding network output at layer L. For l = 1 to L we use x (l) i to denote the output value of node n (l) i and x (l) i (a1, . . . , ad) denotes the output value x (l) i for the input a1, . . . , ad, sometimes abbreviated by x (l) i (a).\nFor the purpose of multi-class classification, outputs in layer L are often transformed into a probability distribution by means of the softmax function\nex (L\u22121) i\n\u2211 j=1,...,dL e x (L\u22121) j .\nIn this way, the output x (L) i is interpreted as the probability of the input to\nbe in class i. For the inputs x (L\u22121) 1 = \u22121, x (L\u22121) 2 = 2, x (L\u22121) 3 = 3 of the nodes in Figure 3, for example, the corresponding outputs (0.0132, 0.2654, 0.7214) for (x (L) 1 , x (L) 2 , x (L) 3 ) sum up to 1."}, {"heading": "3 Arithmetic Encoding of Artificial Neural Networks", "text": "In a first step, we are encoding the behavior of ANNs in terms of linear arithmetic constraints. In addition to [13] we are also considering tan\u22121, max-pooling and softmax nodes as commonly found in many ANNs in practice. These encodings are based on the input-output behavior of every node in the network, and the\nmain challenge is to handle the non-linearities, which are arising from non-linear activation functions (e.g., ReLU and tan\u22121), max-pooling and softmax nodes.\nConstraints for ReLU and tan\u22121 nodes as defined in Figure 2 are separated into, first, an equality constraint (1) for the intermediate value im (l) i and, second, several linear constraints for encoding the non-linear behavior of these nodes.\nim (l) i =\n\u2211\nj=0,...,d(l\u22121)\nw (l) ji x (l\u22121) j (1)\nWe now describe the encoding of the nonlinear functions (x (l) i = max(0, im (l) i ) or x (l) i = tan\u22121(im(l)i )).\nEncoding ReLU activation function. The nonlinearity in ReLU constraints x (l) i = max(0, im (l) i ) is handled using the well-known big-M method [10], which introduces a binary integer variable b (l) i together with a positive constant M (l) i such that \u2212M (l) i \u2264 im (l) i and x (l) i \u2264M (l) i for all possible values of im (l) i and x (l) i . A derivation of the following reduction is listed in the appendix.\nProposition 1 x (l) i = max(0, im (l) i ) iff the constraints (2a) to (4b) hold.\nx (l) i \u2265 0 (2a) x\n(l) i \u2265 im (l) i (2b)\nim (l) i \u2212 b (l) i M (l) i \u2264 0 (3a)\nim (l) i + (1\u2212 b (l) i )M (l) i \u2265 0 (3b)\nx (l) i \u2264 im (l) i + (1\u2212 b (l) i )M (l) i (4a) x (l) i \u2264 b (l) i M (l) i (4b)\nThe efficiency of a MIP-solver via big-M encoding heavily depends on the size of M (l) i , because MIP-solvers typically relax binary integer variables to realvalued variables, resulting in a weak LP-relaxation for large big-Ms. It is therefore essential to choose relatively small values for M (l) i . We apply static analysis [7] based on interval arithmetic for propagating the bounded input values through the network, as the basis for generating \u201cgood\u201d values for M (l) i .\nMax-Pooling. The output x (l) i of a max-pooling node is rewritten as x (l) i = max(im1, im2), where im1 = max(x (l\u22121) j1 , x (l\u22121) j2 ) and im2 = max(x (l\u22121) j3 , x (l\u22121) j4\n). Encoding the max(x1, x2) function into MIP constraints is accomplished by introducing three binary integer variables to encode y = max(x1, x2) using the big-M method.\nProperty-directed encoding of softmax. The exponential function in the definition of softmax, of course, can not be encoded into a linear MIP constraint. However, using the proposition below, one confirms that if the property to be analyzed does not consider the concrete value of output values from neurons but only the ratio ordering, then (1) it suffices to omit the construction of the output layer, and (2) one may rewrite the property by replacing each x (L) i by x (L\u22121) i .\nProposition 2 Given a feed-forward ANN with softmax output layer and a constant \u03b1 > 0, then for all i, j \u2208 {1, . . . , d(L)}:\nx (L) i1 \u2265 \u03b1x(L)i2 \u21d4 x (L\u22121) i1 \u2265 ln(\u03b1) + x(L\u22121)i2 .\nThis equivalence is simply derived by using the definition of softmax, multiplying by the positive denominator, and by applying the logarithm and the resulting inequality. The derivation is listed in the appendix.\nEncoding tan\u22121 with error bounds. The handling of non-linearity in tan\u22121(im) is based on results in digital signal processing for piece-wise approximating tan\u22121(im) with quadratic constraints and error bounds. In case \u22121 \u2264 im \u2264 1 the quadratic approximation methods (Eq. (7) of [22]) are used, and tan\u22121(im) is approximated by \u03c04 im + 0.273 im(1 \u2212 |im|) with a maximum error smaller than 0.0038. The absolute value |im| in the formula is removed by encoding case splits between im \u2265 0 and im < 0 using big-M methods. Otherwise, when considering the case im > 1 or im < \u22121, the symmetry condition of tan\u22121 [24] states that (1) if im > 0 then tan\u22121(im) + tan\u22121( 1im ) = \u03c0 2 , and (2) if im < 0 then tan\u22121(im) + tan\u22121( 1im ) = \u2212\u03c02 . This implies that we can create a variable iminv with a constraint that iminv im = 1, i.e., variable iminv is the inverse of im. By utilizing the fact that \u22121 \u2264 iminv \u2264 1, the value of tan\u22121(iminv) can be computed by the formula in (i).\nMoreover, case splits are encoded using the big-M method as outlined above. Since quadratic terms are used, our approach for handling tan\u22121 nodes requires solving mixed integer quadratic constraint problem (MIQCP) problems.\nUsing these approximations for tan\u22121(imi), we obtain lower and upper bounds for the value of the node variable xi, where the interval between lower and upper bound is determined by the approximation error of tan\u22121. Since the approximation error propagates through the network and using lower and upper bounds instead of an equality constraint relaxes the problem, our method computes approximations for the measure when it is used for ANNs with tan\u22121 as activation function.\nPre-processing based on dataflow analysis. We use interval arithmetic to obtain relatively small values for big-M , in order to avoid a weak LP-relaxation of the MIP. Interval bounds for the values of x (l) i are denoted by [Lo(x (l) i ),Up(x (l) i )]. We are assuming that all input values (at layer l = 0) are bounded, and the output of bias nodes is restricted by the singleton [1, 1] (the value of the bias is given by the weight of a bias node). Interval bounds for the values of node outputs\nx (l) i are obtained from the interval bounds of connected nodes from the previous layers by means of interval arithmetic.\nThe output x (l) i of ReLU nodes is defined by im (l) i = \u2211 j=0,...,d(l\u22121) w (l) ji x (l\u22121) j\nand the ReLU function max(0, im (l) i ). Therefore, interval bounds for x (l) i are computed by first considering the interval bounds Lo(im (l) i ) and Up(im (l) i ), which are determined by weights of the linear sum and the bounds on x (l\u22121) j . The bounds Lo(im (l) i ) and Up(im (l) i ) are obtained from interval arithmetic as follows:\nLo(im (l) i ) =\n\u2211\nj=0,...,d(l\u22121)\nmin ( w\n(l) ij \u00b7 Lo(x (l\u22121) j ), w (l) ij \u00b7 Up(x (l\u22121) j )\n)\nUp(im (l) i ) =\n\u2211\nj=0,...,d(l\u22121)\nmax ( w\n(l) ij \u00b7 Lo(x (l\u22121) j ), w (l) ij \u00b7 Up(x (l\u22121) j )\n) .\nGiven Lo(im (l) i ) and Up(im (l) i ) the bounds on x (l) j are derived using the definition of ReLU, i.e.,\n[Lo(x (l) i ),Up(x (l) i )] = [max(0, Lo(im (l) i )),max(0,Up(im (l) i ))] .\nNote that if Lo(x (l) i ) \u2265 0 or Up(x (l) i ) \u2264 0 these bounds suffice to determine which case of the piece-wise linear ReLU function applies. In this way, the constraints (2)-(4) maybe dropped and the value of x (l) i is directly encoded using linear constraints, which reduces the number of binary variables.\nIn the case of max-pooling nodes, the output x (l) i is simply the maximum\nmax(x (l\u22121) j1 , x (l\u22121) j2 , x (l\u22121) j3 , x (l\u22121) j4\n) of its four inputs. Therefore, the bounds Lo x (l) i\nand Up x (l) i on the output are given by the maximum of the lower and uppers bounds of the four inputs respectively. Interval bounds of the outputs for tan\u22121 are obtained using a polynomial approximation for tan\u22121 (see below). Finally, the output of softmax nodes is a probability in [0, 1] which might also be further refined using interval arithmetic. These bounds on softmax nodes, however, are not used in our encodings, because of the property-driven encoding of softmax output layers as described previously."}, {"heading": "4 Perturbation Bounds", "text": "We define concrete measures for quantifying the resilience of multi-classification neural networks with softmax output neurons. This measure for resilience is defined over all possible inputs of the network. In particular, our developments do not depend on probability distributions of training and test data as in previous work [3]. Maximum resilience of these ANNs is obtained by means of solving corresponding MIP problems (or MIQCPs in the case of tan\u22121 activation functions).\nWe illustrate the underlying principles of maximum resilience using examples from the MNIST database [15] for digit recognition of input images (see Fig. 5).\nInput images in MNIST are of dimension 24 \u00d7 24 and are represented as a vector a1, . . . , a576. Input layers of ANN-based multi-digit classifiers for MNIST therefore consist of 576 input neurons, and the output layer is comprised of 10 softmax neurons. Let the output x (L) 0 , . . . , x (L) 9 at the last layer be the computed probabilities for an input image to be classified to characters \u20180\u2019 to \u20189\u2019.\nTo formally define a perturbation, we allow each input ai (i = 1, . . . , d) to have a small disturbance i, so the input after perturbation is (a1+ 1, . . . , ad+ d). We sometimes use the concise notation of a+ := (a1 + 1, . . . , ad + d) for the perturbed input. The global value of the perturbation is obtained by taking the sum of the absolute values of each disturbance i, i.e., | 1|+ | 2|+ . . .+ | d|.\nDefinition 1 (Maximum Perturbation Bound for m-th classifier) For a given ANN with d(L) neurons in a softmax output layer and given constants \u03b1 \u2265 1 and k \u2208 {1, . . . , d(L) \u2212 1}, we define the maximum perturbation bound for the m-th classifier, denoted by \u03a6m, 2 to be the maximum value such that:\nFor all inputs a = (a1, . . . , ad) where x (L) m (a) \u2265 \u03b1 \u00b7 x(L)j (a) on all other classes j \u2208 {1, . . . , d(L)} \\ {m}, we have that for all perturbations = ( 1, 2, . . . , d) where | 1|+| 2|+. . .+| d| < \u03a6m, there exist at most k \u2212 1 classes j\u2032 \u2208 {0, 1 . . . , d(L)} such that x\n(L) m (a+ ) \u2264 x(L)j\u2032 (a+ ).\nIntuitively, the bound \u03a6m guarantees that for all inputs that strongly (defined by \u03b1) classify to class m, if the total amount of perturbation is limited to a value strictly below \u03a6m, then either (1) the perturbed input can still be classified as m, or (2) the probability of classifying to m is among the k highest probabilities. Dually, \u03a6m is the smallest value such that there exists an input that originally classifies to m, for which the computed probability for class m may not be among the k highest after being perturbed with value greater than or equal to \u03a6m.\n2 For clarity, we usually omit the dependency of \u03a6m from \u03b1.\nFig. 5 illustrates an example of an MNIST image being perturbed, where the neural network considers the perturbed image to be \u20180\u2019 or \u20183\u2019 with at least the probability of being a \u20185\u2019. The \u201cnot among the k highest\u201d property is an indicator that the confidence of classifying to class m has decreased under perturbation, as the perturbed input can be interpreted as at least k other classes. In our experiment evaluations below we used the fixed value k = 2.\nConstant \u03b1 \u2265 1 may be interpreted as indicating the level of confidence of being classified to a class m. When setting \u03b1 to 1, the analysis takes all inputs for which the probability of class m is greater than or equal to the probabilities of the other classes. Since there might exist an image that has the same probability for all classes, setting \u03b1 = 1 may result in a maximum perturbation of zero. Increasing k helps to avoid this effect, because it requires that at most k \u2212 1 other classes have probabilities greater than or equal to the probility of m. By picking an \u03b1 > 1 low-confidence inputs are removed and part (II) of Definition 1 forces the perturbation to be greater than zero. E.g., assume if point B in Fig. 6 is classified to \u20185\u2019 with probability 0.35 and to \u20180\u2019 with probability 0.34, then even by setting \u03b1 = 1.1, point B will not be considered in the analysis. By setting \u03b1 to 25 one already only considers inputs that classifies to m with probability higher than 0.95.\nProvided that \u03a6m can be computed for each class m (as shown below), one defines a measure for safe perturbation by taking the minimum of all \u03a6m, and the measure is computed by computing each \u03a6m independently.\nDefinition 2 (Perturbation Bound for ANN) For an ANN with L layers and d(L) softmax neurons in the output layer, a given \u03b1 \u2265 1, k \u2208 {1, . . . , d(L)\u22121}, and \u03a6m the perturbation bound for the m-th classifier of this ANN from Definition 1, the perturbation bound for ANN is defined as \u039e := min(\u03a61, . . . , \u03a6dL).\nBased on the dual interpretation above of Definition 1 we are now ready to encode the problem of finding \u03a6m in terms of the following optimization problem, where a = (a1, . . . , ad) and a+ = (a1 + 1, . . . , ad + d).\nminimize \u2211\ni=1,...,d\n| i|\nsubject to\nx(L)m (a) \u2265 \u03b1x(L)i (a) \u2200i \u2208 {1, . . . , dL} \\m \u2228\nI \u2286 {1, . . . , dL} \\m |I| = k\n\u2227\n\u2200i\u2208I x(L)m (a+ ) \u2264 x(L)i (a+ )\nand subject to constraints (1)-(4) for ANN encoding. (5)\nProposition 3 For a given \u03b1 \u2265 1 and k \u2208 {1, . . . , d(L)\u22121}, the optimal value of the optimization problem (5) as stated above equals \u03a6m. For ANNs using tan \u22121 problem (5) yields an under-approximation \u03a6\u2032m \u2264 \u03a6m, because the feasible region is relaxed due to the approximation of tan\u22121.\nThe first set of conjunctive constraints specifies that the input a = (a1, . . . , ad) strongly classifies to m (i.e., satisfies condition I in Def. 1), while the second set of disjunctive constraints specifies that by feeding the image after perturbation, the neural network outputs that at least k classes in I are more likely (or equally likely) than classm (i.e., the second condition in Def. 1 is violated). Therefore, for input a = (a1, . . . , ad) and its associated perturbation = ( 1, . . . , d), we have that \u2211 i=1,...,d | i| \u2265 \u03a6m. By computing the minimum objective of \u2211 i=1,...,d | i|\nsatisfying the constraints we obtain \u2211 i=1,...,d | i| = \u03a6m.\nWe now address the following issues in order to transform optimization problem (5) into a MIP: (1) the objective is not linear due to the introduction of the absolute value function, (2) the non-linearity of softmax due to the function x (L) i = e x (L\u22121) i / \u2211 j=1,...,dL e x (L\u22121) j , and (3) the disjunction in the second set of constraints.\n(i) Transforming objectives. Since the objective | 1|+ | 2| . . . , | d| in problem (5) is not linear, we create new variables absi in optimization problem (6), where i \u2208 {1, . . . , d}, such that every absi is greater than i and \u2212 i. Whenever the value is minimized, we have that absi = | i|.\n(ii) Removing softmax output layer. Optimization problem (5) contains the inequality x (L) m (a1, . . . , ad) \u2265 \u03b1x(L)i (a1, . . . , ad). It follows from Proposition 2 that replacing this inequality with x (L\u22121) m (a1, . . . , ad) \u2265 ln(\u03b1) + x(L\u22121)i (a1, . . . , ad) is sufficient, thereby omitting the exponential function.\n(iii) Transforming disjunctive constraints. The disjunctive constraint in problem (5) guarantees at least k classifications with probability equal or higher as m. We rewrite it by introducing a binary variable ci for each class i 6= m. Then we use (1) an integer constraint \u2211 i=1,...,d,i6=m ci \u2265 k to select k classifications and (2) the big-M method to enforce that if classification i is selected (i.e., ci = 1), the probability of classifying to i is higher or equal to the probability of classifying to m.\nBy applying the transformations (i)-(iii) to the optimization problem (5) we obtain problem (6), which is a MIP, and it follows from Proposition 3 that maximum perturbations bounds can be obtained by solving the MIP in (6).\nTheorem 1 For a given \u03b1 \u2265 1 and k \u2208 {1, . . . , d(L) \u2212 1}, the optimum of the MIP in (6) equals \u03a6m for ANNs with ReLU nodes and softmax output layer. For ANNs using tan\u22121 it yields an under-approximation.\nminimize \u03a6m := \u2211 i\u2208{1,...,d} abs i\nsubject to\nx(L\u22121)m (a) \u2265 ln(\u03b1) + x(L\u22121)i (a) \u2200i \u2208 {1, . . . , dL} \\m \u2211\ni\u2208{1,...,dL}\\m ci \u2265 k\nx (L\u22121) i (a+ ) \u2265 x(L\u22121)m (a+ )\u2212M(1\u2212 ci) \u2200i \u2208 {1, . . . , dL} \\m\nabsi \u2265 i \u2200i \u2208 {1, . . . , d}\nabsi \u2265 \u2212 i \u2200i \u2208 {1, . . . , d}\nci \u2208 {0, 1} \u2200i \u2208 {1, . . . , dL} \\m\nand subject to constraints (1)-(4) for ANN encoding. (6)"}, {"heading": "5 Heuristic Problem Encodings", "text": "We list some simple but essential heuristics for efficiently solving MIP problems for the verification of ANNs. Notice that these heuristics are not restricted to computing the resilience of ANNs, and may well be applicable for other verification tasks involving ANNs.\n1. Smaller big-Ms by looking back at multiple layers. The dataflow analysis in Section 3 essentially views neurons at the same layer to be independent. Here we propose a more fine-grained analysis by considering a fixed number of predecessor layers at once. Finding the bound for the output of a neuron x (l) i , for example, can be understood as solving a substantially smaller MIP problem by considering neurons from layer l\u22121 and l\u22122 when considering two preceding layers. These MIP problems are independent for each node in these layers and can therefore be solved in parallel. For each node, we first set the upper bound as a variable to be maximized in the objective, and trigger the MIP-solver to find such a value. Relations over integer binary variables can be derived by applying similar techniques. Notice that these MIPs only generate correct lower and upper bounds if they can be solved to optimality.\n2. Branching priorities. This encoding heuristics uses the given structure of feed-forward ANNs in that binary integer variables originating from lower layers are prioritized for branching. Intuitively, variables from the first hidden layer only depend on the input and it influences all other binary integer variables corresponding to neurons in deeper layers.\n3. Constraint generation from samples and solver initialization. For computing \u03a6m on complex systems via MIP, we use the following three-step process. First, find an input assignment (aini1 , . . . , a ini d ) such that the probability of classifying to m is \u03b1 times larger, i.e., x (L) m (aini1 , . . . , a ini d ) \u2265 \u03b1x (L) j (a ini 1 , . . . , a ini d ) for all j = 1, . . . , d(L), j 6= m. Finding (aini1 , . . . , ainid ) is equivalent to solving a substantially simpler MIP problem without introducing variables 1, . . . , d and abs1 , . . . , abs d . Second, use Eq. (6) to compute the minimum perturbation by considering the domain to be size 1, i.e., {(aini1 , . . . , ainid )}. As the domain is restricted to a single input, all variables aini1 , . . . , a ini d in Eq. (6) are replaced by constants aini1 , . . . , a ini d . This also yields substantially simpler MIP problems, and the computed bound is denoted by \u03a6inim . Third, and finally, initialize the MIP-solver by using the computed values from steps 1 and 2, such that the search directly starts with a feasible solution with objective \u03a6inim. Also, the constraint \u2212\u03a6inim \u2264 \u2211 i=1,...,d i \u2264 \u03a6inim, as \u2211 i=1,...,d i \u2264 \u2211 i=1,...,d | i| = \u03a6m \u2264 \u03a6inim, can be further added to restrict the search space."}, {"heading": "6 Implementation and Evaluation", "text": "We implemented an experimental platform in C++ for verifying and computing perturbation bounds for neural networks, which is based on IBM CPLEX Optimization Studio 12.7 (academic version) for MIP solving. We used three different benchmark sets as the basis for our evaluations: (1) MNIST3 for number characterization, (2) agent games4, and (3) deeptraffic for simulating highway overtaking scenarios5. These benchmarks are denoted by IMNIST, IAgent, and Ideeptraffic respectively, in the following. For each of the benchmarks we created neural networks with different numbers of hidden layers and numbers of neurons, which are shown in Tables 1 and 2. All the networks were trained using ConvNetJS [12].\n\u2013 Agents in agent games have 9 sensors, each pointing into a different direction and returning the distances to an apple, poison or a wall, which amounts to the 27 inputs. Neural networks of various size were trained for an agent that gets rewarded for eating red things (apples) and gets negative reward when it eats green things (poison).\n\u2013 deeptraffic is used as a gamified simulation environment for highway traffic. The controller is trained based on a grid sensor map, and it outputs high-level driving decisions to be taken such as switch lane, accelerate or decelerate.\n\u2013 For MNIST digit recognition [15] has 576 input nodes for the pixels of a gray-scale image, where we trained three networks with different numbers of neurons in the hidden layers.\n3 http://cs.stanford.edu/people/karpathy/convnetjs/demo/mnist.html 4 http://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html 5 http://selfdrivingcars.mit.edu/deeptrafficjs/\nIn our experimental validation we focus on efficiency gains of our MIP encodings and parallelization for verifying neural networks, and the computation of perturbation bound by means of the optimization problem stated in Eq. (6).\nEvaluation of MIP Encodings. To understand how dataflow analysis and our heuristic encodings reduce the overall execution time, we have created synthetic benchmarks where for each example, we only ask for a given input instance (e.g., an image) that classifies to m, whether the perturbation bound is below \u03b4. By restricting ourselves to only verify a single input instance and by not minimizing \u03b4, the problem under verification (local robustness related to an input) is substantially simpler and is similar to those stated in [13,3]. Table 1 gives a summary over results being evaluated using Google Computing Engine (16 CPU and 60 GB RAM) by only allowing 12 threads to be used. Compared to a na\u0308\u0131ve approach that sets M (l) i uniformly to a large constant, applying dataflow analysis can bring benefits for instances that take a longer time to solve. The first two heuristics we have implemented are useful for solving some very difficult problems. Admittedly, it can also result in longer solutions times for simpler instances, but as our ultimate goal is for scalability such an issue is currently minor. More difficult instances (see I4x50MNIST in Table 1) could only be solved using heuristic 1. for preprocessing.\nEffects of Parallelization. For IMNIST we further measured the solution time for local robustness with = 0.01 for 10 test inputs using 8, 16, 24, 32 and 64 threads on machines that have at least as many CPUs as we allow CPLEX to have threads. The results are shown in Figure 7. It is clearly visible that using more threads can bring a significant speed-up till 32 cores, especially for instances that cannot be solved fast with few threads. Interestingly, one can also observe that for this particular problem (200 neurons in hidden layers), increasing the number of threads from 32 to 64 does not improve performance (many lines just flatten from 32 cores onwards). However, for some other problems (e.g., 400 neurons in hidden layers in hidden layers or computing resilience), the parallelization effect can last longer to some larger number of threads. We suspect that for problems that have reached a certain level of simplicity, adding additional parallelization may not further help.\nComputing \u03a6m by solving problem (6). Table 2 shows the result of computing precise \u03a6m. For simpler problems, we can observe from the first 4 rows of Table 2 that the computed \u03a6m increases, when the value of the parameter \u03b1 increases.\nThis is a natural consequence - for inputs being classified with higher confidence, it should allow for more perturbation to bring to ambiguity. Notably, using a value of \u03b1 above its maximum makes the problem infeasible, because there does not exist an input for which the neural network has such high confidence. For complex problems, by setting \u03b1 is closer to its maximum (which can be computed by solving another substantially simpler MIP that maximizes \u03b1 for all inputs that classify to class m), one shrinks the complete input space to inputs with high confidence. Currently, scalability of our approach relies on sometimes setting a high value of \u03b1, as can be observed in the lower part of Table 2."}, {"heading": "7 Concluding Remarks", "text": "Our definition and computation of maximum perturbation bounds for ANNs using MIP-based optimization is novel. By developing specialized encoding heuristics and using parallelization we demonstrate the scalability and possible applicability of our verification approach for neural networks in real-world applications. Our verification techniques also allow to formally and quantitatively compare the resilience of different neural networks. Also, perturbation bounds provide a formal assume-guarantee interface for decoupling the design of sensor sets from the design of the neural network itself. In our case, the network assumes a maximum sensor input error for resilience, and the input sensor sets need to be designed to guarantee the given error bound. These kinds of contract-based interfaces may form the basis for constructing more modularized safety cases for autonomous systems.\nNevertheless, we consider the developments in this paper as only a first tiny step towards realizing the full potential of formal verification techniques for artificial neural networks and their deployment for realizing new safety-critical functionalities such as self-driving cars. For simplicity we have restricted ourselves to 1-norms for measuring perturbations but other vector norms may, of course, also be used depending on the specific needs of the application context. Also, the development of specialized MIP solving strategies for verifying ANNs, which go beyond the encoding heuristics provided in this paper, may result in considerable efficiency gains. Notice also that the offline verification approach as presented here is applied a posteriori to fixed and \u201dfully trained\u201d networks, whereas real-world networks are usually trained and improved in the field and during operation. Furthermore, the exact relationship of our perturbation bounds with the common phenomena of over-fitting in a neural network classifier deserves a closer examination, since perturbation may also be viewed as generalization from samples. And, finally, investigation of further measures of the resilience of ANN is needed, as perturbation bounds do not generally cover the resilience of ANNs to input transformations such as scaling or rotation."}, {"heading": "24. A. Ukil, V. H. Shah, and B. Deck. Fast computation of arctangent functions for", "text": "embedded applications: A comparative analysis. In ISIE, pages 1206\u20131211. IEEE, 2011. 25. Y. Xu, T. K. Ralphs, L. Lada\u0301nyi, and M. J. Saltzman. Computational experience with a software framework for parallel integer programming. INFORMS Journal on Computing, 21(3):383\u2013397, 2009.\nAppendix\nProposition 1 x (l) i = max(0, im (l) i ) iff constraints (2a) to (4b) hold.\nFirst we establish a lemma to assist the proof.\nLemma 1 b (l) i = 1\u21d4 im (l) i \u2265 0.\nProof. (\u21d2) Assume b(l)i = 1, then (3a) holds trivially and (3b) implies im (l) i \u2265 0. (\u21d0) Assume im(l)i \u2265 0, then (3b) holds trivially and (3a) only holds if b (l) i = 1.\nProof. (Prop. 1)\nFirst we rewrite the condition x (l) i = max(0, im (l) i ) to allow further processing.\nx (l) i = max(0, im (l) i )\ndefinition of max\u21d0=========\u21d2 (im(l)i \u2265 0\u21d2 x (l) i = im (l) i ) \u2227 (im (l) i < 0\u21d2 x (l) i = 0)\nReplace im (l) i by b (l) i = 1 using lemma 1\u21d0========================\u21d2 (b(l)i = 1\u21d2 x (l) i = im (l) i ) \u2227 (b (l) i = 0\u21d2 x (l) i = 0)\n(\u21d2) If (b(l)i = 1 \u21d2 x (l) i = im (l) i ) \u2227 (b (l) i = 0 \u21d2 x (l) i = 0) holds, as b (l) i is a 0 \u2212 1 integer variable, we consider both cases:\n(case b (l) i = 1) From the left clause we derive x (l) i = im (l) i . From Lemma 1 we\nhave im (l) i \u2265 0. By injecting b (l) i = 1, x (l) i = im (l) i , and im (l) i \u2265 0 to constraints (2a) to (4b), all constraints hold due to very large M (l) i .\n(case b (l) i = 0) From the right clause we derive x (l) i = 0. From Lemma 1 we have\nim (l) i < 0. By injecting b (l) i = 0, x (l) i = 0, and im (l) i < 0 to constraints (2a) to (4b), all constraints hold due to very large M (l) i . (\u21d0) If all constraints in (2a) to (4b) hold, we do case split to consider cases b (l) i = 0 and b (l) i = 1, and how they make (b (l) i = 1\u21d2 x (l) i = im (l) i ) \u2227 (b (l) i = 0\u21d2 x (l) i = 0) hold.\n(case b (l) i = 1) From (1b) and (3a) we know that x (l) i = im (l) i . (case b (l) i = 0) From (1a) and (3b) we know that x (l) i = 0.\nIn both cases, (b (l) i = 1\u21d2 x (l) i = im (l) i ) \u2227 (b (l) i = 0\u21d2 x (l) i = 0) holds.\nProposition 2 Given a feed-forward ANN with softmax output layer and a constant \u03b1 > 0, then for all i, j \u2208 {1, . . . , d(L)}:\nx (L) i1 \u2265 \u03b1x(L)i2 \u21d4 x (L\u22121) i1 \u2265 ln(\u03b1) + x(L\u22121)i2 .\nProof.\nx (L) i1 \u2265 \u03b1x(L)i2\n\u21d0\u21d2 e x (L\u22121) i1\n\u2211 j=1,...,dL e x (L\u22121) j\n\u2265 \u03b1 e x (L\u22121) i2\n\u2211 j=1,...,dL e x (L\u22121) j\n\u21d0\u21d2 x(L\u22121)i1 \u2265 ln(\u03b1) + x (L\u22121) i2"}], "references": [{"title": "Learning from data, volume 4", "author": ["Y.S. Abu-Mostafa", "M. Magdon-Ismail", "H.-T. Lin"], "venue": "AMLBook New York, NY, USA:,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Concrete problems in ai safety", "author": ["D. Amodei", "C. Olah", "J. Steinhardt", "P. Christiano", "J. Schulman", "D. Man\u00e9"], "venue": "arXiv preprint arXiv:1606.06565,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring neural net robustness with constraints", "author": ["O. Bastani", "Y. Ioannou", "L. Lampropoulos", "D. Vytiniotis", "A. Nori", "A. Criminisi"], "venue": "CoRR, abs/1605.07262,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "\u03bdZ-An Optimizing SMT Solver", "author": ["N. Bj\u00f8rner", "A.-D. Phan", "L. Fleckenstein"], "venue": "In TACAS, pages 194\u2013199. Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Certification considerations for adaptive systems", "author": ["S. Bhattacharyya", "D. Cofer", "D. Musliner", "J. Mueller", "E. Engstrom"], "venue": "In ICUAS, pages 270\u2013279. IEEE,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "End to end learning for self-driving cars", "author": ["M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba"], "venue": "CoRR, abs/1604.07316,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints", "author": ["P. Cousot", "R. Cousot"], "venue": "In POPL, pages 238\u2013252. ACM,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "Linear programming and extensions", "author": ["G. Dantzig"], "venue": "Princeton university press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "arXiv preprint arXiv:1412.6572,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Review of nonlinear mixed-integer and disjunctive programming techniques", "author": ["I.E. Grossmann"], "venue": "Optimization and engineering, 3(3):227\u2013252,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Safety verification of deep neural networks", "author": ["X. Huang", "M. Kwiatkowska", "S. Wang", "M. Wu"], "venue": "CoRR, abs/1610.06940,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Convnetjs: Deep learning in your browser (2014)", "author": ["A. Karpathy"], "venue": "URL http://cs.stanford.edu/people/karpathy/convnetjs,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Reluplex: An efficient SMT solver for verifying deep neural networks", "author": ["G. Katz", "C.W. Barrett", "D.L. Dill", "K. Julian", "M.J. Kochenderfer"], "venue": "CoRR, abs/1702.01135,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Adversarial examples in the physical world", "author": ["A. Kurakin", "I. Goodfellow", "S. Bengio"], "venue": "arXiv preprint arXiv:1607.02533,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "and C", "author": ["Y. LeCun", "C. Cortes"], "venue": "J. Burges. The mnist database of handwritten digits,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In CPVR, pages 427\u2013436,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Practical black-box attacks against deep learning systems using adversarial examples", "author": ["N. Papernot", "P. McDaniel", "I. Goodfellow", "S. Jha", "Z.B. Celik", "A. Swami"], "venue": "arXiv preprint arXiv:1602.02697,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "In Oakland, pages 582\u2013 597. IEEE,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "An abstraction-refinement approach to verification of artificial neural networks", "author": ["L. Pulina", "A. Tacchella"], "venue": "In CAV, pages 243\u2013257. Springer,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Challenging SMT solvers to verify neural networks", "author": ["L. Pulina", "A. Tacchella"], "venue": "AI Communications, 25(2):117\u2013135,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Efficient approximations for the arctangent function", "author": ["S. Rajan", "S. Wang", "R. Inkol", "A. Joyal"], "venue": "IEEE Signal Processing Magazine, 23(3):108\u2013111,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Verification of Artificial Neural Networks", "author": ["K. Scheibler", "L. Winterer", "R. Wimmer", "B. Becker Toward"], "venue": "In MBMV, pages 30\u201340,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast computation of arctangent functions for embedded applications: A comparative analysis", "author": ["A. Ukil", "V.H. Shah", "B. Deck"], "venue": "In ISIE, pages 1206\u20131211. IEEE,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational experience with a software framework for parallel integer programming", "author": ["Y. Xu", "T.K. Ralphs", "L. Lad\u00e1nyi", "M.J. Saltzman"], "venue": "INFORMS Journal on Computing, 21(3):383\u2013397,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications such as medical image processing or semi-autonomous vehicles poses a number of new assurance, verification, and certification challenges [2,5].", "startOffset": 221, "endOffset": 226}, {"referenceID": 4, "context": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications such as medical image processing or semi-autonomous vehicles poses a number of new assurance, verification, and certification challenges [2,5].", "startOffset": 221, "endOffset": 226}, {"referenceID": 13, "context": "For ANN-based end-to-end steering control of self-driving cars, for example, it is important to know how much noisy or even maliciously manipulated sensory input is tolerated [14].", "startOffset": 175, "endOffset": 179}, {"referenceID": 9, "context": "In the MIP reduction, a number of nonlinear expressions are linearized using a variant of the well-known big-M [10] encoding strategy.", "startOffset": 111, "endOffset": 115}, {"referenceID": 6, "context": "We also define a dataflow analysis [7] for generating relatively small big-M as the basis for speeding up MIP solving.", "startOffset": 35, "endOffset": 38}, {"referenceID": 14, "context": "We demonstrate the effectiveness and scalability of our approach and encoding heuristics by computing maximum perturbation bounds for benchmark sets such as MNIST [15] and agent games [16].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "We demonstrate the effectiveness and scalability of our approach and encoding heuristics by computing maximum perturbation bounds for benchmark sets such as MNIST [15] and agent games [16].", "startOffset": 184, "endOffset": 188}, {"referenceID": 24, "context": "Moreover, parallelization of branch-and-bound [25] on different computing cores can yield, up to a certain threshold, linear speed-ups using a high-performance parallelization framework.", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "An overview of concrete problems and various approaches to the safety of machine learning is provided in [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 16, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 50, "endOffset": 59}, {"referenceID": 17, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 50, "endOffset": 59}, {"referenceID": 8, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 50, "endOffset": 59}, {"referenceID": 18, "context": "Techniques including the generation of test cases [17,18,9] or strengthening the resistance of a network with respect to adversarial perturbation [19] are used for validating and improving ANNs.", "startOffset": 146, "endOffset": 150}, {"referenceID": 19, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 168, "endOffset": 172}, {"referenceID": 20, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 252, "endOffset": 262}, {"referenceID": 12, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 252, "endOffset": 262}, {"referenceID": 10, "context": "Formal methods-based approaches for verifying ANNs include abstraction-refinement based approaches [20], bounded model checking for neural network for control problems [23] and neural network verification using SMT solvers or other specialized solvers [21,13,11].", "startOffset": 252, "endOffset": 262}, {"referenceID": 3, "context": "These kinds of problems might also be addressed in SMT-based approaches either by using binary search over SMT or by using SMT solvers that support optimization such as \u03bdZ [4], but it is not clear how well these approaches scale to complex ANNs.", "startOffset": 172, "endOffset": 175}, {"referenceID": 12, "context": "Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples).", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples).", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "Recent work also targets ReLU [13] or application of a single image [11,3] (point-wise robustness or computing measures by taking samples).", "startOffset": 68, "endOffset": 74}, {"referenceID": 12, "context": "Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor.", "startOffset": 53, "endOffset": 62}, {"referenceID": 10, "context": "Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor.", "startOffset": 53, "endOffset": 62}, {"referenceID": 2, "context": "Our proposed resilience measure for ANNs goes beyond [13,11,3] in that it applies to multi-classification network using the softmax descriptor.", "startOffset": 53, "endOffset": 62}, {"referenceID": 10, "context": "Moreover, our proposed measure is a property of the classification network itself rather than just a property of a single image (as in [11]) or by only taking samples from the classifier without guarantee (as in [3]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 2, "context": "Moreover, our proposed measure is a property of the classification network itself rather than just a property of a single image (as in [11]) or by only taking samples from the classifier without guarantee (as in [3]).", "startOffset": 212, "endOffset": 215}, {"referenceID": 0, "context": "We introduce some basic concepts of feed-forward artificial neural networks (ANN) [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 12, "context": "In addition to [13] we are also considering tan\u22121, max-pooling and softmax nodes as commonly found in many ANNs in practice.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "ReLU [1, 1]", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "ReLU [1, 1]", "startOffset": 5, "endOffset": 11}, {"referenceID": 9, "context": "linearity in ReLU constraints x (l) i = max(0, im (l) i ) is handled using the well-known big-M method [10], which introduces a binary integer variable b (l) i together with a positive constant M (l) i such that \u2212M (l) i \u2264 im (l) i and x (l) i \u2264M (l) i for all possible values of im (l) i and x (l) i .", "startOffset": 103, "endOffset": 107}, {"referenceID": 6, "context": "We apply static analysis [7] based on interval arithmetic for propagating the bounded input values", "startOffset": 25, "endOffset": 28}, {"referenceID": 21, "context": "(7) of [22]) are used, and tan\u22121(im) is approximated by \u03c04 im + 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 23, "context": "Otherwise, when considering the case im > 1 or im < \u22121, the symmetry condition of tan\u22121 [24] states that (1) if im > 0 then tan\u22121(im) + tan\u22121( 1 im ) = \u03c0 2 , and (2) if im < 0 then tan\u22121(im) + tan\u22121( 1 im ) = \u22122 .", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "We are assuming that all input values (at layer l = 0) are bounded, and the output of bias nodes is restricted by the singleton [1, 1] (the value of the bias is given by the weight of a bias node).", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "We are assuming that all input values (at layer l = 0) are bounded, and the output of bias nodes is restricted by the singleton [1, 1] (the value of the bias is given by the weight of a bias node).", "startOffset": 128, "endOffset": 134}, {"referenceID": 0, "context": "Finally, the output of softmax nodes is a probability in [0, 1] which might also be further refined using interval arithmetic.", "startOffset": 57, "endOffset": 63}, {"referenceID": 2, "context": "In particular, our developments do not depend on probability distributions of training and test data as in previous work [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 14, "context": "We illustrate the underlying principles of maximum resilience using examples from the MNIST database [15] for digit recognition of input images (see Fig.", "startOffset": 101, "endOffset": 105}, {"referenceID": 11, "context": "All the networks were trained using ConvNetJS [12].", "startOffset": 46, "endOffset": 50}, {"referenceID": 14, "context": "\u2013 For MNIST digit recognition [15] has 576 input nodes for the pixels of a gray-scale image, where we trained three networks with different numbers of neurons in the hidden layers.", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": "By restricting ourselves to only verify a single input instance and by not minimizing \u03b4, the problem under verification (local robustness related to an input) is substantially simpler and is similar to those stated in [13,3].", "startOffset": 218, "endOffset": 224}, {"referenceID": 2, "context": "By restricting ourselves to only verify a single input instance and by not minimizing \u03b4, the problem under verification (local robustness related to an input) is substantially simpler and is similar to those stated in [13,3].", "startOffset": 218, "endOffset": 224}], "year": 2017, "abstractText": "The deployment of Artificial Neural Networks (ANNs) in safety-critical applications poses a number of new verification and certification challenges. In particular, for ANN-enabled self-driving vehicles it is important to establish properties about the resilience of ANNs to noisy or even maliciously manipulated sensory input. We are addressing these challenges by defining resilience properties of ANN-based classifiers as the maximum amount of input or sensor perturbation which is still tolerated. This problem of computing maximum perturbation bounds for ANNs is then reduced to solving mixed integer optimization problems (MIP). A number of MIP encoding heuristics are developed for drastically reducing MIP-solver runtimes, and using parallelization of MIP-solvers results in an almost linear speed-up in the number (up to a certain limit) of computing cores in our experiments. We demonstrate the effectiveness and scalability of our approach by means of computing maximum resilience bounds for a number of ANN benchmark sets ranging from typical image recognition scenarios to the autonomous maneuvering of robots.", "creator": "LaTeX with hyperref package"}}}