{"id": "1705.01265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2017", "title": "On the effectiveness of feature set augmentation using clusters of word embeddings", "abstract": "constraint clusters have generally gradually shown to offer important performance improvements throughout various tasks. despite academic promise, its incorporation in the standard pipeline of feature engineering dependency more on a trial - from - sell procedure indicating one picks each hyper - parameters, like the number vowel clusters to contain used. in developing [ ultimately understand the role of verb entities we deliberately evaluate phrase complexity on four expressions, those groups named vocabulary segmentation include inclusion as well as, areas of five - clause sentiment labeling and quantification. our results strongly suggest that variable membership features significantly the performance.", "histories": [["v1", "Wed, 3 May 2017 06:33:37 GMT  (16kb)", "http://arxiv.org/abs/1705.01265v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["georgios balikas", "ioannis partalas", "massih-reza amini"], "accepted": false, "id": "1705.01265"}, "pdf": {"name": "1705.01265.pdf", "metadata": {"source": "CRF", "title": "On the e\u0080ectiveness of feature set augmentation using clusters of word embeddings", "authors": ["Georgios Balikas", "Ioannis Partalas", "Massih-Reza Amini"], "emails": ["georgios.balikas@imag.fr", "ioannis.partalas@gmail.com", "Massih-Reza.Amini@imag.fr", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n01 26\n5v 1\n[ cs\n.C L\n] 3\nM ay\n2 01\n7\nWord clusters have been empirically shown to o er important performance improvements on various tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-and-error procedure where one evaluates several hyper-parameters, like the number of clusters to be used. In order to be er understand the role of such features we systematically evaluate their e ect on four tasks, those of named entity segmentation and classi cation as well as, those of ve-point sentiment classi cation and quanti cation. Our results strongly suggest that cluster membership features improve the performance.\nKEYWORDS\nText Mining; Word Embeddings; Unsupervised Learning\nACM Reference format: Georgios Balikas, Ioannis Partalas, and Massih-Reza Amini. 2016. On the e ectiveness of feature set augmentation using clusters of word embeddings. In Proceedings of ACM Conference, Washington, DC, USA, July 2017 (Conference\u201917), 5 pages. DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "Many research a empts, motivated by domain knowledge or manual labor, proposed novel features, that improve the performance in particular tasks. Although useful and o en state-of-the-art, adapting such solutions across tasks can be tricky and time-consuming [5]. erefore, simple yet general and powerful feature extraction approaches that performwell across several tasks are valuable [23].\nTraining word embeddings in an unsupervised way became extremely popular lately in Natural Language Processing (NLP) tasks [8]. Embeddings are dense vectors that project words or short text spans like phrases in a vector space where dimensions are supposed to capture text properties. e embeddings are then used either as features with algorithms like Support Vector Machines (SVMs), or to initialize deep learning systems [8]. However, as shown in [24] linear models perform be er in high-dimensional discrete spaces compared to continuous ones. is is probably the\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. Conference\u201917, Washington, DC, USA \u00a9 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 DOI: 10.1145/nnnnnnn.nnnnnnn\nmain reason of the high performance of the vector space model [21] in tasks like text classi cation with linear classi ers. Taking advantage of the expressiveness of text embeddings in the lowdimensional space to boost the performance of linear models is the focus of this work.\nIn this paper, we explore a hybrid approach, that uses text embeddings as a proxy to create features. Motivated by the argument that embeddings manage to encode the semantics of text, we explore how clustering them and incorporating cluster membership features impacts the performance of systems on di erent tasks concerning Named Entity Recognition (NER) and sentiment classi - cation. Word clusters were previously used as features in similar frameworks: Owoputi et al. [2013] use Brown clusters in a POS tagger [3], Kiritchenko et al. [2014] discuss their use on sentiment classi cation, Hee et al. [2016] incorporate them in the task of irony detection in Twi er and Ri er et al. [2011] inject them in a NER tagger. While these works showed that word clusters are bene cial no clear guidelines can be concluded of how and when to use them and the choices involved remain a trial-end-error procedure. Also parameters that impact the performance on the end tasks like the models used to obtain the embeddings, the type of source data, or the numbers of clusters are not discussed. In this work we present an empirical evaluation across diverse tasks to verify whether and when such features are useful.\nIn the rest of the paper, we empirically demonstrate that using di erent types of embeddings on four tasks with Twi er data we achieve be er or near to the state-of-the art performance. Speci - cally, we tackle the problems of (i) NER segmentation, (ii) NER classi cation, (iii) ve-point sentiment classi cation and (iv) ve-point sentiment quanti cation. For each task, adding the cluster membership features consistently bene ts the achieved performance, which indicates their e ectiveness. Moreover, comparing the behavior of the systems across tasks, results in guidelines concerning their use, which we believe to be useful for researchers and practitioners. Importantly, our evaluation compared to previous work [9] that focuses on old and well studied datasets uses recent and challenging Twi er datasets. Although we achieve state-ofthe-art performance, our analysis reveals that the performance in such tasks is far from perfect and, hence, identi es that there is still much space for improvement."}, {"heading": "2 WORD CLUSTERS", "text": "Word embeddings associate words with dense, low-dimensional vectors. Several modelswere proposed in order to train them. Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e ective. Training does not require annotated data and can be done using big amounts of text.\nSuch a model can be seen as a function f that projects a wordw in a D-dimensional space: f (w) \u2208 RD , where D is prede ned. Here, we focus on applications using Twi er data, posing di culties due to being short, using creative vocabulary, abbreviations and slung.\nTo train the embeddings for the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017. A pre-processing step has been applied to replace URLs and user mentions with placeholders and to space-pad punctuation. e nal vocabulary size is \u223c 1.6 millions words.1 Additionally to the in-domain (tweets) corpus we collected, we use GloVe vectors trained on Wikipedia articles in order to investigate the impact of out-of-domain word-vectors.2\nWe cluster the embeddings with k-Means. e k-means clusters are initialized using \u201ck-means++\u201d as proposed in [1], while the algorithm is run for 300 iterations. We try di erent values for k and for each value we repeat the clustering experiment with di erent seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia.\nOverall, our analysis evaluates the impact of the following parameters: (i) the model used to train the embeddings (skipgram, cbow, GloVe), (ii) the dimension D \u2208 {40, 100} of the embeddings, (iii) the window size w \u2208 {5, 10}, which is the size of the context used while training the models, (iv) using domain/out-pf-domain data for training the embeddings and (v) the number of clusters k \u2208 {100, 250, 500, 1000, 2000}."}, {"heading": "3 EXPERIMENTAL EVALUATION", "text": "In this section we present the evaluation se ings we used. To ensure the reproducibility of our results, we opt for using public datasets released with prede ned train/test splits. We use the designated train parts to train the proposedmethods and the test parts to report the scores of the evaluation measures we use."}, {"heading": "3.1 Named-Entity Recognition in Twitter", "text": "NER concerns the classi cation of textual segments in a prede ned set of categories, like person, organization and location. e following tweet is an example of a tweet that contains two named entities. Note the creative language used in the text, the typos and the fact that named entities may span several words:\nCLUB BLU \ufe38 \ufe37\ufe37 \ufe38\nFacil ity\ntonite.. 90\u2019s music.. oldskool night wiith dj nese \ufe38 \ufe37\ufe37 \ufe38\nMusicart ist\nNER can be decomposed into two problems: (i) text segmentation and (ii) entity categorization. e former concerns the identi cation of the entity boundaries, e.g., \u201cCLUB BLU\u201d is an entity spanning twowords that begins at word \u201cCLUB\u201d and ends at \u201cBLU\u201d. For the la er problem, given the words of an entity, one needs to identify its type, e.g. \u201cFacility\u201d for \u201cCLUB BLU\u201d. e BIO encoding is used for a ributing the corresponding labels to the tokens where B-type is used for the rst token of the entity, I-type for inside tokens in case of multi-term entities and O for non entity tokens.\nFor evaluation purposes, we use the data of the last competition in NER for Twi er which was released as a part of the 2nd\n1We trained the word embeddings using the implementations released from the authors of the papers. Unless di erently stated we used the default parameters. 2 e pre-trained vectors are obtained from h p://nlp.stanford.edu/projects/glove/\nWorkshop on Noisy User-generated Text [22]. More speci cally, the organizers provided annotated tweets with 10 named-entity types (person, movie, sportsteam, product etc.) and the task comprised two sub-tasks: 1) the detection of entity bounds and 2) the classi cation of an entity into one of the 10 types. e evaluation measure for both subtasks is the F1 measure.\nLearning algorithm Our model for solving the task is a learning to search for structured prediction tasks approach [4]. Learning to search methods decompose the problem in a search space with states, actions and policies and then learn a hypothesis controlling a policy over the state-action space. More speci cally we follow the feature extraction and learning algorithm choices of [17], which was ranked 2nd among 10 participants in the aforementioned competition [22]. e model uses handcra ed features like n-grams, part-of-speech tags, capitalization and membership in gaze eers. In the rest, we discuss the e ect of augmenting this feature set with cluster membership features for each of these two problems.\nResults Table 1 presents the F1 scores achieved in the entity segmentation and entity categorization tasks. e results are shown for varying number of clusters across for the three di erent models used to train the word embeddings. For all the experiments we keep the same parametrization for the learning algorithm and we present the performance of each run on the o cial test set.\nNotice that adding the membership features in the NER segmentation task improves the performance of the best model up to 1.1 F1-score points while it boosts the performance in the majority of cases. ere is only a single case, the glove40 vectors, where there is a drop of performance across every number of clusters used. As for the number of clusters, the best results are generally obtained when set between 250 and 1000 for all word embedding models.\nese dimensions seem to be su cient for the three-class classication problem we deal with. e di erent models for learning the word vectors perform similarly and thus one cannot privilege a certain model. Interestingly, the clusters learned on theWikipedia GloVe vectors o er competitive performance with respect to the in-domain word vectors used for the other cases showing that one can rely on out-of-domain data for constructing such representations.\nConcerning the NER classi cation task (Table 1, right) we generally observe a drop in performance compared to the NER segmentation task. Given that there are 10 NER tags in the data, the problem essentially corresponds to a multi-class problem with 21 classes: one for the non-entity type and two classes for each entity type. It is to be further noted, that although such tasks may be considered \u201csolved\u201d for formal text and small number of NER tags, in more challenging data (like tweets) and with bigger number of tags there is still much space for improvements. In this setting we notice that the best results are obtained in most cases for higher number of clusters (1000 or 2000) possibly due to a be er discriminatory power in higher dimensions. As in the case of the NER segmentation problemwe do not observe aword vectormodel that clearly outperforms the rest. Finally, we highlight the competitive performance of the Wikipedia word clusters and notably for the glove100,wiki clusters which obtain the best F1-score although trained with out-of-domain data.\nLastly, our results suggest that the size of the window w does not impact the performance greatly. is, however, was expected as tweets are short and self-contained. On the other hand, large values of window are more appropriate for larger spans like documents."}, {"heading": "3.2 Five-Point Sentiment Classi cation", "text": "e task of ve-point sentiment classi cation consists in predicting the sentiment of an input text according to a ve point scale, sentiment \u2208 {VeryNegative, Negative,. . . ,VeryPositive}. Such a scale is commonly used in review websites like Amazon and TripAdvisor. To evaluate the performance of cluster membership features on Twi er data though, we use the se ing of Task 4 of SemEval-2016 challenge, entitled \u201cSentiment Analysis in Twitter\u201d and, in particular, the dataset released by the organizers for subtask C \u201cTweet classi cation according to a ve-point scale\u201d, described in detail at [15]. In total, the training (resp. test) data that were released consist of 9,070 (resp. 20,632) tweets, each associated with a single category describing the sentiment it conveys.\ne evaluation measure selected in [15] is the macro-averaged MeanAbsolute Error (MAEM ). It is ameasure of error, hence lower values are be er. e measure\u2019s goal is to account for the ordering of the classes when penalizing the classi er\u2019s decisions. For instance, misclassifying a very negative example as very positive is a bigger mistake than classifying it as negative or neutral and MAEM captures this. Also, the advantage of using the macro- version instead of the standard version of the measure is the robustness against the class imbalance in the data, whose description we omit due to space limitations and can be found at [15].\nLearning algorithmTo demonstrate the e ciency of clustermembership features we rely on the system of [2] which was ranked 1st among 11 participants and uses a Logistic Regression as a learning algorithm. We follow the same feature extraction steps which consist of extracting n-gram and character n-gram features, partof-speech counts as well as sentiment scores using standard sentiment lexicons such as the Bing Liu\u2019s [11] and the MPQA lexicons [25]. e complete description of the features is available at [2].\nResults To evaluate the performance of the proposed feature augmentation technique, we present in Table 2 the MAEM scores for di erent se ings on the o cial test set of [15]. First, notice that the best score in the test data is achieved using cluster membership features, where the word embeddings are trained using the skipgram model. e achieved score improves the state-of-the art on the dataset, which to the best of our knowledge was by [2]. Also, note that the score on the test data improves for each type of embeddings used, which means that augmenting the feature space using cluster membership features helps the sentiment classi cation task. Note, also, that using the clusters produced by the outof-domain embeddings trained onWikipedia that were released as part of [18] performs suprisingly well. One might have expected their addition to hurt the performance. However, their value probably stems from the sheer amount of data used for their training as well as the relatively simple type of words (like awesome, terrible) which are discriminative for this task. Lastly, note that in each of the se ings, the best results are achieved when the number of clusters is within {500, 1000, 2000} as in the NER tasks. Comparing the performance across the di erent embeddings, one cannot claim that a particular embedding performs be er. It is evident though that augmenting the feature space with feature derived using the proposed method, preferably with in-domain data, helps the classi cation performance and reduces MAEM ."}, {"heading": "3.3 Five-point Sentiment anti cation", "text": "anti cation is the problem of estimating the prevalence of a class in a dataset. While classi cation concerns assigning a category to a single instance, like labeling a tweet with the sentiment it conveys, the goal of quanti cation is, given a set of instances, to estimate the relative frequency of single class. erefore, sentiment quanti cation tries to answer questions like \u201cGiven a set of tweets about the new iPhone, what is the fraction of VeryPositive ones?\u201d. In the rest, we show the e ect of the features derived from the word embeddings clusters in the ve-point quanti cation problem, which was the Subtask E of the SemEval-2016 \u201cSentiment Analysis in Twi er\u201d task [15]. Note that the data of the task are the same with the ve-point classi cation data but they are further associated with subjects (described in full detail at [15]), and, hence, quanti cation is performed for the tweets of a subject. For each of the ve categories of sentiment, the output of the quanti cation approach must be a 5-dimensional vector with the estimated prevalence of the categories.\nLearning Algorithm To perform the quanti cation task, we rely on a classify and count approach [7], which was shown e ective in a related binary quanti cation problem [2]. e idea is that given the tweets of a subject, one rst classi es them and then aggregates the counts. To this end, we use the same feature representation steps and data with the ones used for ve-point classi cation (Section 3.2). Also, we use the same base classi er, which is a Logistic Regression trained using the one-ve-rest approach for the 5-class problem.\ne evaluation measure for the problem is the EarthMovers Distance (EMD) [20]. EMD is a measure of error, hence lower values are be er. It assumes ordered categories, which in our problem is\nnaturally de ned. Further assuming that the distance of consecutive categories (e.g., Positive and VeryPositive) is 1, the measure is calculated by:\nEMD(p, p\u0302) =\n|C |\u22121\u2211\nj=1\n|\nj\u2211\ni=1\np\u0302(ci ) \u2212\nj\u2211\ni=1\np(ci ) |\nwhere |C | is number of categories ( ve in our case) and p\u0302(ci ) and p(ci ) are the true and predicted prevalence respectively [6].\nResults Table 2 presents the results of augmenting the feature set with the proposed features. Notice the positive impact of the features in the performance in the task. Adding the features derived from clustering the embeddings consistently improves the performance. Interestingly, the best performance (0.219) is achieved using the out-of-domain vectors, as in the NER classi cation task. in most of the cases, the skipgram model obtains the best results, while as in the NER tasks, the window size w does not seem to impact the performance. Also, notice how the proposed approach improves over the previous state-of-the-art (0.243) performance in the challenge [15], held by the method of [13]. e improvement over the method of [13] however, does not necessarily entail that classify and count should be preferred for the task. It only implies that the features we used are richer, highlighting the value of robust feature extraction mechanisms which is the subject of this paper.\nFrom the results of Table 2 it is clear that the addition of the cluster membership features improves the sentiment classi cation and quanti cation performance. To be er understand though why these clusters help, we manually examined a sample of the words associated with the clusters. To improve the eligibility of those results we rst removed the hashtags and we lter the results using an English vocabulary. In Table 3 we present sample words from two of the most characteristic clusters with respect to the task of sentiment classi cation. Notice how words with positive and negative meanings are put in the respective clusters."}, {"heading": "4 CONCLUSION", "text": "We have shown empirically the e ectiveness of incorporating cluster membership features in the feature extraction pipeline of NER,\nsentiment classi cation and quanti cation tasks. Our results strongly suggest that incorporating clustermembership features bene t the performance in the tasks. e fact that the performance improvements are consistent in the four taskswe investigated, further highlights their usefulness, both for practitioners and researchers.\nAlthough our study does not identify a clear winner with respect to the type of word vectors (skipgram, cbow, or GloVe), our\nndings suggest that one should rst try skip-gram embeddings of low dimensionality (D = 40), high number of clusters (e.g., K \u2208 {500, 1000, 2000}) and small window values w (e.g., w = 5) as the results obtained using these se ings are consistently competitive. Our results, also suggest that using out-of-domain data, likeWikipedia articles in this case, to construct the word embeddings is a good practice, as the results we obtainedwith these vectors are also competitive. e positive e ect of out-of-domain embeddings and their combination with in-domain ones remains to be further studied."}], "references": [{"title": "k-means++: \u008ce advantages of careful seeding", "author": ["David Arthur", "Sergei Vassilvitskii"], "venue": "In ACM-SIAM@Discrete algorithms", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "TwiSE at SemEval-2016 Task 4: Twi\u008aer Sentiment Classi\u0080cation", "author": ["Georgios Balikas", "Massih-Reza Amini"], "venue": "SemEval@NAACL-HLT", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Class-Based n-gram Models of Natural Language", "author": ["Peter F. Brown", "Peter V. deSouza", "Robert L. Mercer", "Vincent J. Della Pietra", "Jenifer C. Lai"], "venue": "Computational Linguistics", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "E\u0081cient programmable learning to search", "author": ["Hal Daum\u00e9 III", "John Langford", "St\u00e9phane Ross"], "venue": "CoRR abs/1406.1837", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "A Paraphrase and Semantic Similarity Detection System for User Generated Short-Text Content on Microblogs", "author": ["Kuntal Dey", "Ritvik Shrivastava", "Saroj Kaushik"], "venue": "In COLING", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Sentiment \u008banti\u0080cation", "author": ["Andrea Esuli", "Fabrizio Sebastiani"], "venue": "IEEE Intelligent Systems 25,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "\u008bantifying counts and costs via classi\u0080cation", "author": ["George Forman"], "venue": "Data Mining and Knowledge Discovery 17,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Revisiting Embedding Features for Simple Semi-supervised Learning", "author": ["Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu"], "venue": "In EMNLP", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "2016. Monday mornings are my fave : ) #not Exploring the Automatic Recognition of Irony in English tweets", "author": ["Cynthia Van Hee", "Els Lefever", "V\u00e9ronique Hoste"], "venue": "In COLING", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Mining and summarizing customer reviews", "author": ["Minqing Hu", "Bing Liu"], "venue": "In SIGKDD", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Ordinal Text \u008banti\u0080cation", "author": ["Giovanni Da San Martino", "Wei Gao", "Fabrizio Sebastiani"], "venue": "In SIGIR", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "E\u0081cient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Je\u0082rey Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "SemEval-2016 Task 4: Sentiment Analysis in Twi\u008aer", "author": ["Preslav Nakov", "Alan Ri\u008aer", "Sara Rosenthal", "Fabrizio Sebastiani", "Veselin Stoyanov"], "venue": "SemEval@NAACL-HLT", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters", "author": ["Olutobi Owoputi", "BrendanO\u2019Connor", "ChrisDyer", "KevinGimpel", "Nathan Schneider", "Noah A. Smith"], "venue": "In NAACL", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Learning to Search for Recognizing Named Entities in Twi\u008aer", "author": ["Ioannis Partalas", "C\u00e9dric Lopez", "Nadia Derbas", "Ruslan Kalitvianski"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "GloVe: Global Vectors for Word Representation", "author": ["Je\u0082rey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Named Entity Recognition in Tweets: An Experimental Study", "author": ["Alan Ri\u008aer", "Sam Clark", "Mausam", "Oren Etzioni"], "venue": "In EMNLP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "\u008ce earth mover\u2019s distance as a metric for image retrieval", "author": ["Yossi Rubner", "Carlo Tomasi", "Leonidas J Guibas"], "venue": "International journal of computer vision 40,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "A Vector Space Model for Automatic Indexing", "author": ["Gerard Salton", "A. Wong", "C.S. Yang"], "venue": "Commun. ACM 18,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1975}, {"title": "Results of the WNUT16 Named Entity Recognition", "author": ["Benjamin Strauss", "Bethany E. Toma", "Alan Ri\u008aer", "Marie Catherine de Marne\u0082e", "Wei Xu"], "venue": "Shared Task. InWNUT@COLING", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In ACL", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "E\u0082ect of Non-linear Deep Architecture in Sequence Labeling", "author": ["Mengqiu Wang", "Christopher D Manning"], "venue": "In IJCNLP", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis", "author": ["\u008ceresa Wilson", "Janyce Wiebe", "Paul Ho\u0082mann"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Although useful and o\u0089en state-of-the-art, adapting such solutions across tasks can be tricky and time-consuming [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 20, "context": "\u008cerefore, simple yet general and powerful feature extraction approaches that performwell across several tasks are valuable [23].", "startOffset": 123, "endOffset": 127}, {"referenceID": 21, "context": "However, as shown in [24] linear models perform be\u008aer in high-dimensional discrete spaces compared to continuous ones.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "nnnnnnn main reason of the high performance of the vector space model [21] in tasks like text classi\u0080cation with linear classi\u0080ers.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "[2013] use Brown clusters in a POS tagger [3], Kiritchenko et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 7, "context": "Importantly, our evaluation compared to previous work [9] that focuses on old and well studied datasets uses recent and challenging Twi\u008aer datasets.", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e\u0082ective.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e\u0082ective.", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Among others, the skipgram (skipgram) model with negative sampling [14], the continuous bag-of-words (cbow) model [14] and GloVe (glove) [18] have been shown to be e\u0082ective.", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "\u008ce k-means clusters are initialized using \u201ck-means++\u201d as proposed in [1], while the algorithm is run for 300 iterations.", "startOffset": 69, "endOffset": 72}, {"referenceID": 19, "context": "edu/projects/glove/ Workshop on Noisy User-generated Text [22].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "Learning algorithm Our model for solving the task is a learning to search for structured prediction tasks approach [4].", "startOffset": 115, "endOffset": 118}, {"referenceID": 14, "context": "More speci\u0080cally we follow the feature extraction and learning algorithm choices of [17], which was ranked 2nd among 10 participants in the aforementioned competition [22].", "startOffset": 84, "endOffset": 88}, {"referenceID": 19, "context": "More speci\u0080cally we follow the feature extraction and learning algorithm choices of [17], which was ranked 2nd among 10 participants in the aforementioned competition [22].", "startOffset": 167, "endOffset": 171}, {"referenceID": 12, "context": "To evaluate the performance of cluster membership features on Twi\u008aer data though, we use the se\u008aing of Task 4 of SemEval-2016 challenge, entitled \u201cSentiment Analysis in Twitter\u201d and, in particular, the dataset released by the organizers for subtask C \u201cTweet classi\u0080cation according to a \u0080ve-point scale\u201d, described in detail at [15].", "startOffset": 328, "endOffset": 332}, {"referenceID": 12, "context": "\u008ce evaluation measure selected in [15] is the macro-averaged MeanAbsolute Error (MAEM ).", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "Also, the advantage of using the macro- version instead of the standard version of the measure is the robustness against the class imbalance in the data, whose description we omit due to space limitations and can be found at [15].", "startOffset": 225, "endOffset": 229}, {"referenceID": 1, "context": "Learning algorithmTo demonstrate the e\u0081ciency of clustermembership features we rely on the system of [2] which was ranked 1st among 11 participants and uses a Logistic Regression as a learning algorithm.", "startOffset": 101, "endOffset": 104}, {"referenceID": 9, "context": "We follow the same feature extraction steps which consist of extracting n-gram and character n-gram features, partof-speech counts as well as sentiment scores using standard sentiment lexicons such as the Bing Liu\u2019s [11] and the MPQA lexicons [25].", "startOffset": 216, "endOffset": 220}, {"referenceID": 22, "context": "We follow the same feature extraction steps which consist of extracting n-gram and character n-gram features, partof-speech counts as well as sentiment scores using standard sentiment lexicons such as the Bing Liu\u2019s [11] and the MPQA lexicons [25].", "startOffset": 243, "endOffset": 247}, {"referenceID": 1, "context": "\u008ce complete description of the features is available at [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "Results To evaluate the performance of the proposed feature augmentation technique, we present in Table 2 the MAEM scores for di\u0082erent se\u008aings on the o\u0081cial test set of [15].", "startOffset": 169, "endOffset": 173}, {"referenceID": 1, "context": "\u008ce achieved score improves the state-of-the art on the dataset, which to the best of our knowledge was by [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 15, "context": "Note, also, that using the clusters produced by the outof-domain embeddings trained onWikipedia that were released as part of [18] performs suprisingly well.", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "In the rest, we show the e\u0082ect of the features derived from the word embeddings clusters in the \u0080ve-point quanti\u0080cation problem, which was the Subtask E of the SemEval-2016 \u201cSentiment Analysis in Twi\u008aer\u201d task [15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": "sociated with subjects (described in full detail at [15]), and, hence, quanti\u0080cation is performed for the tweets of a subject.", "startOffset": 52, "endOffset": 56}, {"referenceID": 6, "context": "Learning Algorithm To perform the quanti\u0080cation task, we rely on a classify and count approach [7], which was shown e\u0082ective in a related binary quanti\u0080cation problem [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Learning Algorithm To perform the quanti\u0080cation task, we rely on a classify and count approach [7], which was shown e\u0082ective in a related binary quanti\u0080cation problem [2].", "startOffset": 167, "endOffset": 170}, {"referenceID": 17, "context": "\u008ce evaluation measure for the problem is the EarthMovers Distance (EMD) [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "243\u2020 described at [13]) skipgram40,w5 .", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "where |C | is number of categories (\u0080ve in our case) and p\u0302(ci ) and p(ci ) are the true and predicted prevalence respectively [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 12, "context": "243) performance in the challenge [15], held by the method of [13].", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "243) performance in the challenge [15], held by the method of [13].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "\u008ce improvement over the method of [13] however, does not necessarily entail that classify and count should be preferred for the task.", "startOffset": 34, "endOffset": 38}], "year": 2017, "abstractText": "Word clusters have been empirically shown to o\u0082er important performance improvements on various tasks. Despite their importance, their incorporation in the standard pipeline of feature engineering relies more on a trial-and-error procedure where one evaluates several hyper-parameters, like the number of clusters to be used. In order to be\u008aer understand the role of such features we systematically evaluate their e\u0082ect on four tasks, those of named entity segmentation and classi\u0080cation as well as, those of \u0080ve-point sentiment classi\u0080cation and quanti\u0080cation. Our results strongly suggest that cluster membership features improve the performance.", "creator": "LaTeX with hyperref package"}}}