{"id": "1705.03414", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2017", "title": "A Distributed Learning Dynamics in Social Groups", "abstract": "together calculate a simultaneous learning process observed vs ordinary humans versus other social animals. this gap theory appears amid settings in which each instance in everyone group is free to decide over time, in a distributed plan, which option to select among a shared set of options. third, ; consider zero stochastic circuit showing a statistical exhibit which every element selects an activity in the following 10 - step process : ( 1 ) select a token individual as approve the option that individual makes in the previous time step, but ( secondary ) adopt that proposition unless unspecified drastic consequences was exhibited at that time step. various instantiations of dynamic distributed learning occur in research, and contain also cases studied in most traditional science environment. illustrating the perspective among an individual, an attractive feature when this learning phenomenon opens suggesting it is a discrete heuristic process sacrifices extremely limited computational capacities. but what does it explain for these self - - wherein we a symmetric, distributed and constrained memoryless matrix lead evolutionary behavior as a incentive to reproduce optimally? we show that the answer to this question is yes - - this distributed learning is highly affirmative at identifying the best option and is linear : optimal for the group individually. our analysis also gives quantitative methods ; offer fast resolution of these natural dynamics. prior to our experiments you only mainstream work related showing such these dynamics as been either extended deterministic special edition or understanding the asymptotic system. otherwise, our observe that any infinite population constraints reveals any large variant of the matrix multiplicative weights update ( mwu ) method. likewise, we arrive here the earliest interesting converse : the weighted patterns among purely finite society considered here can be viewed under a novel alternative and low - precision implementation including true classic mwu method.", "histories": [["v1", "Mon, 8 May 2017 15:15:18 GMT  (22kb)", "http://arxiv.org/abs/1705.03414v1", "To appear in PODC 2017"]], "COMMENTS": "To appear in PODC 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CY cs.DC cs.DS", "authors": ["l elisa celis", "peter m krafft", "nisheeth k vishnoi"], "accepted": false, "id": "1705.03414"}, "pdf": {"name": "1705.03414.pdf", "metadata": {"source": "CRF", "title": "A Distributed Learning Dynamics in Social Groups", "authors": ["L. Elisa Celis", "Peter M. Krafft", "Nisheeth K. Vishnoi"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n03 41\n4v 1\n[ cs\n.L G\n] 8\nM ay\nWe study a distributed learning process observed in human groups and other social animals. This learning process appears in settings in which each individual in a group is trying to decide over time, in a distributed manner, which option to select among a shared set of options. Specifically, we consider a stochastic dynamics in a group in which every individual selects an option in the following two-step process: (1) select a random individual and observe the option that individual chose in the previous time step, and (2) adopt that option if its stochastic quality was good at that time step. Various instantiations of such distributed learning appear in nature, and have also been studied in the social science literature. From the perspective of an individual, an attractive feature of this learning process is that it is a simple heuristic that requires extremely limited computational capacities. But what does it mean for the group \u2013 could such a simple, distributed and essentially memoryless process lead the group as a whole to perform optimally? We show that the answer to this question is yes \u2013 this distributed learning is highly effective at identifying the best option and is close to optimal for the group overall. Our analysis also gives quantitative bounds that show fast convergence of these stochastic dynamics. We prove our result by first defining a (stochastic) infinite population version of these distributed learning dynamics and then combining its strong convergence properties along with its relation to the finite population dynamics. Prior to our work the only theoretical work related to such learning dynamics has been either in deterministic special cases or in the asymptotic setting. Finally, we observe that our infinite population dynamics is a stochastic variant of the classic multiplicative weights update (MWU) method. Consequently, we arrive at the following interesting converse: the learning dynamics on a finite population considered here can be viewed as a novel distributed and low-memory implementation of the classic MWU method.\n\u2217This research was supported in part by an SNF Project Grant (205121 163385)."}, {"heading": "1 Introduction", "text": "A powerful assumption \u2013 often leveraged in biology, ecology, and evolutionary psychology in order to reason about why humans, animals, and other biological organisms behave in certain ways \u2013 is to suppose that behavior is tuned to alleviate evolutionary pressure. This assumption provides deductive power because once a behavior is assumed to be optimally or near-optimally solving some problem, one can then attempt to discoverwhich problem the systemmight be solving through the computational lens. We study this phenomenon in the context of social behavior. In particular, we consider a class of distributed social learning dynamics which are at once conspicuous in daily life, oft discussed in the social science literature, and also empirically verified; yet are also simple to the point that they appear perhaps suboptimal. Consider the setting consisting of a social group of N individuals presented with a set of m options of different quality. The quality of each option is assumed to be an independent random variable whose parameters are unknown to the individuals and remain fixed over time. At each time step each individual selects one option and observes a stochastic indicator of that option\u2019s quality. The goal of the individual is to identify the best option.\nThe distributed learning dynamics then boils down to individuals copying or imitating the behavior of others in an effort to solve this problem. Such dynamics roughly have the following two steps: At each time step, each individual independently decides which option to select by first\n1. Sampling \u2013 observing the choice of a random member of the group at the last time step, and then\n2. Adopting \u2013 deciding whether or not to adopt the recommended option as a function of the most recent\n(stochastic) signal of that option\u2019s quality.\nInstances of such two-stage distributed learning dynamics in social settings have been widely proposed and validated with data in the literature on human choice behavior (e.g., [7, 10, 29, 32, 34]) and animal behavior (e.g., [40, 43]). They are cognitively simple because individuals need not maintain any history of previous observations; rather they only use the most recent quality signal of one option. Furthermore, as with many other distributed protocols observed in nature (e.g., [27,35]), each step requires only limited communication with other group members. In light of the fact that such distributed learning processes are ubiquitous, the question arises \u2013 why? To answer this, we need to understand the following: What does such a distributed learning dynamics imply for the group as a whole?\nOur Contribution.\nWe consider a general model that captures a wide variety of distributed learning dynamics in social settings as defined above (see Section 2.1 for a formal definition) and study two fundamental algorithmic questions:\n\u2022 do such learning dynamics (despite each individual having a limited memory) have the potential to successfully converge to the best option for the collective population? and if so,\n\u2022 how efficient are such dynamics? We prove that the answer to the first question is yes and provide quantitative bounds for the second in the most general case in which the population is finite and both the sampling and adopting step are stochastic.\nIn general, due to the fact that populations are finite and there is stochasticity in both steps, the social learning dynamics may be chaotic, with no single option dominating, and the popularity of options rising and falling\nover time. However, we prove that social learning leads the population, as a whole, to be competitive \u2013 pretty quickly \u2013 as compared to the best strategy in hindsight; i.e., the dynamics have low regret (see section 2.1). Prior to our work, many instances of such social learning dynamics have been proposed and validated (see, e.g., [39] for an exposition), and, even though on the surface there seem to be several related processes, the only theoretical work has been either in deterministic special cases or in the asymptotic setting where both the size of the population and time goes to infinity; such results (see [12,22] and Section 3) effectively focus on deriving asymptotic (large deviation) bounds. To the best of our knowledge, ours is the first rigorous analysis of this distributed learning dynamics in a realistic setting when the size of the group is finite.\nKey to our results are the following two realizations which we can use to understand the emergent behavior of the distributed learning dynamics:\n\u2022 In the infinite population limit, by rewriting the underlying stochastic equations of the distributed learning dynamics, the individuals are effectively implementing a stochastic variant of the classic mul-\ntiplicative weights update (MWU) method [4] at the group-level.1\n\u2022 While the finite population distributed learning dynamics can be approximated with its infinite population limit for short times, to ensure that the regret bounds remain valid for longer times, further\nnew ideas are required. Here, we show how we can appeal to the strong convergence properties of the infinite population stochastic dynamics to ascertain that the regret remains bounded for all times in the finite population case.\nComputationally, contrasting with typical implementations of the MWU method, in the learning dynamics we consider, no individual keeps track of the weights. Rather, the popularity of the options in the previous time step serve as a proxy for weights, and suffice to propel the process forward. Thus, we arrive at the following interesting conclusion \u2013 the learning dynamics in social groups considered here can inform novel, low-memory, low-communication, distributed implementations of the MWU algorithm in the stochastic setting; perhaps appropriate for low-power devices in distributed settings such as sensor networks or the internet-of-things."}, {"heading": "2 Model, Our Results and Overview", "text": ""}, {"heading": "2.1 The Model", "text": "The learning environment we consider consists of a set ofN individuals repeatedly choosing amongm options during a sequence of T discrete time steps. Each option has an unknown underlying quality, \u03b71 \u2265 \u03b72 \u2265 \u00b7\u00b7 \u00b7 \u2265 \u03b7m \u2208 [0,1], which represents the probability that the option is \u201cgood\u201d at any given time step; we let Rtj = Bernoulli(\u03b7 j) be the indicator random variable for the event that option j is good at time t. The goal of each individual is to select the best option. Now, let X ti j \u2208 {0,1} be the indicator random variable for the event that individual i chooses to adopt option j at time t. The distributed learning dynamics we consider is a two-stage model:\n1This stochastic process is not to be mistaken with the standard deterministic MWU or its continuous\ntime limit, the replicator dynamics.\n(1) Sampling. First, individuals select which option to consider, and then they choose whether or not to adopt that option. To obtain an option to consider at time t + 1, with probability \u00b5 individual i selects an option j \u2208 [m] to consider uniformly at random, and with probability (1\u2212 \u00b5) individual i selects an option j \u2208 [m] proportional to its current popularity:\nQtj = \u2211Ni=1 X t i j\n\u2211mk=1 \u2211 N i=1 X t ik\nis the fraction of the population that adopts option j at time t, and we assume Q0j = 1 m for all j.2 Note that this can be implemented in a distributed manner as suggested in the introduction by letting i select a companion i\u2032 \u2208 {1, . . . ,N} uniformly at random, and observing the choice of individual i\u2032 at time t. The parameter \u00b5 > 0 is small and represents a fraction of the population which may take independent decisions; its role is to ensure that the population does not get stuck in a bad option.\n(2) Adopting. In the second stage, after choosing an option j to consider, individual i must then decide whether to commit to this option or to sit out during this time step. Individual i observes the most recent quality signal associated with j, namely Rt+1j and decides to adopt the option with probability fi(R t+1 j ) \u2208 {0,1} where fi is a stochastic function such that E[ fi(1)]>E[ fi(0)]. Hence, for each i, we can express fi in the following form:\nfi(R t+1 j ) =\n \n\n1 with probability \u03b2i if R t+1 j = 1 1 with probability \u03b1i if R t+1 j = 0 0 otherwise.\nHere, \u03b1i \u2264 \u03b2i are parameters of the model and represent how sensitive individuals are to the most recent signal of goodness as compared to the weight they give to the recommendation. For simplicity in the exposition, we assume that all fi are identical, and drop the index i. This assumption is not essential for our results \u2013 we omit the details. Thus, the two relevant parameters are 0 \u2264 \u03b1 \u2264 \u03b2 \u2264 1.\nExamples of the model. Many instances of social learning in the social sciences and economics literature can be interpreted as special cases of the distributed learning dynamics introduced above. Here we give two concrete examples \u2013 one direct and one indirect; see more discussion in Section 3. The simplest such example [31] corresponds exactly to our model when\u03b1 = 1\u2212\u03b2 for some \u03b2 \u2265 1 2 when \u03b71 > 1 2 =\u03b72 = \u00b7 \u00b7 \u00b7=\u03b7m. The authors validate this model using observational data on the decisions of amateur investors on an online platform in which users are able to copy the actions of others.\nAnother instance, which takes a bit of explanation, appears in the economics literature [22]. We present it here because it illustrates two common ways in which more general-looking models, specifically ones with continuous-valued rewards and reward differences across individuals, can often be reinterpreted in such a way that our framework applies. The authors consider a learning setting where m = 2 and rewards rtj are drawn from a continuous-valued distribution F j. Furthermore, their model incorporates player-specific stochastic shocks, so that if \u03b5 ti j \u223c G is the size of the shock to player i on option j at time t, then the reward to player i is rtj + \u03b5 t i j. The sampling step (1) is similar, except that \u00b5 = 0. In the adoption step (2), if player i sampled\n2In the absence of prior knowledge, we initialize at the point where all options are equally popular. This also simplifies the exposition, but is not crucial to our results \u2013 our results hold from arbitrary initial conditions.\nplayer i\u2032, then player i adopts option 1 if rt1 + \u03b5 t i1 + \u03b5 t i\u20321 > r t 2 + \u03b5 t i2 + \u03b5 t i\u20322, and adopts option 2 otherwise. To convert this to our setting, let Rt1 (and R t 2) be the indicator random variable for the event that r t 1 > r t 2 (and rt1 < r t 2); this occurs with some probability p (and 1\u2212 p) and defines our parameters \u03b71 = p > 1\u2212 p = \u03b72.3 Note that in the adoption step, we can replace \u03b5 ti1 + \u03b5 t i\u20321 \u2212 \u03b5 ti2 \u2212 \u03b5 ti\u20322 by a continuous random variable \u03be . Then \u03b2 = P [\u03be > rt2 \u2212 rt1|rt1 > rt2] and \u03b1 = P [\u03be > rt2 \u2212 rt1|rt2 > rt1]. As the \u03b5 ti js are i.i.d., \u03be has zero mean and is symmetric, hence \u03b1 < \u03b2 and our results apply. The authors consider the infinite population version of this model where a constant fraction of the population updates at each time step, and analyze its asymptotic properties as T \u2192 \u221e."}, {"heading": "2.2 Our Results and Overview", "text": "In order to explain our results for the distributed learning dynamics we first need to quantify a measure of optimality. In the remainder of the paper we let \u03b1 = 1\u2212 \u03b2 . This just simplifies the reading and has no impact on the statements of the theorem \u2013 the same bounds hold with a dependence on\n\u03b2 \u03b1 as opposed to \u03b2 1\u2212\u03b2 .\nLet Qtj be the random variable which measures the fraction of individuals that chose option j at time t, as defined above. Wishfully, we might like to prove that as t becomes large, Qtjs, for all j 6= 1 are close to zero (assuming \u03b71 > \u03b72). However, simple examples show that this may not be the case; the stochastic process is non-monotone, even when there is a significant gap between \u03b71 and \u03b72, and may step away significantly from Qt1 \u2248 1 even for large t. Instead, we consider the average expected performance of the group when compared to that of the best option:\nRegretN(T ) := \u03b71 \u2212 1\nT\nT\n\u2211 t=1\nm\n\u2211 j=1\nE[Qt\u22121j R t j].\nAs the name suggests, this is nothing but the average regret of this process; namely, the difference between the group\u2019s expected average reward if all individuals who adopted an option select the optimal j = 1, and the group\u2019s expected average reward selected according to the distributed learning process up to time T . The following is the main technical contribution of the paper; see Theorem 4.4 for a formal statement of this result.\nDistributed learning achieves near-optimal regret in a social group. For a range of parameters 1/2 \u2264 \u03b2 \u2264 e/(e+1), \u00b5 a small constant, N roughly at least m1/\u03b4 2 and for all T \u2265 lnm\n\u03b4 2 , RegretN(T ) is at most 6\u03b4 where\n\u03b4 = ln (\n\u03b2 1\u2212\u03b2\n)\n. Thus, the closer \u03b2 is to 1/2, the better the regret.\nThe proof of the above result relies on the following connection between our distributed learning dynamics in a finite population and what can be thought of as an infinite population variant of the distributed learning dynamics. The latter can also be seen as a stochastic variant of the MWU method (see Lemma 4.5) which we explain below. Consider m experts where expert j generates a stochastic reward Rt+1j at time t that is 1 with probability \u03b7 j and 0 otherwise. In this setting, a single player maintains weight W t j for option j at time t, which is updated multiplicatively in the following manner:4\n3Note that in their model the Rt1 and R t 2 are correlated as exactly one of them is 1 in every time step; however independence across t remains which suffices for our results. 4It is worth noticing the similarity between the weights update (in particular the first term) and that used in the result of Christiano et al. [18] who developed a variant of the MWU method in the design of a fast algorithm for a flow problem.\nW t+1j :=\n(\n(1\u2212\u00b5)W tj + \u00b5\nm\nm\n\u2211 k=1\nW tk\n)\n\ufe38 \ufe37\ufe37 \ufe38\ndeterministic sampling\n\u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j \ufe38 \ufe37\ufe37 \ufe38\nstochastic rewards\nand W 0j = 1 for all j. We arrive at this update equation by the (non-rigorous) thought process that in the infinite population case, we can replace the stochastic quantities by their expectation in the sampling stage: in this case, the expected fraction of individuals picking the option j in the sampling stage of the social learning dynamics is proportional to (1\u2212 \u00b5)W tj + \u00b5m \u2211mk=1W tk . This gives us the first term in the right-hand side above. The second term is just f (Rt+1j ) with respect to parameter \u03b2 . We note that, since the rewards are stochastic, it is not the standard adversarial MWU setting and, since all the information is known to the population as a whole, it is also not the standard setting of stochastic bandits.\nEven though these weight update equations are arrived at by a heuristic calculation, we can prove that for short time periods, there is a coupling between the infinite and the finite distributed learning dynamics such that the stochastic trajectories corresponding to the weights in the infinite population case remain close to that of the finite population case.\nInfinite vs. finite population distributed social learning. If (Ptj) m j=1 is the probability distribution induced by the weights W tj after time t for a given sequence of rewards, then for all j, 1\u2212 5 t\u221a N \u2264 Ptj/Qtj \u2264 1+ 5 t\u221a N .\nThe proof of this crucially relies on the fact that \u00b5 is strictly positive. On the other hand, the fact that \u00b5 > 0 also makes the analysis quite messy. Note that the closeness deteriorates very quickly and, in particular, the bound becomes uninteresting after about logN time steps. On the other hand, for fixed t, as N \u2192 \u221e, the trajectories of the two processes are identical; put another way, the distributed learning process over infinite populations is identical to the corresponding stochastic MWU method. This is typically the kind of asymptotic result that exists in the literature. The more interesting and challenging direction is to show convergence when N is large, but fixed, and T goes to infinity.\nIn order to leverage this connection between infinite and finite population variants of the distributed learning dynamics, we first analyze the corresponding regret of the stochastic MWU method, which is defined to be\nRegret\u221e(T ) := \u03b71 \u2212 1\nT\nT\n\u2211 t=1\nm\n\u2211 j=1\nE[Pt\u22121j R t j].\nWe can establish the following regret bound in this case; see Theorem 4.3.\nInfinite population distributed social learning dynamics achieves near-optimal regret. For a range of parameters 1/2 \u2264 \u03b2 \u2264 e/(e+1), \u00b5 a small constant, and for all T \u2265 lnm \u03b4 2 , Regret\u221e(T ) is at most 3\u03b4 where \u03b4 = ln (\n\u03b2 1\u2212\u03b2\n)\n.\nThe proof of this obtained by adapting the proof of the MWUmethod to take into account stochastic rewards.\nAt this point we would be done except that the closeness between the probability distributions of finite and infinite distributed learning dynamics holds only for short times \u2013 if we run the process for about lnm \u03b4 2 steps, then for the probability distributions to be close we would need N \u2265 mO(1/\u03b4 2). What about when T \u2265 lnm \u03b4 2 ? To tackle this problem, we need a new idea. The first observation is that, in fact, the regret bound for the\ninfinite population case can be made stronger: roughly, as long as the starting distribution has enough entropy, the regret becomes small in about the same number of steps. Thus, we can break T into epochs of size approximately lnm \u03b4 2\nand observe that at the beginning of each epoch, each option will have about \u00b5/m probability \u2013 again we need crucially that \u00b5 > 0. Thus, we can show convergence starting from such a probability distribution in ln(m/\u00b5)\n\u03b4 2 iterations. As a consequence, in each epoch, we can bound the regret by about\n6\u03b4 for slightly larger N. Rewriting it a different way, having a finite population will have an additive error term of approximately m 1/\u03b4 2\u221a N to the regret obtained by the infinite population. Thus, we rely on the strong attractive properties of the infinite population stochastic dynamics to obtain quantitative regret bounds for the finite population social learning dynamics. This contributes to the growing set of connections between using attractors of dynamical systems to analyze stochastic processes [8, 38, 49, 50]. Details of the proof of Theorem 4.4 appear in Section 4.3."}, {"heading": "3 Related Work", "text": "There is a growing body of work in theoretical computer science, and distributed computing theory in particular, that studies problems arising in the sciences through the computational lens; see, e.g., [1,17,19,27,35,46]. Such studies have also on occasion contributed back to computer science by providing insights into existing techniques, or giving rise to novel bio-inspired algorithms. Our work touches upon both of these aspects.\nAmong related studies in the sciences, while dynamics that only have a sampling step [14, 23] or only an adoption step [30] have been studied, combining both has been shown empirically to result in a better strategy [34]. Indeed, in line with these observations, one can formally see from our analysis that if we only have sampling (\u03b2 = 1\u2212\u03b1 = 1) or only have adoption (\u00b5 = 1), the process does not always converge to the best option. Hence, both steps of the process seem crucial, and many models in sociology and economics are such distributed two-step processes [5, 15, 22, 31, 41]. While some models a priori look different, many can be captured by our formulation; for example, models that have continuous rewards but whose adoption rule depends on whether the reward is above or below a threshold [11,12,26] can be converted to a binary reward structure in a standard way. Similarly, differences across individuals can be captured in the functions fi (see the second example in Section 2.1). While some of these models consider the aggregate popularity of options over time, many (including models for human behavior, e.g., [31]) consider only the current popularity. In the economics literature, some similar finite population models have been studied, many of which also fall into our framework. Their analysis has only been asymptotic as N,T \u2192 \u221e; such results (see, e.g., [9,12,22]) effectively focus on deriving large deviation bounds. In contrast, the main technical contribution of our work includes quantitative bounds of the social learning dynamics for finite populations (N < \u221e). Asymptotic and infinite-population results follow as corollaries.\nThere has been a large body of work on the distributed consensus problem; see for instance [3, 6] and the references therein. The goal in such problems is for all individuals to agree on a single opinion, and various distributed dynamics for doing so have been proposed and analyzed. Our setting differs in that there is additional information \u2013 the repeated stochastic signals associated to the quality of each opinion.\nIn evolutionary game theory, similar-looking deterministic imitator dynamics have been considered (see [37,44] for an overview). A key difference with this work is the learning environment \u2013 we are not attempting to select a strategy in a game, rather are trying to identify the best option of out a collection. Hence, the reward of an individual depends on her choice j, while in evolutionary game theory the reward of an individual\ndepends on the choices of the entire population. In this setting one can still consider the regret of an individual (see, e.g., [13]). While fast convergence of similar dynamics has been shown for some special cases (e.g., potential games [21] and selfish routing [24]), for general games we cannot expect to always converge quickly unless PPAD \u2286 P. In fact, it has been shown that in some games, versions of the replicator dynamics may converge to outcomes that are not optimal, or not even equilibria [26, 42].\nThe fact that an MWU-like method emerges from a simple distributed behavior of individuals in a social setting, is somewhat reminiscent of unrelated results that arise in the context of biological evolution [16] and task allocation among ants [47]. The MWU algorithm [4], or its well-known continuous time limit, the replicator dynamics [45, 48] can also be seen as a special case of our distributed learning dynamics if we remove the randomness from both the sampling and adopting steps and the rewards (effectively taking the process to a deterministic and infinite setting). However, as-is, MWU-type dynamics are different because each individual must effectively maintain full weights (or a mixed strategy vector) at every time step. Furthermore, the lack of stochasticity and the infinite population setting avoid the key technical hurdles in the analysis of our model.\nOur results suggest that the distributed learning dynamics in finite populations can be viewed as a novel distributed and approximate implementation of the MWU method. While parallelized implementations for solving multi-armed bandit problem exist (see, e.g., [2, 25, 28, 33, 36]), in such works each node explicitly maintains a weight vector on all options. The most distinctive aspect of the distributed MWU interpretation of the learning dynamics we consider is that no such memory is required \u2013 the weights are represented implicitly by the popularity of the various options, and the sampling and adopting processes require almost no memory. This difference distinguishes our distributed learning dynamics from prior work on distributed MWU or bandit methods."}, {"heading": "4 Technical Details and Proofs", "text": ""}, {"heading": "4.1 Basic Facts", "text": "We first recall a few theorems and definitions that will be useful in our proofs. Theorem 4.1 (Chernoff-Hoeffding bounds [20]). Let Z1,Z2, . . . ,Zn be independent Bernoulli random variables with mean \u03b3i. Let \u03b3 := 1 n \u2211 n i=1 \u03b3i.When 0 < \u03b4 \u2264 1, we have P [\u2223 \u2223 1 n \u2211 n i=1 Zi \u2212 \u03b3 \u2223 \u2223> \u03b3\u03b4 ] \u2264 2exp ( \u2212n\u03b3\u03b4 2/3 ) . Definition 4.1. For real numbers A, B, and c \u2265 0, the notation A c\u223c B denotes 1 c \u2264 A B \u2264 c. Fact 4.2. For fixed 0 \u2264 \u03b4 \u2264 1 and for all 0 \u2264 x \u2264 1, e\u03b4x \u2264 1+(e\u03b4 \u22121)x."}, {"heading": "4.2 The Infinite Population Distributed Learning Dynamics", "text": "Consider the following stochastic process: W 0j := 1 for all 1 \u2264 j \u2264 m. For t > 0 and for all 1 \u2264 j \u2264 m\nW t+1j :=\n(\n(1\u2212\u00b5)W tj + \u00b5\nm\nm\n\u2211 k=1\nW tk\n)\n\u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j . (1)\nThis definition parallels the two-step finite population distributed learning dynamics and can be thought of as an infinite population distributed learning dynamics: W t+1j is first updated according toW t j (with weight 1\u2212\n\u00b5) and with uniform additive factor \u2211mk=1 W tk/m (with weight \u00b5), and then (stochastically) as the corresponding function of \u03b2 and Rt+1j . The stochasticity is now solely with respect to the R t js. Now, consider the following probability distribution corresponding to these weights, Ptj := W tj\n\u2211mk=1 W t k , which corresponds to the fraction of\nthe (infinite) population that has adopted option j at time t. Let \u03b4 := ln (\n\u03b2 1\u2212\u03b2\n)\n, and note that we can re-write the above as\nW t+1j := (1\u2212\u03b2 ) ( (1\u2212\u00b5)W tj + \u00b5\nm\nm\n\u2211 k=1\nW tk\n)\ne\u03b4R t+1 j .\nThis now takes a more standard form as a multiplicative update; the (1\u2212 \u03b2 ) term can be ignored as it is cancels out in Ptj , so the main difference with the standard MWU is the \u2211mk=1 W t k m term that takes up a \u00b5 fraction of the weight. One can think of this as a regularizing term and bears some similarity to what was considered in the recent breakthrough result on computing flows [18].\nAssuming that \u03b71 \u2265 \u03b7 j for all j 6= 1, the optimal strategy for the population is to select option 1. If they did this, then the average expected gain of the population is \u03b71. On the other hand, the average expected gain of the population over T iterations while following this stochastic process is 1 T \u2211 T t=1 \u2211 m j=1E [ Pt\u22121j R t j ] .We now proceed to understand how the latter compares to the former \u2013 in other words, bound the regret of the infinite population stochastic process. Formally, in the remainder of this subsection, we prove the following result: Theorem 4.3 (Regret of Infinite Population Distributed Learning Dynamics). Let \u03b71 > \u03b72 \u2265 \u00b7\u00b7 \u00b7 \u2265 \u03b7m, let 1 2 < \u03b2 \u2264 e\ne+1 (and hence 0 < \u03b4 \u2264 1), and let 6\u00b5 \u2264 \u03b4 2. Let P0 be the uniform distribution on {1, . . . ,m} and {Pt}Tt=1 be the probability distributions produced by the infinite population distributed learning dynamics with stochastic rewards {Rt}Tt=1. Then for T \u2265 lnm\u03b4 2 , the average expected regret after T steps is\nRegret\u221e(T ) = \u03b71 \u2212 1\nT \u00b7\nT\n\u2211 t=1\nm\n\u2211 j=1 E\n[\nPt\u22121j R t j\n]\n\u2264 3\u03b4 .\nFurthermore, 1 T \u2211 T t=1E [ Pt\u221211 ] \u2265 1\u2212 3\u03b4\u03b71\u2212\u03b72 .\nThe proof of Theorem 4.3 appears in Section 5"}, {"heading": "4.3 Main Result: Regret of the Distributed Learning Dynamics in Finite Populations", "text": "Theorem 4.4 (Regret of the Distributed Learning Process in Finite Populations). Let \u03b71 > \u03b72 \u2265 \u00b7\u00b7 \u00b7 \u2265 \u03b7m, let 1\n2 < \u03b2 \u2264 e e+1 (and hence 0 < \u03b4 \u2264 1), let 6\u00b5 \u2264 \u03b4 2, and let \u03b4 \u2032\u2032 := \u221a 60m ln N (1\u2212\u03b2)\u00b5N , c := 240m (1\u2212\u03b2)\u00b5 , and N is such\nthat\nN\nlnN \u2265\n(\nc 4m\u00b5(1\u2212\u03b2)\n) 2 ln5 \u03b4 2\n\u03b4 \u2032\u20322 and N10 \u2265 24m lnm \u00b5(1\u2212\u03b2 )\u03b4 3 .\nLet Q0 be the uniform distribution on {1, . . . ,m} and {Qt}Tt=1 be the probability distributions produced by the finite population distributed learning dynamics with stochastic rewards {Rt}Tt=1, then for N 10\nm\u03b4 \u2265 T \u2265 lnm\u03b4 2 , the average expected regret after T steps is \u03b71 \u2212 1T \u00b7\u2211Tt=1 \u2211mj=1E [ Qt\u22121j R t j ] \u2264 6\u03b4 .\nHere, N10 is arbitrary and can be made as large as required at the expense of constants.\nIn order to prove this result, we first give an analysis which holds for T = lnm \u03b4 2 , and then show how to leverage this result for large T ."}, {"heading": "4.3.1 Small T", "text": "To conduct the analysis, we first set up some definitions and which correspond to the two different stages of the finite population distributed learning process, and show that, in each step, the finite population dynamics is approximated by the infinite population distributed learning dynamics/MWU stochastic process.\nStage 1. Let Dtj denote the number of people committed to option j at time t and let Q t j :=\nDtj\n\u2211mk=1 D t k be the\nprobability distribution capturing the relative popularity of option j at time t. Let S t+1j \u2286 [N] denote the set of people who select j after the first stage in the sampling process in step t + 1 and let St+1j := |S t+1j |. Let Y t+1i j be the indicator random variable for the event that i chooses j in stage one at time step t +1. Note that these {\nY t+1i j\n}N\ni=1 are independent conditioned on everything up to time t with\nP\n[ Y t+1i j = 1 \u2223 \u2223 \u2223 t ] = ( (1\u2212\u00b5)Qtj + \u00b5\nm\n)\n\u2265 \u00b5 m . (2)\nSince St+1j = \u2211 N i=1Y t+1 i j , it follows from linearity of expectation that E\n[\nSt+1j\n\u2223 \u2223 \u2223 t ] = ( (1\u2212\u00b5)Qtj + \u00b5m ) N.\nProposition 4.1. Let t \u2265 0 be fixed. For \u03b4 \u2032 := \u221a\n30m ln N \u00b5N \u2264 12 , with probability at least 1\u2212 2mN10 (conditioned\non everything up to time t), for all j St+1j 1+2\u03b4 \u2032\u223c\n(\n(1\u2212\u00b5)Qtj + \u00b5 m\n)\nN. Thus, we deduce that (unconditionally)\nwith probability 1\u2212 2m N10 , St+1j \u2265 \u00b5N 2m for all j.\nProof. The proof follows from Chernoff-Hoeffding bound (Theorem 4.1), noting that \u03b3 \u2265 \u00b5 m from (2), and taking a union bound over all j \u2208 [m]. For the latter part, note that for any fixed t and for all j, with probability at least 1\u2212 2m\nN10 , St+1j \u2265 11+2\u03b4 \u2032 \u00b5N m . Since \u03b4 \u2032 \u2264 1 2 , this implies that with probability at least 1\u2212 2m N10 , we have\nSt+1j \u2265 \u00b5N 2m .\nStage 2. There are two outcomes for each option j: Rt+1j = 1 and R t+1 j = 0. Let Z t+1 i j be the indicator random variable for the event that i commits to j in stage two of the process. Note that\nP\n[ Zt+1i j = 1 \u2223 \u2223 \u2223 S t+1 j ,R t+1 j , t ] = \u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j \u2265 (1\u2212\u03b2 ) (3)\nif i \u2208 S t+1j since \u03b2 \u2265 1/2, and P [ Zt+1i j = 1 \u2223 \u2223 \u2223 S t+1 j ,R t+1 j , t ] = 0 otherwise. Let Dt+1j = \u2211 j\u2208S t+1j Z t+1 i j . It follows from linearity of expectation that E [\nDt+1j\n\u2223 \u2223 \u2223 S\nt+1 j ,R t+1 j , t\n]\n= St+1j \u03b2 Rt+1j (1\u2212\u03b2 )1\u2212Rt+1j .\nProposition 4.2. Let t \u2265 0 be fixed. For \u03b4 \u2032\u2032 := \u221a\n60m ln N (1\u2212\u03b2)\u00b5N \u2264 12 , with probability greater than 1\u2212 4mN10 (condi-\ntioned on everything up to time t,S t+1j ,R t+1 j ), for and all j D t+1 j 1+2\u03b4 \u2032\u2032\u223c St+1j \u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j .\nProof. The proof again follows from Chernoff-Hoeffding bound (Theorem 4.1), noting that \u03b3 \u2265 1\u2212\u03b2 from (3), and taking a union bound over all j \u2208 [m]. Here we have also used proposition 4.1 that St+1j \u2265 \u00b5N 2m for large enough N for all j with probability at least 1\u2212 2m N10 .\nCombining proposition 4.1 and proposition 4.2 we obtain the following. Proposition 4.3. Let t \u2265 0 be fixed. Let \u03b4 \u2032\u2032 := \u221a\n60m ln N (1\u2212\u03b2)\u00b5N . With probability greater than 1\u2212 6mN10 (conditioned\non everything up to time t,Rt+1j ) D t+1 j\n1+6\u03b4 \u2032\u2032\u223c (\n(1\u2212\u00b5)Qtj + \u00b5 m\n)\nN\u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j .\nProof. Follows directly from proposition 4.1 and proposition 4.2 and noting that \u03b4 \u2032 \u2264 \u03b4 \u2032\u2032 \u2264 1 2 . Thus\n(1+2\u03b4 \u2032)(1+2\u03b4 \u2032\u2032)\u2264 1+2\u03b4 \u2032+2\u03b4 \u2032\u2032+4\u03b4 \u2032\u03b4 \u2032\u2032 \u2264 1+6\u03b4 \u2032\u2032.\nNow we establish a relationship between the probability distributions Pt and Qt . Note that both processes start with P0 = Q0. Let \u03b4t := 5 t\u03b4 \u2032\u2032. Lemma 4.5. There is a coupling such that Ptj 1+\u03b4t\u223c Qtj with probability at least 1\u2212 6tmN10 for all choices of {Rtj}s.\nProof. As suggested by our notation, we couple Ptj and Q t j so that the realizations of the R t js is the same in both processes for all j and all t.\nThe proof proceeds by induction on t. The statement holds when t = 0 as, by definition, P0 = Q0. Assume it holds for t. Thus, with probability at least 1\u2212 6tm N10 , Ptj 1+\u03b4t\u223c Qtj. Let us condition on this event. Recall that\nPt+1j =\n( (1\u2212\u00b5)Ptj + \u00b5m ) \u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j\n\u2211mk=1 ( (1\u2212\u00b5)Ptk + \u00b5 m ) \u03b2 R t+1 k (1\u2212\u03b2 )1\u2212Rt+1k\n.\nThus, with probability at least 1\u2212 6tm N10 ,\nPt+1j (1+\u03b4t)\n2\n\u223c\n( (1\u2212\u00b5)Qtj + \u00b5m ) \u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j\n\u2211mk=1 ( (1\u2212\u00b5)Qtk + \u00b5 m ) \u03b2 R t+1 k (1\u2212\u03b2 )1\u2212Rt+1k\n.\nHere we used induction for both the numerator and the denominator. From proposition 4.2, we know that\nDt+1j 1+6\u03b4 \u2032\u2032\u223c\n(\n(1\u2212\u00b5)Qtj + \u00b5\nm\n)\nN\u03b2 R t+1 j (1\u2212\u03b2 )1\u2212Rt+1j\nfor all j with probability at least 1\u2212 6m N10 . Thus, we obtain that with probability at least 1\u2212 6(t+1)m N10\nPt+1j (1+\u03b4t)\n2(1+6\u03b4 \u2032\u2032)2\u223c Dt+1j\n\u2211mk=1 D t+1 k\n= Qt+1j .\nNow note that, assuming that \u03b4t = 5 t\u03b4 \u2032\u2032 \u2264 1 and \u03b4 \u2032\u2032 \u2264 1\n40 ,\n(1+\u03b4t) 2(1+6\u03b4 \u2032\u2032)2 \u2264 (1+3\u03b4t)(1+13\u03b4 \u2032\u2032)\u2264 1+3\u03b4t +13\u03b4 \u2032\u2032+39\u03b4 \u2032\u2032\u03b4t \u2264 1+4\u03b4t +13\u03b4 \u2032\u2032 \u2264 1+5t+1\u03b4 \u2032\u2032 = 1+\u03b4t+1\nfor t \u2265 2. For t = 1 the bound can be checked by a direct calculation.\nHence, for any fixed set of Rtjs, the trajectories P t and Qt remain close. Thus, using Theorem 4.3 and Lemma 4.5, we obtain that \u03b71 \u2212 ( 1+5T \u03b4 \u2032\u2032 ) 1 T \u2211 T t=1 \u2211 m j=1E [ Qt\u22121j R t j ] \u2212 6T 2m NT\n\u2264 lnm\u03b4T +2\u03b4 . Here, the first negative term in the left hand side of the equation occurs when Lemma 4.5 applies and the second negative term is when it does not. Rearranging, we obtain \u03b71 \u2212 1T \u2211Tt=1 \u2211mj=1E [ Qt\u22121j R t j ] \u2264 lnm\u03b4T +2\u03b4 +5T \u03b4 \u2032\u2032+ 6mTN10 . Since \u03b4 \u2032\u2032 := \u221a\n240m ln N (1\u2212\u03b2)\u00b5N \u2264\n\u221a c lnN\nN for c = 240m(1\u2212\u03b2)\u00b5 . Thus, when T = lnm \u03b4 2\n, 5T \u03b4 \u2032\u2032 \u2264 m ln5 \u03b4 2 \u221a\nc lnN\u221a N . Hence, when N is such\nthat\nN lnN \u2265 cm\n2 ln5\n\u03b4 2 \u03b4 \u2032\u20322 and N10 \u2265 6m lnm \u03b4 3 , (4)\nthen\nRegretN(T ) = \u03b71 \u2212 1\nT\nT\n\u2211 t=1\nm\n\u2211 j=1 E\n[\nQt\u22121j R t j\n]\n\u2264 5\u03b4 . (5)\nThis concludes the proof of Theorem 4.4 when T = lnm \u03b4 2 ."}, {"heading": "4.3.2 Large T", "text": "For large T we need one new ingredient; here we focus on this additional aspect. We first need a slight generalization of Theorem 4.3 to handle P0 that are not uniform. This is similar to the version of MWUwith restricted distributions as in Theorem 2.4 in [4]. Theorem 4.6 (Regret of Infinite Population Distributed Learning Dynamics with Nonuniform Start). Let \u03b71 \u2265 \u03b72 \u2265 \u00b7\u00b7 \u00b7 \u2265 \u03b7m, let 12 < \u03b2 \u2264 ee+1 (and hence 0 < \u03b4 \u2264 1), and let 6\u00b5 \u2264 \u03b4 2. Let P0j \u2265 \u03b6 for all j \u2208 {1, . . . ,m} and {Pt}Tt=1 be the probability distributions produced by the MWU process with stochastic rewards {Rt}Tt=1. Then for T \u2265 ln(1/\u03b6) \u03b4 2\n, the average expected regret after T steps is Regret\u221e(T ) = \u03b71 \u2212 1T \u00b7 \u2211Tt=1 \u2211 m j=1E [ Pt\u22121j R t j ] \u2264 3\u03b4 .\nThe proof closely follows from that of Theorem 4.3, and we omit the details.\nSimilarly, the results in Section 4.3.1 for small T follow analogously with nonuniform start. This just requires\nus to choseN slightly bigger in particular, instead of (4), wewould need N such that N lnN\n\u2265 c\n( 1 \u03b6 ) 2 ln5 \u03b4 2\n\u03b4 \u2032\u20322 and N 10 \u2265\n6ln m \u03b6\u03b4 3\n. This gives that the regret of the finite population distributed learning dynamics is at most 5\u03b4 when T = ln(1/\u03b6)\n\u03b4 2 . We now complete the proof of Theorem 4.4 for large T , by breaking the time into epochs consisting\nof ln(1/\u03b6)\n\u03b4 2 time steps. In each epoch, we couple an infinite population distributed learning dynamics with the finite population distributed learning dynamics such that the starting points are identical at the beginning of each epoch, and both observe the same sequence of rewards Rtjs.\nIt remains to lower bound \u03b6 appropriately. From proposition 4.3, it follows that for any t, with probability at\nleast 1\u2212 6m N10 , for all j Qtj \u2265 \u00b5(1\u2212\u03b2) 4m . We let \u03b6 := \u00b5(1\u2212\u03b2) 4m , and hence our epochs are of length\nln (\n4m \u00b5(1\u2212\u03b2)\n)\n\u03b4 2 . This\ngives us regret \u03b71 \u2212 1T \u2211Tt=1 \u2211mj=1E [ Qt\u22121j R t j ] \u2264 5\u03b4 + Tm N10 . The additive term of Tm N10 is precisely due to the fact that with probability 6m N10 , the above inequality will not be satisfied, in which case the regret could be at most 1 for that epoch. Finally, note that N10 is arbitrary and can be made as large as required at the expense of constants. This concludes the proof of Theorem 4.4."}, {"heading": "5 Proof of Theorem 4.3", "text": "Let us define the potential function \u03a6T := \u2211mj=1W T j , and recall that e \u03b4 = \u03b2 1\u2212\u03b2 . Then,\n\u03a6T = m\n\u2211 j=1\nW Tj\n= (1\u2212\u03b2 ) m\n\u2211 j=1\n(\n(1\u2212\u00b5)W T\u22121j + \u00b5\nm\nm\n\u2211 k=1\nW T\u22121k\n)\ne\u03b4R T j\n= (1\u2212\u03b2 )\u03a6T\u22121 m\n\u2211 j=1\n(\n(1\u2212\u00b5)PT\u22121j + \u00b5\nm\n)\ne\u03b4R T j\n0\u2264RTj \u22641 , Fact 4.2 \u2264 (1\u2212\u03b2 )\u03a6T\u22121 m\n\u2211 j=1\n(\n(1\u2212\u00b5)PT\u22121j + \u00b5\nm\n)( 1+(e\u03b4 \u22121)RTj )\n= (1\u2212\u03b2 )\u03a6T\u22121 ( 1+(e\u03b4 \u22121) m\n\u2211 j=1\n(\n(1\u2212\u00b5)PT\u22121j + \u00b5\nm\n)\nRTj\n)\n0\u2264RTj \u22641 \u2264 (1\u2212\u03b2 )\u03a6T\u22121\n(\n1+\u00b5(e\u03b4 \u22121)+ (1\u2212\u00b5)(e\u03b4 \u22121) m\n\u2211 j=1\nPT\u22121j R T j\n)\n.\nNow, we let \u03b4 \u2032 := (1\u2212\u00b5)(e \u03b4\u22121)\n1+\u00b5\u03b4 and obtain that\n\u03a6T \u03b4\u2264e\u03b4\u22121 \u2264 (1\u2212\u03b2 )(1+\u00b5(e\u03b4 \u22121))\u03a6T\u22121\n(\n1+ (1\u2212\u00b5)(e\u03b4 \u22121)\n1+\u00b5\u03b4\nm\n\u2211 j=1\nPT\u22121j R T j\n)\n1+\u03b4 \u2032x\u2264e\u03b4 \u2032x \u2264 (1\u2212\u03b2 )(1+\u00b5(e\u03b4 \u22121))\u03a6T\u22121e\u03b4 \u2032 \u2211mj=1 PT\u22121j RTj\n\u03a60=m \u2264 (1\u2212\u03b2 )T (1+\u00b5(e\u03b4 \u22121))T me\u03b4 \u2032 \u2211Tt=1 \u2211mj=1 Pt\u22121j Rtj .\nOn the other hand,\n\u03a6T \u2265 (1\u2212\u03b2 )T (1\u2212\u00b5)T e\u03b4 \u2211Tt=1 Rt1 .\nCombining the lower bound and upper bound and taking logarithms we obtain\n\u03b4 T\n\u2211 t=1\nRt1 \u2264 lnm+T ln (\n1+\u00b5(e\u03b4 \u22121) 1\u2212\u00b5\n)\n+\u03b4 \u2032 T\n\u2211 t=1\nm\n\u2211 j=1\nPt\u22121j R t j.\nThus,\n\u03b4 T\n\u2211 t=1\nRt1 \u2212\u03b4 \u2032 T\n\u2211 t=1\nm\n\u2211 j=1\nPt\u22121j R t j \u2264 lnm+T ln\n(\n1+\u00b5(e\u03b4 \u22121) 1\u2212\u00b5\n)\n.\nNow, for \u00b5 \u2264 1 2 we know that 1 1\u2212\u00b5 \u2264 1+2\u00b5 . Also, 1+\u00b5(e\u03b4 \u22121)\u2264 1+\u00b5(e\u22121). Thus, as \u00b5 \u2264 12 ,\n1+\u00b5(e\u03b4 \u22121) 1\u2212\u00b5 \u2264 (1+(e\u22121)\u00b5)(1+2\u00b5)\u2264 1+(e+1)\u00b5 +2(e\u22121)\u00b5 2 \u2264 1+2e\u00b5 \u2264 1+6\u00b5 .\nHence, using the inequality ln(1+ x)\u2264 x for all x \u2265 0, we obtain\nln\n(\n1+\u00b5(e\u03b4 \u22121) 1\u2212\u00b5\n)\n\u2264 6\u00b5 .\nFurther, using the fact that e\u03b4 \u22121 \u2264 \u03b4 +\u03b4 2 for 0 \u2264 \u03b4 \u2264 1, which is implied by the assumption that \u03b2 \u2264 e 1+e , it can be seen that\n\u03b4 \u2032 = (1\u2212\u00b5)(e\u03b4 \u22121) 1+\u00b5\u03b4 \u2264 (1\u2212\u00b5)\u03b4 (1+\u03b4 ) 1+\u00b5\u03b4 \u2264 \u03b4 (1+\u03b4 ).\nTherefore,\n\u03b4\n( T\n\u2211 t=1\nRt1 \u2212 (1+\u03b4 ) T\n\u2211 t=1\nm\n\u2211 j=1\nPt\u22121j R t j\n)\n\u2264 lnm+6\u00b5T.\nNote that \u2211Tt=1 \u2211 m j=1 P t\u22121 j R t j \u2264 T , hence, the above implies that\n\u03b4\n( T\n\u2211 t=1\nRt1 \u2212 T\n\u2211 t=1\nm\n\u2211 j=1\nPt\u22121j R t j\n)\n\u2264 lnm+(\u03b4 2 +6\u00b5)T.\nTaking expectations and dividing by T \u03b4 we obtain the following regret bound.\n\u03b71 \u2212 1\nT \u00b7\nT\n\u2211 t=1\nm\n\u2211 j=1 E\n[\nPt\u22121j R t j\n]\n\u2264 lnm \u03b4T +\n(\n\u03b4 + 6\u00b5\n\u03b4\n)\n.\nAssuming 6\u00b5 \u2264 \u03b4 2 we obtain\n\u03b71 \u2212 1\nT \u00b7\nT\n\u2211 t=1\nm\n\u2211 j=1 E\n[\nPt\u22121j R t j\n]\n\u2264 lnm \u03b4T +2\u03b4 .\nThus, for T \u2265 lnm \u03b4 2 , we obtain the desired bound\nRegret\u221e(T )\u2264 3\u03b4 .\nFrom this we can derive the lower bound on the probability that the best option j = 1 is selected as stated in the second part of the theorem. Firstly, it follows (as Rtj is independent of P t\u22121 j ) that for all T \u2265 1\n\u03b71 \u2212 1\nT \u00b7\nT\n\u2211 t=1\nm\n\u2211 j=1\n\u03b7 jE [ Pt\u22121j ] \u2264 lnm \u03b4T +2\u03b4 .\nThus,\n\u03b71\n(\n1\u2212 1 T \u00b7\nT\n\u2211 t=1 E\n[ Pt\u221211 ]\n)\n\u2212 \u03b72 T \u00b7 T\n\u2211 t=1\nm\n\u2211 j=2 E\n[\nPt\u22121j\n]\n\u2264 lnm \u03b4T +2\u03b4 .\nFrom this we obtain\n(\u03b71 \u2212\u03b72) (\n1\u2212 1 T \u00b7\nT\n\u2211 t=1 E\n[ Pt\u221211 ]\n)\n\u2264 lnm \u03b4T +2\u03b4 ,\nand consequently for T \u2265 ln m \u03b4 2 ,\n1\nT\nT\n\u2211 t=1 E\n[ Pt\u221211 ] \u2265 1\u2212 3\u03b4\n\u03b71 \u2212\u03b72 .\nThis completes the proof."}, {"heading": "6 Conclusion and Future Work", "text": "In this work we study a fundamental distributed learning dynamics prevalent in various social and biological contexts and provide the first convergence and regret bounds for it in the finite population setting. The connection between this learning dynamics and theMWUmethod suggests a novel distributed and essentially memoryless implementation of the MWU method. Another interpretation of our result comes by looking at the infinite population limit of the distributed learning dynamics; while an individual can be effectively solving a stochastic multi-armed bandit problem, the population as a whole is solving a full-information version of the problem, and hence can be very efficient on the group-level.\nSeveral important directions remain open. The first is to extend our results to the social network setting where individuals can only sample in step (1) from their neighbors. The question here would be whether, and to what extent, the efficiency of the group remains as a function of the network topology. It would also be interesting to explore the distributed learning algorithms when the parameters controlling the quality of the options (\u03b7is) are allowed to change, or when there is dependence across options and time (e.g., when the options represent stocks). Lastly, we note that as an algorithm designer, if we were to implement these learning dynamics as a distributed approximation to the stochastic version of MWUmethod, we can optimize \u03b2 to attain the usual O (\u221a lnm/T ) regret; in the distributed learning dynamics, we are constrained by the\nbehavior of the group \u2013 the regret bound will only be as good as the \u03b2 they use. This naturally raises the question of whether human groups match the ideal values for \u03b2 , perhaps in a context-specific manner, to achieve good regret bounds."}, {"heading": "Acknowledgments", "text": "The authors would like to thank Ashish Goel for useful discussions, and the BIRS-CMO 2016 Workshop on Models and Algorithms for Crowds and Networks, where part of this work was done."}], "references": [{"title": "A biological solution to a fundamental distributed computing problem", "author": ["Yehuda Afek", "Noga Alon", "Omer Barad", "Eran Hornstein", "Naama Barkai", "Ziv Bar-Joseph"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Distributed algorithms for learning and cognitive medium access with logarithmic regret", "author": ["Animashree Anandkumar", "Nithin Michael", "Ao Kevin Tang", "Ananthram Swami"], "venue": "IEEE Journal on Selected Areas in Communications,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Computation in networks of passively mobile finite-state sensors", "author": ["Dana Angluin", "James Aspnes", "Zo\u00eb Diamadi", "Michael J. Fischer", "Ren\u00e9 Peralta"], "venue": "Distributed Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "The multiplicative weights update method: a metaalgorithm and applications", "author": ["Sanjeev Arora", "Elad Hazan", "Satyen Kale"], "venue": "Theory of Computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "A simple model of herd behavior", "author": ["Abhijit V Banerjee"], "venue": "The Quarterly Journal of Economics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "Stabilizing consensus with many opinions", "author": ["L. Becchetti", "A. Clementi", "E. Natale", "F. Pasquale", "L. Trevisan"], "venue": "In Proceedings of the Twenty-seventh Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Strategic social learning and the population dynamics of human behavior: The game of Go", "author": ["Bret Alexander Beheim", "Calvin Thigpen", "Richard McElreath"], "venue": "Evolution and Human Behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Stochastic approximations and differential inclusions", "author": ["Michel Bena\u0131\u0308m", "Josef Hofbauer", "Sylvain Sorin"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Deterministic approximation of stochastic evolution", "author": ["Michel Bena\u0131\u0308m", "J\u00f6rgen W Weibull"], "venue": "in games. Econometrica,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Bose-Einstein condensation in complex networks", "author": ["Ginestra Bianconi", "Albert-L\u00e1szl\u00f3 Barab\u00e1si"], "venue": "Physical Review Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Muddling through: Noisy equilibrium selection", "author": ["Ken Binmore", "Larry Samuelson"], "venue": "journal of economic theory,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Nash equilibrium and evolution by imitation", "author": ["Jonas Bj\u00f6rnerstedt", "J\u00f6rgen Weibull"], "venue": "Technical report, Research Institute of Industrial Economics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Regret minimization and the price of total anarchy", "author": ["Avrim Blum", "MohammadTaghi Hajiaghayi", "Katrina Ligett", "Aaron Roth"], "venue": "In Proceedings of the fortieth annual ACM symposium on Theory of computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Culture and the evolutionary process", "author": ["Robert Boyd", "Peter J Richerson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1988}, {"title": "Stochastic replicator dynamics", "author": ["Antonio Cabrales"], "venue": "International Economic Review,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Algorithms, games, and evolution", "author": ["Erick Chastain", "Adi Livnat", "Christos Papadimitriou", "Umesh Vazirani"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Natural algorithms and influence systems", "author": ["Bernard Chazelle"], "venue": "Commun. ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Electrical flows, laplacian systems, and faster approximation of maximum flow in undirected graphs", "author": ["Paul Christiano", "Jonathan A. Kelner", "Aleksander Madry", "Daniel A. Spielman", "Shang-Hua Teng"], "venue": "In Proceedings of the 43rd ACM Symposium on Theory of Computing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Task allocation in ant colonies", "author": ["Alejandro Cornejo", "Anna Dornhaus", "Nancy Lynch", "Radhika Nagpal"], "venue": "In International Symposium on Distributed Computing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Concentration of measure for the analysis of randomized algorithms", "author": ["Devdatt P Dubhashi", "Alessandro Panconesi"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Once beaten, never again: Imitation in twoplayer potential games", "author": ["Peter Duersch", "J\u00f6rg Oechssler", "Burkhard Schipper"], "venue": "Technical report,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Word-of-mouth communication and social learning", "author": ["Glenn Ellison", "Drew Fudenberg"], "venue": "The Quarterly Journal of Economics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1995}, {"title": "Opinion leaders, independence, and condorcet\u2019s jury theorem", "author": ["David M Estlund"], "venue": "Theory and Decision,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "On the evolution of selfish routing", "author": ["Simon Fischer", "Berthold V\u00f6cking"], "venue": "In European Symposium on Algorithms,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Decentralized online learning algorithms for opportunistic spectrum access", "author": ["Yi Gai", "Bhaskar Krishnamachari"], "venue": "In Global Telecommunications Conference (GLOBECOM", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Learning to be imperfect: The ultimatum game", "author": ["John Gale", "Kenneth G Binmore", "Larry Samuelson"], "venue": "Games and Economic Behavior,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Distributed househunting in ant colonies", "author": ["Mohsen Ghaffari", "Cameron Musco", "Tsvetomira Radeva", "Nancy A. Lynch"], "venue": "In Proceedings of the 2015 ACM Symposium on Principles of Distributed Computing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Online distributed sensor selection", "author": ["Daniel Golovin", "Matthew Faulkner", "Andreas Krause"], "venue": "In Proceedings of the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Integration of social information by human groups", "author": ["Boris Granovskiy", "Jason M Gold", "David JT Sumpter", "Robert L Goldstone"], "venue": "Topics in Cognitive Science,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Cultural transmission and the diffusion of innovations: Adoption dynamics indicate that biased cultural transmission is the predominate force in behavioral change", "author": ["Joseph Henrich"], "venue": "American Anthropologist,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "Human collective intelligence as distributed bayesian inference", "author": ["Peter M Krafft", "Julia Zheng", "Wei Pan", "Nicol\u00e1s Della Penna", "Yaniv Altshuler", "Erez Shmueli", "Joshua B Tenenbaum", "Alex Pentland"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1987}, {"title": "Quantifying social influence in an online cultural market", "author": ["Coco Krumme", "Manuel Cebrian", "Galen Pickard", "Alex Pentland"], "venue": "PLoS ONE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Decentralized multi-armed bandit with multiple distributed players", "author": ["Keqin Liu", "Qing Zhao"], "venue": "In Information Theory and Applications Workshop (ITA),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Beyond existence and aiming outside the laboratory: Estimating frequency-dependent and pay-off-biased social learning strategies", "author": ["Richard McElreath", "Adrian V Bell", "Charles Efferson", "Mark Lubell", "Peter J Richerson", "Timothy Waring"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2008}, {"title": "Ant-inspired density estimation via random walks", "author": ["Cameron Musco", "Hsin-Hao Su", "Nancy Lynch"], "venue": "In Proceedings of the 2016 ACM Symposium on Principles of Distributed Computing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2016}, {"title": "On regret-optimal learning in decentralized multiplayer multi-armed bandits", "author": ["Naumaan Nayyar", "Dileep Kalathil", "Rahul Jain"], "venue": "arXiv preprint arXiv:1505.00553,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Evolutionary dynamics", "author": ["Martin A Nowak"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2006}, {"title": "When are touchpoints limits for generalized p\u00f3lya urns", "author": ["Robin Pemantle"], "venue": "Proceedings of the American Mathematical Society,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1991}, {"title": "Social Physics: How Good Ideas Spread-The Lessons from a New Science", "author": ["Alex Pentland"], "venue": "Penguin,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "An agent-based model of collective nest choice by the ant Temnothorax albipennis", "author": ["Stephen C Pratt", "David JT Sumpter", "Eamonn B Mallon", "Nigel R Franks"], "venue": "Animal Behaviour,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2005}, {"title": "Does biology constrain culture", "author": ["Alan R Rogers"], "venue": "American Anthropologist,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1988}, {"title": "Evolutionary stability in asymmetric games", "author": ["Larry Samuelson", "Jianbo Zhang"], "venue": "Journal of economic theory,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1992}, {"title": "Group decision making in swarms of honey bees", "author": ["Thomas D Seeley", "Susannah C Buhrman"], "venue": "Behavioral Ecology and Sociobiology,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1999}, {"title": "Evolutionary Game Dynamics: American Mathematical Society Short Course", "author": ["Karl Sigmund"], "venue": "January 4-5,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2011}, {"title": "Evolution and the Theory of Games", "author": ["John Maynard Smith"], "venue": "Cambridge university press,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1982}, {"title": "IRLS and slime mold: Equivalence and convergence", "author": ["Damian Straszak", "Nisheeth KVishnoi"], "venue": "Invited at the Innovations in Theoretical Computer Science,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2017}, {"title": "Algorithms for Fundamental Problems in Computer Networks", "author": ["Hsin-Hao Su"], "venue": "PhD thesis, ETH Zu\u0308rich,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Evolutionarily stable strategies with two types of player", "author": ["Peter D Taylor"], "venue": "Journal of applied probability,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1979}, {"title": "The speed of evolution", "author": ["Nisheeth K Vishnoi"], "venue": "In Proc. 26th Annual ACM-SIAM Symp. Discret. Algorithms (SODA),", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2015}, {"title": "Differential equations for random processes and random graphs", "author": ["Nicholas C Wormald"], "venue": "The annals of applied probability,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1995}], "referenceMentions": [{"referenceID": 6, "context": ", [7, 10, 29, 32, 34]) and animal behavior (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 9, "context": ", [7, 10, 29, 32, 34]) and animal behavior (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 28, "context": ", [7, 10, 29, 32, 34]) and animal behavior (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 31, "context": ", [7, 10, 29, 32, 34]) and animal behavior (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 33, "context": ", [7, 10, 29, 32, 34]) and animal behavior (e.", "startOffset": 2, "endOffset": 21}, {"referenceID": 39, "context": ", [40, 43]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 42, "context": ", [40, 43]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 26, "context": ", [27,35]), each step requires only limited communication with other group members.", "startOffset": 2, "endOffset": 9}, {"referenceID": 34, "context": ", [27,35]), each step requires only limited communication with other group members.", "startOffset": 2, "endOffset": 9}, {"referenceID": 38, "context": ", [39] for an exposition), and, even though on the surface there seem to be several related processes, the only theoretical work has been either in deterministic special cases or in the asymptotic setting where both the size of the population and time goes to infinity; such results (see [12,22] and Section 3) effectively focus on deriving asymptotic (large deviation) bounds.", "startOffset": 2, "endOffset": 6}, {"referenceID": 11, "context": ", [39] for an exposition), and, even though on the surface there seem to be several related processes, the only theoretical work has been either in deterministic special cases or in the asymptotic setting where both the size of the population and time goes to infinity; such results (see [12,22] and Section 3) effectively focus on deriving asymptotic (large deviation) bounds.", "startOffset": 288, "endOffset": 295}, {"referenceID": 21, "context": ", [39] for an exposition), and, even though on the surface there seem to be several related processes, the only theoretical work has been either in deterministic special cases or in the asymptotic setting where both the size of the population and time goes to infinity; such results (see [12,22] and Section 3) effectively focus on deriving asymptotic (large deviation) bounds.", "startOffset": 288, "endOffset": 295}, {"referenceID": 3, "context": "Key to our results are the following two realizations which we can use to understand the emergent behavior of the distributed learning dynamics: \u2022 In the infinite population limit, by rewriting the underlying stochastic equations of the distributed learning dynamics, the individuals are effectively implementing a stochastic variant of the classic multiplicative weights update (MWU) method [4] at the group-level.", "startOffset": 392, "endOffset": 395}, {"referenceID": 0, "context": "Each option has an unknown underlying quality, \u03b71 \u2265 \u03b72 \u2265 \u00b7\u00b7 \u00b7 \u2265 \u03b7m \u2208 [0,1], which represents the probability that the option is \u201cgood\u201d at any given time step; we let Rj = Bernoulli(\u03b7 j) be the indicator random variable for the event that option j is good at time t.", "startOffset": 69, "endOffset": 74}, {"referenceID": 30, "context": "The simplest such example [31] corresponds exactly to our model when\u03b1 = 1\u2212\u03b2 for some \u03b2 \u2265 1 2 when \u03b71 > 1 2 =\u03b72 = \u00b7 \u00b7 \u00b7=\u03b7m.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "Another instance, which takes a bit of explanation, appears in the economics literature [22].", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "[18] who developed a variant of the MWU method in the design of a fast algorithm for a flow problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "This contributes to the growing set of connections between using attractors of dynamical systems to analyze stochastic processes [8, 38, 49, 50].", "startOffset": 129, "endOffset": 144}, {"referenceID": 37, "context": "This contributes to the growing set of connections between using attractors of dynamical systems to analyze stochastic processes [8, 38, 49, 50].", "startOffset": 129, "endOffset": 144}, {"referenceID": 48, "context": "This contributes to the growing set of connections between using attractors of dynamical systems to analyze stochastic processes [8, 38, 49, 50].", "startOffset": 129, "endOffset": 144}, {"referenceID": 49, "context": "This contributes to the growing set of connections between using attractors of dynamical systems to analyze stochastic processes [8, 38, 49, 50].", "startOffset": 129, "endOffset": 144}, {"referenceID": 0, "context": ", [1,17,19,27,35,46].", "startOffset": 2, "endOffset": 20}, {"referenceID": 16, "context": ", [1,17,19,27,35,46].", "startOffset": 2, "endOffset": 20}, {"referenceID": 18, "context": ", [1,17,19,27,35,46].", "startOffset": 2, "endOffset": 20}, {"referenceID": 26, "context": ", [1,17,19,27,35,46].", "startOffset": 2, "endOffset": 20}, {"referenceID": 34, "context": ", [1,17,19,27,35,46].", "startOffset": 2, "endOffset": 20}, {"referenceID": 45, "context": ", [1,17,19,27,35,46].", "startOffset": 2, "endOffset": 20}, {"referenceID": 13, "context": "Among related studies in the sciences, while dynamics that only have a sampling step [14, 23] or only an adoption step [30] have been studied, combining both has been shown empirically to result in a better strategy [34].", "startOffset": 85, "endOffset": 93}, {"referenceID": 22, "context": "Among related studies in the sciences, while dynamics that only have a sampling step [14, 23] or only an adoption step [30] have been studied, combining both has been shown empirically to result in a better strategy [34].", "startOffset": 85, "endOffset": 93}, {"referenceID": 29, "context": "Among related studies in the sciences, while dynamics that only have a sampling step [14, 23] or only an adoption step [30] have been studied, combining both has been shown empirically to result in a better strategy [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 33, "context": "Among related studies in the sciences, while dynamics that only have a sampling step [14, 23] or only an adoption step [30] have been studied, combining both has been shown empirically to result in a better strategy [34].", "startOffset": 216, "endOffset": 220}, {"referenceID": 4, "context": "Hence, both steps of the process seem crucial, and many models in sociology and economics are such distributed two-step processes [5, 15, 22, 31, 41].", "startOffset": 130, "endOffset": 149}, {"referenceID": 14, "context": "Hence, both steps of the process seem crucial, and many models in sociology and economics are such distributed two-step processes [5, 15, 22, 31, 41].", "startOffset": 130, "endOffset": 149}, {"referenceID": 21, "context": "Hence, both steps of the process seem crucial, and many models in sociology and economics are such distributed two-step processes [5, 15, 22, 31, 41].", "startOffset": 130, "endOffset": 149}, {"referenceID": 30, "context": "Hence, both steps of the process seem crucial, and many models in sociology and economics are such distributed two-step processes [5, 15, 22, 31, 41].", "startOffset": 130, "endOffset": 149}, {"referenceID": 40, "context": "Hence, both steps of the process seem crucial, and many models in sociology and economics are such distributed two-step processes [5, 15, 22, 31, 41].", "startOffset": 130, "endOffset": 149}, {"referenceID": 10, "context": "While some models a priori look different, many can be captured by our formulation; for example, models that have continuous rewards but whose adoption rule depends on whether the reward is above or below a threshold [11,12,26] can be converted to a binary reward structure in a standard way.", "startOffset": 217, "endOffset": 227}, {"referenceID": 11, "context": "While some models a priori look different, many can be captured by our formulation; for example, models that have continuous rewards but whose adoption rule depends on whether the reward is above or below a threshold [11,12,26] can be converted to a binary reward structure in a standard way.", "startOffset": 217, "endOffset": 227}, {"referenceID": 25, "context": "While some models a priori look different, many can be captured by our formulation; for example, models that have continuous rewards but whose adoption rule depends on whether the reward is above or below a threshold [11,12,26] can be converted to a binary reward structure in a standard way.", "startOffset": 217, "endOffset": 227}, {"referenceID": 30, "context": ", [31]) consider only the current popularity.", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": ", [9,12,22]) effectively focus on deriving large deviation bounds.", "startOffset": 2, "endOffset": 11}, {"referenceID": 11, "context": ", [9,12,22]) effectively focus on deriving large deviation bounds.", "startOffset": 2, "endOffset": 11}, {"referenceID": 21, "context": ", [9,12,22]) effectively focus on deriving large deviation bounds.", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": "There has been a large body of work on the distributed consensus problem; see for instance [3, 6] and the references therein.", "startOffset": 91, "endOffset": 97}, {"referenceID": 5, "context": "There has been a large body of work on the distributed consensus problem; see for instance [3, 6] and the references therein.", "startOffset": 91, "endOffset": 97}, {"referenceID": 36, "context": "In evolutionary game theory, similar-looking deterministic imitator dynamics have been considered (see [37,44] for an overview).", "startOffset": 103, "endOffset": 110}, {"referenceID": 43, "context": "In evolutionary game theory, similar-looking deterministic imitator dynamics have been considered (see [37,44] for an overview).", "startOffset": 103, "endOffset": 110}, {"referenceID": 12, "context": ", [13]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": ", potential games [21] and selfish routing [24]), for general games we cannot expect to always converge quickly unless PPAD \u2286 P.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": ", potential games [21] and selfish routing [24]), for general games we cannot expect to always converge quickly unless PPAD \u2286 P.", "startOffset": 43, "endOffset": 47}, {"referenceID": 25, "context": "In fact, it has been shown that in some games, versions of the replicator dynamics may converge to outcomes that are not optimal, or not even equilibria [26, 42].", "startOffset": 153, "endOffset": 161}, {"referenceID": 41, "context": "In fact, it has been shown that in some games, versions of the replicator dynamics may converge to outcomes that are not optimal, or not even equilibria [26, 42].", "startOffset": 153, "endOffset": 161}, {"referenceID": 15, "context": "The fact that an MWU-like method emerges from a simple distributed behavior of individuals in a social setting, is somewhat reminiscent of unrelated results that arise in the context of biological evolution [16] and task allocation among ants [47].", "startOffset": 207, "endOffset": 211}, {"referenceID": 46, "context": "The fact that an MWU-like method emerges from a simple distributed behavior of individuals in a social setting, is somewhat reminiscent of unrelated results that arise in the context of biological evolution [16] and task allocation among ants [47].", "startOffset": 243, "endOffset": 247}, {"referenceID": 3, "context": "The MWU algorithm [4], or its well-known continuous time limit, the replicator dynamics [45, 48] can also be seen as a special case of our distributed learning dynamics if we remove the randomness from both the sampling and adopting steps and the rewards (effectively taking the process to a deterministic and infinite setting).", "startOffset": 18, "endOffset": 21}, {"referenceID": 44, "context": "The MWU algorithm [4], or its well-known continuous time limit, the replicator dynamics [45, 48] can also be seen as a special case of our distributed learning dynamics if we remove the randomness from both the sampling and adopting steps and the rewards (effectively taking the process to a deterministic and infinite setting).", "startOffset": 88, "endOffset": 96}, {"referenceID": 47, "context": "The MWU algorithm [4], or its well-known continuous time limit, the replicator dynamics [45, 48] can also be seen as a special case of our distributed learning dynamics if we remove the randomness from both the sampling and adopting steps and the rewards (effectively taking the process to a deterministic and infinite setting).", "startOffset": 88, "endOffset": 96}, {"referenceID": 1, "context": ", [2, 25, 28, 33, 36]), in such works each node explicitly maintains a weight vector on all options.", "startOffset": 2, "endOffset": 21}, {"referenceID": 24, "context": ", [2, 25, 28, 33, 36]), in such works each node explicitly maintains a weight vector on all options.", "startOffset": 2, "endOffset": 21}, {"referenceID": 27, "context": ", [2, 25, 28, 33, 36]), in such works each node explicitly maintains a weight vector on all options.", "startOffset": 2, "endOffset": 21}, {"referenceID": 32, "context": ", [2, 25, 28, 33, 36]), in such works each node explicitly maintains a weight vector on all options.", "startOffset": 2, "endOffset": 21}, {"referenceID": 35, "context": ", [2, 25, 28, 33, 36]), in such works each node explicitly maintains a weight vector on all options.", "startOffset": 2, "endOffset": 21}, {"referenceID": 19, "context": "1 (Chernoff-Hoeffding bounds [20]).", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "One can think of this as a regularizing term and bears some similarity to what was considered in the recent breakthrough result on computing flows [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 3, "context": "4 in [4].", "startOffset": 5, "endOffset": 8}], "year": 2017, "abstractText": "We study a distributed learning process observed in human groups and other social animals. This learning process appears in settings in which each individual in a group is trying to decide over time, in a distributed manner, which option to select among a shared set of options. Specifically, we consider a stochastic dynamics in a group in which every individual selects an option in the following two-step process: (1) select a random individual and observe the option that individual chose in the previous time step, and (2) adopt that option if its stochastic quality was good at that time step. Various instantiations of such distributed learning appear in nature, and have also been studied in the social science literature. From the perspective of an individual, an attractive feature of this learning process is that it is a simple heuristic that requires extremely limited computational capacities. But what does it mean for the group \u2013 could such a simple, distributed and essentially memoryless process lead the group as a whole to perform optimally? We show that the answer to this question is yes \u2013 this distributed learning is highly effective at identifying the best option and is close to optimal for the group overall. Our analysis also gives quantitative bounds that show fast convergence of these stochastic dynamics. We prove our result by first defining a (stochastic) infinite population version of these distributed learning dynamics and then combining its strong convergence properties along with its relation to the finite population dynamics. Prior to our work the only theoretical work related to such learning dynamics has been either in deterministic special cases or in the asymptotic setting. Finally, we observe that our infinite population dynamics is a stochastic variant of the classic multiplicative weights update (MWU) method. Consequently, we arrive at the following interesting converse: the learning dynamics on a finite population considered here can be viewed as a novel distributed and low-memory implementation of the classic MWU method. \u2217This research was supported in part by an SNF Project Grant (205121 163385).", "creator": "LaTeX with hyperref package"}}}