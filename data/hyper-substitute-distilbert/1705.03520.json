{"id": "1705.03520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2017", "title": "Integral Policy Iterations for Reinforcement Learning Problems in Continuous Time and Space", "abstract": "policy iteration ( ri ) is a basic use of slope inference and measurement to solve an advanced money - making, ii. g., reinforcement programming ( rl ) or estimation procedure problem presenting the usage as the alternative to modeling rl methods. motivated by integral pi ( ipi ) extensions integrate optimal control and rl methods in directed time driven discrete ( cts ), nonlinear type proposes on - variables theory to solve the true rl application in cts, with its environment modeled roughly applying ordinary differential equation ( ode ). in objective continuous domain, proponents also generate four nonlinear - regulation ipi methods - - - and are classical relative homogeneous forms that provides advantage and link - streaming, specifically, and the other two are natural extensions of different existing off - equilibrium ipi schemes to get general rl framework. compared using the ipi components in optimal procedure, the equivalent linear schemes naturally be targeted to more general situations \u2026 don't require continuous initial strategy query problem proceed ; details are too strongly relevant because the rl schemes between cts such as advantage updating, q - routing, and value - gradient based ( vgb ) greedy algorithms designs. observed on - label behaviour is basically linear - based but can be made genuinely model - made ; in off - policy method constitutes also either partially or altogether model - induced. the characteristic properties of any ipi technique - - - constant, smooth relations, gradually consolidation towards the optimal solution - - - hence fairly intimately integrated, together with exact techniques of onto - and off - policy iteration. finally, the computed methods only executed near an inverted - cone apparatus to support the theory effectively verify the performance.", "histories": [["v1", "Tue, 9 May 2017 20:01:34 GMT  (1585kb,D)", "http://arxiv.org/abs/1705.03520v1", "20 pages, 2 figures (i.e., 6 sub-figures), 2 tables, 5 main ideal algorithms, and 1 algorithm for implementation. For a summary, a short simplified RLDM-conf. version is available at &lt;this http URL&gt;"]], "COMMENTS": "20 pages, 2 figures (i.e., 6 sub-figures), 2 tables, 5 main ideal algorithms, and 1 algorithm for implementation. For a summary, a short simplified RLDM-conf. version is available at &lt;this http URL&gt;", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["jae young lee", "richard s sutton"], "accepted": false, "id": "1705.03520"}, "pdf": {"name": "1705.03520.pdf", "metadata": {"source": "CRF", "title": "Integral Policy Iterations for Reinforcement Learning Problems in Continuous Time and Space ?", "authors": ["Jae Young Lee", "Richard S. Sutton"], "emails": ["jyounglee@ualberta.ca", "rsutton@ualberta.ca"], "sections": [{"heading": null, "text": "Policy iteration (PI) is a recursive process of policy evaluation and improvement to solve an optimal decision-making, e.g., reinforcement learning (RL) or optimal control problem and has served as the fundamental to develop RL methods. Motivated by integral PI (IPI) schemes in optimal control and RL methods in continuous time and space (CTS), this paper proposes on-policy IPI to solve the general RL problem in CTS, with its environment modelled by an ordinary differential equation (ODE). In such continuous domain, we also propose four off-policy IPI methods\u2014two are the ideal PI forms that use advantage and Q-functions, respectively, and the other two are natural extensions of the existing off-policy IPI schemes to our general RL framework. Compared to the IPI methods in optimal control, the proposed IPI schemes can be applied to more general situations and do not require an initial stabilizing policy to run; they are also strongly relevant to the RL algorithms in CTS such as advantage updating, Q-learning, and value-gradient based (VGB) greedy policy improvement. Our on-policy IPI is basically model-based but can be made partially model-free; each off-policy method is also either partially or completely model-free. The mathematical properties of the IPI methods\u2014admissibility, monotone improvement, and convergence towards the optimal solution\u2014are all rigorously proven, together with the equivalence of on- and off-policy IPI. Finally, the IPI methods are simulated with an inverted-pendulum model to support the theory and verify the performance.\nKey words: policy iteration, reinforcement learning, optimization under uncertainties, continuous time and space, iterative schemes, adaptive systems"}, {"heading": "1 Introduction", "text": "Policy iteration (PI) is a recursive process to solve an optimal decision-making/control problem by alternating between policy evaluation to obtain the value function with respect to the current policy (a.k.a. the current control law in control theory) and policy improvement to improve the policy by optimizing it using the obtained value function (Sutton and Barto, 2017; Lewis and Vrabie, 2009). PI was first proposed by Howard (1960) in the stochastic environment known as Markov decision process (MDP) and is strongly relevant to reinforcement learning (RL) and approximate dynamic programming (ADP). PI has served as a fundamental principle to develop RL and ADP methods especially when the underlying environment is modelled or approximated by an MDP in a discrete space. There are also model-free off-\n? The authors gratefully acknowledge the support of Alberta Innovates\u2013Technology Futures, the Alberta Machine Intelligence Institute, Google Deepmind, and the Natural Sciences and Engineering Research Council of Canada. \u2217 Corresponding author. Tel.: +1 587 597 8677.\nEmail addresses: jyounglee@ualberta.ca (Jae Young Lee ), rsutton@ualberta.ca (Richard S. Sutton).\npolicy PI methods using Q-functions and their extensions to incremental RL algorithms (e.g., Lagoudakis and Parr, 2003; Farahmand, Ghavamzadeh, Mannor, and Szepesva\u0301ri, 2009; Maei, Szepesva\u0301ri, Bhatnagar, and Sutton, 2010). Here, offpolicy PI is a class of PI methods whose policy evaluation is done while following a policy, termed as a behavior policy, which is possibly different from the target policy to be evaluated; if the behavior and target policies are same, it is called an on-policy method. When the MDP is finite, all the on- or off-policy PI methods converge towards the optimal solution in finite time. Another advantage is that compared to backward-in-time dynamic programming, the forward-intime computation of PI like the other ADP methods (Powell, 2007) alleviates the problem known as the curse of dimensionality. In continuing tasks, a discount factor \u03b3 is normally introduced to PI and RL to suppress the future reward and thereby have a finite return. Sutton and Barto (2017) gives a comprehensive overview of PI, ADP, and RL algorithms with their practical applications and recent success in the RL field.\nPreprint submitted to Automatica 11 May 2017\nar X\niv :1\n70 5.\n03 52\n0v 1\n[ cs\n.A I]\n9 M\nay 2\n1.1 PI and RL in Continuous Time and Space (CTS)\nDifferent from the MDP in discrete space, the dynamics of real physical world is usually modelled by (ordinary) differential equations (ODEs) inevitably in CTS. PI has been also studied in such continuous domain mainly under the framework of deterministic optimal control, where the optimal solution is characterized by the partial differential Hamilton-Jacobi-Bellman equation (HJBE) which is however extremely difficult or hopeless to be solved analytically, except in a very few special cases. PI in this field is often referred to as successive approximation of the HJBE (to recursively solve it!), and the main difference among them lies in their policy evaluation\u2014the earlier versions of PI solve the associated infinitesimal Bellman equation (a.k.a. Lyapunov or Hamiltonian equation) to obtain the value function for the current policy (e.g., Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017). Motivated by those two approaches above, Vrabie and Lewis (2009) recently proposed a partially model-free PI scheme called integral PI (IPI), which is more relevant to RL/ADP in that the Bellman equation associated with its policy evaluation is of a temporal difference form\u2014see (Lewis and Vrabie, 2009) for a comprehensive overview. By partially model-free, it is meant in this paper that the PI can be done without explicit use of some or any inputindependent part of the dynamics. IPI is then extended to a series of completely or partially model-free off-policy IPI methods (e.g., Lee, Park, and Choi, 2012, 2015; Luo, Wu, Huang, and Liu, 2014; Modares, Lewis, and Jiang, 2016, to name a few), with a hope to further extend them to incremental off-policy RL methods for adaptive optimal control in CTS\u2014see (Vamvoudakis, Vrabie, and Lewis, 2014) for an incremental extension of the on-policy IPI method. The on/off-policy equivalence and the mathematical properties of stability/admissibility/monotone-improvement of the generated policies and convergence towards the optimal solution were also studied in the literatures above all regarding PI in CTS.\nOn the other hand, the aforementioned PI methods in CTS were all designed via Lyapunov\u2019s stability theory (Haddad and Chellaboina, 2008) to guarantee that the generated policies are all asymptotically stable and thereby yield finite returns (at least on a bounded region around an equilibrium state), provided that so is the initial policy. There are two main restrictions, however, for these stability-based works to be extended to the general RL framework. One is the fact that except in the LQR case (e.g., Modares et al., 2016), there is no (or less if any) direct connection between stability and discount factor \u03b3 in RL. This is why in the nonlinear cases, the aforementioned PI methods in CTS only consider the to-\ntal case \u03b3 = 1, but not the discounted case 0 < \u03b3 < 1. 1 The other restriction of those stability-based designs is that the initial policy needs to be asymptotically stabilizing to run the PI methods. This is quite contradictory for IPI methods since they are partially or completely model-free, but it is hard or even impossible to find a stabilizing policy without knowing the dynamics. Besides, compared with the RL frameworks in CTS, e.g., those in (Doya, 2000; Mehta and Meyn, 2009; Fre\u0301maux, Sprekeler, and Gerstner, 2013), this stability-based approach rather restricts the class of the cost (or reward) and the dynamics. For instances, the dynamics was assumed to have at least one equilibrium state, 2 and the goal was always to (locally) stabilize the system in an optimal fashion to that equilibrium state although there may also exist multiple isolated equilibrium states to be considered or bifurcation; for such optimal stabilization, the cost was always crafted to be non-negative for all points and zero at the equilibrium.\nIndependently of the research on PI, several RL methods have come to be proposed also in CTS. Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fre\u0301maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks. Mehta and Meyn (2009) defined the Hamiltonian function as a Q-function and then proposed a Q-learning method in CTS based on stochastic approximation. Unlike in MDP, however, these RL methods in CTS and the related action-dependent (AD) functions such as advantage and Qfunctions are barely relevant to the PI methods in CTS due to the gap between optimal control and RL.\n1.2 Contributions and Organizations\nThe main goal of this paper is to build up a theory on IPI in a general RL framework when the time and the state-action space are all continuous and the environment is modelled by an ODE. As a result, a series of IPI methods are proposed in the general RL framework with mathematical analysis. This also provides the theoretical connection of IPI to the aforementioned RL methods in CTS. The main contributions of this paper can be summarized as follows.\n(1) Motivated by the work of IPI (Vrabie and Lewis, 2009; Lee et al., 2015) in the optimal control framework, we propose the corresponding on-policy IPI scheme in the general RL framework and then prove its mathematical properties of admissibility/monotone-improvement\n1 In RL community, the return or the RL problem (in a discrete space) is said to be discounted if 0 \u2264 \u03b3 < 1 and total if \u03b3 = 1. 2 For an example of a dynamics with no equilibrium state, see (Haddad and Chellaboina, 2008, Example 2.2).\nof the generated policies and the convergence towards the optimal solution (see Section 3). To establish them, we rigorously define the general RL problem in CTS with its environment formed by an ODE and then build up a theory regarding policy evaluation and improvement (see Section 2). (2) Extending on-policy IPI in Section 3, we propose four off-policy IPI methods in CTS\u2014two named integral advantage PI (IAPI) and integral Q-PI (IQPI) are the ideal PI forms of advantage updating (Baird III, 1993; Doya, 2000) and Q-learning in CTS, and the other two named integral explorized PI (IEPI) and integral C-PI (ICPI) are the natural extensions of the existing off-policy IPI methods (Lee et al., 2015) to our general RL problem. All of the off-policy methods are proven to generate the effectively same policies and value functions to those in on-policy IPI\u2014they all satisfy the above mathematical properties of on-policy IPI. These are all shown in Section 4 with detailed discussions and comparisons.\nThe proposed on-policy IPI is basically model-based but can be made partially model-free by slightly modifying its policy improvement (see Section 3.2). IEPI is also partially model-free; IAPI and IQPI are even completely model-free; so is ICPI, but only applicable under the special u-affineand-concave (u-AC) setting shown in Section 3.3. Here, we emphasize that Doya (2000)\u2019s VGB greedy policy improvement is also developed under this u-AC setting, and ICPI provides its model-free version. Finally, to support the theory and verify the performance, simulation results are provided in Section 5 for an inverted-pendulum model. As shown in the simulations and all of the IPI algorithms in this paper, the initial policy is not required to be asymptotically stable to achieve the learning objective. Conclusions follow in Section 6. This theoretical work lies between the fields of optimal control and machine learning and also provides the unified framework of unconstrained and input-constrained formulations in both RL and optimal control (e.g., Doya, 2000; Abu-Khalaf and Lewis, 2005; Mehta and Meyn, 2009; Vrabie and Lewis, 2009; Lee et al., 2015 as the special cases of our framework).\nNotations and Terminologies. N, Z, and R are the sets of all natural numbers, integers, and real numbers, respectively; R .= R\u222a {\u2212\u221e,\u221e} is the set of all extended real numbers; Z+ . = N \u222a {0}, the set of all nonnegative integers; Rn\u00d7m is the set of all n-by-m real matrices; AT and rank (A) denote the transpose and the rank of a matrix A \u2208 Rn\u00d7m, respectively. Rn .= Rn\u00d71 is the n-dimensional Euclidean space; span{xj}mj=1 is the linear subspace of Rn spanned by the vectors x1, \u00b7 \u00b7 \u00b7 , xm \u2208 Rn; \u2016x\u2016 denotes the Euclidean norm of x \u2208 Rn. A subset \u2126 of Rn is said to be compact if it is closed and bounded; for any Lebesque measurable subset E of Rn, we denote the Lebesque measure of E by |E|. A function f : D \u2192 Rm on a domain D \u2286 Rn is said to be C1 if its first-order partial derivatives exist and are all continuous; \u2207f : D \u2192 Rm\u00d7n denotes the gradient of f ; Imf \u2286 Rm is the image of f . The restriction of f on a subset \u2126 \u2286 D is denoted by f |\u2126."}, {"heading": "2 Preliminaries", "text": "In this paper, the state space X is given by X .= Rn, and the action space U \u2286 Rm is an m-dimensional manifold in Rm with (or without) boundary; t \u2265 0 denotes a given specific time instant. The environment considered in this paper is the deterministic one in CTS described by the following ODE:\nX\u0307\u03c4 = f(X\u03c4 , U\u03c4 ), (1)\nwhere \u03c4 \u2208 [t,\u221e) is the time variable; f : X \u00d7 U \u2192 X is a continuous function; X\u03c4 \u2208 X denotes the state vector at time \u03c4 ; the action trajectory U\u00b7 : [t,\u221e) \u2192 U is a right continuous function over [t,\u221e).\nA (non-stationary) policy \u00b5 .= \u00b5(\u03c4, x) refers to a function \u00b5 : [t,\u221e)\u00d7X \u2192 U such that\n(1) for each fixed \u03c4 \u2265 t, \u00b5(\u03c4, \u00b7) is continuous; (2) for each fixed x \u2208 X , \u00b5(\u00b7, x) is right continuous; (3) for each x \u2208 X , the state trajectory X\u00b7 generated under\nU\u03c4 = \u00b5(\u03c4,X\u03c4 ) \u2200\u03c4 \u2265 t and the initial condition Xt = x is uniquely defined over the whole time interval [t,\u221e).\nIt is said to be stationary if there is a function \u03c0 : X \u2192 U such that \u00b5(\u03c4, x) = \u03c0(x) for all (\u03c4, x) \u2208 [t,\u221e) \u00d7 X , in which case we call \u03c0 a (stationary) policy. In this paper, we use \u03c0 to indicate a stationary policy, and \u00b5 to denote any non-stationary behavior policy; the latter will be not shown until Section 4.\nFor notational efficiency and consistency to the classical RL framework, we will use the notation\nE\u03c0[Z|Xt = x] (or its abbreviation Ex\u03c0[Z]),\nwhich plays no stochastic role but just means the deterministic value Z when Xt = x and U\u03c4 = \u03c0(X\u03c4 ) for all \u03c4 \u2265 t. Using this notation, the state vector X\u03c4 at time \u03c4 \u2265 t generated under the initial condition Xt = x and the policy \u03c0 is denoted by E\u03c0[X\u03c4 |Xt = x] or simply Ex\u03c0[X\u03c4 ]. Throughout the paper, we also denote \u2206t > 0 the time difference, and for simplicity, t\u2032 the next time and (X \u2032t, U \u2032 t) the next state and action at that time, i.e., t\u2032 .= t + \u2206t, X \u2032t . = Xt\u2032 , and U \u2032t . = Ut\u2032 . For any differentiable function v : X \u2192 R, its time derivative v\u0307 : X \u00d7 U \u2192 R is defined as\nv\u0307(Xt, Ut) . = lim\n\u2206t\u21920 v(X \u2032t)\u2212 v(Xt) \u2206t ,\nwhich is explicitly expressed by the chain rule as\nv\u0307(Xt, Ut) = \u2207v(Xt)f(Xt, Ut),\nwhere Xt \u2208 X and Ut \u2208 U are free variables.\n2.1 RL Problem in Continuous Time and Space\nThe RL problem considered in this paper is to find the best policy \u03c0\u2217 that maximizes the value function v\u03c0 : X \u2192 R\nv\u03c0(x) . = E\u03c0[Gt|Xt = x], (2)\nwhere Gt is the discounted or total return defined as\nGt . = \u222b \u221e t \u03b3\u03c4\u2212tR\u03c4 d\u03c4,\nwith the immediate reward R\u03c4 . = R(X\u03c4 , U\u03c4 ) \u2208 R at time \u03c4 and the discount factor \u03b3 \u2208 (0, 1]. The reward R .= R(x, u) here is a continuous upper-bounded function of x \u2208 X and u \u2208 U . 3 For any policy \u03c0, we also define the \u03c0-reward R\u03c0 (and R\u03c0\u03c4 ) as R \u03c0(x) . = R(x, \u03c0(x)) and R\u03c0\u03c4 . = R\u03c0(X\u03c4 ). In this paper, an admissible policy \u03c0 is defined as follows.\nDefinition 1 A policy \u03c0 (or its value function v\u03c0) is said to be admissible, denoted by v\u03c0 \u2208 Va, if\n|v\u03c0(x)| <\u221e for all x \u2208 X ,\nwhere Va . = { v\u03c0 : \u03c0 is admissible}, the set of all admissible value functions v\u03c0 .\nTo make our RL problem feasible, this paper assumes that every v\u03c0 \u2208 Va is C1 and that there is an optimal admissible policy \u03c0\u2217 such that v\u03c0 v\u2217 holds for any policy \u03c0. Here, v\u2217 is the optimal value function; the partial ordering v w for any two functions v, w : X \u2192 R means v(x) \u2264 w(x) for all x \u2208 X . There may exist another optimal policy \u03c0\u2032\u2217 than \u03c0\u2217, but the optimal value function is always same by v\u03c0\u2032\u2217 v\u03c0\u2217 and v\u03c0\u2217 v\u03c0\u2032\u2217 . In this paper, \u03c0\u2217 indicates any one of the optimal policies, and v\u2217 denotes the unique optimal value function for them. Also note that in the discounted case \u03b3 \u2208 (0, 1), v\u2217 is upper-bounded since so is R. Hence, for the consistency to the discounted case, we assume only for the total case \u03b3 = 1 that v\u2217 is upper-bounded and\n\u2200admissible \u03c0 : lim sup k\u2192\u221e l\u03c0(x, k; v\u2217) \u2264 0, (3)\nwhere l\u03c0(x, k; v) . = \u03b3k\u2206tE\u03c0[v(Xt+k\u2206t)|Xt = x]. Here, (3) is also true in the discounted case since limk\u2192\u221e \u03b3k\u2206t = 0 and v\u2217 is upper-bounded.\nRemark 1 Admissibility of \u03c0 strongly depends not only on the policy \u03c0 itself, but on the choice ofR and \u03b3 in v\u03c0 and the properties (e.g., boundedness and stability) of the system (1) and the state trajectory X\u00b7 under \u03c0 as examplified below.\n(1) For \u03b3 \u2208 (0, 1], if there exist \u03b1 \u2208 [0, \u03b3\u22121) and a function\n3 By the upper-boundedness of R, we only obviate the situation when the reward R\u03c4 \u2192\u221e as \u2016X\u03c4\u2016 \u2192 \u221e and/or \u2016U\u03c4\u2016 \u2192 \u221e.\nR\u0304 : X \u2192 [0,\u221e) such that R\u03c4 is bounded as\n\u2200x \u2208 X : E\u03c0 [ |R\u03c4 | \u2223\u2223Xt = x ] \u2264 \u03b1\u03c4\u2212tR\u0304(x), (4) then v\u03c0 \u2208 Va since for k = \u2212 1ln\u03b1\u03b3 and each x \u2208 X , |v\u03c0(x)| \u2264 kR\u0304(x) < \u221e. 4 For the total case \u03b3 = 1, the condition (4) implies that the immediate reward R\u03c4 exponentially converges to 0 with the rate \u03b1 \u2208 [0, 1).\n(2) For \u03b3 \u2208 (0, 1), if the reward R or R\u03c0 is lower-bounded, or the trajectory Ex\u03c0[X\u00b7] is bounded for each x \u2208 X , then v\u03c0 \u2208 Va. This is because such boundedness gives the bound (4) for \u03b1 = 1 and some constant R\u0304 > 0. 5 In this case, we have supx\u2208X |v\u03c0(x)| \u2264 k \u00b7 R\u0304 <\u221e, so v\u03c0 is also bounded. Especially, when R is lower-bounded, such property is independent of the policy \u03c0\u2014in other words, v\u03c0 is bounded (and thus admissible) for any given policy \u03c0 (see the simulation setting in Section 5 for a practical example of this bounded R case).\nConsider the Hamiltonian function h : X \u00d7U \u00d7R1\u00d7n \u2192 R\nh(x, u, p) . = R(x, u) + p f(x, u), (5)\nby which the time derivative of any differentiable function v : X \u2192 R can be represented as\nv\u0307(x, u) = h(x, u,\u2207v(x))\u2212R(x, u), (6)\nfor all (x, u) \u2208 X \u00d7 U . Substituting u = \u03c0(x) for any policy \u03c0 into (6), we also have the following expression:\nEx\u03c0[Rt + v\u0307(Xt, Ut)] = h(x, \u03c0(x),\u2207v(x)) \u2200x \u2208 X , (7)\nwhich converts a time-domain expression (the left-hand side) into the Hamiltonian formula expressed in the state-space (the right-hand side). In addition, essentially required in our theory is the following lemma, which also provides conversions of (in)equalities between time domain and state space.\nLemma 1 Let v : X \u2192 R be any differentiable function. Then, for any policy \u03c0,\nv(x) \u2264 E\u03c0 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t v(X \u2032t) \u2223\u2223\u2223\u2223Xt = x] (8) for all x \u2208 X and all \u2206t > 0 iif\n\u2212 ln \u03b3 \u00b7 v(x) \u2264 h(x, \u03c0(x),\u2207v(x)) \u2200x \u2208 X . (9)\nMoreover, the equalities in (8) and (9) are also necessary and sufficient. That is, the equality in (8) holds \u2200x \u2208 X and \u2200\u2206t > 0 iif the equality in (9) is true \u2200x \u2208 X .\n4 An example is the LQR case shown in Section 3.3. 5 R is already upper-bounded, so it is bounded iif lower-bounded; by continuity of R and \u03c0, R(X\u00b7, \u03c0(X\u00b7)) is bounded if so is X\u00b7.\nProof. By the standard calculus,\nd\nd\u03c4\n( \u03b3\u03c4\u2212tv(X\u03c4 ) ) = \u03b3\u03c4\u2212t ( ln \u03b3 \u00b7 v(X\u03c4 ) + v\u0307(X\u03c4 , U\u03c4 ) ) for any x \u2208 X and any \u03c4 \u2265 t. Hence, by (6) and (9), we have for any \u03c4 \u2265 t that 0 \u2264 Ex\u03c0 [ \u03b3\u03c4\u2212t \u00b7 ( h(X\u03c4 , \u03c0(X\u03c4 ),\u2207v(X\u03c4 )) + ln \u03b3 \u00b7 v(X\u03c4 )\n)] = Ex\u03c0 [ \u03b3\u03c4\u2212t \u00b7 ( R\u03c4 + v\u0307(X\u03c4 , U\u03c4 ) + ln \u03b3 \u00b7 v(X\u03c4 )\n)] = E\u03c0 [ \u03b3\u03c4\u2212tR\u03c4 + d\nd\u03c4\n( \u03b3\u03c4\u2212tv(X\u03c4 ) )\u2223\u2223\u2223\u2223Xt = x] \u2200x \u2208 X , and, for any x \u2208 X and any \u2206t > 0, integrating it from t to t\u2032 (= t + \u2206t) yields (8). One can also show that the equality in (9) for all x \u2208 X implies the equality in (8) for all x \u2208 X and \u2206t > 0 by following the same procedure. Finally, the proof of the opposite direction can be easily done by following the similar procedure to the derivation of (11) from (10) below. 2\n2.2 Bellman Equation with Boundary Condition\nBy time-invariance property 6 of a stationary policy \u03c0 and\nGt = \u222b t\u2032 t \u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206tGt\u2032 ,\nwe can see that v\u03c0 \u2208 Va satisfies the Bellman equation:\nv\u03c0(x) = E\u03c0 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t v\u03c0(X \u2032 t) \u2223\u2223\u2223\u2223Xt = x] (10)\nfor any x \u2208 X and any \u2206t > 0. Using (10), we obtain the boundary condition of v\u03c0 \u2208 Va at \u03c4 =\u221e.\nProposition 1 Suppose that \u03c0 is admissible. Then, \u2200x \u2208 X , lim\u03c4\u2192\u221e \u03b3 \u03c4\u2212t \u00b7 Ex\u03c0[v\u03c0(Xt+\u03c4 )] = 0.\nProof. Taking the limit \u2206t\u2192\u221e of (10) yields\nv\u03c0(x) = lim \u2206t\u2192\u221e\nEx\u03c0 [ \u222b t+\u2206t\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t v\u03c0(X \u2032 t) ] = v\u03c0(x) + lim\n\u2206t\u2192\u221e \u03b3\u2206t \u00b7 Ex\u03c0[v\u03c0(X \u2032t)],\nwhich implies lim\u03c4\u2192\u221e \u03b3\u03c4\u2212t \u00b7 Ex\u03c0[v\u03c0(Xt+\u03c4 )] = 0. 2\nCorollary 1 Suppose that \u03c0 is admissible. Then, for any x \u2208 X and any \u2206t > 0, limk\u2192\u221e l\u03c0(x, k; v\u03c0) = 0.\n6 v\u03c0(x) = E\u03c0[Gt1 |Xt1 = x] = E\u03c0[Gt2 |Xt2 = x] \u2200t1, t2 \u2265 0.\nFor an admissible policy \u03c0, rearranging (10) as\n1\u2212 \u03b3\u2206t\n\u2206t \u00b7 v\u03c0(x) =E\u03c0\n[ 1\n\u2206t \u222b t\u2032 t \u03b3\u03c4\u2212tR\u03c4 d\u03c4\n+ \u03b3\u2206t \u00b7 v\u03c0(X \u2032 t)\u2212 v\u03c0(Xt)\n\u2206t \u2223\u2223\u2223\u2223Xt = x], limiting \u2206t\u2192 0, and using (7) yield the infinitesimal form:\n\u2212 ln \u03b3 \u00b7 v\u03c0(x) = h\u03c0(x, \u03c0(x)) = h(x, \u03c0(x),\u2207v\u03c0(x)) \u2200x \u2208 X ,\n(11)\nwhere h\u03c0 : X \u00d7 U \u2192 R is the Hamiltonian function for a given admissible policy \u03c0 and is defined, with a slight abuse of notation, as\nh\u03c0(x, u) . = h(x, u,\u2207v\u03c0(x)), (12)\nwhich is obviously continuous since so are the functions f , R, and\u2207v\u03c0 (see also (5)). Both h\u03c0(x, u) and h(x, u, v\u03c0(x)) will be used interchangeably in this paper for convenience to indicate the same Hamiltonian function for \u03c0; the Hamiltonian function for the optimal policy \u03c0\u2217 will be also denoted by h\u2217(x, u), or equivalently, h\u2217(x, u,\u2207v\u2217(x)).\nThe application of Lemma 1 shows that finding v\u03c0 satisfying (10) and (11) are both equivalent. In the following theorem, we state that the boundary condition (15), the counterpart of that in Corollary 1 is actually necessary and sufficient for a solution v of the Bellman equation (13) or (14) to be equal to the corresponding value function v\u03c0 .\nTheorem 1 Let \u03c0 be admissible and v : X \u2192 R be a function such that either of the followings holds \u2200x \u2208 X :\n(1) v satisfies the Bellman equation for some \u2206t > 0:\nv(x) = E\u03c0 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t v(X \u2032t) \u2223\u2223\u2223\u2223Xt = x]; (13)\n(2) v is differentiable and satisfies\n\u2212 ln \u03b3 \u00b7 v(x) = h(x, \u03c0(x),\u2207v(x)). (14)\nThen, limk\u2192\u221e l\u03c0(x, k; v) = v(x)\u2212 v\u03c0(x) for each x \u2208 X . Moreover, v = v\u03c0 over X if (and only if)\n\u2200x \u2208 X : lim k\u2192\u221e l\u03c0(x, k; v) = 0. (15)\nProof. Since \u03c0 is admissible, E\u03c0[v\u03c0(X\u03c4 )|Xt = x] is finite for all \u03c4 \u2265 t and x \u2208 X . First, let v\u0303(x) .= v(x)\u2212 v\u03c0(x) and suppose v satisfies (13). Then, subtracting (10) from (13) yields v\u0303(x) = \u03b3\u2206t \u00b7Ex\u03c0[v\u0303(X \u2032t)], whose repetitive applications to itself results in v\u0303(x) = \u03b3\u2206t Ex\u03c0[v\u0303(Xt+\u2206t)] = \u00b7 \u00b7 \u00b7 = \u03b3k\u2206t Ex\u03c0 [ v\u0303(Xt+k\u2206t) ] .\nTherefore, by limiting k \u2192 \u221e and using Corollary 1, we obtain limk\u2192\u221e l\u03c0(x, k; v) = v\u0303(x), which also proves v\u0303 = 0 under (15). The proof of the other case for v satisfying (14) instead of (13) is direct by Lemma 1. Conversely, if v = v\u03c0 over X , then (15) is obviously true by Corollary 1. 2\nRemark 2 (15) is always true for any \u03b3 \u2208 (0, 1) and any bounded v. Hence, whenever v\u03c0 is bounded and 0 < \u03b3 < 1, e.g., the second case in Remark 1 including the simulation example in Section 5, any bounded function v satisfying the Bellman equation (13) or (14) is equal to v\u03c0 by Theorem 1.\n2.3 Optimality Principle and Policy Improvement\nNote that the optimal value function v\u2217 \u2208 Va satisfies\nv\u2217(x) = max \u03c0 v\u03c0(x)\nand hence, by principle of optimality, the following Bellman optimality equation:\nv\u2217(x) = max \u03c0\n{ Ex\u03c0 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t v\u2217(X \u2032 t) ]} (16)\nfor all x \u2208 X , where max\u03c0 denotes the maximization among the all stationary policies. Hence, by the similar procedure to derive (11) from (10), we obtain from (16)\n\u2212 ln \u03b3 \u00b7 v\u2217(x) = max \u03c0 h\u2217(x, \u03c0(x),\u2207v\u2217(x)) \u2200x \u2208 X .\nHere, the above maximization formula can be characterized as the following HJBE:\n\u2212 ln \u03b3 \u00b7 v\u2217(x) = max u\u2208U h(x, u,\u2207v\u2217(x)), \u2200x \u2208 X , (17)\nand the optimal policy \u03c0\u2217 as\n\u03c0\u2217(x) \u2208 arg max u\u2208U h\u2217(x, u), \u2200x \u2208 X\nunder the following assumption. 7\nAssumption 1 For any admissible policy \u03c0, there exists a policy \u03c0\u2032 such that for all x \u2208 X ,\n\u03c0\u2032(x) \u2208 arg max u\u2208U h\u03c0(x, u). (18)\nThe following theorems support the argument.\n7 If U is compact, then by continuity of h\u03c0 , arg maxu\u2208U h\u03c0(x, u) in (18) is non-empty for any admissible \u03c0 and any x \u2208 X . This guarantees the existence of a function \u03c0\u2032 satisfying (18). For the other cases, where the action space U is required to be convex, see Section 3.3 (specifically, (30) and (34)).\nTheorem 2 (Policy Improvement Theorem) Suppose \u03c0 is admissible and Assumption 1 holds. Then, the policy \u03c0\u2032 given by (18) is also admissible and satisfies v\u03c0 v\u03c0\u2032 v\u2217.\nProof. Since \u03c0 is admissible, (18) in Assumption 1 and (11) imply that for any x \u2208 X ,\nh\u03c0(x, \u03c0 \u2032(x)) \u2265 h\u03c0(x, \u03c0(x)) = \u2212 ln \u03b3 \u00b7 v\u03c0(x).\nBy Lemma 1, it is equivalent to\nv\u03c0(x) \u2264 Ex\u03c0\u2032 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t v\u03c0(X \u2032 t)\n] ,\nand by the repetitive applications itself,\nv\u03c0(x) \u2264 Ex\u03c0\u2032 [ \u222b t+k\u2206t\nt\n\u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 k\u2206t v\u03c0(Xt+k\u2206t) ] .\nLet V\u2217 \u2208 R be an upper bound of v\u2217. Then, in the limit k \u2192\u221e, we obtain for each x \u2208 X\nv\u03c0(x) \u2264 v\u03c0\u2032(x) + lim sup k\u2192\u221e l\u03c0\u2032(x, k; v\u03c0)\n\u2264 v\u03c0\u2032(x) + lim sup k\u2192\u221e \u03b3k\u2206t \u00b7 Ex\u03c0\u2032 [v\u2217(Xt+k\u2206t)] (19) \u2264 v\u03c0\u2032(x) + max{0, V\u2217},\nfrom which and v\u03c0 \u2208 Va, we can conclude that v\u03c0\u2032(x) for each x \u2208 X has a lower bound; since it also has an upper bound as v\u03c0\u2032(x) \u2264 v\u2217(x) \u2264 V\u2217, \u03c0\u2032 is admissible. Finally, (3) with the admissible policy \u03c0\u2032 and (19) imply that for each x \u2208 X ,\nv\u03c0(x) \u2264 v\u03c0\u2032(x) + lim sup k\u2192\u221e l\u03c0\u2032(x, k; v\u2217) \u2264 v\u03c0\u2032(x) \u2264 v\u2217(x),\nwhich completes the proof. 2\nCorollary 2 Under Assumption 1, v\u2217 satisfies the HJBE (17).\nProof. Under Assumption 1, let \u03c0\u2032\u2217 be a policy such that \u03c0\u2032\u2217(x) \u2208 arg maxu\u2208U h\u2217(x, u). Then, \u03c0\u2032\u2217 is admissible and v\u2217 v\u03c0\u2032\u2217 holds by Theorem 2; trivially, v\u03c0\u2032\u2217 v\u2217. Hence, \u03c0 \u2032 \u2217 is an optimal policy. Noting that any admissible \u03c0 satisfies (11), we obtain from \u201c(11) with \u03c0 = \u03c0\u2032\u2217 and v\u03c0 = v\u2217:\u201d\n\u2212 ln \u03b3\u00b7v\u2217(x) = h(x, \u03c0\u2032\u2217(x),\u2207v\u2217(x)) = max u\u2208U h(x, u,\u2207v\u2217(x))\nfor all x \u2208 X , which is exactly the HJBE (17). 2\nFor the uniqueness of the solution v\u2217 to the HJBE, we further assume throughout the paper that\nAssumption 2 There is one and only one element w\u2217 \u2208 Va over Va that satisfies the HJBE:\n\u2212 ln \u03b3 \u00b7 w\u2217(x) = max u\u2208U h(x, u,\u2207w\u2217(x)), \u2200x \u2208 X .\nCorollary 3 Under Assumptions 1 and 2, v\u2217 = w\u2217. That is, v\u2217 is the unique solution to the HJBE (17) over Va.\nRemark 3 The policy improvement equation (18) in Assumption 1 can be generalized and rewritten as\n\u03c0\u2032(x) \u2208 arg max u\u2208U\n[ \u03ba \u00b7 h\u03c0(x, u) + b\u03c0(x) ] (20)\nfor any constant \u03ba > 0 and any function b\u03c0 : X \u2192 R. 8 Obviously, (18) is the special case of (20) with \u03ba = 1 and b\u03c0(x) = 0; policy improvement of our IPI methods in this paper can be also considered to be equal to \u201c(20) with a special choice of \u03ba and b\u03c0\u201d (as long as the associated functions are perfectly estimated in their policy evaluation)."}, {"heading": "3 On-policy Integral Policy Iteration (IPI)", "text": "Now, we are ready to state our basic primary PI scheme, which is named on-policy integral policy iteration (IPI) and estimate the value function v\u03c0 only (in policy evaluation) based on the on-policy state trajectory X\u00b7 generated under \u03c0 during some finite time interval [t, t\u2032]. The value function estimate obtained in policy evaluation is then utilized in the maximization process (policy improvement) yielding the next improved policy.\nAlgorithm 1a: On-policy IPI for the General Case (1)\u2013(2)\n1 Initialize: { \u03c00 : X \u2192 U , the initial admissible policy; \u2206t > 0, the time difference;\n2 i\u2190 0; 3 repeat 4 Policy Evaluation: given policy \u03c0i, find the solution\nvi : X \u2192 R to the Bellman equation: for any x \u2208 X ,\nvi(x) = Ex\u03c0i [ \u222b t\u2032 t \u03b3\u03c4\u2212tR\u03c4 d\u03c4 + \u03b3 \u2206t vi(X \u2032 t) ] ; (21)\n5 Policy Improvement: find a policy \u03c0i+1 such that\n\u03c0i+1(x) \u2208 arg max u\u2208U h(x, u,\u2207vi(x)) \u2200x \u2208 X ; (22)\n6 i\u2190 i+ 1; until convergence is met.\n8 This is obviously true since the modification term \u201cb\u03c0(x)\u201d does not depend on u \u2208 U and thus not contribute to the maximization.\nAlgorithm 1a describes the whole procedure of on-policy IPI\u2014it starts with an initial admissible policy \u03c00 (line 1) and performs policy evaluation and improvement until vi and/or \u03c0i converge (lines 4\u20137). In policy evaluation (line 4), the agent solves the Bellman equation (21) to find the value function vi = v\u03c0i for the current policy \u03c0i. Then, in policy improvement (line 5), the next policy \u03c0i+1 is obtained by maximizing the associated Hamiltonian function.\n3.1 Admissibility, Monotone Improvement, & Convergence\nAs stated in Theorem 3 below, on-policy IPI guarantees the admissibility and monotone improvement of \u03c0i and the perfect value function estimation vi = v\u03c0i at each i-th iteration under Assumption 1 and the boundary condition:\nAssumption 3a For each i \u2208 Z+, if \u03c0i is admissible, then\nlim k\u2192\u221e l\u03c0i(x, k; vi) = 0 for any x \u2208 X .\nTheorem 3 Let {\u03c0i}\u221ei=0 and {vi}\u221ei=0 be the sequences generated by Algorithm 1a under Assumptions 1 and 3a. Then,\n(P1) \u2200i \u2208 Z+ : vi = v\u03c0i ; (P2) \u2200i \u2208 Z+ : \u03c0i+1 is admissible and satisfies\n\u03c0i+1(x) \u2208 arg max u\u2208U h\u03c0i(x, u); (23)\n(P3) the policy is monotonically improved, i.e.,\nv\u03c00 v\u03c01 \u00b7 \u00b7 \u00b7 v\u03c0i v\u03c0i+1 \u00b7 \u00b7 \u00b7 v\u2217.\nProof. \u03c00 is admissible by the first line of Algorithm 1a. For any i \u2208 Z+, suppose \u03c0i is admissible. Then, since vi satisfies Assumption 3a, vi = v\u03c0i by Theorem 1. Moreover, Theorem 2 under Assumption 1 shows that \u03c0i+1 is admissible and satisfies v\u03c0i v\u03c0i+1 v\u2217. Furthermore, (23) holds by (12), (22), and vi = v\u03c0i . Finally, the proof is completed by mathematical induction. 2\nFrom Theorem 3, one can directly see that for any x \u2208 X , the real sequence {vi(x) \u2208 R}\u221ei=0 satisfies\nv0(x) \u2264 \u00b7 \u00b7 \u00b7 \u2264 vi(x) \u2264 vi+1(x) \u2264 \u00b7 \u00b7 \u00b7 \u2264 v\u2217(x) <\u221e, (24) implying pointwise convergence to some function v\u0302\u2217. Since vi (= v\u03c0i ) is continuous by the C\n1-assumption on every v\u03c0 \u2208 Va (see Section 2.1), the convergence is uniform on any compact subset of X by Dini\u2019s theorem (Thomson, Bruckner, and Bruckner, 2001) provided that v\u0302\u2217 is continuous. This is summarized and sophisticated in the following theorem.\nTheorem 4 Under the same conditions to Theorem 3, there is a Lebesque measurable, lower semicontinuous function v\u0302\u2217 defined as v\u0302\u2217(x) . = supi\u2208Z+ vi(x) such that\n(1) vi \u2192 v\u0302\u2217 pointwisely on X ; (2) for any \u03b5 > 0 and any compact set \u2126 of X , there exists\nits compact subset E \u2286 \u2126 \u2282 X such that \u2223\u2223\u2126 \\E\u2223\u2223 < \u03b5, v\u0302\u2217|E is continuous, and vi \u2192 v\u0302\u2217 uniformly on E.\nMoreover, if v\u0302\u2217 is continuous over X , then the convergence vi \u2192 v\u0302\u2217 is uniform on any compact subset of X .\nProof. By (24), the sequence {vi(x) \u2208 R}\u221ei=0 for any fixed x \u2208 X is monotonically increasing and upper bounded by v\u2217(x) <\u221e. Hence, vi(x) converges to v\u0302\u2217(x) by monotone convergence theorem (Thomson et al., 2001), the pointwise convergence vi \u2192 v\u0302\u2217. Since vi is continuous, v\u0302\u2217 is Lebesque measurable and lower semicontinuous by its construction (Folland, 1999, Propositions 2.7 and 7.11c). Next, by Lusin\u2019s theorem (Loeb and Talvila, 2004), for any \u03b5 > 0 and any compact set \u2126 \u2282 X , there exists a compact subset E \u2286 \u2126 such that |\u2126\\E| < \u03b5 and the restriction v\u0302\u2217|E is continuous. Hence, the monotone sequence vi converges to v\u0302\u2217 uniformly on E (and on any compact subset of X if v\u0302\u2217 is continuous over X ) by Dini\u2019s theorem (Thomson et al., 2001). 2\nNext, we prove the convergence vi \u2192 v\u2217 to the optimal solution v\u2217 using the PI operator T : Va \u2192 Va defined on the space Va of admissible value functions as\nTv\u03c0 . = v\u03c0\u2032\nunder Assumption 1, where \u03c0\u2032 is the next admissible policy that satisfies (18) and is obtained by policy improvement with respect to the given value function v\u03c0 \u2208 Va. Let its N -th recursion TN be defined as TNv\u03c0 . = TN\u22121[Tv\u03c0] and T 0v\u03c0 . = v\u03c0 . Then, any sequence {vi \u2208 Va}\u221ei=0 generated by Algorithm 1a under Assumptions 1 and 3a satisfies\nTNv0 = vN for any N \u2208 N.\nLemma 2 Under Assumptions 1 and 2, the optimal value function v\u2217 is the unique fixed point of TN for all N \u2208 N.\nProof. See Appendix A. 2\nTo precisely state our convergence theorem, let \u2126 be any given compact subset and define the uniform pseudometric d\u2126 : Va \u00d7 Va \u2192 [0,\u221e) on Va as\nd\u2126(v, w) . = sup x\u2208\u2126 \u2223\u2223v(x)\u2212 w(x)\u2223\u2223 for v, w \u2208 Va. Theorem 5 For the value function sequence {vi}\u221ei=0 generated by Algorithm 1a under Assumptions 1, 2, and 3a,\n(C1) there exists a metric d : Va \u00d7 Va \u2192 [0,\u221e) such that T is a contraction (and thus continuous) under d and vi \u2192 v\u2217 in the metric d, i.e., limi\u2192\u221e d(vi, v\u2217) = 0;\n(C2) if v\u0302\u2217 \u2208 Va and for every compact subset \u2126 \u2282 X , T is continuous under d\u2126, then vi \u2192 v\u2217 pointwisely on X and uniformly on any compact subset of X .\nProof. By Lemma 2 and Bessaga (1959)\u2019s converse of the Banach\u2019s fixed point principle, there exists a metric d on Va such that (Va, d) is a complete metric space and T is a contraction (and thus continuous) under d. Moreover, by Lemma 2 and Banach\u2019s fixed point principle (e.g., Kirk and Sims, 2013, Theorem 2.2),\n\u2200v0 \u2208 Va : lim N\u2192\u221e vN = lim N\u2192\u221e TNv0 = v\u2217 in the metric d.\nTo prove the second part, suppose that v\u0302\u2217 \u2208 Va and that T is continuous under d\u2126 for every compact subset \u2126 \u2282 X . Then, since v\u0302\u2217 \u2208 Va is C1 by assumption and thereby, continuous, vi converges to v\u0302\u2217 pointwisely on X and uniformly on every compact \u2126 \u2282 X by Theorem 4; the latter implies vi \u2192 v\u0302\u2217 in d\u2126. Therefore, in the uniform pseudometric d\u2126,\nv\u0302\u2217 = lim i\u2192\u221e vi+1 = lim i\u2192\u221e\nTvi = T (\nlim i\u2192\u221e vi\n) = T v\u0302\u2217\nby continuity of T under d\u2126. That is, v\u0302\u2217|\u2126 = (T v\u0302\u2217)|\u2126 for every compact \u2126 \u2282 X . This implies v\u0302\u2217 = T v\u0302\u2217 and thus, v\u0302\u2217 = v\u2217 by Lemma 2, which completes the proof. 2\n3.2 Partially Model-Free Nature\nPolicy evaluation (21) can be done without using the explicit knowledge of the system dynamics f(x, u) in (1)\u2014there is no explicit term of f shown in (21), and all of the necessary information on f are captured by the observable state trajectory X\u00b7 during a finite time interval [t, t\u2032]. Hence, IPI is model-free as long as so is its policy improvement (22), which is not unfortunately (see the definition (5) of h). Nevertheless, the policy improvement (22) can be modified to yield the partially model-free IPI. To see this, consider the decomposition (25) of the dynamics f below:\nf(x, u) = fd(x) + fc(x, u), (25)\nwhere fd : X \u2192 X is independent of u and called a drift dynamics, and fc : X \u00d7U \u2192 X is the corresponding inputcoupling dynamics. 9 Substituting the definitions (5) and (12) into (20), choosing \u03ba = 1 and b(x) = \u2212\u2207v\u03c0(x)fd(x), and replacing (\u03c0, v\u03c0) with (\u03c0i, vi) then yield\n\u03c0i+1(x) \u2208 arg max u\u2208U\n[ R(x, u) +\u2207vi(x)fc(x, u) ] , (26)\na partially model-free version of (22). In summary, the whole procedure of Algorithm 1a can be done even when the drift dynamics fd is completely unknown.\n9 There are an infinite number of ways of choosing fd and fc; one typical choice is fd(x) = f(x, 0) and fc(x, u) = f(x, u)\u2212fd(x).\n3.3 Case Studies\nThe partially model-free policy improvement (26) can be even more simplified if:\n(1) the system dynamics f(x, u) is affine in u, i.e.,\nf(x, u) = fd(x) + Fc(x)u, (27)\nwhere Fc : X \u2192 Rn\u00d7m is a continuous matrix-valued function; it is (25) with fc(x, u) = Fc(x)u; (2) the action space U \u2286 Rm is convex; (28) (3) the reward R(x, u) is strictly concave in u and given\nby R(x, u) = R0(x)\u2212 S(u), (29)\nwhere R0 : X \u2192 R is a continuous upper-bounded function called the state reward, and S : U \u2192 R, named the action penalty, is a strictly convex C1 function whose restriction S|Uint on the interior Uint of U satisfies Im(\u2207S|TUint) = R m.\nIn this case, solving the maximization in (26) (or (22)) is equivalent to finding the regular point u \u2208 U such that\n\u2212\u2207S(u) +\u2207vi(x)Fc(x) = 0,\nwhere the gradient \u2207S of S is a strictly monotone mapping that is also bijective when its domain U is restricted to its interior Uint. Rearranging it with respect to u, we obtain the explicit closed-form expression of (26) also known as the VGB greedy policy (Doya, 2000):\n\u03c0i+1(x) = \u03c3 ( FTc (x)\u2207vTi (x) ) , (30)\nwhere \u03c3 : Rm \u2192 Uint is defined as \u03c3 . = (\u2207S|TUint) \u22121, which is also strictly monotone, bijective, and continuous. Therefore, in the u-affine-and-concave (u-AC) case (27)\u2013(29), 10 the complicated maximization process in (26) over the continuous action space can be obviated by directly calculating the next policy \u03c0i+1 by (30). Also note that under the u-AC setting (27)\u2013(29), the VGB greedy policy \u03c0\u2032 = \u03c3\u25e6 ( FTc \u2207vT\u03c0\n) for an admissible \u03c0 is the unique policy satisfying (18).\nRemark 4 Whenever each j-th componentU\u03c4,j ofU\u03c4 meets some physical limitation |U\u03c4,j | \u2264 Umax,j for some threshold Umax,j \u2208 (0,\u221e], one can formulate the action space U in (28) as U = { U\u03c4 \u2208 Rm : |U\u03c4,j | \u2264 Umax,j , 1 \u2264 j \u2264 m\n} and determine the action penalty S(u) in (29) as\nS(u) = lim v\u2192u \u222b v 0 (sT)\u22121(w) \u00b7 \u0393 dw (31)\nfor a positive definite matrix \u0393 \u2208 Rm\u00d7m and a continuous function s : Rm \u2192 Uint such that\n10 This includes the frameworks in (Doya, 2000; Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009) as special cases.\n(1) s is strictly monotone, odd, and bijective; (2) S in (31) is finite at any point on the boundary \u2202U . 11\nThis gives the closed-form expression \u03c3(\u03be) = s(\u0393\u22121\u03be) of the function \u03c3 in (30) and includes the sigmoidal example in Section 5 as its special case. 12 Another well-known example is:\nU = Rm (i.e., Umax,j =\u221e, 1 \u2264 j \u2264 m) and s(u) = u/2, (32) in which case (31) becomes S(u) = uT\u0393u.\nThe well-known special case of (27)\u2013(29) is the following linear quadratic regulation (LQR): (31), (32) and\nfd(x) = Ax, Fc(x) = B, R0(x) = \u2212\u2016Cx\u20162, (33)\nwhere (A,B,C) forA \u2208 Rn\u00d7n,B \u2208 Rn\u00d7m, andC \u2208 Rp\u00d7n is stabilizable and detectable. In this LQR case, if the policy \u03c0i is linear, i.e., \u03c0i(x) = Kix (Ki \u2208 Rm\u00d7n), then its value function v\u03c0i , if finite, can be represented in a quadratic form v\u03c0i(x) = x\nTP\u03c0ix (P\u03c0i \u2208 Rn\u00d7n). Moreover, when vi is quadratically represented as vi(x) = xTPix, (30) becomes\n\u03c0i+1(x) = Ki+1x and Ki+1 = \u0393\u22121BTPi, (34)\na linear policy again. This observation gives the policy evaluation and improvement in Algorithm 1b below. Moreover, whenever the given policy \u03c0 is linear and (31)\u2013(33) are all true, the process Z\u03c4 generated by\nZ\u0307\u03c4 = ( A+ ln \u03b32 I ) Z\u03c4 +BU\u03c4 (35)\nyields the following value function expression in terms of Z\u03c4 without the discount factor \u03b3 as\nv\u03c0(x) = E\u0303\u03c0 [ \u222b \u221e\nt\nR(Z\u03c4 , U\u03c4 ) d\u03c4 \u2223\u2223\u2223\u2223Zt = x], 11 \u2202U = {U\u03c4 \u2208 Rm : U\u03c4,j = Umax,j , 1 \u2264 j \u2264 m}. 12 See also (Doya, 2000; Abu-Khalaf and Lewis, 2005).\nAlgorithm 1b: On-policy IPI for the LQR Case (31)\u2013(33)\n1 Initialize: { \u03c00(x) = K0x, the init. admissible policy; \u2206t > 0, the time difference;\n2 i\u2190 0; 3 repeat (under the LQR setting (31), (32), and (33)) 4 Policy Evaluation: given policy \u03c0i(x) = Kix, find the\nsolution vi(x) = xTPix to the Bellman equation (21);\n5 Policy Improvement: Ki+1 = \u0393\u22121BTPi;\n6 i\u2190 i+ 1; until convergence is met.\nwhere E\u0303\u03c0[Y |Zt = x] denotes the value of Y when Zt = x and U\u03c4 = \u03c0(Z\u03c4 ) \u2200\u03c4 \u2265 t. This transforms any discounted LQR problem into the total one (with its state Z\u03c4 in place of X\u03c4 ). Hence, the application of the standard LQR theory (Anderson and Moore, 1989) shows that\n(1) { v\u2217(x) = x\nTP\u2217x \u2264 0 for some P\u2217 \u2208 Rn\u00d7n, \u03c0\u2217(x) = K\u2217x with K\u2217 . = \u0393\u22121BTP\u2217;\n(2) v\u2217 is the unique solution to the HJBE (17).\nFurthermore, the application of the analytical result of IPI (Lee et al., 2014, Theorem 5 and Remark 4 with } \u2192 \u221e) gives the following statements.\nLemma 3 (Policy Improvement Theorem: the LQR Case) Let \u03c0 be linear and admissible. Then, the linear policy \u03c0\u2032 given by \u03c0\u2032(x) = K \u2032x and K \u2032 = \u0393\u22121BTP\u03c0 under the LQR setting (31)\u2013(33) is admissible and satisfies P\u03c0 \u2264 P\u03c0\u2032 \u2264 0.\nTheorem 6 Let {\u03c0i}\u221ei=0 and {vi}\u221ei=0 be the sequences generated by Algorithm 1b and parameterized as \u03c0i(x) = Kix and vi(x) = xTPix. Then,\n(1) \u03c0i is admissible and Pi = P\u03c0i for all i \u2208 Z+; (2) P0 \u2264 P1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 Pi \u2264 Pi+1 \u2264 \u00b7 \u00b7 \u00b7 \u2264 P\u2217 \u2264 0; (3) limi\u2192\u221e Pi = P\u2217 and limi\u2192\u221eKi = K\u2217; (4) the convergence Pi \u2192 P\u2217 is quadratic.\nRemark 5 In the LQR case, Assumptions 1, 2, and 3a are all true\u2014Assumptions 1 and 2 are trivially satisfied as shown above; Assumption 3a is also true by\nlim k\u2192\u221e l\u03c0i(x, k; vi) = lim k\u2192\u221e\nEx\u03c0i [ \u03b3k\u2206t \u00b7XTt+k\u2206tPiXt+k\u2206t ] = lim k\u2192\u221e E\u0303x\u03c0i [ ZTt+k\u2206tPiZt+k\u2206t ] = 0,\nwhere we have used the equality E\u0303x\u03c0[Z\u03c4 ] = Ex\u03c0[(\u03b3/2)\u03c4\u2212tX\u03c4 ] and the fact that a linear policy \u03c0 is admissible iif it stabilizes the Z\u03c4 -system (35) and thus satisfies lim\u03c4\u2192\u221e E\u0303x\u03c0[Z\u03c4 ] = 0 \u2200x \u2208 X (Lee et al., 2014, Section 2). To the best authors\u2019 knowledge, however, the corresponding theory does not exist for the nonlinear discounted case \u2018\u03b3 \u2208 (0, 1).\u2019 See (AbuKhalaf and Lewis, 2005; Vrabie and Lewis, 2009; Lee et al., 2015) for the u-AC case (27)\u2013(29) with \u03b3 = 1 and R \u2264 0."}, {"heading": "4 Extensions to Off-policy IPI Methods", "text": "In this section, we propose a series of completely/partially model-free off-policy IPI methods, which are effectively same to on-policy IPI but uses data generated by a behavior policy \u00b5, rather than the target policy \u03c0i. For this, we first introduce the concept of action-dependent (AD) policy.\nDefinition 2 For a non-empty subset U0 \u2286 U of the action space U , a function \u00b5 : [t,\u221e)\u00d7 X \u00d7 U0 \u2192 U , denoted by \u00b5(\u03c4, x, u) for \u03c4 \u2265 t and (x, u) \u2208 X \u00d7 U0, is said to be an AD policy over U0 (starting at time t) if:\n(1) \u00b5(t, x, u) = u for all x \u2208 X and all u \u2208 U0; (2) for each fixed u \u2208 U0, \u00b5(\u00b7, \u00b7, u) is a policy (starting at\ntime t), which is possibly non-stationary.\nAn AD policy is actually a policy parameterized by u \u2208 U0; the purpose of such a parameterization is to impose the condition Ut = u at the initial time t through the first property in Definition 2 so as to make it possible to explore the stateaction space X \u00d7 U (or its subset X \u00d7 U0), rather than the state space X alone. The simplest form of an AD policy \u00b5 is\n\u00b5(\u03c4, x, u) = u for all \u03c4 \u2265 t and all (x, u) \u2208 X \u00d7 U0\nused in Section 5; another useful important example is\n\u00b5(\u03c4, x, u) = \u03c0(x) + e(\u03c4, x, u) (with U = Rm),\nfor a policy \u03c0 and a probing signal e(\u03c4, x, u) given by e(\u03c4, x, u) = (u\u2212 \u03c0(x))e\u2212\u03c3(\u03c4\u2212t) + N\u2211 j=1 Aj sin ( \u03c9j(\u03c4 \u2212 t) ) ,\nwhere \u03c3 > 0 regulates the vanishing rate of the first term; Aj \u2208 Rm and \u03c9j \u2208 R are the amplitude and the angular frequency of the j-th sin term in the summation, respectively.\nIn what follows, for an AD policy \u00b5 starting at time t \u2265 0, we will interchangeably use\nE\u00b5[Z|Xt = x, Ut = u] and E(x,u)\u00b5 [Z|t], (36)\nwith a slight abuse of notations, to indicate the deterministic value Z when Xt = x and U\u03c4 = \u00b5(\u03c4,X\u03c4 , u) for all \u03c4 \u2265 t. Using this notation, the state vector X\u03c4 at time \u03c4 \u2265 t that starts at time t and is generated under \u00b5 and the initial condition \u201cXt = x and Ut = u\u201d is denoted by E(x,u)\u00b5 [X\u03c4 |t]. For any non-AD policy \u00b5, we also denote E\u00b5[Z|Xt = x] and Ex\u00b5[Z|t], instead of (36), to indicate the value Z when Xt = x and U\u03c4 = \u00b5(\u03c4,X\u03c4 ) \u2200\u03c4 \u2265 t, where the condition Ut = u is obviated.\nEach off-policy IPI method in this paper is designed to generate the policies and value functions satisfying the same properties in Theorems 3 and 4 as those in on-policy IPI (Algorithm 1a), but they are generated using the off-policy trajectories generated by a (AD) behavior policy \u00b5, rather than the target policy \u03c0i. Specifically, each off-policy method estimates vi and/or a (AD) function in policy evaluation using the off-policy state and action trajectories and then employs the estimated function in policy improvement to find a next improved policy. Each off-policy IPI method will be described only with its policy evaluation and improvement steps since the others are all same to those in Algorithm 1a.\n4.1 Integral Advantage Policy Iteration (IAPI)\nFirst, we consider a continuous function a\u03c0 : X \u00d7 U \u2192 R called the advantage function for an admissible policy \u03c0\n(Baird III, 1993; Doya, 2000), which is defined as\na\u03c0(x, u) . = h\u03c0(x, u) + ln \u03b3 \u00b7 v\u03c0(x) (37)\nand satisfies a\u03c0(x, \u03c0(x)) = 0 by (11). Considering (20) in Remark 3 with b\u03c0(x) = ln \u03b3 \u00b7 v\u03c0(x) and \u03ba = 1, we can see that the maximization process (18) can be replaced by\n\u03c0\u2032(x) \u2208 arg max u\u2208U a\u03c0(x, u) \u2200x \u2208 X ; (38)\nthe optimal advantage function a\u2217 . = a\u03c0\u2217 also characterizes the HJBE (17) and the optimal policy \u03c0\u2217 as\nmax u\u2208U a\u2217(x, u) = 0 and \u03c0\u2217(x) \u2208 arg max u\u2208U a\u2217(x, u).\nThese ideas give model-free off-policy IPI named integral advantage policy iteration (IAPI), whose policy evaluation and improvement steps are shown in Algorithm 2 while the other parts are, as mentioned above, all same to those in Algorithm 1a and thus omitted. Given \u03c0i, the agent tries to find/estimate in policy evaluation both vi(x) and ai(x, u) satisfying (40) and the off-policy Bellman equation (39) for \u201can AD policy \u00b5 over the entire action space U .\u201d Here, vi and ai corresponds to the value and the advantage functions with respect to the i-th admissible policy \u03c0i (see Theorem 7 in Section 4.5). Then, the next policy \u03c0i+1 is updated in policy improvement by using the advantage function ai(x, u) only. Notice that this IAPI provides the ideal PI form of advantage updating and the associated ideal Bellman equation\u2014see (Baird III, 1993) for advantage updating and the approximate version of the Bellman equation (39).\nAlgorithm 2: Integral Advantage Policy Iteration (IAPI)\nPolicy Evaluation: given \u03c0i and an AD policy \u00b5 over U , find {\na C1 function vi : X \u2192 R a continuous function ai : X \u00d7 U \u2192 R\n} such that\n(1) for all (x, u) \u2208 X \u00d7 U , vi(x) = E(x,u)\u00b5 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212t Z\u03c4 d\u03c4 + \u03b3 \u2206t vi(X \u2032 t) \u2223\u2223\u2223\u2223 t], (39) where Z\u03c4 = R\u03c4 \u2212 ai(X\u03c4 , U\u03c4 ) + ai(X\u03c4 , \u03c0i(X\u03c4 ));\n(2) ai(x, \u03c0i(x)) = 0 for all x \u2208 X ; (40)\nPolicy Improvement: find a policy \u03c0i+1 such that \u03c0i+1(x) \u2208 arg max\nu\u2208U ai(x, u) \u2200x \u2208 X ; (41)\n4.2 Integral Q-Policy-Iteration (IQPI)\nOur next model-free off-policy IPI named integal Q-policyiteration (IQPI) estimates and uses a general Q-function q\u03c0 :\nAlgorithm 3a: Integral Q-Policy-Iteration (IQPI)\nPolicy Evaluation: given the current policy \u03c0ian weighting factor \u03b2 > 0an AD policy \u00b5 over U ,\nfind a continuous function qi : X \u00d7 U \u2192 R such that for all (x, u) \u2208 X \u00d7 U :\nqi(x, \u03c0i(x)) = E(x,u)\u00b5 [ \u222b t\u2032\nt\n\u03b2\u03c4\u2212t Z\u03c4 d\u03c4 + \u03b2 \u2206tqi(X \u2032 t, \u03c0i(X \u2032 t)) \u2223\u2223\u2223\u2223 t], (42)\nwhere { Z\u03c4 = \u03ba1R\u03c4 \u2212 \u03ba2qi(X\u03c4 , U\u03c4 ) + \u03ba3qi(X\u03c4 , \u03c0i(X\u03c4 )), \u03ba1\u03ba2 > 0 and \u03ba3 . = \u03ba2 \u2212 ln(\u03b3\u22121\u03b2);\nPolicy Improvement: find a policy \u03c0i+1 such that \u03c0i+1(x) \u2208 arg max\nu\u2208U qi(x, u) \u2200x \u2208 X ; (43)\nX \u00d7 U \u2192 R defined as\nq\u03c0(x, u) . = \u03ba1 \u00b7 ( v\u03c0(x) + a\u03c0(x, u)/\u03ba2 ) (44)\nfor an admissible policy \u03c0, where \u03ba1, \u03ba2 \u2208 R are any two nonzero real numbers that have the same sign, so that \u03ba1/\u03ba2 > 0 holds. 13 By its definition and continuities of v\u03c0 and a\u03c0 , the Q-function q\u03c0 is also continuous over its whole domain X \u00d7U . Here, since both \u03ba1 and \u03ba2 are nonzero, the Q-function (44) does not lose both information on v\u03c0 and a\u03c0; thereby, q\u03c0 plays a similar role of the DT Q-function\u2014on one hand, it holds the property\n\u03ba1 \u00b7 v\u03c0(x) = q\u03c0(x, \u03c0(x)), (45)\nand on the other, it replaces (18) and (38) with\n\u03c0\u2032(x) \u2208 arg max u\u2208U q\u03c0(x, u) \u2200x \u2208 X (46)\nby (20) for \u03ba = \u03ba1\u03ba2 > 0 and b\u03c0(x) = \u03ba1(1+ ln \u03b3 \u03ba2 )\u00b7v\u03c0(x); the HJBE (17) and the optimal policy \u03c0\u2217 are also characterized by the optimal Q-function q\u2217 . = q\u03c0\u2217 as\n\u03ba1 \u00b7 v\u2217(x) = max u\u2208U q\u2217(x, u) and \u03c0\u2217(x) \u2208 arg max u\u2208U q\u2217(x, u).\nAlgorithm 3a shows the policy evaluation and improvement of IQPI\u2014the former is derived by substituting (45) and \u03ba1 a\u03c0(x, u) = \u03ba2 \u00b7 ( q\u03c0(x, u)\u2212 q\u03c0(x, \u03c0(x)) ) (obtained from (44) and (45)) into (39) in IAPI, and the latter directly from (46). At each iteration, while IAPI needs to find/estimate both vi and ai, IQPI just estimate and use in its loop qi only. In addition, the constraint on the AD function such as (40)\n13 Our general Q-function q\u03c0 includes the previously proposed Qfunctions in CTS as special cases\u2014Baird III (1993)\u2019s Q-function (\u03ba1 = 1, \u03ba2 = 1/\u2206t); h\u03c0 for \u03b3 \u2208 (0, 1) (\u03ba1 = \u03ba2 = \u2212 ln \u03b3), and its generalization for \u03b3 \u2208 (0, 1] (any \u03ba1 = \u03ba2 > 0) both recognized as Q-functions by Mehta and Meyn (2009).\nin IAPI does not appear in IQPI, making the algorithm simpler. As will be shown in Theorem 7 in Section 4.5, qi in Algorithm 3a corresponds to the Q-function q\u03c0i for the i-th admissible policy \u03c0i, and the policies {\u03c0i}\u221ei=0 generated by IQPI satisfy the same properties to those in on-policy IPI.\nNotice that simplification of IQPI is possible by setting\n\u03ba . = ln(\u03b3\u22121\u03b2) = \u03ba1 = \u03ba2 and \u03b3 6= \u03b2, (47)\nin which case Z\u03c4 in IQPI is dramatically simplified to (49) shown in the Algorithm 3b, the simplified IQPI, and the Qfunction q\u03c0 in its definition (44) becomes\nq\u03c0(x, u) = \u03ba \u00b7 v\u03c0(x) + a\u03c0(x, u). (48)\nIn this case, the gain \u03ba (6= 0) of the integral is the scaling factor of v\u03c0 in the Q-function q\u03c0 , relative to a\u03c0 . As mentioned by Baird III (1993), a bad scaling between v\u03c0 and a\u03c0 in q\u03c0 , e.g., extremely large |\u03ba|, may result in significant performance degradation or extremely slow Q-learning.\nCompared with the other off-policy IPI methods, the use of the weighting factor \u03b2 \u2208 (0,\u221e) is one of the major distinguishing feature of IQPI\u2014\u03b2 plays a similar role to the discount factor \u03b3 \u2208 (0, 1] in the Bellman equation, but can be arbitrarily set in the algorithm; it can be equal to \u03b3 or not. In the special case (47), \u03b2 should not be equal to \u03b3 since the log ratio ln(\u03b2/\u03b3) of the two determines the nonzero scaling gain \u03ba in (48) and (49). Since Algorithm 3b is a special case of IQPI (Algorithm 3a), it also has the same mathematical properties shown in Section 4.5.\nAlgorithm 3b: IQPI with the Simplified Setting (47)\nPolicy Evaluation: given the current policy \u03c0ian weighting factor \u03b2 > 0an AD policy \u00b5 over U ,\nfind a continuous function qi : X \u00d7 U \u2192 R such that (42) holds for all (x, u) \u2208 X \u00d7 U and for Z\u03c4 given by\nZ\u03c4 = \u03ba \u00b7 ( R\u03c4 \u2212 qi(X\u03c4 , U\u03c4 ) ) ; (49)\nPolicy Improvement: find a policy \u03c0i+1 satisfying (43);\n4.3 Integral Explorized Policy Iteration (IEPI)\nThe on-policy IPI (Algorithm 1a) can be easily generalized and extended to its off-policy version without introducing any AD function such as ai in IAPI and qi in IQPI. In this paper, we name it integral explorized policy iteration (IEPI) following the perspectives of Lee et al. (2012) and present its policy evaluation and improvement loop in Algorithm 4a. Similarly to on-policy IPI with its policy improvement (22) replaced by (26), IEPI is also partially modelfree\u2014the input-coupling dynamics fc has to be used in both policy evaluation and improvement while the drift term fd is not when the system dynamics f is decomposed to (25).\nAlgorithm 4a: IEPI for the General Case (1)\u2013(2) Policy Evaluation: given {\nthe current policy \u03c0i a (non-stationary) policy \u00b5\n} ,\nfind a C1 function vi : X \u2192 R such that for all x \u2208 X , vi(x) = Ex\u00b5 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212t Z\u03c4 d\u03c4 + \u03b3 \u2206t vi(X \u2032 t) \u2223\u2223\u2223\u2223Xt = x],(50) where { Z\u03c4 = R \u03c0i \u03c4 \u2212\u2207vi(X\u03c4 ) ( fc(\u03c4)\u2212 f\u03c0ic (\u03c4) ) ,\nfc(\u03c4) . = fc(X\u03c4 , U\u03c4 ), f \u03c0i c (\u03c4) . = fc(X\u03c4 , \u03c0i(X\u03c4 ));\nPolicy Improvement: find a policy \u03c0i+1 satisfying (26);\nNote that the difference of IEPI from on-policy IPI lies in its Bellman equation (50)\u2014it contains the compensating term \u201c\u2207vi(X\u03c4 )(fc(\u03c4) \u2212 f\u03c0ic (\u03c4))\u201d that naturally emerges due to the difference between the behavior policy \u00b5 and the target one \u03c0i. For \u00b5 = \u03c0i, the compensating term becomes identically zero, in which case the Bellman equation (50) becomes (21) in on-policy IPI. For any given policy \u00b5, IEPI in fact generates the same result {(vi, \u03c0i)}\u221ei=0 to its on-policy version (Algorithm 1a) under the same initial condition as shown in Theorem 7 (and Remark 7) in Section 4.6.\nIn what follows, we are particularly interested in IEPI under the u-AC setting (27)\u2013(29) shown in Algorithm 4b. In this case, the maximization process in the policy improvement is simplified to the update rule (30) also known as the VGB greedy policy (Doya, 2000). On the other hand, the compensation term in Z\u03c4 of the Bellman equation (50) is also simplified to \u201c\u2207vi(X\u03c4 )Fc(X\u03c4 )\u03be\u03c0i\u03c4 ,\u201d which is linear in the difference \u03be\u03c0i\u03c4 . = U\u03c4 \u2212 \u03c0i(X\u03c4 ) at time \u03c4 and contains the function \u2207vi \u00b7 Fc also shown in its policy improvement rule (30) in common. This observation brings our next offpolicy IPI method named integral C-policy-iteration (ICPI).\nAlgorithm 4b: IEPI in the u-AC Setting (27)\u2013(29) Policy Evaluation: given {\nthe current policy \u03c0i a (non-stationary) policy \u00b5\n} ,\nfind a C1 function vi : X \u2192 R such that (50) holds for all x \u2208 X and for Z\u03c4 given by\nZ\u03c4 = R \u03c0i \u03c4 \u2212\u2207vi(X\u03c4 )Fc(X\u03c4 ) \u03be\u03c0i\u03c4 ,\nwhere \u03be\u03c0i\u03c4 . = U\u03c4 \u2212 \u03c0i(X\u03c4 ) is the policy difference;\nPolicy Improvement: update the next policy \u03c0i+1 by (30);\n4.4 Integral C-Policy-Iteration (ICPI)\nIn the u-AC setting (27)\u2013(29), we now modify IEPI (Algorithm 4b) to make it model-free by employing a function c\u03c0 : X \u2192 Rm defined for a given admissible policy \u03c0 as\nc\u03c0(x) . = FTc (x)\u2207vT\u03c0 (x), (51)\nwhich is continuous by continuity of Fc and \u2207v\u03c0 . Here, the function c\u03c0 will appear in both policy evaluation and improvement in Common and contains the input-Coupling term Fc, so we call it C-function for an admissible policy \u03c0. Indeed, when (27)\u2013(29) are true, the next policy \u03c0\u2032 satisfying (18) for an admissible policy \u03c0 is explicitly given by\n\u03c0\u2032(x) = (\u03c3 \u25e6 c\u03c0)(x) = \u03c3(c\u03c0(x)).\nIn the same way, if ci(x) = FTc (x)\u2207vTi (x) is true, then (30) in Algorithm 4b can be replaced by (53), and the compensating term\u2207vi(X\u03c4 )Fc(X\u03c4 )\u03be\u03c0i\u03c4 in policy evaluation of IEPI (Algorithm 4b) by cTi (X\u03c4 )\u03be \u03c0i \u03c4 .\nMotivated by the above idea, we propose integral C-policyiteration (ICPI) whose policy evaluation and improvement are shown in Algorithm 5. In the former, the functions v\u03c0i and c\u03c0i for the given (admissible) policy \u03c0i are estimated by solving the associated off-policy Bellman equation for vi and ci, and then the next policy \u03c0i+1 is updated using ci in the latter. In fact, ICPI is a model-free extension of IEPI\u2014while ICPI does not, IEPI obviously needs the knowledge of the input-coupling dynamics Fc to run. A model-free off-policy IPI so-named integral Q-learning 14 by Lee et al. (2012, 2015), which was derived from IEPI under the Lyapunov\u2019s stability framework, also falls into a class of ICPI for the unconstrained total case (U = Rm and \u03b3 = 1).\nCompared with IAPI and IQPI, the advantages of ICPI (at the cost of restricting the RL problem to the u-AC one (27)\u2013 (29)) are as follows.\n(1) As in IEPI, the complicated maximization in the policy improvement of IAPI and IQPI has been replaced by the simple update rule (53), which is a kind of modelfree VGB greedy policy (Doya, 2000).\n14 The name \u2018integral Q-learning\u2019 does not imply that it is involved with our Q-function (44). Instead, its derivation was based on the value function with singularly-perturbed actions (Lee et al., 2012).\nAlgorithm 5: Integral C-Policy-Iteration (ICPI)\n(under the u-AC setting (27)\u2013(29)) Policy Evaluation: given \u03c0i and an AD policy \u00b5 over a\nfinite subset U0 = {u0, u1, \u00b7 \u00b7 \u00b7 , um} of U satisfying (54), find {\na C1 function vi : X \u2192 R a continuous function ci : X \u2192 Rm\n} such that\n(39) holds for each (x, u) \u2208 X \u00d7 U0 and for Z\u03c4 given by\nZ\u03c4 = R \u03c0i \u03c4 \u2212 cTi (x) \u03be\u03c0i\u03c4 (52)\nwhere \u03be\u03c0i\u03c4 . = U\u03c4 \u2212 \u03c0i(X\u03c4 ) is the policy difference;\nPolicy Improvement: update the next policy \u03c0i+1 by \u03c0i+1(x) = \u03c3(ci(x)). (53)\n(2) By virtue of the fact that there is no AD function to be estimated in ICPI as in IEPI, the exploration over its smaller space X \u00d7 {uj}mj=0, rather than the entire state-action space X \u00d7 U , is enough to obtain the desired result \u201cvi = v\u03c0i and ci = c\u03c0i\u201d in its policy evaluation (see Algorithm 5 and Theorem 7 in the next subsection). Here, uj\u2019s are any vectors in U such that\nspan{uj \u2212 uj\u22121}mj=1 = Rm. 15 (54)\nRemark 6 One might consider a general version of ICPI by replacing the term \u2207vi(x)fc(x, u) in the general IEPI (Algorithm 4a) with an AD function, say c0i (x, u). In this case, however, it loses the merits of ICPI over IAPI and IQPI shown above. Furthermore, the solution (vi, c0i ) of the associated Bellman equation is not uniquely determined\u2014a pair of vi and any cbi (x, u) . = c0i (x, u) + b(x) for a continuous function b(x) is also a solution to (39) for Z\u03c4 given by\nZ\u03c4 = R \u03c0i \u03c4 \u2212 cbi (X\u03c4 , U\u03c4 ) + cbi (X\u03c4 , \u03c0i(X\u03c4 )).\n4.5 Mathematical Properties of Off-policy IPI Methods\nNow, we show that every off-policy IPI method is effectively same to on-policy IPI in a sense that the sequences {vi}\u221ei=0 and {\u03c0i}\u221ei=0 generated satisfy Theorems 3 and 4 under the same assumptions and are equal to those in on-policy IPI under the uniqueness of the next policy \u03c0\u2032 in Assumption 1. In the case of IQPI, we let vi . = qi(\u00b7, \u03c0(\u00b7))/\u03ba1 and assume\nAssumption 3b For each i \u2208 Z+, if \u03c0i is admissible, then vi . = qi(\u00b7, \u03c0i(\u00b7))/\u03ba1 is C1 and\nlim k\u2192\u221e\nl\u03c0i ( x, k; vi ) = 0 for all x \u2208 X .\nTheorem 7 Under Assumptions 1 and 3a (or 3b in IQPI), the sequences {\u03c0i}\u221ei=0 and {vi}\u221ei=0 generated by any offpolicy IPI method (IAPI, IQPI, IEPI, or ICPI) satisfy the properties (P1)\u2013(P3) in Theorem 3. Moreover,\nai = a\u03c0i (IAPI), qi = q\u03c0i (IQPI), and ci = c\u03c0i (ICPI)\nfor all i \u2208 Z+; if Assumption 2 also holds, then vi converges towards the optimal solution v\u2217 in a sense that {vi}\u221ei=0 satisfies the convergence properties (C1)\u2013(C2) in Theorem 5.\nProof. See Appendix B. 2\nRemark 7 If the policy \u03c0\u2032 satisfying (18) in Assumption 1 is unique for each admissible \u03c0, then Theorem 7 also shows that {\u03c0i}\u221ei=0 and {vi}\u221ei=0 generated by any off-policy IPI in this paper are even equivalent to those in on-policy IPI under the same initial \u03c00. An example of this is the u-AC case (27)\u2013 (29), where the next policy \u03c0i+1 is always uniquely given by the VGB greedy policy (30) for given vi \u2208 Va.\n15 When U contains the zero vector, any linearly independent subset {uj}mj=1 and u0 = 0 is an example of such uj\u2019s in (54).\nTable 1 Summary of the off-policy IPI methods.\nName Model-free R\u03c4 or R\u03c0\u03c4 Functions involved Search Space Algorithm No. Constraint(s)\nIAPI O R\u03c4 v\u03c0 and a\u03c0 X \u00d7 U 2 (40)\nIQPI O R\u03c4 q\u03c0 X \u00d7 U 3a 3b\nX (47)\nIEPI 4 R\u03c0\u03c4 v\u03c0 X 4a 4b\nX (27)\u2013(29)\nICPI O R\u03c0\u03c4 v\u03c0 and c\u03c0 X \u00d7 {uj} 5 (27)\u2013(29)\n4.6 Summary and Discussions\nThe off-policy IPI methods presented in this section are compared and summarized in Table 1. As shown in Table 1, all of the off-policy IPI methods are model-free except IEPI which needs the full-knowledge of a input-coupling dynamics fc in (25) to run; here, ICPI is actually a model-free version of the u-AC IEPI (Algorithm 4b). While IAPI and IQPI explore the whole state-action space X \u00d7 U to learn their respective functions (v\u03c0, a\u03c0) and q\u03c0 , IEPI and ICPI search only the significantly smaller spaces X and X \u00d7 {uj}mj=0, respectively. This is due to the fact that IEPI and ICPI both learn no AD function such as a\u03c0 and q\u03c0 as shown in the fourth column of Table 1. While IAPI and IQPI employ the reward R\u03c4 , both IEPI and ICPI use the \u03c0i-reward R\u03c0i\u03c4 at each i-th iteration.\nTable 1 also summarizes the constraint(s) on each algorithm. IAPI has the constraint (40) on ai and \u03c0i in the policy evaluation that reflects the equality a\u03c0i(x, \u03c0i(x)) = 0 similarly to advantage updating (Baird III, 1993; Doya, 2000). ICPI is designed under the u-AC setting (27)\u2013(29), which gives:\n(1) the uniqueness of the target solution (vi, ci) = (v\u03c0i , c\u03c0i) of the Bellman equation (39) for Z\u03c4 given by (52);\n(2) the exploration of a smaller space X \u00d7 {uj}mj=0, rather than the whole state-action space X \u00d7 U ;\n(3) the simple update rule (53) in policy improvement, the model-free version of the VGB greedy policy (Doya, 2000), in place of the complicated maximization over U for each x \u2208 X such as (41) and (43) in IAPI and IQPI.\nThe special IEPI scheme (Algorithm 4b) designed under the u-AC setting (27)\u2013(29) also updates the next policy \u03c0i+1 via the simple policy improvement update rule (30) (a.k.a. the VGB greedy policy (Doya, 2000)), rather than performing the maximization (26). IQPI can be also simplified to Algorithm 3b under the different weighting (or discounting) by \u03b2 ( 6= \u03b3) and the gain setting \u03ba1 = \u03ba2 = \u03ba (\u03ba . = ln(\u03b2/\u03b3)) shown in (47). In this case, \u03b2 \u2208 (0,\u221e) determines the gain of the integral in policy evaluation and scales v\u03c0 with respect to a\u03c0 in q\u03c0 (see (48) and (49)).\nFor any of the model-free methods, if U\u03c4 = \u03c0i(X\u03c4 ), rather than U\u03c4 = \u00b5(\u03c4,X\u03c4 , u), then their AD parts summarized in\nTable 2 and shown in their off-policy Bellman equations (or their Z\u03c4 \u2019s) become all zeros and thus no longer detectable\u2014 the need for the behavior policy \u00b5 different from the target policy \u03c0i to obtain or estimate the respective functions in such AD terms. In the case of IEPI, if U\u03c4 = \u03c0i(X\u03c4 ) for all \u03c4 \u2208 [t, t\u2032], then it becomes equal to \u201con-policy IPI with its policy improvement (22) replaced by (26).\u201d"}, {"heading": "5 Inverted-Pendulum Simulation Examples", "text": "To support the theory and verify the performance, we present the simulation results of the IPI methods applied to the 2ndorder inverted-pendulum model (n = 2 and m = 1):\n\u03b8\u0308\u03c4 = \u22120.01\u03b8\u0307\u03c4 + 9.8 sin \u03b8\u03c4 \u2212 U\u03c4 cos \u03b8\u03c4 ,\nwhere \u03b8\u03c4 ,U\u03c4 \u2208 R are the angular position of and the external torque input to the pendulum at time \u03c4 , respectively, with the torque limit given by |U\u03c4 | \u2264 Umax for Umax = 5 [N\u00b7m]. Note that this model is exactly same to that used by Doya (2000) except that the action U\u03c4 , the torque input, is coupled with the term \u2018cos \u03b8\u03c4 \u2019 rather than the constant \u20181,\u2019 which makes our problem more realistic and challenging. Letting X\u03c4 . = [ \u03b8\u03c4 \u03b8\u0307\u03c4 ]\nT, then the inverted-pendulum model can be expressed as (1) and (27) with\nfd(x) =\n[ x2\n9.8 sinx1 \u2212 0.01x2\n] and Fc(x) = [ 0\n\u2212 cosx1\n] ,\nwhere x = [x1 x2 ]T \u2208 R2. Here, our learning objective is to make the pendulum swing up and eventually settle down at the upright position \u03b8\u03c4 = 2\u03c0k for some k \u2208 Z. The reward R to achieve such a goal under the limited torque was therefore set to (29) and (31) with U = [\u2212Umax, Umax], \u0393 = 1, and the functions R0(x) and s(\u03be) given by\nR0(x) = 10 2 cosx1 and s(\u03be) = Umax tanh(\u03be/Umax);\nthe sigmoid function s with \u0393 = 1 then gives the following expressions of the functions \u03c3(\u03be) in (30) and S(u) in (29):\n\u03c3(\u03be) = Umax tanh(\u03be/Umax), S(u) = (U2max/2) \u00b7 ln ( u u+ + \u00b7 u u\u2212 \u2212 ) ,\nwhere u\u00b1 . = 1\u00b1 u/Umax. Here, note that S(u) is finite for all u \u2208 U and has its maximum at the end points u = \u00b1Umax as S(\u00b1Umax) = (U2max ln 4)/2 \u2248 17.3287. The initial policy \u03c00 was given by \u03c00 = 0 and for its admissibility v\u03c00 \u2208 Va, we set the discount factor as \u03b3 = 0.1, less than 1. This is a high gain on the state-reward R0(x) (= 102 cosx1) and low discounting (\u03b3 = 0.1) scheme, which made it possible to achieve the learning objective merely after the first iteration.\nUnder the above u-AC framework, we simulated the four off-policy methods (Algorithms 2, 3b, 4b, and 5) with their parameters \u2206t = 10 [ms] and \u03b2 = 1. On-policy IPI in Section 3 is a special case \u00b5 = \u03c0 of IEPI and thus omitted. The behavior policy \u00b5 used in the simulations was \u00b5 = 0 for IEPI and \u00b5(t, x, u) = u for the others; the next target policy \u03c0i+1 was given by \u03c0i+1(x) = \u03c3(yi(x)), where yi(x) = F T c (x)\u2207vTi (x) in IEPI, yi(x) = ci(x) in ICPI; in IAPI and IQPI, yi(x) is approximately equal to the output of a radial basis function (RBF) networks (RBFNs) to be trained by policy improvement using ai and qi, respectively. The functions vi, ai, qi, and ci were all approximated by RBFNs as well. Instead of the whole spaces X and X \u00d7U , we considered their compact regions \u2126x . = [\u2212\u03c0, \u03c0]\u00d7 [\u22126, 6] and \u2126x \u00d7 U in our whole simulations; since our invertedpendulum system and the value function are 2\u03c0-periodic in the angular position x1, the state value x \u2208 X was normalized to x\u0304 \u2208 [\u2212\u03c0, \u03c0]\u00d7R whenever input to the RBFNs. The details about the RBFNs and the implementation methods of the policy evaluation and improvement are shown in Appendix D. Every IPI method ran up to the 10th iteration.\nFig. 1 shows the estimated values of v\u03c0i(x) for x \u2208 \u2126x after the learning has been completed (at i = 10), where after convergence, v\u03c0i may be considered to be an approximation of the optimal value function v\u2217. Although there are some small ripples in the case of IQPI, the final value function\nestimates shown in Fig. 1 that are generated by different IPI methods (IEPI, ICPI, IAPI, and IQPI) are all consistent to each other. We also generated the state trajectories X\u00b7 shown in Fig. 2 for the initial condition \u03b80 = (1 + 0)\u03c0 with 0 = 0.1 and \u03b8\u03070 = 0 under the estimated policy \u03c0\u0302i of \u03c0i finally obtained at the last iteration (i = 10) of each IPI method. As shown in Fig. 2, all of the policies \u03c0\u030210 obtained by different IPI methods generate the state trajectories that are almost consistent with each other\u2014they all achieved the learning objective at around t = 4 [s], and the whole state trajectories generated are almost same (or very close)\nto each other. Also note that the IPI methods achieved our learning objective without using an initial stabilizing policy that is usually required in the optimal control setting under the total discounting \u03b3 = 1 (e.g., Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009; Lee et al., 2015)."}, {"heading": "6 Conclusions", "text": "In this paper, we proposed the on-policy IPI scheme and four off-policy IPI methods (IAPI, IQPI, IEPI, and ICPI) which solve the general RL problem formulated in CTS. We proved their mathematical properties of admissibility, monotone improvement, and convergence, together with the equivalence of the on- and off-policy methods. It was shown that on-policy IPI can be made partially model-free by modifying its policy improvement, and the off-policy methods are partially model-free (IEPI), completely model-free (IAPI, IQPI), or model-free but only implementable in the u-AC setting (ICPI). The off-policy methods were discussed and compared with each other as listed in Table 1. Numerical simulations were performed with the 2nd-order invertedpendulum model to support the theory and verify the performance, and the results with all algorithms were consistent and approximately equal to each other. Unlike the IPI methods in the stability-based framework, an initial stabilizing policy is not required to run any of the proposed IPI methods. This work also provides the ideal PI forms of RL in CTS such as advantage updating (IAPI), Q-learning in CTS (IQPI), VGB greedy policy improvement (on-policy IPI and IEPI under u-AC setting), and the model-free VGB greedy policy improvement (ICPI). Though the proposed IPI methods are not online incremental RL algorithms, we believe that this work provides the theoretical background and intuition to the (online incremental) RL methods to be developed in the future and developed so far in CTS."}, {"heading": "A Proof of Lemma 2", "text": "In this proof, we first focus on the case N = 1 and then generalize the result. By Theorem 2, Tv\u2217 is an admissible value function and satisfies v\u2217 Tv\u2217, but Tv\u2217 v\u2217 since v\u2217 is the optimal value function. Therefore, Tv\u2217 = v\u2217 and v\u2217 is a fixed point of T .\nClaim A.1 v\u2217 is the unique fixed point of T .\nProof. To show the uniqueness, suppose v\u03c0 \u2208 Va is another fixed point of T and let \u03c0\u2032 be the next policy obtained by policy improvement with respect to the fixed point v\u03c0 . Then, \u03c0\u2032 is admissible by Theorem 2, and it is obvious that\n\u2212 ln \u03b3 \u00b7 ( Tv\u03c0 ) (x) = h ( x, \u03c0\u2032(x),\u2207 ( Tv\u03c0 ) (x) ) x \u2208 X ,\nby v\u03c0\u2032 = Tv\u03c0 and \u201c(11) for the admissible policy \u03c0\u2032.\u201d The substitution of Tv\u03c0 = v\u03c0 into it results in\n\u2212 ln \u03b3\u00b7v\u03c0(x) = h(x, \u03c0\u2032(x),\u2207v\u03c0(x)) = max u\u2208U h(x, u,\u2207v\u03c0(x))\nfor all x \u2208 X , the HJBE. Therefore, v\u03c0 = v\u2217 by Corollary 3 and Assumptions 1 and 2, a contradiction, implying that v\u2217 is the unique fixed point of T . 2\nNow, we generalize the result to the case with any N \u2208 N. Since v\u2217 is the fixed point of T , we have\nTNv\u2217 = T N\u22121[Tv\u2217] = T N\u22121v\u2217 = \u00b7 \u00b7 \u00b7 = Tv\u2217 = v\u2217,\nshowing that v\u2217 is also a fixed point of TN for any N \u2208 N. To prove that v\u2217 is the unique fixed point of TN for all N , suppose that there is some M \u2208 N and v \u2208 Va such that TMv = v. Then, it implies Tv = v since we have\nv Tv T 2v \u00b7 \u00b7 \u00b7 TMv = v\nby the repetitive applications of Theorem 2. Therefore, we obtain v = v\u2217 by Claim A.1, which completes the proof. 2"}, {"heading": "B Proof of Theorem 7", "text": "For the proof, we employ the following lemma regarding the conversion from an time-integral to an algebraic equation. Its proof is given in Appendix C.\nLemma B.1 Let v : X \u2192 R and Z : X\u00d7U \u2192 R be any C1 and continuous functions, respectively. If there exist \u2206t > 0, a weighting factor \u03b2 > 0, and \u201can AD policy \u00b5 over a nonempty subset U0 \u2286 U\u201d such that for each (x, u) \u2208 X \u00d7U0, v(x) = E\u00b5 [ \u222b t\u2032\nt\n\u03b2\u03c4\u2212tZ\u03c4 d\u03c4+\u03b2 \u2206tv(X \u2032t) \u2223\u2223\u2223\u2223Xt = x, Ut = u], (B.1)\nwhere Z\u03c4 . = Z(X\u03c4 , U\u03c4 ) for \u03c4 \u2265 t, then\n\u2212 ln\u03b2 \u00b7 v(x) = Z(x, u) +\u2207v(x)f(x, u)\nholds for all (x, u) \u2208 X \u00d7 U0.\nThe applications of Lemma B.1 to the Bellman equations of the off-policy IPI methods (IAPI, IQPI, IEPI, and ICPI) provides the following claim.\nClaim B.1 If \u03c0i is admissible, then vi and \u03c0i+1 obtained by the i-th policy evaluation and improvement of any off-policy IPI method satisfy vi = v\u03c0i and (23). Moreover,\nai = a\u03c0i (IAPI), qi = q\u03c0i (IQPI), and ci = c\u03c0i (ICPI).\nSuppose that \u03c0i is admissible. Then, if Claim B.1 is true, then \u03c0i+1 in any off-policy IPI method satisfies (23) and hence Theorem 2 with Assumption 1 proves that \u03c0i+1 is also admissible and satisfies v\u03c0i v\u03c0i+1 v\u03c0\u2217 . Since \u03c00 is admissible in the off-policy IPI methods, mathematical induction proves the first part of the theorem. Moreover, now that we have the properties (P1)\u2013(P3), if Assumption 2 additionally holds, then we can easily prove the convergence properties (C1)\u2013(C2) in Theorem 4 by following its proof.\nProof of Claim B.1. (IAPI/IQPI) Applying Lemma B.1 with U0 = U to (39) in IAPI and to (42) in IQPI and then substituting the definition (5) of h(x, u, p) show that (vi, ai) in IAPI and (vi, qi) in IQPI (with vi . = qi(\u00b7, \u03c0i(\u00b7))/\u03ba1) satisfy\n\u2212 ln \u03b3 \u00b7 vi(x) = h(x, u,\u2207vi(x))\u2212 ai(x, u) + ai(x, \u03c0i(x)) (B.2)\n\u2212 ln \u03b3 \u00b7 vi(x) = h(x, u,\u2207vi(x))\u2212 \u03ba2 \u03ba1\n( qi(x, u)\u2212 \u03ba1vi(x) ) (B.3)\nfor all (x, u) \u2208 X \u00d7 U , respectively. Furthermore, the substitutions of u = \u03c0i(x) into (B.2) and (B.3) yield\n\u2212 ln \u03b3 \u00b7 vi(x) = h(x, \u03c0i(x),\u2207vi(x)) \u2200x \u2208 X , (B.4)\nwhich implies vi = v\u03c0i by Theorem 1 and Assumption 3a. Next, substituting vi = v\u03c0i into (B.2) and (B.3) and then rearranging it with (12) (and (40) in the IAPI case) result in\nai(x, u) = h\u03c0i(x, u) + ln \u03b3 \u00b7 v\u03c0i(x), qi(x, u) = \u03ba1 ( v\u03c0i(x) + ai(x, u)/\u03ba2 ) ,\nand hence we obtain ai = a\u03c0i and qi = q\u03c0i by the definitions (37) and (44). By this and the respective policy improvement of IAPI and IQPI, it is obvious that the next policy \u03c0i+1 in each algorithm satisfies\n\u2200x \u2208 X : { \u03c0i+1(x) \u2208 arg maxu\u2208U a\u03c0i(x, u) (IAPI); \u03c0i+1(x) \u2208 arg maxu\u2208U q\u03c0i(x, u) (IQPI).\nSince they are equivalent to (20) with \u03c0 = \u03c0i and some special choices of b\u03c0 and \u03ba > 0, 16 and (20) is equivalent to (18), \u03c0i+1 in both IAPI and IQPI satisfy (23) in (P2).\n(IEPI) By (C.2) in Appendix C and (25), the Bellman equation (50) can be expressed as\n0 = E\u00b5 [ \u222b t\u2032\nt\n\u03b3\u03c4\u2212t \u03c6i(X\u03c4 ) d\u03c4 \u2223\u2223\u2223\u2223Xt = x], where \u03c6i : X \u2192 R is given by\n\u03c6i(x) . = R(x, \u03c0i(x)) + ln \u03b3 \u00b7 vi(x) +\u2207vi(x)f(x, \u03c0i(x)),\nwhich is obviously continuous since so are all functions contained in it. Thus, the term \u201c\u03b3\u03c4\u2212t \u03c6i(X\u03c4 )\u201d is integrable over [t, t\u2032], and Claim C.1 in Appendix C with w(x, u) = \u03c6i(x) for all (x, u) \u2208 X\u00d7U implies \u03c6i = 0, which results in (B.4) and hence vi = v\u03c0i by Theorem 1 and Assumption 3a. Since the policy improvement (26) in IEPI is equivalent to solving (22), it is equivalent to (23) by vi = v\u03c0i and (12).\n(ICPI) Applying Lemma B.1 to policy evaluation of ICPI and rearranging it using (5) and (27), we obtain for each (x, u) \u2208 X \u00d7 U0:\n\u2212 ln \u03b3 \u00b7 vi(x) = R(x, \u03c0i(x))\u2212 cTi (x)(u\u2212 \u03c0i(x)) +\u2207vi(x)f(x, u) = h(x, \u03c0i(x),\u2207vi(x)) + (u\u2212 \u03c0i(x))T\u03c8(x), (B.5)\nwhere \u03c8(x) .= FTc (x)\u2207vTi (x) \u2212 ci(x). Next, let x \u2208 X be an arbitrary fixed value. Then, for each j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,m}, subtracting (B.5) for u = uj\u22121 from the same equation but for u = uj yields 0 = (uj \u2212 uj\u22121)\u03c8(x). This can be rewritten in the following matrix-vector form:\n(E1:m \u2212 E0:m\u22121)\u03c8(x) = 0, (B.6)\nwhere Ek:l . = [uk uk+1 \u00b7 \u00b7 \u00b7 ul] for 0 \u2264 k \u2264 l \u2264 m is the m \u00d7 (l \u2212 k + 1)-matrix constructed by the column vectors uk, uk+1, \u00b7 \u00b7 \u00b7 , ul. Since (54) implies that {uj \u2212 uj\u22121}mj=1 is a basis of Rm, we have rank (E1:m \u2212 E0:m\u22121) = m and by (B.6), \u03c8(x) = 0. Since x \u2208 X is arbitrary, \u03c8 = 0. Now that we have \u03c8(x) = 0 \u2200x \u2208 X , (B.5) becomes (B.4) and thus vi = v\u03c0i by Theorem 1 and Assumption 3a. This also implies ci = c\u03c0i by \u03c8 = 0 and the definition of \u03c8.\n16 See the discussions right below (37) for IAPI and (46) for IQPI.\nMoreover, by vi = v\u03c0i , the policy improvement (53) is equal to \u03c0i+1(x) = \u03c3(c\u03c0i(x)) for all x \u2208 X , which is the closedform solution of (23) in the u-AC setting (27)\u2013(29). 2"}, {"heading": "C Proof of Lemma B.1", "text": "The proof is done using the following claim.\nClaim C.1 Let \u00b5 be a policy starting at t and w : X \u00d7 U be a continuous function. If there exist \u2206t > 0 and \u03b2 > 0 such that for all x \u2208 X ,\n0 = E\u00b5 [ \u222b t\u2032\nt\n\u03b2\u03c4\u2212t w(X\u03c4 , U\u03c4 ) d\u03c4 \u2223\u2223\u2223\u2223Xt = x], (C.1) then, w(x, \u00b5(t, x)) = 0 for all x \u2208 X .\nBy the standard calculus, for any \u2206t > 0 and \u03b2 > 0,\n\u03b2\u03c4\u2212tv(X\u03c4 ) \u2223\u2223\u2223t\u2032 t = \u222b t\u2032 t d d\u03c4 ( \u03b2\u03c4\u2212tv(X\u03c4 ) ) d\u03c4\n= \u222b t\u2032 t \u03b2\u03c4\u2212t [ ln\u03b2 \u00b7 v(X\u03c4 ) + v\u0307(X\u03c4 , U\u03c4 ) ] d\u03c4. (C.2)\nHence, (B.1) can be rewritten for any (x, u) \u2208 X \u00d7 U0 as\n0 = E\u00b5 [ \u222b t\u2032\nt\n\u03b2\u03c4\u2212t w(X\u03c4 , U\u03c4 ) d\u03c4 \u2223\u2223\u2223\u2223Xt = x, Ut = u] (C.3) where w(x, u) .= Z(x, u) + ln\u03b2 \u00b7 v(x) + \u2207v(x)f(x, u). Here, w is continuous since so are v,\u2207v, Z, and f , and thus the term \u201c\u03b2\u03c4\u2212t w(X\u03c4 , U\u03c4 )\u201d is integrable over the compact time interval [t, t\u2032]. Now, fix u \u2208 U0. Then, one can see that\n(1) \u00b5(\u00b7, \u00b7, u) is obviously a policy; (2) the condition Ut = u in (C.3) is obviated for fixed u; (3) \u00b5(t, x, u) = u holds for all x \u2208 X .\nHence, by Claim C.1, we obtain\n0 = w(x, \u00b5(t, x, u)) = w(x, u) for all x \u2208 X .\nSince u \u2208 U0 is arbitrary, we finally have w(x, u) = 0 for all (x, u) \u2208 X \u00d7 U0, which completes the proof.\nProof of Claim C.1. To prove the claim, let x0 \u2208 X and xk . = E\u00b5[X \u2032t \u2223\u2223Xt = xk\u22121] for k = 1, 2, 3, \u00b7 \u00b7 \u00b7 . Then, (C.1) obviously holds for each x = xk \u2208 X (k \u2208 Z+). Denote\nt0 . = t and tk . = t+ k\u2206t for any k \u2208 N\nand define \u00b5\u0304k : [tk,\u221e)\u00d7X \u2192 U for each k \u2208 Z+ as\n\u00b5\u0304k(\u03c4, x) . = \u00b5(\u03c4 \u2212 tk + t, x) for any \u03c4 \u2265 tk and any x \u2208 X .\nThen, obviously, \u00b5\u0304k is a policy that starts at time tk. Moreover, since in our framework, the non-stationarity, i.e., the explicit time-dependency, comes only from, if any, that of the applied policy, we obtain by the above process and (C.1) that\n0 = E\u00b5\u0304k [ \u222b tk+1\ntk\n\u03b2\u03c4\u2212t w(X\u03c4 , U\u03c4 ) d\u03c4 \u2223\u2223\u2223\u2223Xtk = xk ] (C.4) for all k \u2208 Z+. Next, construct \u00b5\u0304 : [t,\u221e)\u00d7X \u2192 U by\n\u00b5\u0304(\u03c4, x) . = \u00b5\u0304k(\u03c4, x) for \u03c4 \u2208 [tk, tk+1) and x \u2208 X .\nThen, for each fixed x \u2208 X , \u00b5\u0304(\u00b7, x) is right continuous since for all k \u2208 Z+, so is \u00b5\u0304k(\u00b7, x) on each time interval [tk, tk+1). In a similar manner, for each fixed \u03c4 \u2208 [t,\u221e), \u00b5\u0304(\u03c4, \u00b7) is continuous over X since for all k \u2208 Z+, so is \u00b5\u0304k(\u03c4, \u00b7) for each fixed \u03c4 \u2208 [tk, tk+1). Moreover, since \u00b5\u0304k is a policy starting at tk, the state trajectory E\u00b5\u0304k [X\u00b7|Xtk = xk] is uniquely defined over [tk, tk+1). Therefore, noting that xk is represented (by definitions and the recursive relation) as\nxk = E\u00b5\u0304k\u22121 [ Xtk \u2223\u2223Xtk\u22121 = xk\u22121] for any k \u2208 N, we conclude that the state trajectory E\u00b5\u0304[X\u00b7|Xt = x0] is also uniquely defined for each x0 \u2208 X over [t,\u221e), implying that \u00b5\u0304 is a policy starting at time t. Finally, using the policy \u00b5\u0304, we obtain from (C.4)\n0 = \u221e\u2211 k=0 ( E\u00b5\u0304k [ \u222b tk+1 tk \u03b2\u03c4\u2212t w(X\u03c4 , U\u03c4 ) d\u03c4 \u2223\u2223\u2223\u2223Xtk = xk ]) = E\u00b5\u0304 [ \u222b \u221e t \u03b2\u03c4\u2212t w(X\u03c4 , U\u03c4 ) d\u03c4\n\u2223\u2223\u2223\u2223Xt = x0]\ufe38 \ufe37\ufe37 \ufe38 . =W (t;x0) ,\nwhich implies W (t;x0) = 0 for all t \u2265 0. Since\n\u2202\n\u2202t \u222b \u221e t \u03b2\u03c4w(X\u03c4 , U\u03c4 ) d\u03c4 = lim \u2206t\u21920 1 \u2206t \u222b t\u2032 t \u03b2\u03c4w(X\u03c4 , U\u03c4 ) d\u03c4\n= w(Xt, Ut)\nby (right) continuity of w, X\u03c4 , and U\u03c4 , we therefore obtain\n0 = \u2202W (t;x0)\n\u2202t = \u2212 ln\u03b2 \u00b7W (t;x0) + \u03b2\u2212t \u00b7 w(x0, \u00b5\u0304(t, x0))\nand thereby, w(x0, \u00b5(t, x0)) = 0. Since x0 \u2208 X is arbitrary, it implies w(x, \u00b5(t, x)) = 0 for all x \u2208 X . 2\nD Inverted-Pendulum Simulation Methods\nD.1 Linear Function Approximations by RBFNs\nTo describe the methods in a unified manner, we denote any network input by z and its corresponding input space by Z .\nThey correspond to z = (x, u) and Z = X \u00d7 U when the network is AD, and z = x and Z = X when it is not. In the simulations in Section 5, the functions vi, ai, qi, and ci are all approximated by RBFNs as shown below: vi(x) \u2248 v\u0302(z; \u03b8vi ) . = \u03c6T(z\u0304)\u03b8vi , ci(x) \u2248 c\u0302(z; \u03b8ci ) . = \u03c6T(z\u0304)\u03b8ci , ai(x, u) \u2248 a\u0302(z; \u03b8ai ) . = \u03c6TAD(z\u0304)\u03b8 a i ,\nqi(x, u) \u2248 q\u0302(z; \u03b8qi ) . = \u03c6TAD(z\u0304)\u03b8 q i ,\n(D.1)\nwhere z\u0304 \u2208 Z represents the input z \u2208 Z to each network whose state-component x is normalized to x\u0304 \u2208 [\u2212\u03c0, \u03c0]\u00d7R by adding \u00b12\u03c0k to its first component x1 for some k \u2208 Z+; \u03b8vi , \u03b8 c i \u2208 RN and \u03b8ai , \u03b8 q i \u2208 RM are the weight vectors of the networks; N,M \u2208 N are the numbers of hidden neurons; the RBFs \u03c6 : Z \u2192 RN with Z = X and \u03c6AD : Z \u2192 RM with Z = X \u00d7 U are defined as\n\u03c6j(x) = e \u2212\u2016x\u2212xj\u20162\u03a31 and \u03c6AD,j(z) = e \u2212\u2016z\u2212zj\u20162\u03a32 .\nHere, \u03c6j and \u03c6AD,j are the j-th components of \u03c6 and \u03c6AD, respectively; \u2016x\u2016\u03a31 and \u2016z\u2016\u03a32 are weighted Euclidean norms defined as \u2016x\u2016\u03a31 . = (xT\u03a31x) 1/2 and \u2016z\u2016\u03a32 . = (zT\u03a32z) 1/2 for the diagonal matrices \u03a31 . = diag{1, 0.5} and \u03a32 . = diag{1, 0.5, 1}; xj \u2208 X for 1 \u2264 j \u2264 N and zj \u2208 X \u00d7U for 1 \u2264 j \u2264M are the center points of RBFs that are uniformly distributed within the compact regions \u2126x and \u2126x \u00d7 U , respectively. In all of the simulations, we chooseN = 132 and M = 133, so we have 132-RBFs in v\u0302 and c\u0302, and 133-RBFs in a\u0302 and q\u0302.\nD.2 Policy Evaluation Methods\nUnder the approximation (D.1), the Bellman equations in Algorithms 2, 3b, 4b, and 5 can be expressed, with the approximation error \u03b5 : Z \u2192 R, as the following unified form:\n\u03c8T(z) \u00b7 \u03b8i = b(z) + \u03b5(z), (D.2) where the parameter vector \u03b8i \u2208 RL to be estimated, with its dimension L, and the associated functions \u03c8 : Z \u2192 R1\u00d7L and b : Z \u2192 R are given in Table D.1 for each IPI method. In Table D.1, I\u03b1(Z), D\u03b1(v), and \u03c6\u03c0iAD defined as\nI\u03b1(Z) . = \u222b t\u2032 t \u03b1\u03c4\u2212tZ(X\u0304\u03c4 , U\u03c4 ) d\u03c4, D\u03b1(v) . = v(X\u0304t)\u2212 \u03b1\u2206tv(X\u0304 \u2032t),\nand \u03c6\u03c0iAD . = \u03c6AD(\u00b7, \u03c0i(\u00b7)) were used for simplicity, where X\u0304\u03c4 is the state value X\u03c4 normalized to [\u2212\u03c0, \u03c0]\u00d7R; 17 we set \u03b2 = 1 in our IQPI simulation.\n17 X\u0304\u03c4 is normalized to X\u0304\u03c4 whenever input to the RBFN(s). Other than that, the use of X\u0304\u03c4 instead of X\u03c4 (or vice versa) does not affect the performance (e.g., R0(X\u03c4 ) = R0(X\u0304\u03c4 ) in our setting).\nIn each i-th policy evaluation of each method, \u03c8(z) and b(z) in (D.2) were evaluated at the given data points z = zinit,j (j = 1, 2, \u00b7 \u00b7 \u00b7 , Linit with L \u2264 Linit) that are uniformly distributed over the respective compact regions \u2126x\u00d7U (IAPI and IQPI), \u2126x (IEPI), and \u2126x\u00d7U0 withU0 = {\u2212Umax, Umax} (ICPI). The trajectory X\u00b7 over [t, t\u2032] with each data point zinit,j used as its initial condition was generated using the 4th-order Runge-Kutta method with its time step \u2206t/10 = 1 [ms], and the trapezoidal approximation:\nI\u03b1(Z) \u2248 ( Z(X\u0304t, Ut) + \u03b1 \u2206tZ(X\u0304 \u2032t, U \u2032 t)\n2\n) \u00b7\u2206t\nwas used in the evaluation of \u03c8 and b. The number Linit of the data points zinit,j used in each IPI algorithm was 173 (IAPI), 25\u00d731\u00d725 (IQPI), 172 (IEPI), and 172\u00d72 (ICPI). In the i-th policy evaluation of IAPI, we also evaluated the vectors \u03c1(x) .= [ 0\n\u03c6AD(x,\u03c0i(x))\n] \u2208 RL at the grid points xgrid,k\n(k = 1, 2, \u00b7 \u00b7 \u00b7 , Lgrid with Lgrid = 502) uniformly distributed in \u2126x, in order to take the constraint\n\u03c1T(x)\u03b8i = \u03b5const(x) (D.3)\nobtained from ai(x, \u03c0i(x)) = 0 into considerations, where \u03b5const : X \u2192 R is the residual error. After evaluating \u03c8(\u00b7) and b(\u00b7) at all points zinit,j (and in addition to that, \u03c1(\u00b7) at all points xgrid,j in the IAPI case), the parameters \u03b8i in (D.2) were estimated using least squares as\n\u03b8\u0302i = ( Linit\u2211 j=1 \u03c8j\u03c8 T j )\u22121( Linit\u2211 j=1 \u03c8jbj ) (D.4)\nin the case of IQPI, IEPI, and ICPI, where \u03c8j . = \u03c8(zinit,j) and bj . = b(zinit,j). This \u03b8\u0302i minimizes the squared error J(\u03b8i) = \u03b5 2(zinit,1) + \u00b7 \u00b7 \u00b7+ \u03b52(zinit,Linit). In IAPI, \u03b8i in (D.2) and (D.3) were estimated also in the least-squares sense as\n\u03b8\u0302i = ( Linit\u2211 j=1 \u03c8j\u03c8 T j + Lgrid\u2211 k=1 \u03c1k\u03c1 T k )\u22121( Linit\u2211 j=1 \u03c8jbj ) , (D.5) where \u03c1k . = \u03c1(xgrid,k); this \u03b8\u0302i minimizes the squared error JIAPI(\u03b8i) = J(\u03b8i)+(\u03b5 2 const(xgrid,1)+ \u00b7 \u00b7 \u00b7+\u03b52const(xgrid,Lgrid)).\nD.3 Policy Improvement Methods\nIn each i-th policy improvement, the next policy \u03c0i+1 was obtained using the estimates \u03b8\u0302vi , \u03b8\u0302 c i , \u03b8\u0302 a i , or \u03b8\u0302 q i obtained at the i-th policy evaluation by (D.4) or (D.5) depending on the algorithms. In all of the simulations, the next policy \u03c0i+1 was parameterized as \u03c0i+1(x) \u2248 \u03c3(y\u0302i(x)), where y\u0302i(x) was directly determined in IEPI and ICPI (Algorithms 4b and 5) as y\u0302i(x) = FTc (x)\u2207v\u0302T(x; \u03b8\u0302vi ) and y\u0302i(x) = c\u0302(x; \u03b8\u0302ci ), respectively. In IAPI and IQPI, y\u0302i(x) is the output of the RBFN we additionally introduced:\ny\u0302i(x) = \u03c6 T(x\u0304)\u03b8\u0302ui\nto perform the respective maximizations (41) and (43). Here, \u03b8\u0302ui \u2208 RN is updated by the mini-batch regularized update descent (RUD) (Aleksandar, Lever, and Barber, 2016) shown in Algorithm D.1, which is a variant of stochastic gradient descent, to perform such maximizations with improved convergence speed. In Algorithm D.1, Jimp(x; \u03b8) is given by\nJimp(x; \u03b8) = { q\u0302(x, u; \u03b8\u0302qi )|u=\u03c3(\u03c6T(x\u0304)\u03b8) in IQPI, a\u0302(x, u; \u03b8\u0302ai )|u=\u03c3(\u03c6T(x\u0304)\u03b8) in IAPI;\nthe error tolerance 0 < \u03b4 1 was set to \u03b4 = 0.01; the smoothing factor \u03bbj \u2208 (0, 1) and the learning rate \u03b7j > 0 were scheduled as \u03bbj = (1\u2212 ) \u00b7 (1\u2212103\u03b7j) with = 10\u22123 and\n\u03b7j = { 10\u22123 for 1 \u2264 j \u2264 30; 10\u22123/(j \u2212 30) for j > 30.\nAlgorithm D.1: Mini-Batch RUD for Policy Improvement\n1 Initialize: { \u03b8\u0302ui = v = 0 \u2208 RN ; 0 < \u03b4 1 be a small constant;\n2 j \u2190 1; 3 repeat\n4 Calculate \u2202Jimp(x; \u03b8\u0302\nu i )\n\u2202\u03b8\u0302ui at all grid points {xgrid,k} Lgrid k=1;\n5 v\u2190 \u03bbjv + \u03b7j ( Lgrid\u2211 k=1 \u2202Jimp(xgrid,k; \u03b8\u0302 u i ) \u2202\u03b8\u0302ui ) ; 6 \u03b8\u0302ui \u2190 \u03b8\u0302ui + v;\n7 j \u2190 j + 1; until \u2016v\u2016 < \u03b4\n8 return \u03b8\u0302ui ;"}], "references": [{"title": "Nearly optimal control laws for nonlinear systems with saturating actuators using a neural network", "author": ["M. Abu-Khalaf", "F.L. Lewis"], "venue": "HJB approach. Automatica,", "citeRegEx": "Abu.Khalaf and Lewis,? \\Q2005\\E", "shortCiteRegEx": "Abu.Khalaf and Lewis", "year": 2005}, {"title": "Nesterov\u2019s accelerated gradient and momentum as approximations to regularised update descent", "author": ["B. Aleksandar", "G. Lever", "D. Barber"], "venue": "arXiv preprint arXiv:1607.01981v2,", "citeRegEx": "Aleksandar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Aleksandar et al\\.", "year": 2016}, {"title": "Optimal control: linear quadratic methods", "author": ["B. Anderson", "J.B. Moore"], "venue": null, "citeRegEx": "Anderson and Moore,? \\Q1989\\E", "shortCiteRegEx": "Anderson and Moore", "year": 1989}, {"title": "Advantage updating", "author": ["III L.C. Baird"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Baird,? \\Q1993\\E", "shortCiteRegEx": "Baird", "year": 1993}, {"title": "Galerkin approximations of the generalized Hamilton-Jacobi-Bellman equation", "author": ["R.W. Beard", "G.N. Saridis", "J.T. Wen"], "venue": null, "citeRegEx": "Beard et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Beard et al\\.", "year": 1997}, {"title": "On the converse of banach \u201cfixed-point principle", "author": ["C. Bessaga"], "venue": "Colloquium Mathematicae,", "citeRegEx": "Bessaga,? \\Q1959\\E", "shortCiteRegEx": "Bessaga", "year": 1959}, {"title": "Reinforcement learning in continuous time and space", "author": ["K. Doya"], "venue": "Neural computation,", "citeRegEx": "Doya,? \\Q2000\\E", "shortCiteRegEx": "Doya", "year": 2000}, {"title": "Regularized policy iteration", "author": ["A.M. Farahmand", "M. Ghavamzadeh", "S. Mannor", "C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Farahmand et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2009}, {"title": "Real analysis: modern techniques and their applications", "author": ["G.B. Folland"], "venue": null, "citeRegEx": "Folland,? \\Q1999\\E", "shortCiteRegEx": "Folland", "year": 1999}, {"title": "Reinforcement learning using a continuous time actor-critic framework with spiking neurons", "author": ["N. Fr\u00e9maux", "H. Sprekeler", "W. Gerstner"], "venue": "PLoS Comput. Biol.,", "citeRegEx": "Fr\u00e9maux et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fr\u00e9maux et al\\.", "year": 2013}, {"title": "Nonlinear dynamical systems and control: a Lyapunov-based approach", "author": ["W.M. Haddad", "V. Chellaboina"], "venue": null, "citeRegEx": "Haddad and Chellaboina,? \\Q2008\\E", "shortCiteRegEx": "Haddad and Chellaboina", "year": 2008}, {"title": "Dynamic drogramming and Markov processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "Howard,? \\Q1960\\E", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Handbook of metric fixed point theory", "author": ["W. Kirk", "B. Sims"], "venue": "Springer Science & Business Media,", "citeRegEx": "Kirk and Sims,? \\Q2013\\E", "shortCiteRegEx": "Kirk and Sims", "year": 2013}, {"title": "On an iterative technique for Riccati equation computations", "author": ["D. Kleinman"], "venue": "IEEE Trans. Autom. Cont.,", "citeRegEx": "Kleinman,? \\Q1968\\E", "shortCiteRegEx": "Kleinman", "year": 1968}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Construction of suboptimal control sequences", "author": ["R.J. Leake", "Liu", "R.-W"], "venue": "SIAM Journal on Control,", "citeRegEx": "Leake et al\\.,? \\Q1967\\E", "shortCiteRegEx": "Leake et al\\.", "year": 1967}, {"title": "Integral Q-learning and explorized policy iteration for adaptive optimal control of continuous-time linear systems", "author": ["J.Y. Lee", "J.B. Park", "Y.H. Choi"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2012}, {"title": "On integral generalized policy iteration for continuous-time linear quadratic regulations", "author": ["J.Y. Lee", "J.B. Park", "Y.H. Choi"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2014}, {"title": "Integral reinforcement learning for continuous-time input-affine nonlinear systems with simultaneous invariant explorations", "author": ["J.Y. Lee", "J.B. Park", "Y.H. Choi"], "venue": "IEEE Trans. Neural Networks and Learning Systems,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Reinforcement learning and adaptive dynamic programming for feedback control", "author": ["F.L. Lewis", "D. Vrabie"], "venue": "IEEE Circuits and Systems Magazine,", "citeRegEx": "Lewis and Vrabie,? \\Q2009\\E", "shortCiteRegEx": "Lewis and Vrabie", "year": 2009}, {"title": "Lusin\u2019s Theorem and Bochner integration", "author": ["P.A. Loeb", "E. Talvila"], "venue": "Scientiae Mathematicae Japonicae,", "citeRegEx": "Loeb and Talvila,? \\Q2004\\E", "shortCiteRegEx": "Loeb and Talvila", "year": 2004}, {"title": "Data-based approximate policy iteration for affine nonlinear continuous-time optimal control", "author": ["B. Luo", "Wu", "H.-N", "T. Huang", "D. Liu"], "venue": "design. Automatica,", "citeRegEx": "Luo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2014}, {"title": "Toward off-policy learning control with function approximation", "author": ["H.R. Maei", "C. Szepesv\u00e1ri", "S. Bhatnagar", "R.S. Sutton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Maei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maei et al\\.", "year": 2010}, {"title": "Q-learning and pontryagin\u2019s minimum principle", "author": ["P. Mehta", "S. Meyn"], "venue": "In Proc. IEEE Int. Conf. Decision and Control, held jointly with the Chinese Control Conference (CDC/CCC),", "citeRegEx": "Mehta and Meyn,? \\Q2009\\E", "shortCiteRegEx": "Mehta and Meyn", "year": 2009}, {"title": "Optimal outputfeedback control of unknown continuous-time linear systems using off-policy reinforcement learning", "author": ["H. Modares", "F.L. Lewis", "Jiang", "Z.-P"], "venue": "IEEE Trans. Cybern.,", "citeRegEx": "Modares et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Modares et al\\.", "year": 2016}, {"title": "Adaptive dynamic programming", "author": ["J.J. Murray", "C.J. Cox", "G.G. Lendaris", "R. Saeks"], "venue": "IEEE Trans. Syst. Man Cybern. Part C-Appl. Rev.,", "citeRegEx": "Murray et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Murray et al\\.", "year": 2002}, {"title": "Approximate dynamic programming: solving the curses of dimensionality", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "Powell,? \\Q2007\\E", "shortCiteRegEx": "Powell", "year": 2007}, {"title": "An approximation theory of optimal control for trainable manipulators", "author": ["G.N. Saridis", "C.S.G. Lee"], "venue": "IEEE Trans. Syst. Man Cybern.,", "citeRegEx": "Saridis and Lee,? \\Q1979\\E", "shortCiteRegEx": "Saridis and Lee", "year": 1979}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Second Edition in Progress,", "citeRegEx": "Sutton and Barto,? \\Q2017\\E", "shortCiteRegEx": "Sutton and Barto", "year": 2017}, {"title": "Elementary real analysis", "author": ["B.S. Thomson", "J.B. Bruckner", "A.M. Bruckner"], "venue": null, "citeRegEx": "Thomson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Thomson et al\\.", "year": 2001}, {"title": "Online adaptive algorithm for optimal control with integral reinforcement learning", "author": ["K.G. Vamvoudakis", "D. Vrabie", "F.L. Lewis"], "venue": "Int. J. Robust and Nonlinear Control,", "citeRegEx": "Vamvoudakis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vamvoudakis et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 28, "context": "the current control law in control theory) and policy improvement to improve the policy by optimizing it using the obtained value function (Sutton and Barto, 2017; Lewis and Vrabie, 2009).", "startOffset": 139, "endOffset": 187}, {"referenceID": 19, "context": "the current control law in control theory) and policy improvement to improve the policy by optimizing it using the obtained value function (Sutton and Barto, 2017; Lewis and Vrabie, 2009).", "startOffset": 139, "endOffset": 187}, {"referenceID": 11, "context": "PI was first proposed by Howard (1960) in the stochastic environment known as Markov decision process (MDP) and is strongly relevant to reinforcement learning (RL) and approximate dynamic programming (ADP).", "startOffset": 25, "endOffset": 39}, {"referenceID": 26, "context": "Another advantage is that compared to backward-in-time dynamic programming, the forward-intime computation of PI like the other ADP methods (Powell, 2007) alleviates the problem known as the curse of dimensionality.", "startOffset": 140, "endOffset": 154}, {"referenceID": 14, "context": ", Lagoudakis and Parr, 2003; Farahmand, Ghavamzadeh, Mannor, and Szepesv\u00e1ri, 2009; Maei, Szepesv\u00e1ri, Bhatnagar, and Sutton, 2010). Here, offpolicy PI is a class of PI methods whose policy evaluation is done while following a policy, termed as a behavior policy, which is possibly different from the target policy to be evaluated; if the behavior and target policies are same, it is called an on-policy method. When the MDP is finite, all the on- or off-policy PI methods converge towards the optimal solution in finite time. Another advantage is that compared to backward-in-time dynamic programming, the forward-intime computation of PI like the other ADP methods (Powell, 2007) alleviates the problem known as the curse of dimensionality. In continuing tasks, a discount factor \u03b3 is normally introduced to PI and RL to suppress the future reward and thereby have a finite return. Sutton and Barto (2017) gives a comprehensive overview of PI, ADP, and RL algorithms with their practical applications and recent success in the RL field.", "startOffset": 2, "endOffset": 906}, {"referenceID": 28, "context": ", Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017).", "startOffset": 357, "endOffset": 381}, {"referenceID": 19, "context": "Motivated by those two approaches above, Vrabie and Lewis (2009) recently proposed a partially model-free PI scheme called integral PI (IPI), which is more relevant to RL/ADP in that the Bellman equation associated with its policy evaluation is of a temporal difference form\u2014see (Lewis and Vrabie, 2009) for a comprehensive overview.", "startOffset": 279, "endOffset": 303}, {"referenceID": 0, "context": ", Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017).", "startOffset": 93, "endOffset": 177}, {"referenceID": 0, "context": ", Leake and Liu, 1967; Kleinman, 1968; Saridis and Lee, 1979; Beard, Saridis, and Wen, 1997; Abu-Khalaf and Lewis, 2005, to name a few); Murray, Cox, Lendaris, and Saeks (2002) proposed a PI algorithm along with the trajectory-based policy evaluation that does not rely on the system model and can be viewed as a deterministic Monte-Carlo policy evaluation (Sutton and Barto, 2017). Motivated by those two approaches above, Vrabie and Lewis (2009) recently proposed a partially model-free PI scheme called integral PI (IPI), which is more relevant to RL/ADP in that the Bellman equation associated with its policy evaluation is of a temporal difference form\u2014see (Lewis and Vrabie, 2009) for a comprehensive overview.", "startOffset": 93, "endOffset": 448}, {"referenceID": 10, "context": "On the other hand, the aforementioned PI methods in CTS were all designed via Lyapunov\u2019s stability theory (Haddad and Chellaboina, 2008) to guarantee that the generated policies are all asymptotically stable and thereby yield finite returns (at least on a bounded region around an equilibrium state), provided that so is the initial policy.", "startOffset": 106, "endOffset": 136}, {"referenceID": 6, "context": ", those in (Doya, 2000; Mehta and Meyn, 2009; Fr\u00e9maux, Sprekeler, and Gerstner, 2013), this stability-based approach rather restricts the class of the cost (or reward) and the dynamics.", "startOffset": 11, "endOffset": 85}, {"referenceID": 23, "context": ", those in (Doya, 2000; Mehta and Meyn, 2009; Fr\u00e9maux, Sprekeler, and Gerstner, 2013), this stability-based approach rather restricts the class of the cost (or reward) and the dynamics.", "startOffset": 11, "endOffset": 85}, {"referenceID": 9, "context": "Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks.", "startOffset": 224, "endOffset": 246}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE.", "startOffset": 35, "endOffset": 52}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE.", "startOffset": 35, "endOffset": 89}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al.", "startOffset": 35, "endOffset": 146}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks.", "startOffset": 35, "endOffset": 413}, {"referenceID": 3, "context": "Advantage updating was proposed by Baird III (1993) and then reformulated by Doya (2000) under the environment represented by an ODE. Doya (2000) also extended TD(\u03bb) to the CTS domain and then combined it with the two policy improvement methods\u2014the continuous actor with its update rule and the value-gradient based (VGB) greedy policy improvement; see also (Fr\u00e9maux et al., 2013) for an extension of Doya (2000)\u2019s continuous actor-critic using spiking neural networks. Mehta and Meyn (2009) defined the Hamiltonian function as a Q-function and then proposed a Q-learning method in CTS based on stochastic approximation.", "startOffset": 35, "endOffset": 492}, {"referenceID": 18, "context": "(1) Motivated by the work of IPI (Vrabie and Lewis, 2009; Lee et al., 2015) in the optimal control framework, we propose the corresponding on-policy IPI scheme in the general RL framework and then prove its mathematical properties of admissibility/monotone-improvement", "startOffset": 33, "endOffset": 75}, {"referenceID": 6, "context": "(2) Extending on-policy IPI in Section 3, we propose four off-policy IPI methods in CTS\u2014two named integral advantage PI (IAPI) and integral Q-PI (IQPI) are the ideal PI forms of advantage updating (Baird III, 1993; Doya, 2000) and Q-learning in CTS, and the other two named integral explorized PI (IEPI) and integral C-PI (ICPI) are the natural extensions of the existing off-policy IPI methods (Lee et al.", "startOffset": 197, "endOffset": 226}, {"referenceID": 18, "context": "(2) Extending on-policy IPI in Section 3, we propose four off-policy IPI methods in CTS\u2014two named integral advantage PI (IAPI) and integral Q-PI (IQPI) are the ideal PI forms of advantage updating (Baird III, 1993; Doya, 2000) and Q-learning in CTS, and the other two named integral explorized PI (IEPI) and integral C-PI (ICPI) are the natural extensions of the existing off-policy IPI methods (Lee et al., 2015) to our general RL problem.", "startOffset": 395, "endOffset": 413}, {"referenceID": 5, "context": "Here, we emphasize that Doya (2000)\u2019s VGB greedy policy improvement is also developed under this u-AC setting, and ICPI provides its model-free version.", "startOffset": 24, "endOffset": 36}, {"referenceID": 29, "context": "Hence, vi(x) converges to v\u0302\u2217(x) by monotone convergence theorem (Thomson et al., 2001), the pointwise convergence vi \u2192 v\u0302\u2217.", "startOffset": 65, "endOffset": 87}, {"referenceID": 20, "context": "Next, by Lusin\u2019s theorem (Loeb and Talvila, 2004), for any \u03b5 > 0 and any compact set \u03a9 \u2282 X , there exists a compact subset E \u2286 \u03a9 such that |\u03a9\\E| < \u03b5 and the restriction v\u0302\u2217|E is continuous.", "startOffset": 25, "endOffset": 49}, {"referenceID": 29, "context": "Hence, the monotone sequence vi converges to v\u0302\u2217 uniformly on E (and on any compact subset of X if v\u0302\u2217 is continuous over X ) by Dini\u2019s theorem (Thomson et al., 2001).", "startOffset": 144, "endOffset": 166}, {"referenceID": 5, "context": "By Lemma 2 and Bessaga (1959)\u2019s converse of the Banach\u2019s fixed point principle, there exists a metric d on Va such that (Va, d) is a complete metric space and T is a contraction (and thus continuous) under d.", "startOffset": 15, "endOffset": 30}, {"referenceID": 6, "context": "Rearranging it with respect to u, we obtain the explicit closed-form expression of (26) also known as the VGB greedy policy (Doya, 2000):", "startOffset": 124, "endOffset": 136}, {"referenceID": 6, "context": "10 This includes the frameworks in (Doya, 2000; Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009) as special cases.", "startOffset": 35, "endOffset": 99}, {"referenceID": 0, "context": "10 This includes the frameworks in (Doya, 2000; Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009) as special cases.", "startOffset": 35, "endOffset": 99}, {"referenceID": 6, "context": "12 See also (Doya, 2000; Abu-Khalaf and Lewis, 2005).", "startOffset": 12, "endOffset": 52}, {"referenceID": 0, "context": "12 See also (Doya, 2000; Abu-Khalaf and Lewis, 2005).", "startOffset": 12, "endOffset": 52}, {"referenceID": 2, "context": "Hence, the application of the standard LQR theory (Anderson and Moore, 1989) shows that", "startOffset": 50, "endOffset": 76}, {"referenceID": 18, "context": "\u2019 See (AbuKhalaf and Lewis, 2005; Vrabie and Lewis, 2009; Lee et al., 2015) for the u-AC case (27)\u2013(29) with \u03b3 = 1 and R \u2264 0.", "startOffset": 6, "endOffset": 75}, {"referenceID": 6, "context": "(Baird III, 1993; Doya, 2000), which is defined as", "startOffset": 0, "endOffset": 29}, {"referenceID": 3, "context": "13 Our general Q-function q\u03c0 includes the previously proposed Qfunctions in CTS as special cases\u2014Baird III (1993)\u2019s Q-function (\u03ba1 = 1, \u03ba2 = 1/\u2206t); h\u03c0 for \u03b3 \u2208 (0, 1) (\u03ba1 = \u03ba2 = \u2212 ln \u03b3), and its generalization for \u03b3 \u2208 (0, 1] (any \u03ba1 = \u03ba2 > 0) both recognized as Q-functions by Mehta and Meyn (2009).", "startOffset": 97, "endOffset": 114}, {"referenceID": 3, "context": "13 Our general Q-function q\u03c0 includes the previously proposed Qfunctions in CTS as special cases\u2014Baird III (1993)\u2019s Q-function (\u03ba1 = 1, \u03ba2 = 1/\u2206t); h\u03c0 for \u03b3 \u2208 (0, 1) (\u03ba1 = \u03ba2 = \u2212 ln \u03b3), and its generalization for \u03b3 \u2208 (0, 1] (any \u03ba1 = \u03ba2 > 0) both recognized as Q-functions by Mehta and Meyn (2009).", "startOffset": 97, "endOffset": 298}, {"referenceID": 3, "context": "As mentioned by Baird III (1993), a bad scaling between v\u03c0 and a\u03c0 in q\u03c0 , e.", "startOffset": 16, "endOffset": 33}, {"referenceID": 16, "context": "In this paper, we name it integral explorized policy iteration (IEPI) following the perspectives of Lee et al. (2012) and present its policy evaluation and improvement loop in Algorithm 4a.", "startOffset": 100, "endOffset": 118}, {"referenceID": 6, "context": "In this case, the maximization process in the policy improvement is simplified to the update rule (30) also known as the VGB greedy policy (Doya, 2000).", "startOffset": 139, "endOffset": 151}, {"referenceID": 6, "context": "(1) As in IEPI, the complicated maximization in the policy improvement of IAPI and IQPI has been replaced by the simple update rule (53), which is a kind of modelfree VGB greedy policy (Doya, 2000).", "startOffset": 185, "endOffset": 197}, {"referenceID": 16, "context": "Instead, its derivation was based on the value function with singularly-perturbed actions (Lee et al., 2012).", "startOffset": 90, "endOffset": 108}, {"referenceID": 6, "context": "IAPI has the constraint (40) on ai and \u03c0i in the policy evaluation that reflects the equality a\u03c0i(x, \u03c0i(x)) = 0 similarly to advantage updating (Baird III, 1993; Doya, 2000).", "startOffset": 144, "endOffset": 173}, {"referenceID": 6, "context": "(1) the uniqueness of the target solution (vi, ci) = (v\u03c0i , c\u03c0i) of the Bellman equation (39) for Z\u03c4 given by (52); (2) the exploration of a smaller space X \u00d7 {uj}j=0, rather than the whole state-action space X \u00d7 U ; (3) the simple update rule (53) in policy improvement, the model-free version of the VGB greedy policy (Doya, 2000), in place of the complicated maximization over U for each x \u2208 X such as (41) and (43) in IAPI and IQPI.", "startOffset": 320, "endOffset": 332}, {"referenceID": 6, "context": "the VGB greedy policy (Doya, 2000)), rather than performing the maximization (26).", "startOffset": 22, "endOffset": 34}, {"referenceID": 6, "context": "Note that this model is exactly same to that used by Doya (2000) except that the action U\u03c4 , the torque input, is coupled with the term \u2018cos \u03b8\u03c4 \u2019 rather than the constant \u20181,\u2019 which makes our problem more realistic and challenging.", "startOffset": 53, "endOffset": 65}, {"referenceID": 18, "context": "Also note that the IPI methods achieved our learning objective without using an initial stabilizing policy that is usually required in the optimal control setting under the total discounting \u03b3 = 1 (e.g., Abu-Khalaf and Lewis, 2005; Vrabie and Lewis, 2009; Lee et al., 2015).", "startOffset": 197, "endOffset": 273}], "year": 2017, "abstractText": "Policy iteration (PI) is a recursive process of policy evaluation and improvement to solve an optimal decision-making, e.g., reinforcement learning (RL) or optimal control problem and has served as the fundamental to develop RL methods. Motivated by integral PI (IPI) schemes in optimal control and RL methods in continuous time and space (CTS), this paper proposes on-policy IPI to solve the general RL problem in CTS, with its environment modelled by an ordinary differential equation (ODE). In such continuous domain, we also propose four off-policy IPI methods\u2014two are the ideal PI forms that use advantage and Q-functions, respectively, and the other two are natural extensions of the existing off-policy IPI schemes to our general RL framework. Compared to the IPI methods in optimal control, the proposed IPI schemes can be applied to more general situations and do not require an initial stabilizing policy to run; they are also strongly relevant to the RL algorithms in CTS such as advantage updating, Q-learning, and value-gradient based (VGB) greedy policy improvement. Our on-policy IPI is basically model-based but can be made partially model-free; each off-policy method is also either partially or completely model-free. The mathematical properties of the IPI methods\u2014admissibility, monotone improvement, and convergence towards the optimal solution\u2014are all rigorously proven, together with the equivalence of onand off-policy IPI. Finally, the IPI methods are simulated with an inverted-pendulum model to support the theory and verify the performance.", "creator": "LaTeX with hyperref package"}}}