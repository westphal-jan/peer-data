{"id": "1705.03751", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "A Survey of Distant Supervision Methods using PGMs", "abstract": "structured database looks to the utility of populating 2d database with consisting of indexed form $ query ( e _ [UNK], e _ _ ) $, where $ r $ is topological relation database $ e _ 1 $, $ e _ 2 $ relative entities. distance supervision program computational mathematical technique ruby tries to automatically provide reference examples based on with existing kb library as freebase. foremost one is topical repository showcasing some of the techniques from distant supervision which primarily rely simple probabilistic complexity models ( pgms ).", "histories": [["v1", "Wed, 10 May 2017 13:19:38 GMT  (233kb,D)", "http://arxiv.org/abs/1705.03751v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["gagan madan"], "accepted": false, "id": "1705.03751"}, "pdf": {"name": "1705.03751.pdf", "metadata": {"source": "CRF", "title": "A Survey of Distant Supervision Methods using PGMs", "authors": ["Gagan Madan"], "emails": ["gaganm.me113@mech.iitd.ac.in"], "sections": [{"heading": "1 Introduction", "text": "In the recent years, there has been an explosion of information on the web. As the size of data available continues to expand rapidly, this provides an interesting challenge as well as an opportunity to researchers in NLP. While managing such large amounts of data, and distilling the relevant information in a form that is manageable is a challenge, the large amount of data available provides an opportunity to do this.\nOne possible way to manage such large amounts of data is to populate a database with tuples of the form relation<entity1, entity2>, extracted from text. Examples of such tuples could be capital-of(Delhi, India), PrimeMinister(Narendra Modi, India). This simple representation is particularly useful, as it allows answering queries on the text directly about a given relation, and entities. To extract tuples of this form is the key objective of Relational Extraction. Relational Extraction is a particularly well studied problem in the NLP domain, with many varied existing approaches to this problem.\nMost of the approaches can be classified as follows:\n1. Bootstrapping Methods: These methods start with a small set of \u201cseed\u201d tuples, and iteratively generate patterns from the seed tuples, recognize more tuples to be utilized as seed tuples, thus bootstrapping into the complete relation table. This was introduced for the first time in DIPRE (Brin, 1998), and later extended in Snowball (Agichtein and Gravano, 2000). Bootstrapping methods typically suffer from semantic drift, and often have poor precision as the number of iterations increases.\n2. Supervised Methods: These methods treat the task of relational extraction as a supervised learning problem, and rely on the availability of extensive training data for extracting relations. Due to the reliance on training data, these methods usually extract tuples only from some given relations. Some interesting approaches in this domain have used tree kernels built on dependency parse trees (Culotta and Sorensen, 2004), training semi-CRFs (Sarawagi et al., 2004), and using shortest path tree kernels (Bunescu and Mooney, 2005).\n3. Open Domain IE Methods: This is a selfsupervised learning model, which utilises no training data, but relies on redundancy of information present in the corpus. This outputs a collection of all possible tuples of the form (e1, r, e2) where e1, e2 are two entities related through a phrase r. The current stateof-the-art system in OpenIE is Open IE 4.0, which is a further improvement on Reverb (Fader et al., 2011) and Ollie (Schmitz et al., 2012).\n4. Distant Supervision: This technique automatically generates training examples and\nar X\niv :1\n70 5.\n03 75\n1v 1\n[ cs\n.A I]\n1 0\nM ay\n2 01\n7\ntries to learn features based on target relational tables in a Knowledge Base (KB). Typically, large KBs such as Freebase are used for this. This technique usually does not require any human intervention. This technique was originally introduced in the context of biological KBs (Craven et al., 1999), but has been successfully extended to any texts (Mintz et al., 2009).\n5. Deep Learning Based Methods: These techniques are relatively new and utilize the word embeddings generated by word2vec (Mikolov et al., 2013). One of the most successful atttempts in this domain, (Zeng et al., 2014) uses Convolution Neural Networks (CNNs) for relational extraction.\nWhile a major focus currently in NLP is on deep learning techniques, as KBs become better with time, it makes sense to focus on techniques that can leverage the power of KBs. With this in mind, it makes sense to focus on Distant Supervision, a technique which requires minimal human intervention and is primarily based on utilising large scale KBs such as Freebase."}, {"heading": "2 Problem Definition", "text": "We define the task of distant supervision formally in the notation given by (Min et al., 2013): Given a knowledge base (KB),D, a set of relations R, the KB D contains tuples of the form r(e1, e2), where r \u2208 R, and e1 and e2 are entities known to be related by the relation r. Further, we are given a corpus, C, which contains natural language text. The task in Distant Supervision is to align the corpus C with the KB D, i.e. to automatically generate training examples for relational extraction by labelling relational mentions in C with relations in D. Formally, this requires labelling set of entity mentions (e1, e2) present in C with some r \u2208 R or OTHER. Most works treat the OTHER class as negative training examples."}, {"heading": "3 Datasets and Knowledge Bases", "text": ""}, {"heading": "3.1 Datasets", "text": "Some of the earliest work in Distant Supervision was motivated by research in the biomedical domain. Therefore, the first model by (Craven et al., 1999) primarily relied on database from medical sources. Subsequently, evaluations have been\nperformed on Wikipedia, NY Times corpus and more recently on the 2010 and 2011 KBP shared tasks (Ji et al., 2010; Owczarzak and Dang, 2011). Some features of the datasets are:\n1. Yeast Protein Database(YPD) (Hodges et al., 1999), PubMed, MEDLINE: The work by (Craven et al., 1999) primarily uses the YPD database, which includes facts about various proteins as well as links to PubMed articles that establish the fact. Further, they use MEDLINE, a database of bibliographic information and abstracts for over nine million articles in biomedical domain for evaluation of their model.\n2. Freebase-Wikipedia: This dataset is a dump of all Wikipedia articles which have been sentence tokenized by Metaweb Technologies, the developers of Freebase. This is the main dataset used by (Mintz et al., 2009) for Distant Supervision.\n3. NY Times: This dataset was developed by (Riedel et al., 2010) by aligning Freebase relations with the NY Times dataset. They use StanfordNER to find relevant entity mentions in the text.\n4. KBP: This dataset was primarily used for evaluation by (Surdeanu et al., 2012). The training relations here are a were generated from the 2010 and 2011 KBP shared tasks (Ji et al., 2010; Owczarzak and Dang, 2011), which is a subset of Wikipedia Infoboxes from 2008."}, {"heading": "3.2 Knowledge Bases", "text": "Most Distant Supervision approaches typically use Freebase (Bollacker et al., 2008) as the KB for the task. Freebase is a publicly available database of semantic data. It contains relations from various sources, primarily from Wikipedia text boxes. It has around 9 million entities and around 7300 relations. Some of the largest relations are shown in Table 1."}, {"heading": "4 Distant Supervision using PGMs", "text": "In this section, we describe the various models proposed for Distant Supervision using Probabilistic Graphical Models (PGMs). We begin our discussion with the simplest model, based on a Naive Bayes classifier, and move to models using more complicated PGMs."}, {"heading": "4.1 Distant Supervision in Biomedical Domain (Craven et al., 1999)", "text": "This paper was the first attempt at using an existing database to generate training examples from a text corpus. While the model itself was very simple, it was in many ways ahead of its time. Since there were no large scale general knowledge databases available around that time, while this model was used successfully in domain specific contexts such as genetics (Jenssen et al., 2001), it wasn\u2019t until when large scale KBs such as Freebase became available that this technique started becoming relevant.\nThe paper considers the task of extracting relations of the type subcellular-localization(Protein, Subcellular-Structure), which represent the various subcellular structures in which proteins may be present. The authors first construct a labelled set of instances which contain the target relation. This is done by selecting six proteins and querying them in MEDLINE, which is a corpus of abstracts of biomedical journals. The abstracts are then hand-annotated with instances of the target relation, subcellular-localization. This resulted in a total of 33 instances of the target relation. This is used to train a Naive Bayes classifier, and later for evaluating the Distant Supervision trained model.\nThe Distant Supervision model, the authors consider the Yeast Protein Database (Hodges et al., 1999), which includes a subcellular-localization field for many proteins, as well as a hyperlink to the reference article in PubMed. Each of these entries, along with the reference is used as a weakkly learned instance for relation extraction. After cleaning up the dataset, the authors generate a set of 336 relational instances described in 633 sentences. The sentences that do not mention the relational instance are treated as negative training ex-\namples. The Figure 1 shows the simple Naive Bayes model. Given a document d of n words, (w1, w2, \u00b7 \u00b7 \u00b7 , wn), the probability that the document belongs to the class cj \u2208 C is given by:\nP (cj |d) = P (cj)\u03a0 n i=1P (wi|cj) P (d)\n(1)\nIn this case, there are only two classes, positive and negative, denoting whether the relation is present or not. In order to test whether r(x, y) is a valid relation, all sentences containing both the entities x and y are considered. Further, two approaches are considered. In the first approach, for each sentence, if the predicted class is positive, the extracted relation is returned, else no relation is returned. In the second approach, for each relation a confidence is calculated as follows:\nconfidence = 1\u2212\u03a0Nk=1[1\u2212P (c = positive|sk)] (2) where sk is the kth sentence that contains both the entities x and y. If the cconfidence is above a threshold, the relation is predicted as positive, and otherwise negative. The authors further evaluate the trained model on the MEDLINE dataset, and observe an increase in performance as compared\nto the Naive Bayes model trained on the MEDLINE dataset, with cross-validation.\nThey further add parse tree based features as well to their model, and learn the features through an algorithm similar to FOIL. This leads to a further increase in the accuracy of the model.\nWhile this model itself is very simple, one of the key contributions of this paper was the novel idea of generating training data through distant supervision and then using the generated data to learn features."}, {"heading": "4.2 Distant Supervision using Freebase", "text": "(Mintz et al., 2009)\nThis paper describes the key idea of Distant Supervision in a domain independent setting, and uses Freebase as the KB for distant supervision.\nThe key assumption is that if two entities participate in a relation, a sentence that contains mentions of both the entities, should express the relation.\nThe intuition behind this approach is to generate a training set of entity pairs that participate in a given set of relations. Entities are tagged with NER tools in the training step and if a sentence contains two entities known to be an instance of some Freebase relation, features are extracted and added to the feature vector for that relation. However, since sentences could possibly express incorrect relations, a multiclass logistic regression is trained to learn weights on the noisy extracted features.\nAt the time of testing, given a sentence, entities are identified using NER tools, and every pair of entities that appear together is considered a potential relation. For each entity pairs, features are extracted and the regression classifier predicts a relation name for every entity pair based on the extracted features.\nThe lexical features extracted are:\n\u2022 Sequence of words between entities\n\u2022 POS tags of these words\n\u2022 A flag indicating the order of appearance\n\u2022 A window of k words to the left of entity 1\n\u2022 A window of k words to the right of entity 2\nThe syntactic features are extracted by building a dependency parse tree. The features are:\n\u2022 Dependency path between entities\n\u2022 A window node for each entity: A window node is a node connected to one of the entities, but is not in the dependency path.\nAll the features in the classifier are used in conjunction. For two features to match, all the subfeatures should match. This is done in order to have high precision features at the cost of low recall.\nIt is shown that this algorithm is able to extract high precision patterns for a large number of relations, while the held out evaluation shows that combination of lexical and syntactic features performs the best.\n4.3 Multi Instance Learning (MIL) (Riedel et al., 2010)\nThis paper argues that the key assumption in current Distant Supervision tecniques, that each sentence which mentions the two related entities is an expression of the given relation is too strong and leads to noisy patterns that hurts the precision.\nThis paper relaxes this assumption to: If two entities participate in a relation, at least one sentence that mentions these two entities might express that relation.\nWhile this assumption is clearly better, it comes with additional complexity in both testing and training. The paper further suggests a undirected graphical model, which addresses both the tasks of predicting relation between entities and predicting which sentences express this relation.\nThe Figure 2 (Riedel et al., 2010) shows an instance for the Factor Graph model. Consider the example shown in the figure. The KB contains the relation founded(Roger McNamee, Elevation Partners). Consider the sentence \u201cElevation Partners , the 1.9 billion private equity group that was founded by Roger McNamee ...\u201d. This sentence contains mentions of both the entities, as well as expresses the correct relation. Now consider the sentence \u201cRoger McNamee , a managing director at Elevation Partners , ...\u201d. While this sentence does contain mentions of both the entities, it does not express the correct relation.\nThe model uses two types of hidden variables. Given a pair of entities, S and D, that appear together in at least sentence, a variable Y denotes the relation between them if it exists and NA otherwise. In Figure 2, Y is set to founded. Further, for the ith sentence that mentions both the entities, the authors define a boolean relation mention\nvariable Zi, which is true iff the ith sentence is indeed mentioning the relation Y . The entity mentions in the ith sentence are refered to as Si and Di respectively. Additional information about the ith sentence is stored in the observed variable xi. This is collected across all sentences that mention the entities to get the vector x. Further, Z is used to denote the state of all mention candidates. The conditional distribution is given by:\nP (Y = y, Z = z|x) = \u03a6 r(y)\u03a6join(y, z, x)\n\u220f i \u03a6 m(zi, xi)\nZx (3)\nwhere \u03a6r denotes the bias of the model towards a relation type y, and is defined as \u03a6r = exp(\u03b8ry). The function \u03a6m is defined as a function over xi as:\n\u03a6m(zi, xi) = exp( \u2211 j \u03b8mj \u03c6 m j (zi, xi)) (4)\nThe feature functions \u03c6mj (zi, xi) are those defined above as lexical and syntactic features, while the factor \u03a6join(y, z, x) is defined as:\n\u03a6join(y, z, x) = exp( \u2211 j \u03b8joinj,y \u03c6 join j (z, x)) (5)\nwhere \u03c6joinj (z, x) is defined as:\n\u03c6joinj (z, x) = 1iff\u2203i : zi = 1&\u03c6 m j (zi, xi) = 1\n(6)\nThe feature \u03c6joinj denotes whether the feature \u03c6 m j is active for any of the relation mentions which are correct w.r.t Y .\nInference in this setting is primarily performed by Gibbs sampling. Learning on the other hand is done by SampleRank, which is a ranked based learning framework. Since each step of inference is usually the bottleneck in learning, SampleRank has been shown to be efficient in training for models in which inference is intractable.\nFinally, in this setting, an error reduction of 31% is observed in the NY Times dataset."}, {"heading": "4.4 MultiR (Hoffmann et al., 2011)", "text": "One of the issues in the model by (Riedel et al., 2010) is that it does not allow relations to overlap, i.e. for any pair of entities e1 and e2, there cannot exist two facts r(e1, e2) and q(e1, e2). This paper firther relaxes this assumption by constructing a graphical model as shown in Figure 3.\nThe other obvious change is in the conditional extraction model, which is now given by:\n(7) P (Y = y, Z = z|x) = 1\nZx \u220f r \u03a6join(yr, z) \u220f i \u03a6extract(zi, xi)\nwhere the functions are the same as those defined above.\nLearning in this setting is done based on approximations which lead to Perceptron-style additive parameter updates (Collins, 2002). The first approximation is to do online learning instead of the full optimization. The second approximation is to replace expectations with maximizations (Viterbi approximation).\nIt was experimentally shown that this model has a better precision and recall as compared to the model proposed by (Riedel et al., 2010)."}, {"heading": "4.5 Multi-Instance Multi-Label (MIML) (Surdeanu et al., 2012)", "text": "This paper also tries to relax the assumption that each relations between entities can overlap. The plate model is shown in Figure 4. The model structure is very similar to that proposed by (Hoffmann et al., 2011), however training is done by Expectation Maximization (EM). In the E step, latent mention labels are assigned using the current model\nand in the M step, the model is retrained to maximize the log likelihood of the data using the current assignments.\nIt was observed that while this model performs better than the one proposed by (Riedel et al., 2010), it does not perform as good as (Hoffmann et al., 2011), especially around extremities. The authors however claim that this model is more stable as it yields a smoother curve."}, {"heading": "4.6 MIML-semi (Min et al., 2013)", "text": "This paper builds on the work by (Surdeanu et al., 2012) and shows that due to the incomplete nature of KBs, a significant number of negative examples generated are actually false negatives. They correct this problem by modelling the bag-level noise caused by incomplete KBs, in addition to modelling instance level noise using the MIML model. The plate diagram is shown in Figure 5. The input to the model is a list of n bags with labels Positive (P) or unlabelled (U) for each relation r. The model adds another set of latent variables, l, which models the true bag level labels, to combine the la-\nbels y and the MIML layers. Training is done by an EM algorithm similar to that proposed by (Surdeanu et al., 2012).\nThis model achieves a better performance than both MIML as well as MultiR on the KBP dataset.\nAnother interesting approach in modelling missing data is presented in (Ritter et al., 2013), which could not be studied due to lack of time. An interesting aspect of Distant Supervision was also presented in (Bunescu and Mooney, 2007), which primarily addresses the task of Multi Instance Learning as a supervised learning problem, solved by SVMs. A summary of the distant supervision techniques along with their key contributions is presented in Table 2."}, {"heading": "5 Conclusions", "text": "Relation Extraction is a particularly well studied problem in the NLP domain. In this paper, we focussed on one of the technique of Distant Supervision, which utilises a large Knowledge Base, such as Freebase to generate training examples for relational extraction. We reviewed some of the models in Distant Supervision, and presented a summary of some of the key contributions of each model."}], "references": [{"title": "Snowball: Extracting relations from large plain-text collections", "author": ["Eugene Agichtein", "Luis Gravano."], "venue": "Proceedings of the fifth ACM conference on Digital libraries. ACM.", "citeRegEx": "Agichtein and Gravano.,? 2000", "shortCiteRegEx": "Agichtein and Gravano.", "year": 2000}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD international conference on Management", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Extracting patterns and relations from the world wide web", "author": ["Sergey Brin."], "venue": "International Workshop on The World Wide Web and Databases. Springer.", "citeRegEx": "Brin.,? 1998", "shortCiteRegEx": "Brin.", "year": 1998}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Razvan Bunescu", "Raymond Mooney."], "venue": "ACL.", "citeRegEx": "Bunescu and Mooney.,? 2007", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2007}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["Razvan C Bunescu", "Raymond J Mooney."], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing. Association for Compu-", "citeRegEx": "Bunescu and Mooney.,? 2005", "shortCiteRegEx": "Bunescu and Mooney.", "year": 2005}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins."], "venue": "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Associa-", "citeRegEx": "Collins.,? 2002", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Constructing biological knowledge bases by extracting information from text sources", "author": ["Mark Craven", "Johan Kumlien"], "venue": "ISMB", "citeRegEx": "Craven and Kumlien,? \\Q1999\\E", "shortCiteRegEx": "Craven and Kumlien", "year": 1999}, {"title": "Dependency tree kernels for relation extraction", "author": ["Aron Culotta", "Jeffrey Sorensen."], "venue": "ACL. Association for Computational Linguistics.", "citeRegEx": "Culotta and Sorensen.,? 2004", "shortCiteRegEx": "Culotta and Sorensen.", "year": 2004}, {"title": "Identifying relations for open information extraction", "author": ["Anthony Fader", "Stephen Soderland", "Oren Etzioni."], "venue": "EMNLP. Association for Computational Linguistics.", "citeRegEx": "Fader et al\\.,? 2011", "shortCiteRegEx": "Fader et al\\.", "year": 2011}, {"title": "The yeast proteome database (ypd): a model for the organization and presentation of genome-wide functional data", "author": ["Peter E Hodges", "Andrew HZ McKee", "Brian P Davis", "William E Payne", "James I Garrels."], "venue": "Nucleic Acids Research .", "citeRegEx": "Hodges et al\\.,? 1999", "shortCiteRegEx": "Hodges et al\\.", "year": 1999}, {"title": "Knowledgebased weak supervision for information extraction of overlapping relations", "author": ["Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computa-", "citeRegEx": "Hoffmann et al\\.,? 2011", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A literature network of human genes for high-throughput analysis of gene expression", "author": ["Tor-Kristian Jenssen", "Astrid L\u00e6greid", "Jan Komorowski", "Eivind Hovig."], "venue": "Nature genetics .", "citeRegEx": "Jenssen et al\\.,? 2001", "shortCiteRegEx": "Jenssen et al\\.", "year": 2001}, {"title": "Overview of the tac 2010 knowledge base population track", "author": ["Heng Ji", "Ralph Grishman", "Hoa Trang Dang", "Kira Griffitt", "Joe Ellis."], "venue": "Third Text Analysis Conference (TAC 2010).", "citeRegEx": "Ji et al\\.,? 2010", "shortCiteRegEx": "Ji et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction with an incomplete knowledge base", "author": ["Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek."], "venue": "HLT-NAACL. pages 777\u2013782.", "citeRegEx": "Min et al\\.,? 2013", "shortCiteRegEx": "Min et al\\.", "year": 2013}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky."], "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference", "citeRegEx": "Mintz et al\\.,? 2009", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "Overview of the tac 2011 summarization track: Guided task and aesop task", "author": ["Karolina Owczarzak", "Hoa Trang Dang."], "venue": "Proceedings of the Text Analysis Conference (TAC 2011), Gaithersburg, Maryland, USA, November.", "citeRegEx": "Owczarzak and Dang.,? 2011", "shortCiteRegEx": "Owczarzak and Dang.", "year": 2011}, {"title": "Unsupervised training of bayesian networks for data clustering", "author": ["Duc Truong Pham", "Gonzalo A Ruz."], "venue": "Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. The Royal Society.", "citeRegEx": "Pham and Ruz.,? 2009", "shortCiteRegEx": "Pham and Ruz.", "year": 2009}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Sebastian Riedel", "Limin Yao", "Andrew McCallum."], "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer.", "citeRegEx": "Riedel et al\\.,? 2010", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Modeling missing data in distant supervision for information extraction. Transactions of the Association for Computational Linguistics", "author": ["Alan Ritter", "Luke Zettlemoyer", "Oren Etzioni"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2013}, {"title": "Semimarkov conditional random fields for information extraction", "author": ["Sunita Sarawagi", "William W Cohen"], "venue": null, "citeRegEx": "Sarawagi and Cohen,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi and Cohen", "year": 2004}, {"title": "Open language learning for information extraction", "author": ["Michael Schmitz", "Robert Bart", "Stephen Soderland", "Oren Etzioni"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Schmitz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schmitz et al\\.", "year": 2012}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning."], "venue": "Proceedings of the 2012 joint conference on empirical methods in natural language processing and compu-", "citeRegEx": "Surdeanu et al\\.,? 2012", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Relation classification via convolutional deep neural network. In COLING", "author": ["Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 2, "context": "This was introduced for the first time in DIPRE (Brin, 1998), and later extended in Snowball (Agichtein and Gravano, 2000).", "startOffset": 48, "endOffset": 60}, {"referenceID": 0, "context": "This was introduced for the first time in DIPRE (Brin, 1998), and later extended in Snowball (Agichtein and Gravano, 2000).", "startOffset": 93, "endOffset": 122}, {"referenceID": 7, "context": "Some interesting approaches in this domain have used tree kernels built on dependency parse trees (Culotta and Sorensen, 2004), training semi-CRFs (Sarawagi et al.", "startOffset": 98, "endOffset": 126}, {"referenceID": 4, "context": ", 2004), and using shortest path tree kernels (Bunescu and Mooney, 2005).", "startOffset": 46, "endOffset": 72}, {"referenceID": 8, "context": "0, which is a further improvement on Reverb (Fader et al., 2011) and Ollie (Schmitz et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 21, "context": ", 2011) and Ollie (Schmitz et al., 2012).", "startOffset": 18, "endOffset": 40}, {"referenceID": 15, "context": ", 1999), but has been successfully extended to any texts (Mintz et al., 2009).", "startOffset": 57, "endOffset": 77}, {"referenceID": 13, "context": "Deep Learning Based Methods: These techniques are relatively new and utilize the word embeddings generated by word2vec (Mikolov et al., 2013).", "startOffset": 119, "endOffset": 141}, {"referenceID": 23, "context": "One of the most successful atttempts in this domain, (Zeng et al., 2014) uses Convolution Neural Networks (CNNs) for relational extraction.", "startOffset": 53, "endOffset": 72}, {"referenceID": 14, "context": "We define the task of distant supervision formally in the notation given by (Min et al., 2013): Given a knowledge base (KB),D, a set of relations R, the KB D contains tuples of the form r(e1, e2), where r \u2208 R, and e1 and e2 are entities known to be related by the relation r.", "startOffset": 76, "endOffset": 94}, {"referenceID": 12, "context": "Subsequently, evaluations have been performed on Wikipedia, NY Times corpus and more recently on the 2010 and 2011 KBP shared tasks (Ji et al., 2010; Owczarzak and Dang, 2011).", "startOffset": 132, "endOffset": 175}, {"referenceID": 16, "context": "Subsequently, evaluations have been performed on Wikipedia, NY Times corpus and more recently on the 2010 and 2011 KBP shared tasks (Ji et al., 2010; Owczarzak and Dang, 2011).", "startOffset": 132, "endOffset": 175}, {"referenceID": 9, "context": "Yeast Protein Database(YPD) (Hodges et al., 1999), PubMed, MEDLINE: The work by (Craven et al.", "startOffset": 28, "endOffset": 49}, {"referenceID": 15, "context": "This is the main dataset used by (Mintz et al., 2009) for Distant Supervision.", "startOffset": 33, "endOffset": 53}, {"referenceID": 18, "context": "NY Times: This dataset was developed by (Riedel et al., 2010) by aligning Freebase relations with the NY Times dataset.", "startOffset": 40, "endOffset": 61}, {"referenceID": 22, "context": "KBP: This dataset was primarily used for evaluation by (Surdeanu et al., 2012).", "startOffset": 55, "endOffset": 78}, {"referenceID": 12, "context": "The training relations here are a were generated from the 2010 and 2011 KBP shared tasks (Ji et al., 2010; Owczarzak and Dang, 2011), which is a subset of Wikipedia Infoboxes from 2008.", "startOffset": 89, "endOffset": 132}, {"referenceID": 16, "context": "The training relations here are a were generated from the 2010 and 2011 KBP shared tasks (Ji et al., 2010; Owczarzak and Dang, 2011), which is a subset of Wikipedia Infoboxes from 2008.", "startOffset": 89, "endOffset": 132}, {"referenceID": 1, "context": "Most Distant Supervision approaches typically use Freebase (Bollacker et al., 2008) as the KB for the task.", "startOffset": 59, "endOffset": 83}, {"referenceID": 15, "context": "Table 1: 10 largest Freebase relations (Mintz et al., 2009) Relation name Size Example", "startOffset": 39, "endOffset": 59}, {"referenceID": 11, "context": "Since there were no large scale general knowledge databases available around that time, while this model was used successfully in domain specific contexts such as genetics (Jenssen et al., 2001), it wasn\u2019t until when large scale KBs such as Freebase became available that this technique started becoming relevant.", "startOffset": 172, "endOffset": 194}, {"referenceID": 9, "context": "The Distant Supervision model, the authors consider the Yeast Protein Database (Hodges et al., 1999), which includes a subcellular-localization field for many proteins, as well as a hyperlink to the reference article in PubMed.", "startOffset": 79, "endOffset": 100}, {"referenceID": 17, "context": "The sentences that do not mention the relational instance are treated as negative training exFigure 1: Naive Bayes Model (Pham and Ruz, 2009)", "startOffset": 121, "endOffset": 141}, {"referenceID": 15, "context": "2 Distant Supervision using Freebase (Mintz et al., 2009)", "startOffset": 37, "endOffset": 57}, {"referenceID": 18, "context": "3 Multi Instance Learning (MIL) (Riedel et al., 2010)", "startOffset": 32, "endOffset": 53}, {"referenceID": 18, "context": "The Figure 2 (Riedel et al., 2010) shows an instance for the Factor Graph model.", "startOffset": 13, "endOffset": 34}, {"referenceID": 18, "context": "Figure 2: Factor Graph Model (Riedel et al., 2010)", "startOffset": 29, "endOffset": 50}, {"referenceID": 10, "context": "4 MultiR (Hoffmann et al., 2011)", "startOffset": 9, "endOffset": 32}, {"referenceID": 18, "context": "One of the issues in the model by (Riedel et al., 2010) is that it does not allow relations to overlap, i.", "startOffset": 34, "endOffset": 55}, {"referenceID": 10, "context": "Figure 3: (a) Plate model of the network (b) an example network for entities Steve Jobs, Apple (Hoffmann et al., 2011)", "startOffset": 95, "endOffset": 118}, {"referenceID": 22, "context": "Figure 4: MIML plate model (Surdeanu et al., 2012)", "startOffset": 27, "endOffset": 50}, {"referenceID": 5, "context": "Learning in this setting is done based on approximations which lead to Perceptron-style additive parameter updates (Collins, 2002).", "startOffset": 115, "endOffset": 130}, {"referenceID": 18, "context": "It was experimentally shown that this model has a better precision and recall as compared to the model proposed by (Riedel et al., 2010).", "startOffset": 115, "endOffset": 136}, {"referenceID": 22, "context": "5 Multi-Instance Multi-Label (MIML) (Surdeanu et al., 2012)", "startOffset": 36, "endOffset": 59}, {"referenceID": 10, "context": "The model structure is very similar to that proposed by (Hoffmann et al., 2011), however training is done by Expectation Maximization (EM).", "startOffset": 56, "endOffset": 79}, {"referenceID": 14, "context": "In the E step, latent mention labels are assigned using the current model Figure 5: MIML-semi plate model (Min et al., 2013)", "startOffset": 106, "endOffset": 124}, {"referenceID": 18, "context": "It was observed that while this model performs better than the one proposed by (Riedel et al., 2010), it does not perform as good as (Hoffmann et al.", "startOffset": 79, "endOffset": 100}, {"referenceID": 10, "context": ", 2010), it does not perform as good as (Hoffmann et al., 2011), especially around extremities.", "startOffset": 40, "endOffset": 63}, {"referenceID": 14, "context": "6 MIML-semi (Min et al., 2013)", "startOffset": 12, "endOffset": 30}, {"referenceID": 22, "context": "This paper builds on the work by (Surdeanu et al., 2012) and shows that due to the incomplete nature of KBs, a significant number of negative examples generated are actually false negatives.", "startOffset": 33, "endOffset": 56}, {"referenceID": 22, "context": "Training is done by an EM algorithm similar to that proposed by (Surdeanu et al., 2012).", "startOffset": 64, "endOffset": 87}, {"referenceID": 19, "context": "Another interesting approach in modelling missing data is presented in (Ritter et al., 2013), which could not be studied due to lack of time.", "startOffset": 71, "endOffset": 92}, {"referenceID": 3, "context": "An interesting aspect of Distant Supervision was also presented in (Bunescu and Mooney, 2007), which primarily addresses the task of Multi Instance Learning as a supervised learning problem, solved by SVMs.", "startOffset": 67, "endOffset": 93}], "year": 2017, "abstractText": "Relation Extraction refers to the task of populating a database with tuples of the form r(e1, e2), where r is a relation and e1, e2 are entities. Distant supervision is one such technique which tries to automatically generate training examples based on an existing KB such as Freebase. This paper is a survey of some of the techniques in distant supervision which primarily rely on Probabilistic Graphical Models (PGMs).", "creator": "LaTeX with hyperref package"}}}