{"id": "1705.03919", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2017", "title": "A Minimal Span-Based Neural Constituency Parser", "abstract": "in this work, we present a single neural model or constituency strings based onto sampling functions of distances within spans. we establish generally this model is rather only usable than distributed algorithms programming techniques, but also admits providing simple hybrid top - down inference code based sequential recursive partitioning of paired input. many add indeed that both search schemes are integrated with recent developments, and are combined with basic extensions experiments the scoring model denies actually reaching reaching state - of - the - art single - model proof on the penn treebank ( 42. 79 f1 ) and strong equivalence on the french treebank ( 38. 23 f1 ).", "histories": [["v1", "Wed, 10 May 2017 18:44:15 GMT  (30kb)", "http://arxiv.org/abs/1705.03919v1", "To appear in ACL 2017"]], "COMMENTS": "To appear in ACL 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mitchell stern", "jacob andreas", "dan klein"], "accepted": true, "id": "1705.03919"}, "pdf": {"name": "1705.03919.pdf", "metadata": {"source": "CRF", "title": "A Minimal Span-Based Neural Constituency Parser", "authors": ["Mitchell Stern", "Jacob Andreas Dan Klein"], "emails": ["mitchell@cs.berkeley.edu", "jda@cs.berkeley.edu", "klein@cs.berkeley.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n03 91\n9v 1\n[ cs\n.C L\n] 1\n0 M\nay 2\n01 7\nmodel for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1)."}, {"heading": "1 Introduction", "text": "This paper presents a minimal but surprisingly effective span-based neural model for constituency parsing. Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences (Vinyals et al., 2015). Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).\nThere are two general approaches to ensuring this structural consistency. The most common is to encode the output as a sequence of operations within a transition system which constructs trees incrementally. This transforms the parsing problem back into a sequence-to-sequence problem, while making it easy to force the decoder to take only actions guaranteed to produce well-formed\noutputs. However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (Thang et al., 2015). Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding (Wiseman and Rush, 2016).\nAn alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015). These models enjoy a number of appealing formal properties, including support for exact inference and structured loss functions. However, previous chart-based approaches have required considerable scaffolding beyond a simple well-formedness potential, e.g. pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model (Hall et al., 2014). Additionally, we are unaware of any recent chartbased models that achieve results competitive with the best transition-based models.\nIn this work, we present an extremely simple chart-based neural parser based on independent scoring of labels and spans, and show how this model can be adapted to support a greedy topdown decoding procedure. Our goal is to preserve the basic algorithmic properties of span-oriented (rather than transition-oriented) parse representations, while exploring the extent to which neural representational machinery can replace the additional structure required by existing chart parsers. On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing\u2014including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)\u2014achieving an F1 score of 91.79. We additionally obtain a strong F1 score of 82.23 on the French Treebank."}, {"heading": "2 Model", "text": "A constituency tree can be regarded as a collection of labeled spans over a sentence. Taking this view as a guiding principle, we propose a model with two components, one which assigns scores to span labels and one which assigns scores directly to span existence. The former is used to determine the labeling of the output, and the latter provides its structure.\nAt the core of both of these components is the issue of span representation. Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015)\nIn particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively. Our representation of the span (i, j) is then the concatenatation the vector differences fj \u2212 fi and bi \u2212 bj . This corresponds to a bidirectional version of the LSTMMinus features first proposed by Wang and Chang (2016).\nOn top of this base, our label and span scoring functions are implemented as one-layer feedforward networks, taking as input the concatenated span difference and producing as output either a vector of label scores or a single span score. More formally, letting sij denote the vector representation of span (i, j), we define\nslabels(i, j) = V\u2113g(W\u2113sij + b\u2113),\nsspan(i, j) = v \u22a4 s g(Wssij + bs),\nwhere g denotes an elementwise nonlinearity. For notational convenience, we also let the score of an individual label \u2113 be denoted by\nslabel(i, j, \u2113) = [slabels(i, j)]\u2113,\nwhere the right-hand side is the corresponding element of the label score vector.\nOne potential issue is the existence of unary chains, corresponding to nested labeled spans with the same endpoints. We take the common approach of treating these as additional atomic labels alongside all elementary nonterminals. To accommodate n-ary trees, our inventory additionally includes a special empty label \u2205 used for spans that\nare not themselves full constituents but arise during the course of implicit binarization.\nOur model shares several features in common with that of Cross and Huang (2016). In particular, our representation of spans and the form of our label scoring function were directly inspired by their work, as were our handling of unary chains and our use of an empty label. However, our approach differs in its treatment of structural decisions, and consequently, the inference algorithms we describe below diverge significantly from their transition-based framework."}, {"heading": "3 Chart Parsing", "text": "Our basic model is compatible with traditional chart-based dynamic programming. Representing a constituency tree T by its labeled spans,\nT := {(\u2113t, (it, jt)) : t = 1, . . . , |T |},\nwe define the score of a tree to be the sum of its constituent label and span scores, stree(T ) = \u2211\n(\u2113,(i,j))\u2208T\n[slabel(i, j, \u2113) + sspan(i, j)] .\nTo find the tree with the highest score for a given sentence, we use a modified CKY recursion. As with classical chart parsing, the running time of our procedure is O(n3) for a sentence of length n."}, {"heading": "3.1 Dynamic Program for Inference", "text": "The base case is a span (i, i + 1) consisting of a single word. Since every valid tree must include all singleton spans, possibly with empty labels, we need not consider the span score in this case and perform only a single maximization over the choice of label:\nsbest(i, i+ 1) = max \u2113 [slabel(i, i+ 1, \u2113)] .\nFor a general span (i, j), we define the score of the split (i, k, j) as the sum of its subspan scores,\nssplit(i, k, j) = sspan(i, k) + sspan(k, j). (1)\nFor convenience, we also define an augmented split score incorporating the scores of the corresponding subtrees,\ns\u0303split(i, k, j) = ssplit(i, k, j)\n+ sbest(i, k) + sbest(k, j).\nUsing these quantities, we can then write the general joint label and split decision as\nsbest(i, j) = max \u2113,k [slabel(i, j, \u2113) + s\u0303split(i, k, j)] .\n(2)\nBecause our model assigns independent scores to labels and spans, this maximization decomposes into two disjoint subproblems, greatly reducing the size of the state space:\nsbest(i, j) = max \u2113 [slabel(i, j, \u2113)]\n+ max k [s\u0303split(i, k, j)] .\nWe also note that the span scores sspan(i, j) for each span (i, j) in the sentence can be computed once at the beginning of the procedure and shared across different subproblems with common left or right endpoints, allowing for a quadratic rather than cubic number of span score computations."}, {"heading": "3.2 Margin Training", "text": "Training the model under this inference scheme is accomplished using a margin-based approach. When presented with an example sentence and its corresponding parse tree T \u2217, we compute the best prediction under the current model using the above dynamic program,\nT\u0302 = argmax T [stree(T )] .\nIf T\u0302 = T \u2217, then our prediction was correct and no changes need to be made. Otherwise, we incur a hinge penalty of the form\nmax ( 0, 1\u2212 stree(T \u2217) + stree(T\u0302 ) )\nto encourage the model to keep a margin of at least 1 between the gold tree and the best alternative. The loss to be minimized is then the sum of penalties across all training examples.\nPrior work has found that it can be beneficial in a variety of applications to incorporate a structured loss function into this margin objective, replacing the hinge penalty above with one of the form\nmax ( 0, \u2206(T\u0302 , T \u2217)\u2212 stree(T \u2217) + stree(T\u0302 ) )\nfor a loss function \u2206 that measures the similarity between the prediction T\u0302 and the reference T \u2217. Here we take \u2206 to be a Hamming loss on labeled spans. To incorporate this loss into the training objective, we modify the dynamic program of\nSection 3.1 to support loss-augmented decoding (Taskar et al., 2005). Since the label decisions are isolated from the structural decisions, it suffices to replace every occurrence of the label scoring function slabel(i, j, \u2113) by\nslabel(i, j, \u2113) + 1(\u2113 6= \u2113 \u2217 ij),\nwhere \u2113\u2217ij is the label of span (i, j) in the gold tree T \u2217. This has the effect of requiring larger margins between the gold tree and predictions that contain more mistakes, offering a greater degree of robustness and better generalization."}, {"heading": "4 Top-Down Parsing", "text": "While we have so far motivated our model from the perspective of classical chart parsing, it also allows for a novel inference algorithm in which trees are constructed greedily from the top down. At a high level, given a span, we independently assign it a label and pick a split point, then repeat this process for the left and right subspans; the recursion bottoms out with length-one spans that can no longer be split. Figure 1 gives an illustration of the process, which we describe in more detail below.\nThe base case is again a singleton span (i, i+1), and follows the same form as the base case for the chart parser. In particular, we select the label \u2113\u0302 that satisfies\n\u2113\u0302 = argmax \u2113 [slabel(i, i + 1, \u2113)] ,\nomitting span scores from consideration since singleton spans cannot be split.\nTo construct a tree over a general span (i, j), we aim to solve the maximization problem\n(\u2113\u0302, k\u0302) = argmax \u2113,k [slabel(i, j, \u2113) + ssplit(i, k, j)] ,\nwhere ssplit(i, k, j) is defined as in Equation (1). The independence of our label and span scoring functions again yields the decomposed form\n\u2113\u0302 = argmax \u2113 [slabel(i, j, \u2113)] ,\nk\u0302 = argmax k\n[ssplit(i, k, j)] , (3)\nleading to a significant reduction in the size of the state space.\nTo generate a tree for the whole sentence, we call this procedure on the full sentence span (0, n) and return the result. As there areO(n) spans each\nrequiring one label evaluation and at most n \u2212 1 split point evaluations, the running time of the procedure is O(n2).\nThe algorithm outlined here bears a strong resemblance to the chart parsing dynamic program discussed in Section 3, but differs in one key aspect. When performing inference from the bottom up, we have already computed the scores of all of the subtrees below the current span, and we can take this knowledge into consideration when selecting a split point. In contrast, when producing a tree from the top down, we can only select a split point based on top-level evaluations of span quality, without knowing anything about the subtrees that will be generated below them. This difference is manifested in the augmented split score s\u0303split used in the definition of sbest in Equation (2), where the scores of the subtrees associated with a split point are included in the chart recursion but necessarily excluded from the top-down recursion.\nWhile this apparent deficiency may be a cause for concern, we demonstrate the surprising empirical result in Section 6 that there is no loss in per-\nformance when moving from the globally-optimal chart parser to the greedy top-down procedure."}, {"heading": "4.1 Margin Training", "text": "As with the chart parsing formulation, we also use a margin-based method for learning under the topdown model. However, rather than requiring separation between the scores of full trees, we instead enforce a local margin at every decision point.\nFor a span (i, j) occurring in the gold tree, let \u2113\u2217 and k\u2217 represent the correct label and split point, and let \u2113\u0302 and k\u0302 be the predictions made by computing the maximizations in Equation (3). If \u2113\u0302 6= \u2113\u2217, meaning the prediction is incorrect, we incur a hinge penalty of the form\nmax ( 0, 1 \u2212 slabel(i, j, \u2113 \u2217) + slabel(i, j, \u2113\u0302) ) .\nSimilarly, if k\u0302 6= k\u2217, we incur a hinge penalty of the form\nmax ( 0, 1 \u2212 ssplit(i, k \u2217, j) + ssplit(i, k\u0302, j) ) .\nTo obtain the loss for a given training example, we trace out the actions corresponding to the gold tree and accumulate the above penalties over all decision points. As before, the total loss to be minimized is the sum of losses across all training examples.\nLoss augmentation is also beneficial for the local decisions made by the top-down model, and can be implemented in a manner akin to the one discussed in Section 3.2."}, {"heading": "4.2 Training with Exploration", "text": "The hinge penalties given above are only defined for spans (i, j) that appear in the example tree. The model must therefore be constrained at training time to follow decisions that exactly reproduce the gold tree, since supervision cannot be provided otherwise. As a result, the model is never exposed to its mistakes, which can lead to a lack of calibration and poor performance at test time.\nTo circumvent this issue, a dynamic oracle can be defined to inform the model about correct behavior even after it has deviated from the gold tree. Cross and Huang (2016) propose such an oracle for a related transition-based parsing system, and prove its optimality for the F1 metric on labeled spans. We adapt their result here to obtain a dynamic oracle for the present model with similar guarantees.\nThe oracle for labeling decisions carries over without modification: the correct label for a span is the label assigned to that span if it is part of the gold tree, or the empty label \u2205 otherwise.\nFor split point decisions, the oracle can be broken down into two cases. If a span (i, j) appears as a constituent in the gold tree T , we let b(i, j) denote the collection of its interior boundary points. For example, if the constituent over (1, 7) has children spanning (1, 3), (3, 6), and (6, 7), then we would have the two interior boundary points, b(1, 7) = {3, 6}. The oracle for a span appearing in the gold tree is then precisely the output of this function. Otherwise, for spans (i, j) not corresponding to gold constituents, we must instead identify the smallest enclosing gold constituent:\n(i\u2217, j\u2217) = min{(i\u2032, j\u2032) \u2208 T : i\u2032 \u2264 i < j \u2264 j\u2032},\nwhere the minimum is taken with respect to the partial ordering induced by span length. The output of the oracle is then the set of interior boundary points of this enclosing span that also lie inside the original, {k \u2208 b(i\u2217, j\u2217) : i < k < j}.\nThe proof of correctness is similar to the proof in Cross and Huang (2016); we refer to the Dynamic Oracle section in their paper for a more detailed discussion.\nAs presented, the dynamic oracle for split point decisions returns a collection of one or more splits rather than a single correct answer. Any of these is a valid choice, with different splits corresponding to different binarizations of the original n-ary tree. We choose to use the leftmost split point for consistency in our implementation, but remark that the oracle split with the highest score could also be chosen at training time to allow for additional flexibility.\nHaving defined the dynamic oracle for our system, we note that training with exploration can be implemented by a single modification to the procedure described in Section 4.1. Local penalties are accumulated as before, but instead of tracing out the decisions required to produce the gold tree, we instead follow the decisions predicted by the model. In this way, supervision is provided at states within the prediction procedure that are more likely to arise at test time when greedy inference is performed."}, {"heading": "5 Scoring and Loss Alternatives", "text": "The model presented in Section 2 is designed to be as simple as possible. However, there are many variations of the label and span scoring functions that could be explored; we discuss some of the options here."}, {"heading": "5.1 Top-Middle-Bottom Label Scoring", "text": "Our basic model treats the empty label, elementary nonterminals, and unary chains each as atomic units, obscuring similarities between unary chains and their component nonterminals or between different unary chains with common prefixes or suffixes. To address this lack of structure, we consider an alternative scoring scheme in which labels are predicted in three parts: a top nonterminal, a middle unary chain, and a bottom nonterminal (each of which is possibly empty).1 This not only allows for parameter sharing across labels with common subcomponents, but also has the added benefit of allowing the model to produce novel unary chains at test time.\n1In more detail, \u2205 decomposes as (\u2205, \u2205, \u2205), X decomposes as (X , \u2205, \u2205), X\u2013Y decomposes as (X , \u2205, Y ), and X\u2013Z1\u2013 \u00b7 \u00b7 \u00b7 \u2013Zk\u2013Y decomposes as (X , Z1\u2013 \u00b7 \u00b7 \u00b7 \u2013Zk , Y ).\nMore precisely, we introduce the decomposition\nslabel(i, j, (\u2113t, \u2113m, \u2113b)) =\nstop(i, j, \u2113t) + smiddle(i, j, \u2113m) + sbottom(i, j, \u2113b),\nwhere stop, smiddle, and sbottom are independent one-layer feedforward networks of the same form as slabel that output scores for all label tops, label middle chains, and label bottoms encountered in the training corpus, respectively. The best label for a span (i, j) is then computed by solving the maximization problem\nmax \u2113t,\u2113m,\u2113b [slabel(i, j, (\u2113t, \u2113m, \u2113b))] ,\nwhich decomposes into three independent subproblems corresponding to the three label components. The final label is obtained by concatenating \u2113t, \u2113m, and \u2113b, with empty components being omitted from the concatenation."}, {"heading": "5.2 Left and Right Span Scoring", "text": "The basic model uses the same span scoring function sspan to assign a score to the left and right subspans of a given span. One simple extension is to replace this by a pair of distinct left and right feedforward networks of the same form, giving the decomposition\nssplit(i, k, j) = sleft(i, k) + sright(k, j)."}, {"heading": "5.3 Span Concatenation Scoring", "text": "Since span scores are only used to score splits in our model, we also consider directly scoring a split by feeding the concatenation of the span representations of the left and right subspans through a single feedforward network, giving\nssplit(i, k, j) = v \u22a4 s g (Ws[sik; skj] + bs) .\nThis is similar to the structural scoring function used by Cross and Huang (2016), although whereas they additionally include features for the outside spans (0, i) and (j, n) in their concatenation, we omit these from our implementation, finding that they do not improve performance."}, {"heading": "5.4 Deep Biaffine Span Scoring", "text": "Inspired by the success of deep biaffine scoring in recent work by Dozat and Manning (2016) for dependency parsing, we also consider a split scoring function of a similar form for our model. Specifically, we let hik = fleft(sik) and hkj =\nfright(skj) be deep left and right span representations obtained by passing the child vectors through corresponding left and right feedforward networks. We then define the biaffine split scoring function\nssplit(i, k, j) = h \u22a4 ikWshkj + v \u22a4 lefthik + v \u22a4 righthkj ,\nwhich consists of the sum of a bilinear form between the two hidden representations together with two inner products."}, {"heading": "5.5 Structured Label Loss", "text": "The three-way label scoring scheme described in Section 5.1 offers one path towards the incorporation of label structure into the model. We additionally consider a structured Hamming loss on labels. More specifically, given two labels \u21131 and \u21132 consisting of zero or more nonterminals, we define the loss as |\u21131 \\ \u21132|+ |\u21132 \\ \u21131|, treating each label as a multiset of nonterminals. This structured loss can be incorporated into the training process using the methods described in Sections 3.2 and 4.1."}, {"heading": "6 Experiments", "text": "We first describe the general setup used for our experiments. We use the Penn Treebank (Marcus et al., 1993) for our English experiments, with standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing. We use the French Treebank from the SPMRL 2014 shared task (Seddah et al., 2014) with its provided splits for our French experiments. No token preprocessing is performed, and only a single <UNK> token is used for unknown words at test time. The inputs to our system are concatenations of 100-dimensional word embeddings and 50-dimensional part-of-speech embeddings. In the case of the French Treebank, we also include 50-dimensional embeddings of each morphological tag. We use automatically predicted tags for training and testing, obtaining predicted part-ofspeech tags for the Penn Treebank using the Stanford tagger (Toutanova et al., 2003) with 10-way jackknifing, and using the provided predicted partof-speech and morphological tags for the French Treebank. Words are replaced by <UNK> with probability 1/(1+freq(w)) during training, where freq(w) is the frequency of w in the training data.\nWe use a two-layer bidirectional LSTM for our base span features. Dropout with a ratio selected from {0.2, 0.3, 0.4} is applied to all non-recurrent\nconnections of the LSTM, including its inputs and outputs. We tie the hidden dimension of the LSTM and all feedforward networks, selecting a size from {150, 200, 250}. All parameters (including word and tag embeddings) are randomly initialized using Glorot initialization (Glorot and Bengio, 2010), and are tuned on development set performance. We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 10. Our system is implemented in C++ using the DyNet neural network library (Neubig et al., 2017).\nWe begin by training the minimal version of our proposed chart and top-down parsers on the Penn Treebank. Out of the box, we obtain test F1 scores of 91.69 for the chart parser and 91.58 for the topdown parser. The higher of these matches the recent state-of-the-art score of 91.7 reported by Liu and Zhang (2016), demonstrating that our simple neural parsing system is already capable of achieving strong results.\nBuilding on this, we explore the effects of different split scoring functions when using either the basic 0-1 label loss or the structured label loss discussed in Section 5.5. Our results are presented in Tables 1a and 1b.\nWe observe that regardless of the label loss, the minimal and deep biaffine split scoring schemes perform a notch below the left-right and concatenation scoring schemes. That the minimal scoring scheme performs worse than the left-right scheme is unsurprising, since the latter is a strict generalization of the former. It is evident, however,\nthat joint scoring of left and right subspans is not required for strong results\u2014in fact, the left-right scheme which scores child subspans in isolation slightly outperforms the concatenation scheme in all but one case, and is stronger than the deep biaffine scoring function across the board.\nComparing results across the choice of label loss, however, we find that fewer trends are apparent. The scores obtained by training with a 0-1 loss are all within 0.1 of those obtained using a structured Hamming loss, being slightly higher in four out of eight cases and slightly lower in the other half. This leads us to conclude that the more elementary approach is sufficient when selecting atomic labels from a fixed inventory.\nWe also perform the same set of experiments under the setting where the top-middle-bottom label scoring function described in Section 5.1 is used in place of an atomic label scoring function. These results are shown in Tables 1c and 1d.\nA priori, we might expect that exposing additional structure would allow the model to make better predictions, but on the whole we find that the scores in this set of experiments are worse than those in the previous set. Trends similar to before hold across the different choices of scoring functions, though in this case the minimal setting has scores closer to those of the left-right setting, even exceeding its performance in the case of a chart parser with a 0-1 label loss.\nOur final test results are given in Table 2, along with the results of other recent single-model parsers trained without external parse data. We\nFinal Parsing Results on Penn Treebank\nFinal Parsing Results on French Treebank\nachieve a new state-of-the-art F1 score of 91.79 with our best model. Interestingly, we observe that our parsers have a noticeably higher gap between precision and recall than do other top parsers, perhaps in part owing to the structured label loss which penalizes mismatching nonterminals more heavily than it does a nonterminal and empty label mismatch. In addition, there is little difference between the best top-down model and the best chart model, indicating that global normalization is not required to achieve strong results. Processing one sentence at a time on a c4.4xlarge Amazon EC2 instance, our best chart and top-down parsers operate at speeds of 20.3 sentences per second and 75.5 sentences per second, respectively, as measured on the test set.\nWe additionally train parsers on the French Treebank using the same settings from our English experiments, selecting the best model of each type based on development performance. We list our test results along with those of several other recent papers in Table 3. Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015)."}, {"heading": "7 Related Work", "text": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007). By contrast, the approach we have described here continues a recent line of work on direct modeling of correlations in the input space, by using rich feature representations to parameterize local potentials that interact with a comparatively unconstrained structured decoder. As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).\nThe best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures. Our approach differs from other recent chart-based neural models (e.g. Durrett and Klein (2015)) in the use of a recurrent input representation, structured loss function, and comparatively simple parameterization of the scoring function. In addition to the globally optimal decoding procedures for which these models were designed, and in contrast to the left-to-right decoder typically employed by transition-based models, our model admits an additional greedy top-to-bottom inference procedure."}, {"heading": "8 Conclusion", "text": "We have presented a minimal span-oriented parser that uses a recurrent input representation to score\ntrees with a sum of independent potentials on their constituent spans and labels. Our model supports both exact chart-based decoding and a novel top-down inference procedure. Both approaches achieve state-of-the-art performance on the Penn Treebank, and our best model achieves competitive performance on the French Treebank. Our experiments show that many of the key insights from recent neural transition-based approaches to parsing can be easily ported to the chart parsing setting, resulting in a pair of extremely simple models that nonetheless achieve excellent performance."}, {"heading": "Acknowledgments", "text": "Wewould like to thank Nick Altieri and the anonymous reviewers for their valuable comments and suggestions. MS is supported by an NSF Graduate Research Fellowship. JA is supported by a Facebook graduate fellowship and a Berkeley AI / Huawei fellowship."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Training with exploration improves a greedy stack-lstm parser", "author": ["Miguel Ballesteros", "Yoav Goldberg", "Chris Dyer", "Noah A Smith."], "venue": "arXiv preprint arXiv:1603.03793 .", "citeRegEx": "Ballesteros et al\\.,? 2016", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2016}, {"title": "The imswroc\u0142aw-szeged-cis entry at the spmrl 2014 shared task: Reranking and morphosyntax meet unlabeled", "author": ["Anders Bj\u00f6rkelund", "Ozlem Cetinoglu", "Agnieszka Falenska", "Rich\u00e1rd Farkas", "Thomas M\u00fcller", "Wolfgang Seeker", "Zsolt Sz\u00e1nt\u00f3"], "venue": null, "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2014}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP. pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Head-driven statistical models for natural language parsing", "author": ["Michael Collins."], "venue": "Computational linguistics 29(4):589\u2013637.", "citeRegEx": "Collins.,? 2003", "shortCiteRegEx": "Collins.", "year": 2003}, {"title": "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles", "author": ["James Cross", "Liang Huang."], "venue": "EMNLP.", "citeRegEx": "Cross and Huang.,? 2016", "shortCiteRegEx": "Cross and Huang.", "year": 2016}, {"title": "Deep biaffine attention for neural dependency parsing", "author": ["Timothy Dozat", "Christopher D. Manning."], "venue": "CoRR abs/1611.01734. http://arxiv.org/abs/1611.01734.", "citeRegEx": "Dozat and Manning.,? 2016", "shortCiteRegEx": "Dozat and Manning.", "year": 2016}, {"title": "Neural crf parsing", "author": ["Greg Durrett", "Dan Klein."], "venue": "arXiv preprint arXiv:1507.03641 .", "citeRegEx": "Durrett and Klein.,? 2015", "shortCiteRegEx": "Durrett and Klein.", "year": 2015}, {"title": "Recurrent neural network grammars", "author": ["Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A Smith."], "venue": "arXiv preprint arXiv:1602.07776 .", "citeRegEx": "Dyer et al\\.,? 2016", "shortCiteRegEx": "Dyer et al\\.", "year": 2016}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["Jenny Rose Finkel", "Alex Kleeman", "Christopher D Manning."], "venue": "ACL. volume 46, pages 959\u2013967.", "citeRegEx": "Finkel et al\\.,? 2008", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Training deterministic parsers with non-deterministic oracles", "author": ["Yoav Goldberg", "Joakim Nivre."], "venue": "Transactions of the association for Computational Linguistics 1:403\u2013414.", "citeRegEx": "Goldberg and Nivre.,? 2013", "shortCiteRegEx": "Goldberg and Nivre.", "year": 2013}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850 .", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Less grammar, more features", "author": ["David Leo Wright Hall", "Greg Durrett", "Dan Klein."], "venue": "ACL (1). pages 228\u2013237.", "citeRegEx": "Hall et al\\.,? 2014", "shortCiteRegEx": "Hall et al\\.", "year": 2014}, {"title": "Discriminative training of a neural network statistical parser", "author": ["James Henderson."], "venue": "Proceedings of the 42nd AnnualMeeting on Association for Computational Linguistics. Association for Computational Linguistics, page 95.", "citeRegEx": "Henderson.,? 2004", "shortCiteRegEx": "Henderson.", "year": 2004}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "arXiv preprint arXiv:1603.04351 .", "citeRegEx": "Kiperwasser and Goldberg.,? 2016", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Accurate unlexicalized parsing", "author": ["Dan Klein", "Christopher D Manning."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 423\u2013430.", "citeRegEx": "Klein and Manning.,? 2003", "shortCiteRegEx": "Klein and Manning.", "year": 2003}, {"title": "Shift-reduce constituent parsing with neural lookahead features", "author": ["Jiangming Liu", "Yue Zhang"], "venue": null, "citeRegEx": "Liu and Zhang.,? \\Q2016\\E", "shortCiteRegEx": "Liu and Zhang.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P. Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Comput. Linguist. 19(2):313\u2013330. http://dl.acm.org/citation.cfm?id=972470.972475.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin."], "venue": "arXiv preprint", "citeRegEx": "Kong et al\\.,? 2017", "shortCiteRegEx": "Kong et al\\.", "year": 2017}, {"title": "Improved inference for unlexicalized parsing", "author": ["Slav Petrov", "Dan Klein."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. Assocation for Computational", "citeRegEx": "Petrov and Klein.,? 2007", "shortCiteRegEx": "Petrov and Klein.", "year": 2007}, {"title": "Introducing the spmrl 2014 shared task on parsing morphologically-rich languages", "author": ["Djam\u00e9 Seddah", "Sandra K\u00fcbler", "Reut Tsarfaty."], "venue": "Proceedings of the First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and", "citeRegEx": "Seddah et al\\.,? 2014", "shortCiteRegEx": "Seddah et al\\.", "year": 2014}, {"title": "Learning structured prediction models: A large margin approach", "author": ["Ben Taskar", "Vassil Chatalbashev", "Daphne Koller", "Carlos Guestrin."], "venue": "Proceedings of the 22Nd International Conference on Machine Learning. ACM,", "citeRegEx": "Taskar et al\\.,? 2005", "shortCiteRegEx": "Taskar et al\\.", "year": 2005}, {"title": "Optimal shift-reduce constituent parsing with structured perceptron", "author": ["Le Quang Thang", "Hiroshi Noji", "Yusuke Miyao."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. volume 1, pages 1534\u20131544.", "citeRegEx": "Thang et al\\.,? 2015", "shortCiteRegEx": "Thang et al\\.", "year": 2015}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Kristina Toutanova", "Dan Klein", "Christopher D. Manning", "Yoram Singer."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the As-", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton."], "venue": "Advances in Neural Information Processing Systems. pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "A unified tagging solution: Bidirectional LSTM recurrent neural network with word embedding", "author": ["Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao."], "venue": "CoRR abs/1511.00215. http://arxiv.org/abs/1511.00215.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Graph-based dependency parsing with bidirectional LSTM", "author": ["Wenhui Wang", "Baobao Chang."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016,", "citeRegEx": "Wang and Chang.,? 2016", "shortCiteRegEx": "Wang and Chang.", "year": 2016}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.02960 .", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}], "referenceMentions": [{"referenceID": 26, "context": "Recent years have seen a great deal of interest in parsing architectures that make use of recurrent neural network (RNN) representations of input sentences (Vinyals et al., 2015).", "startOffset": 156, "endOffset": 178}, {"referenceID": 12, "context": "Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).", "startOffset": 139, "endOffset": 153}, {"referenceID": 3, "context": "Despite evidence that linear RNN decoders are implicitly able to respect some nontrivial well-formedness constraints on structured outputs (Graves, 2013), researchers have consistently found that the best performance is achieved by systems that explicitly require the decoder to generate well-formed tree structures (Chen and Manning, 2014).", "startOffset": 316, "endOffset": 340}, {"referenceID": 24, "context": "However, transition-based models do not admit fast dynamic programs and require careful feature engineering to support exact search-based inference (Thang et al., 2015).", "startOffset": 148, "endOffset": 168}, {"referenceID": 29, "context": "Moreover, models with recurrent state require complex training procedures to benefit from anything other than greedy decoding (Wiseman and Rush, 2016).", "startOffset": 126, "endOffset": 150}, {"referenceID": 9, "context": "An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015).", "startOffset": 190, "endOffset": 236}, {"referenceID": 7, "context": "An alternative line of work focuses on chart parsers, which use log-linear or neural scoring potentials to parameterize a tree-structured dynamic program for maximization or marginalization (Finkel et al., 2008; Durrett and Klein, 2015).", "startOffset": 190, "endOffset": 236}, {"referenceID": 13, "context": "pre-specification of a complete context-free grammar for generating output structures and initial pruning of the output space with a weaker model (Hall et al., 2014).", "startOffset": 146, "endOffset": 165}, {"referenceID": 5, "context": "On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing\u2014including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)\u2014achieving an F1 score of 91.", "startOffset": 158, "endOffset": 181}, {"referenceID": 5, "context": "On the Penn Treebank, our approach outperforms a number of recent models for chart-based and transition-based parsing\u2014including the state-ofthe-art models of Cross and Huang (2016) and Liu and Zhang (2016)\u2014achieving an F1 score of 91.", "startOffset": 158, "endOffset": 206}, {"referenceID": 0, "context": "Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively.", "startOffset": 321, "endOffset": 363}, {"referenceID": 27, "context": "Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively.", "startOffset": 321, "endOffset": 363}, {"referenceID": 0, "context": "Given that a span\u2019s correct label and its quality as a constituent depend heavily on the context in which it appears, we naturally turn to recurrent neural networks as a starting point, since they have previously been shown to capture contextual information suitable for use in a variety of natural language applications (Bahdanau et al., 2014; Wang et al., 2015) In particular, we run a bidirectional LSTM over the input to obtain context-sensitive forward and backward encodings for each position i, denoted by fi and bi, respectively. Our representation of the span (i, j) is then the concatenatation the vector differences fj \u2212 fi and bi \u2212 bj . This corresponds to a bidirectional version of the LSTMMinus features first proposed by Wang and Chang (2016). On top of this base, our label and span scoring functions are implemented as one-layer feedforward networks, taking as input the concatenated span difference and producing as output either a vector of label scores or a single span score.", "startOffset": 322, "endOffset": 759}, {"referenceID": 5, "context": "Our model shares several features in common with that of Cross and Huang (2016). In particular, our representation of spans and the form of our label scoring function were directly inspired by their work, as were our handling of unary chains and our use of an empty label.", "startOffset": 57, "endOffset": 80}, {"referenceID": 23, "context": "1 to support loss-augmented decoding (Taskar et al., 2005).", "startOffset": 37, "endOffset": 58}, {"referenceID": 5, "context": "Cross and Huang (2016) propose such an oracle for a related transition-based parsing system, and prove its optimality for the F1 metric on labeled spans.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "The proof of correctness is similar to the proof in Cross and Huang (2016); we refer to the Dynamic Oracle section in their paper for a more detailed discussion.", "startOffset": 52, "endOffset": 75}, {"referenceID": 5, "context": "This is similar to the structural scoring function used by Cross and Huang (2016), although whereas they additionally include features for the outside spans (0, i) and (j, n) in their concatenation, we omit these from our implementation, finding that they do not improve performance.", "startOffset": 59, "endOffset": 82}, {"referenceID": 6, "context": "Inspired by the success of deep biaffine scoring in recent work by Dozat and Manning (2016) for dependency parsing, we also consider a split scoring function of a similar form for our model.", "startOffset": 67, "endOffset": 92}, {"referenceID": 19, "context": "We use the Penn Treebank (Marcus et al., 1993) for our English experiments, with standard splits of sections 2-21 for training, section 22 for development, and section 23 for testing.", "startOffset": 25, "endOffset": 46}, {"referenceID": 22, "context": "We use the French Treebank from the SPMRL 2014 shared task (Seddah et al., 2014) with its provided splits for our French experiments.", "startOffset": 59, "endOffset": 80}, {"referenceID": 25, "context": "We use automatically predicted tags for training and testing, obtaining predicted part-ofspeech tags for the Penn Treebank using the Stanford tagger (Toutanova et al., 2003) with 10-way jackknifing, and using the provided predicted partof-speech and morphological tags for the French Treebank.", "startOffset": 149, "endOffset": 173}, {"referenceID": 10, "context": "All parameters (including word and tag embeddings) are randomly initialized using Glorot initialization (Glorot and Bengio, 2010), and are tuned on development set performance.", "startOffset": 104, "endOffset": 129}, {"referenceID": 15, "context": "We use the Adam optimizer (Kingma and Ba, 2014) with its default settings for optimization, with a batch size of 10.", "startOffset": 26, "endOffset": 47}, {"referenceID": 18, "context": "7 reported by Liu and Zhang (2016), demonstrating that our simple neural parsing system is already capable of achieving strong results.", "startOffset": 14, "endOffset": 35}, {"referenceID": 5, "context": "8 Cross and Huang (2016) 90.", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "8 Cross and Huang (2016) 90.5 92.1 91.3 Liu and Zhang (2016) 91.", "startOffset": 2, "endOffset": 61}, {"referenceID": 5, "context": "25 Cross and Huang (2016) 81.", "startOffset": 3, "endOffset": 26}, {"referenceID": 5, "context": "Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015).", "startOffset": 49, "endOffset": 72}, {"referenceID": 5, "context": "Although we fall short of the scores obtained by Cross and Huang (2016), we achieve competitive performance relative to the neural CRF parser of Durrett and Klein (2015). 7 Related Work", "startOffset": 49, "endOffset": 170}, {"referenceID": 4, "context": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 266, "endOffset": 330}, {"referenceID": 17, "context": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 266, "endOffset": 330}, {"referenceID": 21, "context": "Many early successful approaches to constituency parsing focused on rich modeling of correlations in the output space, typically by engineering proabilistic context-free grammars with state spaces enriched to capture long-distance dependencies and lexical phenomena (Collins, 2003; Klein and Manning, 2003; Petrov and Klein, 2007).", "startOffset": 266, "endOffset": 330}, {"referenceID": 3, "context": "As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al.", "startOffset": 139, "endOffset": 163}, {"referenceID": 9, "context": "As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008).", "startOffset": 184, "endOffset": 205}, {"referenceID": 13, "context": "Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al., 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al.", "startOffset": 184, "endOffset": 203}, {"referenceID": 14, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).", "startOffset": 35, "endOffset": 78}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016).", "startOffset": 35, "endOffset": 78}, {"referenceID": 16, "context": "the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures.", "startOffset": 38, "endOffset": 70}, {"referenceID": 11, "context": "the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures.", "startOffset": 143, "endOffset": 169}, {"referenceID": 2, "context": "As noted in the introduction, this class of feature-based tree scoring functions can be implemented with either a linear transition system (Chen and Manning, 2014) or a global decoder (Finkel et al., 2008). Kiperwasser and Goldberg (2016) describe an approach closely related to ours but targeted at dependency formalisms, and which easily accommodates both sparse log-linear scoring models (Hall et al.", "startOffset": 140, "endOffset": 239}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016).", "startOffset": 53, "endOffset": 248}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016).", "startOffset": 53, "endOffset": 272}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.", "startOffset": 53, "endOffset": 297}, {"referenceID": 1, "context": ", 2014) and deep neural potentials (Henderson, 2004; Ballesteros et al., 2016). The best-performing constituency parsers in the last two years have largely been transition-based rather than global; examples include the models of Dyer et al. (2016), Cross and Huang (2016) and Liu and Zhang (2016). The present work takes many of the insights developed in these models (e.g. the recurrent representation of spans (Kiperwasser and Goldberg, 2016), and the use of a dynamic oracle and exploration policy during training (Goldberg and Nivre, 2013)) and extends these insights to span-oriented models, which support a wider range of decoding procedures. Our approach differs from other recent chart-based neural models (e.g. Durrett and Klein (2015)) in the use of a recurrent input representation, structured loss function, and comparatively simple parameterization of the scoring function.", "startOffset": 53, "endOffset": 745}], "year": 2017, "abstractText": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).", "creator": "LaTeX with hyperref package"}}}