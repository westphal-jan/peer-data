{"id": "1705.04228", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2017", "title": "Incremental Learning Through Deep Adaptation", "abstract": "designing an existing trained neural scheme, it is often desirable therefore be able of add new capabilities quickly hindering training despite each learned tasks. existing approaches either learn program - optimal functionality, require better training, or deliver a substantial increment in computational number of chips for initial repetitive task, typically as many as four original network. naive propose a method which fully preserves performance using appropriately enhanced task, with about no marginal increase ( around 20 % ) making my median n required parameters while performing on costly but more costly fine - tuning procedures, which potentially double the number of tricks. the programming architecture also stay controlled to switch overlapping various learned representations, enabling another single network to operate identical task from multiple different assumptions. we derive extensive experiments showing the computation before our experiment and explore different patterns of such behavior.", "histories": [["v1", "Thu, 11 May 2017 15:04:10 GMT  (554kb,D)", "http://arxiv.org/abs/1705.04228v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["amir rosenfeld", "john k tsotsos"], "accepted": false, "id": "1705.04228"}, "pdf": {"name": "1705.04228.pdf", "metadata": {"source": "CRF", "title": "Incremental Learning Through Deep Adaptation", "authors": ["Amir Rosenfeld"], "emails": ["amir@cse.yorku.ca", "tsotsos@cse.yorku.ca"], "sections": [{"heading": "1 Introduction", "text": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task. Given two tasks of a totally different modality or nature, such as predicting the next word in a sequence of words versus predicting the class of an object in an image, it stands to reason that each would require a different architecture or computation. However, for a set of related tasks such as classifying images from different domains it is natural to expect that solutions will:\n1. Utilize the same computational pipeline 2. Require a modest increment in the number of required parameters for each added task 3. Be learned without hindering performance of already learned tasks 4. Be learned incrementally, dropping the requirement for joint training such as in cases the\ntraining data for previously learned tasks is no longer available\nOur goal is to enable a network to learn a set of related tasks one by one while adhering to the above requirements. We do so by augmenting a network learned for one task with controller modules which utilize already learned representations for another. The parameters of the controller modules are optimized to minimize a loss on a new task. The training data for the original task is not required at this stage. The network\u2019s output on the original task data stays exactly as it was; any number of controller modules may be added to each layer so that a single network can simultaneously encode multiple distinct tasks, where the transition from one task to another can be done by setting a binary switching vector. We demonstrate the effectiveness of our method on 8 image classification datasets with images from various domains and non-overlapping output classes. We show that despite adding only 22% number of original parameters for each newly learned task (the specific number depends on the network architecture) , the average performance closely approaches that of fine tuning all\nar X\niv :1\n70 5.\n04 22\n8v 1\n[ cs\n.C V\n] 1\n1 M\nay 2\nparameters - without the negative side effects of doubling the number of parameters and catastrophic forgetting.\nOur main contribution is the introduction of a knowledge transfer method which is as effective as fine-tuning all network parameters towards a new task, precisely preserves old task performance, requires a fraction (network dependent, typically 22%) of the cost in terms of new weights and is able to switch between any number of learned tasks.\nIn the next section, we review some related work. Sec. 3 details the proposed method. In Sec. 4 we present various experiments, including comparing our method to more costly baselines, as well as exploring various strategies on how to make our method more effective. We finish with some concluding remarks."}, {"heading": "2 Related Work", "text": "Multi-task Learning. In multi-task learning, the goal is to train one network to perform several tasks simultaneously. This is usually done by jointly training on all tasks. Such training is advantageous in that a single representation is used for all tasks. In addition, multiple losses are said to act as an additional regularizer. Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others. While all of these train different tasks on the same dataset, the recent work of [2] explores the ability of a single network to perform tasks on various image classification datasets. We also aim to classify images from multiple datasets but we propose doing so in a manner which learns them one-by-one rather than jointly. Our work bears some resemblance to that of [21], where two networks are trained jointly, with additional \u201ccross-stitch\u201d units, allowing each layer from one network to have as additional input linear combinations of outputs from a lower layer in another. However, our method does not require joint training and requires significantly fewer parameters.\nIncremental Learning. Adding a new ability to a neural net often results in so-called \u201ccatastrophic forgetting\u201d [7], hindering the network\u2019s ability to perform well on old tasks. The simplest way to overcome this is by fixing all parameters of the network and using the output of its penultimate layer as a feature extractor, upon which a classifier may be trained [4, 27]. While guaranteed to leave the old performance unaltered, it is observed to yield results which are substantially inferior to fine-tuning the entire architecture [8]. The work of [18] provides a succinct taxonomy of various variants of such methods. In addition, they propose a mechanism of fine-tuning the entire network while making sure to preserve old-task performance by incorporating a loss function which encourages the output of the old features to remain constant on newly introduced data. While their method adds a very small number of parameters for each new task, they do not guarantee that the model retains its full ability on the old task. In [26], new representations can be added along old ones, enabling free adaption to new tasks while leaving the old task performance unaffected. However, this comes at a cost of duplicating the number of parameters of the original network for each added task. In [14], the learning rate of neurons is lowered if they are found to be important to the old task. Our method fully preserves the old representation while causing a modest increase in the number of parameters for each added task."}, {"heading": "3 Approach", "text": "We begin by some notation. Let T be some task to be learned. Specifically, we use a deep convolutional neural net (DCNN) in order to learn a classifier to solve T . Most contemporary DCNN\u2019s follow a common structure: for each input x , the DCNN computes a representation of the input by passing it through a set of l layers \u03c6i, i \u2208 1 . . . l interleaved with non-linearities. The initial (lower) layers of the network are convolutions. The output of the network is created by attaching at least one fully connected layer fi, i \u2208 1 . . . c to the output of the last convolutional layers. Let \u03a6FN = \u03c3(\u03c6l) \u25e6 . . . \u03c3(\u03c62) \u25e6\u03c3(\u03c61) be the composition of all of the convolutional layers of the network N , interleaved by non-linearities. For simplicity, we assume an architecture where all non-linearities \u03c3 are the same function, with no tunable parameters. Denote by \u03a6FN (x) the feature representation of x. Similarly, let the classifier part of N be denoted \u03a6CN = fc \u25e6 . . . \u03c3(f2) \u25e6 \u03c3(f1) be the composition of all of the fully-connected layers of N . The final output of the network N is then simply defined as\nN(x) = \u03a6CN \u25e6 \u03a6FN (x) (1)\n10/05/2017 Preview\n1/1\nController Convolution Controller Convolution Controller Convolution \u03b1\n(input) x\nClassifier 1 Classifier 2 Output Dataset Decider Base Network\nNote that batch-normalization layers may be also interleaved between \u03c6i but we dropped them from the above notation for brevity. It is also possible to drop the \u03a6CN term, if the network is fully convolutional, as in [20]."}, {"heading": "3.1 Adapting Representations", "text": "Assume that we are given two tasks, T1 and T2, to be learned, and that we have learned a base network N to solve T1. We augmentN so that it will be able to solve T2 as well. One way to do so is to extend the penultimate layer of N with a new output layer whose weights are then trained for the new task, keeping all of the old weights of N fixed. However, we wish to avoid the sub-optimal solution likely caused by failing to adapt all of the layers of N . Hence, in addition to adding a new \"head\" to the network, we also attach a controller module to each of its convolutional layers. Each controller module uses the existing weights of the corresponding layer of N to create new convolutional filters adapted to the new task T2. For each convolutional layer \u03c6l in N , let Fl \u2208 RCo\u00d7Ci\u00d7k\u00d7k be the set of filters for that layer, where Co is the number of output features, Cl the number of inputs, and k\u00d7k the kernel size (assuming a square kernel), and let bl \u2208 RCo be the bias. Denote by F\u0303l \u2208 RCo\u00d7D the matrix whose rows are the flattened versions of the filters of Fl, where D = Ci \u00b7 k \u00b7 k; let f \u2208 RCi\u00d7k\u00d7k be\na filter from Fl whose values are f1 =  f 1 11 \u00b7 \u00b7 \u00b7 f11k\n. . . f1kk\n , \u00b7 \u00b7 \u00b7 , f i =  f i 11 \u00b7 \u00b7 \u00b7 f i1k\n. . . f ikk . The flattened version of f is a row vector f\u0303 = (f111, \u00b7 \u00b7 \u00b7 , f1kk, \u00b7 \u00b7 \u00b7 , \u00b7 \u00b7 \u00b7 f i11, \u00b7 \u00b7 \u00b7 , f ikk) \u2208 RD. \u201cUnflattening\u201d a row vector f\u0303 reverts it to its tensor form f \u2208 RCi\u00d7k\u00d7k. This way, we can write\nF\u0303 al = Wl \u00b7 F\u0303l (2)\nwhere Wl \u2208 RCo\u00d7Co is a weight matrix defining linear combinations of the flattened filters of Fl, resulting in Co new filters. Unflattening F\u0303 al to its original shape results in F a l \u2208 RCo\u00d7Ci\u00d7k\u00d7k, which we call the adapted filters of layer \u03c6l. Using the symbol a \u2297 b as shorthand for flatten b\u2192matrix multiply by a\u2192unflatten, we can write:\nF al = Wl \u2297 Fl (3)\nFor bias, we instantiate a new weight vector bal instead of the original bl. The output of layer \u03c6l is computed as follows: let xl\u22121 be the input of \u03c6l in the adapted network. For a given switching\nca lte\nch 25\n6\nC IF\nA R\n1 0\nS V\nH N\nG T\nS R\nda im\nle r\npl an\nkt on\nsk et\nch\nom ni\ngl ot\nftlast\ncaltech256\nCIFAR10\nSVHN\nGTSR\ndaimler\nplankton\nsketch\nomniglot\n88.2 57.6 45.9 55.1 84.8 52.1 28.5 32.3\n20.4 92.5 37.2 50.6 85.0 36.4 16.0 13.9\n7.5 22.7 96.2 37.9 63.9 24.3 9.4 9.6\n13.9 34.1 52.0 98.2 69.0 36.3 17.2 19.6\n7.1 25.5 30.9 13.8 92.9 20.8 2.8 1.4\n22.2 45.4 31.1 31.3 72.6 74.5 39.5 59.9\n20.9 45.5 32.4 28.1 74.6 58.3 69.2 73.3\n15.6 39.9 28.5 21.6 78.7 52.6 29.0 86.9\nca lte\nch 25\n6\nC IF\nA R\n1 0\nS V\nH N\nG T\nS R\nda im\nle r\npl an\nkt on\nsk et\nch\nom ni\ngl ot\nftfull\n88.2 84.9 93.5 94.4 94.3 68.7 58.0 76.2\n40.1 92.5 93.9 94.9 96.8 59.8 36.2 52.9\n19.5 73.7 96.2 92.6 93.9 47.8 23.3 32.8\n27.4 78.9 94.0 98.2 94.7 56.3 33.8 43.0\n25.0 82.2 94.5 96.3 92.9 54.3 16.4 19.3\n67.4 85.2 94.8 93.1 92.8 74.5 62.7 82.0\n72.9 86.8 95.6 95.6 94.3 72.9 69.2 86.2\n57.6 85.4 94.6 95.2 93.6 71.2 58.4 86.9\nca lte\nch 25\n6\nC IF\nA R\n1 0\nS V\nH N\nG T\nS R\nda im\nle r\npl an\nkt on\nsk et\nch\nom ni\ngl ot\nftfullbnoff\n88.2 83.3 93.9 93.3 93.0 69.6 54.7 71.9\n45.0 92.5 94.2 92.5 97.8 62.6 40.3 54.9\n29.5 77.1 96.2 92.9 95.2 55.7 30.0 40.1\n33.2 76.8 93.8 98.2 94.1 59.3 35.9 38.5\n27.0 76.0 94.1 91.1 92.9 56.4 26.5 30.4\n75.2 83.3 94.3 86.8 89.5 74.5 59.8 77.9\n79.3 84.0 94.8 90.6 89.7 70.7 69.2 81.7\n58.3 83.0 94.1 87.2 87.6 69.1 57.9 86.9 30\n45\n60\n75\n90\nFigure 2: Transferability of various datasets to each other (last) fine tuning only the last layer (full) fine-tuning all layers (ft-full-bn-off) fine tuning all layers while disallowing batch-normalization layers\u2019 weights to be updated. Overall, networks tend to be more easily transferable to problems from related domain (e.g., natural / drawing). Zoom in to see numbers. It is recommended to view this figure in color on-line.\nparameter \u03b1 \u2208 {0, 1}, we set the output of the modified layer to be the application of the switched convolution parameters and biases:\nxl+1 = [\u03b1(Wl \u2297 Fl) + (1\u2212 \u03b1)Fl] \u2217 xl + \u03b1bal + (1\u2212 \u03b1)bl (4)\nThe fully connected layers fai are learned from scratch. Throughout training & testing, the weights F of the base network as fixed and only used as basis functions for the creation of F a. The weights of the controller modules are learned via back-propagation given the loss function. Weights of any batch normalization (BN) layers, are either kept fixed or learned separately normalization layers for the controller augmented network. The batch-normalized output is switched between the values of the old and new BN layers, similarly to Eq. 4. A visualization of a network augmented with controller-modules can be seen in Fig. 1.\nWhile we test our method on image classification tasks, it can be applied to any architecture with convolutions, i.e, the majority of to-date architectures. However, the choice of filter dimensions can render it less effective in some cases (see next section).\nIn the following, we denote a network learned for a dataset/task S as NS . A controller learned using NS as a base network for the purpose of task T will be denoted as NS\u2192T .\nParameter Efficiency The number of new parameters added for each task is dependent on two factors: the number of filters in each layer and the number of parameters in the fully-connected layers. As the fully connected layers are not reused, they are fully duplicated. Let M = Co \u00d7D be the filter dimensions for some layer in the base network, where D = Ci \u00d7 k \u00d7 k. A controller module for this layer requires Co \u00d7Co new coefficients to define the adapted filters and an additional Co for the new bias. Hence the ratio of new parameters added w.r.t to the old ones for this layer is Co\u00d7(Co+1) Co\u00d7(D+1) = Co+1 D+1 \u2248 Co D (the approximation is for a large enough Co and D). Example: for a layer with Co = Ci = 256 input and output units and a kernel size k = 5 the ratio of new parameters will be 256+1256\u00b752+1 \u2248 0.04. In the architecture we use (see Experiments, Sec. 4), the total number of features added to the convolutional layers is about 11% of the number of original features, and the fully-connected parameters also add another 11% of the total number of weights, leading to a total addition of 22% of the parameters of the base network for each newly learned task. The fraction of added parameters is dependent on the chosen network architecture. Our method can be applied to any network with convolutional layers. If, however, Co \u2265 D, e.g, the number of output filters is greater than the dimension of each input filter, we our method would only increase the number of parameters."}, {"heading": "3.1.1 Multiple Controllers", "text": "The above description mentions one base network and one controller network. However, any number of controller networks can be attached to a single base network, with independently trained controller modules. The amortized number of parameters per task decreases inversely with the number of added controllers For instance, we construct 7 controllers using one base network, we require (1 + 0.22 \u2217 7) \u00b7 P=2.54 \u00b7 P parameters where P is the number for the base network alone. Had each network been trained independently, the total number of parameters would be 8 \u00b7 P , i.e, we use a ratio of 31 % of the number of required parameters, had each network been trained independently. In this case \u03b1 is extended to one-hot vector of values determined by another sub-network, allowing each controller network to have a varying effect on the final output (see refsub:A-Unified-Network)."}, {"heading": "4 Experiments", "text": "We conduct several experiments to verify our method and explore different aspects of its behavior. We begin by establishing a baseline performance by training a separate module for each of several benchmark datasets (4.1.1). Next, we evaluate the merit of transfer learning between pairs of datasets (4.2). We then proceed to learn several variants of a single network which is able to simultaneously, yet effectively classify images from all datasets with only a modest increase in the number of required parameters and compare it to more costly alternatives 4.3. Finally, we show how to augment this network with the ability to discern the input domain and output a proper classification (4.3.4)."}, {"heading": "4.1 Datasets and Evaluation", "text": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24]. We resize all images to a common size of 64 \u00d7 64 pixels, duplicating the gray-scale images so that they have 3 channels as do the RGB ones. We whiten all images by subtracting the mean pixel value and dividing by the variance per channel. This is done for each dataset separately. Like [2] , we select 80% for training and 20% for validation in datasets where no fixed split is provided. Unlike them, we use a simpler network architecture, allowing us to conduct a wide range of experiments within reasonable computational resources. Our network architecture is the B architecture described in [28]. We refer to this architecture as VGG-B. Each convolutional layer is followed by a batch-normalization layer. With roughly 10 million parameters, it performs quite well on the various datasets when trained from scratch (See Tab. 2). Following is a brief description of each dataset.\nCaltech-256 is a well-known object classification benchmark with 256 object classes and an additional background class. CIFAR-10 - 10 object classes depicted in a total of 60,000 32x32 color images. The Daimler Mono Pedestrian Classification Benchmark - a collection of pedestrian and non-pedestrian images, cropped and resized to 18 \u00d7 36 pixels. GTSR - cropped images of 43 traffic signs. Omniglot - 1623 different handwritten characters from 50 different alphabets. As in [2] we include all the character categories in train and test time. Plankton imagery data - a classification benchmark with 30336 images of various organisms ranging from the smallest single-celled protists to copepods, larval fish, and larger jellies. Human Sketch dataset - 20000 human sketches of every day objects such as \u201cbook\u201d, \u201ccar\u201d, \u201chouse\u201d, \u201csun\u201d. The Street View House Numbers (SVHN) - a\n1The Animals with Attributes dataset [17] was excluded since at the time of writing of this paper, copyright issues prevented accessing the benchmark\u2019s images\nreal-world digit recognition dataset with around 70,000 images which are centered around a single character and resized to 32 \u00d7 32 pixels."}, {"heading": "4.1.1 Baselines", "text": "As a baseline, we train our network independently on each of the 8 datasets. All experiments are done with the Adam optimizer [13], with an initial learning rate of 1e-3 or 1e-4, dependent on a few epochs of trial on each dataset. The learning rate is halved after each 10 epochs. Most networks converge within the first 10-20 epochs, with mostly negligible improvements afterwards. The top-1 accuracy (%) is summarized in Table 2. In [2], a more recent architecture was chosen, namely the ResNet-38 [12]. We compare the baseline performance of our chosen architecture those attained by [2] with ResNet-38. Our main goal is not reaching state-of-the-art performance on independently learned datasets. Nevertheless, the relative performance justifies the choice of VGG-B as solid baseline. Admittedly, it is not unlikely that small implementation details (e.g., learning rate selection) are the cause of the gaps in performance in favor of the simpler VGG-B."}, {"heading": "4.2 Transferability", "text": "We aim to use a network trained on one dataset to perform well on others. As an indicator of the representative power of the features of each independently trained networkN , we test the performance on other datasets, using N for fine tuning. We test 3 different scenarios. ft-last: fine-tuning only the last layer, keeping all the rest frozen. ft-full: fine-tuning all layers of the network. ft-full-bn-off: same as ft-full, but freezing the parameters of the batch-normalization layers - this was found to be useful in some cases. We define the transferability of a source dataset S w.r.t a target dataset T as the top-1 accuracy attained by fine-tuning a network trained on S to perform on T . The results are summarized in Fig. 2. The figure shows some interesting phenomena. First, it is very clear that fine-tuning the last layer only is far inferior to fine-tuning the entire network. Second, perhaps due to the small size of the datasets, usually training from scratch is the most beneficial option. Third, we see a distinction between natural images (Caltech-256, CIFAR-10, SVHN, GTSR, Daimler) and unnatural ones (Sketch, Omniglot, Plankton) ; the Plankton images are natural in the rigorous sense, but do seem to exhibit different behavior than the rest. It is evident that features from the natural images are less beneficial for the unnatural images. Interestingly , the converse is not true: training a network starting from Sketch or Omniglot works quite well for most datasets, both natural and unnatural. This is further shown in Table 1 (a) : we calculate the mean transferability of each dataset by the mean value of each rows of the transferability matrix from Fig 2; on average, using a network trained on Caltech-256 as a feature-extractor for shallow fine-tuning works best. However, using the Plankton dataset works as the best starting point for full fine-tuning, closely followed by Caltech-256. Hence they make good candidates for base-networks, as we show next."}, {"heading": "4.3 Control Networks", "text": "We tested the performance of controller networks based on base networks trained initially on the Plankton or Caltech-256 datasets. One question which arises is how to initialize the weights W\nof a control-module. We tested several options. (1) Set W to be an identity matrix (diagonal). This is equivalent to the controller module starting with a state which effectively mimics the behavior of the base network (2) Set W to random noise (random) (3) Train an independent network for the new task from scratch; set W to best linearly approximate the new weights with the base weights (linear_approx). To test the best alternative out of the three mentioned, we trained Nsketch\u2192caltech256 for one epoch with each initialization and observed the loss function. Each experiment was repeated 5 times and the result averaged. The loss curves can seen in Fig. 3(a). It is evident that the diagonal initialization is superior - this means that - perhaps counter-intuitively - there is no need to trained a fully parametrized target network; simply starting with the behavior of the base network and tuning it via the control modules results in faster convergence. With this observation, we train controller modules with the diagonal method, starting from Sketch and Caltech-256, for each of the remaining set of datasets. The best overall mean accuracy (83.7%) is attained using Nsketch as a base-network. Note that this is very close to the performance attained by full transfer learning (84.2 %, see Tab. 1 ), at a fraction of the number of parameters. This is consistent with our transferability measure. To further test the ability of the \u201ctransferability\u201d measure to predict how well a control network would perform with the base network as a specific dataset, we used each dataset as a basis for control networks for all others and measured the mean overall accuracy. The relation between the two can be seen in Fig. 3 (b)."}, {"heading": "4.3.1 Starting from a Randomly Initialized Base Network", "text": "To check the effectiveness of our method, we checked how well it can perform without any prior knowledge, e.g, using a randomly initialized base network. The total number of parameters for this architecture is 12M. However, as 10M have been randomly initialized and only the controller modules and fully-connected layers have been learned, the effective number is actually 2M. We summarize the results in Tab. 2. Notably, the results of this initialization worked surprisingly well; the mean top-1 precision attained by this network was 76.3 %, slightly worse than of Ncaltech\u2212256 (79.9%). This is better than initializing with Ndaimler, which resulted in a mean accuracy of 75%."}, {"heading": "4.3.2 Pre-trained Networks", "text": "We now check how well a network can perform as a base-network, after it has seen ample training examples. We denote by Nimagenet the VGG-B architecture which was pre-trained on the ImageNet [25] dataset. We train control networks for each of the 8 datasets and report the results in Table 2 (Pre-Ctrl). This improves the average performance by a significant amount (83.7 % to 86.5 %), however for both Sketch and Omniglot the performance is in favor of Nsketch. Note these are the only two domains of strictly unnatural images. On Caltech256 we see an improvement from 88.2 % (training from scratch) to 92.2 %. Fine-tuning all parameters of VGG-B yield slightly better results, with an average accuracy of 87.7 %, compared to 86.5 % attained by the control networks, but requiring x5 parameters."}, {"heading": "4.3.3 Multiple Base Networks", "text": "Ideally, a good base network should have features which are generic enough so that a controller network can use them for any target task. In reality (and as we can see from the performance using only a single base network) this is not necessarily the case. To use two base-networks simultaneously, we implemented a dual-controlled network by using both Ncaltech\u2212256 and Nsketch and attaching to them controller networks. The outputs of the feature parts of the resulting sub-networks were concatenated before the fully-connected layer. This resulted in the exact same performance asNsketch alone. However, by using selected controller-modules per group of tasks, we can improve the results dramatically: For each dataset the maximally performing network is the basis for the control module; i.e., we use the pre-trained VGG-B for all the but Omniglot & Sketch. For the latter two we use Nsketch as a base net. While this requires more parameters, it boosts the mean performance to 87.76 % - better than using any single base network."}, {"heading": "4.3.4 A Unified Network", "text": "Finally, we test the possibility of making a single network which can both determine the domain of an image and classify it. We train a classifier to output which dataset an image originates from, using training images from the 8 different datasets. This turns out to be an extremely simple task compared\nto the other classification tasks, and the network (also of architecture VGG-B [28]) quickly learns to perform at 100% accuracy. Armed with this dataset-decider , name Ndc we augment Nsketch so that for each input image, from any of the datasets Di, we set the controller scalar \u03b1i of each Nsketch\u2192Di to 1 if and only if Ndc deemed the image to originate from Di and to 0 otherwise. This produces a network which applies to each input image the correct controllers, classifying it within its own domain."}, {"heading": "5 Conclusions", "text": "We have presented a method to adapt an existing network to new tasks while fully preserving the existing representation. Our method is able to closely approach the average performance of fulltransfer learning though requiring a fraction of the parameters (around 22% vs. 100%) for each newly learned task, given a proper selection of a base network, and surpass the performance when using controller parameters based on two networks. Built into our method is the ability to easily switch the representation between the various learned tasks, enabling a single network to perform seamlessly on various domains. We find it surprising that using combinations of existing representations yield ones which are useful for other tasks almost as training the entire network from scratch. The control parameter \u03b1 can be cast as a real-valued vector, allowing a smooth transition between representations of different tasks. An example of the effect of such a smooth transition can be seen in Fig. 3 (c) where \u03b1 is used to linearly interpolate between the representation of differently learned tasks, allowing one to smoothly control transitions between different behaviors. Allowing each added task to use a convex combination of already existing controllers will potentially utilize controllers more efficiently and decouple the number of controllers from the number of tasks."}], "references": [{"title": "Integrated perception with recurrent multi-task neural networks", "author": ["H. Bilen", "A. Vedaldi"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Universal representations:The missing link between faces, text, planktons, and cat breeds, 2017", "author": ["Hakan Bilen", "Andrea Vedaldi"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2017}, {"title": "Planktonset 1.0: Plankton imagery data collected from fg walton smith in straits of florida from 2014\u201306-03 to 2014\u201306-06 and 8  used in the 2015 national data science bowl (ncei accession 0127422)", "author": ["Robert K Cowen", "S Sponaugle", "K Robinson", "J Luo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In Icml,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["David Eigen", "Rob Fergus"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "How do humans sketch objects", "author": ["Mathias Eitz", "James Hays", "Marc Alexa"], "venue": "ACM Trans. Graph.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Catastrophic forgetting in connectionist networks", "author": ["Robert M French"], "venue": "Trends in cognitive sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Caltech-256 object category dataset", "author": ["Gregory Griffin", "Alex Holub", "Pietro Perona"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Overcoming catastrophic forgetting in neural networks", "author": ["James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning to detect unseen object classes by between-class attribute transfer", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Learning without Forgetting", "author": ["Zhizhong Li", "Derek Hoiem"], "venue": "CoRR, abs/1606.09282,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "A survey on deep learning in medical image analysis", "author": ["Geert Litjens", "Thijs Kooi", "Babak Ehteshami Bejnordi", "Arnaud Arindra Adiyoso Setio", "Francesco Ciompi", "Mohsen Ghafoorian", "Jeroen AWM van der Laak", "Bram van Ginneken", "Clara I S\u00e1nchez"], "venue": "arXiv preprint arXiv:1702.05747,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Cross-stitch Networks for Multi-task Learning", "author": ["Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert"], "venue": "CoRR, abs/1604.03539,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "An experimental study on pedestrian classification", "author": ["Stefan Munder", "Dariu M Gavrila"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS workshop on deep learning and unsupervised feature learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "author": ["Johannes Stallkamp", "Marc Schlipsing", "Jan Salmen", "Christian Igel"], "venue": "Neural networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Face alignment by coarse-to-fine shape searching. In CVPR, pages 4998\u20135006", "author": ["Shizhan Zhu", "Cheng Li", "Chen Change Loy", "Xiaoou Tang"], "venue": "IEEE Computer Society,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}], "referenceMentions": [{"referenceID": 14, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 119, "endOffset": 123}, {"referenceID": 18, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 147, "endOffset": 151}, {"referenceID": 7, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 170, "endOffset": 173}, {"referenceID": 9, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 194, "endOffset": 198}, {"referenceID": 17, "context": "While deep neural networks continue to show remarkable performance gains in various areas such as image classification [16], semantic segmentation [20], object detection [8], speech recognition [10] medical image analysis [19] - and many more - it is still the case that typically a separate model needs to be trained for each new task.", "startOffset": 222, "endOffset": 226}, {"referenceID": 27, "context": "Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others.", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others.", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "Some examples include facial landmark localization [30], semantic segmentation [11], 3D-reasoning [5], object and part detection [1] and others.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "While all of these train different tasks on the same dataset, the recent work of [2] explores the ability of a single network to perform tasks on various image classification datasets.", "startOffset": 81, "endOffset": 84}, {"referenceID": 19, "context": "Our work bears some resemblance to that of [21], where two networks are trained jointly, with additional \u201ccross-stitch\u201d units, allowing each layer from one network to have as additional input linear combinations of outputs from a lower layer in another.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": "Adding a new ability to a neural net often results in so-called \u201ccatastrophic forgetting\u201d [7], hindering the network\u2019s ability to perform well on old tasks.", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "The simplest way to overcome this is by fixing all parameters of the network and using the output of its penultimate layer as a feature extractor, upon which a classifier may be trained [4, 27].", "startOffset": 186, "endOffset": 193}, {"referenceID": 24, "context": "The simplest way to overcome this is by fixing all parameters of the network and using the output of its penultimate layer as a feature extractor, upon which a classifier may be trained [4, 27].", "startOffset": 186, "endOffset": 193}, {"referenceID": 7, "context": "While guaranteed to leave the old performance unaltered, it is observed to yield results which are substantially inferior to fine-tuning the entire architecture [8].", "startOffset": 161, "endOffset": 164}, {"referenceID": 16, "context": "The work of [18] provides a succinct taxonomy of various variants of such methods.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "In [14], the learning rate of neurons is lowered if they are found to be important to the old task.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "It is also possible to drop the \u03a6CN term, if the network is fully convolutional, as in [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 1, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 144, "endOffset": 147}, {"referenceID": 13, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 172, "endOffset": 176}, {"referenceID": 26, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 184, "endOffset": 188}, {"referenceID": 20, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 199, "endOffset": 203}, {"referenceID": 2, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 228, "endOffset": 231}, {"referenceID": 5, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 255, "endOffset": 258}, {"referenceID": 22, "context": "Our evaluation protocol resembles that of [2], in the following ways: first, we test our network on a variety of datasets1, namely: Caltech-256 [9], CIFAR-10 [15], Daimler [23] , GTSR [29], Omniglot [22] , Plankton imagery data [3] , Human Sketch dataset [6] and SVHN [24].", "startOffset": 268, "endOffset": 272}, {"referenceID": 1, "context": "Like [2] , we select 80% for training and 20% for validation in datasets where no fixed split is provided.", "startOffset": 5, "endOffset": 8}, {"referenceID": 25, "context": "Our network architecture is the B architecture described in [28].", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "As in [2] we include all the character categories in train and test time.", "startOffset": 6, "endOffset": 9}, {"referenceID": 15, "context": "The Animals with Attributes dataset [17] was excluded since at the time of writing of this paper, copyright issues prevented accessing the benchmark\u2019s images", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "\u2020according to [2] .", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "All experiments are done with the Adam optimizer [13], with an initial learning rate of 1e-3 or 1e-4, dependent on a few epochs of trial on each dataset.", "startOffset": 49, "endOffset": 53}, {"referenceID": 1, "context": "In [2], a more recent architecture was chosen, namely the ResNet-38 [12].", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [2], a more recent architecture was chosen, namely the ResNet-38 [12].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "We compare the baseline performance of our chosen architecture those attained by [2] with ResNet-38.", "startOffset": 81, "endOffset": 84}, {"referenceID": 23, "context": "We denote by Nimagenet the VGG-B architecture which was pre-trained on the ImageNet [25] dataset.", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "Using a single base network Nsketch, we check the method\u2019s sensitivity to varying values of \u03b1 by varying it in the range [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 25, "context": "to the other classification tasks, and the network (also of architecture VGG-B [28]) quickly learns to perform at 100% accuracy.", "startOffset": 79, "endOffset": 83}], "year": 2017, "abstractText": "Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly finetuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.", "creator": "LaTeX with hyperref package"}}}