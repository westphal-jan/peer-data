{"id": "1705.06366", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2017", "title": "Automatic Goal Generation for Reinforcement Learning Agents", "abstract": "reinforcement input reduces a powerful opportunity to train an agent to resolve a task. nowadays, user agent that is working using reinforcement learning is only capable with achieving the single impulse request is processed including its reward function. such an approach prevents thereby scale efficiently performing settings exactly which an observer needs typically perform this uniformly set of tasks, such as navigating to varying levels in a room or moving cars to varying temperatures. basically, machine propose basic learning structure allows an agent to automatically reveal total existence of tasks that it is suspected of performing. machines use a static network while propose tasks anticipating the emotion to try or achieve, specified as transition states. the training network mechanism optimized using adversarial training helping generate tasks that resolve always at low appropriate level of difficulty for its outcome. our method thus automatically produces one curriculum delivering tasks demanding the reward best gain. we show surprisingly, similarly using naive framework, an agent can efficiently and automatically learn to handle a varied set of tasks without requiring any prior knowledge of its needs. our algorithms can also manage to learn tasks with discrete rewards, which traditionally pose security challenges.", "histories": [["v1", "Wed, 17 May 2017 23:05:46 GMT  (552kb,D)", "https://arxiv.org/abs/1705.06366v1", null], ["v2", "Wed, 31 May 2017 17:46:36 GMT  (1508kb,D)", "http://arxiv.org/abs/1705.06366v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["david held", "xinyang geng", "carlos florensa", "pieter abbeel"], "accepted": false, "id": "1705.06366"}, "pdf": {"name": "1705.06366.pdf", "metadata": {"source": "CRF", "title": "Automatic Goal Generation for Reinforcement Learning Agents", "authors": ["David Held", "Xinyang Geng", "Carlos Florensa"], "emails": ["davheld@berkeley.edu", "young.geng@berkeley.edu", "florensad@berkeley.edu", "pabbeel@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "Reinforcement learning (RL) can be used to train an agent to perform a task by optimizing a reward function. Recently, a number of impressive results have been demonstrated by training agents using reinforcement learning: such agents have been trained to defeat a champion Go player (Silver et al., 2016), to outperform humans in 49 Atari games (Guo et al., 2016; Mnih et al., 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al., 2015; Duan et al., 2016; Levine et al., 2016).\nIn each of the above cases, the agent is trained to optimize a single reward function in order to learn to perform a single task. However, there are many real-world environments in which a robot will need to be able to perform not a single task but a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. To automatically learn to achieve a diverse set of tasks, our algorithm allows an agent to generate its own reward functions, defined with respect to target sub-sets of the state space, called goals. Our method will propose different goals for the agent to try to reach, and then the agent will learn what actions are necessary to reach each\n\u2217Equal contribution.\nar X\niv :1\n70 5.\n06 36\n6v 2\n[ cs\n.L G\n] 3\nproposed goal. After multiple training iterations, the agent learns to reach a wide variety of goals in its state space.\nSpecifically, our approach learns a policy that takes as input not only the current observation but also the goal (i.e. the parameters describing the target state set) currently to be achieved, similar to the universal value functions in Schaul et al. (2015). We consider the problem of maximizing the average success rate of our agent over all possible goals, where success is defined as the probability of successfully reaching each goal by the current policy. In order to efficiently maximize this objective, the algorithm must intelligently choose which goals to focus on at every training stage: goals should be feasible and at the appropriate level of difficulty for the current policy.\nWe generate such goals using a Goal Generative Adversarial Network (Goal GAN), a variation of to the GANs introduced by Goodfellow et al. (2014). A goal discriminator is trained to evaluate whether a goal is feasible and at the appropriate level of difficulty for the current policy, and a goal generator is trained to generate goals that meet this criteria. We show that such a framework allows a policy to quickly learn to reach all feasible goals in its environment, with no prior knowledge about the environment or the tasks being performed. Our method automatically creates a curriculum, in which, at each step, the generator generates goals that are only slightly more difficult than the goals that the agent already knows how to achieve. Due to this curriculum, the policy is able to learn even with a sparse reward function, such as an indicator that rewards the agent for reaching the goal.\nIn summary, our main contribution is a method for automatic curriculum generation that considerably improves the sampling efficiency of learning to reach all feasible goals in the environment. Learning to reach multiple goals is useful for multi-task settings such as navigation or manipulation, in which we want the agent to perform a wide range of tasks. Our method naturally handles sparse reward functions, without needing to manually modify the reward function for every task, based on prior task knowledge. Instead, our method dynamically modifies the probability distribution from which goals are sampled to ensure that the generated goals are always at the appropriate difficulty level, until the agent learns to reach all goals within the feasible goal space."}, {"heading": "2 Related Work", "text": "Intrinsic Motivation: Intrinsic motivation involves learning with an intrinsically specified objective (Schmidhuber, 1991, 2010). Recently there have been various formulations of intrinsic motivation, relating to optimizing surprise (Houthooft et al., 2016; Achiam & Sastry, 2016) or surrogates of state-visitation counts (Bellemare et al., 2016; Tang et al., 2016). All these approaches improve learning in sparse tasks where naive exploration performs poorly. However, these formulations do not have a notion of what states are hard for the learner and the intrinsic motivation is independent of the current performance of the agent. In contrast, our formulation of intrinsic motivation directly relates to our policy improvement: the agent is motivated to train on tasks that push the boundaries of its capabilities.\nSkill-learning: We are often interested in training an agent to perform a collection of tasks rather than a single one, like reaching different positions in the agent\u2019s state-space. Skill learning is a common approach to this problem as it allows the agent to re-use skills, improving learning compared to training for every task from scratch. Discovering useful skills is a challenging task that has mostly been studied for discrete environments (Vigorito & Barto, 2010; Mnih et al., 2016) or for continuous tasks where demonstrations are provided (Konidaris et al., 2011; Ranchod et al., 2015). More recent work overcomes some of these limitations by training low-level modulated locomotor controllers (Heess et al., 2016), or multimodal policies with an information theoretic regularizer to learn a fixed-size set of skills (Florensa et al., 2017). Nevertheless, previous work usually sees the skills learning as a pre-training step from which useful primitives are obtained and can later be used to achieve other tasks. Hence, additional downstream training is required to properly compose the skills in a purposeful way. On the other hand, our approach directly trains policies that take as input the desired goals, enabling generalization to new desired (test) goals without the need for fine tuning.\nThe problem that we are exploring has been referred to as \u201cmulti-task policy search\u201d (Deisenroth et al., 2014) or \u201ccontextual policy search,\u201d in which the task is viewed as the context for the policy (Deisenroth et al., 2013; Fabisch & Metzen, 2014). As opposed to the work of Deisenroth et al. (2014), our work uses a curriculum to perform efficient multi-task learning, even in sparse reward\nsettings. In contrast to Fabisch & Metzen (2014), which trains from a small number of discrete contexts / tasks, our method generates a training curriculum directly in continuous task space.\nCurriculum Learning: The increasing interest on training single agents to perform multiple tasks is leading to new developments on how to optimally present the tasks to the agent during learning. The idea of using a curriculum has been explored in many prior works on supervised learning (Bengio et al., 2009; Zaremba & Sutskever, 2014; Bengio et al., 2015). However, these curricula are usually hand-designed, using the expertise of the system designer. Another line of work takes into explicit consideration which examples are hard for the current learner and allows it to not consider them (Kumar et al., 2010; Jiang et al., 2015). However this has mainly been applied for supervised tasks and most curriculum learning in reinforcement learning still relies on fixed pre-specified sequences of tasks (Karpathy & Van De Panne, 2012). In contrast, we automatically create a curriculum with no prior knowledge by discovering which tasks are easy or difficult for a given policy.\nOther recent work has proposed using a given baseline performance for several tasks to gauge which tasks are the hardest and require more training (Sharma & Ravindran, 2017), but the framework can only handle a finite set of tasks and cannot handle sparse rewards. Our method trains a policy that generalizes to a set of continuously parameterized tasks and is shown to perform well even under sparse rewards by not allocating training effort to tasks that are too hard for the current performance of the agent. Finally, an interesting self-play strategy has recently been proposed that is concurrent to our work (Sukhbaatar et al., 2017), but, contrary to our approach that is designed to generate many training tasks at the right level of difficulty, the asymmetric component of their method could lead to biased exploration. Furthermore, they view their work as simply an exploration bonus for a single target task (and they evaluate their method accordingly); in contrast, we define a new problem of efficiently optimizing a policy across a range of goals, as we explain below."}, {"heading": "3 Problem Definition", "text": ""}, {"heading": "3.1 Goal-parameterized Reward Functions", "text": "In the traditional reinforcement learning framework, at each timestep t, the agent in state st \u2208 S \u2286 Rn takes an action at \u2208 A \u2286 Rm, according to some policy \u03c0(at | st) that maps from the current state st to a probability distribution over actions. Taking this action causes the agent to enter into a new state st+1 according to a transition distribution p(st+1|st, at), and receive a reward rt = r(st, at, st+1). The objective of the agent is to find the policy \u03c0 that maximizes the expected return, defined as the sum of rewardsR = \u2211T t=0 rt, where T is a maximal time given to perform the task. The learned policy corresponds to maximizing the expected return for a single reward function.\nIn our framework, instead of learning to optimize a single reward function, we consider a range of reward functions rg indexed or parametrized by a goal g \u2208 G. Each goal g corresponds to a set of states Sg \u2282 S such that goal g is considered to be achieved when the agent is in any state st \u2208 Sg . Then the objective is to learn a policy that, given any goal g \u2208 G, acts optimally with respect to rg . In this paper, we define a very simple reward function that measures whether the agent has reached the goal\nrg(st, at, st+1) = 1{st+1 \u2208 Sg} , (1)\nwhere 1 is the indicator function. In our case, we use Sg = {st : d(f(st), g) \u2264 }, where f(\u00b7) is a function that projects a state into goal space G, d(\u00b7, \u00b7) is a distance metric in goal space, and is the acceptable tolerance that determines when the goal is reached. However, our method can handle generic sparse rewards (as in Eq. (1)) and does not require a distance metric for learning. Furthermore, we define our MDP such that each episode terminates when st \u2208 Sg . Thus, the return Rg = \u2211T t=0 r g t is a binary random variable whose value indicates whether the agent has reached the set Sg in at most T time-steps. Hence, the return of a trajectory s0, s1, . . . can be expressed as Rg = 1{ \u22c3T t=0 st \u2208 Sg}. Now, policies are also conditioned on the current goal g, written as \u03c0(at | st, g), and the expected return obtained when we take actions sampled from it can then be expressed as the probability of succeeding on each goal within T timesteps, as shown in Eq. (2).\nRg(\u03c0) = E\u03c0(\u00b7 | st,g) 1{ T\u22c3 t=0 st \u2208 Sg} = P ( T\u22c3 t=0 st \u2208 Sg \u2223\u2223\u2223 \u03c0, g) (2)\nThe sparse indicator reward function of Eq. (1) is not only simple but also represents a property of many real-world goal states: in many settings, it may be difficult to tell whether the agent is getting closer to achieving a goal, but easy to tell when a goal has been achieved. For example, for a robot moving in a maze, taking actions that maximally reduce the straight-line distance from the start to the goal is usually not a feasible approach for reaching the goal, due to the presence of obstacles along the path. In theory, one could hand-engineer a meaningful distance function for each task that could be used to create a dense reward function. Instead, we use the indicator function of Eq. (1), which simply captures our objective by measuring whether the agent has reached the goal state. We show that our method is able to learn even with such sparse rewards."}, {"heading": "3.2 Overall Objective", "text": "We desire to find a policy \u03c0(at | st, g) that achieves a high reward for many goals g. We assume that there is a test distribution of goals pg(g) that we would like to perform well on. For simplicity, we assume that the test distribution samples goals uniformly from the set of goals G, although in practice any distribution can be used. The overall objective is then to find a policy \u03c0\u2217 such that\n\u03c0\u2217(at | st, g) = arg max \u03c0 Eg\u223cpg(\u00b7)R g(\u03c0) . (3)\nRecall from Eq. (2) that Rg(\u03c0) is the probability of success for each goal g. Thus the objective of Eq. (3) measures the average probability of success over all goals sampled from pg(g). We refer to the objective in Eq. (3) as the coverage objective."}, {"heading": "4 Method", "text": "Our approach can be broken down into three parts: First, we label a set of goals based on whether they are at the appropriate level of difficulty for the current policy. Second, using these labeled goals, we construct and train a generator to output new goals that are at the appropriate level of difficulty. Finally, we use these new goals to efficiently train the policy, improving its coverage objective. We iterate through each of these steps until the policy converges."}, {"heading": "4.1 Goal Labeling", "text": "As shown in our experiments, sampling goals from pg(g) directly, and training our policy on each sampled goal may not be the most sample efficient way to optimize the coverage objective of Eq. (3). Instead, we modify the distribution from which we sample goals during training: we wish to find the set of goals g in the set Gi = {g : Rmin \u2264 Rg(\u03c0i) \u2264 Rmax} \u2286 G. The justification for this is as follows: due to the sparsity of the reward function, for most goals g, the current policy \u03c0i (at iteration i) obtains no reward. Instead, we wish to train our policy on goals g for which \u03c0i is able to receive some minimum expected return Rg(\u03c0i) > Rmin such that the agent receives enough reward signal for learning. On the other hand, if we only sample from goals for which Rg(\u03c0i) > Rmin, we might sample repeatedly from a small set of already mastered goals. To force our policy to train on goals that still need improvement, we train on the set of goals g for which Rg(\u03c0i) \u2264 Rmax, where Rmax is a hyperparameter setting a maximum level of performance above which we prefer to concentrate on new goals. Thus, training our policy on goals in Gi allows us to efficiently maximize the coverage objective of Eq. (3). Note that from Eq. (2), Rmin and Rmax can be interpreted as a minimum and maximum probability of reaching a goal over T timesteps.\nGiven a set of goals sampled from some distribution pdata(g), we wish to estimate a label yg \u2208 {0, 1} for each goal g that indicates whether g \u2208 Gi. To label a given goal, we empirically estimate the expected return for this goal R\u0304g(\u03c0i) by performing rollouts of our current policy \u03c0i. The label for this goal is then set to yg = 1 { Rmin \u2264 R\u0304g(\u03c0i) \u2264 Rmax } . In the next section we describe how\nwe can generate more goals that also belong to Gi, in addition to the goals that we have labeled."}, {"heading": "4.2 Adversarial Goal Generation", "text": "In order to sample new goals g uniformly from Gi, we introduce an adversarial training procedure called \u201dgoal GAN\u201d, which is a modification of to the procedure used for training Generative\nAdversarial Networks (GANs) (Goodfellow et al., 2014). The modification allows to train the generative model both with positive examples from the distribution we want to approximate, and negative examples sampled from a distribution that does not share support with the desired one. This improves the accuracy of the generative model despite being trained with very few positive samples. Other generative models like Stochastic Neural Networks (Tang & Salakhutdinov, 2013) don\u2019t accept negative examples and don\u2019t have the potential to scale up to higher dimensions as well as GAN approaches.\nIn our particular application, we use a neural network G(z) known as the \u201cgoal generator\u201d to generate goals g from a noise vector z. We train the goal generator to uniformly output goals in Gi, using a second network D(g) known as a \u201cgoal discriminator\u201d. The goal discriminator is trained to distinguish goals that are in Gi from goals that are not in Gi. We optimize our goal generator and discriminator in a manner similar to the training procedure for the Least-Squares GAN (LSGAN) (Mao et al., 2016), but introducing the binary label yg indicating whether g \u2208 Gi. We found that the LSGAN works better than other forms of GAN for our problem. We use a = -1, b = 1, and c = 0, as in Mao et al. (2016).\nmin D V (D) = Eg\u223cpdata(g)\n[ yg(D(g)\u2212 b)2 + (1\u2212 yg)(D(g)\u2212 a)2 ] +Ez\u223cpz(z)[(D(G(z))\u2212 a) 2]\nmin G\nV (G) = Ez\u223cpz(z)[D(G(z))\u2212 c) 2] (4)\nUnlike in the original LSGAN paper (Mao et al., 2016), we have three terms in our value function V (D) rather than the original two. For goals g for which yg = 1, the second term disappears and we are left with only the first and third terms, which are identical to that of the original LSGAN framework. Viewed in this manner, the discriminator is trained to discriminate between goals from pdata(g) with a label yg = 1 and the generated goals G(z). Looking at the second term, our discriminator is also trained with \u201cnegative examples,\u201d i.e. goals with a label yg = 0 which our generator should not generate. The generator is trained to \u201cfool\u201d the discriminator, i.e. to output goals that match the distribution of goals in pdata(g) for which yg = 1."}, {"heading": "4.3 Policy Optimization", "text": "Algorithm 1: Generative Goal Learning Input : Policy \u03c00 Output: Policy \u03c0N (G,D)\u2190 initialize GAN() goalsold \u2190 \u2205 for i\u2190 1 to N do\nz \u2190 sample noise(pz(\u00b7)); goals\u2190 G(z) \u222a sample(goalsold); \u03c0i \u2190 update policy(goals, \u03c0i\u22121); returns\u2190 evaluate policy(goals, \u03c0i); labels\u2190 label goals(returns) (G,D)\u2190\ntrain GAN(goals, labels,G,D); goalsold \u2190 update replay(goals)\nend\nOur full algorithm for training a policy \u03c0(at |st, g) to maximize the coverage objective in Eq. (3) is shown in Algorithm 1. At each iteration i, we generate a set of goals from our goal generator G(z). We use these goals to train our policy using reinforcement learning, with the reward function given by Eq. (1). The training can be done with any Reinforcement Learning algorithm; in our case we use TRPO (Schulman et al., 2015) with GAE (Schulman et al., 2016). After training our policy, we evaluate our policy\u2019s performance on these goals; this performance is used to determine each goal\u2019s label yg , as described in Section 4.1. Next, we use these labels to train our goal generator and our goal discriminator, as described in Section 4.2.\nThe generated goals from the previous iteration are used to compute the Monte Carlo estimate of the expectations with respect to the distribution pdata(g) in Eq. (4). By training on goals within Gi produced by the goal generator, our method efficiently finds a policy that optimizes the coverage objective.\nIn addition to training our policy on the goals that were generated in the current iteration, we also save a list (\u201cregularized replay buffer\u201d) of goals that were generated during previous iterations and use these to train our policy as well, so that our policy does not forget how to achieve goals that it has previously learned. When we generate goals for our policy to train on, we sample two thirds of the goals from the Goal GAN and we sample the one third of the goals uniformly from the replay buffer. To prevent the replay buffer from concentrating in a small portion of goal space, we only insert new goals that are further away than from the goals already in the buffer, where we chose the goal-space metric and to be the same as the ones introduced in Section 3.1.\nThe algorithm described above naturally creates a curriculum for our policy. The goal generator generates goals in Gi, for which our policy obtains an intermediate level of return, and thus such goals are at the appropriate level of difficulty for our current policy \u03c0i. As our policy improves, the generator learns to generate goals in order of increasing difficulty. Hence, our method can be viewed as a way to automatically generate a curriculum of goals. However, the curriculum occurs as a by-product via our optimization, without requiring any prior knowledge of the environment or the tasks that the agent must perform."}, {"heading": "4.4 Goal GAN Initialization", "text": "In order to begin our training procedure, we need to initialize our goal generator to produce an initial set of goals. If we initialize the goal generator randomly (or if we initialize it to sample uniformly from the goal space), it is likely that, for most (or all) of the sampled goals, our initial policy would receives no reward due to the sparsity of the reward function. Thus we might have that all of our initial goals g have R\u0304g(\u03c00) < Rmin, leading to very slow training.\nTo avoid this problem, we initialize our goal generator to output a set of goals that our initial policy is likely to be able to achieve with R\u0304g(\u03c0i) \u2265 Rmin . To accomplish this, we run our initial policy \u03c00(at | st, g) with goals sampled uniformly from the goal space. We then observe the set of states Sv that are visited by our initial policy. These are states that can be easily achieved with the initial policy, \u03c00, so the goals corresponding to such states will likely be contained within SI0 . We then train the goal generator to produce goals that match the state-visitation distribution pv(g), defined as the uniform distribution over the set f(Sv). We can achieve this through traditional GAN training, with pdata(g) = pv(g). This initialization of the generator allows us to bootstrap the Goal GAN training process, and our policy is able to quickly improve its performance."}, {"heading": "5 Experimental Results", "text": "In this section we provide the experimental results to answer the following questions about our goal generation algorithm:\n\u2022 Does the automatic curriculum yield faster maximization of the coverage objective? \u2022 Does the Goal GAN dynamically shift to sample goals of the appropriate difficulty? \u2022 Does it scale to a higher-dimensional state-space with a low-dimensional space of feasible goals?\nTo answer the first two questions, we demonstrate our method in a maze task where the goals are the (x, y) position of the Center of Mass (CoM) of a dynamically complex agent. Hence the feasible goal space is the interior of the maze, and we demonstrate how our goal generation can guide the agent around a turn in the maze. Then, we study how our method scales with the dimension of the state-space in an environment where the feasible region is kept of approximately constant volume in an embedding space that grows in dimension. For the implementation details refer to Appendices A.4 and A.5. Visualization of the learned policies are provided on our website 2.\nWe compare our Goal GAN method against three baselines. Uniform Sampling (baseline 3) is a method that does not use a curriculum at all, training at every iteration on goals uniformly sampled from the state-space. Many goals are unfeasible or too difficult for the current performance of the agent and hence no reward is received, making this method very sample-inefficient. To demonstrate that a straight-forward distance reward can be prone to local minima, Uniform Sampling with L2 loss (baseline 2) samples goals in the same fashion as baseline 3, but instead of the indicator reward that our method uses, it receives the negative L2 distance to the goal as a reward at every step. Finally, On-policy GAN (baseline 1) samples goals from a GAN that is constantly retrained using the state visitation distribution (as in Section 4.4). Finally, we also show the performance of using Rejection Sampling (oracle), where we sample goals uniformly from the feasible state-space and only keep them if they satisfy the criterion defined in Section 4.1. This method is orders of magnitude more expensive in terms of labeling, but serves to estimate an upper-bound for our method in terms of performance."}, {"heading": "5.1 Ant Maze", "text": "2https://sites.google.com/view/goalgeneration4rl\nWe test our method in the challenging environment of a complex robotic agent (Ant) navigating a U-shaped maze, as depicted in Fig. 1. Duan et al. (2016) describe the task of trying to reach the other end of the U-turn and they show that standard Reinforcement Learning methods are unable to solve it. We further extend the task to try to reach every point of the maze, still keeping the sparse indicator reward (here with = 1). Our goal GAN method successfully learn how to cover most of the maze, including the other end. The goal space is two dimensional, allowing us to study the goals generated by the automatic curriculum and how it drives the training through the U-turn. The visualizations and the experimental results will help us answer the first two questions from the previous section.\nWe first explore whether, by training on goals that are generated by our Goal GAN, we are able to improve our policy\u2019s training efficiency, compared to the baselines described above. In Fig. 2 we see that our method leads to faster training compared to the baselines. Using the Goal GAN, we are always training our policy on goals that are at the appropriate level of difficulty, leading to more efficient learning. Interestingly, for this task our method is very close the oracle of rejection sampling. The On-policy GAN baseline is, in this environment, performing reasonably well because we inject high noise in the actions of the ant, making the current policy visit further and further regions. This baseline performs worse in other experiments presented in the supplementary material3. The worse performing baseline is the one rewarding the L2 goal distance at every time-step, because the complex dynamics of the ant and the fact that goals are sampled all around make the optimization fall in the bad local optimal policy of not moving.\nmance such that many of the generated goals are at the appropriate level of difficulty. The corresponding performance of the policy can be shown in Fig. 4."}, {"heading": "5.2 N-dimensional Point Mass", "text": "In most real-world reinforcement learning problems, the set of feasible states is a lower-dimensional subset of the full state space, defined by the constraints of the environment. For example, the kinematic constraints of a robot limit the set of feasible states that the robot can achieve. Therefore, uniformly sampling goals from the full state-space would yield very few achievable goals. In this section we use an N-dimensional Point Mass to explore this issue and demonstrate the performance of our method as the embedding dimension increases.\n3https://sites.google.com/view/goalgeneration4rl\nIn our experiments, the full state-space of the N -dimensional Point Mass is the hypercube [\u22125, 5]N . However, the Point Mass can only move within a small subset of this state space. In the twodimensional case, the set of feasible states corresponds to the [\u22125, 5]\u00d7 [\u22121, 1] rectangle, making up 20% of the full space. For N > 2, the feasible space is the Cartesian product of this 2D strip with [\u2212 , ]N\u22122, where = 0.3. In this higher-dimensional environment, our agent receives a reward of 1 when it moves within N = 0.3 \u221a N\u221a 2\nof the goal state, to account for the increase in average L2 distance between points in higher dimensions. The ratio of the volume of the embedded space to the volume of the full state space decreases as the state-dimension N increases, down to a ratio of 0.00023:1 for 6 dimensions.\nline with an L2 loss as reward have perfect performance, which is expected in this simple task where the optimal policy is just to go in a straight line towards the goal."}, {"heading": "6 Conclusions and Future Work", "text": "We propose a new paradigm in reinforcement learning where the objective is to train a single policy to succeed on a variety of goals, under sparse rewards. To solve this problem we develop a method for automatic curriculum generation that dynamically adapts to the current performance of the agent. The curriculum is obtained without any prior knowledge of the environment or of the tasks being performed. We use generative adversarial training to automatically generate goals for our policy that are always at the appropriate level of difficulty (i.e. not too hard and not too easy). We demonstrated that our method can be used to efficiently train a policy to reach a range of goal states, which can be useful for multi-task learning scenarios."}], "references": [{"title": "Surprise-based intrinsic motivation for deep reinforcement learning", "author": ["Achiam", "Josh", "Sastry", "Shankar"], "venue": "In 2016 Deep Reinforcement Learning Workshop at NIPS,", "citeRegEx": "Achiam et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Achiam et al\\.", "year": 2016}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Bellemare", "Marc G", "Srinivasan", "Sriram", "Ostrovski", "Georg", "Schaul", "Tom", "Saxton", "David", "Munos", "Remi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Curriculum learning", "author": ["Bengio", "Yoshua", "Louradour", "J\u00e9r\u00f4me", "Collobert", "Ronan", "Weston", "Jason"], "venue": "In Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "Bengio et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2009}, {"title": "A survey on policy search for robotics", "author": ["Deisenroth", "Marc Peter", "Neumann", "Gerhard", "Peters", "Jan"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Multi-task policy search for robotics", "author": ["Deisenroth", "Marc Peter", "Englert", "Peter", "Peters", "Jan", "Fox", "Dieter"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Deisenroth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2014}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "Duan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Duan et al\\.", "year": 2016}, {"title": "Active contextual policy search", "author": ["Fabisch", "Alexander", "Metzen", "Jan Hendrik"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fabisch et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fabisch et al\\.", "year": 2014}, {"title": "Stochastic neural networks for hierarchical reinforcement learning", "author": ["Florensa", "Carlos", "Duan", "Yan", "Abbeel", "Pieter"], "venue": "In International Conference in Learning Representations,", "citeRegEx": "Florensa et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Florensa et al\\.", "year": 2017}, {"title": "Generative adversarial networks", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Deep learning for reward design to improve monte carlo tree search in atari games", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lewis", "Richard", "Lee", "Honglak"], "venue": "Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Guo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Learning and transfer of modulated locomotor", "author": ["Heess", "Nicolas", "Wayne", "Greg", "Tassa", "Yuval", "Lillicrap", "Timothy", "Riedmiller", "Martin", "Silver", "David"], "venue": null, "citeRegEx": "Heess et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Heess et al\\.", "year": 2016}, {"title": "Variational information maximizing exploration", "author": ["Houthooft", "Rein", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "De Turck", "Filip", "Abbeel", "Pieter"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Self-paced curriculum learning", "author": ["Jiang", "Lu", "Meng", "Deyu", "Zhao", "Qian", "Shan", "Shiguang", "Hauptmann", "Alexander G"], "venue": "In AAAI,", "citeRegEx": "Jiang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2015}, {"title": "Curriculum learning for motor skills", "author": ["Karpathy", "Andrej", "Van De Panne", "Michiel"], "venue": "In Canadian Conference on Artificial Intelligence,", "citeRegEx": "Karpathy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2012}, {"title": "Autonomous skill acquisition on a mobile manipulator", "author": ["Konidaris", "George", "Kuindersma", "Scott", "Grupen", "Roderic A", "Barto", "Andrew G"], "venue": "In AAAI,", "citeRegEx": "Konidaris et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Konidaris et al\\.", "year": 2011}, {"title": "Self-paced learning for latent variable models", "author": ["M.P. Kumar", "Packer", "Benjamin", "Koller", "Daphne"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kumar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2010}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": null, "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Least squares generative adversarial networks. 2016, arXiv:1611.04076", "author": ["Mao", "Xudong", "Li", "Qing", "Xie", "Haoran", "Lau", "Raymond YK", "Wang", "Zhen", "Smolley", "Stephen Paul"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2016}, {"title": "Humanlevel control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Mnih", "Volodymyr", "Agapiou", "John", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Kavukcuoglu", "Koray"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Nonparametric bayesian reward segmentation for skill discovery using inverse reinforcement learning", "author": ["Ranchod", "Pravesh", "Rosman", "Benjamin", "Konidaris", "George"], "venue": "In International Conference on Intelligent Robots and Systems,", "citeRegEx": "Ranchod et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranchod et al\\.", "year": 2015}, {"title": "Universal value function approximators", "author": ["Schaul", "Tom", "Horgan", "Daniel", "Gregor", "Karol", "Silver", "David"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Curious model-building control systems", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q1991\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 1991}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["Schmidhuber", "J\u00fcrgen"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber and J\u00fcrgen.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber and J\u00fcrgen.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael", "Moritz", "Philipp"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Highdimensional continuous control using generalized advantage estimation", "author": ["Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Schulman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2016}, {"title": "Online Multi-Task Learning", "author": ["Sharma", "Sahil", "Ravindran", "Balaraman"], "venue": "Using Biased Sampling", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["Silver", "David", "Huang", "Aja", "Maddison", "Chris J", "Guez", "Arthur", "Sifre", "Laurent", "Van Den Driessche", "George", "Schrittwieser", "Julian", "Antonoglou", "Ioannis", "Panneershelvam", "Veda", "Lanctot", "Marc"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play", "author": ["Sukhbaatar", "Sainbayar", "Kostrikov", "Ilya", "Szlam", "Arthur", "Fergus", "Rob"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2017}, {"title": "exploration: A study of count-based exploration for deep reinforcement learning", "author": ["Tang", "Haoran", "Houthooft", "Rein", "Foote", "Davis", "Stooke", "Adam", "Chen", "Xi", "Duan", "Yan", "Schulman", "John", "Turck", "Filip De", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": null, "citeRegEx": "Todorov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2013}, {"title": "Intrinsically motivated hierarchical skill learning", "author": ["2012. Vigorito", "Christopher M", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Vigorito et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vigorito et al\\.", "year": 2012}, {"title": "Learning to execute", "author": ["Zaremba", "Wojciech", "Sutskever", "Ilya"], "venue": "CoRR, abs/1410.4615,", "citeRegEx": "2010", "shortCiteRegEx": "2010", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Recently, a number of impressive results have been demonstrated by training agents using reinforcement learning: such agents have been trained to defeat a champion Go player (Silver et al., 2016), to outperform humans in 49 Atari games (Guo et al.", "startOffset": 174, "endOffset": 195}, {"referenceID": 10, "context": ", 2016), to outperform humans in 49 Atari games (Guo et al., 2016; Mnih et al., 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al.", "startOffset": 48, "endOffset": 85}, {"referenceID": 20, "context": ", 2016), to outperform humans in 49 Atari games (Guo et al., 2016; Mnih et al., 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al.", "startOffset": 48, "endOffset": 85}, {"referenceID": 18, "context": ", 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al., 2015; Duan et al., 2016; Levine et al., 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 6, "context": ", 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al., 2015; Duan et al., 2016; Levine et al., 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 17, "context": ", 2015), and to perform a variety of difficult robotics tasks (Lillicrap et al., 2015; Duan et al., 2016; Levine et al., 2016).", "startOffset": 62, "endOffset": 126}, {"referenceID": 23, "context": "the parameters describing the target state set) currently to be achieved, similar to the universal value functions in Schaul et al. (2015). We consider the problem of maximizing the average success rate of our agent over all possible goals, where success is defined as the probability of successfully reaching each goal by the current policy.", "startOffset": 118, "endOffset": 139}, {"referenceID": 9, "context": "We generate such goals using a Goal Generative Adversarial Network (Goal GAN), a variation of to the GANs introduced by Goodfellow et al. (2014). A goal discriminator is trained to evaluate whether a goal is feasible and at the appropriate level of difficulty for the current policy, and a goal generator is trained to generate goals that meet this criteria.", "startOffset": 120, "endOffset": 145}, {"referenceID": 12, "context": "Recently there have been various formulations of intrinsic motivation, relating to optimizing surprise (Houthooft et al., 2016; Achiam & Sastry, 2016) or surrogates of state-visitation counts (Bellemare et al.", "startOffset": 103, "endOffset": 150}, {"referenceID": 1, "context": ", 2016; Achiam & Sastry, 2016) or surrogates of state-visitation counts (Bellemare et al., 2016; Tang et al., 2016).", "startOffset": 72, "endOffset": 115}, {"referenceID": 31, "context": ", 2016; Achiam & Sastry, 2016) or surrogates of state-visitation counts (Bellemare et al., 2016; Tang et al., 2016).", "startOffset": 72, "endOffset": 115}, {"referenceID": 21, "context": "Discovering useful skills is a challenging task that has mostly been studied for discrete environments (Vigorito & Barto, 2010; Mnih et al., 2016) or for continuous tasks where demonstrations are provided (Konidaris et al.", "startOffset": 103, "endOffset": 146}, {"referenceID": 15, "context": ", 2016) or for continuous tasks where demonstrations are provided (Konidaris et al., 2011; Ranchod et al., 2015).", "startOffset": 66, "endOffset": 112}, {"referenceID": 22, "context": ", 2016) or for continuous tasks where demonstrations are provided (Konidaris et al., 2011; Ranchod et al., 2015).", "startOffset": 66, "endOffset": 112}, {"referenceID": 11, "context": "More recent work overcomes some of these limitations by training low-level modulated locomotor controllers (Heess et al., 2016), or multimodal policies with an information theoretic regularizer to learn a fixed-size set of skills (Florensa et al.", "startOffset": 107, "endOffset": 127}, {"referenceID": 8, "context": ", 2016), or multimodal policies with an information theoretic regularizer to learn a fixed-size set of skills (Florensa et al., 2017).", "startOffset": 110, "endOffset": 133}, {"referenceID": 5, "context": "The problem that we are exploring has been referred to as \u201cmulti-task policy search\u201d (Deisenroth et al., 2014) or \u201ccontextual policy search,\u201d in which the task is viewed as the context for the policy (Deisenroth et al.", "startOffset": 85, "endOffset": 110}, {"referenceID": 4, "context": ", 2014) or \u201ccontextual policy search,\u201d in which the task is viewed as the context for the policy (Deisenroth et al., 2013; Fabisch & Metzen, 2014).", "startOffset": 97, "endOffset": 146}, {"referenceID": 4, "context": "The problem that we are exploring has been referred to as \u201cmulti-task policy search\u201d (Deisenroth et al., 2014) or \u201ccontextual policy search,\u201d in which the task is viewed as the context for the policy (Deisenroth et al., 2013; Fabisch & Metzen, 2014). As opposed to the work of Deisenroth et al. (2014), our work uses a curriculum to perform efficient multi-task learning, even in sparse reward", "startOffset": 86, "endOffset": 302}, {"referenceID": 3, "context": "The idea of using a curriculum has been explored in many prior works on supervised learning (Bengio et al., 2009; Zaremba & Sutskever, 2014; Bengio et al., 2015).", "startOffset": 92, "endOffset": 161}, {"referenceID": 2, "context": "The idea of using a curriculum has been explored in many prior works on supervised learning (Bengio et al., 2009; Zaremba & Sutskever, 2014; Bengio et al., 2015).", "startOffset": 92, "endOffset": 161}, {"referenceID": 16, "context": "Another line of work takes into explicit consideration which examples are hard for the current learner and allows it to not consider them (Kumar et al., 2010; Jiang et al., 2015).", "startOffset": 138, "endOffset": 178}, {"referenceID": 13, "context": "Another line of work takes into explicit consideration which examples are hard for the current learner and allows it to not consider them (Kumar et al., 2010; Jiang et al., 2015).", "startOffset": 138, "endOffset": 178}, {"referenceID": 30, "context": "Finally, an interesting self-play strategy has recently been proposed that is concurrent to our work (Sukhbaatar et al., 2017), but, contrary to our approach that is designed to generate many training tasks at the right level of difficulty, the asymmetric component of their method could lead to biased exploration.", "startOffset": 101, "endOffset": 126}, {"referenceID": 9, "context": "Adversarial Networks (GANs) (Goodfellow et al., 2014).", "startOffset": 28, "endOffset": 53}, {"referenceID": 19, "context": "We optimize our goal generator and discriminator in a manner similar to the training procedure for the Least-Squares GAN (LSGAN) (Mao et al., 2016), but introducing the binary label yg indicating whether g \u2208 Gi.", "startOffset": 129, "endOffset": 147}, {"referenceID": 19, "context": "We optimize our goal generator and discriminator in a manner similar to the training procedure for the Least-Squares GAN (LSGAN) (Mao et al., 2016), but introducing the binary label yg indicating whether g \u2208 Gi. We found that the LSGAN works better than other forms of GAN for our problem. We use a = -1, b = 1, and c = 0, as in Mao et al. (2016).", "startOffset": 130, "endOffset": 347}, {"referenceID": 19, "context": "Unlike in the original LSGAN paper (Mao et al., 2016), we have three terms in our value function V (D) rather than the original two.", "startOffset": 35, "endOffset": 53}, {"referenceID": 26, "context": "The training can be done with any Reinforcement Learning algorithm; in our case we use TRPO (Schulman et al., 2015) with GAE (Schulman et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 27, "context": ", 2015) with GAE (Schulman et al., 2016).", "startOffset": 17, "endOffset": 40}, {"referenceID": 6, "context": "Duan et al. (2016) describe the task of trying to reach the other end of the U-turn and they show that standard Reinforcement Learning methods are unable to solve it.", "startOffset": 0, "endOffset": 19}], "year": 2017, "abstractText": "Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized sub-set of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.", "creator": "LaTeX with hyperref package"}}}