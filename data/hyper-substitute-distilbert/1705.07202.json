{"id": "1705.07202", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2017", "title": "Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation", "abstract": "variational graph - encoder ( vae ) is a powerful unsupervised improvement framework for resolution shaping. one recent cost assessment is that application generates blurry images due all its gaussianity assumption doing thus l2 loss. to allow easier generation but altered resolutions visuals by graphics, we increase raw capacity of decoder parameters by employing overlapping blocks and delay connections, which effectively enable efficient optimization. to overcome functional limitation of variable forgetting, we propose to use images yielding a five - item manner versus coarse coarse fine. beginning the experimental use, the patented open - stage vae divides pixel evaluation into two components in which subsequent instruction design generates more data drawing on the repeated images occurring within the first component. since thus second component is instead of the vae model, it can gain other loss functions beyond the l2 loss alongside different learning criteria. the proposed framework x be only performed although contain higher than two ingredients. experiment results on the chip and celeba tests demonstrate their various upcoming multi - stage vae can manipulate further image as compare to those from the original vae.", "histories": [["v1", "Fri, 19 May 2017 21:51:30 GMT  (8140kb,D)", "http://arxiv.org/abs/1705.07202v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["lei cai", "hongyang gao", "shuiwang ji"], "accepted": false, "id": "1705.07202"}, "pdf": {"name": "1705.07202.pdf", "metadata": {"source": "CRF", "title": "Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image Generation", "authors": ["Lei Cai", "Hongyang Gao"], "emails": ["lei.cai@wsu.edu", "hongyang.gao@wsu.edu", "sji@eecs.wsu.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, progress in deep learning has promoted the development of generative models[5, 23, 3, 8, 1] that are able to capture the distributions of high-dimensional dataset and generate new samples. Variational auto-encoder (VAE)[20] is a powerful unsupervised learning framework for deep generative modeling. In VAE, the input data is encoded into latent variables before they are reconstructed by the decoder network. The VAE learns the transformation parameters by optimizing a variational lower bound of the true likelihood. The lower bound consists of two components. The first component is the Kullback-Leibler (KL) divergence between the approximate posterior and a prior distribution, which is commonly a normal distribution. The second component is the reconstruction loss given a latent variable. The VAE assumes that the output follows a normal distribution given the latent variable, thereby leading to an `2 loss in the objective function. It has been shown that the `2 loss leads to blurry images when the data are drawn from multi-modal distributions.\nTo make the VAE generate high quality images, some approaches have been proposed to improve the decoder network [6, 12, 2]. Since the decoder network is usually implemented with convolutional neural networks (CNNs) [15], we can increase the network depth to improve the capacity of decoder networks as in [13, 21, 22]. However, deeper networks can be difficult to optimize. Therefore, we employ the deep residual blocks, which are easy to optimize, to increase the capacity of decoder. By employing residual blocks in the decoder network, the VAE can generate high quality images. However, it still suffers from the effect of `2 loss and thus generates blurry images.\nIn this work, we propose a multi-stage VAE framework to generate high quality images. The key idea of multi-stage VAE is to generate images from coarse to fine. One challenge is that, since the decoder network is trained end-to-end, it is difficult to control the decoder network and make it\nar X\niv :1\n70 5.\n07 20\n2v 1\n[ cs\n.C V\n] 1\n9 M\nay 2\ngenerate images from coarse to fine. A simple solution is to train two models separately in which the first model generates a coarse image and the second model refines the coarse image. A drawback of this simple approach is that it reduces the efficiency of the model and involves more computational costs. To obtain fine images efficiently, we propose to employ an `2 loss in the middle of the decoder network, thus requiring coarse images to be generated in an intermediate stage of the decoder network. The remaining parts of the encoder network can be considered as a model that takes coarse images as inputs and generates refined versions of them as outputs. Indeed, the second network can be considered as a super-resolution network. Following this interpretation, we can employ any loss functions to refine the images in the super-resolution network[10], thereby overcoming the effect of `2 loss. In this way, we can generate images from coarse to fine and alleviate the effect of `2 loss without introducing extra parameters. Experimental results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can capture more details and generate sharper images than the original VAE. Some sample results are given in Figure 1"}, {"heading": "2 Multi-Stage Variational Auto-Encoder", "text": ""}, {"heading": "2.1 Variational Auto-Encoder", "text": "Variational auto-encoder (VAE) [11] is a generative model that is able to capture the probability distribution over high-dimensional datasets. For image generation tasks, given a dataset X = {x(i)}Ni=1, we wish to learn a distribution function that can capture the dependencies among pixels. To tackle this problem, we can train a distribution model p\u03b81(x), parameterized by \u03b81, to approximate the data distribution and optimize the model by maximizing the log likelihood as follows:\nlog p\u03b81(X) = log p\u03b81(x (1), . . . , x(N)) = N\u2211 i=1 log p\u03b81(x (i)). (1)\nHowever, probability distributions in high-dimensional space are very difficult to model. Thus, a lowdimensional latent variable z is usually introduced. It has been shown in [11] that the latent variable models can be optimized efficiently by maximizing a variational lower bound on the likelihood function as\nlog p\u03b81(x) \u2265 Eq\u03c6(z|x)[log p\u03b81(x|z)]\u2212DKL[q\u03c6(z|x)|p\u03b81(z)] = \u2212LV AE , (2)\nwhere LV AE is the loss function we need to minimize in VAE, and q\u03c6(z|x) is an approximate representation of the intractable p\u03b81(z|x) parameterized by q\u03c6. The output distribution in the first term is often Gaussian as:\np\u03b81(x|z) = N (x; f\u03b81(z), \u03c32I) = C \u00d7 exp ( \u2212 (x\u2212 f\u03b81(z)) 2\n2\u03c32\n) , (3)\nwhere C is a constant, and f\u03b81(\u00b7) is computed by CNNs [13]. Therefore, the log likelihood can be expressed as:\nlog p\u03b81(X|z) = N\u2211 i=1 logC \u00d7 exp ( \u2212 (x (i) \u2212 f\u03b81(z(i)))2 2\u03c32 ) , (4)\n= N \u00d7 C \u2212 1 2\u03c32 N\u2211 i=1 (x(i) \u2212 f\u03b81(z(i)))2, (5)\nwhereN\u00d7C is a constant that is irrelevant to f\u03b81(\u00b7) and can be ignored in optimization. The first term in LV AE is a `2 loss between x and f\u03b81(z). The second term corresponds to the Kullback-Leibler (KL) divergence between q\u03c6(z|x) and p\u03b81(z). VAE assumes that q\u03c6(z|x) = N (z;\u00b5\u03c6(x), \u2211 \u03c6(x))\nand p\u03b8(z) = N (z; 0, I). \u00b5\u03c6(x) and \u2211 \u03c6(x) are also implemented by CNNs. The second term in LV AE can be considered as a prior regularization. Therefore, the loss function of VAE can be written as LV AE = L`2 + Lprior, (6) where\nL`2 = \u2212Eq\u03c6(z|x)[log p\u03b81(x|z)] = 1\n2\u03c32 N\u2211 i=1 (x(i) \u2212 f\u03b81(z(i)))2, (7)\nLprior = DKL[q\u03c6(z|x)|p\u03b81(z)]. (8)"}, {"heading": "2.2 Deep Residual Variational Auto-Encoder", "text": "VAE has shown promising results in image generation tasks[4, 26, 14]. However, the images generated by VAE are blurry. This is caused by the `2 loss, which is based on the assumption that the data follow a single Gaussian distribution. When samples in dataset have multi-modal distribution, VAE cannot generate images with sharp edges and fine details. In VAE, images are generated by f\u03b81(\u00b7). It is possible to generate better images by using more complex model for f\u03b81(\u00b7). One solution is to employ the autoregressive model [19][24] for decoder function f\u03b81(\u00b7). In the autoregressive model, each pixel is conditioned on previously generated pixels. The autoregressive model increases the dependency between pixels and generates images with fine details. However, since it must generate images pixel by pixel, the prediction procedure of autoregressive model is much slower compared with other generative models such as VAE.\nSince the decoder of VAE is implemented with CNNs, a direct way to generate better images is to employ deeper networks, resulting in increased capacity of the decoder model [21]. The difficulty that deep neural networks facing is the degradation problem. As the network depth increases, the\nperformance of deep networks initially improves and then degrades rapidly. Although deep neural network models with higher capacity usually yield better performance, it is also challenging to optimize them. To efficiently train deep neural networks, the batch normalization method is proposed in Ioffe and Szegedy [9] by reducing internal covariate shift. Another solution is the residual learning framework proposed in He et al. [7], which employs the residual blocks and skip connection to back-propagate the gradients more efficiently in the network. The introduction of skip connection and residual block makes the optimization of deep neural networks more efficient. It is possible to employ deeper neural works on complex tasks. The residual learning framework has already been successfully applied to image recognition, object detection, and image super-resolution. To increase the capacity of decoder in VAE and optimize the model efficiently, we concatenate the original VAE decoder with several residual blocks. The architecture of deep residual VAE is illustrated in Figure 2. Given the original decoder f\u03b81(z), the deeper decoder networks can be denoted as f\u03b8 = f\u03b82(f\u03b81(z)), where f\u03b82(\u00b7) corresponds to the residual network. Compared with the original VAE decoder, the deeper decoder networks can capture more details. The loss function of deep residual VAE can be written as: LRSV AE = L`2 + Lprior, (9) where\nL`2 = \u2212Eq\u03c6(z|x)[log p\u03b81(x|z)] = 1\n2\u03c32 N\u2211 i=1 (x(i) \u2212 f\u03b82(f\u03b81(z(i))))2, (10)\nLprior = DKL[q\u03c6(z|x)|p\u03b81(z)]. (11)"}, {"heading": "2.3 Multi-Stage Variational Auto-Encoder", "text": "Experiment results in Section 3 show that deep residual VAE can capture more details than the original VAE by adding residual blocks to the decoder network. But the performance of deep residual VAE saturates rapidly as more residual blocks are added. As the depth of decoder network increases, the quality of generated images improves with smaller and smaller margins. This saturation effect is not a surprise as the network still employs `2 loss and thus generates blurry images. On the other hand, it is natural to use a step-by-step procedure to generate high-quality images. Specifically, in image generation, we can generate a coarse image with rough shape and basic colors first and then refine the coarse image to a high quality one. In VAE, the decoder network is trained end-to-end. Thus we cannot control the process of image generation. To make the decoder network generate images step-by-step, we need to divide the decoder network into two components, where the first component generates a coarse image, and the second component refines it to a high quality one. To achieve this, we propose to add a loss function at some location in the decoder network and enforce the network to generate images at that location.\nHere we use two stage deep VAE to illustrate how this idea works. Since in the first stage we only need to generate a coarse image, it is possible for the original VAE to accomplish this using the decode function f\u03b81(\u00b7). Then we need to build a model to refine the coarse images. When we require the sub-network f\u03b81(\u00b7) in the decoder of deep residual VAE to generate a coarse image, the input of f\u03b82(\u00b7) is not some arbitrary intermediate feature maps but a coarse image. In this way, the sub-network f\u03b82(\u00b7) acts as a model to refine the coarse images generated from f\u03b81(z). The architecture of the proposed multi-stage VAE is illustrated in Figure 3. The loss of the multi-stage VAE can be written as:\nLMSVAE = \u2212Eq\u03c6(z|x)[log p\u03b8(x|z)] +DKL[q\u03c6(z|x)|p\u03b8(z)] + Lrf (x, f\u03b82(f\u03b81(z))). (12)\nCompared with deep residual VAE, multi-stage VAE has two cost functions in the decoder network. The cost function of the first stage corresponds to \u2212Eq\u03c6(z|x)[log p\u03b8(x|z)] in the original VAE, and it is used to generate coarse images. The cost function of the second stage corresponds to the third term in Equation 12, and it is used to refine the coarse images. In multi-stage VAE framework, the second network is independent of the VAE model. Therefore, we can employ loss function on Lrf (x, f\u03b82(f\u03b81(z))). It also overcomes the effect of `2 loss under the assumption that data have a single Gaussian distribution. By employing different loss functions, the second model can recover more detailed information from blurry images. The LMSVAE can be written as:\nLMSVAE = L`2 + Lprior + Lrf (x, f\u03b82(f\u03b81(z))), (13)\nwhere\nLl2 = \u2212Eq\u03c6(z|x)[log p\u03b81(x|z)] = 1\n2\u03c32 N\u2211 i=1 (x(i) \u2212 f\u03b81(z(i)))2, (14)\nLprior = DKL[q\u03c6(z|x)|p\u03b81(z)]. (15)\nIn addition, generating higher resolution images (e.g., 128\u00d7128) is challenging for generative models. In multi-stage VAE, the coarse images generated in the first stage provide additional information and subsequently enables the multi-stage VAE to generate high-resolution images. The idea of tackling complex tasks in a multi-stage manner is also employed by Stack GAN [25]. Stack GAN employs two separate models to generate low-resolution images and high-resolution images, respectively. The two models are trained separately. However, our model divides the decoder network into two components with different loss functions, and both networks are trained jointly."}, {"heading": "2.4 Connections with Super-Resolution", "text": "We employ residual networks in the second stage of our multi-stage VAE to refine the coarse images generated in the first stage. The key idea of the second model is similar to the super-resolution residual net (SRResNet) [16]. In SRResNet, a low-resolution image is fed into a network composed of residual blocks and up-sampling layers. Then an image with high resolution is generated by SRResNet.\nIn multi-stage VAE, we employ a pixel-wise loss function to recover the details between lowresolution images and high-resolution images. Minimizing the pixel-wise loss encourages the model to generate the average of plausible solutions, thus leading to poor perceptual quality [4, 18]. A plausible loss function applied in image super-resolution tasks is the combination of Euclidean distances in feature space and adversarial loss. In fact, our multi-stage VAE framework can work with any plausible super-resolution model by replacing the loss function in Lrf and the model architecture of f\u03b82(\u00b7)."}, {"heading": "3 Experiments", "text": "In this section, we evaluate the deep residual VAE and multi-stage VAE on the MNIST and CelebA datasets and compare the quality of generated images with the original VAE. Results show that the proposed multi-stage VAE generates higher-resolution images as compared to those generated by the original VAE and deep residual VAE."}, {"heading": "3.1 Settings", "text": "CelebA [17] is a large scale face dataset that contains 202, 599 face images. The size of each face image is 178\u00d7218. Most prior VAE work using this dataset crops the images to 64\u00d7 64. In order to demonstrate the performance of our multi-stage VAE in generating high-resolution images, we crop the image to 128\u00d7128. We train three models for 200000 iterations. with batch size of 32 and a learning rate of 2e-4. The encoder model of VAE consists of four layers. Each layer consists of a convolution layer with stride 1 followed by a convolution layer with stride 2. The latent variable size of VAE is 512 for the CelebA dataset. The decoder network consists of four deconvolution layers. To generate images with high quality, five residual blocks are employed in the decoder network. The `1 loss is used in the objective function of the second network.\nThe MNIST is a handwritten digits dataset where the size of each image is 28\u00d728. We train three models on the training set of 60, 000 images. Each model is trained for 100, 000 iterations with a batch size of 256 and a learning rate of 1e-3. The encoder model of VAE consists of three convolution layers with a stride of 2. The latent variable size of VAE is set to 128. The decoder reconstructs the image from the latent variable with three deconvolution layers. To increase the complexity of decoder network, we concatenate the original VAE decoder with five residual blocks in the deep residual network. Each residual block consists of two convolution layers followed by a batch normalization layer. In multi-stage VAE, we add an `2 loss function at the location of the output in the original VAE. The residual network is employed to refine the coarse images generated in the first stage. To overcome the blurry effect of `2 loss, we employ `1 loss in the objective function of the second network."}, {"heading": "3.2 Results and Analysis", "text": "Figures 4 and 5 provide some reconstructed images by different models. We can see that the deep residual VAE can capture more details than the original VAE by employing more complex decoder\nnetwork. However, the images generated by deep residual VAE are still blurry due to the effect of `2 loss. We also observe that the effect of `2 loss is largely overcome by employing the multi-stage loss. The blurry region becomes clearer through the multi-stage refine process. These results demonstrate that the proposed multi-stage VAE goes beyond the bottleneck of increasing the capacity of decoder network, thereby effectively overcoming the blurry effect caused by the `2 loss.\nFigures 6 and 7 provide some reconstructed images and intermediate outputs of f\u03b81(\u00b7) by the deep residual VAE and multi-stage VAE. We can see that at the intermediate location in the decoder network of multi-stage VAE, a blurry image is generated, and it is fed into the residual networks. Through the refined operation of the second network, an image with high quality is generated. Since the whole decoder network of deep residual VAE only contains a single loss function, the generation process suffers from the effect of `2 loss. Therefore, the images generated by deep residual VAE are still blurry.\nFigures 8 and 9 provide some sample images generated by the original VAE, deep residual VAE, and multi-stage VAE when the models are trained on the CelebA and MNIST datasets. We can see that the images generated by the multi-stage VAE have higher resolution than those generated by other two methods. Also the images generated by the deep residual VAE are clearer than those generated by the original VAE. These results demonstrates that the proposed multi-stage VAE is effective in generating high resolution images."}, {"heading": "4 Conclusion and Future Work", "text": "In this work, we propose a multi-stage VAE that can generate higher quality images than the original VAE. The original VAE always generated blurry images due to the effect of `2 loss. To generate high quality images, we propose to improve the decoder capacity by increasing the network depth and employing residual blocks and skip connection. Although the deep residual VAE can capture more details, it still suffers from the effect of `2 loss and generates blurry images. To overcome the limitation of `2 loss, we propose to generate images from coarse to fine. To achieve this goal, we\nrequire the decoder network to generate a coarse image by employing a `2 loss function in the first stage. The subsequent stage in the decoder network acts as a super-resolution network that takes a blurry image as input and generates a high quality image. Since the super-resolution network is independent of the VAE model, it can employ other loss functions to overcome the the effect of `2 loss, thereby generating high quality images. Experimental results on the MNIST and CelebA datasets show that the proposed multi-stage VAE can overcome the effect of `2 loss and generate high quality images.\nOne interpretation of our proposed framework is that, the network in the second stage can be considered as a super-resolution module. Following this interpretation, we plan to use other model architectures and loss functions commonly used for super-resolution, such as the adversarial loss [16]. As has been mentioned, the proposed multi-stage framework can be generalized to more than two components. We plan to explore more stages in the future."}, {"heading": "Acknowledgments", "text": "This work was supported in part by National Science Foundation grants IIS-1615035 and DBI1641223, and by Washington State University. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research."}], "references": [{"title": "Deep generative stochastic networks trainable by backprop", "author": ["Yoshua Bengio", "Eric Laufer", "Guillaume Alain", "Jason Yosinski"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Variational lossy autoencoder", "author": ["Xi Chen", "Diederik P Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Density estimation using real nvp", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": "arXiv preprint arXiv:1605.08803,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Generating images with perceptual similarity metrics based on deep networks", "author": ["Alexey Dosovitskiy", "Thomas Brox"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Pixelvae: A latent variable model for natural images", "author": ["Ishaan Gulrajani", "Kundan Kumar", "Faruk Ahmed", "Adrien Ali Taiga", "Francesco Visin", "David Vazquez", "Aaron Courville"], "venue": "arXiv preprint arXiv:1611.05013,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning and releaming in boltzmann machines", "author": ["Geoffrey E Hinton", "Terrence J Sejnowski"], "venue": "Parallel Distrilmted Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1986}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Accurate image super-resolution using very deep convolutional networks", "author": ["Jiwon Kim", "Jung Kwon Lee", "Kyoung Mu Lee"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "arXiv preprint arXiv:1312.6114,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Improving variational autoencoders with inverse autoregressive flow", "author": ["Diederik P Kingma", "Tim Salimans", "Rafal Jozefowicz", "Xi Chen", "Ilya Sutskever", "Max Welling"], "venue": "In Advances In Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "S\u00f8ren Kaae S\u00f8nderby", "Hugo Larochelle", "Ole Winther"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Photo-realistic single image super-resolution using a generative adversarial network", "author": ["Christian Ledig", "Lucas Theis", "Ferenc Husz\u00e1r", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang"], "venue": "arXiv preprint arXiv:1609.04802,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In Proceedings of International Conference on Computer Vision (ICCV),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["Michael Mathieu", "Camille Couprie", "Yann LeCun"], "venue": "arXiv preprint arXiv:1511.05440,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In Proceedings of The 31st International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Conditional image generation with pixelcnn decoders", "author": ["Aaron van den Oord", "Nal Kalchbrenner", "Lasse Espeholt", "Oriol Vinyals", "Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Stackgan: Text to photorealistic image synthesis with stacked generative adversarial networks", "author": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaolei Huang", "Xiaogang Wang", "Dimitris Metaxas"], "venue": "arXiv preprint arXiv:1612.03242,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Towards deeper understanding of variational autoencoding models", "author": ["Shengjia Zhao", "Jiaming Song", "Stefano Ermon"], "venue": "arXiv preprint arXiv:1702.08658,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}], "referenceMentions": [{"referenceID": 4, "context": "In recent years, progress in deep learning has promoted the development of generative models[5, 23, 3, 8, 1] that are able to capture the distributions of high-dimensional dataset and generate new samples.", "startOffset": 92, "endOffset": 108}, {"referenceID": 22, "context": "In recent years, progress in deep learning has promoted the development of generative models[5, 23, 3, 8, 1] that are able to capture the distributions of high-dimensional dataset and generate new samples.", "startOffset": 92, "endOffset": 108}, {"referenceID": 2, "context": "In recent years, progress in deep learning has promoted the development of generative models[5, 23, 3, 8, 1] that are able to capture the distributions of high-dimensional dataset and generate new samples.", "startOffset": 92, "endOffset": 108}, {"referenceID": 7, "context": "In recent years, progress in deep learning has promoted the development of generative models[5, 23, 3, 8, 1] that are able to capture the distributions of high-dimensional dataset and generate new samples.", "startOffset": 92, "endOffset": 108}, {"referenceID": 0, "context": "In recent years, progress in deep learning has promoted the development of generative models[5, 23, 3, 8, 1] that are able to capture the distributions of high-dimensional dataset and generate new samples.", "startOffset": 92, "endOffset": 108}, {"referenceID": 19, "context": "Variational auto-encoder (VAE)[20] is a powerful unsupervised learning framework for deep generative modeling.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "To make the VAE generate high quality images, some approaches have been proposed to improve the decoder network [6, 12, 2].", "startOffset": 112, "endOffset": 122}, {"referenceID": 11, "context": "To make the VAE generate high quality images, some approaches have been proposed to improve the decoder network [6, 12, 2].", "startOffset": 112, "endOffset": 122}, {"referenceID": 1, "context": "To make the VAE generate high quality images, some approaches have been proposed to improve the decoder network [6, 12, 2].", "startOffset": 112, "endOffset": 122}, {"referenceID": 14, "context": "Since the decoder network is usually implemented with convolutional neural networks (CNNs) [15], we can increase the network depth to improve the capacity of decoder networks as in [13, 21, 22].", "startOffset": 91, "endOffset": 95}, {"referenceID": 12, "context": "Since the decoder network is usually implemented with convolutional neural networks (CNNs) [15], we can increase the network depth to improve the capacity of decoder networks as in [13, 21, 22].", "startOffset": 181, "endOffset": 193}, {"referenceID": 20, "context": "Since the decoder network is usually implemented with convolutional neural networks (CNNs) [15], we can increase the network depth to improve the capacity of decoder networks as in [13, 21, 22].", "startOffset": 181, "endOffset": 193}, {"referenceID": 21, "context": "Since the decoder network is usually implemented with convolutional neural networks (CNNs) [15], we can increase the network depth to improve the capacity of decoder networks as in [13, 21, 22].", "startOffset": 181, "endOffset": 193}, {"referenceID": 9, "context": "Following this interpretation, we can employ any loss functions to refine the images in the super-resolution network[10], thereby overcoming the effect of `2 loss.", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "1 Variational Auto-Encoder Variational auto-encoder (VAE) [11] is a generative model that is able to capture the probability distribution over high-dimensional datasets.", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "It has been shown in [11] that the latent variable models can be optimized efficiently by maximizing a variational lower bound on the likelihood function as log p\u03b81(x) \u2265 Eq\u03c6(z|x)[log p\u03b81(x|z)]\u2212DKL[q\u03c6(z|x)|p\u03b81(z)] = \u2212LV AE , (2) where LV AE is the loss function we need to minimize in VAE, and q\u03c6(z|x) is an approximate representation of the intractable p\u03b81(z|x) parameterized by q\u03c6.", "startOffset": 21, "endOffset": 25}, {"referenceID": 12, "context": "where C is a constant, and f\u03b81(\u00b7) is computed by CNNs [13].", "startOffset": 54, "endOffset": 58}, {"referenceID": 3, "context": "2 Deep Residual Variational Auto-Encoder VAE has shown promising results in image generation tasks[4, 26, 14].", "startOffset": 98, "endOffset": 109}, {"referenceID": 25, "context": "2 Deep Residual Variational Auto-Encoder VAE has shown promising results in image generation tasks[4, 26, 14].", "startOffset": 98, "endOffset": 109}, {"referenceID": 13, "context": "2 Deep Residual Variational Auto-Encoder VAE has shown promising results in image generation tasks[4, 26, 14].", "startOffset": 98, "endOffset": 109}, {"referenceID": 18, "context": "One solution is to employ the autoregressive model [19][24] for decoder function f\u03b81(\u00b7).", "startOffset": 51, "endOffset": 55}, {"referenceID": 23, "context": "One solution is to employ the autoregressive model [19][24] for decoder function f\u03b81(\u00b7).", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Since the decoder of VAE is implemented with CNNs, a direct way to generate better images is to employ deeper networks, resulting in increased capacity of the decoder model [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 8, "context": "To efficiently train deep neural networks, the batch normalization method is proposed in Ioffe and Szegedy [9] by reducing internal covariate shift.", "startOffset": 107, "endOffset": 110}, {"referenceID": 6, "context": "[7], which employs the residual blocks and skip connection to back-propagate the gradients more efficiently in the network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "The idea of tackling complex tasks in a multi-stage manner is also employed by Stack GAN [25].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "The key idea of the second model is similar to the super-resolution residual net (SRResNet) [16].", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "Minimizing the pixel-wise loss encourages the model to generate the average of plausible solutions, thus leading to poor perceptual quality [4, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 17, "context": "Minimizing the pixel-wise loss encourages the model to generate the average of plausible solutions, thus leading to poor perceptual quality [4, 18].", "startOffset": 140, "endOffset": 147}, {"referenceID": 16, "context": "1 Settings CelebA [17] is a large scale face dataset that contains 202, 599 face images.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Following this interpretation, we plan to use other model architectures and loss functions commonly used for super-resolution, such as the adversarial loss [16].", "startOffset": 156, "endOffset": 160}], "year": 2017, "abstractText": "Variational auto-encoder (VAE) is a powerful unsupervised learning framework for image generation. One drawback of VAE is that it generates blurry images due to its Gaussianity assumption and thus `2 loss. To allow the generation of high quality images by VAE, we increase the capacity of decoder network by employing residual blocks and skip connections, which also enable efficient optimization. To overcome the limitation of `2 loss, we propose to generate images in a multi-stage manner from coarse to fine. In the simplest case, the proposed multi-stage VAE divides the decoder into two components in which the second component generates refined images based on the course images generated by the first component. Since the second component is independent of the VAE model, it can employ other loss functions beyond the `2 loss and different model architectures. The proposed framework can be easily generalized to contain more than two components. Experiment results on the MNIST and CelebA datasets demonstrate that the proposed multi-stage VAE can generate sharper images as compared to those from the original VAE.", "creator": "LaTeX with hyperref package"}}}