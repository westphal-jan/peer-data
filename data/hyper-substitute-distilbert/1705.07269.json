{"id": "1705.07269", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-May-2017", "title": "Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning", "abstract": "deep reinforcement learning ( drl ) methods later looked toward obtaining an interesting numbering like high - priority visual text generating domains. between all such major decision making problems, those with discrete vector matrices often tend to have underlying compositional structure in the underlying action space. such work spaces uniformly contain actions disguised as go northward, go reverse as well as go diagonally clockwise and left ( abstraction involves explicit composition of the former two actions ). the evolution of control policies to software domains areas traditionally often modeled seemingly missing this inherent domain structure in the action interface. students use fundamentally new algorithm pathway, mapping action value representations ( ran ) wherein we execute distributed control direction vector using separate deep reinforcement linguistic computation into simultaneous components, analogous to decomposing a causal structure terms three weighted vector basis vectors. this architectural algorithm across vector control learning technique allows the agent to create simultaneously positive actions randomly, simply executing zero one of them. scholars demonstrate that far scored considerable gains on top over two drl algorithms in atari 2600 : fara3c destroys a3c ( asynchronous positive sensing abilities ) in 22 out of 14 tasks and faraql outperforms aql ( asynchronous variable - step q - learning ) eliminating 9 out of 162 users.", "histories": [["v1", "Sat, 20 May 2017 07:18:40 GMT  (1662kb,D)", "http://arxiv.org/abs/1705.07269v1", "11 pages + 7 pages appendix"]], "COMMENTS": "11 pages + 7 pages appendix", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["sahil sharma", "aravind suresh", "rahul ramesh", "balaraman ravindran"], "accepted": false, "id": "1705.07269"}, "pdf": {"name": "1705.07269.pdf", "metadata": {"source": "CRF", "title": "Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning", "authors": ["Sahil Sharma", "Balaraman Ravindran"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Traditional Reinforcement Learning (RL) [Sutton & Barto, 1998] algorithms have worked with relatively simple environments (such as grid worlds) wherein policy estimates can be constructed using tabular methods or simple linear parameterizations. The state representation in such problems often consists of hand-crafted features. Recent advances in Deep Learning (DL) [Bengio et al., 2009; LeCun et al., 2015] have enabled RL methods to scale to problems with exponentially larger state spaces and even continuous action domains. This combination of RL based cost functions and DL based compositional and hierarchical representations for the state, policies and value functions has resulted in the field of Deep Reinforcement Learning (DRL). Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al., 2013], MuJoCo [Todorov et al., 2012], TORCS Wymann et al. [2000] and the board game of Go [Silver et al., 2016].\n* These authors contributed equally\nar X\niv :1\n70 5.\n07 26\n9v 1\n[ cs\n.L G\n] 2\nMany such DRL algorithms operate on discrete action space domains such as Atari 2600. The total number of actions in this domain is eighteen. These eighteen actions have been visualized in Figure 1. Observe that these eighteen actions, although presented to the DRL algorithms as the smallest indivisible units of action, are in fact not indivisible at all. There exists an inherent underlying compositional structure in this action space which a DRL algorithm can potentially take advantage of, while training. Consider a DRL agent that executes the action go diagonally up and left and gets some reward corresponding to this action. The key insight in this work is that this feedback can be used to learn not only about the go diagonally up and left action but also the actions go up and go left. Hence, every time a diagonal step is executed, it is possible to learn about the individual action factors as well. In the Atari domain, the action space can be decomposed along three independent dimensions: vertical motion ({go up, go down, don\u2019t move vertically}), horizontal motion ({go left, go right, don\u2019t move horizontally}) and firing ({fire, don\u2019t fire}). This work explicitly factors policies and action-value functions along these dimensions by building the factoring into the DRL agent\u2019s network architecture. The notion of factoring is not new to the DRL setup. Sharma et al. [2017] have demonstrated remarkable improvements in a variety of domains using a framework that factors the policy into one for choosing actions and another for choosing repetitions.\nThe key motivations for our work are principles underlying biological systems, that have shown to learn representations in an independent and orthogonal manner [Baumann et al., 2011; Bell & Sejnowski, 1997]. At an abstract level, our work is also similar to intra-option learning [Sutton et al., 1998] frameworks, which revolves around the idea of learning about an option while executing another. Arguably, humans also make decisions in a similar manner, with the fundamental unit of decisions being the orthogonal directions along which the decision space can be decomposed. While our factoring scheme can be used to extend any DRL algorithm which operates with compositional discrete action spaces, in this paper we extend the Asynchronous Advantage Actor Critic (A3C) and the Asynchronous n-step Q-Learning [Mnih et al., 2016b] algorithms. We demonstrate empirically that our proposed paradigm , Factored Action space Representations (FAR), provides considerable improvements over the A3C and AQL algorithms. We also provide an analysis of the factored policy learned using FARA3C that demonstrates its robustness compared to A3C policies."}, {"heading": "2 Preliminaries", "text": ""}, {"heading": "2.1 Q-learning", "text": "Some control algorithms estimate the optimal action-value function Q\u2217(s, a) which guides the policy executed by the RL agent. One such off-policy TD-learning algorithm [Sutton, 1988] is Q-learning [Watkins & Dayan, 1992]. Q-learning results in the convergence of an estimated Q-function to the optimal Q-function. The Q-learning updates are give by the equation:\nQ(st, at)\u2190 Q(st, at) + \u03b1 ( rt+1 + \u03b3max\na\u2032 Q(st+1, a\n\u2032)\u2212Q(st, at) )\nA policy can be derived from the Q-function estimate by selecting any action a such that a \u2208 argmax\na\u2032 Q(s, a\u2032)."}, {"heading": "2.2 Advantage Actor-Critic", "text": "Actor-Critic algorithms [Konda & Tsitsiklis, 2000] are policy-gradient methods [Sutton & Barto, 1998]. Most Actor Critic algorithms use parametric representations for the actor (\u03c0(s; \u03b8)) and the critic V (s;w). A biased sample-estimate for the policy gradient is given by( \u2207\u03b8log(\u03c0(at|st; \u03b8t) ) Q(st, at). To lower variance in the updates, a baseline term is introduced and\nthe sample-estimate of policy gradient becomes ( \u2207\u03b8t log(\u03c0(at|st; \u03b8t) ) ( Q(st, at)\u2212 V (st) ) . The\nsecond multiplicand in the sample-estimate is the known as the advantage function A(st, at):\nA(st, at) = Q(st, at)\u2212 V (st) = rt+1 + \u03b3V (st+1)\u2212 V (st) where rt+1 + \u03b3V (st+1) is used as an estimate for Q(st, at). This estimation for Q(st, at) allows the agent to model the advantage function by modeling only the value function V (s)."}, {"heading": "2.3 Asynchronous Advantage Actor-Critic (A3C)", "text": "Extending the Advantage Actor-Critic algorithms naively in the DRL setup fails to work because the stochastic gradient descent algorithms commonly used with such DRL setups assume that the input samples are independent and are identically distributed. An on-policy Actor-Critic algorithm parametrized by neural networks has highly correlated inputs and therefore performs poorly when gradient based methods are used. Asynchronous Advantage Actor-Critic (A3C) methods [Mnih et al., 2016b] overcome this problem by using asynchronous parallel actor-learners which simultaneously explore different parts of state space. Each learner maintains its own set of parameters which are routinely synchronized with the other learners. Using parallel actor learners ensures that their exploration of different parts of the state space translates into updates for the neural network that are relatively uncorrelated."}, {"heading": "2.4 Asynchronous N-step Q-learning (AQL)", "text": "A modified version of Q-learning, uses an n-step return based TD-target [Peng & Williams, 1996; Watkins & Dayan, 1992] in order to achieve faster reward propagation and trade-off between bias and variance in the estimation of the action-value function. The modified update equation is given by:\nQ(st, at)\u2190 Q(st, at) + \u03b1 ( n\u2211 i=1 Rt+i + \u03b3max a\u2032 Q(st+n, a \u2032)\u2212Q(st, at) ) Similar to the Advantage Actor-Critic, n-Step Q-learning, has been extended to work in the DRL setup by coming up with asynchronous n-step Q learning [Mnih et al., 2016b]."}, {"heading": "3 Factored Action Representations for Deep Reinforcement Learning", "text": "The Factored Action Representations (FAR) framework can extend any DRL algorithm that operates on problems with compositional discrete action spaces. This includes algorithms which model Q-functions (like DQN [Mnih et al., 2015] or asynchronous n-step Q-learning [Mnih et al., 2016b]) and actor-critic algorithms (like A3C [Mnih et al., 2016b]). In the actor critic methods, FAR extends only the policy modeled by the actor; the critic is not modified. In the deep Q-learning methods, FAR modifies the representation for the action-value function that the DRL algorithm models. Let X represent either the policy of an actor-critic DRL algorithm or the Q-function of a deep Q-learning algorithm. FAR represents X (Xi corresponds to the action ai) using a factored representation. Each of the factors represents a different dimension of the composite action space. We explain FAR with the Atari domain action space as an example. Figure 2 visualizes a factoring of X over the complete Atari action space. Any action in the complete Atari action space can be represented as a tuple (hi, vi, fi) with vi \u2208 {go up, go down, don\u2019t move vertically}, hi \u2208 {go left, go right, don\u2019t move horizontally} and fi \u2208 {fire, don\u2019t fire}. The choice of the factors over which X is decomposed depends on the set of possible actions, for a given task. This decomposed representation of X allows the DRL agent to learn X corresponding to multiple actions simultaneously, while executing a single action. When the action a = up-right-fire is executed, the parameters corresponding to the individual factors of Xa: up, right and fire get updated. Hence Xup-fire, Xright-fire and Xup-left are also adjusted, and not just the Xa.\nLet S denote set of all states in an MDP and A denote the discrete action set for a DRL agent. We claim that often, action spaces A are compositional and thus allow the decomposition of any action a \u2208 A into n independent action-factors [a1, a2, \u00b7 \u00b7 \u00b7 , an] such that ai \u2208 Ai, where Ai is the set of values that factor i can take. We claim that instead of modeling X over A, the agent would be better off, modeling the individual components of X over the factor-spaces Ai. These individual components of X are realized using independent output layers (referred to as factor-layers hereafter) of a neural network f1, f2,..., fn where fi corresponds to Ai and has size |Ai|. X can be written in terms of the factor-layer outputs as: X(s, a) =M(f1(a1|s), f2(a2|s), ..., fn(an|s)) where s \u2208 S . In this equation, the combination function M can have any suitable parameterized or non-parametrized form. If X is a policy, then \u2211 a\u2208AX(a|s) = 1 must be enforced. Detailed training algorithms for training FAR variants proposed in this paper can be found in Appendix B."}, {"heading": "3.1 Visualization of Action Factoring for Atari", "text": "The full action space for Atari has 18 basis actions. These basis actions can be decomposed into three independent action factors. The first factor encodes horizontal movement (ah = left/right/no horizontal movement). The second factor encodes vertical movement (av = up/down/no vertical movement). The third factor encodes whether to fire or not (af = fire/don\u2019t fire). A visualization of this action space decomposition for the Atari domain is shown in Figure 2. This composite action space can be visualized as a 3-dimensional cuboid of dimensions 3\u00d7 3\u00d7 2 where each 1\u00d7 1\u00d7 1 cell represents a composite action. The axes of the cuboid represent the action-factors."}, {"heading": "3.2 FARA3C", "text": "We describe the instantiation of FAR for the A3C algorithm in this subsection. X is the policy of the actor part of A3C. Let ph, pv and pf denote the action-factors corresponding to horizontal, vertical and the firing dimensions. The action-factors are combined using a combination function M = softmax \u25e6m. The action policy is given by:\n\u03c0(a|s) = softmax(m(ph(ah|s), pv(av|s), pf (af |s))) In the equation, a = [ah, av, af ] \u2208 A, ah \u2208 Ah, av \u2208 Av and af \u2208 Af . Figure 3 contains a visual depiction of FARA3C\u2019s architecture."}, {"heading": "3.3 The Additive Combination Function", "text": "The additive function is one non-parametric choice for m in the definition of M . For FARA3C, the additive combination function has the form:\n\u03c0(a|s) = softmax(ph(ah|s) + pv(av|s) + pf (af |s)))\nThis policy can now be re-written as a product of three probability distributions:\n\u03c0(a|s) = softmax(ph(ah|s))\u00d7 softmax(pv(av|s))\u00d7 softmax(pf (af |s)) Consider an action a = [ah, av, af ] \u2208 A, such that ah \u2208 Ah, av \u2208 Av and af \u2208 Af . Sampling a composite action a from \u03c0(a|s) is equivalent to independently sampling ai from \u03c0i = softmax(pi(ai|s)) where i \u2208 {h, v, f}. This sub-section demonstrates that the choice of additive combination function for A3C gives the action policy a nice alternate interpretation in terms a product of constituent factors\u2019 policies (such representations have been explored in other works such as Sharma et al. [2017])."}, {"heading": "3.4 FARAQL", "text": "We describe the instantiation of our framework for the AQL algorithm in this subsection. X is the Q-function modeled by AQL. Let Qh, Qv and Qf denote the set of action-factors corresponding to horizontal, vertical and the firing dimension. The action-factors are combined using a combination function M . In this case:\nQ(s, a) =M(Qh(ah|s), Qv(av|s), Qf (af |s)) where, a = [ah, av, aw] \u2208 A, ah \u2208 Ah, av \u2208 Av and af \u2208 Af ."}, {"heading": "4 Experiments and results", "text": "Our experiments are geared towards answering the following questions:\n1. What kind of combination functions (M) work well for action space factoring? 2. Does action factoring improve the performance of DRL algorithms (A3C & AQL)? 3. Does FAR learn fundamentally robust policy representations, compared to the baselines?"}, {"heading": "4.1 Choice of the Combination Function (M)", "text": "In this subsection we answer the first question. For chosing a good M , many competing FARA3C networks were trained, each implementing a different non-parametric choice of M on the Alien task in the Atari domain. The networks were trained for 50 million steps starting from the same random initialization. The combination functions M that we experimented with were of the form softmax(m(f1(a1|s), f2(a2|s), ..., fn(an|s))), where m \u2208 {Summation,Multiplication,Arithmetic Mean,Harmonic Mean,Geometric Mean,Minimum}. Note that the Arithmetic Mean and the Summation functions are identical barring a scaling factor of n (number of action factors). This subtle difference could however lead to different policies being learned.\nWe observed that choosing m = Summation resulted in the highest performance for FARA3C as indicated by Figure 4. We decided to stick to a similar functional form for M for the AQL case as well, to demonstrate the robustness of our choice of the combination function. Hence the Q-values are computed using the summation of the factor-layer outputs as:\nQ(s, a) = Qh(ah|s) +Qv(av|s) +Qf (af |s)) (1)"}, {"heading": "4.2 Gameplay Experiments and Results: FARA3C", "text": "We trained FARA3C networks as depicted in Figure 3 on 14 tasks in the Atari domain. For each of the tasks, we trained a network with three different random seeds and averaged the results to estimate the performance of the algorithm. A baseline A3C agent was also trained with the same random seeds. A comparison of FARA3C and A3C agents\u2019 performance is in Figure 5. All the results of our experiments are tabulated in Table 1 in Appendix C. The evolution of the game-play performance on each of the tasks was plotted versus the training time. The training curves have been averaged over the three random seeds. Figure 6 contains the training curves for six of the games. Training curves for all the games can be found in Appendix D. Detailed explanations about the training procedure, the evaluation procedure and the hyper-parameters can be found in Appendix A."}, {"heading": "4.3 Gameplay Experiments and Results: FARAQL", "text": "Similar to Figure 3, a modified network was constructed for FARAQL using equation 1 and trained for thirteen tasks in Atari domain. Performance was estimated by averaging the performances across three random seeds. A baseline AQL agent was trained using the same random seeds. A visual comparison of the AQL and the FARAQL agent is presented in Figure 8. Training curves for six of the games have been presented in Figure 7. Training curves for all the games can be found in Appendix D."}, {"heading": "5 Analysis: Test for robustness of policies learned by FARA3C & A3C", "text": "We claim that policies learned by FARA3C are more robust compared to those learned by A3C. This is because using a factored policy representation enables the FARA3C agents to learn about multiple actions while executing one. As a result, one would expect FARA3C to have a better ordering of actions in the policy than A3C. In order to validate this hypothesis empirically, we conducted experiments comparing the robustness of the A3C and FARA3C policies.\n5.1 Uniformly Random from best-K analysis\nThe experiment is as follows: A trained agent is taken. With probability 1 \u2212 , the agent samples actions from the learned policy. With probability , it samples actions uniformly at random from the best k actions. The use of introduces noise in the learned policy and the agents with the more robust policies would demonstrate a lower drop in performance, as increases. The value for was varied from 0 to 0.5 in steps of 0.05 and the corresponding normalized performances were plotted. The individual curves have been normalized using the maximum score to ensure a fair comparison between the FARA3C and the A3C agents by comparing only the relative changes. The experiments in this sub-section were conducted with k = 2. From Figure 10, observe that the FARA3C agent is comparatively more robust against policy corruption based on ."}, {"heading": "5.2 Noise Injection analysis", "text": "In this analysis, a trained agent for a particular task is taken. The policy of this agent is then corrupted by injecting noise based on using the temperature (Z). If the policy of the agent is written as \u03c0(a|s) = softmax(G(s)), the corrupted policy \u03c0cor(a|s) of the agent can be represented as \u03c0cor(a|s) = softmax(G(s)/Z). Increasing Z results in the injection of more noise into the policy. Hence a more robust policy is expected to have a smaller drop in performance on increasing Z. The hyper-parameter Z was varied from 1.0 to 3.0 in steps of 0.25 and the performance was plotted after normalizing it against the maximum score obtained by the agent. From Figure 9 it is clear that for most tasks, FARA3C agents are more robust to noise injection."}, {"heading": "6 Conclusion and Future Work", "text": "We propose a novel framework (FAR) for the decomposition of policies and action-value functions over a discrete action space. FAR factors a policy/value function into independent components and models those using independent output layers of a neural network. The FAR framework allows DRL agents to exploit the underlying compositional structure in discrete action spaces found in common RL domains to learn better policies and action-value functions. We empirically demonstrate the superiority of FAR to the baseline methods considered. A possible extension of this framework would be to combine action-space factoring with the concept of action repetition as discussed by Sharma et al. [2017]. Action repetition could act as yet another factor of the action space and this extension could allow one to capture a large set of macro actions."}, {"heading": "Appendix A: Experimental Details", "text": "In this appendix, we document the experimental details for all of our experiments. Note that all the reported game-play performances and graphs are averages across 3 random seeds to ensure that the comparisons are robust to random starting points for the parameter vectors.\nFARA3C and A3C\nThe human-starts evaluation paradigm proposed by Mnih et al. [2016b] is hard to replicate in the absence of the same human-tester trajectories. Hence, we use the same training and evaluation procedure as Sharma et al. [2017]."}, {"heading": "On hyper-parameters", "text": "We used the LSTM-variant of A3C [Mnih et al. [2016b]] algorithm for the both the FARA3C and the A3C experiments. The async-rmsprop algorithm [Mnih et al. [2016b]] was used for updating parameters with the same hyper-parameters as in Mnih et al. [2016b]. The initial learning rate used was 10\u22123 and it was linearly annealed to 0 over 100 million steps. The n used in n-step returns was 20. Entropy regularization was used to encourage exploration, similar to Mnih et al. [2016b]. The \u03b2 for entropy regularization was found to be 0.01 after hyper-parameter tuning, both for FARA3C and A3C. The \u03b2 was tuned in the set {0.01, 0.02}. The optimal learning rate was found to be 7\u00d7 10\u22124 for both FARA3C and A3C separately. The learning rate was tuned over the set {7\u00d7 10\u22124, 10\u22123}. The discounting factor for rewards was retained at 0.99 since it seems to work well for a large number of methods [Mnih et al., 2016b; Sharma et al., 2017; Jaderberg et al., 2017].\nAll the models were trained for 100 million time steps. Evaluation was done after every 1 million steps of training and followed the strategy described in Sharma et al. [2017]. This evaluation was done for 100 episodes, with each episode\u2019s length capped at 20000 steps, to arrive at an average score. The evolution of this average game-play performance with training progress has been demonstrated for a few games in Figure 6. An expanded version of the figure for all the games can be found in Appendix D. Table 1 in Appendix C contains the raw scores obtained by FARA3C and A3C agents on 15 Atari 2600 games. The evaluation was done using the best agent obtained after training for 100 million steps (where the best agent was chosen according to the model selection paradigm outlines in citedqn).\nArchitecture details\nWe used a low level architecture similar to Mnih et al. [2016b]; Sharma et al. [2017] which in turn uses the same low level architecture as Mnih et al. [2015]. Figure 3 contains a visual depiction of the network used for FARA3C. The same architecture was used for FARA3C and A3C agents and has been described below: The first three layers are convolutional layers with same filter sizes, strides, padding and number of filters as Mnih et al. [2015, 2016b]; Sharma et al. [2017]. These convolutional layers are followed by two fully connected (FC) layers and an LSTM layer. A policy and a value function are derived from the LSTM outputs using two different output heads. The number of neurons in each of the FC layers and the LSTM layers is 256.\nSimilar to Mnih et al. [2016b] the Actor and Critic networks share all but the final output layer. Each of the two functions: policy and value function are realized with a different final output layer, with the value function outputs having no non-linearity and with the policy and having a softmax-non linearity as output non-linearity, to model the multinomial distribution.\nFARAQL and AQL\nWe used a highly tuned open-source implementation of asynchronous Q-learning found at: https://github.com/Kaixhin/Atari.\nAQL\nThe learning rate was set to be 7\u00d7 10\u22124. The value of n for n-step returns was set to be 5. In keeping with the -greedy strategy highlighted for training asynchronous n-step Q-learning in Mnih et al. [2016b] we randomly sampled in each thread and the was decayed to various different values from the set {0.05, 0.1, 0.5}. The training period lasted 80 million time steps. The starting value of epsilon was 1 and it was linearly decayed over 64 million steps to its final value. The same network architecture as Mnih et al. [2013] was used for all the experiments.\nFARAQL\nTo better understand the generalization capabilities of our FAR framework in general and FARQL algorithm in particular, we decided to use the exact same hyper-parameter choices as AQL for FARAQL. None of the hyper-parameters for FARAQL were tuned specifically for this algorithm. This represents a hyper-parameter setting which is not very favorable to FARAQL. Even in this setting, we observe that FARAQL significantly outperforms AQL. Tuning the hyper-parameters specifically for FARAQL could result in a further improvement in performance."}, {"heading": "Appendix B: The FARA3C Algorithm", "text": "In this appendix, we present pseudo-code versions of the training algorithm for FARA3C and FARAQL.\nFARA3C\nAlgorithm 1 FARA3C 1: // Assume global shared parameter vectors \u03b8 and w 2: // Assume global step counter (shared) T = 0 3: 4: n \u2190 Number of independent factors 5: Tmax \u2190 Total number of training steps for the FARA3C agent 6: \u03c0 \u2190 Policy of the agent 7: f \u2190 List of factor heads for independent factors 8: Initialize local thread\u2019s step counter t\u2190 1 9: 10: repeat 11: tinit = t 12: d\u03b8 \u2190 0 13: dw \u2190 0 14: // Assume local thread\u2019s parameters as \u03b8\u2032 and w\u2032 15: Synchronize local thread parameters \u03b8\u2032 = \u03b8 and w\u2032 = w 16: Obtain state stinit 17: repeat 18: \u03c0(a|st) =M(f1(a1|st), f2(a2|st), \u00b7 \u00b7 \u00b7 , fn(an|st)) 19: Sample at \u223c \u03c0(a|st; \u03b8\u2032) 20: Execute action at to obtain reward rt and observe next state st+1 21: t\u2190 t+ 1 22: T \u2190 T + 1 23: Obtain state st 24: until st is terminal or t == tinit + tmax 25: if st is terminal then 26: R \u2190 0 27: else 28: R\u2190 V (st, w\u2032) 29: for i \u2208 {t\u2212 1, . . . tinit} do 30: R\u2190 ri + \u03b3R 31: Accumulate gradients for \u03b8\u2032: d\u03b8 \u2190 d\u03b8 +\u2207\u03b8\u2032 log(\u03c0(ai|si; \u03b8\u2032)(R\u2212 V (si;w\u2032)) 32: Accumulate gradients for w\u2032: dw \u2190 dw + \u2202(R\u2212V (si;w \u2032))2\n\u2202w\u2032\n33: Perform asynchronous update of \u03b8 using d\u03b8 and w using dw 34: until T > Tmax\nFARAQL\nAlgorithm 2 FARAQL 1: // Assume global shared parameter vector \u03b8 2: // Assume global shared parameter vector for target network \u03b8\u2212 3: // Assume global step counter (shared) T = 0 4: 5: n \u2190 Number of independent factors 6: Tmax \u2190 Total number of training steps for the FARAQL agent 7: Q \u2190 Action-value function of the agent 8: q \u2190 List of factor heads for independent factors 9: 10: Initialize local thread\u2019s step counter t\u2190 1 11: Initialize target network parameters \u03b8\u2212 \u2190 \u03b8 12: Initialize local thread\u2019s parameter \u03b8\u2032 = \u03b8 13: repeat 14: tinit = t 15: d\u03b8 \u2190 0 16: dw \u2190 0 17: Synchronize local thread parameters \u03b8\u2032 = \u03b8 18: Obtain state stinit 19: repeat 20: Q(st, a) =M(Q1(a1|st), Q2(a2|st), \u00b7 \u00b7 \u00b7 , Qn(an|st)) 21: at \u223c the \u2212 greedy policy derived from Q(st, a; \u03b8\u2032) 22: Execute action at to obtain reward rt and observe next state st+1 23: t\u2190 t+ 1 24: T \u2190 T + 1 25: Obtain state st 26: until st is terminal or t == tinit + tmax 27: if st is terminal then 28: R \u2190 0 29: else 30: R\u2190 maxa(st, a; \u03b8\u2212) 31: for i \u2208 {t\u2212 1, . . . tinit} do 32: R\u2190 ri + \u03b3R 33: Accumulate gradients for \u03b8\u2032: d\u03b8 \u2190 d\u03b8 +\u2207\u03b8\u2032((R\u2212Q(si, ai; \u03b8\u2032))2\n34: Perform asynchronous update of \u03b8 using d\u03b8 35: until T > Tmax"}, {"heading": "Appendix C: Raw Performance tables", "text": "In this appendix, we document the raw performances of all our agents and the corresponding baseline performances. All the performances have been obtained by averaging across 3 random seeds to ensure a robust comparison.\nFARA3C and A3C\nTable 1 demonstrates that FARA3C outperforms A3C on 9 out of 14 tasks that we experimented with.\nFARAQL and AQL\nTable 2 demonstrates that FARAQL outperforms AQL on 9 out of 13 tasks that we experimented with."}, {"heading": "Appendix D: Training curves", "text": "This appendix contains all the training curves for all the algorithms presented in this work. All training curves have been averaged across three random seeds to ensure a robust comparison."}], "references": [{"title": "Orthogonal representation of sound dimensions in the primate midbrain", "author": ["Baumann", "Simon", "Griffiths", "Timothy D", "Sun", "Li", "Petkov", "Christopher I", "Thiele", "Alexander", "Rees", "Adrian"], "venue": "Nature neuroscience,", "citeRegEx": "Baumann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baumann et al\\.", "year": 2011}, {"title": "The \u201cindependent components\u201d of natural scenes are edge filters", "author": ["Bell", "Anthony J", "Sejnowski", "Terrence J"], "venue": "Vision research,", "citeRegEx": "Bell et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bell et al\\.", "year": 1997}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["Bellemare", "Marc G", "Naddaf", "Yavar", "Veness", "Joel", "Bowling", "Michael"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Guo", "Xiaoxiao", "Singh", "Satinder", "Lee", "Honglak", "Lewis", "Richard L", "Wang", "Xiaoshi"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Reinforcement learning with unsupervised auxiliary tasks", "author": ["Jaderberg", "Max", "Mnih", "Volodymyr", "Czarnecki", "Wojciech Marian", "Schaul", "Tom", "Leibo", "Joel Z", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Jaderberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2017}, {"title": "Actor-critic algorithms. In Advances in neural information processing", "author": ["Konda", "Vijay R", "Tsitsiklis", "John N"], "venue": null, "citeRegEx": "Konda et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Konda et al\\.", "year": 2000}, {"title": "Continuous control with deep reinforcement learning", "author": ["Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan"], "venue": "arXiv preprint arXiv:1509.02971,", "citeRegEx": "Lillicrap et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lillicrap et al\\.", "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Graves", "Alex", "Antonoglou", "Ioannis", "Wierstra", "Daan", "Riedmiller", "Martin"], "venue": "arXiv preprint arXiv:1312.5602,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Kavukcuoglu", "Koray", "Silver", "David", "Rusu", "Andrei A", "Veness", "Joel", "Bellemare", "Marc G", "Graves", "Alex", "Riedmiller", "Martin", "Fidjeland", "Andreas K", "Ostrovski", "Georg", "Petersen", "Stig", "Beattie", "Charles", "Sadik", "Amir", "Antonoglou", "Ioannis", "King", "Helen", "Kumaran", "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Mnih", "Volodymyr", "Agapiou", "John", "Osindero", "Simon", "Graves", "Alex", "Vinyals", "Oriol", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1606.04695,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Mnih", "Volodymyr", "Badia", "Adria Puigdomenech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Incremental multi-step q-learning", "author": ["Peng", "Jing", "Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Peng et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Peng et al\\.", "year": 1996}, {"title": "Prioritized experience replay", "author": ["Schaul", "Tom", "Quan", "John", "Antonoglou", "Ioannis", "Silver", "David"], "venue": "4th International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2015}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Moritz", "Philipp", "Jordan", "Michael I", "Abbeel", "Pieter"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Learning to repeat: Fine grained action repetition for deep reinforcement learning", "author": ["Sharma", "Sahil", "Lakshminarayanan", "Aravind S", "Ravindran", "Balaraman"], "venue": "To appear in 5th International Conference on Learning Representations,", "citeRegEx": "Sharma et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sharma et al\\.", "year": 2017}, {"title": "Learning to predict by the methods of temporal differences", "author": ["Sutton", "Richard S"], "venue": "Machine learning,", "citeRegEx": "Sutton and S.,? \\Q1988\\E", "shortCiteRegEx": "Sutton and S.", "year": 1988}, {"title": "Introduction to reinforcement learning", "author": ["Sutton", "Richard S", "Barto", "Andrew G"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Intra-option learning about temporally abstract actions", "author": ["Sutton", "Richard S", "Precup", "Doina", "Singh", "Satinder P"], "venue": "In ICML,", "citeRegEx": "Sutton et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Todorov", "Emanuel", "Erez", "Tom", "Tassa", "Yuval"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Todorov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2012}, {"title": "Deep reinforcement learning with double q-learning", "author": ["Van Hasselt", "Hado", "Guez", "Arthur", "Silver", "David"], "venue": "In AAAI,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Technical note: Q-learning", "author": ["Watkins", "Christopher J.C. H", "Dayan", "Peter"], "venue": "Mach. Learn.,", "citeRegEx": "Watkins et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Watkins et al\\.", "year": 1992}, {"title": "Torcs, the open racing car simulator. Software available at http://torcs", "author": ["Wymann", "Bernhard", "Espi\u00e9", "Eric", "Guionneau", "Christophe", "Dimitrakakis", "Christos", "Coulom", "R\u00e9mi", "Sumner", "Andrew"], "venue": "sourceforge. net,", "citeRegEx": "Wymann et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Wymann et al\\.", "year": 2000}, {"title": "We used a low level architecture similar to Mnih et al", "author": ["Sharma"], "venue": null, "citeRegEx": "Sharma,? \\Q2017\\E", "shortCiteRegEx": "Sharma", "year": 2017}, {"title": "Figure 3 contains a visual depiction", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "2016b] the Actor and Critic networks share all but the final output", "author": ["Mnih"], "venue": null, "citeRegEx": "Mnih,? \\Q2016\\E", "shortCiteRegEx": "Mnih", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 8, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 13, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 6, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 12, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 4, "context": "Such DRL methods [Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015; Lillicrap et al., 2015; Schaul et al., 2015; Mnih et al., 2016b; Van Hasselt et al., 2016; Mnih et al., 2016a; Bacon et al., 2017; Jaderberg et al., 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al.", "startOffset": 17, "endOffset": 232}, {"referenceID": 2, "context": ", 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al., 2013], MuJoCo [Todorov et al.", "startOffset": 120, "endOffset": 144}, {"referenceID": 18, "context": ", 2013], MuJoCo [Todorov et al., 2012], TORCS Wymann et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 2, "context": ", 2017] perform impressively in many challenging high-dimensional sequential decision making problem such as Atari 2600 [Bellemare et al., 2013], MuJoCo [Todorov et al., 2012], TORCS Wymann et al. [2000] and the board game of Go [Silver et al.", "startOffset": 121, "endOffset": 204}, {"referenceID": 0, "context": "The key motivations for our work are principles underlying biological systems, that have shown to learn representations in an independent and orthogonal manner [Baumann et al., 2011; Bell & Sejnowski, 1997].", "startOffset": 160, "endOffset": 206}, {"referenceID": 16, "context": "At an abstract level, our work is also similar to intra-option learning [Sutton et al., 1998] frameworks, which revolves around the idea of learning about an option while executing another.", "startOffset": 72, "endOffset": 93}, {"referenceID": 9, "context": "Sharma et al. [2017] have demonstrated remarkable improvements in a variety of domains using a framework that factors the policy into one for choosing actions and another for choosing repetitions.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "This includes algorithms which model Q-functions (like DQN [Mnih et al., 2015] or asynchronous n-step Q-learning [Mnih et al.", "startOffset": 59, "endOffset": 78}, {"referenceID": 14, "context": "This sub-section demonstrates that the choice of additive combination function for A3C gives the action policy a nice alternate interpretation in terms a product of constituent factors\u2019 policies (such representations have been explored in other works such as Sharma et al. [2017]).", "startOffset": 259, "endOffset": 280}, {"referenceID": 14, "context": "A possible extension of this framework would be to combine action-space factoring with the concept of action repetition as discussed by Sharma et al. [2017]. Action repetition could act as yet another factor of the action space and this extension could allow one to capture a large set of macro actions.", "startOffset": 136, "endOffset": 157}], "year": 2017, "abstractText": "Deep Reinforcement Learning (DRL) methods have performed well in an increasing numbering of high-dimensional visual decision making domains. Among all such visual decision making problems, those with discrete action spaces often tend to have underlying compositional structure in the said action space. Such action spaces often contain actions such as go left, go up as well as go diagonally up and left (which is a composition of the former two actions). The representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces. We propose a new learning paradigm, Factored Action space Representations (FAR) wherein we decompose a control policy learned using a Deep Reinforcement Learning Algorithm into independent components, analogous to decomposing a vector in terms of some orthogonal basis vectors. This architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously, while executing only one of them. We demonstrate that FAR yields considerable improvements on top of two DRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage Actor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous n-step Q-Learning) in 9 out of 13 tasks.", "creator": "LaTeX with hyperref package"}}}