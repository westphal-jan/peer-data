{"id": "1705.07664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters", "abstract": "the rise of graph - structured data such as social networks, regulatory diagrams, semantic graphs, and functional brain networks, in combination with resounding dynamics inspiring deep learning in various applications, has awakened continued interest including new local learning practices exceeding nano - euclidean domains. in this paper, professors introduce totally new spectral geometry convolutional architecture for deep listening on graphs. the growing ingredient of our application is a spectral class around parametric rational integration functions ( cayley polynomials ) first us efficiently induce localized continuous networks on integers then specialize narrow frequency bands of samples. many algorithms adequately scales around the size of the boundary equations for sparsely - connected graphs, can handle nonlinear configurations of laplacian transforms, not typically requires linear parameters exceeding real models. extensive published review show the superior performance improved shannon approach on various graph smoothing objectives.", "histories": [["v1", "Mon, 22 May 2017 11:05:37 GMT  (539kb,D)", "http://arxiv.org/abs/1705.07664v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ron levie", "federico monti", "xavier bresson", "michael m bronstein"], "accepted": false, "id": "1705.07664"}, "pdf": {"name": "1705.07664.pdf", "metadata": {"source": "CRF", "title": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters", "authors": ["Ron Levie", "Federico Monti", "Xavier Bresson", "Michael M. Bronstein"], "emails": ["ronlevie@post.tau.ac.il", "federico.monti@usi.ch", "michael.bronstein@usi.ch", "xbresson@ntu.edu.sg"], "sections": [{"heading": "1 Introduction", "text": "In many domains, one has to deal with large-scale data with underlying non-Euclidean structure. Prominent examples of such data are social networks, genetic regulatory networks, functional networks of the brain, and 3D shape represented as discrete manifolds. The recent success of deep neural networks and, in particular, convolutional neural networks (CNNs) [20] have raised the interest in geometric deep learning techniques trying to extend these models to data residing on graphs and manifolds. Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few. For a comprehensive presentation of methods and applications of deep learning on graphs and manifolds, we refer the reader to the review paper [5].\nRelated work. In this paper, we are in particular interested in deep learning on graphs. The earliest neural network formulation on graphs was proposed by Scarselli et al. [12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]). The first CNN-type architecture on graphs was proposed by Bruna et al. [6]. One of the key challenges of extending CNNs to graphs is the lack of vector-space structure and shiftinvariance making the classical notion of convolution elusive. Bruna et al. formulated convolutionlike operations in the spectral domain, using the graph Laplacian eigenbasis as an analogy of the Fourier transform [29]. Henaff et al. [14] used smooth parametric spectral filters in order to achieve localization in the spatial domain and keep the number of filter parameters independent of the input size. Defferrard et al. [9] proposed an efficient filtering scheme using recurrent Chebyshev polynomials applied on the Laplacian operator. Kipf and Welling [18] simplified this architecture\n\u2217The first two authors have contributed equally and are listed alphabetically.\nar X\niv :1\n70 5.\n07 66\n4v 1\n[ cs\n.L G\n] 2\n2 M\nay 2\nusing filters operating on 1-hop neighborhoods of the graph. Atwood and Towsley [1] proposed a Diffusion CNN architecture based on random walks on graphs. Monti et al. [24] (and later, [13]) proposed a spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs. In [25], graph CNNs were extended to multiple graphs and applied to matrix completion and recommender system problems.\nMain contribution. In this paper, we construct graph CNNs employing an efficient spectral filtering scheme based on Cayley polynomials that enjoys similar advantages of the Chebyshev filters [9] such as localization and linear complexity. The main advantage of our filters over [9] is their ability to detect narrow frequency bands of importance during training, and to specialize on them while being well-localized on the graph. We demonstrate experimentally that this affords our method greater flexibility, making it perform better on a broad range of graph learning problems.\nNotation. We use a,a, and A to denote scalars, vectors, and matrices, respectively. z\u0304 denotes the conjugate of a complex number, Re{z} its real part, and i is the imaginary unit. diag(a1, . . . , an) denotes an n\u00d7 n diagonal matrix with diagonal elements a1, . . . , an. Diag(A) = diag(a11, . . . , ann) denotes an n \u00d7 n diagonal matrix obtained by setting to zero the off-diagonal elements of A. Off(A) = A\u2212Diag(A) denotes the matrix containing only the off-diagonal elements of A. I is the identity matrix and A \u25e6B denotes the Hadamard (element-wise) product of matrices A and B. Proofs are given in the appendix."}, {"heading": "2 Spectral techniques for deep learning on graphs", "text": "Spectral graph theory. Let G = ({1, . . . , n}, E ,W) be an undirected weighted graph, represented by a symmetric adjacency matrix W = (wij). We define wij = 0 if (i, j) /\u2208 E and wij > 0 if (i, j) \u2208 E . We denote by Nk,m the k-hop neighborhood of vertex m, containing vertices that are at most k edges away from m. The unnormalized graph Laplacian is an n\u00d7n symmetric positive-semidefinite matrix \u2206 = D \u2212W, where D = diag( \u2211 j 6=i wij) is the degree matrix. The normalized graph Laplacian is defined as \u2206n = D\u22121/2\u2206D\u22121/2 = I\u2212D\u22121/2WD\u22121/2. In the following, we use the generic notation \u2206 to refer to some Laplacian.\nSince both normalized and unnormalized Laplacian are symmetric and positive semi-definite matrices, they admit an eigendecomposition \u2206 = \u03a6\u039b\u03a6>, where \u03a6 = (\u03c61, . . .\u03c6n) are the orthonormal eigenvectors and \u039b = diag(\u03bb1, . . . , \u03bbn) is the diagonal matrix of corresponding non-negative eigenvalues (spectrum) 0 = \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bbn. The eigenvectors play the role of Fourier atoms in classical harmonic analysis and the eigenvalues can be interpreted as (the square of) frequencies. Given a signal f = (f1, . . . , fn)> on the vertices of graph G, its graph Fourier transform is given by f\u0302 = \u03a6>f . Given two signals f ,g on the graph, their spectral convolution can be defined as the element-wise product of the Fourier transforms,\nf ? g = \u03a6 ( (\u03a6>g) \u25e6 (\u03a6>f) ) = \u03a6 diag(g\u03021, . . . , g\u0302n)f\u0302 , (1)\nwhich corresponds to the property referred to as the Convolution Theorem in the Euclidean case.\nSpectral CNNs. Bruna et al. [6] used the spectral definition of convolution (1) to generalize CNNs on graphs, with a spectral convolutional layer of the form\nfoutl = \u03be\n( p\u2211\nl\u2032=1\n\u03a6kG\u0302l,l\u2032\u03a6 > k f in l\u2032\n) . (2)\nHere the n \u00d7 p and n \u00d7 q matrices Fin = (f in1 , . . . , f inp ) and Fout = (fout1 , . . . , foutq ) represent respectively the p- and q-dimensional input and output signals on the vertices of the graph, \u03a6k = (\u03c61, . . . ,\u03c6k) is an n \u00d7 k matrix of the first eigenvectors, G\u0302l,l\u2032 = diag(g\u0302l,l\u2032,1, . . . , g\u0302l,l\u2032,k) is a k \u00d7 k diagonal matrix of spectral multipliers representing a learnable filter in the frequency domain, and \u03be is a nonlinearity (e.g., ReLU) applied on the vertex-wise function values. Pooling is\nperformed by means of graph coarsening, which, given a graph with n vertices, produces a graph with n\u2032 < n vertices and transfers signals from the vertices of the fine graph to those of the coarse one.\nThis framework has several major drawbacks. First, the spectral filter coefficients are basis dependent, and consequently, a spectral CNN model learned on one graph cannot be applied to another graph. Second, the computation of the forward and inverse graph Fourier transforms incur expensive O(n2) multiplication by the matrices \u03a6,\u03a6>, as there is no FFT-like algorithms on general graphs. Third, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain (locality property simulates local reception fields [8]); assuming k = O(n) Laplacian eigenvectors are used, a spectral convolutional layer requires O(pqk) = O(n) parameters to train.\nTo address the latter issues, Henaff et al. [14] argued that smooth spectral filter coefficients result in spatially-localized filters (an argument similar to vanishing moments). The filter coefficients are represented as g\u0302i = g(\u03bbi), where g(\u03bb) is a smooth transfer function of frequency \u03bb. Applying such filter to signal f can be expressed as Gf = g(\u2206)f = \u03a6g(\u039b)\u03a6>f = \u03a6 diag(g(\u03bb1), . . . , g(\u03bbn))\u03a6>f , where applying a function to a matrix is understood in the operator functional calculus sense (applying the function to the matrix eigenvalues). Henaff et al. [14] used parametric functions of the form\ng(\u03bb) = r\u2211 j=1 \u03b1j\u03b2j(\u03bb), (3)\nwhere \u03b21(\u03bb), . . . , \u03b2r(\u03bb) are some fixed interpolation kernels such as splines, and \u03b1 = (\u03b11, . . . , \u03b1r) are the interpolation coefficients used as the optimization variables during the network training. In matrix notation, the filter is expressed as Gf = \u03a6diag(B\u03b1)\u03a6>f , where B = (bij) = (\u03b2j(\u03bbi)) is a k \u00d7 r matrix. Such a construction results in filters with r = O(1) parameters, independent of the input size. However, the authors explicitly computed the Laplacian eigenvectors \u03a6, resulting in high complexity.\nChebNet. Defferrard et al. [9] used polynomial filters represented in the Chebyshev basis\ng\u03b1(\u03bb\u0303) = r\u2211 j=0 \u03b1jTj(\u03bb\u0303) (4)\napplied to rescaled frequency \u03bb\u0303 \u2208 [\u22121, 1]; here, \u03b1 is the (r + 1)-dimensional vector of polynomial coefficients parametrizing the filter and optimized for during the training, and Tj(\u03bb) = 2\u03bbTj\u22121(\u03bb)\u2212 Tj\u22122(\u03bb) denotes the Chebyshev polynomial of degree j defined in a recursive manner with T1(\u03bb) = \u03bb and T0(\u03bb) = 1. Chebyshev polynomials form an orthogonal basis for the space of polynomials of order r on [\u22121, 1]. Applying the filter is performed by g\u03b1(\u2206\u0303)f , where \u2206\u0303 = 2\u03bb\u22121n \u2206\u2212 I is the rescaled Laplacian such that its eigenvalues \u039b\u0303 = 2\u03bb\u22121n \u039b\u2212 I are in the interval [\u22121, 1].\nSuch an approach has several important advantages. First, since g\u03b1(\u2206\u0303) = \u2211r j=0 \u03b1jTj(\u2206\u0303) contains only matrix powers, additions, and multiplications by scalar, it can be computed avoiding the explicit expensive O(n3) computation of the Laplacian eigenvectors. Furthermore, due to the recursive definition of the Chebyshev polynomials, the computation of the filter g\u03b1(\u2206)f entails applying the Laplacian r times, resulting in O(rn) operations assuming that the Laplacian is a sparse matrix with O(1) non-zero elements in each row (a valid hypothesis for most real-world graphs that are sparsely connected). Second, the number of parameters is O(r), independent of the graph size n. Third, since the Laplacian is a local operator affecting only 1-hop neighbors of a vertex and a polynomial of degree r of the Laplacian affects only r-hops, the resulting filters have guaranteed spatial localization.\nA key disadvantage of Chebyshev filters is the fact that using polynomials makes it hard to produce narrow-band filters, as such filters require very high order r. This deficiency is especially pronounced when the Laplacian has clusters of eigenvalues concentrated around a few frequencies with large spectral gap. Such a behavior is characteristic of graphs with community structures, which is very common in many real-world graphs, for instance, social networks."}, {"heading": "3 Cayley filters", "text": "A key construction of this paper is a family of complex filters that enjoy the advantages of Chebyshev filters while avoiding some of their drawbacks. A Cayley polynomial of order r is a real-valued function with complex coefficients,\ngc,h(\u03bb) = c0 + 2Re { r\u2211 j=1 cj(h\u03bb\u2212 i)j(h\u03bb+ i)\u2212j }\n(5)\nwhere c = (c0, . . . , cr) is a vector of one real coefficient and r complex coefficients and h > 0 is the spectral zoom parameter. A Cayley filter G is a spectral filter defined on real signals f by\nGf = gc,h(\u2206)f = c0f + 2Re{ r\u2211 j=1 cj(h\u2206\u2212 iI)j(h\u2206 + iI)\u2212jf}, (6)\nwhere the parameters c and h are optimized for during training. Similarly to the Chebyshev filters, Cayley filters involve basic matrix operations such as powers, additions, multiplications by scalars, and also inversions. This implies that application of the filter Gf can be performed without explicit expensive eigendecomposition of the Laplacian operator. In the following, we show that Cayley filters are analytically well behaved; in particular, any smooth spectral filter can be represented as a Cayley polynomial, and low-order filters are localized in the spatial domain. We also discuss numerical implementation and compare Cayley and Chebyshev filters.\nAnalytic properties. Cayley filters are best understood through the Cayley transform, from which their name derives. Denote by eiR = {ei\u03b8 : \u03b8 \u2208 R} the unit complex circle. The Cayley transform C(x) = x\u2212ix+i is a smooth bijection between R and e\niR \\ {1}. The complex matrix C(h\u2206) = (h\u2206\u2212 iI)(h\u2206 + iI)\u22121 obtained by applying the Cayley transform to the scaled Laplacian h\u2206 has its spectrum in eiR and is thus unitary. Since z\u22121 = z for z \u2208 eiR, we can write cjCj(h\u2206) = cjC\u2212j(h\u2206). Therefore, using 2Re{z} = z + z, any Cayley filter (6) can be written as a conjugate-even Laurent polynomial w.r.t. C(h\u2206),\nG = c0I + r\u2211 j=1 cjCj(h\u2206) + cjC\u2212j(h\u2206). (7)\nSince the spectrum of C(h\u2206) is in eiR, the operator Cj(h\u2206) can be thought of as a multiplication by a pure harmonic in the frequency domain eiR for any integer power j,\nCj(h\u2206) = \u03a6diag ([ C(h\u03bb1) ]j , . . . , [ C(h\u03bbn) ]j) \u03a6>.\nA Cayley filter can be thus seen as a multiplication by a finite Fourier expansions in the frequency domain eiR. Since (7) is conjugate-even, it is a (real-valued) trigonometric polynomial.\nNote that any spectral filter can be formulated as a Cayley filter. Indeed, spectral filters g(\u2206) are specified by the finite sequence of values g(\u03bb1), . . . , g(\u03bbn), which can be interpolated by a trigonometric polynomial. Moreover, since trigonometric polynomials are smooth, we expect low order Cayley filters to be well localized in some sense on the graph, as discussed later.\nFinally, in definition (6) we use complex coefficients. If cj \u2208 R then (7) is an even cosine polynomial, and if cj \u2208 iR then (7) is an odd sine polynomial. Since the spectrum of h\u2206 is in R+, it is mapped to the lower half-circle by C, on which both cosine and sine polynomials are complete and can represent any spectral filter. However, it is beneficial to use general complex coefficients, since complex Fourier expansions are overcomplete in the lower half-circle, thus describing a larger variety of spectral filters of the same order without increasing the computational complexity of the filter.\nSpectral zoom. To understand the role of the parameter h in the Cayley filter, consider C(h\u2206). Multiplying \u2206 by h dilates its spectrum, and applying C on the result maps the non-negative spectrum to the complex half-circle. The greater h is, the more the spectrum of h\u2206 is spread apart\nin R+, resulting in better spacing of the smaller eigenvalues of C(h\u2206). On the other hand, the smaller h is, the further away the high frequencies of h\u2206 are from \u221e, the better spread apart are the high frequencies of C(h\u2206) in eiR (see Figure 2). Tuning the parameter h allows thus to \u2018zoom\u2019 in to different parts of the spectrum, resulting in filters specialized in different frequency bands.\nNumerical properties. The numerical core of the Cayley filter is the computation of Cj(h\u2206)f for j = 1, . . . , r, performed in a sequential manner. Let y0, . . . ,yr denote the solutions of the following linear recursive system,\ny0 = f , (h\u2206 + iI)yj = (h\u2206\u2212 iI)yj\u22121 , j = 1, . . . , r. (8)\nNote that sequentially approximating yj in (8) using the approximation of yj\u22121 in the rhs is stable, since C(h\u2206) is unitary and thus has condition number 1.\nEquations (8) can be solved with matrix inversion exactly, but it costs O(n3). An alternative is to use the Jacobi method,1 which provides approximate solutions y\u0303j \u2248 yj . Let J = \u2212(Diag(h\u2206 + iI))\u22121Off(h\u2206+iI) be the Jacobi iteration matrix associated with equation (8). For the unnormalized Laplacian, J = (hD + iI)\u22121hW. Jacobi iterations for approximating (8) for a given j have the form\ny\u0303 (k+1) j = Jy\u0303 (k) j + bj , bj = (Diag(h\u2206 + iI)) \u22121(h\u2206\u2212 iI)y\u0303j\u22121, (9)\ninitialized with y\u0303(0)j = bj and terminated after K iterations, yielding y\u0303j = y\u0303 (K) j . The application of the approximate Cayley filter is given by G\u0303f = \u2211r j=0 cjy\u0303j \u2248 Gf , and takes O(rKn) operations under the previous assumption of a sparse Laplacian. The method can be improved by normalizing \u2016y\u0303j\u20162 = \u2016f\u20162.\nNext, we give an error bound for the approximate filter. For the unnormalized Laplacian, let d = maxj{dj,j} and \u03ba = \u2016J\u2016\u221e = hd\u221a h2d2+1\n< 1. For the normalized Laplacian, we assume that (h\u2206n + iI) is dominant diagonal, which gives \u03ba = \u2016J\u2016\u221e < 1.\n1We remind that the Jacobi method for solving Ax = b consists in decomposing A = Diag(A) + Off(A) and obtaining the solution iteratively as x(k+1) = \u2212(Diag(A))\u22121Off(A)x(k) + (Diag(A))\u22121b.\nProposition 1. Under the above assumptions, \u2016Gf\u2212G\u0303f\u20162\u2016f\u20162 \u2264M\u03ba K , where M = \u221a n \u2211r j=1 j |cj | in\nthe general case, and M = \u2211r j=1 j |cj | if the graph is regular.\nProposition 1 is pessimistic in the general case, while requires strong assumptions in the regular case. We find that in most real life situations the behavior is closer to the regular case. It also follows from Proposition 1 that smaller values of the spectral zoom h result in faster convergence, giving this parameter an additional numerical role of accelerating convergence.\nLocalization. Unlike Chebyshev filters that have the small r-hop support, Cayley filters are rational functions supported on the whole graph. However, it is still true that Cayley filters are well localized on the graph. Let G be a Cayley filter and \u03b4m denote a delta-function on the graph, defined as one at vertex m and zero elsewhere. We show that G\u03b4m decays fast, in the following sense:\nDefinition 2 (Exponential decay on graphs). Let f be a signal on the vertices of graph G, 1 \u2264 p \u2264 \u221e, and 0 < < 1. Denote by S \u2286 {1, . . . , n} a subset of the vertices and by Sc its complement. We say that the Lp-mass of f is supported in S up to if \u2016f |Sc\u2016p \u2264 \u2016f\u2016p, where f |Sc = (fl)l\u2208Sc is the restriction of f to Sc. We say that f has (graph) exponential decay about vertex m, if there exists some \u03b3 \u2208 (0, 1) and c > 0 such that for any k, the Lp-mass of f is supported in Nk,m up to c\u03b3k. Here, Nk,m is the k-hop neighborhood of m.\nRemark 3. Note that Definition 2 is analogous to classical exponential decay on Euclidean space: |f(x)| \u2264 R\u03b3\u2212x iff for every ball B\u03c1 of radius \u03c1 about 0, \u2016f |Bc\u03c1\u2016\u221e \u2264 c\u03b3 \u2212\u03c1 \u2016f\u2016\u221e with c = R \u2016f\u2016\u221e .\nTheorem 4. Let G be a Cayley filter of order r. Then, G\u03b4m has exponential decay about m in L2, with constants c = 2M 1\u2016G\u03b4m\u20162 and \u03b3 = \u03ba 1/r (where M and \u03ba are from Proposition 1).\nCayley vs Chebyshev. Below, we compare the two classes of filters: Spectral zoom and stability. Generally, both Chebyshev polynomials and trigonometric polynomials give stable approximations, optimal for smooth functions. However, this crude statement is oversimplified. One of the drawbacks in Chebyshev filters is the fact that the spectrum of \u2206 is always mapped to [\u22121, 1] in a linear manner, making it hard to specialize in small frequency bands. In Cayley filters, this problem is mitigated with the help of the spectral zoom parameter h. As an example, consider the community detection problem discussed in the next section. A graph with strong communities has a cluster of small eigenvalues near zero. Ideal filters g(\u2206) for extracting the community information should be able to focus on this band of frequencies. Approximating such filters with Cayley polynomials, we zoom in to the band of interest by choosing the right h, and then project g onto the space of trigonometric polynomials of order r, getting a good and stable approximation (Figure 3, bottom right). However, if we project g onto the space of Chebyshev polynomials of order r, the interesting part of g concentrated on a small band is smoothed out and lost (Figure 3, middle right). Thus, projections are not the right way to approximate such filters, and the stability of orthogonal polynomials cannot be invoked. When approximating g on the small band using polynomials, the approximation will be unstable away from this band; small perturbations in g will result in big perturbations in the Chebyshev filter away from the band. For this reason, we say that Cayley filters are more stable than Chebyshev filters. Regularity. We found that in practice, low-order Cayley filters are able to model both very concentrated impulse-like filters, and wider Gabor-like filters. Cayley filters are able to achieve a wider range of filter supports with less coefficients than Chebyshev filters (Figure 1), making the Cayley class more regular than Chebyshev. Complexity. Under the assumption of sparse Laplacians, both Cayley and Chebyshev filters incur linear complexity O(n). The implementation of Cayley filter in TensorFlow is slower due to the use of TensorFlow\u2019s for loop in Jacobi iterations (9). A faster implementation should involve programming the whole Jacobi method as an OP in native CUDA."}, {"heading": "4 Results", "text": "Experimental settings. We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method. All the methods were implemented in TensorFlow. The experiments were executed on a machine with a 3.5GHz Intel Core i7 CPU, 64GB of RAM, and NVIDIA Titan X GPU with 12GB of RAM. SGD+Momentum and Adam [17] optimization methods were used to train the models in MNIST and the rest of the experiments, respectively. Training and testing were always done on disjoint sets.\nCommunity detection. We start with an experiment on a synthetic graph consisting of 15 communities with strong connectivity within each community and sparse connectivity across communities (Figure 3, left). Though rather simple, such a dataset allows to study the behavior of different algorithms in controlled settings. On this graph, we generate noisy step signals, defined as fi = 1 + \u03c3i if i belongs to the community, and fi = \u03c3i otherwise, where \u03c3i \u223c N (0, 0.3) is Gaussian i.i.d. noise. The goal is to classify each such signal according to the community it belongs to. The neural network architecture used for this task consisted of a spectral convolutional layer (based on Chebyshev or Cayley filters) with 32 output features, a mean pooling layer, and a softmax classifier for producing the final classification into one of the 15 classes. The classification accuracy is shown in Figure 3 (right, top) along with examples of learned filters (right, bottom). We observe that CayleyNet significantly outperforms ChebNet for smaller filter orders. Studying the filter responses, we note that due to the capability to learn the spectral zoom parameter, CayleyNet allows to generate band-pass filters in the low-frequency band that discriminate well the communities.\nComplexity. We experimentally validated the computational complexity of our model applying filters of different order r to synthetic 15-community graphs of different size n using exact matrix inversion and approximation with different number of Jacobi iterations (Figure 4, center and right). As expected, approximate inversion guarantees O(n) complexity. We further conclude that typically very few Jacobi iterations are required (Figure 4, left shows that our model with just one Jacobi iteration outperforms ChebNet for low-order filters on the community detection problem).\nMNIST. Following [9, 24], we approached the classical MNIST digits classification as a learning problem on graphs. Each pixel of an image is a vertex of a graph (regular grid with 8-neighbor connectivity), and pixel color is a signal on the graph. We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling layers performing 4-times graph coarsening using the Graclus algorithm [10], and finally a fully-connected layer (this architecture replicates the classical LeNet5 [20] architecture, which is shown for comparison). MNIST classification results are reported in Table 1. CayleyNet achieves the same (near perfect) accuracy as ChebNet with filters of lower order (r = 12 vs 25). Examples of filters learned by ChebNet and CayleyNet are shown in Figure 1.\nCitation network. Next, we address the problem of vertex classification on graphs using the popular CORA citation graph [28]. Each of the 2708 vertices of the CORA graph represents a scientific paper, and an undirected unweighted edge represents a citation (5429 edges in total). For each vertex, a 1433-dimensional binary feature vector representing the content of the paper is given. The task is to classify each vertex into one of the 7 groundtruth classes. We split the graph into training (1,708 vertices), validation (500 vertices) and test (500 vertices) sets, for simulating the labeled and unlabeled information. We use the architecture introduced in [18, 24] (two spectral convolutional layers with 16 and 7 outputs, respectively) for solving the vertex classification task. The spectral zoom h was determined with cross-validation in order to avoid overfitting. The performance is reported in Figure 5 and Table 3. CayleyNet consistently outperforms ChebNet and other methods. We further note that ChebNet is unable to handle unnormalized Laplacians.\nTable 1: Test accuracy obtained with different methods on the MNIST dataset.\nTable 2: Performance (RMSE) of different matrix completion methods on the MovieLens dataset.\nTable 3: Test accuracy of different methods on the CORA dataset.\nRecommender system. In our final experiment, we applied CayleyNet to recommendation system, formulated as matrix completion problem on user and item graphs [24]. The task is, given\na sparsely sampled matrix of scores assigned by users (columns) to items (rows), to fill in the missing scores. The similarities between users and items are given in the form of column and row graphs, respectively. Monti et al. [24] approached this problem as learning with a Recurrent Graph CNN (RGCNN) architecture, using an extension of ChebNets to matrices defined on multiple graphs in order to extract spatial features from the score matrix; these features are then fed into an RNN producing a sequential estimation of the missing scores. Here, we repeated verbatim their experiment on the MovieLens dataset [23], replacing Chebyshev filters with Cayley filters. We used separable RGCNN architecture with two CayleyNets of order r = 4 employing 15 Jacobi iterations. The results are reported in Table 2. Our version of sRGCNN outperforms all the competing methods, including the previous result with Chebyshev filters reported in [24]."}, {"heading": "5 Conclusions", "text": "In this paper, we introduced a new efficient graph CNN architecture that scales linearly with the dimension of the input data. Our architecture is based on a new class of complex rational Cayley filters that are localized in space, can represent any smooth spectral transfer function, and are highly regular. The key property of our model is its ability to specialize in narrow frequency bands with a small number of filter parameters, while still preserving locality in the spatial domain. We validated these theoretical properties experimentally, demonstrating the superior performance of our model in a broad range of graph learning problems. In future work, we will explore more modern linear solvers instead of the Jacobi method, built in native CUDA as a TensorFlow OP."}, {"heading": "Acknowledgment", "text": "FM and MB are supported in part by ERC Starting Grant No. 307047 (COMET), ERC Consolidator Grant No. 724228 (LEMAN), Google Faculty Research Award, Nvidia equipment grant, Radcliffe fellowship from Harvard Institute for Advanced Study, and TU Munich Institute for Advanced Study, funded by the German Excellence Initiative and the European Union Seventh Framework Programme under grant agreement No. 291763. XB is supported in part by NRF Fellowship NRFF2017-10."}, {"heading": "Appendix", "text": ""}, {"heading": "Proof of Proposition 1", "text": "First note the following classical result for the approximation of Ax = b using the Jacobi method: if the initial condition is x(0) = 0, then (x\u2212 x(k)) = Jkx. In our case, note that if we start with initial condition y\u0303(0)j = 0, the next iteration gives y\u0303 (0) j = bj , which is the initial condition from our construction. Therefore, since we are approximating yj = C(h\u2206)y\u0303j\u22121 by y\u0303j = y\u0303(K)j , we have\nC(h\u2206)y\u0303j\u22121 \u2212 y\u0303j = JK+1C(h\u2206)y\u0303j\u22121 (10)\nDefine the approximation error in C(h\u2206)jf by\nej = \u2225\u2225Cj(h\u2206)f \u2212 y\u0303j\u2225\u22252 \u2016Cj(h\u2206)f\u20162 .\nBy the triangle inequality, by the fact that Cj(h\u2206) is unitary, and by (10) ej \u2264 \u2225\u2225Cj(h\u2206)f \u2212 C(h\u2206)y\u0303j\u22121\u2225\u22252\n\u2016Cj(h\u2206)f\u20162 + \u2016C(h\u2206)y\u0303j\u22121 \u2212 y\u0303j\u20162 \u2016Cj(h\u2206)f\u20162\n= \u2225\u2225Cj\u22121(h\u2206)f \u2212 y\u0303j\u22121\u2225\u22252 \u2016Cj\u22121(h\u2206)f\u20162 + \u2225\u2225JK+1C(h\u2206)y\u0303j\u22121\u2225\u22252 \u2016f\u20162\n\u2264ej\u22121 + \u2225\u2225JK+1\u2225\u2225\n2 \u2016C(h\u2206)y\u0303j\u22121\u20162 \u2016f\u20162\n= ej\u22121 + \u2225\u2225JK+1\u2225\u2225\n2 \u2016y\u0303j\u22121\u20162 \u2016f\u20162\n\u2264ej\u22121 + \u2225\u2225JK+1\u2225\u2225\n2 (1 + ej\u22121)\n(11)\nwhere the last inequality is due to \u2016y\u0303j\u22121\u20162 \u2264 \u2225\u2225Cj\u22121(h\u2206)f\u2225\u2225 2 + \u2225\u2225Cj\u22121(h\u2206)f \u2212 y\u0303j\u22121\u2225\u22252 = \u2016f\u20162 + \u2016f\u20162 ej\u22121.\nNow, using standard norm bounds, in the general case we have \u2225\u2225JK+1\u2225\u2225 2 \u2264 \u221a n \u2225\u2225JK+1\u2225\u2225\u221e. Thus, by \u03ba = \u2016J\u2016\u221e we have\nej \u2264 ej\u22121 + \u221a n \u2016J\u2016K+1\u221e (1 + ej\u22121) = (1 + \u221a n\u03baK+1)ej\u22121 + \u221a n\u03baK+1.\nThe solution of this recurrent sequence is\nej \u2264 (1 + \u221a n\u03baK+1)j \u2212 1 = j \u221a n\u03baK+1 +O(\u03ba2K+2).\nIf we use the version of the algorithm, in which each y\u0303j is normalized, we get by (11) ej \u2264 ej\u22121 + \u221a n\u03baK+1. The solution of this recurrent sequence is\nej \u2264 j \u221a n\u03baK+1.\nWe denote in this case Mj = j \u221a n\nIn case the graph is regular, we have D = dI. In the non-normalized Laplacian case,\nJ = \u2212(hdI + iI)\u22121h(\u2206\u2212 dI) = h hd+ i (dI\u2212\u2206) = h hd+ i W. (12)\nThe spectral radius of \u2206 is bounded by 2d. This can be shown as follows. a value \u03bb is not an eigenvalue of \u2206 (namely it is a regular value) if and only if (\u2206\u2212 \u03bbI) is invertible. Moreover, the matrix (\u2206\u2212 \u03bbI) is strictly dominant diagonal for any |\u03bb| > 2d. By Levy\u2013Desplanques theorem, any strictly dominant diagonal matrix is invertible, which means that all of the eigenvalues of \u2206 are less than 2d in their absolute value. As a result, the spectral radius of (dI\u2212\u2206) is realized on the smallest eigenvalue of \u2206, namely it is |d\u2212 0| = d. This means that the specral radius of J is hd\u221a h2d2+1 . As a result \u2016J\u20162 = hd\u221a h2d2+1 = \u03ba. We can now continue from (11) to get\nej \u2264 ej\u22121 + \u2016J\u2016K+12 (1 + ej\u22121) = ej\u22121 + \u03ba K+1(1 + ej\u22121).\nAs before, we get ej \u2264 j\u03baK+1 +O(\u03ba2K+2), and ej \u2264 j\u03baK+1 if each y\u0303j is normalized. We denote in this case Mj = j.\nIn the case of the normalized Laplacian of a regular graph, the spectral radius of \u2206n is bounded by 2, and the diagonal entries are all 1. Equation (12) in this case reads J = hh+i (I\u2212\u2206n), and J has spectral radius h\u221a\nh2+1 . Thus \u2016J\u20162 = h\u221a h2+1 = \u03ba and we continue as before to get ej \u2264 j\u03baK+1 and Mj = j.\nIn all cases, we have by the triangle inequality\u2225\u2225\u2225Gf \u2212 G\u0303f\u2225\u2225\u2225 2\n\u2016f\u20162 \u2264 r\u2211 j=1 |cj | \u2225\u2225Cj(h\u2206)f \u2212 y\u0303j\u2225\u22252 \u2016Cj(h\u2206)f\u20162 = r\u2211 j=1 |cj | ej \u2264 r\u2211 j=1 Mj |cj | \u03baK+1."}, {"heading": "Proof of Theorem 4", "text": "In this proof we approximate G\u03b4m by G\u0303\u03b4m. Note that the signal \u03b4m is supported on one vertex, and in the calculation of G\u0303\u03b4m, each Jacobi iteration increases the support of the signal by 1-hop. Therefore, the support of G\u0303\u03b4m is the r(K + 1)-hop neighborhood Nr(K+1),m of m. Denoting l = r(K + 1), and using Proposition 1, we get\u2225\u2225G\u03b4m \u2212G\u03b4m|Nl,m\u2225\u22252 \u2264 \u2225\u2225\u2225G\u03b4m \u2212 G\u0303\u03b4m\u2225\u2225\u22252 + \u2225\u2225\u2225G\u0303\u03b4m \u2212G\u03b4m|Nl,m\u2225\u2225\u22252\n\u2264 \u2225\u2225\u2225G\u03b4m \u2212 G\u0303\u03b4m\u2225\u2225\u2225 2 + \u2225\u2225\u2225G\u0303\u03b4m \u2212G\u03b4m\u2225\u2225\u2225 2\n= 2 \u2225\u2225\u2225G\u03b4m \u2212 G\u0303\u03b4m\u2225\u2225\u2225\n2 \u2264 2M\u03baK+1 \u2016\u03b4m\u20162\n= 2M(\u03ba1/r)l.\n(13)"}], "references": [{"title": "Diffusion-convolutional neural networks", "author": ["J. Atwood", "D. Towsley"], "venue": "arXiv:1511.02136v2", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning class-specific descriptors for deformable shapes using localized spectral convolutional networks", "author": ["D. Boscaini", "J. Masci", "S. Melzi", "M.M. Bronstein", "U. Castellani", "P. Vandergheynst"], "venue": "Computer Graphics Forum, 34(5):13\u201323", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning shape correspondence with anisotropic convolutional neural networks", "author": ["D. Boscaini", "J. Masci", "E. Rodol\u00e0", "M.M. Bronstein"], "venue": "Proc. NIPS", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Anisotropic diffusion descriptors", "author": ["D. Boscaini", "J. Masci", "E. Rodol\u00e0", "M.M. Bronstein", "D. Cremers"], "venue": "Computer Graphics Forum, 35(2):431\u2013441", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Geometric deep learning: going beyond euclidean data", "author": ["M.M. Bronstein", "J. Bruna", "Y. LeCun", "A. Szlam", "P. Vandergheynst"], "venue": "arXiv:1611.08097", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral networks and locally connected networks on graphs", "author": ["J. Bruna", "W. Zaremba", "A. Szlam", "Y. LeCun"], "venue": "Proc. ICLR", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Candes", "B. Recht"], "venue": "Comm. ACM, 55(6):111\u2013119", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Selecting Receptive Fields in Deep Networks", "author": ["A. Coates", "A. Ng"], "venue": "Proc. NIPS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional neural networks on graphs with fast localized spectral filtering", "author": ["M. Defferrard", "X. Bresson", "P. Vandergheynst"], "venue": "Proc. NIPS", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Weighted graph cuts without eigenvectors a multilevel approach", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "Trans. PAMI, 29(11)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Convolutional networks on graphs for learning molecular fingerprints", "author": ["D.K. Duvenaud"], "venue": "In Proc. NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "A new model for learning in graph domains", "author": ["M. Gori", "G. Monfardini", "F. Scarselli"], "venue": "Proc. IJCNN", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "A generalization of convolutional neural networks to graph-structured data", "author": ["Y. Hechtlinger", "P. Chakravarti", "J. Qin"], "venue": "arXiv:1704.08165", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2017}, {"title": "Deep convolutional networks on graph-structured data", "author": ["M. Henaff", "J. Bruna", "Y. LeCun"], "venue": "arXiv:1506.05163", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Provable inductive matrix completion", "author": ["P. Jain", "I.S. Dhillon"], "venue": "arXiv:1306.0626", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Matrix completion on graphs", "author": ["V. Kalofolias", "X. Bresson", "M.M. Bronstein", "P. Vandergheynst"], "venue": "arXiv:1408.1717", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "arXiv:1412.6980", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised classification with graph convolutional networks", "author": ["T.N. Kipf", "M. Welling"], "venue": "arXiv:1609.02907", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Distance metric learning using graph convolutional networks: Application to functional brain networks", "author": ["S.I. Ktena", "S. Parisot", "E. Ferrante", "M. Rajchl", "M. Lee", "B. Glocker", "D. Rueckert"], "venue": "arXiv:1703.02161", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2017}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE, 86(11):2278\u20132324", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Gated graph sequence neural networks", "author": ["Y. Li", "D. Tarlow", "M. Brockschmidt", "R. Zemel"], "venue": "arXiv:1511.05493", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Geodesic convolutional neural networks on Riemannian manifolds", "author": ["J. Masci", "D. Boscaini", "M.M. Bronstein", "P. Vandergheynst"], "venue": "Proc. 3DRR", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "MovieLens unplugged: experiences with an occasionally connected recommender system", "author": ["B.N. Miller"], "venue": "In Proc. Intelligent User Interfaces,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Geometric deep learning on graphs and manifolds using mixture model CNNs", "author": ["F. Monti", "D. Boscaini", "J. Masci", "E. Rodol\u00e0", "J. Svoboda", "M.M. Bronstein"], "venue": "Proc. CVPR", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Geometric matrix completion with recurrent multi-graph neural networks", "author": ["F. Monti", "M.M. Bronstein", "X. Bresson"], "venue": "arXiv:1704.06803", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2017}, {"title": "Collaborative filtering with graph information: Consistency and scalable methods", "author": ["N. Rao", "H.-F. Yu", "P.K. Ravikumar", "I.S. Dhillon"], "venue": "Proc. NIPS", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "The graph neural network model", "author": ["F. Scarselli", "M. Gori", "A.C. Tsoi", "M. Hagenbuchner", "G. Monfardini"], "venue": "IEEE Trans. Neural Networks, 20(1):61\u201380", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Galligher", "T. Eliassi-Rad"], "venue": "AI Magazine, 29(3):93", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Sig. Proc. Magazine, 30(3):83\u201398", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning multiagent communication with backpropagation", "author": ["S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "arXiv:1605.07736", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Speedup matrix completion with side information: Application to multi-label learning", "author": ["M. Xu", "R. Jin", "Z.-H. Zhou"], "venue": "Proc. NIPS", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "The recent success of deep neural networks and, in particular, convolutional neural networks (CNNs) [20] have raised the interest in geometric deep learning techniques trying to extend these models to data residing on graphs and manifolds.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 1, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 3, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 2, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 23, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 98, "endOffset": 115}, {"referenceID": 18, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "Geometric deep learning approaches have been successfully applied to computer graphics and vision [22, 2, 4, 3, 24], brain imaging [19], and drug design [11] problems, to mention a few.", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "For a comprehensive presentation of methods and applications of deep learning on graphs and manifolds, we refer the reader to the review paper [5].", "startOffset": 143, "endOffset": 146}, {"referenceID": 11, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 114, "endOffset": 122}, {"referenceID": 29, "context": "[12, 27] combining random walks with recurrent neural networks (their paper has recently enjoyed renewed interest [21, 30]).", "startOffset": 114, "endOffset": 122}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "formulated convolutionlike operations in the spectral domain, using the graph Laplacian eigenbasis as an analogy of the Fourier transform [29].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "[14] used smooth parametric spectral filters in order to achieve localization in the spatial domain and keep the number of filter parameters independent of the input size.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] proposed an efficient filtering scheme using recurrent Chebyshev polynomials applied on the Laplacian operator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "Kipf and Welling [18] simplified this architecture", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "Atwood and Towsley [1] proposed a Diffusion CNN architecture based on random walks on graphs.", "startOffset": 19, "endOffset": 22}, {"referenceID": 23, "context": "[24] (and later, [13]) proposed a spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[24] (and later, [13]) proposed a spatial-domain generalization of CNNs to graphs using local patch operators represented as Gaussian mixture models, showing a significant advantage of such models in generalizing across different graphs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "In [25], graph CNNs were extended to multiple graphs and applied to matrix completion and recommender system problems.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In this paper, we construct graph CNNs employing an efficient spectral filtering scheme based on Cayley polynomials that enjoys similar advantages of the Chebyshev filters [9] such as localization and linear complexity.", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "The main advantage of our filters over [9] is their ability to detect narrow frequency bands of importance during training, and to specialize on them while being well-localized on the graph.", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "[6] used the spectral definition of convolution (1) to generalize CNNs on graphs, with a spectral convolutional layer of the form", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Third, there is no guarantee that the filters represented in the spectral domain are localized in the spatial domain (locality property simulates local reception fields [8]); assuming k = O(n) Laplacian eigenvectors are used, a spectral convolutional layer requires O(pqk) = O(n) parameters to train.", "startOffset": 169, "endOffset": 172}, {"referenceID": 13, "context": "[14] argued that smooth spectral filter coefficients result in spatially-localized filters (an argument similar to vanishing moments).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] used parametric functions of the form", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] used polynomial filters represented in the Chebyshev basis", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 17, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 23, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 23, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 63, "endOffset": 78}, {"referenceID": 8, "context": "We test the proposed CayleyNets reproducing the experiments of [9, 18, 24, 24] and using ChebNet [9] as our main baseline method.", "startOffset": 97, "endOffset": 100}, {"referenceID": 16, "context": "SGD+Momentum and Adam [17] optimization methods were used to train the models in MNIST and the rest of the experiments, respectively.", "startOffset": 22, "endOffset": 26}, {"referenceID": 8, "context": "Following [9, 24], we approached the classical MNIST digits classification as a learning problem on graphs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 23, "context": "Following [9, 24], we approached the classical MNIST digits classification as a learning problem on graphs.", "startOffset": 10, "endOffset": 17}, {"referenceID": 9, "context": "We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling layers performing 4-times graph coarsening using the Graclus algorithm [10], and finally a fully-connected layer (this architecture replicates the classical LeNet5 [20] architecture, which is shown for comparison).", "startOffset": 259, "endOffset": 263}, {"referenceID": 19, "context": "We used a graph CNN architecture with two spectral convolutional layers based on Chebyshev and Cayley filters (producing 32 and 64 output features, respectively), interleaved with pooling layers performing 4-times graph coarsening using the Graclus algorithm [10], and finally a fully-connected layer (this architecture replicates the classical LeNet5 [20] architecture, which is shown for comparison).", "startOffset": 352, "endOffset": 356}, {"referenceID": 27, "context": "Next, we address the problem of vertex classification on graphs using the popular CORA citation graph [28].", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "We use the architecture introduced in [18, 24] (two spectral convolutional layers with 16 and 7 outputs, respectively) for solving the vertex classification task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 23, "context": "We use the architecture introduced in [18, 24] (two spectral convolutional layers with 16 and 7 outputs, respectively) for solving the vertex classification task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 6, "context": "Method RMSE MC [7] 0.", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "973 IMC [15, 31] 1.", "startOffset": 8, "endOffset": 16}, {"referenceID": 30, "context": "973 IMC [15, 31] 1.", "startOffset": 8, "endOffset": 16}, {"referenceID": 15, "context": "653 GMC [16] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "996 GRALS [26] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "945 sRGCNNCheby [24] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "Method Accuracy DCNN [1] 86.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "60% ChebNet [9] 87.", "startOffset": 12, "endOffset": 15}, {"referenceID": 17, "context": "12% GCN [18] 87.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "In our final experiment, we applied CayleyNet to recommendation system, formulated as matrix completion problem on user and item graphs [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 23, "context": "[24] approached this problem as learning with a Recurrent Graph CNN (RGCNN) architecture, using an extension of ChebNets to matrices defined on multiple graphs in order to extract spatial features from the score matrix; these features are then fed into an RNN producing a sequential estimation of the missing scores.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Here, we repeated verbatim their experiment on the MovieLens dataset [23], replacing Chebyshev filters with Cayley filters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "Our version of sRGCNN outperforms all the competing methods, including the previous result with Chebyshev filters reported in [24].", "startOffset": 126, "endOffset": 130}], "year": 2017, "abstractText": "The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute localized regular filters on graphs that specialize on frequency bands of interest. Our model scales linearly with the size of the input data for sparsely-connected graphs, can handle different constructions of Laplacian operators, and typically requires less parameters than previous models. Extensive experimental results show the superior performance of our approach on various graph learning problems.", "creator": "LaTeX with hyperref package"}}}