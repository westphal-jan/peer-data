{"id": "1705.07830", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2017", "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning", "abstract": "we propose with inexpensive question answering service that learns, examine techniques delivering quantitative evidence to improve question answering. single agent sits between the user at a question powder odds - answering game currently learns via optimally replicate a system combining natural order reformulations of the random question and to compare the evidence so return the best unexpected answer. intelligent system suggests trained screen - meets - bottom operators maximize answer availability using policy gradient. we perform on searchqa, total dataset of valid questions extracted from jeopardy!. our agent beat f1 by 11 % over a state - of - _ - art base model that uses the natural question / answer pairs.", "histories": [["v1", "Mon, 22 May 2017 16:19:21 GMT  (71kb,D)", "https://arxiv.org/abs/1705.07830v1", null], ["v2", "Wed, 12 Jul 2017 12:21:14 GMT  (66kb,D)", "http://arxiv.org/abs/1705.07830v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["christian buck", "jannis bulian", "massimiliano ciaramita", "wojciech gajewski", "rea gesmundo", "neil houlsby", "wei wang"], "accepted": false, "id": "1705.07830"}, "pdf": {"name": "1705.07830.pdf", "metadata": {"source": "CRF", "title": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning", "authors": ["Christian Buck", "Jannis Bulian"], "emails": ["cbuck@google.com", "jbulian@google.com", "massi@google.com", "wgaj@google.com", "agesmundo@google.com", "neilhoulsby@google.com", "wangwe@google.com"], "sections": [{"heading": null, "text": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning\nChristian Buck cbuck@google.com\nJannis Bulian jbulian@google.com\nMassimiliano Ciaramita massi@google.com\nWojciech Gajewski wgaj@google.com\nAndrea Gesmundo agesmundo@google.com\nNeil Houlsby neilhoulsby@google.com\nWei Wang wangwe@google.com\nGoogle\nJuly 13, 2017\nWe frame Question Answering as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box question-answering system an which learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer.\nThe reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. Our agent improves F1 by 11% over a state-of-the-art base model that uses the original question/answer pairs."}, {"heading": "1 Introduction", "text": "Web and social media have become primary sources of information. Users\u2019 expectations and information seeking activities co-evolve with the increasing sophistication of these resources. Beyond navigation, document retrieval, and simple factual question answering, users seek direct answers to complex and compositional questions. Such search sessions may require multiple iterations, critical assessment and synthesis [Marchionini, 2006].\nar X\niv :1\n70 5.\n07 83\n0v 2\n[ cs\n.C L\n] 1\n2 Ju\nl 2 01\n7\nThe productivity of natural language yields a myriad of ways to formulate a question [Chomsky, 1965]. In the face of complex information needs, humans overcome uncertainty by reformulating questions, issuing multiple searches, and aggregating responses. Inspired by humans\u2019 ability to ask the right questions, we present an agent that learns to carry out this process for the user. The agent sits between the user and a backend QA system that we refer to as the \u2018environment\u2019. The agent aims to maximize the chance of getting the correct answer by reformulating and reissuing a user\u2019s question to the environment. The agent comes up with a single best answer by asking many pertinent questions and aggregating the returned evidence. The internals of the environment are not available to the agent, so it must learn to probe a black-box optimally using only natural language. Our method resembles active learning [Settles, 2010]. In active learning, the learning algorithm chooses which instances to send to an environment for labeling, aiming to collect the most valuable information in order to improve model quality. Similarly, our agent aims to learn how to query an environment optimally, aiming to maximize the chance of revealing the correct answer to the user. Due to this resemblance we call our approach Active Question Answering (AQA). AQA differs from standard active learning in that it searches in the space of natural language questions and selects the question that yields the most relevant response. Further, AQA aims to solve each problem instance (original question) via active reformulation, rather than selecting hard ones for labelling to improve its decision boundary. The key component of our proposed solution, see Figure 1, is a sequence-to-sequence model that is trained using reinforcement learning (RL) with a reward based on the answer given by the QA environment. The second component to AQA combines the evidence from interacting with the environment using a convolutional neural network. We evaluate on a dataset of complex questions taken from Jeopardy!, the SearchQA dataset [Dunn et al., 2017]. These questions are hard to answer by design because they use obfuscated and convoluted language, e.g., Travel doesn\u2019t seem to be an issue for this sorcerer & onetime surgeon; astral projection & teleportation are no prob (answer: Doctor Strange). Thus SearchQA tests the ability of AQA to reformulate questions such that the QA system has the best chance of returning the correct answer. AQA outperforms a deep network built for QA, BiDAF [Seo et al., 2017a], which has produced state-of-the-art results on multiple tasks, by 11% absolute F1, a 32% relative F1 improvement. We conclude by proposing AQA as a general framework for stateful, iterative information seeking tasks."}, {"heading": "2 Active Question Answering", "text": "In this section we detail the components of Active Question Answering as depicted in Figure 1."}, {"heading": "2.1 The Agent-Environment Framework", "text": "Rather than sending a user\u2019s question to a QA system passively, the AQA system actively reformulates the question multiple times and issues the reformulations. The QA system acts as a black-box environment, to which AQA sends questions and receives answers. The environment returns one or more responses, from which the final answer is selected. AQA has no access to the internals of the environment, and the environment is not trained as a component of the AQA agent. The agent must learn to communicate optimally with the environment to maximize the probability of receiving the correct answer. The environment can, in principle, include multiple information sources, possibly providing feedback in different modes: images, structured data from knowledge bases, unstructured text, search results, etc. Here, the information source consists of a pretrained question answering system, that accepts natural language question strings as input, and returns strings as answers."}, {"heading": "2.2 Active Question Answering Agent", "text": "The reformulator is a sequence-to-sequence model, as is popular in neural machine translation (MT) [Sutskever et al., 2014, Bahdanau et al., 2014].1 The model\u2019s architecture consists of a multi-layer bidirectional LSTM, with attention-based decoding. The major departure from the standard MT setting is that our model reformulates utterances in the same language. Unlike in MT, there is little high quality training data available for\n1We build upon the public implementation2 of Britz et al. [2017]. 2https://github.com/google/seq2seq/\nmonolingual paraphrasing. Effective training of highly-parametrized neural networks relies on an abundance of data, thus, our setting presents an additional challenge. We address this first by pre-training the model on a related task and second, by utilizing the end-to-end signals produced though the interaction with the QA environment. This is a common strategy in deep learning, for which we develop appropriate methods.\nIn the downward pass in Figure 1 the reformulator transforms the original question into one or many alternative questions used to probe the environment for candidate answers. The reformulator is trained end-to-end, using an answer quality metric as the objective. This sequence-level loss is non-differentiable, so the model is trained using Reinforcement Learning, detailed in Section 3. In the upward pass in Figure 1, the aggregator selects the best answer. For this we use an additional neural network. The aggregator\u2019s task is to evaluate the candidate answers returned by the environment and select the one to return. Here, we assume that there is a single best answer, as is the case in our evaluation setting; returning multiple answers is a straightforward extension of the model. The aggregator is trained with supervised learning."}, {"heading": "2.3 Question-Answering Environment", "text": "Finally, we require an environment to interact with. For this we use a competitive neural question answering model, BiDirectional Attention Flow (BiDAF) [Seo et al., 2017a].3 BiDAF is an extractive QA system. It takes as input a question and a document and returns as answer a continuous span from the document. The model contains a bidirectional attention mechanism to score document snippets with respect to the question, implemented with multi-layer LSTMs and other components. The environment is opaque, the agent has no access to its internals: parameters, activations, gradients, etc. AQA may only send questions to it, and receive answers. This scenario enables us to design a general framework that permits the use of any backend. However, it means that feedback on the quality of the question reformulations is noisy and indirect, presenting a challenge for training."}, {"heading": "3 Training", "text": "To train AQA we use a combination of reinforcement and supervised learning. We also present a strategy to overcome data paucity in monolingual paraphrasing."}, {"heading": "3.1 Question Answering Environment", "text": "We treat BiDAF [Seo et al., 2017a] as a static black box QA system. We train the model on the training set for the QA task at hand, see Section 4.5 for details. Afterwards, BiDAF becomes part of the environment and its parameters are not updated while training the AQA agent. In principle, we could train both the agent and the environment jointly to further improve performance. However, this is not our desired task: our aim is\n3https://allenai.github.io/bi-att-flow/\nfor the agent to learn to communicate using natural language with an environment over which is has no control. This setting generalizes to interaction with arbitrary information sources."}, {"heading": "3.2 Policy Gradient Training of the Reformulation Model", "text": "For a given question q0, we want to return the best possible answer a, maximizing a reward R(a|q0). Typically, R is the token level F1 score on the answer. The answer a = f(q) is an unknown function of a question q, computed by the environment. Note, that the reward is computed with respect to the original question q0 while the answer is produced using a question q. The question q \u223c \u03c0\u03b8( \u00b7 |q0) is generated according to a policy \u03c0\u03b8 where \u03b8 are the policy\u2019s parameters. The policy, in this case a sequence-to-sequence model, assigns a probability\n\u03c0\u03b8(q|q0) = p\u03b8(q|q0) = T\u220f t=1 p(wt|w1, . . . , wt\u22121, q0) (1)\nto any possible question q = w1, . . . , wT where T is the length of q with tokens wt \u2208 V from a fixed vocubulary V .\nThe goal is to maximize the expected reward of the answer returned under the policy, Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))]. We optimize the reward directly with respect to parameters of the policy using the Policy Gradient algorithm [Sutton and Barto, 1998]. Since the expected reward cannot be computed in closed form, we use Monte Carlo sampling from the policy to compute an unbiased estimator,\nEq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] \u2248 1 N N\u2211 i=1 R(f(qi)), qi \u223c \u03c0\u03b8( \u00b7 |q0) (2)\nTo compute gradients for training we use REINFORCE [Williams and Peng, 1991],\n\u2207Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] = Eq\u223c\u03c0\u03b8( \u00b7 |q0)\u2207\u03b8 log(\u03c0\u03b8(q|q0))R(f(q)) (3)\n\u2248 1 N N\u2211 i=1 \u2207\u03b8 log(\u03c0(qi|q0))R(f(qi)), qi \u223c \u03c0\u03b8( \u00b7 |q0) (4)\nThis estimator is often found to have high variance, leading to unstable training [Greensmith et al., 2004]. We reudce the variance by adding a baseline: B(q0) = Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] [Williams, 1992]. This expectation is also computed by sampling from the policy given q0.\nWe often observed collapse onto a sub-optimal deterministic policy. To address this we use entropy regularization\nH[\u03c0\u03b8(q|q0)] = T\u2211 t=1 \u2211 wt\u2208V log(p\u03b8(wt|w<t, q0)) p\u03b8(wt|w<t, q0) (5)\nThis final objective is:\nEq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))\u2212B(q0)] + \u03bbH[\u03c0(q|q0)], (6)\nwhere \u03bb is the regularization weight."}, {"heading": "3.3 Initialization of the Reformulation Model", "text": "We pre-train the question reformulation model by building a paraphrasing Neural MT model, i.e. a model that can translate English to English. While parallel corpora are available for many language pairs, English-English corpora are scarce, so we cannot train monolingual model directly. Instead, we first produce a multilingual translation system that translates between several languages [Johnson et al., 2016]. This allows us to use available bilingual corpora. Multilingual training requires nothing more than adding two special tokens to the data indicating the source and target languages. 4 The encoder-decoder architecture of the translation model remains unchanged.\nAs Johnson et al. [2016] show, this model can be used for zero-shot translation, i.e. to translate between language pairs for which it has seen no training examples. For example after training English-Spanish, English-French, French-English, and Spanish-English the model has learned a single encoder that encodes English, Spanish, and French and a decoder for the same three languages. Thus, we can use the same model for FrenchSpanish, Spanish-French and also English-English translation by adding the respective tokens, e.g. <from_en> <to_en> to the source.\nJohnson et al. [2016] note that zero-shot translation is generally worse than bridging, an approach that uses the model twice: first, to translate into a pivot language, and then into the target language. However, the performance gap can be closed by running a few training steps for the desired language pair. Thus, we first train on multilingual data, then on a small corpus of monolingual data."}, {"heading": "3.4 Answer Selection", "text": "With either beam search or sampling we can produce many rewrites of a single question from our reformulation system. We issue each rewrite to the QA environment, yielding a set of (query, rewrite, answer) tuples from which we need to pick the best instance. We train another neural network to pick the best answer from the candidates. While this is a ranking problem, we frame it as binary classification, distinguishing between above and below average performance. In training, we compute the F1 score of the answer for every instance. If the rewrite produces an answer with an F1 score greater than the average score of the other rewrites the instance is assigned a positive label. We ignore questions where all rewrites yield equally good/bad answers.\nFor the classifier we evaluated FFNNs, LSTMs and CNNs and found that the performance of all systems was comparable. Since the inputs are triples of variable length sequences the latter two allow us to incorporate the tokens directly without the need for feature engineering. We choose a CNN for computational efficiency. In particular, we use pre-trained embeddings for the tokens of query, rewrite, and answer. For each, we add a 1-D CNN followed by max-pooling. The three resulting vectors are then concatenated and passed through a feed-forward network which produces the binary output.\n4For example, we prefix <from_es> <to_en> to the source side of a Spanish-English training instance.\nIn our experiments we train the answer selection model separately from the reformulator, however, jointly training both models is a promising line of future work."}, {"heading": "4 Experiments", "text": "We experiment on a new and challenging question answering dataset, SearchQA [Dunn et al., 2017]. We show that our environment, BiDAF, already shows good relative performance when run alone and improves over the published baseline. However, low absolute performance indicates that the task is challenging. The trained reformulator improves end-to-end performance using a single rewrite, i.e. without aggregation. Our end-to-end system improves over by BiDAF by 11 F1 points (32% relative)."}, {"heading": "4.1 Question Answering Data", "text": "SearchQA is a recently-released dataset built starting from a set of Jeopardy! clues. Clues are obfuscated queries such as This \u2018Father of Our Country\u2019 didn\u2019t really chop down a cherry tree. Each clue is associated with the correct answer, e.g. George Washington, and a list of snippets from Google\u2019s top search results. SearchQA contains over 140k question/answer pairs and 6.9M snippets. We train our model on the pre-defined training split, perform model selection and tuning on the validation split and report results on the validation and test splits. The training, validation and test sets contain 99,820, 13,393 and 27,248 examples, respectively."}, {"heading": "4.2 Sequence to Sequence Pre-training", "text": "For the pre-training of the reformulator we use the multilingual United Nations Parallel Corpus v1.0 [Ziemski et al., 2016]. This dataset contains 11.4M sentences which are fully aligned across six UN languages: Arabic, English, Spanish, French, Russian, and Chinese. From all bilingual pairs we produce a multilingual training corpus of 30 language pairs. This yields 340M training examples which we use to train the zero-shot neural MT system [Johnson et al., 2016]. We tokenize our data using 16k sentence pieces.5 Following [Britz et al., 2017] we use a bidirectional LSTM as encoder and an 4-layer LSTM with attention [Bahdanau et al., 2016] as decoder. The model converged after training on 400M instances using the Adam optimizer with learning rate 0.001 and batch size 128.\nThe monolingual model trained as described above has poor quality as a source of systematically-related question reformulations. For example, for the question What month, day and year did Super Bowl 50 take place?, the top rewrite is What month and year goes back to the morning and year?. To improve quality, we resume training on a monolingual dataset which is two orders of magnitude smaller than the U.N. corpus. The monolingual data is extracted from the Paralex database of question paraphrases [Fader et al., 2013].6 Unfortunately, this data contains many noisy pairs. We filter many of these pairs out by keeping only those whose the Jaccard coefficient between the sets of\n5 https://github.com/google/sentencepiece 6 http://knowitall.cs.washington.edu/paralex/\nsource and target terms is above 0.5. Further, since the number of paraphrases for each question can vary significantly, we keep at most 4 paraphrases for each question. After processing, we are left with about 1.5M pairs out of the original 35M. The refined model has visibly better quality than the zero-shot one; for the example question above it generates What year did superbowl take place?. We also tried training on the monolingual data alone. The resulting quality was in between the multilingual and refined models, consistent with the findings from Johnson et al. [2016]."}, {"heading": "4.3 RL Training of the Reformulator", "text": "After pre-training the reformulator, we switch the optimizer from Adam to SGD and train for 100k RL steps of batch size 64 with a low learning rate of 0.001. We use an entropy regularization weight of \u03bb = 0.001. For a stopping criterion, we monitor the reward from the best single rewrite, generated via greedy decoding, on the validation set. In contrast to our initial training which we ran on GPUs, this training phase is dominated by latency of the QA system and we run inference and updates on CPU and the BiDAF environment on GPU."}, {"heading": "4.4 Training the Aggregator", "text": "For the aggregator we use supervised learning: first, we train the reformulator, then we generate N = 20 rewrites for each question in the SearchQA training and validation sets. After sending these to the environment we have about 2M (question, rewrite, answer) triples to train the aggregator. We remove queries where all rewrites yield identical rewards, which removes about half of the aggregation training data.\nWe use pre-trained 100-dimensional embeddings [Pennington et al., 2014] for the tokens. Our CNN-based aggregator encodes the three strings into 100 dimensional vectors using a 1D CNN with kernel width 3 and output dimension 100 over the embedded tokens, followed by max-pooling. The vectors are then concatenated and passed through a feed-forward network which produces the binary output, indicating whether the triple performs below of above average, relative to the other reformulations and respective answers.\nThis way, we are using the training portion of the SearchQA data thrice, first for the initial training of the BiDAF model, then for the reinforcement-learning based tuning of the reformulator, and finally for training of the aggregator. We found, however, that performance was similar on both train and development indicating that no severe overfitting occurs. We use the test set only for evaluation of the final model."}, {"heading": "4.5 Baselines and Benchmarks", "text": "As a baseline, we repeat the results reported for a deep learning system developed for SearchQA Dunn et al. [2017]. This is a modified pointer network, called Attention Sum Reader.7\n7 Dunn et al. [2017] also provide a simpler baseline that ranks unigrams from the search snippets by their TF-IDF score. This baseline is not comparable to our experiments as it can only return unigram\nThe BiDAF environment can be used without the reformulator to answer the original question. This corresponds to the raw performance of BiDAF, and is our second baseline. We train BiDAF directly on the SearchQA training data. In the SearchQA task, the answers are augmented with several snippets (50 on average) returned by a Google Search for the question. We join snippets to form the context from which BiDAF selects answer spans. For performance reasons, we limit the context to the top 10 snippets. This corresponds to finding the answer on the first page of Google results. The results are only mildly affected by this limitation, for 10% of the questions there is no answer in this shorter context. These datapoints are all counted as losses. We trained with the Adam optimizer for 4500 steps, using learning rate 0.001, batch size 60.\nWe present two benchmark performance levels. The first is human performance reported in [Dunn et al., 2017] based on a sample of the test set. The second is \u2018Oracle Ranking\u2019, which provides an upper bound on the improvement that can be made by aggregation. For this, we replace the aggregator with an oracle that picks the answer with highest F1 score from the set of those returned for the reformulations."}, {"heading": "4.6 AQA Variants", "text": "We evaluate several variants of AQA. For each query q in the evaluation we generate a list of reformulations qi, for i = 1 . . . N , from the AQA reformulator trained as described in Section 3. We set N = 20 in these experiments.\nAQA Top Hyp. First, we omit aggregation, and just select the top hypothesis generated by the sequence model, q1.\nAQA Voting In addition to the most likely answer span, BiDAF also reports a model score, which we use for a heuristic weighted voting scheme to implement a deterministic aggregator. Let a be the answer returned by BiDAF for query q, with associated score s(a). We pick the answer according to argmaxa \u2211 a\u2032=a s(a\u2032).\nAQA Max Conf. We implement a second heuristic aggregator that selects the answer with the single highest BiDAF score across question reformulations.\nAQA Full Finally, we present the complete system with the learned CNN aggregation model described in Section 2.2."}, {"heading": "4.7 Results", "text": "Table 1 shows the results. We report exact match and F1 metrics, computed on token level between the predicted answer and the gold answer. We present results on the full validation and test sets (referred to as n-gram in [Dunn et al., 2017]). This includes questions that have both unigram and longer answers.\nanswers.\nSearchQA appears to be harder than other recent QA tasks such as SQuAD [Rajpurkar et al., 2016] and CNN/Daily Mail [Hermann et al., 2015], for both machines and humans. BiDAF\u2019s performance drops by 40 F1 points on SearchQA compared to SQuAD and CNN/Daily Mail. However, BiDAF is still competitive on SeachQA, improving over the baseline Attention Sum Reader network by 13.7 F1 points.\nUsing the top hypothesis alone already yields an improvement of 2.2 F1 on test. This improvement without aggregation demonstrates that the reformulator is able to produce questions more easily answered by the environment. Heuristic aggregation via both Voting and Max Conf yield a further performance boost. Both heuristics draw upon the intuition that when BiDAF is confident in its answer it is more likely to be correct, and that multiple instances of the same answer provide positive evidence (for Max Conf, the max operation implicitly rewards having an answer scored with respect to multiple questions). Finally, a trained aggregation function improves performance further, yielding an absolute increase of 11 F1 points (32% relative) over BiDAF with the original questions. In terms of exact match score this more than closes half the gap between BiDAF, a state-of-the-art QA system, and human performance."}, {"heading": "5 Related work", "text": "Bilingual corpora and machine translation have been used to generate paraphrases by pivoting through a second language[Madnani and Dorr, 2010]. Early work extracted word or phrase pairs (e, e\u2032) from a phrase table such that e translates into a foreign phrase f and f translates back to e\u2032 [Bannard and Callison-Burch, 2005]. Extensions extract full sentences using a parsing based MT system [Li et al., 2009, Ganitkevitch et al., 2011, 2013]. Recent work uses neural translation models and multiple pivots[Mallinson et al.]. In contrast, our approach does not use pivoting and, to our knowledge, is the first direct\nneural paraphrasing system. Riezler et al. [2007] propose a phrase-based paraphrasing system for queries which they use to extract synonyms of terms in a given query. In contrast to our approach, the reformulated queries are not used directly, their system uses phrase-tables extracted via pivoting and the use of MT for paraphrasing is mainly introduced to include context, i.e. the other words of the original query, when choosing synonyms. Reinforcement learning is gaining traction in natural language understanding across many problems. For example, Narasimhan et al. [2015] use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and Li et al. [2017] use RL for dialogue generation. Policy gradient methods[Williams, 1992, Sutton et al., 1999] have been investigated recently for MT and other sequence-to-sequence problems. They alleviate limitations inherent to the word-level optimization of the cross-entropy loss, allowing sequence-level reward functions, like BLEU, to be used. Sequence level reward functions based on language models and reconstruction errors are used to bootstrap MT with fewer resources [Xia et al., 2016]. Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward.\nUses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning. The work of Nogueira and Cho [2016] is most related to ours. Their goal is to identify a document containing an answer to a question by following links on a document graph. Evaluating on a set of questions from the game \u201cJeopardy!\u201d, they learn to walk the Wikipedia graph using an RNN until they reach the predicted article/answer. In a recent follow-up Nogueira and Cho [2017] improve document retrieval with an approach inspired by relevance feedback in combination with RL. They reformulate a query by adding terms from documents retrieved from a search engine for the original query. Our work differs in that we generate full reformulations via sequence-to-sequence modeling rather than adding single terms, and we target question-answering, rather than document retrieval. Ensemble methods often boost performance of ML systems, and QA models are no exception. This is usually achieved through randomization; predictions from several replicas of the base system, trained independently, are combined to increase robustness. AQA also combines several predictions, but differs conceptually. A classic ensemble fixes the input and perturbs the model parameters. In contrast, AQA perturbs the input, keeping the model fixed. Our approach also resembles active learning [Settles, 2010] because our agent optimizes the input to an environment from which it collects data to receive the most useful responses. AQA departs from the usual regime of active learning in the following ways.\n1. AQA must choose the optimal input in the space of natural language questions,\nusually active learning algorithms select from real-valued feature vectors. The former, being discrete and highly structured, is challenging to optimize.\n2. AQA optimizes the inputs sent to the environment with respect to expected endto-end performance. Active learning usually optimizes a pre-specified acquisition criterion 8.\n3. AQA seeks the best response per datapoint, i.e. conditioned on a original question. Active learning optimizes inputs to acquire generic useful labels training time. We must ensure that the questions sent to, and answers received from, the environment are useful with respect to the original query.\nFinally, Active QA is related to recent research on fact-checking: Wu et al. [2017] propose to perturb database queries in order to estimate the support of quantitative claims. In Active QA questions are perturbed with a similar purpose, although directly at the surface natural language form."}, {"heading": "6 Conclusion", "text": "We propose a new framework to improve question answering. We call it active question answering (AQA), as it aims to improve answering by systematically perturbing input questions. We investigated a first system of this kind that has three components: a question reformulator, a black box QA system, and a candidate answer aggregator. The reformulator and aggregator form a trainable agent that seek to elicit the best answers from the QA system. Importantly, the agent may only query the environment with natural language questions. Experimental results prove that the approach is highly effective. We improve a sophisticated Deep QA system by 11% absolute F1, 32% relative F1, on a difficult dataset of long, semantically complex, questions."}, {"heading": "6.1 Future Work", "text": "A direct extension is to plug in multiple different environments, such as additional documents or a knowledge base. The reformulator can choose which questions to send to which environments and the aggregator must now accumulate evidence of different modalities. This setting handles multi-task scenarios naturally, where the same AQA agent is trained to interact with multiple backends to solve multiple QA tasks.\nAs a longer-term extension, we will investigate the sequential, iterative aspect of QA, and frame the problem as an end-to-end RL task, thus, closing the loop between the reformulator and the aggregator. Figure 2 depicts the generalized AQA agent-environment framework. The agent (AQA) interacts with the environment (E) in order to answer a question (q0). The environment includes a question answering system (Q&A), and emits observations and rewards. A state at time t is the sequence of observations and\n8Bayes optimal active learning also tries to optimize end-to-end performance, however, these techniques are often computationally prohibitive in large-scale settings [Roy and McCallum, 2001].\nactions generated starting from q0, st = x0, u0, x1, ..., ut\u22121, xt, where xi includes the question asked (qi), the corresponding answer returned by the QA system (ai), and possibly additional information such as features and auxiliary tasks. The agent includes an action scoring component (U), which decides whether to submit a new question to the environment or return a final answer. Formally, ut \u2208 Q \u222a A, where Q is the set of all possible questions, and A is the set of all possible answers. The agent relies on a question reformulation system (QR), that provides candidate follow up questions, and on an answer ranking system (AR), which scores the answers contained in st. Each answer returned is assigned a reward. The objective is to maximize the expected reward over a set of questions.\nWith respect to this work there are a few important extensions. The decision making component, including answer scoring and action type prediction, is jointly trained with the reformulator. This component also plays the role of a critic or Q-function. Additionally, the critic can pass internal states to the reformulator, making the reformulations stateful. Finally, this formulation allows multi-step episodes to be performed."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "An actor-critic algorithm for sequence prediction", "author": ["D. Bahdanau", "P. Brakel", "K. Xu", "A. Goyal", "R. Lowe", "J. Pineau", "A. Courville", "Y. Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2016}, {"title": "Paraphrasing with bilingual parallel corpora", "author": ["C. Bannard", "C. Callison-Burch"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Bannard and Callison.Burch.,? \\Q2005\\E", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Massive exploration of neural machine translation architectures", "author": ["D. Britz", "A. Goldie", "M.-T. Luong", "Q. Le"], "venue": null, "citeRegEx": "Britz et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "Aspects of the Theory of Syntax", "author": ["N. Chomsky"], "venue": null, "citeRegEx": "Chomsky.,? \\Q1965\\E", "shortCiteRegEx": "Chomsky.", "year": 1965}, {"title": "SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine", "author": ["M. Dunn", "L. Sagun", "M. Higgins", "U. Guney", "V. Cirik", "K. Cho"], "venue": null, "citeRegEx": "Dunn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Dunn et al\\.", "year": 2017}, {"title": "Paraphrase-Driven Learning for Open Question Answering", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Fader et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Learning sentential paraphrases from bilingual parallel corpora for text-to-text generation", "author": ["J. Ganitkevitch", "C. Callison-Burch", "C. Napoles", "B. Van Durme"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2011}, {"title": "Ppdb: The paraphrase database", "author": ["J. Ganitkevitch", "B. Van Durme", "C. Callison-Burch"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Comutational Linguistics: Human Language Technologies,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["E. Greensmith", "P.L. Bartlett", "J. Baxter"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Greensmith et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Greensmith et al\\.", "year": 2004}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Ko\u010disk\u00fd", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["J. Li", "W. Monroe", "A. Ritter", "M. Galley", "J. Gao", "D. Jurafsky"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Joshua: An open source toolkit for parsing-based machine translation", "author": ["Z. Li", "C. Callison-Burch", "C. Dyer", "J. Ganitkevitch", "S. Khudanpur", "L. Schwartz", "W.N. Thornton", "J. Weese", "O.F. Zaidan"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation,", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "Neural symbolic machines: Learning semantic parsers on freebase with weak supervision", "author": ["C. Liang", "J. Berant", "Q. Le", "K.D. Forbus", "N. Lao"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Liang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2017}, {"title": "Generating phrasal and sentential paraphrases: A survey of data-driven methods", "author": ["N. Madnani", "B.J. Dorr"], "venue": "Computational Linguistics,", "citeRegEx": "Madnani and Dorr.,? \\Q2010\\E", "shortCiteRegEx": "Madnani and Dorr.", "year": 2010}, {"title": "Exploratory search: From finding to understanding", "author": ["G. Marchionini"], "venue": "Commun. ACM,", "citeRegEx": "Marchionini.,? \\Q2006\\E", "shortCiteRegEx": "Marchionini.", "year": 2006}, {"title": "Language understanding for text-based games using deep reinforcement learning", "author": ["K. Narasimhan", "T. Kulkarni", "R. Barzilay"], "venue": null, "citeRegEx": "Narasimhan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2015}, {"title": "End-to-end goal-driven web navigation", "author": ["R. Nogueira", "K. Cho"], "venue": "In Proceedings of the 30th International Conference on Neural Information Processing Systems,", "citeRegEx": "Nogueira and Cho.,? \\Q2016\\E", "shortCiteRegEx": "Nogueira and Cho.", "year": 2016}, {"title": "Task-oriented query reformulation with reinforcement learning", "author": ["R. Nogueira", "K. Cho"], "venue": null, "citeRegEx": "Nogueira and Cho.,? \\Q2017\\E", "shortCiteRegEx": "Nogueira and Cho.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": null, "citeRegEx": "Ranzato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2015}, {"title": "Statistical machine translation for query expansion in answer retrieval", "author": ["S. Riezler", "A. Vasserman", "I. Tsochantaridis", "V. Mittal", "Y. Liu"], "venue": "In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL", "citeRegEx": "Riezler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2007}, {"title": "Toward optimal active learning through monte carlo estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "ICML, Williamstown,", "citeRegEx": "Roy and McCallum.,? \\Q2001\\E", "shortCiteRegEx": "Roy and McCallum.", "year": 2001}, {"title": "Bidirectional Attention Flow for Machine Comprehension", "author": ["M. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Query-reduction networks for question answering", "author": ["M. Seo", "S. Min", "A. Farhadi", "H. Hajishirzi"], "venue": "In Proceedings of ICLR 2017,", "citeRegEx": "Seo et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2017}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison,", "citeRegEx": "Settles.,? \\Q2010\\E", "shortCiteRegEx": "Settles.", "year": 2010}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Introduction to Reinforcement Learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "In Proceedings of the 12th International Conference on Neural Information Processing Systems,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Mach. Learn.,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["R.J. Williams", "J. Peng"], "venue": "Connection Science,", "citeRegEx": "Williams and Peng.,? \\Q1991\\E", "shortCiteRegEx": "Williams and Peng.", "year": 1991}, {"title": "Computational fact checking through query perturbations", "author": ["Y. Wu", "P.K. Agarwal", "C. Li", "J. Yang", "C. Yu"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "Wu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2017}, {"title": "Dual learning for machine translation", "author": ["Y. Xia", "D. He", "T. Qin", "L. Wang", "N. Yu", "T.-Y. Liu", "W.-Y. Ma"], "venue": "In Proceedings of the 30th International Conference on Neural Information Processing Systems,", "citeRegEx": "Xia et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2016}, {"title": "The united nations parallel corpus v1.0", "author": ["M. Ziemski", "M. Junczys-Dowmunt", "B. Poliquen"], "venue": "In Proceedings of Language Resources and Evaluation (LREC),", "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Such search sessions may require multiple iterations, critical assessment and synthesis [Marchionini, 2006].", "startOffset": 88, "endOffset": 107}, {"referenceID": 4, "context": "The productivity of natural language yields a myriad of ways to formulate a question [Chomsky, 1965].", "startOffset": 85, "endOffset": 100}, {"referenceID": 26, "context": "Our method resembles active learning [Settles, 2010].", "startOffset": 37, "endOffset": 52}, {"referenceID": 5, "context": "We evaluate on a dataset of complex questions taken from Jeopardy!, the SearchQA dataset [Dunn et al., 2017].", "startOffset": 89, "endOffset": 108}, {"referenceID": 3, "context": "1We build upon the public implementation2 of Britz et al. [2017]. 2https://github.", "startOffset": 45, "endOffset": 65}, {"referenceID": 29, "context": "We optimize the reward directly with respect to parameters of the policy using the Policy Gradient algorithm [Sutton and Barto, 1998].", "startOffset": 109, "endOffset": 133}, {"referenceID": 32, "context": "To compute gradients for training we use REINFORCE [Williams and Peng, 1991], \u2207Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] = Eq\u223c\u03c0\u03b8( \u00b7 |q0)\u2207\u03b8 log(\u03c0\u03b8(q|q0))R(f(q)) (3) \u2248 1 N N \u2211", "startOffset": 51, "endOffset": 76}, {"referenceID": 9, "context": "This estimator is often found to have high variance, leading to unstable training [Greensmith et al., 2004].", "startOffset": 82, "endOffset": 107}, {"referenceID": 31, "context": "We reudce the variance by adding a baseline: B(q0) = Eq\u223c\u03c0\u03b8( \u00b7 |q0)[R(f(q))] [Williams, 1992].", "startOffset": 76, "endOffset": 92}, {"referenceID": 5, "context": "We experiment on a new and challenging question answering dataset, SearchQA [Dunn et al., 2017].", "startOffset": 76, "endOffset": 95}, {"referenceID": 35, "context": "0 [Ziemski et al., 2016].", "startOffset": 2, "endOffset": 24}, {"referenceID": 3, "context": "5 Following [Britz et al., 2017] we use a bidirectional LSTM as encoder and an 4-layer LSTM with attention [Bahdanau et al.", "startOffset": 12, "endOffset": 32}, {"referenceID": 1, "context": ", 2017] we use a bidirectional LSTM as encoder and an 4-layer LSTM with attention [Bahdanau et al., 2016] as decoder.", "startOffset": 82, "endOffset": 105}, {"referenceID": 6, "context": "The monolingual data is extracted from the Paralex database of question paraphrases [Fader et al., 2013].", "startOffset": 84, "endOffset": 104}, {"referenceID": 19, "context": "We use pre-trained 100-dimensional embeddings [Pennington et al., 2014] for the tokens.", "startOffset": 46, "endOffset": 71}, {"referenceID": 5, "context": "5 Baselines and Benchmarks As a baseline, we repeat the results reported for a deep learning system developed for SearchQA Dunn et al. [2017]. This is a modified pointer network, called Attention Sum Reader.", "startOffset": 123, "endOffset": 142}, {"referenceID": 5, "context": "5 Baselines and Benchmarks As a baseline, we repeat the results reported for a deep learning system developed for SearchQA Dunn et al. [2017]. This is a modified pointer network, called Attention Sum Reader.7 7 Dunn et al. [2017] also provide a simpler baseline that ranks unigrams from the search snippets by their TF-IDF score.", "startOffset": 123, "endOffset": 230}, {"referenceID": 5, "context": "The first is human performance reported in [Dunn et al., 2017] based on a sample of the test set.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": "We present results on the full validation and test sets (referred to as n-gram in [Dunn et al., 2017]).", "startOffset": 82, "endOffset": 101}, {"referenceID": 20, "context": "SearchQA appears to be harder than other recent QA tasks such as SQuAD [Rajpurkar et al., 2016] and CNN/Daily Mail [Hermann et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 10, "context": ", 2016] and CNN/Daily Mail [Hermann et al., 2015], for both machines and humans.", "startOffset": 27, "endOffset": 49}, {"referenceID": 14, "context": "Bilingual corpora and machine translation have been used to generate paraphrases by pivoting through a second language[Madnani and Dorr, 2010].", "startOffset": 118, "endOffset": 142}, {"referenceID": 2, "context": "Early work extracted word or phrase pairs (e, e\u2032) from a phrase table such that e translates into a foreign phrase f and f translates back to e\u2032 [Bannard and Callison-Burch, 2005].", "startOffset": 145, "endOffset": 179}, {"referenceID": 34, "context": "Sequence level reward functions based on language models and reconstruction errors are used to bootstrap MT with fewer resources [Xia et al., 2016].", "startOffset": 129, "endOffset": 147}, {"referenceID": 26, "context": "Our approach also resembles active learning [Settles, 2010] because our agent optimizes the input to an environment from which it collects data to receive the most useful responses.", "startOffset": 44, "endOffset": 59}, {"referenceID": 13, "context": "Riezler et al. [2007] propose a phrase-based paraphrasing system for queries which they use to extract synonyms of terms in a given query.", "startOffset": 0, "endOffset": 22}, {"referenceID": 11, "context": "For example, Narasimhan et al. [2015] use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and Li et al.", "startOffset": 13, "endOffset": 38}, {"referenceID": 9, "context": "[2015] use RL to learn control policies for multi-user dungeon games where the state of the game is summarized by a textual description, and Li et al. [2017] use RL for dialogue generation.", "startOffset": 141, "endOffset": 158}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT.", "startOffset": 0, "endOffset": 23}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al.", "startOffset": 0, "endOffset": 485}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning.", "startOffset": 0, "endOffset": 564}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning. The work of Nogueira and Cho [2016] is most related to ours.", "startOffset": 0, "endOffset": 727}, {"referenceID": 0, "context": "Bahdanau et al. [2016] extend this line of work using actor-critic training for MT. RL training can also prevent exposure bias; an inconsistency between training and inference time stemming from the fact that the model never sees its own mistakes during training [Ranzato et al., 2015, Shen et al., 2016]. We also use policy gradient to optimize our agent, however, we use end-to-end question answering quality as the reward. Uses of policy gradient for QA include Liang et al. [2017], who train a semantic parser to query a knowledge base, and Seo et al. [2017b] who propose query reduction networks that transform a query to answer questions that involve multi-hop common sense reasoning. The work of Nogueira and Cho [2016] is most related to ours. Their goal is to identify a document containing an answer to a question by following links on a document graph. Evaluating on a set of questions from the game \u201cJeopardy!\u201d, they learn to walk the Wikipedia graph using an RNN until they reach the predicted article/answer. In a recent follow-up Nogueira and Cho [2017] improve document retrieval with an approach inspired by relevance feedback in combination with RL.", "startOffset": 0, "endOffset": 1069}, {"referenceID": 33, "context": "Finally, Active QA is related to recent research on fact-checking: Wu et al. [2017] propose to perturb database queries in order to estimate the support of quantitative claims.", "startOffset": 67, "endOffset": 84}, {"referenceID": 23, "context": "8Bayes optimal active learning also tries to optimize end-to-end performance, however, these techniques are often computationally prohibitive in large-scale settings [Roy and McCallum, 2001].", "startOffset": 166, "endOffset": 190}], "year": 2017, "abstractText": "We frame Question Answering as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box question-answering system an which learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. Our agent improves F1 by 11% over a state-of-the-art base model that uses the original question/answer pairs.", "creator": "LaTeX with hyperref package"}}}