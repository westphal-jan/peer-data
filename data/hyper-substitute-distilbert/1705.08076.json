{"id": "1705.08076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Learning from partial correction", "abstract": "we gain a new model of interactive learning models which explicit insight examines the predictions of a learner must therefore fixes them if they are misleading. although additional kind of feedback is not relevant. se. k., modern approach corresponding generalization bounds demonstrating the intentions of the learned experts.", "histories": [["v1", "Tue, 23 May 2017 05:07:52 GMT  (57kb,D)", "https://arxiv.org/abs/1705.08076v1", null], ["v2", "Wed, 24 May 2017 16:39:52 GMT  (57kb,D)", "http://arxiv.org/abs/1705.08076v2", "10 pages"], ["v3", "Thu, 25 May 2017 16:50:52 GMT  (57kb,D)", "http://arxiv.org/abs/1705.08076v3", "10 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sanjoy dasgupta", "michael luby"], "accepted": false, "id": "1705.08076"}, "pdf": {"name": "1705.08076.pdf", "metadata": {"source": "CRF", "title": "Learning from partial correction", "authors": ["Sanjoy Dasgupta", "Michael Luby"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Partial correction is a natural paradigm for interactive learning. Suppose, for example, that a taxonomy is to be constructed on a large set of species I, using steps of interaction with an expert. To see how one such step might go, let\u2019s say the learner\u2019s current model is some hierarchy h. Since it is likely too large to be fathomed in its entirety, a small set of species q \u2282 I is chosen at random (for instance, q = {dolphin, elephant, mouse, rabbit, whale, zebra}), and the biologist is shown the restriction of h to just these species, denoted h(q). See Figure 1. If this subtree is correct, the biologist accepts it. If not, he or she provides a partial correction in the form of a triplet like ({dolphin, whale}, zebra), meaning \u201cthere should be a cluster that contains dolphin and whale but not zebra\u201d, that the correct tree must satisfy. This is easier than fixing the entire subtree.\nEarlier models of interactive learning have typically adopted a question-answer paradigm: the learner asks a question and the expert answers it completely. In active learning of binary classifiers, for example, the question is a data point and the answer is a single bit, its label. When learning broader families of structures, however, partial correction can be more convenient and intuitive. In the tree case, the minimal question would consist of three species, and the expert would need to provide the restriction of the target hierarchy to these three leaves. But seeing a larger snapshot is helpful: it provides more context, and thus more guidance about the levels of granularity of clusters; it allows the expert to select one especially egregious flaw to fix, rather than having to correct minor mistakes that might in any case go away once the bigger problems are resolved; and, by allowing choice, it also potentially produces more reliable feedback. Finally, if the subtree is correct, the expert can accept it with a single click, and is saved the nuisance of having to enter it.\nFormally, we assume that there is a space of structures H (for instance, trees over a fixed set of species), of which some h\u2217 \u2208 H is the target. Any h \u2208 H can be specified by its answers to a set of questions Q (for instance, all subsets of six species). On each step of learning:\n\u2022 The learner selects some hypothesis h \u2208 H based on feedback received so far.\n\u2022 Some q \u2208 Q is chosen at random.\n\u2022 The learner displays q and h(q) to an expert.\n\u2022 If h(q) is correct, the expert accepts it. Otherwise, the expert fixes some part of it.\nTo formalize this partial correction, we assume that each h(q) contains up to c atomic components, individual pieces that can be corrected. In the tree example, these are triples of species, so c = ( 6 3 ) = 20. We index these components as (q, 1), . . . , (q, c). The expert picks some j for which h(q, j) 6= h\u2217(q, j) and provides h\u2217(q, j). One case of technical interest, which we will later return to, is when the components of q are independently chosen from the same distribution. We will call such a distribution on queries component independent.\nar X\niv :1\n70 5.\n08 07\n6v 3\n[ cs\n.L G\n] 2\n5 M\nay 2\n01 7\nAs another example, suppose each q is a sequence of c video frames of the driver\u2019s view in a car, and h\u2217(q, j) is the appropriate driving action for the jth frame. On each step of interaction, a human labeler is shown c frames, each labeled with an action, and either accepts all these actions as reasonable or corrects one of them. In this case it is unlikely that the distribution on queries is component independent.\nFormally, on each step of interaction, the learner either finds out that its prediction h(q) = (h(q, 1), . . . , h(q, c)) is entirely correct, or receives the correct value h\u2217(q, j) for just one atomic component j. This kind of feedback is not i.i.d.: first, the feedback is constrained to be only one component on which h is incorrect if there is such a component; and second, among possibly several such components, the expert chooses one in some haphazard arbitrary manner. Ideally, the expert\u2019s choices are illustrative and help the learning process, and we will soon see a simple example of this kind. But in this paper we also study the other extreme: is it true that even if the expert adversarially chooses what feedback to give, the same rate of convergence as i.i.d. sampling is always assured? We show that this is indeed the case, and this is a crucial sanity check for the partial correction model. Furthermore, we show that our algorithms are optimal with respect to natural metrics."}, {"heading": "1.1 Learning procedure", "text": "Let \u00b5 be a probability distribution on Q, and let q \u2208\u00b5 Q indicate that q is chosen independently from Q according to \u00b5; in the tree example above, Q is all subsets of six species and \u00b5 is the uniform distribution on Q. On step t = 1, 2, . . . of learning,\n1. Learner selects some ht \u2208 H consistent with all feedback received so far\n2. Choose q \u2208\u00b5 Q, where q has c atomic components, (q, 1), . . . , (q, c).\n3. Learner displays q and ht(q) to expert\n4. If ht(q) is correct:\n\u2022 Expert feeds back that ht(q) is correct \u2022 Feedback implicitly provides, for all j \u2208 [c], h\u2217(q, j) = ht(q, j).\nElse ht(q) is incorrect:\n\u2022 Expert chooses 1 \u2264 j \u2264 c for which ht(q, j) 6= h\u2217(q, j) \u2022 Expert feeds back j and h\u2217(q, j)."}, {"heading": "1.2 Results", "text": "The error of a hypothesis h \u2208 H can be measured in two ways: in terms of full questions q \u2208 Q,\nerr(h) = Prq\u2208\u00b5Q[h(q) 6= h\u2217(q)].\nor in terms of atomic components (q, j):\nerrc(h) = Prq\u2208\u00b5Q,j\u2208R[c][h(q, j) 6= h \u2217(q, j)].\nThese are related by errc(h) \u2264 err(h) \u2264 c \u00b7errc(h). Note that errc(h) \u2248 err(h)/c if \u00b5 is component independent and err(h) is small.\nAn important complexity metric is the expert cost per step to provide feedback. This cost can be substantially lower in the new model: The expert can choose a component that is easiest to determine is incorrect amongst a set of c components, instead of being required to provide feedback for a particular component. We leave to future work the study of this metric in more detail.\nAnother crucial complexity metric is the number of steps of feedback required to learn. We start with a simple one-dimensional example (Section 2) that illustrates how the expert\u2019s choice of feedback can significantly affect this metric. In the example, one feedback strategy reduces the number of steps needed for learning by a factor of up to c (so that each feedback component is about as valuable as c randomly chosen components), while a different strategy increases the number of steps by a factor of \u2126(c) (slows down learning).\nThe example demonstrates that the number of steps needed to learn can vary by wide margins depending on the expert policy. Our main result (Theorem 2) shows that, despite this, there is a reasonable bound on the number of steps to learn no matter how adversarial the expert policy: For any expert policy, for any 0 < \u03b4, < 1, with probability 1\u2212 \u03b4 the base algorithm of Section 1.1 produces a hypothesis h with err(h) \u2264 within O((c/ ) \u00b7 log(|H|/\u03b4)) steps of feedback. Moreover (Theorem 8), after the same number of steps, all remaining consistent hypotheses have errc(h) \u2264 . Section 6 shows that this number of steps is needed for at least some examples.\nIn the standard supervised learning model [1], labeled data is provided in advance, after which a consistent hypothesis is sought. In our protocol, feedback is obtained in steps, and the learner needs to maintain a consistent hypothesis throughout the process. Because it can be expensive to continually select a consistent hypothesis, we introduce the stick-with-it algorithm, a variant of the base algorithm, that might be preferable in practice (Section 5). Rather than always having to select a hypothesis that is consistent with all feedback received so far at each step, it only updates its hypothesis O(c) times during the entire learning process. It works for any concept class of bounded VC dimension d, it obtains a hypothesis h with err(h) \u2264 within O((c/ ) \u00b7 (d+ log(1/\u03b4))) steps of feedback.\nTo obtain these sample complexity bounds, we look at the effective distribution wt over atomic components (q, j) at each time step t, which is a function of previous feedback, the learning algorithm\u2019s choice of current model ht, and the expert\u2019s criterion for selecting what to correct. This can be quite different from the distribution that would be easy to analyze, where q \u2208\u00b5 Q and j is chosen at random; in particular, wt can be zero at many (q, j) with \u00b5(q) > 0. Nonetheless, we show that over time, no matter what policy the expert chooses, wt cannot avoid covering the whole Q\u00d7 [c] space in some suitably amortized sense.\nThe growing area of interactive learning raises many new problems and challenges. Here we have formalized an interactive protocol that is quite natural and intuitive in terms of human-computer interface, but breaks the statistical assumptions that underlie generalization results in other settings like the PAC model [1]. Our key technical contribution is to establish sample complexity bounds in this novel framework."}, {"heading": "2 An illustrative example", "text": "Suppose X = [0, 1] and the goal is to learn a threshold classifier:\nH = {hv : v \u2208 [0, 1]}, hv(x) = 1(x > v).\nSay the target threshold is 0 (that is, h\u2217 = h0), so that the correct label for all points in (0, 1] is 1. If we were learning from random examples (x, h\u2217(x)) then, independent of the distribution on X , after O(1/ ) samples all remaining consistent hypotheses would (with probability close to one) consist entirely of classifiers h with err(h) \u2264 . Thus, after O(1) instances, the error would be lower than any pre-specified constant."}, {"heading": "2.1 Uniformly distributed, component independent queries", "text": "Let \u00b5c be the uniform distribution on atomic components X , and let \u00b5 be the component independent distribution over the space of queries Q = X c induced by \u00b5c. Thus, for any v \u2208 [0, 1], errc(hv) = v and err(hv) = 1\u2212 (1\u2212 v)c, and thus errc(h) \u2248 err(h)/c if err(h) is small. We\u2019ll try to understand how the rate of convergence of vt to zero is affected by c and by the expert labeler\u2019s policy for which errors to correct, where vt is the threshold the learner has determined after t steps, i.e., vt is the smallest component value for which feedback has been provided by the expert and thus only classifiers with threshold at most vt are still consistent with the feedback through t steps.\nWe consider two expert policies:\n\u2022 \u201cLargest\u201d: the expert picks the largest component in error. This corresponds to a natural tendency to fix the biggest mistake, but happens to be the least informative correction.\n\u2022 \u201cSmallest\u201d: the expert picks the smallest component in error. This is the most informative correction.\nThe next query is chosen by picking x1, . . . , xc \u2208R [0, 1]. Let Vt+1 be the random variable that is the threshold value the learner determines at step t+ 1. What is the expected value of Vt+1?\nWhen the labeling policy is \u201clargest\u201d: For any v \u2208 [0, vt], the only way Vt+1 can exceed v is if either all the xi are \u2265 vt (and thus they are all correctly labeled by hvt) or if at least one of the xi lies in (v, vt) (in which case, there is at least one error, but the largest component in error exceeds vt):\nPr(Vt+1 > v | Vt = vt) = Pr(all xi \u2265 vt) + (1\u2212 Pr(no xi in (v, vt))) = (1\u2212 vt)c + (1\u2212 (1\u2212 (vt \u2212 v))c)\nTherefore, by calculation, E[Vt+1 | Vt = vt] = \u222b vt\n0\nPr(Vt+1 > v | Vt = vt)dv = vt \u2212 1\u2212 (1\u2212 vt)c \u00b7 (1 + c \u00b7 vt)\nc+ 1 .\nWhen the labeling policy is \u201csmallest\u201d: For v \u2208 [0, vt], the only way Vt+1 can exceed v is if none of the xi lie in [0, v], so Pr(Vt+1 > v | Vt = vt) = (1\u2212 v)c, whereupon, by a similar integral,\nE[Vt+1 | Vt = vt] = 1\u2212 (1\u2212 vt)c+1\nc+ 1 .\nWhen c = 1, the two policies coincide and E[Vt+1|vt] = vt\u2212 v2t /2, so the expected instantaneous reduction in Vt, that is E[vt \u2212 Vt+1], from seeing a single-point query is v2t /2. How does this compare to the expected instantaneous reduction from queries consisting of c points? The ratio of the expected reduction with c-point queries to the expected reduction with 1-point queries is shown in Figure 2 for c = 4, 8 and for the \u201csmallest\u201d, \u201clargest\u201d labeling policies. The ratio is given at each value vt.\nAs expected, under the \u201csmallest\u201d labeling policy, c-point queries are always more helpful than single-point queries. Under the \u201clargest\u201d policy, this is true only when vt is sufficiently small. In either case, when vt gets close to zero, the single label yielded by a c-point query is roughly as informative as c random labeled points. This can be checked using simple approximations to the expressions obtained above.\nThis toy example shows that the rate of convergence of learning by partial correction depends on the labeler\u2019s policy for deciding which error to fix. Even in a one-dimensional setting with few degrees of freedom, different labeling policies can cause convergence to either slow down or speed up, by factors of up to c. We now formalize lower bounds of this type."}, {"heading": "2.2 A lower bound on component-level error", "text": "We continue with the one-dimensional example, but now turn to distributions that are not component independent.\nWe will consider a learner that begins with a threshold of 1, and at any given time, chooses the largest threshold consistent with all feedback so far: namely, the smallest labeled point it has received."}, {"heading": "2.2.1 A single query, repeated", "text": "To start with an especially simple case, say the distribution \u00b5 over Q is supported on a single point, (1/c, 2/c, . . . , 1). Suppose moreover that the expert labeler behaves as follows: when presented with a labeling of the points 1/c, 2/c, . . . , 1, he/she always chooses to \u201ccorrect the most glaring flaw\u201d, that is, the highest value for which a 0 label is suggested.\nIt is clear that x = 1 is labeled in the first round, x = (c \u2212 1)/c in the second round, x = (c \u2212 2)/c in the third round, and so on. The labeler\u2019s behavior is hardly pathological. And yet, it takes c/2 rounds of interaction to bring the error down to 1/2. If the feedback were on random components, then O(1) rounds would have been sufficient."}, {"heading": "2.2.2 Lower bound", "text": "Pick any > 0, and now consider a distribution \u00b5 over Q that is supported on just two points:( 1 2c , 2 2c , . . . , 1 2 ) probability 2 (\n1 2 + 1 2c , 1 2 + 2 2c , . . . , 1\n) probability 1\u2212 2\nAny hypothesis with errc(hv) \u2264 must have v \u2264 1/4. In order to achieve this, the learner must see the first point at least c/2 times, which requires seeing \u2126(c/ ) samples overall, with high probability.\nWe have established the following.\nTheorem 1 There is a concept class H of VC dimension 1 such that for any > 0, it is necessary to have \u2126(c/ ) rounds of feedback in order to be able to guarantee that with high probability, all hypotheses h consistent with this feedback have errc(h) \u2264 ."}, {"heading": "3 Main result", "text": "For each h \u2208 H, let\nB(h) = {q \u2208 Q : h is incorrect on q}, G(h) = {q \u2208 Q : h is correct on q}\nNote that err(h) = \u00b5(B(h)) is the probability that h is incorrect on a randomly chosen query. We say that hypothesis h is -good if \u00b5(B(h)) \u2264 . On input ( , \u03b4), the goal is to find an h \u2208 H that is -good with probability at least 1\u2212 \u03b4.\nLet ` = log(H/\u03b4), let \u2032 = /2, and let N = c \u00b7 ( ` \u2032 + 1 ) .\nTheorem 2 The base algorithm of Section 1.1 produces an -good hypothesis within 2 \u00b7N steps with probability at least 1\u2212 \u03b4.\nIt is interesting to compare Theorem 2 when \u00b5 is component independent to classical learning theory. Theorem 2 shows that after at most 2 \u00b7N = O(c \u00b7 log(|H|)/ ) steps the output hypothesis h satisfies err(h) \u2264 , which implies errc(h) \u2264 /c if \u00b5 is component independent. This is the same number of steps the classical model would take to achieve component error /c when each question is a single component and the expert provides complete feedback for each question. Of course, the bound of Theorem 2 applies whether or not \u00b5 is component independent.\nThe remainder of this section concentrates on proving Theorem 2. The analysis procedes in two phases: the first phase considers the first N steps, and the second phase considers the subsequent N steps. Let\nQ\u0304 = Q\u00d7 [c], B\u0304(h) = {(q, j) \u2208 Q\u0304 : q \u2208 B(h) and h(q, j) 6= h\u2217(q, j)}, G\u0304(h) = G(h)\u00d7 [c]."}, {"heading": "3.1 Effective sampling distribution", "text": "At the beginning of step t, some hypothesis ht is selected. The analysis defines an effective sampling distribution wt over Q\u0304, as follows:\n\u2022 For all (q, j) \u2208 B\u0304(ht), let \u03b3(q, j) denote the conditional probability that the expert provides feedback on (q, j) when query q is made. Define wt(q, j) = \u00b5(q) \u00b7 \u03b3(q, j).\n\u2022 For all q \u2208 G(ht) calculate wt(q, 1), . . . , wt(q, c), summing to \u00b5(q), as specified below in Lemma 3.\nFinally, let Wt(q, j) = w1(q, j) + \u00b7 \u00b7 \u00b7+ wt(q, j)\ndenote the sum of the individual distributions up to step t. Note that at each step t, for each q \u2208 Q, we have wt(q, [c]) = \u00b5(q) and thus Wt(q, [c]) = t \u00b7 \u00b5(q).\nLemma 3 For all q \u2208 G(ht), non-negative values for wt(q, 1), . . . , wt(q, c), summing to \u00b5(q), can be calculated such that if wt(q, j) > 0 then\nWt(q, j) = Wt\u22121(q, j) + wt(q, j) \u2264 t \u00b7 \u00b5(q) c .\nProof: Since Wt(q, [c]) will be equal to t \u00b7 \u00b5(q), t\u00b7\u00b5(q)c will be the average over j \u2208 [c] of Wt(q, j). Thus, we can choose wt(q, 1), . . . , wt(q, c) as follows. Let j1, . . . , jc be an ordering of the elements of [c] such that\nWt\u22121(q, j1) \u2264Wt\u22121(q, j2) \u2264 \u00b7 \u00b7 \u00b7 \u2264Wt\u22121(q, jc).\nInitialize \u2206 = \u00b5(q), wt(q, j1) = \u00b7 \u00b7 \u00b7 = wt(q, jc) = 0. Repeat the following for i = 1, . . . , c until \u2206 = 0: Reset\nwt(q, ji) = min { t \u00b7 \u00b5(q) c \u2212Wt\u22121(q, ji),\u2206 }\nand reset \u2206 = \u2206\u2212 wt(q, ji)."}, {"heading": "3.2 Eliminating inconsistent hypotheses", "text": "Lemma 4 With probability at least 1 \u2212 \u03b4, the following holds for all h \u2208 H: if there is a step t at which Wt(B\u0304(h)) \u2265 `, then h is not consistent with the feedback received up to that step.\nProof: Pick any h \u2208 H. It is eliminated if feedback is received on any (q, j) \u2208 B\u0304(h). The probability that this happens at step t is at least wt(B\u0304(h)).\nLet t be the first step at which Wt(B\u0304(h)) \u2265 `. The probability that h is not eliminated by the end of step t is at most\n(1\u2212 w1(B\u0304(h))) \u00b7 (1\u2212 w2(B\u0304(h))) \u00b7 \u00b7 \u00b7 (1\u2212 wt(B\u0304(h))) \u2264 exp(\u2212Wt(B\u0304(h))) \u2264 exp(\u2212`) = \u03b4\n|H| .\nTaking a union bound over H, with probability at least 1\u2212 \u03b4, any hypothesis h is eliminated from the version space by the step at which Wt(B\u0304(h)) \u2265 `.\nAt the outset of any step t, we may therefore assume that ht satisfies Wt\u22121(B\u0304(ht)) < `."}, {"heading": "3.3 Analysis for Phase 1", "text": "Consider a first phase consisting of the first N steps. Let\n\u03c4 = N\nc =\n`\n\u2032 + 1\nbe a threshold value. We will think of an atomic question (q, j) as having been adequately sampled when Wt(q, j) reaches \u03c4 \u00b7 \u00b5(q).\nAt the beginning of step t, let\nL\u0304t\u22121 = {(q, j) \u2208 Q\u0304 : Wt\u22121(q, j) \u2264 \u03c4 \u00b7 \u00b5(q)}.\nThus Wt\u22121(L\u0304t\u22121) = \u2211 (q,j)\u2208L\u0304t\u22121 Wt\u22121(q, j) \u2264 c \u00b7 \u03c4 = N\n(substituting a suitable integral over Q if necessary). Let\nL\u0304\u2032t\u22121 = {(q, j) \u2208 Q\u0304 : Wt\u22121(q, j) \u2264 (\u03c4 \u2212 1) \u00b7 \u00b5(q) = ` \u2032 \u00b7 \u00b5(q)}.\nThis is the set of (q, j) in which there is still some remaining capacity.\nLemma 5 At any step t, if Wt\u22121(B\u0304(ht)) < `, then\nwt(B\u0304(ht) \u2229 L\u0304\u2032t\u22121) \u2265 \u00b5(B(ht))\u2212 \u2032.\nProof: Note that\n\u00b5(B(ht)) = wt(B\u0304(ht)) = wt(B\u0304(ht) \u2229 L\u0304\u2032t\u22121) + wt(B\u0304(ht) \\ L\u0304\u2032t\u22121). (1)\nSince ` > Wt\u22121(B\u0304(ht)) \u2265Wt\u22121(B\u0304(ht) \\ L\u0304\u2032t\u22121) \u2265 ` \u2032 \u00b7 wt(B\u0304(ht) \\ L\u0304\u2032t\u22121),\nit follows that wt(B\u0304(ht) \\ L\u0304\u2032t\u22121) \u2264 \u2032 and thus wt(B\u0304(ht) \u2229 L\u0304\u2032t\u22121) \u2265 \u00b5(B(ht))\u2212 \u2032 follows from Equation (1).\nLemma 6 At any step t \u2264 N , wt(L\u0304t) \u2265 1\u2212 \u2032.\nProof: Note that wt(L\u0304t) = wt(B\u0304(ht) \u2229 L\u0304t) + wt(G\u0304(ht) \u2229 L\u0304t).\nSince any (q, j) \u2208 B\u0304(ht)\u2229 L\u0304\u2032t\u22121 satisfies (q, j) \u2208 B\u0304(ht)\u2229 L\u0304t, Lemma 5 implies wt(B\u0304(ht)\u2229 L\u0304t) \u2265 \u00b5(B(ht))\u2212 \u2032. For q \u2208 G(ht), from Lemma 3 and noting that t \u2264 N , any (q, j) with wt(q, j) > 0 satisfies\nWt(q, j) \u2264 t \u00b7 \u00b5(q) c \u2264 \u03c4 \u00b7 \u00b5(q),\nand thus (q, j) \u2208 L\u0304t, and it follows that wt(G\u0304(ht) \u2229 L\u0304t) = \u00b5(G(ht)). Overall,\nwt(L\u0304t) \u2265 \u00b5(B(ht))\u2212 \u2032 + \u00b5(G(ht)) = 1\u2212 \u2032.\nLet W\u0302t(q, j) = min{Wt(q, j), \u03c4 \u00b7 \u00b5(q)}. As we have seen, W\u0302t(Q\u0304) \u2264 N .\nCorollary 7 W\u0302N (Q\u0304) \u2265 (1\u2212 \u2032) \u00b7N .\nProof: An immediate consequence of Lemma (6)."}, {"heading": "3.4 Analysis for Phase 2", "text": "We now finish the proof of Theorem 2. Proof: Consider a second phase of N additional steps. Let ht be the current hypothesis for one of these steps. If \u00b5(B(ht)) \u2265 2 \u00b7 \u2032 then \u00b5(B(ht))\u2212 \u2032 \u2265 \u2032, and the first half of the proof of Lemma 6 implies that W\u0302t(Q\u0304) increases by at least \u2032 during this step. However, since W\u0302t(Q\u0304) \u2264 N , and since W\u0302N (Q\u0304) \u2265 (1\u2212 \u2032) \u00b7N at the beginning of the second phase from Corollary 7, there can be at most N steps in the second phase where W\u0302t(Q\u0304) increases by at least \u2032. Thus, during one of the steps in the second phase \u00b5(B(ht)) \u2264 2 \u00b7 \u2032 = , at which point the base algorithm can select ht as an -good hypothesis and terminate. This concludes the proof of Theorem 2."}, {"heading": "4 Generalization bound", "text": "The following generalization bound holds at the end of Phase 1.\nTheorem 8 With probability at least 1\u2212 \u03b4, any h \u2208 H that remains in the version space at the end of Phase 1 (that is, after N = c \u00b7 ( ` \u2032 + 1) rounds of feedback) has errc(h) < .\nProof: Let \u00b5\u0304 be the distribution over Q\u0304 that corresponds to picking q from \u00b5 and then picking a feature at random: \u00b5\u0304(q, j) = \u00b5(q)/c. Thus for any h \u2208 H, we have errc(h) = \u00b5\u0304(B\u0304(h)).\nAt the end of Phase 1, W\u0302N (Q\u0304) \u2265 (1\u2212 \u2032) \u00b7N . Thus for any h \u2208 H,\nW\u0302N (B\u0304(h)) \u2265  \u2211 (q,j)\u2208B\u0304(h) \u03c4 \u00b7 \u00b5(q) \u2212 \u2032 \u00b7N =  \u2211 (q,j)\u2208B\u0304(h) N \u00b7 \u00b5\u0304(q, j) \u2212 \u2032 \u00b7N = N \u00b7 (\u00b5\u0304(B\u0304(h))\u2212 \u2032). If \u00b5\u0304(B\u0304(h)) \u2265 = 2 \u00b7 \u2032, we get\nWN (B\u0304(h)) \u2265 W\u0302N (B\u0304(h)) \u2265 N \u00b7 \u2032 > c \u00b7 `.\nBy Lemma 4, with probability at least 1\u2212 \u03b4, any such h is eliminated by the end of the Nth step.\nRecall from Theorem 1 that this c/ dependence is inevitable."}, {"heading": "5 Stick-with-it algorithm", "text": "There are some practical issues with the base algorithm of Section 1.1. One issue is that at the beginning of each step a current hypothesis needs to be selected that is consistent with all previous steps. This results in a number of selections equal to the number of steps.\nA second issue is that a separate procedure is needed to evaluate whether a given hypothesis is -good in order to terminate the base algorithm with a hypothesis that is verified to be -good, since a current hypothesis may be the current hypothesis for only one step.\nA third issue is that the analysis doesn\u2019t obviously carry over to the case where H = |H| is unbounded but instead the VC-dimension of H is bounded.\nWe introduce the stick-with-it algorithm, a variant of the base algorithm, that addresses all these issues. We use an integer k \u2265 1 to describe the following changes to the base algorithm:\n\u2022 Instead of selecting a current hypothesis at each step, a current hypothesis is selected each k consecutive steps. Thus, once a current hypothesis is selected it is used as the current hypothesis for k consecutive steps. (This is where \"stick-with-it\" comes from.)\n\u2022 Redefine N = c \u00b7 ( ` \u2032 + k ) . All parameters defined in terms of N are similarly redefined, e.g., \u03c4 = ` \u2032 + k.\n\u2022 Redefine L\u0304\u2032t = {(q, j) \u2208 Q\u0304 : Wt(q, j) \u2264 (\u03c4 \u2212 k) \u00b7 \u00b5(q) = ` \u2032 \u00b7 \u00b5(q)}.\nEssentially the same analysis as described for the base algorithm shows that the stick-with-it algorithm produces an h \u2208 H that is -good with probability at least 1\u2212 \u03b4 within 2 \u00b7N steps.\nSetting k = ` \u2032 = 2\u00b7` results in a stick-with-it algorithm with the following properties:\n\u2022 The number of steps is at most 2 \u00b7N \u2264 8\u00b7c\u00b7` .\n\u2022 The number of steps a current hypothesis needs to be selected that is consistent with all previous steps is at most 2\u00b7Nk \u2264 4 \u00b7 c.\n\u2022 A selected current hypothesis is the current hypothesis for enough steps (k) to determine if it is -good, and if it is -good then the stick-with-it algorithm terminates.\n\u2022 Let d be the VC-dimension of H. The same analysis applies when ` = d+ log(1/\u03b4).\nThe stick-with-it algorithm is optimal in the following metrics:\n\u2022 The bound on the number of steps, including steps to verify that the output hypothesis is -good, is within a constant factor of the number needed for some examples. (See Section 6.)\n\u2022 The bound on the number of selections of a current hypothesis is within a constant factor of the number needed for some examples. (See Section 6.)\n\u2022 The same optimality holds when the VC-dimension of H is bounded."}, {"heading": "6 Lower bound on number of steps and selected hypotheses", "text": "An example showing the number of needed steps can be proportional to c\u00b7log |H| is obtained as follows. Let Q be any fixed subset of Q such that |Q | = \u00b7 |Q|. Let h\u2217 be the correct hypothesis. For i = 1, . . . , c/2, let Hi consist of all classifiers h with the following properties:\n\u2022 h(q, j) = h\u2217(q, j) if q 6\u2208 Q or j /\u2208 {2i, 2i\u2212 1}.\n\u2022 For each q \u2208 Q , either\n\u2013 h(q, 2i\u2212 1) 6= h\u2217(q, 2i\u2212 1) and h(q, 2i) = h\u2217(q, 2i), or\n\u2013 h(q, 2i\u2212 1) = h\u2217(q, 2i\u2212 1) and h(q, 2i) 6= h\u2217(q, 2i)\nNotice that any such h has err(h) = , and that |Hi| = 2|Q |. Finally, define\nH = c/2\u22c3 i=1 Hi \u222a {h\u2217}.\nWrite ` = |Q | = \u00b7Q. Then log |H| \u2248 `+ log c \u2248 `. Let\u2019s say the learner always prefers to select a hypothesis in H1, then H2, and so on, and will only select h\u2217 when all these options have been exhausted. When a hypothesis in Hi is selected, it will make a mistake whenever the query lies in Q . This mistake will provide no information about any other q \u2208 Q , or about any other hypothesis in Hi\u2032 for i\u2032 6= i.\nThus, on average `/ queries will be needed to eliminate Hi for each i = 1, . . . , c/2, for a total query complexity of roughly c\u00b7`2\u00b7 .\nIn this example the number of selected hypotheses is at least c/2, i.e., at least one hypothesis from each of H1, . . . ,Hc/2 is selected as the current hypothesis at some step."}, {"heading": "7 Acknowledgments", "text": "This work is a direct result of the Foundations of Machine Learning program at the Simons Institute, UC Berkeley."}], "references": [{"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1984}], "referenceMentions": [{"referenceID": 0, "context": "In the standard supervised learning model [1], labeled data is provided in advance, after which a consistent hypothesis is sought.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "Here we have formalized an interactive protocol that is quite natural and intuitive in terms of human-computer interface, but breaks the statistical assumptions that underlie generalization results in other settings like the PAC model [1].", "startOffset": 235, "endOffset": 238}, {"referenceID": 0, "context": "Suppose X = [0, 1] and the goal is to learn a threshold classifier:", "startOffset": 12, "endOffset": 18}, {"referenceID": 0, "context": "H = {hv : v \u2208 [0, 1]}, hv(x) = 1(x > v).", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": "Thus, for any v \u2208 [0, 1], errc(hv) = v and err(hv) = 1\u2212 (1\u2212 v), and thus errc(h) \u2248 err(h)/c if err(h) is small.", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": ", xc \u2208R [0, 1].", "startOffset": 8, "endOffset": 14}], "year": 2017, "abstractText": "We introduce a new model of interactive learning in which an expert examines the predictions of a learner and partially fixes them if they are wrong. Although this kind of feedback is not i.i.d., we show statistical generalization bounds on the quality of the learned model.", "creator": "LaTeX with hyperref package"}}}