{"id": "1705.08432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Deep Learning of Grammatically-Interpretable Representations Through Question-Answering", "abstract": "we observe synthetic architecture in which internal concepts, learned by end - to - market skills in complicated deep working cycle following dynamic textual pattern - answering study, begin be interpreted using basic concepts requiring structured intuition. auditory interpretability permits up a glance of simplicity at few percentage - inch reduction in accuracy relative to the original model on which the raw one is processed ( { [ a ] ). using internal representation that is formulated is a virtual output representation : for all input word, the model selects a mapping to encode the word, context draws role upon which to render the symbol, and binds the two actions. the function is via soft interpretation. the language grammar is captured from interpretations around the symbols, imagery generated by the trained model, and interpretations of the roles as used from module design. our find \u201c applying our internal hypothesis knowing symbols can easily characterized as lexical - semantic word meanings, while roles \" be portrayed as recognition of ambiguous roles ( or categories ) such and subject, wh - word, determiner, etc. through our lengthy, fine - grained analysis, we find specific correspondences matching phrases corresponding roles and parts expressing speech as handled by current standard parser [ 2 ], and detect several errors illustrating the model's favor. reflecting this sense, the model learns significant aspects along grammar, language having been linked solely to linguistically separated text, verbs, and answers : gradually prior linguistic knowledge became given than the model. what documents analyzed beyond translated means to convey using categories and languages then sets initial representation favoring use over noun terms an approximately quantitative region.", "histories": [["v1", "Tue, 23 May 2017 17:40:14 GMT  (856kb,D)", "http://arxiv.org/abs/1705.08432v1", null], ["v2", "Mon, 25 Sep 2017 23:49:18 GMT  (343kb,D)", "http://arxiv.org/abs/1705.08432v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hamid palangi", "paul smolensky", "xiaodong he", "li deng"], "accepted": false, "id": "1705.08432"}, "pdf": {"name": "1705.08432.pdf", "metadata": {"source": "CRF", "title": "Deep Learning of Grammatically-Interpretable Representations Through Question-Answering", "authors": ["Hamid Palangi", "Paul Smolensky", "Xiaodong He", "Li Deng"], "emails": ["hpalangi@microsoft.com", "psmo@microsoft.com", "xiaohe@microsoft.com", "l.deng@ieee.org"], "sections": [{"heading": null, "text": "\u2217This work was carried out while PS was on leave from Johns Hopkins University. LD is currently at Citadel.\nar X\niv :1\n70 5.\n08 43\n2v 1\n[ cs\nto represent using symbols and roles and an inductive bias favoring use of these in an approximately discrete manner."}, {"heading": "1 Introduction: Minding the gap", "text": "The difficulty of interpreting internal representations within deep neural networks fundamentally derives from the incommensurability between, on the one hand, the continuous, numerical representations and operations of these networks and, on the other, meaningful interpretations\u2014which are communicable in natural language through relatively discrete, non-numerical conceptual categories structured by conceptual relations. This gap could in principle be reduced if deep neural networks were to incorporate internal representations that are directly interpretable as discrete structures; the categories and relations of these representations might then be understandable conceptually.\nIn the work reported here, we describe how approximately discrete, structured distributed representations can be embedded within deep networks, their categories and structuring relations being learned end-to-end through performance of a task. Applying this approach to a challenging naturallanguage question-answering task, we show how the learned representations can be understood as approximating syntactic and semantic categories and relations. In this sense, the model we present learns significant aspects of syntax/semantics, recognizable using the concepts of linguistic theory, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is built into the model is a general capacity for distributed representation of structures, and an inductive bias favoring discreteness in its deployment.\nSpecifically, the task we address is question answering for the SQuAD dataset [3], in which a text passage and a question are presented as input, and the model\u2019s output identifies a stretch within the passage that contains the answer to the question (see Sec. 4). The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6]. The new model proposed here is built from the BiDAF model proposed in [1] for question answering. It replaces a bidirectional RNN built from LSTM units [7] with one built from TPR units; the architecture is called the Tensor Product Recurrent Network, TPRN. TPRN learns the\nvector embeddings, and learns which abstract symbols to deploy in which abstract roles to represent each of the words in the text-passage and query inputs.\nWe show how the structural roles that TPRN learns can be interpreted through linguistic concepts at multiple levels: morphosyntactic word features, parts of speech, phrase types, and grammatical roles of phrases such as subject and object. The match between standard linguistic concepts and TPRN \u2019s internal representations is approximate, and we identify several discrepancies in the model\u2019s favor.\nThe work reported here illustrates how learning to perform a natural language question-answering task can lead a deep learning system to create representations that are interpretable as encoding abstract grammatical concepts without ever being exposed to data labelled with anything like grammatical structure. This is the type of setting in which children learn their first language, so the work lends plausibility to the hypothesis that abstract notions of linguistic theory do describe representations in speakers\u2019 minds\u2014representations that are learned in the service of performing tasks such as question-answering which (unlike, say, a parsing task) do not explicitly necessitate any such structure."}, {"heading": "2 The Model", "text": "The proposed TPRNarchitecture is built in TensorFlow [8] on the BiDAF model proposed in [1]. BiDAF is constructed from 6 layers: a character embedding layer using CNNs, a word embedding layer using GLOVE [9] vectors, a phrase embedding layer using bidirectional LSTMs for sentence embedding [10], an attention flow layer using a special attention mechanism, a modeling layer using LSTMs, and an output layer that generates pointers to the start and end of an answer in the paragraph. (See Fig. 1 of [1].)\nTPRN replaces the LSTM cells forming the bidirectional RNN in the phrase embedding layer with recurrent TPR cells, described next: see Fig. 1."}, {"heading": "3 TPR: distributed representation of structures", "text": "Although the approach generalizes in obvious ways, for conciseness, here we describe the model whose performance is described in Sec. 4 and whose representations are interpreted in Sec. 5. This TPRNmodel enables the phrase-embedding layer of the model to decide, for each word, how to encode that word by selecting among 100 symbols, each of which it can choose to\ndeploy in any of 20 slots in an abstract structure. The symbols and slots have no meaning prior to training. We hypothesized that the symbol selected by the trained model for encoding a given input word will be interpretable in terms of the lexical-semantic content of the word (e.g., Australia refers to a place) while the slots will be interpretable as grammatical roles such as subject/agent, object/patient, question-restrictor phrase. In Sec. 5, we will test this hypothesis; we will henceforth refer to \u201croles\u201d rather than \u201cslots\u201d. In other words, our hypothesis was that the particular word tokens for which a given symbol was selected would form a lexical-semantically-related class, and the particular word tokens for which a given role was selected would form a grammatically-related class.\nTo function within the network, the symbols and roles must each be embedded as vectors; we used vectors of dimension 10. These 10-dimensional embedding vectors are designed by the network. The network\u2019s parameters, including these embeddings, are driven by back-propagation to minimize an objective function relevant to the model\u2019s question-answering task. The objective function includes a standard cross-entropy error measure, but also quantization, a kind of regularization function biasing the model towards parameters that yield decisions that select, for each word, a single symbol in a single role: the selection of symbols and roles is soft-selection, and we will say that the model\u2019s encoding assigns, to the tth word w(t), a symbol-attention vector aS(t) and a role-attention vector aR(t). The quantization term in the\nobjective function pushes towards attention vectors that are 1-hot. However, as we now see, the encodings in TPRNare generally highly distributed.\nIn the encoding for w(t), the vector s(t) encoding the symbol is the attention-weighted sum of the 100 possible symbols: s(t) = \u2211100 j=1[aS(t)]jsj = SaS(t) where sj is the embedding of the jth symbol in R10, which is the jth column of the symbol matrix S. Similarly, the vector encoding the role assigned to w(t) is r(t) = \u221120 k=1[aR(t)]krk = RaR(t), with rk the embedding of the kth symbol in R10 and the kth column of the role matrix R. (The dimensions of symbol- and role-vectors need not be the same.) Since the symbol {sj}j=1:100 and role {rk}k=1:20 vectors are unconstrained, they generally emerge from the learning process as highly distributed; that is even more true of the overall representations {v(t)}, as we now see.\nThe activation vector v(t) that encodes a single word w(t) combines the word\u2019s symbol-embedding vector, s(t), and its role-embedding vector, r(t), via the outer or tensor product: v(t) = aS(t)aR(t)> = aS(t) \u2297 aR(t). We say that v(t) is the tensor product representation (TPR) of the binding of symbol s to the role r. A convenient expression for v(t) is:\nv(t) \u2261 s(t)(r(t))> = ( SaS(t) ) ( RaR(t) )> = S ( aS(t)aR(t)> ) R> = SB(t)R>\n(1) The matrix B(t) \u2261 aS(t)aR(t)> is the binding matrix for word w(t), which encodes the selection of symbol and role for w(t). This matrix has 100\u00d7 20 = 2K elements, although the actual representation sent to deeper layers is only of size 100 = 10 \u00d7 10 (symbol \u00d7 role embedding dimensions). Equation 1 is depicted graphically in the \u2018TPR unit\u2019 insert in Fig. 1. During I \u2192 O computation, in the forward-directed RNN, the representation vt\u22121 of the previous word is used to compute the attention vectors aS(t), aR(t) which in turn are used to compute the representation of the current word.\nBecause each word is represented (approximately) as the TPR of a single symbol/role binding, we can interpret the internal representations of TPRN \u2019s phrase-embedding layer once we can interpret the symbols and roles it has invented. Such interpretation is carried out in Sec. 5.\nThe interest in TPR lies not only in its interpretability, but also in its power. The present TPRNmodel incorporates TPR to only a modest degree, but it is a proof-of-concept system that paves the way for future models that can import the power of general symbol-structure processing, proven to be within the scope of full-blown TPR architectures [5, 11]. TPRN is designed to scale up to such architectures; design decisions such as factoring the encoding as v(t) = aS(t)aR(t)> are far from arbitrary.\nAs the name TPRNsuggests, the novel representational capacity built\ninto TPRN is an RNN built of TPR units: a forward- and a backwarddirected RNN in each of which the word w(t) generates an encoding which is a TPR: v(t) = SB(t)R>; the binding matrix B(t) varies across words, but a single symbol matrix S and single role matrix R apply for all words {w(t)}.\nIt remains only to specify the quantization function Q (2) which is added (with weight cQ) to the cross-entropy to form the training objective for TPRN : Q generates a bias favoring attention vectors aS(t) and aR(t) that are 1-hot.\nQ = Qa(aS(t)) +Qa(aR(t));Qa(a) = \u03a3i(ai)2(1\u2212 ai)2 + ( \u03a3i(ai)2 \u2212 1 )2 (2)\nThe first term of Q is minimized when each component of a satisfies ai \u2261 [a]i \u2208 {0, 1}; the second term is minimized when \u2016a\u201622 = 1. The sum of these terms is minimized when a is 1-hot. Q leads the weights in the final network to generate aS(t) and aR(t) vectors that are approximately 1-hot, but there is no mechanism within the network for enforcing (even approximately) 1-hot vectors at I \u2192 O computation time."}, {"heading": "4 Experiments", "text": "In this section, we describe details of the experiments for the proposed TPRNmodel on question answering task using the Stanford\u2019s SQuAD dataset [3]. The results of interest are the interpretations of the learned representations, discussed at length in Sec. 5. 1\nPlease note that the goal of this work is not to beat the state-of-the-art system on SQuAD (at the time of writing this paper, r-net, [12]), but to create a question answering system that is interpretable, by exploiting TPR. Therefore, as long as we have a system with TPR cells that performs reasonably well, we can use it to provide proof of concept for the interpretability claims in this work.\nSQuAD is a reading comprehension dataset for question answering. It consists of more than 500 Wikipedia articles and more than 100,000 questionanswer pairs about them, which is significantly larger than previous reading comprehension datasets [3]. The questions and answers are human-generated. The answer to each question is determined by two pointers in the passage, one pointing to the start of the answer and the other one pointing to its end. Two metrics that are used to evaluate models on this dataset are Exact Match (EM) and F1 score.\n1The codes are available at https://github.com/Palang2014/QA_TPR_Public\nFor the experiments, we used the same settings reported in [1] for all layers of TPRNexcept the phrase embedding layer. (Note that these hyperparameters had previously been optimized for the original model, and were not re-optimized for TPRN.) The phrase embedding layer is replaced by our proposed recurrent TPR cells. The full setting of the TPRNmodel for experiments is as follows:\n\u2022 Questions and paragraphs were tokenized by PTB tokenizer.\n\u2022 The concatenation of word embedding using GLOVE [9] and character embedding using Convolutional Neural Networks (CNNs) was used to represent each word. The embedding vector for each character was of length 8 (1-D input to the CNN) and the output of the CNN\u2019s max-pooling layer over each word was a vector of length 100. The embedding size of word embedding using GLOVE was also set to 100.\n\u2022 For TPR, 100 symbols and 20 roles were used. Embedding size of 10 for both symbol vectors and role vectors was used. This means that the matrix S in (1) was of size 10\u00d7 100 and the matrix R in (1) was of size 10\u00d7 20. We used vec(v(t)) as the output of our phrase embedding layer. vec(.) vectorizes the given matrix.\n\u2022 The weight cQ = 0.00001 was used for both filler-related and rolerelated terms in (2).\n\u2022 The optimizer used was AdaDelta [13] with 12 epochs.\nEach experiment for the TPRNmodel took about 13 hours on a single Tesla P100 GPU. Performance results of our single model compared with a single model in [1] are presented in Table 1. From this table we observe that our proposed TPR based model underperforms [1] by about 2 points. This is good enough performance for our proposed TPRNmodel to assume that it has learned appropriate representations for questions and paragraphs. Focussing on queries, we explore the representations in the learned TPRs in considerable detail. Sec. 5.1.2 mentions a few selected, highly targeted performance comparisons against the POS tagging provided by the Stanford parser [2]."}, {"heading": "5 Interpretation of learned TPRs", "text": ""}, {"heading": "5.1 Interpreting learned TPR Roles", "text": "Here we provide interpretation of the TPR roles aR(t) assigned to the words w(t) of the query input in the forward-directed TPR-RNN of TPRN . Just as good learned neural network models in vision typically acquire similar early types of representations of an input image (e.g., [14]), it is reasonable to hypothesize that good learned neural network models in language will typically learn low-level input representations that are generally similar to one another. Thus we can hope for some generality of the types of interpretation discussed here. Convergence on common input representations is expected because these representations capture the regularities among the inputs, useful for many tasks that process such input. The kinds of regularities to be captured in linguistic input have been studied for years by linguists, so there is reason to expect convergence between good learned neural network language-input representations and general linguistic concepts. The following interpretations provide evidence that such an expectation is merited.\nWe consider which word tokens w(t) are \u2018assigned to\u2019 (or \u2018select\u2019) a particular role k, meaning that, for an appropriate threshold \u03b8k, [a\u0302R(t)]k > \u03b8k where a\u0302R(t) is the L2-normalized role-attention vector."}, {"heading": "5.1.1 Grammatical role concepts learned by the model", "text": ""}, {"heading": "A grammatical category\u2014Part of Speech: Determiner \u223c Role #9", "text": "The network assigns to role #9 these words: a significant proportion of the tokens of: the (76%), an (52%), a (46%), its (36%) and a few tokens of of (8%) and Century (3%). The dominant words assigned to role #9 (the, an, a, its) are all determiners. Although not a determiner, of is also an important function word; the 3% of the tokens of Century that activate role #9 can be put aside. Quantitatively, p(w is a determiner|w activates role #9 to > 0.65) = 0.96. This interpretation does not assert that #9 is the only role for\ndeterminers; e.g., p(w activates role #9|w \u2208 {a, an, the}) = 0.70."}, {"heading": "A semantic category: Predicate (verbs and adjectives) \u223c Role #17:", "text": "The words assigned to role #17 are overwhelmingly predicates, a semantic category corresponding to the syntactic categories of verbs and adjectives [e.g., under semantic interpretation, J runs \u2192 runs(J); J is tall \u2192 tall(J)] . While the English word orders of these two types of predication are often opposite (the girl runs vs. the tall girl), the model represents them as both filling the same role, which can be interpreted as semantic rather than syntactic. Quantitatively, p(w is a verb or adjective|w selects role #17) = 0.82. Unlike role #9, which concerns only a small (\u2018closed\u2019) class of words, the class of predicates is large (\u2018open\u2019), and role #17 is assigned to only a rather small fraction of predicate tokens: e.g., p(w is assigned to role #17|w is a verb) = 0.04.\nA grammatical feature: [plural] \u223c Role #10: To observe the representational difference between the singular and plural roles we need to fix on particular words. A case study of area vs. areas revealed a total separation in their attention to role #10 (which has a midpoint level of 0.25): 100% of tokens of singular area have [a\u0302R(t)]10 < 0.25; 100% of tokens of plural areas have [a\u0302R(t)]10 > 0.25. This conclusion is corroborated by pronouns, where he;him each have mean [a\u0302R(t)]10 = 0.1, while they;them have [a\u0302R(t)]10 = 0.4; 0.6 (there are very few tokens of she;her).\nA grammatical phrase-type: wh-operator restrictor \u2018phrase\u2019 \u223c Role #1: Role #1 is assigned to sequences of words including how many teams, what kind of buildings, what honorary title. We interpret these as approximations to a wh-restrictor phrase: a wh-word together with a property that must hold of a valid answer\u2014crucial information for questionanswering. In practice, these \u2018phrases\u2019 span from a wh-word to approximately the first following content word. Other examples are: what was the American, which logo was, what famous event in history.\nGrammatical functions: Subject/agent vs. object/patient \u223c Role #6: A fundamental abstract distinction in syntax/semantics separates subjects/agents from objects/themes. In English the distinction is explicitly marked by distinct word forms only on pronouns: he loves her vs. she loves him. In the model, attention to role #6 is greater for subjects than objects, for both he vs. him and they vs. them (again, too few tokens of she;her). All but 13 of 124 tokens of he and all 77 tokens of they allocate high attention"}, {"heading": "5.1.2 Correcting the Stanford Parser\u2019s POS labeling using learned roles", "text": "When Doctor Who is not a name: Role #7 The TV character Doctor Who (DW ) is named many times in the SQuAD query corpus. Now in . . . DW travels . . . , the phrase DW is a proper noun (\u2018NNP\u2019), with unique referent, but in . . . does the first DW see . . ., the phrase DW must be a common noun (\u2018NN\u2019), with open reference. In such cases the Stanford parser misclassifies Doctor as an NNP in 9 of 18 occurrences: see Table 2a. In . . . the first DW serial . . ., first modifies serial and DW is a proper noun. The parser misparses this as an NN in 37 of 167 cases. Turning to the model, we can interpret it as distinguishing the NN vs. NNP parses of DW via role #7, which it assigns for the NN, but not the NNP, case. Of the Stanford parser\u2019s 9 errors on NNs and 37 errors on NNPs, the model misassigns role #7 only once for each error type (shown in parentheses in Table 2a). The model makes 7 errors total (Table 2b) while the parser makes 46. Focussing on the specific NN instances of the form the nth DW, there are 19 cases: the parser was incorrect on 11, and in every such case the model was correct; the parser was correct in 8 cases and of these the model was also correct on 6.\nWhen Who is a name: Role #1 In Doctor Who travelled, the word Who should not be parsed as a question word (\u2018WP\u2019), but as part of a proper noun (NNP). The Stanford parser makes this error in every one of the 167 occurrences of Who within the NNP Doctor Who. The TPRNmodel, however, usually avoids this error. Recalling that role #1 marks the whrestrictor \u2018phrase\u2019, we note that in 81% of these NNP-Who cases, the model does not assign role #1 to Who (in the remaining cases, it does assign role #1 as it includes Who within its wh-restrictor \u2018phrase\u2019, generated by a distinct genuine wh-word preceding Who). In all 30 instances of Who as a genuine\nquestion word in a sentence containing DW, the model correctly assigns role #1 to the question word. For example, in Who is the producer of Doctor Who? [query 7628], the first Who correctly selects role #1 while the second, correctly, does not. (The model correctly selects role #1 for non-initial who in many cases.)\nWhen to doctor is not a verb: Role #17 The Stanford parser parses Doctor as a verb in 4 of its 5 occurrences in . . . to Doctor Who . . .. The model does not make this mistake on any of these 5 cases: it assigns near-zero activity to role #17, identified above as the predicate role for verbs and adjectives."}, {"heading": "5.2 Interpreting learned TPR symbols", "text": ""}, {"heading": "5.2.1 Meaning of learned symbols: Lexical-semantic coherence of symbol assignments", "text": "To interpret the lexical-semantic content of the TPR symbols s(t) learned by the TPRNnetwork, we:\n1. s(t) = SaS(t) \u2208 R10 is calculated for all (120,950) word tokens w(t)in the validation set.\n2. The cosine similarity is computed between aS(t) and the embedding vector of each symbol.\n3. The symbol with maximum (cosine) similarity is assigned to the corresponding token.\n4. For each symbol, all tokens assigned to it are sorted based on their similarity to it; tokens of the same type are removed, and the top tokens from this list are examined to assess by inspection the semantic coherence of the symbol assignments (see Tables 3 \u2013 5).\nThe results provide significant support for our hypothesis that each symbol corresponds to a particular meaning, assigned to a cloud of semanticallyrelated word tokens. For example, symbol 27 and symbol 6 can be respectively interpreted as meaning \u2018occupation\u2019 and \u2018geopolitical unit\u2019. Symbol 11 is assigned to multiple forms of the verb to be, e.g., was (85.8% of occurrences of token in the validation set), is, (93.2%) being (100%) and be (98%). Symbol 29 is selected by 10 of the 12 month names (along with other word types, more details in supplementary materials). Other symbols with semantically\ncoherent token sets are reported in the supplementary materials. Some symbols, however, lack identifiable coherence; an example is presented in Table 4.\nTable 3: Symbol 27\nToken Similarity printmaker 0.9587 composer 0.8992 who 0.8726 mathematician 0.8675 guitarist 0.8622 musician 0.8055 Whose 0.7774 engineer 0.7753 chemist 0.7485 how 0.7335 strict 0.7207\nTable 4: Symbol 2\nToken Similarity phrase 0.817 wrong 0.8146 mean 0.7972 constitutes 0.7771 call 0.7621 happens 0.752 the 0.7477 God 0.7425 nickname 0.7368 spelled 0.7162 name 0.712 happened 0.6889 as 0.6699 defines 0.647"}, {"heading": "5.2.2 Polysemy", "text": "Each token of the same word, e.g., who, generates its own symbol/role TPR in TPRNand if our hypothesis is correct, tokens with different meaning should select different symbols. Indeed we see three general patterns of symbol selection for who. Who is the producer of Dr. Who? illustrates the main-question-word meaning and the proper-name meaning, respectively, in its two uses of who. Third is the relative pronoun meaning, illustrated by . . . the actor who . . .. The three symbol-selection patterns associated with these three meanings are shown in Table 6.\nWe can interpret the symbols with IDs 25, 52 and 98 as corresponding, respectively, to the meanings of a relative pronoun, a main question word, and a proper noun. The tokens with boldface counts are then correct, while the other counts are errors. Of interest are the further facts that all 18 of the non-sentence-initial main-question-word tokens are correctly identified as such (assigned symbol 52) and that, of the 27 cases of proper-noun-whos mislabeled with the main-question symbol 52, half are assigned role #1, placing them in the wh-restrictor \u2018phrase\u2019 (whereas only one of the 126 correctly-identified proper-noun-whos is). The Symbol-97-meaning of who is at this point unclear."}, {"heading": "5.2.3 Predicting output errors from internal mis-representation", "text": "In processing the test query What type/genre of TV show is Doctor Who? [7632] the model assigns symbol 52 to Who, which we have interpreted as an error since symbol 52 is assigned to every one of the 1062 occurrences of Who as a query-initial main question-word. Although the model strongly tends to give responses of the correct category, here it replies Time Lord, an appropriate type of answer to a true who question but not to the actual question. The model makes 4 errors of this type, of the 9 errors total made when assigning symbol 25; this 44% rate contrasts with the 9% rate when it correctly assigns the \u2018proper-noun symbol\u2019 98 to Who. Although such error analysis is in its infancy with TPRNmodels, it is already beginning to reveal its potential to make it possible, we believe for the first time, to attribute overall output errors of a DNN to specific errors of internal representation."}, {"heading": "6 Related work", "text": "Architecture. In recent years, a number of DNNs have achieved notable success by reintroducing elements of symbolic computation as peripheral modules. This includes, e.g.: (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19]. The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22]. The continuity of these parameters of course is crucial to enabling the overall system to be learnable by gradient-based optimization.\nThe present work constitutes a different approach to reintroducing approximately symbolic representations and rule-based processing into neural network computation over continuous distributed representations. In computation with TPRs, the symbols and rules are internal to the DNN; there is no separation between a central network controller and peripheral quasidiscrete modules. Items in memories are distributed representations that are combined by addition/superposition rather than by being slotted into external discrete locations. Computation over TPRs is massively parallel [5].\nInterpretation. Most methods of interpreting the internal representations of DNNs do so through the input and output representations of DNNs which are by necessity interpretable: these are where the DNN must interface with our description of its problem domain. An internal neuron may be interpreted by looking at the (interpretable) input patterns that activate it, or the (interpretable) output patterns that it activates (e.g., [23]).\nThe method pursued in this paper, by contrast, interprets internal DNN states not via I \u2192 O behavior but via an abstract theory of the system\u2019s problem domain. In the case of a language processing problem, such theories are provided by theoretical linguistics and traditional, symbolic computational linguistics. The elements we have interpreted are TPR roles, and TPR fillers, which are distributed activation vectors incorporated into network representations via the summation of their tensor products; we have designed an architecture in which individual neurons localize the presence of such roles and fillers (aR(t) and aS(t)). Our interpretation rests on the interrelations between activations of the roles and fillers selected to encode words-in-context with the lexical-semantic and grammatical properties attributed to those words-in-context by linguistic theories."}, {"heading": "7 Conclusion", "text": "We introduce a modification of the BiDAF architecture for question-answering with the SQuAD database. This new model, TPRN , uses Tensor Product Representations in recurrent networks to encode input words. Through end-to-end learning the model learns how to deploy 100 symbols into 20 structural roles; the symbols and roles have no meaning prior to learning. We hypothesized that the symbols would acquire lexical meanings and the roles grammatical meanings. We interpret the learned symbols and roles by observing which of them the trained model selects for encoding individual words in context. We observe that the words assigned to a given symbol tend to be semantically related, and the words assigned to a given role correlate with abstract notions of grammatical roles from linguistic theory. Thus the TPRNmodel illustrates how learning to perform a natural language questionanswering task can lead a deep learning system to create representations that are interpretable as encoding abstract grammatical concepts without ever being exposed to data labelled with anything like grammatical structure. This is the type of setting in which children learn their first language, so the work lends plausibility to the hypothesis that abstract notions of linguistic theory do in fact describe representations in speakers\u2019 minds\u2014representations\nthat are learned in the service of performing tasks that do not explicitly necessitate any such structure."}, {"heading": "Acknowledgments", "text": "The work reported here builds on work supported by NSF INSPIRE grant BCS-1344269 to PS and conducted at Johns Hopkins University and Northwestern University; we gratefully acknowledge the support of NSF, the JHU Department of Cognitive Science, and the co-investigators Matthew Goldrick, Geraldine Legendre, Akira Omaki, Kyle Rawlins, Ben Van Durme, and Colin Wilson. We thank collaborator Paul Tupper for suggesting the form of the Q function used here."}, {"heading": "8 Supplementary Materials", "text": "In this section we present more examples that support the lexical-semantic coherence of the words assigned to symbols described in the section 5.2.1.\n8.1 Symbol 29: \u201cmonth of the year\u201d symbol Symbol 29 is attracted to the months of the year. By counting the total number of times a month of the year token has appeared in the validation set, and then counting how many of them are attracted to symbol 29, we will get results in Table 7. Top 30 tokens attracted to this filler are presented in Table 8 (sorted based on cosine similarity, duplicate tokens removed). As observed many of them show a year token."}, {"heading": "8.2 Symbol 26: \u201cwhat\u201d symbol", "text": "100% of \u201cwhat\u201d and \u201cWhat\u201d tokens are attracted to symbol 26."}, {"heading": "8.3 Symbol 20: \u201cdirectional / causal\u201d symbol", "text": "75.8% of \u201cto\u201d tokens and 81.7% of \u201cfrom\u201d tokens are attracted to symbol 20."}, {"heading": "8.4 Symbol 55: \u201cfinance / property\u201d symbol", "text": "Most of the tokens attracted to symbol 55 are finance or property related tokens. Table 9 shows the full list of tokens attracted to symbol 55."}, {"heading": "8.5 Symbol 43: \u201chow\u201d symbol", "text": "100% of \u201cHow\u201d tokens and 62.6% of \u201chow\u201d tokens are attracted to symbol 43."}, {"heading": "8.6 Symbol 22: \u201claw\u201d symbol", "text": "Most of the tokens attracted to symbol 22 are law related tokens. Table 10 shows the full list of tokens attracted to symbol 22."}, {"heading": "8.7 Symbol 44: \u201cterritory\u201d symbol", "text": "Most of the tokens attracted to symbol 44 are territory related tokens. Table 11 shows the full list of tokens attracted to symbol 44."}, {"heading": "8.8 Symbol 61: \u201cyear\u201d symbol", "text": "Most of the tokens attracted to symbol 61 are different year number tokens. Table 12 shows the full list of tokens attracted to symbol 61."}, {"heading": "8.9 Symbol 36: \u201ca / an / about\u201d symbol", "text": "70.8% of \u201ca\u201d tokens, 69.7% of \u201can\u201d tokens and 87% of \u201cabout\u201d tokens are attracted to symbol 36."}, {"heading": "8.10 Symbol 30: \u201c?\u201d symbol", "text": "74% of \u201c?\u201d tokens are attracted to symbol 30."}], "references": [{"title": "Bidirectional attention flow for machine comprehension", "author": ["M.J. Seo", "A. Kembhavi", "A. Farhadi", "H. Hajishirzi"], "venue": "5th International Conference for Learning Representations, San Juan, Puerto Rico, 2016. [Online]. Available: https://arxiv.org/abs/1611.01603", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Featurerich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C. Manning", "Y. Singer"], "venue": "Proceedings of HLT-NAACL 2003, 2003, pp. 252\u2013259. [Online]. Available: https://nlp.stanford.edu/software/tagger.shtml", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "SQuAD: 100,000+ questions for machine comprehension of text", "author": ["P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP, Austin, Texas, USA, 2016. [Online]. Available: http://arxiv.org/abs/1606.05250", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Tensor product variable binding and the representation of symbolic structures in connectionist networks", "author": ["P. Smolensky"], "venue": "Artificial Intelligence, vol. 46, pp. 159\u2013216, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar", "author": ["P. Smolensky", "G. Legendre"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Optimization and quantization in gradient symbol systems: A framework for integrating the 16  continuous and the discrete in cognition", "author": ["P. Smolensky", "M. Goldrick", "D. Mathis"], "venue": "Cognitive Science, vol. 38, pp. 1102\u20131138, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar, October 2014, pp. 1532\u20131543.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 4, pp. 694\u2013707, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Symbolic functions from neural computation", "author": ["P. Smolensky"], "venue": "Philosophical Transactions of the Royal Society \u2014 A: Mathematical, Physical and Engineering Sciences, vol. 370, pp. 3543\u20133569, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Gated selfmatching networks for reading comprehension and question answering", "author": ["W. Wang", "N. Yang", "F. Wei", "B. Chang", "M. Zhou"], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, 2017.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "2012. [Online]. Available: http://arxiv.org/abs/1212.5701", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010, pp. 2528\u20132535. 17", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tracking the world state with recurrent entity networks", "author": ["M. Henaff", "J. Weston", "A. Szlam", "A. Bordes", "Y. LeCun"], "venue": "6th International Conference for Learning Representations, Toulon, France, 2017.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2017}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "Advances in neural information processing systems, 2015, pp. 2440\u20132448.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["A. Neelakantan", "Q.V. Le", "I. Sutskever"], "venue": "5th International Conference for Learning Representations, San Juan, Puerto Rico, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention-based models for speech recognition", "author": ["J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "International Conference on Machine Learning, 2015, pp. 2048\u20132057.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid computing using a neural network with dynamic external memory", "author": ["A. Graves", "G. Wayne", "M. Reynolds", "T. Harley", "I. Danihelka", "A. Grabska- Barwi\u0144ska", "S.G. Colmenarejo", "E. Grefenstette", "T. Ramalho", "J. Agapiou"], "venue": "Nature, vol. 538, no. 7626, pp. 471\u2013476, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "This interpretability comes at a cost of only a few percentage-point reduction in accuracy relative to the original model on which the new one is based (BiDAF [1]).", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "Through extremely detailed, fine-grained analysis, we find specific correspondences between the learned roles and parts of speech as assigned by a standard parser [2], and find several discrepancies in the model\u2019s favor.", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "Specifically, the task we address is question answering for the SQuAD dataset [3], in which a text passage and a question are presented as input, and the model\u2019s output identifies a stretch within the passage that contains the answer to the question (see Sec.", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6].", "startOffset": 346, "endOffset": 355}, {"referenceID": 4, "context": "The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6].", "startOffset": 346, "endOffset": 355}, {"referenceID": 5, "context": "The capacity for distributed representations of structure is provided by Tensor Product Representations, TPRs, in which a discrete symbol structure is encoded as a vector systematically built\u2014through vector addition and the tensor product\u2014from vectors encoding symbols and vectors encoding the roles each symbol plays in the structure as a whole [4, 5, 6].", "startOffset": 346, "endOffset": 355}, {"referenceID": 0, "context": "The new model proposed here is built from the BiDAF model proposed in [1] for question answering.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "It replaces a bidirectional RNN built from LSTM units [7] with one built from TPR units; the architecture is called the Tensor Product Recurrent Network, TPRN.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "The proposed TPRNarchitecture is built in TensorFlow [8] on the BiDAF model proposed in [1].", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "The proposed TPRNarchitecture is built in TensorFlow [8] on the BiDAF model proposed in [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 8, "context": "BiDAF is constructed from 6 layers: a character embedding layer using CNNs, a word embedding layer using GLOVE [9] vectors, a phrase embedding layer using bidirectional LSTMs for sentence embedding [10], an attention flow layer using a special attention mechanism, a modeling layer using LSTMs, and an output layer that generates pointers to the start and end of an answer in the paragraph.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "BiDAF is constructed from 6 layers: a character embedding layer using CNNs, a word embedding layer using GLOVE [9] vectors, a phrase embedding layer using bidirectional LSTMs for sentence embedding [10], an attention flow layer using a special attention mechanism, a modeling layer using LSTMs, and an output layer that generates pointers to the start and end of an answer in the paragraph.", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "1 of [1].", "startOffset": 5, "endOffset": 8}, {"referenceID": 4, "context": "The present TPRNmodel incorporates TPR to only a modest degree, but it is a proof-of-concept system that paves the way for future models that can import the power of general symbol-structure processing, proven to be within the scope of full-blown TPR architectures [5, 11].", "startOffset": 265, "endOffset": 272}, {"referenceID": 10, "context": "The present TPRNmodel incorporates TPR to only a modest degree, but it is a proof-of-concept system that paves the way for future models that can import the power of general symbol-structure processing, proven to be within the scope of full-blown TPR architectures [5, 11].", "startOffset": 265, "endOffset": 272}, {"referenceID": 2, "context": "In this section, we describe details of the experiments for the proposed TPRNmodel on question answering task using the Stanford\u2019s SQuAD dataset [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 11, "context": "1 Please note that the goal of this work is not to beat the state-of-the-art system on SQuAD (at the time of writing this paper, r-net, [12]), but to create a question answering system that is interpretable, by exploiting TPR.", "startOffset": 136, "endOffset": 140}, {"referenceID": 2, "context": "It consists of more than 500 Wikipedia articles and more than 100,000 questionanswer pairs about them, which is significantly larger than previous reading comprehension datasets [3].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "For the experiments, we used the same settings reported in [1] for all layers of TPRNexcept the phrase embedding layer.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "\u2022 The concatenation of word embedding using GLOVE [9] and character embedding using Convolutional Neural Networks (CNNs) was used to represent each word.", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "\u2022 The optimizer used was AdaDelta [13] with 12 epochs.", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "Performance results of our single model compared with a single model in [1] are presented in Table 1.", "startOffset": 72, "endOffset": 75}, {"referenceID": 0, "context": "From this table we observe that our proposed TPR based model underperforms [1] by about 2 points.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "2 mentions a few selected, highly targeted performance comparisons against the POS tagging provided by the Stanford parser [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "Table 1: Performance of the proposed TPRNmodel compared to BiDAF proposed in [1]", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "86 BiDAF [1] 68.", "startOffset": 9, "endOffset": 12}, {"referenceID": 13, "context": ", [14]), it is reasonable to hypothesize that good learned neural network models in language will typically learn low-level input representations that are generally similar to one another.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 109, "endOffset": 121}, {"referenceID": 15, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 109, "endOffset": 121}, {"referenceID": 16, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 109, "endOffset": 121}, {"referenceID": 17, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 275, "endOffset": 283}, {"referenceID": 18, "context": ": (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector [15, 16, 17]; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations [18, 19].", "startOffset": 275, "endOffset": 283}, {"referenceID": 19, "context": "The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22].", "startOffset": 274, "endOffset": 282}, {"referenceID": 20, "context": "The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22].", "startOffset": 274, "endOffset": 282}, {"referenceID": 21, "context": "The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank (\u2018attention\u2019 [20, 21]); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program [22].", "startOffset": 407, "endOffset": 411}, {"referenceID": 4, "context": "Computation over TPRs is massively parallel [5].", "startOffset": 44, "endOffset": 47}], "year": 2017, "abstractText": "We introduce an architecture in which internal representations\u2014 learned by end-to-end optimization in a deep neural network performing a textual question-answering task\u2014can be interpreted using basic concepts from linguistic theory. This interpretability comes at a cost of only a few percentage-point reduction in accuracy relative to the original model on which the new one is based (BiDAF [1]). The internal representation that is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Through extremely detailed, fine-grained analysis, we find specific correspondences between the learned roles and parts of speech as assigned by a standard parser [2], and find several discrepancies in the model\u2019s favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means \u2217This work was carried out while PS was on leave from Johns Hopkins University. LD is currently at Citadel. 1 ar X iv :1 70 5. 08 43 2v 1 [ cs .C L ] 2 3 M ay 2 01 7 to represent using symbols and roles and an inductive bias favoring use of these in an approximately discrete manner.", "creator": "LaTeX with hyperref package"}}}