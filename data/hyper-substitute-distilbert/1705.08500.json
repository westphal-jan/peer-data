{"id": "1705.08500", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-May-2017", "title": "Selective Classification for Deep Neural Networks", "abstract": "selective classification techniques ( also recognised as network option ) have first yet been realised in the phase in numerical neural design ( chat ). both techniques can potentially significantly improve hypothesis prediction performance following closing - off sequences. understanding this paper programs seldom use method to construct a selective classifier given a trained communication network. our method allows your parameter to achieve a desired observation cycle. at idle time, dynamic classifier registers instances as needed, to grant the desired risk ( with high probability ). empirical results connect computers and sage and demonstrate the effectiveness of our options, theoretically opens numerous possibilities to operate comfortably across mission - critical applications. prime example, using pv systems requires incredible 2 % error in top - 5 automated classification measures turn detected using probability 99. 25 %, after almost 60 % test coverage.", "histories": [["v1", "Tue, 23 May 2017 19:43:56 GMT  (198kb,D)", "https://arxiv.org/abs/1705.08500v1", null], ["v2", "Thu, 1 Jun 2017 14:10:34 GMT  (198kb,D)", "http://arxiv.org/abs/1705.08500v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["yonatan geifman", "ran el-yaniv"], "accepted": true, "id": "1705.08500"}, "pdf": {"name": "1705.08500.pdf", "metadata": {"source": "CRF", "title": "Selective Classification for Deep Neural Networks", "authors": ["Yonatan Geifman"], "emails": ["yonatan.g@cs.technion.ac.il", "rani@cs.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "While self-awareness remains an illusive, hard to define concept, a rudimentary kind of selfawareness, which is much easier to grasp, is the ability to know what you don\u2019t know, which can make you smarter. The subfield dealing with such capabilities in machine learning is called selective prediction (also known as prediction with a reject option), which has been around for 60 years [1, 5]. The main motivation for selective prediction is to reduce the error rate by abstaining from prediction when in doubt, while keeping coverage as high as possible. An ultimate manifestation of selective prediction is a classifier equipped with a \u201cdial\u201d that allows for precise control of the desired true error rate (which should be guaranteed with high probability), while keeping the coverage of the classifier as high as possible.\nMany present and future tasks performed by (deep) predictive models can be dramatically enhanced by high quality selective prediction. Consider, for example, autonomous driving. Since we cannot rely on the advent of \u201csingularity\u201d, where AI is superhuman, we must manage with standard machine learning, which sometimes errs. But what if our deep autonomous driving network were capable of knowing that it doesn\u2019t know how to respond in a certain situation,\nar X\niv :1\n70 5.\n08 50\n0v 2\n[ cs\n.L G\ndisengaging itself in advance and alerting the human driver (hopefully not sleeping at that time) to take over? There are plenty of other mission-critical applications that would likewise greatly benefit from effective selective prediction.\nThe literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3]. The reject option has rarely been discussed in the context of neural networks (NNs), and so far has not been considered for deep NNs (DNNs). Existing NN works consider a cost-based rejection model [2, 4], whereby the costs of misclassification and abstaining must be specified, and a rejection mechanism is optimized for these costs. The proposed mechanism for classification is based on applying a carefully selected threshold on the maximal neuronal response of the softmax layer. We that call this mechanism softmax response (SR). The cost model can be very useful when we can quantify the involved costs, but in many applications of interest meaningful costs are hard to reason. (Imagine trying to set up appropriate rejection/misclassification costs for disengaging an autopilot driving system.) Here we consider the alternative risk-coverage view for selective classification discussed in [5].\nEnsemble techniques have been considered for selective (and confidence-rated) prediction, where rejection mechanisms are typically based on the ensemble statistics [18, 7]. However, such techniques are presently hard to realize in the context of DNNs, for which it could be very costly to train sufficiently many ensemble members. Recently, Gal and Ghahramani [9] proposed an ensemble-like method for measuring uncertainty in DNNs, which bypasses the need to train several ensemble members. Their method works via sampling multiple dropout applications of the forward pass to perturb the network prediction randomly. While this Monte-Carlo dropout (MC-dropout) technique was not mentioned in the context of selective prediction, it can be directly applied as a viable selective prediction method using a threshold, as we discuss here.\nIn this paper we consider classification tasks, and our goal is to learn a selective classifier (f, g), where f is a standard classifier and g is a rejection function. The selective classifier has to allow full guaranteed control over the true risk. The ideal method should be able to classify samples in production with any desired level of risk with the optimal coverage rate. It is reasonable to assume that this optimal performance can only be obtained if the pair (f, g) is trained together. As a first step, however, we consider a simpler setting where a (deep) neural classifier f is already given, and our goal is to learn a rejection function g that will guarantee with high probability a desired error rate. To this end, we consider the above two known techniques for rejection (SR and MC-dropout), and devise a learning method that chooses an appropriate threshold that ensures the desired risk. For a given classifier f , confidence level \u03b4, and desired risk r\u2217, our method outputs a selective classifier (f, g) whose test error will be no larger than r\u2217 with probability of at least 1\u2212 \u03b4.\nUsing the well-known VGG-16 architecture, we apply our method on CIFAR-10, CIFAR-100 and ImageNet (on ImageNet we also apply the RESNET-50 architecture). We show that both SR and dropout lead to extremely effective selective classification. On both the CIFAR datasets, these two mechanisms achieve nearly identical results. However, on ImageNet, the simpler SR mechanism is significantly superior. More importantly, we show that almost any desirable risk level can be guaranteed with a surprisingly high coverage. For example, an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage."}, {"heading": "2 Problem Setting", "text": "We consider a standard multi-class classification problem. Let X be some feature space (e.g., raw image data) and Y , a finite label set, Y = {1, 2, 3, . . . , k}, representing k classes. Let P (X,Y ) be a distribution over X \u00d7Y . A classifier f is a function f : X \u2192 Y , and the true risk of f w.r.t. P is R(f |P ) =\u2206 EP (X,Y )[`(f(x), y)], where ` : Y \u00d7 Y \u2192 R+ is a given loss function, for example the 0/1 error. Given a labeled set Sm = {(xi, yi)}mi=1 \u2286 (X \u00d7 Y) sampled i.i.d. from P (X,Y ), the empirical risk of the classifier f is r\u0302(f |Sm) =\u2206 1m \u2211m i=1 `(f(xi), yi).\nA selective classifier [5] is a pair (f, g), where f is a classifier, and g : X \u2192 {0, 1} is a selection function, which serves as a binary qualifier for f as follows,\n(f, g)(x) =\u2206 { f(x), if g(x) = 1; don\u2019t know, if g(x) = 0.\nThus, the selective classifier abstains from prediction at a point x iff g(x) = 0. The performance of a selective classifier is quantified using coverage and risk. Fixing P , coverage, defined to be \u03c6(f, g) =\u2206 EP [g(x)], is the probability mass of the non-rejected region in X . The selective risk of f, g) is\nR(f, g) =\u2206 EP [`(f(x), y)g(x)]\n\u03c6(f, g) . (1)\nClearly, the risk of a selective classifier can be traded-off for coverage. The entire performance profile of such a classifier can be specified by its risk-coverage curve, defined to be risk as a function of coverage [5].\nConsider the following problem. We are given a classifier f , a training sample Sm, a confidence parameter \u03b4 > 0, and a desired risk target r\u2217 > 0. Our goal is to use Sm to create a selection function g such that the selective risk of (f, g) satisfies\nPrSm {R(f, g) > r\u2217} < \u03b4, (2)\nwhere the probability is over training samples, Sm, sampled i.i.d. from the unknown underlying distribution P . Among all classifiers satisfying (2), the best ones are those that maximize the coverage."}, {"heading": "3 Selection with Guaranteed Risk Control", "text": "In this section, we present a general technique for constructing a selection function with guaranteed performance, based on a given classifier f , and a confidence-rate function \u03baf : X \u2192 R+ for f . We do not assume anything on \u03baf , and the interpretation is that \u03ba can rank in the sense that if \u03baf (x1) \u2265 \u03baf (x2), for x1, x2 \u2208 X , the confidence function \u03baf indicates that the confidence in the prediction f(x2) is not higher than the confidence in the prediction f(x1). In this section we are not concerned with the question of what is a good \u03baf (which is discussed in Section 4); our goal is to generate a selection function g, with guaranteed performance for a given \u03baf .\nFor the reminder of this paper, the loss function ` is taken to be the standard 0/1 loss function (unless explicitly mentioned otherwise). Let Sm = {(xi, yi)}mi=1 \u2286 (X \u00d7 Y)m be a training set, assumed to be sampled i.i.d. from an unknown distribution P (X,Y ). Given also are a confidence\nparameter \u03b4 > 0, and a desired risk target r\u2217 > 0. Based on Sm, our goal is to learn a selection function g such that the selective risk of the classifier (f, g) satisfies (2).\nFor \u03b8 > 0, we define the selection function g\u03b8 : X \u2192 {0, 1} as\ng\u03b8(x) = g\u03b8(x|\u03baf ) =\u2206 {\n1, if \u03baf (x) \u2265 \u03b8; 0, otherwise. (3)\nFor any selective classifier (f, g), we define its empirical selective risk with respect to the labeled sample Sm,\nr\u0302(f, g|Sm) =\u2206 1 m\n\u2211m i=1 `(f(xi), yi)g(xi)\n\u03c6\u0302(f, g|Sm) ,\nwhere \u03c6\u0302 is the empirical coverage, \u03c6\u0302(f, g|Sm) =\u2206 1m \u2211m i=1 g(xi). For any selection function g, denote by g(Sm) the g-projection of Sm, g(Sm) =\u2206 {(x, y) \u2208 Sm : g(x) = 1}. The selection with guaranteed risk (SGR) learning algorithm appears in Algorithm 1. The algorithm receives as input a classifier f , a confidence-rate function \u03baf , a confidence parameter \u03b4 > 0, a target risk r\u2217, and a training set Sm. The algorithm performs a binary search to find the optimal bound guaranteeing the required risk with sufficient confidence. The SGR algorithm outputs a selective classifier (f, g) and a risk bound b\u2217. In the rest of this section we analyze the SGR algorithm. We make use of the following lemma, which gives the tightest possible numerical generalization bound for a single classifier, based on a test over a labeled sample.\nAlgorithm 1 Selection with Guaranteed Risk (SGR) 1: SGR(f ,\u03baf ,\u03b4,r\u2217,Sm) 2: Sort Sm according to \u03baf (xi), xi \u2208 Sm (and now assume w.l.o.g. that indices reflect this\nordering). 3: zmin = 1; zmax = m 4: for i = 1 to k =\u2206 dlog2me do 5: z = d(zmin + zmax)/2e 6: \u03b8 = \u03baf (xz) 7: gi = g\u03b8 {(see (3))} 8: r\u0302i = r\u0302(f, gi|Sm) 9: b\u2217i = B\n\u2217(r\u0302i, \u03b4/dlog2me, gi(Sm)) {see Lemma 3.1 } 10: if b\u2217i < r\u2217 then 11: zmax = z 12: else 13: zmin = z 14: end if 15: end for 16: Output- (f, gk) and the bound b\u2217k.\nLemma 3.1 (Gascuel and Caraux, 1992, [10]) Let P be any distribution and consider a classifier f whose true error w.r.t. P is R(f |P ). Let 0 < \u03b4 < 1 be given and let r\u0302(f |Sm) be the empirical error of f w.r.t. to the labeled set Sm, sampled i.i.d. from P . Let B\u2217(r\u0302i, \u03b4, Sm) be the\nsolution b of the following equation,\nm\u00b7r\u0302(f |Sm)\u2211 j=0 ( m j ) bj(1\u2212 b)m\u2212j = \u03b4. (4)\nThen, PrSm{R(f |P ) > B\u2217(r\u0302i, \u03b4, Sm)} < \u03b4.\nWe emphasize that the numerical bound of Lemma 3.1 is the tightest possible in this setting. As discussed in [10], the analytic bounds derived using, e.g., Hoeffding inequality (or other concentration inequalities), approximate this numerical bound and incur some slack.\nFor any selection function, g, let Pg(X,Y ) be the projection of P over g; that is, Pg(X,Y ) =\u2206 P (X,Y |g(X) = 1). The following theorem is a uniform convergence result for the SGR procedure.\nTheorem 3.2 (SGR) Let Sm be a given labeled set, sampled i.i.d. from P , and consider an application of the SGR procedure. For k =\u2206 dlog2me, let (f, gi) and b\u2217i , i = 1, . . . , k, be the selective classifier and bound computed by SGR in its ith iterations. Then,\nPrSm {\u2203i : R(f |Pgi) > B\u2217(r\u0302i, \u03b4/k, gi(Sm))} < \u03b4.\nProof Sketch: For any i = 1, . . . , k, let mi = |gi(Sm)| be the random variable giving the size of accepted examples from Sm on the ith iteration of SGR. For any fixed value of 0 \u2264 mi \u2264 m, by Lemma 3.1, applied with the projected distribution Pgi(X,Y ), and a sample Smi , consisting of mi examples drawn from the product distribution (Pgi)mi ,\nPrSmi\u223c(Pgi )mi {R(f |Pgi) > B \u2217(r\u0302i, \u03b4/k, gi(Sm))} < \u03b4/k. (5)\nThe sampling distribution of mi labeled examples in SGR is determined by the following process: sample a set Sm of m examples from the product distribution Pm and then use gi to filter Sm, resulting in a (randon) number mi of examples. Therefore, the left-hand side of (5) equals\nPrSm\u223cPm {R(f |Pgi) > B\u2217(r\u0302i, \u03b4/k, gi(Sm)) |gi(Sm) = mi} .\nClearly,\nR(f |Pgi) = EPgi [`(f(x), y)] = EP [`(f(x), y)g(x)]\n\u03c6(f, g) = R(f, gi).\nTherefore,\nPrSm{R(f, gi) > B\u2217(r\u0302i, \u03b4/k, gi(Sm))}\n= m\u2211 n=0 PrSm{R(f, gi) > B\u2217(r\u0302i, \u03b4/k, gi(Sm)) | gi(Sm) = n} \u00b7Pr{gi(Sm) = n}\n\u2264 \u03b4 k m\u2211 n=0 Pr{gi(Sm) = n} = \u03b4 k .\nAn application of the union bound completes the proof."}, {"heading": "4 Confidence-Rate Functions for Neural Networks", "text": "Consider a classifier f , assumed to be trained for some unknown distribution P . In this section we consider two confidence-rate functions, \u03baf , based on previous work [9, 2]. We note that an ideal confidence-rate function \u03baf (x) for f , should reflect true loss monotonicity. Given (x1, y1) \u223c P and (x2, y2) \u223c P , we would like the following to hold: \u03baf (x1) \u2264 \u03baf (x2) if and only if `(f(x1), y1) \u2265 `(f(x2), y2). Obviously, one cannot expect to have an ideal \u03baf . Given a confidence-rate functions \u03baf , a useful way to analyze its effectiveness is to draw the risk-coverage curve of its induced rejection function, g\u03b8(x|\u03baf ), as defined in (3). This risk-coverage curve shows the relationship between \u03b8 and R(f, g\u03b8). For example, see Figure 2(a) where a two (nearly identical) risk-coverage curves are plotted. While the confidence-rate functions we consider are not ideal, they will be shown empirically to be extremely effective. 1\nThe first confidence-rate function we consider has been around in the NN folklore for years, and is explicitly mentioned by [2, 4] in the context of reject option. This function works as follows: given any neural network classifier f(x) where the last layer is a softmax, we denote by f(x|j) the soft response output for the jth class. The confidence-rate function is defined as \u03ba =\u2206 maxj\u2208Y(f(x|j)). We call this function softmax response (SR).\nSoftmax responses are often treated as probabilities (responses are positive and sum to 1), but some authors criticize this approach [9]. Noting that, for our purposes, the ideal confidence-rate function should only provide coherent ranking rather than absolute probability values, softmax responses are potentially good candidates for relative confidence rates.\nWe are not familiar with a rigorous explanation for SR, but it can be intuitively motivated by observing neuron activations. For example, Figure 1 depicts average response values of every neuron in the second-to-last layer for true positives and false positives for the class \u20188\u2019 in the MNIST dataset (and qualitatively similar behavior occurs in all MNIST classes). The x-axis corresponds to neuron indices in that layer (1-128); and the y-axis shows the average responses, where green squares are averages of true positives, boldface squares highlight strong responses, and red circles correspond to the average response of false positives. It is evident that the true positive activation response in the active neurons is much higher than the false positive, which is expected to be reflected in the final softmax layer response. Moreover, it can be seen that the large activation values are spread over many neurons, indicating that the confidence signal arises due to numerous patterns detected by neurons in this layer. Qualitatively similar behavior can be observed in deeper layers.\nThe MC-dropout technique we consider was recently proposed to quantify uncertainty in neural networks [9]. To estimate uncertainty for a given instance x, we run a number of feedforward iterations over x, each applied with dropout in the last fully connected layer. Uncertainty is taken as the variance in the responses of the neuron corresponding to the most probable class. We consider minus uncertainty as the MC-dropout confidence rate."}, {"heading": "5 Empirical Results", "text": "In Section 4 we introduced the SR and MC-dropout confidence-rate function, defined for a given model f . We trained VGG models [17] for CIFAR-10, CIFAR-100 and ImageNet. For each\n1While Theorem 3.2 always holds, we note that if \u03baf is severely skewed (far from ideal), the bound of the resulting selective classifier can be far from the target risk.\nof these models f , we considered both the SR and MC-dropout confidence-rate functions, \u03baf , and the induced rejection function, g\u03b8(x|\u03baf ). In Figure 2 we present the risk-coverage curves obtained for each of the three datasets. These curves were obtained by computing a validation risk and coverage for many \u03b8 values. It is evident that the risk-coverage profile for SR and MC-dropout is nearly identical for both the CIFAR datasets. For the ImageNet set we plot the curves corresponding to top-1 (dashed curves) and top-5 tasks (solid curves). On this dataset, we see that SR is significantly better than MC-dropout on both tasks. For example, in the top-1 task and 60% coverage, the SR rejection has 10% error while MC-dropout rejection incurs more than 20% error. But most importantly, these risk-coverage curves show that selective classification can potentially be used to dramatically reduce the error in the three datasets. Due to the relative advantage of SR, in the rest of our experiments we only focus on the SR rating.\nWe now report on experiments with our SGR routine, and apply it on each of the datasets to construct high probability risk-controlled selective classifiers for the three datasets."}, {"heading": "5.1 Selective Guaranteed Risk for CIFAR-10", "text": "We now consider CIFAR-10; see [14] for details. We used the VGG-16 architecture [17] and adapted it to the CIFAR-10 dataset by adding massive dropout, exactly as described in [15]. We used data augmentation containing horizontal flips, vertical and horizontal shifts, and rotations, and trained using SGD with momentum of 0.9, initial learning rate of 0.1, and weight decay of 0.0005. We multiplicatively dropped the learning rate by 0.5 every 25 epochs, and trained for 250 epochs. With this setting we reached validation accuracy of 93.54, and used the resulting network f10 as the basis for our selective classifier.\nWe applied the SGR algorithm on f10 with the SR confidence-rating function, where the training set for SGR, Sm, was taken as half of the standard CIFAR-10 validation set that was randomly split to two equal parts. The other half, which was not consumed by SGR for training, was reserved for testing the resulting bounds. Thus, this training and test sets where each of approximately 5000 samples. We applied the SGR routine with several desired risk values, r\u2217, and obtained, for each such r\u2217, corresponding selective classifier and risk bound b\u2217. All our applications of the SGR routine (for this dataset and the rest) where with a particularly small confidence level \u03b4 = 0.001.2 We then applied these selective classifiers on the reserved test set, and computed, for each selective classifier, test risk and test coverage. The results are summarized in Table 1, where we also include train risk and train coverage that were computed, for each selective classifier, over the training set.\nObserving the results in Table 1, we see that the risk bound, b\u2217, is always very close to the the target risk, r\u2217. Moreover, the test risk is always bounded above by the bound b\u2217, as required. Finally, we see that it is possible to guarantee with this method amazingly small 1% error while covering more than 78% of the domain."}, {"heading": "5.2 Selective Guaranteed Risk for CIFAR-100", "text": "Using the same VGG architechture (now adapted to 100 classes) we trained a model for CIFAR-100 while applying the same data augmentation routine as in the CIFAR-10 experiment. Following precisly the same experimental design as in the CFAR-10 case, we obtained the results of Table 2\nHere again, SGR generated tight bounds, very close to the desired target risk, and the bounds were never violated by the true risk. Also, we see again that it is possible to dramatically reduce the risk with only moderate compromise of the coverage. While the architecture we used is not\n2With this small \u03b4, and small number of reported experiments (6-7 lines in each table) we did not perform a Bonferroni correction (which can be easily added).\nstate-of-the art, with a coverage of 67%, we easily surpassed the best known result for CIFAR-100, which currently stands on 18.85% using the wide residual network architecture [19]. It is very likely that by using ourselves the wide residual network architecture we could obtain significantly better results."}, {"heading": "5.3 Selective Guaranteed Risk for ImageNet", "text": "We used an already trained Image-Net VGG-16 model based on ILSVRC2014 [16]. We repeated the same experimental design but now the sizes of the training and test set were approximately 25,000. The SGR results for both the top-1 and top-5 classification tasks are summarized in Tables 3 and 4, respectively. We also implemented the RESNET-50 architecture [12] in order to see if qualitatively similar results can be obtained with a different architecture. The RESNET-50 results for ImageNet top-1 and top-5 classification tasks are summarized in Tables 5 and 6, respectively.\nThese results show that even for the challenging mageNet, with both the VGG and RESNET architectures, our selective classifiers are extremely effective, and with appropriate coverage compromise, our classifier easily surpasses the best known results for ImageNet. Not surprisingly, RESNET, which is known to achieve better results than VGG on this set, preserves its relative advantage relative to VGG through all r\u2217 values."}, {"heading": "6 Concluding Remarks", "text": "We presented an algorithm for learning a selective classifier whose risk can be fully controlled and guaranteed with high confidence. Our empirical study validated this algorithm on challenging image classification datasets, and showed that guaranteed risk-control is achievable. Our methods can be immediately used by deep learning practitioners, helping them in coping with missioncritical tasks.\nWe believe that our work is only the first significant step in this direction, and many research questions are left open. The starting point in our approach is a trained neural classifier f (supposedly trained to optimize risk under full coverage). While the rejection mechanisms we\nconsidered were extremely effective, it might be possible to identify superior mechanisms for a given classifier f . We believe, however, that the most challenging open question would be to simultaneously train both the classifier f and the selection function g to optimize coverage for a given risk level. Selective classification is intimately related to active learning in the context of linear classifiers [6, 11]. It would be very interesting to explore this potential relationship in the context of (deep) neural classification. In this paper we only studied selective classification under the 0/1 loss. It would be of great importance to extend our techniques to other loss functions and specifically to regression, and to fully control false-positive and false-negative rates.\nThis work has many applications. In general, any classification task where a controlled risk is critical would benefit by using our methods. An obvious example is that of medical applications where utmost precision is required and rejections should be handled by human experts. In such applications the existence of performance guarantees, as we propose here, is essential. Financial investment applications are also obvious, where there are great many opportunities from which one should cherry-pick the most certain ones. A more futuristic application is that of robotic sales representatives, where it could extremely harmful if the bot would try to answer questions it does not fully understand."}, {"heading": "Acknowledgments", "text": "This research was supported by The Israel Science Foundation (grant No. 1890/14)"}], "references": [{"title": "An optimum character recognition system using decision functions", "author": ["Chao K Chow"], "venue": "IRE Transactions on Electronic Computers,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1957}, {"title": "A method for improving classification reliability of multilayer perceptrons", "author": ["Luigi Pietro Cordella", "Claudio De Stefano", "Francesco Tortorella", "Mario Vento"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Boosting with abstention", "author": ["Corinna Cortes", "Giulia DeSalvo", "Mehryar Mohri"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "To reject or not to reject: that is the question-an answer in case of neural classifiers", "author": ["Claudio De Stefano", "Carlo Sansone", "Mario Vento"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "On the foundations of noise-free selective classification", "author": ["R. El-Yaniv", "Y. Wiener"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Active learning via perfect selective classification", "author": ["Ran El-Yaniv", "Yair Wiener"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Generalization bounds for averaged classifiers", "author": ["Yoav Freund", "Yishay Mansour", "Robert E Schapire"], "venue": "Annals of Statistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Support vector machines with embedded reject option. In Pattern recognition with support vector machines", "author": ["Giorgio Fumera", "Fabio Roli"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Dropout as a bayesian approximation: representing model uncertainty in deep learning", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Distribution-free performance bounds with the resubstitution error estimate", "author": ["O. Gascuel", "G. Caraux"], "venue": "Pattern Recognition Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1992}, {"title": "The Relationship Between Agnostic Selective Classification and Active", "author": ["R. Gelbhart", "R. El-Yaniv"], "venue": "ArXiv e-prints,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "The nearest neighbor classification rule with a reject option", "author": ["Martin E Hellman"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1970}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Very deep convolutional neural network based image classification using small training sample size", "author": ["Shuying Liu", "Weihong Deng"], "venue": "In Pattern Recognition (ACPR),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "ImageNet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei- Fei"], "venue": "International Journal of Computer Vision (IJCV),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "A risk bound for ensemble classification with a reject option", "author": ["Kush R Varshney"], "venue": "In Statistical Signal Processing Workshop (SSP), 2011 IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The subfield dealing with such capabilities in machine learning is called selective prediction (also known as prediction with a reject option), which has been around for 60 years [1, 5].", "startOffset": 179, "endOffset": 185}, {"referenceID": 4, "context": "The subfield dealing with such capabilities in machine learning is called selective prediction (also known as prediction with a reject option), which has been around for 60 years [1, 5].", "startOffset": 179, "endOffset": 185}, {"referenceID": 7, "context": "The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3].", "startOffset": 197, "endOffset": 207}, {"referenceID": 12, "context": "The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3].", "startOffset": 197, "endOffset": 207}, {"referenceID": 2, "context": "The literature on the reject option is quite extensive and mainly discusses rejection mechanisms for various hypothesis classes and learning algorithms, such as SVM, boosting, and nearestneighbors [8, 13, 3].", "startOffset": 197, "endOffset": 207}, {"referenceID": 1, "context": "Existing NN works consider a cost-based rejection model [2, 4], whereby the costs of misclassification and abstaining must be specified, and a rejection mechanism is optimized for these costs.", "startOffset": 56, "endOffset": 62}, {"referenceID": 3, "context": "Existing NN works consider a cost-based rejection model [2, 4], whereby the costs of misclassification and abstaining must be specified, and a rejection mechanism is optimized for these costs.", "startOffset": 56, "endOffset": 62}, {"referenceID": 4, "context": ") Here we consider the alternative risk-coverage view for selective classification discussed in [5].", "startOffset": 96, "endOffset": 99}, {"referenceID": 17, "context": "Ensemble techniques have been considered for selective (and confidence-rated) prediction, where rejection mechanisms are typically based on the ensemble statistics [18, 7].", "startOffset": 164, "endOffset": 171}, {"referenceID": 6, "context": "Ensemble techniques have been considered for selective (and confidence-rated) prediction, where rejection mechanisms are typically based on the ensemble statistics [18, 7].", "startOffset": 164, "endOffset": 171}, {"referenceID": 8, "context": "Recently, Gal and Ghahramani [9] proposed an ensemble-like method for measuring uncertainty in DNNs, which bypasses the need to train several ensemble members.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "A selective classifier [5] is a pair (f, g), where f is a classifier, and g : X \u2192 {0, 1} is a selection function, which serves as a binary qualifier for f as follows,", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "The entire performance profile of such a classifier can be specified by its risk-coverage curve, defined to be risk as a function of coverage [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 9, "context": "1 (Gascuel and Caraux, 1992, [10]) Let P be any distribution and consider a classifier f whose true error w.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "As discussed in [10], the analytic bounds derived using, e.", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "In this section we consider two confidence-rate functions, \u03baf , based on previous work [9, 2].", "startOffset": 87, "endOffset": 93}, {"referenceID": 1, "context": "In this section we consider two confidence-rate functions, \u03baf , based on previous work [9, 2].", "startOffset": 87, "endOffset": 93}, {"referenceID": 1, "context": "1 The first confidence-rate function we consider has been around in the NN folklore for years, and is explicitly mentioned by [2, 4] in the context of reject option.", "startOffset": 126, "endOffset": 132}, {"referenceID": 3, "context": "1 The first confidence-rate function we consider has been around in the NN folklore for years, and is explicitly mentioned by [2, 4] in the context of reject option.", "startOffset": 126, "endOffset": 132}, {"referenceID": 8, "context": "Softmax responses are often treated as probabilities (responses are positive and sum to 1), but some authors criticize this approach [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 8, "context": "The MC-dropout technique we consider was recently proposed to quantify uncertainty in neural networks [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 16, "context": "We trained VGG models [17] for CIFAR-10, CIFAR-100 and ImageNet.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "We now consider CIFAR-10; see [14] for details.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "We used the VGG-16 architecture [17] and adapted it to the CIFAR-10 dataset by adding massive dropout, exactly as described in [15].", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "We used the VGG-16 architecture [17] and adapted it to the CIFAR-10 dataset by adding massive dropout, exactly as described in [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "85% using the wide residual network architecture [19].", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "We used an already trained Image-Net VGG-16 model based on ILSVRC2014 [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 11, "context": "We also implemented the RESNET-50 architecture [12] in order to see if qualitatively similar results can be obtained with a different architecture.", "startOffset": 47, "endOffset": 51}, {"referenceID": 5, "context": "Selective classification is intimately related to active learning in the context of linear classifiers [6, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "Selective classification is intimately related to active learning in the context of linear classifiers [6, 11].", "startOffset": 103, "endOffset": 110}], "year": 2017, "abstractText": "Selective classification techniques (also known as reject option) have not yet been considered in the context of deep neural networks (DNNs). These techniques can potentially significantly improve DNNs prediction performance by trading-off coverage. In this paper we propose a method to construct a selective classifier given a trained neural network. Our method allows a user to set a desired risk level. At test time, the classifier rejects instances as needed, to grant the desired risk (with high probability). Empirical results over CIFAR and ImageNet convincingly demonstrate the viability of our method, which opens up possibilities to operate DNNs in mission-critical applications. For example, using our method an unprecedented 2% error in top-5 ImageNet classification can be guaranteed with probability 99.9%, and almost 60% test coverage.", "creator": "LaTeX with hyperref package"}}}