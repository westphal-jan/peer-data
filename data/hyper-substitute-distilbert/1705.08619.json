{"id": "1705.08619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2017", "title": "Dictionary-based Monitoring of Premature Ventricular Contractions: An Ultra-Low-Cost Point-of-Care Service", "abstract": "catastrophic cardiovascular diseases ( las ) are suffering across larger strata, or economically disadvantaged rate increasingly disproportionately affected compared within the high cost of traditional billing clinics. accordingly, achieving local ultra - low - sensitivity demographic, securing insurance to groups very least bottom middle the economic pyramid, slowly emerged as a manpower imperative. contrasting this backdrop, regions embrace an inexpensive yet attractive dose - based electrocardiogram ( ecg ) rating service. broadly, we hopes to provide meter - plus - end monitoring of variable ventricular contractions ( pvcs ), high frequency intermittent respiratory observations indicate the directly triggered historically fatal arrhythmia. note that a traditional ultrasound system acquires the outcome, transmits a past prime professional diagnostic centre without billing, and nearly achieves the inherent accuracy of one bedside setup, albeit at high overall cost. increasing this usage, we aim : reducing cost without significantly sacrificing reliability. to this purpose, professionals recommend a clock - based algorithm currently detects with high density the component beats only which are successfully predicted. we further compress those transmitted beats regarding category - specific encoding subject to suitable reconstruction / spectral fidelity. such a scheme ultimately not only reduce functional mean bandwidth requirement, but improve localising anomalous intervals, furthermore adjusting physicians'burden. finally, citing monte echo cross validation techniques rf / bih arrhythmia database, countries incorporate the performance as successive repeated interventions. in excess, with a magnitude range within 80 average one identified participant throughout 20 hundred communities, and a precise root width bias difference less than 9 % ( a clinically supported optimal averaging cost ), but achieved about 99. 15 % continuous linear bandwidth sensitivity, equivalent to 118 - second this over traditional methodology.", "histories": [["v1", "Wed, 24 May 2017 06:00:57 GMT  (1335kb)", "http://arxiv.org/abs/1705.08619v1", "19 pages, 9 figures and 5 tables"]], "COMMENTS": "19 pages, 9 figures and 5 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["bollepalli s chandra", "challa s sastry", "laxminarayana anumandla", "soumya jana"], "accepted": false, "id": "1705.08619"}, "pdf": {"name": "1705.08619.pdf", "metadata": {"source": "CRF", "title": "Dictionary-based Monitoring of Premature Ventricular Contractions: An Ultra-Low-Cost Point-of-Care Service", "authors": ["Bollepalli S. Chandra", "Challa S. Sastry", "Laxminarayana Anumandla", "Soumya Jana"], "emails": ["bschandra@iith.ac.in", "csastry@iith.ac.in", "laxmin56@gmail.com", "jana@iith.ac.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n08 61\n9v 1\n[ cs\n.L G\n] 2\n4 M\nay 2\nWhile cardiovascular diseases (CVDs) are prevalent across economic strata, the economically disadvantaged population is disproportionately affected due to the high cost of traditional CVD management, involving consultations, testing and monitoring at medical facilities. Accordingly, developing an ultra-low-cost alternative, affordable even to groups at the bottom of the economic pyramid, has emerged as a societal imperative. Against this backdrop, we propose an inexpensive yet accurate home-based electrocardiogram (ECG) monitoring service. Specifically, we seek to provide point-of-care monitoring of premature ventricular contractions (PVCs), high frequency of which could indicate the onset of potentially fatal arrhythmia. Note that a traditional telecardiology system acquires the ECG, transmits it to a professional diagnostic center without processing, and nearly achieves the diagnostic accuracy of a bedside setup, albeit at high bandwidth cost. In this context, we aim at reducing cost without significantly sacrificing reliability. To this end, we develop a dictionary-based algorithm that detects with high sensitivity the anomalous beats only which are then transmitted. We further compress those transmitted beats using class-specific dictionaries subject to suitable reconstruction/diagnostic fidelity. Such a scheme would not only reduce the overall bandwidth requirement, but also localizing anomalous beats, thereby reducing physicians\u2019 burden. Finally, using Monte Carlo cross validation on MIT/BIH arrhythmia database, we evaluate the performance of the proposed system. In particular, with a sensitivity target of at most one undetected PVC in one hundred beats, and a percentage root mean squared difference less than 9% (a clinically acceptable level of fidelity), we achieved about 99.15% reduction in bandwidth cost, equivalent to 118-fold savings over traditional telecardiology. In the process, our algorithm outperforms known algorithms under various measures in the telecardiological context.\nKeywords: Affordable telecardiology, Point-of-care service, Premature ventricular contractions, Dictionary learning, High-sensitivity detection, High-fidelity compression."}, {"heading": "1. Introduction", "text": "Cardiovascular diseases (CVDs) are a leading cause of death across economic strata [1]. Hence a crucial healthcare objective consists in managing those diseases. In this regard, electrocardiogram (ECG) signals acquired from subjects often play a\n\u2217Corresponding author Email addresses: bschandra@iith.ac.in\n(Bollepalli S. Chandra), csastry@iith.ac.in (Challa S. Sastry), laxmin56@gmail.com (Laxminarayana Anumandla), jana@iith.ac.in (Soumya Jana)\nvital role. Specifically, continuous ECG monitoring is central to early diagnosis and improved clinical outcome in certain scenarios. However, such monitoring at a professional facility is often unaffordable to economically disadvantaged individuals due to high cost, low availability and other barriers. Against this backdrop, home-based point-ofcare (POC) monitoring assumes significance. In this paper, we propose a POC monitoring service that is highly affordable.\nSymptoms indicating CVDs often manifest sporadically. Consequently, to detect deviations from\nPreprint submitted to Artificial Intelligence in Medicine May 25, 2017\nthe normal sinus rhythm, subjects should ideally be monitored continuously. Especially, for patients who have suffered myocardial infarction (MI), or developed left ventricular dysfunction (LVD), continuous monitoring has proven essential in promptly detecting sudden deterioration in cardiac functions, and hence preventing mortality [2]. The aforementioned as well as various related conditions are associated with premature ventricular contractions (PVCs) that briefly interrupt the normal rhythm of the heart [3]. Although PVCs occur in healthy individuals as well, high frequency of PVCs is known to foretell serious arrhythmic conditions [4], and significantly correlate with events of mortality [5]. In short, accurate detection of PVCs assumes clinical significance in stratifying high risk patients, and predicting medical emergencies. In this context, we propose a novel personalized service to monitor the PVC burden. In particular, we seek to develop a POC service that would appeal to the economically disadvantaged. Worldwide, about 1.2 billion individuals live on less than US$ 1.25 per day and have little discretionary income [6]. To such individuals, the cost of professional monitoring could often be prohibitive. Further barriers to quality care could include travel and hospital expenses. Fortunately, high penetration of mobile phones even in remote communities has mitigated such barriers in certain scenarios [7]. In the present case, can the mobile network be leveraged to provide reliable PVC monitoring at an attractive cost to the communities living at the bottom of the economic pyramid [8]? In response, we take a frugal engineering approach [9], and propose an ultra-low-cost POC service. As depicted in Figure 1a, a conventional telecardiology system simply records user ECG and transmits it to a diagnostic center staffed by medical professionals, where anomalies are manually detected and medical intervention is initiated, when necessary. Traditionally, ECG signals are transmitted unaltered, resulting in perfect accuracy (subject only to human error), albeit with the attendant high bandwidth cost and without localizing potentially anomalous beats. To reduce cost, we propose a new telecardiology paradigm, depicted in Figure 1b, where each user is equipped with a heartbeat classifier that detects anomalous beats, and then compresses and transmits only those anomalous beats and delimiting neighbors (forming beattrios) along with timestamps. Such a system not only reduces the bandwidth requirement but also\npresents to medical professionals only those beats that warrant closer inspection, thereby potentially improving the responsiveness of the diagnostic center.\nIn this framework, system design involves a tradeoff among three quantities: (i) classifier sensitivity (the fraction of PVC beats correctly identified), which we take as the reliability criterion, (ii) the fidelity of reconstructed signal at the diagnostic center, which determines the ability of experts to authenticate algorithmic classification, and (iii) the transmission bandwidth, which dictates the operating cost. To ensure accurate clinical outcome, one desires both high reliability (classifier sensitivity) and high fidelity. At the same time, one also seeks low transmission bandwidth in order to operate at a low cost. The main difficulty arises due to the complex three-way tradeoff among the above quantities. In particular, the bandwidth usage increases with sensitivity and decreases with specificity, while sensitivity and specificity themselves exhibit a nonlinear inverse inter-relationship dependent on reconstruction fidelity. The above quantities are further affected by signal compression. In this paper, we study the said tradeoff, and propose a natural design framework for telecardiology systems.\nNone of the individual tasks, namely, classification and compression, is new in the field of ECG signal processing. In fact, numerous algorithms have been reported specifically for PVC de-\ntection. Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18]. However, such algorithms have not been designed to achieve compression as well, and are not optimized in the high-sensitivity regime. On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22]. Yet, those algorithms too were not designed to achieve the desired highsensitivity classification. Against this backdrop, we propose a dictionary-based method that attempts to retain the best of both worlds, and achieve the desired classification and the compression goals simultaneously. The proposed approach, however, is different from the (symmetric) joint classification/reconstruction framework [23], where a combination of classification and (class-oblivious) reconstruction indices is minimized subject to a rate constraint. Instead, our setup is inherently classasymmetric as signals detected as normal (excepting delimiting beats) are discarded, i.e., compressed to zero bits, while signals detected as PVCs are compressed at a certain rate so as to meet a target diagnostic fidelity criterion. To this end, we propose to train separate overcomplete dictionaries for the respective classes of normal and PVC beats using labeled data. Specifically, each test beat is approximated as a linear combination of the columns of each dictionary. Intuitively, a signal should admit sparse representation only in the dictionary of the matching class. In accordance, the ratio of sparsity of representation in each dictionary is computed, and a suitable class is assigned by comparing that ratio to a threshold. The sensitivity level is then tuned to a desired level by varying such threshold. Here, the sparsity of representation is dictated by the desired fidelity of reconstruction, and in turn determines the degree of compression. Although low reconstruction fidelity would result in high sparsity (hence high compression), the resulting representation would also tend to miss the information necessary for accurate classification. Interestingly, highfidelity regime may not guarantee accurate classification either. As the signal approximation becomes increasingly accurate, the representations based on rival dictionaries would decrease in sparsity, which in turn leads to poor classification. Consequently,\nour task involves choosing suitable level of fidelity so that high sensitivity and high compression are both achieved.\nThe efficacy of the proposed scheme is demonstrated on the standard MIT/BIH arrhythmia database using Monte Carlo cross validation (MCCV). Presently, we confine to PVC beat detection, however, the same framework can be extended to detection of other as well as multiple classes of anomalies. Specifically, at a high-sensitivity target of 99% (i.e., no more than one undetected PVC beat in one hundred), using only classification and only compression, we respectively reduced the bandwidth requirement to 42.4% and 2.0% compared to the original. Using both classification and compression, we required a bandwidth of only 0.85% of the original, which translates to 118-fold savings in the operating cost, and an ultra-low-cost solution. Finally, we compared results obtained by our technique with those obtained using existing algorithms, and demonstrated the criticality of the proposed high-sensitivity approach in realizing practical ultra-low-cost telecardiology.\nOur key contributions are as follows. We\n1. developed a dictionary-based algorithm that achieves high-sensitivity classification and high-fidelity compression;\n2. demonstrated an affordable POC service based on such algorithm, and evaluated its efficacy using MCCV on the standard MIT/BIH arrhythmia database;\n3. achieved 118-fold cost reduction over classical telecardiology, which improves upon the cost reduction due to known algorithms.\nThe rest of the paper is organized as the following. Sec. 2 details our motivation, and identifies the key medical and social goals. In Sec. 3, the associated signal processing problems are formalized with necessary mathematical treatment. Dictionary-based solutions are developed in Sec. 4. Performance evaluation strategy, experimental setup and simulation results are presented in Secs. 5, 6 and 7, respectively. Finally, Sec. 8 concludes the paper with a discussion."}, {"heading": "2. Motivation and Envisaged System", "text": "We begin by placing the present problem in medical and social contexts."}, {"heading": "2.1. Clinical Imperative", "text": "Cardiac anomalies could be caused by various conditions that overwork and/or damage heart muscles. Continuous monitoring has often proven effective in timely detection of such anomalies. In particular, monitoring PVCs, which are an early depolarization of the myocardium originating in the ventricle [3], assumes significance, even though such beats are found in subjects with as well as without structural heart diseases [4]. In healthy individuals, a PVC prevalence of less than 1% is common, which carries no prognostic significance. In contrast, more frequent PVCs might indicate (or, lead to) structural heart diseases. Specifically, 90% of patients experience PVCs after acute MI [24], and the risk of sudden death in such patients is related to the complexity and frequency of the PVCs. Recent studies also indicate the role of PVCs in inducing cardiomyopathy [25]. More generally, continuous monitoring of PVCs has proven effective in stratifying clinical risk. However, there is no clear demarcation between high and low frequencies of PVCs. Recommended lower threshold for the high-risk subjects, such as those with a history of MI or LVD varies between 10,000 and 20,000 in a 24-hour window [26]. Another recommendation sets 10% as the threshold PVC burden [27]. Besides frequency of PVCs, run of two or more PVCs and their complexity could also indicate an adverse heart condition [5]. Accordingly, in the present work, we propose a PVC monitoring system that detects PVC beats with high sensitivity and communicates those with high fidelity to the diagnostic center, when suitable high-risk criteria are met."}, {"heading": "2.2. Technological Imperative", "text": "A conventional telecardiology system, depicted in Figure 1a, acquires and transmits entire user ECG to the diagnostic center. Such a system not only utilizes the available bandwidth in an inefficient manner but also burdens the medical professional with processing the entire record to identify anomalies. In this framework, telephone based ECG transmission and associated clinical experience were investigated decades ago [28]. With growing ubiquity of mobile phones in recent years, cellular network based as well as ZigBee based wireless systems have been developed [29, 30, 31]. Yet, despite technological progress, the inefficient telecardiology architecture has largely avoided scrutiny.\nIn this backdrop, we propose a novel architecture that makes judicious use of bandwidth while assisting medical professionals by localizing the potential anomalies, without compromising on the quality of care. In this context, note that efforts have already been made to deliver telecardiology services in the remote and rural communities with rickety networks. In particular, a method to encode ECG signals into ASCII characters to enable communication via SMS (short message service) has been reported [32]. In contrast, we assume a reliable network, which is expected to reflect the ground reality better and better with the passage of time in view of the phenomenal advancement in communication technology [33]."}, {"heading": "2.3. Social Imperative: Representative Scenario", "text": "As alluded earlier, we seek to provide a low-cost telecardiology solution for individuals with average daily income of about US$ 1.25. Consider an individual living at the economic threshold of this target population segment, who suffered myocardial infarction in the recent past, and was successfully treated (see [34] for various treatment options). Post treatment, monitoring PVCs over long intervals has now become a clinical priority as mentioned earlier. In this context, we shall investigate the cost associated with such PVC monitoring."}, {"heading": "2.3.1. Cost incurred in traditional telecardiology", "text": "Let us first estimate the cost incurred in traditional telecardiology. Here, we assume that diagnostic services are rendered free of cost. Such an assumption is realistic in various developing and underdeveloped countries, where free healthcare is dispensed from government-run facilities [35]. This welfare paradigm is currently being extended even to the broader context of telemedicine [36]. So the cost incurred would only constitute the data transmission cost. Considering a sampling rate of 360Hz and word length of 11 bits (used in MIT/BIH arrhythmia database [37]), one would generate about 1.78MB of data per hour. As we plan to use existing mobile networks, communicating entire data to the diagnostic center would cost about US 27 per hour at the rate of US 1.5 per 100KB of data usage1. At this rate, the cost of ten-hour monitoring of single channel ECG would amount to US$ 2.7.\n1We use the Indian mobile data tariff of Indian rupee (INR) 1 per 100KB as representative, and an exchange rate of US$ 1 = INR 66.7."}, {"heading": "2.3.2. Affordability as necessity", "text": "In general, healthcare expenses exceeding 10% of household spending is considered catastrophic [38]. In the aforementioned scenario, assuming a household size of four excluding the subject (which approximates the average family size in India [39]), the household income amounts to about US$ 5 a day. Assuming zero savings, the 10-hour PVC monitoring cost of US$ 2.7, calculated in Sec. 2.3.1, amounts to 54% of the household spending, and would clearly be unaffordable. In this situation, as a catastrophic health condition is expected to be detected rarely, the household could be tempted to view the monitoring expenditure as non-essential. In reality, however, timely detection of a life-threatening condition saves life with high probability, and hence periodic monitoring remains crucial for long-term survival. Hence, it becomes imperative that the monitoring cost be drastically reduced to such an affordable level that even an economically disadvantaged person would find little incentive to forego it."}, {"heading": "2.4. Outline of Envisaged System", "text": "To meet the aforementioned imperative, we seek to reduce the volume of data communicated to the diagnostic center. As a means, it appears natural to compress the data before transmission. In fact, to make the system even more efficient, we propose to\ndetect anomalous beats, and communicate a compressed version of only those beats. More precisely, we shall form beat-trios, each consisting of a PVC beat, and normal beats preceding and following it (See Figure 2a for an illustrative example). A representative beat vector for the normal beat and the PVC beat are shown in Figure 2b. If a PVC beat is not isolated, but a run of PVCs (two or more) occur, the normal beats preceding and succeeding the run are used as delimiters. Such beat-trios (and delimited PVC runs) will then be communicated to the diagnostic center along with the timing information. Although this scheme adds a worst-case overhead of two beats for each anomalous beat, it preserves the timing and morphological information of neighboring beats, which are known to facilitate professional diagnosis [17]. As mentioned earlier, additional bandwidth savings is achieved by transmitting the compressed version of those beats. The original and the reconstructed beat-trio signals along with reconstruction errors for various compression factors are presented in Figure 2c. Specific details on the proposed classification and compression techniques are provided later. In summary, we envisage a low-cost system that makes efficient use of bandwidth by suitably classifying and compressing heart beats."}, {"heading": "3. Classification, Compression and Dictionary Learning", "text": "As alluded earlier, signal processing in the present work involves classification and compression of ECG signals. In this section, we pose the associated engineering problems, and provide necessary mathematical preliminaries."}, {"heading": "3.1. ECG classification", "text": "A desired classifier specifies two mutually exclusive and exhaustive subsets \u03931 and \u03932 of set \u0393 of possible ECG beat x as follows. Any beat x \u2208 \u03931 is declared normal, while any beat x \u2208 \u03932 is declared a PVC. Presently, we wish to find \u03932 (and hence \u03931) such that for a given sensitivity Se, i.e., fraction of PVC beats correctly detected as PVC beats, the specificity Sp i.e., fraction of normal beats correctly detected as normal beats is maximized [40]. Next we examine the bandwidth requirement of the aforementioned classifier, assuming that only beats detected as anomalous (PVC) are transmitted to the diagnostic center, which possesses adequate resources to validate and correct, if necessary, the class of each beat it receives. In other words, one fails to detect a PVC beat only if that beat is originally classified as normal and never transmitted. Thus the fraction of undetected PVCs, 1\u2212Se, inversely relates to the reliability of the overall system including the diagnostic center. Perfect reliability is achieved when Se = 1. Denoting by \u03c1 the prevalence rate of PVCs, and taking the bandwidth requirement without classification as the reference, the fraction of actual PVC beats that are classified as PVCs equals Se \u00d7 \u03c1, and the fraction of normal beats that are mistakenly classified as PVCs is given by (1 \u2212 Sp) \u00d7 (1 \u2212 \u03c1). Thus the overall fraction of beats declared as PVC equals (Se\u00d7 \u03c1+ (1\u2212 Sp)(1\u2212 \u03c1)). In the envisaged beat-trio system, assuming a worst-case scenario that each PVC beat is preceded and followed by normal beats, the (conservatively estimated) fraction Bcl of bandwidth usage with only classification and no compression is given by\nBcl = 3(Se\u00d7 \u03c1+ (1\u2212 Sp)(1\u2212 \u03c1)). (1)\nEmploying an ideal classifier (Se = 1, Sp = 1), one would require a bandwidth B = 3\u03c1, amounting to a substantial bandwidth savings (when \u03c1 << 1\n3 ),\nwhile ensuring perfect reliability. Unfortunately, such an ideal classifier is unrealizable. In practice,\nwe seek to significantly save bandwidth, while still achieving high reliability."}, {"heading": "3.2. ECG compression", "text": "In the same vein, assuming the signal set as composed of only normal and PVC beats with a compression ratio of \u03b2N (\u2265 1) and \u03b2V (\u2265 1), respectively, for normal and PVC beats, bandwidth usage is a function of prevalence and given by\nBco = \u03c1\u00d7 1\n\u03b2V + (1 \u2212 \u03c1)\u00d7\n1\n\u03b2N . (2)\nFurther bandwidth savings can be achieved by employing a hybrid scheme, where beat-trios are formed around detected PVC beats, which are then compressed and communicated to the diagnostic center. Employing such a scheme, bandwidth usage diminishes to at most\nBtr = (Se\u00d7 \u03c1+ (1\u2212 Sp)(1\u2212 \u03c1))( 1\n\u03b2V +\n2\n\u03b2N ). (3)\nIn general, the reconstruction fidelity varies inversely with compression ratio, and the tradeoff is beat-type specific. We shall measure reconstruction fidelity using the percentage root mean squared difference (PRD), widely used in the context of ECG:\nPRD = \u2016x\u2212 x\u0302\u20162 \u2016x\u20162 , (4)\nwhere x and x\u0302 stand respectively for the original and the reconstructed signals [41]. Further, from a diagnostic perspective, a PRD of no more than 9% has been found to be \u201cgood\u201d (Table 1) [42]. Accordingly, we set the above fidelity constraint in subsequent analysis."}, {"heading": "3.3. Dictionary-based Technique", "text": "So far, we have envisaged a system with certain target classification accuracy and reconstruction fidelity. Now we require an enabling technology to\nachieve those targets. In this regard, we propose a dictionary-based solution. First we need mathematical preliminaries of compressive sampling and dictionary learning."}, {"heading": "3.3.1. Compressive sampling paradigm", "text": "Compressive sampling (CS) recovers a high dimensional sparse vector \u03b1 \u2208 Rn from a few of its measurements x = \u03a6\u03b1, x \u2208 Rm, m < n, where \u03a6 denotes the measurement matrix [43]. Formally, we seek to solve\nmin \u03b1 \u2016\u03b1\u20160 subject to \u03a6\u03b1 = x, (5)\nwhere \u2016\u00b7\u20160 indicates the l0 (counting) norm. In general, (5) is intractable. Fortunately, under certain technical conditions, solution to (5) remains unaltered if \u2016 \u00b7 \u20160 is replaced by the l1 norm \u2016 \u00b7 \u20161. As l1 solver, we shall use orthogonal matching pursuit (OMP) in view of its simplicity, empirical effectiveness (despite its being greedy) [43], and relatively low computational complexity of O(m2n) [44]."}, {"heading": "3.3.2. Dictionary learning", "text": "The method of dictionary learning identifies a tunable selection of basis vectors providing sparse representation. Given a set of signals {xi} M i=1, KSVD obtains the dictionary D that provides the sparsest representation for each example in this set [45]. It involves a two-step procedure. In the first step, for a given dictionary D, we obtain matrix \u03a8 with sparse columns by solving the following optimization problem:\n\u03a8 = argmin\u0398 \u2211\nl\n\u2016 \u0398l \u20161 subject to X = D\u0398,\n(6) where \u0398l is the l-th column of \u0398, and X is the matrix whose columns are xi\u2019s. Using the above \u03a8, the pair (D,\u03a8) is then updated as\n(D\u0302, \u03a8\u0302) = argmin D,\u03a8\n\u2016X \u2212D\u03a8\u20162F subject to\n\u2016\u03a8i\u20160 \u2264 T0 \u2200i, (7)\nwhere \u03a8i denotes the i-th column of \u03a8, T0 the sparsity parameter, and \u2016 \u00b7 \u2016F indicates the Frobenius norm. The K-SVD algorithm alternates between sparse coding (6), solved by an l1 solver such as OMP (CS theory), and dictionary update (7) based on iterative soft-thresholding, till convergence. The complexity of learning an m\u00d7n dictionary based on\nM training data (signals) is O(m2nM) [44]. However, as such learning is generally performed offline, complexity of projecting a signal vector on a dictionary and finding dictionary coefficients is a more important consideration. Fortunately, that complexity is O(m2n), i.e., the same order as that of OMP. Consequently, the runtime complexity of both dictionary-based classification and compression algorithms is also O(m2n)."}, {"heading": "4. Proposed Dictionary-based Solution", "text": "At this point, we are ready to propose a dictionary-based solution to achieve the desired classification and compression targets."}, {"heading": "4.1. Dictionary-based classification", "text": "Consider labeled dataset {{xil} Ml i=1} K l=1. Here l indicates the class label: l =\u201cN\u201d indicates normal, and l =\u201cV \u201d indicates PVC in a two class problem (K = 2). Further, i indicates beat index, taking values up to Ml, the number of beats present in class l. Based on such labeled dataset, we learn the dictionary Dl \u2208 R\nm\u00d7n for class l. When a test beat x is presented, to achieve beat classification, we first find the sparsest representation \u03b1l of x using each dictionary Dl, l \u2208 {N, V }, by solving\n\u03b1\u0302l = min \u2016\u03b1l\u20161 subject to \u2016x\u2212Dl\u03b1l\u20162 < \u01eb, (8)\nwhere \u01eb > 0 denotes the representation accuracy and is proportional to PRD, the normalized reconstruction fidelity. Here, we denote by PRDclass the target reconstruction fidelity corresponding to the classification subsystem. Operating at PRDclass, as depicted in Figure 3a, x is marked as PVC if the ratio of l0 norm (number of non-zero entries) of \u03b1classV to that of \u03b1classN is less than a suitable threshold \u03c4 , and as normal otherwise. When PRDclass is low, the signal representation tends to be non-sparse and hence our sparsity-based classification could be less accurate. Further, at high levels of PRDclass, both the dictionaries are expected to represent the signal with only a few coefficients, so that a sparser alternative is harder to pick, thereby also decreasing classification accuracy. Accordingly, we choose to operate at a suitable fidelity level that maximizes classification accuracy. Note that, for a given PRDclass, classification accuracy depends only on the ratio of sparsity of representation in rival classes, and does not require the signal to actually be reconstructed. Consequently, PRDclass has no influence on the signal reconstruction fidelity of the overall system, and remains an internal parameter of the classification subsystem. Further, classifier performance is dictated by the choice of the threshold \u03c4 . We plot receiver operating characteristic (ROC) curves for our classifier by varying \u03c4 and pick suitable operating points."}, {"heading": "4.2. Dictionary-based compression", "text": "Recall that each beat marked as anomalous, as well as each delimiting normal beat, is compressed and communicated to the diagnostic center. We intend to maximize compression ratio for a given reconstruction fidelity target PRDcompr using a dictionary based method as shown in Figure 3b. Specifically, we first project the test beat on the class-specific dictionary subject to an intermediate PRD constraint PRDint (\u2264 PRDcompr), and compute the corresponding dictionary coefficients, only a subset of which are expected to be non-zero. Those non-zero coefficients are subsequently quantized such that the PRD degrades enough to meet the overall constraint PRDcompr. Here, PRDint remains internal to compression subsystem and if we set PRDint to be significantly smaller than PRDcompr, the number of non-zero coefficients would be large, which would then require coarse quantization so as to increase the overall PRD sufficiently. On the other hand, if PRDint is set too\nclose to PRDcompr, only a few coefficients are expected to be non-zero, which can only be quantized rather fine because of relatively small room for PRD degradation. In general, PRDint governs the interplay between the number of non-zero coefficients and the coarseness of their quantization; however, it is not straightforward how to optimally set PRDint to obtain the highest compression ratio subject to PRDcompr. So, we perform a search as follows. In particular, we plot overall PRD versus compression ratio for various choices of PRDint, and take the envelop as the plot of PRDcompr versus compression ratio. As discussed earlier, we shall operate at PRDcompr = 9% for each of PVC and normal classes so as to maximize signal compression while preserving the diagnostic integrity of the ECG signal. Next we detail our quantization scheme as well as our encoding scheme for the quantized coefficients, which in turn determines compression ratio. We first generate a quantization table for each dictionary coefficient. Specifically, we rank the non-zero coefficients in the descending order of absolute magnitude. At rank i, we find the maximum and minimum (signed) values W imax and W i min, respectively, and adopt uniform quantization with step size \u2206. Specifically, x is quantized to\nQi(x; \u2206) =\n\n \n \nW imin \u2212 \u2206 2 , x < W imin, k\u2206\u2212 \u2206 2 , x \u2208 [(k \u2212 1)\u2206, k\u2206), W imax + \u2206 2 , x \u2265 W imax.\n(9) Note that the quantizer range depends on rank, but not the quantizer step size. Further, as the step size \u2206 increases, so does compression ratio as well as PRD. Finally, quantized coefficients, coefficient locations, and differential timestamps are encoded using Huffman coding algorithm based on empirical probabilities [46]. Finally, after quantization and encoding of dictionary coefficients, we compute the class-specific beat compression ratio \u03b2 as follows:\n\u03b2l = Number of bits representing x\nNumber of bits representing Cl +Btimel + 1 ,\n(10) where x represents a beat vector, Cl encodes quantized amplitude as well as location of the nonzero elements of sparse dictionary coefficients \u03b1l, l \u2208 V,N , and Btimel represents the number of bits required to encode beat-specific timestamp. Further, one additional bit is used to encode label l."}, {"heading": "4.3. End-to-end System", "text": "At this point, we turn to completing an end-toend system that utilizes the classification and compression subsystems discussed so far. A flowchart of the proposed system is depicted in Figure 4, and\nconsists of the following modules.\nData reading: We begin by acquiring ECG samples from the subject, and store those in a buffer. Simultaneously, we read stored samples from the buffer to form a beat vector Bn. Time Tn of occurrence of corresponding beat is recorded and the communication status flag Sn is set to zero, which would later indicate whether to transmit a specific beat to the diagnostic center.\nClassification and compression: First, to detect anomaly, each beat vector is projected on the prelearned dictionaries of normal and PVC classes to obtain respective sparse representations, \u03b1classN and \u03b1classV , subject to PRD \u2264 PRD\nclass. By comparing the ratio of sparsity of representation in either class to a threshold \u03c4 , each beat is assigned a class label Ln. Internally, we use 0 and 1 to indicate N and V, respectively. Later, beat compression is achieved by projecting the beat vector Bn on the dictionary of the chosen class, subject to PRD \u2264 PRDint, the sparse coefficient vector \u03b1intl , l = N or V . Finally, we encode to Cn only the signed magnitude of non-zero elements and corresponding indices (locations) of \u03b1intl subject to PRD \u2264 PRD\ncompr. Here PRDclass, \u03c4 , PRDint and PRDcompr are design parameters.\nCommand flags: In order to communicate only the beats detected as PVCs and delimiting normal beats, we make use of certain command flags as follows. At instance n\u22121, if a PVC beat is encountered, i.e., Ln\u22121 = V , the communication status flags for the past, the present and the next beats are all set to 1, i.e., Sn\u22122 = Sn\u22121 = Sn = 1. This is repeated after incrementing the counter n. The status flag generation logic is illustrated in Table 2 for three representative label sequences. As the beat label of the current beat could impact the label of the previous beat, our system would incur a delay of two beats.\nTransmission: Finally, we communicate only the marked beats to the diagnostic center based on certain clinical considerations. Common clinical conditions requiring expert attention include: (i) Average PVC burden exceeds certain threshold over a specified interval; (ii) A run of two or more PVCs\nis detected. We illustrate the proposed service in Figure 4 using condition (i). Specifically, we maintain an accumulator flag Acc, which is incremented when an anomalous beat is detected. Once the frequency of the anomalous detections exceed the specified threshold Th, the user is notified and the data (with a delay of two beat intervals) are communicated to the diagnostic center. Alternatively, if the total number of beats reaches the maximum beat count Nth, monitoring stops with a notification to the user. Recall that the specific transmission logic described above is presented only as an illustration. In general, one should adopt suitable logic that embodies the desired condition."}, {"heading": "5. Framework for Performance Evaluation", "text": "We now turn to performance evaluation of the proposed classification subsystem, the compression subsystem and the complete end-to-end system. As customary, we evaluate the classification and the compression subsystems based on the tradeoff between sensitivity (reliability) versus specificity, and compression ratio versus reconstruction fidelity, respectively. Further, we evaluate the endto-end system in terms of bandwidth cost savings subject to clinically motivated reliability and fidelity constraints. For evaluation of various performance indices, we made use of MIT/BIH Arrhythmia database available from the PhysioBank archives [37]. In particular, we first partition the database into training and test sets, and train a common dictionary underlying both the classification and compression subsystems. Later, we test the performance of these subsystems and the endto-end system, and compare with the performance of reported algorithms in the telecardiological context. Clearly, the said partitioning can be carried out in large number of ways. We intend to adopt a partitioning principle that is appropriate for the underlying practical problem."}, {"heading": "5.1. Patient-specific Partitioning", "text": "Traditionally, partitioning of database into training and test sets is performed either in a classoriented or in a subject-oriented manner [47]. In the former, partitioning is based only on the heartbeat label, which allows significant amounts of data from the same patient to be represented in both training and test sets, resulting in overly optimistic performance estimates [18, 48]. In contrast, the\nlatter seeks to account for inter-subject variability, and constitutes training and test sets with beats from distinct subsets of records, leading to an overly conservative estimate of performance [16]. More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification. A few patient-specific beats (generally, segmented from the first 5 minutes of each record) are added to the training set. Such patient-specific approach often provides a reasonable performance estimate, which is less optimistic than the performance estimated using purely classoriented partitioning, and less conservative than that using purely subject-oriented approach. Accordingly, we adopt a patient-specific paradigm complying with ANSI/AAMI EC57:1998 recommendation [49], and compared with known compliant algorithms. It is worth noting that the said recommendation excludes subjects with paced beats (records 102, 104, 107 and 217 of the MIT/BIH Arrhythmia database) and partitions the remaining 44 records into training and test sets."}, {"heading": "5.2. Hand-picked Partitioning", "text": "Even patient-specific partitions are numerous. Traditionally, one research group would handpick one such partition based on subjective criteria. Examples include Partition-1, Partition-2 and Partition-3 given in Table 3. In each case, the total (and relative) numbers of normal and PVC beats are mentioned for each of the training and the test sets. In particular, Partition-1 considers only those records that contain at least one PVC beat. Here, the training and the test sets consist of 13 records with indices in the range 100\u2013124 and 20 records with indices in the range 200\u2013234, respectively [10]. Such partition has relatively a small faction (18.3%) of PVC beats for training. Subsequently, Partition2 generalizes Partition-1 by considering all records with indices in the range 100\u2013124 for training, and those with indices in the range 200\u2013234 for testing, without paying attention to occurence of PVC beats [14]. Though such partitioning improved numerical balance between training and test sets of normal class, PVC class still remain biased, as in the Partition-1. Finally, Partition-3 possesses the property that training and test sets enjoy approximately equal representation from the rival classes of beats [11]. Generally, proposers of a specific algorithm tend to pick a partition that maximizes the\nalgorithmic performance. In this vein, we observed high performance of our classification algorithm for a certain evenly split Partition-4, while adhering to the desired numerical balance (Table 3). We dub as Proposal-1 the proposal to use our algorithm on Partition-4. However, comparison among algorithms in terms of peak performance observed for a hand-picked partition enjoys limited fairness and may not correlate well with user experience."}, {"heading": "5.3. Randomized Partitioning with Even Split", "text": "In response, we advance another Proposal-2, wherein performance is averaged over admissible evenly split partitions. In particular, 22 of 44 subjects\u2019 data are randomly chosen for training, and the remaining subjects\u2019 data for testing. Further, admissible partitions maintain the numerical balance that 45%-55% of the total PVC beats belong to the test set. Here, as the total number of admissible partitions is extremely large (> 1011), we adopted Monte Carlo cross validation (MCCV) approach, wherein the performance is averaged over multiple (100, in our case) random partitions. Compared to Proposal-1, the randomization in Proposal-2 more satisfactorily accounts for the unseen patient data encountered in practice."}, {"heading": "5.4. Randomized Partitioning with Training Set Larger than Test Set", "text": "In our home-based PVC monitoring context, one possesses voluminous historical data, and a few potential subjects to cater. Consequently, partitioning the present database such that the training set is larger than the test set appears more realistic compared to the even split seen in Proposal-1\nand Proposal-2. Accordingly, we modify Proposal-2 such that each partition under consideration has 40 subjects for training and 4 for testing, and call the new Proposal-3. In Proposal-3, we also update the numerical balance between PVC and normal beats so that only those partitions, where the test set accounts for 10% \u2013 20% of the total number of PVC beats, are considered. As admissible partitions still number a large 42,294, we adopt the MCCV approach over 100 randomly chosen partitions as earlier. Of course, historical (training) data should in practice be given even more weightage over test data in view of the overwhelming preponderance of the former. However, we settle for the above split in view of the limited size of the dataset at hand."}, {"heading": "5.5. Recommendation", "text": "In summary, Proposal-1 represents the peak performance, which is overly optimistic and should not be used for practical guidance. Proposal-2 provides average performance, which is more satisfactory than the peak performance in certain sense, and helps highlight the significant gap between the two. Yet, the even split in Proposal-2 does not reflect the preponderance of historical data, and hence is too conservative to guide practical design. We recommend performance figures corresponding to Proposal-3, incorporating both a more realistic split and randomization, as a (slightly conservative) design guide."}, {"heading": "6. Experimental Setup", "text": "At this point, we conduct simulation experiments to demonstrate the efficacy of the proposed system. First we describe the experimental setup."}, {"heading": "6.1. Preprocessing", "text": "Recall that the adopted MIT/BIH Arrhythmia database consists of 30-minute excerpts of two channel ambulatory ECG recordings of 48 subjects [37]. Each channel collects 360 samples per second with a dynamic range of 10 mV peak-to-peak, and digitized to 11-bit words. Further, each beat is annotated per accepted clinical practice. For our experiments, we used the modified limb lead II (MLII) channel only.\nOn each record, we performed the following steps. First, the baseline wander was removed using two median filters of respective window sizes 200ms and 600ms in a sequential manner [11]. Next the annotated R-peak location in each beat was noted, and 150 samples before, 150 samples after and the Rpeak sample were collected in a vector of length 301 [21]. Such a signal vector included most of the information contained in one heart cycle. Currently, we considered only PVC and normal beats (Figure 2b). Although signal vectors chosen in this manner sometimes overlapped, individual beats still preserved morphological information essential for clinical diagnosis. These signal vectors were used for training dictionaries, and will be called beats from now on for the sake of simplicity."}, {"heading": "6.2. Dictionary Size", "text": "In the proposed dictionary-based classification/compression approach, we trained an overcomplete dictionary for each of normal and PVC classes using K-SVD algorithm. In this regard, the dictionary size assumed importance, as (i) smaller size required less computation, and (ii) larger column size led to sparser representation, both of which properties are desirable. However, sparsity saturates with increasing column size, and has negligible effect on classification performance beyond certain threshold [50]. Accordingly, we seek to choose the smallest dictionary that provides acceptable level of sparsity. Empirically, \u201cgood\u201d overcomplete dictionaries have been shown to possess a ratio of column to row size between approximately 2 and 5 [45]. Fortunately, we achieved satisfactory classification and compression performance, even while operating at the lower limit of the said ratio range. In particular, recalling that the row size equals the signal vector length of 301, we made use of dictionaries of size 301\u00d7600."}, {"heading": "7. Experimental Results", "text": "In this section, we present experimental results, and performance analysis for the classification subsystem and the compression subsystem separately, as well as for the overall system. To this end, we made use of MIT/BIH Arrhythmia database [37], adopted patient specific partitioning, evaluated the performance of our dictionary based method according to Proposal-1, Proposal-2 and Proposal3, and compared with the performance of known algorithms, when relevant. For normal and PVC classes, separate dictionaries were obtained based on the training set, while the performance was evaluated using the test set. See Figure 5 for a typical normal beat, a typical PVC beat as well as the three most frequently used atoms of each dictionary. Notice the differing beat morphologies, and how those are captured by the depicted dictionary atoms. For simulations, we used MATLAB v.2014b on a desk-\ntop computer with an Intel core i7 3.4 GHz 64-bit processor with 16 GB of memory, and required approximately 0.6 milliseconds to complete both classification and compression, which is several orders faster than real time, and indicates suitability of our system for practical deployment. Now we turn to reporting our results, beginning with classification performance."}, {"heading": "7.1. Classification Performance", "text": "As mentioned earlier, classification performance depends on the reconstruction fidelity target PRDclass, internal to classification subsystem, which we choose first.\n7.1.1. Key tradeoffs and choice of PRDclass\nTo this end, we studied the relationship among sensitivity, specificity and PRDclass of the classifier for Proposal-3. Specifically, we plotted in Figure 6a the tradeoff between specificity and PRDclass at various sensitivity levels. At the sensitivity (reliability) target of 99%, we observed specificity to be maximized at around PRDclass = 9%. To appreciate the phenomenon from a different perspective, we plotted in Figure 6b various ROC (Se versus 1 \u2212 Sp) curves. There we found that the optimal ROC curve, obtained by varying PRDclass, is well approximated by the ROC curve at the fixed value PRDclass = 9%. Accordingly, we aimed at achieving a target PRDclass of 9%.\nAs anticipated in Sec. 4.1, we observed in Figure 6a that specificity indeed exhibits steep rise, near constancy (plateau) and steep fall as PRDclass increases while keeping sensitivity levels fixed. Equivalently, in Figure 6b, significantly lower (5%) and higher (30%) values of PRDclass compared to the target 9% lead to poor approximation of the optimal ROC. As an interesting aside, we noticed in Figure 6a that the plateau region shrinks with increasing sensitivity. Further, recall that various levels of sensitivity Se \u2208 [0, 1], and hence various points on the ROC, are obtained by varying a threshold \u03c4 on the sparsity ratio (see Sec. 4.1). Plotting \u03c4 versus Se in Figure 6c, we noticed that the range of \u03c4 shrinks, as the PRDclass increases."}, {"heading": "7.1.2. Performance statistics", "text": "Now, operating at PRDclass = 9%, we compared the performance of the proposed classifier with various algorithms that adopted patient specific evaluation scheme. Specifically, we report in Table 4 sensitivity and specificity of existing classifiers and the proposed classifier along with specific information on training data. Recall that we set a sensitivity target of 99% for our proposals. Now, comparing the peak performance, our Proposal-1 performs better than most of the reported algorithms. However, Proposal-1 represents an overly optimistic performance specific to Partition-4, and may not capture the performance variation due to randomly chosen partitioning. As a remedy, we reported the performance of our Proposal-2 and Proposal-3, where uncertainty is handled more realistically. Specifically, operating at the target sensitivity of 99%, we reported the mean and standard deviation of specificity over 100 randomly chosen training and\ntest sets. Subject to evenly split training and testing sets, the mean specificity obtained in Proposal2 improves upon the peak specificity achieved in Proposal-1 in terms of fairness, although the former is significantly lower as expected. However, the notion of even split diverges from the practical situation, where significantly more data are available for training than testing. Accordingly, we recommend Proposal-3 that incorporates a realistic division with larger proportion of training data, as well as randomization. In this case, desirably, the mean specificity is higher, and the standard deviation is lower.\nSo far, we furnished in Table 4 pairs of sensitivity and specificity at the operating point of various algorithms. However, this information does not allow us to compare between those algorithms. Consequently, we used ROC curves to indicate the performance of our classifier across admissible sensitivity and specificity values (Figure 7). In particular, we plotted ROC curves for Proposal-1, Proposal-2 and Proposal-3 (recommended). Alongside, ROC curves of our classifiers evaluated on Partition-1,\nPartition-2, Partition-3 and Partition-4 (Proposal1) are also plotted. As ROCs of existing algorithms remain unavailable, we could only locate their operating points on the same plot. Encouragingly, Proposal-3 offers significant improvement over Proposal-2 as well as the peak performances\nof majority of reported results evaluated on handpicked sets."}, {"heading": "7.2. Compression Performance", "text": "Recall from Sec 4.2 that compression performance is determined by the number of non-zero elements in the dictionary coefficients, which is in turn dictated by the intermediate reconstruction fidelity PRDint. We now choose PRDint that maximizes compression ratio while maintaining desired PRDcompr.\n7.2.1. Key tradeoffs and choice of PRDint\nTo this end, we first considered Proposal-3, and plotted PRDcompr versus compression ratio for various PRDint values for PVC beats (Figure 8a). As mentioned earlier, for a small PRDint, the number of non-zero elements of dictionary coefficients is large. In this setting, for a small increase in quantization step size \u2206, which produces a small increment in compression ratio \u03b2V , quantization error from all those coefficients accumulate to result in a steep increment in PRDcompr. In contrast, for a large PRDint, non-zero coefficients are\nfew, and hence a similar increase in \u2206, producing a similarly small increment in \u03b2V , now allows accumulation of quantization error from relatively few coefficients, resulting in only a gradual increment in PRDcompr. So, to plot the optimal curve of PRDcompr versus compression ratio, we took the envelop of PRDcompr versus compression ratio curves for various PRDint values. At this point, recall from Table 1 that to preserve the diagnostic integrity of the ECG signal we should operate at at least PRDcompr = 9%, which is indicated by the dotted line. For the choice PRDcompr = 9%, we observed that PRDint \u2248 8.8% maximizes the compression ratio. The above steps were then repeated for normal beats, and optimal PRDint \u2248 8.8% was again observed. At this point, optimal PRDcompr versus compression ratio curves for both PVC and normal beats were presented on the same plots in Figure 8b. Those curves are similar with PVC beats allowing slightly higher compression ratio for any PRDcompr."}, {"heading": "7.2.2. Performance statistics", "text": "Compression performance too depends on the partitioning between training and test data. To remove such dependency, we again adopted MCCV approach to evaluate our compression algorithm and reported the performance statistics for Proposal-1, Proposal-2 and Proposal-3 (Table 5). Specifically, operating at PRDcompr = 9%, we reported the mean and standard deviation of \u03b2N and \u03b2V , compression ratios corresponding to normal beats and PVC beats, respectively. Not unexpectedly, with larger training data (Proposal-3), mean compression performance increased. Interestingly, standard deviation of compression performance also increased. Here, unlike in the case of classification, the proposed compression technique cannot fairly be compared with state of the art algorithms. To ap-\npreciate this, note that certain algorithms achieve high compression ratio for a signal consisting of several beats by stacking such beats before compressing [19]. In contrast, we avoid stacking to prevent delays. Further, specific fixed partitioning is sometimes chosen so as to maximize reported compression ratio [21, 22]. As mentioned earlier, such unrandomized results cannot be used as guides for practical system design."}, {"heading": "7.3. System Performance", "text": "We now present the overall system performance in terms of savings in bandwidth cost. Operating at the target reliability of Se = 99% and reconstruction fidelity of PRDcompr = 9%, the proposed beattrio communication system would achieve about 57.6% savings of original bandwidth if classification alone was used. Using compression alone, bandwidth savings is increased to 97.99%, while ignoring the communication overheads. Using both classification and compression, the proposed method achieved 99.15% saving in bandwidth usage, which translates to a proportionate savings in the operating cost.\nNow, let us revisit the representative scenario presented in Section 2, and recall that conventional telecardiology costs about US$ 2.7 for 10-hour ECG monitoring. Against this reference cost, in Figure 9\nwe graphically depicted the performance of our system as well as other reported algorithms in the telecardiological context. Specifically, we located various systems in a reliability versus bandwidth/cost plane. In y-axis, we plot the complement of reliability (1 \u2212 Se), i.e., the number of PVCs undetected per one hundred beats, and in x-axis, the bandwidth usage (bottom) as well as the cost (top) for ten-hour monitoring. Now, adopting beat-trio transmission, and assuming a PVC prevalence rate \u03c1 = 10%, we plotted reliability versus the bandwidth cost for various rival algorithms, and our Proposal-1, Proposal-2 and Proposal-3, without as well as with compression. Employing only classification, our recommended proposal required only 42.4% of bandwidth. Notice that a number of reported classifiers did not perform close to the reliability target of Se = 99%, i.e., one undetected PVC in one hundred as indicated by horizontal dashed line. The nearest in this respect, the classifier proposed by Chazal et al. [11], requires 36.4% of the reference bandwidth, while missing about six PVCs in one hundred beats, i.e., operating at a rate sixfold higher than the target. Using only compression, our proposal reduced the bandwidth requirement to only 2% of the original bandwidth. However, such a scheme would burden the medical professional with processing entire record for diagnosis. In comparison, the proposed classifier employing both classification and compression would not only reduce the bandwidth requirement but also assist medical professionals by localizing potential anomalies. Specifically, our system would use only 0.85% of the original bandwidth, achieving additional 98% and 57.5% savings over the bandwidth required for classification alone and compression alone, respectively. This would bring down the operating cost to US 2.3. At this rate, the healthcare expenses of the household, mentioned in Section 2, would be reduced to an affordable 0.46% of the household income from the original 54%. We believe that a drastic cost reduction of this scale should enable the targeted BOP communities to opt for continuous monitoring service without severe economic burden."}, {"heading": "8. Discussion", "text": "We conclude by summarizing our contributions, remarking on the anticipated user experience, and reflecting on broader impact of our work."}, {"heading": "8.1. Summary", "text": "In this paper, we presented an ultra-low-cost POC service for PVC monitoring that ensures high accuracy. In particular, we proposed a dictionarybased technique that achieves high-sensitivity classification and high-fidelity compression. We demonstrated the efficacy of our method using Monte Carlo cross validation on the MIT/BIH arrhythmia database [37, 51]. In particular, the threeway tradeoff between bandwidth, reliability and reconstruction fidelity was characterized. With a reliability target of at most one undetected PVC in one hundred beats, and a reconstruction fidelity of 9% level of PRD, we achieved about forty-fold savings in bandwidth and the associated cost. Our service would cost only US 2.3 for ten-hour monitoring, which, we believe, should be attractive to the economically marginalized."}, {"heading": "8.2. User Experience", "text": "While using our service, the experience of users (both subjects and medical professionals) is anticipated to remain essentially the same as that associated with conventional telecardiology. Specifically, at the subject end, the same transducers are still used to collect the ECG signals from the patient. From the medical professionals\u2019 perspective, the inference has to be made from the electronic records at essentially the same quality (PRD \u2264 9%, from Table 1 [42]) as the gold (quality) standard of unprocessed signals. In fact, the time and effort required of the medical professional are anticipated to be less than that in the traditional situation, as the proposed method automatically identifies PVCs and presents only delimited anomalous beats. In a nutshell, subjects familiar with convectional telecardiology would require no additional training, whereas medical professionals would only need to focus on the presented beats (beat-trios), and ignore blank spaces, which would just indicate normal (uninformative) beats."}, {"heading": "8.3. Broader Impact", "text": "Monitoring of PVCs is clinically significant in broader scenarios than considered so far. Specifically, high PVC burden could presage adverse heart conditions even in individuals without prior structural heart disease [5]. In such contexts, our technique with slight modifications could facilitate preventive care. Further, apart from PVCs, the proposed dictionary-based method could be extended\nto other anomalous indicators such as supraventricular arrhythmias and atrial fibrillation [52]. In addition, incorporating medical professionals\u2019 feedback and adaptively learning personalized dictionaries could potentially improve both classification and compression performance levels [53]."}, {"heading": "Acknowledgment", "text": "This work was partially supported by the Department of Electronics and Information Technology (DeitY), Govt. of India, under the Cyber Physical Systems Innovation Project: 13(6)/2010- CC&BT."}], "references": [{"title": "Cardiac arrhythmia: mechanisms, diagnosis, and management", "author": ["P.J. Podrid", "P.R. Kowey"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "The ECG: a two-step approach to diagnosis", "author": ["M. Gertsch"], "venue": "Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "The prognostic significance of ventricular premature contractions in healthy people and in people with coronary heart disease., Acta cardiologica", "author": ["L. Hinkle", "S. Carver", "D. Argyros"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1974}, {"title": "Treating patients with ventricular ectopic beats, Heart", "author": ["G.A. Ng"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "How mobile devices are transforming healthcare, Issues in technology innovation", "author": ["D. West"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "The Fortune at the Bottom of the Pyramid", "author": ["C.K. Prahalad"], "venue": "Pearson Education India,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Frugal engineering: An emerging innovation paradigm", "author": ["N. Kumar", "P. Puranam"], "venue": "Ivey Business Journal", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A patientadaptable ecg beat classifier using a mixture of experts approach, IEEE transactions on biomedical engineering", "author": ["Y.H. Hu", "S. Palreddy", "W.J. Tompkins"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "A patient-adapting heartbeat classifier using ecg morphology and heartbeat interval features", "author": ["P. de Chazal", "R.B. Reilly"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Classification of electrocardiogram signals with support vector machines and particle swarm optimization", "author": ["F. Melgani", "Y. Bazi"], "venue": "IEEE Transactions on Information Technology in Biomedicine", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Block-based neural networks for personalized ecg signal classification", "author": ["W. Jiang", "S.G. Kong"], "venue": "IEEE Transactions on Neural Networks", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Automated patientspecific classification of premature ventricular contractions", "author": ["T. Ince", "S. Kiranyaz", "M. Gabbouj"], "venue": "in: 2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Heartbeat classification using disease-specific feature selection, Computers in biology and medicine", "author": ["Z. Zhang", "J. Dong", "X. Luo", "K.S. Choi", "X. Wu"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Comparison of four methods for premature ventricular contraction and normal beat clustering, in: Computers in Cardiology", "author": ["G. Bortolan", "I. Jekova", "I. Christov"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "A 2-d ecg compression method based on wavelet transform and modified spiht", "author": ["S.-C. Tai", "C. Sun", "W.-C. Yan"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Ecg signal compression using analysis by synthesis coding", "author": ["Y. Zigel", "A. Cohen", "A. Katz"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2000}, {"title": "An ecg signals compression method and its validation using nns", "author": ["C.M. Fira", "L. Goras"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "Ecg compression retaining the best natural basis k-coefficients via sparse decomposition", "author": ["A. Adamo", "G. Grossi", "R. Lanzarotti", "J. Lin"], "venue": "Biomedical Signal Processing and Control", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Optimality of klt for high-rate transform coding of gaussian vector-scale mixtures: Application to reconstruction, estimation, and classification", "author": ["S. Jana", "P. Moulin"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "Coronary Heart Disease: Clinical, Pathological, Imaging, and Molecular Profiles", "author": ["Z. Vlodaver", "R.F. Wilson", "D. Garry"], "venue": "Springer Science & Business Media,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Premature ventricular contraction-induced cardiomyopathy a treatable condition, Circulation: Arrhythmia and Electrophysiology", "author": ["Y.-M. Cha", "G.K. Lee", "K.W. Klarich", "M. Grogan"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Prognostic significance of frequent premature ventricular contractions originating from the ventricular outflow tract in patients with normal left ventricular function, Heart", "author": ["S. Niwano", "Y. Wakisaka", "H. Niwano", "H. Fukaya", "S. Kurokawa", "M. Kiryu", "Y. Hatakeyama", "T. Izumi"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Relationship between burden of premature ventricular complexes and left ventricular function, Heart Rhythm", "author": ["T.S. Baman", "D.C. Lange", "K.J. Ilg", "S.K. Gupta", "T.- Y. Liu", "C. Alguire", "W. Armstrong", "E. Good", "A. Chugh", "K. Jongnarangsin"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Pervasive healthcare and wireless health monitoring, Mobile Networks and Applications", "author": ["U. Varshney"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "We-care: an intelligent mobile telecardiology system to enable mhealth applications, IEEE journal of biomedical and health informatics", "author": ["A. Huang", "C. Chen", "K. Bian", "X. Duan", "M. Chen", "H. Gao", "C. Meng", "Q. Zheng", "Y. Zhang", "B. Jiao"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A lossless ecg data compression technique using ascii character encoding, Computers & Electrical Engineering", "author": ["S.K. Mukhopadhyay", "S. Mitra", "M. Mitra"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}, {"title": "Esc guidelines for the management of acute myocardial infarction in patients presenting with st-segment elevation, European heart journal (2012) ehs215", "author": ["P.G. Steg", "S.K. James", "D. Atar", "L.P. Badano", "C.B. Lundqvist", "M.A. Borger", "C. Di Mario", "K. Dickstein", "G. Ducrocq", "F. Fernandez-Aviles"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Private initiatives and policy options: recent health system experience in india, Health policy and planning", "author": ["B.C. Purohit"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Saberwal, In ehealth in india today, the nature of work, the challenges and the finances: an interview-based study, BMC medical informatics and decision making", "author": ["G.S. Jaros lawski"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "Physiobank, physiotoolkit, and physionet components of a new research resource for complex physiologic signals, Circulation", "author": ["A.L. Goldberger", "L.A. Amaral", "L. Glass", "J.M. Hausdorff", "P.C. Ivanov", "R.G. Mark", "J.E. Mietus", "G.B. Moody", "C.-K. Peng", "H.E. Stanley"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2000}, {"title": "On the problem of the most efficient tests of statistical hypotheses", "author": ["J. Neyman", "E.S. Pearson"], "venue": "in: Breakthroughs in Statistics,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1992}, {"title": "Bioelectrical signal processing", "author": ["L. S\u00f6rnmo", "P. Laguna"], "venue": "in cardiac and neurological applications,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2005}, {"title": "The weighted diagnostic distortion (wdd) measure for ecg signal compression", "author": ["Y. Zigel", "A. Cohen", "A. Katz"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2000}, {"title": "Sparse and Redundant Representations: From Theory to Applications in Signal and Image Processing, 1st Edition", "author": ["M. Elad"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2010}, {"title": "Efficient implementation of the k-svd algorithm using batch orthogonal matching pursuit, CS Technion", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": null, "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2008}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation, IEEE Transactions on signal processing", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2006}, {"title": "Heartbeat classification using morphological and dynamic features of ecg signals", "author": ["C. Ye", "B.V. Kumar", "M.T. Coimbra"], "venue": "IEEE Transactions on Biomedical Engineering", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2012}, {"title": "Reliable low-cost telecardiology: High-sensitivity detection of ventricular beats using dictionaries", "author": ["B.S. Chandra", "C.S. Sastry", "S. Jana"], "venue": "in: 16th IEEE Healthcom,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "A survey of cross-validation procedures for model selection, Statistics surveys", "author": ["S. Arlot", "A. Celisse"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2010}, {"title": "Asymptomatic arrhythmias in patients with symptomatic paroxysmal atrial fibrillation and paroxysmal supraventricular tachycardia., Circulation", "author": ["R.L. Page", "W.E. Wilkinson", "W.K. Clair", "E.A. Mc- Carthy", "E. Pritchett"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1994}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "in: Proceedings of the 26th annual international conference on machine learning,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Especially, for patients who have suffered myocardial infarction (MI), or developed left ventricular dysfunction (LVD), continuous monitoring has proven essential in promptly detecting sudden deterioration in cardiac functions, and hence preventing mortality [2].", "startOffset": 259, "endOffset": 262}, {"referenceID": 1, "context": "The aforementioned as well as various related conditions are associated with premature ventricular contractions (PVCs) that briefly interrupt the normal rhythm of the heart [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 2, "context": "Although PVCs occur in healthy individuals as well, high frequency of PVCs is known to foretell serious arrhythmic conditions [4], and significantly correlate with events of mortality [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Although PVCs occur in healthy individuals as well, high frequency of PVCs is known to foretell serious arrhythmic conditions [4], and significantly correlate with events of mortality [5].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "Fortunately, high penetration of mobile phones even in remote communities has mitigated such barriers in certain scenarios [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 5, "context": "In the present case, can the mobile network be leveraged to provide reliable PVC monitoring at an attractive cost to the communities living at the bottom of the economic pyramid [8]?", "startOffset": 178, "endOffset": 181}, {"referenceID": 6, "context": "In response, we take a frugal engineering approach [9], and propose an ultra-low-cost POC service.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 122, "endOffset": 130}, {"referenceID": 9, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 155, "endOffset": 159}, {"referenceID": 10, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 11, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 12, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 13, "context": "Examples include machine learning algorithms, such as mixture of experts [10], linear and quadratic discriminant analyses [11, 12], support vector machine [13], and artificial neural networks [14, 15, 16, 17, 18].", "startOffset": 192, "endOffset": 212}, {"referenceID": 14, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 15, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 141, "endOffset": 149}, {"referenceID": 16, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 198, "endOffset": 206}, {"referenceID": 17, "context": "On the other hand, reported ECG compression algorithms are based on techniques, ranging from the classical time and transform domain methods [19, 20], to the recent overcomplete dictionary learning [21, 22].", "startOffset": 198, "endOffset": 206}, {"referenceID": 18, "context": "The proposed approach, however, is different from the (symmetric) joint classification/reconstruction framework [23], where a combination of classification and (class-oblivious) reconstruction indices is minimized subject to a rate constraint.", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "In particular, monitoring PVCs, which are an early depolarization of the myocardium originating in the ventricle [3], assumes significance, even though such beats are found in subjects with as well as without structural heart diseases [4].", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "In particular, monitoring PVCs, which are an early depolarization of the myocardium originating in the ventricle [3], assumes significance, even though such beats are found in subjects with as well as without structural heart diseases [4].", "startOffset": 235, "endOffset": 238}, {"referenceID": 19, "context": "Specifically, 90% of patients experience PVCs after acute MI [24], and the risk of sudden death in such patients is related to the complexity and frequency of the PVCs.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "Recent studies also indicate the role of PVCs in inducing cardiomyopathy [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "Recommended lower threshold for the high-risk subjects, such as those with a history of MI or LVD varies between 10,000 and 20,000 in a 24-hour window [26].", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "Another recommendation sets 10% as the threshold PVC burden [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "Besides frequency of PVCs, run of two or more PVCs and their complexity could also indicate an adverse heart condition [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 23, "context": "With growing ubiquity of mobile phones in recent years, cellular network based as well as ZigBee based wireless systems have been developed [29, 30, 31].", "startOffset": 140, "endOffset": 152}, {"referenceID": 24, "context": "With growing ubiquity of mobile phones in recent years, cellular network based as well as ZigBee based wireless systems have been developed [29, 30, 31].", "startOffset": 140, "endOffset": 152}, {"referenceID": 25, "context": "In particular, a method to encode ECG signals into ASCII characters to enable communication via SMS (short message service) has been reported [32].", "startOffset": 142, "endOffset": 146}, {"referenceID": 26, "context": "Consider an individual living at the economic threshold of this target population segment, who suffered myocardial infarction in the recent past, and was successfully treated (see [34] for various treatment options).", "startOffset": 180, "endOffset": 184}, {"referenceID": 27, "context": "Such an assumption is realistic in various developing and underdeveloped countries, where free healthcare is dispensed from government-run facilities [35].", "startOffset": 150, "endOffset": 154}, {"referenceID": 28, "context": "This welfare paradigm is currently being extended even to the broader context of telemedicine [36].", "startOffset": 94, "endOffset": 98}, {"referenceID": 29, "context": "Considering a sampling rate of 360Hz and word length of 11 bits (used in MIT/BIH arrhythmia database [37]), one would generate about 1.", "startOffset": 101, "endOffset": 105}, {"referenceID": 30, "context": ", fraction of normal beats correctly detected as normal beats is maximized [40].", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": "Table 1: Relation between PRD and the diagnostic content of the ECG signal [42].", "startOffset": 75, "endOffset": 79}, {"referenceID": 31, "context": "where x and x\u0302 stand respectively for the original and the reconstructed signals [41].", "startOffset": 81, "endOffset": 85}, {"referenceID": 32, "context": "Further, from a diagnostic perspective, a PRD of no more than 9% has been found to be \u201cgood\u201d (Table 1) [42].", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "Compressive sampling (CS) recovers a high dimensional sparse vector \u03b1 \u2208 R from a few of its measurements x = \u03a6\u03b1, x \u2208 R, m < n, where \u03a6 denotes the measurement matrix [43].", "startOffset": 166, "endOffset": 170}, {"referenceID": 33, "context": "As l1 solver, we shall use orthogonal matching pursuit (OMP) in view of its simplicity, empirical effectiveness (despite its being greedy) [43], and relatively low computational complexity of O(mn) [44].", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "As l1 solver, we shall use orthogonal matching pursuit (OMP) in view of its simplicity, empirical effectiveness (despite its being greedy) [43], and relatively low computational complexity of O(mn) [44].", "startOffset": 198, "endOffset": 202}, {"referenceID": 35, "context": "Given a set of signals {xi} M i=1, KSVD obtains the dictionary D that provides the sparsest representation for each example in this set [45].", "startOffset": 136, "endOffset": 140}, {"referenceID": 34, "context": "M training data (signals) is O(mnM) [44].", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "For evaluation of various performance indices, we made use of MIT/BIH Arrhythmia database available from the PhysioBank archives [37].", "startOffset": 129, "endOffset": 133}, {"referenceID": 36, "context": "Traditionally, partitioning of database into training and test sets is performed either in a classoriented or in a subject-oriented manner [47].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "In the former, partitioning is based only on the heartbeat label, which allows significant amounts of data from the same patient to be represented in both training and test sets, resulting in overly optimistic performance estimates [18, 48].", "startOffset": 232, "endOffset": 240}, {"referenceID": 12, "context": "In contrast, the latter seeks to account for inter-subject variability, and constitutes training and test sets with beats from distinct subsets of records, leading to an overly conservative estimate of performance [16].", "startOffset": 214, "endOffset": 218}, {"referenceID": 7, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 8, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 10, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 11, "context": "More recently, a hybrid scheme called patient-specific training has been proposed [10, 11, 12, 14, 15, 17], in which a subject-oriented approach is taken with the following modification.", "startOffset": 82, "endOffset": 106}, {"referenceID": 7, "context": "Here, the training and the test sets consist of 13 records with indices in the range 100\u2013124 and 20 records with indices in the range 200\u2013234, respectively [10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Subsequently, Partition2 generalizes Partition-1 by considering all records with indices in the range 100\u2013124 for training, and those with indices in the range 200\u2013234 for testing, without paying attention to occurence of PVC beats [14].", "startOffset": 232, "endOffset": 236}, {"referenceID": 8, "context": "Finally, Partition-3 possesses the property that training and test sets enjoy approximately equal representation from the rival classes of beats [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "Recall that the adopted MIT/BIH Arrhythmia database consists of 30-minute excerpts of two channel ambulatory ECG recordings of 48 subjects [37].", "startOffset": 139, "endOffset": 143}, {"referenceID": 8, "context": "First, the baseline wander was removed using two median filters of respective window sizes 200ms and 600ms in a sequential manner [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "Next the annotated R-peak location in each beat was noted, and 150 samples before, 150 samples after and the Rpeak sample were collected in a vector of length 301 [21].", "startOffset": 163, "endOffset": 167}, {"referenceID": 37, "context": "However, sparsity saturates with increasing column size, and has negligible effect on classification performance beyond certain threshold [50].", "startOffset": 138, "endOffset": 142}, {"referenceID": 35, "context": "Empirically, \u201cgood\u201d overcomplete dictionaries have been shown to possess a ratio of column to row size between approximately 2 and 5 [45].", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "To this end, we made use of MIT/BIH Arrhythmia database [37], adopted patient specific partitioning, evaluated the performance of our dictionary based method according to Proposal-1, Proposal-2 and Proposal3, and compared with the performance of known algorithms, when relevant.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": ", 1997 [10] 82.", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": ", 2006 [11] 94.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": ", 2007 [14] 86.", "startOffset": 7, "endOffset": 11}, {"referenceID": 11, "context": ", 2009 [15] 84.", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "preciate this, note that certain algorithms achieve high compression ratio for a signal consisting of several beats by stacking such beats before compressing [19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Further, specific fixed partitioning is sometimes chosen so as to maximize reported compression ratio [21, 22].", "startOffset": 102, "endOffset": 110}, {"referenceID": 17, "context": "Further, specific fixed partitioning is sometimes chosen so as to maximize reported compression ratio [21, 22].", "startOffset": 102, "endOffset": 110}, {"referenceID": 8, "context": "[11], requires 36.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "We demonstrated the efficacy of our method using Monte Carlo cross validation on the MIT/BIH arrhythmia database [37, 51].", "startOffset": 113, "endOffset": 121}, {"referenceID": 38, "context": "We demonstrated the efficacy of our method using Monte Carlo cross validation on the MIT/BIH arrhythmia database [37, 51].", "startOffset": 113, "endOffset": 121}, {"referenceID": 32, "context": "From the medical professionals\u2019 perspective, the inference has to be made from the electronic records at essentially the same quality (PRD \u2264 9%, from Table 1 [42]) as the gold (quality) standard of unprocessed signals.", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "Specifically, high PVC burden could presage adverse heart conditions even in individuals without prior structural heart disease [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 39, "context": "Further, apart from PVCs, the proposed dictionary-based method could be extended to other anomalous indicators such as supraventricular arrhythmias and atrial fibrillation [52].", "startOffset": 172, "endOffset": 176}, {"referenceID": 40, "context": "In addition, incorporating medical professionals\u2019 feedback and adaptively learning personalized dictionaries could potentially improve both classification and compression performance levels [53].", "startOffset": 190, "endOffset": 194}], "year": 2017, "abstractText": "While cardiovascular diseases (CVDs) are prevalent across economic strata, the economically disadvantaged population is disproportionately affected due to the high cost of traditional CVD management, involving consultations, testing and monitoring at medical facilities. Accordingly, developing an ultra-low-cost alternative, affordable even to groups at the bottom of the economic pyramid, has emerged as a societal imperative. Against this backdrop, we propose an inexpensive yet accurate home-based electrocardiogram (ECG) monitoring service. Specifically, we seek to provide point-of-care monitoring of premature ventricular contractions (PVCs), high frequency of which could indicate the onset of potentially fatal arrhythmia. Note that a traditional telecardiology system acquires the ECG, transmits it to a professional diagnostic center without processing, and nearly achieves the diagnostic accuracy of a bedside setup, albeit at high bandwidth cost. In this context, we aim at reducing cost without significantly sacrificing reliability. To this end, we develop a dictionary-based algorithm that detects with high sensitivity the anomalous beats only which are then transmitted. We further compress those transmitted beats using class-specific dictionaries subject to suitable reconstruction/diagnostic fidelity. Such a scheme would not only reduce the overall bandwidth requirement, but also localizing anomalous beats, thereby reducing physicians\u2019 burden. Finally, using Monte Carlo cross validation on MIT/BIH arrhythmia database, we evaluate the performance of the proposed system. In particular, with a sensitivity target of at most one undetected PVC in one hundred beats, and a percentage root mean squared difference less than 9% (a clinically acceptable level of fidelity), we achieved about 99.15% reduction in bandwidth cost, equivalent to 118-fold savings over traditional telecardiology. In the process, our algorithm outperforms known algorithms under various measures in the telecardiological", "creator": "LaTeX with hyperref package"}}}