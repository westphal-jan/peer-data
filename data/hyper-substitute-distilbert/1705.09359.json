{"id": "1705.09359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Time-Based Label Refinements to Discover More Precise Process Models", "abstract": "subject mining is a research field focused on the analysis of active records with the aim and producing memories responsive to activity measurement. applying process alignment criteria upon behaviors from smart household environments assumes the aim to stimulate valuable insights using ( un ) healthy actions and to apply to ambient human living solutions. finding the right stimuli labels both enable reactive application without process modeling alternatives but however far at trivial, as simply lacking the triggering sensor combination the handler nor sensor events results in passive models unfortunately allow analyzing too much behavior ( overgeneralizing ). barriers to sensor level associated labels suggested effective sensors maker have been shown to possess high potential potential precise for insightful process models. however, exists being no standard approach to generate mapping of system labels despite its context of process mining. under this case we propose a framework proposing the automated generation of label refinements based on the time attribute of constraints, supporting us to distinguish those different instances of the same physical type based on their time attribute. we started on a case study with real life smart home event data each has automatically generated refined strategies in process discovery, we can find more specific, and enhance individually insightful, effect models. hirsch concludes that one label content levels have an effect on user usefulness & other label phrases when used together. conversely, we follow ineffective strategies in generate useful lists or multiple label refinements nor evaluate those affecting even another life smart home event logs.", "histories": [["v1", "Thu, 25 May 2017 21:01:20 GMT  (1944kb)", "http://arxiv.org/abs/1705.09359v1", null], ["v2", "Tue, 31 Oct 2017 16:22:43 GMT  (5445kb,D)", "http://arxiv.org/abs/1705.09359v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.DB", "authors": ["niek tax", "emin alasgarov", "natalia sidorova", "wil m p van der aalst", "reinder haakma"], "accepted": false, "id": "1705.09359"}, "pdf": {"name": "1705.09359.pdf", "metadata": {"source": "CRF", "title": "Time-Based Label Refinements to Discover More Precise Process Models", "authors": ["Niek Tax", "Reinder Haakma"], "emails": ["n.tax@tue.nl", "n.sidorova@tue.nl", "w.m.p.v.d.aalst@tue.nl", "r.haakma@philips.com", "ealasgarov@bol.com"], "sections": [{"heading": null, "text": "ar X\nof extracting insights related to dynamic behavior. Applying process mining techniques on data from smart home environments has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions. Finding the right event labels to enable the application of process mining techniques is however far from trivial, as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior (overgeneralizing). Refinements of sensor level event labels suggested by domain experts have been shown to enable discovery of more precise and insightful process models. However, there exists no automated approach to generate refinements of event labels in the context of process mining. In this paper we propose a framework for the automated generation of label refinements based on the time attribute of events, allowing us to distinguish behaviourally different instances of the same event type based on their time attribute. We show on a case study with real life smart home event data that using automatically generated refined labels in process discovery, we can find more specific, and therefore more insightful, process models. We observe that one label refinement could have an effect on the usefulness of other label refinements when used together. Therefore, we explore four strategies to generate useful combinations of multiple label refinements and evaluate those on three real life smart home event logs.\nCCorresponding author"}, {"heading": "1. Introduction", "text": "Process mining is a fast growing discipline that combines knowledge and techniques from data mining, process modeling, and process model analysis [1]. Process mining techniques analyze events that are logged during process execution. Today, such event logs are readily available and contain information on what was done, by whom, for whom, where, when, etc. Events can be grouped into cases (process instances), e.g. per patient for a hospital log, or per insurance claim for an insurance company. Process discovery plays an important role in process mining, focusing on extracting interpretable models of processes from event logs. One of the attributes of the events is usually used as its label and its values become transition/activity labels in the process models generated by process discovery algorithms.\nThe scope of process mining have broadened in recent years from business process management to other application domains, one of them being the analysis of events of human behavior with data originating from sensors in smart home environments [2, 3, 4]. Table 1 shows an example of such an event log. Events in the event log are generated by e.g. motion sensors placed in the home, power sensors placed on appliances, open/close sensors placed on closets and cabinets, etc. Particularly challenging in applying process mining in this application domain is the extraction of meaningful event labels that allow for the discovery of insightful process models. Simply using the sensor that generates an event (the sensor column in Table 1) as event label is shown to produce non-informative process models that overgeneralize the event log and allow for too much behavior [3]. Abstracting sensor-level events into events at the level of human activity (e.g. eating, sleeping, etc.) using techniques closely related to techniques used in the activity recognition field helps to discover more behaviorally constrained and more insightful process models [4], but applicability of this approach relies on the availability of a reliable diary of human behavior at the activity level, which is often just impossible to obtain. Alternatively, insight in the human routines can be obtained through discovery of Local Process Models [5], which bridges process mining and sequential pattern mining by finding patterns that include high-level process model constructs such as (exclusive) choices, loops, and concurrency. However, Local Process Models, as opposed to process discovery, only give insight in frequent subroutines of behavior and do not provide the global picture of the behavior throughout the day from start to end.\nIn our earlier work [3] we showed that better process models can be discovered by taking the name of the sensor that generated the event as a starting point for the event label and then refining these labels using information on the time within the day at which the event occurred. The refinements used in [3] were based on domain knowledge, and not identified automatically from the data. In this paper, we aim at the automatic generation of semantically interpretable label refinements that can be explained to the user, by basing label refinements on data attributes of events. We explore methods to bring parts of the timestamp information to the event label in an intelligent and fully automated way, with the end goal of discovering behaviorally more precise and therefore more insightful process models.\nWe start by introducing basic concepts and notations used in this paper in Section 2. In Section 3, we introduce a framework for the generation of event labels refinements based on the time attribute. In Section 4, we apply this framework on a real life smart home data set and show the effect of the refined event labels on process discovery. In Section 5 we address the case of applying multiple label refinements together. We continue by describing related work in Section 6 and conclude in Section 7."}, {"heading": "2. Preliminaries", "text": "In this section we introduce basic notions related to event logs and relabeling functions for traces and then define the notions of refinements and abstractions. We also introduce some Petri net basics.\nWe use the usual sequence definition, and denote a sequence by listing its elements, e.g. we write \u3008a1, a2, . . . , an\u3009 for a (finite) sequence s : {1, . . . , n} \u2192 A of elements from some alphabet A, where s(i) = ai for any i \u2208 {1, . . . , n}; |s| denotes the length of sequence s; s1s2 denotes the concatenation of sequences s1 and s2. A language L over an alphabet A is a set of sequences over A.\nAn event is the most elementary element of an event log. Let I be a set of event identifiers, T be the time domain, and A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 An be an attribute domain consisting of n attributes (e.g. resource, activity name, cost, etc.). An event is a tuple e = (i, at, a1, . . . , an), with i \u2208 I , at \u2208 T , and (a1, . . . , an) \u2208 A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7An. The event label of an event is the attribute set (a1, . . . , an). Functions id(e), label (e), and time(e) respectively return the id, the event label and the timestamp of event e. E = I \u00d7 T \u00d7A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7An is a universe of events over A1, . . . ,An. The rows of Table 1 are events from an event universe over the event attributes sensor, address, and sensor value.\nEvents are often considered in the context of other events. We call E \u2286 E an event set if E does not contain any events with the same event identifier. The events in Table 1 together form an event set. A trace \u03c3 is a finite sequence formed by the events from an event set E \u2286 E that respects the time ordering of events, i.e. for all k,m \u2208 N, 1 \u2264 k < m \u2264 |E|, we have: time(\u03c3(k)) \u2264 time(\u03c3(m)). We define the universe of traces over event universe E , denoted \u03a3(E), as the set of all possible traces over E . We omit E in \u03a3(E) and use the shorter notation \u03a3 when the event universe is clear from the context.\nOften it is useful to partition an event set into smaller sets in which events belong together according to some criterion. We might e.g. be interested in discovering the typical behavior within a household over the course of a day. In order to do so, we can e.g. group together events with the same address and the same day-part of the timestamp, as indicated by the horizontal lines in Table 1. For each of these event sets, we can construct a trace; timestamps define the ordering of events within the trace. For events of a trace having the same timestamps, an arbitrary ordering can be chosen within a trace.\nAn event partitioning function is a function ep : E \u2192 Tid that defines the partitioning of an arbitrary set of events E \u2286 E from a given event universe E into event sets E1, . . . , Ej , . . . where each\nEj is the maximal subset of E such that for any e1, e2 \u2208 Ej , ep(e1) = ep(e2); the value of ep shared by all the elements of Ej defines the value of the trace attribute Tid. Note that multidimensional trace attributes are also possible, i.e. a combination of the name of the person performing the event activity and the date of the event, so that every trace contains activities of one person during one day. The event sets obtained by applying an event partitioning can be transformed into traces (respecting the time ordering of events).\nAn event log L is a finite set of traces L \u2286 \u03a3(E) such that \u2200\u03c3 \u2208 L : \u2200e1, e2 \u2208 \u03c3 : ep(e1) = ep(e2). AL \u2286 A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 An denotes the alphabet of event labels that occur in log L. The traces of a log are often transformed before doing further analysis: very detailed but not necessarily informative event descriptions are transformed into some coarse-grained and interpretable labels. For the labels of the log in Table 1, the sensor values could be abstracted to on and off, or labels can be redefined to a subset of the event attributes, e.g. leaving the sensor values out completely.\nAfter this relabeling step, some traces of the log can become identically labeled (the event id\u2019s would still be different). The information about the number of occurrences of a sequence of labels in an event log is highly relevant for process mining, since it allows differentiating between the mainstream behavior of a process (frequently occurring behavioral patterns) and exceptional behavior.\nLet E1, E2 be event universes. A function l : E1 \u2192 E2 is an event relabeling function when it satisfies id(e) = id(l(e)) and time(e) = time(l(e)). A relabeling function can be used to obtain more useful event labels than the full set of event attribute values. We lift l to event logs. Let E , E1, E2 be event universes with E , E1, E2 being pairwise different. Let l1 : E \u2192 E1 and l2 : E \u2192 E2 be event relabeling functions. Relabeling function l1 is a refinement of relabeling function l2, denoted by l1 l2, iff \u2200e1,e2\u2208E : label(l1(e1)) = label (l1(e2)) =\u21d2 label (l2(e1)) = label(l2(e2)); l2 is then called an abstraction of l1.\nThe goal of process discovery is to discover a process model that represents the behavior seen in an event log. A frequently used process modeling notation in the process mining field is the Petri net notation [6]. Petri nets are directed bipartite graphs consisting of transitions and places, connected by arcs. Transitions represent activities, while places represent the enabling conditions of transitions. Labels are assigned to transitions to indicate the type of activity that they model. A special label \u03c4 is used to represent invisible transitions, which are only used for routing purposes and not recorded in the log.\nA labeled Petri net N = \u3008P, T, F,AM , \u2113\u3009 is a tuple where P is a finite set of places, T is a finite set of transitions such that P \u2229T = \u2205, F \u2286 (P \u00d7T )\u222a (T \u00d7P ) is a set of directed arcs, called the flow relation, AM is an alphabet of labels representing activities, with \u03c4 /\u2208 AM being a label representing invisible events, and \u2113 : T \u2192 AM \u222a {\u03c4} is a labeling function that assigns a label to each transition. For a node n \u2208 (P \u222a T ) we use \u2022n and n\u2022 to denote the set of input and output nodes of n, defined as \u2022n = {n|(n, n\u2032) \u2208 F} and n\u2022 = {n\u2032|(n, n\u2032) \u2208 F}. An example of a Petri net can be seen in Figure 1, where circles represent places and squares represent transitions. Gray transitions having a smaller width represent \u03c4 transitions.\nA state of a Petri net is defined by its marking M \u2208 NP being a multiset of places. A marking is graphically denoted by putting M(p) tokens on each place p \u2208 P . A pair (N,M) is called a marked Petri net. State changes occur through transition firings. A transition t is enabled (can fire) in a given marking M if each input place p \u2208 \u2022t contains at least one token. Once a transition fires, one token is removed from each input place of t and one token is added to each output\nplace of t, leading to a new marking. An accepting Petri net is a 3-tuple (N,Mi,Mf ) with N a labeled Petri net, Mi an initial marking, and Mf a set of final markings. Many process modeling notations, including accepting Petri nets, have formal executional semantics and a model defines a language of accepting traces L. For the Petri net in Figure 1, the language of accepting traces is {\u3008A,B,D,E, F \u3009, \u3008A,B,D,F,E\u3009, \u3008A,C,D,E, F \u3009, \u3008A,C,D,F,E\u3009}."}, {"heading": "3. A Framework for Time-based Label Refinements", "text": "In this section we describe a framework to create an event label containing partial information about the event timestamp, in order to make the event labels more specific while remaining interpretable. We take a clustering based approach by identifying dense areas in time space for each label. The time part of the timestamps consists of values between 00:00:00 and 23:59:59, equivalent to the timestamp attribute from Table 1 with the day-part of the timestamp removed. This timestamp can be transformed into a real number time representation in interval [0, 24). We chose to apply soft clustering (also referred to as fuzzy clustering), which has the benefit of assigning to each data point a likelihood of belonging to each cluster. A well-known approach to soft clustering is based on the combination of the Expectation-Maximization (EM) algorithm with mixture models, which are probability distributions consisting of multiple components of the same probability distribution. Each component in the mixture represents one cluster and the probability of a data point belonging to that cluster is the probability that this cluster generated that data point. The EM algorithm is used to obtain a maximum likelihood estimate of the mixture model parameters, i.e. the parameters of the probability distributions in the mixture.\nA well-known type of mixture model is the Gaussian Mixture Model (GMM), where the components in the mixture distributions are normal distributions. The data space of time is, however, non-Euclidean: it has a circular nature, e.g. 23.99 is closer to 0 than to 23. This circular nature of the data space introduces problems for GMMs, as illustrated by the example in Figure 2. The GMM fitted to the timestamps of the sensor events consists of two components, one with the mean at 9.05 and one with a mean at 20. The histogram representation of the same data shows that some events happened just after midnight, which on the clock is actually closer to 20 than to 9.05. The GMM however is unaware of the circularity of the clock, which results in a mixture model that seems inappropriate when visually comparing with the histogram. The standard deviation of the mixture component with a mean at 9.05 is much higher than one would expect based on the histogram as a result of the mixture model trying to explain the data points that occurred just after midnight. The field of circular statistics\n(also referred to as directional statistics), concerns analysis of such circular data spaces (cf. [7]). In this paper we use a mixture of von Mises distributions to capture the daily patterns.\nHere, we introduce a framework for generating refinements of event labels based on time attributes\nusing techniques from the field of circular statistics. This framework consists of three stages:\nData-model pre-fitting stage A known problem with many clustering techniques is that they return\nclusters even when the data should not be clustered. In this stage we assess how many clusters the events of a sensor type contain.\nData-model fitting stage In this stage we cluster the events of a sensor type by timestamp using a\nmixture consisting of components that take into account the circularity of the data.\nData-model post-fitting stage In this stage the quality of the label refinements is assessed from both\na cluster quality perspective and a process model (event ordering statistics) perspective."}, {"heading": "3.1. Data-model pre-fitting stage", "text": "We now describe a test for uniformity, a test for unimodality, and a method to select the number of clusters in the data.\nUniformity Check Rao\u2019s spacing test [8] tests the uniformity of the timestamps of the events from a sensor around the circular clock. This test is based on the idea that uniform circular data is distributed evenly around the circle, and n observations are separated from each other 2\u03c0 n radii. The null hypothesis is that the data is uniform around the circle.\nGiven n successive observations f1, . . . , fn, either clockwise or counterclockwise, the test statistics U for Rao\u2019s Spacing Test is defined as U = 12 \u2211n i=1 | Ti \u2212 \u03bb |, where \u03bb = 2\u03c0 n , Ti = fi+1 \u2212 fi for 1 \u2264 i \u2264 n\u2212 1 and Tn = (2\u03c0 \u2212 fn) + f1. Unimodality Check Hartigan\u2019s dip tests [9] the null hypothesis that the data follows a unimodal distribution on a circle. When the null hypothesis can be rejected, we know that the distribution of the data is at least bimodal. Hartigan\u2019s dip test measures the maximum difference between the empirical distribution function and the unimodal distribution function that minimizes that maximum difference.\nNumber of Component Selection The Bayesian Information Criterion (BIC) [10] introduces a penalty for the number of model parameters to the evaluation of a mixture model. Adding a component to a mixture model increases the number of parameters of the mixture with the number of parameters of the distribution of the added component. The likelihood of the data given the model can only increase by adding extra components, adding the BIC penalty results in a trade-off between number of components and the likelihood of the data given the mixture model. BIC is formally defined as BIC = \u22122 \u2217 lnL\u0302+ k \u2217 ln(n), where L\u0302 is a maximized value for the data likelihood, n is the sample size, and k is the number of parameters to be estimated. A lower BIC value indicates a better model. We start with 1 component, and iteratively increase from k to k+1 components as long as the decrease in BIC is larger than 10, which is the threshold for decisive evidence of high BIC [11]."}, {"heading": "3.2. Data-model fitting stage", "text": "A generic approach to estimate a probability distribution from data that lies on a circle or any other type of manifold (e.g. the torus and sphere) was proposed by Cohen and Welling in [12]. However, their approach estimates the probability distribution on a manifold in an non-parametric manner and it does not use multiple probability distribution components, making it unsuitable as a basis for clustering.\nWe cluster events generated by one sensor using a mixture model consisting of components of the von Mises distribution, which is the circular equivalent of the normal distribution. This technique is based on the approach of Banerjee et al. [13] that introduces a clustering method based on a mixture of von Mises-Fisher distribution components, which is a generalization of the 2-dimensional von Mises distribution to n-dimensional spheres. A probability density function for a von Mises distribution with mean direction \u00b5 and concentration parameter \u03ba is defined as pdf(\u03b8 | \u00b5, \u03ba) =\n1 2\u03c0I0(\u03ba) e\u03ba cos(\u03b8\u2212\u00b5), where mean \u00b5 and data point \u03b8 are expressed in radians on the circle, such that 0 \u2264 \u03b8 \u2264 2\u03c0, 0 \u2264 \u00b5 \u2264 2\u03c0, \u03ba \u2265 0. I0 represents the modified Bessel function of order 0, defined as I0(k) = 1 2\u03c0 \u222b 2\u03c0 0 e\n\u03ba cos(\u03b8)d\u03b8. As \u03ba approaches 0, the distribution becomes uniform around the circle. As \u03ba increases, the distribution becomes relatively concentrated around the mean \u00b5 and the von Mises distribution starts to approximate a normal distribution. We fit a mixture model of von Mises components using the package movMF [14] provided in R."}, {"heading": "3.3. Data-model post-fitting stage", "text": "After fitting a mixture of von Mises distributions to the sensor events, we perform a goodness-offit test to check whether the data could have been generated from this distribution. We describe the Watson U2 statistic [15], a goodness-of-fit assessment based on hypothesis testing. The Watson U2 statistic measures the discrepancy between the cumulative distribution function F (\u03b8) and the empirical distribution function Fn(\u03b8) of some sample \u03b8 drawn from some population and is defined as U 2 = n \u222b 2\u03c0 0 [ Fn(\u03b8)\u2212 F (\u03b8)\u2212 \u222b 2\u03c0 0 { Fn(\u03c6)\u2212 F (\u03c6) } dF (\u03c6) ]2 dF (\u03b8).\nThe clustering obtained can be used as a label refinement where we refine the original event label into a new label for each cluster. We assess the quality of this label refinement from a process perspective using the label refinement evaluation method described in [3]. This method tests whether the log statistics that are used internally in many process discovery algorithms become significantly\nmore deterministic by applying the label refinement. Hence, we test whether the models become more precise after time-based label refinement. An example of such a log statistic is the direct successor statistic: #+L,>(b, c) is the number of occurrences of b in the traces of L that are directly followed by c, i.e. in some \u03c3 \u2208 L, i \u2208 {1, . . . , |\u03c3|} we have label([\u03c3(i)]) = b and label([\u03c3(i + 1)]) = c, likewise, #\u2212L,>(b, c) is the number of occurrences of b which are not directly followed by c. This control-flow test [3] outputs a p-value that indicates whether such log statistics of refined activities a1, a2, . . . of some activity a change with statistical significance. When #+L,>(b, c) = # \u2212\nL,>(b, c) the entropy of b being directly followed by c is 1 bit, equal to a coin toss. In addition to the p-value, the test returns an information gain, which indicates the ratio of the decrease in the total bits of entropy in the log statistics as a result of applying the label refinement. Information gain can be used as a selection criterion for label refinements when there are multiple activities that can be refined according to the three steps of this framework. While the entropy of a single log statistic cannot increase by applying a label refinement, the information gain of a refinement can still be negative when it is not useful, as it increases the number activities in the log and therefore also increases the total number of log statistics."}, {"heading": "4. Case Study", "text": "We apply our time-based label refinements approach to the real life smart home data set described in Van Kasteren et al. [16]. The Van Kasteren data set consists of 1285 events divided over fourteen different sensors. We segment in days from midnight to midnight to define cases. Figure 4a shows the process model discovered on this event log with the Inductive Miner infrequent [17] with 20% filtering, which discovers a process model that describes the most frequent 80% of behavior in the log. Note that this process model overgeneralizes, i.e., it allows for too much behavior. At the beginning a (possibly repeated) choice is made between five transitions. At the end of the process, the model allows any sequence over the alphabet of five activities, where each activity occurs at least once.\nWe illustrate the framework by applying it to the bedroom door sensor. Rao\u2019s spacing test results in a test statistic of 241.0 with 152.5 being the critical value for significance level 0.01, indicating that we can reject the null hypothesis of a uniformly distributed set of bedroom door timestamps. Hartigan\u2019s dip test results in a p-value of 3.95\u00d710\u22124, indicating that we can reject the null hypothesis\nthat there is only one cluster in the bedroom door data. Figure 3 shows the BIC values for different numbers of components in the model. The figure indicates that there are two clusters in the data, as this corresponds to the lowest BIC value. Table 2 shows the mean and \u03ba parameters of the two clusters found by optimizing the von Mises mixture model with the EM algorithm. A value of 0 \u2261 2\u03c0 radii equals midnight. After applying the von Mises mixture model to the bedroom door events and assigning each event to the maximum likelihood cluster we obtain a time range of [3.08-10.44] for cluster 1 and a time range of [17.06-0.88] for cluster 2. The Watson U2 test results in a test statistic of 0.368 and 0.392 for cluster 1 and 2 respectively with a critical value of 0.141 for a 0.01 significance level, indicating that the data is likely to be generated by the two von Mises distributions found. The label refinement evaluation method [3] finds statistically significant differences between the events from the two bedroom door clusters with regard to their control-flow relations with other activities in the log for 10 other activities using the significance level of 0.01, indicating that the two clusters are different from a control-flow perspective. Figure 4b shows the process model discovered with the Inductive Miner infrequent with 20% filtering after applying this label refinement to the Van Kasteren event log. The process model still overgeneralizes the overall process, but the label refinement does help restricting the behavior, as it shows that the evening bedroom door events are succeeded by one or more events of type groceries cupboard, freezer, cups cupboard, fridge, plates cupboard, or pans\ncupboard, while the morning bedroom door events are followed by one or more frontdoor events. It seems that this person generally goes to the bedroom in-between coming home from work and starting to cook. The loop of the frontdoor events could be caused by the person leaving the house in the morning for work, resulting in no logged events until the person comes home again by opening the frontdoor. Note that in Figure 4a bedroom door and frontdoor events can occur an arbitrary number of times in any order. Figure 4a furthermore does not allow for the bedroom door to occur before the whole block of kitchen-located events at the beginning of the net. In the process mining field multiple quality criteria exist to express the fit between a process model and an event log. Two of those criteria are fitness [18], which measures the degree to which the behavior that is observed in the event log can be replayed on the process model, and precision [19], which measures the degree to which the behavior that was never observed in the event log cannot be replayed on the process model. Low precision typically is indicates an overly general process model, that allows for too much behavior. Typically we aim for process models with both high fitness and precision, therefore one can consider the harmonic mean of the two, often referred to as F-score. The bedroom door label refinement described above improves the precision of the process model found with the Inductive Miner infrequent (20% filtering) [17] from 0.3577 when applied on the original event log to 0.4447 when applied on the refined event log and improves the F-score from 0.5245 to 0.6156.\nThe label refinement framework allows for refinement of multiple activities in the same log. For example, label refinements can be applied iteratively. Figure 5 shows the effect of a second label refinement step, where Plates cupboard using the same methodology is refined into two labels, representing time ranges [7.98-14.02] and [16.05-0.92] respectively. This refinement shows the additional insight that the evening version of the Plates cupboard occurs in directly before or after the microwave. Generating multiple label refinements, however, comes with the problem that the control-flow test [3] is sensitive to the order in which label refinements are applied. Because labels refinements change the event log, it is possible that after applying some label refinement A, some other label refinement B starts passing the control-flow test that did not pass this test before, or fails the test while it passed before. Additionally, applying one label refinement can change the information gain of applying another label refinement afterwards. For example, when#+L,>(b, c) = # \u2212\nL,>(b, c), i.e., b is followed by c 50% of the time, the entropy of this log statistic is 1, equal to a coin toss. Some label refinement A which refines b into b1, b2 where b1 is always followed by c and b2 is never followed by c is a good label refinement from an information gain point of view, as it decreases the entropy of the log statistic to zero. Some other label refinement B, which refines c into c1, c2 such that all b\u2019s are directly followed\nby c1\u2019s and never by c2\u2019s also leads to information gain. However, applying refinement B after having already applied refinement A before does not lead to any further information gain, since refinement A already made it deterministic whether or not b is followed by any c. Ineffective label refinements might even harm process discovery, as each refinement decreases the frequencies with which activities are observed, thereby decreasing the amount of evidence for certain control-flow relations."}, {"heading": "5. Ordering of Label Refinements", "text": "In this section we explore the effect of the ordering of label refinements on real life event logs. We explore four strategies for applying multiple label refinements:\nAll-at-once In this strategy we ignore the influence of ordering of label refinements on the outcome\nof the control flow test and select the best label refinements in a single step based on their information gain that would have been obtained when they would have been applied to the original event log.\nGreedy Search We iteratively apply the best label refinement in terms of information gain and calcu-\nlate the information gain of the next label refinement on the log to which all label refinements of earlier steps have been applied.\nExhaustive Search This strategy exhaustively tries all combinations of label refinements and searches\nfor the label refinement combinations that jointly lead to the largest information gain. While the label refinement combinations that are found with this strategy are optimal in terms of information gain, this strategy can quickly become computationally intractable for event logs that contain many activities that can be refined.\nBeam Search In Beam Search only a predetermined number b (called the beam size) of best partial solutions are kept as candidates, i.e., the best b combinations in terms of information gain of n label refinements are explored to search for a new set of n + 1 label refinements. This is an intermediate strategy in-between greedy and exhaustive search, with beam search with b = 1 being greedy search and b = \u221e being exhaustive search.\nWe apply these four strategies on three event logs from the human behavior domain and measure the fitness, precision, and F-score of the model discovered with the Inductive Miner infrequent [17] with 20% filtering after each label refinement. The first event log is the Van Kasteren [16] event log which we introduced in Section 4. The other two event logs are two different households of a smart home experiment conducted by MIT [20]. The log Household A of the MIT experiment contains 2701 events spread over 16 days, with 26 different sensors. The Household B log contains 1962 events spread over 17 days and 20 different sensors. Figure 6 shows the results. On all three event logs the precision can be improved considerably through label refinements. Note that when applying only one label refinement all four strategies are identical. When refining a second label the four strategies all select the same label refinement on all three logs, therefore the F-score, fitness, and precision for two refined labels happen to be identical. Figure 6 shows that there are 7 activities that can be refined for MIT household A, 10 for MIT household B and 8 for Van Kasteren. However, since the F-score for all strategies drops again after a few label refinements, not all of those label refinements lead to\nbetter process models. The four strategies perform very similar in terms of F-score. Exhaustive search outperforms the other strategies for a few refinements on some logs, however, such improvements come with considerable computation times. On the MIT household B log, which has 10 possible label refinements, it takes about 25 minutes on an Intel i7 processor to evaluate all possible combinations of refinements. On logs with even more possible refinements the exhaustive strategy can quickly become computationally infeasible. The all-at-once strategy, which is computationally very fast and only takes milliseconds to compute, shows almost identical performance for MIT household A and Van Kasteren. When making six or more refinements on the MIT household B log, the performance of the all-at-once strategy lags behind the other strategies, indicating that the label refinements that were applied earlier cause the later label refinements to be less effective. However, the optimum in F-score for this log lies at three refinements, therefore the sixth refinement where the performance difference starts occurring should not be made with any of the strategies in the first place.\nSince the F-score decreases again when applying too many label refinements it is important to have a stopping criterion that prevents refining the event log too much. The dashed line in Figure 6 shows the results when we only refine a label when the information gain of the refinement is larger than zero. On the MIT households A and B logs this stopping criterion causes all strategies to stop at the best combination of label refinements in F-score, which consists of one and three refinements respectively.\nAll strategies except the exhaustive search strategy suggest a fourth refinement for MIT B that decreases the F-score sharply, to increase it again with a fifth refinement. This is caused by an unhelpful refinement being found as fourth refinement by those strategies, which causes the frequencies and therefore the follows statistics to drop below the filtering threshold of the Inductive Miner, leading to a more flower like model. At the fifth refinement the follows statistics of other activities drop as well,\ncausing the follows statistics that dropped in the fourth refinement to be relatively higher and above the threshold again. On the Van Kasteren log the optimum in F-score is to make only one refinement, although the F-score after applying the second and third refinement as found by the exhaustive and beam search is almost identical. The all-at-once strategy stops after applying only two refinements while the other strategies apply a third refinement. The best refinement combination found with the all-at-once strategy using the stopping criterion is identical to the what is found with the other strategies, suggesting that in practice the differences between the four approaches are small. On real life smart home environment event logs the effect that one label refinement influences the control flow test outcome of others is limited."}, {"heading": "6. Related Work", "text": "Refining event labels in the event log is closely related to the task of mining process models with duplicate activities, in which the resulting process model can contain multiple transitions/nodes with the same label. From the point of view of the behavior allowed by a process model, it makes no difference whether a process model is discovered on an event log with refined labels, or whether a process model is discovered with duplicate activities such that each transition/node of the duplicate activity precisely covers one versions of the refined label. However, a refined label may also provide additional insights as the new labels are explainable in terms of time. The first process discovery algorithm capable of discovering duplicate tasks was proposed by Herbst and Karagiannis in 2004 [21], after which many others have been proposed, including the Evolutionary Tree Miner [22], the \u03b1\u2217-algorithm [23], the \u03b1#-algorithm [24], the EnhancedWFMiner [25]. An alternative approach has been proposed by Va\u0301zques-Barreiros [26] et al., who describe a local search based approach to repair a process model to include duplicate activities, starting from an event log and a process model without duplicate activities. Existing work on mining models with duplicate activities all base their duplicate activities on how well the event log fits the process model, and do not try to find semantic differences between the different versions of the activities in the form of attribute differences.\nThe work that is closest to our work is the work by Lu et al. [27], who describe an approach to pre-process an event log by refining event labels with the goal of discovering a process model with duplicate activities. The method proposed by Lu et al., however, does not base the relabelings on data attributes of those events and only uses the control flow context, leaving uncertainty whether two events relabeled differently are actually semantically different.\nAnother area of related work is data-aware process mining, where the aim is to discover rules with regard to data attributes of events that decide decision points in the process. De Leoni and van der Aalst [28] proposed a method that discovers data guards for decision points in the process based on alignments and decision tree learning. This approach relies on the discovery of a behaviorally well-fitting process model from the original event log. When only overgeneralizing process models (i.e. allowing for too much behavior) can be discovered from an event log, the correct decision points might not be present in the discovered process model at all, resulting in this approach not being able to discover the data dependencies that are in the event log. Our label refinements use data attributes prior to process discovery to enable the discover of more behaviorally constrained process models by bringing parts of the event attribute space to the event label."}, {"heading": "7. Conclusion & Future Work", "text": "We have proposed a framework based on techniques from the field of circular statistics to refine event labels automatically based on their timestamp attribute. We have shown through a proof of concept on a real life event log that this framework can be used to discover label refinements that allow for the discovery of more insightful and behaviorally more specific process models. Additionally, we explored four strategies to search combinations of label refinements. We found that the difference between an all-at-once strategy, which ignores that one label refinement can have an effect on the usefulness of other label refinements, and other more computationally expensive strategies is often limited. An interesting area of future work is to explore the use of other types of event data attributes to refine labels, e.g. power values of sensors. A next research step would be to explore label refinements based on a combination of data attributes combined. This introduces new challenges, such as the clustering on partially circular and partially Euclidean data spaces. Additionally, other time-based types of circles than the daily circle described in this paper, such as the week, month, or year circle, are worth investigating."}], "references": [{"title": "Process mining: data science in action", "author": ["van der Aalst WMP"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Discovery of personal processes from labeled sensor data \u2013 an application of process mining to personalized health care", "author": ["T Sztyler", "J V\u00f6lker", "J Carmona", "O Meier", "H. Stuckenschmidt"], "venue": "Proceedings of the International Workshop on Algorithms & Theories for the Analysis of Event Data", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Log-based evaluation of label splits for process models", "author": ["N Tax", "N Sidorova", "R Haakma", "WMP. van der Aalst"], "venue": "Procedia Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Event abstraction for process mining using supervised learning techniques", "author": ["N Tax", "N Sidorova", "R Haakma", "WMP. van der Aalst"], "venue": "Proceedings of the SAI Conference on Intelligent Systems. IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Mining local process models", "author": ["N Tax", "N Sidorova", "R Haakma", "WMP. van der Aalst"], "venue": "Journal of Innovation in Digital Ecosystems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Lectures on Petri nets I: basic models: advances in Petri nets, volume 1491", "author": ["W Reisig", "G. Rozenberg"], "venue": "Springer Science & Business Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Directional statistics, volume 494", "author": ["Mardia KV", "Jupp PE"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Some tests based on arc-lengths for the circle", "author": ["J. Rao"], "venue": "Sankhya\u0304: The Indian Journal of Statistics, Series B,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1976}, {"title": "The dip test of unimodality", "author": ["Hartigan JA", "Hartigan PM"], "venue": "The Annals of Statistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1985}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1978}, {"title": "Bayes factors", "author": ["Kass RE", "Raftery AE"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Harmonic exponential families on manifolds", "author": ["T Cohen", "M. Welling"], "venue": "Proceedings of The 32nd International Conference on Machine Learning. JMLR W&CP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Clustering on the unit hypersphere using von Mises-Fisher distributions", "author": ["A Banerjee", "IS Dhillon", "J Ghosh", "S. Sra"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "movMF: an R package for fitting mixtures of von Mises-Fisher distributions", "author": ["K Hornik", "B. Gr\u00fcn"], "venue": "Journal of Statistical Software,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Goodness-of-fit tests on a circle", "author": ["Watson GS"], "venue": "II. Biometrika,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1962}, {"title": "Accurate activity recognition in a home setting", "author": ["T van Kasteren", "A Noulas", "G Englebienne", "B. Kr\u00f6se"], "venue": "Proceedings of the 10th International Conference on Ubiquitous Computing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Discovering block-structured process models from event logs containing infrequent behaviour", "author": ["SJJ Leemans", "D Fahland", "WMP. van der Aalst"], "venue": "In: International Conference on Business Process Management. Springer,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Conformance checking of processes based on monitoring real behavior", "author": ["Rozinat A", "van der Aalst WMP"], "venue": "Information Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "A fresh look at precision in process conformance", "author": ["J Munoz-Gama", "J. Carmona"], "venue": "In: International Conference on Business Process Management. Springer,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Activity recognition in the home using simple and ubiquitous sensors", "author": ["EM Tapia", "SS Intille", "K. Larson"], "venue": "In: International Conference on Pervasive Computing. Springer,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Workflow mining with InWoLvE", "author": ["J Herbst", "D. Karagiannis"], "venue": "Computers in Industry,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "On the role of fitness, precision, generalization and simplicity in process discovery", "author": ["Buijs JCAM", "Van Dongen BF", "van der Aalst WMP"], "venue": "OTM Confederated International Conferences \u201dOn the Move to Meaningful Internet Systems\u201d. Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Process mining: Extending \u03b1-algorithm to mine duplicate tasks in process logs. In: Advances in Web and Network Technologies, and InformationManagement", "author": ["J Li", "D Liu", "B. Yang"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Workflow mining: Extending the \u03b1-algorithm to mine duplicate tasks", "author": ["CQ Gu", "HY Chang", "Y. Yi"], "venue": "International Conference on Machine Learning and Cybernetics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Discovering expressive process models from noised log data", "author": ["F Folino", "G Greco", "A Guzzo", "L. Pontieri"], "venue": "Proceedings of the International Database Engineering & Applications Symposium. ACM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Mining duplicate tasks from discovered processes", "author": ["B V\u00e1zquez-Barreiros", "M Mucientes", "M. Lama"], "venue": "Proceedings of the International Workshop on Algorithms & Theories for the Analysis of Event Data", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Handling duplicated tasks in process discovery by refining event labels", "author": ["Lu X", "Fahland D", "van den Biggelaar FJHM", "van der Aalst WMP"], "venue": "In: International Conference on Business Process Management. Springer,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Data-aware process mining: discovering decisions in processes using alignments", "author": ["de Leoni M", "van der Aalst WMP"], "venue": "Proceedings of the 28th Annual ACM Symposium on Applied Computing", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "R: a language and environment for statistical computing", "author": ["R Core Team"], "venue": "R Foundation for Statistical Computing, Vienna, Austria,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Mixtools: An R package for analyzing finite mixture models", "author": ["T Benaglia", "D Chauveau", "D Hunter", "D. Young"], "venue": "Journal of Statistical Software,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Process mining is a fast growing discipline that combines knowledge and techniques from data mining, process modeling, and process model analysis [1].", "startOffset": 146, "endOffset": 149}, {"referenceID": 1, "context": "The scope of process mining have broadened in recent years from business process management to other application domains, one of them being the analysis of events of human behavior with data originating from sensors in smart home environments [2, 3, 4].", "startOffset": 243, "endOffset": 252}, {"referenceID": 2, "context": "The scope of process mining have broadened in recent years from business process management to other application domains, one of them being the analysis of events of human behavior with data originating from sensors in smart home environments [2, 3, 4].", "startOffset": 243, "endOffset": 252}, {"referenceID": 3, "context": "The scope of process mining have broadened in recent years from business process management to other application domains, one of them being the analysis of events of human behavior with data originating from sensors in smart home environments [2, 3, 4].", "startOffset": 243, "endOffset": 252}, {"referenceID": 2, "context": "Simply using the sensor that generates an event (the sensor column in Table 1) as event label is shown to produce non-informative process models that overgeneralize the event log and allow for too much behavior [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 3, "context": ") using techniques closely related to techniques used in the activity recognition field helps to discover more behaviorally constrained and more insightful process models [4], but applicability of this approach relies on the availability of a reliable diary of human behavior at the activity level, which is often just impossible to obtain.", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "Alternatively, insight in the human routines can be obtained through discovery of Local Process Models [5], which bridges process mining and sequential pattern mining by finding patterns that include high-level process model constructs such as (exclusive) choices, loops, and concurrency.", "startOffset": 103, "endOffset": 106}, {"referenceID": 2, "context": "In our earlier work [3] we showed that better process models can be discovered by taking the name of the sensor that generated the event as a starting point for the event label and then refining these labels using information on the time within the day at which the event occurred.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "The refinements used in [3] were based on domain knowledge, and not identified automatically from the data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "A frequently used process modeling notation in the process mining field is the Petri net notation [6].", "startOffset": 98, "endOffset": 101}, {"referenceID": 6, "context": "[7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Uniformity Check Rao\u2019s spacing test [8] tests the uniformity of the timestamps of the events from a sensor around the circular clock.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Unimodality Check Hartigan\u2019s dip tests [9] the null hypothesis that the data follows a unimodal distribution on a circle.", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "Number of Component Selection The Bayesian Information Criterion (BIC) [10] introduces a penalty for the number of model parameters to the evaluation of a mixture model.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "We start with 1 component, and iteratively increase from k to k+1 components as long as the decrease in BIC is larger than 10, which is the threshold for decisive evidence of high BIC [11].", "startOffset": 184, "endOffset": 188}, {"referenceID": 11, "context": "the torus and sphere) was proposed by Cohen and Welling in [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "[13] that introduces a clustering method based on a mixture of von Mises-Fisher distribution components, which is a generalization of the 2-dimensional von Mises distribution to n-dimensional spheres.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "We fit a mixture model of von Mises components using the package movMF [14] provided in R.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "We describe the Watson U2 statistic [15], a goodness-of-fit assessment based on hypothesis testing.", "startOffset": 36, "endOffset": 40}, {"referenceID": 2, "context": "We assess the quality of this label refinement from a process perspective using the label refinement evaluation method described in [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "This control-flow test [3] outputs a p-value that indicates whether such log statistics of refined activities a1, a2, .", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Figure 4a shows the process model discovered on this event log with the Inductive Miner infrequent [17] with 20% filtering, which discovers a process model that describes the most frequent 80% of behavior in the log.", "startOffset": 99, "endOffset": 103}, {"referenceID": 2, "context": "The label refinement evaluation method [3] finds statistically significant differences between the events from the two bedroom door clusters with regard to their control-flow relations with other activities in the log for 10 other activities using the significance level of 0.", "startOffset": 39, "endOffset": 42}, {"referenceID": 17, "context": "Two of those criteria are fitness [18], which measures the degree to which the behavior that is observed in the event log can be replayed on the process model, and precision [19], which measures the degree to which the behavior that was never observed in the event log cannot be replayed on the process model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "Two of those criteria are fitness [18], which measures the degree to which the behavior that is observed in the event log can be replayed on the process model, and precision [19], which measures the degree to which the behavior that was never observed in the event log cannot be replayed on the process model.", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "The bedroom door label refinement described above improves the precision of the process model found with the Inductive Miner infrequent (20% filtering) [17] from 0.", "startOffset": 152, "endOffset": 156}, {"referenceID": 2, "context": "Generating multiple label refinements, however, comes with the problem that the control-flow test [3] is sensitive to the order in which label refinements are applied.", "startOffset": 98, "endOffset": 101}, {"referenceID": 16, "context": "We apply these four strategies on three event logs from the human behavior domain and measure the fitness, precision, and F-score of the model discovered with the Inductive Miner infrequent [17] with 20% filtering after each label refinement.", "startOffset": 190, "endOffset": 194}, {"referenceID": 15, "context": "The first event log is the Van Kasteren [16] event log which we introduced in Section 4.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "The other two event logs are two different households of a smart home experiment conducted by MIT [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "The first process discovery algorithm capable of discovering duplicate tasks was proposed by Herbst and Karagiannis in 2004 [21], after which many others have been proposed, including the Evolutionary Tree Miner [22], the \u03b1-algorithm [23], the \u03b1#-algorithm [24], the EnhancedWFMiner [25].", "startOffset": 124, "endOffset": 128}, {"referenceID": 21, "context": "The first process discovery algorithm capable of discovering duplicate tasks was proposed by Herbst and Karagiannis in 2004 [21], after which many others have been proposed, including the Evolutionary Tree Miner [22], the \u03b1-algorithm [23], the \u03b1#-algorithm [24], the EnhancedWFMiner [25].", "startOffset": 212, "endOffset": 216}, {"referenceID": 22, "context": "The first process discovery algorithm capable of discovering duplicate tasks was proposed by Herbst and Karagiannis in 2004 [21], after which many others have been proposed, including the Evolutionary Tree Miner [22], the \u03b1-algorithm [23], the \u03b1#-algorithm [24], the EnhancedWFMiner [25].", "startOffset": 234, "endOffset": 238}, {"referenceID": 23, "context": "The first process discovery algorithm capable of discovering duplicate tasks was proposed by Herbst and Karagiannis in 2004 [21], after which many others have been proposed, including the Evolutionary Tree Miner [22], the \u03b1-algorithm [23], the \u03b1#-algorithm [24], the EnhancedWFMiner [25].", "startOffset": 257, "endOffset": 261}, {"referenceID": 24, "context": "The first process discovery algorithm capable of discovering duplicate tasks was proposed by Herbst and Karagiannis in 2004 [21], after which many others have been proposed, including the Evolutionary Tree Miner [22], the \u03b1-algorithm [23], the \u03b1#-algorithm [24], the EnhancedWFMiner [25].", "startOffset": 283, "endOffset": 287}, {"referenceID": 25, "context": "An alternative approach has been proposed by V\u00e1zques-Barreiros [26] et al.", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "[27], who describe an approach to pre-process an event log by refining event labels with the goal of discovering a process model with duplicate activities.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "De Leoni and van der Aalst [28] proposed a method that discovers data guards for decision points in the process based on alignments and decision tree learning.", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "Process mining is a research field focused on the analysis of event data with the aim of extracting insights related to dynamic behavior. Applying process mining techniques on data from smart home environments has the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions. Finding the right event labels to enable the application of process mining techniques is however far from trivial, as simply using the triggering sensor as the label for sensor events results in uninformative models that allow for too much behavior (overgeneralizing). Refinements of sensor level event labels suggested by domain experts have been shown to enable discovery of more precise and insightful process models. However, there exists no automated approach to generate refinements of event labels in the context of process mining. In this paper we propose a framework for the automated generation of label refinements based on the time attribute of events, allowing us to distinguish behaviourally different instances of the same event type based on their time attribute. We show on a case study with real life smart home event data that using automatically generated refined labels in process discovery, we can find more specific, and therefore more insightful, process models. We observe that one label refinement could have an effect on the usefulness of other label refinements when used together. Therefore, we explore four strategies to generate useful combinations of multiple label refinements and evaluate those on three real life smart home event logs. Corresponding author 1002 N. Tax, et al. / Time-Based Label Refinements to Discover More Precise Process Models", "creator": "LaTeX with hyperref package"}}}