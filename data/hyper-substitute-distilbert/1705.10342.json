{"id": "1705.10342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2017", "title": "Deep Learning for Ontology Reasoning", "abstract": "in this work, we present more novel theme promoting ontology reasoning that projects based on deep learning rather than stimulus - shaped nonlinear reasoning. within an approximation, we introduce, consistent model from statistical relational learning that is really an deep interactive neural networks, this make reasonable support that analytics can economically compete with, perhaps even competitor, existing logic - based reasoners through the task of descriptive reasoning. more precisely, moore compared our implemented system implementation predictions of the stronger logic - based performance reasoners at one, arguing, on why number of large standard system datasets, their found instances per system attained high confidence quality, while being dedicated to two classes of magnitude agile.", "histories": [["v1", "Mon, 29 May 2017 18:17:52 GMT  (19kb)", "http://arxiv.org/abs/1705.10342v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["patrick hohenecker", "thomas lukasiewicz"], "accepted": false, "id": "1705.10342"}, "pdf": {"name": "1705.10342.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["thomas.lukasiewicz}@cs.ox.ac.uk"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 34\n2v 1\n[ cs\n.A I]\n2 9\nM ay"}, {"heading": "1 Introduction", "text": "In the last few years, there has been an increasing interest in the application of machine learning (ML) to the field of knowledge representation and reasoning (KRR), or, more generally, in learning to reason over symbolic data\u2014cf., e.g., Gabrilovoch et al. (2015). The main motivation behind this is that most KRR formalisms used today are rooted in symbolic logic, which allows for answering queries accurately by employing formal reasoning, but also comes with a number of issues, like difficulties with handling incomplete, conflicting, or uncertain information and scalability problems.\nHowever, many of these issues can be dealt with effectively by using methods of ML, which are in this context often subsumed under the notion of statistical relational learning (SRL; Getoor and Taskar, 2007)\u2014cf. Nickel et al. (2016) for a recent survey. Notice, though, that the use of ML for reasoning is a tradeoff. On the one hand, ML models are often highly scalable, more resistant to disturbances in the data, and can provide predictions even if formal reasoning fails. On the other hand, however, their predictions are correct with a certain probability only. In contrast to this, formal reasoners are often obstructed by the above problems, but if they can provide inferences, then these are correct with certainty.\nWe believe that the combination of both fields, i.e., ML and KRR, is an important step towards human-level artificial intelligence. However, while there exist elaborate reasoning systems already, SRL is a rather young field that has, we believe, not hit its boundaries yet. Therefore, in this work, we introduce a new approach to SRL based on deep learning, and apply it to the task of reasoning over ontological knowledge bases (OKBs). These are knowledge bases (KBs) that consist of a set of facts together with a formal description of the domain of interest\u2014the so-called ontology. The reason why we chose this very task is its practical significance as well as the fact that it commonly comprises extensive formal reasoning.\nThe motivation for employing deep learning, however, which refers to the use of neural networks (NNs) that perform many sequential steps of computation, should be fairly obvious. In the last ten years, deep learning has been applied to a wide variety of problems with tremendous success, and constitutes the state-of-the-art in fields like computer vision and natural language processing (NLP) today. Interestingly, there are also a few published attempts to realize formal reasoning by means\nof deep NNs. However, these focus on rather restricted logics, like natural logic (Bowman, 2013) or real logic (Serafini and d\u2019Avila Garcez, 2016), and do not consider reasoning in its full generality. Besides this, \u00bbreasoning\u00ab appears in connection with deep learning mostly in the context of NLP\u2014 e.g., Socher et al. (2013).\nThe main contributions of this paper are briefly as follows:\n\u2022 We present a novel method for SRL that is based on deep learning with recursive NNs, and apply it to ontology reasoning.\n\u2022 Furthermore, we provide an experimental comparison of the suggested approach with one of the best logic-based ontology reasoners at present, RDFox (Nenov et al., 2015), on several large standard benchmarks. Thereby, our model achieves a high reasoning quality while being up to two orders of magnitude faster.\n\u2022 To the best of our knowledge, we are the first to investigate ontology reasoning based on deep learning on such large and expressive OKBs.\nThe rest of this paper is organized as follows. In the next section, we review a few concepts that our approach is built upon. Section 3 introduces the suggested model in full detail, and Section 4 discusses how to apply it to ontology reasoning. In Section 5, we evaluate our model on four datasets, and compare its performance with RDFox. We conclude with a summary of the main results, and give an outlook on future research."}, {"heading": "2 Background", "text": "As mentioned in the introduction already, our work lies at the intersection of two, traditionally quite separated, fields, namely ML and KRR. Therefore, in this section, we review the most important concepts, from both areas, that are required to follow the subsequent elaborations."}, {"heading": "2.1 Ontological Knowledge Bases (OKBs)", "text": "A central idea in the field of KRR is the use of so-called ontologies. In this context, an ontology is a formal description of a concept or a domain, e.g., a part of the real world, and the word \u00bbformal\u00ab emphasizes that such a description needs to be specified by means of some knowledge representation language with clearly defined semantics. This, in turn, allows us to employ formal reasoning in order to draw conclusions based on such an ontology.\nAn important aspect to note is that an ontology is situated on the meta-level, which means that it might specify general concepts or relations, but does not contain any facts. However, in the sequel we only talk about a number of facts together with an ontology that describes the domain of interest, and we refer to such a setting as an ontological knowledge base (OKB).\nIn practice, and in the context of description logics (Baader et al., 2007), ontologies are usually defined in terms of unary and binary predicates. Thereby, unary predicates are usually referred to as concepts or classes, and define certain categories, e.g., of individuals that possess a particular characteristic. In contrast to this, binary predicates define relationships that might exist between a pair of individuals, and are usually referred to as relations or roles.\nWhat is really appealing about ontologies is that they usually not just define those predicates, but also rules that allow us to draw conclusions based on them. This could encompass simple inferences like every individual of class women belongs to class human as well, but also much more elaborate reasoning that takes several classes and relations into account. Notice further that we can view almost any relational dataset as an OKB with an ontology that does not specify anything except the classes and relations that exist in the data.\nBased on the fact that we hardly ever encounter ontologies with predicates of arity greater than two in practice, we confine ourselves to this particular case in the subsequent treatment\u2014the approach introduced in this work can be easily extended to the general case, though. Any OKB that is defined in terms of unary and binary predicates only has a natural representation as labeled directed multigraph1 if individuals are interpreted as vertices and every occurrence of a binary predicate as a\n1If we really need to account for predicates of arity greater than two, then we can view any such dataset as a hypergraph, and extend the RTN model introduced in the next section with convolutional layers as appropriate.\ndirected edge. Thereby, edges are labeled with the name of the according relation, and vertices with an incidence vector that indicates which classes they belong to. Notice, however, that, depending on the used formalism, OKBs may adhere to the so-called open-world assumption (OWA). In this case, a fact can be true, false, or unknown, which is, e.g., different from classical first-order logic. The presence of the OWA is reflected by according three-valued incidence vectors, whose elements may be any of 1, \u22121, or 0, respectively, and indicate that an individual belongs to a class, is not a member of the same, or that this is unknown."}, {"heading": "2.2 Recursive Neural Tensor Networks (RNTNs)", "text": "Recursive NNs (Pollack, 1990) are a special kind of network architecture that was introduced in order to deal with training instances that are given as trees rather than, as more commonly, feature vectors. In general, they can deal with any directed acyclic graph (DAG), since any such graph can be unrolled as a tree, and the only requirement is that the leaf nodes have vector representations attached to them. An example from the field of NLP is the parse tree of a sentence, where each node represents one word and is given as either a one-hot-vector or a previously learned word embedding.\nUnlike feed-forward networks, recursive NNs do not have a fixed network structure, but only define a single recursive layer, which accepts two vectors as input and maps them to a common embedding. This layer is used to reduce a provided tree step by step in a bottom-up fashion until only one single vector is left. The resulting vector can be regarded as an embedding of the entire graph, and may be used, e.g., as input for a subsequent prediction task.\nIn this work, we make use of the following recursive layer, which defines what is referred to as recursive neural tensor network (RNTN; Socher et al., 2013):\ng(x, R,y) = URf\n(\nxTW [1:k] R y +VR\n[\nx y\n]\n+ bR\n)\n, (1)\nwhere x,y\u2208Rd, UR \u2208R d\u00d7k, VR \u2208R k\u00d72d, WR \u2208R d\u00d7d\u00d7k, bR \u2208R k, and f is a nonlinearity that is applied element-wise, commonly tanh. Thereby, the term xTW [1:k] R y denotes a bilinear tensor product, and is computed by multiplying x and y with every slice ofWR separately. So, if z is the computed tensor product, then zi = x TW [i] R y. In addition to the actual input vectors, x and y, the tensor layer accepts another parameter R, which may be used to specify a certain relation between the provided vectors. This makes the model more powerful, since we use a separate set of weights for each kind of relation.\nIn general, recursive NNs are trained by means of stochastic gradient descent (SGD) together with a straightforward extension of standard backpropagation, called backpropagation through structure (BPTS; Goller and K\u00fcchler, 1996)."}, {"heading": "3 Relational Tensor Networks (RTNs)", "text": "In this section, we present a new model for SRL, which we\u2014due to lack of a better name\u2014refer to as relational tensor network (RTN). An RTN is basically an RNTN that makes use of a modified bilinear tensor layer. The underlying intuition, however, is quite different, and the term \u00bbrelational\u00ab emphasizes the focus on relational datasets."}, {"heading": "3.1 The Basic Model", "text": "As described in the previous section, recursive NNs allow for computing embeddings of training instances that are given as DAGs. If we face a relational dataset, though, then the training samples are actually vertices of a graph, namely the one that is induced by the entire relational dataset, rather than a graph itself. However, while this does not fit the original framework of recursive networks, we can still make use of a recursive layer in order to update the representations of individuals based on the structure of dataset. In an RTN, this deliberation is reflected by the following modified tensor layer:\ng\u0303(x, R,y) = x+URf ( xTW [1:m] R y +VRy ) , (2)\nwhere the notation is the same as in Equation 1 except thatVR \u2208R k\u00d7d.\nThe intuition here is quite straightforward. While individuals in a relational dataset are initially represented by their respective feature vectors, big parts of the total information that we have are actually hidden in the relations among them. However, we can use a recursive network, composed of tensor layers like the one denoted in Equation 2, to incorporate these data into an individual\u2019s embedding. Intuitively, this means that we basically apply a recursive NN to an update tree of an individual, and thus compute an according vector representation based on the relations that it is involved in. For the RTN, we adopted the convention that a tensor layer g\u0303 updates the individual represented by x based on an instance (x, R,y) of relationR that is present in the data. Furthermore, if the relations in the considered dataset are not symmetric, then we have to distinguish whether an individual is the source or the target of an instance of a relation. Accordingly, the model has to contain two sets of parameters for such a relation, one for updating the source and one for the target, and we denote these as R\u22b2 and R\u22b3, respectively. This means, e.g., that g\u0303(x, R\u22b3,y) denotes that the embedding of x is updated based on (y, R,x).\nThe foregoing considerations also explain the differences between Equation 2 and the original tensor layer given in Equation 1 (Socher et al., 2013). First and foremost, we see that in our model x is added to what basically used to be the tensor layer before, which is predicated on the fact that we want to update this very vector. Furthermore, x does not affect the argument of the nonlinearity f independently of y, since x by itself should not determine the way that it is updated. Lastly, there is no bias term on the right-hand side of Equation 2 to prevent that there is some kind of default update irrespective of the individuals involved.\nWe also considered to add another application of the hyperbolic tangent on top of the calculations given in Equation 2 in order to keep the elements of the created embeddings in [\u22121, 1]. This would ensure that there cannot be any embeddings with an oddly large norm due to individuals being involved in a large number of relations. However, since we did not encounter any problems like this in our experiments, we decided against the use of this option, as it could introduce additional problems like vanishing gradients."}, {"heading": "3.2 Training", "text": "As already suggested before, we usually employ RTNs in order to compute embeddings for individuals that are used as input for some specific prediction task. Therefore, it makes sense to train an RTN together with the model that is used for computing these predictions, and whenever we talk about an RTN in the sequel, we shall assume that it is used together with some predictor on top of it. If we only care about individual embeddings irrespective of any particular subsequent task, then we can simply add a feed-forward layer\u2014or some other differentiable learning model\u2014on top of the RTN, and train the model to reconstruct the provided feature vectors. This way, an RTN can be used as a kind of relational autoencoder.\nTraining such a model is straightforward, and switches back and forth between computing embeddings and making predictions based on them. In each training iteration, we start from the feature vectors of the individuals as they are provided in the dataset. Then, as a first step, we sample mini-batches of triples from the dataset, and randomly update the current embedding of one of the individuals in each triple by means of our RTN. The total number of mini-batches that are considered in this step is a hyperparameter, and we found during our experiments that it is in general not necessary to consider the entire dataset.\nNext, we sample mini-batches of individuals from the dataset, and compute predictions for them based on the embeddings that we created in the previous step. In doing so, it makes sense to consider both individuals that have been updated as well as some that still have their initial feature vectors as embeddings. This is important for the model to learn how to deal with individuals that are involved in very few relations or maybe no one at all, which is not a rare case in practice. Therefore, in our experiments, we used mini-batches that were balanced with respect to this, and switched back to step number one as soon as each of the previously updated individuals has been sampled once.\nThe loss function as well as the optimization strategy employed depends, as usual, on the concrete task, and is chosen case by case."}, {"heading": "3.3 Related Models", "text": "In the field of SRL, there exist a few other approaches that model the effects of relations on individual embeddings in terms of (higher-order) tensor products\u2014cf., e.g., Nickel et al. (2011, 2012). However, these methods, which belong to the category of latent variable models, are based on the idea of factorizing a tensor that describes the structure of a relational dataset into a product of an embedding matrix as well as another tensor that represents the relations present in the data. The actual learning procedure is then cast as a regularized minimization problem based on this formulation. In contrast to this, an RTN computes embeddings, both during training and application, by means of a random process, and is thus fundamentally different from this idea."}, {"heading": "4 Reasoning with RTNs", "text": ""}, {"heading": "4.1 Applying RTNs to OKBs", "text": "As discussed in Section 2.1, OKBs can be viewed as DAGs, and thus the application of an RTN to this kind of data is straightforward. Therefore, we are only left with specifying the prediction model that we want to use on top of the RTN. In the context of an OKB, there are two kinds of predictions that we are interested in, namely the membership of individuals to classes, on the one hand, and the existence of relations, on the other hand. From a ML perspective, these are really two different targets, and we can describe them more formally as follows: letK be an OKB that contains (exactly) the unary predicates P1, . . . , Pk and (exactly) the binary predicatesQ1, . . . , Q\u2113, and T \u2286 K the part of the OKB that we have as training set. Then t(1) and t(2) are two target functions defined as\nt(1) :\n{\nindividuals(K) \u2192 {\u22121, 0, 1}k i 7\u2192 x(i)\nand\nt(2) :\n{\nindividuals(K)2 \u2192 {\u22121, 0, 1}\u2113 (i, j) 7\u2192 y(i,j)\nsuch that x (i) m equals 1, if K |= Pm(i), \u22121, if K |= \u00acPm(i), and 0, otherwise, and y (i,j) m is defined accordingly with respect to Qm(i, j).\nNotice that all of the arguments of the functions t(1) and t(2) are individuals, and can thus be represented as embeddings produced by an RTN. For computing actual predictions from these embeddings, we can basically employ an ML model of our choice. In this work, however, we confine ourselves to multinomial logistic regression for t(1), i.e., we simply add a single feed-forward layer as well as a softmax on top it to the RTN. For t(2), we first add an additional original tensor layer as given in Equation 1, like it was used by Socher et al. (2013), and use multinomial logistic regression on top of it as well."}, {"heading": "4.2 Predicting Classes and Relations Simultaneously", "text": "While the targets t(1) and t(2) may be regarded as independent with respect to prediction, this is clearly not the case for computing individual embeddings. We require an embedding to reflect all of the information that we have about a single individual as specified by the semantics of the considered OKB. Therefore, the tensor layers of an RTN need to learn how to adjust individual vectors in view of both unary and binary predicates, i.e., classes and relations. To account for this, we train RTNs\u2014facing the particular use case of ontology reasoning\u2014on mini-batches that consist of training samples for both of the prediction targets."}, {"heading": "5 Evaluation", "text": "To evaluate the suggested approach in a realistic scenario, we implemented a novel triple store, called NeTS (Neural Triple Store), that achieves ontology reasoning solely by means of an RTN. NeTS provides a simple, SPARQL-like, query interface that allows for submitting atomic queries as well as conjunctions of such (see Figure 1).\nWhen the system is started, then the first step it performs is to load a set of learned weights from the disk\u2014the actual learning process is not part of NeTS right now, and may be incorporated in future versions. Next, it observes whether there are previously generated embeddings of the individuals stored on disk already, and loads them as well, if any. If this is not the case, however, then NeTS creates such embeddings as described above. This step is comparable with what is usually referred to as materialization in the context of database systems. Traditionally, a database would compute all valid inferences that one may draw based on the provided data, and store them somehow in memory or on disk. In contrast to this, NeTS accounts for these inferences simply by adjusting the individuals\u2019 embeddings by means of a trained RTN, which obviously has great advantages regarding its memory requirements. Note further that we do not store any actual inferences at this time, but rather compute them on demand later on if this happens to become necessary.\nSubsequent processing of queries is entirely based on these embeddings, and does not employ any kind of formal reasoning at all. This, in turn, allows for speeding up the necessary computations significantly, since we can dispatch most of the the \u00bbheavy-lifting\u00ab to a GPU.\nOur system is implemented in Python 3.4, and performs, as mentioned above, almost all numeric computations on a GPU using PyCUDA 2016.1.2 (Kl\u00f6ckner et al., 2012). For learning the weights of our RTNs, we again used Python 3.4, along with TensorFlow 0.11.0 (Abadi et al., 2015)."}, {"heading": "5.1 Test Data", "text": "To maintain comparability, we evaluated our approach on the same datasets that Motik et al. (2014) used for their experiments with RDFox (Nenov et al., 2015).2 As mentioned earlier, RDFox is indeed a great benchmark, since it has been shown to be the most efficient triple store at present. For a comparison with other systems, however, we refer the interested reader to Motik et al. (2014).\nThe test data consists of four Semantic Web KBs of different sizes and characteristics. Among these are two real-world datasets, a fraction of DBpedia (Bizer et al., 2009) and the Claros KB3, as well as two synthetic ones, LUBM (Guo et al., 2005) and UOBM (Ma et al., 2006). Their characteristics are summarized in Table 1.\nWhile all these data are available in multiple formats, we made use of the ontologies specified in OWL and the facts provided as n-triples for our experiments. Furthermore, we considered only those predicates that appear for at least 5% of the individuals in a database. This is a necessary restriction to ensure that there is enough data for an RTN to learn properly."}, {"heading": "5.2 Experimental Setup", "text": "All our experiments were conducted on a server with 24 CPUs of type Intel Xeon E5-2620 (6\u00d72.40GHz), 64GB of RAM, and an Nvidia GeForce GTX Titan X. The test system hosted Ubuntu Server 14.04 LTS (64 Bit) with CUDA 8.0 and cuDNN 5.1 for GPGPU. Notice, however, that NeTS does not make any use of multiprocessing or -threading besides GPGPU, which means that the only kind of parallelization takes place on the GPU. Therefore, in terms of CPU and RAM, NeTS had about half of the resources at its disposal that RDFox utilized in the experiments conducted by Motik et al. (2014).\n2All of these datasets are available at http://www.cs.ox.ac.uk/isg/tools/RDFox/2014/AAAI/. 3 http://www.clarosnet.org\nPredicated on the use of the RTN model, the datasets, including all of their inferences, were converted into directed graphs using Apache Jena 2.13.04 and the OWL reasoner Pellet 2.4.05\u2014all of the import times reported in Table 3 refer to these graphs. This reduced the size of the data, as stored on disk, to approximately on third of the original dataset. Furthermore, we removed a total of 50,000 individuals during training, together with all of the predicates that these were involved in, as test set from each of the datasets, and similarly another 50,000 for validation\u2014the results described in Table 2 were retrieved for these test sets."}, {"heading": "5.3 Results", "text": "In order to assess the quality of NeTS, we have to evaluate it on two accounts. First, we need to consider its predictive performance based on the embeddings computed by the underlying RTN model, and second, we must ascertain the efficiency of the system with respect to time consumption.\nWe start with the former. To that end, consider Table 2, which reports the accuracies as well as F1 scores that NeTS achieved on the held-out test sets, averaged over all classes and relations, respectively. We see that the model consistently achieves great scores with respect to both measures. Notice, however, that the F1 score is the more critical criterion, since all the predicates are strongly imbalanced. Nevertheless, the RTN effectively learns embeddings that allow for discriminating positive from negative instances.\nTable 3, in contrast, lists the times for NeTS to import and materialize each of the datasets along with the respective measurements for RDFox (Motik et al., 2014). As mentioned before, materialization refers to the actual computation of inferences, and usually depends on the expressivity of the ontology as well as the number of facts available. We see that NeTS is significantly faster at the materialization step, while RDFox is faster at importing the data. This is explained as follows. First, NeTS realizes reasoning by means of vector manipulations on a GPU, which is of course much faster than the symbolic computations performed by RDFox. As for the second point, RDFox makes use of extensive parallelization, also for importing data, while NeTS runs as a single process with a single thread on a CPU.\n4 https://jena.apache.org 5 https://github.com/Complexible/pellet\nHowever, from a practical point of view, materialization is usually more critical than import. This is because an average database is updated with new facts quite frequently, while it is imported only once in a while.\nNotice, however, that neither of the measures reported for NeTS contains the time for training the model. The reason for this is that we train an RTN, as mentioned earlier, with respect to an ontology rather than an entire OKB. Therefore, one can actually consider the training step as part of the setup of the database system. For the datasets used in our experiments, training took between three and four days each."}, {"heading": "6 Summary and Outlook", "text": "We have presented a novel method for SRL based on deep learning, and used it to develop a highly efficient, learning-based system for ontology reasoning. Furthermore, we have provided an experimental comparison with one of the best logic-based ontology reasoners at present, RDFox, on several large standard benchmarks, and showed that our approach attains a high reasoning quality while being up to two orders of magnitude faster.\nAn interesting topic for future research is to explore ways to further improve our accuracy on ontology reasoning. This could be achieved, e.g., by incorporating additional synthetic data and/or slight refinements of the RTN architecture."}, {"heading": "Acknowledgments", "text": "This work was supported by the Engineering and Physical Sciences Research Council (EPSRC), under the grants EP/J008346/1, EP/L012138/1, and EP/M025268/1, as well as the Alan Turing Institute, under the EPSRC grant EP/N510129/1. Furthermore, Patrick is supported by the EPSRC, under grant OUCL/2016/PH, and the Oxford-DeepMindGraduate Scholarship, under grant GAF1617_OGSMF-DMCS_1036172."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["cent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vanhoucke et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2015}, {"title": "The Description Logic Handbook: Theory, Implementation, and Applications", "author": ["Franz Baader", "Diego Calvanese", "Deborah L. McGuinness", "Daniele Nardi", "Peter F. PatelSchneider"], "venue": null, "citeRegEx": "Baader et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Baader et al\\.", "year": 2007}, {"title": "DBpedia\u2014A crystallization point for the Web of Data", "author": ["Christian Bizer", "Jens Lehmann", "Georgi Kobilarov", "S\u00f6ren Auer", "Christian Becker", "Richard Cyganiak", "Sebastian Hellmann"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Bizer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bizer et al\\.", "year": 2009}, {"title": "Can recursive neural tensor networks learn logical reasoning", "author": ["Samuel R. Bowman"], "venue": null, "citeRegEx": "Bowman.,? \\Q2013\\E", "shortCiteRegEx": "Bowman.", "year": 2013}, {"title": "Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches", "author": ["Evgeniy Gabrilovoch", "Ramanathan Guha", "Andrew McCallum", "Kevin Murphy", "editors"], "venue": "Palo Alto, California,", "citeRegEx": "Gabrilovoch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gabrilovoch et al\\.", "year": 2015}, {"title": "Introduction to Statistical Relational Learning. Adaptive Computation and Machine Learning", "author": ["Lise Getoor", "Ben Taskar"], "venue": null, "citeRegEx": "Getoor and Taskar.,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar.", "year": 2007}, {"title": "Learning Task-Dependent Distributed Representations by Backpropagation Through Structure", "author": ["Christoph Goller", "Andreas K\u00fcchler"], "venue": "In IEEE International Conference on Neural Networks,", "citeRegEx": "Goller and K\u00fcchler.,? \\Q1996\\E", "shortCiteRegEx": "Goller and K\u00fcchler.", "year": 1996}, {"title": "LUBM: A benchmark for OWL knowledge base systems", "author": ["Yuanbo Guo", "Zhengxiang Pan", "Jeff Heflin"], "venue": "Web Semantics: Science, Services and Agents on the World Wide Web,", "citeRegEx": "Guo et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2005}, {"title": "PyCUDA and PyOpenCL: A Scripting-Based Approach to GPU Run-Time Code Generation", "author": ["Andreas Kl\u00f6ckner", "Nicolas Pinto", "Yunsup Lee", "B. Catanzaro", "Paul Ivanov", "Ahmed Fasih"], "venue": "Parallel Computing,", "citeRegEx": "Kl\u00f6ckner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kl\u00f6ckner et al\\.", "year": 2012}, {"title": "Towards a Complete OWL Ontology Benchmark", "author": ["Li Ma", "Yang Yang", "Zhaoming Qiu", "Guotong Xie", "Yue Pan", "Shengping Liu"], "venue": "In Proceedings of the 3rd European Semantic Web Conference (ESWC", "citeRegEx": "Ma et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2006}, {"title": "Parallel Materialisation of Datalog Programs in Centralised, Main-Memory RDF Systems", "author": ["Boris Motik", "Yavor Nenov", "Robert Piro", "Ian Horrocks", "Dan Olteanu"], "venue": "In Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI", "citeRegEx": "Motik et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Motik et al\\.", "year": 2014}, {"title": "RDFox: A Highly-Scalable RDF Store", "author": ["Yavor Nenov", "Robert Piro", "Boris Motik", "Ian Horrocks", "Zhe Wu", "Jay Banerjee"], "venue": "In Proceedings of the 14th International Semantic Web Conference (ISWC 2015),", "citeRegEx": "Nenov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nenov et al\\.", "year": 2015}, {"title": "A Three-WayModel for Collective Learning on Multi-Relational Data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Factorizing YAGO", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of the 21st International Conference on World Wide Web,", "citeRegEx": "Nickel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2012}, {"title": "A Review of Relational Machine Learning for Knowledge Graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Nickel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Recursive distributed representations", "author": ["Jordan B. Pollack"], "venue": "Artificial Intelligence,", "citeRegEx": "Pollack.,? \\Q1990\\E", "shortCiteRegEx": "Pollack.", "year": 1990}, {"title": "Logic tensor networks: Deep learning and logical reasoning from data and knowledge", "author": ["Luciano Serafini", "Artur d\u2019Avila Garcez"], "venue": null, "citeRegEx": "Serafini and Garcez.,? \\Q2016\\E", "shortCiteRegEx": "Serafini and Garcez.", "year": 2016}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "However, many of these issues can be dealt with effectively by using methods of ML, which are in this context often subsumed under the notion of statistical relational learning (SRL; Getoor and Taskar, 2007)\u2014cf.", "startOffset": 177, "endOffset": 207}, {"referenceID": 4, "context": ", Gabrilovoch et al. (2015). The main motivation behind this is that most KRR formalisms used today are rooted in symbolic logic, which allows for answering queries accurately by employing formal reasoning, but also comes with a number of issues, like difficulties with handling incomplete, conflicting, or uncertain information and scalability problems.", "startOffset": 2, "endOffset": 28}, {"referenceID": 4, "context": ", Gabrilovoch et al. (2015). The main motivation behind this is that most KRR formalisms used today are rooted in symbolic logic, which allows for answering queries accurately by employing formal reasoning, but also comes with a number of issues, like difficulties with handling incomplete, conflicting, or uncertain information and scalability problems. However, many of these issues can be dealt with effectively by using methods of ML, which are in this context often subsumed under the notion of statistical relational learning (SRL; Getoor and Taskar, 2007)\u2014cf. Nickel et al. (2016) for a recent survey.", "startOffset": 2, "endOffset": 588}, {"referenceID": 3, "context": "However, these focus on rather restricted logics, like natural logic (Bowman, 2013) or real logic (Serafini and d\u2019Avila Garcez, 2016), and do not consider reasoning in its full generality.", "startOffset": 69, "endOffset": 83}, {"referenceID": 3, "context": "However, these focus on rather restricted logics, like natural logic (Bowman, 2013) or real logic (Serafini and d\u2019Avila Garcez, 2016), and do not consider reasoning in its full generality. Besides this, \u00bbreasoning\u00ab appears in connection with deep learning mostly in the context of NLP\u2014 e.g., Socher et al. (2013). The main contributions of this paper are briefly as follows:", "startOffset": 70, "endOffset": 313}, {"referenceID": 11, "context": "\u2022 Furthermore, we provide an experimental comparison of the suggested approach with one of the best logic-based ontology reasoners at present, RDFox (Nenov et al., 2015), on several large standard benchmarks.", "startOffset": 149, "endOffset": 169}, {"referenceID": 1, "context": "In practice, and in the context of description logics (Baader et al., 2007), ontologies are usually defined in terms of unary and binary predicates.", "startOffset": 54, "endOffset": 75}, {"referenceID": 15, "context": "Recursive NNs (Pollack, 1990) are a special kind of network architecture that was introduced in order to deal with training instances that are given as trees rather than, as more commonly, feature vectors.", "startOffset": 14, "endOffset": 29}, {"referenceID": 17, "context": "In this work, we make use of the following recursive layer, which defines what is referred to as recursive neural tensor network (RNTN; Socher et al., 2013):", "startOffset": 129, "endOffset": 156}, {"referenceID": 6, "context": "In general, recursive NNs are trained by means of stochastic gradient descent (SGD) together with a straightforward extension of standard backpropagation, called backpropagation through structure (BPTS; Goller and K\u00fcchler, 1996).", "startOffset": 196, "endOffset": 228}, {"referenceID": 17, "context": "The foregoing considerations also explain the differences between Equation 2 and the original tensor layer given in Equation 1 (Socher et al., 2013).", "startOffset": 127, "endOffset": 148}, {"referenceID": 17, "context": "For t, we first add an additional original tensor layer as given in Equation 1, like it was used by Socher et al. (2013), and use multinomial logistic regression on top of it as well.", "startOffset": 100, "endOffset": 121}, {"referenceID": 8, "context": "2 (Kl\u00f6ckner et al., 2012).", "startOffset": 2, "endOffset": 25}, {"referenceID": 11, "context": "(2014) used for their experiments with RDFox (Nenov et al., 2015).", "startOffset": 45, "endOffset": 65}, {"referenceID": 10, "context": "To maintain comparability, we evaluated our approach on the same datasets that Motik et al. (2014) used for their experiments with RDFox (Nenov et al.", "startOffset": 79, "endOffset": 99}, {"referenceID": 10, "context": "To maintain comparability, we evaluated our approach on the same datasets that Motik et al. (2014) used for their experiments with RDFox (Nenov et al., 2015). As mentioned earlier, RDFox is indeed a great benchmark, since it has been shown to be the most efficient triple store at present. For a comparison with other systems, however, we refer the interested reader to Motik et al. (2014).", "startOffset": 79, "endOffset": 390}, {"referenceID": 2, "context": "Among these are two real-world datasets, a fraction of DBpedia (Bizer et al., 2009) and the Claros KB, as well as two synthetic ones, LUBM (Guo et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 7, "context": ", 2009) and the Claros KB, as well as two synthetic ones, LUBM (Guo et al., 2005) and UOBM (Ma et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 9, "context": ", 2005) and UOBM (Ma et al., 2006).", "startOffset": 17, "endOffset": 34}, {"referenceID": 10, "context": "Therefore, in terms of CPU and RAM, NeTS had about half of the resources at its disposal that RDFox utilized in the experiments conducted by Motik et al. (2014).", "startOffset": 141, "endOffset": 161}, {"referenceID": 10, "context": "Table 3, in contrast, lists the times for NeTS to import and materialize each of the datasets along with the respective measurements for RDFox (Motik et al., 2014).", "startOffset": 143, "endOffset": 163}, {"referenceID": 10, "context": "For RDFox, these are the numbers reported by Motik et al. (2014) for computing a lower (left) and upper bound (right), respectively, on the possible inferences.", "startOffset": 45, "endOffset": 65}], "year": 2017, "abstractText": "In this work, we present a novel approach to ontology reasoning that is based on deep learning rather than logic-based formal reasoning. To this end, we introduce a new model for statistical relational learning that is built upon deep recursive neural networks, and give experimental evidence that it can easily compete with, or even outperform, existing logic-based reasoners on the task of ontology reasoning. More precisely, we compared our implemented system with one of the best logic-based ontology reasoners at present, RDFox, on a number of large standard benchmark datasets, and found that our system attained high reasoning quality, while being up to two orders of magnitude faster.", "creator": "LaTeX with hyperref package"}}}