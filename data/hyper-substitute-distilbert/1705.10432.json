{"id": "1705.10432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning", "abstract": "recent advances included robotic deep validation via reinforcement learning tools shown a promising path for shaping new control agents that can learn similar policies for challenging interaction norms. today new instruments address the systemic limitations underlying multiple reinforcement network methods such as integrated feature engineering through solving global / state space intervention requirements. in its paper, we leverage one of the state - of - our - art applied learning abilities, known as trust region policy optimization, ultimately produce intersection management for autonomous cities. we show that using this method, we can align thin - grained cooperative control of autonomous structures in a safe street plan together build a global design objective.", "histories": [["v1", "Tue, 30 May 2017 02:04:29 GMT  (1010kb,D)", "http://arxiv.org/abs/1705.10432v1", "Accepted in IEEE Smart World Congress 2017"]], "COMMENTS": "Accepted in IEEE Smart World Congress 2017", "reviews": [], "SUBJECTS": "cs.AI cs.RO cs.SY", "authors": ["hamid mirzaei", "tony givargis"], "accepted": false, "id": "1705.10432"}, "pdf": {"name": "1705.10432.pdf", "metadata": {"source": "CRF", "title": "Fine-grained acceleration control for autonomous intersection management using deep reinforcement learning", "authors": ["Hamid Mirzaei"], "emails": ["mirzaeib@uci.edu", "givargis@uci.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nPrevious works on autonomous intersection management (AIM) in urban areas have mostly focused on intersection arbitration as a shared resource among a large number of autonomous vehicles. In these works [1][2], high-level control of the vehicles is implemented such that the vehicles are self-contained agents that only communicate with the intersection management agent to reserve space-time slots in the intersection. This means that low-level vehicle navigation which involves acceleration and speed control is performed by each individual vehicle independent of other vehicles and intersection agents. This approach is appropriate for minor arterial roads where a large number of vehicles utilize the main roads at similar speeds while the adjacent intersections are far away.\nIn scenarios involving local roads, where the majority of the intersections are managed by stop signs, the flow of traffic is more efficiently managed using a fine-grained vehicle control methodology. For example, when two vehicles are crossing the intersection of two different roads at the same time, one vehicle can decelerate slightly to avoid collision with the other one or it can take another path to avoid confronting the other vehicle completely. Therefore, the nature of the AIM problem is a combination of route planning and real-time acceleration control of the vehicles. In this paper, we propose a novel AIM formulation which is the combination of route planning and fine-grained acceleration control. The main objective of the control task is to minimize travel time of the vehicles while avoiding collisions between them and other obstacles. In this\ncontext, since the movement of a vehicle is dependent on the other vehicles in the same vicinity, the motion data of all vehicles is needed in order to solve the AIM problem.\nTo explain the proposed AIM scheme, let us define a \u201czone\u201d as a rectangular area consisting of a number of intersections and segments of local roads. An agent for each zone collects the motion data and generates the acceleration commands for all autonomous vehicles within the zone\u2019s boundary. All the data collection and control command generation should be done in real-time. This centralized approach cannot be scaled to a whole city, regardless of the algorithm used, due to the large number of vehicles moving in a city which requires enormous computational load and leads to other infeasible requirements such as low-latency communication infrastructure. Fortunately, the spatial independence (i.e., the fact that navigation of the vehicles in one zone is independent of the vehicles in another zone that is far enough away) makes AIM an inherently local problem. Therefore, we can assign an agent for each local zone in a cellular scheme.\nThe cellular solution nevertheless leads to other difficulties that should be considered for a successful design of the AIM system. One issue is the dynamic nature of the transportation problem. Vehicles can enter or leave a zone controlled by an agent or they might change their planned destinations from time to time. To cope with these issues, the receding horizon control method can be employed where the agent repeatedly recalculates the acceleration command over a moving time horizon to take into account the mentioned changes. Additionally, two vehicles that are moving toward the same point on the boundary of two adjacent zones simultaneously might collide because the presence of each vehicle is not considered by the agent of the adjacent zone. This problem can be solved by adequate overlap between adjacent zones. Furthermore, any planned trip for a vehicle typically crosses multiple zones. Hence, a higher level planning problem should be solved first that determines the entry and exit locations of a vehicle in a zone.\nIn this paper we focus on the subproblem of acceleration control of the vehicles moving in a zone to minimize the total travel time. We use a deep reinforcement learning (RL) approach to tackle the fine-grained acceleration control\nar X\niv :1\n70 5.\n10 43\n2v 1\n[ cs\n.A I]\n3 0\nM ay\n2 01\nproblem since conventional control methods are not applicable because of the non-convex collision avoidance constraints [3]. Furthermore, if we want to incorporate more elements into the problem, such as obstacles or reward/penalty terms for gas usage, passenger comfort, etc., the explicit modeling becomes intractable and an optimal control law derivation will be computationally unattainable.\nRL methods can address the above mentioned limitations caused by the explicit modeling requirement and conventional control method limitations. The main advantage of RL is that most of the RL algorithms are \u201cmodel-free\u201d or at most need a simulation model of the physical system which is easier to develop than an explicit model. Moreover, the agent can learn optimal policies just by interacting with the environment or executing the simulation model. However, conventional RL techniques are only applicable in small-scale problem settings and require careful design of approximation functions. Emerging Deep RL methods [4] that leverage the deep neural networks to automatically extract features seem like promising solutions to shortcomings of the classical RL methods.\nThe main contributions of this paper are: (1) Definition and formulation of the AIM problem for local road settings where vehicles arecoordinated by fine-grained acceleration commands. (2) Employing TRPO proposed in [5] to solve the formulated AIM problem. (3) Incorporating collision avoidance constraint in the definition of RL environment as a safety mechanism."}, {"heading": "II. RELATED WORK", "text": "Advances in autonomous vehicles in recent years have revealed a portrait of a near future in which all vehicles will be driven by artificially intelligent agents. This emerging technology calls for an intelligent transportation system by redesigning the current transportation system which is intended to be used by human drivers. One of the interesting topics that arises in intelligent transportation systems is AIM. Dresner et al. have proposed a multi-agent AIM system in which vehicles communicate with intersection management agents to reserve a dedicated spatio-temporal trajectory at the intersection [2].\nIn [6], authors have proposed a self-organizing control framework in which a cooperative multi-agent control scheme is employed in addition to each vehicle\u2019s autonomy. The authors have proposed a priority-level system to determine the right-of-way through intersections based on vehicles\u2019 characteristics or intersection constraints.\nZohdy et al. presented an approach in which the Cooperative Adaptive Cruise Control (CACC) systems are leveraged to minimize delays and prevent clashes [7]. In this approach, the intersection controller communicates with the vehicles to recommend the optimal speed profile based on the vehicle\u2019s characteristics, motion data, weather conditions and intersection properties. Additionally, an optimization problem is solved to minimize the total difference of actual arrival times at the Intersection and the optimum times subject to conflictfree temporal constraints.\nA decentralized optimal control formulation is proposed in [8] in which the acceleration/deceleration of the vehicles are minimized subject to collision avoidance constraints.\nMakarem et al. introduced the notion of fluent coordination where smoother trajectories of the vehicles are achieved through a navigation function to coordinate the autonomous vehicles along predefined paths with expected arrival time at intersections to avoid collisions.\nIn all the aforementioned works, the AIM problem is formulated for only one intersection and no global minimum travel time objective is considered directly. Hausknecht et al. extended the approach proposed in [2] to multiintersection settings via dynamic traffic assignment and dynamic lane reversal [1]. Their problem formulation is based on intersection arbitration which is well suited to main roads with a heavy load of traffic.\nIn this paper, for the first time, we introduce fine-grained acceleration control for AIM. In contrast to previous works, Our proposed AIM scheme is applicable to local road intersections. We also propose an RL-based solution using Trust Region Policy Optimization to tackle the defined AIM problem."}, {"heading": "III. REINFORCEMENT LEARNING", "text": "In this section, we briefly review RL and introduce the notations used in the rest of the paper. In Fig. 1, the agentenvironment model of RL is shown. The \u201cagent\u201d interacts with the \u201cenvironment\u201d by applying \u201cactions\u201d that influence the environment state at the future time steps and observes the state and \u201creward\u201d in the next time step resulting from the action taken. The \u201creturn\u201d is defined as the sum of all the rewards from the current step to the end of current \u201cepisode\u201d:\nGt = T\u2211 i=t ri (1)\nwhere ri are future rewards and T is the total number of steps in the episode. An \u201cepisode\u201d is defined as a sequence of agent-environment interactions. In the last step of an episode the control task is \u201cfinished.\u201d Episode termination is defined specifically for the control task of the application.\nFor example, in the cart-pole balancing task, the agent is the controller, the environment is the cart-pole physical system, the action is the force command applied to the cart, and the reward can be defined as r = 1 as long as the pole is nearly\nin an upright position and a large negative number when the pole falls. The system states are cart position, cart speed, pole angle and pole angular speed. The agent task is to maximize the return Gt, which is equivalent to prevent pole from falling for the longest possible time duration.\nIn RL, a control policy is defined as a mapping of the system state space to the actions:\na = \u03c0(s) (2)\nwhere a is the action, s is the state and \u03c0 is the policy. An optimal policy is one that maximizes the return for all the states, i.e.:\nv\u03c0\u2217(s) \u2265 v\u03c0(s), for all s, \u03c0 (3)\nwhere v is the return function defined as the return achievable from state s by following policy \u03c0. Equation (3) means that the expected return under optimal policy \u03c0\u2217 is equal to or greater than any other policy for all the system states.\nThe concepts mentioned above to introduce RL are all applicable to deterministic cases, but generally we should be able to deal with inherent system uncertainty, measurement noise, or both. Therefore, we model the system as a Markov Decision Process (MDP) assuming that the environment has the Markov property [9]. However, contrary to most of the control design methods, many RL algorithms do not require the models to be known beforehand. The elimination of the requirement to model the system under control is a major strength of RL.\nA system has the Markov property if at a certain time instant, t, the system history can be captured in a set of state variables. Therefore, the next state of the system has a distribution which is only conditioned on the current state and the taken action at the current time, i.e.:\nst+1 \u223c P (st+1|st, at) (4)\nThe Markov property holds for many cyber-physical system application domains and therefore MDP and RL can be applied as the control algorithm. We can also define the stochastic policy which is a generalized version of (2) as a probability distribution of actions conditioned on the current state, i.e.:\nat \u223c \u03c0(at|st) (5)\nThe expected return function which is the expected value of \u2018return\u2019 defined in (1) can be written as:\nv\u03c0(st) = E a\u03c4\u223c\u03c0,\u03c4\u2265t [ \u221e\u2211 i=0 \u03b3irt+i ] (6)\nThis equation is defined for infinite episodes and the constant 0 < \u03b3 < 1 is introduced to ensure that the defined expected return is always a finite value, assuming the returns are bounded.\nAnother important concept in RL is the action-value function, Q\u03c0(s, a) defined as the expected return (value) if\naction at is taken at time t under policy \u03c0:\nQ\u03c0(st, at) = E a\u03c4\u223c\u03c0,\u03c4>t [ \u221e\u2211 i=0 \u03b3irt+i ] (7)\nThere are two main categories of methods to find the optimal policy. In the first category, Q\u03c0(s, a) is parameterized as Q\u03b8\u03c0(s, a) and the optimal action-value parameter vector \u03b8 is estimated in an iterative process. The optimal policy can be defined implicitly from Q\u03c0(s, a). For example, the greedy policy is the one that maximizes Q\u03c0(s, a) in each step:\nat = argmax a\n{Q\u03c0(s, a)} (8)\nIn the second category, which is called policy optimization and has been successfully applied to large-scale and continuous control systems [4], the policy is parameterized directly as \u03c0\u03b8(at|st) and the parameter vector of the optimal policy \u03b8 is estimated. The Trust Region Policy Method (TRPO) [5] is an example of the second category of methods that guarantees monotonic policy improvement and is designed to be scalable to large-scale settings. In each iteration of TRPO, a number of MDP trajectories are simulated (or actually experienced by the agent) and \u03b8 is updated to improve the policy. A high level description of TRPO is shown in algorithm 1."}, {"heading": "IV. PROBLEM STATEMENT", "text": "There is a set of vehicles in a grid street plan area consisting of a certain number of intersections. For simplicity, we assume that all the initial vehicle positions and the desired destinations are located at the intersections. There is a control agent for the entire area. The agent\u2019s task is to calculate the acceleration command for the vehicles in real-time (see Fig. 2). We assume that there are no still or moving obstacles other than vehicles\u2019 or street boundaries.\nThe input to the agent is the real-time state of the vehicles which consists of their positions and speeds. We are assuming that vehicles are point masses and their angular dynamics are ignored. However, to take the collision avoidance in the problem formulation, we define a safe radius for each vehicle and no objects (vehicles or street boundaries) should be closer than the safe radius to the vehicle.\nAlgorithm 1: High-Level description of Trust Region Optimization\nData: S . Actual system or Simulation model \u03c0\u03b8 . Parameterized Policy Result: \u03b8\u2217 . Optimal parameters\n1 repeat 2 Use S to generate trajectories of the system using current \u03c0\u03b8; 3 Perform one iteration of policy optimization using Monte Carlo method to get \u03b8new ; 4 \u03b8 \u2190 \u03b8new 5 until no more improvements; 6 return \u03b8\nThe objective is to drive all the vehicles to their respective destinations in a way that the total travel time is minimized. Furthermore, no collision should occur between any two vehicles or a vehicle and the street boundaries.\nTo minimize the total travel time, a positive reward is assigned to the terminal state in which all the vehicles approximately reach the destinations within some tolerance. A discount factor \u03b3 strictly less than one is used. Therefore, the agent should try to reach the terminal state as fast as possible to maximize the discounted return. However, by using only this reward, too many random walk trajectories are needed to discover the terminal state. Therefore, a negative reward is defined for each state, proportional to the total distance of the vehicles to their destinations as a hint of how far the terminal state is. This negative reward is not in contradiction with the main goal which is to minimize total travel time.\nTo avoid collisions, two different approaches can be considered: we can add large negative rewards for the collision states or we can incorporate a collision avoidance mechanism into the environment model. Our experiments show that the first approach makes the agent too conservative about moving the vehicles to minimize the probability of collisions. This might lead to extremely slow learning which makes it infeasible. Furthermore, collisions are inevitable even with large negative rewards which limits the effectiveness of learned policies in practice.\nFor the above mentioned reasons, the second approach is employed, i.e. the safety mechanism that is used in practice is included in the environment definition. The safety mechanism is activated whenever two vehicles are too close to each other or a vehicle is too close to the street boundary. In these cases, the vehicle built-in collision avoidance system will control the vehicle\u2019s acceleration and the acceleration commands from the RL agent are ignored as long as the distance is near the allowed safe radius of the vehicle. In the agent learning process these cases are simulated in a way that the vehicles\ncome to a full stop when they are closer than the safe radius to another vehicle or boundary. By applying this heuristic in the simulation model, the agent should avoid any \u201cnear collision\u201d situations explained above because the deceleration and acceleration cycles take a lot of time and will decrease the expected return.\nBased on the problem statement explained above, we can describe the RL formulation in the rest of the subsection. The state is defined as the following vector:\nst = ( x1t , y 1 t , v 1 xt, v 1 yt , . . . , xnt , y n t , v n x t, v n y t )\u1d40 (9)\nwhere (xit, y i t) and (v i xt, v i yt ) are the position and speed of vehicle i at time t. The action vector is defined as:\nat = ( a1xt, a 1 yt , . . . , anxt, a n y t )\u1d40 (10)\nwhere (aixt, a i yt ) is the acceleration command of vehicle i at time t. The reward function is defined as:\nr(s) = { 1 if \u2016(xi \u2212 dix, yi \u2212 dix)\u1d40\u2016 < \u03b7 (1 \u2264 i \u2264 n) \u2212\u03b1 \u2211n i=1\u2016(xi \u2212 dix, yi \u2212 dix)\u1d40\u2016 otherwise\n(11)\nwhere (dix, d i y) is the destination coordinates of vehicle i, \u03b7 is the distance tolerance and \u03b1 is a positive constant. Assuming no collision occurs, the state transition equations for the environment are defined as follows:\nxit+1 = satx,x(x i t + hvx i t) yit+1 = saty,y(y i t + hvy i t)\nvx i t+1 = satvm,vm(vx i t + hax i t) vy i t+1 = satvm,vm(vy i t + hay i t) (12)\nwhere h is the sampling time, (x, x, y, y) defines area limits, vm is the maximum speed and satw,w(.) is the saturation function defined as:\nsatw,w(x) =  w x \u2264 w w x \u2265 w x otherwise.\n(13)\nTo model the collisions, we should check certain conditions and set the speed to zero. A more detailed description of collision modeling is presented in Algorithm 2."}, {"heading": "A. Solving the AIM problem using TRPO", "text": "The simulation model can be implemented based on the RL formulation described in Section IV. To use TRPO, we need a parameterized stochastic policy, \u03c0\u03b8(at|st), in addition to the simulation model. The policy should specify the probability distribution for each element of the action vector defined in (10) as a function of the current state st.\nWe have used the sequential deep neural network (DNN) policy representation as described in [5]. The input layer receives the state containing the position and speed of the vehicles (defined in (9)). There are a number of hidden layers, each followed by tanh activation functions [10]. Finally, the\noutput layer generates the mean of a gaussian distribution for each element of the action vector.\nTo execute the optimal policy learned by TRPO in each sampling time, the agent calculates the forward-pass of DNN using the current state. Next, assuming that all the action elements have the same variance, the agent samples from the action gaussian distributions and applies the sampled actions to the environment as the vehicle acceleration commands."}, {"heading": "V. EVALUATION", "text": ""}, {"heading": "A. Baseline Method", "text": "To the best of our knowledge there is no other solution proposed for the fine-grained acceleration AIM problem introduced in this paper. Therefore, we use conventional optimization methods to study how close the proposed solution is to the optimal solution. Furthermore, we will see that the conventional optimization is able to solve the AIM problem only for very small-sized problems. This confirms that the proposed RL-based solution is a promising alternative to the conventional methods.\nTheoretically, the best solution to the problem defined in section IV can be obtained if we reformulate it as a conventional optimization problem. The following equations and inequalities describe the AIM optimization problem:\nat \u2217 = argmax\nat T\u22121\u2211 t=0 n\u2211 i=1 \u2016(xit \u2212 dix, yit \u2212 dix)\u1d40\u2016 (14)\ns. t. x \u2264 xit \u2264 x (1 \u2264 i \u2264 n) (15) y \u2264 yit \u2264 y (1 \u2264 i \u2264 n) (16)\nvm \u2264 vxit \u2264 vm (1 \u2264 i \u2264 n) (17) vm \u2264 vyit \u2264 vm (1 \u2264 i \u2264 n) (18)\nAlgorithm 2: State Transition Function\nData: st . State at time t at . Action at time t Result: st+1 . State at time t+ 1\n1 ax i t \u2190 satam,am(axit) ; 2 ay i t \u2190 satam,am(ay i t) ; 3 st+1 \u2190 updated state using (12) ; 4 vc1 \u2190 find all the vehicles colliding with street\nboundaries ; 5 speed elements of vc1in st+1 \u2190 0 ; 6 location elements of vc1in st+1 \u2190 closest point on the\nstreet boundary with the margin of ; 7 vc2 \u2190 find all the vehicles colliding with some other\nvehicle ; 8 speed elements of vc2in st+1 \u2190 0 ; 9 location elements of vc2in st+1 \u2190 pushed back location\nwith the distance of 2\u00d7 safe radius to the collided vehicle;\n10 return st+1\nam \u2264 axit \u2264 am (1 \u2264 i \u2264 n) (19) am \u2264 ayit \u2264 am (1 \u2264 i \u2264 n) (20)\n\u2212bN/2c \u2264 rit \u2264 bN/2c (1 \u2264 i \u2264 n, rit \u2208 Z) (21) \u2212bM/2c \u2264 cit \u2264 bM/2c (1 \u2264 i \u2264 n, cit \u2208 Z) (22)\nxi0 = s i x, y i 0 = s i y (1 \u2264 i \u2264 n) (23)\nxiT\u22121 = d i x, y i T\u22121 = d i y (1 \u2264 i \u2264 n) (24)\nvx i 0 = 0, vy i 0 = 0 (1 \u2264 i \u2264 n) (25)\nxit+1 = x i t + vx i t.h (1 \u2264 i \u2264 n) (26) yit+1 = y i t + vy i t.h (1 \u2264 i \u2264 n) (27)\nvx i t+1 = vx i t + ax i t.h (1 \u2264 i \u2264 n) (28) vy i t+1 = vy i t + ay i t.h (1 \u2264 i \u2264 n) (29)\n(xit \u2212 x j t ) 2 + (yit \u2212 y j t ) 2 \u2265 (2R)2 (1 \u2264 i < j \u2264 n) (30)\n|xit \u2212 cit.bw| \u2264 ( l\n2 \u2212R) or\n|yit \u2212 rit.bh| \u2264 ( l\n2 \u2212R) (1 \u2264 i \u2264 n) (31)\nwhere rit and c i t are the row number and column number of vehicle at time t, respectively, assuming the zone is a perfect rectangular grid; N and M are the number of rows and columns, respectively; bw and bh are block width and block height; l is the street width; R is the vehicle clearance radius; T is number of sampling times; and (six, s i y) is the source coordinates of vehicle i. In the above mentioned problem setting, (15) to (20) are the physical limit constraints. (23) to (25) describe the initial and final conditions. (26) to (29) are dynamic constraints. (30) is the vehicle-to-vehicle collision avoidance constraint and finally (31) is the vehicle-to-boundaries collision avoidance constraint.\nThe basic problem with the above formulation is that constraint (30) leads to a non-convex function and convex optimization algorithms cannot solve this problem. Therefore, a Mixed-Integer Nonlinear Programming (MINLP) algorithm should be used to solve this problem. Our experiments show that even a small-sized problem with two vehicles and 2\u00d72 grid cannot be solved with an MINLP algorithm, i.e. AOA[11]. To overcome this issue, we should reformulate the optimization problem using 1-norm and introduce new integer variables for the distance between vehicles using the ideas proposed in [12].\nTo achieve the best convergence and execution time by using a Mixed-integer Quadratic Programming (MIQP), the cost function and all constraints should be linear or quadratic. Furthermore, the \u201cor\u201d logic in (31) should be implemented using integer variables. The full MIQP problem can be written as the following equations and inequalities:\nat \u2217 = argmax\nat T\u22121\u2211 t=0 n\u2211 i=1 (xit \u2212 dix)2 + (yit \u2212 dix)2 (32)\ns. t. (15) to (29)\nbx i t, by i t \u2208 {0, 1} (1 \u2264 i \u2264 n) (33)\nbx i t + by i t \u2265 1 (1 \u2264 i \u2264 n) (34)\ncx i,j t ,cy i,j t , dx i,j t , dy i,j t \u2208 {0, 1} (1 \u2264 i < j \u2264 n) (35)\ncx i,j t +cy i,j t + dx i,j t + dy i,j t \u2265 1 (1 \u2264 i < j \u2264 n) (36)\nxit \u2212 x j t \u2265 2Rcx i,j t \u2212M(1\u2212 cx i,j t ) (1 \u2264 i < j \u2264 n)\n(37)\nxit \u2212 x j t \u2264 \u22122Rdx i,j t +M(1\u2212 dx i,j t ) (1 \u2264 i < j \u2264 n)\n(38)\nyit \u2212 y j t \u2265 2Rcyi,jt \u2212M(1\u2212 cy i,j t ) (1 \u2264 i < j \u2264 n)\n(39)\nyit \u2212 y j t \u2264 \u22122Rdy i,j t +M(1\u2212 dy i,j t ) (1 \u2264 i < j \u2264 n)\n(40)\nxit \u2212 citbw \u2264 ( l\n2 \u2212R)bxit +M(1\u2212 bx i t) (1 \u2264 i \u2264 n) (41)\nxit \u2212 citbw \u2265 \u2212( l\n2 \u2212R)bxit \u2212M(1\u2212 bx i t) (1 \u2264 i \u2264 n)\n(42)\nyit \u2212 citbw \u2264 ( l\n2 \u2212R)byit +M(1\u2212 by i t) (1 \u2264 i \u2264 n) (43)\nyit \u2212 citbw \u2265 \u2212( l\n2 \u2212R)byit \u2212M(1\u2212 by i t) (1 \u2264 i \u2264 n)\n(44)\nwhere M is a large positive number. (37) to (40) represent the vehicle-to-vehicle collision avoidance constraint using 1-norm:\n\u2016(xit, yit)\u1d40 \u2212 (x j t , y j t ) \u1d40\u20161 \u2265 2R (45)\nfor any two distinct vehicles i and j. This constraint is equivalent to the following:\n|xit \u2212 x j t | \u2265 2R or |yit \u2212 y j t | \u2265 2R \u2200t, (1 \u2264 i < j \u2264 n)\n(46)\nThe absolute value function displayed in (46) should be replaced by logical \u201cor\u201d of two linear conditions to avoid\nnonlinearity. Therefore we have the following four constraints represented by (37) to (40):\nxit \u2212 x j t \u2265 2R or xit \u2212 x j t \u2264 \u22122R or yit \u2212 y j t \u2265 2R or yit \u2212 y j t \u2264 \u22122R \u2200t, (1 \u2264 i < j \u2264 n)\n(47)\n(35) implements the \u201cor\u201d logic required in (47). (41) to (44) describe the vehicle-to-boundaries collision avoidance constraint:\n|xit \u2212 citbw| \u2264 ( l\n2 \u2212R)bxit or\n|yit \u2212 ritbw| \u2264 ( l\n2 \u2212R)byit \u2200t, (1 \u2264 i \u2264 n) (48)\nwhich is equivalent to:\n(xit \u2212 citbw \u2264 ( l\n2 \u2212R)bxit and xit \u2212 citbw \u2265 \u2212(\nl 2 \u2212R)bxit) or\n(yit \u2212 ritbw \u2264 ( l\n2 \u2212R)byit and y i t \u2212 ritbw \u2265 \u2212(\nl 2 \u2212R)byit)\n\u2200t, (1 \u2264 i \u2264 n) (49)\nThe \u201cor\u201d logic in this constraint is realized in (34). We will show in the next subsection that the explained conventional optimization formulation is not feasible except for very small-sized problems. Another limitation that makes the conventional method impractical is that this formulation works only for a perfect rectangular grid. However, the proposed RL method in this paper can be extended to arbitrary street layouts."}, {"heading": "B. Simulation results", "text": "The implementation of the TRPO in rllab library [4] is used to simulate the RL formulation of the AIM problem described in Section IV. For this purpose, the AIM state transition and reward calculation are implemented as an OpenAI Gym [13] environment.\nThe neural network used to approximate the policy is an MLP which consists of three hidden layers. Each hidden layer has 100 nodes (Fig. 6). Table I lists the parameters for the simulation. To speed up simulation, normalized units are used for the physical properties of the environment instead of realworld quantities.\nFig. 3 shows the small and large grid plans used for the simulation. The small and large circles represent the source and destination locations, respectively. The vehicles are placed at the intersections randomly at the beginning of each episode.\nThe destinations are also chosen randomly. When the simulator is reset, the same set of source and destination locations are used.\nThe small grid can be solved both by the baseline method and by the proposed RL method. However, the large grid can only be solved by the RL method because the MIQP algorithm could not find a feasible solution (which is not optimal necessarily) and was stopped after around 68 hours and using 21 GB of system memory. On the other hand, the RL method can solve the problem using 560 MB of system memory and 101 MB of GPU memory.\nTable II and Fig. 4 show the comparison of proposed RL and baseline method results. In Table II the total travel time is provided for both methods and Fig. 4 shows the vehicles\u2019 trajectories by running the navigation policy obtained by both solutions for the small examples.\nThe learning curve of the RL agent which is the expected return vs the training epoch number is shown in Fig. 7 for the large grid example. This figure shows that the learning rate is higher at the beginning which corresponds to the stage where in the agent is learning the very basics of driving and avoiding collisions, but improving the policy towards the optimal policy takes considerably more time. The increase in learning occurs after two epochs when the agent discovers the policy that successfully drives all the vehicles to the destination and the positive terminal reward is gained. Moreover, the trajectories of vehicles are depicted in Fig. 5 at three stages of the learning process, i.e. at the early stage, at epoch 2 where the learning curve slope suddenly decreases, and the end of the training.\nThe total number of \u201cnear collision\u201d incidents discussed in Section IV is shown in Fig. 8. Fig. 9 shows the total travel time as a function of training iteration."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have shown that Deep RL can be a promising solution for the problem of intelligent intersection management in local road settings where the number of vehicles is limited and fine-grained acceleration control and\nmotion planning can lead to a more efficient navigation of the autonomous vehicles. We proposed an RL environment definition in which collisions are avoided using a safety mechanism. Using this method instead of large penalties for collision in the reward function, the agent can learn the optimal policy faster and the learned policy can be used in practice where the safety mechanism is actually implemented. The experiments show that the conventional optimization methods are not able to solve the problem with the sizes that are solvable by the proposed method.\nSimilar to the learning process of human beings, the main benefit of the RL approach is that an explicit mathematical modeling of the system is not required and, more importantly, the challenging task of control design for a complex system is eliminated. However, since the automotive systems demand a high safety requirement, training of the RL agent using a simulation model is inevitable in most cases. However, developing a simulation model for a system is considerably simpler task compared to explicit modeling especially for systems with uncertainty.\nWhile the work at hand is a promising first step towards using RL in autonomous intersection management, a number of potential improvements can be mentioned that might be interesting to address in future work. First, the possibility of developing pre-trained DNNs similar to the works in\nother mainstream deep learning domains that reduce learning time can be a studied in future. Furthermore, a more advanced rewards system that includes gas usage penalties is another track to developing a practical intelligent intersection management algorithm."}, {"heading": "VII. ACKNOWLEDGEMENT", "text": "We would like to thank Nvidia for their generous hardware donation. This work was supported in part by the National Science Foundation under NSF grant number 1563652."}], "references": [{"title": "Autonomous intersection management: Multi-intersection optimization", "author": ["M. Hausknecht", "T.-C. Au", "P. Stone"], "venue": "2011 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2011, pp. 4581\u20134586.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A multiagent approach to autonomous intersection management", "author": ["K. Dresner", "P. Stone"], "venue": "Journal of artificial intelligence research, vol. 31, pp. 591\u2013656, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "A comparison of motion planning algorithms for cooperative collision avoidance of multiple cognitive automobiles", "author": ["C. Frese", "J. Beyerer"], "venue": "Intelligent Vehicles Symposium (IV), 2011 IEEE. IEEE, 2011, pp. 1156\u20131162.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M.I. Jordan", "P. Abbeel"], "venue": "CoRR, abs/1502.05477, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-organizing control framework for driverless vehicles", "author": ["M.N. Mladenovi\u0107", "M.M. Abbas"], "venue": "16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013). IEEE, 2013, pp. 2076\u20132081.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Intersection management for autonomous vehicles using icacc", "author": ["I.H. Zohdy", "R.K. Kamalanathsharma", "H. Rakha"], "venue": "2012 15th International IEEE Conference on Intelligent Transportation Systems. IEEE, 2012, pp. 1109\u20131114.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A decentralized optimal control framework for connected and automated vehicles at urban intersections", "author": ["A.A. Malikopoulos", "C.G. Cassandras", "Y.J. Zhang"], "venue": "arXiv preprint arXiv:1602.03786, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Performance analysis of various activation functions in generalized mlp architectures of neural networks", "author": ["B. Karlik", "A.V. Olgac"], "venue": "International Journal of Artificial Intelligence and Expert Systems, vol. 1, no. 4, pp. 111\u2013122, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "The aimms outer approximation algorithm for minlp", "author": ["M. Hunting"], "venue": "Paragon Decision Technology, Haarlem, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Mixed integer programming for multi-vehicle path planning", "author": ["T. Schouwenaars", "B. De Moor", "E. Feron", "J. How"], "venue": "Control Conference (ECC), 2001 European. IEEE, 2001, pp. 2603\u20132608.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Openai gym", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang", "W. Zaremba"], "venue": "2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In these works [1][2], high-level control of the vehicles is implemented such that the vehicles are self-contained agents that only communicate with the intersection management agent to reserve space-time slots in the intersection.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "In these works [1][2], high-level control of the vehicles is implemented such that the vehicles are self-contained agents that only communicate with the intersection management agent to reserve space-time slots in the intersection.", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "problem since conventional control methods are not applicable because of the non-convex collision avoidance constraints [3].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "Emerging Deep RL methods [4] that leverage the deep neural networks to automatically extract features seem like promising solutions to shortcomings of the classical RL methods.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "(2) Employing TRPO proposed in [5] to solve the formulated AIM problem.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "have proposed a multi-agent AIM system in which vehicles communicate with intersection management agents to reserve a dedicated spatio-temporal trajectory at the intersection [2].", "startOffset": 175, "endOffset": 178}, {"referenceID": 5, "context": "In [6], authors have proposed a self-organizing control framework in which a cooperative multi-agent control scheme is employed in addition to each vehicle\u2019s autonomy.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "presented an approach in which the Cooperative Adaptive Cruise Control (CACC) systems are leveraged to minimize delays and prevent clashes [7].", "startOffset": 139, "endOffset": 142}, {"referenceID": 7, "context": "A decentralized optimal control formulation is proposed in [8] in which the acceleration/deceleration of the vehicles are minimized subject to collision avoidance constraints.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "extended the approach proposed in [2] to multiintersection settings via dynamic traffic assignment and dynamic lane reversal [1].", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "extended the approach proposed in [2] to multiintersection settings via dynamic traffic assignment and dynamic lane reversal [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "Therefore, we model the system as a Markov Decision Process (MDP) assuming that the environment has the Markov property [9].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "In the second category, which is called policy optimization and has been successfully applied to large-scale and continuous control systems [4], the policy is parameterized directly as \u03c0(at|st) and the parameter vector of the optimal policy \u03b8 is estimated.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "The Trust Region Policy Method (TRPO) [5] is an example of the second category of methods that guarantees monotonic policy improvement and is designed to be scalable to large-scale settings.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "We have used the sequential deep neural network (DNN) policy representation as described in [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 9, "context": "There are a number of hidden layers, each followed by tanh activation functions [10].", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "AOA[11].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "To overcome this issue, we should reformulate the optimization problem using 1-norm and introduce new integer variables for the distance between vehicles using the ideas proposed in [12].", "startOffset": 182, "endOffset": 186}, {"referenceID": 3, "context": "The implementation of the TRPO in rllab library [4] is used to simulate the RL formulation of the AIM problem described in Section IV.", "startOffset": 48, "endOffset": 51}, {"referenceID": 12, "context": "For this purpose, the AIM state transition and reward calculation are implemented as an OpenAI Gym [13] environment.", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Recent advances in combining deep learning and Reinforcement Learning have shown a promising path for designing new control agents that can learn optimal policies for challenging control tasks. These new methods address the main limitations of conventional Reinforcement Learning methods such as customized feature engineering and small action/state space dimension requirements. In this paper, we leverage one of the state-of-the-art Reinforcement Learning methods, known as Trust Region Policy Optimization, to tackle intersection management for autonomous vehicles. We show that using this method, we can perform fine-grained acceleration control of autonomous vehicles in a grid street plan to achieve a global design objective.", "creator": "LaTeX with hyperref package"}}}