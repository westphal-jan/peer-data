{"id": "1705.10694", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Deep Learning is Robust to Massive Label Noise", "abstract": "deep encoding networks thriving on biased supervised datasets have led to impressive results in recent years. however, recognizing well - annotated data which be prohibitively spent and brain - consuming while collect, recent work explicitly explored such use using correlated versus noisy datasets that has be still easily observed. in some dimension, we investigate the behavior about traditional output networks for training sets with massively distorted labels. some show that successful learning is adequate even with an essentially arbitrary amount delay noise. for example, on mnist we find enough accuracy of executing all artifacts is still attainable even when the results last been diluted with the noisy examples for each clean example. consistency consistency applies across clear stimuli of label noise, even when noisy labels are biased during making decisions. further, we examining whether the required dataset size toward good picking increases with substantial label noise. historically, we found possible actionable techniques through controlling efficiency in the regime advocating high context recall.", "histories": [["v1", "Tue, 30 May 2017 15:10:51 GMT  (102kb,D)", "https://arxiv.org/abs/1705.10694v1", null], ["v2", "Wed, 31 May 2017 02:02:56 GMT  (102kb,D)", "http://arxiv.org/abs/1705.10694v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["david rolnick", "reas veit", "serge belongie", "nir shavit"], "accepted": false, "id": "1705.10694"}, "pdf": {"name": "1705.10694.pdf", "metadata": {"source": "CRF", "title": "Deep Learning is Robust to Massive Label Noise", "authors": ["David Rolnick", "Andreas Veit", "Serge Belongie", "Nir Shavit"], "emails": ["drolnick@mit.edu,", "av443@cornell.edu,", "sjb344@cornell.edu,", "shanir@csail.mit.edu"], "sections": null, "references": [{"title": "Learning with annotation noise", "author": ["Eyal Beigman", "Beata Beigman Klebanov"], "venue": "In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Identifying mislabeled training data", "author": ["Carla E Brodley", "Mark A Friedl"], "venue": "Journal of artificial intelligence research (JAIR),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Learning visual features from large weakly supervised data", "author": ["Armand Joulin", "Laurens van der Maaten", "Allan Jabri", "Nicolas Vasilache"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Openimages: A public dataset for large-scale multi-label and multiclass image classification", "author": ["I Krasin", "T Duerig", "N Alldrin", "A Veit", "S Abu-El-Haija", "S Belongie", "D Cai", "Z Feng", "V Ferrari", "V Gomes"], "venue": "Dataset available from https://github. com/openimages,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "The unreasonable effectiveness of noisy data for fine-grained recognition", "author": ["Jonathan Krause", "Benjamin Sapp", "Andrew Howard", "Howard Zhou", "Alexander Toshev", "Tom Duerig", "James Philbin", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "One weird trick for parallelizing convolutional neural networks", "author": ["Alex Krizhevsky"], "venue": "arXiv preprint arXiv:1404.5997,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning multiple layers of features from tiny", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Quoc V Le"], "venue": "In International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Noise tolerance under risk minimization", "author": ["Naresh Manwani", "PS Sastry"], "venue": "IEEE Transactions on cybernetics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels", "author": ["Ishan Misra", "C Lawrence Zitnick", "Margaret Mitchell", "Ross Girshick"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Learning with noisy labels. In Advances in neural information processing", "author": ["Nagarajan Natarajan", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Ambuj Tewari"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "The curious robot: Learning visual representations via physical interactions", "author": ["Lerrel Pinto", "Dhiraj Gandhi", "Yuanfeng Han", "Yong-Lae Park", "Abhinav Gupta"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Training convolutional networks with noisy labels", "author": ["Sainbayar Sukhbaatar", "Joan Bruna", "Manohar Paluri", "Lubomir Bourdev", "Rob Fergus"], "venue": "arXiv preprint arXiv:1406.2080,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Intriguing properties of neural networks", "author": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "YFCC100M: The new data in multimedia research", "author": ["Bart Thomee", "David A Shamma", "Gerald Friedland", "Benjamin Elizalde", "Karl Ni", "Douglas Poland", "Damian Borth", "Li-Jia Li"], "venue": "Communications of the ACM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection", "author": ["Grant Van Horn", "Steve Branson", "Ryan Farrell", "Scott Haber", "Jessie Barry", "Panos Ipeirotis", "Pietro Perona", "Serge Belongie"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Learning from noisy large-scale datasets with minimal supervision", "author": ["Andreas Veit", "Neil Alldrin", "Gal Chechik", "Ivan Krasin", "Abhinav Gupta", "Serge Belongie"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Residual networks behave like ensembles of relatively shallow networks", "author": ["Andreas Veit", "Michael Wilber", "Serge Belongie"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Unsupervised learning of visual representations using videos", "author": ["Xiaolong Wang", "Abhinav Gupta"], "venue": "In International Conference on Computer Vision, (ICCV),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Learning from massive noisy labeled data for image classification", "author": ["Tong Xiao", "Tian Xia", "Yi Yang", "Chang Huang", "Xiaogang Wang"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Understanding deep learning requires rethinking generalization", "author": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2017}, {"title": "Semi-supervised learning literature survey", "author": ["Xiaojin Zhu"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "For example, collecting the popular ImageNet dataset [3] required more than a year of human labor on Amazon Mechanical Turk.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "To address this limitation, other training paradigms have been investigated to alleviate the need for expensive annotations, such as unsupervised learning [10], self-supervised learning [15, 22] and learning from noisy annotations [5, 14, 20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 13, "context": "To address this limitation, other training paradigms have been investigated to alleviate the need for expensive annotations, such as unsupervised learning [10], self-supervised learning [15, 22] and learning from noisy annotations [5, 14, 20].", "startOffset": 186, "endOffset": 194}, {"referenceID": 20, "context": "To address this limitation, other training paradigms have been investigated to alleviate the need for expensive annotations, such as unsupervised learning [10], self-supervised learning [15, 22] and learning from noisy annotations [5, 14, 20].", "startOffset": 186, "endOffset": 194}, {"referenceID": 4, "context": "To address this limitation, other training paradigms have been investigated to alleviate the need for expensive annotations, such as unsupervised learning [10], self-supervised learning [15, 22] and learning from noisy annotations [5, 14, 20].", "startOffset": 231, "endOffset": 242}, {"referenceID": 12, "context": "To address this limitation, other training paradigms have been investigated to alleviate the need for expensive annotations, such as unsupervised learning [10], self-supervised learning [15, 22] and learning from noisy annotations [5, 14, 20].", "startOffset": 231, "endOffset": 242}, {"referenceID": 18, "context": "To address this limitation, other training paradigms have been investigated to alleviate the need for expensive annotations, such as unsupervised learning [10], self-supervised learning [15, 22] and learning from noisy annotations [5, 14, 20].", "startOffset": 231, "endOffset": 242}, {"referenceID": 5, "context": ", [6, 18]) can often be attained, for example from web sources, with partial or unreliable annotation.", "startOffset": 2, "endOffset": 9}, {"referenceID": 16, "context": ", [6, 18]) can often be attained, for example from web sources, with partial or unreliable annotation.", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": ", [1, 5, 7, 12, 13, 19].", "startOffset": 2, "endOffset": 23}, {"referenceID": 4, "context": ", [1, 5, 7, 12, 13, 19].", "startOffset": 2, "endOffset": 23}, {"referenceID": 6, "context": ", [1, 5, 7, 12, 13, 19].", "startOffset": 2, "endOffset": 23}, {"referenceID": 10, "context": ", [1, 5, 7, 12, 13, 19].", "startOffset": 2, "endOffset": 23}, {"referenceID": 11, "context": ", [1, 5, 7, 12, 13, 19].", "startOffset": 2, "endOffset": 23}, {"referenceID": 17, "context": ", [1, 5, 7, 12, 13, 19].", "startOffset": 2, "endOffset": 23}, {"referenceID": 1, "context": ", [2].", "startOffset": 2, "endOffset": 5}, {"referenceID": 24, "context": "To address this challenge, they often use semi-supervised approaches by combining noisy data with a small set of clean labels [26].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "Some approaches model the label noise as conditionally independent from the input image [14, 16] and some propose image-conditional noise models [20, 23].", "startOffset": 88, "endOffset": 96}, {"referenceID": 14, "context": "Some approaches model the label noise as conditionally independent from the input image [14, 16] and some propose image-conditional noise models [20, 23].", "startOffset": 88, "endOffset": 96}, {"referenceID": 18, "context": "Some approaches model the label noise as conditionally independent from the input image [14, 16] and some propose image-conditional noise models [20, 23].", "startOffset": 145, "endOffset": 153}, {"referenceID": 21, "context": "Some approaches model the label noise as conditionally independent from the input image [14, 16] and some propose image-conditional noise models [20, 23].", "startOffset": 145, "endOffset": 153}, {"referenceID": 19, "context": "[21] show that network architectures with residual connections have a high redundancy in terms of parameters and are robust to the deletion of multiple complete layers during test time.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] investigate the robustness of neural networks to adversarial examples.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": ", [16, 19, 25]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 17, "context": ", [16, 19, 25]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 23, "context": ", [16, 19, 25]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": ", [16, 19, 25]), in which an increase in noise also implies a decrease in the absolute number of correct examples.", "startOffset": 2, "endOffset": 14}, {"referenceID": 17, "context": ", [16, 19, 25]), in which an increase in noise also implies a decrease in the absolute number of correct examples.", "startOffset": 2, "endOffset": 14}, {"referenceID": 23, "context": ", [16, 19, 25]), in which an increase in noise also implies a decrease in the absolute number of correct examples.", "startOffset": 2, "endOffset": 14}, {"referenceID": 15, "context": "Deep neural networks have been shown to be exceedingly brittle to adversarial noise patterns [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "For this experiment we focus on the task of image classification and work with two commonly used datasets, MNIST [11] and CIFAR-10 [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "We compare various architectures of neural networks: multilayer perceptrons with different numbers of hidden layers, convolutional networks (ConvNets) with different numbers of convolutional layers, and on CIFAR-10 additionally a Residual Network (ResNet) with 101 layers [4].", "startOffset": 272, "endOffset": 275}, {"referenceID": 23, "context": "For white noise, performance does not drop regardless of noise level; this is in line with prior work that has shown that neural networks are able to fit random input [25].", "startOffset": 167, "endOffset": 171}, {"referenceID": 22, "context": "All models are trained with AdaDelta [24] as optimizer and a batch size of 128.", "startOffset": 37, "endOffset": 41}, {"referenceID": 2, "context": ", [3]) that traditional deep learning relies upon large datasets.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "In particular, it has been shown that the smaller the batch size the lower the optimal learning rate [8].", "startOffset": 101, "endOffset": 104}], "year": 2017, "abstractText": "Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.", "creator": "LaTeX with hyperref package"}}}