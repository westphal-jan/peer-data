{"id": "1705.10744", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Knowledge Base Completion: Baselines Strike Back", "abstract": "many studies have appeared published on an knowledge engine regression index in the past semester,. most of these introduce novel evidence concerning learning relationships that are evaluated on standard networks such as sec and wn18. this argument shows off the accuracy of standard 200 models available upon 3d prototype can be outperformed so an appropriately judged baseline - accurate reimplementation of the distmult model. project feedback cast doubt on the claim that the timing improvements of recent models are due with architectural changes as being even random - harmonic functions or parameter temperature scales. this should meet future criticism and re - consider how the development of models is evaluated and reported.", "histories": [["v1", "Tue, 30 May 2017 16:54:19 GMT  (43kb,D)", "http://arxiv.org/abs/1705.10744v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["rudolf kadlec", "ondrej bajgar", "jan kleindienst"], "accepted": false, "id": "1705.10744"}, "pdf": {"name": "1705.10744.pdf", "metadata": {"source": "CRF", "title": "Knowledge Base Completion: Baselines Strike Back", "authors": ["Rudolf Kadlec", "Ondrej Bajgar", "Jan Kleindienst"], "emails": ["jankle}@cz.ibm.com", "Hits@10"], "sections": [{"heading": "1 Introduction", "text": "Projects such as Wikidata1 or earlier Freebase (Bollacker et al., 2008) have successfully accumulated a formidable amount of knowledge in the form of \u3008entity1 - relation - entity2\u3009 triplets. Given this vast body of knowledge, it would be extremely useful to teach machines to reason over such knowledge bases. One possible way to test such reasoning is knowledge base completion (KBC).\nThe goal of the KBC task is to fill in the missing piece of information into an incomplete triple. For instance, given a query \u3008Donald Trump, president of, ?\u3009 one should predict that the target entity is USA.\nMore formally, given a set of entities E and a set of binary relations R over these entities, a knowledge base (sometimes also referred to as a knowl-\n1https://www.wikidata.org/\nedge graph) can be specified by a set of triplets \u3008h, r, t\u3009 where h, t \u2208 E are head and tail entities respectively and r \u2208 R is a relation between them. In entity KBC the task is to predict either the tail entity given a query \u3008h, r, ?\u3009, or to predict the head entity given \u3008?, r, t\u3009.\nNot only can this task be useful to test the generic ability of a system to reason over a knowledge base, but it can also find use in expanding existing incomplete knowledge bases by deducing new entries from existing ones.\nAn extensive amount of work has been published on this task (for a review see (Nickel et al., 2015; Nguyen, 2017), for a plain list of citations see Table 2). Among those DistMult (Yang et al., 2015) is one of the simplest.2 Still this paper shows that even a simple model with proper hyperparameters and training objective evaluated using the standard metric of Hits@10 can outperform 27 out of 29 models which were evaluated on two standard KBC datasets, WN18 and FB15k (Bordes et al., 2013).\nThis suggests that there may be a huge space for improvement in hyper-parameter tuning even for the more complex models, which may be in many ways better suited for relational learning, e.g. can capture directed relations."}, {"heading": "2 The Model", "text": "Inspired by the success of word embeddings in natural language processing, distributional models for KBC have recently been extensively studied. Distributional models represent the entities and sometimes even the relations as N -dimensional real vectors3, we will denote these vectors by bold font, h, r, t \u2208 RN .\n2We could even say too simple given that it assumes symmetry of all relations which is clearly unrealistic.\n3Some models represent relations as matrices instead.\nar X\niv :1\n70 5.\n10 74\n4v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 7\nThe DistMult model was introduced by Yang et al. (2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss. Trouillon et al. (2016) obtained even better empirical result on the FB15k dataset just by changing DistMult\u2019s hyper-parameters.\nDistMult model computes a score for each triplet \u3008h, r, t\u3009 as\ns(h, r, t) = hT \u00b7Wr \u00b7 t = N\u2211 i=1 hiriti\nwhere Wr is a diagonal matrix with elements of vector r on its diagonal. Therefore the model can be alternatively rewritten as shown in the second equality.\nIn the end our implementation normalizes the scores by a softmax function. That is\nP (t|h, r) = exp(s(h, r, t))\u2211 t\u0304\u2208Eh,r exp(s(h, r, t\u0304))\nwhere Eh,r is a set of candidate answer entities for the \u3008h, r, ?\u3009 query."}, {"heading": "3 Experiments", "text": "Datasets. In our experiments we use two standard datasets WN18 derived from WordNet (Fellbaum, 1998) and FB15k derived from the Freebase knowledge graph (Bollacker et al., 2008).\nMethod. For evaluation, we use the filtered evaluation protocol proposed by Bordes et al. (2013). During training and validation we transform each triplet \u3008h, r, t\u3009 into two examples: tail query \u3008h, r, ?\u3009 and head query \u3008?, r, t\u3009. We train the model by minimizing negative log-likelihood (NLL) of the ground truth triplet \u3008h, r, t\u3009 against randomly sampled pool of M negative triplets \u3008h, r, t\u2032\u3009, t\u2032 \u2208 E \\ {t} (this applies for tail queries, head queries are handled analogically).\nIn the filtered protocol we rank the validation or test set triplet against all corrupted (supposedly untrue) triplets \u2013 those that do not appear in the train, valid and test dataset (excluding the test set triplet in question itself). Formally, for a query \u3008h, r, ?\u3009 where the correct answer is t, we compute the rank of \u3008h, r, t\u3009 in a candidate set Ch,r = {\u3008h, r, t\u2032\u3009 : \u2200t\u2032 \u2208 E} \\ (Train\u222aV alid\u222aTest)\u222a {\u3008h, r, t\u3009}, where Train, V alid and Test are sets\nof true triplets. Head queries \u3008?, r, t\u3009 are handled analogically. Note that softmax normalization is suitable under the filtered protocol since exactly one correct triplet is guaranteed to be among the candidates.\nIn our preliminary experiments on FB15k, we varied the batch size b, embedding dimensionality N , number of negative samples in training M , L2 regularization parameter and learning rate lr. Based on these experiments we fixed lr=0.001, L2=0.0 and we decided to focus on influence of batch size, embedding dimension and number of negative samples. For final experiments we trained several models from hyperparameter range: N \u2208 {128, 256, 512, 1024}, b \u2208 {16, 32, 64, 128, 256, 512, 1024, 2048} and M \u2208 {20, 50, 200, 500, 1000, 2000}.\nWe train the final models using Adam (Kingma and Ba, 2015) optimizer (lr = 0.001, \u03b21 = 0.9, \u03b22 = 0.999, = 10\n\u22128, decay = 0.0). We also performed limited experiments with Adagrad, Adadelta and plain SGD. Adagrad usually required substantially more iterations than ADAM to achieve the same performance. We failed to obtain competitive performance with Adadelta and plain SGD. On FB15k and WN18 validation datasets the best hyper-parameter combinations were N = 512, b = 2048, M = 2000 and N = 256, b = 1024, M = 1000, respectively. Note that we tried substantially more hyperparameter combinations on FB15k than on WN18. Unlike most previous works we do not normalize neither entity nor relation embeddings.\nTo prevent over-fitting, we stop training once Hits@10 stop improving on the validation set. On the FB15k dataset our Keras (Chollet, 2015) based implementation with TensorFlow (Abadi et al., 2015) backend needed about 4 hours to converge when run on a single GeForce GTX 1080 GPU.\nResults. Besides single models, we also evaluated performance of a simple ensemble that averages predictions of multiple models. This technique consistently improves performance of machine learning models in many domains and it slightly improved results also in this case.\nThe results of our experiments together with previous results from the literature are shown in Table 2. DistMult with proper hyperparameters twice achieves the second best score and once the third best score in three out of four commonly reported benchmarks (mean rank (MR) and\nHits@10 on WN18 and FB15k). On FB15k only the IRN model (Shen et al., 2016) shows better Hits@10 and the ProjE (Shi and Weniger, 2017) has better MR.\nOur implementation has the best reported mean reciprocal rank (MRR) on FB15k, however this metric is not reported that often. MRR is a metric of ranking quality that is less sensitive to outliers than MR.\nOn WN18 dataset again the IRN model together with R-GCN+ shows better Hits@10. However, in MR and MRR DistMult performs poorly. Even though DistMult\u2019s inability to model asymmetric relations still allows it to achieve competitive results in Hits@10 the other metrics clearly show its limitations. These results highlight qualitative differences between FB15k and WN18 datasets.\nInterestingly on FB15k recently published models (including our baseline) that use only r and h or t as their input outperform models that utilize richer features such as text or knowledge base path information. This shows a possible gap for future improvement.\nTable 1 shows accuracy (Hits@1) of several models that reported this metric. On WN18 our implementation performs worse than HolE and ComplEx models (that are equivalent as shown by Hayashi and Shimbo (2017)). On FB15k our implementation outperforms all other models."}, {"heading": "3.1 Hyper-parameter influence on FB15k", "text": "In our experiments on FB15k we found that increasing the number of negative examples M had a positive effect on performance.\nAnother interesting observation is that batch size has a strong influence on final performance. Larger batch size always lead to better results, for instance Hits@10 improved by 14.2% absolute when the batch size was increased from 16 to 2048. See Figure 1 for details.\nCompared to previous works that trained DistMult on these datasets (for results see bottom of Table 2) we use different training objective than Yang et al. (2015) and Trouillon et al. (2017) that optimized max margin objective and NLL of softplus activation function (softplus(x) = ln(1 + ex)), respectively. Similarly to Toutanova and Chen (2015) we use NLL of softmax function, however we use ADAM optimizer instead of RProp (Riedmiller and Braun, 1993)."}, {"heading": "4 Conclusion", "text": "Simple conclusions from our work are: 1) Increasing batch size dramatically improves performance of DistMult, which raises a question whether other models would also significantly benefit from similar hyper-parameter tuning or different training objectives; 2) In the future it might be better to focus more on metrics less frequently used in this domain, like Hits@1 (accuracy) and MRR since for instance on WN18 many models achieve similar, very high Hits@10, however even models that are competitive in Hits@10 underperform in Hits@1, which is the case of our DistMult implementation.\nA lot of research focus has recently been centred on the filtered scenario which is why we decided to use it in this study. An advantage is that it is easy to evaluate. However the scenario trains the model to expect that there is only a single correct answer among the candidates which is unrealistic in the context of knowledge bases. Hence\nfuture research could focus more on the raw scenario which however requires using other information retrieval metrics such as mean average precision (MAP), previously used in KBC for instance by Das et al. (2017).\nWe see this preliminary work as a small contribution to the ongoing discussion in the machine learning community about the current strong focus on state-of-the-art empirical results when it might be sometimes questionable whether they were achieved due to a better model/algorithm or just by more extensive hyper-parameter search. For broader discussion see (Church, 2017).\nIn light of these results we think that the field would benefit from a large-scale empirical comparative study of different KBC algorithms, similar to a recent study of word embedding models (Levy et al., 2015)."}], "references": [{"title": "TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems", "author": ["Zheng"], "venue": null, "citeRegEx": "2015.,? \\Q2015\\E", "shortCiteRegEx": "2015.", "year": 2015}, {"title": "Freebase: A collaboratively created graph database for structuring human knowledge", "author": ["Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor."], "venue": "Proceedings of the 2008 ACM SIGMOD International", "citeRegEx": "Bollacker et al\\.,? 2008", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "A semantic matching energy function for learning with multi-relational data", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio."], "venue": "Machine Learning 94(2):233\u2013259.", "citeRegEx": "Bordes et al\\.,? 2014", "shortCiteRegEx": "Bordes et al\\.", "year": 2014}, {"title": "Translating embeddings for modeling multirelational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."], "venue": "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger,", "citeRegEx": "Bordes et al\\.,? 2013", "shortCiteRegEx": "Bordes et al\\.", "year": 2013}, {"title": "Emerging trends: I did it, I did it, I did it, but.", "author": ["Kenneth Ward Church"], "venue": "Natural Language Engineering", "citeRegEx": "Church.,? \\Q2017\\E", "shortCiteRegEx": "Church.", "year": 2017}, {"title": "Chains of Reasoning over Entities, Relations, and Text using Recurrent Neural Networks", "author": ["Rajarshi Das", "Arvind Neelakantan", "David Belanger", "Andrew Mccallum."], "venue": "EACL .", "citeRegEx": "Das et al\\.,? 2017", "shortCiteRegEx": "Das et al\\.", "year": 2017}, {"title": "WordNet", "author": ["Christiane Fellbaum."], "venue": "Wiley Online Library.", "citeRegEx": "Fellbaum.,? 1998", "shortCiteRegEx": "Fellbaum.", "year": 1998}, {"title": "GAKE: Graph Aware Knowledge Embedding", "author": ["Jun Feng", "Minlie Huang", "Yang Yang", "Xiaoyan Zhu."], "venue": "Proceedings of the 27th International Conference on Computational Linguistics (COLING\u201916). pages 641\u2013651.", "citeRegEx": "Feng et al\\.,? 2015", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Composing Relationships with Translations", "author": ["Alberto Garc\u0131\u0301a-Dur\u00e1n", "Antoine Bordes", "Nicolas Usunier"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Garc\u0131\u0301a.Dur\u00e1n et al\\.", "year": 2015}, {"title": "Combining Two And Three-Way Embeddings Models for Link Prediction in Knowledge Bases", "author": ["Alberto Garcia-Duran", "Antoine Bordes", "Nicolas Usunier", "Yves Grandvalet."], "venue": "Journal of Artificial Intelligence Research 55:715\u2014-742.", "citeRegEx": "Garcia.Duran et al\\.,? 2016", "shortCiteRegEx": "Garcia.Duran et al\\.", "year": 2016}, {"title": "On the Equivalence of Holographic and Complex Embeddings for Link Prediction pages 1\u20138", "author": ["Katsuhiko Hayashi", "Masashi Shimbo."], "venue": "http://arxiv.org/abs/1702.05563.", "citeRegEx": "Hayashi and Shimbo.,? 2017", "shortCiteRegEx": "Hayashi and Shimbo.", "year": 2017}, {"title": "Learning to Represent Knowledge Graphs with Gaussian Embedding", "author": ["Shizhu He", "Kang Liu", "Guoliang Ji", "Jun Zhao."], "venue": "CIKM \u201915 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management pages 623\u2013", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Knowledge Graph Embedding via Dynamic Mapping Matrix", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Ji et al\\.,? 2015", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Knowledge Graph Completion with Adaptive Sparse Transfer Matrix", "author": ["Guoliang Ji", "Kang Liu", "Shizhu He", "Jun Zhao."], "venue": "Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016) pages 985\u2013991.", "citeRegEx": "Ji et al\\.,? 2016", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba."], "venue": "International Conference on Learning Representations pages 1\u2013", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan."], "venue": "Transactions of the Association for Computational Linguistics 3:211\u2013225. https://doi.org/10.1186/1472-6947-15-", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun."], "venue": "CoRR abs/1506.00379. http://arxiv.org/abs/1506.00379.", "citeRegEx": "Lin et al\\.,? 2015a", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning Entity and Relation Embeddings for Knowledge Graph Completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence Learning pages 2181\u2013", "citeRegEx": "Lin et al\\.,? 2015b", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Hierarchical random walk inference in knowledge graphs", "author": ["Qiao Liu", "Liuyi Jiang", "Minghao Han", "Yao Liu", "Zhiguang Qin."], "venue": "Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval.", "citeRegEx": "Liu et al\\.,? 2016", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "An overview of embedding models of entities and relationships for knowledge base completion https://arxiv.org/pdf/1703.08098.pdf", "author": ["Dat Quoc Nguyen"], "venue": null, "citeRegEx": "Nguyen.,? \\Q2017\\E", "shortCiteRegEx": "Nguyen.", "year": 2017}, {"title": "STransE: a novel embedding model of entities and relationships in knowledge bases", "author": ["Dat Quoc Nguyen", "Kairit Sirts", "Lizhen Qu", "Mark Johnson."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "A Review of Relational Machine Learning for Knowledge Graph", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich."], "venue": "Proceedings of the IEEE (28):1\u201323. https://doi.org/10.1109/JPROC.2015.2483592.", "citeRegEx": "Nickel et al\\.,? 2015", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "Holographic Embeddings of Knowledge Graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio."], "venue": "AAAI pages 1955\u20131961. http://arxiv.org/abs/1510.04935.", "citeRegEx": "Nickel et al\\.,? 2016", "shortCiteRegEx": "Nickel et al\\.", "year": 2016}, {"title": "Discriminative gaifman models", "author": ["Mathias Niepert."], "venue": "Advances in Neural Information Processing Systems. pages 3405\u20133413.", "citeRegEx": "Niepert.,? 2016", "shortCiteRegEx": "Niepert.", "year": 2016}, {"title": "A direct adaptive method for faster backpropagation learning: The rprop algorithm", "author": ["Martin Riedmiller", "Heinrich Braun."], "venue": "Neural Networks, 1993., IEEE International Conference on. IEEE, pages 586\u2013591.", "citeRegEx": "Riedmiller and Braun.,? 1993", "shortCiteRegEx": "Riedmiller and Braun.", "year": 1993}, {"title": "Modeling Relational Data with Graph Convolutional Networks http://arxiv.org/abs/1703.06103", "author": ["Michael Schlichtkrull", "Thomas N. Kipf", "Peter Bloem", "Rianne van den Berg", "Ivan Titov", "Max Welling"], "venue": null, "citeRegEx": "Schlichtkrull et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Schlichtkrull et al\\.", "year": 2017}, {"title": "Implicit reasonet: Modeling large-scale structured relationships with shared memory", "author": ["Yelong Shen", "Po-Sen Huang", "Ming-Wei Chang", "Jianfeng Gao."], "venue": "arXiv preprint arXiv:1611.04642 .", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "ProjE : Embedding Projection for Knowledge Graph Completion", "author": ["Baoxu Shi", "Tim Weniger."], "venue": "AAAI .", "citeRegEx": "Shi and Weniger.,? 2017", "shortCiteRegEx": "Shi and Weniger.", "year": 2017}, {"title": "Reasoning With Neural Tensor Networks for Knowledge Base Completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng."], "venue": "Proceedings of the Advances in Neural Information Processing Systems 26 (NIPS 2013) .", "citeRegEx": "Socher et al\\.,? 2013", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Observed versus latent features for knowledge base and text inference", "author": ["Kristina Toutanova", "Danqi Chen."], "venue": "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality pages 57\u201366.", "citeRegEx": "Toutanova and Chen.,? 2015", "shortCiteRegEx": "Toutanova and Chen.", "year": 2015}, {"title": "Knowledge Graph Completion via Complex Tensor Factorization http://arxiv.org/abs/1702.06879", "author": ["Th\u00e9o Trouillon", "Christopher R. Dance", "Johannes Welbl", "Sebastian Riedel", "\u00c9ric Gaussier", "Guillaume Bouchard"], "venue": null, "citeRegEx": "Trouillon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Trouillon et al\\.", "year": 2017}, {"title": "Complex Embeddings for Simple Link Prediction", "author": ["Th\u00e9o Trouillon", "Johannes Welbl", "Sebastian Riedel", "Eric Gaussier", "Guillaume Bouchard."], "venue": "Proceedings of ICML 48:2071\u20132080. http://arxiv.org/pdf/1606.06357v1.pdf.", "citeRegEx": "Trouillon et al\\.,? 2016", "shortCiteRegEx": "Trouillon et al\\.", "year": 2016}, {"title": "Knowledge Graph Embedding by Translating on Hyperplanes", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."], "venue": "AAAI Conference on Artificial Intelligence pages 1112\u20131119.", "citeRegEx": "Wang et al\\.,? 2014", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Text-enhanced representation learning for knowledge graph", "author": ["Zhigang Wang", "Juanzi Li."], "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence. AAAI Press, pages 1293\u20131299.", "citeRegEx": "Wang and Li.,? 2016", "shortCiteRegEx": "Wang and Li.", "year": 2016}, {"title": "Ssp: Semantic space projection for knowledge graph embedding with text descriptions", "author": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu."], "venue": "Pro- ceedings of the 31st AAAI Conference on Artificial In- telligence.", "citeRegEx": "Xiao et al\\.,? 2017", "shortCiteRegEx": "Xiao et al\\.", "year": 2017}, {"title": "Embedding Entities and Relations for Learning and Inference in Knowledge Bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "ICLR page 12. http://arxiv.org/abs/1412.6575.", "citeRegEx": "Yang et al\\.,? 2015", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "A Translation-Based Knowledge Graph Embedding Preserving Logical Property of Relations", "author": ["Hee-geun Yoon", "Hyun-je Song", "Seong-bae Park", "Se-young Park."], "venue": "Naacl pages 1\u20139.", "citeRegEx": "Yoon et al\\.,? 2016", "shortCiteRegEx": "Yoon et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "Projects such as Wikidata1 or earlier Freebase (Bollacker et al., 2008) have successfully accumulated a formidable amount of knowledge in the form of \u3008entity1 - relation - entity2\u3009 triplets.", "startOffset": 47, "endOffset": 71}, {"referenceID": 21, "context": "An extensive amount of work has been published on this task (for a review see (Nickel et al., 2015; Nguyen, 2017), for a plain list of citations see Table 2).", "startOffset": 78, "endOffset": 113}, {"referenceID": 19, "context": "An extensive amount of work has been published on this task (for a review see (Nickel et al., 2015; Nguyen, 2017), for a plain list of citations see Table 2).", "startOffset": 78, "endOffset": 113}, {"referenceID": 35, "context": "Among those DistMult (Yang et al., 2015) is one of the simplest.", "startOffset": 21, "endOffset": 40}, {"referenceID": 3, "context": "standard KBC datasets, WN18 and FB15k (Bordes et al., 2013).", "startOffset": 38, "endOffset": 59}, {"referenceID": 31, "context": "The DistMult model was introduced by Yang et al. (2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss.", "startOffset": 37, "endOffset": 56}, {"referenceID": 0, "context": "(2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss.", "startOffset": 1, "endOffset": 47}, {"referenceID": 0, "context": "(2015). Subsequently Toutanova and Chen (2015) achieved better empirical results with the same model by changing hyper-parameters of the training procedure and by using negative-log likelihood of softmax instead of L1-based max-margin ranking loss. Trouillon et al. (2016) obtained even better empirical result on the FB15k dataset just by changing DistMult\u2019s hyper-parameters.", "startOffset": 1, "endOffset": 273}, {"referenceID": 6, "context": "dard datasets WN18 derived from WordNet (Fellbaum, 1998) and FB15k derived from the Freebase knowledge graph (Bollacker et al.", "startOffset": 40, "endOffset": 56}, {"referenceID": 1, "context": "dard datasets WN18 derived from WordNet (Fellbaum, 1998) and FB15k derived from the Freebase knowledge graph (Bollacker et al., 2008).", "startOffset": 109, "endOffset": 133}, {"referenceID": 14, "context": "We train the final models using Adam (Kingma and Ba, 2015) optimizer (lr = 0.", "startOffset": 37, "endOffset": 58}, {"referenceID": 26, "context": "On FB15k only the IRN model (Shen et al., 2016) shows better Hits@10 and the ProjE (Shi and Weniger, 2017) has better MR.", "startOffset": 28, "endOffset": 47}, {"referenceID": 27, "context": ", 2016) shows better Hits@10 and the ProjE (Shi and Weniger, 2017) has better MR.", "startOffset": 43, "endOffset": 66}, {"referenceID": 10, "context": "ComplEx models (that are equivalent as shown by Hayashi and Shimbo (2017)).", "startOffset": 48, "endOffset": 74}, {"referenceID": 31, "context": "Compared to previous works that trained DistMult on these datasets (for results see bottom of Table 2) we use different training objective than Yang et al. (2015) and Trouillon et al.", "startOffset": 144, "endOffset": 163}, {"referenceID": 0, "context": "(2015) and Trouillon et al. (2017) that optimized max margin objective and NLL of softplus activation function (softplus(x) = ln(1 + ex)), respectively.", "startOffset": 1, "endOffset": 35}, {"referenceID": 0, "context": "(2015) and Trouillon et al. (2017) that optimized max margin objective and NLL of softplus activation function (softplus(x) = ln(1 + ex)), respectively. Similarly to Toutanova and Chen (2015) we use NLL of softmax function, however we use ADAM optimizer instead of", "startOffset": 1, "endOffset": 192}, {"referenceID": 24, "context": "RProp (Riedmiller and Braun, 1993).", "startOffset": 6, "endOffset": 34}, {"referenceID": 22, "context": "Results marked by \u2020, \u2021 and ] are from (Nickel et al., 2016), (Trouillon et al.", "startOffset": 38, "endOffset": 59}, {"referenceID": 30, "context": ", 2016), (Trouillon et al., 2017) and (Schlichtkrull et al.", "startOffset": 9, "endOffset": 33}, {"referenceID": 25, "context": ", 2017) and (Schlichtkrull et al., 2017), respectively.", "startOffset": 12, "endOffset": 40}, {"referenceID": 2, "context": "N on e Unstructured (Bordes et al., 2014) 304 38.", "startOffset": 20, "endOffset": 41}, {"referenceID": 3, "context": "3 TransE (Bordes et al., 2013) 251 89.", "startOffset": 9, "endOffset": 30}, {"referenceID": 32, "context": "TransH (Wang et al., 2014) 303 86.", "startOffset": 7, "endOffset": 26}, {"referenceID": 17, "context": "4 TransR (Lin et al., 2015b) 225 92.", "startOffset": 9, "endOffset": 28}, {"referenceID": 17, "context": "7 CTransR (Lin et al., 2015b) 218 92.", "startOffset": 10, "endOffset": 29}, {"referenceID": 11, "context": "2 KG2E (He et al., 2015) 331 92.", "startOffset": 7, "endOffset": 24}, {"referenceID": 12, "context": "0 TransD (Ji et al., 2015) 212 92.", "startOffset": 9, "endOffset": 26}, {"referenceID": 36, "context": "3 lppTransD (Yoon et al., 2016) 270 94.", "startOffset": 12, "endOffset": 31}, {"referenceID": 13, "context": "7 TranSparse (Ji et al., 2016) 211 93.", "startOffset": 13, "endOffset": 30}, {"referenceID": 9, "context": "5 TATEC (Garcia-Duran et al., 2016) - - - 58 76.", "startOffset": 8, "endOffset": 35}, {"referenceID": 28, "context": "7 NTN (Socher et al., 2013) - 66.", "startOffset": 6, "endOffset": 27}, {"referenceID": 22, "context": "25 HolE (Nickel et al., 2016) - 94.", "startOffset": 8, "endOffset": 29}, {"referenceID": 20, "context": "524 STransE (Nguyen et al., 2016) 206 93.", "startOffset": 12, "endOffset": 33}, {"referenceID": 30, "context": "ComplEx (Trouillon et al., 2017) - 94.", "startOffset": 8, "endOffset": 32}, {"referenceID": 27, "context": "692 ProjE wlistwise (Shi and Weniger, 2017) - - - 34 88.", "startOffset": 20, "endOffset": 43}, {"referenceID": 26, "context": "4 IRN (Shen et al., 2016) 249 95.", "startOffset": 6, "endOffset": 25}, {"referenceID": 8, "context": "RTransE (Garc\u0131\u0301a-Dur\u00e1n et al., 2015) - - - 50 76.", "startOffset": 8, "endOffset": 36}, {"referenceID": 16, "context": "2 PTransE (Lin et al., 2015a) - - - 58 84.", "startOffset": 10, "endOffset": 29}, {"referenceID": 7, "context": "Pa th GAKE (Feng et al., 2015) - - - 119 64.", "startOffset": 11, "endOffset": 30}, {"referenceID": 23, "context": "8 Gaifman (Niepert, 2016) 352 93.", "startOffset": 10, "endOffset": 25}, {"referenceID": 18, "context": "2 Hiri (Liu et al., 2016) - 90.", "startOffset": 7, "endOffset": 25}, {"referenceID": 25, "context": "603 R-GCN+ (Schlichtkrull et al., 2017) - 96.", "startOffset": 11, "endOffset": 39}, {"referenceID": 29, "context": "NLFeat (Toutanova and Chen, 2015) - 94.", "startOffset": 7, "endOffset": 33}, {"referenceID": 33, "context": "Te xt TEKE H (Wang and Li, 2016) 114 92.", "startOffset": 13, "endOffset": 32}, {"referenceID": 34, "context": "SSP (Xiao et al., 2017) 156 93.", "startOffset": 4, "endOffset": 23}, {"referenceID": 35, "context": "DistMult (orig) (Yang et al., 2015) - 94.", "startOffset": 16, "endOffset": 35}, {"referenceID": 29, "context": "N on e DistMult (Toutanova and Chen, 2015) - - - - 79.", "startOffset": 16, "endOffset": 42}, {"referenceID": 30, "context": "555 DistMult (Trouillon et al., 2017) - 93.", "startOffset": 13, "endOffset": 37}, {"referenceID": 29, "context": "\u201cNLFeat\u201d abbreviates Node+LinkFeat model from (Toutanova and Chen, 2015).", "startOffset": 46, "endOffset": 72}, {"referenceID": 28, "context": "The results for NTN (Socher et al., 2013) listed in this table are taken from Yang et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "This table was adapted from (Nguyen, 2017).", "startOffset": 28, "endOffset": 42}, {"referenceID": 0, "context": "\u201cNLFeat\u201d abbreviates Node+LinkFeat model from (Toutanova and Chen, 2015). The results for NTN (Socher et al., 2013) listed in this table are taken from Yang et al. (2015). This table was adapted from (Nguyen, 2017).", "startOffset": 67, "endOffset": 171}, {"referenceID": 4, "context": "For broader discussion see (Church, 2017).", "startOffset": 27, "endOffset": 41}, {"referenceID": 4, "context": "future research could focus more on the raw scenario which however requires using other information retrieval metrics such as mean average precision (MAP), previously used in KBC for instance by Das et al. (2017). We see this preliminary work as a small contribution to the ongoing discussion in the machine learning community about the current strong focus on state-of-the-art empirical results when it might be sometimes questionable whether they were achieved due to a better model/algorithm or just by more extensive hyper-parameter search.", "startOffset": 195, "endOffset": 213}, {"referenceID": 15, "context": "els (Levy et al., 2015).", "startOffset": 4, "endOffset": 23}], "year": 2017, "abstractText": "Many papers have been published on the knowledge base completion task in the past few years. Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18. This paper shows that the accuracy of almost all models published on the FB15k can be outperformed by an appropriately tuned baseline \u2014 our reimplementation of the DistMult model. Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyperparameter tuning or different training objectives. This should prompt future research to re-consider how the performance of models is evaluated and reported.", "creator": "LaTeX with hyperref package"}}}