{"id": "1705.10823", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Practical Neural Network Performance Prediction for Early Stopping", "abstract": "in comparative neural network domain, methods for hyperparameter optimization see texture - modeling proven fatally impaired due to the problems to distinguish steady decreasing number of classical network configurations. in this paper, contestants need immediately a simple test arises, based on support vector machines, to compare very final performance atop effectively trained neural line configurations using analytics like statistical network architectures, hyperparameters, and time - series validation performance data. we extend this regression model to create an early management software for kernel disk configurations. with this early stopping strategy, we estimate significant speedups in specific hyperparameter optimization including lambda - modeling. particularly in the context concerning meta - verification, mathematical method can react to see final performance aboard emotionally diverse architectures and is seamlessly tolerant of this learning - based architecture selection technique. finally, analysis show that our function is simpler, faster, although more accurate than initial methods for behavioral integration prediction.", "histories": [["v1", "Tue, 30 May 2017 19:00:53 GMT  (326kb,D)", "http://arxiv.org/abs/1705.10823v1", "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA"]], "COMMENTS": "Submitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.NE", "authors": ["bowen baker", "otkrist gupta", "ramesh raskar", "nikhil naik"], "accepted": false, "id": "1705.10823"}, "pdf": {"name": "1705.10823.pdf", "metadata": {"source": "CRF", "title": "Practical Neural Network Performance Prediction for Early Stopping", "authors": ["Bowen Baker", "Otkrist Gupta", "Ramesh Raskar", "Nikhil Naik"], "emails": ["naik}@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In recent years, neural networks have achieved significant performance gains in machine learning tasks [10]. However, significant human expertise and labor is still required for designing neural network architectures and successfully training them on different datasets across domains. Ongoing research in two areas\u2014meta-modeling and hyperparameter optimization\u2014attempts to reduce the amount of human intervention required for these tasks. While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch. Both sets of algorithms require training a large number of neural network configurations in order to identify the right network architecture or the right set of hyperparameters\u2014and are hence computationally expensive.\nWhen sampling many different model configurations, it is likely that many subpar configurations will be explored. Human experts are quite adept at recognizing and terminating suboptimal model configurations by inspecting their partially observed learning curves. In this paper we seek to emulate this behavior and automatically identify and terminate subpar model configurations in order to speedup both meta-modeling and hyperparameter optimization methods for deep neural networks. In Figure 1 we show the potential benefits of automated early termination for deep convolutional neural networks. Our method parameterizes learning curve trajectories using simple features derived from model architectures, training hyperparameters, and early time-series measurements from the learning curve. We demonstrate that a simple, fast, and accurate regression model (based on support vector machines) can be trained to predict the final validation accuracy of partially observed neural network configurations using a small training set of fully observed curves. We can use these predictions\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 5.\n10 82\n3v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\nand empirical variance estimates to construct a simple early stopping algorithm that can drastically speedup both meta-modeling and hyperparameter optimization methods.\nWhile there is some prior work on neural network performance prediction [4, 7], our work is the first to show that these methods are viable for deep convolutional neural network architecture searches. In addition, with extensive experimentation, we demonstrate that our method is significantly more accurate, accessible, and efficient as compared to prior work. We hope that our work leads to inclusion of neural network performance prediction in the practical neural network training pipeline."}, {"heading": "2 Related Work", "text": "Neural Network Performance Prediction: There has been limited work on predicting neural network performance during the training process [4, 7]. Domhan et al. [4] introduce a weighted probabilistic model for learning curves and utilize this model for speeding up hyperparameter search in small convolutional neural networks (CNNs) and fully-connected networks (FCNs). Their experimental setup contains large search spaces for hyperparameters such as learning rate, batch size, and weight decay, along with limited search spaces for number of layers and number of units. In contrast to our work, this method does not utilize the learning curve information from different neural architectures; it independently models the learning curve for each network. We also note that Swersky et al. [21] develop a Gaussian Process-based method for predicting individual learning curves for logistic regression models (among others), but not for neural networks. Building on Domhan et al. [4], Klein et al. [7] train Bayesian neural networks for predicting unobserved learning curves using a training set of fully and partially observed learning curves. Our method\u2014based on support vector machines\u2014is simpler, more efficient, and more accurate than Bayesian neural nets in a variety of settings, including meta-modeling and hyperparameter optimization. We summarize the related work on these topics next.\nMeta-modeling: In the context of neural networks, we define meta-modeling as an algorithmic approach for designing network architectures from scratch. The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22]. Suganuma et al. [20] use Cartesian genetic programming to obtain competitive results on image classification tasks. Saxena and Verbeek [13] use densely connected networks of layers to search for well-performing networks. Another popular tool for meta-modeling is Bayesian optimization [16]. Saliently, Bergstra et al. [3] utilize Tree of Parzen Estimators (TPE) to design feed-forward networks. Finally, reinforcement learning has recently become popular for neural network architecture design. Baker et al. [1] show that a Q-learning agent can design competitive CNNs for image classification. Zoph and Le [25] use policy gradients to design CNNs and Recurrent cell architectures.\nHyperparameter Optimization: In the context of neural networks, we define hyperparameter optimization as an algorithmic approach for finding optimal values of design-independent hyperparameters such as learning rate and batch size, along with a limited search through the network design space, usually through the space of filter types and sizes. A variety of Bayesian optimization\nmethods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3]. To improve on the scalability of Bayesian methods, Snoek et al. [18] utilize neural networks to efficiently model distributions over functions. However, random search or grid search [2] is most commonly used in practical settings. Recently, Li et al. [11] introduced Hyperband, a multi-armed bandit-based efficient random search technique that outperforms state-of-the-art Bayesian optimization methods."}, {"heading": "3 Background", "text": "We empirically experiment with our early stopping algorithm by integrating it with both a metamodeling and hyperparameter optimization algorithm, namely MetaQNN [1] and Hyperband [11]. Below we give a brief background on MetaQNN and Hyperband."}, {"heading": "3.1 MetaQNN", "text": "Baker et al. [1] train a Q-learning [24] agent to design convolutional neural networks. In this method, the agent samples architectures from a large, finite space by traversing a path from input to termination layer through nodes that are different layer types. The neural network model defined by the agent\u2019s trajectory is trained on the task and its validation accuracy is presented as reward to the agent. Using an -greedy exploration strategy [23] and experience replay [12], the agent becomes better with time at discovering designs that obtain high performance on the learning task.\nHowever, the MetaQNN method uses 100 GPU-days to train 2700 neural architectures and the similar experiment by Zoph and Le [25] utilized 10,000 GPU-days to train 12,800 models on CIFAR-10. The amount of computing resources required for these approaches makes them prohibitively expensive for large datasets (e.g., Imagenet) and larger search spaces. The main computational expense of reinforcement learning-based meta-modeling methods is training the neural net configuration to T epochs (where T is typically a large number at which the network stabilizes to peak accuracy). yT , the network accuracy at epoch T is presented as reward to the learning agent."}, {"heading": "3.2 Hyperband", "text": "Recently, Li et al. [11] introduced Hyperband, a random search technique based on multi-armed bandits that obtains state-of-the-art performance in hyperparameter optimization in a variety of settings. The Hyperband algorithm trains a population of models with different hyperparameter configurations and iteratively discards models below a certain percentile in performance among the population until the computational budget is exhausted or satisfactory results are obtained.\nHyperband uses a \u201clast seen value\u201d heuristic, essentially meaning it uses the last seen validation set performance to judge which models should be thrown out. While this heuristic works very well already, we show that using a predictor trained on the partially observed learning curve can be used to do even more aggressive early stopping and speedup overall performance when compared to the original Hyperband algorithm."}, {"heading": "4 Method", "text": "We first describe our model for neural network performance prediction, followed by a method for early termination of under-performing network architectures."}, {"heading": "4.1 Modeling Learning Curves", "text": "Our goal is to model the validation accuracy v(x, t) of a neural net configuration x \u2208 X \u2282 Rd at epoch t \u2208 R using noisy observations y(x, t) drawn from an IID distribution. For each cofiguration x trained for T epochs, we record a time-series y(t) = y1, y2, . . . , yT of validation accuracies. We train a population of n configurations, obtaining a set S = {(x1, y1(t)), (x2, y2(t)), . . . , (xn, yn(t))}. Note that this problem formulation is very similar to Klein et al. [7].\nWe propose to use a set of features ux, derived from the neural net configuration x, along with a subset of time-series accuracies y(t)1\u2013\u03c4 = (yt)t=1,2,...,\u03c4 (where 1 \u2264 \u03c4 < T ) from S to train a regression model for estimating yT . Our model predicts yT of a neural network configuration using a\nfeature set xf = {ux, y(t)1\u2013\u03c4}. We utilize \u03bd-Support Vector Regression (\u03bd-SVR) [15] for training a model for yT .\nGiven a set of input feature vectors x and their corresponding labels y, the goal of SVR is to obtain a regression function f(x) that approximates y, such that,\nf(x) = (w \u00b7 \u03c6(x)) + b, w,x \u2208 RN , b \u2208 R (1)\nwhere \u03c6(x) is linear or non-linear map on x, w is a set of weights, and b is a bias term. Please see Sch\u00f6lkopf et al. [15] for further details on \u03bd-SVR."}, {"heading": "4.2 Early Stopping", "text": "To speed up hyperparameter optimization and meta-modeling methods, we develop an algorithm to determine whether to continue training a partially trained model configuration using our \u03bdSVR predictor. If we would like to sample N total neural network configurations, we begin by sampling and training n N configurations to create a training set S. We then train a model f(xf ) with \u03bd-SVR to predict yT . Now, given the current best performance observed yBEST, we would like to terminate training a new configuration x\u2032 given its partial observed learning curve y\u2032(t)1\u2013\u03c4 if f(xf \u2032) = y\u0302T \u2264 yBEST so as to not waste computational resources exploring a suboptimal configuration.\nHowever, in case f(xf ) has poor out-of-sample generalization, we may mistakingly terminate the optimal configuration. Thus, if we assume that our estimate can be modeled as a Gaussian perturbation of the true value y\u0302T \u223c N (yT , \u03c3(x, \u03c4)), then we can find the probability p(y\u0302T \u2264 yBEST|\u03c3(x, \u03c4)) = \u03a6(yBEST; yT , \u03c3), where \u03a6(\u00b7;\u00b5, \u03c3) is the CDF of N (\u00b5, \u03c3). Note that in general the uncertainty will depend on both the configuration and \u03c4 , the number of points observed from the learning curve. Because frequentist models don\u2019t admit a natural estimate of uncertainty, we assume that \u03c3 is independent of x yet still dependent on \u03c4 and estimate it via Leave One Out Cross Validation.\nNow that we can estimate the model uncertainty, given a new configuration x\u2032 and an observed learning curve y\u2032(t)1\u2013\u03c4 , we may set our termination criteria to be p(y\u0302T \u2264 yBEST) \u2265 \u2206. \u2206 balances the trade-off between increased speedups and risk of prematurely terminating good configurations. In many cases, one may want several configurations that are close to optimal, for the purpose of ensembling. We offer two modifications in this case. First, one may relax the termination criterion to p(y\u0302T \u2264 yBEST \u2212 \u03b4) \u2265 \u2206, which will allow configurations within \u03b4 of optimal performance to complete training. One can alternatively set the criterion based on the nth best configuration observed, gaurenteeing that with high probability the top n configurations will be fully trained."}, {"heading": "5 Experiments and Results", "text": "We now evaluate the performance of our algorithm in three separate settings. First, we analyze the ability a \u03bd-SVR model to predict the final validation accuracy of a trained neural network. Second, we integrate the \u03bd-SVR-based early stopping model into MetaQNN [1] to show that it can speed up the meta-modeling process without perturbing the reward function to the point that the agent learns suboptimal policies. Finally, we show that our early stopping model can also speed up the Hyperband algorithm [11]. We first describe the process we use to train our performance prediction model.\nFor all experiments, we train the \u03bd-SVR model with random search over 1000 hyperparameter configurations from the space C \u223c LogUniform(10\u22125, 10), \u03bd \u223c Uniform(0, 1), and \u03b3 \u223c LogUniform(10\u22125, 10) (when using the RBF kernel). We use a combination of features to train the SVR. For all experiments described in the paper, we use the following time-series features: (i) the validation accuracies y\u2032(t)1\u2013\u03c4 = (yt)t=1,2,...,\u03c4 (where 1 \u2264 \u03c4 < T ), (ii) the first-order differences of validation accuracies (i.e., yt\u2032 = (yt \u2212 yt\u22121)), and (iii) the second-order differences of validation accuracies (i.e., yt\u2032\u2032 = (yt\u2032 \u2212 yt\u22121)\u2032). For experiments in which we vary the CNN architectures (MetaQNN CNNs and Deep Resnet), we include the total number of weights, number of layers, and learning rate into the feature space. For experiments in which we vary the optimization hyperparameters (Cuda-Convnet and AlexNet), we include all hyperparameters for training the neural networks as features (see Supplement Table 2 for the list of hyperparameters)."}, {"heading": "5.1 Datasets and Training Procedures", "text": "We now describe the datasets and training procedures used in our experiments. We generate learning curves from randomly sampled models in four different hyperparameter search spaces. We experiment with standard datasets on both small convolutional networks and very deep convolutional architectures.\nMetaQNN CNNs: We sample 1,000 model architectures from the search space detailed by Baker et al. [1], which allows for varying the numbers and orderings of convolution, pooling, and fully connected layers. We experiment with both the SVHN and CIFAR-10 datasets and use the same preprocessing and optimization hyperparameters. The search space, preprocessing details, and hyperparameters used by Baker et al. [1] are reproduced in the Supplementary Materials.\nCuda-Convnet: We also experiment with the Cuda-Convnet architecture [8]. We vary initial learning rate, learning rate reduction step size, weight decay for convolutional and fully connected layers and scale and power of local response normalization layers as detailed in Section 1 of the Supplementary Materials. We experiment with both the CIFAR-10 and SVHN datasets. CIFAR-10 models are trained for 60 epochs and SVHN models are trained for 12 epochs. A total of 8489 and 16582 configurations were randomly sampled for CIFAR-10 and SVHN respectively.\nAlexNet: We train the AlexNet [9] model on the ILSVRC12 dataset. It was considerably more expensive to train many configurations on ILSVRC12 because of larger image and dataset size. To compensate for our limited computation resources, we randomly sample 10% of dataset, trained each configuration for 10 epochs, and only vary learning rate and learning rate reduction, with the same search spaces as detailed in Section 1 of the Supplementary Materials. We sampled and trained 1,376 hyperparameter configurations.\nDeep Resnet: We sample 500 ResNet [5] architectures from a search space similar to Zoph et al. [25]. Each architecture consists of 39 layers: 12 conv, a 2x2 max pool, 9 conv, a 2x2 max pool, 15 conv, and softmax. Each conv layer is followed by batch normalization and a ReLU nonlinearity. Each block of 3 conv layers are densely connected via residual connections and also share the same kernel width, kernel height, and number of learnable kernels. Kernel height and width are independently sampled from {1, 3, 5, 7} and number of kernels is sampled from {6, 12, 24, 36}. Finally, we randomly sample residual connections between each block of conv layers. Each network is trained for 50 epochs using the RMSProp optimizer, with weight decay 10\u22124, initial learning rate 0.001, and a learning rate reduction to 10\u22125 at epoch 30 on the CIFAR-10 dataset."}, {"heading": "5.2 Prediction Performance", "text": "We now evaluate the ability of \u03bd-SVR, trained with linear and RBF kernels, to predict the final performance of partially trained neural networks. We compare against the Bayesian Neural Network (BNN) presented by Klein et al. [7] using a Hamiltonian Monte Carlo sampler. When training the BNN, we not only present it with the subset of fully observed learning curves but also all other partially observed learning curves from the training set. While we do not present the partially observed curves to the \u03bd-SVR model for training, we felt this was a fair comparison as \u03bd-SVR uses the entire partially observed learning curve during inference. Furthermore, we compare against the learning curve extrapolation (LCE) method introduced by Domhan et al. [4] and the last seen value (LastSeenValue) heuristic [11], both of which don\u2019t incorporate prior learning curves during training. For all experiments, we obtain a training set of 100 neural net configurations randomly sampled from a dataset. We obtain the best performing \u03bd-SVR using random hyperparameter search over 3-fold cross-validation on this training set. We then compute the regression performance over the remaining points in the dataset. We repeat this experiment 10 times and report the results with standard errors in Figure 2.\nFigure 2 shows the Coefficient of Determination (R2) obtained by each method for predicting the final performance versus the percent of the learning curve used for training the model. We see that in all neural network configuration spaces and across all datasets, both \u03bd-SVR models drastically outperform the competing methods. In fact, \u03bd-SVR achievesR2 > 0.8 on three out of six experiments with only 10% of the learning curve observed. For deeper models (Resnets and Alexnet), \u03bd-SVR obtains R2 > 0.6 after observing only 40% of the learning curve. In comparison, we find that the LastSeenValue heuristic only becomes viable when the models are near convergence, and its performance is much worse than \u03bd-SVR for very deep models. We also find that the LCE model does poorly at the prediction task in all experiments. BNN also has relatively mediocre performance, but tends to do better than LastSeenValue and LCE when only a few iterations have been observed.\nTo further demonstrate the remarkable fit obtained by the \u03bd-SVR model, we show the predicted versus true values of final validation accuracy for the MetaQNN, Cuda-Convnet, and Resnet search spaces on the CIFAR-10 dataset in Figure 3. Each plot is generated using \u03bd-SVR with a Gaussian kernel, using 25% the learning curve as training data, along with the features obtained from the architecture, and hyperparameters. We also compared RBF kernel ordinary least squares and found it to perform almost as well as \u03bd-SVR, further demonstrating the advantage of simple regression models over Bayesian methods for this task. Finally, we analyze which features in our model are the most informative in Table 1."}, {"heading": "5.3 Early Stopping for Meta-modeling", "text": "We now detail the performance of \u03bd-SVR in speeding up architecture search using sequential configuration selection. First, we take 1,000 random models from the MetaQNN [1] search space. We simulate the MetaQNN algorithm by taking 10 random orderings of each set and running the algorithms presented in Section 4.2. We compare against the early stopping algorithm proposed by Domhan et al. [4] as a baseline, which has a similar probability threshold termination criterion. The \u03bd-SVR model trains off of the first 100 fully observed curves, while the LCE model trains from each individual partial curve and can begin early termination immediately. Despite this \u201cburn in\u201d time needed by the \u03bd-SVR model, it is still able to outperform the LCE model quite drastically, as\ncan be seen in Figure 4. The plotted results are only in terms of training the CNNs; however, in our experience it takes anywhere from 1 to 3 minutes to fit the LCE model to a learning curve on a modern CPU due to expensive MCMC sampling. Additionally, each time a new point on the learning curve is observed, a new LCE model must be fit. Thus, for an experiment on the order of that of [1, 25], many days of computation time would be added as overhead to fit the LCE models when compared to our simple model.\nWe furthermore simulate early stopping for the deep resnets search space. In our experiment we found that only the probability threshold \u2206 = 0.99 resulted in recovering the top model consistently. However, even with such a conservative threshold, the search was sped up by a factor of 3.4 over the baseline. While we do not have the computational resources\u2014even with this speedup\u2014to run the full experiment from Zoph et al. [25], we believe that this is a promising result for future large scale architecture searches.\nIt is not enough, however, to simply simulate the speedup because sequential configuration selection algorithms typically use the observed performance in order to update some type of acquisition function. In the reinforcement learning setting, the performance is given to the agent as a reward, so we also empirically verify that substituting y\u0302T for yT does not cause the MetaQNN agent to converge to subpar policies. We replicate the MetaQNN experiment on the CIFAR-10 dataset (Figure 5). We find that integrating early stopping with the Q-learning procedure does not disrupt learning and resulted in a speedup of 3.8x; note that for this experiment we set \u2206 = 0.99 which explains why the speedup is relatively low. After training the top models to 300 epochs, we also find that the resulting performance is on par with the originally published numbers without early stopping (just under 93%)."}, {"heading": "5.4 Early Stopping for Hyperparameter Optimization", "text": "For our final empirical test, we turn towards the task of searching over optimization hyperparameters, such as learning rate, regularization weight, etc. In this section we present results on the Hyperband algorithm [11]. To be able to use our early stopping algorithm, we need on the order of 20-100 fully observed learning curves (See Supplement Section 2). So, while incorporating our algorithm into the Hyperband framework, we allow for an initial \u2018burn in\u2019 period where we train 100 models to max iterations and use these to train \u03bd-SVR performance predictors. Then, we use these predictors to do early stopping within the Hyperband successive halvings, resulting in a more aggressive termination criterion. Figure 6 shows that our early stopping algorithm evaluates the same number of unique configurations as Hyperband within half the compute time, while achieving the same final accuracy within standard error.\n6 Conclusion\nIn this paper we introduce a simple, fast, and accurate model for predicting future neural network performance using features derived from network architectures, hyperparameters, and time-series performance data. To our knowledge, this is the first study showing that the performance of drastically different network architectures can be jointly learnt and predicted. Using our simple algorithm, we can speedup hyperparameter search techniques with complex acquisition functions, such as a Q-learning agent, by a factor of 3-6x and Hyperband\u2014a state-of-the-art hyperparameter search method\u2014by a factor of 2x, without disturbing the search procedure. We outperform all competing methods for performance prediction in terms of accuracy, train and test time, and speedups obtained on hyperparamter search methods.\nWe hope that the simplicity and success of our method will allow it to be easily incorporated into current hyperparameter optimization pipelines for deep neural networks. With the advent of large scale automated architecture search [1, 25], methods such as ours will be vital in exploring even larger and more complex search spaces."}], "references": [{"title": "Designing neural network architectures using reinforcement learning", "author": ["Bowen Baker", "Otkrist Gupta", "Nikhil Naik", "Ramesh Raskar"], "venue": "International Conference on Learning Representations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2017}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "JMLR, 13(Feb):281\u2013305,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures", "author": ["James Bergstra", "Daniel Yamins", "David D Cox"], "venue": "ICML (1),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves", "author": ["Tobias Domhan", "Jost Tobias Springenberg", "Frank Hutter"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Sequential model-based optimization for general algorithm configuration", "author": ["Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown"], "venue": "In International Conference on Learning and Intelligent Optimization,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Learning curve prediction with bayesian neural networks", "author": ["Aaron Klein", "Stefan Falkner", "Jost Tobias Springenberg", "Frank Hutter"], "venue": "International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Hyperband: A novel bandit-based approach to hyperparameter optimization", "author": ["Lisha Li", "Kevin Jamieson", "Giulia DeSalvo", "Afshin Rostamizadeh", "Ameet Talwalkar"], "venue": "International Conference on Learning Representations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Long-Ji Lin"], "venue": "Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1992}, {"title": "Convolutional neural fabrics", "author": ["Shreyas Saxena", "Jakob Verbeek"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Combinations of genetic algorithms and neural networks: A survey of the state of the art", "author": ["J David Schaffer", "Darrell Whitley", "Larry J Eshelman"], "venue": "International Workshop on Combinations of Genetic Algorithms and Neural Networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "New support vector algorithms", "author": ["Bernhard Sch\u00f6lkopf", "Alex J Smola", "Robert C Williamson", "Peter L Bartlett"], "venue": "Neural computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Taking the human out of the loop: A review of bayesian optimization", "author": ["Bobak Shahriari", "Kevin Swersky", "Ziyu Wang", "Ryan P Adams", "Nando de Freitas"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P Adams"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Scalable bayesian optimization using deep neural networks", "author": ["Jasper Snoek", "Oren Rippel", "Kevin Swersky", "Ryan Kiros", "Nadathur Satish", "Narayanan Sundaram", "Mostofa Patwary", "Mr Prabhat", "Ryan Adams"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Evolving neural networks through augmenting topologies", "author": ["Kenneth O Stanley", "Risto Miikkulainen"], "venue": "Evolutionary Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "A genetic programming approach to designing convolutional neural network architectures", "author": ["Masanori Suganuma", "Shinichi Shirakawa", "Tomoharu Nagao"], "venue": "arXiv preprint arXiv:1704.00764,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Freeze-thaw bayesian optimization", "author": ["Kevin Swersky", "Jasper Snoek", "Ryan Prescott Adams"], "venue": "arXiv preprint arXiv:1406.3896,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Generative neuroevolution for deep learning", "author": ["Phillip Verbancsics", "Josh Harguess"], "venue": "arXiv preprint arXiv:1312.5355,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Multi-armed bandit algorithms and empirical evaluation", "author": ["Joannes Vermorel", "Mehryar Mohri"], "venue": "European Conference on Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Learning from delayed rewards", "author": [], "venue": "PhD thesis,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1989}], "referenceMentions": [{"referenceID": 1, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 5, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 8, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 14, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 15, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 42, "endOffset": 60}, {"referenceID": 0, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 201, "endOffset": 215}, {"referenceID": 2, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 201, "endOffset": 215}, {"referenceID": 19, "context": "While hyperparameter optimization methods [2, 6, 11, 17, 18] focus primarily on obtaining good optimization hyperparameter configurations for training human-designed networks, meta-modeling algorithms [1, 3, 22, 25] aim to design neural network architectures from scratch.", "startOffset": 201, "endOffset": 215}, {"referenceID": 0, "context": "Figure 1: Early Stopping Example: (Left) 1000 learning curves sampled from the MetaQNN [1] search space.", "startOffset": 87, "endOffset": 90}, {"referenceID": 3, "context": "While there is some prior work on neural network performance prediction [4, 7], our work is the first to show that these methods are viable for deep convolutional neural network architecture searches.", "startOffset": 72, "endOffset": 78}, {"referenceID": 6, "context": "While there is some prior work on neural network performance prediction [4, 7], our work is the first to show that these methods are viable for deep convolutional neural network architecture searches.", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "2 Related Work Neural Network Performance Prediction: There has been limited work on predicting neural network performance during the training process [4, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "2 Related Work Neural Network Performance Prediction: There has been limited work on predicting neural network performance during the training process [4, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 3, "context": "[4] introduce a weighted probabilistic model for learning curves and utilize this model for speeding up hyperparameter search in small convolutional neural networks (CNNs) and fully-connected networks (FCNs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[21] develop a Gaussian Process-based method for predicting individual learning curves for logistic regression models (among others), but not for neural networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4], Klein et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] train Bayesian neural networks for predicting unobserved learning curves using a training set of fully and partially observed learning curves.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22].", "startOffset": 93, "endOffset": 105}, {"referenceID": 16, "context": "The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22].", "startOffset": 93, "endOffset": 105}, {"referenceID": 19, "context": "The earliest meta-modeling approaches for neural net design were based on genetic algorithms [14, 19, 22].", "startOffset": 93, "endOffset": 105}, {"referenceID": 17, "context": "[20] use Cartesian genetic programming to obtain competitive results on image classification tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Saxena and Verbeek [13] use densely connected networks of layers to search for well-performing networks.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "Another popular tool for meta-modeling is Bayesian optimization [16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "[3] utilize Tree of Parzen Estimators (TPE) to design feed-forward networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] show that a Q-learning agent can design competitive CNNs for image classification.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "methods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 14, "context": "methods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3].", "startOffset": 159, "endOffset": 163}, {"referenceID": 2, "context": "methods have been proposed for hyperparameter optimization, including methods based on sequential model-based optimization (SMAC) [6], Gaussian processes (GP) [17], and TPE [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 15, "context": "[18] utilize neural networks to efficiently model distributions over functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "However, random search or grid search [2] is most commonly used in practical settings.", "startOffset": 38, "endOffset": 41}, {"referenceID": 8, "context": "[11] introduced Hyperband, a multi-armed bandit-based efficient random search technique that outperforms state-of-the-art Bayesian optimization methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "We empirically experiment with our early stopping algorithm by integrating it with both a metamodeling and hyperparameter optimization algorithm, namely MetaQNN [1] and Hyperband [11].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "We empirically experiment with our early stopping algorithm by integrating it with both a metamodeling and hyperparameter optimization algorithm, namely MetaQNN [1] and Hyperband [11].", "startOffset": 179, "endOffset": 183}, {"referenceID": 0, "context": "[1] train a Q-learning [24] agent to design convolutional neural networks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[1] train a Q-learning [24] agent to design convolutional neural networks.", "startOffset": 23, "endOffset": 27}, {"referenceID": 20, "context": "Using an -greedy exploration strategy [23] and experience replay [12], the agent becomes better with time at discovering designs that obtain high performance on the learning task.", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Using an -greedy exploration strategy [23] and experience replay [12], the agent becomes better with time at discovering designs that obtain high performance on the learning task.", "startOffset": 65, "endOffset": 69}, {"referenceID": 8, "context": "[11] introduced Hyperband, a random search technique based on multi-armed bandits that obtains state-of-the-art performance in hyperparameter optimization in a variety of settings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "We utilize \u03bd-Support Vector Regression (\u03bd-SVR) [15] for training a model for yT .", "startOffset": 47, "endOffset": 51}, {"referenceID": 12, "context": "[15] for further details on \u03bd-SVR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Second, we integrate the \u03bd-SVR-based early stopping model into MetaQNN [1] to show that it can speed up the meta-modeling process without perturbing the reward function to the point that the agent learns suboptimal policies.", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "Finally, we show that our early stopping model can also speed up the Hyperband algorithm [11].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "We compare our method against BNN [7], LCE [4], and a \u201clast seen value\u201d heuristic [11].", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "We compare our method against BNN [7], LCE [4], and a \u201clast seen value\u201d heuristic [11].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "We compare our method against BNN [7], LCE [4], and a \u201clast seen value\u201d heuristic [11].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "[1], which allows for varying the numbers and orderings of convolution, pooling, and fully connected layers.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] are reproduced in the Supplementary Materials.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "AlexNet: We train the AlexNet [9] model on the ILSVRC12 dataset.", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "Deep Resnet: We sample 500 ResNet [5] architectures from a search space similar to Zoph et al.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "[7] using a Hamiltonian Monte Carlo sampler.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] and the last seen value (LastSeenValue) heuristic [11], both of which don\u2019t incorporate prior learning curves during training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[4] and the last seen value (LastSeenValue) heuristic [11], both of which don\u2019t incorporate prior learning curves during training.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "First, we take 1,000 random models from the MetaQNN [1] search space.", "startOffset": 52, "endOffset": 55}, {"referenceID": 3, "context": "[4] as a baseline, which has a similar probability threshold termination criterion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Thus, for an experiment on the order of that of [1, 25], many days of computation time would be added as overhead to fit the LCE models when compared to our simple model.", "startOffset": 48, "endOffset": 55}, {"referenceID": 8, "context": "In this section we present results on the Hyperband algorithm [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Figure 5: MetaQNN on CIFAR-10 with Early Stopping: A full run of the MetaQNN algorithm [1] on the CIFAR-10 dataset with early stopping.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "With the advent of large scale automated architecture search [1, 25], methods such as ours will be vital in exploring even larger and more complex search spaces.", "startOffset": 61, "endOffset": 68}], "year": 2017, "abstractText": "In the neural network domain, methods for hyperparameter optimization and metamodeling are computationally expensive due to the need to train a large number of neural network configurations. In this paper, we show that a simple regression model, based on support vector machines, can predict the final performance of partially trained neural network configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We use this regression model to develop an early stopping strategy for neural network configurations. With this early stopping strategy, we obtain significant speedups in both hyperparameter optimization and meta-modeling. Particularly in the context of meta-modeling, our method can learn to predict the performance of drastically different architectures and is seamlessly incorporated into reinforcement learningbased architecture selection algorithms. Finally, we show that our method is simpler, faster, and more accurate than Bayesian methods for learning curve prediction.", "creator": "LaTeX with hyperref package"}}}