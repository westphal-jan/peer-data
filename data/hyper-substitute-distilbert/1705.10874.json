{"id": "1705.10874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2017", "title": "Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments", "abstract": "potentially the negative effect of potentially non - lethal environmental noise is that low - standing research topic for simple recognition devices remains an important challenge indeed. once address optimization issue, traditional robotic sensory learning facilities seem to actively touched the market. many, user - driven based computer investigations, particularly the ones heading toward deep context, have similarly developed featuring potential alternatives. in this light, we are going to comprehensively use \u2026 recently developed technologies most representative autonomous access strategies to deal with newly raised problem in this light, avoiding the target of presenting guidelines for those who are going deeply addressing the field in utilizing robust object recognition. to better introduce specific approaches, we categorise them into user - and distributed - agent techniques, each within which encompasses specifically described broadly seemingly front - facing, incorporating back - projection, through general joint framework in speech recognition systems. in those meanwhile, insights describe fundamental pros and cons of these approaches, well as the relationships producing them, which can probably benefit future research.", "histories": [["v1", "Tue, 30 May 2017 21:31:25 GMT  (55kb)", "http://arxiv.org/abs/1705.10874v1", null], ["v2", "Sat, 8 Jul 2017 09:44:32 GMT  (104kb)", "http://arxiv.org/abs/1705.10874v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL cs.LG", "authors": ["zixing zhang", "j\\\"urgen geiger", "jouni pohjalainen", "amr el-desoky mousa", "wenyu jin", "bj\\\"orn schuller"], "accepted": false, "id": "1705.10874"}, "pdf": {"name": "1705.10874.pdf", "metadata": {"source": "CRF", "title": "Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments", "authors": ["Zixing Zhang", "J\u00fcrgen Geiger", "Jouni Pohjalainen", "Amr El-Desoky Mousa", "Bj\u00f6rn Schuller"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n70 5.\n10 87\n4v 1\n[ cs\n.S D\n] 3\n0 M\nay 2\n01 7\nEliminating the negative effect of highly non-stationary environmental noise is a long-standing research topic for speech recognition but remains an important challenge nowadays. To address this issue, traditional unsupervised signal processing methods seem to have touched the ceiling. However, data-driven based supervised approaches, particularly the ones designed with deep learning, have recently emerged as potential alternatives. In this light, we are going to comprehensively summarise the recently developed and most representative deep learning approaches to deal with the raised problem in this article, with the aim of providing guidelines for those who are going deeply into the field of environmentally robust speech recognition. To better introduce these approaches, we categorise them into single- and multi-channel techniques, each of which is specifically described at the front-end, the back-end, and the joint framework of speech recognition systems. In the meanwhile, we describe the pros and cons of these approaches as well as the relationships among them, which can probably benefit future research."}, {"heading": "1. Introduction", "text": "Recently, Automatic Speech Recognition (ASR) has achieved tremendous success in both academic and industrial areas [1, 2, 3]. A large amount of relevant applications have started to become as an important part of our daily life, including various smartphone assistants (e. g., Siri, Cortana, Google Now), Amazon echo, kinect Xbox, and the like. However, one of the vital issues which severely degrades their performance in real life relates to the corruption effect of various environmental noises on speech.\nThe ambient noises that widely corrupt the clean speech s(t) in the realistic scenarios mainly include i) various additive noises a(t), such as stationary white noise (e. g., constant hum from machinery), and non-stationary interfering talking, music, and natural sounds; and ii) non-stationary convolutional noise r(t) that is largely caused by the room reverberation and the propagation channels. Hence, the distorted speech y(t) can be expressed as\ny(t) = s(t) \u2217 r(t) + a(t), (1)\nwhere t denotes the time index. Provided that it is possible to reliably detect instants of the absence of the target signal (i. e., the speech of interest), short-term stationary additive noise can be adequately tackled with standard, unsupervised noise reduction signal processing techniques developed in the 1970s and 1980s [4]. However, detecting and reducing the impact of unknown non-stationary noise, or competing non-stationary sound sources, is still very challenging in practice, owing to the unstable characteristics of non-stationary noise [5, 6, 7, 8, 9].\nTo overcome this problem, a new wave of research efforts have been made over the past few years, for example,\nthe organisations of REVERB and a series of CHiME challenges [7, 10, 8, 6]. In such a new research stage, data-driven based approaches in a supervised machine learning paradigm has received increased attention, and have emerged as influential alternatives for enhancing the noise-robustness (i. e., nonstationary noise) of ASR systems [11]. The primary objective of these approaches is, via learning from large amounts of training data, to either obtain cleaner speech signals or features from noisy speech, or directly explore the discriminating phoneme representations from noisy speech. To this end, a revolutionary technique called deep learning has particularly played a central role in the recent developments [12, 13, 14]. The essential idea of deep learning is to extract high-level and complex abstractions of data by using neural networks in a deep structure (i. e., multiple layers). Deep learning has been repeatedly verified to be powerful in making use of massive training data to build complex and dedicated analysis systems, and has achieved considerable success in a variety of fields, such as gaming [15], visual recognition [16], language translation [17], and ASR [18, 19]. All these achievements have leveraged increasing research efforts on deep learning to efficiently improve the robustness of ASR in noisy environments.\nIn this survey, we provide a systematic overview of relevant deep learning approaches that are designed to address the noise robustness problem for speech recognition. Rather than enumerating all related approaches, we endeavour to establish a taxonomy of the most promising approaches, which are categorised by the two principles: i) According to the addressed channel number, the approaches can be grouped into singlechannel and multi-channel techniques. ii) According to the processing stages where deep learning is applied to, these ap-\nPreprint submitted to ** June 1, 2017\nproaches can be generally classified into front-end, back-end, and joint front- and back-end techniques (as shown in Fig. 1). We then highlight their advantages and disadvantages, and establish their interrelations and distinctions among the prominent techniques.\nWhilst several related surveys are available in the literature (e. g., [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning. This point, however, is deemed as one of the most considerable advances for the addressed problem in recent years, and forms the key concentration of this survey.\nThe remainder of this article is organised as follows. In Section 2, we briefly introduce the principle of some basic neural networks. In Sections 3 to 5, we comprehensively summarise the representative single-channel algorithms at the front-end, the back-end, and the joint front- and back-end of speech recognition systems, respectively. In Section 6, we then review the most promising multi-channel algorithms, before drawing the conclusions in Section 7."}, {"heading": "2. Related Neural Networks", "text": "Before going deeply into the review, we shortly describe the widely used neural networks for deep learning as priori knowledge. These neural networks mainly include Deep Boltzmann Machines, Stacked Autoencoders, Recurrent Neural Networks, and Convolutional Neural Networks. For more details about these networks, the reader is referred to [25] and [26].\n2.1. Deep Boltzmann Machines and Stacked Autoencoders\nTwo of the most popular architectures at the early development stage of deep learning are Deep Boltzmann Machines (DBMs) [27] and Stacked AutoEncoders (SAEs) [28]. Each of these is obtained by stacking multiple layers of Restricted Boltzmann Machines (RBMs) or feedforward autoencoders, respectively. The essential idea of these networks is to utilise deep architectures to model the complex and potentially non-linear functions that represent high-level abstractions, via an unsupervised pre-training in a greedy layerwise fashion [27, 29]. Each layer is trained with an encoder f (\u00b7) and a decoder g(\u00b7) by minimising the reconstruction error for its input y:\ng( f (y)) \u2248 y. (2)\nThe output of the encoder f (y) forms an alternative representation of the input y, and is then fed into a successive layer as input. This procedure is repeated layer-by-layer until all predefined layers are initialised. Training the stacked layers in this manner allows a deep network to incrementally learn a representation that is more robust than the one learnt by training the whole network in ensemble from a random initialisation of weights. For a detailed description as to why pre-training autoencoders yields better performance, the reader is referred to [30]. An extension of SAE is Stacked Denoising AutoEncoder (SDAE) [31, 32], where the initial inputs y are corrupted into a partially destroyed version y\u0303 by means of stochastic mapping y\u0303 \u223c qd (\u0303y|y). By doing this, the generative capability of the high-level representations is improved [31, 32].\nFurther, a variation of DBMs is Deep Belief Network (DBNs). Different from DBMs of which all connections are undirected, the top two layers of DBNs form an undirected graph and the remaining layers form a belief net with directed top-down connections [33]. In this article, unless otherwise stated the term Deep Neural Networks (DNNs) particularly refers to as DBMs or DBNs.\n2.2. Recurrent Neural Networks\nIn contrast to the aforementioned neural networks, where connections are only available between two adjacent layers, Recurrent Neural Networks (RNNs) allow cyclical connections. These connections consequently endow the RNNs with the capability of accessing previously processed inputs. That is, the hidden state st at time step t is obtained by the previous hidden state st\u22121 at time step t \u2212 1 and the input yt at the time step t:\nst = f (Uyt +Wst\u22121), (3)\nwhereU andW are the associated weight matrices; the function f is usually a nonlinearity, such as tanh, sigmoid, or ReLU.\nThe standard RNNs, however, cannot access long-range context since the backpropagated error when training either blows up or decays over time (the vanishing gradient problem) [34]. To overcome this limitation, Hochreiter and Schmidhuber introduced Long Short-Term Memory (LSTM) RNNs [34], which are able to store the information in memory cells over a long period. The LSTM-RNNs replace the traditional neurons of RNNs by so-called memory blocks. Analogous to the cyclic connections in RNNs, these memory blocks are recurrently connected. Every memory block consists of self-connected linear memory cells and three multiplicative gate units: input, output, and forget gates. The input and output gates scale the input and output of the cell, respectively, while the forget gate scales the internal state. In other words, the input, output, and forget gates are responsible for writing, reading, and resetting the memory cell values, respectively. Therefore, LSTMRNNs could also be regarded as a natural extension of deep neural networks for temporal sequential data, where the deepness comes from layers through time. Furthermore, similar to the DBM and the SAE, stacking RNN blocks into a deep structure has attracted increased attention, leading to a Deep RNN (DRNN) [35].\n2.3. Convolutional Neural Networks\nConvolutional Neural Network (CNN) is a biologically inspired variant of Multilayer Perceptron (MLP) originally developed for visual perception tasks [36, 37, 38]. It consists of one, or more, convolutional layers (often with a pooling layer), and then followed by one or more fully connected layers.\nTypically, convolutional layer is referred to as feature extraction layer; it includes multiple filters (aka kernels), the size of which is normally much smaller than the dimension of the input. Each neuron of the filter is connected with a local receptive field of previous layers and extracts a local feature representation. Each of the filter is convolved with the input, giving rise to produce multiple corresponding feature maps. The feature maps are then subsampled, typically with mean or max pooling, over other small regions to reduce the feature dimension. Several such convolutional and subsample layers are concatenated in a deep structure, finally followed by fully connected layers for classification or regression. One of the advantages of CNN is that it can extract efficient features by automatic learning. For speech processing, as the time-series property of speech, 1D filter instead of 2D filter is usually considered to take input samples."}, {"heading": "3. Front-End Techniques", "text": "The block diagram of single-channel based ASR front-end is illustrated in Fig. 2. It is given the noisy speech signal y(t), which is obtained from the clean speech s(t) corrupted by the convolutional noise r(t) and the additive noise a(t) (as shown in Eq. (1)). Then, the mixed noisy signal y(t) is preprocessed for framing, etc. After that, a series of processes is executed to extract the features of Mel-Frequency Cepstral Coefficients (MFCCs) for speech recognition, including Short-Time Fourier Transform (STFT), square magnitude, Mel-frequency filterbank, log Mel-frequency filterbank, and Discrete Cosine Transform (DCT). For better introduction of related approaches, we separately term the data spaces after each processing stage as temporal, Time-Frequency (T-F) spectral, power spectral, Mel spectral, logMel spectral, and cepstral domains.\nIn such a set of processes, enhancement techniques can be theoretically applied to each domain, i. e., from the raw signal in the temporal domain to the MFCCs in the cepstral domain. The enhancement techniques often relate to speech enhancement, source separation, and feature enhancement. Specifically, the objective of speech enhancement is to improve the quality and intelligibility of the estimated target speech s\u0302(t); whilst source separation aims to split multiple mixed sources so as to acquire multiple independent speech sources (e. g., speakers). Thus, source separation can be viewed as a specific case of\nspeech enhancement, where all other interfering sound sources are regarded as noise. Both speech enhancement and source separation attempt to obtain the estimated temporal signals as clean as possible, which can certainly be used for any speech applications like teleconferencing. Feature enhancement, however, mainly focuses on purifying the derived distorted features, such as MFCCs, which are largely designed for specific intelligent tasks (i. e., ASR here). In this survey, we unified all three terms as enhancement techniques, as they often share the same or similar algorithms.\nBefore discussing the deep-learning based enhancement methods at length, we introduce the most common objective performancemeasures for speech enhancement and source separation, and briefly review the central traditional enhancement techniques as priori knowledge.\n3.1. Objective Measures\nThe de facto standard metric to evaluate the ASR system performance is the Word Error Rate (WER). However, for speech enhancement and source separation, there are several independent ways available for evaluating the quality and intelligibility of enhanced speech. The most general and reliable way is performing subjective listening tests [39]. This method, however, is notoriously expensive and time consuming. For this reason, the research community established a variety of objectivemeasurements to quantify the similarity of the enhanced and the original speech [40, 41, 42, 4].\nSpecifically, segmental Signal-to-Noise Ratio (segSNR) is one of the most popular objective measures [40, 41]. Rather than working on the whole signal, segSNR averages the SNRs over speech segments calculates the average of the SNR values over segments of the speech signals. Compared with SNR, segSNR gives better results for waveform encoders. In addition, distance measures are used to evaluate the difference between the clean and enhanced (or noisy) signal in different representation domains (e. g., spectral or cepstral); and Source-toDistortion Ratio (SDR) is also frequently used in evaluating the performance of source separation algorithms [43]. Further, Perceptual Evaluation of Speech Quality (PESQ) [44, 45] is a test methodology developed for quality assessment of telephoneband speech. It makes use of psychoacoustic models and has been widely adopted for speech quality assessment in place of subjective listening tests. It has also shown some potential in predicting speech intelligibility but this appears to depend on the noise conditions [46].\n3.2. Traditional Enhancement Techniques\nAutomatic removal of non-stationary noise is a central task in speech enhancement and many unsupervised signal processing\nmethods have been developed for it [4]. The advantage of these methods over supervised approaches is that they do not require training with noisy speech and an exactly corresponding clean target. In the context of this survey, however, we generally assume that such training material with the expected type of noise is available.\nIn general, these unsupervised algorithms model the noise based on the same noisy inputs that they aim to enhance. The simplest approach to automatically constructing a noise model is by means of a Voice Activity Detector (VAD) such that the noise model is updated during non-active segments. As the performance of the unsupervised noise reduction methods is strongly affected by the noise modelling approach, more sophisticated noise estimation techniques have also been developed, that track spectral minima also during speech activity [47, 48].\nWhilst an instantaneous representation of the noise is obtained, noise reduction methods typically use it to construct an enhancement filter in order to remove the noise in each shortterm analysis frame, and then combine the enhanced frames by the overlap-add method [49] in order to resynthesise the (enhanced) signal. Classical single-channel noise reduction methods include variants of spectral subtraction [50, 51, 52], where an averaged noise spectrum (magnitude or power spectrum) is subtracted from the noisy signal spectrum, while keeping the resultant spectral magnitudes positive. Spectral subtraction only affects the spectrum magnitudes, while the spectrum phases are obtained from the noisy signal. While spectral subtractive methods rely on deterministic noise models, Wiener filtering [4] adopts stochastic models and is often implemented in practice using iterative approaches which base new estimates of the Wiener filter on the enhanced signal obtained by the previous iteration\u2019s Wiener filter estimate [53]. Noniterative approaches based on a priori SNR estimation have also been developed for implementing Wiener filtering [54, 55, 4]. Another popular family of techniques comprises the minimum mean square error (MMSE) [56] and log-spectral amplitude MMSE (LogMMSE) short-time spectral amplitude (STSA) estimators [57]. Their performance is still considered to be among the best of the published general-purpose speech noise reduction methods [58] and in comparison to many other such methods, they exhibit lower amounts of musical noise (spectral components left as residual after enhancement) [59]. In part, this can be attributed to the temporal smoothing of their \u201cdecision-directed\u201d estimation approach, where the spectral estimate of each frame is based partially on the estimates from previous frames via the a priori SNR estimate updated by using a memory coefficient [59, 54, 55]. More recently, spectral subtraction [60] and the decision-directed MMSE [58] methods have also been applied in the spectral modulation domain in order to better handle nonstationary noise.\nParticularly, a supervised learning-based method, namely Non-negative Matrix Factorisation (NMF), was recently frequently utilised [61]. The main idea is to find a dictionary (basis) matrix W and an activation matrix H, so that their multiplications can be used to represent the original matrix V of noisy speech spectrum, i. e., V \u2248 WH, where both W and H are with\na constraint of non-negativity. The columns of W are dictionary atoms w, representing the spectra of acoustic events. To enhance the target speech, the dictionaries of speech and noise are separately trained on clean speech and noise only [62, 63]. Nevertheless, it is noted that it highly relies on the assumption that the training speech or noise are in the same acoustic distribution with the unseen speech and noise data. To improve the generalisation ability, an exemplar-based approach has been proposed to obtain larger dictionaries [64], which is designed to cover multiple speakers and pronunciation variants in a speech dictionary. In realistic scenarios, the applied environments or the speakers may vary and are not easy to be predicted. A compromise solution to address this issue is semi-supervised NMF [65]. That is, either the speech dictionary or the noise dictionary is pre-defined in the training stage. Then, in the test time, it estimates the unknown dictionary Wi for the source i alongside with the activations H of all dictionary atoms. This method has been examined in [66, 67], and has shown its efficiency.\n3.3. Deep-Learning-based Enhancement Techniques\nFor better reviewing, we set the input of learning model (i. e., deep neural networks) as y that is extracted from noisy speech, and the target as x. Based on the types of training target x, the enhancementmethods can be categorised into i)mapping-based methods and ii) masking-basedmethods.\n3.3.1. Mapping-based Methods\nThe mapping-basedmethods is to learn a non-linear mapping function from the observed noisy speech y(t) into the desired clean speech s(t), as\ny(t) F \u2212\u2192 s(t), (4)\nwhere F is the learnt mapping function. Owing to the highdimension and fast-variation problems of raw speech signals, such a learning strategy is often applied to the spectral and cepstral domains rather than the temporal domain. That is, the data used for network training can be absolute magnitude Y(n), power magnitude |Y(n)|2, Mel-frequency filterbank Ymel(n), log Mel-frequency filterbank Y logMel(n), or MFCC Ym fcc(n), as shown in Fig. 2. In doing this, the data dimensions can be significantly reduced, leading to less computational complexity.\nTo learn F, the neural network is trained to reconstruct the target features x (extracted from the clean speech s(t)) from the corresponding input features y (extracted from the corrupted speech y(t)). The parameters of the model \u03b8 are determined by minimising the objective function of Mean Squared Error (MSE):\nJ(\u03b8) = 1\nN\nN\u2211\nn=1\n\u2016F(yn) \u2212 xn\u2016 2, (5)\nwhere \u2016\u00b7\u20162 is the squared loss, and n denotes the frame index. After the estimated clean features x\u0302n = F(yn) obtained, they will be then inversed back to the time-domain signal s\u0302(t) by using the phase information from original noisy speech, and evaluated by the objective measures as mentioned in Section 3.1.\n1) Based on SAE or DBM: Specifically, in 2013 Lu et al. [68] employed SAE to map the noisy speech to the clean speech in the Mel spectral domain. Given an AE that includes one non-linear encoding stage and one linear decoding stage for real valued speech as\nh(y) = g(W1y + b)\nx\u0302 = W2h(y) + b, (6)\nwhere W1 and W2 are the weight matrices of encoding and decoding, b is the bias, and g denotes the activation function. The training pair for the first AE is y and x, and then the training pair for the next AE will be h(y) and h(x) if weight matrices of encoder and decoder are tied, i. e., W1 = W T 2 = W. The empirical results indicate that SAE-based enhancement method notablely outperforms the traditional method like MMSE for enhancing the speech distorted by factory and car noises [68].\nAnalogous to this, another successful work has been done in [69], where DBM was utilised to estimate the complex mapping function. In the pre-training stage, noisy speech was used to train RBMs layer-by-layer in a standard unsupervised greedy fashion to obtain a deep generative model [29]; whereas in the fine-tuning process, the desired clean speech was set as to the target by minimising the objective function as Eq. (13). Similar research efforts were also extensively made on the T-F [70] and the logMel spectrum domains [71], respectively.\nMotivated by the fact that the same distortion in different frequency bands has different effects on speech quality, a weighted SAE was proposed in [72] which has shown positive performance for denoising. In detail, a weighted reconstruction loss function is employed to train SAE on the power spectrum as\nJ(\u03b8) = 1\nN\nN\u2211\nn=1\n\u03bbw\u2016F(yn) \u2212 xn\u2016 2, (7)\nwhere \u03bb is the weighting function.\nFurther, similar work was also done in [73] and [74], where the authors utilised SDAE to enhance the Mel filterbank features corrupted by either additive or convolutional noise for ASR. The networks were pre-trained with multi-condition data, and fine-tuned by mapping the noisy speech to the clean speech. The experiments conducted on CENSREC-4 database indicate that the SDAE-based mapping method remarkably outperforms the spectral subtraction method [73] in ASR.\n2) Based on LSTM-RNN: For the mapping-based speech enhancement, context information of speech and noise is considered to be important [75]. However, the aforementioned networks (i. e., SAE, DBM, and SDAE) are evaluated to be less capable in this aspect, although certain naive solutions were performed, such as expanding several sequential frames as a long vector input [69]. RNN, espcially the ones equiped with LSTM blocks (namely LSTM-RNN), has been frequently verified to be highly capable of capturing the context information in long sequence [76, 77], as mentioned in Section 2.2.\nIn this light, Maas et al. [11] introduced RNN to purify the input features (i. e., MFCCs). Specifically, the model was trained to predict clean features given noisy input frame by frame.\nEvaluated by ASR systems, this enhancement model is shown to be competitive with other DNN-based mapping models at various levels of SNR [11]. Following this work, Wo\u0308llmer et al. [78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82]. With the help of LSTMRNN, the speech recognition systems performmuch better than the one without LSTM-RNN when decoding the noisy speech.\n3) Based on CNN: More recently, an interesting research topic has emerged towards automatically learning robust phoneme representations from original noisy speech. For example, Chang et al. [83] proposed a novel feature extraction framework that concatenates a CNN and a fully connected deep neural network. Specifically, Gabor filter kernels are integrated with CNN, which aims to capture the spectrotemporal receptive field as it is designed to model human auditory processing. The bottleneck features generated from the deep neural networks are then used for training traditional HiddenMarkovModel (HMM), providing superior performance on two standard noisy speech databases (i. e., Aurora-4 and noisy WSJ) [83].\n3.3.2. Masking-based Methods\nDifferent from mapping-based methods, masking-based methods aim to learn a regression function from noisy speech spectrum Y(n, f ) to T-F mask M(n, f ). That is,\nY(n, f ) F \u2212\u2192 M(n, f ). (8)\nGenerally, there are two typical masks available in the literature: Ideal Binary Mask (IBM) [84] and Ideal Ratio Mask (IRM) [85]. For the IBM, a T-F mask unit is set to as 1 if the the local SNR is greater than a threshold R (indicating clean speech domination), or 0 if otherwise (indicating noise domination). That is,\nMb(n, f ) =\n{ 1, if S NR(n, f ) > R,\n0, otherwise, (9)\nwhere S NR(n, f ) denotes the local SNR within the T-F unit at frame index n and frequency bin f . Hence, the IBM is a binary matrix.\nThe IBM is indeed a binary approximation to IRM. For the IRM, a T-F mask unit is assigned by the soft ratio of the clean speech energy and the noisy (mixture) speech energy, as follows:\nMs(n, f ) = |S (n, f )|2\n|S (n, f )|2 + |N(n, f )|2 , (10)\nwhere S (n, f ) and N(n, f ) are the clean speech and noise in the T-F spectral domain, respectively. Therefore, the IRM is closely related to the Wiener filter, and can be viewed as its instantaneous version.\nIn the network training stage, given the input y from the T-F spectrum of mixed noisy signals Y(n, f ) and the target x from the calculated T-F mask M(n, f ), the parameters of neural networks \u03b8 are determined by the called Mask Approximation (MA) objective function. That is, it attempts to minimise the\nMSE between the estimated mask and the target mask as follows\nJ(\u03b8) = 1\nN\nN\u2211\nn=1\n\u2016F(yn) \u2212 M(n, f )\u2016 2, (11)\nwhere \u2016\u00b7\u20162 is the squared loss, n denotes the frame index, and F(yn) is restricted to the range [0,1].\nIn the test stage, to filter out the noise, the estimated mask M\u0302(n, f ) = F(yn) is sequentially applied to the spectrum of the mixed noisy signal y by\nx\u0302n = yn \u2297 M\u0302(n, f ), (12)\nwhere \u2297 denotes the elementwise multiplication. After that, it transforms the estimated clean spectrum x\u0302 back to the timedomain signal s\u0302(t) by inverse STFT.\nSpecifically, Wang and Wang [86] first introduced DNNs to perform IBM estimation for speech separation, which significantly outperforms other methods without deep learning. Subsequently, Wang et al. [87] compared a variety of masks and indicated that the IRM is superior to the IBM in terms of objective intelligibility and quality metrics. Such a conclusion was further verified by the work in [88], where the obtained results suggested that IRM achieves better ASR performance than IBM. Nevertheless, the work done by Grais et al. [89] showed that combining IBM and IRM can deliver better performance than each of them used independently for source separation.\nRather than estimating the masks in the T-F spectral domain, the masking-based approaches were also successfully applied to a reduced feature space \u2013 Mel frequency domain [90, 88] and its logarithmic scale [91] that have frequently been proven to be effective for ASR in deep learning. The experimental results in [90] showed that the masking-based approaches in the Mel frequency domain perform better than the ones in the T-F spectral domain in terms of SDR.\nFurther, another trend of the masking-based approaches towards replacing DNNs with LSTM-RNNs as the mask learning model [91, 92, 90], since LSTM-RNNs have shown to be capable of learning the speech and noise context information in a long temporal range [76, 77] which is vitally important for such a sequence learning task. The research efforts made in [90] have demonstrated the LSTM-RNNs can notably outperform the DNNs in mask estimation, as well as NMF, for source separation.\nApart from employing MA-based objective function to optimise the model, more and more studies have recently started to use Signal Approximation (SA) objective function [93, 94, 90]. Such an alternative straightforwardly targets at minimising the MSE between the estimated clean spectrum x\u0302 = y \u2297 F(y) and the target clean spectrum x by\nJ(\u03b8) = 1\nN\nN\u2211\nn=1\n\u2016yn \u2297 F(yn) \u2212 xn\u2016 2. (13)\nThis is indeed similar to the objective function used for the mapping-based methods (cf. Section 3.3.1). Empirically, employing the objective function based on SA performs better than\nthe one based on MA for source separation [90]. Moreover, the conclusions found in [90] and [95] indicated that combining the two objective functions (i. e., MA and SA) can further improve the speech enhancement performance in both the T-F and the Mel domains.\nMost of these methods, however, have only considered the magnitude or power spectrum when calculating the target ideal mask. The distorted phase information is completely ignored, even though it was proved to be helpful for speech enhancement [96]. To this end, the studies done by Erdogan et al. [97] and Weninger et al. [92] have taken the phase information into account, contributing to better performance in terms of SDR. Similarly, Williamson et al. [98] proposed complex ratio masking for DNN-based monaural speech separation, which learns the real and imaginary components of complex spectrograms jointly in the Cartesian coordinate system instead of learning magnitude spectrograms only in the traditional polar coordinate system. Notable performance improvement was observed as well.\nAdditionally, a multi-task learning framework has proposed in [93, 94] to jointly learn multiple sources (i. e., speech and noise) and the mask simultaneously. The assumption behind this idea is that the relationship between noise and its caused speech distortion could be learnt and help for estimating the clean speech. The experimental results have shown that such a joint training framework is superior to the isolated training way [93].\nAlthough the masking-based approaches were initially designed for removing additive noise, recent research has showed that they are capable of eliminating convolutional noise as well [91, 97, 92]."}, {"heading": "4. Back-End Techniques", "text": "The back-end techniques are also known as model-based techniques. They leave the noisy observation unchanged, and instead let the neural networks automatically find out the relationship between the observed speech and the phonetic targets. The most widely used approach in robust ASR often involves with multi-condition training [99]. In doing this, various acoustic variations caused by different noises are provided in the training process, reducing the acoustic distribution gap between the training and the test data.\nAnother common way to compensate the acoustic distribution mismatch is model adaptation, which is performed on the pre-trained Acoustic Modelling (AM). However, modifying the entire weights of the neural networks with small adaptation data easily leads to overfitting and results in extremely large noise dependent parameter sets. Alternatively, small subsets of neural network weights can be modified. For example, the authors of the work [100] added extra layer with linear activation to the network of input, hidden layers, or output, for model adaptation, which contributes to a considerable system robustness in noisy conditions.\nRather than forcing the pre-trained AM to adapt to various noisy conditions, an alternative way aims to let the network-\nbased AM be informed about the noise information when training, which is often termed as Noise-Aware Training (NAT) [99]. In this case, a noise estimate present in the signal serves as an augmented input and is incorporate with the original observation input, i. e., [y, n\u0302]. In this way, the DNN is being given additional cues in order to automatically learn the relationship between noisy speech and noise in a way that is beneficial to predict phonetic target [99]. Experimental results on Aurora-4 for speech recognition showed that the DNN-based AM trained with NAT has remarkable noise robustness. A similar work was also done in [101], where the noise estimation was replaced by the room information as an augmented input for dereverberation.\nFurther, a more general way to involve with the noise information in the network input comes to employing ivector, which was originally developed for speaker recognition. In [102], Karanasou et al. introduced an i-vector to represent acoustic environment, except another conventional one for speaker. Then, the i-vectors are concatenated to the original acoustic features for every frames of the data. These expanded features form the input for neural network training and decoding. Analogous to this, Yu et al. [103] used the extracted i-vector to represent the noisy environment; however, the ivectors are calculated from the Vector Taylor Series (VTS) enhanced features and bottleneck features rather than MFCCs.\nRecently, a quite successful work was reported by [12], where a double-stream HMM architecture was used for fusing two AMs. One is traditional Gaussian Mixture Model (GMM) based AM, and other one is LSTM-RNN-based AM. Specifically, given the HMM emission state s and the input vector y, at every time frame n the double stream HMM has access to two independent information sources, pG(yn|sn) and pL(yn|sn), the acoustic likelihoods of the GMM and the LSTM predictions, respectively. Particularly, the LSTM-RNN-based AM was discriminatively trained to generate frame-wise phone prediction. The double-stream emission probability is computed as\np(yn|sn) = pG(yn|sn) \u03bbpL(yn|sn) 1\u2212\u03bb, (14)\nwhere the variable \u03bb \u2208 [0, 1] denotes the stream weight. With the help of LSTM-RNN-based AM, the performance of traditional GMM-HMM system is boosted for noisy speech recognition. Moreover, the effectiveness of such a system has already been shown as a winner of the second CHiME Challenge.\nFollowing research was introduced in [104] and found that the GMM-based AM is even unnecessary when the LSTM-RNN-based AM was trained to predict HMM states (i. e., senone, rather than phonemes), resulting in a hybrid LSTM/HMMmodel. That is, only the likelihoods of the LSTM predictions are employed for HMM, i. e., \u03bb = 0.0 in Eq. (14). The work also revealed that combining the LSTM-RNN-based AM for phoneme or state predictions can further improve the performance.\nRecently, a multi-task learning based AM has attracted increasing attention. For example, the work of [101] and [105] respectively introduced similar multi-task learning architectures but different network types (i. e., one is DNN and the other one\nis LSTM-RNN) for noisy speech recognition, where the primary task is senone classification and the augmented task is reconstructing the clean speech features. In these architectures, the objective function is calculated by\nJ(\u03b8) = \u03bbEc + (1 \u2212 \u03bb)Er, (15)\nwhere Ec and Er indicate the senone classification error and the reconstruction error, respectively. The underlying assumption of this kind of multi-task learning is that the representations that are good for producing clean speech should be easier to classify."}, {"heading": "5. Jointed Front- and Back-End Techniques", "text": "To better take advantage of the techniques in both the frontend and the back-end, more and more interests are focusing on jointing the two. The straightforward way to do this is employing the enhanced features in the front-end for re-training the AM in the back-end [81].\nMore sophisticatedly, Lee et al. [106] proposed a cascaded DNN structure, which concatenated two independent fine-tuned DNNs. The first DNN performs the reconstruction of the clean features from noisy features augmented by the noise estimation. The second DNN attempts to learn the mapping between the reconstructed features and the phonetic targets [106]. The assumption of such a cascaded structure is that it could learn a most discriminative representation for speech recognition when reconstructing the clean feature from the noisy one by feature enhancement in the front-end.\nA similar work has also been done by Wang et al. [107], who concatenated a DNN-based speech separation front-end and a DNN-based AM back-end to build a larger neural network, and jointly adjust the weights in each model. In doing this, the separation front-end is able to provide enhanced speech desired by the AM back-end, and the AM back-end can guide the separation front-end to produce more discriminative enhancement [107].\nTo utmost explore the potential of deep neural networks at different stages in the speech recognition chain, end-to-end systems have attracted increasing interests in recent years and have achieved tremendous achievement in ASR [2, 108]. The central idea is to jointly optimise the parameters of the networks at the front-end which automatically learn the inherent representations for the task at hand, and the networks at the back-end which provide final predictions.\nA quite recent and well-developed framework was reported in [14], where two tasks were evaluated: the Aurora-4 task with multiple additive noise types and channel mismatch, and the AMI meeting transcription task with significant reverberation. In this framework, a variety of very deep CNNs with many convolutional layers were implemented, and each of them is followed by four fully connected layers and one softmax output layer for senone prediction. Compared with the feedforward DNN, the CNNs have these advantages [14]: 1) It is well suited to model the local correlations in both time and frequency in\nspeech spectrogram; 2) Translational invariance, such as frequency shift due to speaking styles or speaker variations, can be more easily captured by CNNs. The reported results on AMI by using the proposed end-to-end framework is much higher than the traditional DNN and is competitive to the LSTM-RNNbased AM, and the results on Aurora-4 achieve the best compared with any other published results on this database, even without performing any speech and feature enhancement approaches."}, {"heading": "6. Multi-Channel Techniques", "text": "Recent advance in robust ASR has also highly acknowledged the practical importance of using microphone arrays and multichannel processing (cf. the recent CHiME and REVERB challenges [8, 6] and their successful contributions which introduced improvements on the multi-channel techniques over the\nbaseline, e. g., [109, 110, 111, 112]). A central technique is acoustical beamforming, i. e., spatio-temporal filtering that operates on the outputs of the microphone array and converts it to a single-channel signal while focusing on the desired speech and attenuating the noise coming from other directions. Beamforming is a long-standing research topic in array signal processing with many applications, not limited to spatial audio processing [113, 114]. Beamformer output is often further enhanced by a microphone array post-filter [115, 116]. In this section, we first review the standard microphone array enhancement techniques, and then discuss the deep learning in a supportive way or an independent way.\n6.1. Beamforming and Post-Filtering\nBeamformers in general require a Direction-Of-Arrival (DOA) estimate for the target signal. In Delay-and-Sum (DS)\nbeamforming, which is one of the simplest approaches, the signals received by the different microphones are considered to have been delayed according to the target signal DOA. Fixed delay operators that compensate for the arrival delays are applied to the signals of the different microphones before summing them, so as to focus on the desired target direction; signal statistics are not considered by the DS beamformer (apart from a possible DOA estimation step). In contrast, adaptive beamforming techniques estimate statistics from the observed signal and determine the beamformer filter coefficients based on these statistics according to some criterion. In particular, the Minimum Variance Distortionless Response (MVDR) or Capon beamformer works in the frequency domain and aims to minimise the energy at the beamformer output, while simultaneously keeping the gain in the direction of the target signal fixed at unity. The complex-valued signal model is Y(n) = S (n)d + A(n), where the vector Y(n) = (Y1(n), . . . , YM(n)) T contains the instantaneous noisy observations at the nth time instant on a given discrete frequency bin as registered by the M microphones, S (n) is the corresponding complex frequency bin of the unknown transmitted signal, the steering vector d is the desired signal spatial signature encoding its direction of arrival and A(n) is a (M \u00d7 1) vector containing the noise and interference contributions. Both the signal and the noise are assumed to have zero mean. In operation, the beamformer computes a linear combination of a complex weight vector w and the observation vector Y(n) as\nx(n) = wHY(n), (16)\nwhere (\u00b7)H denotes the Hermitian transpose. The general approach of Eq. (16) is known as filter-and-sum beamforming. In determining w using the MVDR criterion, the spatial covariance matrix representing the covariance of the noise plus interference will be needed. It is generally unknown but can be estimated as a sample covariance matrix of a suitable segment of N observations as RVV = (1/N) \u2211 n Y(n)Y H(n) [117]. By then minimising wHRVVw with respect to w subject to the constraint wHd = 1, as mentioned above, the MVDR weight vector for the discrete frequency bin in question is given by [118]\nw\u0302MVDR = R\u22121 VV d\ndHR\u22121 VV\nd . (17)\nIn multi-channel audio enhancement, the enhancement and noise reduction provided by the beamformer alone is typically not sufficient, as reducing noise and reverberation at low frequencies would require very large arrays [115]. Therefore, the output of a beamformer is often further enhanced by post-filtering relying on a single-channel Wiener filtering approach with statistics estimated from the multi-channel observations [119, 116, 120]. An optimal multi-channel Wiener filter formulation according to the MMSE criterion gives rise to the MVDR beamformer followed by a single-channel Wiener post-filter [115, 116, 120]. The Zelinski post-filter [119, 115] shows reasonable performance but is based on simplified assumptions including a perfectly incoherent noise field with zero\ncorrelation between the noise on different channels, an assumption that does not hold with low noise frequencies and closely spaced microphones. In order to address this issue, McCowan and Bourlard [116] proposed a generalisation of the Zelinski post-filter that assumes prior knowledge of the complex coherence of the noise field and demonstrated it with a diffuse noise model.\nThe MVDR beamformer is not robust against an inaccurately estimated steering vector d [121]. In contrast, Generalised EigenValue (GEV) beamforming requires no DOA estimate and is based on maximising the output signal-to-noise ratio [122]. The beamformer filter coefficients for a given frequency bin are found as the principal eigenvector of a generalised eigenvalue problem as required by [122]\nw\u0302GEV = argmax w\nwHRS S w wHRVVw , (18)\nwhere RS S and RVV are the required estimates of the spatial covariance matrices of the target speech and noise/interference, respectively. When applied to broadband signals such as audio, this principle leads to considerable distortion of the speech due to independent SNR optimisation of each frequency bin. In [122], this is compensated by single-channel post-filtering in order to achieve a performance comparable to the MVDR beamformer.\n6.2. Neural-Network-Supported Beamformers\nTo calculate the spatial covariance matrices RS S and RVV of the GEV beamformer (see Eq. (18)), Heymann et al. [123] proposed in the third CHiME challenge (CHiME-3) to use LSTMRNNs to firstly estimate two Ideal Binary Masks (IBMs) for each microphone channel: one to indicate which T-F bins are presumably dominated by speech, and another to indicate which T-F bins are dominated by noise. This idea is similar to the one in [88], where a DNN was employed to predict the noise mask. To train the neural networks, the authors in [123] further used a multi-task learning framework with the input of noisy speech, and two corresponding IBM targets respectively from clean speech and noise in separate output layers. After the masks for each channel are obtained, they are then condensed to a single speech and a single noise mask by a median filter, which are sequentially used for estimating the spatial covariance matrices RS S and RVV and in turn the beamformer coefficients w\u0302GEV .\nThe aforementioned supervised neural network training process, however, requires both speech and noise counterparts of the noisy speech for each microphone channel. In this case, only the simulated data is possible to be employed for network training. To relax this requirement to some extent, a followup work was presented in [124], where only the clean speech was employed for mask estimation. Such a slight improvement enables it to utilise more realistic noisy and clean speech pair, which can be recorded simultaneously by close microphone (for clean speech) and distant microphone array (for noisy speech). The experimental results shown in [124] were competitive with\nprevious work. Furthermore, the effectiveness of the neuralnetwork-supportedGEV beamformer has been demonstrated in the recent 4th CHiME Challenge [112, 125].\nIn CHiME-3, Sivasankaran et al. [126] used a DS beamformer to obtain single-channel noisy spectra, and then used a DNN to map them into speech and noise spectra, which were then employed in constructing a multi-channelWiener filter according to [136], in order to replace the basic MVDR beamformer used in the challenge.\nWhile deep learning approaches have shown moderate success in CHiME-3 in estimating speech and noise statistics for multi-channel Wiener filtering and beamforming, the winning contribution was still able to rely on the baseline MVDR beamformer complemented with improved DOA estimate (the steering vector) based on T-F masks estimated using complex GMMs [109, 137]. Recent studies have, however, started to investigate the use of DNNs from the earliest stages of multichannel analysis, such as in DOA and steering vector estimation. DOA estimation using DNNs was studied in [127] and a follow-up study investigated the implementation of DS beamforming in an ASR system directly using a feedforward DNN that is used to predict the beamformer\u2019s weight vector [128]. In both cases, the network was trained with generalised cross correlation from simulated multi-channel data from a given array geometry using all possible DOA angles. In the latter paper, the future work envisioned by the authors includes the use of spatial covariance matrices as input to the network, so that they could also use speech and noise statistics in predicting (adaptive) beamformer parameters.\nAs for post-filtering, very few recent papers appear to have\nused neural networks for this purpose. One such study evaluated a non-deep MLP network in predicting the post-filter parameters for a circular microphone array [129].\n6.3. End-to-End Multi-Channel Approaches\nRather than using neural networks to support traditional beamformers and post-filters for speech enhancement, end-toend multichannel ASR systems have recently attracted more attention with a straightforward target of WER decrease [130, 131, 132]. In [133], the individual features extracted from each microphone channel are concatenated as a long single feature vector and fed into DNN for AM. Whilst such a feature concatenation operation is simple, it was still found to be effective on the AMI dataset [133], and was further verified in [130].\nRecently, a more sophisticated approach was proposed in [131]. In this work, the authors utilised a joint network structure of single convolutional layer followed by several fully connected DNN layers. In more detail, the convolutional layer was operated on each channel independently with the magnitude spectrum as input, while max pooling was proceeded across channels to choose the channel with the largest respond in each node. This algorithm was found to perform better than the one by applying CNN after a DS beamformer output [131].\nMotivated by the success of this work as well as the research trend of end-to-end ASR systems, a study [132] extended the work of [131] on the raw signals and without the operation of cross-layer max pooling. The advantage of this extended work is that it can automatically exploit the spatial information found in the fine time structure, which greatly lies in the previously discarded FFT phase value, of the multichannel signals.\nA follow-up work was reported in [134], where the authors employed two convolutional layers, instead of one layer, at the front-end. The assumption is that the spatial and spectral filtering process can be separately processed by two convolutional layers. That is, the first layer is designed to be spatially selective, and second layer is implemented to decompose frequency that are shared across all spatial filters. By factoring the spatial and the spectral filters as separate layers in the network, the performance of the investigated system was notably improved in terms of WER [134]."}, {"heading": "7. Conclusion", "text": "In this survey, we provided a comprehensive review on the state-of-the-art and most promising deep learning approaches with the goal of improving the environmental robustness of speech recognition systems. These technologies are mainly introduced in the viewpoint of single-channel and multi-channel, representative works of which are respectively summarised in Table 1 and Table 2. Both single and multi-channel techniques include the approaches that can be carried out in different ASR processing stages, i. e., the front-end, the back-end, or the jointed front- and back-end.\nThanks to the advance of deep learning, major achievements have been accomplished in the past four years, as shown in the literature. However, an obvious performance gap still remains between the enhanced system and the one evaluated in the clean environment. Thus, further efforts are still required for speech recognition to overcome the adverse effect of environmental noise [8, 6, 9].\nWe hope that this review could provide researchers and developers an opportunity to stand on the frontier of development in this field and to make greater breakthroughs."}, {"heading": "Acknowledgements", "text": "This work was supported by the Huawei Technologies\nCo. Ltd."}], "references": [{"title": "The IBM 2016 english conversational telephone speech recognition system", "author": ["G. Saon", "T. Sercu", "S. Rennie", "H.-K.J. Kuo"], "venue": "Proc. INTER- SPEECH, San Francisco, CA, 2016, 7\u201311.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "Proc. International Conference on Machine Learning (ICML), New York City, NY, 2016, 10 pages.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Achieving human parity in conversational speech recognition", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "Microsoft Research, Tech. Rep. MSR-TR-2016-71, Oct 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech enhancement: theory and practice", "author": ["P.C. Loizou"], "venue": "Abingdon, UK: Taylor Francis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Making machines understand us in reverberant rooms: Robustness against reverberation for automatic speech recognition", "author": ["T. Yoshioka", "A. Sehr", "M. Delcroix", "K. Kinoshita", "R. Maas", "T. Nakatani", "W. Kellermann"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 114\u2013126, Nov 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "A summary of the REVERB challenge: state-of-the-art and remaining challenges in reverberant speech processing research", "author": ["K. Kinoshita", "M. Delcroix", "S. Gannot", "E.A. Habets", "R. Haeb-Umbach", "W. Kellermann", "V. Leutnant", "R. Maas", "T. Nakatani", "B. Raj"], "venue": "EURASIP Journal on Advances in Signal Processing, vol. 2016, no. 1, pp. 1\u201319, Dec 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "The PAS- CAL CHiME speech separation and recognition challenge", "author": ["J. Barker", "E. Vincent", "N. Ma", "H. Christensen", "P. Green"], "venue": "Computer Speech & Language, vol. 27, no. 3, pp. 621\u2013633, May 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "The third \u2018CHiME\u2019 speech separation and recognition challenge: Dataset, task and baselines", "author": ["J. Barker", "R. Marxer", "E. Vincent", "S. Watanabe"], "venue": "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Scottsdale, AZ, 2015, pp. 504\u2013511.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition", "author": ["E. Vincent", "S. Watanabe", "A.A. Nugraha", "J. Barker", "R. Marxer"], "venue": "Computer Speech & Language, 2016, in press.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "The second \u2018CHiME\u2019 speech separation and recognition challenge: Datasets, tasks and baselines", "author": ["E. Vincent", "J. Barker", "S. Watanabe", "J. Le Roux", "F. Nesta", "M. Matassoni"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver, Canada, 2013, pp. 126\u2013130.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural networks for noise reduction in robust ASR", "author": ["A.L. Maas", "Q.V. Le", "T.M. O\u0143eil", "O. Vinyals", "P. Nguyen", "A.Y. Ng"], "venue": "Proc. INTERSPEECH, Portland, OR, 2012, pp. 22\u201325.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Memory-enhanced neural networks and NMF for robust ASR", "author": ["J.T. Geiger", "F. Weninger", "J.F. Gemmeke", "M. W\u00f6llmer", "B. Schuller", "G. Rigoll"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 6, pp. 1037\u20131046, Jun 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Channel mapping using bidirectional long short-term memory for dereverberation in hand-free voice controlled devices", "author": ["Z. Zhang", "J. Pinto", "C. Plahl", "B. Schuller", "D. Willett"], "venue": "IEEE Transactions on Consumer Electronics, vol. 60, no. 3, pp. 525\u2013533, Aug 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional neural networks for noise robust speech recognition", "author": ["Y. Qian", "M. Bi", "T. Tan", "K. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2263\u2013 2276, Dec 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, Feb 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, Dec 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "author": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "venue": "arXiv preprint arXiv:1609.08144, Oct 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 1, pp. 30\u201342, Jan 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A. r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath", "B. Kingsbury"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech recognition in noisy environments: A survey", "author": ["Y. Gong"], "venue": "Speech communication, vol. 16, no. 3, pp. 261\u2013291, Apr 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "Acoustical and environmental robustness in automatic speech recognition", "author": ["A. Acero"], "venue": "Berlin, Germany: Springer Science & Business Media,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Front-end, back-end, and hybrid techniques for noise-robust speech recognition", "author": ["L. Deng"], "venue": "Robust Speech Recognition of Uncertain or Missing Data. Berlin/Heidelburg, Germany: Springer, 2011, pp. 67\u201399.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Techniques for noise robustness in automatic speech recognition", "author": ["T. Virtanen", "R. Singh", "B. Raj"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "An overview of noiserobust automatic speech recognition", "author": ["J. Li", "L. Deng", "Y. Gong", "R. Haeb-Umbach"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745\u2013777, Apr 2014. 11", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, May 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, Jan 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), Vancouver, Canada, 2006, pp. 153\u2013160.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, Jul 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning?", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P.-A. Manzagol", "P. Vincent", "S. Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "Proc. International Conference on Machine Learning (ICML), Helsinki, Finland, 2008, pp. 1096\u20131103.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 3371\u20133408, Dec 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "Proc. International Conference on Artificial Intelligence and Statistics Conference (AISTATS), Clearwater Beach, FL, 2009, pp. 448\u2013455.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver, Canada, 2013, pp. 6645\u20136649.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, vol. 1, no. 4, pp. 541\u2013551, 1989.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1989}, {"title": "Receptive fields and functional architecture of monkey striate cortex", "author": ["D.H. Hubel", "T.N. Wiesel"], "venue": "The Journal of physiology, vol. 195, no. 1, pp. 215\u2013243, Nov 1968.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1968}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov 1998.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1998}, {"title": "Perceptual coding of digital audio", "author": ["T. Painter", "A. Spanias"], "venue": "Proceedings of the IEEE, vol. 88, no. 4, pp. 451\u2013515, Apr 2000.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "An effective quality evaluation protocol for speech enhancement algorithms", "author": ["J. Hansen", "B. Pellom"], "venue": "Proc. International Conference on Spoken Language Processing (ICSLP), Sydney, Australia, 1998, pp. 2819\u20132822.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1998}, {"title": "Evaluation of objective quality measures for speech enhancement", "author": ["Y. Hu", "P.C. Loizou"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 1, pp. 229\u2013238, Jan 2008.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462\u20131469, Jul 2006.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2006}, {"title": "Perceptual evaluation of speech quality (PESQ)\u2013a new method for speech quality assessment of telephone networks and codecs", "author": ["A.W. Rix", "J.G. Beerends", "M.P. Hollier", "A.P. Hekstra"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Salt Lake City, UT, 2001, pp. 749\u2013752.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Perceptual evaluation of speech quality (PESQ), the new ITU standard for end-toend speech quality assessment. part ii \u2013 psychoacoustic model", "author": ["J.G. Beerends", "A.P. Hekstra", "A.W. Rix", "M.P. Hollier"], "venue": "Journal of the Audio Engineering Society, vol. 50, no. 10, pp. 765\u2013778, Oct 2002.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2002}, {"title": "Objective measures for predicting speech intelligibility in noisy conditions based on new band-importance functions", "author": ["J. Ma", "Y. Hu", "P. Loizou"], "venue": "Journal of the Acoustical Society of America, vol. 125, no. 5, pp. 3387\u20133405, May 2009.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Noise power spectral density estimation based on optimal  smoothing and minimum statistics", "author": ["R. Martin"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 5, pp. 504\u2013512, Jul 2001.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2001}, {"title": "Noise spectrum estimation in adverse environments: Improved minima controlled recursive averaging", "author": ["I. Cohen"], "venue": "IEEE Transactions on speech and audio processing, vol. 11, no. 5, pp. 466\u2013475, Sep 2003.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "Enhancement of speech corrupted by acoustic noise", "author": ["M. Berouti", "R. Schwartz", "J. Makhoul"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Washington, D.C., 1979, pp. 208\u2013211.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1979}, {"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["S. Boll"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 27, no. 2, pp. 113\u2013120, Apr 1979.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1979}, {"title": "Enhancement and bandwidth compression of noisy speech", "author": ["J.S. Lim", "A.V. Oppenheim"], "venue": "Proceedings of the IEEE, vol. 67, no. 12, pp. 1586\u20131604, Dec 1979.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1979}, {"title": "Constrained iterative speech enhancement with application to speech recognition", "author": ["J.H. Hansen", "M.A. Clements"], "venue": "IEEE Transaction on Signal Processing, vol. 39, no. 4, pp. 795\u2013805, Apr 1991.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 1991}, {"title": "Speech enhancement based on a priori signal to noise estimation", "author": ["P. Scalart", "J. Filho"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Atlanta, GA, 1996, pp. 629\u2013 632.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1996}, {"title": "Speech enhancement based on wavelet thresholding the multitaper spectrum", "author": ["Y. Hu", "P.C. Loizou"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 12, no. 1, pp. 59\u201367, Jan 2004.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech enhancement using a minimummean square error short-time spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, Dec 1984.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1984}, {"title": "Speech enhancement using a minimum mean-square error logspectral amplitude estimator", "author": ["\u2014\u2014"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 23, no. 2, pp. 443\u2013445, Apr 1985.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1985}, {"title": "Speech enhancement using a minimum mean-square error short-time spectral modulation magnitude estimator", "author": ["K. Paliwal", "B. Schwerin", "K.W\u00f3jcicki"], "venue": "Speech Communication, vol. 54, no. 2, pp. 282\u2013305, Feb 2012.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2012}, {"title": "Elimination of the musical noise phenomenon with the Ephraim and Malah noise suppressor", "author": ["O. Capp\u00e9"], "venue": "IEEE Transaction on Speech and Audio Processing, vol. 2, no. 2, pp. 345\u2013349, Apr 1994.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1994}, {"title": "Single-channel speech enhancement using spectral subtraction in the short-time modulation domain", "author": ["K. Paliwal", "K. W\u00f3jcicki", "B. Schwerin"], "venue": "Speech Communication, vol. 52, no. 5, pp. 450\u2013475, May 2010.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, Oct 1999.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1999}, {"title": "Nonnegative matrix factorization as noise-robust feature extractor for speech recognition", "author": ["B. Schuller", "F. Weninger", "M. Wllmer", "Y. Sun", "G. Rigoll"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Dallas, TX, 2010, pp. 4562\u2013 4565.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Investigating NMF speech enhancement for neural network based acoustic models", "author": ["J.T. Geiger", "J.F. Gemmeke", "B. Schuller", "G. Rigoll"], "venue": "Proc. INTERSPEECH, Singapore, 2014, pp. 2405\u20132409.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Exemplar-based sparse representations for noise robust automatic speech recognition", "author": ["J.F. Gemmeke", "T. Virtanen", "A. Hurmalainen"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2067\u20132080, Sep 2011.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised and semisupervised separation of sounds from single-channel mixtures", "author": ["P. Smaragdis", "B. Raj", "M. Shashanka"], "venue": "Proc. International Conference on Independent Component Analysis and Signal Separation (ICA), London, UK, 2007, pp. 414\u2013421.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2007}, {"title": "Real-time speech separation by semi-supervised nonnegative matrix factorization", "author": ["C. Joder", "F. Weninger", "F. Eyben", "D. Virette", "B. Schuller"], "venue": "Proc. International Conference on Latent Variable Analysis and Signal Separation, Tel-Aviv, Israel, 2012, pp. 322\u2013329.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2012}, {"title": "Supervised and semi-supervised suppression of background music in monaural speech recordings", "author": ["F. Weninger", "J. Feliu", "B. Schuller"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Kyoto, Japan, 2012, pp. 61\u201364. 12", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Speech enhancement based on deep denoising autoencoder.", "author": ["X. Lu", "Y. Tsao", "S. Matsuda", "C. Hori"], "venue": "in Proc. INTERSPEECH, Lyon, France,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2013}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.R. Dai", "C.H. Lee"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, Jan 2014.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spectral mapping for speech dereverberation and denoising", "author": ["K. Han", "Y. Wang", "D. Wang", "W.S. Woods", "I. Merks", "T. Zhang"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 982\u2013992, Apr 2015.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2015}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.R. Dai", "C.H. Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, Jan 2015.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement with weighted denoising autoencoder.", "author": ["B. Xia", "C. Bao"], "venue": "in Proc. INTERSPEECH, Lyon, France,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2013}, {"title": "Reverberant speech recognition based on denoising autoencoder", "author": ["T. Ishii", "H. Komiyama", "T. Shinozaki", "Y. Horiuchi", "S. Kuroiwa"], "venue": "Proc. INTERSPEECH, Lyon, France, 2013, pp. 3512\u20133516.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech feature denoising and dereverberation via deep autoencoders for noisy reverberant speech recognition", "author": ["X. Feng", "Y. Zhang", "J. Glass"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 1759\u20131763.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, Nov 1997.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 1997}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves"], "venue": "arXiv preprint arXiv:1308.0850, Aug 2013.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining long short-term memory and dynamic bayesian networks for incremental emotion-sensitive artificial listening", "author": ["M. Wollmer", "B. Schuller", "F. Eyben", "G. Rigoll"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 5, pp. 867\u2013881, Oct 2010.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature enhancement by bidirectional LSTMnetworks for conversational speech recognition in highly non-stationary noise", "author": ["M.W\u00f6llmer", "Z. Zhang", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver, Canada, 2013, pp. 6822\u20136826.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2013}, {"title": "Feature enhancement by deep LSTM networks for ASR in reverberant multisource environments", "author": ["F. Weninger", "J. Geiger", "M. W\u00f6llmer", "B. Schuller", "G. Rigoll"], "venue": "Computer Speech & Language, vol. 28, no. 4, pp. 888\u2013902, Jul 2014.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2014}, {"title": "The MERL / MELCO / TUM system for the REVERB challenge using deep recurrent neural network feature enhancement", "author": ["F.Weninger", "S.Watanabe", "J. Le Roux", "J. Hershey", "Y. Tachioka", "J. Geiger", "B. Schuller", "G. Rigoll"], "venue": "Proc. REVERB Workshop, held in conjunction with ICASSP 2014 and HSCMA 2014, Florence, Italy, 2014, pp. 1\u20138.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2014}, {"title": "The munich feature enhancement approach to the 2nd CHiME challenge using BLSTM recurrent neural networks", "author": ["F. Weninger", "J. Geiger", "M. W\u00f6llmer", "B. Schuller", "G. Rigoll"], "venue": "Proc. 2nd CHiME workshop on machine listening in multisource environments, Vancouver, Canada, 2013, pp. 86\u201390.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2013}, {"title": "Facing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with LSTM neural networks", "author": ["Z. Zhang", "F. Ringeval", "J. Han", "J. Deng", "E. Marchi", "B. Schuller"], "venue": "Proc. IN- TERSPEECH, San Francisco, CA, 2016, 3593\u20133597.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust CNN-based speech recognition with gabor filter kernels.", "author": ["S.-Y. Chang", "N. Morgan"], "venue": "in Proc. INTERSPEECH, Singapore,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2014}, {"title": "On Ideal Binary Mask As the Computational Goal of Auditory Scene Analysis", "author": ["D. Wang"], "venue": null, "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2005}, {"title": "Binary and ratio timefrequency masks for robust speech recognition", "author": ["S. Srinivasan", "N. Roman", "D. Wang"], "venue": "Speech Communication, vol. 48, no. 11, pp. 1486\u20131501, Nov 2006.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards scaling up classification-based speech separation", "author": ["Y. Wang", "D. Wang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 21, no. 7, pp. 1381\u20131390, Jul 2013.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2013}, {"title": "On training targets for supervised speech separation", "author": ["Y. Wang", "A. Narayanan", "D. Wang"], "venue": "IEEE/ACM transactions on audio, speech, and language processing, vol. 22, no. 12, pp. 1849\u20131858, Dec 2014.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1849}, {"title": "Ideal ratio mask estimation using deep neural networks for robust speech recognition", "author": ["A. Narayanan", "D. Wang"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing  (ICASSP), Vancouver, Canada, 2013, pp. 7092\u20137096.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining mask estimates for single channel audio source separation using deep neural networks", "author": ["E. Grais", "G. Roma", "A.J. Simpson", "M.D. Plumbley"], "venue": "Proc. INTERSPEECH, San Francisco, CA, 2016, pp. 3339\u20133343.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminatively trained recurrent neural networks for single-channel speech separation", "author": ["F. Weninger", "J.R. Hershey", "J. Le Roux", "B. Schuller"], "venue": "Proc. IEEE Global Conference on Signal and Information Processing (GlobalSIP), Atlanta, GA, 2014, pp. 577\u2013581.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2014}, {"title": "Single-channel speech separation with memory-enhanced recurrent neural networks", "author": ["F. Weninger", "F. Eyben", "B. Schuller"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 3709\u20133713.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech enhancement with LSTM recurrent neural networks and its application to noise-robust ASR", "author": ["F. Weninger", "H. Erdogan", "S. Watanabe", "E. Vincent", "J. Le Roux", "J.R. Hershey", "B. Schuller"], "venue": "Proc. International Conference on Latent Variable Analysis and Signal Separation, Liberec, Czech Republic, 2015, pp. 91\u201399.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for monaural speech separation", "author": ["P.S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 1562\u20131566.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2014}, {"title": "Joint optimization of masks and deep recurrent neural networks for monaural source separation", "author": ["P.-S. Huang", "M. Kim", "M. Hasegawa-Johnson", "P. Smaragdis"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 12, pp. 2136\u20132147, Dec 2015.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2015}, {"title": "A deep neural network for time-domain signal reconstruction", "author": ["Y. Wang", "D. Wang"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 4390\u20134394.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2015}, {"title": "The importance of phase in speech enhancement", "author": ["K. Paliwal", "K. W\u00f3jcicki", "B. Shannon"], "venue": "speech communication, vol. 53, no. 4, pp. 465\u2013 494, Apr 2011.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2011}, {"title": "Phasesensitive and recognition-boosted speech separation using deep recurrent neural networks", "author": ["H. Erdogan", "J.R. Hershey", "S. Watanabe", "J. Le Roux"], "venue": "Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 708\u2013712.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2015}, {"title": "Complex ratio masking for monaural speech separation", "author": ["D.S. Williamson", "Y. Wang", "D. Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 3, pp. 483\u2013492, Mar 2016.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2016}, {"title": "An investigation of deep neural networks for noise robust speech recognition", "author": ["M.L. Seltzer", "D. Yu", "Y. Wang"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver, Canada, 2013, pp. 7398\u20137402.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2013}, {"title": "A study on deep neural network acoustic model adaptation for robust far-field speech recognition", "author": ["S. Mirsamadi", "J.H. Hansen"], "venue": "Proc. INTERSPEECH, Dresden, Germany, 2015, pp. 2430\u20132434.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving speech recognition in reverberation using a room-aware deep neural network and multitask learning", "author": ["R. Giri", "M.L. Seltzer", "J. Droppo", "D. Yu"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 5014\u20135018.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptation of deep neural network acoustic models using factorised i-vectors.", "author": ["P. Karanasou", "Y. Wang", "M.J. Gales", "P.C. Woodland"], "venue": "in Proc. INTERSPEECH, Singapore,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2014}, {"title": "Robust i-vector extraction for neural network adaptation in noisy environment", "author": ["C. Yu", "A. Ogawa", "M. Delcroix", "T. Yoshioka", "T. Nakatani", "J.H. Hansen"], "venue": "Proc. INTERSPEECH, Dresden, Germany, 2015, pp. 2854\u20132857.", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling.", "author": ["J.T. Geiger", "Z. Zhang", "F. Weninger", "B. Schuller", "G. Rigoll"], "venue": "in Proc. INTERSPEECH, Singapore,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2014}, {"title": "Speech enhancement and recognition using multi-task learning of long short-term memory recurrent neural networks", "author": ["Z. Chen", "S. Watanabe", "H. Erdo\u011fan", "J.R. Hershey"], "venue": "Proc. INTERSPEECH, Dresden, Germany, 2015, pp. 1\u20135.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-stage noise aware training using asymmetric deep denoising autoencoder", "author": ["K.H. Lee", "S.J. Kang", "W.H. Kang", "N.S. Kim"], "venue": "Proc. 13  IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5765\u20135769.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2016}, {"title": "A joint training framework for robust automatic speech recognition", "author": ["Z.Q. Wang", "D. Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 4, pp. 796\u2013806, Apr 2016.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional, long short-term memory, fully connected deep neural networks", "author": ["T.N. Sainath", "O. Vinyals", "A. Senior", "H. Sak"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 4580\u20134584.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2015}, {"title": "The NTT CHiME- 3 system: Advances in speech enhancement and recognition for mobile multi-microphone devices", "author": ["T. Yoshioka", "N. Ito", "M. Delcroix", "A. Ogawa", "K. Kinoshita", "M. Fujimoto", "C. Yu", "W.J. Fabian", "M. Espi", "T. Higuchi"], "venue": "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Scottsdale, AZ, 2015, pp. 436\u2013443.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2015}, {"title": "The MERL / SRI system for the 3rd chime challenge using beamforming, robust feature extraction, and advanced speech recognition", "author": ["T. Hori", "Z. Chen", "H. Erdogan", "J.R. Hershey", "J. Le Roux", "V. Mitra", "S. Watanabe"], "venue": "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Scottsdale, AZ, 2015, pp. 475\u2013481.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2015}, {"title": "The USTC-iFlytek system for CHiME-4 challenge", "author": ["J. Du", "Y.-H. Tu", "L. Sun", "F. Ma", "H.-K. Wang", "J. Pan", "C. Liu", "J.-D. Chen", "C.-H. Lee"], "venue": "Proc. 4th International Workshop on Speech Processing in Everyday Environments (CHiME), San Francisco, CA, 2016, pp. 36\u201338.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2016}, {"title": "The RWTH /UPB/FORTH system combination for the 4th CHiME challenge evaluation", "author": ["T. Menne", "J. Heymann", "A. Alexandridis", "K. Irie", "A. Zeyer", "M. Kitza", "P. Golik", "I. Kulikov", "L. Drude", "R. Schl\u00fcter", "H. Ney", "R. Haeb-Umbach", "A. Mouchtaris"], "venue": "Proc. 4th International Workshop on Speech Processing in Everyday Environments (CHiME), San Francisco, CA, USA, 2016, pp. 49\u201351.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2016}, {"title": "Beamforming: a versatile approach to spatial filtering", "author": ["B.D.V. Veen", "K.M. Buckley"], "venue": "IEEE ASSP Magazine, vol. 5, no. 2, pp. 4\u201324, Apr 1988.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 1988}, {"title": "Two decades of array signal processing research: the parametric approach", "author": ["H. Krim", "M. Viberg"], "venue": "IEEE Signal Processing Magazine, vol. 13, no. 4, pp. 67\u201394, July 1996.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 1996}, {"title": "Analysis of noise reduction and dereverberation techniques based on microphone arrays with postfiltering", "author": ["C. Marro", "Y. Mahieux", "K.U. Simmer"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 6, no. 3, pp. 240\u2013259, May 1998.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 1998}, {"title": "Microphone array post-filter based on noise field coherence", "author": ["I.A. McCowan", "H. Bourlard"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 11, no. 6, pp. 709\u2013716, Nov 2003.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2003}, {"title": "On diagonal loading for minimum variance beamformers", "author": ["X. Mestre", "M.A. Lagunas"], "venue": "Proc. 3rd IEEE International Symposium on Signal Processing and Information Technology, Darmstadt, Germany, 2003, pp. 459\u2013462.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust adaptive beamforming", "author": ["H. Cox", "R.M. Zeskind", "M.M. Owen"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 35, no. 10, pp. 1365\u20131376, Oct 1987.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 1987}, {"title": "A microphone array with adaptive post-filtering for noise reduction in reverberant rooms", "author": ["R. Zelinski"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), New York, NY, USA, 1988, pp. 2578\u20132581.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1988}, {"title": "A generalized estimation approach for linear and nonlinear microphone array post-filters", "author": ["S. Lefkimmiatis", "P. Maragos"], "venue": "Speech Communication, vol. 49, no. 7, pp. 657\u2013666, Aug 2007.", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust adaptive beamforming based on steering vector estimation with as little as possible prior information", "author": ["A. Khabbazibasmenj", "S.A. Vorobyov", "A. Hassanien"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2974\u20132987, Jun 2012.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 2012}, {"title": "Blind acoustic beamforming based on generalized eigenvalue decomposition", "author": ["E.Warsitz", "R. Haeb-Umbach"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 5, pp. 1529\u20131539, Jul  2007.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 2007}, {"title": "BLSTM supported GEV beamformer front-end for the 3rd CHiME challenge", "author": ["J. Heymann", "L. Drude", "A. Chinaev", "R. Haeb-Umbach"], "venue": "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Scottsdale, AZ, 2015, pp. 444\u2013451.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network based spectral mask estimation for acoustic beamforming", "author": ["J. Heymann", "L. Drude", "R. Haeb-Umbach"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 196\u2013200.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2016}, {"title": "Wide residual BLSTM network with discriminative speaker adaptation for robust speech recognition", "author": ["\u2014\u2014"], "venue": "Proc. 4th International Workshop on Speech Processing in Everyday Environments (CHiME), San Francisco, CA, 2016, pp. 12\u201317.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2016}, {"title": "Robust ASR using neural network based speech enhancement and feature simulation", "author": ["S. Sivasankaran", "A.A. Nugraha", "E. Vincent", "J.A. Morales-Cordovilla", "S. Dalmia", "I. Illina", "A. Liutkus"], "venue": "Proc. IEEEWorkshop on Automatic Speech Recognition and Understanding (ASRU), Scottsdale, AZ, 2015, pp. 482\u2013489.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2015}, {"title": "A learning-based approach to direction of arrival estimation in noisy and reverberant environments", "author": ["X. Xiao", "S. Zhao", "X. Zhong", "D.L. Jones", "E.S. Chng", "H. Li"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 2814\u20132818.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep beamforming networks for multi-channel speech recognition", "author": ["X. Xiao", "S. Watanabe", "H. Erdogan", "L. Lu", "J. Hershey", "M.L. Seltzer", "G. Chen", "Y. Zhang", "M. Mandel", "D. Yu"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5745\u20135749.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2016}, {"title": "Microphone array post-filtering using supervised machine learning for speech enhancement", "author": ["P. Pertil\u00e4", "J. Nikunen"], "venue": "Proc. INTER- SPEECH, Singapore, 2014, pp. 2675\u20132679.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2014}, {"title": "Using neural network front-ends on far field multiple microphones based speech recognition", "author": ["Y. Liu", "P. Zhang", "T. Hain"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Florence, Italy, 2014, pp. 5542\u20135546.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for distant speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 9, pp. 1120\u20131124, Sep 2014.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech acoustic modeling from rawmultichannel waveforms", "author": ["Y. Hoshen", "R.J. Weiss", "K.W. Wilson"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, 2015, pp. 4624\u20134628.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid acoustic models for distant and multichannel large vocabulary speech recognition", "author": ["P. Swietojanski", "A. Ghoshal", "S. Renals"], "venue": "Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), Olomouc, Czech Republic, 2013, pp. 285\u2013290.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2013}, {"title": "Factored spatial and spectral multichannel raw waveform CLDNNs", "author": ["T.N. Sainath", "R.J. Weiss", "K.W. Wilson", "A. Narayanan", "M. Bacchiani"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5075\u20135079.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural network adaptive beamforming for robust multichannel speech recognition", "author": ["B. Li", "T.N. Sainath", "R.J. Weiss", "K.W. Wilson", "M. Bacchiani"], "venue": "Proc. INTERSPEECH, San Francisco, CA, 2016, pp. 1976\u20131980.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2016}, {"title": "Under-determined reverberant audio source separation using a full-rank spatial covariance model", "author": ["N.Q. Duong", "E. Vincent", "R. Gribonval"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 7, pp. 1830\u20131840, Sep 2010.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 1830}, {"title": "Robust MVDR beamforming using time-frequency masks for online/offline ASR in noise", "author": ["T. Higuchi", "N. Ito", "T. Yoshioka", "T. Nakatani"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, 2016, pp. 5210\u20135214. 14", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Recently, Automatic Speech Recognition (ASR) has achieved tremendous success in both academic and industrial areas [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 1, "context": "Recently, Automatic Speech Recognition (ASR) has achieved tremendous success in both academic and industrial areas [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 2, "context": "Recently, Automatic Speech Recognition (ASR) has achieved tremendous success in both academic and industrial areas [1, 2, 3].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": ", the speech of interest), short-term stationary additive noise can be adequately tackled with standard, unsupervised noise reduction signal processing techniques developed in the 1970s and 1980s [4].", "startOffset": 196, "endOffset": 199}, {"referenceID": 4, "context": "However, detecting and reducing the impact of unknown non-stationary noise, or competing non-stationary sound sources, is still very challenging in practice, owing to the unstable characteristics of non-stationary noise [5, 6, 7, 8, 9].", "startOffset": 220, "endOffset": 235}, {"referenceID": 5, "context": "However, detecting and reducing the impact of unknown non-stationary noise, or competing non-stationary sound sources, is still very challenging in practice, owing to the unstable characteristics of non-stationary noise [5, 6, 7, 8, 9].", "startOffset": 220, "endOffset": 235}, {"referenceID": 6, "context": "However, detecting and reducing the impact of unknown non-stationary noise, or competing non-stationary sound sources, is still very challenging in practice, owing to the unstable characteristics of non-stationary noise [5, 6, 7, 8, 9].", "startOffset": 220, "endOffset": 235}, {"referenceID": 7, "context": "However, detecting and reducing the impact of unknown non-stationary noise, or competing non-stationary sound sources, is still very challenging in practice, owing to the unstable characteristics of non-stationary noise [5, 6, 7, 8, 9].", "startOffset": 220, "endOffset": 235}, {"referenceID": 8, "context": "However, detecting and reducing the impact of unknown non-stationary noise, or competing non-stationary sound sources, is still very challenging in practice, owing to the unstable characteristics of non-stationary noise [5, 6, 7, 8, 9].", "startOffset": 220, "endOffset": 235}, {"referenceID": 6, "context": "To overcome this problem, a new wave of research efforts have been made over the past few years, for example, the organisations of REVERB and a series of CHiME challenges [7, 10, 8, 6].", "startOffset": 171, "endOffset": 184}, {"referenceID": 9, "context": "To overcome this problem, a new wave of research efforts have been made over the past few years, for example, the organisations of REVERB and a series of CHiME challenges [7, 10, 8, 6].", "startOffset": 171, "endOffset": 184}, {"referenceID": 7, "context": "To overcome this problem, a new wave of research efforts have been made over the past few years, for example, the organisations of REVERB and a series of CHiME challenges [7, 10, 8, 6].", "startOffset": 171, "endOffset": 184}, {"referenceID": 5, "context": "To overcome this problem, a new wave of research efforts have been made over the past few years, for example, the organisations of REVERB and a series of CHiME challenges [7, 10, 8, 6].", "startOffset": 171, "endOffset": 184}, {"referenceID": 10, "context": ", nonstationary noise) of ASR systems [11].", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "To this end, a revolutionary technique called deep learning has particularly played a central role in the recent developments [12, 13, 14].", "startOffset": 126, "endOffset": 138}, {"referenceID": 12, "context": "To this end, a revolutionary technique called deep learning has particularly played a central role in the recent developments [12, 13, 14].", "startOffset": 126, "endOffset": 138}, {"referenceID": 13, "context": "To this end, a revolutionary technique called deep learning has particularly played a central role in the recent developments [12, 13, 14].", "startOffset": 126, "endOffset": 138}, {"referenceID": 14, "context": "Deep learning has been repeatedly verified to be powerful in making use of massive training data to build complex and dedicated analysis systems, and has achieved considerable success in a variety of fields, such as gaming [15], visual recognition [16], language translation [17], and ASR [18, 19].", "startOffset": 223, "endOffset": 227}, {"referenceID": 15, "context": "Deep learning has been repeatedly verified to be powerful in making use of massive training data to build complex and dedicated analysis systems, and has achieved considerable success in a variety of fields, such as gaming [15], visual recognition [16], language translation [17], and ASR [18, 19].", "startOffset": 248, "endOffset": 252}, {"referenceID": 16, "context": "Deep learning has been repeatedly verified to be powerful in making use of massive training data to build complex and dedicated analysis systems, and has achieved considerable success in a variety of fields, such as gaming [15], visual recognition [16], language translation [17], and ASR [18, 19].", "startOffset": 275, "endOffset": 279}, {"referenceID": 17, "context": "Deep learning has been repeatedly verified to be powerful in making use of massive training data to build complex and dedicated analysis systems, and has achieved considerable success in a variety of fields, such as gaming [15], visual recognition [16], language translation [17], and ASR [18, 19].", "startOffset": 289, "endOffset": 297}, {"referenceID": 18, "context": "Deep learning has been repeatedly verified to be powerful in making use of massive training data to build complex and dedicated analysis systems, and has achieved considerable success in a variety of fields, such as gaming [15], visual recognition [16], language translation [17], and ASR [18, 19].", "startOffset": 289, "endOffset": 297}, {"referenceID": 19, "context": ", [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning.", "startOffset": 2, "endOffset": 25}, {"referenceID": 20, "context": ", [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning.", "startOffset": 2, "endOffset": 25}, {"referenceID": 21, "context": ", [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning.", "startOffset": 2, "endOffset": 25}, {"referenceID": 22, "context": ", [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning.", "startOffset": 2, "endOffset": 25}, {"referenceID": 23, "context": ", [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning.", "startOffset": 2, "endOffset": 25}, {"referenceID": 4, "context": ", [20, 21, 22, 23, 24, 5]), all these works have no focus on the usage of deep learning.", "startOffset": 2, "endOffset": 25}, {"referenceID": 24, "context": "For more details about these networks, the reader is referred to [25] and [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "Two of the most popular architectures at the early development stage of deep learning are Deep Boltzmann Machines (DBMs) [27] and Stacked AutoEncoders (SAEs) [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "Two of the most popular architectures at the early development stage of deep learning are Deep Boltzmann Machines (DBMs) [27] and Stacked AutoEncoders (SAEs) [28].", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "The essential idea of these networks is to utilise deep architectures to model the complex and potentially non-linear functions that represent high-level abstractions, via an unsupervised pre-training in a greedy layerwise fashion [27, 29].", "startOffset": 231, "endOffset": 239}, {"referenceID": 27, "context": "The essential idea of these networks is to utilise deep architectures to model the complex and potentially non-linear functions that represent high-level abstractions, via an unsupervised pre-training in a greedy layerwise fashion [27, 29].", "startOffset": 231, "endOffset": 239}, {"referenceID": 28, "context": "For a detailed description as to why pre-training autoencoders yields better performance, the reader is referred to [30].", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "An extension of SAE is Stacked Denoising AutoEncoder (SDAE) [31, 32], where the initial inputs y are corrupted into a partially destroyed version \u1ef9 by means of stochastic mapping \u1ef9 \u223c qd (\u0303y|y).", "startOffset": 60, "endOffset": 68}, {"referenceID": 30, "context": "An extension of SAE is Stacked Denoising AutoEncoder (SDAE) [31, 32], where the initial inputs y are corrupted into a partially destroyed version \u1ef9 by means of stochastic mapping \u1ef9 \u223c qd (\u0303y|y).", "startOffset": 60, "endOffset": 68}, {"referenceID": 29, "context": "By doing this, the generative capability of the high-level representations is improved [31, 32].", "startOffset": 87, "endOffset": 95}, {"referenceID": 30, "context": "By doing this, the generative capability of the high-level representations is improved [31, 32].", "startOffset": 87, "endOffset": 95}, {"referenceID": 31, "context": "Different from DBMs of which all connections are undirected, the top two layers of DBNs form an undirected graph and the remaining layers form a belief net with directed top-down connections [33].", "startOffset": 191, "endOffset": 195}, {"referenceID": 32, "context": "The standard RNNs, however, cannot access long-range context since the backpropagated error when training either blows up or decays over time (the vanishing gradient problem) [34].", "startOffset": 175, "endOffset": 179}, {"referenceID": 32, "context": "To overcome this limitation, Hochreiter and Schmidhuber introduced Long Short-Term Memory (LSTM) RNNs [34], which are able to store the information in memory cells over a long period.", "startOffset": 102, "endOffset": 106}, {"referenceID": 33, "context": "Furthermore, similar to the DBM and the SAE, stacking RNN blocks into a deep structure has attracted increased attention, leading to a Deep RNN (DRNN) [35].", "startOffset": 151, "endOffset": 155}, {"referenceID": 34, "context": "Convolutional Neural Network (CNN) is a biologically inspired variant of Multilayer Perceptron (MLP) originally developed for visual perception tasks [36, 37, 38].", "startOffset": 150, "endOffset": 162}, {"referenceID": 35, "context": "Convolutional Neural Network (CNN) is a biologically inspired variant of Multilayer Perceptron (MLP) originally developed for visual perception tasks [36, 37, 38].", "startOffset": 150, "endOffset": 162}, {"referenceID": 36, "context": "Convolutional Neural Network (CNN) is a biologically inspired variant of Multilayer Perceptron (MLP) originally developed for visual perception tasks [36, 37, 38].", "startOffset": 150, "endOffset": 162}, {"referenceID": 37, "context": "The most general and reliable way is performing subjective listening tests [39].", "startOffset": 75, "endOffset": 79}, {"referenceID": 38, "context": "For this reason, the research community established a variety of objectivemeasurements to quantify the similarity of the enhanced and the original speech [40, 41, 42, 4].", "startOffset": 154, "endOffset": 169}, {"referenceID": 39, "context": "For this reason, the research community established a variety of objectivemeasurements to quantify the similarity of the enhanced and the original speech [40, 41, 42, 4].", "startOffset": 154, "endOffset": 169}, {"referenceID": 3, "context": "For this reason, the research community established a variety of objectivemeasurements to quantify the similarity of the enhanced and the original speech [40, 41, 42, 4].", "startOffset": 154, "endOffset": 169}, {"referenceID": 38, "context": "Specifically, segmental Signal-to-Noise Ratio (segSNR) is one of the most popular objective measures [40, 41].", "startOffset": 101, "endOffset": 109}, {"referenceID": 40, "context": ", spectral or cepstral); and Source-toDistortion Ratio (SDR) is also frequently used in evaluating the performance of source separation algorithms [43].", "startOffset": 147, "endOffset": 151}, {"referenceID": 41, "context": "Further, Perceptual Evaluation of Speech Quality (PESQ) [44, 45] is a test methodology developed for quality assessment of telephoneband speech.", "startOffset": 56, "endOffset": 64}, {"referenceID": 42, "context": "Further, Perceptual Evaluation of Speech Quality (PESQ) [44, 45] is a test methodology developed for quality assessment of telephoneband speech.", "startOffset": 56, "endOffset": 64}, {"referenceID": 43, "context": "It has also shown some potential in predicting speech intelligibility but this appears to depend on the noise conditions [46].", "startOffset": 121, "endOffset": 125}, {"referenceID": 3, "context": "methods have been developed for it [4].", "startOffset": 35, "endOffset": 38}, {"referenceID": 44, "context": "As the performance of the unsupervised noise reduction methods is strongly affected by the noise modelling approach, more sophisticated noise estimation techniques have also been developed, that track spectral minima also during speech activity [47, 48].", "startOffset": 245, "endOffset": 253}, {"referenceID": 45, "context": "As the performance of the unsupervised noise reduction methods is strongly affected by the noise modelling approach, more sophisticated noise estimation techniques have also been developed, that track spectral minima also during speech activity [47, 48].", "startOffset": 245, "endOffset": 253}, {"referenceID": 46, "context": "Classical single-channel noise reduction methods include variants of spectral subtraction [50, 51, 52], where an averaged noise spectrum (magnitude or power spectrum) is subtracted from the noisy signal spectrum, while keeping the resultant spectral magnitudes positive.", "startOffset": 90, "endOffset": 102}, {"referenceID": 47, "context": "Classical single-channel noise reduction methods include variants of spectral subtraction [50, 51, 52], where an averaged noise spectrum (magnitude or power spectrum) is subtracted from the noisy signal spectrum, while keeping the resultant spectral magnitudes positive.", "startOffset": 90, "endOffset": 102}, {"referenceID": 48, "context": "Classical single-channel noise reduction methods include variants of spectral subtraction [50, 51, 52], where an averaged noise spectrum (magnitude or power spectrum) is subtracted from the noisy signal spectrum, while keeping the resultant spectral magnitudes positive.", "startOffset": 90, "endOffset": 102}, {"referenceID": 3, "context": "While spectral subtractive methods rely on deterministic noise models, Wiener filtering [4] adopts stochastic models and is often implemented in practice using iterative approaches which base new estimates of the Wiener filter on the enhanced signal obtained by the previous iteration\u2019s Wiener filter estimate [53].", "startOffset": 88, "endOffset": 91}, {"referenceID": 49, "context": "While spectral subtractive methods rely on deterministic noise models, Wiener filtering [4] adopts stochastic models and is often implemented in practice using iterative approaches which base new estimates of the Wiener filter on the enhanced signal obtained by the previous iteration\u2019s Wiener filter estimate [53].", "startOffset": 310, "endOffset": 314}, {"referenceID": 50, "context": "Noniterative approaches based on a priori SNR estimation have also been developed for implementing Wiener filtering [54, 55, 4].", "startOffset": 116, "endOffset": 127}, {"referenceID": 51, "context": "Noniterative approaches based on a priori SNR estimation have also been developed for implementing Wiener filtering [54, 55, 4].", "startOffset": 116, "endOffset": 127}, {"referenceID": 3, "context": "Noniterative approaches based on a priori SNR estimation have also been developed for implementing Wiener filtering [54, 55, 4].", "startOffset": 116, "endOffset": 127}, {"referenceID": 52, "context": "Another popular family of techniques comprises the minimum mean square error (MMSE) [56] and log-spectral amplitude MMSE (LogMMSE) short-time spectral amplitude (STSA) estimators [57].", "startOffset": 84, "endOffset": 88}, {"referenceID": 53, "context": "Another popular family of techniques comprises the minimum mean square error (MMSE) [56] and log-spectral amplitude MMSE (LogMMSE) short-time spectral amplitude (STSA) estimators [57].", "startOffset": 179, "endOffset": 183}, {"referenceID": 54, "context": "Their performance is still considered to be among the best of the published general-purpose speech noise reduction methods [58] and in comparison to many other such methods, they exhibit lower amounts of musical noise (spectral components left as residual after enhancement) [59].", "startOffset": 123, "endOffset": 127}, {"referenceID": 55, "context": "Their performance is still considered to be among the best of the published general-purpose speech noise reduction methods [58] and in comparison to many other such methods, they exhibit lower amounts of musical noise (spectral components left as residual after enhancement) [59].", "startOffset": 275, "endOffset": 279}, {"referenceID": 55, "context": "In part, this can be attributed to the temporal smoothing of their \u201cdecision-directed\u201d estimation approach, where the spectral estimate of each frame is based partially on the estimates from previous frames via the a priori SNR estimate updated by using a memory coefficient [59, 54, 55].", "startOffset": 275, "endOffset": 287}, {"referenceID": 50, "context": "In part, this can be attributed to the temporal smoothing of their \u201cdecision-directed\u201d estimation approach, where the spectral estimate of each frame is based partially on the estimates from previous frames via the a priori SNR estimate updated by using a memory coefficient [59, 54, 55].", "startOffset": 275, "endOffset": 287}, {"referenceID": 51, "context": "In part, this can be attributed to the temporal smoothing of their \u201cdecision-directed\u201d estimation approach, where the spectral estimate of each frame is based partially on the estimates from previous frames via the a priori SNR estimate updated by using a memory coefficient [59, 54, 55].", "startOffset": 275, "endOffset": 287}, {"referenceID": 56, "context": "More recently, spectral subtraction [60] and the decision-directed MMSE [58] methods have also been applied in the spectral modulation domain in order to better handle nonstationary noise.", "startOffset": 36, "endOffset": 40}, {"referenceID": 54, "context": "More recently, spectral subtraction [60] and the decision-directed MMSE [58] methods have also been applied in the spectral modulation domain in order to better handle nonstationary noise.", "startOffset": 72, "endOffset": 76}, {"referenceID": 57, "context": "Particularly, a supervised learning-based method, namely Non-negative Matrix Factorisation (NMF), was recently frequently utilised [61].", "startOffset": 131, "endOffset": 135}, {"referenceID": 58, "context": "To enhance the target speech, the dictionaries of speech and noise are separately trained on clean speech and noise only [62, 63].", "startOffset": 121, "endOffset": 129}, {"referenceID": 59, "context": "To enhance the target speech, the dictionaries of speech and noise are separately trained on clean speech and noise only [62, 63].", "startOffset": 121, "endOffset": 129}, {"referenceID": 60, "context": "To improve the generalisation ability, an exemplar-based approach has been proposed to obtain larger dictionaries [64], which is designed to cover multiple speakers and pronunciation variants in a speech dictionary.", "startOffset": 114, "endOffset": 118}, {"referenceID": 61, "context": "A compromise solution to address this issue is semi-supervised NMF [65].", "startOffset": 67, "endOffset": 71}, {"referenceID": 62, "context": "This method has been examined in [66, 67], and has shown its efficiency.", "startOffset": 33, "endOffset": 41}, {"referenceID": 63, "context": "This method has been examined in [66, 67], and has shown its efficiency.", "startOffset": 33, "endOffset": 41}, {"referenceID": 64, "context": "[68] employed SAE to map the noisy speech to the clean speech in the Mel spectral domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 64, "context": "The empirical results indicate that SAE-based enhancement method notablely outperforms the traditional method like MMSE for enhancing the speech distorted by factory and car noises [68].", "startOffset": 181, "endOffset": 185}, {"referenceID": 65, "context": "Analogous to this, another successful work has been done in [69], where DBM was utilised to estimate the complex mapping function.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "In the pre-training stage, noisy speech was used to train RBMs layer-by-layer in a standard unsupervised greedy fashion to obtain a deep generative model [29]; whereas in the fine-tuning process, the desired clean speech was set as to the target by minimising the objective function as Eq.", "startOffset": 154, "endOffset": 158}, {"referenceID": 66, "context": "Similar research efforts were also extensively made on the T-F [70] and the logMel spectrum domains [71], respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 67, "context": "Similar research efforts were also extensively made on the T-F [70] and the logMel spectrum domains [71], respectively.", "startOffset": 100, "endOffset": 104}, {"referenceID": 68, "context": "Motivated by the fact that the same distortion in different frequency bands has different effects on speech quality, a weighted SAE was proposed in [72] which has shown positive performance for denoising.", "startOffset": 148, "endOffset": 152}, {"referenceID": 69, "context": "Further, similar work was also done in [73] and [74], where the authors utilised SDAE to enhance the Mel filterbank features corrupted by either additive or convolutional noise for ASR.", "startOffset": 39, "endOffset": 43}, {"referenceID": 70, "context": "Further, similar work was also done in [73] and [74], where the authors utilised SDAE to enhance the Mel filterbank features corrupted by either additive or convolutional noise for ASR.", "startOffset": 48, "endOffset": 52}, {"referenceID": 69, "context": "The experiments conducted on CENSREC-4 database indicate that the SDAE-based mapping method remarkably outperforms the spectral subtraction method [73] in ASR.", "startOffset": 147, "endOffset": 151}, {"referenceID": 71, "context": "2) Based on LSTM-RNN: For the mapping-based speech enhancement, context information of speech and noise is considered to be important [75].", "startOffset": 134, "endOffset": 138}, {"referenceID": 65, "context": ", SAE, DBM, and SDAE) are evaluated to be less capable in this aspect, although certain naive solutions were performed, such as expanding several sequential frames as a long vector input [69].", "startOffset": 187, "endOffset": 191}, {"referenceID": 72, "context": "RNN, espcially the ones equiped with LSTM blocks (namely LSTM-RNN), has been frequently verified to be highly capable of capturing the context information in long sequence [76, 77], as mentioned in Section 2.", "startOffset": 172, "endOffset": 180}, {"referenceID": 73, "context": "RNN, espcially the ones equiped with LSTM blocks (namely LSTM-RNN), has been frequently verified to be highly capable of capturing the context information in long sequence [76, 77], as mentioned in Section 2.", "startOffset": 172, "endOffset": 180}, {"referenceID": 10, "context": "[11] introduced RNN to purify the input features (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Evaluated by ASR systems, this enhancement model is shown to be competitive with other DNN-based mapping models at various levels of SNR [11].", "startOffset": 137, "endOffset": 141}, {"referenceID": 74, "context": "[78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82].", "startOffset": 131, "endOffset": 151}, {"referenceID": 75, "context": "[78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82].", "startOffset": 131, "endOffset": 151}, {"referenceID": 76, "context": "[78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82].", "startOffset": 131, "endOffset": 151}, {"referenceID": 77, "context": "[78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82].", "startOffset": 131, "endOffset": 151}, {"referenceID": 78, "context": "[78] proposed to use LSTM-RNN to handle highly nonstationary additive noise, which was then extended to cope with reverberation in [13, 79, 80, 81, 82].", "startOffset": 131, "endOffset": 151}, {"referenceID": 79, "context": "[83] proposed a novel feature extraction framework that concatenates a CNN and a fully connected deep neural network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 79, "context": ", Aurora-4 and noisy WSJ) [83].", "startOffset": 26, "endOffset": 30}, {"referenceID": 80, "context": "Generally, there are two typical masks available in the literature: Ideal Binary Mask (IBM) [84] and Ideal Ratio Mask (IRM) [85].", "startOffset": 92, "endOffset": 96}, {"referenceID": 81, "context": "Generally, there are two typical masks available in the literature: Ideal Binary Mask (IBM) [84] and Ideal Ratio Mask (IRM) [85].", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "where \u2016\u00b7\u2016 is the squared loss, n denotes the frame index, and F(yn) is restricted to the range [0,1].", "startOffset": 95, "endOffset": 100}, {"referenceID": 82, "context": "Specifically, Wang and Wang [86] first introduced DNNs to perform IBM estimation for speech separation, which significantly outperforms other methods without deep learning.", "startOffset": 28, "endOffset": 32}, {"referenceID": 83, "context": "[87] compared a variety of masks and indicated that the IRM is superior to the IBM in terms of objective intelligibility and quality metrics.", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "Such a conclusion was further verified by the work in [88], where the obtained results suggested that IRM achieves better ASR performance than IBM.", "startOffset": 54, "endOffset": 58}, {"referenceID": 85, "context": "[89] showed that combining IBM and IRM can deliver better performance than each of them used independently for source separation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 86, "context": "Rather than estimating the masks in the T-F spectral domain, the masking-based approaches were also successfully applied to a reduced feature space \u2013 Mel frequency domain [90, 88] and its logarithmic scale [91] that have frequently been proven to be effective for ASR in deep learning.", "startOffset": 171, "endOffset": 179}, {"referenceID": 84, "context": "Rather than estimating the masks in the T-F spectral domain, the masking-based approaches were also successfully applied to a reduced feature space \u2013 Mel frequency domain [90, 88] and its logarithmic scale [91] that have frequently been proven to be effective for ASR in deep learning.", "startOffset": 171, "endOffset": 179}, {"referenceID": 87, "context": "Rather than estimating the masks in the T-F spectral domain, the masking-based approaches were also successfully applied to a reduced feature space \u2013 Mel frequency domain [90, 88] and its logarithmic scale [91] that have frequently been proven to be effective for ASR in deep learning.", "startOffset": 206, "endOffset": 210}, {"referenceID": 86, "context": "The experimental results in [90] showed that the masking-based approaches in the Mel frequency domain perform better than the ones in the T-F spectral domain in terms of SDR.", "startOffset": 28, "endOffset": 32}, {"referenceID": 87, "context": "Further, another trend of the masking-based approaches towards replacing DNNs with LSTM-RNNs as the mask learning model [91, 92, 90], since LSTM-RNNs have shown to be capable of learning the speech and noise context information in a long temporal range [76, 77] which is vitally important for such a sequence learning task.", "startOffset": 120, "endOffset": 132}, {"referenceID": 88, "context": "Further, another trend of the masking-based approaches towards replacing DNNs with LSTM-RNNs as the mask learning model [91, 92, 90], since LSTM-RNNs have shown to be capable of learning the speech and noise context information in a long temporal range [76, 77] which is vitally important for such a sequence learning task.", "startOffset": 120, "endOffset": 132}, {"referenceID": 86, "context": "Further, another trend of the masking-based approaches towards replacing DNNs with LSTM-RNNs as the mask learning model [91, 92, 90], since LSTM-RNNs have shown to be capable of learning the speech and noise context information in a long temporal range [76, 77] which is vitally important for such a sequence learning task.", "startOffset": 120, "endOffset": 132}, {"referenceID": 72, "context": "Further, another trend of the masking-based approaches towards replacing DNNs with LSTM-RNNs as the mask learning model [91, 92, 90], since LSTM-RNNs have shown to be capable of learning the speech and noise context information in a long temporal range [76, 77] which is vitally important for such a sequence learning task.", "startOffset": 253, "endOffset": 261}, {"referenceID": 73, "context": "Further, another trend of the masking-based approaches towards replacing DNNs with LSTM-RNNs as the mask learning model [91, 92, 90], since LSTM-RNNs have shown to be capable of learning the speech and noise context information in a long temporal range [76, 77] which is vitally important for such a sequence learning task.", "startOffset": 253, "endOffset": 261}, {"referenceID": 86, "context": "The research efforts made in [90] have demonstrated the LSTM-RNNs can notably outperform the DNNs in mask estimation, as well as NMF, for source separation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 89, "context": "Apart from employing MA-based objective function to optimise the model, more and more studies have recently started to use Signal Approximation (SA) objective function [93, 94, 90].", "startOffset": 168, "endOffset": 180}, {"referenceID": 90, "context": "Apart from employing MA-based objective function to optimise the model, more and more studies have recently started to use Signal Approximation (SA) objective function [93, 94, 90].", "startOffset": 168, "endOffset": 180}, {"referenceID": 86, "context": "Apart from employing MA-based objective function to optimise the model, more and more studies have recently started to use Signal Approximation (SA) objective function [93, 94, 90].", "startOffset": 168, "endOffset": 180}, {"referenceID": 86, "context": "Empirically, employing the objective function based on SA performs better than the one based on MA for source separation [90].", "startOffset": 121, "endOffset": 125}, {"referenceID": 86, "context": "Moreover, the conclusions found in [90] and [95] indicated that combining the two objective functions (i.", "startOffset": 35, "endOffset": 39}, {"referenceID": 91, "context": "Moreover, the conclusions found in [90] and [95] indicated that combining the two objective functions (i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 92, "context": "The distorted phase information is completely ignored, even though it was proved to be helpful for speech enhancement [96].", "startOffset": 118, "endOffset": 122}, {"referenceID": 93, "context": "[97] and Weninger et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 88, "context": "[92] have taken the phase information into account, contributing to better performance in terms of SDR.", "startOffset": 0, "endOffset": 4}, {"referenceID": 94, "context": "[98] proposed complex ratio masking for DNN-based monaural speech separation, which learns the real and imaginary components of complex spectrograms jointly in the Cartesian coordinate system instead of learning magnitude spectrograms only in the traditional polar coordinate system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 89, "context": "Additionally, a multi-task learning framework has proposed in [93, 94] to jointly learn multiple sources (i.", "startOffset": 62, "endOffset": 70}, {"referenceID": 90, "context": "Additionally, a multi-task learning framework has proposed in [93, 94] to jointly learn multiple sources (i.", "startOffset": 62, "endOffset": 70}, {"referenceID": 89, "context": "The experimental results have shown that such a joint training framework is superior to the isolated training way [93].", "startOffset": 114, "endOffset": 118}, {"referenceID": 87, "context": "Although the masking-based approaches were initially designed for removing additive noise, recent research has showed that they are capable of eliminating convolutional noise as well [91, 97, 92].", "startOffset": 183, "endOffset": 195}, {"referenceID": 93, "context": "Although the masking-based approaches were initially designed for removing additive noise, recent research has showed that they are capable of eliminating convolutional noise as well [91, 97, 92].", "startOffset": 183, "endOffset": 195}, {"referenceID": 88, "context": "Although the masking-based approaches were initially designed for removing additive noise, recent research has showed that they are capable of eliminating convolutional noise as well [91, 97, 92].", "startOffset": 183, "endOffset": 195}, {"referenceID": 95, "context": "The most widely used approach in robust ASR often involves with multi-condition training [99].", "startOffset": 89, "endOffset": 93}, {"referenceID": 96, "context": "For example, the authors of the work [100] added extra layer with linear activation to the network of input, hidden layers, or output, for model adaptation, which contributes to a considerable system robustness in noisy conditions.", "startOffset": 37, "endOffset": 42}, {"referenceID": 95, "context": "based AM be informed about the noise information when training, which is often termed as Noise-Aware Training (NAT) [99].", "startOffset": 116, "endOffset": 120}, {"referenceID": 95, "context": "In this way, the DNN is being given additional cues in order to automatically learn the relationship between noisy speech and noise in a way that is beneficial to predict phonetic target [99].", "startOffset": 187, "endOffset": 191}, {"referenceID": 97, "context": "A similar work was also done in [101], where the noise estimation was replaced by the room information as an augmented input for dereverberation.", "startOffset": 32, "endOffset": 37}, {"referenceID": 98, "context": "In [102], Karanasou et al.", "startOffset": 3, "endOffset": 8}, {"referenceID": 99, "context": "[103] used the extracted i-vector to represent the noisy environment; however, the ivectors are calculated from the Vector Taylor Series (VTS) enhanced features and bottleneck features rather than MFCCs.", "startOffset": 0, "endOffset": 5}, {"referenceID": 11, "context": "Recently, a quite successful work was reported by [12], where a double-stream HMM architecture was used for fusing two AMs.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "where the variable \u03bb \u2208 [0, 1] denotes the stream weight.", "startOffset": 23, "endOffset": 29}, {"referenceID": 100, "context": "Following research was introduced in [104] and found that the GMM-based AM is even unnecessary when the LSTM-RNN-based AM was trained to predict HMM states (i.", "startOffset": 37, "endOffset": 42}, {"referenceID": 97, "context": "For example, the work of [101] and [105] respectively introduced similar multi-task learning architectures but different network types (i.", "startOffset": 25, "endOffset": 30}, {"referenceID": 101, "context": "For example, the work of [101] and [105] respectively introduced similar multi-task learning architectures but different network types (i.", "startOffset": 35, "endOffset": 40}, {"referenceID": 77, "context": "The straightforward way to do this is employing the enhanced features in the front-end for re-training the AM in the back-end [81].", "startOffset": 126, "endOffset": 130}, {"referenceID": 102, "context": "[106] proposed a cascaded DNN structure, which concatenated two independent fine-tuned DNNs.", "startOffset": 0, "endOffset": 5}, {"referenceID": 102, "context": "The second DNN attempts to learn the mapping between the reconstructed features and the phonetic targets [106].", "startOffset": 105, "endOffset": 110}, {"referenceID": 103, "context": "[107], who concatenated a DNN-based speech separation front-end and a DNN-based AM back-end to build a larger neural network, and jointly adjust the weights in each model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 103, "context": "In doing this, the separation front-end is able to provide enhanced speech desired by the AM back-end, and the AM back-end can guide the separation front-end to produce more discriminative enhancement [107].", "startOffset": 201, "endOffset": 206}, {"referenceID": 1, "context": "To utmost explore the potential of deep neural networks at different stages in the speech recognition chain, end-to-end systems have attracted increasing interests in recent years and have achieved tremendous achievement in ASR [2, 108].", "startOffset": 228, "endOffset": 236}, {"referenceID": 104, "context": "To utmost explore the potential of deep neural networks at different stages in the speech recognition chain, end-to-end systems have attracted increasing interests in recent years and have achieved tremendous achievement in ASR [2, 108].", "startOffset": 228, "endOffset": 236}, {"referenceID": 13, "context": "A quite recent and well-developed framework was reported in [14], where two tasks were evaluated: the Aurora-4 task with multiple additive noise types and channel mismatch, and the AMI meeting transcription task with significant reverberation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "Compared with the feedforward DNN, the CNNs have these advantages [14]: 1) It is well suited to model the local correlations in both time and frequency in", "startOffset": 66, "endOffset": 70}, {"referenceID": 64, "context": "13 [68] front mapping-based DSAE add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 65, "context": "14 [69] front mapping-based DBM add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 66, "context": "15 [70] front mapping-based DBM add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 67, "context": "15 [71] front mapping-based DBM add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 69, "context": "13 [73] front mapping-based DBM con.", "startOffset": 3, "endOffset": 7}, {"referenceID": 70, "context": "14 [74] front mapping-based SDAE add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "12 [11] front mapping-based DRNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 74, "context": "13 [78] front mapping-based LSTM-RNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "14 [13] front mapping-based LSTM-RNN con.", "startOffset": 3, "endOffset": 7}, {"referenceID": 75, "context": "14 [79] front mapping-based LSTM-DRNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 76, "context": "14 [80] front mapping-based LSTM-DRNN con.", "startOffset": 3, "endOffset": 7}, {"referenceID": 77, "context": "13 [81] front mapping-based LSTM-RNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 79, "context": "2nd ChiME Chang & Morgen 14 [83] front mapping-based CNN add.", "startOffset": 28, "endOffset": 32}, {"referenceID": 82, "context": "Wang & Wang 13 [86] front masking-based DNN-SVM add.", "startOffset": 15, "endOffset": 19}, {"referenceID": 84, "context": "TIMIT Narayanan & Wang 13 [88] front masking-based DNN add.", "startOffset": 26, "endOffset": 30}, {"referenceID": 83, "context": "14 [87] front masking-based DNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 86, "context": "14 [90] front masking-based LSTM-DRNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 93, "context": "15 [97] front masking-based DRNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 88, "context": "15 [92] front masking-based LSTM-RNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 90, "context": "15 [94] front masking-based DRNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 85, "context": "16 [89] front masking-based DNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 95, "context": "13 [99] back multi-condition, NAT DNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 96, "context": "15 [100] back model adaptation DNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 97, "context": "15 [101] back multi-task, NAT DNN con.", "startOffset": 3, "endOffset": 8}, {"referenceID": 98, "context": "14 [102] back factorised i-vector DNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 11, "context": "14 [12] back multi-stream LSTM-RNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 100, "context": "14 [104] back hybrid LSTM-RNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 101, "context": "15 [105] back multi-task LSTM-RNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 102, "context": "16 [106] jointed cascaded, NAT DNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 103, "context": "Aurora-5 Wang & Wang 16 [107] jointed cascaded DNN add.", "startOffset": 24, "endOffset": 29}, {"referenceID": 1, "context": "16 [2] jointed end-to-end CNN+DNN add.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "16 [14] jointed end-to-end Very Deep CNN add.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "the recent CHiME and REVERB challenges [8, 6] and their successful contributions which introduced improvements on the multi-channel techniques over the baseline, e.", "startOffset": 39, "endOffset": 45}, {"referenceID": 5, "context": "the recent CHiME and REVERB challenges [8, 6] and their successful contributions which introduced improvements on the multi-channel techniques over the baseline, e.", "startOffset": 39, "endOffset": 45}, {"referenceID": 105, "context": ", [109, 110, 111, 112]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 106, "context": ", [109, 110, 111, 112]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 107, "context": ", [109, 110, 111, 112]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 108, "context": ", [109, 110, 111, 112]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 109, "context": "Beamforming is a long-standing research topic in array signal processing with many applications, not limited to spatial audio processing [113, 114].", "startOffset": 137, "endOffset": 147}, {"referenceID": 110, "context": "Beamforming is a long-standing research topic in array signal processing with many applications, not limited to spatial audio processing [113, 114].", "startOffset": 137, "endOffset": 147}, {"referenceID": 111, "context": "Beamformer output is often further enhanced by a microphone array post-filter [115, 116].", "startOffset": 78, "endOffset": 88}, {"referenceID": 112, "context": "Beamformer output is often further enhanced by a microphone array post-filter [115, 116].", "startOffset": 78, "endOffset": 88}, {"referenceID": 113, "context": "It is generally unknown but can be estimated as a sample covariance matrix of a suitable segment of N observations as RVV = (1/N) \u2211 n Y(n)Y (n) [117].", "startOffset": 144, "endOffset": 149}, {"referenceID": 114, "context": "By then minimising wRVVw with respect to w subject to the constraint wd = 1, as mentioned above, the MVDR weight vector for the discrete frequency bin in question is given by [118]", "startOffset": 175, "endOffset": 180}, {"referenceID": 111, "context": "In multi-channel audio enhancement, the enhancement and noise reduction provided by the beamformer alone is typically not sufficient, as reducing noise and reverberation at low frequencies would require very large arrays [115].", "startOffset": 221, "endOffset": 226}, {"referenceID": 115, "context": "Therefore, the output of a beamformer is often further enhanced by post-filtering relying on a single-channel Wiener filtering approach with statistics estimated from the multi-channel observations [119, 116, 120].", "startOffset": 198, "endOffset": 213}, {"referenceID": 112, "context": "Therefore, the output of a beamformer is often further enhanced by post-filtering relying on a single-channel Wiener filtering approach with statistics estimated from the multi-channel observations [119, 116, 120].", "startOffset": 198, "endOffset": 213}, {"referenceID": 116, "context": "Therefore, the output of a beamformer is often further enhanced by post-filtering relying on a single-channel Wiener filtering approach with statistics estimated from the multi-channel observations [119, 116, 120].", "startOffset": 198, "endOffset": 213}, {"referenceID": 111, "context": "An optimal multi-channel Wiener filter formulation according to the MMSE criterion gives rise to the MVDR beamformer followed by a single-channel Wiener post-filter [115, 116, 120].", "startOffset": 165, "endOffset": 180}, {"referenceID": 112, "context": "An optimal multi-channel Wiener filter formulation according to the MMSE criterion gives rise to the MVDR beamformer followed by a single-channel Wiener post-filter [115, 116, 120].", "startOffset": 165, "endOffset": 180}, {"referenceID": 116, "context": "An optimal multi-channel Wiener filter formulation according to the MMSE criterion gives rise to the MVDR beamformer followed by a single-channel Wiener post-filter [115, 116, 120].", "startOffset": 165, "endOffset": 180}, {"referenceID": 115, "context": "The Zelinski post-filter [119, 115] shows reasonable performance but is based on simplified assumptions including a perfectly incoherent noise field with zero correlation between the noise on different channels, an assumption that does not hold with low noise frequencies and closely spaced microphones.", "startOffset": 25, "endOffset": 35}, {"referenceID": 111, "context": "The Zelinski post-filter [119, 115] shows reasonable performance but is based on simplified assumptions including a perfectly incoherent noise field with zero correlation between the noise on different channels, an assumption that does not hold with low noise frequencies and closely spaced microphones.", "startOffset": 25, "endOffset": 35}, {"referenceID": 112, "context": "In order to address this issue, McCowan and Bourlard [116] proposed a generalisation of the Zelinski post-filter that assumes prior knowledge of the complex coherence of the noise field and demonstrated it with a diffuse noise model.", "startOffset": 53, "endOffset": 58}, {"referenceID": 117, "context": "The MVDR beamformer is not robust against an inaccurately estimated steering vector d [121].", "startOffset": 86, "endOffset": 91}, {"referenceID": 118, "context": "In contrast, Generalised EigenValue (GEV) beamforming requires no DOA estimate and is based on maximising the output signal-to-noise ratio [122].", "startOffset": 139, "endOffset": 144}, {"referenceID": 118, "context": "The beamformer filter coefficients for a given frequency bin are found as the principal eigenvector of a generalised eigenvalue problem as required by [122]", "startOffset": 151, "endOffset": 156}, {"referenceID": 118, "context": "In [122], this is compensated by single-channel post-filtering in order to achieve a performance comparable to the MVDR beamformer.", "startOffset": 3, "endOffset": 8}, {"referenceID": 119, "context": "[123] proposed in the third CHiME challenge (CHiME-3) to use LSTMRNNs to firstly estimate two Ideal Binary Masks (IBMs) for each microphone channel: one to indicate which T-F bins are presumably dominated by speech, and another to indicate which T-F bins are dominated by noise.", "startOffset": 0, "endOffset": 5}, {"referenceID": 84, "context": "This idea is similar to the one in [88], where a DNN was employed to predict the noise mask.", "startOffset": 35, "endOffset": 39}, {"referenceID": 119, "context": "To train the neural networks, the authors in [123] further used a multi-task learning framework with the input of noisy speech, and two corresponding IBM targets respectively from clean speech and noise in separate output layers.", "startOffset": 45, "endOffset": 50}, {"referenceID": 120, "context": "To relax this requirement to some extent, a followup work was presented in [124], where only the clean speech was employed for mask estimation.", "startOffset": 75, "endOffset": 80}, {"referenceID": 120, "context": "The experimental results shown in [124] were competitive with", "startOffset": 34, "endOffset": 39}, {"referenceID": 119, "context": "15 [123] front NN-GEV LSTM-RNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 120, "context": "16 [124] front NN-MVDR/GEV LSTM-RNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 108, "context": "16 [112] front/back superdirective, MVDR, LSTM-RNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 121, "context": "16 [125] front/back NN-GEV; WRBR AM WRBR add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 122, "context": "15 [126] front PSD esti.", "startOffset": 3, "endOffset": 8}, {"referenceID": 105, "context": "15 [109] back NIN-CNN NIN-CNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 123, "context": "15 [127] front DOA esti.", "startOffset": 3, "endOffset": 8}, {"referenceID": 124, "context": "16 [128] front beamform.", "startOffset": 3, "endOffset": 8}, {"referenceID": 125, "context": "14 [129] front post-filtering esti.", "startOffset": 3, "endOffset": 8}, {"referenceID": 126, "context": "Liu & Nikunen 14 [130] jointed channel concatenation DNN add.", "startOffset": 17, "endOffset": 22}, {"referenceID": 127, "context": "14 [131] jointed max-pooling across CNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 128, "context": "15 [132] jointed end-to-end CNN-DNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 129, "context": "13 [133] jointed concatenated features, DNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 130, "context": "16 [134] jointed factoring spatial & CLDNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 131, "context": "16 [135] jointed adaptive spatial filtering LSTM-RNN, CLDNN add.", "startOffset": 3, "endOffset": 8}, {"referenceID": 108, "context": "Furthermore, the effectiveness of the neuralnetwork-supportedGEV beamformer has been demonstrated in the recent 4th CHiME Challenge [112, 125].", "startOffset": 132, "endOffset": 142}, {"referenceID": 121, "context": "Furthermore, the effectiveness of the neuralnetwork-supportedGEV beamformer has been demonstrated in the recent 4th CHiME Challenge [112, 125].", "startOffset": 132, "endOffset": 142}, {"referenceID": 122, "context": "[126] used a DS beamformer to obtain single-channel noisy spectra, and then used a DNN to map them into speech and noise spectra, which were then employed in constructing a multi-channelWiener filter according to [136], in order to replace the basic MVDR beamformer used in the challenge.", "startOffset": 0, "endOffset": 5}, {"referenceID": 132, "context": "[126] used a DS beamformer to obtain single-channel noisy spectra, and then used a DNN to map them into speech and noise spectra, which were then employed in constructing a multi-channelWiener filter according to [136], in order to replace the basic MVDR beamformer used in the challenge.", "startOffset": 213, "endOffset": 218}, {"referenceID": 105, "context": "While deep learning approaches have shown moderate success in CHiME-3 in estimating speech and noise statistics for multi-channel Wiener filtering and beamforming, the winning contribution was still able to rely on the baseline MVDR beamformer complemented with improved DOA estimate (the steering vector) based on T-F masks estimated using complex GMMs [109, 137].", "startOffset": 354, "endOffset": 364}, {"referenceID": 133, "context": "While deep learning approaches have shown moderate success in CHiME-3 in estimating speech and noise statistics for multi-channel Wiener filtering and beamforming, the winning contribution was still able to rely on the baseline MVDR beamformer complemented with improved DOA estimate (the steering vector) based on T-F masks estimated using complex GMMs [109, 137].", "startOffset": 354, "endOffset": 364}, {"referenceID": 123, "context": "DOA estimation using DNNs was studied in [127] and a follow-up study investigated the implementation of DS beamforming in an ASR system directly using a feedforward DNN that is used to predict the beamformer\u2019s weight vector [128].", "startOffset": 41, "endOffset": 46}, {"referenceID": 124, "context": "DOA estimation using DNNs was studied in [127] and a follow-up study investigated the implementation of DS beamforming in an ASR system directly using a feedforward DNN that is used to predict the beamformer\u2019s weight vector [128].", "startOffset": 224, "endOffset": 229}, {"referenceID": 125, "context": "One such study evaluated a non-deep MLP network in predicting the post-filter parameters for a circular microphone array [129].", "startOffset": 121, "endOffset": 126}, {"referenceID": 126, "context": "Rather than using neural networks to support traditional beamformers and post-filters for speech enhancement, end-toend multichannel ASR systems have recently attracted more attention with a straightforward target of WER decrease [130, 131, 132].", "startOffset": 230, "endOffset": 245}, {"referenceID": 127, "context": "Rather than using neural networks to support traditional beamformers and post-filters for speech enhancement, end-toend multichannel ASR systems have recently attracted more attention with a straightforward target of WER decrease [130, 131, 132].", "startOffset": 230, "endOffset": 245}, {"referenceID": 128, "context": "Rather than using neural networks to support traditional beamformers and post-filters for speech enhancement, end-toend multichannel ASR systems have recently attracted more attention with a straightforward target of WER decrease [130, 131, 132].", "startOffset": 230, "endOffset": 245}, {"referenceID": 129, "context": "In [133], the individual features extracted from each microphone channel are concatenated as a long single feature vector and fed into DNN for AM.", "startOffset": 3, "endOffset": 8}, {"referenceID": 129, "context": "Whilst such a feature concatenation operation is simple, it was still found to be effective on the AMI dataset [133], and was further verified in [130].", "startOffset": 111, "endOffset": 116}, {"referenceID": 126, "context": "Whilst such a feature concatenation operation is simple, it was still found to be effective on the AMI dataset [133], and was further verified in [130].", "startOffset": 146, "endOffset": 151}, {"referenceID": 127, "context": "Recently, a more sophisticated approach was proposed in [131].", "startOffset": 56, "endOffset": 61}, {"referenceID": 127, "context": "This algorithm was found to perform better than the one by applying CNN after a DS beamformer output [131].", "startOffset": 101, "endOffset": 106}, {"referenceID": 128, "context": "Motivated by the success of this work as well as the research trend of end-to-end ASR systems, a study [132] extended the work of [131] on the raw signals and without the operation of cross-layer max pooling.", "startOffset": 103, "endOffset": 108}, {"referenceID": 127, "context": "Motivated by the success of this work as well as the research trend of end-to-end ASR systems, a study [132] extended the work of [131] on the raw signals and without the operation of cross-layer max pooling.", "startOffset": 130, "endOffset": 135}, {"referenceID": 130, "context": "A follow-up work was reported in [134], where the authors employed two convolutional layers, instead of one layer, at the front-end.", "startOffset": 33, "endOffset": 38}, {"referenceID": 130, "context": "By factoring the spatial and the spectral filters as separate layers in the network, the performance of the investigated system was notably improved in terms of WER [134].", "startOffset": 165, "endOffset": 170}, {"referenceID": 7, "context": "Thus, further efforts are still required for speech recognition to overcome the adverse effect of environmental noise [8, 6, 9].", "startOffset": 118, "endOffset": 127}, {"referenceID": 5, "context": "Thus, further efforts are still required for speech recognition to overcome the adverse effect of environmental noise [8, 6, 9].", "startOffset": 118, "endOffset": 127}, {"referenceID": 8, "context": "Thus, further efforts are still required for speech recognition to overcome the adverse effect of environmental noise [8, 6, 9].", "startOffset": 118, "endOffset": 127}], "year": 2017, "abstractText": "Eliminating the negative effect of highly non-stationary environmental noise is a long-standing research topic for speech recognition but remains an important challenge nowadays. To address this issue, traditional unsupervised signal processing methods seem to have touched the ceiling. However, data-driven based supervised approaches, particularly the ones designed with deep learning, have recently emerged as potential alternatives. In this light, we are going to comprehensively summarise the recently developed and most representative deep learning approaches to deal with the raised problem in this article, with the aim of providing guidelines for those who are going deeply into the field of environmentally robust speech recognition. To better introduce these approaches, we categorise them into singleand multi-channel techniques, each of which is specifically described at the front-end, the back-end, and the joint framework of speech recognition systems. In the meanwhile, we describe the pros and cons of these approaches as well as the relationships among them, which can probably benefit future research.", "creator": "LaTeX with hyperref package"}}}