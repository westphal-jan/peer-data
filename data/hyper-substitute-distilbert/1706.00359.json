{"id": "1706.00359", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2017", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "abstract": "topic models have been independently explored as probabilistic generative models of turbulence. objective inference advocates have sought closed - form derivations for spatial parameter models, however as the usage of these mechanisms grows, currently does practical difficulty is utilizing fast and economical algorithms over spatial parameters. this paper presents alternative neural considerations to topic modelling whereas providing parameterisable distributions claiming topics should reflect training statistical constraints in the framework of distributed variational filtering. in addition, with the help being a doubt - breaking assumption, candidates propose a mesh network that is encouraged to discover a more unbounded grammatical response bits, analogous to sophisticated non - binary regression models. experimental diagrams unlike standard mxm song lyrics, 20newsgroups for frequent interactive forums distinguish shared effectiveness among efficiency from simpler neural topic models.", "histories": [["v1", "Thu, 1 Jun 2017 15:55:42 GMT  (849kb,D)", "http://arxiv.org/abs/1706.00359v1", "ICML 2017"]], "COMMENTS": "ICML 2017", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.IR cs.LG", "authors": ["yishu miao", "edward grefenstette", "phil blunsom"], "accepted": true, "id": "1706.00359"}, "pdf": {"name": "1706.00359.pdf", "metadata": {"source": "META", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "authors": ["Yishu Miao", "Edward Grefenstette", "Phil Blunsom"], "emails": ["<yishu.miao@cs.ox.ac.uk>."], "sections": [{"heading": "1. Introduction", "text": "Probabilistic models for inducing latent topics from documents are one of the great success stories of unsupervised learning. Starting with latent semantic analysis (LSA (Landauer et al., 1998)), models for uncovering the underlying semantic structure of a document collection have been widely applied in data mining, text processing and information retrieval. Probabilistic topic models (e.g. PLSA (Hofmann, 1999), LDA (Blei et al., 2003) and HDPs (Teh et al., 2006)) provide a robust, scalable, and theoretically sound foundation for document modelling by introducing latent variables for each token to topic assignment.\nFor the traditional Dirichlet-Multinomial topic model, efficient inference is available by exploiting conjugacy with\n1University of Oxford, Oxford, United Kingdom 2DeepMind, London, United Kingdom. Correspondence to: Yishu Miao <yishu.miao@cs.ox.ac.uk>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\neither Monte Carlo or Variational techniques (Jordan et al., 1999; Attias, 2000; Beal, 2003)). However, as topic models have grown more expressive, in order to capture topic dependencies or exploit conditional information, inference methods have become increasingly complex. This is especially apparent for non-conjugate models (Carlin & Polson, 1991; Blei & Lafferty, 2007; Wang & Blei, 2013).\nDeep neural networks are excellent function approximators and have shown great potential for learning complicated non-linear distributions for unsupervised models. Neural variational inference (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) approximates the posterior of a generative model with a variational distribution parameterised by a neural network. This allows both the generative model and the variational network to be jointly trained with backpropagation. For models with continuous latent variables associated with particular distributions, such as Gaussians, there exist reparameterisations (Kingma & Welling, 2014; Rezende et al., 2014) of the distribution permitting unbiased and low-variance estimates of the gradients with respect to the parameters of the inference network. For models with discrete latent variables, MonteCarlo estimates of the gradient must be employed. Recently, algorithms such as REINFORCE have been used effectively to decrease variance and improve learning (Mnih & Gregor, 2014; Mnih et al., 2014).\nIn this work we propose and evaluate a range of topic models parameterised with neural networks and trained with variational inference. We introduce three different neural structures for constructing topic distributions: the Gaussian Softmax distribution (GSM), the Gaussian Stick Breaking distribution (GSB), and the Recurrent Stick Breaking process (RSB), all of which are conditioned on a draw from a multivariate Gaussian distribution. The Gaussian Softmax topic model constructs a finite topic distribution with a softmax function applied to the projection of the Gaussian random vector. The Gaussian Stick Breaking model also constructs a discrete distribution from the Gaussian draw, but this time employing a stick breaking construction to provide a bias towards sparse topic distributions. Finally, the Recurrent Stick Breaking process employs a recurrent neural network, again conditioned on the Gaussian draw, to progressively break the stick, yielding a neural analog of a Dirichlet Process topic model (Teh et al., 2006). ar X\niv :1\n70 6.\n00 35\n9v 1\n[ cs\n.C L\n] 1\nJ un\n2 01\n7\nOur neural topic models combine the merits of both neural networks and traditional probabilistic topic models. They can be trained efficiently by backpropagation, scaled to large data sets, and easily conditioned on any available contextual information. Further, as probabilistic graphical models, they are interpretable and explicitly represent the dependencies amongst the random variables. Previous neural document models, such as the neural variational document model (NVDM) (Miao et al., 2016), belief networks document model (Mnih & Gregor, 2014), neural auto-regressive document model (Larochelle & Lauly, 2012) and replicated softmax (Hinton & Salakhutdinov, 2009), have not explicitly modelled latent topics. Through evaluations on a range of data sets we compare our models with previously proposed neural document models and traditional probabilistic topic models, demonstrating their robustness and effectiveness."}, {"heading": "2. Parameterising Topic Distributions", "text": "In probabilistic topic models, such as LDA (Blei et al., 2003), we use the latent variables \u03b8d and zn for the topic proportion of document d, and the topic assignment for the observed word wn, respectively. In order to facilitate efficient inference, the Dirichlet distribution (or Dirichlet process (Teh et al., 2006)) is employed as the prior to generate the parameters of the multinomial distribution \u03b8d for each document. The use of a conjugate prior allows the tractable computation of the posterior distribution over the latent variables\u2019 values. While alternatives have been explored, such as log-normal topic distributions (Blei & Lafferty, 2006; 2007), extra approximation (e.g. the Laplace approximation (Wang & Blei, 2013)) is required for closed form derivations. The generative process of LDA is:\n\u03b8d \u223c Dir(\u03b10), for d \u2208 D zn \u223c Multi(\u03b8d), for n \u2208 [1, Nd] wn \u223c Multi(\u03b2zn), for n \u2208 [1, Nd]\nwhere \u03b2zn represents the topic distribution over words given topic assignment zn and Nd is the number of tokens in document d. \u03b2zn can be drawn from another Dirichlet distribution, but here we consider it a model parameter. \u03b10 is the hyper-parameter of the Dirichlet prior and Nd is the total number of words in document d. The marginal likelihood for a document in collection D is:\np(d|\u03b10, \u03b2)= \u222b \u03b8 p(\u03b8|\u03b10) \u220f n \u2211 zn p(wn|\u03b2zn)p(zn|\u03b8)d\u03b8. (1)\nIf we employ mean-field variational inference, the updates for the variational parameters q(\u03b8) and q(zn) can be directly derived in closed form.\nIn contrast, our proposed models introduce a neural network to parameterise the multinomial topic distribution.\nThe generative process is:\n\u03b8d \u223c G(\u00b50, \u03c320), for d \u2208 D zn \u223c Multi(\u03b8d), for n \u2208 [1, Nd] wn \u223c Multi(\u03b2zn), for n \u2208 [1, Nd]\nwhere G(\u00b50, \u03c320) is composed of a neural network \u03b8 = g(x) conditioned on a isotropic Gaussian x \u223c N(\u00b50, \u03c320)1. The marginal likelihood is:\np(d|\u00b50, \u03c30, \u03b2) = \u222b \u03b8\np(\u03b8|\u00b50, \u03c320) (2)\u220f n \u2211 zn p(wn|\u03b2zn)p(zn|\u03b8)d\u03b8.\nCompared to Equation (1), here we parameterise the latent variable \u03b8 by a neural network conditioned on a draw from a Gaussian distribution. To carry out neural variational inference (Miao et al., 2016), we construct an inference network q(\u03b8|\u00b5(d), \u03c3(d)) to approximate the posterior p(\u03b8|d), where \u00b5(d) and \u03c3(d) are functions of d that are implemented by multilayer perceptrons (MLP). By using a Gaussian prior distribution, we are able to employ the re-parameterisation trick (Kingma & Welling, 2014) to build an unbiased and low-variance gradient estimator for the variational distribution. Without conjugacy, the updates of the parameters can still be derived directly and easily from the variational lower bound. We defer discussion of the inference process until the next section. Here we introduce several alternative neural networks for g(x) which transform a Gaussian sample x into the topic proportions \u03b8."}, {"heading": "2.1. The Gaussian Softmax Construction", "text": "In deep learning, an energy-based function is generally used to construct probability distributions (LeCun et al., 2006). Here we pass a Gaussian random vector through a softmax function to parameterise the multinomial document topic distributions. Thus \u03b8 \u223c GGSM(\u00b50, \u03c320) is defined as:\nx \u223c N (\u00b50, \u03c320) \u03b8 = softmax(WT1 x)\nwhere W1 is a linear transformation, and we leave out the bias terms for brevity. \u00b50 and \u03c320 are hyper-parameters which we set for a zero mean and unit variance Gaussian."}, {"heading": "2.2. The Gaussian Stick Breaking Construction", "text": "In Bayesian non-parametrics, the stick breaking process (Sethuraman, 1994) is used as a constructive definition of the Dirichlet process, where sequentially drawn Beta\n1Throughout this presentation we employ isotropic Gaussian distributions. As such we use N(\u00b5, \u03c32) to represent the Gaussian distributions, where \u03c32 is the diagonal of the covariance matrix.\nrandom variables define breaks from a unit stick. In our case, following Khan et al. (2012), we transform the modelling of multinomial probability parameters into the modelling of the logits of binomial probability parameters using Gaussian latent variables. More specifically, conditioned on a Gaussian sample x \u2208 RH , the breaking proportions \u03b7 \u2208 RK\u22121 are generated by applying the sigmoid function \u03b7 = sigmoid(WT2 x) where W \u2208 RH\u00d7K\u22121. Starting with the first piece of the stick, the probability of the first category is modelled as a break of proportion \u03b71, while the length of the remainder of the stick is left for the next break. Thus each dimension can be deterministically computed by \u03b8k = \u03b7k \u220fk\u22121 i=1 (1 \u2212 \u03b7i) until k=K\u22121, and the remaining length is taken as the probability of the Kth category \u03b8K = \u220fK\u22121 i=1 (1\u2212 \u03b7i).\nFor instance assume K = 3, \u03b8 is generated by 2 breaks where \u03b81 = \u03b71, \u03b82 = \u03b72(1 \u2212 \u03b71) and the remaining stick \u03b83 = (1 \u2212 \u03b72)(1 \u2212 \u03b71). If the model proceeds to break the stick for K = 4, the remaining stick \u03b83 is broken into (\u03b8\u20323, \u03b8 \u2032 4), where \u03b8 \u2032 3 = \u03b73 \u00b7 \u03b83, \u03b8\u20324 = (1 \u2212 \u03b73) \u00b7 \u03b83 and \u03b83 = \u03b8 \u2032 3 + \u03b8 \u2032 4. Hence, for different values of K, it al-\nways satisfies \u2211K k=1 \u03b8k = 1. The stick breaking construction fSB(\u03b7) is illustrated in Figure 1 and the distribution \u03b8 \u223c GGSB(\u00b50, \u03c320) is defined as:\nx \u223c N (\u00b50, \u03c320) \u03b7 = sigmoid(WT2 x) \u03b8 = fSB(\u03b7)\nAlthough the Gaussian stick breaking construction breaks exchangeability, compared to the stick breaking definition of the Dirichlet process, it does provide a more amenable form for neural variational inference. More interestingly, this stick breaking construction introduces a nonparametric aspect to neural topic models."}, {"heading": "2.3. The Recurrent Stick Breaking Construction", "text": "Recurrent Neural Networks (RNN) are commonly used for modelling sequences of inputs in deep learning. Here we consider the stick breaking construction as a sequential\ndraw from an RNN, thus capturing an unbounded number of breaks with a finite number of parameters. Conditioned on a Gaussian latent variable x, the recurrent neural network fSB(x) produces a sequence of binomial logits which are used to break the stick sequentially. The fRNN(x) is decomposed as:\nhk = RNNSB(hk\u22121) \u03b7k = sigmoid(hTk\u22121x)\nwhere hk is the output of the kth state, which we feed into the next state of the RNNSB as an input. Figure 2 shows the recurrent neural network structure. Now \u03b8 \u223c GRSB(\u00b50, \u03c320) is defined as:\nx \u223c N (\u00b50, \u03c320) \u03b7 = fRNN(x)\n\u03b8 = fSB(\u03b7)\nwhere fSB(\u03b7) is equivalent to the stick breaking function used in the Gaussian stick breaking construction. Here, the RNN is able to dynamically produce new logits to break the stick ad infinitum. The expressive power of the RNN to model sequences of unbounded length is still bounded by the parametric model\u2019s capacity, but for topic modelling it is adequate to model the countably infinite topics amongst the documents in a truncation-free fashion."}, {"heading": "3. Models", "text": "Given the above described constructions for the topic distributions, in this section we introduce our family of neural topic models and corresponding inference methods."}, {"heading": "3.1. Neural Topic Models", "text": "Assume we have finite number of topics K, the topic distribution over words given a topic assignment zn is p(wn|\u03b2, zn) = Multi(\u03b2zn). Here we introduce topic vectors t \u2208 RK\u00d7H , word vectors v \u2208 RV\u00d7H and generate the topic distributions over words by:\n\u03b2k = softmax(v \u00b7 tTk ).\nTherefore, \u03b2\u2208RK\u00d7V is a collection of simplexes achieved by computing the semantic similarity between topics and words. Following the notation introduced in Section 2, the prior distribution is defined as G(\u03b8|\u00b50, \u03c320) in which x \u223c N (x|\u00b50, \u03c320) and the projection network generates \u03b8 = g(x) for each document. Here, g(x) can be the Gaussian Softmax gGSM(x), Gaussian Stick Breaking gGSB(x), or Recurrent Stick Breaking gRSB(x) constructions with fixed length RNNSB. We derive a variational lower bound for the document log-likelihood according to Equation (2):\nLd = Eq(\u03b8|d) [\u2211N n=1 log \u2211 zn [p(wn|\u03b2zn)p(zn|\u03b8)] ] \u2212DKL [ q(\u03b8|d)||p(\u03b8|\u00b50, \u03c320) ] (3)\nwhere q(\u03b8|d) is the variational distribution approximating the true posterior p(\u03b8|d). Following the framework of neural variational inference (Miao et al., 2016; Kingma & Welling, 2014; Rezende et al., 2014), we introduce an inference network conditioned on the observed document d to generate the variational parameters \u00b5(d) and \u03c3(d) so that we can estimate the lower bound by sampling \u03b8 from q(\u03b8|d) = G(\u03b8|\u00b5(d), \u03c32(d)). In practise we reparameterise \u03b8\u0302 = \u00b5(d) + \u0302 \u00b7 \u03c3(d) with the sample \u0302 \u2208 N (0, I).\nSince the generative distribution p(\u03b8|\u00b50, \u03c320) = p(g(x)|\u00b50, \u03c320) = p(x|\u00b50, \u03c320) and the variational distribution q(\u03b8|d) = q(g(x)|d) = q(x|\u00b5(d), \u03c32(d)), the KL term in Equation (3) can be easily integrated as a Gaussian KL-divergence. Note that, the parameterisation network g(x) and its parameters are shared across all the documents. In addition, given a sampled \u03b8\u0302, the latent variable zn can be integrated out as:\nlog p(wn|\u03b2, \u03b8\u0302) = log \u2211\nzn\n[ p(wn|\u03b2zn)p(zn|\u03b8\u0302) ] = log(\u03b8\u0302 \u00b7 \u03b2) (4)\nThus there is no need to introduce another variational ap-\nproximation for the topic assignment z. The variational lower bound is therefore:\nLd \u2248 L\u0302d = \u2211N\nn=1\n[ log p(wn|\u03b2, \u03b8\u0302) ] \u2212DKL [q(x|d)||p(x)]\nWe can directly derive the gradients of the generative parameters \u0398, including t, v and g(x). While for the variational parameters \u03a6, including \u00b5(d) and \u03c3(d), we use the gradient estimators:\n\u2207\u00b5(d)L\u0302d \u2248 \u2207\u03b8\u0302L\u0302d, \u2207\u03c3(d)L\u0302d \u2248 \u0302 \u00b7 \u2207\u03b8\u0302L\u0302d.\n\u0398 and \u03a6 are jointly updated by stochastic gradient backpropagation. The structure of this variational auto-encoder is illustrated in Figure 3."}, {"heading": "3.2. Recurrent Neural Topic Models", "text": "For the GSM and GSB models the topic vectors t \u2208 RK\u00d7H have to be predefined for computing the topic distribution over words \u03b2. With the RSB construction we can model an unbounded number of topics, however in addition to the RNNSB that generates the topic proportions \u03b8 \u2208 R\u221e for each document, we must introduce another neural network RNNTopic to produce the topics t \u2208 R\u221e\u00d7H dynamically, so as to avoid the need to truncate the variational inference.\nFor comparison, in finite neural topic models we have topic vectors t \u2208 RK\u00d7H , while in unbounded neural topic models the topics t \u2208 R\u221e\u00d7H are dynamically generated by RNNTopic and the order of the topics corresponds to the order of the states in RNNSB. The generation of \u03b2 follows:\ntk = RNNTopic(tk\u22121), \u03b2k = softmax(v \u00b7 tTk ),\nwhere v \u2208 RV\u00d7H represents the word vectors, tk is the kth topic generated by RNNTopic and k < \u221e. Figure 4 illustrates the neural structure of RNNTopic.\nFor the unbounded topic models we introduce a truncationfree neural variational inference method which enables the\nmodel to dynamically decide the number of active topics. Assume the current active number of topics is i, RNNTopic generates ti \u2208 Ri\u00d7H by an i\u22121 step stick-breaking process (the logit for the nth topic is the remaining stick after i\u2212 1 breaks). The variational lower bound for a document d is:\nLid \u2248 \u2211N\nn=1\n[ log p(wn|\u03b2i, \u03b8\u0302i) ] \u2212DKL [q(x|d)||p(x)] ,\nwhere \u03b8\u0302i corresponds to the topic distribution over words \u03b2t. In order to dynamically increase the number of topics, the model proposes the ith break on the stick to split the (i+ 1)th topic. In this case, RNNTopic proceeds to the next state and generates topic t(i+1) for \u03b2(i+1) and the RNNSB generates \u03b8\u0302(i+1) by an extra break of the stick. Firstly, we compute the likelihood increase brought by topic i across the documents D:\nI = \u2211D\nd\n[ Lid \u2212 Li\u22121d ] / \u2211D\nd [Lid]\nThen, we employ an acceptance hyper-parameter \u03b3 to decide whether to generate a new topic. If I > \u03b3, the previous proposed new topic (the ith topic) contributes to the generation of words and we increase the active number of topics i by 1, otherwise we keep the current i unchanged. Thus \u03b3 controls the rate at which the model generates new topics. In practise, the increase of the lower bound is computed over mini-batches so that the model is able to generate new topics before the current epoch is finished. The details of the algorithm are described in Algorithm 1."}, {"heading": "3.3. Topic vs. Document Models", "text": "In most topic models documents are modelled by a mixture of topics, and each word is associated with a single topic latent variable. Miao et al. (2016) proposed a neural variational document model (NVDM) implemented as a variational auto-encoder (Kingma & Welling, 2014), which has a very similar neural structure to our models. The major difference is that NVDM employs a softmax decoder (Equation (5)) to generate all of the words of a document conditioned on an unnormalised vector:\nlog p(wn|\u03b2, \u03b8\u0302) = log softmax(\u03b8\u0302 \u00b7 \u03b2). (5)\nIn the GSM construction, if we replace the generative distribution (Equation 4) with the above distribution (Equation 5) and remove the softmax function over \u03b8, it reduces to a variant of the NVDM model (GSM applies topic and word vectors to compute \u03b2, while NVDM directly models \u03b2). Srivastava & Sutton (2016) interpret the above decoder as a weighted product of experts topic model, but do not model the topics explicitly. Here we refer to such models that do not directly assign topics to words as document models. We can also convert our constructions to document models by employing the softmax decoder. We include these models in the experimental evaluation section.\nAlgorithm 1 Unbounded Recurrent Neural Topic Model 0: Initialise \u0398 and \u03a6; Set active topic number i 1: repeat 2: for s \u2208 minibatches S do 3: for k \u2208 [1, i] do 4: Compute topic vector tk = RNNTopic(tk\u22121) 5: Compute topic distribution \u03b2k = softmax(v \u00b7 tTk ) 6: end for 7: for d \u2208 Ds do 8: Sample topic proportion \u03b8\u0302 \u223c GRSB(\u03b8|\u00b5(d), \u03c32(d)) 9: for w \u2208 document d do 10: Compute log-likelihood log p(w|\u03b8\u0302, \u03b2) 11: end for 12: Compute lowerbound Li\u22121d and L i d 13: Compute gradients\u2207\u0398,\u03a6Lid and update 14: end for 15: Compute likelihood increase I 16: if I > \u03b3 then 17: Increase active topic number i = i+ 1 18: end if 19: end for 20: until Convergence"}, {"heading": "4. Related Work", "text": "Topic models have been extensively studied for a variety of applications in document modelling and information retrieval. Beyond LDA, significant extensions have sought to capture topic correlations (Blei & Lafferty, 2007), model temporal dependencies (Blei & Lafferty, 2006) and discover an unbounded number of topics (Teh et al., 2006). Topic models have been extended to capture extra context information such as time (Wang & McCallum, 2006), authorship (Rosen-Zvi et al., 2004), and class labels (Mcauliffe & Blei, 2008). Such extensions often require carefully tailored graphical models, and associated inference algorithms, to capture the desired context. Neural models provide a more generic and extendable option and a number of works have sought to leverage these, such as the Replicated Softmax (Hinton & Salakhutdinov, 2009), the Auto-Regressive Document Model (Larochelle & Lauly, 2012), Sigmoid Belief Document Model (Mnih & Gregor, 2014), Variational Auto-Encoder Document Model (NVDM) (Miao et al., 2016) and TopicRNN Model (Dieng et al., 2016). However, these neural works do not explicitly capture topic assignments.\nThe recent work of Srivastava & Sutton (2016) also employs neural variational inference to train topic models and is closely related to our work. Their model follows the original LDA formulation in keeping the Dirichlet-Multinomial parameterisation and applies a Laplace approximation to allow gradient to back-propagate to the variational distribution. In contrast, our models directly parameterise the multinomial distribution with neural networks and jointly learn the model and variational parameters during inference. Nalisnick & Smyth (2016) proposes a reparameter-\nisation approach for continuous latent variables with Beta prior, which enables neural variational inference for Dirichlet process. However, Taylor expansion is required to approximate the KL Divergence while having multiple draws from the Kumaraswamy variational distribution. In our case, we can easily apply the Gaussian reparametersation trick with only one draw from the Gaussian distribution."}, {"heading": "5. Experiments", "text": "We perform an experimental evaluation employing three datasets: MXM2 song lyrics, 20NewsGroups3 and Reuters RCV1-v24 news. MXM is the official lyrics collection of the Million Song Dataset with 210,519 training and 27,143 testing datapoints respectively. The 20NewsGroups corpus is divided into 11,314 training and 7,531 testing documents, while the RCV1-v2 corpus is a larger collection with 794,414 training and 10,000 test cases from Reuters newswire stories. We employ the original 5,000 vocabulary provided for MXM, while the other two datasets are processed by stemming, filtering stopwords and taking the most frequent 2,0005 and 10,000 words as the vocabularies.\n2http://labrosa.ee.columbia.edu/millionsong/musixmatch (Bertin-Mahieux et al., 2011)\n3http://qwone.com/ jason/20Newsgroups 4http://trec.nist.gov/data/reuters/reuters.html 5We use the vocabulary provided by Srivastava & Sutton\n(2016) for direct comparison.\nThe hidden dimension of the MLP for constructing q(\u03b8|d) is 256 for all the neural topic models and the benchmarks that apply neural variational inference (e.g. NVDM, proLDA, NVLDA), and 0.8 dropout is applied on the output of the MLP before parameterising the isotropic Gaussian distribution. Grid search is carried out on learning rate and batch size for achieving the held-out perplexity. For the recurrent stick breaking construction we use a one layer LSTM cell (256 hidden units) for constructing the recurrent neural network. For the finite topic models we set the maximum number of topics K as 50 and 200. The models are trained by Adam (Kingma & Ba, 2015) and only one sample is used for neural variational inference. We follow the optimisation strategy of Miao et al. (2016) by alternately updating the model parameters and the inference network. To alleviate the redundant topics issue, we also apply topic diversity regularisation (Xie et al., 2015) while carrying out neural variational inference. The details can be found in the Appendix B."}, {"heading": "5.1. Evaluation", "text": "We use Perplexity as the main metric for assessing the generalisation ability of our generative models. Here we use the variational lower bound to estimate the document perplexity: exp(\u2212 1D \u2211D d 1 Nd\nlog p(d)) following Miao et al. (2016). Table 1 presents the test document perplexities of the topic models on the three datasets. Amongst the finite topic models, the Gaussian softmax construction (GSM) achieves the lowest perplexity in most cases, while all of the GSM, GSB and RSB models are significantly better than the benchmark LDA and NVLDA models. Amongst our selection of unbounded topic models, we compare our truncation-free RSB model, which applies an RNN to dynamically increase the active topics (\u03b3 is empirically set\nas 5e\u22125), with the traditional non-parametric HDP topic model (Teh et al., 2006). Here we see that the recurrent neural topic model performs significantly better than the HDP topic model on perplexity.\nNext we evaluate our neural network parameterisations as document models with the implicit topic distribution introduced in Section 3.3. Table 2 compares the proposed neural document models with the benchmarks. According to our experimental results, the generalisation abilities of the GSM, GSB and RSB models are all improved by switching to an implicit topic distribution, and their performance is also significantly better than the NVDM and ProdLDA. We hypothesise that this effect is due to the models not needing to infer the topic-word assignments, which makes optimisation much easier. Interestingly, the RSB model performs better than the GSM and GSB on 20NewsGroups in both the 50 and 200 topic settings. This is possibly due to the fact that GSM and GSB apply linear transformations W1 andW2 to generate the hidden variable \u03b8 and breaking proportions \u03b7 from a Gaussian draw, while the RSB applies recurrent neural networks to produce \u03b7 in a sequence which induces dependencies in \u03b7 and helps escape local minima. It is worth noting that the recurrent neural network uses more parameters than the other two models. As mentioned in Section 3.3, GSM is a variant of NVDM that applies topic and word vectors to construct the topic distribution over words instead of directly modelling a multinomial distribution by a softmax function, which further simplifies optimisation. If it is not necessary to model the explicit topic distribution over words, using an implicit topic distribution may lead to better generalisation.\nTo further demonstrate the effectiveness of the stickbreaking construction, Figure 5 presents the average probability of each topic by estimating the posterior probability q(z|d) of each document from 20NewsGroups. Here we set the number of topics to 400, which is large enough for this dataset. Figure 5a shows that the topics with higher probability are evenly distributed. While in Figure 5a the higher probability ones are placed in the front, and we can see a small tail on the topics after 300. Due to the sparsity inducing property of the stick-breaking construction, the topics on the tail are less likely to be sampled. This is also the advantage of stick-breaking construction when we apply the RSB-TF as a non-parameteric topic model, since the model activates the topics according to the knowledge learned from data and it becomes less sensitive to the hyperparameter controlling the initial number of topics. Figure 6 shows the impact on test perplexity for the neural topic models when the maximum number of topics is increased. We can see that the performance of the GSM model gets worse if the maximum number of topics exceeds 400, but the GSB and RSB are stable even though the number of topics far outstrips that which the model requires. In addition, the\nRSB model performs better than GSB when the number of topics is under 200, but it becomes slightly worse than GSB when the number exceeds 400, possibly due to the difficulty of learning long sequences with RNNs.\nFigure 7 shows the convergence process of the truncationfree RSB (RSB-TF) model on the 20NewsGroups. With different initial number of topics, 10, 30, and 50. The RSBTF dynamically increases the number of active topics to achieve a better variational lower bound. We can see the training perplexity keeps decreasing while the RSB-TF activates more topics. The numbers of active topics will stabilise when the convergence point is approaching (normally between 200 and 300 active topics on the 20NewsGroups). Hence, as a non-parametric model, RSB-TF is not sensitive to the initial number of active topics.\nIn addition since the quality of the discovered topics is not directly reflected by perplexity (i.e. a function of loglikelihood), we evaluate the topic observed coherence by normalised point-wise mutual information (NPMI) (Lau et al., 2014). Table 3 shows the topic observed coherence achieved by the finite neural topic models. According to these results, there does not appear to be a significant difference in topic coherence amongst the neural topic models. We observe that in both the GSB and RSB, the NPMI scores of the former topics in the stick breaking order are\nhigher than the latter ones. It is plausible as the stickbreaking construction implicitly assumes the order of the topics, the former topics obtain more sufficient gradients to update the topic distributions. Likewise we present the results obtained by the neural document models with implicit topic distributions. Though the topic probability distribution over words does not exist, we could rank the words by the positiveness of the connections between the words and each dimension of the latent variable. Interestingly the performance of these document models are significantly better than their topic model counterparts on topic coherence. The results of RSB-TF and HDP are not presented due to the fact that the number of active topics is dynamic, which makes these two models not directly comparable to the others. To further demonstrate the quality of the topics, we produce a t-SNE projection for the estimated topic proportions of each document in Figure 8.\n6The best scores we obtained are 0.222 and 0.175 for 50 and 200 topics respectively, but here we report the higher scores from Srivastava & Sutton (2016)."}, {"heading": "6. Conclusion", "text": "In this paper we have introduced a family of neural topic models using the Gaussian Softmax, Gaussian StickBreaking and Recurrent Stick-Breaking constructions for parameterising the latent multinomial topic distributions of each document. With the help of the stick-breaking construction, we are able to build neural topic models which exhibit similar sparse topic distributions as found with traditional Dirichlet-Multinomial models. By exploiting the ability of recurrent neural networks to model sequences of unbounded length, we further present a truncation-free variational inference method that allows the number of topics to dynamically increase. The evaluation results show that our neural models achieve state-of-the-art performance on a range of standard document corpora."}, {"heading": "A. Discovered Topics", "text": "Table 4 presents the topics by the words with highest probability (top-10 words) achieved by different neural topic models on 20NewsGroups dataset."}, {"heading": "B. Topic Diversity", "text": "An issue that exists in both probabilistic and neural topic models is redundant topics. In neural models it straightforward regularises the distance between each of the topic vectors in order to diversify the topics. Following Xie et al. (2015), we apply such topic diversity regularisation while carrying out neural variational inference. We use the cosine distance to measure the distance between two topics a(ti, tj) = arccos(\n|ti\u00b7\u03b2j | ||ti||\u00b7||tj || ). The mean angle of all pairs of K topics is \u03b6 = 1K2 \u2211 i \u2211 j a(ti, tj), and the variance\nis \u03bd = 1K2 \u2211 i \u2211 j(a(ti, tj) \u2212m)2. We add the following topic diversity regularisation to the variational objective:\nJ = L+ \u03bb(\u03b6 \u2212 \u03bd),\nwhere \u03bb is a hyper-parameter for the regularisation that is empirically set as 0.1. Though in practise diversity regularisation does not provide a significant improvement to perplexity (2\u223c5 in most cases), it helps reduce topic redundancy and can be easily applied on topic vectors instead of the simplex over the full vocabulary."}], "references": [{"title": "A variational bayesian framework for graphical models", "author": ["Attias", "Hagai"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Attias and Hagai.,? \\Q2000\\E", "shortCiteRegEx": "Attias and Hagai.", "year": 2000}, {"title": "Variational algorithms for approximate Bayesian inference", "author": ["Beal", "Matthew James"], "venue": "University of London,", "citeRegEx": "Beal and James.,? \\Q2003\\E", "shortCiteRegEx": "Beal and James.", "year": 2003}, {"title": "The million song dataset", "author": ["Bertin-Mahieux", "Thierry", "Ellis", "Daniel P.W", "Whitman", "Brian", "Lamere", "Paul"], "venue": "In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR),", "citeRegEx": "Bertin.Mahieux et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bertin.Mahieux et al\\.", "year": 2011}, {"title": "Dynamic topic models", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "In Proceedings of ICML,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "A correlated topic model of science", "author": ["Blei", "David M", "Lafferty", "John D"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Blei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Truly nonparametric online variational inference for hierarchical dirichlet processes", "author": ["Bryant", "Michael", "Sudderth", "Erik B"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Bryant et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bryant et al\\.", "year": 2012}, {"title": "Inference for nonconjugate bayesian models using the gibbs sampler", "author": ["Carlin", "Bradley P", "Polson", "Nicholas G"], "venue": "Canadian Journal of statistics,", "citeRegEx": "Carlin et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 1991}, {"title": "Topicrnn: A recurrent neural network with long-range semantic dependency", "author": ["Dieng", "Adji B", "Wang", "Chong", "Gao", "Jianfeng", "Paisley", "John"], "venue": "arXiv preprint arXiv:1611.01702,", "citeRegEx": "Dieng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieng et al\\.", "year": 2016}, {"title": "Replicated softmax: an undirected topic model", "author": ["Hinton", "Geoffrey E", "Salakhutdinov", "Ruslan"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Hinton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2009}, {"title": "Online learning for latent dirichlet allocation", "author": ["Hoffman", "Matthew", "Bach", "Francis R", "Blei", "David M"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["Hofmann", "Thomas"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "Hofmann and Thomas.,? \\Q1999\\E", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "An introduction to variational methods for graphical models", "author": ["Jordan", "Michael I", "Ghahramani", "Zoubin", "Jaakkola", "Tommi S", "Saul", "Lawrence K"], "venue": "Machine learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "A stick-breaking likelihood for categorical data analysis with latent gaussian models", "author": ["Khan", "Mohammad Emtiyaz", "Mohamed", "Shakir", "Marlin", "Benjamin M", "Murphy", "Kevin P"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Khan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In Proceedings of ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "An introduction to latent semantic analysis", "author": ["Landauer", "Thomas K", "Foltz", "Peter W", "Laham", "Darrell"], "venue": "Discourse processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "A neural autoregressive topic model", "author": ["Larochelle", "Hugo", "Lauly", "Stanislas"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Larochelle et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Larochelle et al\\.", "year": 2012}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["Lau", "Jey Han", "Newman", "David", "Baldwin", "Timothy"], "venue": "In Proceedings of EACL,", "citeRegEx": "Lau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lau et al\\.", "year": 2014}, {"title": "A tutorial on energy-based learning", "author": ["LeCun", "Yann", "Chopra", "Sumit", "Hadsell", "Raia"], "venue": "Predicting structured data,", "citeRegEx": "LeCun et al\\.,? \\Q2006\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 2006}, {"title": "Supervised topic models", "author": ["Mcauliffe", "Jon D", "Blei", "David M"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mcauliffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mcauliffe et al\\.", "year": 2008}, {"title": "Neural variational inference for text processing", "author": ["Miao", "Yishu", "Yu", "Lei", "Blunsom", "Phil"], "venue": "In Proceedings of ICML,", "citeRegEx": "Miao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miao et al\\.", "year": 2016}, {"title": "Neural variational inference and learning in belief networks", "author": ["Mnih", "Andriy", "Gregor", "Karol"], "venue": "In Proceedings of ICML,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["Mnih", "Volodymyr", "Heess", "Nicolas", "Graves", "Alex"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Deep generative models with stick-breaking priors", "author": ["Nalisnick", "Eric", "Smyth", "Padhraic"], "venue": "arXiv preprint arXiv:1605.06197,", "citeRegEx": "Nalisnick et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nalisnick et al\\.", "year": 2016}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Rezende", "Danilo J", "Mohamed", "Shakir", "Wierstra", "Daan"], "venue": "In Proceedings of ICML,", "citeRegEx": "Rezende et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "The author-topic model for authors and documents", "author": ["Rosen-Zvi", "Michal", "Griffiths", "Thomas", "Steyvers", "Mark", "Smyth", "Padhraic"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "A constructive definition of dirichlet priors", "author": ["Sethuraman", "Jayaram"], "venue": "Statistica sinica, pp", "citeRegEx": "Sethuraman and Jayaram.,? \\Q1994\\E", "shortCiteRegEx": "Sethuraman and Jayaram.", "year": 1994}, {"title": "Neural variational inference for topic models", "author": ["Srivastava", "Akash", "Sutton", "Charles"], "venue": "Bayesian deep learning workshop,", "citeRegEx": "Srivastava et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2016}, {"title": "Hierarchical dirichlet processes", "author": ["Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M"], "venue": "Journal of the American Statistical Asociation,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Variational inference in nonconjugate models", "author": ["Wang", "Chong", "Blei", "David M"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Online variational inference for the hierarchical dirichlet process", "author": ["Wang", "Chong", "Paisley", "John William", "Blei", "David M"], "venue": "In Proceedings of AISTATS,", "citeRegEx": "Wang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "author": ["Wang", "Xuerui", "McCallum", "Andrew"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Wang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2006}, {"title": "Diversifying restricted boltzmann machine for document modeling", "author": ["Xie", "Pengtao", "Deng", "Yuntian", "Xing", "Eric"], "venue": "In Proceedings of KDD,", "citeRegEx": "Xie et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "Starting with latent semantic analysis (LSA (Landauer et al., 1998)), models for uncovering the underlying semantic structure of a document collection have been widely applied in data mining, text processing and information retrieval.", "startOffset": 44, "endOffset": 67}, {"referenceID": 5, "context": "PLSA (Hofmann, 1999), LDA (Blei et al., 2003) and HDPs (Teh et al.", "startOffset": 26, "endOffset": 45}, {"referenceID": 29, "context": ", 2003) and HDPs (Teh et al., 2006)) provide a robust, scalable, and theoretically sound foundation for document modelling by introducing latent variables for each token to topic assignment.", "startOffset": 17, "endOffset": 35}, {"referenceID": 12, "context": "either Monte Carlo or Variational techniques (Jordan et al., 1999; Attias, 2000; Beal, 2003)).", "startOffset": 45, "endOffset": 92}, {"referenceID": 25, "context": "Neural variational inference (Kingma & Welling, 2014; Rezende et al., 2014; Mnih & Gregor, 2014) approximates the posterior of a generative model with a variational distribution parameterised by a neural network.", "startOffset": 29, "endOffset": 96}, {"referenceID": 25, "context": "For models with continuous latent variables associated with particular distributions, such as Gaussians, there exist reparameterisations (Kingma & Welling, 2014; Rezende et al., 2014) of the distribution permitting unbiased and low-variance estimates of the gradients with respect to the parameters of the inference network.", "startOffset": 137, "endOffset": 183}, {"referenceID": 22, "context": "Recently, algorithms such as REINFORCE have been used effectively to decrease variance and improve learning (Mnih & Gregor, 2014; Mnih et al., 2014).", "startOffset": 108, "endOffset": 148}, {"referenceID": 29, "context": "Finally, the Recurrent Stick Breaking process employs a recurrent neural network, again conditioned on the Gaussian draw, to progressively break the stick, yielding a neural analog of a Dirichlet Process topic model (Teh et al., 2006).", "startOffset": 216, "endOffset": 234}, {"referenceID": 21, "context": "Previous neural document models, such as the neural variational document model (NVDM) (Miao et al., 2016), belief networks document model (Mnih & Gregor, 2014), neural auto-regressive document model (Larochelle & Lauly, 2012) and replicated softmax (Hinton & Salakhutdinov, 2009), have not explicitly modelled latent topics.", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "In probabilistic topic models, such as LDA (Blei et al., 2003), we use the latent variables \u03b8d and zn for the topic proportion of document d, and the topic assignment for the observed word wn, respectively.", "startOffset": 43, "endOffset": 62}, {"referenceID": 29, "context": "In order to facilitate efficient inference, the Dirichlet distribution (or Dirichlet process (Teh et al., 2006)) is employed as the prior to generate the parameters of the multinomial distribution \u03b8d for each document.", "startOffset": 93, "endOffset": 111}, {"referenceID": 21, "context": "To carry out neural variational inference (Miao et al., 2016), we construct an inference network q(\u03b8|\u03bc(d), \u03c3(d)) to approximate the posterior p(\u03b8|d), where \u03bc(d) and \u03c3(d) are functions of d that are implemented by multilayer perceptrons (MLP).", "startOffset": 42, "endOffset": 61}, {"referenceID": 19, "context": "In deep learning, an energy-based function is generally used to construct probability distributions (LeCun et al., 2006).", "startOffset": 100, "endOffset": 120}, {"referenceID": 13, "context": "In our case, following Khan et al. (2012), we transform the modelling of multinomial probability parameters into the modelling of the logits of binomial probability parameters using Gaussian latent variables.", "startOffset": 23, "endOffset": 42}, {"referenceID": 21, "context": "Following the framework of neural variational inference (Miao et al., 2016; Kingma & Welling, 2014; Rezende et al., 2014), we introduce an inference network conditioned on the observed document d to generate the variational parameters \u03bc(d) and \u03c3(d) so that we can estimate the lower bound by sampling \u03b8 from q(\u03b8|d) = G(\u03b8|\u03bc(d), \u03c3(d)).", "startOffset": 56, "endOffset": 121}, {"referenceID": 25, "context": "Following the framework of neural variational inference (Miao et al., 2016; Kingma & Welling, 2014; Rezende et al., 2014), we introduce an inference network conditioned on the observed document d to generate the variational parameters \u03bc(d) and \u03c3(d) so that we can estimate the lower bound by sampling \u03b8 from q(\u03b8|d) = G(\u03b8|\u03bc(d), \u03c3(d)).", "startOffset": 56, "endOffset": 121}, {"referenceID": 21, "context": "Miao et al. (2016) proposed a neural variational document model (NVDM) implemented as a variational auto-encoder (Kingma & Welling, 2014), which has a very similar neural structure to our models.", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "Beyond LDA, significant extensions have sought to capture topic correlations (Blei & Lafferty, 2007), model temporal dependencies (Blei & Lafferty, 2006) and discover an unbounded number of topics (Teh et al., 2006).", "startOffset": 197, "endOffset": 215}, {"referenceID": 26, "context": "Topic models have been extended to capture extra context information such as time (Wang & McCallum, 2006), authorship (Rosen-Zvi et al., 2004), and class labels (Mcauliffe & Blei, 2008).", "startOffset": 118, "endOffset": 142}, {"referenceID": 21, "context": "Neural models provide a more generic and extendable option and a number of works have sought to leverage these, such as the Replicated Softmax (Hinton & Salakhutdinov, 2009), the Auto-Regressive Document Model (Larochelle & Lauly, 2012), Sigmoid Belief Document Model (Mnih & Gregor, 2014), Variational Auto-Encoder Document Model (NVDM) (Miao et al., 2016) and TopicRNN Model (Dieng et al.", "startOffset": 338, "endOffset": 357}, {"referenceID": 8, "context": ", 2016) and TopicRNN Model (Dieng et al., 2016).", "startOffset": 27, "endOffset": 47}, {"referenceID": 10, "context": "Finite Topic Model MXM 20News RCV1 50 200 50 200 50 200 GSM 306 272 822 830 717 602 GSB 309 296 838 826 788 634 RSB 311 297 835 822 750 628 OnlineLDA 312 342 893 1015 1062 1058 (Hoffman et al., 2010) NVLDA 330 357 1073 993 791 797 (Srivastava & Sutton, 2016)", "startOffset": 177, "endOffset": 199}, {"referenceID": 31, "context": "RSB-TF 303 825 622 HDP (Wang et al., 2011) 370 937 918", "startOffset": 23, "endOffset": 42}, {"referenceID": 10, "context": "We compare our neural topic models with the Gaussian Softmax (GSM), Gaussian Stick Breaking (GSB) and Recurrent Stick Breaking (RSB) constructions to the online variational LDA (onlineLDA) (Hoffman et al., 2010) and neural variational inference LDA (NVLDA) (Srivastava & Sutton, 2016) models.", "startOffset": 189, "endOffset": 211}, {"referenceID": 31, "context": "The lower section shows the results for the unbounded topic models, including our truncation-free RSB (RSB-TF) and the online HDP topic model (Wang et al., 2011).", "startOffset": 142, "endOffset": 161}, {"referenceID": 2, "context": "edu/millionsong/musixmatch (Bertin-Mahieux et al., 2011) http://qwone.", "startOffset": 27, "endOffset": 56}, {"referenceID": 21, "context": "Finite Document Model MXM 20News RCV1 50 200 50 200 50 200 GSM 270 267 787 829 653 521 GSB 285 275 816 815 712 544 RSB 286 283 785 792 662 534 NVDM 345 345 837 873 717 588 (Miao et al., 2016) ProdLDA 319 326 1009 989 780 788 (Srivastava & Sutton, 2016)", "startOffset": 172, "endOffset": 191}, {"referenceID": 2, "context": "edu/millionsong/musixmatch (Bertin-Mahieux et al., 2011) http://qwone.com/ jason/20Newsgroups http://trec.nist.gov/data/reuters/reuters.html We use the vocabulary provided by Srivastava & Sutton (2016) for direct comparison.", "startOffset": 28, "endOffset": 202}, {"referenceID": 21, "context": "The table compares the results for a fixed dimension latent variable, 50 or 200, achieved by our neural document models to Product of Experts LDA (prodLDA) (Srivastava & Sutton, 2016) and the Neural Variational Document Model (NVDM) (Miao et al., 2016).", "startOffset": 233, "endOffset": 252}, {"referenceID": 33, "context": "To alleviate the redundant topics issue, we also apply topic diversity regularisation (Xie et al., 2015) while carrying out neural variational inference.", "startOffset": 86, "endOffset": 104}, {"referenceID": 21, "context": "We follow the optimisation strategy of Miao et al. (2016) by alternately updating the model parameters and the inference network.", "startOffset": 39, "endOffset": 58}, {"referenceID": 21, "context": "Here we use the variational lower bound to estimate the document perplexity: exp(\u2212 1 D \u2211D d 1 Nd log p(d)) following Miao et al. (2016). Table 1 presents the test document perplexities of the topic models on the three datasets.", "startOffset": 117, "endOffset": 136}, {"referenceID": 29, "context": "as 5e\u22125), with the traditional non-parametric HDP topic model (Teh et al., 2006).", "startOffset": 62, "endOffset": 80}, {"referenceID": 18, "context": "a function of loglikelihood), we evaluate the topic observed coherence by normalised point-wise mutual information (NPMI) (Lau et al., 2014).", "startOffset": 122, "endOffset": 140}], "year": 2017, "abstractText": "Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closedform derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.", "creator": "LaTeX with hyperref package"}}}