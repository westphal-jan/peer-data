{"id": "1706.02124", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2017", "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "abstract": "reinforcement networks are a constantly promising step influencing the field of semi - supervised thinking by showing next - of - just - art use in discrete recognition tasks within being compatible without such existing neural architectures. we present the recurrent ladder structure, essentially novel fork of the original lattice, delivering a - supervised learning of robust decision clusters which we evaluate with a phoneme recognition signal namely the timit operator. our results promise that a model is able directly consistently outperform both baseline and achieve nearly - matched baseline performance with only 2 % of outcome labels predicted allow that the model is capable of using unsupervised outputs as requires improved regulariser.", "histories": [["v1", "Wed, 7 Jun 2017 10:50:47 GMT  (403kb,D)", "http://arxiv.org/abs/1706.02124v1", null], ["v2", "Mon, 18 Sep 2017 18:49:26 GMT  (403kb,D)", "http://arxiv.org/abs/1706.02124v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marian tietz", "tayfun alpay", "johannes twiefel", "stefan wermter"], "accepted": false, "id": "1706.02124"}, "pdf": {"name": "1706.02124.pdf", "metadata": {"source": "CRF", "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "authors": ["Marian Tietz", "Tayfun Alpay", "Johannes Twiefel", "Stefan Wermter"], "emails": ["tietz@informatik.uni-hamburg.de", "alpay@informatik.uni-hamburg.de", "twiefel@informatik.uni-hamburg.de", "wermter@informatik.uni-hamburg.de"], "sections": [{"heading": null, "text": "Keywords: semi-supervised learning, recurrent neural networks, ladder networks, phoneme recognition"}, {"heading": "1 Introduction", "text": "There is no doubt that the recent success of deep learning is tied to the rising availability of labelled data. While tasks such as image or text classification have greatly benefited from this availability, there are still a number of domains, e.g. speech recognition, where the majority of the research community has no free access to large amounts of labelled data. One promising approach towards this problem is semi-supervised learning where models trained with labelled data can be further improved by training with unlabelled data.\nRecent methods, such as graph-supported training [10], sparse autoencoders ([4]; SSSAE) and especially the Ladder Network (LN) [11], a stacked Denoising Autoencoder (DAE) with shortcut connections, show promising results for semisupervised training of feed-forward neural networks. The LN has been shown to deliver state-of-the-art results in semi-supervised image classification while still being compatible with many existing feed-forward neural networks [11].\nHowever, this novel architecture has not yet been explored on more complex sequential tasks, such as speech recognition, where Recurrent Neural Network (RNN) architectures, like Gated Recurrent Units (GRU; [1]), are the current state of the art. We therefore propose a novel Recurrent Ladder Network\nar X\niv :1\n70 6.\n02 12\n4v 1\n[ cs\n.C L\n] 7\nJ un\n2 01\n7\n(RLN) architecture and evaluate it on the TIMIT phoneme recognition benchmark [5]. We introduce a novel recurrent layer for the LN decoder in order to find better-suited abstractions for semi-supervised learning and test two noise injection schemes tailored to support recurrent dynamics to increase the regularising nature of the RLN. Our results show that after hyper-parameter optimization the model is able to significantly outperform the baseline in all experiments using unsupervised data as a regulariser and achieves fully-supervised baseline performance while training only on 75% of the labelled data."}, {"heading": "2 The Ladder Network Architecture", "text": "The basic idea of the LN architecture [11], depicted in Fig. 1, is to make autoencoders more expressive by adding shortcut connections from the encoder to the decoder. Each decoder layer is then able to combine the preactivation of the encoder layer with the reconstruction of the previous decoder layer by means of a combinator function g(\u00b7, \u00b7). Therefore, the encoder does not have to carry all reconstruction information since the shortcuts can compensate for it. Since the shortcuts allow perfect reconstruction by simply copying the encoder input to the decoder output, Gaussian noise N (0, \u03c32) is added to prevent the direct usage of these short-circuits and enforce learning in the intermediate layers, i.e. we use a denoising autoencoder. To ensure that the noise can be removed, the decoder\u2019s (noisy) reconstruction z\u0302(l) is compared to the encoder\u2019s (clean) preactivation z(l) and added to the unsupervised objective function:\nCDAE = n\u2211 l \u03bblC (l) d with C (l) d = \u2016 z(l) \u2212 z\u0302(l)\u20162 , (1)\nwhere n is the total amount of layers, z(l) is the preactivation vector of the l-th encoder layer without noise and z\u0302(l) the l-th decoder layer reconstruction from noisy input. The hyper-parameter \u03bbi controls the targeted similarity between the encoder and decoder layers and prevents short-circuits by punishing direct copies of the noisy data by weighting the difference between the layers. For semisupervised learning the encoder path is also used for the supervised task, i.e. its output is evaluated with a supervised objective function Csup and combined with the unsupervised objective function CDAE: Csemsup = Csup +CDAE. When using the encoder in a supervised task the shortcuts help with reconstruction as the needed information may also be retrieved over the shortcuts [11].\nThe combinator function g(\u00b7, \u00b7) models p(z(l) | z(l+1)) and is responsible for creating the reconstruction of the l-th layer z\u0302(l) with the help of the reconstruction of the previous layer z\u0302(l+1) and the shortcut value of the l-th layer z\u0303(l), i.e., z\u0302(l) = g(z\u0303(l), z\u0302(l+1)). The function may attempt to remove the noise from z\u0303(l) with the help of the previous reconstruction, infer the inverse mapping z\u0302(l+1) \u2192 z\u0302(l) or do a combination of both."}, {"heading": "3 Recurrent Ladder Networks", "text": "In this section, we will elaborate our modelling choices for the RLN. In order to extend the original LN to support recurrence in the encoder, both the noise injection scheme and the decoder have to be adapted since recurrent layers use additional context layers. Overall, we are proposing two noise injection methods and two decoder variants (see Fig. 2). Our supervised baseline model will be the encoder of the RLN since it encodes the task closely to the full RLN but has no means of using unsupervised data. The resulting six model combinations are No-Decoder with Feed-Forward Noise (ND-FFN), No-Decoder with Recurrent Noise (ND-RN), Recurrent Decoder with Feed-Forward Noise (RD-FFN) and Recurrent Noise (RD-RN) as well as a Feed-Forward Decoder with Feed-Forward Noise (FFD-FFN) and Recurrent Noise (FFD-RN)."}, {"heading": "3.1 Noise Injection", "text": "In the feed-forward case, noise is applied directly to the preactivations so that the output of the layer and the shortcut are affected, i.e. z\u0303 = W x\u0303 + n with n \u223c N (0, \u03c32). This would, however, introduce noise into the context memory of recurrent layers even after receiving the noisy output from the previous layer, effectively amplifying the noise even further. Therefore, we apply noise only to the preactivation and the shortcut without direct perturbation of the context memory. A hidden layer ht and its noisy counterpart h\u0303t are therefore updated as follows:\nht = f(zt) = f(W x\u0303t + Uht\u22121), (2)\nh\u0303t = f(z\u0303t) = f(zt + n), (3)\nwhere f(\u00b7) is the activation function, xt the input, W the input weight matrix, and U the hidden-to-hidden weights, updated at each time step t.\nThis noise injection method will be referred to as recurrent noise from here on. Another method of noise injection that we tested, referred to as feed-forward noise, is to not inject additional noise at the recurrent layer, i.e. feed-forward layers will be injected with noise but recurrent layers will not."}, {"heading": "3.2 Recurrent Decoder", "text": "The decoder path in an autoencoder models the inverse information flow of the encoder path. We propose two modelling options for the decoder path in an RLN. The first (Fig. 2c) is a recurrent layer with g(\u00b7, \u00b7) as activation function:\nu (l) t = V z\u0302 (l+1) t +Oz\u0302 (l) t\u22121, (4) z\u0302 (l) t = g(z\u0303 (l) t ,u (l) t ), (5)\nwhere V are the input weights, O the hidden-to-hidden weights, u (l) t the preactivation of the recurrent decoder and z\u0303 (l) t the noisy preactivation of the l-th encoder layer at time-step t from the shortcut. The second modelling option is to simply use a feed-forward network (Fig. 2d) in the decoder [11].\nBatch normalisation is heavily used in the LN both for normalisation of the layer-wise reconstruction cost and for normalisation of layer activations. It was considered problematic with recurrent networks until the introduction of recurrent batch normalisation [2]. Since it potentially requires tuning of another hyper-parameter we decided to model the RLN without batch normalisation with the exception of the layer-wise reconstruction cost function C (l) d which is computed exactly as described by Rasmus et al. [11]."}, {"heading": "4 Experiments", "text": "We evaluate the RLN on the TIMIT phoneme recognition benchmark [5], a widely used test corpus which allows comparing our architecture to previous approaches. The audio samples of the corpus are reduced in dimensionality by using libROSA1 to compute 13 Mel Frequency Cepstral Components (MFCC) [3] and their first and second derivative with 20ms frames and 10ms frame skip, similar to related work [4]. The 39-dimensional feature vectors are normalised to have zero mean and unit variance. We grouped easily confused phonemes of the English phoneme alphabet as described by Halberstadt [8] resulting in 39 phoneme classes to predict.\nWe use Connectionist Temporal Classification (CTC) [6] for the supervised cost Csup to solve the problem of label alignment. Phoneme Error Rate (PER) is used for evaluation and computed using the Levenshtein distance of all label sequences to the predictions, normalised to the total length of all label sequences.\n1 https://librosa.github.io\nThe predictions are obtained by using best path decoding [6], i.e. choosing the phoneme class with the highest probability at each time step.\nTo build the supervised and unsupervised training sets we keep all input data for unsupervised training and reduce the supervised set by drawing samples from the full dataset until the least represented phonemes are drawn a minimum number of times to prevent under-representing a class while keeping the distribution intact. We cycle the supervised dataset to match sizes with the unsupervised set, similar to the implementation by Rasmus et al. [11]."}, {"heading": "4.1 Training Procedure", "text": "All networks have been trained using Adam [9] with a learning rate of 0.002 for at least 100 epochs until the validation error stopped improving. The models are four-layer networks consisting of one GRU layer with 192 units with tanh(\u00b7) activation and one feed-forward output layer with softmax activation, as well as the inverse layers in the decoder. The noisy softmax output is used to classify phonemes during training for additional regularisation. Since the performance of the encoder is likely to correlate with RLN performance, hyper-parameters, including layer sizes and learning rate, were determined empirically by a grid search using the encoder described in section 3, i.e. an RLN with \u03bbi = 0, which also serves as the baseline. DAE cost weights (\u03bb0, \u03bb1, \u03bb2) = (1000, 10, 0.1) and the MLP combinator g(\u00b7, \u00b7) were both adopted from Rasmus et al. [11].\nWe test the semi-supervised learning capabilities of the six RLN variants from section 3, by varying the labels for the supervised part of the architecture in steps of 25% (940), 50% (1856), 75% (2754), and 100% (3696) of labelled\nsequences while the unsupervised part of the model always receives all available unlabelled data."}, {"heading": "5 Results & Discussion", "text": "An overview of our results can be seen in Fig. 3 where the different modelling choices are directly compared against each other. The overall best results after hyper-parameter optimization for each supervised data split, as well as results of other approaches, are shown in Table 1.\nAs can be seen in Table 1, the RLN consistently outperforms the baseline configuration, even in fully-supervised training and is able to achieve the same performance as the baseline with 25% less labelled data which shows that the RLN complements the encoder well and demonstrates the compatibility of the LN with existing models. On average, the RD models perform better than the FFD models for most \u03c3, more so with fewer labels, suggesting that the recurrent decoder is better at filtering noise. This also explains why the RD models work better with higher \u03c3 compared to FFD.\nThe noise injection method and the chosen \u03c3 greatly impact the overall performance. The performance curves are roughly concave and shift towards stronger noise with less available labels because the network overfits easily with fewer labels which is prevented by the higher noise. Performance degrades for higher \u03c3 because the network needs to be trained significantly longer to remove the noise which the chosen training parameters do not allow.\nRecurrent noise injection was expected to achieve better regularisation due to the additional noise at the recurrent layer but does not. By observing the encoder layers we found that their outputs often differed significantly which causes unrecoverable perturbations in the recurrent layers when applying equally strong noise to all layers instead of noise relative to each layer\u2019s output. Employing batch normalisation might solve this, as hypothesised in related work [12]: normalising the preactivation of each layer to unit variance before adding noise makes the change in variance relative to the preactivation, therefore coupling noise and layer activation strength with the benefit of reducing the search space for \u03c3 significantly. We predict that this will lead to an increase in performance when using fewer labels.\nEven though our best results for the RLN are slightly lower ranked when compared with related approaches, our model has significantly fewer parameters (e.g. differing by a factor of 160 when compared to SSSAE [4]). We therefore hypothesise that an increase of parameters and more complex layer architectures will result in even better performance. This is indicated by our best RLN achieving similar results (31.66% PER, 175k parameters) as the Bi-directional Long Short-Term Memory (BLSTM) ([6]; 31.25% PER, 114k parameters) while using only half of the labels."}, {"heading": "6 Conclusion", "text": "We have shown that the recurrent ladder network is able to perform as good as similarly parametrised BLSTM models while using only 50% of the labelled data, demonstrating the RLN\u2019s ability to effectively regularise itself using unsupervised training data. Current state-of-the-art methods performed better overall but this does not come as a surprise given that these models use up to 160 times more parameters. We argue that this gap could potentially be closed by scaling up our models, as demonstrated for BLSTM models by Graves et al. [7].\nThe proposed recurrent decoder proved to be better at denoising than the feed-forward decoder. Additionally, we found that recurrent noise injection does not perform as expected and we hypothesise that it needs the help of normalisation (e.g. batch normalisation) to work efficiently.\nIn the future, we would also like to take advantage of the semi-supervised learning abilities of the RLN in conjunction with more complex recurrent models such as bidirectional and attention-based RNNs to utilise unlabelled data even more effectively and explore how the learning framework scales with more complex temporal dynamics in more challenging tasks such as end-to-end speech recognition or question answering.\nAcknowledgments. The authors gratefully acknowledge partial support from the German Research Foundation DFG under project CML (TRR 169), the European Union under project SECURE (No 642667), and the Hamburg Landesforschungsfo\u0308rderungsprojekt CROSS."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Ladder networks are a notable new concept in the field of<lb>semi-supervised learning by showing state-of-the-art results in image<lb>recognition tasks while being compatible with many existing neural archi-<lb>tectures. We present the recurrent ladder network, a novel modification of<lb>the ladder network, for semi-supervised learning of recurrent neural net-<lb>works which we evaluate with a phoneme recognition task on the TIMIT<lb>corpus. Our results show that the model is able to consistently outper-<lb>form the baseline and achieve fully-supervised baseline performance with<lb>only 75% of all labels which demonstrates that the model is capable of<lb>using unsupervised data as an effective regulariser.", "creator": "LaTeX with hyperref package"}}}