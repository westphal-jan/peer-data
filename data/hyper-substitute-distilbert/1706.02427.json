{"id": "1706.02427", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Content-Based Table Retrieval for Web Queries", "abstract": "also the resemblance between unstructured text and bi - structured table identify relatively important yet key problem in natural language linguistics. towards this stage, we focus on match - finding table retrieval. given a query, the urgency is to discover every most relevant table from coherent collection of tables. further insights towards fulfilling this area requires powerful models of semantic layout and richer training and evaluation features. means prevent this, we learn a category based approach, and compare both structurally designed features and managed performance architecture whereby measure the paths between a query given the content of a proposal. afterwards, we release an open - domain dataset and includes 21, 250 index queries supporting 15, 262 tables. we conduct comprehensive audit on both real documents and synthetic data. results verify the model \u2013 comparative approach which present the structure this new task.", "histories": [["v1", "Thu, 8 Jun 2017 02:03:32 GMT  (381kb,D)", "http://arxiv.org/abs/1706.02427v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["zhao yan", "duyu tang", "nan duan", "junwei bao", "yuanhua lv", "ming zhou", "zhoujun li"], "accepted": false, "id": "1706.02427"}, "pdf": {"name": "1706.02427.pdf", "metadata": {"source": "CRF", "title": "Content-Based Table Retrieval for Web Queries", "authors": ["Zhao Yan", "Duyu Tang", "Nan Duan", "Junwei Bao", "Yuanhua Lv", "Ming Zhou", "Zhoujun Li"], "emails": ["lizj}@buaa.edu.cn", "baojunwei001@gmail.com", "mingzhou}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Table1 is a special and valuable information that could be found almost everywhere from the Internet. We target at the task of content-based table retrieval in this work. Given a query, the task is to find the most relevant table from a collection of tables. Table retrieval is of great importance for both natural language processing and information retrieval. On one hand, it could improve existing information retrieval systems. The well-organized information from table, such as product comparison from different aspects and flights between two\n\u2217 Contribution during internship at Microsoft Research. 1https://en.wikipedia.org/wiki/Table_\n(information)\nspecific cities, could be used to directly respond to web queries. On the other hand, the retrieved table could be used as the input for question answering (Pasupat and Liang, 2015).\nUnlike existing studies in database community (Cafarella et al., 2008; Balakrishnan et al., 2015) that utilize surrounding text of a table or pagerank score of a web page, we focus on making a thorough exploration of table content in this work. We believe that content-based table retrieval has the following challenges. The first challenge is how to effectively represent a table, which is semi-structured and includes many aspects such as headers, cells and caption. The second challenge is how to build a robust model that measures the relevance between an unstructured natural language query and a semi-structured table. Table retrieval could be viewed as a multi-modal task because the query and the table are of different forms. Moreover, to the best of our knowledge, there is no publicly available dataset for table retrieval. Further progress towards improving this area requires richer training and evaluation resources.\nTo address the aforementioned challenges, we develop a ranking based approach. We separate the approach into two cascaded steps to trade-off between accuracy and efficiency. In the first step, it finds a small set (e.g. 50 or 100) of candidate tables using a basic similarity measurement. In the second step, more sophisticated features are used to measure the relevance between the query and each candidate table. We implement two types of features, including manually designed features inspired by expert knowledge and neural network models jointly learned from data. Both strategies take into account the relevance between query and table at different levels of granularity. We also introduce a new dataset WebQueryTable for table retrieval. It includes 21,113 web queries from search ar X\niv :1\n70 6.\n02 42\n7v 1\n[ cs\n.C L\n] 8\nJ un\n2 01\n7\nlog, and 273,816 web tables from Wikipedia. We conduct comprehensive experiments on two datasets, a real world dataset introduced by us, and a synthetic dataset WikiTableQuestions (Pasupat and Liang, 2015) which has been widely used for table-based question answering. Results in various conditions show that neural network models perform comparably with carefully designed features, and combining them both could obtain further improvement. We study the influence of each aspect of table for table retrieval, and show what depth of table understanding is required to do well on this task. Results show the difference between question and web query, and present future challenges for this task.\nThis paper has the following contributions. We develop both feature-based and neural network based approaches, and conduct thorough experiments on real world and synthetic datasets. We release an open-domain dataset for table retrieval."}, {"heading": "2 Task Definition", "text": "We formulate the task of table retrieval in this section. Given a query q and a collection of tables T = {t1, ..., tN}, the goal of table search is to find a table ti that is most relevant to q.\nTypically, a query q is a natural language expression that consists of a list of words, such as \u201cmajor cities of netherlands\u201d. A table t is a set of data elements arranged by vertical columns and horizontal rows. Formally, we define a table as a triple t = {headers, cells, caption} that consists of three aspects. A table could have multiple headers, each of which indicates the property of a column and could be used to identify a column. A table could have multiple cells, each of which is a unit where a row and a column intersects. A table could have a caption, which is typically an explanatory text about the table. Figure 1 gives an example to illustrate different aspects of a table.\nIt is helpful to note that tables from the web are not always \u201cregular\u201d. We regard a table as a \u201cregular\u201d table if it contains header, cell and caption, and the number of cells in each row is equal to the number of header cells. In this work, we make a comprehensive study of table retrieval on regular tables, and would like to release benchmark datasets of good quality. It is trivial to implement heuristic rules so as to convert the irregular tables to regular one, so we leave it to the future work."}, {"heading": "3 Approach Overview", "text": "In this section, we give an overview of the proposed approach. To build a system with high efficiency, we separate the task into two cascaded modules, including candidate table retrieval and table ranking. Candidate table retrieval aims to find a small set of tables, such as 50 or 100. These candidate tables will be further used in the table ranking step, which uses more sophisticated features to measure the relevance between a query and a table. In the following subsections, we will give the work-flow of candidate table retrieval and table ranking. The detailed feature representation will be described in the next section."}, {"heading": "3.1 Candidate Table Retrieval", "text": "Candidate table retrieval aims to get a small candidate table set from the whole table set of large scale, which is hundreds of thousands in our experiment. In order to guarantee the efficiency of the searching process, we calculate the similarity between table and query with Okapi BM25 (Robertson et al., 1995), which is computationally efficient and has been successfully used in information retrieval. Specifically, we represent a query as bag-of-words, and represent table with plain text composed by the words from caption and headers. Given a query q = x1, x2, ..., xn, a table t and the whole table set T , the BM25 score of query q and table t is calculated as follows.\nBM25(q, t) = n\u2211 i=1 idf(xi) tf(xi, t) \u00b7 (k1 + 1) tf(xi, T ) + k1(1\u2212 b+ b |t|avgtl )\nwhere tf(xi, t) is the term frequency of word xi in t, idf(xi) is its inverse document frequency, avgtl is the average sequence length in the whole table set T , and k1 and b are hyper-parameters."}, {"heading": "3.2 Table Ranking", "text": "The goal of table ranking is to rank a short list of candidate tables by measuring the relevance between a query and a table. We develop a featurebased approach and a neural network approach, both of them effectively take into account the structure of table. The details about the features will be described in next section. We use each feature to calculate a relevance score, representing the similarity between a query and a table from some perspective. Afterwards, we use LambdaMART (Burges, 2010), a successful algorithm for solving real world ranking problem, to get the final ranking score of each table.2 The basic idea of LambdaMART is that it constructs a forest of decision trees, and its output is a linear combination of the results of decision trees. Each binary branch in a decision tree specifies a threshold to apply to a single feature, and each leaf node is real value. Specifically, for a forest of N trees, the relevance score of a query-table pair is calculated as follow,\ns(q, t) = N\u2211 i=1 witri(q, t)\nwhere wi is the weight associated with the i-th regression tree, and tri(\u00b7) is the value of a leaf node obtained by evaluating i-th tree with features [f1(q, t), ..., fK(q, t)]. The values of wi and the parameters in tri(\u00b7) are learned with gradient descent during training."}, {"heading": "4 Matching between Query and Table", "text": "Measuring the relevance between a query and a table is of great importance for table retrieval. In this section, we present carefully designed features and neural network architectures for matching between a query and a table."}, {"heading": "4.1 Matching with Designed Features", "text": "We carefully design a set of features to match query and table from word-level, phrase-level and sentence-level, respectively. The input of a feature function are two strings, one query string q and one aspect string ta. We separately apply each of the following features to each aspect of a table, resulting in a list of feature scores. As described in Section 2, a table has three aspects, including\n2We also implemented a ranker with linear regression, however, its performance was obviously worse than LambdaMART in our experiment.\nheaders, cells and caption. We represent each aspect as word sequence in this part.\n(1) Word Level. We design two word matching features fwmt and fmwq. The intuition is that a query is similar to an aspect of table if they have a large amount of word overlap. fwmt and fwmq are calculated based on number of words shared by q and ta. They are also normalized with the length of q and ta, calculated as follows,\nfwmt(ta, q) =\n\u2211 w\u2208ta \u03b4(w, q) \u00b7 idf(w)\u2211\nw\u2032\u2208ta idf(w \u2032)\nfwmq(ta, q) =\n\u2211 w\u2208ta \u03b4(w, q) \u00b7 idf(w)\u2211\nw\u2032\u2208q idf(w \u2032)\nwhere idf(w) denotes the inverse document frequency of word w in ta. \u03b4(yj , q) is an indicator function which is equal to 1 if yj occurs in q, and 0 otherwise. Larger values of fwmt(\u00b7) and fwmq(\u00b7) correspond to larger amount of word overlap between ta and q.\n(2) Phrase Level. We design a paraphrasebased feature fpp to deal with the case that a query and a table use different expressions to describe the same meaning. In order to learn a strong and domain-independent paraphrase model, we leverage existing statistical machine translation (SMT) phrase tables. A phrase table is defined as a quadruple, namely PT = {\u3008srci, trgi, p(trgi|srci), p(srci|trgi)\u3009}, where srci (or trgi) denotes a phrase, in source (or target) language, p(trgi|srci) (or p(srci|trgi)) denotes the translation probability from srgi (or trgi) to trgi (or srci). We use an existing SMT approach (Koehn et al., 2003) to extract a phrase table PT from a bilingual corpus. Afterwards, we use PT to calculate the relevance between a query and a table in paraphrase level. The intuition is that, two source phrases that are aligned to the same target phrase tend to be paraphrased. The phrase level score is calculated as follows, where N is the maximum n-gram order, which is set as 3, and srcati,n and src q j,n are the phrase in ta and q starts from the i-th and j-th word with the length of n, and i \u2208 {1, ..., |ta| \u2212 n+ 1} and j \u2208 {1, ..., |q| \u2212 n+ 1}.\nfpp(ta, q) = 1\nN N\u2211 n=1\n\u2211 i,j score(src tq i,n, src q j,n)\n|ta| \u2212N + 1 score(srcx; srcy) = \u2211 PT p(tgtk|srcx) \u00b7 p(srcy|tgtk)\n(3) Sentence Level. We design features to match a query with a table at the sentence level. We use CDSSM (Shen et al., 2014), which has been successfully applied in text retrieval. The basic computational component of CDSSM is subword, which makes it very suitable for dealing the misspelling queries in web search. The model composes sentence vector from sub-word embedding via convolutional neural network. We use the same model architecture to get query vector and table aspect vector, and calculate their relevance with cosine function.\nfs1(ta, q) = cosine(cdssm(ta), cdssm(q))\nWe train model parameters on WikiAnswers dataset (Fader et al., 2013), which contains almost 12M question-similar question pairs. In addition, since vector average is an intuitive way to compute sentence vector and does not induce additional parameters, we calculate another relevance score by representing a query and a table aspect with element-wise vector average. We use a publicly available word embedding which is released by Mikolov et al. (2013).\nfs2(ta, q) = cosine(vec avg(ta), vec avg(q))"}, {"heading": "4.2 Matching with Neural Networks", "text": "We present neural network models for matching a query with a table. As a table includes different aspects such as headers, cells and caption, we develop different strategies to measure the relevance between a query and a table from different perspectives. In this subsection, we first describe the model to compute query representation, and then present the method that measures the relevance between a query and each aspect.\nA desirable query representation should be sensitive to word order as reversing or shuffling the words in a query might result in totally different intention. For example, \u201clist of flights london to berlin\u201d and \u201clist of flights berlin to london\u201d have different intentions. We use recurrent neural network (RNN) to map a query of variable length to a fixed-length vector. To avoid the problem of gradient vanishing, we use gated recurrent unit (GRU) (Cho et al., 2014) as the basic computation unit, which adaptively forgets the history and remembers the input, and has proven to be effective in sequence modeling (Chung et al., 2014). It recursively transforming current word vector eqt with the output vector of the previous step ht\u22121.\nzi = \u03c3(Wze q i + Uzhi\u22121) ri = \u03c3(Wre q i + Urhi\u22121) h\u0303i = tanh(Whe q i + Uh(ri hi\u22121))\nhi = zi h\u0303i + (1\u2212 zi) hi\u22121 where zi and ri are update and reset gates of GRU. We use a bi-directional RNN to get the meaning of a query from both directions, and use the concatenation of two last hidden states as the final query representation vq = [ \u2212\u2192 h n, \u2190\u2212 h n].\nA table has different types of information, including headers, cells and caption. We develop different mechanisms to match the relevance between a query and each aspect of a table. An important property of a table is that randomly exchanging two rows or tow columns will not change the meaning of a table (Vinyals et al., 2015). Therefore, a matching model should ensure that exchanging rows or columns will result in the same output. We first describe the method to deal with headers. To satisfy these conditions, we represent each header as an embedding vector, and regard a set of header embeddings as external memory Mh \u2208 Rk\u00d7d, where d is the dimension of word embedding, and k is the number of header cells. Given a query vector vq, the model first assigns a probability \u03b1i to each memory cell mi, which is a header embedding in this case. Afterwards, a query-specific header vector is obtained through weighted average (Bahdanau et al., 2015; Sukhbaatar et al., 2015), namely vheader =\u2211k\ni=1 \u03b1imi, where \u03b1i \u2208 [0, 1] is the weight of mi calculated as below and \u2211 i \u03b1i = 1.\n\u03b1i = exp(tanh(W [mi; vq] + b))\u2211k j=1 exp(tanh(W [mj ; vq] + b))\nSimilar techniques have been successfully applied in table-based question answering (Yin et al., 2015b; Neelakantan et al., 2015). Afterwards, we feed the concatenation of vq and vheader to a linear layer followed by a softmax function whose output length is 2. We regard the output of the first category as the relevance between query and header. We use NN1() to denote this model.\nfnn(header, q) = NN1(Mh, vq)\nSince headers and cells have similar characteristics, we use a similar way to measure the relevance between a query and table cells. Specifically, we derive three memories Mcel, Mrow and\nMcol from table cells in order to match from cell level, row level and column level. Each memory cell in Mcel represents the embedding of a table cell. Each cell in Mrow represent the vector a row, which is computed with weighted average over the embeddings of cells in the same row. We derive the column memory Mcol in an analogous way. We use the same module NN1() to calculate the relevance scores for these three memories.\nfnn(cell, q) = NN1(Mcel, vq)\nfnn(column, q) = NN1(Mcol, vq)\nfnn(row, q) = NN1(Mrow, vq)\nSince a table caption is typically a descriptive word sequence. We model it with bi-directional GRU-RNN, the same strategy we have used for modeling the query. We concatenate the caption vector vcap with vq, and feed the results to a linear layer followed by softmax.\nfnn(caption, q) = NN2(vcap, vq)\nWe separately train the parameters for each aspect with back-propagation. We use negative loglikelihood as the loss function.3\nloss = \u2212 1 |D| \u2211 (ta,q)\u2208D log(fnn(ta, q))"}, {"heading": "5 Experiment", "text": "We describe the experimental setting and analyze the results in this section."}, {"heading": "5.1 Dataset and Setting", "text": "To the best of our knowledge, there is no publicly available dataset for table retrieval. We introduce WebQueryTable, an open-domain dataset consisting of query-table pairs. We use search logs from a commercial search engine to get a list of queries that could be potentially answered by web tables. Each query in query logs is paired with a list of web pages, ordered by the number of user clicks for the query. We select the tables occurred in the top ranked web page, and ask annotators to label whether a table is relevant to a query or not. In this way, we get 21,113 query-table pairs. In the real scenario of table retrieval, a system is required to find a table from a huge collection of tables.\n3We also implemented a ranking based loss function max(0, 1\u2212fnn(ta, q)+fnn(t\u2217a, q)), but it performed worse than the negative log-likelihood in our experiment.\nTherefore, in order to enlarge the search space of our dataset, we extract 252,703 web tables from Wikipedia and regard them as searchable tables as well. Data statistics are given in Table 1.\nWe sampled 200 examples to analyze the distribution of the query types in our dataset. We observe that 69.5% queries are asking about \u201ca list of XXX\u201d, such as \u201clist of countries and capitals\u201d and \u201cmajor cities in netherlands\u201d, and about 24.5% queries are asking about an attribute of an object, such as \u201cdensity of liquid water temperature\u201d. We randomly separate the dataset as training, validation, test with a 70:10:20 split.\nWe also conduct a synthetic experiment for table retrieval on WikiTableQuestions (Pasupat and Liang, 2015), which is a widely used dataset for table-based question answering. It contains 2,108 HTML tables extracted from Wikipedia. Workers from Amazon Mechanical Turk are asked to write several relevant questions for each table. Since each query is written for a specific table, we believe that each pair of query-table can also be used as an instance for table retrieval. The difference between WikiTableQuestions and WebQueryTable is that the questions in WikiTableQuestions mainly focus on the local regions, such as cells or columns, of a table while the queries in WebQueryTable mainly focus on the global content of a table. The number of table index in WikiTableQuestions is 2,108, which is smaller than the number of table index in WebQueryTable. We randomly split the 22,033 question-table pairs into training (70%), development (10%) and test (20%).\nIn the candidate table retrieval phase, we encode a table as bag-of-words to guarantee the efficiency of the approach. Specifically, on WebQueryTable dataset we represent a table with caption and headers. On WikiTableQuestions dataset we represent a table with caption, headers and cells. The re-\ncalls of the candidate table retrieval step on WikiTableQuestions and WebQueryTable datasets are 56.91% and 69.57%, respectively. The performance of table ranking is evaluated with Mean Average Precision (MAP) and Precision@1 (P@1) (Manning et al., 2008). When evaluating the performance on table ranking, we filter out the following special cases that only one candidate table is returned or the correct answer is not contained in the retrieved tables in the first step. Hyper parameters are tuned on the validation set."}, {"heading": "5.2 Results on WebQueryTable", "text": "Table 2 shows the performance of different approaches on the WebQueryTable dataset.\nWe compare between different features for table ranking. An intuitive baseline is to represent a table as bag-of-words, represent a query with bag-of-words, and calculate their similarity with cosine similarity. Therefore, we use the BM25 score which is calculated in the candidate table retrieval step. This baseline is abbreviated as BM25. We also report the results of using designed features (Feature) described in Section 4.1 and neural networks (NeuralNet) described in Section 4.2. Results from Table 2 show that the neural networks perform comparably with the designed features, and obtain better performance than the BM25 baseline. This results reflect the necessary of taking into account the table structure for table retrieval. Furthermore, we can find that combining designed features and neural networks could achieve further improvement, which indicates the complementation between them.\nWe further investigate the effects of headers, cells and caption for table retrieval on WebQueryTable. We first use each aspect separately and then increasingly combine different aspects. Results are given in Table 3. We can find that in general the performance of an aspect in designed features is consistent with its performance in neural networks. Caption is the most effective aspect on WebQueryTable. This is reasonable as we find\nthat majority of the queries are asking about a list of objects, such as \u201cpolish rivers\u201d, \u201cworld top 5 mountains\u201d and \u201clist of american cruise lines\u201d. These intentions are more likely to be matched in the caption of a table. Combining more aspects could get better results. Using cells, headers and caption simultaneously gets the best results.\nMoreover, we investigate whether using a higher threshold could obtain a better precision. Therefore, we increasingly use a set of thresholds, and calculate the corresponding precision and recall in different conditions. An instance is considered to be correct if the top ranked table is correct and its ranking score is greater than the threshold. Results of our NeuralNet approach on WebQueryTable are given in 2. We can see that using larger threshold results in lower recall and higher precision. The results are consistent with our intuition.\nWe conduct case study on our NeuralNet approach and find that the performance is sensitive to the length of queries. Therefore, we split the test set to several groups according to the length of queries. Results are given in Figure 4. We can find that the performance of the approach decreases with the increase of query length. When the query length changes from 6 to 7, the performance of P@1 decreases rapidly from 58.12%\nto 50.23%. Through doing case study, we find that long queries contain more word dependencies. Therefore, having a good understanding about the intention of a query requires deep query understanding. Leveraging external knowledge to connect query and table is a potential solution to deal with long queries.\nWe illustrate two examples generated by our NeuralNet approach in Figure 3. The example in Figure 3(a) is a satisfied case that the top ranked result is the correct answer. We can find that the model uses evidences from different aspects to match between a query and a table. In this example, the supporting evidences come from caption (\u201cramadan\u201d and \u201cmalaysia\u201d), headers (\u201cdates\u201d) and cells (\u201c2016\u201d). The example in Figure 3(b) is a dissatisfied case. We can find that the top ranked result contains \u201clife expectancy\u201d in both caption and header, however, it is talking about the people in U.S. rather than \u201cgerman shepherd\u201d. Despite the correct table contains a cell whose content is \u201cgerman shepherd\u201d, it still does not obtain a higher rank than the left table. The reason might be that the weight for header is larger than the weight for cells."}, {"heading": "5.3 Results on WikiTableQuestions", "text": "Table 4 shows the results of table ranking on the WikiTableQuestions dataset.\nWe implement two baselines. The first baseline is BM25, which is the same baseline we have used for comparison on the WebQueryTable dataset. The second baseline is header grounding, which is partly inspired by Venetis et al. (2011) who show the effectiveness of the semantic relationship between query and table header. We implement a CDSSM (Shen et al., 2014) approach to match between a table header and a query. We train the model by minimizing the cross-entropy error, where the ground truth is the header of the answer. Results are given in Table 4. We can find that designed features perform comparably with neural networks, and both of them perform better than BM25 and column grounding baselines. Combining designed features and neural networks obtains further improvement.\nWe also study the effects of different aspects on the WikiTableQuestions dataset. Results are given in Table 5. We can find that the effects of different aspect in designed features and neural networks are consistent. Using more aspects could achieve better performance. Using all aspects obtains the best performance. We also find that the most effective aspect for WikiTableQuestions is header. This is different from the phenomenon in Web-\nQueryTable that the most effective aspect is caption. We believe that this is because the questions in WikiTableQuestions typically include content constrains from cells or headers. Two randomly sampled questions are \u201cwhich country won the 1994 europeans men\u2019s handball championship\u2019s preliminary round?\u201d and \u201cwhat party had 7,115 inactive voters as of october 25, 2005?\u201d. On the contrary, queries from WebTableQuery usually do not use information from specific headers or cells. Examples include \u201cpolish rivers\u201d, \u201cworld top 5 mountains\u201d and \u201clist of american cruise lines\u201d. From Table 1, we can also find that the question in WikiTableQuestions are longer than the queries in WebQueryTable. In addition, we observe that not all the questions from WikiTableQuestions are suitable for table retrieval. An example is \u201cwhat was the first player to be drafted in this table?\u201d."}, {"heading": "6 Related Work", "text": "Our work connects to the fields of database and natural language processing.\nThere exists several works in database community that aims at finding related tables from keyword queries. A representative work is given by Cafarella et al. (2008), which considers table search as a special case of document search task and represent a table with its surrounding text and page title. Limaye et al. (2010) use YAGO ontology to annotate tables with column and relationship labels. Venetis et al. (2011) go one step further and use labels and relationships extracted from the web. Pimplikar and Sarawagi (2012) focus on the queries that describe table columns, and retrieve tables based on column mapping. There also exists table-related studies such as searching related tables from a table (Das Sarma et al., 2012), assembling a table from list in web page (Gupta and Sarawagi, 2009) and extracting tables using tabular structure from web page (Gatter-\nbauer et al., 2007). Our work differs from this line of research in that we focus on exploring the content of table to find relevant tables from web queries.\nOur work relates to a line of research works that learn continuous representation of structured knowledge with neural network for natural language processing tasks. For example, Neelakantan et al. (2015); Yin et al. (2015b) develop neural operator on the basis of table representation and apply the model to question answering. Yin et al. (2015a) introduce a KB-enhanced sequenceto-sequence approach that generates natural language answers to simple factoid questions based on facts from KB. Mei et al. (2016) develop a LSTM based recurrent neural network to generate natural language weather forecast and sportscasting commentary from database records. Serban et al. (2016) introduce a recurrent neural network approach, which takes fact representation as input and generates factoid question from a fact from Freebase. Lebret et al. (2016) presented an neural language model that generates biographical sentences from Wikipedia infobox.\nOur neural network approach relates to the recent advances of attention mechanism and reasoning over external memory in artificial intelligence (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016). Researchers typically represent a memory as a continuous vector or matrix, and develop neural network based controller, reader and writer to reason over the memory. The memory could be addressed by a \u201csoft\u201d attention mechanism trainable by standard backpropagation methods or a \u201chard\u201d attention mechanism trainable by REINFORCE (Williams, 1992). In this work, we use the soft attention mechanism, which could be easily optimized and has been successfully applied in nlp tasks (Bahdanau et al., 2015; Sukhbaatar et al., 2015)."}, {"heading": "7 Conclusion", "text": "In this paper, we give an empirical study of content-based table retrieval for web queries. We implement a feature-based approach and a neural network based approach, and release a new dataset consisting of web queries and web tables. We conduct comprehensive experiments on two datasets. Results not only verify the effectiveness of our approach, but also present future challenges for content-based table retrieval."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR) .", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Applying webtables in practice", "author": ["Sreeram Balakrishnan", "Alon Y Halevy", "Boulos Harb", "Hongrae Lee", "Jayant Madhavan", "Afshin Rostamizadeh", "Warren Shen", "Kenneth Wilder", "Fei Wu", "Cong Yu."], "venue": "Proceedings of Conference on Innovative Data", "citeRegEx": "Balakrishnan et al\\.,? 2015", "shortCiteRegEx": "Balakrishnan et al\\.", "year": 2015}, {"title": "From ranknet to lambdarank to lambdamart: An overview", "author": ["Christopher JC Burges."], "venue": "Microsoft Research Technical Report MSR-TR-2010-82 11(23581):81.", "citeRegEx": "Burges.,? 2010", "shortCiteRegEx": "Burges.", "year": 2010}, {"title": "Webtables: exploring the power of tables on the web", "author": ["Michael J Cafarella", "Alon Halevy", "Daisy Zhe Wang", "Eugene Wu", "Yang Zhang."], "venue": "Proceedings of the VLDB Endowment 1(1):538\u2013549.", "citeRegEx": "Cafarella et al\\.,? 2008", "shortCiteRegEx": "Cafarella et al\\.", "year": 2008}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555 .", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Finding related tables", "author": ["Anish Das Sarma", "Lujun Fang", "Nitin Gupta", "Alon Halevy", "Hongrae Lee", "Fei Wu", "Reynold Xin", "Cong Yu."], "venue": "Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data. ACM, pages", "citeRegEx": "Sarma et al\\.,? 2012", "shortCiteRegEx": "Sarma et al\\.", "year": 2012}, {"title": "Paraphrase-driven learning for open question answering", "author": ["Anthony Fader", "Luke Zettlemoyer", "Oren Etzioni."], "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Towards domain-independent information extraction from web tables", "author": ["Wolfgang Gatterbauer", "Paul Bohunsky", "Marcus Herzog", "Bernhard Kr\u00fcpl", "Bernhard Pollak."], "venue": "Proceedings of the 16th international conference on World Wide Web (WWW).", "citeRegEx": "Gatterbauer et al\\.,? 2007", "shortCiteRegEx": "Gatterbauer et al\\.", "year": 2007}, {"title": "Answering table augmentation queries from unstructured lists on the web", "author": ["Rahul Gupta", "Sunita Sarawagi."], "venue": "Proceedings of the VLDB Endowment 2(1):289\u2013300.", "citeRegEx": "Gupta and Sarawagi.,? 2009", "shortCiteRegEx": "Gupta and Sarawagi.", "year": 2009}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Neural text generation from structured data with application to the biography domain", "author": ["R\u00e9mi Lebret", "David Grangier", "Michael Auli."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language (EMNLP).", "citeRegEx": "Lebret et al\\.,? 2016", "shortCiteRegEx": "Lebret et al\\.", "year": 2016}, {"title": "Annotating and searching web tables using entities, types and relationships", "author": ["Girija Limaye", "Sunita Sarawagi", "Soumen Chakrabarti."], "venue": "Proceedings of the VLDB Endowment 3(1-2):1338\u2013 1347.", "citeRegEx": "Limaye et al\\.,? 2010", "shortCiteRegEx": "Limaye et al\\.", "year": 2010}, {"title": "What to talk about and how? selective generation using lstms with coarse-to-fine alignment", "author": ["Hongyuan Mei", "Mohit Bansal", "Matthew R. Walter."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Associ-", "citeRegEx": "Mei et al\\.,? 2016", "shortCiteRegEx": "Mei et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems (NIPS). pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V Le", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1511.04834 .", "citeRegEx": "Neelakantan et al\\.,? 2015", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "Compositional semantic parsing on semi-structured tables", "author": ["Panupong Pasupat", "Percy Liang."], "venue": "Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL) .", "citeRegEx": "Pasupat and Liang.,? 2015", "shortCiteRegEx": "Pasupat and Liang.", "year": 2015}, {"title": "Answering table queries on the web using column keywords", "author": ["Rakesh Pimplikar", "Sunita Sarawagi."], "venue": "Proceedings of the VLDB Endowment 5(10):908\u2013 919.", "citeRegEx": "Pimplikar and Sarawagi.,? 2012", "shortCiteRegEx": "Pimplikar and Sarawagi.", "year": 2012}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Iulian Vlad Serban", "Alberto Garc\u0131\u0301a-Dur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil."], "venue": "Proceedings of the Conference on Information and Knowledge Management (CIKM). pages", "citeRegEx": "Shen et al\\.,? 2014", "shortCiteRegEx": "Shen et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus."], "venue": "Advances in Neural Information Processing Systems (NIPS). pages 2431\u20132439.", "citeRegEx": "Sukhbaatar et al\\.,? 2015", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Recovering semantics of tables on the web", "author": ["Petros Venetis", "Alon Halevy", "Jayant Madhavan", "Marius Pa\u015fca", "Warren Shen", "Fei Wu", "Gengxin Miao", "Chung Wu."], "venue": "Proceedings of the VLDB Endowment 4(9):528\u2013538.", "citeRegEx": "Venetis et al\\.,? 2011", "shortCiteRegEx": "Venetis et al\\.", "year": 2011}, {"title": "Order matters: Sequence to sequence for sets", "author": ["Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur."], "venue": "arXiv preprint arXiv:1511.06391 .", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "citeRegEx": "Williams.,? 1992", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Neural generative question answering", "author": ["Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li."], "venue": "arXiv preprint arXiv:1512.01337 .", "citeRegEx": "Yin et al\\.,? 2015a", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Neural enquirer: Learning to query tables with natural language", "author": ["Pengcheng Yin", "Zhengdong Lu", "Hang Li", "Ben Kao."], "venue": "arXiv preprint arXiv:1512.00965 .", "citeRegEx": "Yin et al\\.,? 2015b", "shortCiteRegEx": "Yin et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 16, "context": "On the other hand, the retrieved table could be used as the input for question answering (Pasupat and Liang, 2015).", "startOffset": 89, "endOffset": 114}, {"referenceID": 3, "context": "Unlike existing studies in database community (Cafarella et al., 2008; Balakrishnan et al., 2015) that utilize surrounding text of a table or pagerank score of a web page, we focus on making a thorough exploration of table content in this work.", "startOffset": 46, "endOffset": 97}, {"referenceID": 1, "context": "Unlike existing studies in database community (Cafarella et al., 2008; Balakrishnan et al., 2015) that utilize surrounding text of a table or pagerank score of a web page, we focus on making a thorough exploration of table content in this work.", "startOffset": 46, "endOffset": 97}, {"referenceID": 16, "context": "We conduct comprehensive experiments on two datasets, a real world dataset introduced by us, and a synthetic dataset WikiTableQuestions (Pasupat and Liang, 2015) which has been widely used for table-based question answering.", "startOffset": 136, "endOffset": 161}, {"referenceID": 2, "context": "Afterwards, we use LambdaMART (Burges, 2010), a successful algorithm for solving real world ranking problem, to get the final ranking score of each table.", "startOffset": 30, "endOffset": 44}, {"referenceID": 10, "context": "We use an existing SMT approach (Koehn et al., 2003) to extract a phrase table PT from a bilingual corpus.", "startOffset": 32, "endOffset": 52}, {"referenceID": 19, "context": "We use CDSSM (Shen et al., 2014), which has been successfully applied in text retrieval.", "startOffset": 13, "endOffset": 32}, {"referenceID": 7, "context": "We train model parameters on WikiAnswers dataset (Fader et al., 2013), which contains almost 12M question-similar question pairs.", "startOffset": 49, "endOffset": 69}, {"referenceID": 7, "context": "We train model parameters on WikiAnswers dataset (Fader et al., 2013), which contains almost 12M question-similar question pairs. In addition, since vector average is an intuitive way to compute sentence vector and does not induce additional parameters, we calculate another relevance score by representing a query and a table aspect with element-wise vector average. We use a publicly available word embedding which is released by Mikolov et al. (2013).", "startOffset": 50, "endOffset": 454}, {"referenceID": 4, "context": "To avoid the problem of gradient vanishing, we use gated recurrent unit (GRU) (Cho et al., 2014) as the basic computation unit, which adaptively forgets the history and remembers the input, and has proven to be effective in sequence modeling (Chung et al.", "startOffset": 78, "endOffset": 96}, {"referenceID": 5, "context": ", 2014) as the basic computation unit, which adaptively forgets the history and remembers the input, and has proven to be effective in sequence modeling (Chung et al., 2014).", "startOffset": 153, "endOffset": 173}, {"referenceID": 22, "context": "An important property of a table is that randomly exchanging two rows or tow columns will not change the meaning of a table (Vinyals et al., 2015).", "startOffset": 124, "endOffset": 146}, {"referenceID": 0, "context": "Afterwards, a query-specific header vector is obtained through weighted average (Bahdanau et al., 2015; Sukhbaatar et al., 2015), namely vheader = \u2211k i=1 \u03b1imi, where \u03b1i \u2208 [0, 1] is the weight of mi calculated as below and \u2211 i \u03b1i = 1.", "startOffset": 80, "endOffset": 128}, {"referenceID": 20, "context": "Afterwards, a query-specific header vector is obtained through weighted average (Bahdanau et al., 2015; Sukhbaatar et al., 2015), namely vheader = \u2211k i=1 \u03b1imi, where \u03b1i \u2208 [0, 1] is the weight of mi calculated as below and \u2211 i \u03b1i = 1.", "startOffset": 80, "endOffset": 128}, {"referenceID": 25, "context": "Similar techniques have been successfully applied in table-based question answering (Yin et al., 2015b; Neelakantan et al., 2015).", "startOffset": 84, "endOffset": 129}, {"referenceID": 15, "context": "Similar techniques have been successfully applied in table-based question answering (Yin et al., 2015b; Neelakantan et al., 2015).", "startOffset": 84, "endOffset": 129}, {"referenceID": 16, "context": "We also conduct a synthetic experiment for table retrieval on WikiTableQuestions (Pasupat and Liang, 2015), which is a widely used dataset for table-based question answering.", "startOffset": 81, "endOffset": 106}, {"referenceID": 21, "context": "The second baseline is header grounding, which is partly inspired by Venetis et al. (2011) who show the effectiveness of the semantic relationship between query and table header.", "startOffset": 69, "endOffset": 91}, {"referenceID": 19, "context": "CDSSM (Shen et al., 2014) approach to match between a table header and a query.", "startOffset": 6, "endOffset": 25}, {"referenceID": 9, "context": ", 2012), assembling a table from list in web page (Gupta and Sarawagi, 2009) and extracting tables using tabular structure from web page (Gatterbauer et al.", "startOffset": 50, "endOffset": 76}, {"referenceID": 8, "context": ", 2012), assembling a table from list in web page (Gupta and Sarawagi, 2009) and extracting tables using tabular structure from web page (Gatterbauer et al., 2007).", "startOffset": 137, "endOffset": 163}, {"referenceID": 3, "context": "A representative work is given by Cafarella et al. (2008), which considers table search as a special case of document search task and represent a table with its surrounding text and page title.", "startOffset": 34, "endOffset": 58}, {"referenceID": 3, "context": "A representative work is given by Cafarella et al. (2008), which considers table search as a special case of document search task and represent a table with its surrounding text and page title. Limaye et al. (2010) use YAGO ontology to annotate tables with column and relationship labels.", "startOffset": 34, "endOffset": 215}, {"referenceID": 3, "context": "A representative work is given by Cafarella et al. (2008), which considers table search as a special case of document search task and represent a table with its surrounding text and page title. Limaye et al. (2010) use YAGO ontology to annotate tables with column and relationship labels. Venetis et al. (2011) go one step further and use labels and relationships extracted from the web.", "startOffset": 34, "endOffset": 311}, {"referenceID": 3, "context": "A representative work is given by Cafarella et al. (2008), which considers table search as a special case of document search task and represent a table with its surrounding text and page title. Limaye et al. (2010) use YAGO ontology to annotate tables with column and relationship labels. Venetis et al. (2011) go one step further and use labels and relationships extracted from the web. Pimplikar and Sarawagi (2012) focus on the queries that describe table columns, and retrieve tables based on column mapping.", "startOffset": 34, "endOffset": 418}, {"referenceID": 13, "context": "For example, Neelakantan et al. (2015); Yin et al.", "startOffset": 13, "endOffset": 39}, {"referenceID": 13, "context": "For example, Neelakantan et al. (2015); Yin et al. (2015b) develop neural operator on the basis of table representation and apply the model to question answering.", "startOffset": 13, "endOffset": 59}, {"referenceID": 13, "context": "For example, Neelakantan et al. (2015); Yin et al. (2015b) develop neural operator on the basis of table representation and apply the model to question answering. Yin et al. (2015a) introduce a KB-enhanced sequenceto-sequence approach that generates natural language answers to simple factoid questions based on facts from KB.", "startOffset": 13, "endOffset": 182}, {"referenceID": 12, "context": "Mei et al. (2016) develop a LSTM based recurrent neural network to generate natural language weather forecast and sportscasting commentary from database records.", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "Mei et al. (2016) develop a LSTM based recurrent neural network to generate natural language weather forecast and sportscasting commentary from database records. Serban et al. (2016) introduce a recurrent neural network approach, which takes fact representation as input and generates factoid question from a fact from Freebase.", "startOffset": 0, "endOffset": 183}, {"referenceID": 11, "context": "Lebret et al. (2016) presented an neural language model that generates biographical sentences from Wikipedia infobox.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Our neural network approach relates to the recent advances of attention mechanism and reasoning over external memory in artificial intelligence (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016).", "startOffset": 144, "endOffset": 213}, {"referenceID": 20, "context": "Our neural network approach relates to the recent advances of attention mechanism and reasoning over external memory in artificial intelligence (Bahdanau et al., 2015; Sukhbaatar et al., 2015; Graves et al., 2016).", "startOffset": 144, "endOffset": 213}, {"referenceID": 23, "context": "The memory could be addressed by a \u201csoft\u201d attention mechanism trainable by standard backpropagation methods or a \u201chard\u201d attention mechanism trainable by REINFORCE (Williams, 1992).", "startOffset": 163, "endOffset": 179}, {"referenceID": 0, "context": "In this work, we use the soft attention mechanism, which could be easily optimized and has been successfully applied in nlp tasks (Bahdanau et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 130, "endOffset": 178}, {"referenceID": 20, "context": "In this work, we use the soft attention mechanism, which could be easily optimized and has been successfully applied in nlp tasks (Bahdanau et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 130, "endOffset": 178}], "year": 2017, "abstractText": "Understanding the connections between unstructured text and semi-structured table is an important yet neglected problem in natural language processing. In this work, we focus on content-based table retrieval. Given a query, the task is to find the most relevant table from a collection of tables. Further progress towards improving this area requires powerful models of semantic matching and richer training and evaluation resources. To remedy this, we present a ranking based approach, and implement both carefully designed features and neural network architectures to measure the relevance between a query and the content of a table. Furthermore, we release an open-domain dataset that includes 21,113 web queries for 273,816 tables. We conduct comprehensive experiments on both real world and synthetic datasets. Results verify the effectiveness of our approach and present the challenges for this task.", "creator": "LaTeX with hyperref package"}}}