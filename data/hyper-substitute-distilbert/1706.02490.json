{"id": "1706.02490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "Where is my forearm? Clustering of body parts from simultaneous tactile and linguistic input using sequential mapping", "abstract": "humans wherein animals react constantly exposed to relatively continuous stream splitting digital information from contrasting modalities. preceding the present tempo, they form somewhat effective representations like labels giving shapes. in things that use words, this situation is further described by this interaction, resulting a mapping both the sensorimotor concepts and linguistic behaviors needs to go established. serves as evidence that squirrels might be learning language users repeatedly expressing potential meanings measured between changing exposures to representations employing different contexts ( cross - situational learning ). manipulating existing behaviors, the alignment between modalities is carefully defined in adaptive relay step by directly using frequencies of referent and sequential stimulus - occurrences. in this paper, we direct additional idea upon this deux - step trend and present a newly simplified sequential mapping methodology together with a publicly distributed matlab implementation. for demonstration, we exhibit evolved way less constrained scenario : instead of learning to target objects with identical names, algorithms focus on body representations. making humanoid robot continually carrying tactile learning on its limbs, actually spending the resulting time listening to utterances of the body having names ( an. g., hand, brain and chest ). with the researcher at arriving calling the input \" stimulus categories \", we demonstrate how a sequential mapping algorithm outperforms one - state mapping. in addition, an effect of data perceived size versus interpretation through actual linguistic input are studied.", "histories": [["v1", "Thu, 8 Jun 2017 09:31:42 GMT  (622kb,D)", "http://arxiv.org/abs/1706.02490v1", "pp. 155-162"]], "COMMENTS": "pp. 155-162", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.LG cs.RO", "authors": ["karla stepanova", "matej hoffmann", "zdenek straka", "frederico b klein", "angelo cangelosi", "michal vavrecka"], "accepted": false, "id": "1706.02490"}, "pdf": {"name": "1706.02490.pdf", "metadata": {"source": "CRF", "title": "Where is my forearm? Clustering of body parts from simultaneous tactile and linguistic input using sequential mapping", "authors": ["Karla \u0160t\u011bp\u00e1nov\u00e1", "Mat\u011bj Hoffmann", "Zden\u011bk Straka", "Frederico B. Klein", "Angelo Cangelosi", "Michal Vavre\u010dka"], "emails": [], "sections": [{"heading": null, "text": "Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct \u201cbody categories\u201d, we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied."}, {"heading": "1 Introduction", "text": "Body representation has been the topic of psychological, neuroanatomical and neurophysiological studies for many decades. Spurred by the account of Head and Holmes (1911) and their proposal of superficial and postural schema, a number of different concepts has been proposed since: body schema, body image, and corporeal schema being only some of them. Body schema is usually thought of as more \u201clow-level\u201d, sensorimotor representation of the body used for action. Body image\nis an umbrella term uniting higher level representations, for perception more than for action, and accessible to consciousness. Schwoebel and Coslett (2005) amassed evidence for distinguishing between three types of body representations: body schema, body structural description, and body semantics\u2014constituting a kind of hierarchy. The body structural description is a topological map of locations derived primarily from visual input that defines body part boundaries and proximity relationships. Finally, body semantics is a lexical\u2013semantic representation of the body including body part names, functions, and relations with artifacts (e.g., shoes are used on the feet, and feet can be used to kick a football).\nWhile the details of every particular taxonomy or hierarchy can be discussed, clearly, there is a trend from continuous, modality-specific representations (like the tactile homunculus) to multimodal, more aggregated representations. This may be first instantiated by increasing receptive field size and combining sensory modalities, as it is apparent in somatosensory processing, e.g. areas relatively specialized on proprioception or touch and with small receptive fields (like Brodmann areas 3a and 3b), touch and proprioception are getting increasingly combined in areas 1 and 2. Then, going from anterior to posterior parietal cortex, the receptive fields grow further and somatosensory information is combined with visual. One can then ask whether this process of bottom-up integration or aggregation may give rise to discrete entities, or categories, similar to individual body parts. Vignemont et al. (2009) focused on how body segmentation between hand and arm could appear based on a combined tactile and visual perception. They explored category boundary effect which appeared when two tactile stimuli were presented: these stimuli felt farther away when they were applied across the wrist than when they were applied within a single body part (palm or forearm). In conclusion, they suggest that the representation of the body is structured in categorical body parts delineated by joints, and that this categorical representation modulates tactile spatial perception.\nNext to the essentially bottom-up clustering of\n1\nar X\niv :1\n70 6.\n02 49\n0v 1\n[ cs\n.N E\n] 8\nJ un\n2 01\n7\nmultimodal body-related information, an additional \u201ccategorization\u201d of body parts is imposed through language, such as when the infant hears her parents naming the body parts. Interestingly, recent research (Majid, 2010) showed that there are some cross-linguistic variabilities in naming body parts and this may in turn override or influence the \u201cbottom-up\u201d multimodal (nonlinguistic) body part categorization.\nWhile the field is relatively rich in experimental observations, the mechanisms behind the development and operation of these representations are still not well understood. Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots. Petit and Demiris (2016) developed an algorithm for the iCub humanoid robot to associate labels for body parts and later proto-actions with their embodied counterparts. These could then be recombined in a hierarchical fashion (e.g., \u201cclose hand\u201d consists of folding individual fingers). Mimura et al. (2017) used Dirichlet process Gaussian mixture model with latent joint to provide a Bayesian body schema estimation based on tactile information. Their results suggest that kinematic structure could be estimated directly from tactile information provided by a moving fetus without any additional visual information\u2014albeit with a lower accuracy. Our own work on the iCub humanoid robot has thus far focused on learning primary representations\u2014tactile (Hoffmann et al., 2017) and proprioceptive (Hoffmann and Bednarova, 2016). In this work, we use the former (the \u201ctactile homunculus\u201d) as input for further processing\u2014interaction with linguistic input.\nIn this work, we strive to find segmentation of body parts based on a simultaneous tactile and linguistic information. However, body part categorization and mapping to body part names is one instance of a more general problem of segmenting objects from the environment, learning compressed representations (loosely speaking: concepts, categories, symbols) to stand in for them and associating them with words to which the infant is often exposed simultaneously. Borghi et al. (2004), for example, studied the interaction of object names with situated action on the same objects.\nWe made use of a newly proposed sequential mapping algorithm which extends an idea of one-step mapping (Smith et al., 2006) and compared its overall accuracy to one-step mapping as well as to accuracies of segmenting individual body parts. We further explore how the accuracy of the learned mapping is influenced by a level of noise in the linguistic domain and data set size. The sequential mapping strategy was shown to be very robust as it can find the mapping under circumstances of very noisy input and clearly outperformed the one-step mapping.\nComplete source code used for generating results in this article is publicly available at https://github.\ncom/stepakar/sequential-mapping. This article is structured as follows. The inputs and their preprocessing and the mapping algorithms are described in Section 2. This is followed by Results (Section 3) and a Discussion and Conclusion."}, {"heading": "2 Materials and Methods", "text": "In this section, we will first present the inputs and their preprocessing pipelines: tactile input (Section 2.1) and linguistic input (Section 2.2). In total, 9 body parts of the right half of the robot\u2019s upper body were stimulated: torso/chest, upper arm, forearm, palm and 5 fingertips. Tactile stimulation coincided with an utterance of the body part\u2019s name. Then, the one-step and sequential mapping algorithms (sections 2.3.1 and 2.4) are presented, and a description of the evaluation (Section 2.5)."}, {"heading": "2.1 Tactile inputs and processing", "text": "To generate tactile stimulation pertaining to different body parts, we built on our previous work on the iCub humanoid robot. In particular, the \u201ctactile homunculus\u201d (Hoffmann et al., 2017)\u2014a primary representation of the artificial sensitive skin the robot is covered with (see Fig. 1 \u2013 one half of the robot\u2019s upper body). In the current work, the skin was not physically stimulated anymore, but the activations were emulated and then relayed to the \u201chomunculus\u201d, as detailed below."}, {"heading": "2.1.1 Emulated tactile input", "text": "We created a YARP (Metta et al., 2006) software module to generate virtual skin contacts1. A skin part was randomly selected and then stimulated. The number of pressure-sensitive elements (henceforth taxels) for different skin parts was 440 for the torso, 380 for upper arm, 230 for forearm, and 104 for the hand (44 for palm and 5 \u00d7 12 for fingertips)\u20141154 taxels in total. Once the skin part was randomly selected, a small region was also randomly picked within that part for the tactile stimulation\u201410 taxels at a time, corresponding to the triangular modules the skin is composed of. For the hand, the situation was slightly different: the entire hand was treated as one skin part. Then, within the hand, a random choice was made between 5 subregions on the palm skin (8 to 10 taxels) and 5 fingertips (12 taxels each). Data was collected for 100 minutes, corresponding to approximately 2000 individual 3 second stimulations. For all skin parts, the stimulation lasted for 3 seconds and was sampled at 10 Hz. A label\u2013body part name\u2013was saved along with the tactile data. These labels are used to generate the linguistic input and for performance evaluation later, but do not directly take\n1https://github.com/robotology/peripersonal-space/\ntree/master/modules/virtualContactGeneration\npart in the clustering of tactile information. Please note that there were separate labels for the palm and individual fingers, while these were all treated as one \u201cskin part\u201d in the virtual touch generation and hence the number of samples per finger, for example, was lower than for other non-hand body parts."}, {"heading": "2.1.2 First layer \u2013 \u201ctactile homunculus\u201d", "text": "The input layer of the \u201ctactile homunculus\u201d (Hoffmann et al., 2017) consists of a vector, a(t), of activations of 1154 taxels at time t\u2014the output of the previous section\u2014that have binary values (1 when a taxel is stimulated, 0 otherwise). The output layer then forms a 7 \u00d7 24 (168 \u201cneurons\u201d in total) grid \u2013 see Figure 1 B. This layer is a compressed representation of the skin surface\u2014the receptive fields of neurons (the parts of skin they respond to) are schematically color-coded. However, this code (and \u201cclustering\u201d) is not available as part of the tactile input. The output layer will be represented as a single vector x(t) = [x1(t), ..., x168(t)]. The activations of the output neurons, xi(t), are calculated as dot products of the weight vector ui corresponding to the i-th output neuron and the tactile activation vector a(t) as follows:\nxi(t) = ui \u00b7 a(t) (1)"}, {"heading": "2.1.3 Second layer \u2013 GMM", "text": "The output of the first layer, vector x(t) (168 elements, continuous-valued) serves as input to the second tactile processing layer. This layer aims to cluster individual body parts and represent them as abstract models. Resulting models T j are subsequently mapped in the multimodal layer to clusters found in the language layer. To process the outputs from the first layer, we used a Gaussian mixture model (GMM), which is a convex mixture of D-dimensional Gaussian densities l(x|\u03b8 j). In this case, each tactile model T j is described by a set of parameters \u03b8 j. The posterior probabilities p(\u03b8 j|x) are computed as follows:\np(\u03b8 j|x) = J\u2211\nj=1\nrkj l(x|\u03b8 j), (2)\nl(x|\u03b8 j) = 1\u221a (2\u03c0)D \u221a |S j| exp[\u22121 2 (x\u2212m j)T (S j)\u22121(x\u2212m j)], (3) where x is a set of D-dimensional continuous-valued data vectors, rkj are the mixture weights, J is the number of tactile models, parameters \u03b8 j are cluster centers m j and covariance matrices S j. Mixture of Gaussians is trained by the EM algorithm (Dempster et al., 1977). Number of tactile models J is in this model preset based on the number of different linguistic labels. In future, we plan to\nuse an adaptive extension of GMM algorithm such as gmGMM (S\u030ctepa\u0301nova\u0301 and Vavrec\u030cka, 2016) to detect this number autonomously. An output of this layer for each data point x(t) is the vector y(t) of J output parameters describing the data point (the likelihood that the data point belongs to each individual cluster in a mixture). This corresponds to the fuzzy memberships (distributed representation)."}, {"heading": "2.2 Linguistic inputs and processing", "text": "Tactile stimulation of a body part was accompanied with the corresponding utterance. In our case, where we have 9 separate body parts, these are \u2019torso\u2019, \u2019upper arm\u2019, \u2019forearm\u2019, \u2019palm\u2019, \u2019little finger\u2019, \u2019ring finger\u2019, \u2019middle finger\u2019, \u2019index finger\u2019 and \u2019thumb\u2019. Linguistic and tactile inputs are processed simultaneously. We conducted experiments with spoken language input\u2014one-word utterances pronounced by a nonnative English speaker. To process this data, we made use of CMU Sphinx (an open-source flexible Markov model-based speech recognizer system) (Lamere et al., 2003) and achieved 100% accuracy of word recognition. The word-forms are extracted from the audio input and compared to prelearned language models by means of the log-scale scores p(wnt |Li) of the audio matching. Based on these data, posterior probability can be computed. However, in the current work, we employed a shortcut and used the labels (ground truth) directly. This allowed us to fully explore the effect of misclassification in linguistic subdomain to mapping accuracy. The noise to the language data was added subsequently and evenly to all classes (a given proportion of labels was randomly permuted)."}, {"heading": "2.3 Cross-situational learning", "text": "One possible way how to establish mapping between sensorimotor concepts and linguistic elements is to use frequencies of referent and meaning co-occurrences, that is, the ones with the highest co-occurrence are mapped together (Smith et al., 2006; Xu and Tenenbaum, 2007). This method is usually called crosssituational learning and supposes the availability of the ideal associative learner who can keep track and store all co-occurrences in all trials, internally memorizing and representing the word\u2013object co-occurrence matrix of input. This allows the learner to subsequently choose the most strongly associated referent (Yu and Smith, 2012)."}, {"heading": "2.3.1 One-step mapping", "text": "The simplest one-step word-to-referent learning algorithm only accumulates word-referent pairs. This can be viewed as Hebbian learning: the connection between a\nword and an object is strengthened if the pair co-occurs in a trial. To extend this basic idea, we can enable also forgetting by introducing a parameter \u03b7, which can capture the memory decay (Yu and Smith, 2012). Supposing that at each trial t we observe an object ont and hear a corresponding word wnt (Nt possible associations), we can describe the update of the strength of the association between word model L(i) and object\u2014in our case tactile model T ( j)\u2014as follows:\nA(i, j) = R\u2211\nt=1\n\u03b7(t) Nt\u2211\nn=1\n\u03b4(wnt , i)\u03b4(o n t , j), (4)\nwhere R is the number of trials, \u03b4 is the Kronecker delta function (equal to 1 when both arguments are identical and 0 otherwise), wnt and o n t indicate the nth word\u2013object association that the model attends to and attempts to learn in the trial t and \u03b7(t) is the parameter controlling the gain of the strength of association. Now let\u2019s assume that the word w(i) is modeled by the model Li in the language domain and object (referent) o( j) is modeled by the model Tm(i) in the tactile domain. Our goal is to find the corresponding model Tm(i) from tactile subdomain for each model Li from language domain to assign them together. Indices m(i) are found as follows:\n\u2200i : m(i) = argmax i A(i, j), (5)\nwhere A is the co-occurrence matrix computed in the Eq. 4 (element A(i, j) captures co-occurrence between the word w(i) and object o( j))."}, {"heading": "2.4 Sequential mapping algorithm", "text": "To capture dynamic competition among models, we extend the basic one-step mapping algorithm for crosssituational learning by sequential addition of inhibitory connections. The inhibitory mechanisms and situationtime dynamics were already partially included into the model of cross-situational learning proposed by McMurray et al. (2012). Even though our model shares some similarities with the model proposed by McMurray, it stems from different computational mechanisms. After a reliable assignment between a language and tactile model is found, inhibitory connections among this tactile model and all other language models are added. Thanks to this mechanism, mutual exclusivity principle (the fact that children prefer mapping where object has only one label to multiple labels (Markman, 1990)) is guaranteed. The assignment between tactile models T j and language models L j is found using the following iterative procedure:\n1. Tactile and language data are clustered separately and the corresponding posterior probabilities are found.\n2. For each data point the most probable tactile and language clusters are selected and the data point is assigned to these clusters.\n3. Co-occurrence matrix with elements A(i, j) is computed and the best assignment is selected:\n[im,m(im)] = argmax i argmax j A(i, j). (6)\nIn this step, the tactile model Tm(im) is assigned to the language model Lim.\n4. Inhibitory connections are added between the assigned tactile model Tm(im) and all language models Li, where i , im (mutual exclusivity).\n5. Assigned data points (data points which belong to both Tm(im) and Lim) are deleted from the data set.\n6. If data set is not empty or not all tactile clusters are assigned to some language cluster go to (1), else stop."}, {"heading": "2.5 Evaluation", "text": "Accuracy of the learned mapping is calculated in the following manner: We cluster output activations from the tactile homunculus and assign each data point to the most probable cluster. Then, we find indices m(i) for all clusters as defined in equation 5 for one-step mapping and equation 6 for sequential mapping. Based on this mapping we can assign each data point to the language label. These language labels are subsequently compared to the ground truth (the body part name is equivalent to the language label prior to the application of noise). Accuracy is then computed as:\nacc = T P/N (7)\nwhere T P (true positive) is the number of correctly assigned data points and N is the number of all data points."}, {"heading": "3 Results", "text": "We studied the performance of one-step vs. the sequential mapping algorithms on the ability to cluster individual body parts from simultaneous tactile and linguistic input. That is, all the skin regions on the same body part should \u201clearn\u201d that they belong together (to the forearm, say), thanks to the co-occurrences with the body part labels. In addition, the effect of data set size and levels of noise in the linguistic domain are investigated (Section 3.1). A detailed analysis of the mapping accuracy for individual body parts and a backward projection onto the tactile homunculus are shown in sections 3.2 and 3.3 respectively."}, {"heading": "3.1 Comparison of accuracy of one-step mapping to sequential mapping", "text": "The performance of the one-step and sequential mapping algorithms is shown in Fig. 2. The comparison is provided for different data set sizes (namely for 6 different data sets with number of data points from 64 to 63806) and noise levels. As can be seen, the accuracy of sequential mapping remains very stable and outperforms one-step mapping for all values of the noise (in\nthe linguistic domain) and all data set sizes. For smaller data sets, we can see a steeper drop in accuracy with increasing noise in the language data."}, {"heading": "3.2 Accuracy of mapping for individual body parts", "text": "The accuracy calculated in the previous section and Fig. 2 is an overall accuracy and we don\u2019t take into account the number of data points per individual body part. To explore the performance in more detail, we focused also on the accuracy of sequential mapping for individual body parts. The results for the data set with 3190 and 638 data points can be seen in Fig. 3 top and bottom panel, respectively. The accuracy for all body parts decreases with increasing noise in the linguistic input. The accuracy for fingers is significantly lower\u2014this is due to the lower number of samples per finger (see Section 2.1.1). Comparing the top and bottom panel in Fig. 3 demonstrates poorer performance with higher variance, especially for the fingers."}, {"heading": "3.3 Projecting results of sequential mapping back onto homunculus", "text": "After tactile data from homunculus are clustered and these clusters are mapped to appropriate language clusters (representing body parts utterances), we can project these labels back onto the original tactile homunculus. Considering that xi(t) are activations of neuron i in the homunculus, D is the whole data set consisting of vector of homunculus activations for each data point, and LangLabel(d) is the language label assigned to a data point d based on the sequential mapping procedure described in the Section 2.4, we can project results of sequential mapping onto the homunculus in a following manner. First, we compute strength of activation nki of each neuron i for a given language label k as follows:\nnki = \u2211\nx(t)\u2208Dk xi(t), i \u2208 {1, . . . , 168}, (8)\nwhere Dk = {d \u2208 D|LangLabel(d) = k} and k = {torso, upper arm, forearm, palm, little finger, ring finger, middle finger, index finger, thumb}.\nAfterwards, we visualize for each neuron how much it is activated for individual body parts. Results for data sets of differing size and level of noise in the linguistic domain can be seen in Fig. 4. Clearly, for large enough data sets and limited noise, the mapping from language to the tactile modality is successful in delineating the body part categories (the fingers with fewer data points being more challenging)\u2014as can be seen by comparing panels A and B."}, {"heading": "4 Discussion and Conclusion", "text": "To study the problem of associating (mapping) between sensorimotor or multimodal information, concepts or\ncategories, and language or symbols, we have chosen a specific but less studied instance of this problem: segmentation and labeling of body parts. Perhaps, from a developmental perspective, this could be plausible, as\nthe body may be the first \u201cobject\u201d the infant is discovering. The self-exploration occurs in the sensorimotor domain, but at the same time or slightly later, the infant is exposed to utterances of body part names. In this work, we study the mapping between the tactile modality and body part labels from linguistic input.\nWe present a new algorithm for mapping language to sensory modalities (sequential mapping), compare it to one-step mapping and test it on the body part categorization scenario. Our results suggest that this mapping procedure is robust, resistant against noise, and sequential mapping shows better performance than onestep mapping for all data set sizes and also slower performance degradation with increasing noise in the linguistic input. Furthermore, we explored accuracy of the sequential mapping for individual body parts, revealing that body parts less represented in the data set\u2014 fingers\u2014were categorized less accurately. This problem might be mitigated with increased overall data set size; yet, dealing with clusters with uneven data point number is a common problem of clustering algorithms (in our case GMM).\nProjecting the labels or categories induced by language back onto the tactile homunculus showed that the\nbody part categories are quite accurate. Given the nature of the tactile input\u2014the skin is a continuous receptor surface\u2014and the random-uniform tactile input generator used, the linguistic input was the only one that can facilitate cluster formation. However, more realistic, non-uniform touch and, in particular, the addition of additional modalities (proprioception, vision) should enable bottom-up non-linguistic body part category formation, as described by (Vignemont et al., 2009), for example. These constitute possible directions of our future work: the \u201cmodal\u201d cluster formation will interact with the labels imposed by language. Furthermore, thus far, only one half of the body was considered\u2014 corresponding to the lateralized representations in the tactile homunculus\u2014, but one can imagine stimulating both left and right arm, for example, while hearing always the same utterance: \u2018upper arm\u2019. Further study of the brain areas involved in this processing is needed, in order to develop models more closely inspired by the functional cortical networks, like in (Caligiore et al., 2010) that model the experimental findings of (Borghi et al., 2004).\nFor our experiments we used artificially generated linguistic input (i.e., body part labels) with added noise (i.e. wrong labels with a certain probability). In the future, we are planning to use actual auditory input (spoken words) with real noise. This will also add the additional dimension of similarity in the auditory domain: \u2018arm\u2019 and \u2018forearm\u2019 are phonetically closer to each other than to, say, \u2018torso\u2019. Thus, the linguistic modality will not constitute crisp, discrete labels anymore, but these will have to be extracted first\u2014opening up further possibilities for bidirectional interaction with other modalities."}, {"heading": "5 Acknowledgement", "text": "K.S. and M.H. were supported by the Czech Science Foundation under Project GA17-15697Y. M.H. was additionally supported by a Marie Curie Intra European Fellowship (iCub Body Schema 625727) within the 7th European Community Framework Programme. Z.S. was supported by The Grant Agency of the CTU Prague project SGS16/161/OHK3/2T/13. M.V. was supported by European research project TRADR funded by the EU FP7 Programme, ICT: Cognitive systems, interaction, robotics (Project Nr. 609763)."}], "references": [{"title": "Putting words in perspective", "author": ["A.M. Borghi", "A.M. Glenberg", "M.P. Kaschak"], "venue": "Memory & Cognition,", "citeRegEx": "Borghi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Borghi et al\\.", "year": 2004}, {"title": "Tropicals: A computational embodied", "author": ["D. Caligiore", "A.M. Borghi", "D. Parisi", "G. Baldassarre"], "venue": null, "citeRegEx": "Caligiore et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Caligiore et al\\.", "year": 2010}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the royal statistical society. Series B (methodological),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Sensory disturbances from cerebral lesions", "author": ["H. Head", "H.G. Holmes"], "venue": null, "citeRegEx": "Head and Holmes,? \\Q1911\\E", "shortCiteRegEx": "Head and Holmes", "year": 1911}, {"title": "The encoding of proprioceptive inputs in the brain: knowns and unknowns from a robotic perspective", "author": ["M. Hoffmann", "N. Bednarova"], "venue": "In Kognice a ume\u030cly\u0301 z\u030civot XVI [Cognition and Artificial", "citeRegEx": "Hoffmann and Bednarova,? \\Q2016\\E", "shortCiteRegEx": "Hoffmann and Bednarova", "year": 2016}, {"title": "Body schema in robotics: A review", "author": ["M. Hoffmann", "H. Marques", "A. Hernandez Arieta", "H. Sumioka", "M. Lungarella", "R. Pfeifer"], "venue": "Autonomous Mental Development, IEEE Transactions", "citeRegEx": "Hoffmann et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2010}, {"title": "Robotic homunculus: Learning of artificial skin representation in a humanoid robot motivated by primary somatosensory cortex", "author": ["M. Hoffmann", "Z. Straka", "I. Farkas", "M. Vavrecka", "G. Metta"], "venue": "IEEE Transactions on Cognitive and Developmental Sys-", "citeRegEx": "Hoffmann et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2017}, {"title": "The cmu sphinx-4 speech recognition system", "author": ["P. Lamere", "P. Kwok", "E. Gouvea", "B. Raj", "R. Singh", "W. Walker", "M. Warmuth", "P. Wolf"], "venue": "In IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP 2003), Hong Kong,", "citeRegEx": "Lamere et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lamere et al\\.", "year": 2003}, {"title": "Words for parts of the body. Words and the mind: How words capture human experience", "author": ["A. Majid"], "venue": null, "citeRegEx": "Majid,? \\Q2010\\E", "shortCiteRegEx": "Majid", "year": 2010}, {"title": "Constraints children place on word meanings", "author": ["E.M. Markman"], "venue": "Cognitive Science,", "citeRegEx": "Markman,? \\Q1990\\E", "shortCiteRegEx": "Markman", "year": 1990}, {"title": "Word learning emerges from the interaction of online referent selection and slow associative learning", "author": ["B. McMurray", "J.S. Horst", "L.K. Samuelson"], "venue": "Psychological review,", "citeRegEx": "McMurray et al\\.,? \\Q2012\\E", "shortCiteRegEx": "McMurray et al\\.", "year": 2012}, {"title": "Yarp: yet another robot platform", "author": ["G. Metta", "P. Fitzpatrick", "L. Natale"], "venue": "International Journal on Advanced Robotics Systems,", "citeRegEx": "Metta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Metta et al\\.", "year": 2006}, {"title": "Bayesian body schema estimation using tactile information obtained through coordinated random movements", "author": ["T. Mimura", "Y. Hagiwara", "T. Taniguchi", "T. Inamura"], "venue": "Advanced Robotics,", "citeRegEx": "Mimura et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mimura et al\\.", "year": 2017}, {"title": "Hierarchical action learning by instruction through interactive grounding of body parts and proto-actions", "author": ["M. Petit", "Y. Demiris"], "venue": "In Robotics and Automation (ICRA),", "citeRegEx": "Petit and Demiris,? \\Q2016\\E", "shortCiteRegEx": "Petit and Demiris", "year": 2016}, {"title": "Exploration behaviors, body representations, and simulation processes for the development of cognition in artificial agents", "author": ["G. Schillaci", "V.V. Hafner", "B. Lara"], "venue": "Frontiers in Robotics and AI,", "citeRegEx": "Schillaci et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schillaci et al\\.", "year": 2016}, {"title": "Evidence for multiple, distinct representations of the human body", "author": ["J. Schwoebel", "H.B. Coslett"], "venue": "Journal of cognitive neuroscience,", "citeRegEx": "Schwoebel and Coslett,? \\Q2005\\E", "shortCiteRegEx": "Schwoebel and Coslett", "year": 2005}, {"title": "Cross-situational learning: a mathematical approach", "author": ["K. Smith", "A.D. Smith", "R.A. Blythe", "P. Vogt"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "Smith et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2006}, {"title": "Estimating number of components in gaussian mixture model using combination of greedy and merging algorithm", "author": ["K. \u0160tep\u00e1nov\u00e1", "M. Vavre\u010dka"], "venue": "Pattern Analysis and Applications,", "citeRegEx": "\u0160tep\u00e1nov\u00e1 and Vavre\u010dka,? \\Q2016\\E", "shortCiteRegEx": "\u0160tep\u00e1nov\u00e1 and Vavre\u010dka", "year": 2016}, {"title": "Segmenting the body into parts: evidence from biases in tactile perception", "author": ["Vignemont", "d. F", "A. Majid", "C. Jola", "P. Haggard"], "venue": null, "citeRegEx": "Vignemont et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Vignemont et al\\.", "year": 2009}, {"title": "Word learning as bayesian inference", "author": ["F. Xu", "J.B. Tenenbaum"], "venue": "Psychological review,", "citeRegEx": "Xu and Tenenbaum,? \\Q2007\\E", "shortCiteRegEx": "Xu and Tenenbaum", "year": 2007}, {"title": "Modeling crosssituational word\u2013referent learning: Prior questions", "author": ["C. Yu", "L.B. Smith"], "venue": "Psychological review,", "citeRegEx": "Yu and Smith,? \\Q2012\\E", "shortCiteRegEx": "Yu and Smith", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Spurred by the account of Head and Holmes (1911) and their proposal of superficial and postural schema, a number of different concepts has been proposed since: body schema, body image, and corporeal schema being only some of them.", "startOffset": 26, "endOffset": 49}, {"referenceID": 3, "context": "Spurred by the account of Head and Holmes (1911) and their proposal of superficial and postural schema, a number of different concepts has been proposed since: body schema, body image, and corporeal schema being only some of them. Body schema is usually thought of as more \u201clow-level\u201d, sensorimotor representation of the body used for action. Body image is an umbrella term uniting higher level representations, for perception more than for action, and accessible to consciousness. Schwoebel and Coslett (2005) amassed evidence for distinguishing between three types of body representations: body schema, body structural description, and body semantics\u2014constituting a kind of hierarchy.", "startOffset": 26, "endOffset": 511}, {"referenceID": 18, "context": "Vignemont et al. (2009) focused on how body segmentation between hand and arm could appear based on a combined tactile and visual perception.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Interestingly, recent research (Majid, 2010) showed that there are some cross-linguistic variabilities in naming body parts and this may in turn override or influence the \u201cbottom-up\u201d multimodal (nonlinguistic) body part categorization.", "startOffset": 31, "endOffset": 44}, {"referenceID": 5, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots.", "startOffset": 67, "endOffset": 114}, {"referenceID": 14, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots.", "startOffset": 67, "endOffset": 114}, {"referenceID": 6, "context": "Our own work on the iCub humanoid robot has thus far focused on learning primary representations\u2014tactile (Hoffmann et al., 2017) and proprioceptive (Hoffmann and Bednarova, 2016).", "startOffset": 105, "endOffset": 128}, {"referenceID": 4, "context": ", 2017) and proprioceptive (Hoffmann and Bednarova, 2016).", "startOffset": 27, "endOffset": 57}, {"referenceID": 4, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots. Petit and Demiris (2016) developed an algorithm for the iCub humanoid robot to associate labels for body parts and later proto-actions with their embodied counterparts.", "startOffset": 68, "endOffset": 178}, {"referenceID": 4, "context": "Here, computational and in particular robotic modeling ties in\u2014see (Hoffmann et al., 2010; Schillaci et al., 2016) for surveys on body schema in robots. Petit and Demiris (2016) developed an algorithm for the iCub humanoid robot to associate labels for body parts and later proto-actions with their embodied counterparts. These could then be recombined in a hierarchical fashion (e.g., \u201cclose hand\u201d consists of folding individual fingers). Mimura et al. (2017) used Dirichlet process Gaussian mixture model with latent joint to provide a Bayesian body schema estimation based on tactile information.", "startOffset": 68, "endOffset": 461}, {"referenceID": 0, "context": "Borghi et al. (2004), for example, studied the interaction of object names with situated action on the same objects.", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "We made use of a newly proposed sequential mapping algorithm which extends an idea of one-step mapping (Smith et al., 2006) and compared its overall accuracy to one-step mapping as well as to accuracies of segmenting individual body parts.", "startOffset": 103, "endOffset": 123}, {"referenceID": 6, "context": "In particular, the \u201ctactile homunculus\u201d (Hoffmann et al., 2017)\u2014a primary representation of the artificial sensitive skin the robot is covered with (see Fig.", "startOffset": 40, "endOffset": 63}, {"referenceID": 11, "context": "We created a YARP (Metta et al., 2006) software module to generate virtual skin contacts1.", "startOffset": 18, "endOffset": 38}, {"referenceID": 6, "context": "The input layer of the \u201ctactile homunculus\u201d (Hoffmann et al., 2017) consists of a vector, a(t), of activations of 1154 taxels at time t\u2014the output of the previous section\u2014that have binary values (1 when a taxel is stimulated, 0 otherwise).", "startOffset": 44, "endOffset": 67}, {"referenceID": 2, "context": "Mixture of Gaussians is trained by the EM algorithm (Dempster et al., 1977).", "startOffset": 52, "endOffset": 75}, {"referenceID": 17, "context": "In future, we plan to use an adaptive extension of GMM algorithm such as gmGMM (\u0160tep\u00e1nov\u00e1 and Vavre\u010dka, 2016) to detect this number autonomously.", "startOffset": 79, "endOffset": 109}, {"referenceID": 7, "context": "To process this data, we made use of CMU Sphinx (an open-source flexible Markov model-based speech recognizer system) (Lamere et al., 2003) and achieved 100% accuracy of word recognition.", "startOffset": 118, "endOffset": 139}, {"referenceID": 16, "context": "One possible way how to establish mapping between sensorimotor concepts and linguistic elements is to use frequencies of referent and meaning co-occurrences, that is, the ones with the highest co-occurrence are mapped together (Smith et al., 2006; Xu and Tenenbaum, 2007).", "startOffset": 227, "endOffset": 271}, {"referenceID": 19, "context": "One possible way how to establish mapping between sensorimotor concepts and linguistic elements is to use frequencies of referent and meaning co-occurrences, that is, the ones with the highest co-occurrence are mapped together (Smith et al., 2006; Xu and Tenenbaum, 2007).", "startOffset": 227, "endOffset": 271}, {"referenceID": 20, "context": "This allows the learner to subsequently choose the most strongly associated referent (Yu and Smith, 2012).", "startOffset": 85, "endOffset": 105}, {"referenceID": 6, "context": "Arrows illustrate the relationship in orientation between skin parts and the learned map (Hoffmann et al., 2017).", "startOffset": 89, "endOffset": 112}, {"referenceID": 20, "context": "To extend this basic idea, we can enable also forgetting by introducing a parameter \u03b7, which can capture the memory decay (Yu and Smith, 2012).", "startOffset": 122, "endOffset": 142}, {"referenceID": 9, "context": "Thanks to this mechanism, mutual exclusivity principle (the fact that children prefer mapping where object has only one label to multiple labels (Markman, 1990)) is guaranteed.", "startOffset": 145, "endOffset": 160}, {"referenceID": 9, "context": "The inhibitory mechanisms and situationtime dynamics were already partially included into the model of cross-situational learning proposed by McMurray et al. (2012). Even though our model shares some similarities with the model proposed by McMurray, it stems from different computational mechanisms.", "startOffset": 142, "endOffset": 165}, {"referenceID": 18, "context": "However, more realistic, non-uniform touch and, in particular, the addition of additional modalities (proprioception, vision) should enable bottom-up non-linguistic body part category formation, as described by (Vignemont et al., 2009), for example.", "startOffset": 211, "endOffset": 235}, {"referenceID": 1, "context": "Further study of the brain areas involved in this processing is needed, in order to develop models more closely inspired by the functional cortical networks, like in (Caligiore et al., 2010) that model the experimental findings of (Borghi et al.", "startOffset": 166, "endOffset": 190}, {"referenceID": 0, "context": ", 2010) that model the experimental findings of (Borghi et al., 2004).", "startOffset": 48, "endOffset": 69}], "year": 2017, "abstractText": "Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities. At the same time, they form more compressed representations like concepts or symbols. In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established. There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning). In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences. In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation. For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations. A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso). With the goal at arriving at the correct \u201cbody categories\u201d, we demonstrate how a sequential mapping algorithm outperforms one-step mapping. In addition, the effect of data set size and noise in the linguistic input are studied.", "creator": "LaTeX with hyperref package"}}}