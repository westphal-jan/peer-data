{"id": "1706.02686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2017", "title": "What Does a Belief Function Believe In ?", "abstract": "logical conditioning in the dempster - wagner theory # ai has been defined ( by implementing \\ math { 86 : 90 } while verification of a belief function and performs an \" event \" dependent threshold authentication.", "histories": [["v1", "Thu, 8 Jun 2017 17:17:23 GMT  (14kb)", "http://arxiv.org/abs/1706.02686v1", "13 pages"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["andrzej matuszewski", "mieczys{\\l}aw a k{\\l}opotek"], "accepted": false, "id": "1706.02686"}, "pdf": {"name": "1706.02686.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["lAW A. K lOPOTEK"], "emails": ["klopotek@ipipan.waw.pl"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n02 68\nThe conditioning in the Dempster-Shafer Theory of Evidence has been defined (by Shafer [15] as combination of a belief function and of an \u201devent\u201d via Dempster rule. On the other hand Shafer [15] gives a \u201dprobabilistic\u201d interpretation of a belief function (hence indirectly its derivation from a sample). Given the fact that conditional probability distribution of a sample-derived probability distribution is a probability distribution derived from a subsample (selected on the grounds of a conditioning event), the paper investigates the empirical nature of the Dempster- rule of combination. It is demonstrated that the so-called \u201dconditional\u201d belief function is not a belief function given an event but rather a belief function given manipulation of original empirical data. Given this, an interpretation of belief function different from that of Shafer is proposed. Algorithms for construction of belief networks from data are derived for this interpretation."}, {"heading": "1 Introduction", "text": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol. 1990:4 No. 5/6 and Vol. 1992:6 No.3; see also [6], [4].\nThis paper is intended to shed some light onto the dispute over adequacy of the DST from the technical point of view. The authors of this paper have been engaged in a project having as its goal the implementation of an expert system dealing with uncertainty via DST methodology mixed with the Bayesian approach [7]. Knowledge is represented by a belief network to enable application of the reasoning system based on the work of Shenoy and Shafer [16] (Shenoy-Shafer\u2019s axiomatic framework encompasses both DST and Bayesian-like reasoning scheme). It has been an ultimate goal of the developers of that expert system to support also knowledge acquisition from data. Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18]\nAs \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data. However, as the above-mentioned discussion in IJAR demonstrates, the relationship between empirical frequencies and DS belief functions seems to be far from being clear.\nWe agree with Smets [17] that fundamental deviation of DST from any probabilistic measure of uncertainty lies in the DS rule of combination (\u2295) which serves as a way of conditioning the overall DS distribution on some event (see [15]).\nIn this paper we assume that DST notions like basic probability assignment or mass function m, belief function Bel, pseudo-mass and pseudo-belief functions, combination \u2295, marginalization \u2193 and empty extension \u2191 operations are well understood [14], [16]."}, {"heading": "2 Nature of Conditioning in DST", "text": "Let us consider for a moment how \u201dempirical\u201d conditioning may be viewed in the probability theory. Let a probability distribution be defined as relative frequency over a (large) population. Let us want to condition on an event, say {\u03c9|\u03b1(\u03c9)} where \u03b1 is a predicate - a logical expression in variables describing the population. Then we select all the objects \u03c9 of the population which match the predicate \u03b1(\u03c9) and the conditional distribution P (.|\u03b1) will be the relative frequencies within this subpopulation.\nLet us try to do the same with DS Bel\u2019s. Following Shafer [15] we may be tempted to interpret a valuation of an object \u03c9 of a population with a set A as a statement that our variable of interest takes for this object one of the values mentioned in A, but we do not know which one of them (and are not ready even to select any proper subset of A). A will be treated as our most specific commitment to the value of the variable. Under these circumstances, the basic probability assignment function m may be understood as the probability distribution (read: relative frequencies) of such commitments within our population. This is, in fact, the way as the \u201dgeneralized probability\u201d in [6], or families of probability distributions in [10] may be understood. Now let us go over to conditioning on an event {\u03c9|\u03b1(\u03c9)} (after [15]). It is the beauty of the DST that there exists always a set B that corresponds exactly to such an event. If BelB is the simple support function capturing the evidence of the set B (that is mB(B) = 1, mB(A) = 0 for any A 6= B, [15]) then by definition Bel(.|B) = Bel \u2295 BelB is the belief of Bel conditioned on the event B [15]. But how does this definition \u201drun\u201d on a population of objects ? It has been demonstrated [8] that we can view it the following way: We take the predicate \u03b1\nand check the population object by object. Some of them deny the predicate. We reject them as the frequentist model of conditional probability does. Some of them meet the predicate - and we select them for our subpopulation - as conditional probability model does. But there are some objects for which we are actually unable to decide whether they meet the predicate or not (as our commitment is not specific enough). And what do we have to do in this case to meet numerically the DS rule of combination ? We have to accept them ! But (Alas!) this is not enough. We have to change our commitment - we have to make our commitment for a particular object more specific so that it meets the predicate \u03b1. So even if our commitment to the value of the attribute for this object may have been correct prior to conditioning (that is the variable of interest took for the object one of the values mentioned in the prior commitment), it may be not correct after the conditioning. That is, after the conditioning we work with a subpopulation with partially incorrect valuations (not corresponding with empirical reality), and we combine evidence further ..... . (Classical!) probability theory does not do things like this - it assumes that measuring sequence has no impact on the value of a variable (Of course, there are several non-classical probability theories which took into account possibility of disagreement between various observations if the sequence of making observations is changed, but obviously most frequentist interpretations of DST didn\u2019t consider them).\nWe feel that this (essentially numerical) argument explains most of apparent contradictions derived from frequentist interpretations of the DST. But the DST will not be helped with if left with the impression of telling us lies about the population (we have to say, plausible lies, because by definition we are unable to check by observation if strengthening of a commitment for an object is in fact correct or not, because if we were able to carry out such an observation then our prior commitment would have been more specific). So instead of saying that the variable takes for the object one of the values in A, we could say that the variable is set-valued and takes for the object all the values in A (this would\nbe a kind of random set interpretation [11]). In this case conditioning could be viewed as rejecting some of the values of the object which are not of interest. Then after conditioning the object would have a commitment corresponding to the values it really takes, though some other values were ignored as not of interest. However, this view would contradict the usage of Bel\u2019s to represent material implication P (\u03c9) \u2192 Q(\u03c9) in form of a set {(P (\u03c9), Q(\u03c9)), (\u00acP (\u03c9), Q(\u03c9)), (\u00acP (\u03c9),\u00acQ(\u03c9))} because it would lead to the impression that at the same time P (\u03c9) \u2227Q(\u03c9) and \u00acP (\u03c9) \u2227 Q(\u03c9) hold which is counterintuitive as P (\u03c9) \u2227 \u00acP (\u03c9) = FALSE."}, {"heading": "3 An Alternative View of DST", "text": "Hence an alternative view of DST is required. One has been developed in [8]. We present it here informally. Instead of saying that the set A expresses that \u201dthe variable of interest takes one of the values in A\u201d as well as instead of saying that the set A expresses that \u201dthe variable of interest takes all of the values in A\u201d a compromise is proposed: it is assumed that the variable cannot be observed directly, but only via some measurement procedure (with some special properties ensuring consistence), and the set A expresses that \u201dthe measurement procedure yielded TRUE when testing if X = ai (X - the variable of interest) for all the ai \u2208 A and for no ai 6\u2208 A\u201d. If we make conditioning, the objects are labeled, and the measurement method takes into account the labeling of objects by refraining from carrying out tests on variable values outside of the label. In this way the following is achieved:\n\u2022 before and after every conditioning the interpretation of the commitment A is the\nsame for not rejected objects: \u201dthe measurement procedure yielded TRUE when testing if X = ai (X - the variable of interest) for all the ai \u2208 A and for no ai 6\u2208 A\u201d.\n\u2022 the impact of conditioning onto measurement results is taken into account - via\nlabeling\n\u2022 any logical contradictions resulting from random set interpretation are avoided:\nwe do not say \u201dX takes the value\u201d but that \u201dX has been measured to be\u201d, and contradictions resolve in imprecision of measurement method.\nThe importance of such an interpretation is not to be underestimated: a way is paved\ntowards experimental studies of populations with DS belief distributions.\nTo demonstrate this a development of a method of a tree/polytree factorization of a joint DS belief distribution for purposes of Shenoy/Shafer uncertainty propagation [16] is briefly outlined.\nWe define mk-conditional belief function BelX|Xi(A) as any pseudo-belief function solving the equation Bel = Bel\u2193Xi \u2295 BelX|Xi Notice, that in general this equation has no unique solution, and a solution being a proper belief function does not always exist.\nA DS Belief network be [9] a pair (D,Bel) where D is a dag (directed acyclic graph) and Bel is a DS belief distribution called the underlying distribution. Each node i in D corresponds to a variable Xi in Bel, a set of nodes I corresponds to a set of variables XI and xi, xI denote values drawn from the domain of Xi and from the (cross product) domain of XI respectively. Each node in the network is regarded as a storage cell for any distribution Bel\u2193{Xi}\u222aX\u03c0(i)|X\u03c0(i) where X\u03c0(i) is a set of nodes corresponding to the parent nodes \u03c0(i) of i. The underlying distribution represented by a DS belief network is computed via:\nBel = n\u2295\ni=1\nBel\u2193{Xi}\u222aX\u03c0(i)|X\u03c0(i)\nLet, after [5] I(J,K|L)D denote d-separation of J from K by L in a directed acyclic graph D, where J,K and L are three disjoint sets of nodes in this dag D. We shall then define [9]\nIf XJ , XK , XL are three disjoint sets of variables of a distribution Bel, then XJ , XK\nare said to be conditionally independent given XL (denoted I(XJ , XK |XL)Bel iff\nBel\u2193XJ\u222aXK\u222aXL|XL \u2295Bel\u2193XL = Bel\u2193XJ\u222aXL|XL \u2295 Bel\u2193XK\u222aXL|XL \u2295 Bel\u2193XL\nI(XJ , XK |XL)Bel is called a (conditional independence) statement\nTHEOREM 1 [9] Let BelD = {Bel|(D,Bel) is a DS belief network}. Then:\nI(J,K|L)D\niff\nI(XJ , XK |XL)Bel\nfor all Bel \u2208 BelD.\nMany authors have connected causality with the notion of statistical dependence or non-independence. We parallel here [18] in formulating the following principles, while understanding independence as defined above\nLet Vbe a set of random DS variables with a joint DS-belief distribution. We say that variables X,Y \u2208Vare directly causally dependent if and only if there is a causal dependency between X,Y (either the value of X influences the value of Y or the value of Y influences the value of X or the value of a third variable not in Vinfluences the values of both X and Y) that does not involve any other variable in V.\nPrinciple I: For all X,Y in V, X and Y are directly causally dependent if and only if for every subset Sof Vnot containing X or Y, X and Y are not statistically independent conditional on S.\nWe say that B is directly causally dependent on A provided that A and B are causally\ndependent and the direction of causal influence is from A to B.\nPrinciple II: if A and B are directly causally dependent and B and C are directly causally dependent, but A and C are not, then: B is causally dependent on A, and B is causally dependent on C if and only if A and C are statistically dependent conditional on any set of variables containing B and not containing A or C.\nPrinciple III: A directed acyclic graph represents a DS-belief distribution on the\nvariables that are vertices of the graph if and only if for all vertices X,Y and all sets Sof vertices in the graph (X,Y /\u2208 S), Sd-separates X and Y if and only if X and Y are independent conditional on S.\nTHEOREM 2 [9] Let Bel be a DS-belief distribution represented by an acyclic directed graph G according to Principle III. Then G is an orientation (G has the undirected structure) of the undirected graph U that represents Bel according to Principle I.\nTHEOREM 3 [9] Principle III implies Principle II.\nTHEOREM 4 [9] Let \u0393 be the set of directed graphs that represent DS-belief distribution Bel according to Principle III. Then \u0393 is also the set of directed graphs obtained from P by Principles I and II."}, {"heading": "4 Belief Networks from Data under New Interpreta-", "text": "tion\nBased on these purely theoretical considerations it was tried to develop some practical algorithms for recovery of belief network structure from data for some limited classes of belief networks. It was started with the most successful structures of Bayesian networks: the tree and the polytree structures. In these efforts, corresponding Bayesian algorithms\nwere exploited as general frameworks, though details had to be elaborated anew. It is also worth mentioning, that, unlike in probabilistic case, a randomized generation of a belief function possessing given belief network structure is not a trivial task due to the data-changing nature of DS combination.\nLet us present briefly these new algorithms:\nThe algorithm of Chow and Liu [1] for recovery of tree structure of a probability distribution is well known and has been deeply investigated, so we will omit its description in this paper. To accommodate it for the needs of DST one needs to introduce a definition of distance between variables. Regrettably, no such definition having the nice properties of the Chow and Liu exists, so a similar one has been elaborated: Let p be a mass function and x be a pseudo-mass function. Let f(x; p) = \u2211\nA;p(A)>0 p(A) \u00b7 ln x(A), where the\nassumption is made that natural logarithm of a non-positive number is minus infinity. The values of f in variable x with parameter p have range:(\u2212\u221e, f(p; p)].Let g(x; p) = f(x;p) f(p;p) .The values of g in variable x with parameter p range:[1,+\u221e).Let a(x; p) = e1\u2212g(x;p). The values of a in variable x with parameter p range:[0, 1]\nBy the ternary joint distribution of the variables X1, X2 with background X3 we un-\nderstand the function:\nm\u2193X1\u00d7X2[X3] =\n= (m\u2193X1\u00d7X3|X3 \u2295m\u2193X2\u00d7X3|X3 \u2295m\u2193X3)\u2193X1\u00d7X2\nBy the distance (for use with Chow/Liu algorithm) DEP (X1, X2) we understand the\nfunction:\nDEP0DS(X1, X2) = 1\u2212max(a(m \u2193X1 \u2295m\u2193X2 ;m\u2193X1\u00d7X2), max\nX3;X3\u2208V\u2212X1,X2\na(m\u2193X1\u00d7X2[X3];m\u2193X1\u00d7X2))\nwith V being the set of all variables.\nFor randomly generated tree-like DS belief distributions, if we were working directly with these distributions, as expected, the algorithm yielded perfect decomposition into the original tree. For random samples generated from such distributions, the structure was recovered properly for reasonable sample sizes (200 for up to 8 variables). Recovery of the joint distribution was not too perfect, as the space of possible value combination is tremendous and probably quite large sample sizes would be necessary. It is worth mentioning, that even with some departures from truly tree structure a distribution could be obtained which reasonable approximated the original one.\nA well known algorithm for recovery of polytree from data for probability distributions is that of Pearl [12], [13], we refrain from describing it here. To accommodate it for usage with DS belief distributions we had to change the dependence criterion of two variables given a third one.\nCriterion(X1 \u2192 X3, X2 \u2192 X3) = (1\u2212 a(m \u2193X1\u00d7X2[X3];m\u2193X1\u00d7X2))\u2212\n\u2212(1\u2212 a(m\u2193X1 \u2295m\u2193X2 ;m\u2193X1\u00d7X2))\nIf the above function Criterion is positive, we assume head-to-head meeting of edges X1, X3 and X2, X3. The rest of the algorithm runs as that of Pearl.\nFor randomly generated polytree-like DS belief distributions, if we were working directly with these distributions, as expected, the algorithm yielded perfect decomposition into the original polytree. For random samples generated from such distributions, the structure was recovered properly only for very large sample sizes (5000 for 6 variables),\nwith growing sample sizes leading to spurious indications of head-to-head meetings not present in the original distribution. Recovery of the joint distribution was also not too perfect, due to immense size of space of possible value combinations.\nOther distance and dependence measures than those mentioned above have been tried\nbut no clear winner could have been decided so far.\nThough we are still far away from our goal of developing an efficient algorithm for recovery of general DS belief network structure from empirical data, our efforts demonstrated, that there exists at least one way of connecting the formalism of the DempsterShafer Theory with frequencies from empirical data (though this may not be the one the creators of this theory had in mind). At the same time our view of the nature of the DS belief functions was shifted from traditional frequentist view to one with accepting changing valuation of objects while running the conditioning process. This proved helpful when comparing results of reasoning of the inference engine with the empirical distribution given by data.One of the consequences of this changing valuation of data during a reasoning process is the crucial difference between probabilistic and DS belief networks: In probabilistic networks the conditioning of a whole distribution on a set of variables has exactly the same meaning as conditionality contained in a node of a network. That is, if the variable Xn represented by a node n depends on the set of variables X\u03c0(n) then if we calculate the conditional probability P (Xn|X\u03c0(n) on a whole network e.g. via Shenoy/Shafer algorithm [16], then the result will be exactly the same as is the valuation attached to the node n of the network. The situation is entirely different in case of DS networks: the Shaferian Bel(Xn|X\u03c0(n)) calculated from the overall network is (and usually must be) in general distinct from the valuation (mk-conditioning) Bel\u2193{Xn}\u222aX\u03c0(n)|X\u03c0(n) we attach to a node of the network. Clearly, the Shenoy/Shafer uncertainty propagation algorithm [16] is fully unaffected by the lack of identity between these two notions of conditioning,\nand in fact a node valuation neither in probabilistic nor in DS case is required to have anything to do with any notion of conditionality. But attachment of conditionality to a node of a belief network is important for understanding the contents of a belief network which was invented as a means of representing causal dependencies [18]. Our notion of mk-conditionality Bel\u2193{Xn}\u222aX\u03c0(n)|X\u03c0(n) gives a node in a DS belief network a local meaning: it can be estimated from data using only variables engaged, that is {Xn}\u222aX\u03c0(n). Notably, this does not hold for the general view of belief networks (that is without reference to conditionality) presented by Shenoy and Shafer [16].To verify the validity of valuation of any node of a general form hypertree considered in [16] one may be forced to consider the entire hypertree at once."}], "references": [{"title": "Approximating discrete probability distributions with dependence trees", "author": ["C.K. Chow", "C.N. Liu"], "venue": "IEEE Transactions on Information Theory, Vol. IT-14,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1968}, {"title": "Herskovits: A Bayesian method for the induction of probabilistic networks from data, Machine Learning", "author": ["E.G.F. Cooper"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Upper and lower probabilities induced by a multi-valued mapping", "author": ["A.P. Dempster"], "venue": "Ann. Math. Stat", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1967}, {"title": "Halpern: Uncertainty, belief, and probability", "author": ["J.Y.R. Fagin"], "venue": "Comput. Intell", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "Pearl: d-Separation: From theorems to algorithms, in : Uncertainty in Artificial Intelligence 5 (M.Henrion, R.D.Shachter, L.N.Kamal and J.F.Lemmer Eds)", "author": ["D. Geiger", "J.T. Verma"], "venue": "Elsevier Science Publishers B.V. (North-Holland),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Two views of belief: belief as generalized probability and belief as evidence,Artificial Intelligence", "author": ["J.Y. Halpern", "R. Fagin"], "venue": "WHAT DOES A BELIEF FUNCTION BELIEVE IN", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Bayesian and non-Bayesian evidential updating", "author": ["H.E. Kyburg Jr."], "venue": "Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1987}, {"title": "On random sets and belief functions", "author": ["H.T. Nguyen"], "venue": "J. Math. Anal. Appl", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1978}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Influence", "author": ["J. Pearl"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Pearl: The recovery of causal poly-trees from statistical data, [in] Uncertainty in Artificial Intelligence", "author": ["J.G. Rebane"], "venue": "Elsevier Science Publishers B.V.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "A Mathematical Theory of Evidence", "author": ["G. Shafer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1976}, {"title": "Pearl eds: Readings in Uncertain Reasoning, (Morgan Kaufmann Publishers Inc", "author": ["G. Shafer"], "venue": "Belief Functions. Introduction,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1990}, {"title": "Axioms for probability and belief-function propagation", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "Uncertainty in Artificial Intelligence", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1990}, {"title": "Resolving misunderstandings about belief functions, International Journal of Approximate Reasoning 1992:6:321-344", "author": ["Ph. Smets"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Causality from probability, [w:] G.McKee (Ed.): Evolving knowledge in natural and artificial intelligence", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1990}], "referenceMentions": [{"referenceID": 11, "context": "The conditioning in the Dempster-Shafer Theory of Evidence has been defined (by Shafer [15] as combination of a belief function and of an \u201devent\u201d via Dempster rule.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "On the other hand Shafer [15] gives a \u201dprobabilistic\u201d interpretation of a belief function (hence indirectly its derivation from a sample).", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "The Dempster-Shafer (DS) Theory (DST) or the Theory of Evidence [14], [3] is considered by many researchers as an appropriate tool to represent various aspects of human dealing with uncertain knowledge, especially for representation of partial ignorance [17], though this view has been challenged by various authors (compare the presentations and discussions in International Journal of Approximate Reasoning (IJAR), special issues in Vol.", "startOffset": 254, "endOffset": 258}, {"referenceID": 5, "context": "3; see also [6], [4].", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "3; see also [6], [4].", "startOffset": 17, "endOffset": 20}, {"referenceID": 12, "context": "Knowledge is represented by a belief network to enable application of the reasoning system based on the work of Shenoy and Shafer [16] (Shenoy-Shafer\u2019s axiomatic framework encompasses both DST and Bayesian-like reasoning scheme).", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 197, "endOffset": 200}, {"referenceID": 9, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 213, "endOffset": 217}, {"referenceID": 1, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 262, "endOffset": 265}, {"referenceID": 14, "context": "Literature provides with many methods of recovery of Bayesian belief network from data (and in certain cases from additional appropriate hints of an expert), if the belief network should be a tree [1], a polytree [13], or a general-type (usually sparse) network [2], [18] As \u201dgeneralized probability\u201d is a term frequently used to characterize the DS belief function, it seems plausible to try to generalize Bayesian methods onto recovery of Dempster-Shafer belief networks from data.", "startOffset": 267, "endOffset": 271}, {"referenceID": 13, "context": "We agree with Smets [17] that fundamental deviation of DST from any probabilistic measure of uncertainty lies in the DS rule of combination (\u2295) which serves as a way of conditioning the overall DS distribution on some event (see [15]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "We agree with Smets [17] that fundamental deviation of DST from any probabilistic measure of uncertainty lies in the DS rule of combination (\u2295) which serves as a way of conditioning the overall DS distribution on some event (see [15]).", "startOffset": 229, "endOffset": 233}, {"referenceID": 10, "context": "In this paper we assume that DST notions like basic probability assignment or mass function m, belief function Bel, pseudo-mass and pseudo-belief functions, combination \u2295, marginalization \u2193 and empty extension \u2191 operations are well understood [14], [16].", "startOffset": 243, "endOffset": 247}, {"referenceID": 12, "context": "In this paper we assume that DST notions like basic probability assignment or mass function m, belief function Bel, pseudo-mass and pseudo-belief functions, combination \u2295, marginalization \u2193 and empty extension \u2191 operations are well understood [14], [16].", "startOffset": 249, "endOffset": 253}, {"referenceID": 11, "context": "Following Shafer [15] we may be tempted to interpret a valuation of an object \u03c9 of a population with a set A as a statement that our variable of interest takes for this object one of the values mentioned in A, but we do not know which one of them (and are not ready even to select any proper subset of A).", "startOffset": 17, "endOffset": 21}, {"referenceID": 5, "context": "This is, in fact, the way as the \u201dgeneralized probability\u201d in [6], or families of probability distributions in [10] may be understood.", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "This is, in fact, the way as the \u201dgeneralized probability\u201d in [6], or families of probability distributions in [10] may be understood.", "startOffset": 111, "endOffset": 115}, {"referenceID": 11, "context": "Now let us go over to conditioning on an event {\u03c9|\u03b1(\u03c9)} (after [15]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "If BelB is the simple support function capturing the evidence of the set B (that is mB(B) = 1, mB(A) = 0 for any A 6= B, [15]) then by definition Bel(.", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "|B) = Bel \u2295 BelB is the belief of Bel conditioned on the event B [15].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "be a kind of random set interpretation [11]).", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "To demonstrate this a development of a method of a tree/polytree factorization of a joint DS belief distribution for purposes of Shenoy/Shafer uncertainty propagation [16] is briefly outlined.", "startOffset": 167, "endOffset": 171}, {"referenceID": 4, "context": "i=1 Beli\u03c0(i)\u03c0(i) Let, after [5] I(J,K|L)D denote d-separation of J from K by L in a directed acyclic graph D, where J,K and L are three disjoint sets of nodes in this dag D.", "startOffset": 28, "endOffset": 31}, {"referenceID": 14, "context": "We parallel here [18] in formulating the following principles, while understanding independence as defined above Let Vbe a set of random DS variables with a joint DS-belief distribution.", "startOffset": 17, "endOffset": 21}, {"referenceID": 0, "context": "The algorithm of Chow and Liu [1] for recovery of tree structure of a probability distribution is well known and has been deeply investigated, so we will omit its description in this paper.", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "The values of a in variable x with parameter p range:[0, 1] By the ternary joint distribution of the variables X1, X2 with background X3 we understand the function:", "startOffset": 53, "endOffset": 59}, {"referenceID": 8, "context": "A well known algorithm for recovery of polytree from data for probability distributions is that of Pearl [12], [13], we refrain from describing it here.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "A well known algorithm for recovery of polytree from data for probability distributions is that of Pearl [12], [13], we refrain from describing it here.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "via Shenoy/Shafer algorithm [16], then the result will be exactly the same as is the valuation attached to the node n of the network.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "Clearly, the Shenoy/Shafer uncertainty propagation algorithm [16] is fully unaffected by the lack of identity between these two notions of conditioning,", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "But attachment of conditionality to a node of a belief network is important for understanding the contents of a belief network which was invented as a means of representing causal dependencies [18].", "startOffset": 193, "endOffset": 197}, {"referenceID": 12, "context": "Notably, this does not hold for the general view of belief networks (that is without reference to conditionality) presented by Shenoy and Shafer [16].", "startOffset": 145, "endOffset": 149}, {"referenceID": 12, "context": "To verify the validity of valuation of any node of a general form hypertree considered in [16] one may be forced to consider the entire hypertree at once.", "startOffset": 90, "endOffset": 94}], "year": 2017, "abstractText": "The conditioning in the Dempster-Shafer Theory of Evidence has been defined (by Shafer [15] as combination of a belief function and of an \u201devent\u201d via Dempster rule. On the other hand Shafer [15] gives a \u201dprobabilistic\u201d interpretation of a belief function (hence indirectly its derivation from a sample). Given the fact that conditional probability distribution of a sample-derived probability distribution is a probability distribution derived from a subsample (selected on the grounds of a conditioning event), the paper investigates the empirical nature of the Dempsterrule of combination. It is demonstrated that the so-called \u201dconditional\u201d belief function is not a belief function given an event but rather a belief function given manipulation of original empirical data. Given this, an interpretation of belief function different from that of Shafer is proposed. Algorithms for construction of belief networks from data are derived for this interpretation. 2 ANDRZEJ MATUSZEWSKI, MIECZYS lAW A. K lOPOTEK", "creator": "dvips(k) 5.996 Copyright 2016 Radical Eye Software"}}}