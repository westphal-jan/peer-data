{"id": "1706.03196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2017", "title": "Online Learning for Neural Machine Translation Post-editing", "abstract": "transforming process editing languages introduced a revolution illuminating the field. nevertheless, post - editing the outputs of current system is mandatory for tasks requiring high translation endurance. post - editing offers a unique opportunity for comparing neural machine translation protocols, utilize various networking techniques and that the posts - editing translations presents new, practical training packages. colleagues review classical learning methods and examine adding new optimization algorithm. we thoroughly research online communication options for potential post - editing scenario. comparisons show significant improvements concerning translation quality and effort reduction.", "histories": [["v1", "Sat, 10 Jun 2017 07:41:22 GMT  (192kb,D)", "http://arxiv.org/abs/1706.03196v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["\\'alvaro peris", "luis cebri\\'an", "francisco casacuberta"], "accepted": false, "id": "1706.03196"}, "pdf": {"name": "1706.03196.pdf", "metadata": {"source": "CRF", "title": "Online Learning for Neural Machine Translation Post-editing", "authors": ["\u00c1lvaro Peris", "Luis Cebri\u00e1n", "Francisco Casacuberta"], "emails": ["fcn}@prhlt.upv.es", "luicebch@inf.upv.es"], "sections": [{"heading": null, "text": "1 Online Learning for Neural Machine Translation Post-editing\nNeural machine translation has meant a revolution of the field. Nevertheless, postediting the outputs of the system is mandatory for tasks requiring high translation quality. Post-editing offers a unique opportunity for improving neural machine translation systems, using online learning techniques and treating the post-edited translations as new, fresh training data. We review classical learning methods and propose a new optimization algorithm. We thoroughly compare online learning algorithms in a post-editing scenario. Results show significant improvements in translation quality and effort reduction."}, {"heading": "1 Introduction", "text": "In recent years, the statistical machine translation (SMT) field has evolved by leaps and bounds. Nevertheless, SMT systems still require human supervision in order to produce high-quality translations. This correction process is known as postediting. In the machine translation (MT) industry, the reduction of the post-editing effort has a great interest, as it leads to larger productivity (Arenas, 2008; Plitt and Masselot, 2010).\nThe text obtained during the post-editing process can be converted into new training data, useful for adapting the system to a different domain or to a changing environment.\nOnline learning (OL) techniques allow the system to learn from post-edited samples and, aiming to avoid the committed mistakes.\nThe application of OL to the classical phrasebased SMT systems has been thoroughly studied (Ortiz-Mart\u0131\u0301nez, 2016). Nevertheless, over the last years, the new neural machine translation\n(NMT) technology has shaken the machine translation field (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems.\nThe neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al. (2014). From there to now, NMT has surpassed the classical phrase-based systems in many tasks and currently is one the most active research topics (e.g. Erk and Smith (2016)). Furthermore, the translation industry is also shifting to this new translation paradigm (Crego et al., 2016; Wu et al., 2016).\nNMT relies on the encoder-decoder framework. An encoder recurrent neural network (RNN) processes the source sentence, computing a dense, continuous representation of it. From this representation, a decoder RNN generates, word by word, the translated sentence. In order to cope with the vanishing gradient problem (Bengio et al., 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014).\nThe vanilla encoder-decoder model has been steadily improved. The inclusion of attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015a) overcame the problem of processing long source sentences. The management of out-ofvocabulary words has also been explored (Jean et al., 2015; Luong et al., 2015b). Closely related to this, the NMT vocabulary size limitation has been tackled either developing strategies for taking into account large vocabularies (Jean et al., 2015) or shifting from word-level translations to character (Chung et al., 2016) or sub-word-level (Sennrich et al., 2016) translations.\nar X\niv :1\n70 6.\n03 19\n6v 1\n[ cs\n.L G\n] 1\n0 Ju\nn 20\n17\n2 Such NMT systems are usually trained from parallel corpora by means of stochastic gradient descent (SGD) (Robbins and Monro, 1951). SGD can be applied sample by sample, fitting into the OL paradigm. In this paper, we evaluate the most common SGD optimizers. Moreover, we propose a new SGD variant for NMT, based on passive-aggressive (PA) techniques (Crammer et al., 2006). This method aims to apply the minimum modification to the model (passiveness) required to satisfy a correctness criterion (aggressiveness). The contributions of this work are the following: \u2022 We study the application of OL to the NMT framework. To the best of our knowledge, this is the first work that applies OL to NMT in a post-editing scenario. \u2022 We present a new OL algorithm, inspired by PA techniques and implemented using subgradient methods. \u2022 We conduct a wide experimentation, testing the NMT engine in three different scenarios. We compare well-established learning algorithms together with the newly proposed PA algorithms. \u2022 Results show that OL algorithms are able to significantly reduced the post-editing effort required by an NMT system. Moreover, we demonstrate the capability of NMT for adapting to different domains by means of OL techniques. \u2022 In order to make research reproducible, we make public the source code of the NMT system and the training algorithms1. The rest of the paper is structured as follows: after this introduction, Section 2 reviews the related work. An NMT system is briefly described in Section 3. The classical SGD methods are revisited in Section 4. Section 5 details the newly proposed online algorithm. Section 6 presents the experimental framework designed to assess our method. Results are shown and discussed in Section 7. Finally, we conclude in Section 8, tracing future research lines."}, {"heading": "2 Related work", "text": "Online learning is a profusely studied topic in machine learning. In the field of SMT, OL techniques 1https://github.com/lvapeab/nmt-keras/ tree/interactive_NMT are mainly employed for tuning the weights of the log-linear model (Zens et al., 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010). More specifically, PA-based techniques, such as the margin infuse relaxed algorithm (MIRA) (Crammer and Singer, 2001) are especially well-suited when dealing with a large number of features. Therefore, MIRA is usually applied to models dealing with lots of sparse features (Watanabe et al., 2007; Chiang, 2012). Estimating the model features\u2014in addition to the weights of the log-linear model\u2014is a less explored application of OL techniques. Most relating this, were developed under the umbrella of the CasMaCat project (Alabau et al., 2013). As colophon of it, Ortiz-Mart\u0131\u0301nez (2016) developed an incremental derivation of the EM algorithm. Another application of OL techniques in SMT is the adaptation of a trained system to a postediting scenario. Mart\u0131\u0301nez-Go\u0301mez et al. (2012) and Mathur et al. (2013) used OL for adapting a SMT under a CAT framework. These works are close to ours in the scenarios on which the MT systems are applied. Nevertheless, in our work, we employ an NMT system."}, {"heading": "3 Background: NMT", "text": "In this work, we use an attentional NMT system similar to the one described by Bahdanau et al. (2015), but using LSTM networks. The input is a sequence of words x in the source language. Each word is linearly projected to a continuous space by means of an embedding matrix. The sequence of word embeddings feeds a bidirectional LSTM, which analyzes the input sequence in both directions, from left to right and vice versa. This network computes a sequence of annotations by concatenating the hidden states from the forward and backward layers. At each decoding timestep, an attention mechanism weights each element from the sequence of annotations, according to the previous decoding state, and computes a joint context vector. The decoder is another LSTM network that takes into account the context vector, the previously generated word and its previous hidden state. Finally, a deep output layer (Pascanu et al., 2014) is employed to compute a distribution over the target language vocabulary. At decoding time, the model approximates the target sentence with a beam-search\n3 method (Sutskever et al., 2014)."}, {"heading": "3.1 Training", "text": "To estimate the model parameters \u0398 (i.e. the weight matrices), the training objective is to minimize a loss function L\u0398, usually the minus loglikelihood over a bilingual parallel corpus T = {(xt,yt)}Tt=1, consisting of T source\u2013target sentence pairs (xt and yt, respectively), with respect to \u0398: \u0398\u0302 = arg min \u0398 L\u0398 = arg min \u0398 T\u2211 t=1 It\u2211 i=1 \u2212 log(p(yt | yt,<i,xt; \u0398)) (1) where It is the length of the t-th target sentence. SGD techniques are the predominant way of optimizing neural networks. The goal is to minimize a given loss function L\u0398, parametrized by the network parameters. SGD updates the parameters in the opposite direction of the gradient of L\u0398. The update size is controlled by a learning rate. According to when this updates occur, SGD techniques can be classified in batch, minibatch or online modes. Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014)."}, {"heading": "4 Online gradient descent", "text": "Online SGD updates the model parameters after each sample. Therefore, for a single sample (xt,yt) the parameters update is computed as: \u2206\u0398t = \u2212\u03c1\u2207L\u0398t(xt,yt) (2) where \u03c1 is the learning rate that controls the step size and \u2207L\u0398t is the gradient of the objective function L with respect to \u0398t. Most optimizers used in deep learning, are variants of this vanilla SGD. Therefore, they can also be applied under the OL framework. In this work, we compared Adagrad, Adadelta and Adam, whose update rules are briefly commented below. Adagrad (Duchi et al., 2011) aims to perform larger updates for infrequent parameters, defining its update rule as: \u2206\u0398t = \u2212 \u03c1\u221a Gt + \u2207L\u0398t(xt,yt) where Gt is a vector that contains the sum of the squares of the past gradients with respect to \u0398, is the element-wise multiplication, the division is also applied element-wise and is a small value, introduced for numerical stability reasons. Adadelta (Zeiler, 2012) is a less aggressive variant of Adadelta, which avoids its constant learning rate decay. Adadelta updates the parameters according to the root mean square (RMS) of the gradients and corrects these updates according to the RMS of the previous update: \u2206\u0398t = \u2212\u03c1 RMS(\u2206\u0398t\u22121) RMS(\u2207L\u0398t(xt,yt)) \u2207L\u0398t(xt,yt) The default value for \u03c1 is 1. Nevertheless, in an OL scenario, this value must be lowered, in order to perform less aggressive updates (see Section 7). Finally, Adam (Kingma and Ba, 2014) computes decaying averages for the past gradients (vt) and the past squared gradients (mt). Such averages are computed following: mt = \u03b21mt\u22121 + (1\u2212 \u03b21)\u2207L\u0398t(xt,yt) vt = \u03b22vt\u22121 + (1\u2212 \u03b22\u2207L2\u0398t(xt,yt)) where \u03b21 and \u03b22 are two constant values. The parameter update performed by Adam is: \u2206\u0398t = \u2212 \u03c1\u221a v\u0302t + m\u0302t where v\u0302t = vt1\u2212\u03b2t2 and m\u0302t = mt1\u2212\u03b2t1 . The divisions and the square root are computed elementwise and \u03b2t denotes \u03b2 powered to t."}, {"heading": "5 Passive-aggressive via subgradient techniques", "text": "We propose a new version of SGD, inspired by PA techniques such as MIRA (Crammer and Singer, 2001). The method aims to perform the minimum modification to the model parameters for compelling with a correctness criterion. Our criterion is directly based on the model loss function (Eq. (1)). Nevertheless, we could employ other loss functions, such as BLEU, although this could be costly. We declined towards this loss due to efficiency reasons. Let ht be the hypothesis generated by the NMT (using the current parameters \u0398t) of the source sentence xt. We consider that \u0398t is incorrect if\n4 the model assigns a lower probability to the target reference sentence yt than to ht: p\u0398t(yt | xt) < p\u0398t(ht | xt) (3) In this case, we want to search for a \u0398\u0302 such that \u0398\u0302 is close to \u0398t and p\u0398\u0302(yt | xt) > p\u0398\u0302(ht | xt). This can be expressed with the loss function `: `(\u0398\u0302,xt,yt,ht) = log p \u0398\u0302 (ht | xt)\u2212 log p\u0398\u0302(yt | xt) \u2264 \u03be (4) being \u03be \u2265 0 a slack variable, included for providing more flexibility to the method. This can be formulated as a minimization problem with constraints: \u0398\u0302 = arg min \u0398 1 2 \u2016\u0398\u2212\u0398t\u20162 + C \u03be s.t. `(\u0398,xt,yt,ht) \u2264 \u03be and \u03be \u2265 0 (5) where C is a parameter that controls the aggressiveness of the algorithm (Crammer et al., 2006). From Eq. (5), \u03be \u2265 max(0, `(\u0398,xt,yt,ht)). We define Ft as the function to optimize: Ft(\u0398,xt,yt,ht) = 1 2 \u2016\u0398\u2212\u0398t\u20162 + C max(0, `(\u0398,xt,yt,ht)) aiming to find the set of parameters that minimize this function, i.e.: \u0398\u0302 = arg min \u0398 Ft(\u0398,xt,yt,ht) (6) For obtaining \u0398\u0302, we use a subgradient method (Shor et al., 2003). Subgradient methods are iterative algorithms aimed to solve minimization problems with non-differentiable objective functions. Since our function has discontinuity points, such techniques are well-suited. Therefore, we iteratively apply the following weight update: \u2206\u0398t = \u2212\u03c1 \u2202\u0398Ft(\u0398,xt,yt,ht)|\u0398k (7) where \u03c1 is the learning rate and \u2202\u0398Ft is the subgradient of Ft with respect to \u0398. We initialize \u0398k=0 = \u0398t. This update is applied k times, until reaching some convergence criterion. We denote this passive-aggressive via subgradient methods update rule as PAS."}, {"heading": "5.1 Projected subgradient", "text": "An extension of the PAS method is the projected subgradient method (PPAS), in which the optimization problem is reformulated (Boyd et al., 2003). Let us define Gt(\u0398,xt,yt,ht) as max(0, `(\u0398,xt,yt,ht)). Then, Eq. (5) can be rewritten as: \u0398\u0302 = arg min \u0398 Gt(\u0398,xt,yt,ht)) s.t. \u2016\u0398\u2212\u0398t\u20162 \u2264 C (8) With this expression, we can compute the intermediate weight update: \u0398\u0304k+1 = \u0398k \u2212 \u03c1 \u2202\u0398Gt(\u0398,xt,yt,ht)|\u0398k As in the previous case, we initialize \u0398k=0 = \u0398t and iterate following some convergence criterion. The final weight update defined as: \u2206\u0398t = \u0398\u0304k+1 \u2212\u0398t \u2016\u0398\u0304k+1 \u2212\u0398t\u2016 C (9)"}, {"heading": "6 Experiments and results", "text": "In this section, we describe experimental setup used to conduct the experimentation. We define the assessment metrics, describe the corpora and detail the configuration of the NMT systems."}, {"heading": "6.1 Experimental framework", "text": "We tested our OL methods in a post-editing task. Within this scope we define three different scenarios: 1. We are post-editing samples from a different domain to that on which our system has been trained. 2. We are post-editing the outputs of a system trained on the same domain of our task, but we have available out-of-domain data. 3. We only have the in-domain data, either for training and testing. For each scenario, we adapt the system on-thefly, applying OL to the post-edited sentences from the test set. The final goal is to reduce the postediting effort required by upcoming samples. The main difference is that in the first case, we exclusively rely on the OL techniques for performing an adaptation of the system to the translation domain; while in the later cases, the NMT systems are already adapted. We measure up to what extent can OL refine this adapted system. Note that we work under a pure OL framework, i.e., each training sample is seen only once.\n5 In a real post-editing scenario, the system produces a translation hypothesis. The user reviews it and corrects the mistakes committed by the system. To test our proposals in this real post-editing scenario it would require real users, which makes it prohibitively costly. Therefore, we simulate the post-editing process by using the reference sentences as post-edited ones. In this simulation, the system generates a hypothesis for a given source sentence. Next, this hypothesis is corrected, producing a new training sample (i.e., the reference sentence). The system updates its parameters with this new sample and advances to the next sentence. This procedure is repeated until the translation process is finished."}, {"heading": "6.2 Metrics and evaluation", "text": "For approximating the post-editing effort, we used the translation edit rate (TER) (Snover et al., 2006). This metric computes the edit distance between hypotheses and reference sentences. The edit operations considered are addition, deletion, substitution and shifting. Moreover, we used BLEU (Papineni et al., 2002) and Meteor (Lavie and Denkowski, 2009) for assessing the translation quality of the systems. BLEU compares the ratio of n-gram structures shared between the system hypotheses and reference sentences, while Meteor computes the F1 score of precision and recall between hypotheses and references, additionally considering exact, stemmed, synonyms and paraphrase matches. For all results shown in this work, we compute 95% confidence intervals by means of bootstrap resampling (Koehn, 2004). The evaluation procedure is as follows: for a given source sentence, we produce its translation. This translation is stored for the posterior evaluation. Next, we take the post-edited sentence (i.e. the reference sentence) and update the NMT system with it. This is repeated until the full set of sentences are translated. We evaluate the stored hypotheses, hoping for a steady improvement as the translation process goes on."}, {"heading": "6.3 Corpora", "text": "As out-of-domain data (scenarios 1 and 2), we use the well-known, publicly available Europarl (Koehn, 2005) corpus, using the partition newstest2013 as development set. As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al., 2009) and TED (Federico et al., 2011) corpora. The domains of them are medical, printer manuals and TED talks, respectively. We used the standard partitions for all corpora. We used the English\u2013French language pair for all experiments. Table 1 shows the main figures of the corpora. We tokenized the text using the script from Moses, keeping sentences truecase. For all tasks, we use the joint byte pair encoding (BPE) algorithm for translating at a sub-word level, as described by Sennrich et al. (2016). We learned 32,000 merge operations. In addition to address the unknown word problem, sub-word-level translation allows us to naturally apply an NMT system from one domain to another. We extracted the BPE codes from the out-of-domain corpus (Europarl) and applied them to each in-domain corpus. Therefore, each corpus is segmented according to the out-of-domain corpus. By using this strategy, the vocabulary coverage is extremely high (more than 99.5% in all cases)."}, {"heading": "6.4 NMT systems", "text": "The NMT2 systems and OL algorithms were implemented with the Keras and Theano (Theano Development Team, 2016) libraries. For choosing the main hyperparameters of the system, we take advantage of the vast exploration made by Britz et al. (2017). Therefore, as described in Section 3, the system consists in an encoder-decoder LSTM network equipped with the attention mechanism described by Bahdanau et al. (2015). For practical reasons, we used single-layered LSTMs. The size of each LSTM is 512, as well as the word embedding size and the attention mechanism layer. As regularization methods, we applied layer normalization (Ba et al., 2016) and Gaussian noise to the weights (\u03c3 = 0.01) (Graves, 2011). We also early stopped the training if the BLEU on the development set did not improve in 100, 000 updates. The original NMT systems were trained using Adadelta (Zeiler, 2012), with the default parameters. The norm of the gradients was clipped to 1 in order to avoid the exploding gradient problem (Pascanu et al., 2012). The size of the beam was set to 6 in all experiments. Online learning hyperparameters were estimated on the Europarl validation set through a grid search, with \u03c1 and C taking the values 10\u2212a, a \u2208 2Implementation publicly available at: https: //github.com/lvapeab/nmt-keras/tree/ interactive_NMT\n6\nTable 2: Algorithm hyperparameters. \u03c1 refers to the learning rate andC to the aggressiveness of PA algorithms.\nSGD Adagrad Adadelta Adam PAS PPAS \u03c1 10\u22123 10\u22124 10\u22121 10\u22123 1 10\u22122 C \u2013 \u2013 \u2013 \u2013 10\u22122 10\u22122 {0,\u22121,\u22122,\u22123,\u22124,\u22125,\u22126}. We set = 10\u22128 and the rest of the Adam values to their default (\u03b21 = 0.9, \u03b22 = 0.999). Table 2 shows the value of the hyperparameters for each algorithm."}, {"heading": "7 Results and discussion", "text": "In this section, we show and analyze the experimentation conducted. We applied OL to the NMT system with the post-edited samples from the test set, with the goal of reducing the post-editing effort required by the following samples. As stated in Section 6.3, we pose three different scenarios: 1. We have a general corpus and no in-domain data. Therefore, in order to reduce the postediting effort, we must take advantage exclusively from the in-domain test samples by means of OL. 2. We have the general corpus, but we also have enough in-domain data for training a system. In this case, we first train with the out-ofdomain corpus and next with the in-domain corpus. OL can also fine-tune the resulting system, for obtaining a more tailored system and consequently, reducing the post-editing effort. 3. We only have in-domain corpora. This situation is more artificial than the previous ones, but illustrates up to which extent the NMT system can be improved and adapted to a test set by means of OL. Since the final scope of this work is a real application, we should check whether the OL algo-\nrithms consume an excessive amount of time. The mean update time3 was similar for each algorithm (approximately 65 ms), which is affordable."}, {"heading": "7.1 Adapting without in-domain", "text": "In this scenario, we assume that we only have an out-of-domain corpus. The text to translate comes from an unknown domain. Therefore, we must adapt the system on-the-fly, according to the postedited sentences: as soon as a sentence is translated and corrected, we modify our system, in order to take this sample into account. As baseline, we take an offline system, which remains static along the translation process. First, we study the evolution of the different learning algorithms. Fig. 1 shows the difference in terms of BLEU between the baseline and the online systems, for the Emea test set as a function of the number of processed sentences. We show the average of BLEU up to the n-th sentence. As we can see in Fig. 1, the vanilla SGD algorithm remained steady along the learning process. Adaptive algorithms (Adagrad, Adadelta, Adam) had a more unstable behavior at the early learning stages. This is because these algorithms rely on the accumulation of the past gradients for updating the model weights. Therefore, at the beginning, they present a chaotic behavior, but once they are stabilized, the learning improves. The Adam algorithm clearly exhibits this: it took 400 sentences to improve the baseline, but from here, the enhancement was acute, eventually yielding the best overall performance. On the other hand, PA algorithms remain close to the original system, which denotes an excessive passivity. Fig. 2 shows the overall differences between OL algorithms in terms of BLEU. The dashed line represents our baseline. All three adaptive SGD algorithms were able to significantly improve over 3Tested on a Nvidia GTX1080 GPU with CuDNN 5005.\n7\nthe baseline. Vanilla SGD also yielded small enhancements, although they were non-significant. PA-like algorithms obtained a similar performance to the baseline.\nFinally, Table 3 condenses the results of the adaptation without an in-domain corpus. For the sake of clarity, we only show results for the best performing algorithm for each task. In almost every case, the online system performed significantly better than the offline one. The Emea and TED tasks had a similar difficulty for the outof-domain system and the improvements obtained through OL were alike.\nThe NMT system performed much worse in the XRCE task. This is probably due to the corpus characteristics: since it is extracted from printer manuals, many of its structures are nonexistent in\nthe out-of-domain corpus. Therefore, the performance drops. OL also succeeded in adapting the model to this task, but its contribution was limited by the size of the test set: OL could adapt the model to the test set, but only to some extent."}, {"heading": "7.2 Adapting with an in-domain", "text": "In this case, we assume that we have a collection of in-domain data, in addition to our out-ofdomain corpus. Therefore, we first train a general system on the out-of-domain data and fine-tune it with the in-domain training set. Next, we study if OL can still improve the model, although it is already adapted to the in-domain task. As before, we apply OL techniques to the test set.\nFig. 3 shows the BLEU differences between OL algorithms. In this case, the behavior was more\n8\nstable for all algorithms. Although some variance can still be observed at the early stages of the process, the system learned faster and better than in the previous case. Since the effective learning started early, the cumulated enhancements were larger. From Fig. 3 we can also extract the importance of the adaptive SGD implementations, which make the learning process to converge faster and to a better point. Absolute BLEU improvement is shown in Fig. 4. Again, the dashed line represents the offline baseline. In this case, the gains obtained by the adaptive SGD algorithms are even higher than in the previous experiment. Adagrad, Adadelta and Adam yielded to almost the same value, producing improvements of more than 7 BLEU points with respect to the baseline. SG D A da gr ad A da de lt a A da m PA S P PA S 15 20 25 30 % B L E U\nFigure 4: Effect of OL in an NMT system trained on Europarl and Emea. We use the Emea test set for applying OL. We show BLEU and 95% confidence intervals. The horizontal line refers to the offline system baseline.\nTable 4 shows the overall results for this scenario. Again, we only show figures of the best method for each task. The use of in-domain training data allowed the NMT system to yield much better translation quality than in the previous case. Especially dramatics were the improvements in the TED and XRCE corpora (+10.4 and +26.4 BLEU points, respectively). For such tasks, the use of OL only affected the system to a negligible extent. On the other hand, the improvements observed in the Emea task were smaller than those obtained in other corpora. Nevertheless, this is clearly complemented by the online system. In this case, OL was able to adapt the system to the test set. The system was significantly improved. The postediting effort was reduced approximately by 15%.\nTable 4: Offline vs online systems, trained with out-of-domain data and adapted with additional in-domain data. Bold results indicate a significant improvement of the online system with respect to the offline. We also indicate the best performing algorithm for each task.\nEmea TED XRCE O ffl in e BLEU 21.3\u00b1 1.0 31.3\u00b1 1.0 34.7\u00b1 2.2 Meteor 39.5\u00b1 1.1 52.0\u00b1 1.0 52.2\u00b1 2.3 TER 66.9\u00b1 1.5 52.3\u00b1 1.0 55.0\u00b1 2.4 O nl in e BLEU 28.6\u00b1 1.2 31.4\u00b1 1.0 35.1\u00b1 2.2Meteor 48.8\u00b1 1.1 51.8\u00b1 0.9 53.0\u00b1 2.1 TER 57.0\u00b1 1.5 52.2\u00b1 1.0 55.8\u00b1 2.2 Algorithm Adam PPAS PPAS"}, {"heading": "7.3 Refining an NMT system", "text": "Finally, we move to the scenario in which we have enough in-domain data for training an NMT system. The system is trained exclusively on the indomain data. OL learning techniques can be used for refining the system at test time. Fig. 5 shows the evolution of the BLEU scores along the online learning process. Again, algorithms with adaptive learning rate perform better than the rest of them. Nevertheless, such differences are lower than in other tasks. The PAS algorithm is especially effective at the first iterations of the process. 0 200 400 600 800 1,000 0 5 10 Sentences % B L E U d iff er en ce Emea test SGD Adagrad Adadelta Adam PAS PPAS\nFigure 5: Evolution of a system trained on Europarl and Emea with the own Emea test set. We show the BLEU difference (averaged up to the nth sentence) with respect to an offline system.\nAll OL algorithms significantly outperform the offline baseline, as shown in Fig. 6. The performance of the PAS algorithm is comparable to\n9\nTable 5 collapses the performance of the systems in the different tasks.\nTable 5: Offline vs online systems, trained solely on the in-domain data. Bold results indicate a significant improvement of the online system with respect to the offline. We also indicate the best performing algorithm for each task.\nEmea TED XRCE O ffl in e BLEU 17.6\u00b1 0.9 27.1\u00b1 0.9 31.0\u00b1 2.2 Meteor 37.1\u00b1 0.9 47.6\u00b1 1.0 49.8\u00b1 2.0 TER 74.2\u00b1 2.2 57.9\u00b1 1.0 62.8\u00b1 2.7 O nl in e BLEU 22.4\u00b1 1.0 28.2\u00b1 0.9 36.7\u00b1 2.3Meteor 41.1\u00b1 0.9 49.1\u00b1 1.0 54.2\u00b1 2.0 TER 63.8\u00b1 1.4 56.0\u00b1 1.1 56.5\u00b1 2.6 Algorithm Adadelta Adadelta Adagrad Even when the systems have been exclusively trained on in-domain data, OL helps to perform a more fine-grained adaptation to the text to translate. In the Emea and XRCE tasks we could observe significant improvements (approximately 5 points of BLEU). This is probably due to regularities in the test set, which allow to exploit OL to its full. OL also improved the performance in TED task, although to a lower extent."}, {"heading": "8 Conclusions and future work", "text": "In this work, we studied the implementation of OL strategies for NMT. We empirically demonstrated the capacity of NMT for adapting to new\ndomains, by means of gradient descent. Moreover, we discussed and compared a wide range of gradient descent algorithms. In addition, we proposed two novel methods, inspired by a PA strategy and tackled by means of subgradient optimization methods. PA-based methods offer a competitive performance. In almost every case, they outperformed vanilla SGD. Nevertheless, adaptive SGD algorithms performed generally better than PA. We found two cases in which the performance of the PA method was superior to adaptive SGD algorithms, although differences were small. One of the main drawbacks relating the NMT is the independence (up to some point) of the network objective function (probability of the target sentence given the source) with respect to the evaluation metric (e.g. BLEU or TER). Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al., 2016). In this work, we set the basis for using BLEU or other non-differentiable loss function, since our PAS and PPAS algorithms cope with nondifferentiable functions. As future work, we intend to directly optimize the evaluation metric, integrating it into the online learning framework (Mart\u0131\u0301nez-Go\u0301mez et al., 2012). A major challenge is keeping the update time at a reasonable time. Finally, interactive machine translation (IMT) is tightly related with this work. IMT consists in an evolution of the classical post-editing scenario, striving for profiting the human effort made in the post-editing process. Under the IMT paradigm, human and computer collaborate in order to minimize the user effort. The development of interactive NMT systems has been recently addressed (Knowles and Koehn, 2016; Peris et al., 2017). The inclusion of OL techniques into the interactive framework is the next natural step to take, in order to develop more adaptive and productive translation systems."}, {"heading": "Acknowledgments", "text": "The research leading to these results has received funding from the Generalitat Valenciana under grant PROMETEOII/2014/030. We also acknowledge NVIDIA for the donation of a GPU used in this work.\n10"}], "references": [{"title": "CASMACAT: An open source workbench for advanced computer aided translation", "author": ["Trilles", "Chara Tsoukala."], "venue": "The Prague Bulletin of Mathematical Linguistics 100:101\u2013112.", "citeRegEx": "Trilles and Tsoukala.,? 2013", "shortCiteRegEx": "Trilles and Tsoukala.", "year": 2013}, {"title": "Productivity and quality in the post-editing of outputs from translation memories and machine translation", "author": ["Ana Guerberof Arenas."], "venue": "Localisation Focus 7(1):11\u201321.", "citeRegEx": "Arenas.,? 2008", "shortCiteRegEx": "Arenas.", "year": 2008}, {"title": "Layer normalization", "author": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey Hinton."], "venue": "arXiv:1607.06450.", "citeRegEx": "Ba et al\\.,? 2016", "shortCiteRegEx": "Ba et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Statistical approaches to computer-assisted translation", "author": ["Sergio Barrachina", "Oliver Bender", "Francisco Casacuberta", "Jorge Civera", "Elsa Cubel", "Shahram Khadivi", "Antonio Lagarda", "Hermann Ney", "Jes\u00fas Tom\u00e1s", "Enrique Vidal", "Juan-Miguel Vilar."], "venue": "Com-", "citeRegEx": "Barrachina et al\\.,? 2009", "shortCiteRegEx": "Barrachina et al\\.", "year": 2009}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi."], "venue": "IEEE transactions on neural networks 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Subgradient methods", "author": ["Stephen Boyd", "Lin Xiao", "Almir Mutapcic."], "venue": "lecture notes of EE392o, Stanford University, Autumn Quarter.", "citeRegEx": "Boyd et al\\.,? 2003", "shortCiteRegEx": "Boyd et al\\.", "year": 2003}, {"title": "Massive exploration of neural machine translation architectures", "author": ["Denny Britz", "Anna Goldie", "Thang Luong", "Quoc Le."], "venue": "arXiv:1703.03906.", "citeRegEx": "Britz et al\\.,? 2017", "shortCiteRegEx": "Britz et al\\.", "year": 2017}, {"title": "Hope and fear for discriminative training of statistical translation models", "author": ["David Chiang."], "venue": "Journal of Machine Learning Research 13(Apr):1159\u20131187.", "citeRegEx": "Chiang.,? 2012", "shortCiteRegEx": "Chiang.", "year": 2012}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of the Workshop on Syntax, Semantic and Structure in Statistical Transla-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Online passive-aggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer."], "venue": "Journal of Machine Learning Research 7:551\u2013585.", "citeRegEx": "Crammer et al\\.,? 2006", "shortCiteRegEx": "Crammer et al\\.", "year": 2006}, {"title": "Ultraconservative online algorithms for multiclass problems", "author": ["Koby Crammer", "Yoram Singer."], "venue": "Proceedings of the Annual Conference on Computational Learning Theory. pages 99\u2013115.", "citeRegEx": "Crammer and Singer.,? 2001", "shortCiteRegEx": "Crammer and Singer.", "year": 2001}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Overview of the IWSLT evaluation campaign", "author": ["Marcello Federico", "Luisa Bentivogli", "Michael Paul", "Sebastian St\u00fcker."], "venue": "Proceedings of the International Workshop on Spoken Language Translation. pages 11\u201327.", "citeRegEx": "Federico et al\\.,? 2011", "shortCiteRegEx": "Federico et al\\.", "year": 2011}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves."], "venue": "Advances in Neural Information Processing Systems. pages 2348\u20132356.", "citeRegEx": "Graves.,? 2011", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the Interna-", "citeRegEx": "Jean et al\\.,? 2015", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 1700\u20131709.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural interactive translation prediction", "author": ["Rebecca Knowles", "Philipp Koehn."], "venue": "Proceedings of the Association for Machine Translation in the Americas. pages 107\u2013120.", "citeRegEx": "Knowles and Koehn.,? 2016", "shortCiteRegEx": "Knowles and Koehn.", "year": 2016}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn."], "venue": "Proceedings of the Machine Translation Summit. pages 79\u201386.", "citeRegEx": "Koehn.,? 2005", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "A process study of computeraided translation", "author": ["Philipp Koehn."], "venue": "Machine Translation 23(4):241\u2013 263.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "The METEOR metric for automatic evaluation of machine translation", "author": ["Alon Lavie", "Michael J Denkowski."], "venue": "Machine translation 23(2-3):105\u2013115.", "citeRegEx": "Lavie and Denkowski.,? 2009", "shortCiteRegEx": "Lavie and Denkowski.", "year": 2009}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages 1412\u20131421.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics and the International", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Online adaptation strategies for statistical machine translation in postediting scenarios", "author": ["Pascual Mart\u0131\u0301nez-G\u00f3mez", "Germ\u00e1n Sanchis-Trilles", "Francisco Casacuberta"], "venue": "Pattern Recognition", "citeRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mart\u0131\u0301nez.G\u00f3mez et al\\.", "year": 2012}, {"title": "Online learning approaches in computer assisted translation", "author": ["Prashant Mathur", "Mauro Cettolo", "Marcello Federico", "FBK-Fondazione Bruno Kessler."], "venue": "Proceedings of the Eighth Workshop on Statistical Machine Translation. pages 301\u2013308.", "citeRegEx": "Mathur et al\\.,? 2013", "shortCiteRegEx": "Mathur et al\\.", "year": 2013}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 160\u2013167.", "citeRegEx": "Och.,? 2003", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 295\u2013302.", "citeRegEx": "Och and Ney.,? 2002", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "Online learning for statistical machine translation", "author": ["Daniel Ortiz-Mart\u0131\u0301nez"], "venue": "Computational Linguistics", "citeRegEx": "Ortiz.Mart\u0131\u0301nez.,? \\Q2016\\E", "shortCiteRegEx": "Ortiz.Mart\u0131\u0301nez.", "year": 2016}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv:1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2014", "shortCiteRegEx": "Pascanu et al\\.", "year": 2014}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "arXiv:1211.5063.", "citeRegEx": "Pascanu et al\\.,? 2012", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "Interactive neural machine translation", "author": ["\u00c1lvaro Peris", "Miguel Domingo", "Francisco Casacuberta."], "venue": "Computer Speech & Language 45:201\u2013220.", "citeRegEx": "Peris et al\\.,? 2017", "shortCiteRegEx": "Peris et al\\.", "year": 2017}, {"title": "A productivity test of statistical machine translation post-editing in a typical localisation context", "author": ["Mirko Plitt", "Fran\u00e7ois Masselot."], "venue": "The Prague bulletin of mathematical linguistics 93:7\u201316.", "citeRegEx": "Plitt and Masselot.,? 2010", "shortCiteRegEx": "Plitt and Masselot.", "year": 2010}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro."], "venue": "The Annals of Mathematical Statistics pages 400\u2013407.", "citeRegEx": "Robbins and Monro.,? 1951", "shortCiteRegEx": "Robbins and Monro.", "year": 1951}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics. pages 1715\u20131725.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for", "citeRegEx": "Shen et al\\.,? 2016", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Algorithms of nondifferentiable optimization: Development and application", "author": ["N.Z. Shor", "N.G. Zhurbenko", "A.P. Likhovid", "P.I. Stetsyuk."], "venue": "Cybernetics and Systems Analysis 39(4):537\u2013548.", "citeRegEx": "Shor et al\\.,? 2003", "shortCiteRegEx": "Shor et al\\.", "year": 2003}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Matthew Snover", "Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul."], "venue": "Proceedings of the Association for Machine Translation in the Americas. pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "Proceedings of the Advances in Neural Information Processing Systems, volume 27, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv:1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "News from OPUS - A collection of multilingual parallel corpora with tools and interfaces", "author": ["J\u00f6rg Tiedemann."], "venue": "Recent Advances in Natural Language Processing, volume 5, pages 237\u2013248.", "citeRegEx": "Tiedemann.,? 2009", "shortCiteRegEx": "Tiedemann.", "year": 2009}, {"title": "Online large-margin training for statistical machine translation", "author": ["Taro Watanabe", "Jun Suzuki", "Hajime Tsukada", "Hideki Isozaki."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational", "citeRegEx": "Watanabe et al\\.,? 2007", "shortCiteRegEx": "Watanabe et al\\.", "year": 2007}, {"title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "author": ["O. Vinyals", "G. Corrado", "M. Hughes", "J. Dean."], "venue": "arXiv:1609.08144.", "citeRegEx": "Vinyals et al\\.,? 2016", "shortCiteRegEx": "Vinyals et al\\.", "year": 2016}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Phrase-based statistical machine translation", "author": ["Richard Zens", "Franz Josef Och", "Hermann Ney."], "venue": "Proceedings of the Annual German Conference on Advances in Artificial Intelligence. volume 2479, pages 18\u201332.", "citeRegEx": "Zens et al\\.,? 2002", "shortCiteRegEx": "Zens et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 1, "context": "In the machine translation (MT) industry, the reduction of the post-editing effort has a great interest, as it leads to larger productivity (Arenas, 2008; Plitt and Masselot, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 36, "context": "In the machine translation (MT) industry, the reduction of the post-editing effort has a great interest, as it leads to larger productivity (Arenas, 2008; Plitt and Masselot, 2010).", "startOffset": 140, "endOffset": 180}, {"referenceID": 31, "context": "The application of OL to the classical phrasebased SMT systems has been thoroughly studied (Ortiz-Mart\u0131\u0301nez, 2016).", "startOffset": 91, "endOffset": 114}, {"referenceID": 42, "context": "Nevertheless, over the last years, the new neural machine translation (NMT) technology has shaken the machine translation field (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 128, "endOffset": 192}, {"referenceID": 3, "context": "Nevertheless, over the last years, the new neural machine translation (NMT) technology has shaken the machine translation field (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016).", "startOffset": 128, "endOffset": 192}, {"referenceID": 5, "context": "In order to cope with the vanishing gradient problem (Bengio et al., 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al.", "startOffset": 53, "endOffset": 74}, {"referenceID": 16, "context": ", 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al.", "startOffset": 78, "endOffset": 112}, {"referenceID": 9, "context": ", 1994), such RNNs feature gated units, such as long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRU) (Cho et al., 2014).", "startOffset": 144, "endOffset": 162}, {"referenceID": 3, "context": "The inclusion of attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015a) overcame the problem of processing long source sentences.", "startOffset": 38, "endOffset": 82}, {"referenceID": 25, "context": "The inclusion of attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015a) overcame the problem of processing long source sentences.", "startOffset": 38, "endOffset": 82}, {"referenceID": 17, "context": "The management of out-ofvocabulary words has also been explored (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 64, "endOffset": 104}, {"referenceID": 26, "context": "The management of out-ofvocabulary words has also been explored (Jean et al., 2015; Luong et al., 2015b).", "startOffset": 64, "endOffset": 104}, {"referenceID": 17, "context": "Closely related to this, the NMT vocabulary size limitation has been tackled either developing strategies for taking into account large vocabularies (Jean et al., 2015) or shifting from word-level translations to character (Chung et al.", "startOffset": 149, "endOffset": 168}, {"referenceID": 10, "context": ", 2015) or shifting from word-level translations to character (Chung et al., 2016) or sub-word-level (Sennrich et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 38, "context": ", 2016) or sub-word-level (Sennrich et al., 2016) translations.", "startOffset": 26, "endOffset": 49}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al.", "startOffset": 8, "endOffset": 278}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al.", "startOffset": 8, "endOffset": 365}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al. (2014). From there to now, NMT has surpassed the classical phrase-based systems in many tasks and currently is one the most active research topics (e.", "startOffset": 8, "endOffset": 393}, {"referenceID": 3, "context": ", 2014; Bahdanau et al., 2015; Wu et al., 2016). Therefore, there is a need to reduce the post-editing effort in NMT systems. The neural approach to MT has a short but meteoric history. The first fully neural translation system was introduced by Kalchbrenner and Blunsom (2013), being the first competitive NMT systems simultaneously developed by Cho et al. (2014) and Sutskever et al. (2014). From there to now, NMT has surpassed the classical phrase-based systems in many tasks and currently is one the most active research topics (e.g. Erk and Smith (2016)).", "startOffset": 8, "endOffset": 560}, {"referenceID": 37, "context": "2 Such NMT systems are usually trained from parallel corpora by means of stochastic gradient descent (SGD) (Robbins and Monro, 1951).", "startOffset": 107, "endOffset": 132}, {"referenceID": 11, "context": "Moreover, we propose a new SGD variant for NMT, based on passive-aggressive (PA) techniques (Crammer et al., 2006).", "startOffset": 92, "endOffset": 114}, {"referenceID": 48, "context": "com/lvapeab/nmt-keras/ tree/interactive_NMT are mainly employed for tuning the weights of the log-linear model (Zens et al., 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010).", "startOffset": 111, "endOffset": 149}, {"referenceID": 30, "context": "com/lvapeab/nmt-keras/ tree/interactive_NMT are mainly employed for tuning the weights of the log-linear model (Zens et al., 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010).", "startOffset": 111, "endOffset": 149}, {"referenceID": 23, "context": ", 2002; Och and Ney, 2002) from classical phrase-based SMT systems (Koehn, 2010).", "startOffset": 67, "endOffset": 80}, {"referenceID": 12, "context": "More specifically, PA-based techniques, such as the margin infuse relaxed algorithm (MIRA) (Crammer and Singer, 2001) are especially well-suited when dealing with a large number of features.", "startOffset": 91, "endOffset": 117}, {"referenceID": 45, "context": "Therefore, MIRA is usually applied to models dealing with lots of sparse features (Watanabe et al., 2007; Chiang, 2012).", "startOffset": 82, "endOffset": 119}, {"referenceID": 8, "context": "Therefore, MIRA is usually applied to models dealing with lots of sparse features (Watanabe et al., 2007; Chiang, 2012).", "startOffset": 82, "endOffset": 119}, {"referenceID": 31, "context": "As colophon of it, Ortiz-Mart\u0131\u0301nez (2016) developed an incremental derivation of the EM algorithm.", "startOffset": 19, "endOffset": 42}, {"referenceID": 27, "context": "Mart\u0131\u0301nez-G\u00f3mez et al. (2012) and Mathur et al.", "startOffset": 0, "endOffset": 30}, {"referenceID": 27, "context": "Mart\u0131\u0301nez-G\u00f3mez et al. (2012) and Mathur et al. (2013) used OL for adapting a SMT under a CAT framework.", "startOffset": 0, "endOffset": 55}, {"referenceID": 3, "context": "In this work, we use an attentional NMT system similar to the one described by Bahdanau et al. (2015), but using LSTM networks.", "startOffset": 79, "endOffset": 102}, {"referenceID": 33, "context": "Finally, a deep output layer (Pascanu et al., 2014) is employed to compute a distribution over the target language vocabulary.", "startOffset": 29, "endOffset": 51}, {"referenceID": 42, "context": "3 method (Sutskever et al., 2014).", "startOffset": 9, "endOffset": 33}, {"referenceID": 13, "context": "Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014).", "startOffset": 116, "endOffset": 171}, {"referenceID": 47, "context": "Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014).", "startOffset": 116, "endOffset": 171}, {"referenceID": 19, "context": "Moreover, depending on how the gradient is computed and applied, a large variety of SGD variants have been proposed (Duchi et al., 2011; Zeiler, 2012; Kingma and Ba, 2014).", "startOffset": 116, "endOffset": 171}, {"referenceID": 13, "context": "Adagrad (Duchi et al., 2011) aims to perform larger updates for infrequent parameters, defining its update rule as:", "startOffset": 8, "endOffset": 28}, {"referenceID": 47, "context": "Adadelta (Zeiler, 2012) is a less aggressive variant of Adadelta, which avoids its constant learning rate decay.", "startOffset": 9, "endOffset": 23}, {"referenceID": 19, "context": "Finally, Adam (Kingma and Ba, 2014) computes decaying averages for the past gradients (vt) and the past squared gradients (mt).", "startOffset": 14, "endOffset": 35}, {"referenceID": 12, "context": "We propose a new version of SGD, inspired by PA techniques such as MIRA (Crammer and Singer, 2001).", "startOffset": 72, "endOffset": 98}, {"referenceID": 11, "context": "where C is a parameter that controls the aggressiveness of the algorithm (Crammer et al., 2006).", "startOffset": 73, "endOffset": 95}, {"referenceID": 40, "context": "For obtaining \u0398\u0302, we use a subgradient method (Shor et al., 2003).", "startOffset": 46, "endOffset": 65}, {"referenceID": 6, "context": "An extension of the PAS method is the projected subgradient method (PPAS), in which the optimization problem is reformulated (Boyd et al., 2003).", "startOffset": 125, "endOffset": 144}, {"referenceID": 41, "context": "For approximating the post-editing effort, we used the translation edit rate (TER) (Snover et al., 2006).", "startOffset": 83, "endOffset": 104}, {"referenceID": 32, "context": "Moreover, we used BLEU (Papineni et al., 2002) and Meteor (Lavie and Denkowski, 2009) for assessing the translation quality of the systems.", "startOffset": 23, "endOffset": 46}, {"referenceID": 24, "context": ", 2002) and Meteor (Lavie and Denkowski, 2009) for assessing the translation quality of the systems.", "startOffset": 19, "endOffset": 46}, {"referenceID": 21, "context": "For all results shown in this work, we compute 95% confidence intervals by means of bootstrap resampling (Koehn, 2004).", "startOffset": 105, "endOffset": 118}, {"referenceID": 22, "context": "As out-of-domain data (scenarios 1 and 2), we use the well-known, publicly available Europarl (Koehn, 2005) corpus, using the partition newstest2013 as development set.", "startOffset": 94, "endOffset": 107}, {"referenceID": 44, "context": "As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al.", "startOffset": 58, "endOffset": 75}, {"referenceID": 4, "context": "As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al., 2009) and TED (Federico et al.", "startOffset": 82, "endOffset": 107}, {"referenceID": 14, "context": ", 2009) and TED (Federico et al., 2011) corpora.", "startOffset": 16, "endOffset": 39}, {"referenceID": 4, "context": "As in-domain corpora (scenarios 2 and 3), we use the Emea (Tiedemann, 2009), XRCE (Barrachina et al., 2009) and TED (Federico et al., 2011) corpora. The domains of them are medical, printer manuals and TED talks, respectively. We used the standard partitions for all corpora. We used the English\u2013French language pair for all experiments. Table 1 shows the main figures of the corpora. We tokenized the text using the script from Moses, keeping sentences truecase. For all tasks, we use the joint byte pair encoding (BPE) algorithm for translating at a sub-word level, as described by Sennrich et al. (2016). We learned 32,000 merge operations.", "startOffset": 83, "endOffset": 607}, {"referenceID": 2, "context": "As regularization methods, we applied layer normalization (Ba et al., 2016) and Gaussian noise to the weights (\u03c3 = 0.", "startOffset": 58, "endOffset": 75}, {"referenceID": 15, "context": "01) (Graves, 2011).", "startOffset": 4, "endOffset": 18}, {"referenceID": 47, "context": "The original NMT systems were trained using Adadelta (Zeiler, 2012), with the default parameters.", "startOffset": 53, "endOffset": 67}, {"referenceID": 34, "context": "The norm of the gradients was clipped to 1 in order to avoid the exploding gradient problem (Pascanu et al., 2012).", "startOffset": 92, "endOffset": 114}, {"referenceID": 5, "context": "For choosing the main hyperparameters of the system, we take advantage of the vast exploration made by Britz et al. (2017). Therefore, as described in Section 3, the system consists in an encoder-decoder LSTM network equipped with the attention mechanism described by Bahdanau et al.", "startOffset": 103, "endOffset": 123}, {"referenceID": 2, "context": "Therefore, as described in Section 3, the system consists in an encoder-decoder LSTM network equipped with the attention mechanism described by Bahdanau et al. (2015). For practical reasons, we used single-layered LSTMs.", "startOffset": 144, "endOffset": 167}, {"referenceID": 29, "context": "Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 8, "context": "Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al.", "startOffset": 102, "endOffset": 127}, {"referenceID": 39, "context": "Minimum risk training aims to overcome this gap and it has successfully been applied to classical SMT (Och, 2003; Chiang, 2012) and recently to NMT (Shen et al., 2016).", "startOffset": 148, "endOffset": 167}, {"referenceID": 27, "context": "As future work, we intend to directly optimize the evaluation metric, integrating it into the online learning framework (Mart\u0131\u0301nez-G\u00f3mez et al., 2012).", "startOffset": 120, "endOffset": 150}, {"referenceID": 20, "context": "The development of interactive NMT systems has been recently addressed (Knowles and Koehn, 2016; Peris et al., 2017).", "startOffset": 71, "endOffset": 116}, {"referenceID": 35, "context": "The development of interactive NMT systems has been recently addressed (Knowles and Koehn, 2016; Peris et al., 2017).", "startOffset": 71, "endOffset": 116}], "year": 2017, "abstractText": "Neural machine translation has meant a revolution of the field. Nevertheless, postediting the outputs of the system is mandatory for tasks requiring high translation quality. Post-editing offers a unique opportunity for improving neural machine translation systems, using online learning techniques and treating the post-edited translations as new, fresh training data. We review classical learning methods and propose a new optimization algorithm. We thoroughly compare online learning algorithms in a post-editing scenario. Results show significant improvements in translation quality and effort reduction.", "creator": "LaTeX with hyperref package"}}}