{"id": "1706.03729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Channel-Recurrent Variational Autoencoders", "abstract": "variational autoencoder ( vae ) is each influential mechanism in modeling natural images containing probabilistic filter spaces. essentially, considering the input image become complex, vr becomes less redundant, potentially due to increasing oversimplification of its latent statistical dimensions. in theoretical role, it attempted to integrate recurrent models across machines to identify inference / generation steps besides vae. sequentially building up explicit possibilities defining high - level implementations including this structure allows us : capture global - event - aggregate and coarse - near - precision signals inside the input data spaces. accidents benefit while our channel - recurrent vae improves rigorous approaches in new approaches : ( e ) it develops sufficient empirical log - likelihood compared forecast offerings on mnist ; when trained adversarially, ( 2 ) ems gives face and bird images with substantially higher mathematical quality than the method - of - dual - systems vae - gan and ( pr ) q - paths producing potentially complex vector representations ; as ( vs ) it achieves competitive outcome results on stl - 10 amid limited semi - customized setup.", "histories": [["v1", "Mon, 12 Jun 2017 17:01:34 GMT  (7356kb,D)", "http://arxiv.org/abs/1706.03729v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["wenling shang", "kihyuk sohn", "zeynep akata", "yuandong tian"], "accepted": false, "id": "1706.03729"}, "pdf": {"name": "1706.03729.pdf", "metadata": {"source": "CRF", "title": "Channel-Recurrent Variational Autoencoders", "authors": ["Wenling Shang", "Kihyuk Sohn", "Zeynep Akata", "Yuandong Tian"], "emails": ["shangw@fb.com", "ksohn@nec-labs.com", "z.akata@uva.nl", "yuandong@fb.com"], "sections": [{"heading": "1 Introduction", "text": "Synthesizing diverse natural images is challenging due to abundant details to attend and a wide range of content to cover. Deep generative models sample from latent spaces that capture the details of the observed data in a generalizable way to generate realistic images. In Variational Autoencoder (VAE) [1], a bottom-up inference network and a top-down generation network parameterized by deep neural networks are jointly trained to maximize the variational lower-bound (VLB) of the data log-likelihood. The lower-bound sums two terms: a reconstruction term that asks for better fitting of the data with latent representations, and the KL-divergence that pushes the approximated posteriors towards a specified prior. In many situations, the specified prior is mathematically elegant but often too simple to model the true latent space accurately.\nEfforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9]. An orthogonal direction is to improve the latent space through network architecture design. A predominant architecture [1] models the latent space using the activation of a fully-connected (FC) layer, which is too rigid to model rich local details well due to excessive global constraints. VAE with convolutional latent space [10] or convolutional VAE (cVAE), with fewer global constraints on the latent space, better preserves the visual details of its reconstructed samples (Figure 2(c)), however its generated samples (Figure 2(g)) appear to be chaotic due to lack of high-level consistency. One remedy is to impose complicated graphical structures to the latent space, which can significantly increase the number of model parameters and pose challenges in optimization.\nWe propose to model the global structure while preserving the local details via our channel-recurrent VAE, or crVAE. Similar to cVAE, to preserve local details, the latent space is directly built upon convolutional features without FC layers. On the other hand, the channels of the latent space (latent channels) are progressively generated via recurrent networks (LSTMs), driven by noise following standard Multivariate Gaussian (standard MVN) distribution. This incorporates global structures of the input images through channel-wise communications. Interestingly, crVAE learns to assign latent channels located in early/late generation order with different semantically meaningful factors\n1: Facebook Inc. 2: University of Amsterdam. 3: NEC Labs.\nar X\niv :1\n70 6.\n03 72\n9v 1\n[ cs\n.L G\n] 1\n2 Ju\nn 20\n17\nof variation in an unsupervised manner. In particular, latent channels that are generated first focus on global structures (e.g., face or hair silhouette), whereas later ones on the details (e.g., expressions or accessories).\nOur crVAE model significantly improves the test set negative log-likelihood (NLL) on MNIST over baseline VAE, from 82.04 to 80.84. We then integrate crVAE into the state-of-the-art image generation framework VAE-GAN [11], which we refer to as crVAE-GAN, and present several qualitative studies, such as image completion and synthesis of high-resolution (128\u00d7128) faces and birds. Our crVAE exhibits disentangling effects in an unsupervised manner as we sequentially build up the latent space. Moreover, crVAE-GAN achieves a competitive classification accuracy of 75.86% on STL-10 dataset in the semi-supervised setting, comparing against 74.91% from VAE-GAN baseline. Finally, our model can be further enhanced by combining with other advanced techniques in training deep latent Gaussian models (DLGMs), such as a training protocol of importance weighted autoencoder [6] (Table 1)."}, {"heading": "2 Related Works", "text": "Improving variational inference in deep generative models has recently gained interest in the machine learning community. To enable flexible approximated posterior or prior, the family of normalizing flows [3] iterates an initial estimated posterior density through a sequence of invertible mappings to eventually reach a complex distribution, potentially more faithful to the true posterior. A specific form of normalizing flow is proposed in [4] based on a series of inverse autoregressive transformations and has attained close to state-of-the-art density estimation performance. Furthermore, variational lossy autoencoder [5] transforms a simple standard MVN prior to some more complex distribution via similar autoregressive flow. Our proposed approach is similar in explicitly manipulating the inference and generation networks in the hope of reaching a more compatible posterior approximation and prior. However, rather than following these restricted sets of invertible transformation flows, we construct the mappings with more flexible (e.g., not necessarily invertible) and scalable neural network modules namely LSTMs across latent channels, which leads to efficient modeling of large scale datasets and generating high-resolution images of size 128\u00d7128. It is also worth noting that our method is complementary and can be easily combined with normalizing flow-based distribution transformation methods.\nThe channel-recurrent construction also shares the spirit of deep autoregressive networks (DARN) [12] in the sense of building lateral latent variable connections. In DARN, latent variables are sequentially sampled conditioned on previously-drawn samples through shared weights. However, in crVAE, channels of latent representations are sampled independently and simultaneously from a simple standard MVN before being connected via LSTMs.\nDisentangling factors of variation in an unsupervised manner, an essential topic in representation learning, has been explored through deep generative models. For example, InfoGAN [13] shows unsupervised discovery of basic visual concepts by maximizing mutual information between the inputs and the predicted latent variables. \u03b2-VAE [14] shows similar behaviors with VAEs by applying stronger regularizations on the KL-divergence term to enforce individual latent variable to account for a specific factor of variation. Our model, on the other hand, achieves a natural disentanglement by sequentially processing convolutional features and latent variables via LSTMs, allowing general outlines to be described at early stages and detailed attributes to be adjusted later on during the generation process."}, {"heading": "3 Channel-Recurrent VAE", "text": "In this section, we progressively develop our proposed crVAE. We start from standard VAE model of [1], followed by discussions and experiments on cVAE to explore spatially-correlated latent spaces. Inspired by our observations detailed below, we reach another intermediate step, bcVAE that imposes spatial structure to blocks of latent channels independently. Finally, we build our crVAE by adding recurrent connections to bcVAE.\nVAE [1]. Variational autoencoder (VAE) approximates the intractable posterior of directed graphical models with deep neural networks (Figure 1(a)). The variational lower bound (VLB) of the log likelihood yields the training objective:\nLVAE = \u2212Eq\u03c6(z|x) [ log p\u03b8(x|z) ] +DKL(q\u03c6(z|x)\u2016p(z)) \u2265 \u2212 log(p(x)) (1)\nwhere the approximate posterior, q\u03c6(z|x) is modeled as diagonal MVNs and the prior p(z) as standard MVNs. Conventionally, q\u03c6(z|x) is referred to as an inference network and p\u03b8(x|z) as a generation network. One drawback of VAE is the simple treatment of the posterior approximations. Thus, the associated prior does not necessarily reflect the complicated structure of the true input space [15]. As a consequence, the content from VAE samples lacks in variety, resulting in overly smoothed reconstruction or generation of images (see Figure 2(b) and Figure 2(f), trained on CelebA faces [16]).\nconvolutional VAE. Standard VAE discards an ineligible amount of spatial information from latent variables since it flattens \u00b5 and \u03c3 into vectors using a fully-connected (FC) layer. To preserve more spatial information, we explore convolutional VAE [10] (cVAE, Figure 1(b)), whose inference and generation networks are fully convolutional [17]. We observe improved image reconstruction quality using cVAE in Figure 2(c) compared to those from standard VAE in Figure 2(b). However, for image generation as shown in Figure 2(g), cVAE ignores global structure of face shapes. We hypothesize that, since cVAE embeds the spatial information in its approximate posterior (or inference network activation), the embedded spatial information cannot be effectively retrieved when samples are drawn from the simple standard MVN prior.\nIn effort to resolve such incoherence and verify our hypothesis, we perform additional exploratory experiments by estimating empirical MVN prior of a trained cVAE model using samples drawn from\napproximate posteriors. Specifically, the mean vector (\u00b5\u0302k \u2208 Rw\u00d7h, k = 1, \u00b7 \u00b7 \u00b7 , c) and the covariance matrix (\u03a3\u0302k \u2208 R(w\u00d7h)\u00d7(w\u00d7h), k = 1, \u00b7 \u00b7 \u00b7 , c) of each sliced convolutional latent channel are estimated separately for computational efficiency, as follows:\n\u00b5\u0302k = 1\nN N\u2211 n=1 znk , \u03a3\u0302k = 1 N \u2212 1 N\u2211 n=1 (znk \u2212 \u00b5\u0302k)(znk \u2212 \u00b5\u0302k)>, k = 1, \u00b7 \u00b7 \u00b7 , c.\nWith estimated \u00b5\u0302k and \u03a3\u0302k, we draw samples from the following new prior:\nzk = A\u0302k + \u00b5\u0302k, A\u0302kA\u0302 > k = \u03a3\u0302k, \u223c N (0, I).\nWe refer the model as an empirical cVAE in Figure 2(h). To our surprise, assigning the empirical MVN prior to the latent space manages to preserve much more essential spatial information and results in substantially enhanced image generation quality.\nblock-covariance VAE. Our exploratory experiments with cVAE suggest that using an appropriate and potentially more complex prior, such as MVN prior, is important in attaining globally coherent image generation. Moreover, rather than employing an FC layer that transforms convolutional features of the inference networks into flattened vectors, learning correlations among latent variables, especially across spatial dimension, is essential for the global structure. Although appealing, training a model with MVN prior or by adding direct lateral connection between latent variables can significantly complicate the optimization of the DLGMs [18, 12].\nInstead, we explore the possibility of achieving similar effects by augmenting the network architecture for inference and generation. Specifically, we propose to factorize the convolutional features (at inference, connecting to \u03c3i) or latent variables (at generation, connecting to zi) of size w\u00d7 h\u00d7 c into T blocks of size w \u00d7 h\u00d7 cT (Figure 1(c)) and connect each block with an FC layer individually. In addition, inspired by the effectiveness of the mean estimation from the empirical cVAE, we subtract the mean before the inference and add it back after the generation. The samples that bcVAE generates in Figure 2(i) demonstrate significant improvement over cVAE.\nchannel-recurrent VAE. In bcVAE, different blocks are processed independently and they may end up learning inefficient and duplicated representations that hamper generating high quality images. To prevent such behavior, we further allow communications between 3D convolutional feature maps or latent variable blocks of size w \u00d7 h \u00d7 cT . Specifically, we propose to connect the blocks via LSTMs [19], both in the inference and generation networks of VAE to encourage channel-level interaction, as shown in Figure 1(d). We refer to this model as channel-recurrent VAE, or crVAE. During inference, we substract the mean as in bcVAE and feed the convolution output \u03c3i\u2019s sequentially into an LSTM layer to output \u03c3rnni , as the variances of the approximated posteriors (MVN).\nDuring generation, the MVN samples z = [z1, z2, \u00b7 \u00b7 \u00b7 zT ] are passed through another LSTM layer to obtain a more involved representation u = [u1, u2, \u00b7 \u00b7 \u00b7uT ]. We argue that the channel-recurrency on the generation network also contributes to the performance lift. To give an intuition, consider the directed graphical model from z to u (red box in Figure 1(d)). We derive the VLB of log p(u = [u1, u2 \u00b7 \u00b7 \u00b7un]):1\nlog p(u) \u2265\u2212DKL(q(z1|u1)\u2016p(z1)) + n\u2211 i=2 \u2212DKL(q(zi|ui, ui\u22121)\u2016p(zi)) (2)\n+ Eq(z1|u1) [ log p(u1|z1) ] + n\u2211 i=2 Eq(zi|ui,ui\u22121) [ log p(ui|zi, ui\u22121) ] , (3)\nwhere Equation (2) implies that the above graphical structure has p(zi) as an approximation to the posterior conditioned on ui and ui\u22121, instead of directly tackling the approximation to the joint posterior p(z|u) altogether. In other words, since the posterior at time step i does not need to explicitly consider uj for j < i\u2212 1, which should have been addressed by earlier time steps, or uj for j > i, which will be attended to in the future steps, it can potentially present a simpler distribution than the joint posterior and can be better approximated. Therefore, by imposing the above sequential structure, we introduce more flexible transformation of simple priors to construct complex representations.\n1In practice, u is kept deterministic rather than being sampled. See Supplementary Materials for derivation.\nWe further discuss the empirical advantages of our proposed design in Section 4.1. As shown in Figure 2(e) and 2(j), crVAE both reconstructs and generates images with higher visual quality than other VAE variants."}, {"heading": "4 Experiments", "text": "In this section, we first introduce datasets and describe important implementation details of our setup. Next we assess different model designs via ablation studies on MNIST. Then we provide results on faces and birds generations, image completion, disentangling factors of variation and semi-supervised object recognition tasks.\nDatasets. MNIST [20] is composed of 60, 000 images of 28\u00d728 handwritten digits for training and 10, 000 for testing. CelebA [16] contains 162, 770 training and 19, 867 validation examples which are cropped and scaled to 64\u00d764 or 128\u00d7128. In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images. The ROIs are cropped and scaled to 128\u00d7128. Finally, STL-10 [24] consists of 100, 000 unlabeled examples, 5000 labeled training examples split into 10-fold cross validations, and 8000 labeled testing examples from 10 classes.\nImplementation Details. All models share the standard VAE work flow and are optimized with SGD using ADAM [25]. For MNIST, we augment the input data by randomly binarizing the training examples based on their pixel intensities. For CelebA and Birds, we augment the input data by random horizontal flips. We refer to Section G and H in the Supplementary Materials for more details.\nWeighting KL Objective. Due to the nature of LSTM, later time steps are under heavy influence from earlier ones, hence it is hard to distinguish their contributions. Nonetheless, our unique channel recurrency design allows us to weight the KL objective differently on each time step. For CelebA and Birds only, we use larger weights on the first few time steps and less on the later time steps to balance the influence. Such weighting not only resolves the imbalance caused by LSTM but also acts as an additional venue to manipulate the latent space, providing the flexibility of having a desired time step to be more influential than the others."}, {"heading": "4.1 Ablation Studies on MNIST", "text": "We ablate our model architecture on MNIST, by comparing models including standard VAE, cVAE, bcVAE, crVAEs with channel recurrency on the inference path only, the generation path only and on both paths. The negative log-likelihood (NLL) of MNIST has been used as a benchmark to evaluate probabilistic generative models. NLL is approximated via importance sampling [26], i.e. for each\nimage from the statistically binarized MNIST test set [27], we sample from its approximated posterior 10K times. We keep the same latent dimension for all models. The results are summarized in Table 1. The baseline, standard VAE [1], obtains 82.04 NLL. As expected, cVAE shows worse performance (83.12), because it can not well capture globally coherent patterns. Albeit bcVAE reduces the NLL to 82.17, it is still inferior to crVAE models where lateral connections across subsets of channels are implemented. Among crVAE models, positioning the recurrency on either the inference path or the generation path marginally improves upon the baseline standard VAE. The most substantial improvement happens when the two are combined, achieving 80.84 NLL. From now on, crVAE refers to the framework with LSTM on both paths. Since our crVAE advances deep variational inference in a novel direction, it is compatible with many other advanced training methods of directed graphical models. For example, by integrating k-sample importance weighted estimation [6], we can improve the NLL to 80.02. In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively."}, {"heading": "4.2 Generating Images of Faces and Birds", "text": "As pointed out in [15], the KL divergence term in VAE objective hampers realistic image generation (Figure 2) since it focuses on stretching the latent space over the entire training set in case of assigning too low probability to any of them. On the other hand, the Jensen-Shannon divergence of GAN [29] concentrates on a limited, but high probability regions, resulting in crispier images. Towards preserving the expressiveness of the latent space while generating realistic images, we extend crVAE into crVAE-GAN by regularizing the generation output with an adversarial discriminator:\nmin \u03c6,\u03b8 max D \u2212Eq\u03c6(z|x)\n[ log p\u03b8(x|z) ] +DKL(q\u03c6(z|x)\u2016p(z)) (4)\n+Ex\u223cX [logD(x)] + Ez\u223cq\u03c6(z|x) [log(1\u2212D(p\u03b8(x|z))] + Ez\u223cp(z) [log(1\u2212D(p\u03b8(x|z))] .\nWe compare our crVAE-GAN against the VAE-GAN [11] for generating 128\u00d7128 images in Figure 3. Although the baseline VAE-GAN generates sharp face images (Figure 3(b)), our crVAE-GAN produces visibly more realistic and highly versatile facial parts such as skin tone, age, accessories, hair style, and expressions (Figure 3(c)). Modeling bird images is much more difficult than faces since bird images are more spatially diverse, e.g., not well aligned. The baseline VAE-GAN generates colorful and sharp images (Figure 3(b)), but the details are largely unrealistic, implying that the latent space conveys mostly low-level features such as color and edges but not as much high-level semantic information. By clear contrast, crVAE-GAN, as shown in Figure 3(c), grants significantly more realistic bird images with decent diversity in terms of color, background and shape. We provide more examples in Section F of the Supplementary Materials. Our results corroborate that channel recurrency empowers the latent spaces to better encode both abstract semantic information and fine-grained details of the input spaces."}, {"heading": "4.3 Image Completion on CelebA", "text": "To verify how faithfully the latent manifold from crVAE-GAN reflects the semantic meaning of the input space, we perform image completion experiments on 64\u00d764 CelebA dataset. Specifically, we occlude parts such as right-half of the face, eye or mouth regions of each validation image and optimize their latent representations z\u2019s to fill in the missing parts [30]:\nmin z\n[ \u2016gen(z) m\u2212 x m\u201622 + \u03b3 logN (z; 0, I) + \u03c4 log(1\u2212D(gen(z)) ] , (5)\nwhere gen(\u00b7) refers to the output of the generation network, m \u2208 {0, 1}3\u00d764\u00d764 is a mask whose entries are 0 if corresponding pixel locations are occluded and 1 otherwise, and represents elementwise multiplication. The ground truth images x, occluded x and the final outputs gen(z\u2217) (1 \u2212 m) + x m of each model are visualized in Figure 4. The baseline VAE-GAN model struggles to reason the missing regions, e.g., right half of the faces in Figure 4(a) and sunglasses in Figure 4(c). By contrast, crVAE-GAN is more competent in retracting off-orbit latent points back to the actual manifold by better embedding the high-level semantic information to the latent manifold."}, {"heading": "4.4 Disentangling Factors of Variation on CelebA", "text": "Another interesting aspect of crVAE is that the latent variables are processed sequentially. This allows variables in each time step to learn a global-to-local and coarse-to-fine visual concepts as time step evolves. For example, in Figure 5(a), we set all latent variables to be zero at t = 0 and progressively add contents to the latent blocks at each time step by randomly drawing samples from standard MVN, while fixing the samples drawn at the previous time step. We observe that latent variables at different time step discover different semantic factors of variation, such as global facial structure in an early stage and expression changes later on. To further verify the disentangling property of our model, we first draw random samples from standard MVN prior across 8 time steps, denoted by z = [z1, \u00b7 \u00b7 \u00b7 , zt, \u00b7 \u00b7 \u00b7 , z8]. We then select a time step t and draw a new sample z\u0303 = [z1, \u00b7 \u00b7 \u00b7 , z\u0303t, \u00b7 \u00b7 \u00b7 , z8] that shows semantically meaningful changes from z. We finally generate images by interpolating between zt and z\u0303t while fixing other zi\u2019s. Visualizations of the generated samples are in the middle row of Figure 5. Hairstyle in Figure 5(b) (t = 1), facial expression in Figure 5(c) (t = 7), and pose in Figure 5(d) (t = 5) are manipulated distinctively. Here, z\u0303t are shared by the two rows of each figure, and can also be used to manipulate aforementioned visual attributes of other samples.\nIn addition, we note that, unlike other unsupervised disentangling methods such as \u03b2-VAE [14] or InfoGAN [13], our model associates a latent subspace instead of a single latent unit to each factor, which grants more freedom for semantic variations. By distributing interpretation to subspaces instead of individual latent units, our model manages to host a high dimensional latent space (1024) than the other unsupervised disentangling methods (\u224810) and thus encodes a more complex data space. This allows us to generate different styles of the same factor by sampling different zt\u2019s while fixing the other zi\u2019s, whereas existing algorithms are limited to shift the factor along a single direction only. The bottom figures in Figure 5 demonstrates this aspect of our model. For instance, Figure 5(e) not only shows the glasses on/off but also switches from eyeglasses to sunglasses, from thin frame to thick frame; the same person wearing a variety of hair styles in Figure 5(f); a young girl can grow up to have pointed chin or squared chin while an elderly man could have had different looks back in the day (Figure 5(g)); finally, we display several distinguished expressions in Figure 5(h). More samples are in the Supplementary Materials.\nIn the meanwhile, channel recurrency is not perfect in disentangling factors of variation. Certain time steps appear to dominate the control of explanatory factors over some other time steps. There also lacks a principled way in determining the direction that manipulates a single factor of variation for a specific example besides visual inspection. Nonetheless, the experimental demonstrations of this intriguing property of crVAE-GAN shed light on important future research directions, such as better disentangling each attribute into a single time step and systematically classifying subspace directions representing a certain attribute within each time step."}, {"heading": "4.5 Semi-Supervised Object Recognition on STL-10", "text": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task. Our experimental setup, such as architecture, training/evaluation protocol, hyperparameters and data preprocessing, closely follows [32], where the discriminator not only classifies real-fake images but also the category. Additionally, to simplify the pipeline, our inputs do not have random patches removed and the discriminator takes the fully reconstructed instead of inpainted images. We replace the generator in this framework with our proposed crVAE, comparing against models that have deterministic autoencoders [32] or standard VAE as generators (Table 2). Our crVAE outperforms other autoencoding systems as generators, achieving 75.86% mean classification accuracy, on par with the current state-of-the-art 75.40% from Exemplar CNNs [35]."}, {"heading": "5 Conclusion", "text": "We propose a channel recurrent scheme, i.e. crVAE, to learn flexible latent representations inside the VAE framework. Channel recurrency improves over standard VAE on a variety of tasks. Moreover, we explore the latent manifold that our crVAE shows that it is able to naturally disentangle explanatory factors into latent subspaces, allowing for richer semantic variations for each kind of attribute. Future research includes building more interpretable features via channel recurrency and extrapolating our framework to other domain such as videos."}, {"heading": "Acknowledgments", "text": "We thank Justin Chiu for his helpful comments."}, {"heading": "C More Image Completion Examples.", "text": "(a) right half (b) mouth (c) eyes\nFigure S3: Each block from left to right: ground truth; occluded; VAE-GAN; crVAE-GAN."}, {"heading": "D More Subspace Interpolation Examples", "text": "(a) hair (b) smile (c) azimuth\n(d) face shape (e) illumination (f) facial hair\nFigure S4: Interpolations between z = [z1, \u00b7 \u00b7 \u00b7 , zt, \u00b7 \u00b7 \u00b7 , z8] and z\u0303 = [z1, \u00b7 \u00b7 \u00b7 , z\u0303t, \u00b7 \u00b7 \u00b7 , z8]. All figures are generated from standard MVN prior; all figures in each block share the same z\u0303t, i.e. ending at the same latent representation for time step t. (a)(b)(c) are additional samples of those appear in the main text. We can see how traversing along one direction at one time step gradually shifts a certain factor of variation. Though due to the high dimensionality of the subspaces we are manipulating, at times highly correlated factors can interwine. For example, (f) shows that beard is correlated with gender: reducing the amount of facial hair can cause change of gender.\nE Interpretable Subspace Samples\n(a) glasses (b) hair style (c) age (d) emotion\n(e) gender (f) background (g) facial hair (h) skin tone\nFigure S5: Change the style of an attribute by sampling different zt\u2019s for a selected time step t while fixing other zi\u2019s. All figures are generated from standard MVN prior; all figures in each block have their latent representations changing over the same time step t only. Thanks to zt being high dimensional, we can sample versatile looks of the same attribute. Though for the same reason, while majority of the content is well perserved, changing zt can potentially influence other attributes besides the target one. For example, in (g), some of the background is affected while facial hair is in fact the attribute being manipulated.\nF More 128\u00d7128 Samples for CelebA and Birds\n.\n(a) VAE-GAN for CelebA\n(b) crVAE-GAN for CelebA\nFigure S6: Generated 128\u00d7128 samples from standard MVN prior with VAE-GAN (top) and crVAEGAN (bottom) for CelebA. The proposed crVAE-GAN outputs more realistic images.\n(a) VAE-GAN for Birds\n(b) crVAE-GAN for Birds\nFigure S7: Generated 128\u00d7128 samples from standard MVN prior with VAE-GAN (top) and crVAEGAN (bottom) for Birds. The proposed crVAE-GAN outputs more realistic images."}, {"heading": "G Details of Network Architecture", "text": "This section describes the building blocks used to construct the networks in our experiments. Throughout this section, we adapt the following notations:\n\u2022 Csn refers to a convolutional layer with n filters and stride s.\n\u2022 Dn refers to a deconvolutional layer with n filters; in our networks, deconv layers are of stride 2 and kernel size 4\u00d74.\n\u2022 FCn refers to a fully-connected layer with n filters.\n\u2022 P k refers to k \u00d7 k pooling. \u2022 LSTMtn refers to a T -time-step LSTM with n hidden units.\n\u2022 BN refers to Batch Normalization.\n\u2022 ReLU refers to ReLU activation, CReLU CReLU activation and Leaky Leaky ReLU activation with 0.1 slope.\nG.1 Inference Feature Extractor Architecture\nThe following models are used to extract convolutional features for inference:\nMNIST. All convolutions are of 3\u00d73 kernel size except for the first one (5\u00d75). We have the following architecture:\nC232 + BN + CReLU\u2192 C164 + BN + Leaky\u2192 C264 + BN + CReLU\u2192 C1128 + BN + Leaky \u2192 C2128 + BN + CReLU\u2192 C1256 + BN + Leaky\nCelebA 64\u00d764. Again, all convolutions here are of 3\u00d73 kernel size except for the first one (5\u00d75):\nC232 + BN + CReLU\u2192 C264 + BN + CReLU\u2192 C2128 + BN + CReLU\u2192 C1256 + BN + CReLU \u2192 C1256 + BN + Leaky\u2192 C1256 + BN + Leaky\nCelebA and Birds 128\u00d7128. All convolutions here are of 3\u00d73 convolutional kernels except for the first and second conv layers (5\u00d75)\nC232 + BN + CReLU\u2192 C264 + BN + CReLU\u2192 C2128 + BN + CReLU\u2192 C1256 + BN + CReLU \u2192 C1256 + BN + Leaky\u2192 C1256 + BN + Leaky\nG.2 Deconvolutional Network Architecture\nThe following architectures are used to generate images from latent representations u.\nMNIST. All convolutions are of 3\u00d73 kernel size except for the one before Sigmoid (1\u00d71).\nD256 + BN + Leaky\u2192 C1256 + BN + Leaky\u2192 D128 + BN + Leaky\u2192 C1128 + BN + Leaky \u2192 D64 + BN + Leaky\u2192 C164 + BN + Leaky\u2192 D32 + BN + Leaky\u2192 C132 + BN + Leaky \u2192 C11 + BN + Leaky\u2192 Sigmoid\nCelebA and Birds. All convolutions are of 3\u00d73 kernels except for the one before Tanh (1\u00d71).\nD256 + BN + Leaky\u2192 C1256 + BN + Leaky\u2192 D128 + BN + Leaky\u2192 D64 + BN + Leaky \u2192 D32 + BN + Leaky\u2192 C13 + BN + Leaky\u2192 Tanh\nG.3 Inference Path and Latent Decoding\nNow we specify the architecture that connects the extracted CNN features to mean and variance estimations as well as that connects the sampled latent representations to the deconvolutional networks.\nstandard VAE. We flatten the convolutional features to 1-D vectors and apply fully-connected layers during inference. Similarly, we apply FC layers to transform z to u then reshape u before going to deconvolution.\n\u2022 mean inference: FC1024 \u2022 variance inference: FC1024 \u2022 decoding z: FC256\u00d74\u00d74\ncVAE. We replace FC layers from standard VAE with convolutional layers without flattening the convolutional variables:\n\u2022 mean inference: C164 \u2022 variance inference: C164 \u2022 decoding z: C1256\nbcVAE. For variance, we slice the convolutional features into T (T = 64 for celebA and T = 8 for MNIST) subsets along the channel direction, then connect each subset with an independent FC layer. Same with decoding z. Also, we add an additional element-wise addition to mean inference and after the FC layers for decoding path.\n\u2022 mean inference: C164 \u2212 \u00b50 \u2022 variance inference: (FC1024/T )\u00d7 T \u2022 decoding z: (FC256\u00d74\u00d74/T )\u00d7 T + \u00b50\ncrVAE. Similar to bcVAE, but instead of FC layers, we use LSTM to connect the sliced subsets. For MNIST, we use FC layer instead of conv layer for mean inference path due to MNIST\u2019s simple content is more suitable for FC layer.\n\u2022 mean inference: C164 \u2212 \u00b50 or FC1024 \u2212 \u00b50 (MNIST) \u2022 variance inference: LSTM8256 \u2022 decoding z: LSTM8256 + \u00b50\nG.4 Discriminator\nFor all VAE-GAN and crVAE-GAN experiments, the following Discriminator architecture is used:\nC232 + P 2 + ReLU + Dropout\u2192 C264 + P 2 + ReLU + Dropout\n\u2192 C2128 + P 2 + ReLU + Dropout\u2192 C2128 + P 2 + ReLU + Dropout \u2192 FC512 + Norm(1) + Dropout + ReLU + BD + FC1 + Sigmoid,\nwhere all conv kernels are of size 5\u00d75 except for the last one (3\u00d73), Dropout ratio 0.5, Norm(1) refers to normalizing L2 norm of the incoming vectors to 1 and BD refers to Batch Discrimination layer [S39].\nG.5 STL-10 Models\nFor STL-10, we closely follow the model presented by [S32]. Instead of using deterministic autoencoders, we replace it with standard VAE as another baseline and our crVAE. The latent dimension is 2048 for all models and T = 8 for LSTM.\nH Implementation Details\nOptimization. Our experiments are optimized with ADAM [S25], where we set = 1 \u00d7 10\u22128, \u03b21 = 0.9, \u03b22 = 0.999. For the discriminator, we use a variation of ADAM with additional thresholding [S40]: if the classification accuracy of a batch consisting of half real and half fake images is over 90%, then we do not backprop.\nObjectives. Recall the VAE objective: LVAE = \u2212Eq\u03c6(z|x) [ log p\u03b8(x|z) ]\ufe38 \ufe37\ufe37 \ufe38\nLrecon\n+DKL(q\u03c6(z|x)\u2016p(z))\ufe38 \ufe37\ufe37 \ufe38 Lprior \u2265 \u2212 log(p(x)). (S2)\nConventionally, for RGB images, p\u03b8(x|z), a diagonal MVN, is assumed to have fixed variance \u03c3 and only the mean values are estimated which correspond to pixel values. We follow this convention, practically apply MSE loss at the pixel level, which we find adequately effective without feature matching, and defer \u03c3 to be adjusted through weighting the Lprior, i.e. the KL-divergence term, by introducing an additional hyperparamter \u03b1. Moreover, for VAE-GAN framework, there is an additional adversarial loss, which is weighted by another hyperparameter \u03b2. Together, we have the following training objective for RGB images:\nLVAE-GAN = Lrecon + \u03b1Lprior + \u03b2LD, (S3)\nwhere LD = \u2212 log(D(x))\u2212 log(1\u2212D(gen(z))), z \u223c p(z).\nNote that Lrecon is divided not only by the batch size, but also by the channel, width and height of the images for implementation convenience, while Lprior or LD are divided by the batch size only. We set \u03b1 = 0.00033 for celebA and Birds and progressively reduces it to 0.0002 over time for crVAE-GAN models. For \u03b2 values, celebA works the best with \u03b2 = 0.01 and Birds \u03b2 = 0.025.\nOther hyperparameters. All the other initial learning rates are 0.0003 except for crVAE and crVAE-GAN, where lr = 0.003. We decay learning rate by half every 50 epochs. MNIST are trained over 200 epochs and the other 150 epochs. We use batch size of 128. For 128\u00d7128 images, we use batch size of 32 and lr = 0.0003.\nSTL-10. We maintain most hyperparameters and training/evaluation protocols from [S32]. But we set the initial learning rate for crVAE to 0.002. We set \u03b2 = 0.001 and \u03b1 = 0.0001. Also, the fake examples for the discriminator in this case all come from reconstructed samples.\n3In our implementation, \u03b1 = 0.0003 corresponds to 0.0003\u00d7 3\u00d7 64\u00d7 64 \u2248 3.69."}], "references": [{"title": "Auto-encoding variational bayes", "author": ["Diederik P Kingma", "Max Welling"], "venue": "In ICLR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Markov chain monte carlo and variational inference: Bridging the gap", "author": ["Tim Salimans", "Diederik Kingma P", "Max Welling"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Variational inference with normalizing flows", "author": ["Danilo Rezende", "Shakir Mohamed"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Improving variational inference with inverse autoregressive flow", "author": ["Diederik Kingma", "Tim Salimans", "Rafal Jozefowicz", "Xi Chen", "Ilya Sutskever", "Max Welling"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Variational lossy autoencoder", "author": ["Xi Chen", "Diederik P Kingma", "Tim Salimans", "Yan Duan", "Prafulla Dhariwal", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "arXiv preprint arXiv:1611.02731,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Importance weighted autoencoders", "author": ["Yuri Burda", "Roger Grosse", "Ruslan Salakhutdinov"], "venue": "ICLR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Generating sentences from a continuous space", "author": ["Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Dai Andrew", "Rafal Jozefowicz", "Samy Bengio"], "venue": "In arXiv,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Ladder variational autoencoders", "author": ["Casper Kaae S\u00f8nderby", "Tapani Raiko", "Lars Maal\u00f8e", "S\u00f8ren Kaae S\u00f8nderby", "Ole Winther"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Epitomic variational autoencoder", "author": ["Serena Yeung", "Anitha Kannan", "Yann Dauphin", "Li Fei-fei"], "venue": "In ICLR 2017 workshop submission,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2017}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Anders Boesen Lindbo Larsen", "Soren Kaae Sonderby", "Hugo Larochelle", "Ole Winther"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep autoregressive networks", "author": ["Karol Gregor", "Ivo Danihelka", "Andriy Mnih", "Charles Blundell", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "beta-vae: Learning basic visual concepts with a constrained variational framework", "author": ["Irina Higgins", "Loic Matthey", "Arka Pal", "Christopher Burgess", "Xavier Glorot", "Matthew Botvinick", "Shakir Mohamed", "Alexander Lerchner"], "venue": "In ICLR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2017}, {"title": "A note on the evaluation of generative models", "author": ["Lucas Theis", "Aaron van den Oord", "Matthias Bethge"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Deep learning face attributes in the wild", "author": ["Ziwei Liu", "Ping Luo", "Xiaogang Wang", "Xiaoou Tang"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Derivations for linear algebra and optimization", "author": ["John Duchi"], "venue": "Berkeley, California,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "Jurgen Schmidhuber"], "venue": "In Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Birdsnap: Large-scale fine-grained visual categorization of birds", "author": ["Thomas Berg", "Jiongxin Liu", "Seung Woo Lee", "Michelle L Alexander", "David W Jacobs", "Peter N Belhumeur"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection", "author": ["Grant Van Horn", "Steve Branson", "Ryan Farrell", "Scott Haber", "Jessie Barry", "Panos Ipeirotis", "Pietro Perona", "Serge Belongie"], "venue": "In CVPR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "The caltech-ucsd birds-200-2011 dataset", "author": ["Catherine Wah", "Steve Branson", "Peter Welinder", "Pietro Perona", "Serge Belongie"], "venue": "In Report,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["Adam Coates", "Honglak Lee", "Andrew Y Ng"], "venue": "In AISTATS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "In ICLR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Binarized mnist dataset", "author": ["Hugo Larochelle"], "venue": "http://www.cs.toronto.edu/~larocheh/ public/datasets/binarized_mnist,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Draw: A recurrent neural network for image generation", "author": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Attribute2image: Conditional image generation from visual attributes", "author": ["Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Unsupervised and semi-supervised learning with categorical generative adversarial networks", "author": ["Jost Tobias Springenberg"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Semi-supervised learning with context-conditional generative adversarial networks", "author": ["Emily Denton", "Sam Gross", "Rob Fergus"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In NIPS,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Stacked what-where autoencoders", "author": ["Junbo Zhao", "Michael Mathieu", "Ross Goroshin", "Yann Lecun"], "venue": "In ICLR Workshop,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Discriminative unsupervised feature learning with convolutional neural networks", "author": ["Alexey Dosovitskiy", "Jost Tobias Springenberg", "Martin Riedmiller", "Thomas Brox"], "venue": "In NIPS,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "An analysis of unsupervised pre-training in light of recent advances", "author": ["Tom Le Paine", "Pooya Khorrami", "Wei Han", "Thomas S Huang"], "venue": "ICLR Workshop,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Density estimation using real nvp", "author": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Better mixing via deep representations", "author": ["Yoshua Bengio", "Gregoire Mesnil", "Yann Dauphin", "Salah Rifai"], "venue": "In ICML,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Improved techniques for training gans", "author": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "venue": "In NIPS,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In Variational Autoencoder (VAE) [1], a bottom-up inference network and a top-down generation network parameterized by deep neural networks are jointly trained to maximize the variational lower-bound (VLB) of the data log-likelihood.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 96, "endOffset": 105}, {"referenceID": 2, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 96, "endOffset": 105}, {"referenceID": 3, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 96, "endOffset": 105}, {"referenceID": 4, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 5, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 216, "endOffset": 225}, {"referenceID": 7, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 216, "endOffset": 225}, {"referenceID": 8, "context": "Efforts have been made to improve the performance of VAE by improving the approximate posterior [2, 3, 4] and prior [5], tightening the variational lower-bound [6], or resolving under-utilization of model capacities [7, 8, 9].", "startOffset": 216, "endOffset": 225}, {"referenceID": 0, "context": "A predominant architecture [1] models the latent space using the activation of a fully-connected (FC) layer, which is too rigid to model rich local details well due to excessive global constraints.", "startOffset": 27, "endOffset": 30}, {"referenceID": 9, "context": "VAE with convolutional latent space [10] or convolutional VAE (cVAE), with fewer global constraints on the latent space, better preserves the visual details of its reconstructed samples (Figure 2(c)), however its generated samples (Figure 2(g)) appear to be chaotic due to lack of high-level consistency.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "(a) standard VAE [1] Sampler", "startOffset": 17, "endOffset": 20}, {"referenceID": 10, "context": "We then integrate crVAE into the state-of-the-art image generation framework VAE-GAN [11], which we refer to as crVAE-GAN, and present several qualitative studies, such as image completion and synthesis of high-resolution (128\u00d7128) faces and birds.", "startOffset": 85, "endOffset": 89}, {"referenceID": 5, "context": "Finally, our model can be further enhanced by combining with other advanced techniques in training deep latent Gaussian models (DLGMs), such as a training protocol of importance weighted autoencoder [6] (Table 1).", "startOffset": 199, "endOffset": 202}, {"referenceID": 2, "context": "To enable flexible approximated posterior or prior, the family of normalizing flows [3] iterates an initial estimated posterior density through a sequence of invertible mappings to eventually reach a complex distribution, potentially more faithful to the true posterior.", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "A specific form of normalizing flow is proposed in [4] based on a series of inverse autoregressive transformations and has attained close to state-of-the-art density estimation performance.", "startOffset": 51, "endOffset": 54}, {"referenceID": 4, "context": "Furthermore, variational lossy autoencoder [5] transforms a simple standard MVN prior to some more complex distribution via similar autoregressive flow.", "startOffset": 43, "endOffset": 46}, {"referenceID": 11, "context": "The channel-recurrent construction also shares the spirit of deep autoregressive networks (DARN) [12] in the sense of building lateral latent variable connections.", "startOffset": 97, "endOffset": 101}, {"referenceID": 12, "context": "For example, InfoGAN [13] shows unsupervised discovery of basic visual concepts by maximizing mutual information between the inputs and the predicted latent variables.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "\u03b2-VAE [14] shows similar behaviors with VAEs by applying stronger regularizations on the KL-divergence term to enforce individual latent variable to account for a specific factor of variation.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "We start from standard VAE model of [1], followed by discussions and experiments on cVAE to explore spatially-correlated latent spaces.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "VAE [1].", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "Thus, the associated prior does not necessarily reflect the complicated structure of the true input space [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "As a consequence, the content from VAE samples lacks in variety, resulting in overly smoothed reconstruction or generation of images (see Figure 2(b) and Figure 2(f), trained on CelebA faces [16]).", "startOffset": 191, "endOffset": 195}, {"referenceID": 9, "context": "To preserve more spatial information, we explore convolutional VAE [10] (cVAE, Figure 1(b)), whose inference and generation networks are fully convolutional [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "To preserve more spatial information, we explore convolutional VAE [10] (cVAE, Figure 1(b)), whose inference and generation networks are fully convolutional [17].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "Although appealing, training a model with MVN prior or by adding direct lateral connection between latent variables can significantly complicate the optimization of the DLGMs [18, 12].", "startOffset": 175, "endOffset": 183}, {"referenceID": 11, "context": "Although appealing, training a model with MVN prior or by adding direct lateral connection between latent variables can significantly complicate the optimization of the DLGMs [18, 12].", "startOffset": 175, "endOffset": 183}, {"referenceID": 18, "context": "Specifically, we propose to connect the blocks via LSTMs [19], both in the inference and generation networks of VAE to encourage channel-level interaction, as shown in Figure 1(d).", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Model specifications NLL standard VAE [1] \u2013 82.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "02 IWAE [6]\u2020 1 stochastic layer 84.", "startOffset": 8, "endOffset": 11}, {"referenceID": 11, "context": "78 DARN [12]\u2020 1 hidden layer 84.", "startOffset": 8, "endOffset": 12}, {"referenceID": 3, "context": "13 VAE-IAF [4]\u2020 1 hidden layer 79.", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "10 Model trained and evaluated on binarized MNIST DLGM-NF [3]\u2020 planar normalizing flow (k=80) 85.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "10 DLGM-HVI [2]\u2020 8 leapfrog steps 85.", "startOffset": 12, "endOffset": 15}, {"referenceID": 25, "context": "Negative log-likelihood (NLL) is estimated using importance weighted sampling [26], i.", "startOffset": 78, "endOffset": 82}, {"referenceID": 19, "context": "MNIST [20] is composed of 60, 000 images of 28\u00d728 handwritten digits for training and 10, 000 for testing.", "startOffset": 6, "endOffset": 10}, {"referenceID": 15, "context": "CelebA [16] contains 162, 770 training and 19, 867 validation examples which are cropped and scaled to 64\u00d764 or 128\u00d7128.", "startOffset": 7, "endOffset": 11}, {"referenceID": 20, "context": "In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images.", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images.", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "In addition, we combine three large scale bird datasets, namely Birdsnap [21], NABirds [22] and Caltech-UCSD birds-200-2011 (CUB) [23], to one congregated dataset referred to as Birds, in total of 106, 474 images.", "startOffset": 130, "endOffset": 134}, {"referenceID": 23, "context": "Finally, STL-10 [24] consists of 100, 000 unlabeled examples, 5000 labeled training examples split into 10-fold cross validations, and 8000 labeled testing examples from 10 classes.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "All models share the standard VAE work flow and are optimized with SGD using ADAM [25].", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "NLL is approximated via importance sampling [26], i.", "startOffset": 44, "endOffset": 48}, {"referenceID": 26, "context": "image from the statistically binarized MNIST test set [27], we sample from its approximated posterior 10K times.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The baseline, standard VAE [1], obtains 82.", "startOffset": 27, "endOffset": 30}, {"referenceID": 5, "context": "For example, by integrating k-sample importance weighted estimation [6], we can improve the NLL to 80.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 126, "endOffset": 135}, {"referenceID": 1, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 126, "endOffset": 135}, {"referenceID": 3, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 126, "endOffset": 135}, {"referenceID": 27, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "In terms of NLL performance, crVAE is competative to other related methods (Table 1), such as the family of normalizing flows [3, 2, 4] that aim at closing the gap between true and approximated posteriors, DRAW [28] and DARN [12] that autoregress on the entire latent space and on individual latent variables, respectively.", "startOffset": 225, "endOffset": 229}, {"referenceID": 14, "context": "As pointed out in [15], the KL divergence term in VAE objective hampers realistic image generation (Figure 2) since it focuses on stretching the latent space over the entire training set in case of assigning too low probability to any of them.", "startOffset": 18, "endOffset": 22}, {"referenceID": 28, "context": "On the other hand, the Jensen-Shannon divergence of GAN [29] concentrates on a limited, but high probability regions, resulting in crispier images.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "We compare our crVAE-GAN against the VAE-GAN [11] for generating 128\u00d7128 images in Figure 3.", "startOffset": 45, "endOffset": 49}, {"referenceID": 29, "context": "Specifically, we occlude parts such as right-half of the face, eye or mouth regions of each validation image and optimize their latent representations z\u2019s to fill in the missing parts [30]:", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "In addition, we note that, unlike other unsupervised disentangling methods such as \u03b2-VAE [14] or InfoGAN [13], our model associates a latent subspace instead of a single latent unit to each factor, which grants more freedom for semantic variations.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "In addition, we note that, unlike other unsupervised disentangling methods such as \u03b2-VAE [14] or InfoGAN [13], our model associates a latent subspace instead of a single latent unit to each factor, which grants more freedom for semantic variations.", "startOffset": 105, "endOffset": 109}, {"referenceID": 30, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 31, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 38, "endOffset": 46}, {"referenceID": 32, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 69, "endOffset": 77}, {"referenceID": 33, "context": "Inspired by recent works on GAN-based [31, 32] and autoencoder-based [33, 34] semi-supervised learning, we adapt the VAE-GAN framework to STL-10 object recognition task.", "startOffset": 69, "endOffset": 77}, {"referenceID": 31, "context": "Our experimental setup, such as architecture, training/evaluation protocol, hyperparameters and data preprocessing, closely follows [32], where the discriminator not only classifies real-fake images but also the category.", "startOffset": 132, "endOffset": 136}, {"referenceID": 31, "context": "We replace the generator in this framework with our proposed crVAE, comparing against models that have deterministic autoencoders [32] or standard VAE as generators (Table 2).", "startOffset": 130, "endOffset": 134}, {"referenceID": 34, "context": "40% from Exemplar CNNs [35].", "startOffset": 23, "endOffset": 27}, {"referenceID": 31, "context": "Model Accuracy Model Accuracy Deterministic AE [32]\u2020 73.", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "81 Stacked What-Where AE [34]\u2020 74.", "startOffset": 25, "endOffset": 29}, {"referenceID": 34, "context": "91 Exemplar CNN [35]\u2020 75.", "startOffset": 16, "endOffset": 20}, {"referenceID": 35, "context": "86 Unsupervised Pre-train [36]\u2020 70.", "startOffset": 26, "endOffset": 30}], "year": 2017, "abstractText": "Variational Autoencoder (VAE) is an efficient framework in modeling natural images with probabilistic latent spaces. However, when the input spaces become complex, VAE becomes less effective, potentially due to the oversimplification of its latent space construction. In this paper, we propose to integrate recurrent connections across channels to both inference and generation steps of VAE. Sequentially building up the complexity of high-level features in this way allows us to capture global-to-local and coarse-to-fine structures of the input data spaces. We show that our channel-recurrent VAE improves existing approaches in multiple aspects: (1) it attains lower negative log-likelihood than standard VAE on MNIST; when trained adversarially, (2) it generates face and bird images with substantially higher visual quality than the state-of-the-art VAE-GAN and (3) channel-recurrency allows learning more interpretable representations; finally (4) it achieves competitive classification results on STL-10 in a semi-supervised setup.", "creator": "LaTeX with hyperref package"}}}