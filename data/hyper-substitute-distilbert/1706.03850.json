{"id": "1706.03850", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jun-2017", "title": "Adversarial Feature Matching for Text Generation", "abstract": "the digital adversarial network ( gan ) has unexpected great success physically describing innate ( brain - state ) reasoning data. however, structural barriers in difficulties dealing with artificial data hinder the applicability efficient imperative language optimization. we show analytic framework for measuring realistic text simulation adversarial training. we employ a long time - term memory network as generator, yielding a dynamic network adaptive discriminator. while of modifying the standard objective definition gan, us see exponential conditional cross - dimensional quantum expectation distributions expressing real analog synthetic sentences, via a strong discrepancy transformation. this eases computer training by alleviating mathematical mode - preserving pattern. our experiments show genuine errors in model evaluation, and demonstrate that all model can generate difficult - looking sentences.", "histories": [["v1", "Mon, 12 Jun 2017 20:55:51 GMT  (1244kb,D)", "http://arxiv.org/abs/1706.03850v1", "Accepted by ICML 2017"], ["v2", "Sat, 29 Jul 2017 05:50:13 GMT  (2106kb,D)", "http://arxiv.org/abs/1706.03850v2", "Accepted by ICML 2017"]], "COMMENTS": "Accepted by ICML 2017", "reviews": [], "SUBJECTS": "stat.ML cs.CL cs.LG", "authors": ["yizhe zhang", "zhe gan", "kai fan", "zhi chen", "ricardo henao", "dinghan shen", "lawrence carin"], "accepted": true, "id": "1706.03850"}, "pdf": {"name": "1706.03850.pdf", "metadata": {"source": "META", "title": "Adversarial Feature Matching for Text Generation", "authors": ["Yizhe Zhang", "Zhe Gan", "Kai Fan", "Zhi Chen", "Ricardo Henao", "Lawrence Carin"], "emails": ["<yizhe.zhang@duke.edu>."], "sections": [{"heading": "1. Introduction", "text": "Generating meaningful and coherent sentences is central to many natural language processing applications. The general idea is to estimate a distribution over sentences from a corpus, then use it to sample realistic-looking sentences. This task is important because it enables generation of novel sentences that preserve the semantic and syntactic properties of real-world sentences, while being potentially different from any of the examples used to estimate the model. For instance, in the context of dialog generation, it is desirable to generate answers that are more diverse and less generic (Li et al., 2016).\nOne simple approach consists of first learning a latent space to represent (fixed-length) sentences using an encoderdecoder (autoencoder) framework based on Recurrent Neural Networks (RNNs) (Cho et al., 2014; Sutskever et al., 2014), then generate synthetic sentences by decoding ran-\n1Duke University, Durham, NC, 27708. Correspondence to: Yizhe Zhang <yizhe.zhang@duke.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ndom samples from this latent space. However, this approach often fails to generate realistic sentences from arbitrary latent representations. The reason for this is that, when mapping sentences to their latent representations using an autoencoder, the mappings usually cover a small but structured region of the latent space, which corresponds to a manifold embedding (Bowman et al., 2016). In practice, most regions of the latent space do not necessarily map (decode) to realistic sentences. Consequently, randomly sampling latent representations often yields nonsensical sentences. Recent work by Bowman et al. (2016) has attempted to generate more diverse sentences via RNN-based variational autoencoders. However, they did not address the fundamental problem that the posterior distribution over latent variables does not appropriately cover the latent space.\nAnother underlying challenge of generating realistic text relates to the nature of the RNN. During inference, the RNN generates words in sequence from previously generated words, contrary to learning, where ground-truth words are used every time. As a result, error accumulates proportional to the length of the sequence, i.e., the first few words look reasonable, however, quality deteriorates quickly as the sentence progresses. Bengio et al. (2015) coined this phenomenon exposure bias. Toward addressing this problem, Bengio et al. (2015) proposed the scheduled sampling approach. However, Husz\u00e1r (2015) showed that scheduled sampling is a fundamentally inconsistent training strategy, in that it produces largely unstable results in practice.\nThe Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is an appealing and natural answer to the above issues. GAN matches the distributions of synthetic and real data by introducing an adversarial game between a generator and a discriminator. The GAN objective seeks to constitute a generator, that functionally maps samples from a given (simple) prior distribution, to synthetic data that appear to be realistic. The GAN setup explicitly seeks that the latent representations from real data (via encoding) be distributed in a manner consistent with the specified prior (e.g., Gaussian or uniform). Due to the nature of adversarial training, the discriminator compares real and synthetic sentences, rather than their individual words, which in principle should alleviate the exposure-bias issue. Recent work (Lamb et al., 2016) has incorporated an additional discriminator to train a sequence-to-sequence language model that better preserves ar X\niv :1\n70 6.\n03 85\n0v 1\n[ st\nat .M\nL ]\n1 2\nJu n\n20 17\nlong-term dependencies.\nEffort has also been made to generate realistic-looking sentences via adversarial training. For instance, by borrowing ideas from reinforcement learning, Yu et al. (2017); Li et al. (2017) treat the sentence generation as a sequential decision making process. Despite the success of these methods, two fundamental problems of the GAN framework limit their use in practice: (i) the generator tends to produce a single observation for multiple latent representations, i.e., mode collapsing (Metz et al., 2017), and (ii) the generator\u2019s contribution to the learning signal is insubstantial when the discriminator is close to its local optimum, i.e., vanishing gradient behavior (Arjovsky & Bottou, 2017).\nIn this paper we propose a new framework, TextGAN, to alleviate the problems associated with generating realisticlooking sentences via GAN. Specifically, the Long ShortTerm Memory (LSTM) (Hochreiter & Schmidhuber, 1997) RNN is used as generator, and the Convolutional Neural Network (CNN) (Kim, 2014) is used as discriminator. We consider a kernel-based moment-matching scheme over a Reproducing Kernel Hilbert Space (RKHS), to force the empirical distributions of real and synthetic sentences to have matched moments in latent-feature space. As a consequence, our approach ameliorates the mode-collapsing issue associated with standard GAN training. This strategy encourages the model to learn representations that are both informative of the original sentences (via the autoencoder) and discriminative w.r.t. synthetic sentences (via the discriminator). We also propose several complementary techniques, including initialization strategies and discretization approximations to ease GAN training, and to achieve superior performance compared to related approaches."}, {"heading": "2. Model", "text": ""}, {"heading": "2.1. Generative Adversarial Networks", "text": "GAN (Goodfellow et al., 2014) aims to obtain the equilibrium of the following optimization objective\nLGAN = Ex\u223cpx logD(x) + Ez\u223cpz log[1\u2212D(G(z))], (1)\nwhere LGAN is maximized w.r.t. D(\u00b7) and minimized w.r.t. G(\u00b7). Note that the first term of (1) does not depend on G(\u00b7). Observed (real) data, x, are sampled from empirical distribution px(\u00b7). The latent code, z, that feeds into the generator, G(z), is drawn from a simple prior distribution pz(\u00b7). When the discriminator is optimal, solving this adversarial game is equivalent to minimizing the Jenson-Shannon Divergence (JSD) (Arjovsky & Bottou, 2017) between the real data distribution px(\u00b7) and the synthetic data distribution px\u0303(\u00b7) , p(G(z)) , where z \u223c pz(\u00b7) (Goodfellow et al., 2014). However, in most cases, the saddle-point solution of the objective in (1) is intractable. Therefore, a procedure to\niteratively update D(\u00b7) and G(\u00b7) is often applied.\nArjovsky & Bottou (2017) pointed out that the standard GAN objective in (1) suffers from an unstably weak learning signal when the discriminator gets close to local optimal, due to the gradient-vanishing effect. This is because the JSD implied by the original GAN loss becomes a constant if px(\u00b7) and px\u0303(\u00b7) share no support, thus minimizing the JSD yields no learning signal. This problem also exists in the recently proposed energy-based GAN (EBGAN) (Zhao et al., 2017), as the distance metric implied by EBGAN is the Total Variance Distance (TVD), which has the same issue w.r.t. JSD, as shown by Arjovsky et al. (2017)."}, {"heading": "2.2. TextGAN", "text": "Given a sentence corpus S , instead of directly optimizing the objective from standard GAN in (1), we adopt an approach that is similar to the feature matching scheme of Salimans et al. (2016). Specifically, we consider the objective\nLD = LGAN \u2212 \u03bbrLrecon + \u03bbmLMMD2 (2) LG = LMMD2 (3)\nLGAN = Es\u223cS logD(s) + Ez\u223cpz log[1\u2212D(G(z))] Lrecon = ||z\u0302 \u2212 z||2 ,\nwhere LD and LG are iteratively maximized w.r.t D(\u00b7) and minimized w.r.t. G(\u00b7), respectively. LGAN is the standard objective of GAN in (1). Lrecon is the Euclidean distance between the reconstructed latent code, z\u0302, and the original code, z, drawn from prior distribution pz(\u00b7). We denote the synthetic sentences as s\u0303 , G(z), where z \u223c pz(\u00b7). LMMD2 represents the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) between the empirical distribution of sentence embeddings f\u0303 and f , for synthetic and real data, respectively. The model framework is illustrated in Figure 1 and detailed below.\nWe first consider LG in (3). The generator G(\u00b7) attempts to adjust itself to produce synthetic sentence s\u0303, with features\nf\u0303 , encoded by D(\u00b7), to mimic the real sentence features f (also encoded by D(\u00b7)). This is achieved by matching the empirical distributions of f\u0303 and f via the MMD objective.\nConcisely, MMD measures the mean squared difference between two sets of samples X and Y , where X = {xi}i=1:Nx , xi \u2208 Rd, Y = {yi}i=1:Ny , yi \u2208 Rd, d is the dimensionality of the samples, and Nx and Ny are sample sizes for X and Y , respectively. The MMD metric characterizes the differences between X and Y over a Reproducing Kernel Hilbert Space (RKHS), H, associated with kernel function k(\u00b7) : Rd \u00d7 Rd 7\u2192 R. The kernel can be written as an inner product overH: k(x, x\u2032) = \u3008k(x, \u00b7), k(x\u2032, \u00b7)\u3009H, and \u03c6(x) , k(x, \u00b7) \u2208 H is denoted as the feature mapping (Gretton et al., 2012). Formally, the MMD for two empirical distributions X and Y is given by\nLMMD2 = ||Ex\u223cX\u03c6(x)\u2212 Ey\u223cY\u03c6(y)||2H (4) = Ex\u223cXEx\u2032\u223cX [k(x, x\u2032)] + Ey\u223cYEy\u2032\u223cY [k(y, y\u2032)]\u2212 2Ex\u223cXEy\u223cY [k(x, y)].\nNote that LMMD2 reaches its minimum when the two empirical distributions X and Y (in general) match exactly. For example, with a polynomial kernel, k(x, y) = (xT y + c)L, minimizing LMMD2 can be understood as matching moments of two empirical distributions up to order L. With a universal kernel like the Gaussian kernel, k(x, y) = exp(\u2212 ||x\u2212y|| 2\n2\u03c3 ), with bandwidth \u03c3, minimizing the MMD objective will match moments of all orders (Gretton et al., 2012). Here, we use MMD to match the empirical distribution of f\u0303 and f using a Gaussian kernel.\nThe adversarial discriminator D(\u00b7) associated with the loss in (2) aims to produce sentence features that are most discriminative, representative and challenging. These aims are explicitly represented as the three components of (2), namely, (i) LGAN requires f\u0303 and f to be discriminative of real and synthesized sentences; (ii) Lrecon requires f\u0303 and f to preserve maximum reconstruction information for the latent code z that generates synthetic sentences; and (iii) LMMD2 forces D(\u00b7) to select the most challenging features for the generator to match.\nIn the situation for which simple features are enough for the discrimination/reconstruction task, this additional loss seeks to estimate complex features that are difficult for the current generator, thus improving in terms of generation ability. In our experience, we find the reconstruction and MMD loss in D serve as regularizer to the binary classification loss, in that by adding these losses, discriminator features tend to be more spread-out in the feature space.\nIn summary, the adversarial game associated with (2) and (3) is the following: D(\u00b7) attempts to select informative sentence features, while G(\u00b7) aims to match these features. Parameters \u03bbr and \u03bbm act as trade-off between discrim-\nination ability, and reconstruction and moment matching precision, respectively. We argue that this framework has several advantages over the standard GAN objective in (1).\nThe original GAN objective has been shown to be prone to mode collapsing, especially when the so-called logD alternative for the generator loss is used (Metz et al., 2017), i.e., replacing the second term of (1) by \u2212Ez\u223cpz log[D(G(z))]. This is because when logD is used, fake-looking samples are penalized more severely than less diverse samples (Arjovsky & Bottou, 2017), thus grossly underestimating the variance of latent features. The loss in (3), on the other hand, forces the generator to produce highly diverse sentences to match the variation of real sentences, by latent moment matching, thus alleviating the mode-collapsing problem. We believe that leveraging MMD is general enough to be useful as a framework in other data domains, e.g., images. Presumably, the discrete nature of text data makes standard GAN prone to mode-collapsing. This is manifested by close neighbors in latent code space producing the same text output. In our approach, MMD and feature matching are introduced to alleviate mode collapsing with text data as motivating domain. However, whether such an objective is free from the convergence issues of the standard GAN, due to vanishing gradient from the generator, is known to be problem specific (Arjovsky & Bottou, 2017).\nArjovsky & Bottou (2017) demonstrated that JSD yields weak gradient signals when the real and synthetic data are far apart. To deliver stable gradients, a smoother distance metric over the data domain is required. In (4), we are essentially employing a Neural Network (NN) embedding via Gaussian kernel for matching s and s\u0303, i.e., ks(s, s\n\u2032) = \u03c6(g(s))T\u03c6(g(s\u2032)), where g(\u00b7) denotes the NN embedding that maps from the data to the feature domain. Under the assumption that g(\u00b7) is a bijective mapping, i.e., distinct sentences have different embedded feature vectors, in the Supplementary Material we prove that if the original kernel function k(x, y) = \u03c6(x)T\u03c6(y) is universal, the composed kernel ks(s, s\u2032) is also universal. As shown in Gretton et al. (2012), the MMD is a proper metric when the kernel is universal. In fact, if the kernel function is universal, the MMD metric will be no worse than TVD in terms of vanishing gradients (Arjovsky et al., 2017). However, if the bandwidth of the kernel is too small, much smaller than the average distance between data points, the vanishing gradient problem remains (Arjovsky et al., 2017).\nAdditionally, seeking to match the sentence features provides a more achievable and informative objective than directly trying to mislead the discriminator as in standard GAN. Specifically, the loss in (3) implies a clearer aim for the generator, as it requires matching the latent features (distribution-wise) as opposed to uniquely trying to fake a binary classifier.\nNote that if the latent features from real and synthetic data have similar distributions it is unlikely that the discriminator, that uses these features as inputs, will be able to tell them apart. Implementation-wise, the updating signal from the generator does not need to propagate all the way back from the discriminator, but rather directly from the features layer, thus less prone to fading. We believe there may be other possible approaches for text generation using GAN, however, we hope to provide a first attempt toward overcoming some of the difficulties associated with it."}, {"heading": "2.3. Alternative (data efficient) objectives", "text": "One limitation of the proposed approach is that the dimensionality of features f\u0303 and f could be much larger than the size of the subset of data (minibatch) used during learning, hence the empirical distribution may not be sufficiently representative. In fact, a reliable Gaussian kernel MMD twosample test generally requires the size of the minibatch to be proportional to the number of dimensions (Ramdas et al., 2014). To alleviate this issue, we consider two strategies.\nCompressing network We map f\u0303 and f into a lowerdimensional feature space using a compressing network with fully connected layers, also learned by D(\u00b7). This is sensible because the discriminator will still encourage the most challenging features to be abstracted (compressed) from the original features f\u0303 and f . This approach provides significant computational savings, as computation of the MMD in (4) scales with O(d2df ), where df denotes the dimensionality of the feature vector. However, a lower-dimensional mapping may miss valuable information. Besides, finding the optimal mapping dimension may be difficult in practice. There exists a tradeoff between fast estimation and a richer feature vector, by setting df appropriately.\nGaussian covariance matching We could also avoid using the kernel trick, as was used in (4). Instead, we can replace LMMD2 by L (c) G (below), where we accumulate (Gaussian) sufficient statistics from multiple minibatches, thus alleviating the inadequate-minibatch-size issue. Specifically,\nL(c)G = tr(\u03a3\u0303 \u22121\u03a3 + \u03a3\u22121\u03a3\u0303)\n+ (\u00b5\u0303\u2212 \u00b5)T (\u03a3\u0303\u22121 + \u03a3\u22121)(\u00b5\u0303\u2212 \u00b5) , (5)\nwhere \u03a3\u0303 and \u03a3 represent the covariance matrices of synthetic and real sentence feature vectors f\u0303 and f , respectively. \u00b5\u0303 and \u00b5 denote the mean vectors of f\u0303 and f , respectively. By setting \u03a3\u0303 = \u03a3 = I, (5) reduces to the first-moment feature matching technique from Salimans et al. (2016). Note that this loss L(c)G is an upper bound of the JSD (omitting constant, proved in the Supplementary Material) between two multivariate Gaussian distribution N (\u00b5,\u03a3) and\nN (\u00b5\u0303, \u03a3\u0303), which is more tractable than directly minimizing JSD. The feature vectors used in (5) are the neural net outputs before applying any non-linear activation function. We note that the Gaussian assumption may still be strong in many cases. In practice, we use a moving average of the most recent m minibatches for estimating all sufficient statistics \u03a3\u0303,\u03a3, \u00b5\u0303 and \u00b5. Further, \u03a3\u0303 and \u03a3 are initialized to be I to prevent numerical problems."}, {"heading": "2.4. Model specification", "text": "Let wt denote the t-th word in sentence s. Each word wt is embedded into a k-dimensional word vector xt = We[wt], where We \u2208 Rk\u00d7V is a (learned) word embedding matrix, V is the vocabulary size, and notation We[v] denotes the v-th column of matrix We.\nCNN discriminator We use the CNN architecture in Kim (2014); Collobert et al. (2011) for sentence encoding. It consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map. A sentence of length T (padded where necessary) is represented as a matrix X \u2208 Rk\u00d7T , by concatenating its word embeddings as columns, i.e., the t-th column of X is xt.\nAs shown in Figure 2(top), a convolution operation involves a filter Wc \u2208 Rk\u00d7h, applied to a window of h words to produce a new feature. Following Collobert et al. (2011), we induce a latent feature map c = \u03b3(X\u2217Wc+b) \u2208 RT\u2212h+1, where \u03b3(\u00b7) is a nonlinear activation function (we use the hyperbolic tangent, tanh), b \u2208 RT\u2212h+1 is a bias vector,\nand \u2217 denotes the convolutional operator. We then apply a max-over-time pooling operation (Collobert et al., 2011) to the feature map and take its maximum value, i.e., c\u0302 = max{c}, as the feature corresponding to this particular filter. Convolving the same filter with the h-gram at every position in the sentence allows features to be extracted independently of their position in the sentence. This pooling scheme tries to capture the most salient feature, i.e., the one with the highest value, for each feature map, effectively filtering out less informative compositions of words. Further, this pooling scheme also guarantees that the extracted features are independent of the length of the input sentence.\nThe above process describes how one feature is extracted from one filter. In practice, the model uses multiple filters with varying window sizes. Each filter can be considered as a linguistic feature detector that learns to recognize a specific class of h-grams. Assume we have m window sizes, and for each window size, we use p filters, then we obtain a mp-dimensional vector f to represent a sentence. On top of this mp-dimensional feature vector, we specify a softmax layer to map the input sentence to an output D(X) \u2208 [0, 1], representing the probability of X being from the data distribution (real), rather than from the adversarial generator (synthesized).\nThere are other CNN architectures in the literature (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson & Zhang, 2015). We adopt the CNN model of Kim (2014); Collobert et al. (2011) due to its simplicity and excellent performance on sentence classification tasks.\nLSTM generator We specify an LSTM generator to translate a latent code vector, z, into a synthetic sentence s\u0303. This is illustrated in Figure 2(bottom). The probability of a length-T sentence, s\u0303, given the encoded feature vector, z, is defined as\np(s\u0303|z) = p(w\u03031|z) T\u220f t=2 p(w\u0303t|w\u0303<t, z) , (6)\nwhere w\u0303t denotes the t-th generated token. Specifically, we generate the first word w\u03031, deterministically from z, with (w\u03031|z) = argmax(Vh1), where h1 = tanh(Cz). Bias terms are omitted for simplicity. All other words in the sentence are sequentially generated using the RNN, based on previously generated words, until the end-sentence symbol is generated. The t-th word w\u0303t is generated as (w\u0303t|w\u0303<t, z) = argmax(Vht), where < t , {1, . . . , t\u2212 1}, and the hidden units ht are recursively updated through ht = U(yt\u22121,ht\u22121, z). V is a weight matrix used for computing a distribution over words. The input yt\u22121 for the t-th step is the embedding vector of the previous generated word w\u0303t\u22121, i.e.,\nyt\u22121 = We[w\u0303 t\u22121] . (7)\nThe synthetic sentence s\u0303 = [w\u03031, \u00b7 \u00b7 \u00b7 , w\u0303L] is deterministically obtained given z by concatenating the generated words. In experiments, the transition function, U(\u00b7), is implemented with an LSTM (Hochreiter & Schmidhuber, 1997). Details are provided in the Supplementary Material."}, {"heading": "2.5. Training Techniques", "text": "Soft-argmax approximation To train the generator G(\u00b7), which contains discrete variables, direct application of the gradient estimation may be difficult (Yu et al., 2017). Scorefunction-based approaches, such as the REINFORCE algorithm (Williams, 1992), achieve unbiased gradient estimation for discrete variables using Monte Carlo estimation. However, in our experiments, we found that the variance of the gradient estimation is very large, which is consistent with Maddison et al. (2017). Here we consider a soft-argmax operator (Zhang et al., 2016), similar to the Gumbel-softmax (Gumbel & Lieblein, 1954; Jang et al., 2017), when performing learning, as an approximation to (7):\nyt\u22121 = Wesoftmax(Vht\u22121 L) . (8)\nwhere represents the element-wise product. Note that when L\u2192\u221e, this approximation approaches (7).\nPre-training Previous literature (Goodfellow et al., 2014; Salimans et al., 2016) has discussed the fundamental difficulty of training GANs using gradient-based methods. In general, gradient descent optimization schemes may fail to converge to the equilibrium by moving along the orbit trajectory among saddle points (Salimans et al., 2016). Intuitively, good initialization can facilitate convergence. Toward this end, we initialize the LSTM parameters of the generator by pre-training a standard CNN-LSTM autoencoder (Gan et al., 2016). For the discriminator/encoder initialization, we use a permutation training strategy. For each sentence in the corpus, we randomly swap two words to construct a slightly tweaked sentence counterpart. The discriminator is pre-trained to distinguish the tweaked sentences from the true sentences. The swapping operation is preferred here because it constitutes a much more challenging task for the discriminator to learn, compared to adding or deleting words, where the structure of real sentences is more strongly disrupted, thus making it easier for the discriminator. The permutation pre-training is important because it requires the discriminator to learn features characteristic of sentences\u2019 long dependencies. We empirically found this provides a better initialization (compared to no pre-training) for the discriminator to learn good features.\nWe also utilized other training techniques to stabilize training, such as soft-labeling (Salimans et al., 2016). Details of these are provided in the Supplementary Material."}, {"heading": "3. Related Work", "text": "Generative Moment Matching Networks (GMMNs) (Dziugaite et al., 2015; Li et al., 2015) are closely related to our approach. However, these methods either directly match the empirical distribution in the data domain, or extract features using a pre-trained autoencoder (Li et al., 2015). If the goal is to perform matching in the data domain when generating sentences, the dimensionality of input data would be T \u00d7 k (higher than 10,000 in our case). Note that the minibatch size required to obtain reasonable statistical power grows linearly with the number of dimension (Ramdas et al., 2014), and the computational cost of MMD grows quadratically with the size of data points. Therefore, directly applying GMMNs is often computationally prohibitive. Furthermore, directly matching in the data domain via GMMNs implies word-by-word discrepancy, which yields less smooth gradients. This happens because a word-by-word discrepancy ignores sentence structure. For example, two sentences \u201ca boy is swimming\u201d and \u201cboy is swimming\u201d will be far apart in a word-by-word metric, when they are indeed close in a sentence-by-sentence feature space.\nA two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above. However, in Li et al. (2015) the feature encoder is fixed once pre-trained, limiting the potential to adjust features during the training phase. Alternatively, our approach matches the real and synthetic data on a sentence feature space, where features are dynamically and adversarially adapted to focus on the most challenging features for the generator to mimic. In addition, features are designed to maintain both discrimination and reconstruction ability, instead of merely focusing on reconstruction as in Li et al. (2015).\nRecent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016). They demonstrated superior performance on image generation. Our approach is similar to these approaches; however, we attempt to learn the reconstruction of the latent code, instead of the input data (sentences). Donahue et al. (2017) learned a reverse mapping from data space to latent space. In our approach we enforce the discriminator and encoder to share a latent structure, with the aim of learning a representation for both discrimination and latent code reconstruction. Chen et al. (2016) maximized the mutual information between the generated data and the latent codes by leveraging a network-adapted variational proposal distribution. In our case, we minimize the distance between the original and reconstructed latent codes.\nOur approach attempts to minimize a NN-based embedded MMD distance of two empirical distributions. Aside from MMD, kernel-based discrepancy metrics such as kernelized\nStein discrepancy (Liu et al., 2016; Wang & Liu, 2016) have been shown to be computationally tractable, while maintaining statistical power. We leave the investigation of using Stein for moment matching as a promising future direction. Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al. (2017). The EM metric yields stable gradients, thus avoiding the collapsing mode and vanishing gradient problem of the latter two. We note that our approach is equivalent to minimizing a MMD loss over the data domain, however, with a NNbased embedded Gaussian kernel. As shown in Arjovsky et al. (2017), MMD is a proper metric when the kernel is universal. Because of the similarity of the conditions, our approach enjoys the advantages of Wasserstein GAN, namely, ameliorating the gradient vanishing problems."}, {"heading": "4. Experiments", "text": "Data and Experimental Setup Our model is trained using a combination of two datasets: (i) the BookCorpus dataset (Zhu et al., 2015), which consists of 70 million sentences from over 7000 books; and (ii) the ArXiv dataset, which consists of 5 million sentences from abstracts of papers from various subjects, obtained from the arXiv website. The motivation for merging two different corpora is to investigate whether the model can generate sentences that integrate both scientific and informal writing styles. We randomly choose 0.5 million sentences from BookCorpus and 0.5 million sentences from arXiv to construct training and validation sets, i.e., 1 million sentences for each. For testing, we randomly select 25,000 sentences from both corpus, for a total of 50,000 sentences.\nWe train the generator and discriminator/encoder iteratively. Provided that the LSTM generator typically involves more parameters and is more difficult to train than the CNN discriminator, we perform one optimization step for the discriminator for every K = 5 steps of the generator. We use a mixture of 5 isotropic Gaussian (RBF) kernels with different bandwidths \u03c3 as in Li et al. (2015). Bandwidth parameters are selected to be close to the median distance (in our case around 20) of feature vectors encoded from real sentences. \u03bbr and \u03bbm are selected based on the performance on the validation set. The validation performance is evaluated by loss of generator and corpus-level BLEU score (Papineni et al., 2002), described below.\nFor the CNN discriminator/encoder, we use filter windows (h) of sizes {3,4,5} with 300 feature maps each, hence each sentence is represented as a 900-dimensional vector. The dimensionality of z and z\u0302 is also 900. The feature vector is then fed into a 900-200-2 fully connected network for the discriminator and 900-900-900 for encoder, with sigmoid\nactivation units connecting the intermediate layers and softmax/tanh units for the top layer of discriminator/encoder. We did not observe performance changes by adding dropout. For the LSTM sentence generator, we use one hidden layer of 500 units.\nGradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014). Adam (Kingma & Ba, 2015) with learning rate 5 \u00d7 10\u22125 for both discriminator and generator is utilized for optimization. The size of the minibatch is set to 256.\nBoth the generator and the discriminator are pre-trained using the strategies described in Section 2. We also employed a warm-up training during the first two epochs, as we found it improves convergence during the initial stage of learning. Specifically, we use a mean-matching objective for the generator loss, i.e., ||Ef\u2212Ef\u0303 ||2, as in Salimans et al. (2016). Further details of the experimental design are provided in the the Supplementary Material. All experiments are implemented in Theano (Bastien et al., 2012), using one NVIDIA GeForce GTX TITAN X GPU with 12GB memory. The model was trained for 50 epochs in roughly 3 days. Learning curves are shown in the Supplementary Material.\nMatching feature distributions We first examine the generator\u2019s ability to produce synthetic features similar to those obtained from real data. For this purpose, we calculate the empirical expectation of the 900-dimensional sentence feature vector over 2,000 real sentences and 2,000 synthetic sentences. As shown in Figure 3(left), the expectation of these 900 feature dimensions from synthetic sentences matches well with the feature expectation from the real sentences. We also compared the estimated covariance matrix elements \u03a3\u0303i,j,f (including 900 \u2217 899/2 off-diagonal elements and 900 diagonal elements) from real data against the covariance matrix elements \u03a3\u0303i,j,f\u0303 estimated from synthetic data, in Figure 3(right). We observe that the covariance structure of the 900-dimensional features from real and synthetic sentences in general match well. The full covariance matrices for real and synthetic sentences are provided in\nthe Supplementary Material. We observe that the (mapped) synthetic features nicely cover the real sentence features density, while \u201ccompleting\u201d other areas of low density.\nQuantitative comparison We evaluate the generatedsentence quality using the BLEU score (Papineni et al., 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al. (2016). For comparison, we consider textGAN with 4 different loss objectives: Mean Matching (MM) as in Salimans et al. (2016), Covariance Matching (CM) as in (5), MMD and MMD with compressed network (MMD-L), by mapping the original 900-dimensional features to 200-dimensional, as described in Section 2.3. We also compare to a baseline autoencoder (AE) model. The AE uses a CNN as encoder and an LSTM as decoder, where the CNN and LSTM network structures are set to be identical as the CNN and LSTM used in textGAN. We finally consider a Variational Autoencoder (VAE) implemented as in Bowman et al. (2016). To train the VAE model, we use annealing to gradually increase the KL divergence between the prior and approximated posterior. The details are provided in the the Supplementary Material. We also compare with seqGAN (Yu et al., 2017). For seqGAN we follow the authors\u2019 guidelines of running 350 pre-training epochs followed by 50 discriminator training epochs, to generate 320 sentences. For AE, VAE and textGAN, we first uniformly sample 320 latent codes from the latent code space, and use the corresponding generator (or decoder, in the AE/VAE case) to generate sentences.\nFor BLEU score evaluation, we follow the strategy in Yu et al. (2017) of using the entire test set as the reference. For KDE evaluation, the lengths of the generated sentences are different, thus we first embed all the sentences to a 900- dimensional vector. Since no standard sentence encoder is available, we use the encoder learned from AE. The covariance matrix for the Parzen kernel in KDE is set to be the covariance of feature vectors for real tested sentences. Despite the fact that the KDE approach, as a log-likelihood estimator tends to have high variance (Theis et al., 2016), the KDE score tracks well with our BLEU score evaluation.\nThe results are shown in Table 1. MMD and MMD-L generally score higher in sentences quality. MMD-L seems better at capturing 2-grams (BLEU-2), while MMD outperforms MMD-L in 4-grams (BLEU-4). We also observed that when using CM, the generated sentences tend to be shorter than\nMMD (not shown).\nGenerated sentences Table 2 shows six sentences generated by textGAN. Note that the generated sentences seem to be able to produce novel phrases by imagining concept combinations, e.g., in Table 2(b,c,f), or to borrow words from a different corpus to compose novel sentences, e.g., in Table 2(d,e). In many cases, it learns to automatically match the parentheses and quotation marks, e.g., in Table 2(a), and can synthesize relatively long sentences, e.g., in 2(a,f). In general, the synthetic sentences seem syntactically reasonable. However, the semantic meaning is less well preserved especially in sentence of more than 20 words, e.g., in Table 2(e,f).\nWe observe that the discriminator can still sufficiently distinguish the synthetic sentences from the real ones (the probability to predict synthetic data as real is around 0.05), even when the synthetic sentences seems to perserve reasonable grammatical structure and use proper wording. It is likely that the CNN is able to accurately characterize the semantic meaning and differentiate sentences, while the generator may get trapped into a local optimum, where any slight modification would result in a higher loss (3) for the generator. Presumably, long-range distance features are not difficult to abstract by the discriminator/encoder, however, is less likely to be imitated by the generator. One promising direction is to leverage reinforcement learning strategies as in Yu et al. (2017), where the updating for LSTM can be more effectively steered. Nevertheless, investigation on how to improve the the long-range behavior is left as interesting future work.\nLatent feature space trajectories Following Bowman et al. (2016), we further empirically evaluate whether the latent variable space can \u201cdensely\u201d encode sentences. We visualize the transition from one sentence to another by constructing a linear path between two randomly selected points in latent feature space, to then generate the intermediate sentences along the linear trajectory. For comparison, a baseline autoencoder (AE) is trained for 20 epochs. The results for textGAN and AE are presented in Table 3. Compared to AE, the sentences produced by textGAN are generally\nmore syntactically and semantically reasonable. The transition suggest \u201csmoothness\u201d and interpretability, however, the wording choices and sentence structure showed dramatic changes in some regions in the latent feature space. This seems to indicate that local \u201ctransition smoothness\u201d varies from region to region."}, {"heading": "5. Conclusion", "text": "We have introduced a novel approach for text generation using adversarial training, termed TextGAN, and have discussed several techniques to specify and train such a model. We demonstrated that the proposed model delivers superior performance compared to related approaches, can produce realistic sentences, and that the learned latent representation space can \u201csmoothly\u201d encode plausible sentences. We quantitatively evaluate the proposed methods with baseline models and existing methods. The results indicate superior performance of TextGAN.\nIn future work, we will attempt to apply conditional GAN models (Mirza & Osindero, 2014) to disentangle the latent representations for different writing styles. This would enable a smooth lexical and grammatical transition between different writing styles. It would be also interesting to generate text by conditioning on observed images (Pu et al., 2016). In addition, we plan to leverage an additional refining stage where a reverse-order LSTM (Graves & Schmidhuber, 2005) is applied after the sentence is first generated, to produce sentences with better long-term semantical interpretation."}, {"heading": "Acknowledgments", "text": "This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF."}], "references": [{"title": "Towards principled methods for training generative adversarial networks", "author": ["Arjovsky", "Martin", "Bottou", "L\u00e9on"], "venue": "In ICLR,", "citeRegEx": "Arjovsky et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Arjovsky et al\\.", "year": 2017}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam"], "venue": "In NIPS,", "citeRegEx": "Bengio et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Generating sentences from a continuous space", "author": ["Bowman", "Samuel R", "Vilnis", "Luke", "Vinyals", "Oriol", "Dai", "Andrew M", "Jozefowicz", "Rafal", "Bengio", "Samy"], "venue": "In CoNLL,", "citeRegEx": "Bowman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2016}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Chen", "Xi", "Duan", "Yan", "Houthooft", "Rein", "Schulman", "John", "Sutskever", "Ilya", "Abbeel", "Pieter"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In EMNLP,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "In JMLR,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Adversarial feature learning", "author": ["Donahue", "Jeff", "Kr\u00e4henb\u00fchl", "Philipp", "Darrell", "Trevor"], "venue": "In ICLR,", "citeRegEx": "Donahue et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2017}, {"title": "Training generative neural networks via maximum mean discrepancy optimization", "author": ["Dziugaite", "Gintare Karolina", "Roy", "Daniel M", "Ghahramani", "Zoubin"], "venue": null, "citeRegEx": "Dziugaite et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dziugaite et al\\.", "year": 2015}, {"title": "Unsupervised learning of sentence representations using convolutional neural networks", "author": ["Gan", "Zhe", "Pu", "Yunchen", "Henao", "Ricardo", "Li", "Chunyuan", "He", "Xiaodong", "Carin", "Lawrence"], "venue": "arXiv preprint arXiv:1611.07897,", "citeRegEx": "Gan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gan et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Goodfellow", "Ian", "Pouget-Abadie", "Jean", "Mirza", "Mehdi", "Xu", "Bing", "Warde-Farley", "David", "Ozair", "Sherjil", "Courville", "Aaron", "Bengio", "Yoshua"], "venue": "In NIPS,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Alex", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "A kernel two-sample test", "author": ["Gretton", "Arthur", "Borgwardt", "Karsten M", "Rasch", "Malte J", "Sch\u00f6lkopf", "Bernhard", "Smola", "Alexander"], "venue": null, "citeRegEx": "Gretton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gretton et al\\.", "year": 2012}, {"title": "Statistical theory of extreme values and some practical applications: a series of lectures", "author": ["Gumbel", "Emil Julius", "Lieblein", "Julius"], "venue": null, "citeRegEx": "Gumbel et al\\.,? \\Q1954\\E", "shortCiteRegEx": "Gumbel et al\\.", "year": 1954}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "In Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "In NIPS,", "citeRegEx": "Hu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "How (not) to train your generative model: Scheduled sampling, likelihood, adversary", "author": ["Husz\u00e1r", "Ferenc"], "venue": null, "citeRegEx": "Husz\u00e1r and Ferenc.,? \\Q2015\\E", "shortCiteRegEx": "Husz\u00e1r and Ferenc.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Ioffe", "Sergey", "Szegedy", "Christian"], "venue": "In ICML,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Categorical reparameterization with gumbel-softmax", "author": ["Jang", "Eric", "Gu", "Shixiang", "Poole", "Ben"], "venue": "In ICLR,", "citeRegEx": "Jang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Jang et al\\.", "year": 2017}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["R. Johnson", "T. Zhang"], "venue": "In NAACL HLT,", "citeRegEx": "Johnson and Zhang,? \\Q2015\\E", "shortCiteRegEx": "Johnson and Zhang", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In EMNLP,", "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In ICLR,", "citeRegEx": "Kingma and Ba,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2015}, {"title": "Auto-encoding variational bayes", "author": ["Kingma", "Diederik P", "Welling", "Max"], "venue": "In ICLR,", "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Professor forcing: A new algorithm for training recurrent networks", "author": ["Lamb", "Alex M", "GOYAL", "Anirudh Goyal ALIAS PARTH", "Zhang", "Ying", "Saizheng", "Courville", "Aaron C", "Bengio", "Yoshua"], "venue": null, "citeRegEx": "Lamb et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lamb et al\\.", "year": 2016}, {"title": "Autoencoding beyond pixels using a learned similarity metric", "author": ["Larsen", "Anders Boesen Lindbo", "S\u00f8nderby", "S\u00f8ren Kaae", "Larochelle", "Hugo", "Winther", "Ole"], "venue": null, "citeRegEx": "Larsen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsen et al\\.", "year": 2016}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Ritter", "Alan", "Galley", "Michel", "Gao", "Jianfeng", "Jurafsky", "Dan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Adversarial learning for neural dialogue generation", "author": ["Li", "Jiwei", "Monroe", "Will", "Shi", "Tianlin", "Ritter", "Alan", "Jurafsky", "Dan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Li et al\\.", "year": 2017}, {"title": "Generative moment matching networks", "author": ["Li", "Yujia", "Swersky", "Kevin", "Zemel", "Richard S"], "venue": "In ICML,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "A kernelized stein discrepancy for goodness-of-fit tests", "author": ["Liu", "Qiang", "Lee", "Jason D", "Jordan", "Michael I"], "venue": "In ICML,", "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Maaten", "Laurens van der", "Hinton", "Geoffrey"], "venue": null, "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "The concrete distribution: A continuous relaxation of discrete random variables", "author": ["Maddison", "Chris J", "Mnih", "Andriy", "Teh", "Yee Whye"], "venue": "In ICLR,", "citeRegEx": "Maddison et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Maddison et al\\.", "year": 2017}, {"title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks", "author": ["Mescheder", "Lars", "Nowozin", "Sebastian", "Geiger", "Andreas"], "venue": "In ICML,", "citeRegEx": "Mescheder et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Mescheder et al\\.", "year": 2017}, {"title": "Unrolled generative adversarial networks", "author": ["Metz", "Luke", "Poole", "Ben", "Pfau", "David", "Sohl-Dickstein", "Jascha"], "venue": "In ICLR,", "citeRegEx": "Metz et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Metz et al\\.", "year": 2017}, {"title": "Conditional generative adversarial nets", "author": ["Mirza", "Mehdi", "Osindero", "Simon"], "venue": null, "citeRegEx": "Mirza et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mirza et al\\.", "year": 2014}, {"title": "f-gan: Training generative neural samplers using variational divergence minimization", "author": ["Nowozin", "Sebastian", "Cseke", "Botond", "Tomioka", "Ryota"], "venue": "In NIPS,", "citeRegEx": "Nowozin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nowozin et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni", "Kishore", "Roukos", "Salim", "Ward", "Todd", "Zhu", "Wei-Jing"], "venue": "In ACL,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Variational autoencoder for deep learning of images, labels and captions", "author": ["Pu", "Yunchen", "Gan", "Zhe", "Henao", "Ricardo", "Yuan", "Xin", "Li", "Chunyuan", "Stevens", "Andrew", "Carin", "Lawrence"], "venue": null, "citeRegEx": "Pu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Pu et al\\.", "year": 2016}, {"title": "On the high-dimensional power of linear-time kernel two-sample testing under mean-difference alternatives", "author": ["Ramdas", "Aaditya", "Reddi", "Sashank J", "Poczos", "Barnabas", "Singh", "Aarti", "Wasserman", "Larry"], "venue": null, "citeRegEx": "Ramdas et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ramdas et al\\.", "year": 2014}, {"title": "Improved techniques for training gans", "author": ["Salimans", "Tim", "Goodfellow", "Ian", "Zaremba", "Wojciech", "Cheung", "Vicki", "Radford", "Alec", "Chen", "Xi"], "venue": "In NIPS,", "citeRegEx": "Salimans et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Salimans et al\\.", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. Le"], "venue": "In NIPS,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A note on the evaluation of generative models", "author": ["Theis", "Lucas", "Oord", "A\u00e4ron van den", "Bethge", "Matthias"], "venue": "In ICLR,", "citeRegEx": "Theis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2016}, {"title": "Learning to draw samples: With application to amortized mle for generative adversarial learning", "author": ["Wang", "Dilin", "Liu", "Qiang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Williams", "Ronald J"], "venue": "Machine learning,", "citeRegEx": "Williams and J.,? \\Q1992\\E", "shortCiteRegEx": "Williams and J.", "year": 1992}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["Yu", "Lantao", "Zhang", "Weinan", "Wang", "Jun", "Yong"], "venue": "In AAAI,", "citeRegEx": "Yu et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2017}, {"title": "Generating text via adversarial training", "author": ["Zhang", "Yizhe", "Gan", "Zhe", "Carin", "Lawrence"], "venue": "In NIPS Workshop on Adversarial Training,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Energy-based generative adversarial network", "author": ["Zhao", "Junbo", "Mathieu", "Michael", "LeCun", "Yann"], "venue": "In ICLR,", "citeRegEx": "Zhao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "author": ["Y. Zhu", "R. Kiros", "R. Zemel", "R. Salakhutdinov", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "In ICCV,", "citeRegEx": "Zhu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 25, "context": "For instance, in the context of dialog generation, it is desirable to generate answers that are more diverse and less generic (Li et al., 2016).", "startOffset": 126, "endOffset": 143}, {"referenceID": 4, "context": "One simple approach consists of first learning a latent space to represent (fixed-length) sentences using an encoderdecoder (autoencoder) framework based on Recurrent Neural Networks (RNNs) (Cho et al., 2014; Sutskever et al., 2014), then generate synthetic sentences by decoding ran-", "startOffset": 190, "endOffset": 232}, {"referenceID": 39, "context": "One simple approach consists of first learning a latent space to represent (fixed-length) sentences using an encoderdecoder (autoencoder) framework based on Recurrent Neural Networks (RNNs) (Cho et al., 2014; Sutskever et al., 2014), then generate synthetic sentences by decoding ran-", "startOffset": 190, "endOffset": 232}, {"referenceID": 2, "context": "The reason for this is that, when mapping sentences to their latent representations using an autoencoder, the mappings usually cover a small but structured region of the latent space, which corresponds to a manifold embedding (Bowman et al., 2016).", "startOffset": 226, "endOffset": 247}, {"referenceID": 2, "context": "The reason for this is that, when mapping sentences to their latent representations using an autoencoder, the mappings usually cover a small but structured region of the latent space, which corresponds to a manifold embedding (Bowman et al., 2016). In practice, most regions of the latent space do not necessarily map (decode) to realistic sentences. Consequently, randomly sampling latent representations often yields nonsensical sentences. Recent work by Bowman et al. (2016) has attempted to generate more diverse sentences via RNN-based variational autoencoders.", "startOffset": 227, "endOffset": 478}, {"referenceID": 1, "context": "Bengio et al. (2015) coined this phenomenon exposure bias.", "startOffset": 0, "endOffset": 21}, {"referenceID": 1, "context": "Bengio et al. (2015) coined this phenomenon exposure bias. Toward addressing this problem, Bengio et al. (2015) proposed the scheduled sampling approach.", "startOffset": 0, "endOffset": 112}, {"referenceID": 1, "context": "Bengio et al. (2015) coined this phenomenon exposure bias. Toward addressing this problem, Bengio et al. (2015) proposed the scheduled sampling approach. However, Husz\u00e1r (2015) showed that scheduled sampling is a fundamentally inconsistent training strategy, in that it produces largely unstable results in practice.", "startOffset": 0, "endOffset": 177}, {"referenceID": 9, "context": "The Generative Adversarial Network (GAN) (Goodfellow et al., 2014) is an appealing and natural answer to the above issues.", "startOffset": 41, "endOffset": 66}, {"referenceID": 23, "context": "Recent work (Lamb et al., 2016) has incorporated an additional discriminator to train a sequence-to-sequence language model that better preserves ar X iv :1 70 6.", "startOffset": 12, "endOffset": 31}, {"referenceID": 32, "context": ", mode collapsing (Metz et al., 2017), and (ii) the generator\u2019s contribution to the learning signal is insubstantial when the discriminator is close to its local optimum, i.", "startOffset": 18, "endOffset": 37}, {"referenceID": 39, "context": "For instance, by borrowing ideas from reinforcement learning, Yu et al. (2017); Li et al.", "startOffset": 62, "endOffset": 79}, {"referenceID": 25, "context": "(2017); Li et al. (2017) treat the sentence generation as a sequential decision making process.", "startOffset": 8, "endOffset": 25}, {"referenceID": 20, "context": "Specifically, the Long ShortTerm Memory (LSTM) (Hochreiter & Schmidhuber, 1997) RNN is used as generator, and the Convolutional Neural Network (CNN) (Kim, 2014) is used as discriminator.", "startOffset": 149, "endOffset": 160}, {"referenceID": 9, "context": "GAN (Goodfellow et al., 2014) aims to obtain the equilibrium of the following optimization objective", "startOffset": 4, "endOffset": 29}, {"referenceID": 9, "context": "When the discriminator is optimal, solving this adversarial game is equivalent to minimizing the Jenson-Shannon Divergence (JSD) (Arjovsky & Bottou, 2017) between the real data distribution px(\u00b7) and the synthetic data distribution px\u0303(\u00b7) , p(G(z)) , where z \u223c pz(\u00b7) (Goodfellow et al., 2014).", "startOffset": 267, "endOffset": 292}, {"referenceID": 45, "context": "This problem also exists in the recently proposed energy-based GAN (EBGAN) (Zhao et al., 2017), as the distance metric implied by EBGAN is the Total Variance Distance (TVD), which has the same issue w.", "startOffset": 75, "endOffset": 94}, {"referenceID": 0, "context": "JSD, as shown by Arjovsky et al. (2017).", "startOffset": 17, "endOffset": 40}, {"referenceID": 38, "context": "Given a sentence corpus S , instead of directly optimizing the objective from standard GAN in (1), we adopt an approach that is similar to the feature matching scheme of Salimans et al. (2016). Specifically, we consider the objective", "startOffset": 170, "endOffset": 193}, {"referenceID": 11, "context": "LMMD2 represents the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) between the empirical distribution of sentence embeddings f\u0303 and f , for synthetic and real data,", "startOffset": 52, "endOffset": 74}, {"referenceID": 11, "context": "The kernel can be written as an inner product overH: k(x, x\u2032) = \u3008k(x, \u00b7), k(x\u2032, \u00b7)\u3009H, and \u03c6(x) , k(x, \u00b7) \u2208 H is denoted as the feature mapping (Gretton et al., 2012).", "startOffset": 143, "endOffset": 165}, {"referenceID": 11, "context": "With a universal kernel like the Gaussian kernel, k(x, y) = exp(\u2212 ||x\u2212y|| 2 2\u03c3 ), with bandwidth \u03c3, minimizing the MMD objective will match moments of all orders (Gretton et al., 2012).", "startOffset": 162, "endOffset": 184}, {"referenceID": 32, "context": "The original GAN objective has been shown to be prone to mode collapsing, especially when the so-called logD alternative for the generator loss is used (Metz et al., 2017), i.", "startOffset": 152, "endOffset": 171}, {"referenceID": 0, "context": "In fact, if the kernel function is universal, the MMD metric will be no worse than TVD in terms of vanishing gradients (Arjovsky et al., 2017).", "startOffset": 119, "endOffset": 142}, {"referenceID": 0, "context": "However, if the bandwidth of the kernel is too small, much smaller than the average distance between data points, the vanishing gradient problem remains (Arjovsky et al., 2017).", "startOffset": 153, "endOffset": 176}, {"referenceID": 10, "context": "As shown in Gretton et al. (2012), the MMD is a proper metric when the kernel is universal.", "startOffset": 12, "endOffset": 34}, {"referenceID": 37, "context": "In fact, a reliable Gaussian kernel MMD twosample test generally requires the size of the minibatch to be proportional to the number of dimensions (Ramdas et al., 2014).", "startOffset": 147, "endOffset": 168}, {"referenceID": 38, "context": "By setting \u03a3\u0303 = \u03a3 = I, (5) reduces to the first-moment feature matching technique from Salimans et al. (2016). Note that this loss L G is an upper bound of the JSD (omitting constant, proved in the Supplementary Material) between two multivariate Gaussian distribution N (\u03bc,\u03a3) and This", "startOffset": 87, "endOffset": 110}, {"referenceID": 19, "context": "CNN discriminator We use the CNN architecture in Kim (2014); Collobert et al.", "startOffset": 49, "endOffset": 60}, {"referenceID": 5, "context": "CNN discriminator We use the CNN architecture in Kim (2014); Collobert et al. (2011) for sentence encoding.", "startOffset": 61, "endOffset": 85}, {"referenceID": 5, "context": "Following Collobert et al. (2011), we induce a latent feature map c = \u03b3(X\u2217Wc+b) \u2208 RT\u2212h+1, where \u03b3(\u00b7) is a nonlinear activation function (we use the hyperbolic tangent, tanh), b \u2208 RT\u2212h+1 is a bias vector,", "startOffset": 10, "endOffset": 34}, {"referenceID": 5, "context": "We then apply a max-over-time pooling operation (Collobert et al., 2011) to the feature map and take its maximum value, i.", "startOffset": 48, "endOffset": 72}, {"referenceID": 19, "context": "There are other CNN architectures in the literature (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson & Zhang, 2015).", "startOffset": 52, "endOffset": 119}, {"referenceID": 14, "context": "There are other CNN architectures in the literature (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson & Zhang, 2015).", "startOffset": 52, "endOffset": 119}, {"referenceID": 13, "context": ", 2014; Hu et al., 2014; Johnson & Zhang, 2015). We adopt the CNN model of Kim (2014); Collobert et al.", "startOffset": 8, "endOffset": 86}, {"referenceID": 5, "context": "We adopt the CNN model of Kim (2014); Collobert et al. (2011) due to its simplicity and excellent performance on sentence classification tasks.", "startOffset": 38, "endOffset": 62}, {"referenceID": 43, "context": "Soft-argmax approximation To train the generator G(\u00b7), which contains discrete variables, direct application of the gradient estimation may be difficult (Yu et al., 2017).", "startOffset": 153, "endOffset": 170}, {"referenceID": 44, "context": "Here we consider a soft-argmax operator (Zhang et al., 2016), similar to the Gumbel-softmax (Gumbel & Lieblein, 1954; Jang et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 17, "context": ", 2016), similar to the Gumbel-softmax (Gumbel & Lieblein, 1954; Jang et al., 2017), when performing learning, as an approximation to (7):", "startOffset": 39, "endOffset": 83}, {"referenceID": 29, "context": "However, in our experiments, we found that the variance of the gradient estimation is very large, which is consistent with Maddison et al. (2017). Here we consider a soft-argmax operator (Zhang et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 9, "context": "Pre-training Previous literature (Goodfellow et al., 2014; Salimans et al., 2016) has discussed the fundamental difficulty of training GANs using gradient-based methods.", "startOffset": 33, "endOffset": 81}, {"referenceID": 38, "context": "Pre-training Previous literature (Goodfellow et al., 2014; Salimans et al., 2016) has discussed the fundamental difficulty of training GANs using gradient-based methods.", "startOffset": 33, "endOffset": 81}, {"referenceID": 38, "context": "In general, gradient descent optimization schemes may fail to converge to the equilibrium by moving along the orbit trajectory among saddle points (Salimans et al., 2016).", "startOffset": 147, "endOffset": 170}, {"referenceID": 8, "context": "Toward this end, we initialize the LSTM parameters of the generator by pre-training a standard CNN-LSTM autoencoder (Gan et al., 2016).", "startOffset": 116, "endOffset": 134}, {"referenceID": 38, "context": "We also utilized other training techniques to stabilize training, such as soft-labeling (Salimans et al., 2016).", "startOffset": 88, "endOffset": 111}, {"referenceID": 7, "context": "Generative Moment Matching Networks (GMMNs) (Dziugaite et al., 2015; Li et al., 2015) are closely related to our approach.", "startOffset": 44, "endOffset": 85}, {"referenceID": 27, "context": "Generative Moment Matching Networks (GMMNs) (Dziugaite et al., 2015; Li et al., 2015) are closely related to our approach.", "startOffset": 44, "endOffset": 85}, {"referenceID": 27, "context": "However, these methods either directly match the empirical distribution in the data domain, or extract features using a pre-trained autoencoder (Li et al., 2015).", "startOffset": 144, "endOffset": 161}, {"referenceID": 37, "context": "Note that the minibatch size required to obtain reasonable statistical power grows linearly with the number of dimension (Ramdas et al., 2014), and the computational cost of MMD grows quadratically with the size of data points.", "startOffset": 121, "endOffset": 142}, {"referenceID": 25, "context": "A two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above.", "startOffset": 68, "endOffset": 85}, {"referenceID": 25, "context": "A two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above. However, in Li et al. (2015) the feature encoder is fixed once pre-trained, limiting the potential to adjust features during the training phase.", "startOffset": 68, "endOffset": 150}, {"referenceID": 25, "context": "A two-step method, where a feature encoder is generated first as in Li et al. (2015) helps alleviate the problems above. However, in Li et al. (2015) the feature encoder is fixed once pre-trained, limiting the potential to adjust features during the training phase. Alternatively, our approach matches the real and synthetic data on a sentence feature space, where features are dynamically and adversarially adapted to focus on the most challenging features for the generator to mimic. In addition, features are designed to maintain both discrimination and reconstruction ability, instead of merely focusing on reconstruction as in Li et al. (2015).", "startOffset": 68, "endOffset": 649}, {"referenceID": 45, "context": "Recent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 24, "context": "Recent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 31, "context": "Recent work considered combining autoencoders or variational autoencoders (Kingma & Welling, 2014) with GAN (Zhao et al., 2017; Larsen et al., 2016; Makhzani et al., 2015; Mescheder et al., 2017; Wang & Liu, 2016).", "startOffset": 108, "endOffset": 213}, {"referenceID": 5, "context": "Donahue et al. (2017) learned a reverse mapping from data space to latent space.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "Chen et al. (2016) maximized the mutual information between the generated data and the latent codes by leveraging a network-adapted variational proposal distribution.", "startOffset": 0, "endOffset": 19}, {"referenceID": 28, "context": "Aside from MMD, kernel-based discrepancy metrics such as kernelized Stein discrepancy (Liu et al., 2016; Wang & Liu, 2016) have been shown to be computationally tractable, while maintaining statistical power.", "startOffset": 86, "endOffset": 122}, {"referenceID": 0, "context": "Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 9, "context": ", 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al.", "startOffset": 135, "endOffset": 160}, {"referenceID": 0, "context": "Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al. (2017). The EM metric yields stable gradients, thus avoiding the collapsing mode and vanishing gradient problem of the latter two.", "startOffset": 17, "endOffset": 225}, {"referenceID": 0, "context": "Wasserstein GAN (Arjovsky et al., 2017) considers an Earth-Mover (EM) distance of the real data and synthetic data distribution, instead of the JSD as in standard GAN (Goodfellow et al., 2014) or TVD as in Zhao et al. (2017). The EM metric yields stable gradients, thus avoiding the collapsing mode and vanishing gradient problem of the latter two. We note that our approach is equivalent to minimizing a MMD loss over the data domain, however, with a NNbased embedded Gaussian kernel. As shown in Arjovsky et al. (2017), MMD is a proper metric when the kernel is universal.", "startOffset": 17, "endOffset": 521}, {"referenceID": 46, "context": "Data and Experimental Setup Our model is trained using a combination of two datasets: (i) the BookCorpus dataset (Zhu et al., 2015), which consists of 70 million sentences from over 7000 books; and (ii) the ArXiv dataset, which consists of 5 million sentences from abstracts of papers from various subjects, obtained from the arXiv website.", "startOffset": 113, "endOffset": 131}, {"referenceID": 35, "context": "The validation performance is evaluated by loss of generator and corpus-level BLEU score (Papineni et al., 2002), described below.", "startOffset": 89, "endOffset": 112}, {"referenceID": 25, "context": "We use a mixture of 5 isotropic Gaussian (RBF) kernels with different bandwidths \u03c3 as in Li et al. (2015). Bandwidth parameters are selected to be close to the median distance (in our case around 20) of feature vectors encoded from real sentences.", "startOffset": 89, "endOffset": 106}, {"referenceID": 39, "context": "Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).", "startOffset": 68, "endOffset": 92}, {"referenceID": 38, "context": ", ||Ef\u2212Ef\u0303 ||, as in Salimans et al. (2016). Further details of the experimental design are provided in the the Supplementary Material.", "startOffset": 21, "endOffset": 44}, {"referenceID": 35, "context": "Quantitative comparison We evaluate the generatedsentence quality using the BLEU score (Papineni et al., 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al.", "startOffset": 87, "endOffset": 110}, {"referenceID": 43, "context": "We also compare with seqGAN (Yu et al., 2017).", "startOffset": 28, "endOffset": 45}, {"referenceID": 8, "context": ", 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al.", "startOffset": 51, "endOffset": 76}, {"referenceID": 8, "context": ", 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al. (2016). For comparison, we consider textGAN with 4 different loss objectives: Mean Matching (MM) as in Salimans et al.", "startOffset": 51, "endOffset": 99}, {"referenceID": 8, "context": ", 2002) and Kernel Density Estimation (KDE), as in Goodfellow et al. (2014); Nowozin et al. (2016). For comparison, we consider textGAN with 4 different loss objectives: Mean Matching (MM) as in Salimans et al. (2016), Covariance Matching (CM) as in (5), MMD and MMD with compressed network (MMD-L), by mapping the original 900-dimensional features to 200-dimensional, as described in Section 2.", "startOffset": 51, "endOffset": 218}, {"referenceID": 2, "context": "We finally consider a Variational Autoencoder (VAE) implemented as in Bowman et al. (2016). To train the VAE model, we use annealing to gradually increase the KL divergence between the prior and approximated posterior.", "startOffset": 70, "endOffset": 91}, {"referenceID": 40, "context": "Despite the fact that the KDE approach, as a log-likelihood estimator tends to have high variance (Theis et al., 2016), the KDE score tracks well with our BLEU score evaluation.", "startOffset": 98, "endOffset": 118}, {"referenceID": 42, "context": "For BLEU score evaluation, we follow the strategy in Yu et al. (2017) of using the entire test set as the reference.", "startOffset": 53, "endOffset": 70}, {"referenceID": 43, "context": "One promising direction is to leverage reinforcement learning strategies as in Yu et al. (2017), where the updating for LSTM can be more effectively steered.", "startOffset": 79, "endOffset": 96}, {"referenceID": 2, "context": "Latent feature space trajectories Following Bowman et al. (2016), we further empirically evaluate whether the latent variable space can \u201cdensely\u201d encode sentences.", "startOffset": 44, "endOffset": 65}, {"referenceID": 36, "context": "It would be also interesting to generate text by conditioning on observed images (Pu et al., 2016).", "startOffset": 81, "endOffset": 98}], "year": 2017, "abstractText": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (realvalued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long shortterm memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.", "creator": "LaTeX with hyperref package"}}}