{"id": "1706.03935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Exact Learning from an Honest Teacher That Answers Membership Queries", "abstract": "given a teacher that holds $ function $ f :... \\ cod $ $ from basic optional key functions $ c $. that dictionary can receive from reverse page selection algorithm ~ $ & $ in base code $ x $ ( a query ) > returns the value [ whatever function in $ bc $, $ re ( k ) \\ in r $. the learner goal objectives to adjust $ f $ with a small number of jumps, reduced time complexity, and optimal arithmetic.", "histories": [["v1", "Tue, 13 Jun 2017 07:40:27 GMT  (1000kb,D)", "http://arxiv.org/abs/1706.03935v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nader h bshouty"], "accepted": false, "id": "1706.03935"}, "pdf": {"name": "1706.03935.pdf", "metadata": {"source": "CRF", "title": "Exact Learning from an Honest Teacher That Answers Membership Queries", "authors": ["Nader H. Bshouty"], "emails": ["bshouty@cs.technion.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n03 93\n5v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\nTable of Contents\nExact Learning from an Honest Teacher That Answers Membership Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nNader H. Bshouty 1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1 The Learning Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Domain and Range . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Classes of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Learning Algorithms and Complexity . . . . . . . . . . . . . . . . . . . . . . . . 14 1.5 Polynomially, Efficiently and Optimally Learnable . . . . . . . . . . . . . 15 1.6 Strongly Polynomially, Efficiently and Optimally Learnable . . . . . 16 1.7 Testing Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2 Bounds on OPT for Boolean Functions and Algorithms . . . . . . . . . . . . . 18 2.1 OPT for Adaptive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.2 Constructing Adaptive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.3 OPT for Non-Adaptive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.4 OPT for Classes of Small VC-dimension . . . . . . . . . . . . . . . . . . . . . . 23 3 Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.1 Reductions for Adaptive Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.2 Reductions for Strong Adaptive Algorithms . . . . . . . . . . . . . . . . . . . 25 3.3 Reductions for Non-Adaptive Algorithms . . . . . . . . . . . . . . . . . . . . . 26 3.4 Reductions from the Exact Learning Model . . . . . . . . . . . . . . . . . . . 27 3.5 Reductions from the PAC Learning Model . . . . . . . . . . . . . . . . . . . . 28 4 Learning d-MClause and Group Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.1 Group Testing and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.2 Known Results for Learning d-MClause . . . . . . . . . . . . . . . . . . . . . . 29 4.3 Bounds for OPT(d-MClause) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.4 Non-Adaptive Learning d-MClause . . . . . . . . . . . . . . . . . . . . . . . . . . 31 4.5 Adaptive Learning d-MClause . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.6 Two-Round Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.7 Other Related Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5 Learning s-Term r-Monotone DNF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.1 Learning a Hypergraph and its Applications . . . . . . . . . . . . . . . . . . 34 5.2 Cover Free Families . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5.3 Non-Adaptive Learning s-Term r-MDNF . . . . . . . . . . . . . . . . . . . . . 35 5.4 Adaptive Learning s-Term r-MDNF . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.5 Learning Subclasses of s-term r-MDNF . . . . . . . . . . . . . . . . . . . . . . 38 6 Learning Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.1 Bounds on OPT(DTd) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.2 Adaptive Learning Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 6.3 Non-Adaptive Learning Decision Tree . . . . . . . . . . . . . . . . . . . . . . . . 42 7 Other Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3 7.1 Other Boolean Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 7.2 Classes of Arithmetic Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 8 Non-Honest Teacher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 8.1 Models of Non-Honest Teacher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 8.2 Some Results in Learning with Non-honest Teacher . . . . . . . . . . . . 54 9 Problems and Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4"}, {"heading": "1 Introduction", "text": "Robert Dorfman\u2019s paper in 1943 introduced the field of Group Testing. The motivation arose during the Second World War when the United States Public Health Service and the Selective service embarked upon a large scale project. The objective was to weed out all syphilitic men called up for induction. However, syphilis testing back then was expensive and testing every soldier individually would have been very cost heavy and inefficient. A basic breakdown of a test is: Draw sample from a given individual, perform required tests and determine the presence or absence of syphilis. Suppose we have n soldiers. Then this method of testing leads to n tests. Our goal is to achieve effective testing in a scenario where it does not make sense to test 100, 000 people to get (say) 10 positives. The feasibility of a more effective testing scheme hinges on the following property. We can combine blood samples and test a combined sample together to check if at least one soldier has syphilis [277].\nLet S be the set of the n soldiers and let I \u2286 S be the set of the sick soldiers. Suppose we know that the number of sick soldiers, |I|, is bounded by some integer d. If T is the set of soldiers for which their blood samples is combined, then the test is positive if and only if I \u2229 T is not empty. Thus, we can regard the set of sick soldiers I as a Boolean function fI : 2\nS \u2192 {0, 1} and the answer of the test \u201cIs I \u2229 T is not empty\u201d as fI(T ) = 1 if and only if I \u2229 T 6= \u00d8. The goal is to identify the function fI (and therefore the sick soldiers) from a minimal number of substitutions (tests) and optimal time. We can also identify the set of soldiers with the set [n] := {1, 2, . . . , n} and regard each test as an assignment a \u2208 {0, 1}n, where ai = 1 if and only if the ith soldier blood is in the test. Then the set S = {0, 1}n is the set of all possible tests. The set of sick soldiers I \u2286 [n] corresponds to a Boolean function f \u2032I : S \u2192 {0, 1} where f \u2032I(x1, . . . , xn) = \u2228 i\u2208I xi and \u2228 is the Boolean or (disjunction). So this problem is also equivalent to the problem of identifying, a hidden Boolean conjunction of up to d variables, with a minimal number of substitutions and optimal time.\nAnother interesting problem is the problem of learning decision tree with a minimal number of queries. Let\u2019s say one has a restaurant and she wants to learn each customer tastes preference in food. For every customer, she offers a sample of a meal that was never ordered by the customer before and then receives some feedback. The customer tastes preference depends on some attributes of the food. For example, \u201csweet\u201d, \u201csour\u201d, \u201csalty\u201d, \u201cumami\u201d, \u201cbitter\u201d, \u201cgreasy\u201d, \u201chot\u201d etc. Those are the attributes. The goal is to learn (find out) the customer tastes preference from a minimal number of samples. Each sample can be regarded as a set of attributes. The customer tastes preference is the objective function. This function depends on the attributes, and the value of the function is the customer feedback. In many cases, the target function can be described as a decision tree. See the example in Figure 1.\nIn the following subsection, we give a framework to the above problems and many other similar problems."}, {"heading": "1.1 The Learning Model", "text": "Let the domain (instance space) be the set Xn \u2208 {Xj}j\u22651 and the range be the set Rn \u2208 {Rj}j\u22651. Let Cn be a class of representations of functions f : Xn \u2192 Rn (target class, concept class). Given a teacher (black box, opponent player, responder) that holds a (target) function (concept) f from the class Cn. The learner (player, questioner) can ask the teacher membership queries (for Boolean functions. i.e. Rn = {0, 1}) or substitution queries (for non-Boolean functions), i.e., it can send the teacher an element d of the domain Xn and the teacher returns f(d). The learner knows {Cj , Xj , Rj}j\u22651. Our (the learner) ultimate goal is to write an (exact) learning algorithm that learns C = \u222aj\u22651Cj with a minimum number of queries and optimal resources. That is,\n1. Input: The learning algorithm receives the input n and has access to an oracle MQf that answers membership/substitution queries for the target function f \u2208 Cn. 2. Query complexity: It asks the teacher a minimum number of membership/substitution queries. 3. Exact learning: It either learns (finds, outputs) g \u2208 Cn such that g is logically equivalent to f , g = f , (proper learning) or learns h \u2208 Hn \u2287 Cn such that h = f (non-proper learning from Hn). 4. Resources Complexity: It runs in linear/polynomial/optimal time complexity, optimal space complexity, an optimal number of random bits or/and other optimal resources.\n6 The following decision problems are also considered in the literature\n1. Equivalent test: Given two teachers that have two functions from Cn each. Test whether the two functions are equivalent. 2. Identity test from Hn: Given a teacher that has a function f from Cn. Given a function h \u2208 Hn. Test whether f = h. 3. Zero test: Given a teacher that has a function f from Cn. Test whether f = 0.\nThe number of queries (query complexity) and the resources complexities are expressed as functions in n and some other parameters that depend on the class being learned. In the literature, there are many other variations of the above problems, and we will mention some of them in this survey.\nThis problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273]. The decision problems are also called Testing, Functional Verification, Teaching, Hitting Set, and when f is polynomial, it is called Black Box polynomial identity testing (PIT) [239,255].\nThere are many other learning models, but, throughout this survey, when we say exact learning or learning we mean exact learning from membership queries or substitution queries only.\nIn this survey, we present some of the results known from the literature, different techniques used and some open problems."}, {"heading": "1.2 Domain and Range", "text": "Throughout this survey, we will omit the subscript n from Cn, Xn and Rn. In principle, the domain X and the range R can be any two sets, but since mathematical models can explain many natural phenomena, most of the sets considered in the literature are either finite or have some algebraic structure such as rings, fields, integers and real numbers.\nTherefore, the domains and ranges considered in the literature are: The Boolean set that can be either {0, 1}, {\u22121,+1}, {+,\u2212} or the binary field F2. The finite discrete set can be any finite set or a finite set with some algebraic structure such as the ring Zn of integers modulo n, or the finite field Fq with q elements (q is a power of prime). The infinite discrete set can be any countably infinite set such as the set of integers Z or the set of rational numbers Q. The infinite set (uncountable) can be any set with some algebraic structure such as the real numbers < or the complex numbers C. Also, the cartesian product of any finite number of the above sets is considered in the literature."}, {"heading": "1.3 Classes of Functions", "text": "In this section, we will list the most studied classes in the literature, in different fields of computer science.\n7 Boolean Function Classes: When the range of the function is Rn = {0, 1} we call the function Boolean function. Here we will consider classes C of Boolean functions when the domain is Xn = {0, 1}n. For any class defined below when we say that f is C, we mean that f \u2208 C. Abusing the terminology, every function f \u2208 C is regarded as a representation of the function (formula) and as a function, and we will use both interchangeably.\nThe most studied classes in the literature are:\n1. Variable (Var): The class Var is the class of functions {x1, . . . , xn}, where for a \u2208 {0, 1}n, xi(a) = ai. We also define Lit= {x1, . . . , xn} \u222a {x\u03041, . . . , x\u0304n} the class of literals. Here x\u0304 is the logic negation of x. Learning the class Var is equivalent to playing the Re\u0301nyi-Ulam game, [224,227,265].\n2. d-Monotone Clause (d-MClause) and MClause: The class d-MClause is the class of all functions fS : {0, 1}n \u2192 {0, 1} where S \u2286 [n] := {1, 2, . . . , n} and |S| \u2264 d such that fS(x1, . . . , xn) = 1 if and only if xi = 1 for some i \u2208 S. When S = \u2205 then f\u2205 = 0. Such function can also be expressed as a logic monotone clause fS = xi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xik where S = {i1, . . . , ik}, k \u2264 d and \u2228 is the logic \u201cor\u201d function (disjunction). We denote n-MClause by MClause. Learning d-MClause is equivalent to group testing, [111,113,114]. See many other equivalent problems in [226] and reference within.\n3. d-Clause and Clause: The class d-Clause is the class of all functions fS,R : {0, 1}n \u2192 {0, 1} where S \u2229 R = \u2205, S \u222a R \u2286 [n] and |S \u222a R| \u2264 d such that fS,R(x1, . . . , xn) = 1 if and only if xi = 1 for some i \u2208 S or xj = 0 for some j \u2208 R. Such function can be expressed as a logic clause fS,R = xi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xik \u2228 x\u0304j1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 x\u0304jr where S = {i1, . . . , ik}, R = {j1, . . . , jr}, and r + k \u2264 d. We denote n-Clause by Clause.\n4. d-Monotone Term (d-MTerm), d-Term (d-Term), MTerm and Term: The same as the above classes, but replace \u2228 with the logic \u201cand\u201d function \u2227 (Conjunction). The functions in MTerm are sometimes called monomials, and the class MTerm is also denoted by Monomial. That is, a monomial is a conjunction of variables, i.e., xj1 \u2227 xj2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 xjr where 1 \u2264 j1 < j2 < \u00b7 \u00b7 \u00b7 < jr \u2264 n. Here we will sometimes use the arithmetic \u00d7 of the field F2 for \u2227 and write xj1 \u2227 xj2 \u2227 \u00b7 \u00b7 \u00b7 \u2227 xjr as xj1xj2 \u00b7 \u00b7 \u00b7xjr . For a class C, the dual class of C is the class\nCD = { f(x\u03041, . . . , x\u0304n) \u2223\u2223\u2223 f \u2208 C} . Obviously, (CD)D = C, d-ClauseD=d-Term and d-MClauseD=d-MTerm.\n5. d-XOR and XOR: The same as the d-Term class, but replace \u2227 with the logic exclusive or function \u2295. Here, we will instead use the arithmetic + of the finite field F2 = {0, 1}. Since x\u0304 = x+ 1, every function in XOR is of the form f = xi1 + \u00b7 \u00b7 \u00b7+xik + \u03be where 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < ik \u2264 n and \u03be \u2208 {0, 1}. 6. d-Junta: Let f : {0, 1}n \u2192 {0, 1}. A variable xi is said to be relevant in f if there are two assignments a, b \u2208 {0, 1}n such that ai 6= bi, for all j 6= i we\n8 have aj = bj , and f(a) 6= f(b). The class d-Junta is the class of all Boolean functions with at most d relevant variable. This function can be represented by a truth table of size 2d of all the relevant variables.\n7. d-MJunta: For two assignments a, b \u2208 {0, 1}n we write a \u2264 b if for every i, ai \u2264 bi. A Boolean function f : {0, 1}n \u2192 {0, 1} is monotone if for every two assignments a, b \u2208 {0, 1}n, if a \u2264 b then f(a) \u2264 f(b). It is easy to see that Monotone functions are closed under disjunction and conjunction. That is, if f and g are monotone functions then f \u2227g and f \u2228g are monotone functions. The class d-MJunta is the class of all monotone functions in d-Junta. That is, the class of all monotone functions with at most d relevant variables.\n8. Decision Tree (DT): One of the important representations of Boolean functions f : {0, 1}n \u2192 {0, 1} is decision tree. A decision tree formula is defined as follows: The constant functions 0 and 1 are decision trees. If f0 and f1 are decision trees then, for all i,\n\u201cf \u2032 =[if xi = 0 then f0 else f1]\u201d\nis a decision tree (can also be expressed as f \u2032 = xif1 \u2228 x\u0304if0 or f \u2032 = xif1 + x\u0304if0). Every decision tree f\n\u2032 can be represented as a tree T (f \u2032). If f \u2032 = 1 or 0 then T (f \u2032) is a node labeled with 1 or 0, respectively. If f \u2032 =[if xi = 0 then f0 else f1], then T (f\n\u2032) has a root labeled with xi and has two outgoing edges. The first edge is labeled with 0 and is pointing to the root of T (f0) and the second is labeled with 1 and is pointing to the root of T (f1). See Figure 2. The depth of the decision tree f \u2032 is the depth of the tree T (f \u2032). That is the number of edges of the longest path from the root to a leaf in a tree. The size of the decision tree f \u2032 is the number of leaves in T (f \u2032), that is, the number nodes in T (f \u2032) that are labeled with 0 and 1. Every Boolean function f : {0, 1}n \u2192 {0, 1} can be represented as a DT. The representation is not unique. The following are subclasses of DT.\n(a) Depth d Size s Decision Tree (DTd,s): The class DTd,s is the class of all decision trees of depth at most d and size at most s. (b) Depth d Decision Tree (DTd): The class DTd is the class of all decision trees of depth at most d. That is, DTd = DTd,2d . (c) Monotone DT (MDTd,s, MDTd): functions in the above classes that are monotone. (d) Decision List (DL),[228]: functions f \u2208DT where every internal node in T (f) is pointing to at least one leaf. (e) Depth d-Decision List (d-DL): d-DL is a decision list of depth at most d.\nLearning decision tree is equivalent to solving problems in databases, decision table programming, concrete complexity theory, switching theory, pattern recognition, and taxonomy, [206], computer vision, [23].\n9. Disjunctive Normal Form (DNF): A DNF is another important representation of Boolean function f : {0, 1}n \u2192 {0, 1}. A DNF formula is a\nformula of the form\nf = s\u2228 i=1 Ti\nwhere each Ti \u2208Term is a term. The size of f is s. Every Boolean function f : {0, 1}n \u2192 {0, 1} can be represented as a DNF. The representation is not unique. It is easy to see that every decision tree of size s can be represented as DNF of size at most s. The subclasses of DNF considered in the literature are\n(a) r-DNF: The class of DNFs with terms from r-Term. (b) s-term DNF: The class of DNFs with at most s terms. (c) s-term r-DNF: The class of DNFs with at most s terms each of which\nis an r-Term. (d) Read-Once C: Here C is one of the above classes. Read-Once C is the\nclass of functions f in C where each variable appears at most once in f . (e) Read-Twice, Read-Thrice, Read-t C: The class of functions f in\nC where each variable appears at most twice (resp. three times and t times) in f .\n10. Monotone DNF (MDNF): The class MDNF is the class of DNF with monotone terms (i.e., terms in MTerm). Every monotone function (See the definition in item 7) has a monotone DNF representation. This representation is one of the most popular canonical structures for representing Boolean functions. If f = M1 \u2228M2 \u2228 \u00b7 \u00b7 \u00b7 \u2228Ms where each Mi is a monomial and no two monomials Mi and Mj , i 6= j satisfies Mi \u2227Mj = Mi, then we say that\n10\nf is a reduced monotone DNF. Every monotone Boolean function f has a unique representation as a reduced monotone DNF [1]. This representation is uniquely determined by the minterms of the function. That is, the assignments a \u2208 {0, 1}n where f(a) = 1 and flipping any entry that is 1 in a to 0 changes the value of the function to zero. Each minterm a of f corresponds, one-to-one, to a monomial M = \u2228ai=1xi in the reduced monotone DNF representation of f . The following are subclasses of MDNF. (a) r-MDNF: The class of MDNFs with monomials of size at most r. That\nis, terms from r-MTerm. (b) MDNF: The class MDNF is n-MDNF. (c) s-term MDNF: The class of MDNFs with at most s monomials. (d) s-term r-MDNF: The class MDNFs with at most s monomials of size\nat most r. (e) Read-Once, Read-Twice, Read-Thrice, Read-t C, where C is one\nof the above classes, is the class of functions f in C where each variable appears at most once (resp. twice, three times and t times) in f . Learning Monotone DNF and subclasses of Monotone DNF equivalent to problems in computational biology that arises in whole-genome shotgun sequencing, [11], and DNA phisical mapping, [144].\n11. Conjunctive Normal Form (CNF): The class CNF is the dual class (See the definition in item 4) of DNF (where \u2227 is replaced with \u2228 and vice versa). In a similar way as above we define the classes r-MCNF, MCNF, s-clause CNF, s-clause MCNF, s-clause r-CNF and s-clause r-MCNF.\n12. CDNF. The class of CDNF is the class of formulas of the form (f, g) where f is a DNF, g is a CNF and f = g. The size of (f, g) as s + t where f is s-term DNF and g is t-clause CNF. The following are subclasses of CDNF (a) CDNFs,t. The class of CDNFs,t is the class CDNF, (f, g), where f is a\nDNF of size at most s and g is a CNF of size at most t. (b) r-CDNFs,t: The class of (f, g) \u2208CDNFs,t where f is r-DNF of size at\nmost s and g is r-CNF of size at most t. (c) r-CDNF: The class \u222as,tCDNFs,t. (d) MCDNF: The class of Monotone CDNF. (e) MCDNFs,t: The class of Monotone CDNFs,t. (f) r-MCDNF: The class of Monotone r-CDNF. Learning CDNF is equivalent to problems in data-mining, graph theory and reasoning and knowledge representation, [118].\n13. Boolean Multivariate Polynomial (BMP). The class BMP is the class of multivariate polynomials over the binary field F2. That is, a function f : Fn2 \u2192 F2 of the form\nf = M1 +M2 + \u00b7 \u00b7 \u00b7+Ms where each Mi is a monomial. The size of f is s. Every Boolean function f : Fn2 \u2192 F2 can be represented as a BMP. The representation is unique. It is easy to see that every decision tree of size s and depth t can be represented as BMP of size at most 2ts.\n11\n(a) r-BMP: The class of BMPs with monomials of size at most r, i.e., in r-MTerm. This class is also called the class of multivariate polynomial of degree r over F2. (b) s-monomial BMP: The class BMPs with at most s monomials. This class is also called the class of sparse multivariate polynomial over F2. (c) s-monomial r-BMP: The class of BMPs with at most s monomials of size at most r. This class is also called the class of sparse multivariate polynomial of degree r over F2.\n14. XOR of Terms (XT): The class XT is the class of XOR of terms, T1 + T2 + \u00b7 \u00b7 \u00b7+ Ts where Ti \u2208 Term. (a) r-XT: The class of XTs with terms of size at most r. (b) s-term XT: The class of XTs with at most s terms. (c) s-term r-XT: The class of XTs with at most s terms of size at most r.\nNotice that XT with terms from MTerm is BMP. Since every term of size r can be represented as 2r-monomial r-BMP, every s-term r-XT is (2rs)monomial r-BMP.\n15. Deterministic Finite Automaton (DFA),[210]: A DFA is a 5-tuple A = (Q,\u03a3, \u03b4, q0, F ) that can be also represented as a directed graph G = (V,E) with labeled edges where V = Q is a finite set of states (the vertices), and q0 \u2208 Q is the start state. \u03a3 is a finite set of symbols called the alphabet. \u03b4 is the transition function \u03b4 : Q\u00d7\u03a3 \u2192 Q. The edge (v, u) \u2208 E in G is labeled with \u03c3 \u2208 \u03a3 if and only if \u03b4(v, \u03c3) = u. This transition function defines, for every string s \u2208 \u222ai\u22650\u03a3i, a unique path in the graph q0, q1, . . . , q|s| (here, |s| is the number of symbols in s) that starts from q0 and for every 0 \u2264 i \u2264 |s|\u22121, \u03b4(qi\u22121, si) = qi. We denote the final state in this path q|s| as \u03b4(q0, s). The set F \u2282 Q is the set of accept states. Every DFA A defines a Boolean function f : \u222ai\u22650\u03a3i \u2192 {0, 1} where f(s) = 1 if and only if \u03b4(q0, s) \u2208 F . When \u03a3 = {0, 1} then a DFA for the Boolean function f : {0, 1}n \u2192 {0, 1} is a DFA such that: for every a \u2208 {0, 1}n we have f(a) = 1 if and only if \u03b4(q0, a) \u2208 F .\n16. Boolean Multiplicity Automata Function (BMAF),[244]: A Boolean Multiplicity Automata Function is a function of the form:\nf(x1, . . . , xn) = A1(x1)A2(x2) \u00b7 \u00b7 \u00b7An(xn)\nwhere each Ai(xi) is si\u00d7 si+1 matrix that its entries are Boolean univariate polynomials in xi over F2, i.e., axi + b for a, b \u2208 F2, and s1 = sn+1 = 1. The size of a BMAF is defined as maxi si. See [44] for other ways to represent this class.\n17. Boolean Halfspace (Perceptron, Threshold) (BHS): A Boolean Halfspace is a function f : {0, 1}n \u2192 {0, 1} of the following form:\nf(x1, . . . , xn) = { 1 if w1x1 + w2x2 + \u00b7 \u00b7 \u00b7+ wnxn \u2265 u 0 otherwise\n12\nwhere w1, . . . , wn, u are real numbers. The constants w1, . . . , wn are called the weights of the Halfspace, and u is called the threshold. For W \u2286 < we define (a) BHS(W ): The class of Boolean Halfspaces with weights wi \u2208W . (b) d-BHS(W ): The class of functions in BHS(W ) with at most d relevant\nvariables.\n18. Boolean Circuit (BC) and Boolean Formula (BF) A Boolean circuit over the set of variables x1, . . . , xn is a directed acyclic graph where every node in it with indegree zero is called an input gate and is labeled by either a variable xi or a Boolean constant {0, 1}. Every other gate is either a node with indegree one and is labeled \u00ac (unary NOT) or a node with indegree two and is labeled by either, \u2227 (binary AND) or \u2228 (binary OR). A Boolean formula is a circuit in which every gate has outdegree one. The size of a Boolean circuit is the number of gates in it, and its depth is the length of the longest directed path in it.\n(a) Monotone Boolean Circuit (MBC) and Monotone Boolean Formula (MBF) MBC and MBF are Boolean circuit and Boolean formula, respectively, with no \u00ac gate. (b) Read Once Formula (ROF). The class of Boolean read-once formula. A Boolean read-once formula is a formula such that every input variable xi appears in at most one input gate. (c) Monotone Read Once Formula (MROF). The class of monotone read-once formula. (d) Read-Once, Read-Twice, Read-Thrice, Read-t C, where C is one of the above classes, is the class of functions f in C where each variable appears at most once (resp. twice, three times and t times) in f .\nSee other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251]. Here are relations between some of the classes mentioned above.\n13\nFor two classes C1 and C2 we write C1 \u2286 C2 (written as C1 \u2192 C2 in the above diagram) if every function in C1 of size s is equivalent to a function in C2 of size O(s).\nAs for functions that are not Boolean, the literature is poor in studying the exact learnability of classes of functions with finite discrete domain or/and range from membership queries only. On the other hand, there is a substantial body of literature on learning and testing arithmetic classes.\nWe now give some of the arithmetic classes defined in the literature\nArithmetic Classes: Arithmetic classes represent function f : X \u2192 R where R is an algebraic structure such as field or ring. For exact learning, the most investigated arithmetic classes in the literature are\n1. (r, V )-Linear Functions ((r, V )-LF), where r is an integer, V \u2282 < and < is the set of real numbers. An (r, V )-LF is a function f : {0, 1}n \u2192 < of the form v1xi1 + \u00b7 \u00b7 \u00b7 + vr\u2032xir\u2032 where r\n\u2032 \u2264 r and vi \u2208 V for all i = 1, . . . , r\u2032. The class r-LF is the class (r, {0, 1})-LF and LF is the class n-LF. Learning (r, V )-LF is equivalent to coin weighing problem [37] and signature coding problem [50].\n2. (r, V )-Quadratic Functions ((r, V )-QF), where r is an integer and V \u2282 <. A (r, V )-QF is a function f : {0, 1}n \u2192 < of the form xTAx where x \u2208 {0, 1}n and A is a symmetric n\u00d7 n matrix with at most r non-zero entries from V . The class r-QF is the class (r, {0, 1})-QF. Learning (r, V )-QF is equivalent to problems in molecular biology [55].\n3. Multivariate Polynomial (MP): Let F be a field. A multivariate polynomial over F is a function f : Fn \u2192 F of the form\nf = \u2211 i\u2208I aix i1 1 \u00b7 \u00b7 \u00b7xinn\nwhere I \u2286 Nn, N = {0, 1, 2, \u00b7 \u00b7 \u00b7 } and ai \u2208 F . The size of f is |f | := |I|. The term xi11 \u00b7 \u00b7 \u00b7xinn is called monomial. The monomial is called t-monomial if |{j | ij 6= 0}| \u2264 t. The multivariate polynomial is said to be of degree d if i1 + \u00b7 \u00b7 \u00b7+ in \u2264 d for all i \u2208 I, s-sparse if |I| \u2264 s and with t-monomials if all its monomials are t-monomials. When the field F is finite then every function f : Fn \u2192 F can be represented as a multivariate polynomial. This fact is not true for infinite fields.\n4. Multiplicity Automata Function: A Multiplicity Automata Function (MAF) over the field F is a function of the form\nf(x1, . . . , xn) = A1(x1)A2(x2) \u00b7 \u00b7 \u00b7An(xn)\nwhere each Ai(xi) is si \u00d7 si+1 matrix that its entries are linear functions in (x1, . . . , xn) (i.e., \u2211 i aixi + b where ai, b \u2208 <) and s1 = sn+1 = 1. The size of a MAF f is maxi si.\n14\nThis class contains the class MP in a sense that every MP of size s has a MAF of size s. See [44] for other representations of MAF.\n5. Arithmetic Circuit (AC) and Arithmetic Formula (AF) An arithmetic circuit over the field F and the set of variables x1, . . . , xn is a directed acyclic graph where every node in it with indegree zero is called an input gate and is labeled by either a variable xi or a field element. Every other gate is labeled by either + or \u00d7, in the first case, it is a sum gate and in the second a product gate. An arithmetic formula is a circuit in which every gate has outdegree one. The size of a circuit is the number of gates in it, and its depth is the length of the longest directed path in it. The degree of a circuit is equal to the degree of the polynomial output by the circuit.\n6. Arithmetic Read-Once Formula (AROF). An arithmetic read-once formula is a formula such that every input variable xi appears in at most one input gate.\nHere are relations between some of the classes we\u2019ve defined"}, {"heading": "AF \u2192 AC\u2197 \u2197MP\u2192 MAF", "text": "See other classes in [43,239,240,255] and references therein."}, {"heading": "1.4 Learning Algorithms and Complexity", "text": "The learning algorithm can be sequential or parallel, deterministic or randomized and adaptive (AD), r-round (r-RAD) or non-adaptive (NAD).\nIn the adaptive algorithm, the queries can depend on the answers to the previous ones. In the non-adaptive algorithm they are independent of the previous one and; therefore, one can ask all the queries in one parallel step. We say that an adaptive algorithm is r-round adaptive (r-RAD) if it runs in r stages where each stage is non-adaptive. That is, the queries may depend on the answers to the queries in the previous stages but independent on the answers to the queries of the current stage.\nThe randomized algorithm can be either Monte Carlo (MC) or Las Vegas (LV). A Monte Carlo algorithm is a randomized algorithm whose running time is deterministic, but whose output may be incorrect with probability at most \u03b4. A Las Vegas algorithm is a randomized algorithm that always gives a correct hypothesis. That is, it always produces a hypothesis that is equivalent to the target function. The complexity of a Las Vegas algorithm is measured by the expected running time, the expected number of queries and the expected number of rounds.\nThe goal is to ask the minimum number of queries and minimize the running time and space complexity of the algorithm and/or other resources such as the number of processors (for parallel algorithms) or the number of random bits (for randomized algorithms).\n15"}, {"heading": "1.5 Polynomially, Efficiently and Optimally Learnable", "text": "In this subsection and the next, we try to unify the different definitions used in the literature of the efficiency of the query complexity and time complexity of exact learning algorithms. We will use the following new terminologies defined below: \u201clearnable\u201d, \u201cpolynomially learnable\u201d \u201cefficiently learnable\u201d, \u201calmost optimally learnable\u201d and \u201coptimally learnable\u201d.\nLet C be a class of functions. Let OPTA(C) be the minimum number of membership queries that a learner, with unlimited computational power, needs to learn C with algorithms of type A. The algorithm type, A, can be adaptive (AD), non-adaptive (NAD) or r-round (r-RAD). For example, we will use OPTAD for the adaptive algorithm and OPTNAD for the non-adaptive algorithm. When the algorithm is randomized we also add, as a subscript, MC for Monte Carlo algorithms and LV for Las Vegas algorithms.\nIn complexity theory, a polynomial time algorithm is an algorithm that runs in polynomial time in the input size. In the exact learning model, the time complexity of learning the class C is, at least, the query complexity, OPTA(C), which can be exponential in the target function size. Therefore, polynomial time learning algorithm for C will be defined as a learning algorithm that asks poly(OPTA(C), n) queries and runs in time poly(OPTA(C), n), where n is the size of the elements in the domain X. Such classes are called polynomially learnable or just learnable classes. This is the definition used in the literature for learnability of classes.\nSince the time complexity of any learning algorithm for C is at least n \u00b7 OPTA(C) we may say that learning algorithms that run in time poly(OPTA(C), n) are \u201cefficient algorithms\u201d in time. However, this is not true for the query complexity. We will argue here, by the following example, that the above definition of poly(OPTA(C), n) for the query complexity is not the best definition for queryefficiency of exact learning from membership queries.\nTake for example the class C = d-MClause. We will show in Subsection 4.5 that OPTAD(C) = \u0398(d log n). Therefore, one would expect that a query-efficient learning algorithm for this class asks poly(d, log n) queries and not poly(d log n, n) = poly(n) queries as defined above. The time complexity cannot be less than n \u00b7 OPTA(C), so the definition of poly(OPTA(C), n) in the time complexity is passable.\nTherefore, we will suggest the following definition for efficient learning. If the algorithm for learning C asks poly(OPTA(C)) queries (rather than poly(OPTA(C) , n)) and runs in time poly(OPTA(C), n), then we call the class efficiently learnable1.\nAnother concern with this new definition is that in many areas, (such as combinatorial group testing and game theory) membership query is considered to be very costly. Therefore, one must find polynomial time learning algorithms that ask a minimum number of queries. Therefore, we will introduce here two\n1 We will not use the term \u201cpolynomially learnable\u201d for this case to avoid confusion with the definition in the literature.\n16\nother definitions: If there is a learning algorithm for C that asks OPTA(C ) 1+o(1) queries and runs in time poly(OPTA(C), n), then we call the class almost optimally learnable. If there is a learning algorithm for C that asks O(OPTA(C)) queries and runs in time poly(OPTA(C) , n), then we call the class optimally learnable.\nIn many cases, the query complexity OPTA(C) is a function of several parameters that are related to the class C. For example, the query complexity \u0398(d log n) of d-MClause also depends on d. We say that the query complexity of a learning algorithm is optimal (resp. almost optimal, efficient or polynomial) in some parameter if assuming the other parameters are constant, the query complexity of the algorithm is optimal (resp. almost optimal, efficient or polynomial). So a learning algorithm for d-MClause that asks d \u00b7 poly(log n) queries is efficient, optimal in d and efficient in n.\nWe say that the class C is query-polynomially (resp. query-efficiently, almost query-optimally or query-optimally) learnable in time T if the number of queries is as above (for polynomially, efficiently, almost optimally and optimally, respectively) but the time complexity is T .\nWe summarize all the above definitions in the following table:\nTerminology Query Complexity Time Complexity Polynomially Learnable or Learnable poly(OPTA(C), n) poly(OPTA(C), n) Efficiently Learnable poly(OPTA(C)) poly(OPTA(C), n) Almost Optimally Learnable OPTA(C) 1+o(1) poly(OPTA(C), n) Optimally Learnable O(OPTA(C)) poly(OPTA(C), n) Optimally Learnable in n O(OPTA(C)) poly(OPTA(C), n)\nwhen the other parameters are constant\nQuery-Optimally Learnable O(OPTA(C)) T (n) in time T (n)"}, {"heading": "1.6 Strongly Polynomially, Efficiently and Optimally Learnable", "text": "Let C be a class of functions. Suppose there is an integer parameter r and classes Cr such that C = \u222ar\u22650Cr. We say that C is strongly r-polynomially learnable if there is a learning algorithm for C such that, for every target function f \u2208 Cr, the algorithm runs in time poly(OPT(Cr), n) and asks at most poly(OPT(Cr), n) queries. In the same way as in the above subsection we define strongly r-efficiently learnable, strongly almost r-optimally learnable, strongly r-optimally learnable and r learnable in time T .\nFor example, it is known that OPTAD(d-MClause) = \u0398(d log n). Obviously, MClause= \u222ad\u22650d-MClause. For f \u2208MClause, let d(f) be the minimum integer such that f \u2208 d(f)-MClause. That is, d(f) is the number of relevant variables of f . The class MClause is adaptively strongly d-optimally learnable if there is an adaptive algorithm that with a target function f that is MClause, the algorithm\n17\nruns in time poly(n) and asks \u0398(d(f) log n) queries. That is, the algorithm runs in time \u0398(d log n) even when d is not known to the learner.\nRecall that, for a function f : {0, 1}n \u2192 {0, 1}, we say that xi is relevant in f if there is an assignment a such that f(a) 6= f(a + ei), where {ej}j is the standard bases, and + is the bitwise exclusive or. That is, if f depends on the variable xi. We say that xi is irrelevant in f if it is not relevant in f . For a class C, let Cd be the set of all functions in C that have at most d relevant variables. Then C = \u222ad\u22650Cd. A strongly d-efficiently learnable class C will be called strongly attribute-efficiently learnable. The same definition applies for attribute-polynomially, almost attribute-optimally, attribute-optimally and attribute learnable in time T .\nThe definition of \u201cstrongly attribute-efficient\u201d in [62] is equivalent to our definition of \u201cstrongly attribute-optimally learnable in n\u201d."}, {"heading": "1.7 Testing Problems", "text": "The following problems are also considered in the literature\n1. Equivalence Testing: Given two teachers where each one has a function from C. The learner can ask each one membership queries. Test whether the two functions are equivalent. The minimum number of queries is denoted by OPTETA (C).\nFor the non-adaptive algorithms, this is equivalent to constructing a set of assignments A such that for every f, g \u2208 C, f 6= g there is a \u2208 A such that f(a) 6= g(a). Such a set is called an equivalent test set or universal identification sequence [146]. Obviously, for deterministic algorithms,\nOPTETNAD(C) = 2 \u00b7OPTNAD(C). (1)\nAlso, it is easy to show that.\nOPTETAD(C) = 2 \u00b7OPTAD(C).\nWe give a proof sketch of the latter for completeness\nProof. Let A be an adaptive algorithm that learns C. We run A. Each time it asks a membership query a, we ask both teachers that membership query. Since A learns C, for some assignment we get different answers. Therefore OPTETAD(C) \u2264 2 \u00b7OPTAD(C). Let B be an adaptive algorithm for identity testing the class C. Let T be the teacher with the target function f \u2208 C. We run B with T and a dummy teacher T \u2032 that always gives the same answer as T as long as there is a function g \u2208 C\\{f} that is consistent with f on all the answers to the queries. Finally, no other function is consistent with f and f is uniquely determined. Therefore OPTETAD(C) \u2265 2 \u00b7OPTAD(C). ut\n18\n2. Identity Testing from H and Teaching Dimension: Given a teacher that has a function f from C. Given a function h \u2208 H \u2287 C. The learner has h and can ask the teacher membership queries. Test whether f = h. A non-adaptive algorithm is equivalent to constructing, for every h \u2208 H, a set of assignments Ah such that for every f \u2208 C, f 6= h there is a \u2208 Ah such that f(a) 6= h(a). Such a set is called an identity test set for h with respect to C. Notice that if h \u2208 C then an identity testing set for h uniquely determines h. If h 6\u2208 C then an identity testing set for h gives a proof that h does not belong to C. Therefore, we will also call this set membership test set. The maximum, over all the functions h \u2208 H, of the minimum size identity test set for h is denoted by TD(H,C) or OPTITNAD(H,C). When H = C the set Ah is called a teaching set, and TD(C,C) is denoted by TD(C) or OPTITNAD(C) and is called the teaching dimension of the concept C, [142,151,249]. We have OPTAD(C) \u2265 TD(C). To show this fact, suppose A is an algorithm that adaptively learns C. Run the algorithm with the target h. The set of all the queries asked is a teaching set for h. For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249]. 3. Constant Testing: Given a teacher that has a function f from C. Test, with membership queries, whether f is not a constant function. This is equivalent to constructing a set of assignments A such that, for every non-constant function f \u2208 C, there is a, b \u2208 A such that f(a) 6= f(b). Such a set is called a constant test set for C. OPTCS(C) denotes the minimum size constant test set for C. 4. Zero Testing: Given a teacher that has a function f 6= 0 from C. Test whether f = 0 with membership queries. This is equivalent to constructing a set of assignments A such that, for every non-zero function f \u2208 C, there is a \u2208 A such that f(a) 6= 0. Such a set is called a zero test set or a hitting set for C. The minimum size hitting set for C is denoted by OPTHS(C). It is easy to show that\nTD(H,C) = max h\u2208H\nOPTHS(C \u2212 h)\nand\nOPTNAD(C) = OPT HS(C \u2212 C) (2)\nwhere C\u2212h = {f \u2212h | f \u2208 C} and C\u2212C = {f \u2212 g | f, g \u2208 C}. For Boolean functions, f and g, f \u2212 g = f \u2295 g where \u2295 is the exclusive or operation.\nSee other results in the above references and the references therein."}, {"heading": "2 Bounds on OPT for Boolean Functions and Algorithms", "text": "In this section, we give some bounds for OPTA(C) for classes of Boolean functions C and some exponential time algorithms that are query-efficient for any class C.\n19"}, {"heading": "2.1 OPT for Adaptive Algorithms", "text": "We first state the following information-theoretic lower bound for deterministic learning algorithm. Throughout the paper, we write log x for log2 x.\nLemma 1. Let C be a class of Boolean functions. Any deterministic learning algorithm for C must ask at least log |C| membership queries. That is\nOPTAD(C) \u2265 log |C|.\nIn fact, the bound is also true for Monte Carlo and Las Vegas algorithms. See for example [12]\nLemma 2. Let C be any class of Boolean functions. Any Monte Carlo (and therefore, Las Vegas) randomized learning algorithm that learns C with probability at least 3/4 must ask at least log |C| \u2212 1 membership queries. That is\nOPTAD,LV(C) \u2265 OPTAD,MC(C) \u2265 log |C| \u2212 1.\nWe now give upper and lower bounds for OPTA(C) using the following combinatorial measure that is defined in [158,208]. Let C be a class of Boolean functions f : X \u2192 {0, 1}. Let h : X \u2192 {0, 1} be any function. We say that a set Th \u2286 X is a specifying set for h with respect to C if\n|{f \u2208 C | (\u2200x \u2208 Th)h(x) = f(x)}| \u2264 1.\nThat is, there is at most one concept in C that is consistent with h on Th. Denote T (C, h) the minimum size of a specifying set for h with respect to C. The extended teaching dimension of C is\nETD(C) = max h\u22082X T (C, h).\nLet h : X \u2192 {0, 1} be any function. For every f \u2208 C, f 6= h there is an assignment xf \u2208 X such that f(xf ) 6= h(xf ). Therefore {xf | f \u2208 C\\{h}} is a specifying set for h and T (C, h) \u2264 |C|. Therefore\nETD(C) \u2264 |C|. (3)\nNotice that for every Boolean function h 6\u2208 C if Th is a specifying set for h and f \u2208 C is a function that is consistent with h on Th then adding an assignment a to Th where f(a) 6= h(a) gives an identity testing set for h with respect to C. Therefore,\nETD(C) \u2264 TD(2X , C) \u2264 ETD(C) + 1. In [158,208], Moshkov proves the following bounds. Here, we will give another\nproof that gives, asymptotically, the same upper bound for OPTAD(C).\nLemma 3. [158,208] Let C be any class of Boolean functions. Then\nOPTAD(C) \u2264 ETD(C) log ETD(C) log |C|+ ETD(C) \u2264 2 \u00b7 ETD(C) log ETD(C) log |C| (4)\nand OPTAD(C) \u2265 max(ETD(C), log |C|).\n20\nProof. The second inequality in (4) follows from (3). Consider the following algorithm. After the ith query, the algorithm defines a set Ci \u2286 C of all the functions that are consistent with the membership queries that were asked so far. Consider any 0 < < 1/2. Now the algorithm searches for an assignment a \u2208 X such that\n|Ci| \u2264 |{f \u2208 Ci | f(a) = 0}| \u2264 (1\u2212 )|Ci|.\nIf such a \u2208 X exists, then it asks the membership queries MQ(a). Define Ci+1 = {f \u2208 Ci | f(a) = MQ(a)}. Obviously, in that case,\n|Ci+1| \u2264 (1\u2212 )|Ci|.\nIf no such a \u2208 X exists, then the algorithm finds a specifying set Th for h := Majority(Ci), where \u201cMajority\u201d is the majority function. It then asks membership queries for all the assignments in Th. If the answers are consistent with h on Th, then there is a unique concept c \u2208 Ci consistent with the answers, and the algorithm outputs this concept. Otherwise, there is a \u2208 Th such that MQ(a) 6= h(a). It is easy to see that in that case\n|Ci+1| \u2264 |Ci|.\nDenote by k the number of times when the algorithm is left without such an a \u2208 X. Then the number rounds when it does find such an a \u2208 X is\nlog |C| \u2212 k log(1/ ) log(1/(1\u2212 ))\nand so the number of queries is upper bounded by\nlog |C| \u2212 k log(1/ ) log(1/(1\u2212 )) + ETD(C)k\n= log |C|)\nlog(1/(1\u2212 )) + k\n( ETD(C)\u2212 log(1/ )\nlog(1/(1\u2212 ))\n) (5)\nWhen is chosen so that the last term becomes 0, then is roughly log ETD(C)/ ETD(C). In case of this , (5) becomes\n(1 + o(1)) ETD(C)\nlog ETD(C) log |C|.ut\nIn [158,208], Moshkov gives, for any two integers t and `, an example of a class Ct,` where\nOPTAD(Ct,`) = \u2126\n( ETD(Ct,`)\nlog ETD(Ct,`) log |Ct,`|\n) .\nSo the upper bound in the above lemma is the best possible. See also the other dimensions, bounds, and techniques in [1,2,4,19,156,213].\n21"}, {"heading": "2.2 Constructing Adaptive Algorithms", "text": "Given a class of Boolean functions C as an input. Can one construct an algorithm that learns C with OPTAD(C) queries? Obviously, with unlimited computational power, this can be done so the question is: How close to OPTAD(C) can one get when polynomial time (or any other time) is allowed for the construction?\nAn exponential time algorithm follows from the following\nOPTAD(C) = min x\u2208X max(OPTAD(Cx,0),OPTAD(Cx,1)) (6)\nwhere Cx,\u03be = {f \u2208 C | f(x) = \u03be}. See also [18,135]. Can one do it in poly(|C|, |X|) time? Hyafil and Rivest, [168], show that the problem of finding OPTAD is NP-Complete. Laber and Nogueira, [202], show that this problem does not admit an o(log |C|)-approximation unless P=NP. The reduction of Laber and Nogueira, [202], of set cover to this problem with the inapproximability result of Dinur and Steurer [121] for set cover implies that it cannot be approximated to (1\u2212 o(1)) \u00b7 ln |C| in polynomial time unless P=NP.\nThe query complexity of the algorithm in Lemma 3 is within a factor of\nmin ( 1 +\nlog |C| log ETD(C) , ETD(C) log |C| + ETD(C) log ETD(C) ) from OPT. However, unfortunately, the problem of finding a minimum size specifying set for h is NP-Hard, [9,142,249].\nIn [23], Arkin et al. gives the following algorithm: Let Ci be the set of functions that are consistent with the answers to the first i membership queries. The i + 1th membership query of the algorithm is an assignment a that partitions the set Ci as evenly as possible, that is, an assignment a that maximizes min(|{f \u2208 Ci|f(a) = 0}|, |{f \u2208 Ci|f(a) = 1}|). Arkin et al. show in [23] that the query complexity of this algorithm is within a factor of c ln |C| from OPT for some c > 1. Moshkov in [209] gives the exact ratio of ln |C|. This algorithm runs in time poly(|C|, |X|). In particular, we have the following result. Here, we will give a very simple proof\nLemma 4. [23]. There is a learning algorithm that runs in time poly(|C|, |X|) and learns C with at most\n(ln |C|)OPTAD(C) \u2264 0.693 \u00b7OPTAD(C)2\nqueries. In particular, any class C is adaptively query-efficiently learnable in time poly(|C|, |X|).\nProof. First, we have\nOPTAD(C) \u2265 |C|\nmaxa min(|Ca,0|, |Ca,1|)\n22\nwhere Cx,\u03be = {f \u2208 C | f(x) = \u03be}. This follows from the fact that every membership query can eliminate at most maxa min(|Ca,0|, |Ca,1|) functions from C. Therefore, there is b \u2208 X such that\nmax a\nmin(|Ca,0|, |Ca,1|) = min(|Cb,0|, |Cb,1|) \u2265 |C|\nOPTAD(C) .\nThus, using b as the first query, eliminates at least |C|/OPTAD(C) functions from C. After the `th query the number of functions that remain (consistent with the answers to the membership queries) is at most\n|C| (\n1\u2212 1 OPTAD(C)\n)` .\nSo the number of queries of the algorithm is at most ` = (ln |C|)OPTAD(C). By Lemma 1, ` \u2264 0.693 \u00b7OPTAD(C)2. ut\nBshouty et al., [47], show that using the NP-oracle all Boolean classes are efficiently learnable in randomized expected polynomial time. See other results in [47]."}, {"heading": "2.3 OPT for Non-Adaptive Algorithms", "text": "In this subsection, we give some bounds for non-adaptive learning classes Boolean functions. The following result follows from (1) and (2)\nLemma 5. We have\n1. OPTNAD(C) \u2265 OPTAD(C).\n2. OPTNAD(C) is equal to half the minimum size equivalent test set for C. 3. The set A is an equivalent test set for C if and only if A is a hitting set for\nC \u2295 C := {h\u2295 g | h, g \u2208 C}. That is,\nOPTNAD(C) = OPT HS(C \u2295 C).\nWe now prove\nLemma 6. [142,176]. There is a learning algorithm that runs in time poly(|X|, |C|) and finds a hitting set for C of size at most\n(ln |C|)OPTHS(C). (7)"}, {"heading": "In particular,", "text": "1. There is a non-adaptive learning algorithm that runs in time poly(|X|, |C|) and learns C using at most\n(2 ln |C|)OPTNAD(C) \u2264 1.386 \u00b7OPTNAD(C)2 (8)\nqueries.\n23\n2. Any class C is non-adaptively query-efficiently learnable in time poly(|C|, |X|).\nProof. Define for every x \u2208 X the set Cx of all the functions f \u2208 C such that f(x) = 1. Now the hitting set problem is equivalent to the set cover problem, i.e., find the minimal number of elements S \u2282 X such that \u222ax\u2208SCx = C. It is known that the greedy algorithm that at each stage, chooses the set that contains the largest number of uncovered elements, achieves an approximation ratio of ln |C|, [84].\nNow (8) follows from (7), Lemma 1 and 3 in Lemma 5. ut\nThe above reduction shows that the problem of finding a small hitting set is equivalent to finding a small set cover, and therefore, the minimum hitting set problem cannot be approximated in polynomial time to within of factor of less than (1\u2212 o(1)) ln |C|, [24,121,129,234].\nA Hitting set for C \u2295 C is also a hitting set for C except probably one function. This follows from the fact that if A is a hitting set for C \u2295C then for each two distinct functions f, g \u2208 C there is a \u2208 A such that f(a) 6= g(a) and therefore a hits one of them. This implies that there is no learning algorithm that runs in time poly(|X|, |C|) and non-adaptively learns C with less than (ln |C|)OPTNAD(C) unless P=NP."}, {"heading": "2.4 OPT for Classes of Small VC-dimension", "text": "We have seen in the previous subsection that the query complexity of nonadaptive learning the class C is equal to the minimum size hitting set for C \u2295C and finding a small hitting set is equivalent to finding a small set cover. We now give another way to construct a small hitting set for classes with small VapnikChervonenkis (VCdim) dimension. We first define the VCdim of a class C.\nFor a class C and a set S \u2286 X, we say that S is shattered by C if for any T \u2286 S there is a function f \u2208 C such that for all a \u2208 T we have f(a) = 0 and for all a \u2208 S\\T we have f(a) = 1. The VCdim of a class C, VCdim(C), is the maximum integer d such that there is a set S of size d that is shattered by C.\nAnother way to construct a hitting set for C is by choosing a distribution P on the domain X and then repeatedly chooses elements x \u2208 X according to the distribution P until we get a hitting set. This approach can also be used to prove an upper bound for the hitting set size.\nLemma 7. Suppose one can define a distribution P over X such that for every f \u2208 C, PrP [f(x) 6= 0] \u2265 P . Then a set of\nTP (C) := O\n( min ( 1\nP log |C| , VCdim(C) P log 1 P )) elements chosen according to the distribution P is a hitting set for C with probability at least 1/2.\nIn particular, OPTHS(C) \u2264 TP(C) := min\nP TP (C).\n24\nProof. The result follows from the -net theorem [169]. See also Chapter 13 in [28]. ut\nIn particular, this also gives the following upper bound for OPTNAD\nLemma 8. Suppose one can define a distribution P over X such that for every f, g \u2208 C we have PrP [f(x) 6= g(x)] \u2265 . Then a set of TP (C\u2295C) elements chosen according to the distribution P is an equivalent test set for C with probability at least 1/2."}, {"heading": "In particular,", "text": "OPTNAD(C) \u2264 TP(C \u2295 C).\nBro\u0308nnimann and Goodrich, [51], show that there is an algorithm that runs in time poly(|X|, |C|) and finds a hitting set for C of size at most\nO(VCdim(C) log(OPTHS(C)) \u00b7OPTHS(C).\nSee also [128]."}, {"heading": "3 Reductions", "text": "In this section, we give some reductions that change an existing algorithm to an algorithm with a better query complexity and an algorithm in another learning model to an algorithm that learns from membership queries. We will show in the sequel how to apply those reductions."}, {"heading": "3.1 Reductions for Adaptive Algorithms", "text": "In this subsection, we show some reductions from one exact adaptive learning algorithm to another one. Those reductions change the query complexity to be optimal in some of the parameters of the class.\nFor a class C of functions f : {0, 1}n \u2192 {0, 1} we say that C is projection closed if for any f \u2208 C, 1 \u2264 i \u2264 n and \u03be \u2208 {0, 1} we have f |xi\u2190\u03be \u2208 C. That is, projecting any variable xi to any value \u03be \u2208 {0, 1} keeps the function in the class C. We say that the class C is embedding closed if for any f \u2208 C and any 1 \u2264 k, i \u2264 n we have f |xi\u2190xk \u2208 C. We note here that almost all the classes considered in the literature are projection and embedding closed.\nHere we use the parameter r for the number of relevant variables. For class C, the class Cr contains all the functions in C with at most r relevant variables. All the classes below are projection closed. It is easy to show that for projection closed class C, Cr \u2287 C1 and C1 contains at least n literals. Therefore, OPTAD(Cr) \u2265 log n.\nIn [62], Blum et al. show\nLemma 9. [62] Let C be a class that is projection closed. If C is adaptively learnable in time T (n) with Q(n) queries and there is a constant testing set for\n25\nCr of size P (n, r) that can be constructed in time t(n) then Cr is adaptively learnable in time poly(n, T (r), Q(r), t(n), P (n, r)) with\nO(r \u00b7Q(r)P (n, r) + r \u00b7 log n)\nqueries. In particular, if P (n, r) = O(log n) (when the other parameters of the class C are constants), then Cr is optimally learnable in n.\nThe results in [57] show that if the class is also embedding closed then P (n, r) = O(r6 \u00b7Q(r2) log n). Therefore, with Lemma 9, we get the query complexity\nO((r7 \u00b7Q(r) \u00b7Q(r2) + r) log n).\nWhen the class is also embedding closed, Bshouty and Hellerstein,[57], show\nLemma 10. [41,57] Let C be a projection and embedding closed class. If C is adaptively learnable in time T (n) with Q(n) queries, then Cr is adaptively learnable in time poly(T (n)) with\nO(r6 \u00b7Q(r2) log n)\nqueries. In particular, if C is learnable then Cr is optimally learnable in n.\nFor the randomized algorithm, we have\nLemma 11. [57] Let C be a projection and embedding closed class. If C is adaptively learnable with a Monte Carlo algorithm in time T (n) with Q(n) queries then Cr is adaptively learnable with a Mote Carlo algorithm in time poly(T (n)) with\nO(r3 \u00b7Q(2r2) log n)\nqueries."}, {"heading": "3.2 Reductions for Strong Adaptive Algorithms", "text": "In some cases, one can achieve strong learning. The following result is from [62]. See the definition of strong learning in Subsection 1.6.\nLemma 12. [62] Let C be a class that is projection closed. If C is adaptively learnable in time T (n) with Q(n) queries and there is a constant testing set for C of size P (n) that can be constructed in time t(n) then C is adaptively strongly-attribute learnable in time poly(n, T (r), Q(r), t(n), P (n)) with\nO(r \u00b7Q(r)P (n) + r \u00b7 log n)\nqueries where r is the number of relevant variables of the target. In particular, if P (n) = O(log n) (when the other parameters of the class C are constants), then C is strongly attribute-optimally learnable in n.\n26\nSince {0n, 1n} (the all zero and all one assignments) is a constant testing set for all the monotone functions, P (n) = 2, in particular, we have\nLemma 13. [62] Let C be a projection closed class that contains monotone functions. If C is adaptively learnable in time T (n) with Q(n) queries, then C is adaptively strongly attribute learnable in time poly(n, T (r), Q(r)) with\nO(r \u00b7Q(r) + r \u00b7 log n)\nqueries."}, {"heading": "In particular, C is strongly attribute-optimally learnable in n.", "text": "So with the above results, one can change algorithms that learn C to algorithms that optimally and strongly attribute-optimally learn C in n."}, {"heading": "3.3 Reductions for Non-Adaptive Algorithms", "text": "In this subsection, we give a reduction from one exact non-adaptive learning algorithm to another one that changes the query complexity to be optimal in some of the parameters.\nIn [13], Abasi et al. gave the following reduction. We sketch the proof for completeness\nLemma 14. Let C be an embedding closed class such that |Cr| = \u2126(n). Let H be a class of Boolean functions and suppose there is an algorithm that, for input h \u2208 H, finds the relevant variables of h in time R(n)."}, {"heading": "If C is non-adaptively learnable from H (the output hypothesis is from H)", "text": "in time T (n) with Q(n) membership queries then Cr is non-adaptively learnable from H in time poly(n, T (q), R(q)) with\nO\n( r2Q(q)\nlog(q/r2) log n ) membership queries where q \u2265 2r2 is any integer."}, {"heading": "In particular, Cr is non-adaptively optimally learnable in n.", "text": "Proof. We use a perfect hash family P that map the variables X = {x1, . . . , xn} to a new set of variables Y = {y1, . . . , yq} where q > r2. This family contains O(r2/ log(q/r2) log n) hash functions and ensures that for almost all the hash functions, different relevant variables of the target are mapped to different variables in Y . It also insures that for every non-relevant variable xi, almost all the hash functions map the relevant variables and xi to different variables in Y .\nWe learn f(\u03c6(X)) for each hash function \u03c6 \u2208 P with Q(q) membership queries (when possible) and use the majority rule to find the relevant variables and to recover the target function. ut\nSee also [49] for a reduction for the randomized non-adaptive learning.\n27"}, {"heading": "3.4 Reductions from the Exact Learning Model", "text": "In this subsection and the next one, we give two other models and show some conditions in which learning in those models can be reduced to learning from membership queries only.\nThe first learning model is the exact learning model from membership and equivalence queries. In this model, the goal is to learn the target function exactly with membership queries and equivalence queries. In the Equivalence Query (EQ) model, [1], the learning algorithm sends the teacher a hypothesis h from some class of hypothesis H \u2287 C. The teacher answers \u201cYES\u201d if h is equivalent to the target f , otherwise, it provides the learner a counterexample, i.e., an assignment a where f(a) 6= h(a). We say that a class C is exactly learnable from H from membership and equivalence queries in time t, m membership queries and e equivalence queries if there is an algorithm that runs in time t, asks at most m membership queries and at most e equivalence queries and outputs h \u2208 H that is equivalent to f .\nThere are several polynomial time exact learning algorithms available in the literature that learns from membership and equivalence queries for classes mentioned in this survey and others. Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35]. See also references therein and [253]. Some of the algorithms are proper (i.e., H = C) and others are non-proper.\nThe reduction from this model to the model of learning from membership queries only is done as follows.\nLemma 15. Let C and H \u2287 C be two classes of functions. Suppose C is learnable from H from membership and equivalence queries in time t, m membership queries and e equivalence queries. Then\n1. If for every h \u2208 H an identity testing set Sh for h with respect to C can be constructed in time t\u2032 and |Sh| \u2264 s then C is learnable in time poly(t\u2032, t,m, e, s) with m+ es membership queries. 2. If an equivalent test set S for H can be constructed in time t\u2032 and |S| = s then C is learnable in time poly(t\u2032, t,m, e, s) with m+s membership queries.\nProof. In 1, the algorithm replaces each equivalence query for h to membership queries to all the assignments in Sh. If h is consistent with f on Sh then h = f and the algorithm outputs h. Otherwise, there is a \u2208 Sh such that f(a) 6= h(a) and a can be used as an answer to the equivalence query.\nIn 2, the algorithm first constructs S and asks membership queries to all the assignments in S. Then for each equivalence query with h, it finds a \u2208 S such that h(a) 6= f(a) and returns the answer a to the algorithm. If no counterexample exists then, it outputs h. ut\nA hardness result in learning a class C from equivalent and membership queries does not imply hardness in learning C from membership queries only.\n28\nFor example, the hardness result of learning read-thrice DNF, C, in [21] cannot be used as a hardness result for this class in the exact learning model from membership queries. This is because, in our definition of efficiency, the query complexity is allowed to be polynomial in OPTA(C) which might be exponential in n. For example, it is easy to see that OPTAD(read-thrice DNF) = 2 n (even for one term) so this class is optimally learnable (by asking all the queries {0, 1}n). So hardness in learning in this model does not imply hardness in learning from membership queries. On the other hand, hardness results for proper learning, [10], do give hardness results for proper learning from membership queries."}, {"heading": "3.5 Reductions from the PAC Learning Model", "text": "In this section, we provide a reduction from PAC-learning with membership queries to learning from membership queries only.\nIn the probably approximately correct learning model (PAC learning model), [268], with membership queries the teacher has a function f : X \u2192 {0, 1} from some class C. The learner can ask the teacher membership queries and is required to learn a function that is -close (defined below) to the target with high probability. Let P be a distribution on the domain X. We say that an algorithm A PAC-learns C from H with membership queries according to the distribution P if the algorithm A, for the input < 1 and \u03b4 < 1, with probability at least 1\u2212 \u03b4, outputs a function h \u2208 H that is -close to f , i.e., PrP [f(x) 6= h(x)] \u2264 .\nThere are many PAC learning algorithms with membership queries in the literature for classes mentioned in this survey and others. Classes such as decision trees and multivariate polynomials under distributions that support small terms, [68], DNF under the uniform distribution, [130,172], constant depth circuits under the uniform distribution, [201] and intersections of halfspaces, [189]. If a class is learnable from equivalence and membership queries, then it is PAC-learnable with membership queries according to any distribution, [2].\nWe say that H is a class of distance \u03b7 with respect to the distribution P if for every h1, h2 \u2208 H, h1 6= h2 we have PrP [h1(x) 6= h2(x)] > \u03b7. The following lemma shows how to change a PAC-learning algorithm to a learning algorithm with membership queries.\nLemma 16. Let C and H \u2287 C be two classes of functions. Suppose C is PAClearnable with membership queries from H according to the distribution P in time t( , \u03b4) and m( , \u03b4) membership queries. If H is a class of distance \u03b7 with respect to the distribution P , then C is randomized Monte Carlo learnable with m(\u03b7, \u03b4) membership queries in time t(\u03b7, \u03b4).\nProof. Run the algorithm with = \u03b7. Let h be the output. Then with probability at least 1 \u2212 \u03b4 we have PrP [f 6= h] \u2264 \u03b7. Since C \u2286 H we also have f \u2208 H and since H is a class of distance \u03b7 we must have f = h. ut\n29\n4 Learning d-MClause and Group Testing\nIn this section, we give an example of a class that has been extensively studied in the literature. Consider the class d-MClause. That is the class of monotone clauses with at most d variables."}, {"heading": "4.1 Group Testing and Applications", "text": "In group testing (or pooling design), the task is to determine the positive members F = {oi1 , . . . , oik}, k \u2264 d, of a set of objects O = {o1, . . . , on} by asking queries of the form \u201cdoes the subset S \u2286 O contain a positive object?\u201d That is, \u201cdoes F \u2229 S 6= \u2205?\u201d. A negative answer to this question informs the learner that all the items belonging to S are non-positive. The aim of group testing is to identify the unknown subset F using as few queries as possible.\nGroup testing was originally introduced as a potential approach to economical mass blood testing [111]. However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104]. See a brief history and other applications in [85,113,114,217] and references therein.\nGroup testing is equivalent to learning the class d-MClause from membership queries. We have n = |O|; the target is \u2228oi\u2208Fxi and each query \u201cdoes F\u2229S 6= \u2205?\u201d is equivalent to a membership query with the assignment a \u2208 {0, 1}n where ai = 1 if and only if oi \u2208 S.\n4.2 Known Results for Learning d-MClause\nThe following table summarizes the results known for the asymptotic number of membership queries for learning the class d-MClause. We will assume that d \u2264 \u221a n.\nNon-Adaptive Two-Round Adaptive\nDeterministic Upper Bound d2 log n d log n d log n Deterministic Poly Time d2 log n d logO(1) n d log n d1+o(1) log n Deterministic Lower Bound d 2\nlog d log n d log n d log n\nLV Randomized Poly Time d2 log n d logO(1) n d log n LV Randomized Lower Bound d2 log n d log n d log n MC Randomized Poly Time d log n d log n d log n MC Randomized Lower Bound d log n d log n d log n\nThere is a (folklore) deterministic adaptive algorithm that runs in polynomial time and asks O(d log n) queries. See Subsection 4.5. This implies all the results\n30\nfor the upper bounds in the fourth column of the above table. The number of functions in d-MClause is(\nn\nd\n) + ( n\nd\u2212 1\n) + \u00b7 \u00b7 \u00b7+ ( n\n0 ) and therefore by Lemma 2, any Monte Carlo learning algorithm for d-MClause must ask at least \u2126(d log n) queries. This implies all the lower bounds in the table except for the non-adaptive lower bound for the deterministic and Las Vegas algorithm that follows from [120]. See Subsection 4.4. The upper bound of\nO ( d ( log n+ log 1\n\u03b4 )) for the Monte Carlo non-adaptive algorithm follows from a simple randomized argument. See Subsection 4.4. Porat and Rothschild, [226], gave the first polynomial time deterministic non-adaptive learning algorithm that asks O(d2 log n) queries. The deterministic upper bound for non-adaptive learning follows from a simple probabilistic argument. See Also [123,124]. The last two results are the deterministic two-round algorithms with O(d log n) queries and the polynomial time deterministic two-round learning algorithm that asks min(d logO(1) n, d1+o(1) log n) queries. This follows from [56,81,170]. See Subsection 4.6. In particular,\nTheorem 1. We have\n1. The class d-MClause is adaptively optimally learnable. 2. The class d-MClause is non-adaptively almost optimally learnable and opti-\nmally learnable in n. 3. The class d-MClause is MC randomized adaptively optimally learnable. 4. The class d-MClause is two-round efficiently learnable, two-round optimally\nlearnable in d and two-round optimally learnable in n.\n4.3 Bounds for OPT(d-MClause)\nConsider a non-adaptive algorithmA for learning d-MClause and let x(1), . . . , x(t) be the queries asked by the algorithm. Consider the t \u00d7 n matrix M such that its ith row is x(i). Let Mi be the ith column of M . If the target function is xi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xid\u2032 , d\n\u2032 \u2264 d then the vector of all the answers to the queries is Mi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228Mid\u2032 (bitwise or). LetM be the set of all Mj1 \u2228 \u00b7 \u00b7 \u00b7 \u2228Mjd , ji \u2208 [n] for i = 1, . . . , d. That is, the set of all possible answers to the queries for all possible target functions. Since each vector in M uniquely determines the target function, the matrix M must satisfy the following property: For every S1, S2 \u2286 [n], where |S1|, |S2| \u2264 d and S1 6= S2, we have\u2228\nj\u2208S1 Mj 6= \u2228 j\u2208S2 Mj .\n31\nA matrix that satisfies this property is called d-separable matrix. Therefore, OPTNAD(d-Mclause) is equal to the minimum t such that a d-separable t \u00d7 n matrix exists.\nA t \u00d7 n matrix M is called d-disjunct if for every d + 1 distinct columns i1, \u00b7 \u00b7 \u00b7 , id+1 there is a row j such that Mj,id+1 = 1 and Mj,ik = 0 for all k = 1, . . . , d. It is easy to show that, [190],\nM is d-disjunct \u21d2 M is d-seperable \u21d2 M is (d\u2212 1)-disjunct.\nTherefore, it is enough to construct a d-disjunct matrix. Using a probabilistic method, [28], it is easy to show that a d-disjunct t \u00d7 n matrix exists where t = O(d2 log n). Just take each entry in the matrixM to be 1 with probability 1/d and 0 with probability 1\u2212 1/d and show that for t = O(d2 log n) the probability that the matrix is not d-disjunct is less than 1. This implies\nOPTNAD(d-MClause) = O ( d2 log n ) .\nSee also [123,124,219]. There is also an almost tight lower bound for t [120,131,230]\nOPTNAD(d-MClause) = \u2126\n( d2\nlog d log n\n) .\n4.4 Non-Adaptive Learning d-MClause\nWe now show that the class d-MClause is non-adaptively learnable in polynomial time with O(d2 log n) queries. In [226], Porat and Rothschild gave the first polynomial time algorithm for constructing a d-disjunct matrix of size t \u00d7 n where t = O(d2 log n). Now the learning algorithm is as follows. We first use the Porat and Rothschild [226] algorithm to construct a d-disjunct t\u00d7 n matrix M of size t = O(d2 log n) in polynomial time. Set Z = {x1, x2, . . . , xn}. Then for every query M (i) (row i in M) if f(M (i)) = 0 then for every j where M\n(i) j = 1, we\nremove xj from Z. The remaining variables in Z are the variables that appear in the target. This is because if f = xi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xid\u2032 and i 6\u2208 {i1, . . . , id\u2032} then the variable xi will be removed from Z by the row M\n(j) that assign 0 to all xij , j = 1, . . . , d \u2032 and one to xi. Such row exists since M is d-disjunct. On the other hand, for every row M (k) where M (k) ij = 1 for some j = 1, . . . , d\u2032 we have f(M (k)) = 1 and, therefore, no variable in the target is removed from Z. This gives a polynomial time algorithm that asks O(d2 log n) queries. Since the lower bound for the query complexity of non-adaptively learning the class d-MClause is \u2126(d2 log n/ log d), the class d-MClause is non-adaptively almost optimally learnable and non-adaptively optimally learnable in n.\nClosing the gap between the lower bound and upper bound is one of the longstanding open problem in group testing. Bshouty proved in [38,39] that a lower bound of d2 log n/\u03c9(1) implies that for a power of prime q = O(d) one cannot simulate a black-box multiplication of d elements in the finite field Fqt with O(dt) black-box multiplications in Fq. This is one of the hard problems in algebraic complexity.\n32\nFor a randomized non-adaptive learning algorithm, just randomly choose t = O(d(log n+ log(1/\u03b4))) assignments a(1), . . . , a(t) in {0, 1}n where each a(i)j is one with probability 1/d and zero with probability 1\u22121/d. Define Z = {x1, . . . , xn}. Then for each assignment that satisfies f(a(i)) = 0 remove all xj from Z for which a (i) j = 1. It is easy to show that with probability at least 1 \u2212 \u03b4, the variables that remain in Z are the variables of the target. The problem of strongly attribute learnability of MClause, which is equivalent to the problem of group testing when d is not known to the learner, was studied by Damaschke and Muhammad, [115,116]. They show that for deterministic nonadaptive algorithms, determining the exact number of the relevant variables d is as difficult as learning the target function. For randomized non-adaptive learning algorithms, they gave the upper bound of O(log n) to approximate d and the lower bound (with some constraints) of \u2126(log n).\n4.5 Adaptive Learning d-MClause\nIn this subsection, we present the folklore algorithm for adaptively learning the class d-MClause. The algorithm runs in polynomial time and has a query complexity that matches the lower bound and therefore d-MClause is optimally learnable.\nWe first give the lower bound\nLemma 17. Any deterministic (or even randomized) algorithm for d-MClause must ask at least d log(n/d) queries.\nProof. Follows from Lemma 2 and the fact that |C| = \u2211d i=0 ( n i ) . ut\nWe now give the folklore algorithm. Let f be the target function. For a subset R \u2286 [n], define 1R the assignment that is one in the entries that are in R and 0 in the other entries. At the first stage, the algorithm defines a set S1,1 := [n]. At stage i, the algorithm has disjoint sets Si,1, Si,2, \u00b7 \u00b7 \u00b7 , Si,ji \u2286 [n] where f(1Si,k) = 1 for all k = 1, . . . , ji. The algorithm at stage i partitions each set Si,k, k = 1, . . . , ji into two (almost) equal disjoint sets Si,k,1 \u222a Si,k,2 and asks two queries f(1Si,k,1) and f(1Si,k,2). The sets that will survive to the following stage, i+ 1, are the sets T for which f(1T ) = 1. Those will be assigned to Si+1,1, . . . , Si+1,ji+1 . The algorithm stops when the sizes of those sets are 1. Then each Si+1,k will be holding an index of a variable in the target.\nObviously, throughout the algorithm we have ji \u2264 d and |Si,k| \u2264 dn/2i\u22121e for all k = 1, 2, . . . , ji. The algorithm has at most log n stages. At each stage, it asks at most 2d queries, and therefore, the total number of queries is\n2d log n = O(d log n).\nA more precise analysis gives the upper bound 2d log(n/d) +O(d). See also the algorithms in [61,90,119,125,254,266] and references therein. The above adaptive algorithm runs even if d is unknown to the learner. Therefore, the class d-MClause is adaptively strongly attribute-optimally learnable. This implies\n33\nTheorem 2. [160] The class d-MClause is strongly attribute-optimally learnable with O(d log(n/d)) queries.\nFor randomized adaptive algorithms see [117] and reference within. When d is unknown, Cheng [82] shows that there is a randomized adaptive learning algorithm that asks O(d log d) queries and finds d with probability at least 1 \u2212 1/d\u0398(1)."}, {"heading": "4.6 Two-Round Learning", "text": "In [56], De Bonis et al. shows that there is a two-round adaptive algorithm for learning d-MClause that asks O(d log n) queries. See also [88,126]. This is asymptotically as efficient as the best fully adaptive learning algorithms. Therefore\nOPT2-RAD(d-MClause) = \u0398(d log n).\nThe algorithm uses (2d, d+1, n)-selector. A (d,m, n)-selector is a t\u00d7n Boolean matrix such that any d columns contain at least m distinct rows of Hamming weight 1. It is known that there is a (2d, d+ 1, n)-selector of size t = O(d log n). This follows from the following simple probabilistic argument: randomly choose t \u00d7 n matrix where each entry is 1 with probability p = 1/(2d) and 0 with probability 1\u2212p. Then show that the probability that the matrix is not a (2d, d+ 1, n)-selector is less than one.\nGiven a (2d, d + 1, n)-selector, the algorithm is as follows. Let f = xi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xid\u2032 , d\n\u2032 \u2264 d be the target function. At the first round, the algorithm asks t = O(d log n) queries that are the rows of a (2d, d + 1, n)-selector. Let X = {x1, . . . , xn}. The algorithm then eliminates all the variables xi in X where there is a query a for which f(a) = 0 and ai = 1. At the second round, for each variable xi \u2208 X (that was not eliminated in the first round) the algorithm asks the query e(i) where e\n(i) i = 1 and e (i) j = 0 for all j 6= i. Then xi in the target if\nand only if f(e(i)) = 1.\nNow we show that the number of variables that are not eliminated in the first round is at most 2d\u22121. Suppose for the contrary that there are 2d variables X \u2032 = {xj1 , . . . , xj2d} that are not eliminated in the first round. By the same argument as in Subsection 4.4, {xi1 , . . . , xid\u2032} \u2282 X\n\u2032. By the property of (2d, d + 1, n)selectors and since d\u2032 \u2264 d there is an assignment a where ai1 = \u00b7 \u00b7 \u00b7 = aid\u2032 = 0 and ajk = 1 for some jk 6\u2208 {i1, . . . , id\u2032}. This implies that f(a) = 0 and ajk = 1 and the variable xjk was eliminated in the first round. This is a contradiction.\nIndyk shows in [170] how to construct an explicit (2d, d+1, n)-selector of size d\u00b7poly(log n). This construction gives a polynomial time learning algorithm for dMClause with d \u00b7poly(log n) queries. Therefore the class d-MClause is two-round efficiently learnable and two-round optimally learnable in d. Cheraghchi, [81], used recent results in extractors to prove that d-MClause is two-round almost optimally learnable in n. His algorithm asks d1+o(1) log n queries.\n34"}, {"heading": "4.7 Other Related Problems", "text": "The group testing with inhibitors (GTI) model was introduced in [133]. In this model, in addition to positive items and regular items, there is also a category of items called inhibitors. The inhibitors are the items that interfere with the test by hiding the presence of positive items. As a consequence, a test yields a positive feedback if and only if the tested pool contains one or more positives and no inhibitors. This problem is equivalent to learning functions of the form\n(xi1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xir ) \u2227 xj1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xjs .\nThis problem is studied in [76,94,112,133,162,167]. See other related problems in [86,217] and references therein.\n5 Learning s-Term r-Monotone DNF\nConsider the class s-term r-MDNF. That is, the class of monotone DNF with s monotone terms (monomials) where each term is of size at most r. Torney, [262], first introduced the problem and gave some applications in molecular biology. In this section, we present some results known from the literature for learning this class."}, {"heading": "5.1 Learning a Hypergraph and its Applications", "text": "A hypergraph is H = (V,E) where V is the set of vertices, and E \u2286 2V is the set of edges. The dimension or rank of the hypergraph H is the cardinality of the largest set in E. A hypergraph is called Sperner hypergraph if no edge is a subset of another. For a set S \u2286 V , the edge-detecting queries QH(S) is answered \u201cYes\u201d or \u201cNo\u201d, indicating whether S contains all the vertices of at least one edge of H. Learning the class s-term r-MDNF is equivalent to learning a Sperner hidden hypergraph of dimension at most r with at most s edges using edge-detecting queries [14].\nThis problem has many applications in chemical reactions, molecular biology, and genome sequencing. In chemical reactions, we are given a set of chemicals, some of which react and some which do not. When multiple chemicals are combined in one test tube, a reaction is detectable if and only if at least one set of the chemicals in the tube reacts. The goal is to identify which sets react using as few experiments as possible.\nSee [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications. This problem is also called, \u201csets of positive subsets\u201d [262] \u201ccomplex group testing\u201d [114,211] and \u201cgroup testing in hypergraph\u201d [139].\nIn all of the above applications, the size of the terms r is much smaller than the number of terms s and both are much smaller than the number of vertices n. Therefore, all the results in the literature, except [13], assumes that r, s \u2264 \u221a n, although they do not mention this constraint explicitly. For ease of the presentation of the results, we will also adopt this constraint throughout this section.\n35"}, {"heading": "5.2 Cover Free Families", "text": "One of the tools used in the literature for learning s-term r-MDNF is cover-free families (CFF). A (n, (s, r))-cover free family ((n, (s, r))-CFF), [190], is a set A \u2286 {0, 1}n such that for every 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < id \u2264 n where d = s+ r and every J \u2286 [d] of size |J | = s there is a \u2208 A such that aik = 0 for all k \u2208 J and aij = 1 for all j 6\u2208 J . Denote by N(n, (s, r)) the minimum size of such set. The lower bound in [123,215,260] is\nN(n, (s, r)) \u2265 \u2126 (N(s, r) \u00b7 log n) (9)\nwhere\nN(s, r) = s+ r log ( s+r r )(s+ r r ) . (10)\nIt is known, [12], that a set of\nm = O ( min(s, r)1.5 ( log ( s+ r\nmin(s, r)\n))( N(s, r) \u00b7 log n+ N(s, r)\ns+ r log\n1\n\u03b4 )) = N(s, r)1+o(1)(log n+ log(1/\u03b4)) (11)\nrandom vectors a \u2208 {0, 1}n, where each aj is 1 with probability r/(s + r), is a (n, (s, r))-CFF with probability at least 1\u2212 \u03b4.\nIt follows from [38,41,52,134] that there is a polynomial time (in the size of the CFF) deterministic construction of (n, (s, r))-CFF of size\nN(s, r) 1+o(1) log n (12)\nwhere the o(1) is with respect to min(r, s). When r = o(s), the construction can be done in linear time [41,52].\n5.3 Non-Adaptive Learning s-Term r-MDNF\nIn this section, we give a non-adaptive learning algorithm for the class of s-term r-MDNF.\nWe first give a lower bound\nTheorem 3. [13,114] Let n \u2265 r + s. Any equivalent test set A \u2286 {0, 1}n for s-term r-MDNF is (n, (s, r \u2212 1))-CFF and (n, (s \u2212 1, r))-CFF. Therefore, Any non-adaptive algorithm for learning s-term r-MDNF must ask at least\nN(n, (s\u2212 1, r)) +N(n, (s, r \u2212 1)) = \u2126 (N(s, r) log n)\nqueries. In particular, when r is constant, the number of queries is at least\nOPTNAD(s-term r-MDNF) = \u2126\n( sr+1\nlog s log n\n) .\n36\nProof. Consider any distinct 1 \u2264 i1, \u00b7 \u00b7 \u00b7 , ir+s\u22121 \u2264 n. To be able to distinguish between the two functions f1 = (xi1 \u00b7 \u00b7 \u00b7xir ) \u2228 xir+1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xir+s\u22121 and f2 = (xi1 \u00b7 \u00b7 \u00b7xir\u22121)\u2228 xir+1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xir+s\u22121 we must have an assignment a that satisfies ai1 = \u00b7 \u00b7 \u00b7 = air\u22121 = 1 and air = \u00b7 \u00b7 \u00b7 = air+s\u22121 = 0. Therefore A is (n, (s, r \u2212 1))- CFF.\nTo be able to distinguish between the two functions g1 = (xi1 \u00b7 \u00b7 \u00b7xir )\u2228xir+1\u2228 \u00b7 \u00b7 \u00b7 \u2228 xir+s\u22121 and g2 = xir+1 \u2228 \u00b7 \u00b7 \u00b7 \u2228 xir+s\u22121 we must have an assignment a that satisfies ai1 = \u00b7 \u00b7 \u00b7 = air = 1 and air+1 = \u00b7 \u00b7 \u00b7 = air+s\u22121 = 0. Therefore A is (n, (s\u2212 1, r))-CFF. ut\nWe now give a simple upper bound\nTheorem 4. Any (n, (s, r))-CFF A is an equivalent test set for s-term r-MDNF. Therefore, there is a non-adaptive learning algorithm for s-term r-MDNF that asks\nN(n, (s, r)) = O (\u221a r ( s+ r\nr\n) (s+ r) log n ) queries. In particular, when r is constant,\nOPTNAD(s-term r-MDNF) \u2264 N(n, (s, r)) = O ( sr+1 log n ) Proof. Let A be a (n, (s, r))-CFF. Let f1, f2 be any two non-equivalent s-term rMDNF. Suppose f1 = M1,1\u2228M1,2\u2228\u00b7 \u00b7 \u00b7\u2228M1,s1 and f2 = M2,1\u2228M2,2\u2228\u00b7 \u00b7 \u00b7\u2228M2,s2 where s1, s2 \u2264 s. Let b \u2208 {0, 1}n be an assignment such that (w.l.o.g.) f1(b) = 1 and f2(b) = 0. Then M2,i(b) = 0 for all i = 1, . . . , s2 and M1,j(b) = 1 for some 1 \u2264 j \u2264 s1. Let M1,j = xk1xk2 \u00b7 \u00b7 \u00b7xkr\u2032 , r\n\u2032 \u2264 r. Then for every M2,i, i = 1, . . . , s2, there is a variable xji in M2,i where bji = 0 and for all the variables xk1 , xk2 , . . . , xkr\u2032 in M1,j we have bk1 = \u00b7 \u00b7 \u00b7 = bkr\u2032 = 1.\nNow take a \u2208 A such that aji = 0, i = 1, . . . , s2 and ak1 = ak2 = \u00b7 \u00b7 \u00b7 = akr\u2032 = 1. Such a exists since A is (n, (s, r))-CFF. Then we have f1(a) = 1 and f2(a) = 0. This completes the proof. ut\nThe first explicit non-adaptive learning algorithm for s-term r-MDNF was given by Gao et al., [139]. They show that this class can be learned with (n, (s, r))-CFF. Given such a (n, (s, r))-CFF, the algorithm simply takes all the monomials M of size at most r that satisfy (\u2200a \u2208 A)(M(a) = 1 \u21d2 f(a) = 1). It is easy to see that the disjunction of all such monomials is equivalent to the target function. Assuming a set of (n, (s, r))-CFF of size N can be constructed in time T , the above algorithm learns s-term r-MDNF with N queries in time O( ( n r ) N + T ). This with (12) gives\nTheorem 5. There is a non-adaptive learning algorithm for s-term r-MDNF that asks\nQ = N(s, r) 1+o(1) log n queries and runs in poly( ( n r ) , Q) time.\nWhen r is constant, the algorithm asks\nQ\u2032 = O(sr+1 log n)\n37\nqueries and runs in poly(n) time. In particular, for constant r, the class s-term r-MDNF is non-adaptively almost optimally learnable and optimally learnable in n.\nWhen r = \u03c9(1) we can use Lemma 14 to prove\nTheorem 6. Let r = \u03c9(1). There is a non-adaptive learning algorithm for sterm r-MDNF that asks\nQ = N(s, r) 1+o(1) log n\nqueries and runs in poly(n, ( 2(rs)2\nr\n) ) time.\nIn particular, for r \u2264 sc for some c < 1, the class s-term r-MDNF is nonadaptively almost optimally learnable and optimally learnable in n.\nProof. Follows from Lemma 14 and Theorem 5 and the fact that any s-term r-MDNF has at most sr relevant variables. ut\nOne can now use (11) in a straightforward manner to get a randomized nonadaptive algorithm with better time and query complexity. Recently, Abasi et al. [13], gave an almost optimal learning algorithm for all n, r and s.\n5.4 Adaptive Learning s-Term r-MDNF\nIn this section, we give results on adaptive algorithms for learning s-term rMDNF.\nAdaptive algorithms for learning s-term r-MDNF is studied in [14,15] and [12]. The information theoretic lower bound for this class is rs log n. Angluin and Chen gave in [15] the lower bound \u2126((2s/r)r/2 + rs log n) when s > r and Abasi et al. gave in [12] the lower bound \u2126((r/s)s\u22121 + rs log n) when s \u2264 r. Angluin and Chen gave a polynomial time adaptive algorithm for learning s-term 2-MDNF that asks O(s log n) queries. Therefore, the class s-term 2-MDNF is adaptively optimally learnable. In [12] Abasi et al. gave a polynomial time learning algorithm for s-term r-MDNF that asks rs log n + (r/s)s+o(s) queries when r > s and rs log n + sr/2+o(r) queries when r \u2264 s. They also gave some randomized algorithms.\nThe following table summarizes the latest results: Det. and Rand. stand for deterministic algorithm and randomized algorithm, respectively.\nRand./ r, s Lower Bound Ref. Det. Upper Bound\nr = 2 s log n [15] Det. s log n r > s rs log n+ ( r s )s\u22121 [12] Det. rs log n+ ( r s )s+o(s) [12] Rand. rs log n+ (log r) \u221a ses ( r s + 1\n)s r \u2264 s rs log n+ ( 2s r )r/2 [12,15] Det. rs log n+ sr/2+o(r)\n[12,15] Rand. rs log n+ \u221a r(3e)r(log s)sr/2+1\n38\n5.5 Learning Subclasses of s-term r-MDNF\nLearning subclasses of graphs and hypergraphs from edge-detecting queries received considerable attention in the literature due to its diverse applications [14,15,42,87]. This is equivalent to learning subclasses of s-term 2-MDNF and s-term r-MDNF, respectively. Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15]. The class of Read-Once 2-MDNF is equivalent to learning matchings [11]."}, {"heading": "6 Learning Decision Tree", "text": "In this section we study the learnability of the class of Depth d-DT (DTd), i.e., the class of all decision trees of depth at most d.\n6.1 Bounds on OPT(DTd)\nWe say that a set of assignments A \u2286 {0, 1}n is (n, d)-universal set if for every 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < id \u2264 n and every b \u2208 {0, 1}d there is an a \u2208 A such that aij = bj for all j = 1, . . . , d.\nIt is known that any (n, d)-universal set is of size \u2126(2d log n) [191,246]. The probabilistic method with the union bound gives the upper bound O(d2d log n). The best known polynomial time, poly(n, 2d), construction gives an (n, d)-universal set of size O(2d+log 2 d log n) [218]. It is easy to show that a random uniform set A \u2286 {0, 1}n of size O(d2d log n) is (n, d)-universal set with probability at least 1/nO(d). For d \u2264 log n/ log log n, an (n, d)-universal set of size dO(1)2d log n can be constructed in polynomial time [218].\nWe now prove\nTheorem 7. A set of assignments A \u2286 {0, 1}n is a hitting set for DTd if and only if A is an (n, d)-universal set. In particular\n\u2126(2d log n) \u2264 OPTHS(DTd) \u2264 O(d2d log n).\nProof. Let A be an (n, d)-universal set. Let f 6\u2261 0 be a decision tree of depth at most d. Consider a path p from the root of f to a leaf that is labeled with 1. Let p = v1 \u03be1\u2192 v2 \u03be2\u2192 \u00b7 \u00b7 \u00b7 \u03be d\u2032\u22121\u2192 vd\u2032 \u03be d\u2032\u2192 vd\u2032+1, d\u2032 \u2264 d where vd\u2032+1 is the leaf that is labeled with 1, vi is labeled with xji and the edge vj \u2192\u03bej vj+1 is labeled with \u03bej \u2208 {0, 1}. If A is (n, d)-universal set then there is a \u2208 A such that aji = \u03bei for all i = 1, . . . , d\u2032 and f(a) = 1. Therefore, A is a hitting set for DTd.\nThe other direction follows from the fact that any term T = x\u03be1j1 \u00b7 \u00b7 \u00b7x \u03bed jd\nwhere 1 \u2264 j1 < \u00b7 \u00b7 \u00b7 < jd \u2264 n and \u03bei \u2208 {0, 1} is a decision tree of depth d. Recall that x\u03be = x if \u03be = 1 and x\u03be = x\u0304 if \u03be = 0. To hit T we need an assignment a that satisfies aj1 = \u03be1, . . . , ajd = \u03bed. ut\n39\nFor equivalent test set, adaptive and non-adaptive learning, we prove\nTheorem 8. We have\n\u2126(2d log n) \u2264 OPTAD(C) \u2264 OPTNAD(C) \u2264 O(d22d log n).\nProof. Let A be an adaptive algorithm that learns DTd. We run A and answer 0 for all the membership queries. The algorithm must stop and return the function 0 as the target function. Let A be the set of all the assignments asked in the membership queries. If A is not (n, d)-universal set, then there is 1 \u2264 j1 < \u00b7 \u00b7 \u00b7 < jd \u2264 n and (\u03be1, . . . , \u03bed) \u2208 {0, 1}d such that (aj1 , . . . , ajd) 6= (\u03be1, . . . , \u03bed). Then the term T = x\u03be1j1 \u00b7 \u00b7 \u00b7x \u03bed jd \u2208 DTd is zero on all the assignments of A, and we get a contradiction. We now prove that DTd \u2295 DTd \u2282 DT2d and then the other results follow from 3 in Lemma 5. For any two functions f, g \u2208 DTd and the corresponding decision trees T (f) and T (g), one can construct a tree for f\u2295g from T (f) and T (g) as follows. First g+ 1 is equivalent to the tree T (g+ 1) that is the same as T (g) where the labels in the leafs are flipped from 0 to 1 and from 1 to 0. Now in the tree T (f), replace each leaf that is labeled with 0 with the tree T (g) and each leaf that is labeled with 1 with the tree T (g + 1). It is easy to show that this tree computes f \u2295 g, and its depth is at most 2d. ut"}, {"heading": "6.2 Adaptive Learning Decision Tree", "text": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251]. One of the powerful techniques used in the literature is the discrete Fourier transform DFT. In DFT, one regards the Boolean function f : {0, 1}n \u2192 {0, 1} as a real function f : {0, 1}n \u2192 {+1,\u22121} in <{0,1}n , represent it as a linear combination of orthonormal basis functions, and then learns the coefficients.\nIn [187], Kushilevitz and Mansour used this technique for learning the class of decision trees as follows. Consider the set A = {\u03c7a : {0, 1}n \u2192 {\u22121,+1}|a \u2208 {0, 1}n} where \u03c7a(x) = \u220fn i=1(\u22121)aixi . It is easy to see that A is an orthonormal basis for the set of functions f : {0, 1}n \u2192 <. Therefore, every function f : {0, 1}n \u2192 < can be represented as\nf = \u2211\na\u2286{0,1}n f\u0302(a)\u03c7a\nwhere f\u0302(a) \u2208 <. This representation is called the Fourier representation of f and f\u0302(a) is called the Fourier coefficient of \u03c7a. It is easy to see that the Fourier coefficients of \u03c7a is f\u0302(a) = Ex[f(x)\u03c7a(x)] where x \u2208 {0, 1}n, and the expectation is over the uniform distribution on {0, 1}n. So every coefficient can be estimated using Chernoff bound. It remains to show that for a decision tree of depth d the number of nonzero Fourier coefficients f\u0302(a) is small, and they can be found\n40\nexactly and efficiently2. We demonstrate the algorithm with the help of the following simple example.\nConsider the decision tree in Figure 4: f = x4(x2 + x\u03042x3) + x\u03044(x3 + x\u03043x\u03041). In this example, the depth of f is 3, and f is a sum of terms of size 3. First notice that since the terms are disjoint (no two terms are equal to 1 for the same assignment), the \u201c+\u201d operation can be replaced by the arithmetic \u201c+\u201d operation in <. To change the values of the function to +1,\u22121 values, we take g = 1\u2212 2f . In general, every decision tree of size s and depth d can be written as a sum (in <) of s terms of size at most d. Now take any term, say t = x4x\u03042x3. Over the real numbers <, we can express x as (1\u2212 (\u22121)x)/2 and x\u0304 as (1+(\u22121)x)/2. Then the term t can be expressed as\nt = 1\u2212 (\u22121)x4 2 \u00b7 1 + (\u22121) x2 2 \u00b7 1\u2212 (\u22121) x3 2\n= \u03c7(0000) + \u03c7(0100) \u2212 \u03c7(0010) \u2212 \u03c7(0001) \u2212 \u03c7(0110) \u2212 \u03c7(0101) + \u03c7(0011) + \u03c7(0111)\n8 .\nIn general, every term of size d has a Fourier representation that contains 2d nonzero coefficients f\u0302(a) each is \u00b11/2d and wt(a) \u2264 d where wt(a) is the Hamming weight of a, i.e, the number of ones in a. Therefore, every decision tree of size s\n2 An efficient learning algorithm for decision tree of depth d is one that asks poly(2d, logn) queries and runs in poly(2d, n) time. See Section 1.5.\n41\nand depth d has a Fourier representation that contains at most s2d+1 \u2264 22d+1 non-zero Fourier coefficients, each has one of the values in {\u00b1k/2d | k \u2208 [s2d]}\u222a {0}. In fact, using Parseval\u2019s identity, one can prove that \u2211 a\u2208{0,1}n f\u0302(a) 2 = 1 and, therefore, the Fourier coefficients have values from {\u00b1k/2d | k \u2208 [2d]}\u222a{0}. Also each non-zero coefficient f\u0302(a) satisfies wt(a) \u2264 d. Now using Chernoff bound, for each assignment a of weight at most d, one can exactly find each coefficient f\u0302(a) = Ex[f(x)\u03c7a(x)] with poly(s, 2\nd) queries. The problem with this algorithm is that since the number of assignments a that satisfies wt(a) \u2264 d is nO(d) the time complexity is exponential nO(d).\nKushilevitz and Mansour in [187] and Goldreich and Levin in [149] gave an adaptive algorithm that finds the non-zero coefficients in poly(2d, n) time and queries. Kushilevitz and Mansour algorithm (KM-algorithm) is based on the fact that for any \u03b1 \u2208 {0, 1}r we have\nF\u03b1 := E y,z\u2208{0,1}r,x\u2208{0,1}n\u2212r\n[f(yx)f(zx)\u03c7\u03b1(y)\u03c7\u03b1(z)] = \u2211\nx\u2208{0,1}n\u2212r f\u0302(\u03b1x)2,\nwhere for y = (y1, . . . , yr) and x = (x1, . . . , xs), yx = (y1, . . . , yr, x1, . . . , xs). Notice that F\u03b1 \u2264 1 can be computed exactly (with high probability) with Chernoff bound. Now KM-algorithm uses divide and conquer technique with the above identity to find the non-zero coefficients in poly(2d, n) time. Notice that\nF() = \u2211\nx\u2208{0,1}n f\u0302(x)2, F(0) = \u2211 x\u2208{0,1}n\u22121 f\u0302(0x)2, F(1) = \u2211 x\u2208{0,1}n\u22121 f\u0302(1x)2\nand therefore F() = F(0) + F(1). The algorithm first computes F(0) and F(1) and lets T1 = {\u03be \u2208 {0, 1}|F(\u03be) 6= 0}. At some stage, it holds a set Tr = {\u03b1(1), . . . , \u03b1(t)} \u2286 {0, 1}r where F\u03b1 6= 0 for all \u03b1 \u2208 Tr and F\u03b1 = 0 for all \u03b1 6\u2208 Tr. Now for each \u03b1 \u2208 Tr it computes F(\u03b1,0) and F(\u03b1,1). Since 0 6= F\u03b1 = F(\u03b1,0)+F(\u03b1,1), at least one of them is not zero. Then it defines Tr+1 = {(\u03b1, \u03be) | F(\u03b1,\u03be) 6= 0}. Since the number of the non-zero Fourier coefficients of a decision tree of depth at most d is less than 22d+1, the number of elements in Tr+1 is less than 2\n2d+1. Notice that for \u03b1 \u2208 {0, 1}n\nF\u03b1 = E y,z\u2208{0,1}n [f(y)f(z)\u03c7\u03b1(y)\u03c7\u03b1(z)]\n= E y\u2208{0,1}n [f(y)\u03c7\u03b1(y)] E z\u2208{0,1}n\n[f(z)\u03c7\u03b1(z)] = f\u0302(\u03b1) 2.\nand therefore, Tn contains all the assignments \u03b1 \u2208 {0, 1}n for which f\u0302(\u03b1) 6= 0. It is easy to see that this algorithm runs in time poly(2d, n). Therefore\nTheorem 9. [187] There is an adaptive Monte Carlo learning algorithm that learns DTd in poly(2 d, n) time and O(26d \u00b7 n) membership queries.\nKushilevitz and Mansour use a derandomization technique to change the algorithm to deterministic. They prove\n42\nTheorem 10. [187] There is an adaptive deterministic learning algorithm that learns DTd in poly(2 d, n) time and O(210d \u00b7 n log2 n) membership queries.\nBy Lemma 9, we have\nTheorem 11. There is an adaptive deterministic learning algorithm that learns DTd in poly(2\nd, n) time and O(213d+o(d) \u00b7 log n) membership queries. In particular, DTd is efficiently adaptively learnable and optimally adaptively\nlearnable in n.\nProof. We use Lemma 9. Since decision trees of depth d have at most 2d relevant variables, we can set r = 2d. By Theorem 10, Q(r) = O(d2211d). An (n, d)universal set is a constant testing set for DTd. See the proof of Theorem 8. The best known polynomial time, poly(n, 2d), construction gives an (n, d)-universal set of size O(2d+o(d) log n) [218]. Therefore P (n, r) = O(2d+o(d) log n). Then the reduction in Lemma 9 gives a polynomial time adaptive learning algorithm that asks\nO(r \u00b7Q(r)P (n, r) + r log n) = 213d+o(d) log n\nmembership queries. ut\nSee other randomized algorithms in [68,251] that use different techniques. The algorithm in [251] uses membership and equivalence queries, and it is easy to see that every equivalence query can be simulated by randomized membership queries."}, {"heading": "6.3 Non-Adaptive Learning Decision Tree", "text": "In this subsection, we give a sketch of the results in [65,149] and then of [130] that gave the first polynomial time Monte Carlo non-adaptive learning algorithm for DTd.\nThe following is the result of Hofmeister in [159]\nLemma 18. [159] There is a polynomial time deterministic non-adaptive algorithm for C = {\u03c7a(x)|wt(a) \u2264 d} that asks O(d log n) membership queries.\nIn particular, there is a set of assignments B = {b(1), . . . , b(t)} of size t = O(d log n) that can be constructed in polynomial time and an algorithm A such that: Given \u03c7a(b (1)), . . . , \u03c7a(b (t)) for some a of weight at most d, the algorithm A finds the assignment a in polynomial time.\nThe main idea of the learning algorithm of DTd is to use pairwise independent assignments for estimating Ex[f(x)\u03c7a(x)], rather than totally independent assignments. Since pairwise independent assignments can be generated with a small number of random bits, the problem is reduced to finding the Fourier coefficient of a function that depends on a small number of variable. Using those coefficients one can recover the assignments a with large Fourier coefficients f\u0302(a) in f . We now give a sketch of the algorithm and its correctness.\n43\nIt is easy to see that for the function fb(x) := f(x+ b) we have\nf\u0302b(a) = \u03c7a(b)f\u0302(a). (13)\nTherefore\n\u03c7a(b) \u00b7 sign(f\u0302(a)) = sign(f\u0302b(a)) = sign (Ex[f(x+ b)\u03c7a(x)])\nwhere sign(x) = 1 if x \u2265 0 and = \u22121 otherwise. Therefore\n\u03c7a(b) = sign(f\u0302(a)) \u00b7 sign (Ex[fb(x)\u03c7a(x)]) .\nAssuming we know sign(f\u0302(a)), to compute \u03c7a(b) for some b we only need to know the sign of Ex[fb(x)\u03c7a(x)]. Notice that |Ex[fb(x)\u03c7a(x)]| = |f\u0302b(a)| = |f\u0302(a)|. Now assuming |Ex[fb(x)\u03c7a(x)]| = |f\u0302(a)| is not zero (and therefore \u2265 1/2d), to compute the sign of Ex[fb(x)\u03c7a(x)] it is enough to use Bienayme-Chebyshev bound rather than Chernoff bound. That is, to estimate fb(x)\u03c7a(x) on pairwise independent assignments rather than totally independent assignments. To generate such assignments, consider a random uniform k \u00d7 n matrix R over the binary field F2 = {0, 1} where k = logm. We will determine m later. Then the assignments in the set {pR | p \u2208 {0, 1}k} are pairwise independent. Combining all the above ideas with Bienayme-Chebyshev bound we prove\nLemma 19. With probability at least 1\u2212 4/(m(f\u0302(a))2) we have\n\u03c7a(b) = sign(f\u0302(a)) \u00b7 sign(f\u0302R,b(aRT ))\nwhere fR,b(x1, . . . , xk) = f(xR T + b).\nProof. Let {z1, . . . , zm} = {pR|p \u2208 {0, 1}k}. Consider a random uniform x \u2208 {0, 1}n and the random variable X(x) = f(x)\u03c7a(x). We have \u00b5 = E[X] = f\u0302(a) and \u03c3 = Var[X] = E[X2] \u2212 E[X]2 = 1 \u2212 f\u0302(a)2. Since Xi = X(zi) are pairwise independent, by Bienayme-Chebyshev bound we get\nPr [\u2223\u2223\u2223\u2223\u2211mi=1Xim \u2212 f\u0302(a) \u2223\u2223\u2223\u2223 \u2265 |f\u0302(a)|2 ] \u2264 \u03c3 m ( |f\u0302(a)|\n2\n)2 \u2264 4 mf\u0302(a)2 .\nTherefore, with probability at least 1\u2212 2/(mf\u0302(a)2) we have\nsign(f\u0302(a)) = sign\n(\u2211m i=1 f(zi)\u03c7a(zi)\nm\n) .\nBy (13), with probability at least 1\u2212 4/(mf\u0302(a)2) we have\n\u03c7a(b) \u00b7 sign(f\u0302(a)) = sign (\u2211m i=1 fb(xi)\u03c7a(xi)\nm\n) .\n44\nSince \u2211m i=1 fb(zi)\u03c7a(zi)\nm =\n1\nm \u2211 p\u2208{0,1}k fb(pR)\u03c7a(pR)\n= 1\nm \u2211 p\u2208{0,1}k fR,b(p)\u03c7aRT (p) = f\u0302R,b(aR T )\nThe result follows. ut\nNotice that fR,b is a function in k = logm variables. This is the key lemma. It shows that if f\u0302(a) 6= 0, there is a positive probability that \u03c7a(b) can be computed (modulo the sign(f\u0302(a))) using the sign of some Fourier coefficient of f\u0302R,b. Since fR,b depends on a small number of variable and a membership to fR,b can be simulated by a membership to f (since fR,b(a1, . . . , ak) = f((a1, . . . , ak)R\nT +b)), all its Fourier coefficients can be easily found. Now if \u03c7a(b) is computed for all b \u2208 B = {b(1), . . . , b(t)}, where B is the set in Lemma 18, then a can be found with Hofmeister\u2019s algorithm. In the following, we give more details.\nBy Lemma 19 and using the union bound we have\nLemma 20. Let B = {b(1), . . . , b(t)} be as in Lemma 18 and t = |B|. Let m = 22d+3t. For any a, if |f\u0302(a)| \u2265 1/2d, there is z \u2208 {0, 1}k and \u03be \u2208 {\u22121,+1} such that with probability at least 1\u2212 (22d+2t)/m \u2265 1/2 we have\n(\u03c7a(b (1)), . . . , \u03c7a(b (t))) = \u03be \u00b7 (sign(f\u0302R,b(1)(z)), . . . , sign(f\u0302R,b(t)(z)))\nwhere fR,b(i)(x1, . . . , xk) = f(xR T + b(i)).\nNow since fR,b(i) depends on k = logm variables we can find all its Fourier coefficients in time poly(m) and 2k = m = O(22dd log n) membership queries. Therefore, in poly(m, t) time and 2kt = O(22dd2 log2 n) membership queries we can find\nW\u03be,z := \u03be \u00b7 (sign(f\u0302R,b(1)(z)), . . . , sign(f\u0302R,b(t)(z)))\nfor all z and \u03be. If f\u0302(a) is not zero then |f\u0302(a)| \u2265 1/2d and then by Lemma 20, with probability at least 1/2, some z and \u03be satisfies (\u03c7a(b (1)), . . . , \u03c7a(b (t))) = W\u03be,z. Then by Lemma 18, a can be recovered in polynomial time. So from all W\u03be,z and using the algorithm A in Lemma 18 we find a set of assignments Q such that: if f\u0302(a) is not zero then a \u2208 Q with probability at least 1/2. This implies that on average, Q contains half of the assignments that correspond to the non-zero Fourier coefficients of f . The size of Q is at most 2k+1 = O(22dd log n). Then we find the Fourier coefficient f\u0302a = Ex[f(x)\u03c7a(x)] for all a \u2208 Q using Chernoff bound and the union bound with O(2d log |Q|) = o(2k) additional membership queries. We can repeat the above log(22d/\u03b4) time to find all the non-zero Fourier coefficients of f with probability at least 1\u2212 \u03b4.\nPutting all the above ideas together, it follows that\n45\nLemma 21. [130]. There is a non-adaptive Monte Carlo learning algorithm that learns DTd in polynomial time and O(d 322d log2 n) membership queries.\nBy Lemma 14, we get\nLemma 22. There is a non-adaptive Monte Carlo learning algorithm that learns DTd in polynomial time and O(d\n424d log n) membership queries. In particular, DTd is MC efficiently non-adaptively learnable and MC opti-\nmally non-adaptively learnable in n.\nProof. We use Lemma 14. Since a decision tree of depth at most d contains at most 2d relevant variables, we can take r = 2d. We take q = 24d. By Lemma 14, Q(q) = O(d322d log2 q). Then the number of membership queries is\nO\n( r2Q(q)\nlog(q/r2) log n\n) = O(d424d log n).\nut\nA better query complexity can be obtained from the reduction in [49]. See the following Table.\nThe outputs of the above algorithms are the Fourier representation of the decision tree and, therefore, they are non-proper learning algorithms.\nThe following paper summarizes the current state of the art results in learning DTd\nAdaptive/ Deterministic/ References of Non-Adaptive Randomized Query Complexity Alg.+Reduction Adaptive Randomized d422d log2 n [130] Adaptive Randomized d622d log n [130,49] Adaptive Deterministic 213d+o(d) log n [187,62] Non-Adaptive Randomized d422d log2 n [130] Non-Adaptive Randomized d622d log n [130,49] Non-Adaptive Deterministic poly(2d, log n) OPEN"}, {"heading": "7 Other Results", "text": "In this section, we give some results for learning other Boolean classes, arithmetic classes."}, {"heading": "7.1 Other Boolean Classes", "text": "d-MTerm: This class is the dual of d-MClause. That is,\nd-MTerm=d-MClauseD := {f(x1, . . . , xn) | f \u2208 d-CLause}.\n46\nAny algorithm A that learns a class C can be converted to an algorithm B that learns CD with the same query complexity. This can be done as follows: Algorithm B runs algorithmA and for each query a thatA asks, algorithm B asks the query (a1, . . . , an). For each answer b received by the teacher, algorithm B returns the answer b to A. If algorithm A outputs h then algorithm B outputs\nhD := h(x1, . . . , xn).\nd-Term (the dual of d-Clause) We first recall the definition of (n, d)-universal set and then show how to use it for learning d-Term.\nA d-restriction problem [24,38,218] is a problem of the following form: Given \u03a3 = {0, 1}, a length n and a set B \u2286 \u03a3d of assignments. Find a set A \u2286 \u03a3n of small size such that: For any 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < id \u2264 n and b \u2208 B there is a \u2208 A such that (ai1 , . . . , aid) = b.\nWhen B = {0, 1}d then A is called (n, d)-universal set. The lower bound for the size |A| of (n, d)-universal set is [191,246]\n\u2126(2d log n). (14)\nUsing a simple probabilistic method, one can get the upper bound\nO(d2d log n). (15)\nAlso, a random uniform set of O(2d(d log n+log(1/\u03b4))) assignments in {0, 1}n is, with probability at least 1\u2212 \u03b4, (n, d)-universal set. The best known polynomial time (poly(2d, n)) construction for (n, d)-universal set is of size\n2d+O(log 2 d) log n\n[218]. For d \u2264 log n/ log log n, a (n, d)-universal set of size dO(1)2d log n can be constructed in polynomial time [218].\nNow consider the class of d-Term. Let A be an adaptive algorithm that learns this class. Suppose the target function is the zero term and let S \u2282 {0, 1}n be the set of queries that the algorithm asks with this target. Then S must satisfy the following property: For every 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < id \u2264 n and every b \u2208 {0, 1}d there is a \u2208 S such that (ai1 , . . . , aid) = b. Otherwise, the algorithm cannot distinguish between the zero term and the term T = xb1i1 \u00b7 \u00b7 \u00b7x bd id\nwhere x1 = x and x0 = x\u0304. This is because T is also zero on all the assignments in S. Therefore, S must be an (n, d)-universal set and then the query complexity of the algorithm is at least \u2126(2d log n).\nNow it is easy to see that any (n, d+ 1)-universal set S can be used to learn non-adaptively the class d-Term. Just take all the positive assignments, i.e., the assignments a \u2208 S such that f(a) = 1, and find the entries that have the same value in all of them. This uniquely determines the term. Therefore\nO(d2d log n) = OPTNAD(d-Term) \u2265 OPTAD(d-Term) = \u2126(2d log n).\n47\nThis also gives a non-adaptive learning algorithm that asks 2d+O(log 2 d) log n queries and runs in poly(2d, n) time. Therefore the class d-Term is non-adaptively almost optimally learnable.\nXOR: The class XOR is of size 2n+1 and therefore, by Lemma 2, any adaptive learning algorithm for XOR must ask at least n + 1 queries. Now the trivial algorithm that asks the n+1 queries {0}\u222a{ei}i=1,...,n, where ei is the assignment that is 1 in entry i and zero elsewhere, learns XOR. Therefore, the class XOR is optimally learnable.\nd-XOR: Since\n|d-XOR| \u2265 ( n+1 d ) ,\nby Lemma 2, the lower bound for the number of queries for any randomized learning this class is \u2126(d log n). Uehara et al. gives in [266] an adaptive algorithm that learns d-XOR in O(d log n) queries. Therefore d-XOR is adaptively optimally learnable. Hofmeister gives in [159] a non-adaptive algorithm that learns d-XOR in O(d log n) queries. Therefore d-XOR is also non-adaptively optimally learnable.\nd-Junta: The class of d-Juntas is studied by Damaschke in [107,108,109] and Bshouty and Costa in [49]. In [108], Damaschke shows that\n\u2126(2d log n) = OPTNAD(d-Junta) = O(d 22d + d2d log n).\nHe then shows that d-Junta is almost optimally learnable in d and efficiently learnable in n [107,109]. Using Lemma 14 with this result, we get an algorithm that asks 2d+o(d) log n queries and runs in time poly(2d, n). Therefore the class d-Junta is almost optimally learnable. Bshouty and Costa, [49], close the above gap and showed that\nOPTNAD(d-Junta) = O(d2 d log n).\nThey also showed that randomness does not help improving the query complexity. See also other results for randomized algorithms in [49,107], optimal algorithms for small d with a constant number of rounds and bounds for the number of rounds in [49,109].\nThe following is a simple adaptive learning algorithm [108]. First ask the queries of an (n, d)-universal set. Then take any two assignment a and b such that f(a) 6= f(b). Then find a relevant variable by a binary search on the bits that differ between a and b. Let Y be a subset of the relevant variables that is found so far. To find another relevant variable, we search for two assignments a and b that give the same values for the variables in Y and f(a) 6= f(b). If no such assignments exist, then, Y is the set of all the relevant variables and then just learn the truth table over Y . Otherwise, the binary search between a and b gives a new relevant variable. It is easy to see that the query complexity of this\n48\nalgorithm is s+ d log n where s is the size of the (n, d)-universal set. This shows that\n\u2126(2d log n) = OPTAD(d-Junta) = O(d2 d log n)\nand, therefore, the class d-Junta is almost optimally adaptively learnable.\nd-MJunta: The results in [49,108,109,220] show that\n\u2126(2d/ \u221a d+ d log n) \u2264 OPTAD(d-MJunta) \u2264 O(2d + d log n)\nand \u2126(2d log n) = OPTNAD(d-MJunta) = O(d2 d log n).\nUsing Lemma 14 with the result of Damaschke in [109], we get a non-adaptive learning algorithm for d-MJunta that asks 2d+o(d) log n queries and runs in time poly(2d, n). Therefore the class d-MJunta is almost optimally non-adaptively learnable.\nThe class of n-MJunta is studied in [154,180] where the exact value\nOPTAD(MDNF) =\n( n\nbn/2c\n) + ( n\nbn/2c+ 1 ) was found. Now, by Lemma 13, d-MJunta is adaptively learnable in time poly(n, 2d) and O( \u221a d2d+d log n) queries. Thus, the class d-MJunta is adaptively almost optimally learnable.\nDecision Trees (DTd). See Section 6.\nDNF: This class and its subclasses are not studied in the literature for the model of exact learning from membership queries only.\nMonotone DNF: See Section 5.\nCNF: The dual class of DNF.\nCDNF: This class is not studied in the literature for the model of exact learning from membership queries only. Some non-optimal results can be achieved using the algorithm in [33] and the reductions in Subsection 3.4.\nMonotone CDNF: The learnability of monotone CDNF is studied in [47,110,118]. Domingo, [110], show that the class of monotone CDNF is learnable with a polynomial number of queries in time sO(log s) where s is the size of the monotone CDNF. That is, the size of the MDNF and MCNF of the target. In [118] Domingo et al. study the learnability of the class Read k-MCDNF. This is the class of monotone CDNF functions (f, g) where each variable appears at most k times in its MDNF representation f and any number of times in its MCNF representation g. See also [110] for other subclasses of monotone CDNF that are learnable\n49\nfrom membership queries. Bshouty et al., [47], show that the class of MCDNF and O(log n)-CDNF are learnable from membership queries and the NP-oracle.\nBoolean Multivariate Polynomial: The efficient randomized learnability of multivariate polynomial follows from [68]. All the other algorithms in the literature require asking membership queries from an extension field. See for example [147].\nXT, DFA, BMAF, ROF, BC, BF. No results are known for exact learning of those classes from membership queries only, except for the trivial result that when all the variables are relevant then OPTAD(C) = 2 n.\nBoolean Halfspace (BHS): Hegedu\u0308s, [156], shows that BHS(0, 1) (with zeroone weights) are adaptively learnable in polynomial time with O(n) queries. He also gives a lower bound \u2126(n) for the number of queries. Therefore, BHS(0, 1) is adaptively optimally learnable. See also [266]. Hegedu\u0308s and Indyk, [163], give a non-adaptive polynomial time learning algorithm for BHS(0, 1) that asks O(n2) queries.\nAbboud et al., [8], show that BHS[k] (Boolean Halfspaces with weights in {0, 1, . . . , k}) is constant-round learnable in nO(k5) time and queries. They also gave the lower bound n\u2126(k). Abasi et al. [7] give a non-adaptive algorithm for BHS[k] that asks nO(k 3) and a two-round algorithm that asks nO(k) queries and runs in time nO(k). Therefore, the class BHS[k] is adaptively efficiently learnable.\nAbboud et al. [8] give a lower bound \u2126(2n/ \u221a n) for BH(\u22121, 0, 1) (Boolean Halfspaces with weights {\u22121, 0,+1}). Therefore, BH(\u22121, 0, 1) is non-adaptive almost optimally learnable. Just ask all the {0, 1}n queries.\nUehara et al. study some restricted classes of BHS(0, 1), [266].\nShevchenko and Zolotykh [261] studied halfspace function over the domain {0, 1, . . . , k\u22121}n when n is fixed and no constraints on the coefficients. They gave the lower bound \u2126(logn\u22122 k) for learning this class from membership queries. Hegedu\u0308s [158] proves the upper boundO(logn k/ log log k). For fixed n, Shevchenko and Zolotykh [276] gave a polynomial time algorithm (in log k) for this class. Applying Theorem 3 in [158], the upper bound O(logn\u22122 k) for the teaching dimension of a halfspace, [106], gives the upper bound O(logn\u22121 k/ log log k).\nMROF. A monotone Boolean read-once formula is a monotone formula such that every input variable xi appears in at most one input gate. Angluin et al. gave a polynomial time algorithm that learns MROF with O(n2) queries [20,164]. The best lower bound for the number of queries is the information theoretic lower bound \u2126(n log n) that follows from Lemma 2.\nBshouty shows in [36] that MROF cannot be learned efficiently in parallel (poly(log n) time).\nOther Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].\n50"}, {"heading": "7.2 Classes of Arithmetic Functions", "text": "In this section, we give few results from the literature on learning arithmetic classes.\n(r, V )-Linear Functions ((r, V )-LF). The problem of learning LF is studied in [1,77,101,127,198,199,207,250]. Many authors independently proved that it is optimally learnable with\n\u0398\n( n\nlog n ) queries. They do not address the time complexity, although one can show that the constructions also give simple algorithms that run in polynomial time.\nThe class r-LF is studied in [37,78,79,113,145,200,204,264,266]. It is shown that\nOPTAD(r-LF) = OPTNAD(r-LF) = \u0398\n( r log(n/r)\nlog r\n) .\nNote here that in the literature they use log(n/r) to mean log(2n/r). In [37], Bshouty shows that it is optimally adaptively learnable. The problem is still open for the non-adaptive learning.\nThe problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179]. Bshouty and Mazzawi, [70], show that\nOPTAD((r,<)-LF) = OPTNAD((r,<)-LF) = \u0398 ( r log(n/r)\nlog r\n) .\nThe results are derived from non-constructive probabilistic proofs. All the learning algorithms for this class are either for restricted subclasses or randomized algorithms with success probability that depend on r or non-optimal.\nSee other subclasses in [37,145,225,236]. Similar problems are studied in other areas such as coding theory [229] compressed sensing [183] Multiple Access Channels [50] (e.g., adder channels [98]) and combinatorial group testing [113,114] (e.g., coin weighing problem [37]).\n(r, V )-Quadratic Functions ((r, V )-QF). This problem is equivalent to learning a weighted graph from additive queries, [144], where, for an additive query, one chooses a set of vertices and asks the sum of the weights of edges with both ends in the set.\nThe r-QF was studied in [1,96,97,136,144,145,205,235]. The (r, V )-QF for different V \u2286 < was studied in [70,71,72,83,97]. Bshouty and Mazzawi, [70], proved that\nOPTNAD(r-QF) = OPTNAD((r,<)-QF) = \u0398 ( r log n\nlog r\n) .\nThe results are derived from non-constructive probabilistic proofs. For the positive real numbers <+, Bshouty and Mazzawi gave in [72] a polynomial time\n51\nalgorithm that adaptively learns the class (r,<+)-QF with O(r log n/ log r + r log log r) queries. This is the only known deterministic adaptive algorithm that runs in polynomial time. Choi, [83], gave a polynomial time randomized adaptive learning algorithm for (r,<)-QF that asks O(r log n/ log r) queries.\nBshouty and Mazzawi extended some of the above results to multilinear forms of constant degree [69].\nMultivariate Polynomial: This class has been extensively studied in the literature. Ben-Or and Tiwari [75] gave the first deterministic non-adaptive polynomial time learning algorithm for sparse multivariate polynomial over a large field with an optimal number of queries. See also [141,147,148,185,193].\nFor identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.\nMultiplicity Automata Function: This class was first defined and studied in [44]. It is efficiently learnable from queries with a randomized MC algorithm [44].\nArithmetic Circuit and Arithmetic Formula: In [267] Valiant suggests an algebraic analog of P vs. NP, the VP vs. VNP problem. A multivariate polynomial family {pn(x1, . . . , xn)}n\u22651 is in VP if there exists a constant c > 0 such that for all n, deg(pn) \u2264 nc and pn has a circuit of size bounded by nc. Polynomial family {qn}n\u22651 is in VNP if there exists a family {pn} \u2208VP such that for every n\nqn(x1, . . . , xn) = \u2211\ny\u2208{0,1}n p2n(x1, . . . , xn, y1, . . . , yn).\nValiant shows in [267] that permanent is complete for VNP, i.e., for every polynomial family {qn}n\u22651 in VNP, there is a constant c > 0 such that for every n \u2265 1, qn can be expressed as permanent of a matrix of size nc\u00d7nc. It is believed that VP 6=VNP. This remains an outstanding open problem.\nIn [31], Agrawal and Vinay show that if there exists a deterministic polynomial time zero testing for arithmetic circuits of degree d and depth 4 then there exists a polynomial family {qn}n\u22651, computable in exponential time, that is not in VP. So an efficient deterministic zero testing for such circuits leads to a proof of circuit subexponential lower bounds that may be beyond our proof techniques.\nKabanets and Impagliazzo show in [184] that even if the zero testing algorithm gets the arithmetic circuit as an input (white box) if there exists a deterministic polynomial time algorithm for zero testing for VP then either NEXP6\u2208P/poly or VP 6=VNP. Therefore, any deterministic algorithm implies solving outstanding open problems in complexity. See [29,255] for other negative results.\nOn the other hand, the following Schwartz-Zippel lemma, [241,275], gives a very simple MC randomized optimal zero testing algorithm for any arithmetic circuit with a bounded degree\n52\nLemma 23. (Schwartz-Zippel) Let f \u2208 F [x1, . . . , xn] be any non-zero polynomial of degree d and S \u2282 F . Then for y1, . . . , yn selected randomly uniformly from S we have\nPr y\u2208Sn\n[f(y1, . . . , yn) 6= 0] \u2265 1\u2212 d\n|S| .\nFor the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein. Some other results in the literature investigate the problem of minimizing the number of random bits used for identity testing. See for example [32,64,193].\nArithmetic Read-Once Formulas (AROF): Arithmetic Read-Once Formula is a formula where each variable appears at most once. In [59] Bshouty et al. gave an MC randomized polynomial time algorithm for AROF (with the division operation) over a large enough field F . In [48] Bshouty and Cleve gave a polynomial time (poly(log)) randomized parallel algorithm for this class. In [43], Bshouty and Bshouty extended the result of [59] to include the exponentiation operation. Shpilka and Volkovich in [257] gave a deterministic algorithm for learning depth d AROF in time nO(d). In [256] Shpilka and Volkovich gave a deterministic learning algorithm for AROF that asks nlogn queries. They also studied the class of sum of k AROFs. Recently, Volkovich gave in [214] a polynomial time algorithm for learning any AROF.\nOther Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein."}, {"heading": "8 Non-Honest Teacher", "text": "Although the aim of this survey is to summarize the results of learning from an honest teacher, we feel a need to give here some of the models of non-honest teacher and some results."}, {"heading": "8.1 Models of Non-Honest Teacher", "text": "In this survey, the teacher model is the honest teacher model where with a query d \u2208 X, the teacher answers f(d).\nFor non-honest teacher, there are many models. One can consider a persistent teacher [27,223] or a non-persistent teacher. For persistent teacher (or permanently faulty [223]) if the answer to the query d is y then no matter how many times the learner asks the same query the answer will be y. A non-persistent teacher is a teacher that is not persistent. In the literature the following nonhonest teacher models are considered (each one can be either persistent or nonpersistent):\n53\n1. Incomplete Model [27]: The incomplete teacher, with a query d, answers f(d) with probability p and answers \u201c?\u201d (I DON\u2019T KNOW) with probability 1\u2212p. In the persistent model, repeated queries to d will give the same answer with probability 1. In the non-adaptive model, the learner knows p or some upper bound for p.\n2. Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p. The learner knows p or some upper bound for p.\n3. Limited Incomplete Model [22]: The limited incomplete teacher gives answers \u201c?\u201d (I DON\u2019T KNOW) to at most m queries of its choice. In the non-adaptive model, the learner knows m or some upper bound for m.\n4. Limited Malicious Model [22,265]: (Also called the constant number of error model [16,231]) The limited malicious teacher gives arbitrary/random wrong answers to at most m queries of its choice. The learner knows m or some upper bound for m.\n5. Prefix-Bounded Error Fraction Model [222]: (Also called linearly bounded model [16]) In the adaptive model, the teacher after t queries can give at most pt wrong answers. In the r-round model, at each round with T queries and for any 1 \u2264 t \u2264 T , the learner can give pt wrong answers to the first t queries in this round. The learner knows p or some upper bound for p.\n6. Globally Bounded Error Fraction Model [222]: In the adaptive model, if the algorithm asks T queries then the teacher can give at most pT wrong answers. In the r-round model, at each round with T queries, the learner can give at most pT wrong answers. The learner knows p or some upper bound for p.\nNotice that in the globally bounded error fraction model the first pT queries can be all wrong while in the prefix-bounded error fraction model only p2T queries of the first pT queries can be wrong.\n7. Incomplete Prefix-Bounded Error Fraction Model: In the adaptive model, the teacher after t queries can give at most pt \u201c?\u201d answers. In the r-round model, at each round with T queries and for any 1 \u2264 t \u2264 T , the learner can give pt \u201c?\u201d answers to the first t queries in this round. In the non-adaptive model, the learner knows p or some upper bound for p.\n8. Incomplete Globally Bounded Error Fraction Model [37]: In the adaptive model, if the algorithm asks T queries then the teacher can give at most pT \u201c?\u201d answers. In the r-round model, at each round with T queries, the learner can give at most pT \u201c?\u201d answers. In the non-adaptive model, the learner knows p or some upper bound for p.\n9. E-Sided Error Models: (Also called half-error [224], or one-sided error [231], for Boolean functions) Can be defined for any one of the above models where the wrong or\u201c?\u201d answers only applied when f(d) is in some set E \u2282 R.\n54\nFor the persistent model we define the output hypothesis to be equivalent to the target function if it agrees with the target function on all the elements of the domain except the ones for which the teacher answer \u201c?\u201d or gave a wrong answer."}, {"heading": "8.2 Some Results in Learning with Non-honest Teacher", "text": "In this subsection, we give some results of learning with a non-honest teacher. Adaptively learning Var= {x1, . . . , xn} in non-honest teacher model is equivalent to the problem of \u201csearching with lies\u201d [265]. Ulam [265] proposed the following game. Someone thinks of a number between one and one million (which is just less than 220). Another person is allowed to ask up to twenty questions, to each of which the first person is supposed to answer only yes or no. Obviously, the number can be guessed by asking first: is the number in the first half-million? Then again reduce the reservoir of numbers in the next question by one-half, and so on. Finally, the number is obtained in less than log2 1000000 questions. The number h corresponds to the target variable xh in the class Var and each question \u201cIs h \u2208 H?\u201d corresponds to the query (a1, . . . , a1000000) where ai = 1 if and only if i \u2208 H.\nUlam asked the following question: Now suppose one were allowed to lie once or twice, then how many questions would one need to get the right answer? This problem is equivalent to learning the class Var in the limited malicious model. Re\u0301nyi [227] asked a similar question and therefore, the game is called Re\u0301nyi-Ulam game.\nThis problem is completely solved with an asymptotically optimal number of queries in the limited malicious model [1,137,221,245]. See also the references in [224] for results when the number of lies is small. Learning this class in two-round is studied in [102,103,105].\nThe problem is solved with an asymptotically optimal number of queries in the linearly bounded model [16,222,259]. It is also noted by several authors that finding a non-adaptive algorithm in this model is equivalent to constructing a t-error correcting code [224].\nSee the survey in [224] for results in other models of non-honest teacher. For learning d-MClause and s-term r-MDNF with non-honest teacher see\n[5,16,80,81,88,94,95,113,203,231,271] and references therein."}, {"heading": "9 Problems and Open Problems", "text": "In this section, we give some problems and open problems\nSection 1\n1.1. In real life problems, the target function may change in time. Define a realistic learning model for learning functions that change in time.\n55\n1.2. In the results of this survey and almost all papers in the literature, the space complexities of learning algorithms are polynomial in OPTA(C) which, for many classes C, is exponential in n and/or other parameters that depend on the class. It is interesting to investigate learning algorithms that use small space complexity.\n1.3. It is interesting to minimize the number of random bits used in randomized learning algorithms. See for example item 4.8.\n1.4. It is interesting to study the exact learnability of a random function in a class C from membership queries. See, for example, some models in [173,174,263].\n1.5. An LV randomized non-adaptive algorithm with query complexity Q of complexity T is an algorithm that asks at most Q queries and runs in expected time T . So any LV randomized non-adaptive algorithm is deterministic in choosing the queries. We suggest the following definition that allows expected query complexity in non-adaptive learning algorithms: A weak LV randomized non-adaptive algorithm with complexity T is a non-adaptive algorithm that (1) generates queries that are independent of the answers to the previous queries. (2) Finds the target function with probability 1. (3) The expected number of queries is Q and the expected time is T .\n1.6. In this survey, we have shown some results for the testing problems. Some of those results are not true for LV/MC randomized algorithms. For example, in deterministic algorithms, the query complexity of non-adaptive learning is equal to the minimum size equivalent test. For randomized algorithms, one can non-adaptively equivalent test the class XOR with O(log 1/\u03b4) random queries whereas learning XOR by a randomized algorithm takes at least n queries. It is interesting to study MC and LV randomized equivalent test and other types of tests in the adaptive and non-adaptive model.\n1.7. Investigate testing in the deterministic/randomized r-round model.\n1.8. There are very few results in the literature on parallel learning from membership queries. That is, learning in poly(log) time. Study parallel learning.\n1.9. To the best of my knowledge, all the Monte Carlo learning algorithms in the literature ignore minimizing the effect of the success probability 1 \u2212 \u03b4 in computing the number of queries. Some of the results even ignore \u03b4 by assuming that it is constant. It is interesting to investigate the role of \u03b4 in the query complexity.\n1.10. We say that a non-adaptive algorithm is strongly nonadaptive if the queries are constructed by different learners (one query for each learner) without any communication between them. It is interesting to study this model or any model with minimum communication between the learners.\nSection 2\n2.1. In the bound\n2 \u00b7 ETD(C) log ETD(C) log |C| \u2265 OPTAD(C) \u2265 max(ETD(C), log |C|),\n56\nfind some conditions on C for which tighter bounds can be obtained.\n2.2. Many lower bounds in the literature for OPT(C) are based on finding a subset of functions C \u2032 \u2286 C such that for each membership query there is an answer that eliminates at most small fraction of the functions. The best possible bound that one can get using this technique is denoted by DEN(C). In [67] Bshouty and Makhoul show that ETD(C) \u2265 DEN(C) \u2212 1. Find a new combinatorial measure that is a lower bound for OPT(C) and exceeds ETD(C).\n2.3. The algorithm in (6) runs in time nO(2 h) where h is the depth of the tree.\nFind an algorithm with a better exponential complexity.\n2.4. Find a non-adaptive learning algorithm that runs in time poly(|X|, |C|) and learns C using at most (c ln |C|)OPTNAD(C) queries for some c < 2.\n2.5. Study the above bounds and find approximation algorithms for randomized adaptive learning.\n2.6. Study bounds and find approximation algorithms for adaptive learning of classes with small VC-dimension.\n2.7. Study the above bounds and find approximation algorithms for r-round learning.\n2.8. Is NP-oracle enough for deterministic/randomized optimal learnability? What other oracle gives learning with minimum number of membership queries?\n2.9. In [53] some techniques were used in the model of exact learning from membership and equivalence queries to minimize the number of equivalence queries. Can those be used to find more query-efficient algorithms?\n2.10. Study the above bounds and find approximation algorithms for classes with small extended teaching dimension.\n2.11. Study bounds for LV and MC randomized algorithms.\nSection 3\n3.1. The reductions in subsection 3.1 are for adaptive and non-adaptive learning. It is interesting to find reduction results for r-round deterministic and randomized algorithms.\n3.2. The reductions in subsection 3.1 are for the number of relevant variables. Find reductions for other parameters, for example, the number of terms (e.g. for MP or MDNF).\n3.3. Find reductions that give algorithms that are optimally learnable or almost optimally learnable from r.\n3.4. Lemma 15 is implicitly used for some of the results in the literature for learning some classes. For example, the Halving algorithm is an algorithm that asks equivalence query with \u201cMajority(C \u2032)\u201d at each stage, where C \u2032 \u2286 C are the functions in C that are consistent with the counterexamples seen so far.\n57\nLemma 3 is just a reduction from the Halving algorithm. It is interesting to study learnability of the classes mentioned in this survey with this technique.\n3.5. There are many polynomial time exact learning algorithms from membership and equivalence queries in the literature for classes mentioned in this survey and others. See [2,3,17,26,33,34,35,44,58,74,182,251,253]. It is interesting to study the reduction of those algorithms to learning from membership queries only when some of the parameters of the class is restricted. For example, can Angluin-Frazier-Pitt learning algorithm for conjunctions of horn clauses, [17], be changed to learning from membership queries when the number of terms is bounded by d or/and the size of each clause is bounded by k.\n3.6. Let H be a family of functions h : [n] \u2192 [q]. For d \u2264 q we say that H is an (n, q, d)-perfect hash family ((n, q, d)-PHF) [24] if for every subset S \u2286 [n] of size |S| = d there is a hash function h \u2208 H such that h|S is injective (one-to-one) on S, i.e., |h(S)| = d. In [41] it is shown that for q \u2265 2d2. There is a (n, q, d)-PHF of size O ( d2 log n/log(q/d2) ) that can be constructed in\ntime O(qd2n log n/ log(q/d2)). This construction is used for many reduction in learning. It is known that there is a (n,O(d2), d)-PHF of size O (d log n). Finding a polynomial time construction for (n,O(d2), d)-PHF of such size improves the query complexity of many reductions.\nSection 4\n4.1. Non-adaptive randomized algorithms have been proposed in [46,66,114,127,161,165]. The following models are studied in the literature for constructing the m\u00d7n random test matrix M\n\u2013 Random incidence design (RID algorithms). The entries in M are chosen randomly and independently to be 1 with probability p and 0 with probability 1\u2212 p.\n\u2013 Random r-size design (RrSD algorithms). The rows in M are chosen randomly and independently from the set of all vectors {0, 1}n of weight r.\n\u2013 Random k-set design (RkSD algorithms) The columns in M are chosen randomly and independently from the set of all vectors {0, 1}m of weight k. Find lower and upper bounds for the constant in \u03b8(d log n) of the number of membership queries for the above non-adaptive learning algorithms.\n4.2. Find a polynomial time O(1)-round algorithm for learning d-MClause that asks O(d log n) queries.\n4.3. Find a deterministic non-adaptive learning algorithm for d-MClause that asks O(d2 log n / log d) queries.\n4.4. A construction of a d-disjunct matrix is called globally explicit construction if it is deterministic polynomial time in the size of the construction. A locally explicit construction is a construction where one can find any entry in the construction in deterministic poly-log time in the size of the construction. In particular, a locally explicit construction is also globally explicit. The\n58\nconstructions in the literature for d-disjunct matrices are globally explicit constructions. Find a locally explicit construction of d-disjunct matrix of size O(d2 log n).\n4.5. There are few results in the literature about learning d-MClause when d unknown to the learner. It is interesting to study this problem.\n4.6. Let F be a set of n functions f : X \u2192 {0, 1}. Define d-MClause(F) the set of all functions f1\u2228f2\u2228\u00b7 \u00b7 \u00b7\u2228fd\u2032 where d\u2032 \u2264 d and fi \u2208 F for all i = 1, . . . , d\u2032. Study the learnability of the class d-MClause(F).\n4.7. Study the learnability of the class of monotone clauses with constant number of negated variables.\n4.8. Any deterministic algorithm for non-adaptive learning d-MClause has query complexity \u2126(d2 log n/ log d) while there is a Monte Carlo non-adaptive learning algorithm that asks O(d log n) queries only and uses O(dn log n) random bits. What is the tradeoff between the number of random bits and the query complexity?\nSection 5\n5.1. Find strong learning algorithms for s-term r-MDNF with the parameter r or/and s.\n5.2. Many results in the literature for learning sub-classes of s-term r-MDNF are query-efficient, but are not time-efficient. It is interesting to find polynomial time learning algorithms for those classes.\n5.3. Find a non-adaptive efficient learning algorithm for the class s-term r-MDNF when r = \u03c9(1).\n5.4. Angluin and Chen gave in [15] a polynomial time 5-round Las Vegas algorithm for learning s-term 2-MDNF that asks O(s log n+ \u221a s log2 n) queries.\nCan this class be learned in O(1)-round with O(s log n) queries?\n5.5. Find OPTR\u2212RAD(s-term 2-MDNF) for R = 2, 3, 4.\n5.6. Give an optimal learning algorithm for s-term r-MDNF for constant r > 2.\n5.7. The class of Read-Once 2-MDNF is equivalent to learning matchings [11]. Alon et al. gave bounds for deterministic, randomized and r-round learning this class. Extend the results to other related classes such as Read-Once r-MDNF, Read-Twice 2-MDNF and Read-Once 2-DNF.\nSection 6\n6.1. We show that\n\u2126(2d log n) \u2264 OPTAD(C) \u2264 OPTNAD(C) \u2264 O(d22d log n).\nClose the gap between the lower and upper bound.\n6.2. What are the query complexities of the randomized learning algorithms for DTd in [68,251]?\n59\n6.3. The deterministic adaptive algorithm of Kushilevitz-Mansour [187] asksO(210d\nn log n) queries. Find a more query-efficient algorithm.\n6.4. Find a proper learning algorithm for DTd. Can DTd be learned from DTpoly(d)?\n6.5. Let X be a finite set and Y be any set. One of the important representations of functions f : Xn \u2192 Y is decision tree over the alphabet Y with output X. A decision tree over Y with output X is defined as follows: The constant functions y \u2208 Y are decision trees. If fi is a decision trees for i = 1, . . . , t and S1, . . . , St is a partition of X then, for all j = 1, . . . , n,\n\u201cf \u2032 =[if xj \u2208 S1 then f1 else if j \u2208 S2 then f2 \u00b7 \u00b7 \u00b7 else if j \u2208 St then ft]\u201d (16)\nis a decision tree (can also be expressed as f \u2032 = [xi \u2208 S1]f1+\u00b7 \u00b7 \u00b7+[xi \u2208 St]ft. Here [x \u2208 S] = 1 if x \u2208 S and 0 if x 6\u2208 S. Every decision tree f \u2032 can be represented as a tree T (f \u2032). If f \u2032 = y for some y \u2208 Y then T (f \u2032) is a node labeled with y. If f \u2032 is as in (16), then T (f \u2032) has a root labeled with xi and has t outgoing edges. The ith edge is labeled with Si and is pointing to the root of T (fi). See for example the decision tree of tastes preference in Figure 1. Find an efficient learning algorithm for decision trees over large alphabet.\n6.6. Find an efficient deterministic non-adaptive learning algorithm for DTd.\n6.7. Study the learnability of DTd,s, MDTd,s and DL.\nSection 7\n7.1. The randomized MC query complexity of d-Term is less than the deterministic query complexity. It is interesting to study r-round LV randomized algorithms for this class.\n7.2. Close the gap between the upper bound and the lower bound of OPTNAD(dMJunta).\n7.3. Study the learnability of the subclasses of DNF and CDNF defined in survey.\n7.4. Find OPTA(CDNF) and OPTA(MCDNF) for adaptive and non-adaptive algorithms.\n7.5. Study the learnability of the classes XT, DFA and BMAF.\n7.6. Find OPTNAD(BHS[k]). The current upper bound is n O(k3), and the lower\nbound is n\u2126(k).\n7.7. Find OPTAD(MROF).\n7.8. Study the learnability of the conjunction and disjunction of two MROF.\n7.9. Find a non-adaptive algorithm for r-LF (resp. r-QF) with O(r log n/\u03c9(1)) queries.\n7.10. Find a randomized algorithm for (r,<)-LF (resp. (r,<)-QF) with an optimal number of queries with success probability 1\u2212 1/poly(n).\n60\n7.11. Find a deterministic efficient learning algorithm for multiplicity automata function."}], "references": [{"title": "Queries and concept learning", "author": ["D. Angluin"], "venue": "Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Learning regular sets from queries and counterexamples", "author": ["D. Angluin"], "venue": "Information and Computation", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Queries revisited", "author": ["D. Angluin"], "venue": "ALT", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "New construction of error-tolerant pooling designs", "author": ["R. Ahlswede", "H.K. Aydinian"], "venue": "Information Theory, Combinatorics, and Search Theory. pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning a hidden subgraph", "author": ["N. Alon", "V. Asodi"], "venue": "SIAM J. Discrete Math", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Learning Boolean halfspaces with small weights from membership queries", "author": ["H. Abasi", "A.Z. Abdi", "N.H. Bshouty"], "venue": "ALT", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning Threshold functions with small weights using membership queries", "author": ["E. Abboud", "N. Agha", "N.H. Bshouty", "N. Radwan", "F. Saleh"], "venue": "COLT", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "On exact specification by examples", "author": ["M. Anthony", "G. Brightwell", "D. Cohen", "J. Shawe-Taylor"], "venue": "COLT", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "The complexity of properly learning simple concept classes", "author": ["M. Alekhnovich", "M. Braverman", "V. Feldman", "A.R. Klivans", "T. Pitassi"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Learning a hidden matching", "author": ["N. Alon", "R. Beigel", "S. Kasif", "S. Rudich", "B. Sudakov"], "venue": "SIAM J. Comput", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "On exact learning monotone DNF from membership queries", "author": ["H. Abasi", "N.H. Bshouty", "H. Mazzawi"], "venue": "ALT", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Non-Adaptive learning a hidden hypergraph", "author": ["H. Abasi", "N.H. Bshouty", "H. Mazzawi"], "venue": "ALT", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Learning a hidden hypergraph", "author": ["D. Angluin", "J. Chen"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Learning a hidden graph using O(logn) queries per edge", "author": ["D. Angluin", "J. Chen"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Searching in the presence of linearly bounded errors", "author": ["J.A. Aslam", "A. Dhagat"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "Learning conjunctions of horn clauses", "author": ["D. Angluin", "M. Frazier", "L. Pitt"], "venue": "Machine Learning", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Point probe decision trees for geometric concept classes", "author": ["E.M. Arkin", "M.T. Goodrich", "J.S.B. Mitchell", "D.M. Mount", "C.D. Piatko", "S. Skiena"], "venue": "WADS", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1993}, {"title": "Complexity theoretic hardness results for query learning", "author": ["H. Aizenstein", "T. Heged\u00fcs", "L. Hellerstein", "L. Pitt"], "venue": "Computational Complexity", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Learning read-once formulas with queries", "author": ["D. Angluin", "L. Hellerstein", "M. Karpinski"], "venue": "J. ACM", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "Read-Thrice DNF is hard to learn with membership and equivalence queries", "author": ["H. Aizenstein", "L. Hellerstein", "L. Pitt"], "venue": "FOCS", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1992}, {"title": "Malicious omissions and errors in answers to membership queries", "author": ["D. Angluin", "M. Krikis", "R.H. Sloan", "G. Tur\u00e1n"], "venue": "Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Decision trees for geometric models", "author": ["E.M. Arkin", "H. Meijer", "J.S.B. Mitchell", "D. Rappaport", "S. Skiena"], "venue": "Symposium on Computational Geometry", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Algorithmic construction of sets for krestrictions", "author": ["N. Alon", "D. Moshkovitz", "S. Safra"], "venue": "ACM Transactions on Algorithms", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Derandomizing polynomial identity testing for multilinear constant-read formulae", "author": ["M. Anderson", "D. van Melkebeek", "I. Volkovich"], "venue": "CCC", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Exact learning of read-twice DNF formulas", "author": ["H. Aizenstein", "L. Pitt"], "venue": "FOCS", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Randomly fallible teachers: learning monotone DNF with an incomplete membership oracle", "author": ["D. Angluin", "D.K. Slonim"], "venue": "Machine Learning", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1994}, {"title": "The probabilistic method", "author": ["N. Alon", "J.H. Spencer"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Classifying polynomials and identity testing", "author": ["M. Agrawal", "R. Saptharishi"], "venue": "Current Trends in Science, http://www.cse.iitk.ac.in/users /manindra/survey/Identity.pdf.3,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Quasi-polynomial hitting-set for set-depth-\u2206 formulas", "author": ["M. Agrawal", "C. Saha", "N. Saxena"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2013}, {"title": "Arithmetic circuits: a chasm at depth four", "author": ["M. Agrawal", "V. Vinay"], "venue": "FOCS", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Pseudorandom generators for low degree polynomials", "author": ["A. Bogdanov"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Exact learning Boolean function via the monotone theory", "author": ["N.H. Bshouty"], "venue": "Inf. Comput", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1995}, {"title": "Simple learning algorithms using divide and conquer", "author": ["N.H. Bshouty"], "venue": "Computational Complexity", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1997}, {"title": "A new composition theorem for learning algorithms", "author": ["N.H. Bshouty"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1998}, {"title": "Exact learning of formulas in parallel", "author": ["N.H. Bshouty"], "venue": "Machine Learning", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1997}, {"title": "On the coin weighing problem with the presence of noise", "author": ["N.H. Bshouty"], "venue": "APPROXRANDOM", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2012}, {"title": "Testers and their applications", "author": ["N.H. Bshouty"], "venue": "Electronic Colloquium on Computational Complexity (ECCC) 19:11,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "Multilinear complexity is equivalent to optimal tester size", "author": ["N.H. Bshouty"], "venue": "Electronic Colloquium on Computational Complexity (ECCC)", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2013}, {"title": "Exact learning from membership queries: some techniques, results and new directions", "author": ["N.H. Bshouty"], "venue": "ALT", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2013}, {"title": "Linear time constructions of some d-restriction problems", "author": ["N.H. Bshouty"], "venue": "CIAC. pp", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "An optimal procedure for gap closing in whole genome shotgun sequencing", "author": ["R. Beigel", "N. Alon", "S. Kasif", "M.S. Apaydin", "L. Fortnow"], "venue": "RECOMB", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2001}, {"title": "On interpolating arithmetic read-once formulas with exponentiation", "author": ["D. Bshouty", "N.H. Bshouty"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1998}, {"title": "Learning functions represented as multiplicity automata", "author": ["A. Beimel", "F. Bergadano", "N.H. Bshouty", "E. Kushilevitz", "S. Varricchio"], "venue": "J. ACM", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2000}, {"title": "Learning with errors in answering to membership queries", "author": ["L. Bisht", "N.H. Bshouty", "L. Khoury"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "A comparative survey of nonadaptive pooling designs. Genetic Mapping and DNA Sequencing. IMA volumes in mathematics and its applications", "author": ["D.J. Balding", "W.J. Bruno", "E. Knill", "D.C. Torney"], "venue": null, "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1995}, {"title": "Oracles and queries that are sufficient for exact learning", "author": ["N.H. Bshouty", "R. Cleve", "R. Gavald\u00e0", "S. Kannan", "C. Tamon"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1996}, {"title": "Interpolating arithmetic read-once formulas in parallel", "author": ["N.H. Bshouty", "R. Cleve"], "venue": "SIAM J. Comput", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1998}, {"title": "Exact Learning of Juntas from Membership Queries", "author": ["N.H. Bshouty", "A. Costa"], "venue": "ALT", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2016}, {"title": "Multiple access channels: theory and practice", "author": ["E. Biglieri", "L. Gyorfi"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2007}, {"title": "Almost optimal set covers in finite VC-dimension", "author": ["H. Br\u00f6nnimann", "M.T. Goodrich"], "venue": "Discrete and Computational Geometry", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1995}, {"title": "Almost optimal cover-free family", "author": ["N.H. Bshouty", "A. Gabizon"], "venue": "CoRR abs/1507", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Asking questions to minimize errors", "author": ["N.H. Bshouty", "S.A. Goldman", "T.R. Hancock", "S. Matar"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1996}, {"title": "Combining geometry and combinatorics: A unified approach to sparse signal recovery", "author": ["R. Berinde", "A.C. Gilbert", "P. Indyk", "H.J. Karloff", "M.J. Strauss"], "venue": "CoRR abs/0804.4666", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2008}, {"title": "Combinatorial search on graphs motivated by bioinformatics applications: a brief survey", "author": ["M. Bouvel", "V. Grebinski", "G. Kucherov"], "venue": "WG", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2005}, {"title": "Optimal two-stage algorithms for group testing problems", "author": ["A. De Bonis", "L. Gasieniec", "U. Vaccaro"], "venue": "SIAM J. Comput", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2005}, {"title": "Attribute-efficient learning in query and mistakebound models", "author": ["N.H. Bshouty", "L. Hellerstein"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1998}, {"title": "Learning Boolean read-once formulas over generalized bases", "author": ["N.H. Bshouty", "T.R. Hancock", "L. Hellerstein"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 1995}, {"title": "Learning arithmetic read-once formulas", "author": ["N.H. Bshouty", "T.R. Hancock", "L. Hellerstein"], "venue": "SIAM J. Comput", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 1995}, {"title": "An Algorithm to learn read-once threshold formulas, and transformations between learning models", "author": ["N.H. Bshouty", "T.R. Hancock", "L. Hellerstein", "M. Karpinski"], "venue": "Computational Complexity", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 1994}, {"title": "A new competitive algorithm for group testing", "author": ["A. Bar-Noy", "F.K. Hwang", "H. Kessler", "S. Kutten"], "venue": "Discr. Appl. Math", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1994}, {"title": "Learning in the presence of finitely or infinitely many irrelevant attributes", "author": ["A. Blum", "L. Hellerstein", "N. Littlestone"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 1995}, {"title": "Deterministically testing sparse polynomial identities of unbounded degree", "author": ["M. Bl\u00e4ser", "M. Hardt", "R.J. Lipton", "N.K. Vishnoi"], "venue": "Inf. Process. Lett", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2009}, {"title": "Asymptotically optimal hitting sets against polynomials", "author": ["M. Bl\u00e4ser", "M. Hardt", "D. Steurer"], "venue": null, "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2008}, {"title": "More efficient PAC-learning of DNF with membership queries under the uniform distribution", "author": ["N.H. Bshouty", "J.C. Jackson", "C. Tamon"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Simple learning algorithms for decision trees and multivariate polynomials", "author": ["N.H. Bshouty", "Y. Mansour"], "venue": "SIAM J. Comput", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2002}, {"title": "Optimal query complexity for reconstructing hypergraphs", "author": ["N.H. Bshouty", "H. Mazzawi"], "venue": "STACS", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2010}, {"title": "On parity check (0, 1)-matrix over Zp", "author": ["N.H. Bshouty", "H. Mazzawi"], "venue": "SODA", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2011}, {"title": "Reconstructing weighted graphs with minimal query complexity", "author": ["N.H. Bshouty", "H. Mazzawi"], "venue": "Theor. Comput. Sci", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}, {"title": "Toward a deterministic polynomial time algorithm with optimal additive query complexity", "author": ["N.H. Bshouty", "H. Mazzawi"], "venue": "Theor. Comput. Sci", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2012}, {"title": "Fast identification of geometric objects with membership queries", "author": ["W.J. Bultman", "W. Maass"], "venue": "Inf. Comput", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1995}, {"title": "Fast learning of k-term DNF formulas with queries", "author": ["A. Blum", "S. Rudich"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1995}, {"title": "A deterministic algorithm for sparse multivariate polynomial interpolation", "author": ["M. Ben-Or", "P. Tiwari"], "venue": null, "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1988}, {"title": "Improved algorithms for group testing with inhibitors", "author": ["A. De Bonis", "U. Vaccaro"], "venue": "Inform. Process. Lett", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1998}, {"title": "Determining a set from the cardinalities of its intersections with other sets", "author": ["D.G. Cantor"], "venue": "Canadian Journal of Mathematics", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1964}, {"title": "Tree algorithms for packet broadcast channels", "author": ["J. Capetanakis"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 1979}, {"title": "Generalized TDMA: The multi-accessing tree protocol", "author": ["J. Capetanakis"], "venue": "IEEE Transactions on Communications", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 1979}, {"title": "Derandomization and group testing", "author": ["M. Cheraghchi"], "venue": "CoRR abs/1010.0433", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2010}, {"title": "Noise-resilient group testing: Limitations and constructions", "author": ["M. Cheraghchi"], "venue": "Discrete Applied Mathematics", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2013}, {"title": "An efficient randomized group testing procedure to determine the number of defectives", "author": ["Y. Cheng"], "venue": "Oper. Res. Lett", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2011}, {"title": "Polynomial Time optimal query algorithms for finding graphs with arbitrary real weights", "author": ["S.-S. Choi"], "venue": "COLT", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2013}, {"title": "A greedy heuristic for the set-covering problem", "author": ["V. Chvatal"], "venue": "Mathematics of Operations Research", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 1979}, {"title": "Fault-tolerant search algorithms - Reliable computation with unreliable information", "author": ["F. Cicalese"], "venue": "Monographs in Theoretical Computer Science. An EATCS Series,", "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2013}, {"title": "Reconstruction of hidden graphs and threshold group testing", "author": ["H. Chang", "H.-B. Chen", "H.-L. Fu", "C.-H. Shi"], "venue": "J. Comb. Optim", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2011}, {"title": "New constructions of one and two stage pooling designs", "author": ["Y. Cheng", "D.Z. Du"], "venue": "J. Comput. Biol", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2008}, {"title": "On zero-testing and interpolation of k-sparse multivariate polynomials over finite fields", "author": ["M. Clausen", "A.W.M. Dress", "J. Grabmeier", "M. Karpinski"], "venue": "Theor. Comput. Sci", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 1991}, {"title": "A zig-zag approach for competitive group testing", "author": ["Y. Cheng", "D.-Z. Du", "Y. Xu"], "venue": "INFORMS Journal on Computing", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2014}, {"title": "Learning a hidden graph", "author": ["H. Chang", "H-L. Fu", "C-H. Shih"], "venue": "Optim. Lett.. pp", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2014}, {"title": "Learning read-constant polynomials of constant degree modulo composites", "author": ["A. Chattopadhyay", "R. Gavald\u00e0", "K.A. Hansen", "D. Th\u00e9rien"], "venue": "Theory Comput. Syst", "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2014}, {"title": "Learning nearly monotone k-term DNF", "author": ["J. Castro", "D. Guijarro", "V. Lav\u0301\u0131n"], "venue": "EuroCOLT", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 1997}, {"title": "A survey on non-adaptive group testing algorithms through the angle of decoding", "author": ["H-B. Chen", "F.K. Hwang"], "venue": "J. Comb. Optim", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2008}, {"title": "GROTESQUE: noisy group testing (quick and efficient)", "author": ["S. Cai", "M. Jahangoshahi", "M. Bakshi", "S. Jaggi"], "venue": "CoRR abs/1307.2811", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2013}, {"title": "Optimal query complexity bounds for finding graphs", "author": ["S.S. Choi", "J.H. Kim"], "venue": null, "citeRegEx": "96", "shortCiteRegEx": "96", "year": 2008}, {"title": "Optimal query complexity bounds for finding graphs", "author": ["S.S. Choi", "J.H. Kim"], "venue": "Artificial Intelligence", "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2010}, {"title": "User identification by signature code for noisy multiple-access adder channel", "author": ["J. Cheng", "K. Kamoi", "Y. Watanabe"], "venue": "IEEE International Symposium on Information Theory. pp. 1974\u20131977", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2006}, {"title": "Non-adaptive complex group testing with multiple positive sets", "author": ["F.Y.L. Chin", "H.C.M. Leung", "S.-M. Yiu"], "venue": "Theor. Comput. Sci", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2013}, {"title": "Determination of a subset from certain combinatorial properties", "author": ["D.G. Cantor", "W.H. Mills"], "venue": "Canadian Journal of Mathematics", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 1966}, {"title": "Optimal binary search with two unreliable tests and minimum adaptiveness", "author": ["F. Cicalese", "D. Mundici"], "venue": "ESA", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 1999}, {"title": "Perfect two-fault tolerant search with minimum adaptiveness", "author": ["F. Cicalese", "D. Mundici"], "venue": "Adv. Appl. Math", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2000}, {"title": "Whats hot and whats not: Tracking most frequent items dynamically", "author": ["G. Cormode", "S. Muthukrishnan"], "venue": "Proceedings of the 22nd ACM Symposium on Principles of Database Systems,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2003}, {"title": "Least adaptive optimal search with unreliable tests", "author": ["F. Cicalese", "D. Mundici", "U. Vaccaro"], "venue": "Proc. Scand. Workshop on Algorithm Theory(SWAT", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2000}, {"title": "On the number of irreducible points in polyhedra", "author": ["A. Yu. Chirkov", "N. Yu. Zolotykh"], "venue": null, "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2013}, {"title": "Computational Aspects of parallel attribute-efficient learning", "author": ["P. Damaschke"], "venue": "ALT", "citeRegEx": "107", "shortCiteRegEx": "107", "year": 1998}, {"title": "Adaptive versus non-adaptive attribute-efficient learning", "author": ["P. Damaschke"], "venue": "Machine Learning", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2000}, {"title": "On parallel attribute-efficient learning", "author": ["P. Damaschke"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "109", "shortCiteRegEx": "109", "year": 2003}, {"title": "Exact learning of subclasses of CDNF Formulas with Membership Queries", "author": ["C. Domingo"], "venue": "COCOON", "citeRegEx": "110", "shortCiteRegEx": "110", "year": 1997}, {"title": "The detection of defective members of large populations", "author": ["R. Dorfman"], "venue": "Ann. Math. Statist", "citeRegEx": "111", "shortCiteRegEx": "111", "year": 1943}, {"title": "Optimal two-stage algorithms for group testing problems", "author": ["A. De Bonis", "L. Gasieniec", "U. Vaccaro"], "venue": "SIAM J. Comput", "citeRegEx": "112", "shortCiteRegEx": "112", "year": 2005}, {"title": "Combinatorial group testing and its applications", "author": ["D. Du", "F. K Hwang"], "venue": "World Scientific Pub Co Inc", "citeRegEx": "113", "shortCiteRegEx": "113", "year": 2000}, {"title": "Pooling design and nonadaptive group testing: important tools for DNA sequencing", "author": ["D. Du", "F. K Hwang"], "venue": null, "citeRegEx": "114", "shortCiteRegEx": "114", "year": 2006}, {"title": "Competitive group testing and learning hidden vertex covers with minimum adaptivity", "author": ["P. Damaschke", "A.S. Muhammad"], "venue": "Discrete Mathematics, Algorithms and Applications", "citeRegEx": "115", "shortCiteRegEx": "115", "year": 2010}, {"title": "Bounds for nonadaptive group tests to estimate the amount of defectives", "author": ["P. Damaschke", "A.S. Muhammad"], "venue": "Proc. 4th International Conference on Combinatorial Optimization and Applications. Lecture Notes in Computer Science,", "citeRegEx": "116", "shortCiteRegEx": "116", "year": 2010}, {"title": "Randomized Group Testing Both QueryOptimal and Minimal Adaptive", "author": ["P. Damaschke", "A.S. Muhammad"], "venue": "SOFSEM", "citeRegEx": "117", "shortCiteRegEx": "117", "year": 2012}, {"title": "Efficient read-restricted monotone cnf/dnf dualization by learning with membership queries", "author": ["C. Domingo", "N. Mishra", "L. Pitt"], "venue": "Machine Learning", "citeRegEx": "118", "shortCiteRegEx": "118", "year": 1999}, {"title": "On competitive group testing", "author": ["D.Z. Du", "H. Park"], "venue": "SIAM J. Comput", "citeRegEx": "119", "shortCiteRegEx": "119", "year": 1994}, {"title": "Bounds on the length of disjunctive codes", "author": ["A.G. D\u2019yachkov", "V.V. Rykov"], "venue": "Problemy Peredachi Informatsii", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 1982}, {"title": "Analytical approach to parallel repetition", "author": ["I. Dinur", "D. Steurer"], "venue": null, "citeRegEx": "121", "shortCiteRegEx": "121", "year": 2014}, {"title": "Families of finite sets in which no intersection of ` sets is covered by the union of s others", "author": ["A.G. D\u2019yachkov", "P. Vilenkin", "A. Macula", "D. Torney"], "venue": "J. Comb Theory Ser A", "citeRegEx": "122", "shortCiteRegEx": "122", "year": 2002}, {"title": "Bounds on the rate of disjunctive codes", "author": ["A.G. D\u2019yachkov", "I.V. Vorob\u2019ev", "N.A. Polyansky", "V. Yu. Shchukin"], "venue": "Problems of Information Transmission,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 2014}, {"title": "Erratum to: Bounds on the rate of disjunctive codes", "author": ["A.G. D\u2019yachkov", "I.V. Vorob\u2019ev", "N.A. Polyansky", "V. Yu. Shchukin"], "venue": "Problems of Information Transmission", "citeRegEx": "124", "shortCiteRegEx": "124", "year": 2016}, {"title": "Modifications of competitive group testing", "author": ["D.Z. Du", "G. Xue", "S.Z. Sun", "S.W. Cheng"], "venue": "SIAM J. Comput", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 1994}, {"title": "Improved combinatorial group testing algorithms for real-world problem sizes", "author": ["D. Eppstein", "M.T. Goodrich", "D.S. Hirschberg"], "venue": "SIAM J. Comput", "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2007}, {"title": "On two problems of information theory", "author": ["P. Erd\u00f6s", "A. R\u00e9nyi"], "venue": "Publications of the Mathematical Institute of the Hungarian Academy of Sciences", "citeRegEx": "127", "shortCiteRegEx": "127", "year": 1963}, {"title": "Hitting sets when the VC-dimension is small", "author": ["G. Even", "D. Rawitz", "S. Shahar"], "venue": "Inf. Process. Lett", "citeRegEx": "128", "shortCiteRegEx": "128", "year": 2005}, {"title": "Attribute-efficient and non-adaptive learning of parities and DNF", "author": ["V. Feldman"], "venue": "expressions. JMLR", "citeRegEx": "130", "shortCiteRegEx": "130", "year": 2007}, {"title": "On r-cover free families", "author": ["Z. F\u00fcredi"], "venue": "Journal of Combinatorial Theory A", "citeRegEx": "131", "shortCiteRegEx": "131", "year": 1996}, {"title": "Learning from a consistently ignorant teacher", "author": ["M. Frazier", "S.A. Goldman", "N. Mishra", "L. Pitt"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "132", "shortCiteRegEx": "132", "year": 1996}, {"title": "Group testing problems with sequences in experimental molecular biology", "author": ["M. Farach", "S. Kannan", "E.H. Knill", "S. Muthukrishnan"], "venue": "Proceedings of Compression and Complexity of Sequences,", "citeRegEx": "133", "shortCiteRegEx": "133", "year": 1997}, {"title": "Efficient computation of representative sets with applications in parameterized and exact algorithms", "author": ["F.V. Fomin", "D. Lokshtanov", "S. Saurabh"], "venue": "SODA", "citeRegEx": "134", "shortCiteRegEx": "134", "year": 2014}, {"title": "Optimal binary identification procedures", "author": ["M. Garey"], "venue": "SIAM J. Appl. Math.,", "citeRegEx": "135", "shortCiteRegEx": "135", "year": 1972}, {"title": "On the power of additive combinatorial search model", "author": ["V. Grebinski"], "venue": "In Proceedings of the 4th Annual International Conference on Computing and Combinatorics,", "citeRegEx": "136", "shortCiteRegEx": "136", "year": 1998}, {"title": "Ulam\u2019s searching game with two lies", "author": ["W. Guzicki"], "venue": "J. Comb. Theory, Ser. A", "citeRegEx": "137", "shortCiteRegEx": "137", "year": 1990}, {"title": "Theory testing logic devices (Teoriya testirovaniya logicheskikh ustroystv)", "author": ["E.E. Gasanov", "O.A. Dolotova", "G.R. Pogosyan V.B. Kudryavtsev"], "venue": "(in Russian),", "citeRegEx": "138", "shortCiteRegEx": "138", "year": 2006}, {"title": "Construction of d(H)-disjunct matrix for group testing in hypergraphs", "author": ["H. Gao", "F.K. Hwang", "M.T. Thai", "W. Wu", "T. Znati"], "venue": "J. Comb. Optim", "citeRegEx": "139", "shortCiteRegEx": "139", "year": 2006}, {"title": "Sparse recovery using sparse matrices", "author": ["A. Gilbert", "P. Indyk"], "venue": "Proceedings of the IEEE", "citeRegEx": "140", "shortCiteRegEx": "140", "year": 2010}, {"title": "Algorithms for sparse rational interpolation", "author": ["D. Grigoriev", "M. Karpinski"], "venue": "ISSAC", "citeRegEx": "141", "shortCiteRegEx": "141", "year": 1991}, {"title": "On the complexity of teaching", "author": ["S.A. Goldman", "M.J. Kearns"], "venue": "J. of Comput. Syst. Sci", "citeRegEx": "142", "shortCiteRegEx": "142", "year": 1995}, {"title": "Optimal query bounds for reconstructing a hamiltonian cycle in complete graphs", "author": ["V. Grebinski", "G. Kucherov"], "venue": "ISTCS", "citeRegEx": "143", "shortCiteRegEx": "143", "year": 1997}, {"title": "Reconstructing a hamiltonian cycle by querying the graph: application to dna physical mapping", "author": ["V. Grebinski", "G. Kucherov"], "venue": "Discrete Applied Mathematics", "citeRegEx": "144", "shortCiteRegEx": "144", "year": 1998}, {"title": "Optimal reconstruction of graphs under the additive model. Algorithmica", "author": ["V. Grebinski", "G. Kucherov"], "venue": null, "citeRegEx": "145", "shortCiteRegEx": "145", "year": 2000}, {"title": "Exact identification of read-once formulas using fixed points of amplification functions", "author": ["S.A. Goldman", "M.J. Kearns", "R.E. Schapire"], "venue": "SIAM J. Comput", "citeRegEx": "146", "shortCiteRegEx": "146", "year": 1993}, {"title": "Interpolation of sparse rational functions without knowing bounds on exponents", "author": ["D. Grigoriev", "M. Karpinski", "M.F. Singer"], "venue": null, "citeRegEx": "147", "shortCiteRegEx": "147", "year": 1990}, {"title": "Fast parallel algorithms for sparse multivariate polynomial interpolation over finite fields", "author": ["D. Grigoriev", "M. Karpinski", "M.F. Singer"], "venue": "SIAM J. Comput", "citeRegEx": "148", "shortCiteRegEx": "148", "year": 1990}, {"title": "A Hard-core Predicate for all One-way Functions", "author": ["O. Goldreich", "L.A. Levin"], "venue": null, "citeRegEx": "149", "shortCiteRegEx": "149", "year": 1989}, {"title": "Learning monotone term decision lists", "author": ["D. Guijarro", "V. Lav\u0301\u0131n", "V. Raghavan"], "venue": "EuroCOLT", "citeRegEx": "150", "shortCiteRegEx": "150", "year": 1997}, {"title": "Learning binary relations and total orders (Extended Abstract)", "author": ["S.A. Goldman", "R.L. Rivest", "R.E. Schapire"], "venue": null, "citeRegEx": "151", "shortCiteRegEx": "151", "year": 1989}, {"title": "Learning via queries", "author": ["W.I. Gasarch", "C.H. Smith"], "venue": "J. ACM", "citeRegEx": "152", "shortCiteRegEx": "152", "year": 1992}, {"title": "An algebraic perspective on Boolean function learning", "author": ["R. Gavald\u00e0", "D. Th\u00e9rien"], "venue": "ALT", "citeRegEx": "153", "shortCiteRegEx": "153", "year": 2009}, {"title": "Testing polynomial identities with fewer random bits", "author": ["M. Hardt"], "venue": "Master\u2019s Thesis", "citeRegEx": "155", "shortCiteRegEx": "155", "year": 2007}, {"title": "Combinatorial results on the complexity of teaching and learning", "author": ["T. Heged\u00fcs"], "venue": "MFCS", "citeRegEx": "156", "shortCiteRegEx": "156", "year": 1994}, {"title": "Geometrical Concept Learning and Convex Polytopes", "author": ["T. Heged\u00fcs"], "venue": "COLT", "citeRegEx": "157", "shortCiteRegEx": "157", "year": 1994}, {"title": "Generalized teaching dimensions and the query complexity of learning", "author": ["T. Heged\u00fcs"], "venue": "In Proceedings of the Eighth Annual Conference on Computational Learning Theory (COLT). pp", "citeRegEx": "158", "shortCiteRegEx": "158", "year": 1995}, {"title": "An application of codes to attribute-efficient learning", "author": ["T. Hofmeister"], "venue": "EuroCOLT. pp", "citeRegEx": "159", "shortCiteRegEx": "159", "year": 1999}, {"title": "A method for detecting all defective members in a population by group testing", "author": ["F.K. Hwang"], "venue": "Journal of the American Statistical Association", "citeRegEx": "160", "shortCiteRegEx": "160", "year": 1972}, {"title": "Random k-set pool designs with distinct columns", "author": ["F.K. Hwang"], "venue": "Probability in Engneering and Informational Sciences,", "citeRegEx": "161", "shortCiteRegEx": "161", "year": 2000}, {"title": "The identification of positive clones in a general inhibitor model", "author": ["F.K. Hwang", "F.H. Chang"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "162", "shortCiteRegEx": "162", "year": 2007}, {"title": "On learning disjunctions of zero-one threshold functions with queries", "author": ["T. Heged\u00fcs", "P. Indyk"], "venue": "ALT", "citeRegEx": "163", "shortCiteRegEx": "163", "year": 1997}, {"title": "Learning read-once formulas using membership queries", "author": ["L. Hellerstein", "M. Karpinski"], "venue": "COLT", "citeRegEx": "164", "shortCiteRegEx": "164", "year": 1989}, {"title": "The expected number of unresolved positive clones in various random pool designs", "author": ["F.K. Hwang", "Y.C. Liu"], "venue": "Probability in Engineering and Informational Sciences", "citeRegEx": "165", "shortCiteRegEx": "165", "year": 2001}, {"title": "Group testing for image compression", "author": ["E.H. Hong", "R.E. Ladner"], "venue": "IEEE Trans. Image Process", "citeRegEx": "166", "shortCiteRegEx": "166", "year": 2002}, {"title": "Error-tolerant pooling designs with inhibitors", "author": ["F.K. Hwang", "Y.C. Liu"], "venue": "J. Comput. Biol", "citeRegEx": "167", "shortCiteRegEx": "167", "year": 2003}, {"title": "Constructing optimal binary decision trees is NP-complete", "author": ["L. Hyafil", "R.L. Rivest"], "venue": "Inform. Process. Lett", "citeRegEx": "168", "shortCiteRegEx": "168", "year": 1976}, {"title": "Nets and simplex range queries", "author": ["D. Haussler", "E. Welzl"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "169", "shortCiteRegEx": "169", "year": 1987}, {"title": "Explicit constructions of selectors and related combinatorial structures, with applications", "author": ["P. Indyk"], "venue": "SODA", "citeRegEx": "170", "shortCiteRegEx": "170", "year": 2002}, {"title": "Near-optimal sparse recovery in the L1 norm", "author": ["P. Indyk", "M. Ruzic"], "venue": "FOCS", "citeRegEx": "171", "shortCiteRegEx": "171", "year": 2008}, {"title": "An Efficient membership-query algorithm for learning DNF with respect to the uniform distribution", "author": ["J.C. Jackson"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "172", "shortCiteRegEx": "172", "year": 1997}, {"title": "Learning random monotone DNF", "author": ["J.C. Jackson", "H.K. Lee", "R.A. Servedio", "A. Wan"], "venue": "Discrete Applied Mathematics", "citeRegEx": "173", "shortCiteRegEx": "173", "year": 2011}, {"title": "On learning random dnf formulas under the uniform distribution", "author": ["J.C. Jackson", "R.A. Servedio"], "venue": "Theory of Computing", "citeRegEx": "174", "shortCiteRegEx": "174", "year": 2006}, {"title": "Learning with queries corrupted by classification noise", "author": ["J. Jackson", "E. Shamir", "C. Shwartzman"], "venue": "Discrete Applied Mathematics", "citeRegEx": "175", "shortCiteRegEx": "175", "year": 1999}, {"title": "Reducibility among combinatorial problems", "author": ["R.M. Karp"], "venue": "Complexity and Computer Computations. pp", "citeRegEx": "176", "shortCiteRegEx": "176", "year": 1972}, {"title": "Combinatorial search problems. A survey of combinatorial theory, North Holland", "author": ["G.O.H. Katona"], "venue": null, "citeRegEx": "177", "shortCiteRegEx": "177", "year": 1973}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["M.J. Kearns"], "venue": "J. ACM", "citeRegEx": "178", "shortCiteRegEx": "178", "year": 1998}, {"title": "Finding weighted graphs by combinatorial search", "author": ["J.H. Kim"], "venue": null, "citeRegEx": "179", "shortCiteRegEx": "179", "year": 2012}, {"title": "On monotone functions of algebra of logic. Prob.Cyb", "author": ["V. Korobkov"], "venue": null, "citeRegEx": "180", "shortCiteRegEx": "180", "year": 1965}, {"title": "On teaching and learning intersection-closed concept classes", "author": ["C. Kuhlmann"], "venue": "EuroCOLT", "citeRegEx": "181", "shortCiteRegEx": "181", "year": 1999}, {"title": "A simple algorithm for learning O(logn)-term DNF", "author": ["E. Kushilevitz"], "venue": "Inf. Process. Lett", "citeRegEx": "182", "shortCiteRegEx": "182", "year": 1997}, {"title": "Compressed Sensing. Theory and applications", "author": ["G. Kutyniok"], "venue": "CoRR abs/1203.3815,", "citeRegEx": "183", "shortCiteRegEx": "183", "year": 2012}, {"title": "Derandomizing polynomial identity tests means proving circuit lower bounds", "author": ["V. Kabanets", "R. Impagliazzo"], "venue": null, "citeRegEx": "184", "shortCiteRegEx": "184", "year": 2003}, {"title": "Improved sparse multivariate polynomial interpolation algorithms", "author": ["E. Kaltofen", "Y.N. Lakshman"], "venue": "ISSAC", "citeRegEx": "185", "shortCiteRegEx": "185", "year": 1988}, {"title": "Learning in the presence of malicious errors", "author": ["M.J. Kearns", "M. Li"], "venue": "SIAM J. Comput", "citeRegEx": "186", "shortCiteRegEx": "186", "year": 1993}, {"title": "Learning decision trees using the fourier spectrum", "author": ["E. Kushilevitz", "Y. Mansour"], "venue": "SIAM J. Comput", "citeRegEx": "187", "shortCiteRegEx": "187", "year": 1993}, {"title": "Deterministic identity testing of depth-4 multilinear circuits with bounded top fan-in", "author": ["Z.S. Karnin", "P. Mukhopadhyay", "A. Shpilka", "I. Volkovich"], "venue": null, "citeRegEx": "188", "shortCiteRegEx": "188", "year": 2010}, {"title": "PAC learning intersections of halfspaces with membership", "author": ["S. Kwek", "L. Pitt"], "venue": "queries. Algorithmica", "citeRegEx": "189", "shortCiteRegEx": "189", "year": 1998}, {"title": "Nonrandom binary superimposed codes", "author": ["W.H. Kautz", "R.C. Singleton"], "venue": "IEEE Trans. Inform. Theory", "citeRegEx": "190", "shortCiteRegEx": "190", "year": 1964}, {"title": "Families of k-independent sets", "author": ["D.J. Kleitman", "J. Spencer"], "venue": "Discrete Mathematics", "citeRegEx": "191", "shortCiteRegEx": "191", "year": 1972}, {"title": "On some approximation problems concerning sparse polynomials over finite fields", "author": ["M. Karpinski", "I. Shparlinski"], "venue": "Theor. Comput. Sci", "citeRegEx": "192", "shortCiteRegEx": "192", "year": 1996}, {"title": "Randomness efficient identity testing of multivariate polynomials", "author": ["A. Klivans", "D.A. Spielman"], "venue": null, "citeRegEx": "193", "shortCiteRegEx": "193", "year": 2001}, {"title": "Black box polynomial identity testing of generalized depth-3 arithmetic circuits with bounded top fan-in", "author": ["Z.S. Karnin", "A. Shpilka"], "venue": "CCC", "citeRegEx": "194", "shortCiteRegEx": "194", "year": 2008}, {"title": "Reconstruction of generalized depth-3 arithmetic circuits with bounded top fan-in", "author": ["Z.S. Karnin", "A. Shpilka"], "venue": "IEEE Conference on Computational Complexity", "citeRegEx": "195", "shortCiteRegEx": "195", "year": 2009}, {"title": "Blackbox polynomial identity testing for depth 3 circuits", "author": ["N. Kayal", "S. Saraf"], "venue": "Electronic Colloquium on Computational Complexity (ECCC) 16:", "citeRegEx": "196", "shortCiteRegEx": "196", "year": 2009}, {"title": "A sequential method for screening experimental variables", "author": ["C.H. Li"], "venue": "J. Amer. Statist. Assoc", "citeRegEx": "197", "shortCiteRegEx": "197", "year": 1962}, {"title": "On a combinatory detection problem I", "author": ["B. Lindstr\u00f6m"], "venue": "Publications of the Mathematical Institute of the Hungarian Academy of Sciences,", "citeRegEx": "198", "shortCiteRegEx": "198", "year": 1964}, {"title": "On a combinatorial problem in number theory", "author": ["B. Lindstr\u00f6m"], "venue": "Canadian Mathematical Bulletin", "citeRegEx": "199", "shortCiteRegEx": "199", "year": 1965}, {"title": "Determining subsets by unramified experiments", "author": ["B. Lindstr\u00f6m"], "venue": "A Survey of Statistical Designs and Linear Models,", "citeRegEx": "200", "shortCiteRegEx": "200", "year": 1975}, {"title": "Constant depth circuits, Fourier. transform and learnability", "author": ["N. Linial", "Y. Mansour", "N. Nisan"], "venue": "Journal of the ACM", "citeRegEx": "201", "shortCiteRegEx": "201", "year": 1993}, {"title": "On the hardness of the minimum height decision tree problem", "author": ["E.S. Laber", "L.T. Nogueira"], "venue": "Discrete Applied Mathematics", "citeRegEx": "202", "shortCiteRegEx": "202", "year": 2004}, {"title": "Error-tolerant trivial two-stage group testing for complexes using almost separable and almost disjunct matrices", "author": ["W. Lang", "Y. Wang", "J. Yu", "S. Gao", "W. Wu"], "venue": "Discrete Math., Alg. and Appl", "citeRegEx": "203", "shortCiteRegEx": "203", "year": 2009}, {"title": "Collision-resolution algorithms and random-access communications", "author": ["J.L. Massey"], "venue": "Multi-user communications systems, CISM Courses and Lecture Notes", "citeRegEx": "204", "shortCiteRegEx": "204", "year": 1981}, {"title": "Optimally reconstructing weighted graphs using queries", "author": ["H. Mazzawi"], "venue": "SODA", "citeRegEx": "205", "shortCiteRegEx": "205", "year": 2010}, {"title": "Decision trees and diagrams", "author": ["B. Moret"], "venue": "ACM Computing Surveys", "citeRegEx": "206", "shortCiteRegEx": "206", "year": 1982}, {"title": "The second moment method in combinatorial analysis", "author": ["L. Moser"], "venue": "In Combinatorial Structures and Their Applications. Proceedings of the Calgary International Conference on Combinatorial Structures and Their Applications held at the University of Calgary", "citeRegEx": "207", "shortCiteRegEx": "207", "year": 1969}, {"title": "On conditional tests", "author": ["M.Y. Moshkov"], "venue": "Problemy Kibernetiki,", "citeRegEx": "208", "shortCiteRegEx": "208", "year": 1983}, {"title": "Greedy Algorithm of Decision Tree Construction for Real Data Tables", "author": ["M.J. Moshkov"], "venue": "Transactions on Rough Sets I. pp", "citeRegEx": "209", "shortCiteRegEx": "209", "year": 2004}, {"title": "A logical calculus of the ideas imminent in nervous activity", "author": ["W.S. McCulloch", "E. Pitts"], "venue": "Bulletin of Mathematical Biophysics", "citeRegEx": "210", "shortCiteRegEx": "210", "year": 1943}, {"title": "Trivial two-stage group testing for complexes using almost disjunct matrices", "author": ["A.J. Macula", "V.V. Rykov", "S. Yekhanin"], "venue": "Discrete Applied Mathematics", "citeRegEx": "212", "shortCiteRegEx": "212", "year": 2004}, {"title": "Lower bound methods and separation results for on-line learning models", "author": ["W. Maass", "G. Tur\u00e1n"], "venue": "Machine Learning", "citeRegEx": "213", "shortCiteRegEx": "213", "year": 1992}, {"title": "Complete Derandomization of Identity Testing and Reconstruction of Read-Once Formulas", "author": ["D. Minahan", "I. Volkovich"], "venue": "Electronic Colloquium on Computational Complexity (ECCC)", "citeRegEx": "214", "shortCiteRegEx": "214", "year": 2016}, {"title": "On bounds of cover-free families", "author": ["X. Ma", "R. Wei"], "venue": "Designs, Codes and Cryptography", "citeRegEx": "215", "shortCiteRegEx": "215", "year": 2004}, {"title": "On learning Boolean functions", "author": ["B.K. Natarajan"], "venue": null, "citeRegEx": "216", "shortCiteRegEx": "216", "year": 1987}, {"title": "A survey on combinatorial group testing algorithms with applications to DNA library screening", "author": ["H.Q. Ngo", "D-Z. Du"], "venue": "DIMACS Series in Discrete Mathematics and Theoretical Computer Science", "citeRegEx": "217", "shortCiteRegEx": "217", "year": 2000}, {"title": "Splitters and near-optimal derandomization", "author": ["M. Naor", "L.J. Schulman", "A. Srinivasan"], "venue": "FOCS 95. pp", "citeRegEx": "218", "shortCiteRegEx": "218", "year": 1995}, {"title": "Bounds on constant weight binary superimposed codes", "author": ["N.A. Quang", "T. Zeisel"], "venue": "Problems of Control and Information Theory", "citeRegEx": "219", "shortCiteRegEx": "219", "year": 1988}, {"title": "On learning monotone Boolean functions with irrelevant variables", "author": ["V.V. Osokin"], "venue": "Discrete Math. Appl.,", "citeRegEx": "220", "shortCiteRegEx": "220", "year": 2010}, {"title": "Solution of Ulam\u2019s problem on searching with a lie", "author": ["A. Pelc"], "venue": "J. Comb. Theory, Ser. A", "citeRegEx": "221", "shortCiteRegEx": "221", "year": 1987}, {"title": "Coding with bounded error fraction", "author": ["A. Pelc"], "venue": "Ars Combin. 24. pp", "citeRegEx": "222", "shortCiteRegEx": "222", "year": 1987}, {"title": "Searching with permanently faulty tests", "author": ["A. Pelc"], "venue": "Ars Combin", "citeRegEx": "223", "shortCiteRegEx": "223", "year": 1994}, {"title": "Bounds on the performance of protocols for a multiple-access broadcast channel", "author": ["N. Pippenger"], "venue": "IEEE Trans. on Information Theory", "citeRegEx": "225", "shortCiteRegEx": "225", "year": 1981}, {"title": "Explicit nonadaptive combinatorial group testing schemes", "author": ["E. Porat", "A. Rothschild"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "226", "shortCiteRegEx": "226", "year": 2011}, {"title": "On a problem of information theory", "author": ["A. R\u00e9nyi"], "venue": "MTA Mat. Kut. Int. Kozl", "citeRegEx": "227", "shortCiteRegEx": "227", "year": 1961}, {"title": "Learning decision lists", "author": ["R.L. Rivest"], "venue": "Machine Learning", "citeRegEx": "228", "shortCiteRegEx": "228", "year": 1987}, {"title": "Introduction to coding theory", "author": ["R. Roth"], "venue": null, "citeRegEx": "229", "shortCiteRegEx": "229", "year": 2007}, {"title": "On the upper bound of the size of r-cover-free families", "author": ["M. Ruszink\u00f3"], "venue": "Journal of Combinatorial Theory A", "citeRegEx": "230", "shortCiteRegEx": "230", "year": 1994}, {"title": "Coping with errors in binary search procedures", "author": ["R.L. Rivest", "A.R. Meyer", "D.J. Kleitman", "K. Winklmann", "J. Spencer"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "231", "shortCiteRegEx": "231", "year": 1980}, {"title": "Deterministic polynomial identity testing in non-commutative models", "author": ["R. Raz", "A. Shpilka"], "venue": "Computational Complexity", "citeRegEx": "232", "shortCiteRegEx": "232", "year": 2005}, {"title": "Testing geometric objects", "author": ["K. Romanik", "C. Smith"], "venue": "Comput. Geom. 4. pp", "citeRegEx": "233", "shortCiteRegEx": "233", "year": 1994}, {"title": "A sub-constant error-probability low-degree test, and a sub-constant error-probability PCP characterization of NP", "author": ["R. Raz", "S. Safra"], "venue": "STOC \u201997", "citeRegEx": "234", "shortCiteRegEx": "234", "year": 1997}, {"title": "Learning and verifying graphs using queries with a focus on edge counting", "author": ["L. Reyzin", "N. Srivastava"], "venue": "ALT", "citeRegEx": "235", "shortCiteRegEx": "235", "year": 2007}, {"title": "How an Erd\u00f6s-R\u00e9nyi-type search approach gives an explicit code construction of rate 1 for random access with multiplicity feedback", "author": ["M. Ruszink\u00f3", "P. Vanroose"], "venue": "IEEE Trans. on Information Theory", "citeRegEx": "236", "shortCiteRegEx": "236", "year": 1997}, {"title": "On learning from queries and counterexamples in the presence of noise", "author": ["Y. Sakakibara"], "venue": "Inf. Process. Lett", "citeRegEx": "237", "shortCiteRegEx": "237", "year": 1991}, {"title": "Unified Approaches to Polynomial Identity Testing and lower bounds", "author": ["R. Saptharishi"], "venue": "Ph.D. thesis, Department of CSE, IIT Kanpur, India,", "citeRegEx": "238", "shortCiteRegEx": "238", "year": 2013}, {"title": "Progress on polynomial identity testing", "author": ["N. Saxena"], "venue": "Bulletin of the EATCS", "citeRegEx": "239", "shortCiteRegEx": "239", "year": 2009}, {"title": "Progress on polynomial identity testing - II", "author": ["N. Saxena"], "venue": "CoRR abs/1401.0976", "citeRegEx": "240", "shortCiteRegEx": "240", "year": 2014}, {"title": "Fast probabilistic algorithms for verification of polynomial identities", "author": ["J.T. Schwartz"], "venue": "Journal of the ACM", "citeRegEx": "241", "shortCiteRegEx": "241", "year": 1980}, {"title": "On the limits of efficient teachability", "author": ["R.A. Servedio"], "venue": "Information Processing Letters", "citeRegEx": "242", "shortCiteRegEx": "242", "year": 2001}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "Computer sciences Technical Report 1648,", "citeRegEx": "243", "shortCiteRegEx": "243", "year": 2010}, {"title": "On the definition of a family of automata", "author": ["M.P. Sh\u00fctzenberger"], "venue": "Inf. Control", "citeRegEx": "244", "shortCiteRegEx": "244", "year": 1961}, {"title": "Ulam\u2019s searching game with a fixed number of lies", "author": ["J. Spencer"], "venue": "Theor. Comput. Sci", "citeRegEx": "245", "shortCiteRegEx": "245", "year": 1992}, {"title": "Vector sets for exhaustive testing of logic circuits", "author": ["G. Seroussi", "N.H. Bshouty"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "246", "shortCiteRegEx": "246", "year": 1988}, {"title": "Learning with a helpful teacher", "author": ["S. Salzberg", "A.L. Delcher", "D.G. Heath", "S. Kasif"], "venue": "IJCAI", "citeRegEx": "247", "shortCiteRegEx": "247", "year": 1991}, {"title": "Group testing to eliminate efficiently all defectives in a binomial sample", "author": ["M. Sobel", "P.A. Groll"], "venue": "Bell System Tech. J", "citeRegEx": "248", "shortCiteRegEx": "248", "year": 1959}, {"title": "Teachability in computational learning", "author": ["A. Shinohara", "S. Miyano"], "venue": "New Generation Computing", "citeRegEx": "249", "shortCiteRegEx": "249", "year": 1991}, {"title": "A combinatory detection problem", "author": ["S. S\u00f6derberg", "H.S. Shapiro"], "venue": "American Mathematical Monthly", "citeRegEx": "250", "shortCiteRegEx": "250", "year": 1963}, {"title": "Learning sparse multivariate polynomials over a field with queries and counterexamples", "author": ["R.E. Schapire", "L.M. Sellie"], "venue": "COLT", "citeRegEx": "251", "shortCiteRegEx": "251", "year": 1996}, {"title": "Blackbox identity testing for bounded top fanin depth-3 circuits: the Field doesn\u2019t matter", "author": ["N. Saxena", "C. Seshadhr"], "venue": null, "citeRegEx": "252", "shortCiteRegEx": "252", "year": 2011}, {"title": "Learning Boolean functions with queries", "author": ["R.H. Sloan", "B. Sz\u00f6r\u00e9nyi", "G. Tur\u00e1n"], "venue": "Encyclopedia of Mathematics and its Applications (No", "citeRegEx": "253", "shortCiteRegEx": "253", "year": 2010}, {"title": "Improved results for competitive group testing", "author": ["J. Schlaghoff", "E. Triesch"], "venue": "Combin. Probab. Comput", "citeRegEx": "254", "shortCiteRegEx": "254", "year": 2005}, {"title": "Arithmetic circuits: A survey of recent results and open questions", "author": ["A. Shpilka", "A. Yehudayoff"], "venue": "Foundations and Trends in Theoretical Computer Science", "citeRegEx": "255", "shortCiteRegEx": "255", "year": 2010}, {"title": "Improved polynomial identity testing for read-once", "author": ["A. Shpilka", "I. Volkovich"], "venue": "formulas. APPROX-RANDOM", "citeRegEx": "256", "shortCiteRegEx": "256", "year": 2009}, {"title": "Read-once polynomial identity testing", "author": ["A. Shpilka", "I. Volkovich"], "venue": "Electronic Colloquium on Computational Complexity (ECCC)", "citeRegEx": "257", "shortCiteRegEx": "257", "year": 2010}, {"title": "Black-box identity testing of depth-4 multilinear circuits", "author": ["S. Saraf", "I. Volkovich"], "venue": null, "citeRegEx": "258", "shortCiteRegEx": "258", "year": 2011}, {"title": "Three thresholds for a liar", "author": ["J. Spencer", "P. Winkler"], "venue": "Combin. Probab. Comput", "citeRegEx": "259", "shortCiteRegEx": "259", "year": 1992}, {"title": "Some new bounds for cover-free families", "author": ["D.R. Stinson", "R. Wei", "L. Zhu"], "venue": "Journal of Combinatorial Theory, Series A", "citeRegEx": "260", "shortCiteRegEx": "260", "year": 2000}, {"title": "Lower bounds for the complexity of learning half-spaces with membership queries", "author": ["V.N. Shevchenko", "N. Yu. Zolotykh"], "venue": "ALT 1998", "citeRegEx": "261", "shortCiteRegEx": "261", "year": 1998}, {"title": "Sets pooling designs", "author": ["D.C. Torney"], "venue": "Ann. Comb", "citeRegEx": "262", "shortCiteRegEx": "262", "year": 1999}, {"title": "A group testing problem for hypergraphs of bounded rank", "author": ["E. Triesch"], "venue": "Discrete Applied Mathematics", "citeRegEx": "263", "shortCiteRegEx": "263", "year": 1996}, {"title": "Free synchronous packet access in a broadcast channel with feedback", "author": ["B. Tsybakov", "V. Mikhailov"], "venue": "Problemy Peredachi Informassi", "citeRegEx": "264", "shortCiteRegEx": "264", "year": 1978}, {"title": "Adventures of a mathematician", "author": ["S.M. Ulam"], "venue": null, "citeRegEx": "265", "shortCiteRegEx": "265", "year": 1976}, {"title": "Optimal attribute-efficient learning of disjunction, parity and threshold functions", "author": ["R. Uehara", "K. Tsuchida", "I. Wegener"], "venue": "EuroCOLT", "citeRegEx": "266", "shortCiteRegEx": "266", "year": 1997}, {"title": "Completeness classes in algebra", "author": ["L.G. Valiant"], "venue": null, "citeRegEx": "267", "shortCiteRegEx": "267", "year": 1979}, {"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "Communications of the ACM", "citeRegEx": "268", "shortCiteRegEx": "268", "year": 1984}, {"title": "Learning disjunction of conjunctions", "author": ["L.G. Valiant"], "venue": "IJCAI", "citeRegEx": "269", "shortCiteRegEx": "269", "year": 1985}, {"title": "A Guide to Learning Arithmetic Circuits", "author": ["I. Volkovich"], "venue": "COLT", "citeRegEx": "270", "shortCiteRegEx": "270", "year": 2016}, {"title": "The complexity of sparse polynomial interpolation over finite fields", "author": ["K. Werther"], "venue": "Appl. Algebra Eng. Commun. Comput", "citeRegEx": "272", "shortCiteRegEx": "272", "year": 1994}, {"title": "Born again group testing: Multiaccess communications", "author": ["J. Wolf"], "venue": "IEEE Trans. Inform. Theory", "citeRegEx": "274", "shortCiteRegEx": "274", "year": 1985}, {"title": "Probabilistic algorithms for sparse polynomials", "author": ["R. Zippel"], "venue": "EUROSAM", "citeRegEx": "275", "shortCiteRegEx": "275", "year": 1979}, {"title": "Deciphering threshold functions of kvalued logic", "author": ["N.Yu. Zolotykh", "V.N. Shevchenko"], "venue": "Discrete Analysis and Operations Research. Novosibirsk", "citeRegEx": "276", "shortCiteRegEx": "276", "year": 1995}], "referenceMentions": [{"referenceID": 200, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 89, "endOffset": 94}, {"referenceID": 169, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 117, "endOffset": 122}, {"referenceID": 71, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 138, "endOffset": 142}, {"referenceID": 107, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 172, "endOffset": 177}, {"referenceID": 0, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 218, "endOffset": 221}, {"referenceID": 145, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 233, "endOffset": 238}, {"referenceID": 139, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 252, "endOffset": 257}, {"referenceID": 131, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 276, "endOffset": 281}, {"referenceID": 233, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 299, "endOffset": 304}, {"referenceID": 187, "context": "This problem has different names in different areas: Conditional and unconditional Tests [208], Combinatorial Search [177], Interpolation [75], Combinatorial Group Testing [113], Exact Learning from Membership Queries [2], Inferring [152], Identifying [146], Test Recognition [138], Active Learning [243], Reconstruction [195] and Guessing Game [273].", "startOffset": 321, "endOffset": 326}, {"referenceID": 229, "context": "The decision problems are also called Testing, Functional Verification, Teaching, Hitting Set, and when f is polynomial, it is called Black Box polynomial identity testing (PIT) [239,255].", "startOffset": 178, "endOffset": 187}, {"referenceID": 245, "context": "The decision problems are also called Testing, Functional Verification, Teaching, Hitting Set, and when f is polynomial, it is called Black Box polynomial identity testing (PIT) [239,255].", "startOffset": 178, "endOffset": 187}, {"referenceID": 217, "context": "Learning the class Var is equivalent to playing the R\u00e9nyi-Ulam game, [224,227,265].", "startOffset": 69, "endOffset": 82}, {"referenceID": 255, "context": "Learning the class Var is equivalent to playing the R\u00e9nyi-Ulam game, [224,227,265].", "startOffset": 69, "endOffset": 82}, {"referenceID": 105, "context": "Learning d-MClause is equivalent to group testing, [111,113,114].", "startOffset": 51, "endOffset": 64}, {"referenceID": 107, "context": "Learning d-MClause is equivalent to group testing, [111,113,114].", "startOffset": 51, "endOffset": 64}, {"referenceID": 108, "context": "Learning d-MClause is equivalent to group testing, [111,113,114].", "startOffset": 51, "endOffset": 64}, {"referenceID": 216, "context": "See many other equivalent problems in [226] and reference within.", "startOffset": 38, "endOffset": 43}, {"referenceID": 218, "context": "(d) Decision List (DL),[228]: functions f \u2208DT where every internal node in T (f) is pointing to at least one leaf.", "startOffset": 23, "endOffset": 28}, {"referenceID": 198, "context": "Learning decision tree is equivalent to solving problems in databases, decision table programming, concrete complexity theory, switching theory, pattern recognition, and taxonomy, [206], computer vision, [23].", "startOffset": 180, "endOffset": 185}, {"referenceID": 21, "context": "Learning decision tree is equivalent to solving problems in databases, decision table programming, concrete complexity theory, switching theory, pattern recognition, and taxonomy, [206], computer vision, [23].", "startOffset": 204, "endOffset": 208}, {"referenceID": 9, "context": "Learning Monotone DNF and subclasses of Monotone DNF equivalent to problems in computational biology that arises in whole-genome shotgun sequencing, [11], and DNA phisical mapping, [144].", "startOffset": 149, "endOffset": 153}, {"referenceID": 137, "context": "Learning Monotone DNF and subclasses of Monotone DNF equivalent to problems in computational biology that arises in whole-genome shotgun sequencing, [11], and DNA phisical mapping, [144].", "startOffset": 181, "endOffset": 186}, {"referenceID": 112, "context": "Learning CDNF is equivalent to problems in data-mining, graph theory and reasoning and knowledge representation, [118].", "startOffset": 113, "endOffset": 118}, {"referenceID": 202, "context": "Deterministic Finite Automaton (DFA),[210]: A DFA is a 5-tuple A = (Q,\u03a3, \u03b4, q0, F ) that can be also represented as a directed graph G = (V,E) with labeled edges where V = Q is a finite set of states (the vertices), and q0 \u2208 Q is the start state.", "startOffset": 37, "endOffset": 42}, {"referenceID": 234, "context": "Boolean Multiplicity Automata Function (BMAF),[244]: A Boolean Multiplicity Automata Function is a function of the form:", "startOffset": 46, "endOffset": 51}, {"referenceID": 42, "context": "See [44] for other ways to represent this class.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 15, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 24, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 31, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 32, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 56, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 69, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 87, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 88, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 104, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 112, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 125, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 150, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 241, "context": "See other classes in [1,2,17,26,33,34,58,73,92,93,110,118,132,158,251].", "startOffset": 21, "endOffset": 70}, {"referenceID": 35, "context": "Learning (r, V )-LF is equivalent to coin weighing problem [37] and signature coding problem [50].", "startOffset": 59, "endOffset": 63}, {"referenceID": 48, "context": "Learning (r, V )-LF is equivalent to coin weighing problem [37] and signature coding problem [50].", "startOffset": 93, "endOffset": 97}, {"referenceID": 53, "context": "Learning (r, V )-QF is equivalent to problems in molecular biology [55].", "startOffset": 67, "endOffset": 71}, {"referenceID": 42, "context": "See [44] for other representations of MAF.", "startOffset": 4, "endOffset": 8}, {"referenceID": 41, "context": "See other classes in [43,239,240,255] and references therein.", "startOffset": 21, "endOffset": 37}, {"referenceID": 229, "context": "See other classes in [43,239,240,255] and references therein.", "startOffset": 21, "endOffset": 37}, {"referenceID": 230, "context": "See other classes in [43,239,240,255] and references therein.", "startOffset": 21, "endOffset": 37}, {"referenceID": 245, "context": "See other classes in [43,239,240,255] and references therein.", "startOffset": 21, "endOffset": 37}, {"referenceID": 60, "context": "The definition of \u201cstrongly attribute-efficient\u201d in [62] is equivalent to our definition of \u201cstrongly attribute-optimally learnable in n\u201d.", "startOffset": 52, "endOffset": 56}, {"referenceID": 139, "context": "Such a set is called an equivalent test set or universal identification sequence [146].", "startOffset": 81, "endOffset": 86}, {"referenceID": 135, "context": "When H = C the set Ah is called a teaching set, and TD(C,C) is denoted by TD(C) or OPT NAD(C) and is called the teaching dimension of the concept C, [142,151,249].", "startOffset": 149, "endOffset": 162}, {"referenceID": 144, "context": "When H = C the set Ah is called a teaching set, and TD(C,C) is denoted by TD(C) or OPT NAD(C) and is called the teaching dimension of the concept C, [142,151,249].", "startOffset": 149, "endOffset": 162}, {"referenceID": 239, "context": "When H = C the set Ah is called a teaching set, and TD(C,C) is denoted by TD(C) or OPT NAD(C) and is called the teaching dimension of the concept C, [142,151,249].", "startOffset": 149, "endOffset": 162}, {"referenceID": 135, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 144, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 149, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 173, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 207, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 223, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 232, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 237, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 239, "context": "For studies in teaching dimension see the following and reference therein [142,151,157,181,216,233,242,247,249].", "startOffset": 74, "endOffset": 111}, {"referenceID": 10, "context": "See for example [12]", "startOffset": 16, "endOffset": 20}, {"referenceID": 150, "context": "We now give upper and lower bounds for OPTA(C) using the following combinatorial measure that is defined in [158,208].", "startOffset": 108, "endOffset": 117}, {"referenceID": 200, "context": "We now give upper and lower bounds for OPTA(C) using the following combinatorial measure that is defined in [158,208].", "startOffset": 108, "endOffset": 117}, {"referenceID": 150, "context": "In [158,208], Moshkov proves the following bounds.", "startOffset": 3, "endOffset": 12}, {"referenceID": 200, "context": "In [158,208], Moshkov proves the following bounds.", "startOffset": 3, "endOffset": 12}, {"referenceID": 150, "context": "[158,208] Let C be any class of Boolean functions.", "startOffset": 0, "endOffset": 9}, {"referenceID": 200, "context": "[158,208] Let C be any class of Boolean functions.", "startOffset": 0, "endOffset": 9}, {"referenceID": 150, "context": "In [158,208], Moshkov gives, for any two integers t and `, an example of a class Ct,` where", "startOffset": 3, "endOffset": 12}, {"referenceID": 200, "context": "In [158,208], Moshkov gives, for any two integers t and `, an example of a class Ct,` where", "startOffset": 3, "endOffset": 12}, {"referenceID": 0, "context": "See also the other dimensions, bounds, and techniques in [1,2,4,19,156,213].", "startOffset": 57, "endOffset": 75}, {"referenceID": 2, "context": "See also the other dimensions, bounds, and techniques in [1,2,4,19,156,213].", "startOffset": 57, "endOffset": 75}, {"referenceID": 17, "context": "See also the other dimensions, bounds, and techniques in [1,2,4,19,156,213].", "startOffset": 57, "endOffset": 75}, {"referenceID": 148, "context": "See also the other dimensions, bounds, and techniques in [1,2,4,19,156,213].", "startOffset": 57, "endOffset": 75}, {"referenceID": 204, "context": "See also the other dimensions, bounds, and techniques in [1,2,4,19,156,213].", "startOffset": 57, "endOffset": 75}, {"referenceID": 16, "context": "See also [18,135].", "startOffset": 9, "endOffset": 17}, {"referenceID": 128, "context": "See also [18,135].", "startOffset": 9, "endOffset": 17}, {"referenceID": 160, "context": "Can one do it in poly(|C|, |X|) time? Hyafil and Rivest, [168], show that the problem of finding OPTAD is NP-Complete.", "startOffset": 57, "endOffset": 62}, {"referenceID": 194, "context": "Laber and Nogueira, [202], show that this problem does not admit an o(log |C|)-approximation unless P=NP.", "startOffset": 20, "endOffset": 25}, {"referenceID": 194, "context": "The reduction of Laber and Nogueira, [202], of set cover to this problem with the inapproximability result of Dinur and Steurer [121] for set cover implies that it cannot be approximated to (1\u2212 o(1)) \u00b7 ln |C| in polynomial time unless P=NP.", "startOffset": 37, "endOffset": 42}, {"referenceID": 115, "context": "The reduction of Laber and Nogueira, [202], of set cover to this problem with the inapproximability result of Dinur and Steurer [121] for set cover implies that it cannot be approximated to (1\u2212 o(1)) \u00b7 ln |C| in polynomial time unless P=NP.", "startOffset": 128, "endOffset": 133}, {"referenceID": 7, "context": "However, unfortunately, the problem of finding a minimum size specifying set for h is NP-Hard, [9,142,249].", "startOffset": 95, "endOffset": 106}, {"referenceID": 135, "context": "However, unfortunately, the problem of finding a minimum size specifying set for h is NP-Hard, [9,142,249].", "startOffset": 95, "endOffset": 106}, {"referenceID": 239, "context": "However, unfortunately, the problem of finding a minimum size specifying set for h is NP-Hard, [9,142,249].", "startOffset": 95, "endOffset": 106}, {"referenceID": 21, "context": "In [23], Arkin et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "show in [23] that the query complexity of this algorithm is within a factor of c ln |C| from OPT for some c > 1.", "startOffset": 8, "endOffset": 12}, {"referenceID": 201, "context": "Moshkov in [209] gives the exact ratio of ln |C|.", "startOffset": 11, "endOffset": 16}, {"referenceID": 21, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": ", [47], show that using the NP-oracle all Boolean classes are efficiently learnable in randomized expected polynomial time.", "startOffset": 2, "endOffset": 6}, {"referenceID": 45, "context": "See other results in [47].", "startOffset": 21, "endOffset": 25}, {"referenceID": 135, "context": "[142,176].", "startOffset": 0, "endOffset": 9}, {"referenceID": 168, "context": "[142,176].", "startOffset": 0, "endOffset": 9}, {"referenceID": 80, "context": "It is known that the greedy algorithm that at each stage, chooses the set that contains the largest number of uncovered elements, achieves an approximation ratio of ln |C|, [84].", "startOffset": 173, "endOffset": 177}, {"referenceID": 22, "context": "The above reduction shows that the problem of finding a small hitting set is equivalent to finding a small set cover, and therefore, the minimum hitting set problem cannot be approximated in polynomial time to within of factor of less than (1\u2212 o(1)) ln |C|, [24,121,129,234].", "startOffset": 258, "endOffset": 274}, {"referenceID": 115, "context": "The above reduction shows that the problem of finding a small hitting set is equivalent to finding a small set cover, and therefore, the minimum hitting set problem cannot be approximated in polynomial time to within of factor of less than (1\u2212 o(1)) ln |C|, [24,121,129,234].", "startOffset": 258, "endOffset": 274}, {"referenceID": 224, "context": "The above reduction shows that the problem of finding a small hitting set is equivalent to finding a small set cover, and therefore, the minimum hitting set problem cannot be approximated in polynomial time to within of factor of less than (1\u2212 o(1)) ln |C|, [24,121,129,234].", "startOffset": 258, "endOffset": 274}, {"referenceID": 161, "context": "The result follows from the -net theorem [169].", "startOffset": 41, "endOffset": 46}, {"referenceID": 26, "context": "See also Chapter 13 in [28].", "startOffset": 23, "endOffset": 27}, {"referenceID": 49, "context": "Br\u00f6nnimann and Goodrich, [51], show that there is an algorithm that runs in time poly(|X|, |C|) and finds a hitting set for C of size at most", "startOffset": 25, "endOffset": 29}, {"referenceID": 122, "context": "See also [128].", "startOffset": 9, "endOffset": 14}, {"referenceID": 60, "context": "In [62], Blum et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 60, "context": "[62] Let C be a class that is projection closed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "The results in [57] show that if the class is also embedding closed then P (n, r) = O(r \u00b7Q(r) log n).", "startOffset": 15, "endOffset": 19}, {"referenceID": 55, "context": "When the class is also embedding closed, Bshouty and Hellerstein,[57], show", "startOffset": 65, "endOffset": 69}, {"referenceID": 39, "context": "[41,57] Let C be a projection and embedding closed class.", "startOffset": 0, "endOffset": 7}, {"referenceID": 55, "context": "[41,57] Let C be a projection and embedding closed class.", "startOffset": 0, "endOffset": 7}, {"referenceID": 55, "context": "[57] Let C be a projection and embedding closed class.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "The following result is from [62].", "startOffset": 29, "endOffset": 33}, {"referenceID": 60, "context": "[62] Let C be a class that is projection closed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "[62] Let C be a projection closed class that contains monotone functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "In [13], Abasi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 47, "context": "See also [49] for a reduction for the randomized non-adaptive learning.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 38, "endOffset": 41}, {"referenceID": 15, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 95, "endOffset": 109}, {"referenceID": 32, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 95, "endOffset": 109}, {"referenceID": 70, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 95, "endOffset": 109}, {"referenceID": 174, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 95, "endOffset": 109}, {"referenceID": 24, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 126, "endOffset": 130}, {"referenceID": 31, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 137, "endOffset": 141}, {"referenceID": 31, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 158, "endOffset": 165}, {"referenceID": 42, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 158, "endOffset": 165}, {"referenceID": 241, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 199, "endOffset": 207}, {"referenceID": 42, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 199, "endOffset": 207}, {"referenceID": 42, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 231, "endOffset": 235}, {"referenceID": 56, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 255, "endOffset": 259}, {"referenceID": 33, "context": "Classes such as Monotone DNF [2], DFA [3], Conjunction of Horn Clauses [17], O(log n)-term DNF [33,34,74,182], read-twice DNF [26], CDNF [33], decision trees [33,44], Boolean multivariate polynomial [251,44], multiplicity automata [44], read-once formula [58] and geometric objects [35].", "startOffset": 282, "endOffset": 286}, {"referenceID": 243, "context": "See also references therein and [253].", "startOffset": 32, "endOffset": 37}, {"referenceID": 19, "context": "For example, the hardness result of learning read-thrice DNF, C, in [21] cannot be used as a hardness result for this class in the exact learning model from membership queries.", "startOffset": 68, "endOffset": 72}, {"referenceID": 8, "context": "On the other hand, hardness results for proper learning, [10], do give hardness results for proper learning from membership queries.", "startOffset": 57, "endOffset": 61}, {"referenceID": 258, "context": "In the probably approximately correct learning model (PAC learning model), [268], with membership queries the teacher has a function f : X \u2192 {0, 1} from some class C.", "startOffset": 75, "endOffset": 80}, {"referenceID": 64, "context": "Classes such as decision trees and multivariate polynomials under distributions that support small terms, [68], DNF under the uniform distribution, [130,172], constant depth circuits under the uniform distribution, [201] and intersections of halfspaces, [189].", "startOffset": 106, "endOffset": 110}, {"referenceID": 123, "context": "Classes such as decision trees and multivariate polynomials under distributions that support small terms, [68], DNF under the uniform distribution, [130,172], constant depth circuits under the uniform distribution, [201] and intersections of halfspaces, [189].", "startOffset": 148, "endOffset": 157}, {"referenceID": 164, "context": "Classes such as decision trees and multivariate polynomials under distributions that support small terms, [68], DNF under the uniform distribution, [130,172], constant depth circuits under the uniform distribution, [201] and intersections of halfspaces, [189].", "startOffset": 148, "endOffset": 157}, {"referenceID": 193, "context": "Classes such as decision trees and multivariate polynomials under distributions that support small terms, [68], DNF under the uniform distribution, [130,172], constant depth circuits under the uniform distribution, [201] and intersections of halfspaces, [189].", "startOffset": 215, "endOffset": 220}, {"referenceID": 181, "context": "Classes such as decision trees and multivariate polynomials under distributions that support small terms, [68], DNF under the uniform distribution, [130,172], constant depth circuits under the uniform distribution, [201] and intersections of halfspaces, [189].", "startOffset": 254, "endOffset": 259}, {"referenceID": 0, "context": "If a class is learnable from equivalence and membership queries, then it is PAC-learnable with membership queries according to any distribution, [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 105, "context": "Group testing was originally introduced as a potential approach to economical mass blood testing [111].", "startOffset": 97, "endOffset": 102}, {"referenceID": 238, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 115, "endOffset": 120}, {"referenceID": 182, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 157, "endOffset": 162}, {"referenceID": 189, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 211, "endOffset": 216}, {"referenceID": 182, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 295, "endOffset": 304}, {"referenceID": 262, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 295, "endOffset": 304}, {"referenceID": 158, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 323, "endOffset": 328}, {"referenceID": 98, "context": "However it has been proven to be applicable in a variety of problems, including quality control in product testing [248], searching files in storage systems [190], sequential screening of experimental variables [197], efficient contention resolution algorithms for multiple-access communication [190,274], data compression [166], and computation in the data stream model [104].", "startOffset": 371, "endOffset": 376}, {"referenceID": 107, "context": "See a brief history and other applications in [85,113,114,217] and references therein.", "startOffset": 46, "endOffset": 62}, {"referenceID": 108, "context": "See a brief history and other applications in [85,113,114,217] and references therein.", "startOffset": 46, "endOffset": 62}, {"referenceID": 208, "context": "See a brief history and other applications in [85,113,114,217] and references therein.", "startOffset": 46, "endOffset": 62}, {"referenceID": 114, "context": "This implies all the lower bounds in the table except for the non-adaptive lower bound for the deterministic and Las Vegas algorithm that follows from [120].", "startOffset": 151, "endOffset": 156}, {"referenceID": 216, "context": "Porat and Rothschild, [226], gave the first polynomial time deterministic non-adaptive learning algorithm that asks O(d log n) queries.", "startOffset": 22, "endOffset": 27}, {"referenceID": 117, "context": "See Also [123,124].", "startOffset": 9, "endOffset": 18}, {"referenceID": 118, "context": "See Also [123,124].", "startOffset": 9, "endOffset": 18}, {"referenceID": 54, "context": "This follows from [56,81,170].", "startOffset": 18, "endOffset": 29}, {"referenceID": 77, "context": "This follows from [56,81,170].", "startOffset": 18, "endOffset": 29}, {"referenceID": 162, "context": "This follows from [56,81,170].", "startOffset": 18, "endOffset": 29}, {"referenceID": 182, "context": "It is easy to show that, [190],", "startOffset": 25, "endOffset": 30}, {"referenceID": 26, "context": "Using a probabilistic method, [28], it is easy to show that a d-disjunct t \u00d7 n matrix exists where t = O(d log n).", "startOffset": 30, "endOffset": 34}, {"referenceID": 117, "context": "See also [123,124,219].", "startOffset": 9, "endOffset": 22}, {"referenceID": 118, "context": "See also [123,124,219].", "startOffset": 9, "endOffset": 22}, {"referenceID": 210, "context": "See also [123,124,219].", "startOffset": 9, "endOffset": 22}, {"referenceID": 114, "context": "There is also an almost tight lower bound for t [120,131,230]", "startOffset": 48, "endOffset": 61}, {"referenceID": 124, "context": "There is also an almost tight lower bound for t [120,131,230]", "startOffset": 48, "endOffset": 61}, {"referenceID": 220, "context": "There is also an almost tight lower bound for t [120,131,230]", "startOffset": 48, "endOffset": 61}, {"referenceID": 216, "context": "In [226], Porat and Rothschild gave the first polynomial time algorithm for constructing a d-disjunct matrix of size t \u00d7 n where t = O(d log n).", "startOffset": 3, "endOffset": 8}, {"referenceID": 216, "context": "We first use the Porat and Rothschild [226] algorithm to construct a d-disjunct t\u00d7 n matrix M of size t = O(d log n) in polynomial time.", "startOffset": 38, "endOffset": 43}, {"referenceID": 36, "context": "Bshouty proved in [38,39] that a lower bound of d log n/\u03c9(1) implies that for a power of prime q = O(d) one cannot simulate a black-box multiplication of d elements in the finite field Fqt with O(dt) black-box multiplications in Fq.", "startOffset": 18, "endOffset": 25}, {"referenceID": 37, "context": "Bshouty proved in [38,39] that a lower bound of d log n/\u03c9(1) implies that for a power of prime q = O(d) one cannot simulate a black-box multiplication of d elements in the finite field Fqt with O(dt) black-box multiplications in Fq.", "startOffset": 18, "endOffset": 25}, {"referenceID": 109, "context": "The problem of strongly attribute learnability of MClause, which is equivalent to the problem of group testing when d is not known to the learner, was studied by Damaschke and Muhammad, [115,116].", "startOffset": 186, "endOffset": 195}, {"referenceID": 110, "context": "The problem of strongly attribute learnability of MClause, which is equivalent to the problem of group testing when d is not known to the learner, was studied by Damaschke and Muhammad, [115,116].", "startOffset": 186, "endOffset": 195}, {"referenceID": 59, "context": "See also the algorithms in [61,90,119,125,254,266] and references therein.", "startOffset": 27, "endOffset": 50}, {"referenceID": 85, "context": "See also the algorithms in [61,90,119,125,254,266] and references therein.", "startOffset": 27, "endOffset": 50}, {"referenceID": 113, "context": "See also the algorithms in [61,90,119,125,254,266] and references therein.", "startOffset": 27, "endOffset": 50}, {"referenceID": 119, "context": "See also the algorithms in [61,90,119,125,254,266] and references therein.", "startOffset": 27, "endOffset": 50}, {"referenceID": 244, "context": "See also the algorithms in [61,90,119,125,254,266] and references therein.", "startOffset": 27, "endOffset": 50}, {"referenceID": 256, "context": "See also the algorithms in [61,90,119,125,254,266] and references therein.", "startOffset": 27, "endOffset": 50}, {"referenceID": 152, "context": "[160] The class d-MClause is strongly attribute-optimally learnable with O(d log(n/d)) queries.", "startOffset": 0, "endOffset": 5}, {"referenceID": 111, "context": "For randomized adaptive algorithms see [117] and reference within.", "startOffset": 39, "endOffset": 44}, {"referenceID": 78, "context": "When d is unknown, Cheng [82] shows that there is a randomized adaptive learning algorithm that asks O(d log d) queries and finds d with probability at least 1 \u2212 1/d.", "startOffset": 25, "endOffset": 29}, {"referenceID": 54, "context": "In [56], De Bonis et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 83, "context": "See also [88,126].", "startOffset": 9, "endOffset": 17}, {"referenceID": 120, "context": "See also [88,126].", "startOffset": 9, "endOffset": 17}, {"referenceID": 162, "context": "Indyk shows in [170] how to construct an explicit (2d, d+1, n)-selector of size d\u00b7poly(log n).", "startOffset": 15, "endOffset": 20}, {"referenceID": 77, "context": "Cheraghchi, [81], used recent results in extractors to prove that d-MClause is two-round almost optimally learnable in n.", "startOffset": 12, "endOffset": 16}, {"referenceID": 126, "context": "The group testing with inhibitors (GTI) model was introduced in [133].", "startOffset": 64, "endOffset": 69}, {"referenceID": 72, "context": "This problem is studied in [76,94,112,133,162,167].", "startOffset": 27, "endOffset": 50}, {"referenceID": 89, "context": "This problem is studied in [76,94,112,133,162,167].", "startOffset": 27, "endOffset": 50}, {"referenceID": 106, "context": "This problem is studied in [76,94,112,133,162,167].", "startOffset": 27, "endOffset": 50}, {"referenceID": 126, "context": "This problem is studied in [76,94,112,133,162,167].", "startOffset": 27, "endOffset": 50}, {"referenceID": 154, "context": "This problem is studied in [76,94,112,133,162,167].", "startOffset": 27, "endOffset": 50}, {"referenceID": 159, "context": "This problem is studied in [76,94,112,133,162,167].", "startOffset": 27, "endOffset": 50}, {"referenceID": 81, "context": "See other related problems in [86,217] and references therein.", "startOffset": 30, "endOffset": 38}, {"referenceID": 208, "context": "See other related problems in [86,217] and references therein.", "startOffset": 30, "endOffset": 38}, {"referenceID": 252, "context": "Torney, [262], first introduced the problem and gave some applications in molecular biology.", "startOffset": 8, "endOffset": 13}, {"referenceID": 12, "context": "Learning the class s-term r-MDNF is equivalent to learning a Sperner hidden hypergraph of dimension at most r with at most s edges using edge-detecting queries [14].", "startOffset": 160, "endOffset": 164}, {"referenceID": 4, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 9, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 10, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 12, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 13, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 40, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 53, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 82, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 86, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 89, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 94, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 108, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 109, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 116, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 132, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 137, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 203, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 225, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 252, "context": "See [6,11,12,14,15,42,55,87,91,94,100,114,115,122,139,144,211,212,235,262] for more details on the problem, learnability of subclasses of s-term r-MDNF and other applications.", "startOffset": 4, "endOffset": 74}, {"referenceID": 252, "context": "This problem is also called, \u201csets of positive subsets\u201d [262] \u201ccomplex group testing\u201d [114,211] and \u201cgroup testing in hypergraph\u201d [139].", "startOffset": 56, "endOffset": 61}, {"referenceID": 108, "context": "This problem is also called, \u201csets of positive subsets\u201d [262] \u201ccomplex group testing\u201d [114,211] and \u201cgroup testing in hypergraph\u201d [139].", "startOffset": 86, "endOffset": 95}, {"referenceID": 132, "context": "This problem is also called, \u201csets of positive subsets\u201d [262] \u201ccomplex group testing\u201d [114,211] and \u201cgroup testing in hypergraph\u201d [139].", "startOffset": 130, "endOffset": 135}, {"referenceID": 11, "context": "Therefore, all the results in the literature, except [13], assumes that r, s \u2264 \u221a n, although they do not mention this constraint explicitly.", "startOffset": 53, "endOffset": 57}, {"referenceID": 182, "context": "A (n, (s, r))-cover free family ((n, (s, r))-CFF), [190], is a set A \u2286 {0, 1} such that for every 1 \u2264 i1 < i2 < \u00b7 \u00b7 \u00b7 < id \u2264 n where d = s+ r and every J \u2286 [d] of size |J | = s there is a \u2208 A such that aik = 0 for all k \u2208 J and aij = 1 for all j 6\u2208 J .", "startOffset": 51, "endOffset": 56}, {"referenceID": 117, "context": "The lower bound in [123,215,260] is", "startOffset": 19, "endOffset": 32}, {"referenceID": 206, "context": "The lower bound in [123,215,260] is", "startOffset": 19, "endOffset": 32}, {"referenceID": 250, "context": "The lower bound in [123,215,260] is", "startOffset": 19, "endOffset": 32}, {"referenceID": 10, "context": "It is known, [12], that a set of", "startOffset": 13, "endOffset": 17}, {"referenceID": 36, "context": "It follows from [38,41,52,134] that there is a polynomial time (in the size of the CFF) deterministic construction of (n, (s, r))-CFF of size", "startOffset": 16, "endOffset": 30}, {"referenceID": 39, "context": "It follows from [38,41,52,134] that there is a polynomial time (in the size of the CFF) deterministic construction of (n, (s, r))-CFF of size", "startOffset": 16, "endOffset": 30}, {"referenceID": 50, "context": "It follows from [38,41,52,134] that there is a polynomial time (in the size of the CFF) deterministic construction of (n, (s, r))-CFF of size", "startOffset": 16, "endOffset": 30}, {"referenceID": 127, "context": "It follows from [38,41,52,134] that there is a polynomial time (in the size of the CFF) deterministic construction of (n, (s, r))-CFF of size", "startOffset": 16, "endOffset": 30}, {"referenceID": 39, "context": "When r = o(s), the construction can be done in linear time [41,52].", "startOffset": 59, "endOffset": 66}, {"referenceID": 50, "context": "When r = o(s), the construction can be done in linear time [41,52].", "startOffset": 59, "endOffset": 66}, {"referenceID": 11, "context": "[13,114] Let n \u2265 r + s.", "startOffset": 0, "endOffset": 8}, {"referenceID": 108, "context": "[13,114] Let n \u2265 r + s.", "startOffset": 0, "endOffset": 8}, {"referenceID": 132, "context": ", [139].", "startOffset": 2, "endOffset": 7}, {"referenceID": 11, "context": "[13], gave an almost optimal learning algorithm for all n, r and s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Adaptive algorithms for learning s-term r-MDNF is studied in [14,15] and [12].", "startOffset": 61, "endOffset": 68}, {"referenceID": 13, "context": "Adaptive algorithms for learning s-term r-MDNF is studied in [14,15] and [12].", "startOffset": 61, "endOffset": 68}, {"referenceID": 10, "context": "Adaptive algorithms for learning s-term r-MDNF is studied in [14,15] and [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Angluin and Chen gave in [15] the lower bound \u03a9((2s/r) + rs log n) when s > r and Abasi et al.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "gave in [12] the lower bound \u03a9((r/s)s\u22121 + rs log n) when s \u2264 r.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "In [12] Abasi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "r = 2 s log n [15] Det.", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "r > s rs log n+ ( r s )s\u22121 [12] Det.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "[12] Rand.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "r \u2264 s rs log n+ ( 2s r )r/2 [12,15] Det.", "startOffset": 28, "endOffset": 35}, {"referenceID": 13, "context": "r \u2264 s rs log n+ ( 2s r )r/2 [12,15] Det.", "startOffset": 28, "endOffset": 35}, {"referenceID": 10, "context": "rs log n+ s [12,15] Rand.", "startOffset": 12, "endOffset": 19}, {"referenceID": 13, "context": "rs log n+ s [12,15] Rand.", "startOffset": 12, "endOffset": 19}, {"referenceID": 12, "context": "Learning subclasses of graphs and hypergraphs from edge-detecting queries received considerable attention in the literature due to its diverse applications [14,15,42,87].", "startOffset": 156, "endOffset": 169}, {"referenceID": 13, "context": "Learning subclasses of graphs and hypergraphs from edge-detecting queries received considerable attention in the literature due to its diverse applications [14,15,42,87].", "startOffset": 156, "endOffset": 169}, {"referenceID": 40, "context": "Learning subclasses of graphs and hypergraphs from edge-detecting queries received considerable attention in the literature due to its diverse applications [14,15,42,87].", "startOffset": 156, "endOffset": 169}, {"referenceID": 82, "context": "Learning subclasses of graphs and hypergraphs from edge-detecting queries received considerable attention in the literature due to its diverse applications [14,15,42,87].", "startOffset": 156, "endOffset": 169}, {"referenceID": 13, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 82, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 69, "endOffset": 81}, {"referenceID": 136, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 69, "endOffset": 81}, {"referenceID": 137, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 69, "endOffset": 81}, {"referenceID": 9, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 93, "endOffset": 103}, {"referenceID": 40, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 93, "endOffset": 103}, {"referenceID": 82, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 93, "endOffset": 103}, {"referenceID": 4, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 111, "endOffset": 117}, {"referenceID": 82, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 111, "endOffset": 117}, {"referenceID": 4, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 127, "endOffset": 133}, {"referenceID": 82, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 127, "endOffset": 133}, {"referenceID": 9, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 188, "endOffset": 192}, {"referenceID": 13, "context": "Subclasses include Graphs of bounded degree [15], Hamiltonian cycles [87,143,144], matchings [11,42,87], stars [6,87], cliques [6,87], families of graphs that are closed under isomorphism [11] r-uniform hypergraph and almost uniform hypergraph [15].", "startOffset": 244, "endOffset": 248}, {"referenceID": 9, "context": "The class of Read-Once 2-MDNF is equivalent to learning matchings [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 183, "context": "It is known that any (n, d)-universal set is of size \u03a9(2 log n) [191,246].", "startOffset": 64, "endOffset": 73}, {"referenceID": 236, "context": "It is known that any (n, d)-universal set is of size \u03a9(2 log n) [191,246].", "startOffset": 64, "endOffset": 73}, {"referenceID": 209, "context": "The best known polynomial time, poly(n, 2), construction gives an (n, d)-universal set of size O(2 2 d log n) [218].", "startOffset": 110, "endOffset": 115}, {"referenceID": 209, "context": "For d \u2264 log n/ log log n, an (n, d)-universal set of size d2 log n can be constructed in polynomial time [218].", "startOffset": 105, "endOffset": 110}, {"referenceID": 36, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 37, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 63, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 64, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 123, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 164, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 179, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 241, "context": "The adaptive learnability of decision tree of depth d follows from many papers [38,39,65,68,130,172,187,251].", "startOffset": 79, "endOffset": 108}, {"referenceID": 179, "context": "In [187], Kushilevitz and Mansour used this technique for learning the class of decision trees as follows.", "startOffset": 3, "endOffset": 8}, {"referenceID": 0, "context": "In fact, using Parseval\u2019s identity, one can prove that \u2211 a\u2208{0,1}n f\u0302(a) 2 = 1 and, therefore, the Fourier coefficients have values from {\u00b1k/2 | k \u2208 [2]}\u222a{0}.", "startOffset": 148, "endOffset": 151}, {"referenceID": 179, "context": "Kushilevitz and Mansour in [187] and Goldreich and Levin in [149] gave an adaptive algorithm that finds the non-zero coefficients in poly(2, n) time and queries.", "startOffset": 27, "endOffset": 32}, {"referenceID": 142, "context": "Kushilevitz and Mansour in [187] and Goldreich and Levin in [149] gave an adaptive algorithm that finds the non-zero coefficients in poly(2, n) time and queries.", "startOffset": 60, "endOffset": 65}, {"referenceID": 179, "context": "[187] There is an adaptive Monte Carlo learning algorithm that learns DTd in poly(2 , n) time and O(2 \u00b7 n) membership queries.", "startOffset": 0, "endOffset": 5}, {"referenceID": 179, "context": "[187] There is an adaptive deterministic learning algorithm that learns DTd in poly(2 , n) time and O(2 \u00b7 n log n) membership queries.", "startOffset": 0, "endOffset": 5}, {"referenceID": 209, "context": "The best known polynomial time, poly(n, 2), construction gives an (n, d)-universal set of size O(2 log n) [218].", "startOffset": 106, "endOffset": 111}, {"referenceID": 64, "context": "See other randomized algorithms in [68,251] that use different techniques.", "startOffset": 35, "endOffset": 43}, {"referenceID": 241, "context": "See other randomized algorithms in [68,251] that use different techniques.", "startOffset": 35, "endOffset": 43}, {"referenceID": 241, "context": "The algorithm in [251] uses membership and equivalence queries, and it is easy to see that every equivalence query can be simulated by randomized membership queries.", "startOffset": 17, "endOffset": 22}, {"referenceID": 63, "context": "In this subsection, we give a sketch of the results in [65,149] and then of [130] that gave the first polynomial time Monte Carlo non-adaptive learning algorithm for DTd.", "startOffset": 55, "endOffset": 63}, {"referenceID": 142, "context": "In this subsection, we give a sketch of the results in [65,149] and then of [130] that gave the first polynomial time Monte Carlo non-adaptive learning algorithm for DTd.", "startOffset": 55, "endOffset": 63}, {"referenceID": 123, "context": "In this subsection, we give a sketch of the results in [65,149] and then of [130] that gave the first polynomial time Monte Carlo non-adaptive learning algorithm for DTd.", "startOffset": 76, "endOffset": 81}, {"referenceID": 151, "context": "The following is the result of Hofmeister in [159]", "startOffset": 45, "endOffset": 50}, {"referenceID": 151, "context": "[159] There is a polynomial time deterministic non-adaptive algorithm for C = {\u03c7a(x)|wt(a) \u2264 d} that asks O(d log n) membership queries.", "startOffset": 0, "endOffset": 5}, {"referenceID": 123, "context": "[130].", "startOffset": 0, "endOffset": 5}, {"referenceID": 47, "context": "A better query complexity can be obtained from the reduction in [49].", "startOffset": 64, "endOffset": 68}, {"referenceID": 123, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 40, "endOffset": 45}, {"referenceID": 123, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 75, "endOffset": 83}, {"referenceID": 47, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 75, "endOffset": 83}, {"referenceID": 179, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 115, "endOffset": 123}, {"referenceID": 60, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 115, "endOffset": 123}, {"referenceID": 123, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 157, "endOffset": 162}, {"referenceID": 123, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 196, "endOffset": 204}, {"referenceID": 47, "context": "+Reduction Adaptive Randomized d2 log n [130] Adaptive Randomized d2 log n [130,49] Adaptive Deterministic 2 log n [187,62] Non-Adaptive Randomized d2 log n [130] Non-Adaptive Randomized d2 log n [130,49] Non-Adaptive Deterministic poly(2, log n) OPEN", "startOffset": 196, "endOffset": 204}, {"referenceID": 22, "context": "A d-restriction problem [24,38,218] is a problem of the following form: Given \u03a3 = {0, 1}, a length n and a set B \u2286 \u03a3 of assignments.", "startOffset": 24, "endOffset": 35}, {"referenceID": 36, "context": "A d-restriction problem [24,38,218] is a problem of the following form: Given \u03a3 = {0, 1}, a length n and a set B \u2286 \u03a3 of assignments.", "startOffset": 24, "endOffset": 35}, {"referenceID": 209, "context": "A d-restriction problem [24,38,218] is a problem of the following form: Given \u03a3 = {0, 1}, a length n and a set B \u2286 \u03a3 of assignments.", "startOffset": 24, "endOffset": 35}, {"referenceID": 183, "context": "The lower bound for the size |A| of (n, d)-universal set is [191,246]", "startOffset": 60, "endOffset": 69}, {"referenceID": 236, "context": "The lower bound for the size |A| of (n, d)-universal set is [191,246]", "startOffset": 60, "endOffset": 69}, {"referenceID": 209, "context": "[218].", "startOffset": 0, "endOffset": 5}, {"referenceID": 209, "context": "For d \u2264 log n/ log log n, a (n, d)-universal set of size d2 log n can be constructed in polynomial time [218].", "startOffset": 104, "endOffset": 109}, {"referenceID": 256, "context": "gives in [266] an adaptive algorithm that learns d-XOR in O(d log n) queries.", "startOffset": 9, "endOffset": 14}, {"referenceID": 151, "context": "Hofmeister gives in [159] a non-adaptive algorithm that learns d-XOR in O(d log n) queries.", "startOffset": 20, "endOffset": 25}, {"referenceID": 101, "context": "d-Junta: The class of d-Juntas is studied by Damaschke in [107,108,109] and Bshouty and Costa in [49].", "startOffset": 58, "endOffset": 71}, {"referenceID": 102, "context": "d-Junta: The class of d-Juntas is studied by Damaschke in [107,108,109] and Bshouty and Costa in [49].", "startOffset": 58, "endOffset": 71}, {"referenceID": 103, "context": "d-Junta: The class of d-Juntas is studied by Damaschke in [107,108,109] and Bshouty and Costa in [49].", "startOffset": 58, "endOffset": 71}, {"referenceID": 47, "context": "d-Junta: The class of d-Juntas is studied by Damaschke in [107,108,109] and Bshouty and Costa in [49].", "startOffset": 97, "endOffset": 101}, {"referenceID": 102, "context": "In [108], Damaschke shows that", "startOffset": 3, "endOffset": 8}, {"referenceID": 101, "context": "He then shows that d-Junta is almost optimally learnable in d and efficiently learnable in n [107,109].", "startOffset": 93, "endOffset": 102}, {"referenceID": 103, "context": "He then shows that d-Junta is almost optimally learnable in d and efficiently learnable in n [107,109].", "startOffset": 93, "endOffset": 102}, {"referenceID": 47, "context": "Bshouty and Costa, [49], close the above gap and showed that", "startOffset": 19, "endOffset": 23}, {"referenceID": 47, "context": "See also other results for randomized algorithms in [49,107], optimal algorithms for small d with a constant number of rounds and bounds for the number of rounds in [49,109].", "startOffset": 52, "endOffset": 60}, {"referenceID": 101, "context": "See also other results for randomized algorithms in [49,107], optimal algorithms for small d with a constant number of rounds and bounds for the number of rounds in [49,109].", "startOffset": 52, "endOffset": 60}, {"referenceID": 47, "context": "See also other results for randomized algorithms in [49,107], optimal algorithms for small d with a constant number of rounds and bounds for the number of rounds in [49,109].", "startOffset": 165, "endOffset": 173}, {"referenceID": 103, "context": "See also other results for randomized algorithms in [49,107], optimal algorithms for small d with a constant number of rounds and bounds for the number of rounds in [49,109].", "startOffset": 165, "endOffset": 173}, {"referenceID": 102, "context": "The following is a simple adaptive learning algorithm [108].", "startOffset": 54, "endOffset": 59}, {"referenceID": 47, "context": "d-MJunta: The results in [49,108,109,220] show that", "startOffset": 25, "endOffset": 41}, {"referenceID": 102, "context": "d-MJunta: The results in [49,108,109,220] show that", "startOffset": 25, "endOffset": 41}, {"referenceID": 103, "context": "d-MJunta: The results in [49,108,109,220] show that", "startOffset": 25, "endOffset": 41}, {"referenceID": 211, "context": "d-MJunta: The results in [49,108,109,220] show that", "startOffset": 25, "endOffset": 41}, {"referenceID": 103, "context": "Using Lemma 14 with the result of Damaschke in [109], we get a non-adaptive learning algorithm for d-MJunta that asks 2 log n queries and runs in time poly(2, n).", "startOffset": 47, "endOffset": 52}, {"referenceID": 172, "context": "The class of n-MJunta is studied in [154,180] where the exact value", "startOffset": 36, "endOffset": 45}, {"referenceID": 31, "context": "Some non-optimal results can be achieved using the algorithm in [33] and the reductions in Subsection 3.", "startOffset": 64, "endOffset": 68}, {"referenceID": 45, "context": "Monotone CDNF: The learnability of monotone CDNF is studied in [47,110,118].", "startOffset": 63, "endOffset": 75}, {"referenceID": 104, "context": "Monotone CDNF: The learnability of monotone CDNF is studied in [47,110,118].", "startOffset": 63, "endOffset": 75}, {"referenceID": 112, "context": "Monotone CDNF: The learnability of monotone CDNF is studied in [47,110,118].", "startOffset": 63, "endOffset": 75}, {"referenceID": 104, "context": "Domingo, [110], show that the class of monotone CDNF is learnable with a polynomial number of queries in time s s) where s is the size of the monotone CDNF.", "startOffset": 9, "endOffset": 14}, {"referenceID": 112, "context": "In [118] Domingo et al.", "startOffset": 3, "endOffset": 8}, {"referenceID": 104, "context": "See also [110] for other subclasses of monotone CDNF that are learnable", "startOffset": 9, "endOffset": 14}, {"referenceID": 45, "context": ", [47], show that the class of MCDNF and O(log n)-CDNF are learnable from membership queries and the NP-oracle.", "startOffset": 2, "endOffset": 6}, {"referenceID": 64, "context": "Boolean Multivariate Polynomial: The efficient randomized learnability of multivariate polynomial follows from [68].", "startOffset": 111, "endOffset": 115}, {"referenceID": 140, "context": "See for example [147].", "startOffset": 16, "endOffset": 21}, {"referenceID": 148, "context": "Boolean Halfspace (BHS): Heged\u00fcs, [156], shows that BHS(0, 1) (with zeroone weights) are adaptively learnable in polynomial time with O(n) queries.", "startOffset": 34, "endOffset": 39}, {"referenceID": 256, "context": "See also [266].", "startOffset": 9, "endOffset": 14}, {"referenceID": 155, "context": "Heged\u00fcs and Indyk, [163], give a non-adaptive polynomial time learning algorithm for BHS(0, 1) that asks O(n) queries.", "startOffset": 19, "endOffset": 24}, {"referenceID": 6, "context": ", [8], show that BHS[k] (Boolean Halfspaces with weights in {0, 1, .", "startOffset": 2, "endOffset": 5}, {"referenceID": 5, "context": "[7] give a non-adaptive algorithm for BHS[k] that asks n ) and a two-round algorithm that asks n queries and runs in time n.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] give a lower bound \u03a9(2/ \u221a n) for BH(\u22121, 0, 1) (Boolean Halfspaces with weights {\u22121, 0,+1}).", "startOffset": 0, "endOffset": 3}, {"referenceID": 256, "context": "study some restricted classes of BHS(0, 1), [266].", "startOffset": 44, "endOffset": 49}, {"referenceID": 251, "context": "Shevchenko and Zolotykh [261] studied halfspace function over the domain {0, 1, .", "startOffset": 24, "endOffset": 29}, {"referenceID": 150, "context": "Heged\u00fcs [158] proves the upper boundO(log k/ log log k).", "startOffset": 8, "endOffset": 13}, {"referenceID": 264, "context": "For fixed n, Shevchenko and Zolotykh [276] gave a polynomial time algorithm (in log k) for this class.", "startOffset": 37, "endOffset": 42}, {"referenceID": 150, "context": "Applying Theorem 3 in [158], the upper bound O(logn\u22122 k) for the teaching dimension of a halfspace, [106], gives the upper bound O(logn\u22121 k/ log log k).", "startOffset": 22, "endOffset": 27}, {"referenceID": 100, "context": "Applying Theorem 3 in [158], the upper bound O(logn\u22122 k) for the teaching dimension of a halfspace, [106], gives the upper bound O(logn\u22121 k/ log log k).", "startOffset": 100, "endOffset": 105}, {"referenceID": 18, "context": "gave a polynomial time algorithm that learns MROF with O(n) queries [20,164].", "startOffset": 68, "endOffset": 76}, {"referenceID": 156, "context": "gave a polynomial time algorithm that learns MROF with O(n) queries [20,164].", "startOffset": 68, "endOffset": 76}, {"referenceID": 34, "context": "Bshouty shows in [36] that MROF cannot be learned efficiently in parallel (poly(log n) time).", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 58, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 139, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 143, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 146, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 155, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 251, "context": "Other Classes: See classes of discrete functions and other classes in [34,60,146,150,153,163,261].", "startOffset": 70, "endOffset": 97}, {"referenceID": 73, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 95, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 121, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 190, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 191, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 199, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 240, "context": "The problem of learning LF is studied in [1,77,101,127,198,199,207,250].", "startOffset": 41, "endOffset": 71}, {"referenceID": 35, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 74, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 75, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 107, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 138, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 192, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 196, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 254, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 256, "context": "The class r-LF is studied in [37,78,79,113,145,200,204,264,266].", "startOffset": 29, "endOffset": 63}, {"referenceID": 35, "context": "In [37], Bshouty shows that it is optimally adaptively learnable.", "startOffset": 3, "endOffset": 7}, {"referenceID": 52, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 65, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 66, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 67, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 79, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 92, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 133, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 163, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 171, "context": "The problem of learning (r,<)-LF is studied in [54,69,70,71,83,97,140,171,179].", "startOffset": 47, "endOffset": 78}, {"referenceID": 66, "context": "Bshouty and Mazzawi, [70], show that", "startOffset": 21, "endOffset": 25}, {"referenceID": 35, "context": "See other subclasses in [37,145,225,236].", "startOffset": 24, "endOffset": 40}, {"referenceID": 138, "context": "See other subclasses in [37,145,225,236].", "startOffset": 24, "endOffset": 40}, {"referenceID": 215, "context": "See other subclasses in [37,145,225,236].", "startOffset": 24, "endOffset": 40}, {"referenceID": 226, "context": "See other subclasses in [37,145,225,236].", "startOffset": 24, "endOffset": 40}, {"referenceID": 219, "context": "Similar problems are studied in other areas such as coding theory [229] compressed sensing [183] Multiple Access Channels [50] (e.", "startOffset": 66, "endOffset": 71}, {"referenceID": 175, "context": "Similar problems are studied in other areas such as coding theory [229] compressed sensing [183] Multiple Access Channels [50] (e.", "startOffset": 91, "endOffset": 96}, {"referenceID": 48, "context": "Similar problems are studied in other areas such as coding theory [229] compressed sensing [183] Multiple Access Channels [50] (e.", "startOffset": 122, "endOffset": 126}, {"referenceID": 93, "context": ", adder channels [98]) and combinatorial group testing [113,114] (e.", "startOffset": 17, "endOffset": 21}, {"referenceID": 107, "context": ", adder channels [98]) and combinatorial group testing [113,114] (e.", "startOffset": 55, "endOffset": 64}, {"referenceID": 108, "context": ", adder channels [98]) and combinatorial group testing [113,114] (e.", "startOffset": 55, "endOffset": 64}, {"referenceID": 35, "context": ", coin weighing problem [37]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 137, "context": "This problem is equivalent to learning a weighted graph from additive queries, [144], where, for an additive query, one chooses a set of vertices and asks the sum of the weights of edges with both ends in the set.", "startOffset": 79, "endOffset": 84}, {"referenceID": 91, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 92, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 129, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 137, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 138, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 197, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 225, "context": "The r-QF was studied in [1,96,97,136,144,145,205,235].", "startOffset": 24, "endOffset": 53}, {"referenceID": 66, "context": "The (r, V )-QF for different V \u2286 < was studied in [70,71,72,83,97].", "startOffset": 50, "endOffset": 66}, {"referenceID": 67, "context": "The (r, V )-QF for different V \u2286 < was studied in [70,71,72,83,97].", "startOffset": 50, "endOffset": 66}, {"referenceID": 68, "context": "The (r, V )-QF for different V \u2286 < was studied in [70,71,72,83,97].", "startOffset": 50, "endOffset": 66}, {"referenceID": 79, "context": "The (r, V )-QF for different V \u2286 < was studied in [70,71,72,83,97].", "startOffset": 50, "endOffset": 66}, {"referenceID": 92, "context": "The (r, V )-QF for different V \u2286 < was studied in [70,71,72,83,97].", "startOffset": 50, "endOffset": 66}, {"referenceID": 66, "context": "Bshouty and Mazzawi, [70], proved that", "startOffset": 21, "endOffset": 25}, {"referenceID": 68, "context": "For the positive real numbers <, Bshouty and Mazzawi gave in [72] a polynomial time", "startOffset": 61, "endOffset": 65}, {"referenceID": 79, "context": "Choi, [83], gave a polynomial time randomized adaptive learning algorithm for (r,<)-QF that asks O(r log n/ log r) queries.", "startOffset": 6, "endOffset": 10}, {"referenceID": 65, "context": "Bshouty and Mazzawi extended some of the above results to multilinear forms of constant degree [69].", "startOffset": 95, "endOffset": 99}, {"referenceID": 71, "context": "Ben-Or and Tiwari [75] gave the first deterministic non-adaptive polynomial time learning algorithm for sparse multivariate polynomial over a large field with an optimal number of queries.", "startOffset": 18, "endOffset": 22}, {"referenceID": 134, "context": "See also [141,147,148,185,193].", "startOffset": 9, "endOffset": 30}, {"referenceID": 140, "context": "See also [141,147,148,185,193].", "startOffset": 9, "endOffset": 30}, {"referenceID": 141, "context": "See also [141,147,148,185,193].", "startOffset": 9, "endOffset": 30}, {"referenceID": 177, "context": "See also [141,147,148,185,193].", "startOffset": 9, "endOffset": 30}, {"referenceID": 185, "context": "See also [141,147,148,185,193].", "startOffset": 9, "endOffset": 30}, {"referenceID": 30, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 62, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 141, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 147, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 184, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 185, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 261, "context": "For identity testing and zero testing of sparse multivariate polynomials see [32,64,148,155,192,193,272] and references therein.", "startOffset": 77, "endOffset": 104}, {"referenceID": 42, "context": "Multiplicity Automata Function: This class was first defined and studied in [44].", "startOffset": 76, "endOffset": 80}, {"referenceID": 42, "context": "It is efficiently learnable from queries with a randomized MC algorithm [44].", "startOffset": 72, "endOffset": 76}, {"referenceID": 257, "context": "Arithmetic Circuit and Arithmetic Formula: In [267] Valiant suggests an algebraic analog of P vs.", "startOffset": 46, "endOffset": 51}, {"referenceID": 257, "context": "Valiant shows in [267] that permanent is complete for VNP, i.", "startOffset": 17, "endOffset": 22}, {"referenceID": 29, "context": "In [31], Agrawal and Vinay show that if there exists a deterministic polynomial time zero testing for arithmetic circuits of degree d and depth 4 then there exists a polynomial family {qn}n\u22651, computable in exponential time, that is not in VP.", "startOffset": 3, "endOffset": 7}, {"referenceID": 176, "context": "Kabanets and Impagliazzo show in [184] that even if the zero testing algorithm gets the arithmetic circuit as an input (white box) if there exists a deterministic polynomial time algorithm for zero testing for VP then either NEXP6\u2208P/poly or VP 6=VNP.", "startOffset": 33, "endOffset": 38}, {"referenceID": 27, "context": "See [29,255] for other negative results.", "startOffset": 4, "endOffset": 12}, {"referenceID": 245, "context": "See [29,255] for other negative results.", "startOffset": 4, "endOffset": 12}, {"referenceID": 231, "context": "On the other hand, the following Schwartz-Zippel lemma, [241,275], gives a very simple MC randomized optimal zero testing algorithm for any arithmetic circuit with a bounded degree", "startOffset": 56, "endOffset": 65}, {"referenceID": 263, "context": "On the other hand, the following Schwartz-Zippel lemma, [241,275], gives a very simple MC randomized optimal zero testing algorithm for any arithmetic circuit with a bounded degree", "startOffset": 56, "endOffset": 65}, {"referenceID": 23, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 27, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 28, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 61, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 84, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 180, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 186, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 188, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 228, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 229, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 230, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 242, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 245, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 246, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 248, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 260, "context": "For the deterministic identity testing of arithmetic circuits of depth 3, restricted depth 4 circuits, circuits that compute sparse polynomials and other restricted circuits see the results in [25,29,30,63,89,99,188,194,196,238,239,240,252,255,256,258,270] and references therein.", "startOffset": 193, "endOffset": 256}, {"referenceID": 30, "context": "See for example [32,64,193].", "startOffset": 16, "endOffset": 27}, {"referenceID": 62, "context": "See for example [32,64,193].", "startOffset": 16, "endOffset": 27}, {"referenceID": 185, "context": "See for example [32,64,193].", "startOffset": 16, "endOffset": 27}, {"referenceID": 57, "context": "In [59] Bshouty et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "In [48] Bshouty and Cleve gave a polynomial time (poly(log)) randomized parallel algorithm for this class.", "startOffset": 3, "endOffset": 7}, {"referenceID": 41, "context": "In [43], Bshouty and Bshouty extended the result of [59] to include the exponentiation operation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 57, "context": "In [43], Bshouty and Bshouty extended the result of [59] to include the exponentiation operation.", "startOffset": 52, "endOffset": 56}, {"referenceID": 247, "context": "Shpilka and Volkovich in [257] gave a deterministic algorithm for learning depth d AROF in time n.", "startOffset": 25, "endOffset": 30}, {"referenceID": 246, "context": "In [256] Shpilka and Volkovich gave a deterministic learning algorithm for AROF that asks n queries.", "startOffset": 3, "endOffset": 8}, {"referenceID": 205, "context": "Recently, Volkovich gave in [214] a polynomial time algorithm for learning any AROF.", "startOffset": 28, "endOffset": 33}, {"referenceID": 27, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 188, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 222, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 229, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 228, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 230, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 246, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 242, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 245, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 247, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 248, "context": "Other Classes: See other results and other classes in [1,29,99,196,232,239,238,240,256,252,255,257,258] and references therein.", "startOffset": 54, "endOffset": 103}, {"referenceID": 25, "context": "One can consider a persistent teacher [27,223] or a non-persistent teacher.", "startOffset": 38, "endOffset": 46}, {"referenceID": 214, "context": "One can consider a persistent teacher [27,223] or a non-persistent teacher.", "startOffset": 38, "endOffset": 46}, {"referenceID": 214, "context": "For persistent teacher (or permanently faulty [223]) if the answer to the query d is y then no matter how many times the learner asks the same query the answer will be y.", "startOffset": 46, "endOffset": 51}, {"referenceID": 25, "context": "Incomplete Model [27]: The incomplete teacher, with a query d, answers f(d) with probability p and answers \u201c?\u201d (I DON\u2019T KNOW) with probability 1\u2212p.", "startOffset": 17, "endOffset": 21}, {"referenceID": 178, "context": "Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p.", "startOffset": 16, "endOffset": 33}, {"referenceID": 217, "context": "Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p.", "startOffset": 16, "endOffset": 33}, {"referenceID": 227, "context": "Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p.", "startOffset": 16, "endOffset": 33}, {"referenceID": 259, "context": "Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p.", "startOffset": 16, "endOffset": 33}, {"referenceID": 217, "context": "Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p.", "startOffset": 61, "endOffset": 66}, {"referenceID": 164, "context": "Malicious Model [186,227,237,269]: (Also called random error [227] and classification noise [172]) The malicious teacher, with a query d, answers f(d) with probability p and gives an arbitrary/random wrong answer with probability 1\u2212 p.", "startOffset": 92, "endOffset": 97}, {"referenceID": 20, "context": "Limited Incomplete Model [22]: The limited incomplete teacher gives answers \u201c?\u201d (I DON\u2019T KNOW) to at most m queries of its choice.", "startOffset": 25, "endOffset": 29}, {"referenceID": 20, "context": "Limited Malicious Model [22,265]: (Also called the constant number of error model [16,231]) The limited malicious teacher gives arbitrary/random wrong answers to at most m queries of its choice.", "startOffset": 24, "endOffset": 32}, {"referenceID": 255, "context": "Limited Malicious Model [22,265]: (Also called the constant number of error model [16,231]) The limited malicious teacher gives arbitrary/random wrong answers to at most m queries of its choice.", "startOffset": 24, "endOffset": 32}, {"referenceID": 14, "context": "Limited Malicious Model [22,265]: (Also called the constant number of error model [16,231]) The limited malicious teacher gives arbitrary/random wrong answers to at most m queries of its choice.", "startOffset": 82, "endOffset": 90}, {"referenceID": 221, "context": "Limited Malicious Model [22,265]: (Also called the constant number of error model [16,231]) The limited malicious teacher gives arbitrary/random wrong answers to at most m queries of its choice.", "startOffset": 82, "endOffset": 90}, {"referenceID": 213, "context": "Prefix-Bounded Error Fraction Model [222]: (Also called linearly bounded model [16]) In the adaptive model, the teacher after t queries can give at most pt wrong answers.", "startOffset": 36, "endOffset": 41}, {"referenceID": 14, "context": "Prefix-Bounded Error Fraction Model [222]: (Also called linearly bounded model [16]) In the adaptive model, the teacher after t queries can give at most pt wrong answers.", "startOffset": 79, "endOffset": 83}, {"referenceID": 213, "context": "Globally Bounded Error Fraction Model [222]: In the adaptive model, if the algorithm asks T queries then the teacher can give at most pT wrong answers.", "startOffset": 38, "endOffset": 43}, {"referenceID": 35, "context": "Incomplete Globally Bounded Error Fraction Model [37]: In the adaptive model, if the algorithm asks T queries then the teacher can give at most pT \u201c?\u201d answers.", "startOffset": 49, "endOffset": 53}, {"referenceID": 221, "context": "E-Sided Error Models: (Also called half-error [224], or one-sided error [231], for Boolean functions) Can be defined for any one of the above models where the wrong or\u201c?\u201d answers only applied when f(d) is in some set E \u2282 R.", "startOffset": 72, "endOffset": 77}, {"referenceID": 255, "context": ", xn} in non-honest teacher model is equivalent to the problem of \u201csearching with lies\u201d [265].", "startOffset": 88, "endOffset": 93}, {"referenceID": 255, "context": "Ulam [265] proposed the following game.", "startOffset": 5, "endOffset": 10}, {"referenceID": 217, "context": "R\u00e9nyi [227] asked a similar question and therefore, the game is called R\u00e9nyi-Ulam game.", "startOffset": 6, "endOffset": 11}, {"referenceID": 130, "context": "This problem is completely solved with an asymptotically optimal number of queries in the limited malicious model [1,137,221,245].", "startOffset": 114, "endOffset": 129}, {"referenceID": 212, "context": "This problem is completely solved with an asymptotically optimal number of queries in the limited malicious model [1,137,221,245].", "startOffset": 114, "endOffset": 129}, {"referenceID": 235, "context": "This problem is completely solved with an asymptotically optimal number of queries in the limited malicious model [1,137,221,245].", "startOffset": 114, "endOffset": 129}, {"referenceID": 96, "context": "Learning this class in two-round is studied in [102,103,105].", "startOffset": 47, "endOffset": 60}, {"referenceID": 97, "context": "Learning this class in two-round is studied in [102,103,105].", "startOffset": 47, "endOffset": 60}, {"referenceID": 99, "context": "Learning this class in two-round is studied in [102,103,105].", "startOffset": 47, "endOffset": 60}, {"referenceID": 14, "context": "The problem is solved with an asymptotically optimal number of queries in the linearly bounded model [16,222,259].", "startOffset": 101, "endOffset": 113}, {"referenceID": 213, "context": "The problem is solved with an asymptotically optimal number of queries in the linearly bounded model [16,222,259].", "startOffset": 101, "endOffset": 113}, {"referenceID": 249, "context": "The problem is solved with an asymptotically optimal number of queries in the linearly bounded model [16,222,259].", "startOffset": 101, "endOffset": 113}, {"referenceID": 3, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 14, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 76, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 77, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 83, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 89, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 90, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 107, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 195, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 221, "context": "For learning d-MClause and s-term r-MDNF with non-honest teacher see [5,16,80,81,88,94,95,113,203,231,271] and references therein.", "startOffset": 69, "endOffset": 106}, {"referenceID": 165, "context": "See, for example, some models in [173,174,263].", "startOffset": 33, "endOffset": 46}, {"referenceID": 166, "context": "See, for example, some models in [173,174,263].", "startOffset": 33, "endOffset": 46}, {"referenceID": 253, "context": "See, for example, some models in [173,174,263].", "startOffset": 33, "endOffset": 46}, {"referenceID": 51, "context": "In [53] some techniques were used in the model of exact learning from membership and equivalence queries to minimize the number of equivalence queries.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 1, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 15, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 24, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 31, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 32, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 33, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 42, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 56, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 70, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 174, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 241, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 243, "context": "See [2,3,17,26,33,34,35,44,58,74,182,251,253].", "startOffset": 4, "endOffset": 45}, {"referenceID": 15, "context": "For example, can Angluin-Frazier-Pitt learning algorithm for conjunctions of horn clauses, [17], be changed to learning from membership queries when the number of terms is bounded by d or/and the size of each clause is bounded by k.", "startOffset": 91, "endOffset": 95}, {"referenceID": 22, "context": "For d \u2264 q we say that H is an (n, q, d)-perfect hash family ((n, q, d)-PHF) [24] if for every subset S \u2286 [n] of size |S| = d there is a hash function h \u2208 H such that h|S is injective (one-to-one) on S, i.", "startOffset": 76, "endOffset": 80}, {"referenceID": 39, "context": "In [41] it is shown that for q \u2265 2d.", "startOffset": 3, "endOffset": 7}, {"referenceID": 44, "context": "Non-adaptive randomized algorithms have been proposed in [46,66,114,127,161,165].", "startOffset": 57, "endOffset": 80}, {"referenceID": 108, "context": "Non-adaptive randomized algorithms have been proposed in [46,66,114,127,161,165].", "startOffset": 57, "endOffset": 80}, {"referenceID": 121, "context": "Non-adaptive randomized algorithms have been proposed in [46,66,114,127,161,165].", "startOffset": 57, "endOffset": 80}, {"referenceID": 153, "context": "Non-adaptive randomized algorithms have been proposed in [46,66,114,127,161,165].", "startOffset": 57, "endOffset": 80}, {"referenceID": 157, "context": "Non-adaptive randomized algorithms have been proposed in [46,66,114,127,161,165].", "startOffset": 57, "endOffset": 80}, {"referenceID": 13, "context": "Angluin and Chen gave in [15] a polynomial time 5-round Las Vegas algorithm for learning s-term 2-MDNF that asks O(s log n+ \u221a s log n) queries.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "The class of Read-Once 2-MDNF is equivalent to learning matchings [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 64, "context": "What are the query complexities of the randomized learning algorithms for DTd in [68,251]?", "startOffset": 81, "endOffset": 89}, {"referenceID": 241, "context": "What are the query complexities of the randomized learning algorithms for DTd in [68,251]?", "startOffset": 81, "endOffset": 89}, {"referenceID": 179, "context": "The deterministic adaptive algorithm of Kushilevitz-Mansour [187] asksO(2 n log n) queries.", "startOffset": 60, "endOffset": 65}], "year": 2017, "abstractText": "Given a teacher that holds a function f : X \u2192 R from some class of functions C. The teacher can receive from the learner an element d in the domain X (a query) and returns the value of the function in d, f(d) \u2208 R. The learner goal is to find f with a minimum number of queries, optimal time complexity, and optimal resources. In this survey, we present some of the results known from the literature, different techniques used, some new problems, and open problems. ar X iv :1 70 6. 03 93 5v 1 [ cs .L G ] 1 3 Ju n 20 17", "creator": "LaTeX with hyperref package"}}}