{"id": "1706.04097", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations", "abstract": "non - negative matrix factorization is a basic step for arranging data into easily associated independent distribution matrices under non - negativity constraints, and in practice is predominantly solved in the alternating minimization framework. curiously, it looks doubted whether such tests can recover the ground - truth feature matrices given the statements for different features are automatically correlated, which is common for results. current paper proposes pure simple and natural alternating gradient sum based form, and shows - with moderately mild initialization it provably recovers the ground - truth in explicit presence with interacting correlations. except theoretically interesting cases, external correlation groups be allocated the same order as the others possible. this analysis repeatedly exhibits absolutely infinitely favorable interactions involving robustness polynomial analyze. we complement our theoretical statements further empirical advice on semi - synthetic techniques, exhibiting identical effect over several popular models essentially recovering the weight - response.", "histories": [["v1", "Tue, 13 Jun 2017 14:39:59 GMT  (1519kb,D)", "http://arxiv.org/abs/1706.04097v1", "Accepted to the International Conference on Machine Learning (ICML), 2017"]], "COMMENTS": "Accepted to the International Conference on Machine Learning (ICML), 2017", "reviews": [], "SUBJECTS": "cs.LG cs.DS cs.NA stat.ML", "authors": ["yuanzhi li", "yingyu liang"], "accepted": true, "id": "1706.04097"}, "pdf": {"name": "1706.04097.pdf", "metadata": {"source": "META", "title": "Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations", "authors": ["Yuanzhi Li", "Yingyu Liang"], "emails": ["<yuanzhil@cs.princeton.edu>,", "<yingyul@cs.princeton.edu>."], "sections": [{"heading": "1. Introduction", "text": "Non-negative matrix factorization (NMF) is an important tool in data analysis and is widely used in image processing, text mining, and hyperspectral imaging (e.g., (Lee & Seung, 1997; Blei et al., 2003; Yang & Leskovec, 2013)). Given a set of observations Y = {y(1), y(2), . . . , y(n)}, the goal of NMF is to find a feature matrix A = {a1, a2, . . . , aD} and a non-negative weight matrix X = {x(1), x(2), . . . , x(n)} such that y(i) \u2248 Ax(i) for any i, or Y \u2248 AX for short. The intuition of NMF is to write each data point as a non-negative combination of the features.\nAuthors listed in alphabetic order. 1Princeton University, Princeton, NJ, USA. Correspondence to: Yuanzhi Li <yuanzhil@cs.princeton.edu>, Yingyu Liang <yingyul@cs.princeton.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nBy doing so, one can avoid cancellation of different features and improve interpretability by thinking of each x(i) as a (unnormalized) probability distribution over the features. It is also observed empirically that the non-negativity constraint on the coefficients can lead to better features and improved downstream performance of the learned features.\nUnlike the counterpart which factorizes Y \u2248 AX without assuming non-negativity of X, NMF is usually much harder to solve, and can even by NP-hard in the worse case (Arora et al., 2012b). This explains why, despite all the practical success, NMF largely remains a mystery in theory. Moreover, many of the theoretical results for NMF were based on very technical tools such has algebraic geometry (e.g., (Arora et al., 2012b)) or tensor decomposition (e.g. (Anandkumar et al., 2012)), which undermine their applicability in practice. Arguably, the most widely used algorithms for NMF use the alternative minimization scheme: In each iteration, the algorithm alternatively keeps A or X as fixed and tries to minimize some distance between Y and AX. Algorithms in this framework, such as multiplicative update (Lee & Seung, 2001) and alternative non-negative least square (Kim & Park, 2008), usually perform well on real world data. However, alternative minimization algorithms are usually notoriously difficult to analyze. This problem is poorly understood, with only a few provable guarantees known (Awasthi & Risteski, 2015; Li et al., 2016). Most importantly, these results are only for the case when the coordinates of the weights are from essentially independent distributions, while in practice they are known to be correlated, for example, in correlated topic models (Blei & Lafferty, 2006). As far as we know, there exists no rigorous analysis of practical algorithms for the case with strong correlations.\nIn this paper, we provide a theoretical analysis of a natural algorithm AND (Alternative Non-negative gradient Descent) that belongs to the practical framework, and show that it probably recovers the ground-truth given a mild initialization. It works under general conditions on the feature matrix and the weights, in particular, allowing strong correlations. It also has multiple favorable features that are unique to its success. We further complement our theoretical analysis by experiments on semi-synthetic data, demon-\nar X\niv :1\n70 6.\n04 09\n7v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n17\nstrating that the algorithm converges faster to the groundtruth than several existing practical algorithms, and providing positive support for some of the unique features of our algorithm. Our contributions are detailed below."}, {"heading": "1.1. Contributions", "text": "In this paper, we assume a generative model of the data points, given the ground-truth feature matrix A\u2217. In each round, we are given y = A\u2217x,1 where x is sampled i.i.d. from some unknown distribution \u00b5 and the goal is to recover the ground-truth feature matrix A\u2217. We give an algorithm named AND that starts from a mild initialization matrix and provably converges to A\u2217 in polynomial time. We also justify the convergence through a sequence of experiments. Our algorithm has the following favorable characteristics."}, {"heading": "1.1.1. SIMPLE GRADIENT DESCENT ALGORITHM", "text": "The algorithm AND runs in stages and keeps a working matrix A(t) in each stage. At the t-th iteration in a stage, after getting one sample y, it performs the following:\n(Decode) z = \u03c6\u03b1 ( (A(0))\u2020y ) ,\n(Update) A(t+1) = A(t) + \u03b7 ( yz> \u2212A(t)zz> ) ,\nwhere \u03b1 is a threshold parameter,\n\u03c6\u03b1(x) = { x if x \u2265 \u03b1, 0 otherwise,\n(A(0))\u2020 is the Moore-Penrose pesudo-inverse of A(0), and \u03b7 is the update step size. The decode step aims at recovering the corresponding weight for the data point, and the update step uses the decoded weight to update the feature matrix. The final working matrix at one stage will be used as the A(0) in the next stage. See Algorithm 1 for the details.\nAt a high level, our update step to the feature matrix can be thought of as a gradient descent version of alternative nonnegative least square (Kim & Park, 2008), which at each iteration alternatively minimizes L(A,Z) = \u2016Y \u2212AZ\u20162F by fixing A or Z. Our algorithm, instead of performing an complete minimization, performs only a stochastic gradient descent step on the feature matrix. To see this, consider one data point y and consider minimizing L(A, z) = \u2016y \u2212 Az\u20162F with z fixed. Then the gradient of A is just \u2212\u2207L(A) = (y\u2212Az)z>, which is exactly the update of our feature matrix in each iteration.\nAs to the decode step, when \u03b1 = 0, our decoding can be regarded as a one-shot approach minimizing \u2016Y \u2212AZ\u20162F\n1We also consider the noisy case; see 1.1.5.\nrestricted to Z \u2265 0. Indeed, if for example projected gradient descent is used to minimize \u2016Y \u2212 AZ\u20162F , then the projection step is exactly applying \u03c6\u03b1 to Z with \u03b1 = 0. A key ingredient of our algorithm is choosing \u03b1 to be larger than zero and then decreasing it, which allows us to outperform the standard algorithms.\nPerhaps worth noting, our decoding only uses A(0). Ideally, we would like to use (A(t))\u2020 as the decoding matrix in each iteration. However, such decoding method requires computing the pseudo-inverse of A(t) at every step, which is extremely slow. Instead, we divide the algorithm into stages and in each stage, we only use the starting matrix in the decoding, thus the pseudo-inverse only needs to be computed once per stage and can be used across all iterations inside. We can show that our algorithm converges in polylogarithmic many stages, thus gives us to a much better running time. These are made clear when we formally present the algorithm in Section 4 and the theorems in Section 5 and 6."}, {"heading": "1.1.2. HANDLING STRONG CORRELATIONS", "text": "The most notable property of AND is that it can provably deal with highly correlated distribution \u00b5 on the weight x, meaning that the coordinates of x can have very strong correlations with each other. This is important since such correlated x naturally shows up in practice. For example, when a document contains the topic \u201cmachine learning\u201d, it is more likely to contain the topic \u201ccomputer science\u201d than \u201cgeography\u201d (Blei & Lafferty, 2006).\nMost of the previous theoretical approaches for analyzing alternating between decoding and encoding, such as (Awasthi & Risteski, 2015; Li et al., 2016; Arora et al., 2015), require the coordinates of x to be pairwiseindependent, or almost pairwise-independent (meaning E\u00b5[xixj ] \u2248 E\u00b5[xi]E\u00b5[xj ]). In this paper, we show that algorithm AND can recover A\u2217 even when the coordinates are highly correlated. As one implication of our result, when the sparsity of x is O(1) and each entry of x is in {0, 1}, AND can recover A\u2217 even if each E\u00b5[xixj ] = \u2126(min{E\u00b5[xi],E\u00b5[xj ]}), matching (up to constant) the highest correlation possible. Moreover, we do not assume any prior knowledge about the distribution \u00b5, and the result also extends to general sparsities as well."}, {"heading": "1.1.3. PSEUDO-INVERSE DECODING", "text": "One of the feature of our algorithm is to use Moore-Penrose pesudo-inverse in decoding. Inverse decoding was also used in (Li et al., 2016; Arora et al., 2015; 2016). However, their algorithms require carefully finding an inverse such that certain norm is minimized, which is not as efficient as the vanilla Moore-Penrose pesudo-inverse. It was also observed in (Arora et al., 2016) that Moore-Penrose\npesudo-inverse works equally well in practice, but the experiment was done only when A = A\u2217. In this paper, we show that Moore-Penrose pesudo-inverse also works well when A 6= A\u2217, both theoretically and empirically.\n1.1.4. THRESHOLDING AT DIFFERENT \u03b1\nThresholding at a value \u03b1 > 0 is a common trick used in many algorithms. However, many of them still only consider a fixed \u03b1 throughout the entire algorithm. Our contribution is a new method of thresholding that first sets \u03b1 to be high, and gradually decreases \u03b1 as the algorithm goes. Our analysis naturally provides the explicit rate at which we decrease \u03b1, and shows that our algorithm, following this scheme, can provably converge to the ground-truth A\u2217 in polynomial time. Moreover, we also provide experimental support for these choices."}, {"heading": "1.1.5. ROBUSTNESS TO NOISE", "text": "We further show that the algorithm is robust to noise. In particular, we consider the model y = A\u2217x + \u03b6, where \u03b6 is the noise. The algorithm can tolerate a general family of noise with bounded moments; we present in the main body the result for a simplified case with Gaussian noise and provide the general result in the appendix. The algorithm can recover the ground-truth matrix up to a small blow-up factor times the noise level in each example, when the groundtruth has a good condition number. This robustness is also supported by our experiments."}, {"heading": "2. Related Work", "text": "Practical algorithms. Non-negative matrix factorization has a rich empirical history, starting with the practical algorithms of (Lee & Seung, 1997; 1999; 2001). It has been widely used in applications and there exist various methods for NMF, e.g., (Kim & Park, 2008; Lee & Seung, 2001; Cichocki et al., 2007; Ding et al., 2013; 2014). However, they do not have provable recovery guarantees.\nTheoretical analysis. For theoretical analysis, (Arora et al., 2012b) provided a fixed-parameter tractable algorithm for NMF using algebraic equations. They also provided matching hardness results: namely they show there is no algorithm running in time (mW )o(D) unless there is a sub-exponential running time algorithm for 3-SAT. (Arora et al., 2012b) also studied NMF under separability assumptions about the features, and (Bhattacharyya et al., 2016) studied NMF under related assumptions. The most related work is (Li et al., 2016), which analyzed an alternating minimization type algorithm. However, the result only holds with strong assumptions about the distribution of the weight x, in particular, with the assumption that the coordinates of x are independent.\nTopic modeling. Topic modeling is a popular generative model for text data (Blei et al., 2003; Blei, 2012). Usually, the model results in NMF type optimization problems with \u2016x\u20161 = 1, and a popular heuristic is variational inference, which can be regarded as alternating minimization in KL-divergence. Recently, there is a line of theoretical work analyzing tensor decomposition (Arora et al., 2012a; 2013; Anandkumar et al., 2013) or combinatorial methods (Awasthi & Risteski, 2015). These either need strong structural assumptions on the word-topic matrix A\u2217, or need to know the distribution of the weight x, which is usually infeasible in applications."}, {"heading": "3. Problem and Definitions", "text": "We use \u2016M\u20162 to denote the 2-norm of a matrix M. \u2016x\u20161 is the 1-norm of a vector x. We use [M]i to denote the ith row and [M]i to denote the i-th column of a matrix M. \u03c3max(M)(\u03c3min(M)) stands for the maximum (minimal) singular value of M, respectively. We consider a generative model for non-negative matrix factorization, where the data y is generated from2\ny = A\u2217x, A\u2217 \u2208 RW\u00d7D\nwhere A\u2217 is the ground-truth feature matrix, and x is a nonnegative random vector drawn from an unknown distribution \u00b5. The goal is to recover the ground-truth A\u2217 from i.i.d. samples of the observation y.\nSince the general non-negative matrix factorization is NPhard (Arora et al., 2012b), some assumptions on the distribution of x need to be made. In this paper, we would like to allow distributions as general as possible, especially those with strong correlations. Therefore, we introduce the following notion called (r, k,m, \u03bb)-general correlation conditions (GCC) for the distribution of x.\nDefinition 1 (General Correlation Conditions, GCC). Let \u2206 := E[xx>] denote the second moment matrix.\n1. \u2016x\u20161 \u2264 r and xi \u2208 [0, 1],\u2200i \u2208 [D].\n2. \u2206i,i \u2264 2kD ,\u2200i \u2208 [D].\n3. \u2206i,j \u2264 mD2 ,\u2200i 6= j \u2208 [D].\n4. \u2206 kD\u03bbI.\nThe first condition regularizes the sparsity of x.3 The second condition regularizes each coordinate of xi so that there is no xi being large too often. The third condition\n2Section 6.2 considers the noisy case. 3Throughout this paper, the sparsity of x refers to the `1 norm, which is much weaker than the `0 norm (the support sparsity). For example, in LDA, the `1 norm of x is always 1.\nregularizes the maximum pairwise correlation between xi and xj . The fourth condition always holds for \u03bb = 0 since E[xx>] is a PSD matrix. Later we will assume this condition holds for some \u03bb > 0 to avoid degenerate cases. Note that we put the weight k/D before \u03bb such that \u03bb defined in this way will be a positive constant in many interesting examples discussed below.\nTo get a sense of what are the ranges of k,m, and \u03bb given sparsity r, we consider the following most commonly studied non-negative random variables.\nProposition 1 (Examples of GCC).\n1. If x is chosen uniformly over s-sparse random vectors with {0, 1} entries, then k = r = s, m = s2 and \u03bb = 1\u2212 1s .\n2. If x is uniformly chosen from Dirichlet distribution with parameter \u03b1i = sD , then r = k = 1 andm = 1 sD\nwith \u03bb = 1\u2212 1s .\nFor these examples, the result in this paper shows that we can recover A\u2217 for aforementioned random variables x as long as s = O(D1/6). In general, there is a wide range of parameters (r, k,m, \u03bb) such that learning A\u2217 is doable with polynomially many samples of y and in polynomial time.\nHowever, just the GCC condition is not enough for recovering A\u2217. We will also need a mild initialization.\nDefinition 2 (`-initialization). The initial matrix A0 satisfies for some ` \u2208 [0, 1),\n1. A0 = A\u2217(\u03a3 + E), for some diagonal matrix \u03a3 and off-diagonal matrix E.\n2. \u2016E\u20162 \u2264 `, \u2016\u03a3\u2212 I\u20162 \u2264 14 .\nThe condition means that the initialization is not too far away from the ground-truth A\u2217. For any i \u2208 [D], the ith column [A0]i = \u03a3i,i[A\u2217]i + \u2211 j 6=i Ej,i[A\n\u2217]j . So the condition means that each feature [A0]i has a large fraction of the ground-truth feature [A\u2217]i and a small fraction of the other features. \u03a3 can be regarded as the magnitude of the component from the ground-truth in the initialization, while E can be regarded as the magnitude of the error terms. In particular, when \u03a3 = I and E = 0, we have A0 = A\n\u2217. The initialization allows \u03a3 to be a constant away from I, and the error term E to be ` (in our theorems ` can be as large as a constant).\nIn practice, such an initialization is typically achieved by setting the columns of A0 to reasonable \u201cpure\u201d data points that contain one major feature and a small fraction of some others (e.g. (lda, 2016; Awasthi & Risteski, 2015)).\nAlgorithm 1 Alternating Non-negative gradient Descent (AND) Input: Threshold values {\u03b10, \u03b11, . . . , \u03b1s}, T , A0\n1: A(0) \u2190 A0 2: for j = 0, 1, . . . , s do 3: for t = 0, 1, . . . , T do 4: On getting sample y(t), do: 5: z(t) \u2190 \u03c6\u03b1j ( (A(0))\u2020y(t)\n) 6: A(t+1) \u2190 A(t) + \u03b7 ( y(t) \u2212A(t)z(t) ) (z(t))> 7: end for 8: A(0) \u2190 A(T+1) 9: end for\nOutput: A\u2190 A(T+1)"}, {"heading": "4. Algorithm", "text": "The algorithm is formally describe in Algorithm 1. It runs in s stages, and in the j-th stage, uses the same threshold \u03b1j and the same matrix A(0) for decoding, where A(0) is either the input initialization matrix or the working matrix obtained at the end of the last stage. Each stage consists of T iterations, and each iteration decodes one data point and uses the decoded result to update the working matrix. It can use a batch of data points instead of one data point, and our analysis still holds.\nBy running in stages, we save most of the cost of computing (A(0))\u2020, as our results show that only polylogarithmic stages are needed. For the simple case where x \u2208 {0, 1}D, the algorithm can use the same threshold value \u03b1 = 1/4 for all stages (see Theorem 1), while for the general case, it needs decreasing threshold values across the stages (see Theorem 4). Our analysis provides the hint for setting the threshold; see the discussion after Theorem 4, and Section 7 for how to set the threshold in practice."}, {"heading": "5. Result for A Simplified Case", "text": "In this section, we consider the following simplified case:\ny = A\u2217x, x \u2208 {0, 1}D. (5.1)\nThat is, the weight coordinates xi\u2019s are binary. Theorem 1 (Main, binary). For the generative model (5.1), there exists ` = \u2126(1) such that for every (r, k,m, \u03bb)GCC x and every > 0, Algorithm AND with T = poly(D, 1 ), \u03b7 =\n1 poly(D, 1 ) , {\u03b1i}si=1 = { 14} s i=1 for s =\npolylog(D, 1 ) and an ` initialization matrix A0, outputs a matrix A such that there exists a diagonal matrix \u03a3 12I with \u2016A \u2212A\u2217\u03a3\u20162 \u2264 using poly(D, 1 ) samples and iterations, as long as\nm = O\n( kD\u03bb4\nr5\n) .\nTherefore, our algorithm recovers the ground-truth A\u2217 up to scaling. The scaling in unavoidable since there is no assumption on A\u2217, so we cannot, for example, distinguish A\u2217 from 2A\u2217. Indeed, if we in addition assume each column of A\u2217 has norm 1 as typical in applications, then we can recover A\u2217 directly. In particular, by normalizing each column of A to have norm 1, we can guarantee that \u2016A\u2212A\u2217\u20162 \u2264 O( ).\nIn many interesting applications (for example, those in Proposition 1), k, r, \u03bb are constants. The theorem implies that the algorithm can recover A\u2217 even when m = O(D). In this case, E\u00b5[xixj ] can be as large as O(1/D), the same order as min{E\u00b5[xi],E\u00b5[xj ]}, which is the highest possible correlation."}, {"heading": "5.1. Intuition", "text": "The intuition comes from assuming that we have the \u201ccorrect decoding\u201d, that is, suppose magically for every y(t), our decoding z(t) = \u03c6\u03b1j (A\n\u2020y(t)) = x(t). Here and in this subsection, A is a shorthand for A(0). The gradient descent is then A(t+1) = A(t) + \u03b7(y(t) \u2212A(t)x(t))(x(t))>. Subtracting A\u2217 on both side, we will get\n(A(t+1) \u2212A\u2217) = (A(t) \u2212A\u2217)(I\u2212 \u03b7x(t)(x(t))>)\nSince x(t)(x(t))> is positive semidefinite, as long as E[x(t)(x(t))>] 0 and \u03b7 is sufficiently small, A(t) will converge to A\u2217 eventually.\nHowever, this simple argument does not work when A 6= A\u2217 and thus we do not have the correct decoding. For example, if we just let the decoding be z\u0303(t) = A\u2020y(t), we will have y(t)\u2212Az\u0303(t) = y(t)\u2212A\u2020Ay(t) = (I\u2212A\u2020A)A\u2217x(t). Thus, using this decoding, the algorithm can never make any progress once A and A\u2217 are in the same subspace.\nThe most important piece of our proof is to show that after thresholding, z(t) = \u03c6\u03b1(A\u2020y(t)) is much closer to x(t) than z\u0303(t). Since A and A\u2217 are in the same subspace, inspired by (Li et al., 2016) we can write A\u2217 as A(\u03a3 + E) for a diagonal matrix \u03a3 and an off-diagonal matrix E, and thus the decoding becomes z(t) = \u03c6\u03b1(\u03a3x(t) + Ex(t)). Let us focus on one coordinate of z(t), that is, z(t)i = \u03c6\u03b1(\u03a3i,ix (t) i + Eix\n(t)), where Ei is the i-th row of Ei. The term \u03a3i,ix (t) i is a nice term since it is just a rescaling of x (t) i , while Eix\n(t) mixes different coordinates of x(t). For simplicity, we just assume for now that x(t)i \u2208 {0, 1} and \u03a3i,i = 1. In our proof, we will show that the threshold will remove a large fraction of Eix(t) when x (t) i = 0, and keep a large fraction of \u03a3i,ix (t) i when x (t) i = 1. Thus, our decoding is much more accurate than without thresholding. To show this, we maintain a crucial property that for our decoding matrix, we always have \u2016Ei\u20162 = O(1). Assuming\nthis, we first consider two extreme cases of Ei.\n1. Ultra dense: all coordinates of Ei are in the order of 1\u221a d . Since the sparsity of x(t) is r, as long as r =\no( \u221a d)\u03b1, Eix(t) will not pass \u03b1 and thus z (t) i will be decoded to zero when x(t)i = 0.\n2. Ultra sparse: Ei only has few coordinate equal to \u2126(1) and the rest are zero. Unless x(t) has those exact coordinates equal to 1 (which happens not so often), then z (t) i will still be zero when x (t) i = 0.\nOf course, the real Ei can be anywhere in between these two extremes, and thus we need more delicate decoding lemmas, as shown in the complete proof.\nFurthermore, more complication arises when each x(t)i is not just in {0, 1} but can take fractional values. To handle this case, we will set our threshold \u03b1 to be large at the beginning and then keep shrinking after each stage. The intuition here is that we first decode the coordinates that we are most confident in, so we do not decode z(t)i to be nonzero when x(t)i = 0. Thus, we will still be able to remove a large fraction of error caused by Eix(t). However, by setting the threshold \u03b1 so high, we may introduce more errors to the nice term \u03a3i,ix (t) i as well, since \u03a3i,ix (t) i might not be larger than \u03b1when x(t)i 6= 0. Our main contribution is to show that there is a nice trade-off between the errors in Ei terms and those in \u03a3i,i terms such that as we gradually decreases \u03b1, the algorithm can converge to the ground-truth."}, {"heading": "5.2. Proof Sketch", "text": "For simplicity, we only focus on one stage and the expected update. The expected update of A(t) is given by\nA(t+1) = A(t) + \u03b7(E[yz>]\u2212A(t)E[zz>]).\nLet us write A(0) = A\u2217(\u03a30 + E0) where \u03a30 is diagonal and E0 is off-diagonal. Then the decoding is given by\nz = \u03c6\u03b1((A (0))\u2020y) = \u03c6\u03b1((\u03a30 + E0) \u22121x).\nLet \u03a3,E be the diagonal part and the off-diagonal part of (\u03a30 + E0) \u22121.\nThe key lemma for decoding says that under suitable conditions, z will be close to \u03a3x in the following sense. Lemma 2 (Decoding, informal). Suppose E is small and \u03a3 \u2248 I. Then with a proper threshold value \u03b1, we have\nE[\u03a3xx>] \u2248 E[zx>], E[\u03a3xz>] \u2248 E[zz>].\nNow, let us write A(t) = A\u2217(\u03a3t + Et). Then applying the above decoding lemma, the expected update of \u03a3t + Et is\n\u03a3t+1+Et+1 = (\u03a3t+Et)(I\u2212\u03a3\u2206\u03a3)+\u03a3\u22121(\u03a3\u2206\u03a3)+Rt\nwhere \u2206 = E[xx>] and Rt is a small error term.\nOur second key lemma is about this update.\nLemma 3 (Update, informal). Suppose the update rule is\n\u03a3t+1 + Et+1 = (\u03a3t + Et)(1\u2212 \u03b7\u039b) + \u03b7Q\u039b + \u03b7Rt\nfor some PSD matrix \u039b and \u2016Rt\u20162 \u2264 C \u2032\u2032. Then\n\u2016\u03a3t + Et \u2212Q\u20162 \u2264 \u2016\u03a30 + E0 \u2212Q\u20162(1\u2212 \u03b7\u03bbmin(\u039b))t\n+ C \u2032\u2032\n\u03bbmin(\u039b) .\nApplying this on our update rule with Q = \u03a3\u22121 and \u039b = \u03a3\u2206\u03a3, we know that when the error term is sufficiently small, we can make progress on \u2016\u03a3t+Et\u2212\u03a3\u22121\u20162. Furthermore, by using the fact that \u03a30 \u2248 I and E0 is small, and the fact that \u03a3 is the diagonal part of (\u03a30 +E0)\u22121, we can show that after sufficiently many iterations, \u2016\u03a3t \u2212 I\u20162 blows up slightly, while \u2016Et\u20162 is reduced significantly. Repeating this for multiple stages completes the proof.\nWe note that most technical details are hidden, especially for the proofs of the decoding lemma, which need to show that the error term Rt is small. This crucially relies on the choice of \u03b1, and relies on bounding the effect of the correlation. These then give the setting of \u03b1 and the bound on the parameter m in the final theorem."}, {"heading": "6. More General Results", "text": ""}, {"heading": "6.1. Result for General x", "text": "This subsection considers the general case where x \u2208 [0, 1]D. Then the GCC condition is not enough for recovery, even for k, r,m = O(1) and \u03bb = \u2126(1). For example, GCC does not rule out the case that x is drawn uniformly over (r \u2212 1)-sparse random vectors with { 1D , 1} entries, when one cannot recover even a reasonable approximation of A\u2217 since a common vector 1D \u2211 i[A \u2217]i shows up in all the samples. This example shows that the difficulty arises if each xi constantly shows up with a small value. To avoid this, a general and natural way is to assume that each xi, once being non-zero, has to take a large value with sufficient probability. This is formalized as follows.\nDefinition 3 (Decay condition). A distribution of x satisfies the order-q decay condition for some constant q \u2265 1, if for all i \u2208 [D], xi satisfies that for every \u03b1 > 0,\nPr[xi \u2264 \u03b1 | xi 6= 0] \u2264 \u03b1q.\nWhen q = 1, each xi, once being non-zero, is uniformly distributed in the interval [0, 1]. When q gets larger, each xi, once being non-zero, will be more likely to take larger\nvalues. We will show that our algorithm has a better guarantee for larger q. In the extreme case when q =\u221e, xi will only take {0, 1} values, which reduces to the binary case.\nIn this paper, we show that this simple decay condition, combined with the GCC conditions and an initialization with constant error, is sufficient for recovering A\u2217.\nTheorem 4 (Main). There exists ` = \u2126(1) such that for every (r, k,m, \u03bb)-GCC x satisfying the order-q condition, every > 0, there exists T, \u03b7 and a sequence of {\u03b1i} 4 such that Algorithm AND, with `-initialization matrix A0, outputs a matrix A such that there exists a diagonal matrix \u03a3 12I with \u2016A \u2212A\n\u2217\u03a3\u20162 \u2264 with poly(D, 1 ) samples and iterations, as long as\nm = O\n( kD1\u2212 1 q \u03bb4+ 4 q\nr5+ 6 q+1\n) .\nAs mentioned, in many interesting applications, k = r = \u03bb = \u0398(1), where our algorithm can recover A\u2217 as long as m = O(D1\u2212 1 q+1 ). This means E\u00b5[xixj ] = O(D\u22121\u2212 1 q+1 ), a factor of D\u2212 1 q+1 away from the highest possible correlation min{E\u00b5[xi],E\u00b5[xj ]} = O(1/D). Then, the larger q, the higher correlation it can tolerate. As q goes to infinity, we recover the result for the case x \u2208 {0, 1}D, allowing the highest order correlation.\nThe analysis also shows that the decoding threshold should be \u03b1 = ( \u03bb\u2016E0\u20162\nr\n) 2 q+1\nwhere E0 is the error matrix at the beginning of the stage. Since the error decreases exponentially with stages, this suggests to decrease \u03b1 exponentially with stages. This is crucial for AND to recover the groundtruth; see Section 7 for the experimental results."}, {"heading": "6.2. Robustness to Noise", "text": "We now consider the case when the data is generated from y = A\u2217x+ \u03b6, where \u03b6 is the noise. For the sake of demonstration, we will just focus on the case when xi \u2208 {0, 1} and \u03b6 is random Gaussian noise \u03b6 \u223c \u03b3N ( 0, 1W I ) . 5 A more general theorem can be found in the appendix.\nDefinition 4 ((`, \u03c1)-initialization). The initial matrix A0 satisfies for some `, \u03c1 \u2208 [0, 1),\n1. A0 = A\u2217(\u03a3 + E) + N, for some diagonal matrix \u03a3 and off-diagonal matrix E.\n2. \u2016E\u20162 \u2264 `, \u2016\u03a3\u2212 I\u20162 \u2264 14 , \u2016N\u20162 \u2264 \u03c1.\nTheorem 5 (Noise, binary). Suppose each xi \u2208 {0, 1}. There exists ` = \u2126(1) such that for every (r, k,m, \u03bb)-GCC x, every > 0, Algorithm AND with T = poly(D, 1 ), \u03b7 =\n4In fact, we will make the choice explicit in the proof. 5we make this scaling so \u2016\u03b6\u20162 \u2248 \u03b3.\n1 poly(D, 1 ) , {\u03b1i}si=1 = { 14} 4 i=1 and an (`, \u03c1)-initialization A0 for \u03c1 = O(\u03c3min(A\u2217)), outputs A such that there exists a diagonal matrix \u03a3 12I with\n\u2016A\u2212A\u2217\u03a3\u20162 \u2264 O ( + r \u03c3max(A \u2217)\n\u03c3min(A\u2217)\u03bb \u03b3 ) using poly(D, 1 ) iterations, as long as m = O ( kD\u03bb4\nr5\n) .\nThe theorem implies that the algorithm can recover the ground-truth up to r \u03c3max(A\n\u2217) \u03c3min(A\u2217)\u03bb\ntimes \u03b3, the noise level in each sample. Although stated here for Gaussian noise for simplicity, the analysis applies to a much larger class of noises, including adversarial ones. In particular, we only need to the noise \u03b6 have sufficiently bounded \u2016E[\u03b6\u03b6>]\u20162; see the appendix for the details. For the special case of Gaussian noise, by exploiting its properties, it is possible to improve the error term with a more careful calculation, though not done here."}, {"heading": "7. Experiments", "text": "To demonstrate the advantage of AND, we complement the theoretical analysis with empirical study on semi-synthetic datasets, where we have ground-truth feature matrices and can thus verify the convergence. We then provide support for the benefit of using decreasing thresholds, and test its robustness to noise. In the appendix, we further test its robust to initialization and sparsity of x, and provide qualitative results in some real world applications. 6\nSetup. Our work focuses on convergence of the solution to the ground-truth feature matrix. However, realworld datasets in general do not have ground-truth. So we construct semi-synthetic datasets in topic modeling: first take the word-topic matrix learned by some topic modeling method as the ground-truth A\u2217, and then draw x from some specific distribution \u00b5. For fair comparison, we use one not learned by any algorithm evaluated here. In particular, we used the matrix with 100 topics computed by the algorithm in (Arora et al., 2013) on the NIPS papers dataset (about 1500 documents, average length about 1000). Based on this we build two semi-synthetic datasets:\n1. DIR. Construct a 100 \u00d7 5000 matrix X, whose columns are from a Dirichlet distribution with parameters (0.05, 0.05, . . . , 0.05). Then the dataset is Y = A\u2217X.\n2. CTM. The matrix X is of the same size as above, while each column is drawn from the logistic normal prior in the correlated topic model (Blei & Lafferty, 2006). This leads to a dataset with strong correlations.\n6The code is public on https://github.com/ PrincetonML/AND4NMF.\nNote that the word-topic matrix is non-negative. While some competitor algorithms require a non-negative feature matrix, AND does not need such a condition. To demonstrate this, we generate the following synthetic data:\n3. NEG. The entries of the matrix A\u2217 are i.i.d. samples from the uniform distribution on [\u22120.5, 0.5). The matrix X is the same as in CTM.\nFinally, the following dataset is for testing the robustness of AND to the noise:\n4. NOISE. A\u2217 and X are the same as in CTM, but Y = A\u2217X + N where N is the noise matrix with columns drawn from \u03b3N ( 0, 1W I ) with the noise level \u03b3.\nCompetitors. We compare the algorithm AND to the following popular methods: Alternating Non-negative Least Square (ANLS (Kim & Park, 2008)), multiplicative update (MU (Lee & Seung, 2001)), LDA (online version (Hoffman et al., 2010)),7 and Hierarchical Alternating Least Square (HALS (Cichocki et al., 2007)).\nEvaluation criterion. Given the output matrix A and the ground truth matrix A\u2217, the correlation error of the i-th column is given by\n\u03b5i(A,A \u2217) = min j\u2208[D],\u03c3\u2208R {\u2016[A\u2217]i \u2212 \u03c3[A]j\u20162}.\nThus, the error measures how well the i-th column of A\u2217 is covered by the best column of A up to scaling. We find the best column since in some competitor algorithms, the columns of the solution A may only correspond to a permutation of the columns of A\u2217.8\nWe also define the total correlation error as\n\u03b5(A,A\u2217) = D\u2211 i=1 \u03b5i(A,A \u2217).\nWe report the total correlation error in all the experiments.\nInitialization. In all the experiments, the initialization matrix A0 is set to A0 = A\u2217(I + U) where I is the identity matrix and U is a matrix whose entries are i.i.d. samples from the uniform distribution on [\u22120.05, 0.05). Note that this is a very weak initialization, since [A0]i = (1 + Ui,i)[A \u2217]i + \u2211 j 6=i Uj,i[A\n\u2217]j and the magnitude of the noise component \u2211 j 6=i Uj,i[A\n\u2217]j can be larger than the signal part (1 + Ui,i)[A\u2217]i.\n7We use the implementation in the sklearn package (http: //scikit-learn.org/)\n8In the Algorithm AND, the columns of A correspond to the columns of A\u2217 without permutation.\nHyperparameters and Implementations. For most experiments of AND, we used T = 50 iterations for each stage, and thresholds \u03b1i = 0.1/(1.1)i\u22121. For experiments on the robustness to noise, we found T = 100 leads to better performance. Furthermore, for all the experiments, instead of using one data point at each step, we used the whole dataset for update."}, {"heading": "7.1. Convergence to the Ground-Truth", "text": "Figure 1 shows the convergence rate of the algorithms on the three datasets. AND converges in linear rate on all three datasets (note that the y-axis is in log-scale). HALS converges on the DIR and CTM datasets, but the convergence is in slower rates. Also, on CTM, the error oscillates. Furthermore, it doesn\u2019t converge on NEG where the groundtruth matrix has negative entries. ANLS converges on DIR and CTM at a very slow speed due to the non-negative least square computation in each iteration. 9 All the other algo-\n9We also note that even the thresholding of HALS and ALNS designed for non-negative feature matrices is removed, they still\nrithms do not converge to the ground-truth, suggesting that they do not have recovery guarantees."}, {"heading": "7.2. The Threshold Schemes", "text": "Figure 2(a) shows the results of using different thresholding schemes on DIR, while Figure 2(b) shows that those on CTM. When using a constant threshold for all iterations, the error only decreases for the first few steps and then stop decreasing. This aligns with our analysis and is in strong contrast to the case with decreasing thresholds."}, {"heading": "7.3. Robustness to Noise", "text": "Figure 2(c) shows the performance of AND on the NOISE dataset with various noise levels \u03b3. The error drops at the first few steps, but then stabilizes around a constant related to the noise level, as predicted by our analysis. This shows that it can recover the ground-truth to good accuracy, even when the data have a significant amount of noise.\ndo not converge on NEG."}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grants CCF1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329. This work was done when Yingyu Liang was visiting the Simons Institute."}, {"heading": "A. Complete Proofs", "text": "We now recall the proof sketch.\nFor simplicity, we only focus on one stage and the expected update. The expected update of A(t) is given by\nA(t+1) = A(t) + \u03b7(E[yz>]\u2212A(t)E[zz>]).\nLet us write A = A\u2217(\u03a30 + E0) where \u03a30 is diagonal and E0 is off-diagonal. Then the decoding is given by\nz = \u03c6\u03b1(A \u2020x) = \u03c6\u03b1((\u03a30 + E0) \u22121x).\nLet \u03a3,E be the diagonal part and the off-diagonal part of (\u03a30 + E0)\u22121.\nThe first step of our analysis is a key lemma for decoding. It says that under suitable conditions, z will be close to \u03a3x in the following sense: E[\u03a3xx>] \u2248 E[zx>], E[\u03a3xz>] \u2248 E[zz>]. This key decoding lemma is formally stated in Lemma 6 (for the simplified case where x \u2208 {0, 1}D) and Lemma 8 (for the general case where x \u2208 [0, 1]D).\nNow, let us write A(t) = A\u2217(\u03a3t + Et). Then applying the above decoding lemma, the expected update of \u03a3t + Et is\n\u03a3t+1 + Et+1 = (\u03a3t + Et)(I\u2212\u03a3\u2206\u03a3) + \u03a3\u22121(\u03a3\u2206\u03a3) + Rt\nwhere Rt is a small error term.\nThe second step is a key lemma for updating the feature matrix: for the update rule\n\u03a3t+1 + Et+1 = (\u03a3t + Et)(1\u2212 \u03b7\u039b) + \u03b7Q\u039b + \u03b7Rt\nwhere \u039b is a PSD matrix and \u2016Rt\u20162 \u2264 C \u2032\u2032, we have\n\u2016\u03a3t + Et \u2212Q\u20162 \u2264 \u2016\u03a30 + E0 \u2212Q\u20162(1\u2212 \u03b7\u03bbmin(\u039b))t + C \u2032\u2032\n\u03bbmin(\u039b) .\nThis key updating lemma is formally stated in Lemma 10.\nApplying this on our update rule with Q = \u03a3\u22121 and \u039b = \u03a3\u2206\u03a3, we know that when the error term is sufficiently small, we can make progress on \u2016\u03a3t + Et \u2212\u03a3\u22121\u20162. Then, by using the fact that \u03a30 \u2248 I and E0 is small, and the fact that \u03a3 is the diagonal part of (\u03a30 + E0)\u22121, we can show that after sufficiently many iterations, \u2016\u03a3t \u2212 I\u20162 blows up slightly, while \u2016Et\u20162 is reduced significantly (See Lemma 11 for the formal statement). Repeating this for multiple stages completes the proof.\nOrganization. Following the proof sketch, we first present the decoding lemmas in Section A.1, and then the update lemmas in Section A.2. Section A.3 then uses these lemmas to prove the main theorems (Theorem 1 and Theorem 4). Proving the decoding lemmas is highly non-trivial, and we collect the lemmas needed in Section A.4.\nFinally, the analysis for the robustness to noise follows a similar proof sketch. It is presented in Section A.5."}, {"heading": "A.1. Decoding", "text": "A.1.1. xi \u2208 {0, 1}\nHere we present the following decoding Lemma when xi \u2208 {0, 1}. Lemma 6 (Decoding). For every ` \u2208 [0, 1), every off-diagonal matrix E\u2032 such that \u2016E\u2032\u20162 \u2264 ` and every diagonal matrix \u03a3\u2032 such that \u2016\u03a3\u2032 \u2212 I\u20162 \u2264 12 , let z = \u03c6\u03b1((\u03a3 \u2032 + E\u2032)x) for \u03b1 \u2264 14 . Then for every \u03b2 \u2208 (0, 1/2],\n\u2016E[(\u03a3\u2032x\u2212 z)x>]\u20162, \u2016E[(\u03a3\u2032x\u2212 z)z>]\u20162 = O(C1)\nwhere\nC1 = kr D + m`4r2 \u03b13D2 + `2 \u221a kmr1.5 D1.5\u03b2 + `4r3m \u03b22D2 + `5r2.5m D2\u03b12\u03b2 .\nProof of Lemma 6. We will prove the bound on \u2016E[(\u03a3\u2032x \u2212 z)z>]\u20162, and a simlar argument holds for that on \u2016E[(\u03a3\u2032x \u2212 z)x>]\u20162.\nFirst consider the term |\u03a3\u2032i,ixi \u2212 zi| for a fixed i \u2208 [D]. Due to the decoding, we have zi = \u03c6\u03b1([(\u03a3\u2032 + E\u2032)]ix) = \u03c6\u03b1(\u03a3 \u2032 i,ixi + \u3008ei, x\u3009) where ei is the i-th row of E\u2032.\nClaim 7.\n\u03a3\u2032i,ixi \u2212 zi = ax,1\u03c6\u03b1(\u2212\u3008ei, x\u3009) + ax,2\u03c6\u03b1(\u3008ei, x\u3009)\u2212 \u3008ei, x\u3009xi (A.1)\nwhere ax,1, ax,2 \u2208 [\u22121, 1] that depends on x.\nProof. To see this, we split into two cases:\n1. When xi = 0, then |\u03a3\u2032i,ixi \u2212 zi| = |zi| \u2264 \u03c6\u03b1(\u3008ei, x\u3009).\n2. When xi = 1, then zi = 0 only when \u2212\u3008ei, x\u3009 \u2265 12 \u2212 \u03b1 \u2265 \u03b1, which implies that |\u03a3 \u2032 i,ixi \u2212 zi + \u3008ei, x\u3009| \u2264 \u03b1 \u2264\n\u03c6\u03b1(\u2212\u3008ei, x\u3009). When \u03a3\u2032i,ixi \u2212 zi + \u3008ei, x\u3009 6= 0, then \u03a3\u2032i,ixi \u2212 zi = \u2212\u3008ei, x\u3009.\nPutting everything together, we always have:\n|\u03a3\u2032i,ixi \u2212 zi + \u3008ei, x\u3009xi| \u2264 \u03c6\u03b1(|\u3008ei, x\u3009|)\nwhich means that there exists ax,1, ax,2 \u2208 [\u22121, 1] that depend on x such that\n\u03a3\u2032i,ixi \u2212 zi = ax,1\u03c6\u03b1(\u2212\u3008ei, x\u3009) + ax,2\u03c6\u03b1(\u3008ei, x\u3009)\u2212 \u3008ei, x\u3009xi.\nConsider the term \u3008ei, x\u3009xi, we know that for every \u03b2 \u2265 0,\n|\u3008ei, x\u3009 \u2212 \u03c6\u03b2(\u3008ei, x\u3009) + \u03c6\u03b2(\u2212\u3008ei, x\u3009)| \u2264 \u03b2.\nTherefore, there exists bx \u2208 [\u2212\u03b2, \u03b2] that depends on x such that\n\u3008ei, x\u3009xi = \u03c6\u03b2(\u3008ei, x\u3009)\u2212 \u03c6\u03b2(\u2212\u3008ei, x\u3009)\u2212 bxxi.\nPutting into (A.1), we get:\n\u03a3\u2032i,ixi \u2212 zi = ax,1\u03c6\u03b1(\u2212\u3008ei, x\u3009) + ax,2\u03c6\u03b1(\u3008ei, x\u3009)\u2212 \u03c6\u03b2(\u3008ei, x\u3009)xi + \u03c6\u03b2(\u2212\u3008ei, x\u3009)xi + bxxi.\nFor notation simplicity, let us now write zi = (\u03a3 \u2032 i,i \u2212 bx)xi + ai + bi\nwhere ai = \u2212ax,1\u03c6\u03b1(\u2212\u3008ei, x\u3009)\u2212 ax,2\u03c6\u03b1(\u3008ei, x\u3009), bi = \u03c6\u03b2(\u3008ei, x\u3009)xi \u2212 \u03c6\u03b2(\u2212\u3008ei, x\u3009)xi.\nWe then have (\u03a3\u2032i,ixi \u2212 zi)zj = (bxxi \u2212 ai \u2212 bi)((\u03a3\u2032j,j \u2212 bx)xj + aj + bj).\nLet us now construct matrix M1, \u00b7 \u00b7 \u00b7M9, whose entries are given by\n1. (M1)i,j = bxxi(\u03a3\u2032j,j \u2212 bx)xj\n2. (M2)i,j = bxxiaj\n3. (M3)i,j = bxxibj\n4. (M4)i,j = \u2212ai(\u03a3\u2032j,j \u2212 bx)xj\n5. (M5)i,j = \u2212aiaj\n6. (M6)i,j = \u2212aibj\n7. (M7)i,j = \u2212bi(\u03a3\u2032j,j \u2212 bx)xj\n8. (M8)i,j = \u2212biaj\n9. (M9)i,j = \u2212bibj\nThus, we know that E[(\u03a3\u2032x\u2212z)z>] = \u22119 i=1 E[Mi]. It is sufficient to bound the spectral norm of each matrices separately, as we discuss below.\n1. M2,M4: these matrices can be bounded by Lemma 14, term 1.\n2. M5: this matrix can be bounded by Lemma 14, term 2.\n3. M6,M8: these matrices can be bounded by Lemma 15, term 3.\n4. M3,M7: these matrices can be bounded by Lemma 15, term 2.\n5. M9: this matrix can be bounded by Lemma 15, term 1.\n6. E[M1]: this matrix is of the form E[bxx(x dx)>], where dx is a vector whose j-th entry is (\u03a3\u2032j,j \u2212 bx). To bound the this term, we have that for any u, v such that \u2016u\u20162 = \u2016v\u20162 = 1,\nu>E[bxx(x dx)>]v = E[bx\u3008u, x\u3009\u3008v, x dx\u3009].\nWhen \u03b2 \u2264 12 , since x is non-negative, we know that the maximum of E[bx\u3008u, x\u3009\u3008v, x dx\u3009] is obtained when bx = \u03b2, dx = (2, \u00b7 \u00b7 \u00b7 , 2) and u, v are all non-negative, which gives us\nE[bx\u3008u, x\u3009\u3008v, x dx\u3009] \u2264 2\u03b2\u2016E[xx>]\u20162 \u2264 \u221a \u2016E[xx>]\u20161\u2016E[xx>]\u2016\u221e = \u2016E[xx>]\u20161.\nNow, for each row of \u2016E[xx>]\u20161, we know that [E[xx>]]i \u2264 E[xi \u2211 j xj ] \u2264 2rk D , which gives us\n\u2016E[M1]\u20162 \u2264 4\u03b2rk\nD .\nPutting everything together gives the bound on \u2016E[(\u03a3\u2032x \u2212 z)z>]\u20162. A similar proof holds for the bound on \u2016E[(\u03a3\u2032x \u2212 z)x>]\u20162.\nA.1.2. GENERAL xi\nWe have the following decoding lemma for the general case when xi \u2208 [0, 1] and the distribution of x satisfies the order-q decay condition.\nLemma 8 (Decoding II). For every ` \u2208 [0, 1), every off-diagonal matrix E\u2032 such that \u2016E\u2032\u20162 \u2264 ` and every diagonal matrix \u03a3\u2032 such that \u2016\u03a3\u2032 \u2212 I\u20162 \u2264 12 , let z = \u03c6\u03b1((\u03a3 \u2032 + E\u2032)x) for \u03b1 \u2264 14 . Then for every \u03b2 \u2208 (0, \u03b1],\n\u2016E[(\u03a3\u2032x\u2212 z)x>]\u20162, \u2016E[(\u03a3\u2032x\u2212 z)z>]\u20162 = O(C2)\nwhere\nC2 = `4r3m \u03b1\u03b22D2 + `5r2.5m D2\u03b12.5\u03b2 + `2kr D\u03b2 ( m Dk ) q 2q+2 + `3r2 \u221a km D1.5\u03b12 + `6r4m \u03b14D2 + k\u03b2 ( r D ) 2q+1 2q+2 + kr D \u03b1 q+1 2 .\nProof of Lemma 8. We consider the bound on \u2016E[(\u03a3\u2032x \u2212 z)z>]\u20162, and that on E[(\u03a3\u2032x \u2212 z)x>]\u20162 can be proved by a similar argument.\nAgain, we still have zi = \u03c6\u03b1([(\u03a3 \u2032 + E\u2032)]ix) = \u03c6\u03b1(\u03a3 \u2032 i,ixi + \u3008ei, x\u3009).\nHowever, this time even when xi 6= 0, xi can be smaller than \u03b1. Therefore, we need the following inequality.\nClaim 9. |\u03a3\u2032i,ixi \u2212 zi + \u3008ei, x\u30091xi\u22654\u03b1| \u2264 \u03c6\u03b1/2(|\u3008ei, x\u3009|) + 2xi1xi\u2208(0,4\u03b1).\nProof. To see this, we can consider the following four events:\n1. xi = 0, then |\u03a3\u2032i,ixi \u2212 zi + \u3008ei, x\u30091xi\u22652\u03b1| = |zi| \u2264 \u03c6\u03b1(\u3008ei, x\u3009)\n2. xi \u2265 4\u03b1. |\u03a3\u2032i,ixi \u2212 zi + \u3008ei, x\u30091xi\u22654\u03b1| = |\u03a3\u2032i,ixi + \u3008ei, x\u3009 \u2212 \u03c6\u03b1(\u03a3\u2032i,ixi + \u3008ei, x\u3009)|. Since \u03a3\u2032i,ixi \u2265 2\u03b1, we can get the same bound.\n3. xi \u2208 (\u03b1/4, 4\u03b1): then if zi 6= 0, |\u03a3\u2032i,ixi \u2212 zi + \u3008ei, x\u3009| = 0. Which implies that\n|\u03a3\u2032i,ixi \u2212 zi| \u2264 |\u3008ei, x\u3009| \u2264 \u03c6\u03b1/2(|\u3008ei, x\u3009|) + \u03b1\n2 \u2264 \u03c6\u03b1/2(|\u3008ei, x\u3009|) + 2xi1xi\u2208(0,4\u03b1)\nIf zi = 0, then |\u03a3\u2032i,ixi \u2212 zi| = \u03a3\u2032i,ixi \u2264 2xi1xi\u2208(0,2\u03b1)\n4. xi \u2208 (0, \u03b1/4), then \u03a3\u2032i,ixi \u2264 \u03b12 , therefore, zi 6= 0 only when \u3008ei, x\u3009 \u2265 \u03b1 2 . We still have: |\u03a3 \u2032 i,ixi\u2212zi| \u2264 \u03c6\u03b1/2(\u3008ei, x\u3009)\nIf z0 = 0, then |\u03a3\u2032i,ixi \u2212 zi| \u2264 2xi1xi\u2208(0,4\u03b1) as before.\nPutting everything together, we have the claim.\nFollowing the exact same calculation as in Lemma 6, we can obtain\n\u03a3\u2032i,ixi \u2212 zi = ax,1\u03c6\u03b1/2(\u2212\u3008ei, x\u3009) + ax,2\u03c6\u03b1/2(\u3008ei, x\u3009) \u2212 \u03c6\u03b2(\u3008ei, x\u3009)1xi\u22654\u03b1 + \u03c6\u03b2(\u2212\u3008ei, x\u3009)1xi\u22654\u03b1 + bx1xi\u22654\u03b1 + cx2xi1xi\u2208(0,4\u03b1)\nfor ax,1, ax,1, cx \u2208 [\u22121, 1] and bx \u2208 [\u2212\u03b2, \u03b2].\nTherefore, consider a matrix M whose (i, j)-th entry is (\u03a3\u2032i,ixi \u2212 zi)zj . This entry can be written as the summation of the following terms.\n1. Terms that can be bounded by Lemma 14. These include\nax,1\u03c6\u03b1/2(\u2212\u3008ei, x\u3009)xj , ax,2\u03c6\u03b1/2(\u3008ei, x\u3009)xj , ax,uax,v\u03c6\u03b1/2((\u22121)u\u3008ei, x\u3009)\u03c6\u03b1/2((\u22121)v\u3008ej , x\u3009)\nfor u, v \u2208 {1, 2}, and\nax,ubx\u03c6\u03b1/2((\u22121)u\u3008ei, x\u3009)1xi\u22654\u03b1, 2ax,ucx\u03c6\u03b1/2((\u22121)u\u3008ei, x\u3009)xj1xj\u2208(0,4\u03b1)\nby using 0 \u2264 1xj\u22654\u03b1 \u2264 xj 4\u03b1 and 0 \u2264 xj1xj\u2208(0,4\u03b1) \u2264 xj .\n2. Terms that can be bounded by Lemma 21. These include\n\u2212\u03c6\u03b2(\u3008ei, x\u3009)1xi\u22654\u03b1xj , \u03c6\u03b2(\u2212\u3008ei, x\u3009)1xi\u22654\u03b1xj ,\n(\u22121)uax,v\u03c6\u03b2((\u22121)1+u\u3008ei, x\u3009)1xi\u22654\u03b1\u03c6\u03b1/2((\u22121)v\u3008ej , x\u3009),\n(\u22121)u+v\u03c6\u03b2((\u22121)1+u\u3008ei, x\u3009)1xi\u22654\u03b11xj\u22654\u03b1\u03c6\u03b2((\u22121)1+v\u3008ej , x\u3009)\nfor u, v \u2208 {1, 2}. Also include\n2(\u22121)ucx\u03c6\u03b2((\u22121)1+u\u3008ei, x\u3009)1xi\u22654\u03b1xj1xj\u2208(0,4\u03b1)\nby using0 \u2264 xj1xj\u2208(0,4\u03b1) \u2264 xj . Also include\n2(\u22121)ubx\u03c6\u03b2((\u22121)1+u\u3008ei, x\u3009)1xi\u22654\u03b11xj\u22654\u03b1\nby using 0 \u2264 1xj\u22654\u03b1 \u2264 xj 4\u03b1 .\n3. Terms that can be bounded by Lemma 18. These include\nbx1xi\u22654\u03b1xj , b 2 x1xi\u22654\u03b11xj\u22654\u03b1, 2bxcx1xi\u22654\u03b1xi1xj\u2208(0,4\u03b1)xj .\nWhere agin we use the fact that 0 \u2264 1xi\u22654\u03b1 \u2264 xj 4\u03b1 and 0 \u2264 xi, 1xj\u2208(0,4\u03b1) \u2264 1\n4. Terms that can be bounded by Lemma 17. These include\ncx2xi1xi\u2208(0,4\u03b1)xj , 4c 2 xxi1xi\u2208(0,4\u03b1)xj1xj\u2208(0,4\u03b1).\nWhere we use the fact that 0 \u2264 1xj\u2208(0,4\u03b1) \u2264 1.\nPutting everything together, when 0 < \u03b2 \u2264 \u03b1,\n\u2016E[(\u03a3\u2032x\u2212 z)z>]\u20162 = O (C2)\nwhere\nC2 = `4r3m \u03b1\u03b22D2 + `5r2.5m D2\u03b12.5\u03b2 + `2kr D\u03b2 ( m Dk ) q 2q+2 + `3r2 \u221a km D1.5\u03b12 + `6r4m \u03b14D2 + k\u03b2 ( r D ) 2q+1 2q+2 + kr D \u03b1 q+1 2 .\nThis gives the bound on E[(\u03a3\u2032x\u2212 z)z>]\u20162. The bound on E[(\u03a3\u2032x\u2212 z)x>]\u20162 can be proved by a similar argument."}, {"heading": "A.2. Update", "text": ""}, {"heading": "A.2.1. GENERAL UPDATE LEMMA", "text": "Lemma 10 (Update). Suppose \u03a3t is diagonal and Et is off-diagonal for all t. Suppose we have an update rule that is given by \u03a3t+1 + Et+1 = (\u03a3t + Et)(1\u2212 \u03b7\u2206) + \u03b7\u03a3\u2206 + \u03b7Rt for some positive semidefinite matrix \u2206 and some Rt such that \u2016Rt\u20162 \u2264 C \u2032\u2032. Then for every t \u2265 0,\n\u2016\u03a3t + Et \u2212\u03a3\u20162 \u2264 \u2016\u03a30 + E0 \u2212\u03a3\u20162(1\u2212 \u03b7\u03bbmin(\u2206))t + C \u2032\u2032\n\u03bbmin(\u2206) .\nProof of Lemma 10. We know that the update is given by\n\u03a3t+1 + Et+1 \u2212\u03a3 = (\u03a3t + Et \u2212\u03a3)(1\u2212 \u03b7\u2206) + \u03b7Rt.\nIf we let \u03a3t + Et \u2212\u03a3 = (\u03a30 + E0 \u2212\u03a3)(1\u2212 \u03b7\u2206)t + Ct.\nThen we can see that the update rule of Ct is given by\nC0 = 0,\nCt+1 = Ct(1\u2212 \u03b7\u2206) + \u03b7Rt\nwhich implies that \u2200t \u2265 0, \u2016Ct\u20162 \u2264 C \u2032\u2032\n\u03bbmin(\u2206) .\nPutting everything together completes the proof.\nLemma 11 (Stage). In the same setting as Lemma 10, suppose initially for `1, `2 \u2208 [0, 18 ) we have\n\u2016\u03a30 \u2212 I\u20162 \u2264 `1, \u2016E0\u2016 \u2264 `2,\u03a3 = (Diag[(\u03a30 + E0)\u22121])\u22121.\nMoreover, suppose in each iteration, the error Rt satisfies that \u2016Rt\u20162 \u2264 \u03bbmin(\u2206)160 `2.\nThen after t = log 400`2 \u03b7\u03bbmin(\u2206) iterations, we have\n1. \u2016\u03a3t \u2212 I\u20162 \u2264 `1 + 4`2,\n2. \u2016Et\u20162 \u2264 140`2.\nProof of Lemma 11. Using Taylor expansion, we know that\nDiag[(\u03a30 + E0)\u22121] = \u03a3\u221210 + \u221e\u2211 i=1 \u03a3 \u22121/2 0 Diag[(\u2212\u03a3 \u22121/2 0 E0\u03a3 \u22121/2 0 ) i]\u03a3 \u22121/2 0 .\nSince \u2016Diag(M)\u20162 \u2264 \u2016M\u20162 for any matrix M,\n\u2016Diag[(\u03a30 + E0)\u22121]\u2212\u03a3\u221210 \u20162 = \u2016 \u221e\u2211 i=1 \u03a3 \u22121/2 0 Diag[(\u2212\u03a3 \u22121/2 0 E0\u03a3 \u22121/2 0 ) i]\u03a3 \u22121/2 0 \u20162\n\u2264 \u2016 \u221e\u2211 i=1 \u03a3 \u22121/2 0 (\u2212\u03a3 \u22121/2 0 E0\u03a3 \u22121/2 0 ) i\u03a3 \u22121/2 0 \u20162 = \u2016[(\u03a30 + E0)\u22121](\u2212E0\u03a3\u221210 )\u20162\n\u2264 `2 (1\u2212 `1)(1\u2212 `1 \u2212 `2) \u2264 32 21 `2.\nTherefore,\n\u2016Diag[(\u03a30 + E0)\u22121]\u03a30 \u2212 I\u20162 \u2264 `2(1 + `1) (1\u2212 `1)(1\u2212 `1 \u2212 `2) \u2264 ` := 12 7 `2.\nwhich gives us\n\u2016Diag[(\u03a30 + E0)\u22121]\u22121\u03a3\u221210 \u2212 I\u20162 \u2264 ` 1\u2212 ` \u2264 24 11 `2.\nThis then leads to\n\u2016\u03a3\u2212\u03a30\u20162 \u2264 \u2016Diag[(\u03a30 + E0)\u22121]\u22121 \u2212\u03a30\u20162 \u2264 (1 + `1)`\n1\u2212 ` \u2264 3`2.\nNow since \u2016\u03a30 + E0 \u2212\u03a3\u20162 \u2264 4`2 \u2264 1, after t = log 400`2 \u03b7\u03bbmin(\u2206) iterations, we have\n\u2016\u03a3t + Et \u2212\u03a3\u20162 \u2264 1\n80 `2.\nThen since \u03a3t \u2212\u03a3 = Diag[\u03a3t + Et \u2212\u03a3], we have\n\u2016\u03a3t \u2212\u03a3\u20162 \u2264 \u2016\u03a3t + Et \u2212\u03a3\u20162 \u2264 1\n80 `2.\nThis implies that\n\u2016Et\u20162 \u2264 \u2016\u03a3t \u2212\u03a3\u20162 + \u2016\u03a3t + Et \u2212\u03a3\u20162 \u2264 1\n40 `2\nand \u2016\u03a3t \u2212 I\u20162 \u2264 \u2016\u03a3t \u2212\u03a3\u20162 + \u2016\u03a3\u2212\u03a30\u20162 + \u2016\u03a30 \u2212 I\u20162 \u2264 1\n80 `2 + 3`2 + `1 \u2264 `1 + 4`2.\nCorollary 12 (Corollary of Lemma 11). Under the same setting as Lemma 11, suppose initially `1 \u2264 117 , then\n1. `1 \u2264 18 holds true through all stages,\n2. `2 \u2264 ( 1 40 )t after t stages.\nProof of Corollary 12. The second claim is trivial. For the first claim, we have\n(`1)stage s+1 \u2264 (`1)stage s + 4(`2)stage s \u2264 \u00b7 \u00b7 \u00b7 \u2264 1\n17 +\n1\n8 \u2211 i (1/40)i \u2264 1 8 ."}, {"heading": "A.3. Proof of the Main Theorems", "text": "With the update lemmas, we are ready to prove the main theorems.\nProof of Theorem 1. For simplicity, we only focus on the expected update. The on-line version can be proved directly from this by noting that the variance of the update is polynomial bounded and setting accordingly a polynomially small \u03b7. The expected update of A(t) is given by\nA(t+1) = A(t) + \u03b7(E[yz>]\u2212A(t)E[zz>])\nLet us pick \u03b1 = 14 , focus on one stage and write A = A \u2217(\u03a30 + E0). Then the decoding is given by\nz = \u03c6\u03b1(A \u2020x) = \u03c6\u03b1((\u03a30 + E0) \u22121x).\nLet \u03a3,E be the diagonal part and the off diagonal part of (\u03a30 + E0)\u22121. By Lemma 6,\n\u2016E[(\u03a3x\u2212 z)x>\u03a3]\u20162, \u2016E[(\u03a3x\u2212 z)z>\u20162 = O(C1).\nNow, if we write A(t) = A\u2217(\u03a3t + Et), then the expected update of \u03a3t + Et is given by\n\u03a3t+1 + Et+1 = (\u03a3t + Et)(I\u2212\u03a3\u2206\u03a3) + \u03a3\u22121(\u03a3\u2206\u03a3) + Rt\nwhere \u2016Rt\u20162 = O(C1). By Lemma 11, as long as C1 = O(\u03c3min(\u2206)\u2016E0\u20162) = O ( k\u03bb D \u2016E0\u20162 ) , we can make progress. Putting in the expression of C1 with ` \u2265 \u2016E0\u20162, we can see that as long as\n\u03b2kr D + m`4r2 \u03b13D2 + `2 \u221a kmr1.5 D1.5\u03b2 + `4r3m \u03b22D2 + `5r2.5m D2\u03b12\u03b2 = O\n( k\u03bb\nD `\n) ,\nwe can make progress. By setting \u03b2 = O ( \u03bb` r ) , with Corollary 12 we completes the proof.\nProof of Theorem 4. For simplicity, we only focus on the expected update. The on-line version can be proved directly from this by setting a polynomially small \u03b7. The expected update of A(t) is given by\nA(t+1) = A(t) + \u03b7(E[yz>]\u2212A(t)E[zz>]).\nLet us focus on one stage and write A = A\u2217(\u03a30 + E0). Then the decoding is given by\nz = \u03c6\u03b1(A \u2020x) = \u03c6\u03b1((\u03a30 + E0) \u22121x).\nLet \u03a3,E be the diagonal part and the off diagonal part of (\u03a30 + E0)\u22121. By Lemma 8,\n\u2016E[(\u03a3x\u2212 z)x>\u03a3]\u20162, \u2016E[(\u03a3x\u2212 z)z>\u20162 = O(C2).\nNow, if we write A(t) = A\u2217(\u03a3t + Et), then the expected update of \u03a3t + Et is given by\n\u03a3t+1 + Et+1 = (\u03a3t + Et)(I\u2212\u03a3\u2206\u03a3) + \u03a3\u22121(\u03a3\u2206\u03a3) + Rt\nwhere \u2016Rt\u20162 = O(C2). By Lemma 11, as long as C2 = O(\u03c3min(\u2206)\u2016E0\u20162) = O ( k\u03bb D \u2016E0\u20162 ) , we can make progress. Putting in the expression of C2 with ` \u2265 \u2016E0\u20162, we can see that as long as\nC2 = `4r3m \u03b1\u03b22D2 + `5r2.5m D2\u03b12.5\u03b2 + `2kr D\u03b2 ( m Dk ) q 2q+2 + `3r2 \u221a km D1.5\u03b12 + `6r4m \u03b14D2 + k\u03b2 ( r D ) 2q+1 2q+2 + kr D \u03b1 q+1 2 = O ( k\u03bb D ` ) ,\nwe can make progress. Now set\n\u03b2 = \u03bb` D ( r D ) 2q+1 2q+2 , \u03b1 =\n( \u03bb`\nr\n) 2 q+1\nand thus in C2,\n1. First term `2\u2212 2 q+1 k0r 5q+6 q+1 m\n\u03bb2+ 2 q+1D2\u2212 1 q+1\n2. Second term `4\u2212 5 q+1 k0r 7q+16 2q+2 m\n\u03bb1+ 5 q+1D2\u2212 1 2q+2\n3. Third term `1k q+2 2q+2 r 4q+3 2q+2m q 2q+2\n\u03bbD 3q+1 2q+2\n4. Fourth term `3\u2212 4 q+1 k 1 2 r 4 q+1+2m 1 2\n\u03bb 4 q+1D\u2212 3 2\n5. Fifth term `6\u2212 8 q+1 k0r4+ 8 q+1m\n\u03bb 8 q+1D2\nWe need each term to be smaller than \u03bbk`D , which implies that (we can ignore the constant ` )\n1. First term:\nm \u2264 kD 1\u2212 1q+1\u03bb3+ 2 q+1\nr5+ 1 q+1\n2. Second term:\nm \u2264 kD 1\u2212 12q+2\u03bb2+ 5 q+1\nr 7 2+ 9 2q+2\n3. Third term:\nm \u2264 kD q\u22121 q \u03bb4+ 4 q\nr4+ 2 q\n4. Fourth term:\nm \u2264 kD\u03bb 2+ 8q+1\nr4+ 8 q+1\n5. Fifth term:\nm \u2264 kD\u03bb 1+ 8q+1\nr4+ 8 q+1\nThis is satisfied by our choice of m in the theorem statement.\nThen with Corollary 12 we completes the proof."}, {"heading": "A.4. Expectation Lemmas", "text": "In this subsection, we assume that x follows (r, k,m, \u03bb)-GCC. Then we show the following lemmas."}, {"heading": "A.4.1. LEMMAS WITH ONLY GCC", "text": "Lemma 13 (Expectation). For every ` \u2208 [0, 1), every vector e such that \u2016e\u20162 \u2264 `, for every \u03b1 such that \u03b1 > 2`, we have\nE[\u03c6\u03b1(\u3008e, x\u3009)] \u2264 16m`4r2\n\u03b12(\u03b1\u2212 2`)D2 .\nProof of Lemma 13. Without lose of generality, we can assume that all the entries of e are non-negative. Let us denote a new vector g such that\ngi = { ei if ei \u2265 \u03b12r ; 0 otherwise.\nDue to the fact that \u2016x\u20161 \u2264 r, we can conclude \u3008e\u2212 g, x\u3009 \u2264 \u03b12r \u00d7 r = \u03b1 2 , which implies\n\u03c6\u03b1 2 (\u3008g, x\u3009) \u2265 1 2 \u03c6\u03b1(\u3008e, x\u3009).\nNow we can only focus on g. Since \u2016g\u20162 \u2264 `, we know that g has at most 4` 2r2\n\u03b12 non-zero entries. Let us then denote the set of non-zero entries of g as E , so we have |E| \u2264 4` 2r2\n\u03b12 .\nSuppose the all the x such that \u3008g, x\u3009 \u2265 \u03b12 forms a set S of size S, each x (s) \u2208 S has probability pt. Then we have:\nE[\u03c6\u03b1(\u3008e, x\u3009)] \u2264 2 \u2211 s\u2208[S] ps\u3008g, x(s)\u3009 = 2 \u2211 s\u2208[S],i\u2208E psgix (s) i .\nOn the other hand, we have:\n1. \u2200s \u2208 [S] : \u2211 i\u2208E gix (s) i \u2265 \u03b12 .\n2. \u2200i 6= j \u2208 [D] : \u2211 s\u2208[S] psx (s) i x (s) j \u2264 mD2 . This is by assumption 5 of the distribution of x.\nUsing (2) and multiply both side by gigj , we get\u2211 s\u2208[S] ps(gix (s) i )(gjx (s) j ) \u2264 mgigj D2\nSum over all j \u2208 E , j 6= i, we have:\n\u2211 s\u2208[S] \u2211 j\u2208E,j 6=i ps(gix (s) i )(gjx (s) j ) \u2264 mgi D2  \u2211 j\u2208E,j 6=i gj  \u2264 mgi D2 \u2211 j\u2208E gj\nBy (1), and since \u2211 j\u2208E gjx (s) j \u2265 \u03b12 and gi \u2264 `, x (s) i \u2264 1, we can obtain \u2211 j\u2208E,j 6=i gjx (s) j \u2265 \u03b12 \u2212 `. This implies\n\u2211 s\u2208[S] ps(gix (s) i ) \u2264 1\u2211 j\u2208E,j 6=i gjx (s) j mgi D2 \u2211 j\u2208E gj  \u2264 2m (\u03b1\u2212 2`)D2 gi \u2211 j\u2208E gj .\nSumming over i,\n\u2211 s\u2208[S],i\u2208E psgix (s) i \u2264\n2m\n(\u03b1\u2212 2`)D2 \u2211 j\u2208E gj 2 \u2264 2m (\u03b1\u2212 2`)D2 |E|\u2016g\u201622 \u2264 8m`4r2 \u03b12(\u03b1\u2212 2`)D2 .\nPutting everything together we complete the proof.\nLemma 14 (Expectation, Matrix). For every `, `\u2032 \u2208 [0, 1), every matrices E,E\u2032 \u2208 RD\u00d7D such that \u2016E\u20162, \u2016E\u2032\u20162 \u2264 `, \u03b1 \u2265 4` and every bx \u2208 [\u22121, 1] that depends on x, the following hold.\n1. Let M be a matrix such that [M]i,j = bx\u03c6\u03b1(\u3008[E]i, x\u3009)xj , then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 8`3r2 \u221a km\nD1.5\u03b12 .\n2. Let M be a matrix such that [M]i,j = bx\u03c6\u03b1(\u3008[E]i, x\u3009)\u03c6\u03b1(\u3008[E\u2032]j , x\u3009), then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 32`6r4m\n\u03b14D2 .\nProof of Lemma 14. Since all the \u03c6\u03b1(\u3008[E]i, x\u3009) and xi are non-negative, without lose of generality we can assume that bx = 1.\n1. We have \u2211 j\u2208[D] E[Mi,j ] = E \u03c6\u03b1(\u3008[E]i, x\u3009) \u2211 j\u2208[D] xj  \u2264 rE[\u03c6\u03b1(\u3008[E]i, x\u3009)] \u2264 16`4r3m \u03b12(\u03b1\u2212 2`)D2 .\nOn the other hand,\n\u2211 i\u2208[D] E[Mi,j ] = E \u2211 i\u2208[D] \u03c6\u03b1(\u3008[E]i, x\u3009) xj  \u2264 E[(uxEx)xj ]\nwhere ux is a vector with each entry either 0 or 1 depend on \u3008[E]i, x\u3009 \u2265 \u03b1 or not. Note that \u2211 i\u3008[E]i, x\u30092 \u2264 `2r, so ux can only have at most ` 2r \u03b12 entries equal to 1, so \u2016ux\u20162 \u2264 ` \u221a r \u03b1 . This implies that\n(uxEx) \u2264 `2r\n\u03b1 .\nTherefore, \u2211 i\u2208[D] E[Mi,j ] \u2264 2`2rk \u03b1D , which implies that\n\u2016E[M]\u20162 \u2264 4 \u221a 2`3r2 \u221a km D1.5\u03b11.5 \u221a (\u03b1\u2212 2`) .\n2. We have\n\u2016E[M]\u20161 \u2264 max i \u2211 j\u2208[D] E[Mi,j ] = max i E \u03c6\u03b1(\u3008[E]i, x\u3009) \u2211 j\u2208[D] \u03c6\u03b1(\u3008[E\u2032]j , x\u3009)  \u2264 `2r \u03b1 16`4r3m D2\u03b12(\u03b1\u2212 2`) .\nIn the same way we can bound \u2016E[M]\u2016\u221e and get the desired result.\nA.4.2. LEMMAS WITH xi \u2208 {0, 1}\nHere we present some expectation lemmas when xi \u2208 {0, 1}. Lemma 15 (Expectation, Matrix). For every `, `\u2032 \u2208 [0, 1), every matrices E,E\u2032 \u2208 RD\u00d7D such that \u2016E\u20162, \u2016E\u2032\u20162 \u2264 `, and \u2200i \u2208 [D], |Ei,i||E\u2032i,i| \u2264 `\u2032, then for every \u03b2 > 4`\u2032 and \u03b1 \u2265 4` and every bx \u2208 [\u22121, 1] that depends on x, the following hold.\n1. Let M be a matrix such that [M]i,j \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e = bx\u03c6\u03b2(\u3008[E]i, x\u3009)xixj , then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 8`1.5r1.25m\nD1.5\u03b20.5 .\n2. Let M be a matrix such that [M]i,j = bx\u03c6\u03b2(\u3008[E]i, x\u3009)xixj\u03c6\u03b2(\u3008[E\u2032]j , x\u3009), then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 8`4r3m\n\u03b22D2 .\n3. Let M be a matrix such that [M]i,j = bx\u03c6\u03b2(\u3008[E]i, x\u3009)xi\u03c6\u03b1(\u3008[E\u2032]j , x\u3009), then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 16`5r2.5m\nD2\u03b12\u03b2 .\nProof. This Lemma is a special case of Lemma 21 by setting \u03b3 = 1.\nA.4.3. LEMMAS WITH GENERAL xi\nHere we present some expectation lemmas for the general case where xi \u2208 [0, 1] and the distribution of x satisfies the order-q decay condition.\nLemma 16 (General expectation). Suppose the distribution of x satisfies the order-q decay condition.\n\u2200i \u2208 [D], E[xi] \u2264 Pr[xi 6= 0] \u2264 (q + 2)2k\nqD .\nProof. Denote s = Pr[xi 6= 0]. By assumption, Pr[xi \u2264 \u221a \u03b1 | xi 6= 0] \u2264 \u03b1q/2, which implies that Pr[xi > \u221a \u03b1] > s(1\u2212 \u03b1q/2). Now, since\nE[x2i ] = \u222b 1 0 Pr[x2i \u2265 \u03b1] = \u222b 1 0 Pr[xi \u2265 \u221a \u03b1] \u2264 2k D ,\nWe obtain s \u2264 2k\nD 1\u222b 1 0 (1\u2212 \u03b1q/2)d\u03b1 \u2264 q + 2 q 2k D .\nLemma 17 (Truncated covariance). For every \u03b1 > 0, every bx \u2208 [\u22121, 1] that depends on x, the following holds. Let M be a matrix such that [M]i,j = bx1xi\u2264\u03b1xixj , then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 6kr\nD \u03b1 q+1 2 .\nProof of Lemma 17. Again, without lose of generality we can assume that bx are just 1. On one hand, \u2211 j\u2208[D] E[1xi\u2264\u03b1xixj ] \u2264 rE[1xi\u2264\u03b1xi] \u2264 r\u03b1E[10<xi\u2264\u03b1] = r\u03b1Pr[xi \u2208 (0, \u03b1]].\nBy Lemma 16,\nPr[xi \u2208 (0, \u03b1]] = Pr[xi 6= 0] Pr[xi \u2264 \u03b1 | xi 6= 0] \u2264 (q + 2)2k\nqD \u03b1q,\nand thus \u2211 j\u2208[D] E[1xi\u2264\u03b1xixj ] \u2264 2(q + 2)kr qD \u03b1q+1 \u2264 6kr D \u03b1q+1.\nOn the other hand, \u2211 i\u2208[D] E[1xi\u2264\u03b1xixj ] \u2264 E \u2211 i\u2208[D] xi xj  \u2264 6kr D .\nPutting everything together we completes the proof.\nLemma 18 (Truncated half covariance). For every \u03b1 > 0, every bx \u2208 [\u22121, 1] that depends on x, the following holds. Let M be a matrix such that [M]i,j = bx1xi\u2265\u03b1xj , then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 12k ( r D ) 2q+1 2q+2\nProof of Lemma 18. Without lose of generality, we can assume bx = 1. We know that\nE[1xi\u2265\u03b1xj ] \u2264 Pr[xi 6= 0]E[xj | xi 6= 0]\n\u2264 1 s Pr[xi 6= 0]E[1xi\u2265sxixj | xi 6= 0] + Pr[xi 6= 0]E[1xi<s | xi 6= 0] \u2264 1 s E[xixj ] + sq Pr[xi 6= 0].\nFrom Lemma 16 we know that Pr[xi 6= 0] \u2264 6kD , which implies that\u2211 i\u2208[D] E[xixj ] = E[ \u2211 i\u2208[D] xixj ] \u2264 rE[xj ] \u2264 rPr[xj 6= 0] \u2264 6kr D .\nTherefore, \u2211 i\u2208[D] E[1xi\u2265\u03b1xj ] \u2264 6ksq + 1 s 6kr D\nChoosing the optimal s, we are able to obtain\n\u2211 i\u2208[D] E[1xi\u2265\u03b1xj ] \u2264 ( r qD )q/(q+1) 6k(q + 1) \u2264 24k ( r D )q/(q+1) .\nOn the other hand, \u2211 j\u2208[D] E[1xi\u2265\u03b1xj ] \u2264 rE[1xi>0] \u2264 6k D r.\nPutting everything together we get the desired bound.\nLemma 19 (Expectation). For every ` \u2208 [0, 1), every vector e such that \u2016e\u20162 \u2264 `, for every i \u2208 [D], \u03b1 > 2|ei|, \u03b3 > 0, the following hold.\n1.\n\u2200i \u2208 [D] : E[\u03c6\u03b1(\u3008e, x\u3009)1xi\u2265\u03b3 ] \u2264 4`2rm\n\u03b3D2(\u03b1\u2212 2|ei|) .\n2. If ei = 0, then\n\u2200i \u2208 [D] : E[\u03c6\u03b1(\u3008e, x\u3009)1xi\u2265\u03b3 ] \u2264 24kr`2\nD\u03b1 ( m Dk )q/(q+1) .\nProof of Lemma 19. We define g as in Lemma 13. We still have \u2016g\u20161 \u2264 \u2016g\u2016 2 2\n\u03b1 2r \u2264 2r`\n2\n\u03b1 .\n1. The value \u03c6\u03b1(\u3008e, x\u3009)1xi\u2265\u03b3 is non-zero only when xi \u2265 \u03b3. Therefore, we shall only focus on this case. Let us again suppose x such that \u3008g, x\u3009 \u2265 \u03b12 and xi \u2265 \u03b3 forms a set S of size S, each x (s) \u2208 S has probability ps.\nClaim 20. (1) \u2200s \u2208 [S] : \u2211 j\u2208E gjx (s) j \u2265 \u03b12 .\n(2) \u2200j 6= i \u2208 [D] : \u2211 s\u2208[S] psx (s) j x (s) i \u2264 mD2 . (3) By Lemma 18,\n\u2200j 6= i \u2208 [D] : \u2211 a\u2208[S] pax (a) j \u2264 1 s m D2 + sq 6k D = 6k(q + 1) D ( m 6Dkq )q/(q+1) by choosing optimal s. Moreover, we can directly calculate that\n6k(q + 1)\nD\n( m\n6Dkq )q/(q+1) \u2264 6k D ( m Dk )q/(q+1) .\nWith Claim 20(2), multiply both side by gj and taking the summation,\u2211 s\u2208[S],j\u2208E,j 6=i psgjx (s) j x (s) i \u2264 m D2 \u2211 j\u2208E,j 6=i gj .\nUsing the fact that x(s)i \u2265 \u03b3 for every s \u2208 [S], we obtain\u2211 s\u2208[S],j\u2208E,j 6=i psgjx (s) j \u2264 m \u03b3D2 \u2211 j\u2208E,j 6=i gj .\nOn the other hand, by Claim 20(1) and the fact that |ei| \u2265 gi \u2265 0, we know that\u2211 j\u2208E,j 6=i gjx (s) j \u2265 \u03b1 2 \u2212 |ei|.\nUsing the fact that x(s)i \u2265 \u03b3 for every s \u2208 [S], we obtain\u2211 s\u2208[S] ps \u2264 2m \u03b3(\u03b1\u2212 2|ei|)D2 \u2211 j\u2208E,j 6=i gj .\nTherefore, since gi \u2264 |ei|,\u2211 s\u2208[S],j\u2208E psgjx (s) j \u2264 \u2211 s\u2208[S],j\u2208E,j 6=i psgjx (s) j + \u2211 s\u2208[S] psgix (s) i\n\u2264 m \u03b3D2\n( 1 +\n2gi \u03b1\u2212 2|ei| ) \u2211 j\u2208E,j 6=i gj \u2264 m \u03b3D2 \u03b1 \u03b1\u2212 2|ei| \u2016g\u20161 \u2264\n2`2rm\n\u03b3D2(\u03b1\u2212 2|ei|) .\n2. When ei = 0, in the same manner, but using Claim 20(3), we obtain\u2211 s\u2208[S] psgjx (s) j \u2264 6k D ( m Dk )q/(q+1) gj .\nSumming over j \u2208 E , j 6= i we have:\n\u2211 s\u2208[S],j\u2208E,j 6=i psgjx (s) j \u2264 6k D ( m Dk )q/(q+1) 2r`2 \u03b1 .\nLemma 21 (Expectation, Matrix). For every `, `\u2032 \u2208 [0, 1), every matrices E,E\u2032 \u2208 RD\u00d7D such that \u2016E\u20162, \u2016E\u2032\u20162 \u2264 `, and \u2200i \u2208 [D], |Ei,i|, |E\u2032i,i| \u2264 `\u2032, every \u03b2 > 4`\u2032 and \u03b1 \u2265 4`, every \u03b3 > 0 and every bx \u2208 [\u22121, 1] that depends on x, the following hold.\n1. Let M be a matrix such that [M]i,j = bx\u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3xj , then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 min\n{ 8`2 \u221a kr1.5 \u221a m\n\u221a \u03b3D1.5\u03b2\n, 12k`2r\nD\u03b2 ( m Dk )q/(2q+2)} .\n2. Let M be a matrix such that [M]i,j = bx\u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b31xj\u2265\u03b3\u03c6\u03b2(\u3008[E\u2032]j , x\u3009), then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264 8`4r2m\n\u03b3\u03b22D2 .\n3. Let M be a matrix such that [M]i,j = bx\u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3\u03c6\u03b1(\u3008[E\u2032]j , x\u3009), then\n\u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e \u2264\n16`5r2.5m \u221a \u03b3D2\u03b12\u03b2 .\nProof of Lemma 21. Without loss of generality, assume bx = 1.\n1. Since every entry of M is non-negative, by Lemma 19,\n\u2211 j\u2208[D] E[Mi,j ] = E \u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3 \u2211 j\u2208[D] xj  \u2264 rE[\u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3 ] \u2264 4`2r2m\u03b3D2(\u03b2 \u2212 2`\u2032) and \u2211\nj\u2208[D]\nE[Mi,j ] \u2264 24kr`2\nD\u03b2 ( m Dk )q/(q+1) .\nOn the other hand, as in Lemma 14, we know that\u2211 j\u2208[D] \u03c6\u03b2(\u3008[E\u2032]j , x\u3009) \u2264 `2r \u03b2 .\nTherefore, \u2211 i\u2208[D] E[\u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3xj ] \u2264 `2r \u03b2 E[xj ] \u2264 6k`2r \u03b2D . Now, since each entry of M is non-negative, using \u2016E[M]\u20162 \u2264 \u221a \u2016E[M]\u20161\u2016E[M]\u2016\u221e, we obtain the desired bound.\n2. Since now M is a \u201csymmetric\u201d matrix, we only need to look at \u2211 j\u2208[D] E[Mi,j ], and a similar bound holds for\u2211\ni\u2208[D] E[Mi,j ].\n\u2211 j\u2208[D] E[Mi,j ] = E \u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3 \u2211 j\u2208[D] \u03c6\u03b2(\u3008[E\u2032]j , x\u3009)1xj\u2265\u03b3  \u2264 `2r \u03b2\n4`2rm\n\u03b3D2(\u03b2 \u2212 2`\u2032) .\nThe conclusion then follows.\n3. On one hand,\n\u2211 j\u2208[D] E[Mi,j ] = E \u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3 \u2211 j\u2208[D] \u03c6\u03b1(\u3008[E\u2032]j , x\u3009)  \u2264 `2r \u03b1\n4`2rm\n\u03b3D2(\u03b2 \u2212 2`\u2032) .\nOn the other hand,\n\u2211 i\u2208[D] E[Mi,j ] = E \u2211 i\u2208[D] \u03c6\u03b2(\u3008[E]i, x\u3009)1xi\u2265\u03b3 \u03c6\u03b1(\u3008[E\u2032]j , x\u3009)  \u2264 `2r \u03b2 16`4r2m D2\u03b12(\u03b1\u2212 2`) .\nTherefore,\n\u2016E[M]\u20162 \u2264 8`5r2.5m\n\u221a \u03b3D2\u03b11.5\u03b20.5 \u221a \u03b2 \u2212 2`\u2032 \u221a \u03b1\u2212 2`\n\u2264 16` 5r2.5m\n\u221a \u03b3D2\u03b12\u03b2 ."}, {"heading": "A.5. Robustness", "text": "In this subsection, we show that our algorithm is also robust to noise. To demonstrate the idea, we will present a proof for the case when xi \u2208 {0, 1}. The general case when xi \u2208 [0, 1] follows from the same argument, just with more calculations. Lemma 22 (Expectation). For every `, \u03bd \u2208 [0, 1), every vector e such that \u2016e\u20162 \u2264 `, every \u03b1 such that \u03b1 > 2` + 2\u03bd, the following hold.\n1. E[\u03c6\u03b1(\u3008e, x\u3009+ \u03bd)] \u2264 16m` 4r2\n\u03b12(\u03b1\u22122`\u22122\u03bd)D2 . 2. If ei,i = 0, then E[|\u3008ei, x\u3009|xi] \u2264 \u221a 2mkr D3 .\nProof of Lemma 22. The proof of this lemma is almost the same as the proof of Lemma 13 with a few modifications.\n1. Without lose of generality, we can assume that all the entries of e are non-negative. Let us denote a new vector g such that\ngi = { ei if ei \u2265 \u03b12r , 0 otherwise.\nDue to the fact that \u2016x\u20161 \u2264 r, we can conclude \u3008e\u2212 g, x\u3009 \u2264 \u03b12r \u00d7 r = \u03b1 2 , which implies\n\u03c6\u03b1 2 (\u3008g, x\u3009+ \u03bd) \u2265 1 2 \u03c6\u03b1(\u3008e, x\u3009+ \u03bd).\nNow we can only focus on g. Since \u2016g\u20162 \u2264 `, we know that g has at most 4` 2r2\n\u03b12 non-zero entries. Let us then denote the set of non-zero entries of g as E . Then we have |E| \u2264 4` 2r2\n\u03b12 .\nSuppose the all the x such that \u3008g, x\u3009 \u2265 \u03b12 \u2212 \u03bd forms a set S of size S, each x (s) \u2208 S has probability pt. Then\nE[\u03c6\u03b1(\u3008e, x\u3009)] \u2264 2 \u2211 s\u2208[S] ps\u3008g, x(s)\u3009 = 2 \u2211 s\u2208[S],i\u2208E psgix (s) i .\nOn the other hand, we have the following claim. Claim 23. 1. \u2200s \u2208 [S] : \u2211 i\u2208E gix (s) i \u2265 \u03b12 \u2212 \u03bd.\n2. \u2200i 6= j \u2208 [D] : \u2211 s\u2208[S] psx (s) i x (s) j \u2264 mD2 . This is by the GCC conditions of the distribution of x.\nUsing (2) and multiply both side by gigj , we get\u2211 s\u2208[S] ps(gix (s) i )(gjx (s) j ) \u2264 mgigj D2 .\nSum over all j \u2208 E , j 6= i,\n\u2211 s\u2208[S] \u2211 j\u2208E,j 6=i ps(gix (s) i )(gjx (s) j ) \u2264 mgi D2  \u2211 j\u2208E,j 6=i gj  \u2264 mgi D2 \u2211 j\u2208E gj .\nUsing (1), and that \u2211 j\u2208E gjx (s) j \u2265 \u03b12 \u2212 \u03bd and gi \u2264 `, x\n(s) i \u2264 1, we can obtain\u2211\nj\u2208E,j 6=i\ngjx (s) j \u2265\n\u03b1 2 \u2212 \u03bd \u2212 `.\nThis implies \u2211 s\u2208[S] ps(gix (s) i ) \u2264 1\u2211 j\u2208E,j 6=i gjx (s) j mgi D2 \u2211 j\u2208E gj  \u2264 2m (\u03b1\u2212 2\u03bd \u2212 2`)D2 gi \u2211 j\u2208E gj .\nSumming over i,\n\u2211 s\u2208[S],i\u2208E psgix (s) i \u2264\n2m\n(\u03b1\u2212 4`)D2 \u2211 j\u2208E gj 2 \u2264 2m (\u03b1\u2212 2\u03bd \u2212 2`)D2 |E|\u2016g\u201622 \u2264 8m`4r2 \u03b12(\u03b1\u2212 2`\u2212 2\u03bd)D2 .\n2. We can directly bound this term as follows.\nE[|\u3008e, x\u3009|xi] \u2264 \u2211 j 6=i |ej |E[xixj ] \u2264 ` \u221a\u2211 j 6=i E[xixj ]2 \u2264 ` \u221a m D2 \u2211 j 6=i E[xixj ]2 \u2264 ` \u221a 2mkr D3 .\nWe show the following lemma saying that even with noise, A\u2020A\u2217 is roughly (\u03a3 + E)\u22121.\nLemma 24 (Noisy inverse). Let A \u2208 RW\u00d7D be a matrix such that A = A\u2217(\u03a3 + E) + N, for diagonal matrix \u03a3 12I, off diagonal matrix E with \u2016E\u20162 \u2264 ` \u2264 18 and \u2016N\u20162 \u2264 1 4\u03c3min(A \u2217). Then\n\u2016A\u2020A\u2217 \u2212 (\u03a3 + E)\u22121\u20162 \u2264 2\u2016N\u20162(\n1 2 \u2212 3 2` ) \u03c3min(A\u2217)\u2212 \u2016N\u20162 \u2264 32\u2016N\u20162 \u03c3min(A\u2217) ."}, {"heading": "Proof of Lemma 24.", "text": "\u2016A\u2020A\u2217 \u2212 (\u03a3 + E)\u22121\u20162 \u2264 \u2016A\u2020(A\u2217(\u03a3 + E) + N)(\u03a3 + E)\u22121 \u2212 (\u03a3 + E)\u22121\u20162 + \u2016A\u2020N\u20162\u2016(\u03a3 + E)\u22121\u20162 \u2264 \u2016A\u2020N\u20162\u2016(\u03a3 + E)\u22121\u2016\n\u2264 2 (1\u2212 `)\u03c3min(A) \u2016N\u20162.\nSince A = A\u2217(\u03a3 + E) + N,\n\u03c3min(A) \u2265 \u03c3min(A\u2217(\u03a3 + E))\u2212 \u2016N\u20162 \u2265 ( 1 2 \u2212 ` ) \u03c3min(A \u2217)\u2212 \u2016N\u20162.\nPutting everything together, we are able to obtain\n\u2016A\u2020A\u2217 \u2212 (\u03a3 + E)\u22121\u20162 \u2264 2 (1\u2212 `) ( 1 2 \u2212 ` ) \u03c3min(A\u2217)\u2212 \u2016N\u20162 \u2016N\u20162 \u2264 2\u2016N\u20162( 1 2 \u2212 3 2` ) \u03c3min(A\u2217)\u2212 \u2016N\u20162 .\nLemma 25 (Noisy decoding). Suppose we have z = \u03c6\u03b1((\u03a3\u2032 + E\u2032)x + \u03bex) for diagonal matrix \u2016\u03a3\u2032 \u2212 I\u20162 \u2264 12 and off diagonal matrix E\u2032 such that \u2016E\u2032\u20162 \u2264 ` \u2264 18 and random variable \u03be\nx depend on x such that \u2016\u03bex\u2016\u221e \u2264 \u03bd. Then if 1 4 > \u03b1 > 4`+ 4\u03bd,m \u2264 D r2 , we have\n\u2016E[(\u03a3x\u2212 z)x>]\u20162, \u2016E[(\u03a3x\u2212 z)z>]\u20162 = O (C3)\nwhere\nC3 = (\u03bd + \u03b2) kr D + m`4r2 \u03b13D2 + `2 \u221a kmr1.5 D1.5\u03b2 + `4r3m \u03b22D2 + `5r2.5m D2\u03b12\u03b2 .\nProof of Lemma 25. Since we have now\nzi = \u03c6\u03b1(\u03a3 \u2032 i,ixi + \u3008ei, x\u3009+ \u03bexi ).\nLike in Lemma 6, we can still show that\n|\u03a3\u2032i,ixi + \u3008ei, x\u3009xi + \u03bexi xi \u2212 zi| \u2264 \u03c6\u03b1(\u3008ei, x\u3009+ \u03bexi ) \u2264 \u03c6\u03b1(\u3008ei, x\u3009+ \u03bd)\nwhich implies that there exists ax,\u03be \u2208 [\u22121, 1] that depends on x, \u03be such that\nzi \u2212\u03a3\u2032i,ixi = \u3008ei, x\u3009xi + \u03bexi xi + ax,\u03be\u03c6\u03b1(\u3008ei, x\u3009+ \u03bd).\nTherefore,\nE[z2i ] \u2264 3(\u03a3\u2032i,i + \u03bd)2E[x2i ] + 3E[\u3008ei, x\u30092x2i ] + 3E[\u03c6\u03b1(\u3008ei, x\u3009+ \u03bd)2]\n\u2264 6(2 + \u03bd) 2k\nD + 3`2r\n\u221a 2mk\nD3 +\n48(` \u221a r + \u03bd)m`4r2\n\u03b12(\u03b1\u2212 2\u03bd \u2212 2`)D2 \u2264 O ( k\nD\n) .\nAgain, from zi \u2212\u03a3\u2032i,ixi = \u3008ei, x\u3009xi + \u03bexi xi + ax,\u03be\u03c6\u03b1(\u3008ei, x\u3009 + \u03bd), following the exact same calculation as in Lemma 6, but using Lemma 22 instead of Lemma 13, we obtain the result.\nDefinition 5 ((\u03b31, \u03b32)-rounded). A random variable \u03b6 is (\u03b31, \u03b32) rounded if\n\u2016E[\u03b6\u03b6>]\u20162 \u2264 \u03b31, \u2016\u03b6\u20162 \u2264 \u03b32.\nTheorem 26 (Noise). Suppose A0 is (`, \u03c1)-initialization for ` = O(1), \u03c1 = O(\u03c3min(A\u2217)). Suppose that the data is generated from y(t) = A\u2217x(t) + \u03b6(t), where \u03b6(t) is (\u03b31, \u03b32)-rounded, and \u03b32 = O(\u03c3min(A\u2217)).\nThen after poly(D, 1 ) iterations, Algorithm 1 outputs a matrix A such that there exists diagonal matrix \u03a3\u0303 1 2I with\n\u2016A\u2212A\u2217\u03a3\u0303\u20162 = O ( r \u03b32 \u03bb \u03c3max(A \u2217) \u03c3min(A\u2217) + \u221a \u03b31 \u03bb \u221a D k + \u03b5 ) .\nProof of Theorem 26. For notation simplicity, we only consider one stages, and we drop the round number here and let A\u0303 = A(t+1) and A = A(t), and we denote the new decomposition as A\u0303 = A\u2217(\u03a3\u0303 + E\u0303) + N\u0303.\nThus, the decoding of z is given by\nz = \u03c6\u03b1(A \u2020 0(A \u2217x+ \u03b6)).\nBy Lemma 24, there exists a matrix R such that \u2016R\u20162 \u2264 32\u2016N0\u20162\u03c3min(A\u2217) with A \u2020 0A \u2217 = (\u03a30 + E0) \u22121 + R. Now let \u03a3\u2032 + E\u2032 = (\u03a30 + E0) \u22121 + R, where \u03a3\u2032 is diagonal and E\u2032 is off-diagonal. Then\nz = \u03c6\u03b1((\u03a3 \u2032 + E\u2032)x+ A\u20200\u03b6)\nwhere\n\u03bd := \u2016A\u20200\u03b6\u2016\u221e \u2264 16\u2016\u03b6\u20162 \u03c3min(A\u2217) \u2264 16\u03b32 \u03c3min(A\u2217) .\nFor simplicity, we only focus on the expected update. The on-line version can be proved directly from this by setting a polynomially small \u03b7. The expected update is given by\nA\u0303 = A + \u03b7E[(A\u2217x+ \u03b6)z> \u2212Azz>].\nTherefore,\nA\u2217(\u03a3\u0303 + E\u0303) + N\u0303 = A + \u03b7E[(A\u2217x+ \u03b6)z> \u2212Azz>] = A\u2217[(\u03a3 + E)(I\u2212 \u03b7E[zz>]) + \u03b7E[xz>]] + N(I\u2212 \u03b7E[zz>]) + E[\u03b6z>].\nSo we still have\n\u03a3\u0303 + E\u0303 = (\u03a3 + E)(I\u2212 \u03b7E[zz>]) + \u03b7E[xz>], N\u0303 = N(I\u2212 \u03b7E[zz>]) + E[\u03b6z>].\nBy Lemma 25, \u03a3\u0303 + E\u0303 = (\u03a3 + E)(I\u2212 \u03b7\u03a3\u2032\u2206\u03a3\u2032) + \u03b7\u2206\u03a3\u2032 + C1\nN\u0303 = N(I\u2212 \u03b7\u03a3\u2032\u2206\u03a3\u2032) + E[\u03b6z>] + NC2.\nwhere \u2016C1\u20162, \u2016C2\u20162 \u2264 C3, and\nC3 = (\u03bd + \u03b2) kr D + m`4r2 \u03b13D2 + `2 \u221a kmr1.5 D1.5\u03b2 + `4r3m \u03b22D2 + `5r2.5m D2\u03b12\u03b2 .\nFirst, consider the update on \u03a3\u0303+ E\u0303. By a similar argument as in Lemma 11, we know that as long as C3 = O ( k D\u03bb\u2016E0\u20162 ) and \u03bd = O(`), we can reduce the norm of E by a constant factor in polynomially many iterations. To satisfy the requirement on C3, we will choose \u03b1 = 1/4, \u03b2 = \u03bbr . Then to make the terms in C3 small, m is set as follows.\n1. Second term:\nm \u2264 Dk\u03bb r2 .\n2. Third term:\nm \u2264 D\u03bb 4k\nr5 .\n3. Fourth term:\nm \u2264 D\u03bb 3k\nr5 .\n4. Fifth term:\nm \u2264 D\u03bb 2k\nr3.5 .\nThis implies that after poly( 1\u03b5 ) stages, the final E will have\n\u2016E\u20162 = O (\nr\u03b32 \u03bb\u03c3min(A\u2217) + \u03b5\n) .\nNext, consider the update on N\u0303. Since the chosen value satisfies C3 \u2264 12\u03c3min(\u03a3 \u2032\u2206\u03a3\u2032), we have\n\u2016N\u0303\u20162 \u2264 max { \u2016N0\u20162,\n2\u2016E[\u03b6z>]\u20162 \u03c3min(\u03a3\u2032\u2206\u03a3\u2032)\n} .\nFor the term E[\u03b6z>], we know that for every vectors u, v with norm 1,\nu>E[\u03b6z>]v \u2264 E[|\u3008u, \u03b6\u3009||\u3008z, v\u3009|].\nSince z is non-negative, we might without loss of generality assume that v is all non-negative, and obtain\nE[|\u3008u, \u03b6\u3009||\u3008z, v\u3009|] \u2264 \u221a E[\u3008u, \u03b6\u30092]E[\u3008z, v\u30092] \u2264 \u221a E[\u3008u, \u03b6\u30092] \u221a max i\u2208[D] E[z2i ] = O (\u221a k\u03b31 D ) .\nPutting everything together and applying Corollary 12 across stages complete the proof.\nProof of Theorem 5. The theorem follows from Theorem 26 and noting that \u221a \u03b31 \u03bb \u221a D k is smaller than r \u03b32 \u03bb \u03c3max(A \u2217) \u03c3min(A\u2217)\nin order."}, {"heading": "B. Additional Experiments", "text": "Here we provide additional experimental results. The first set of experiments in Section B.1 evaluates the performance of our algorithm in the presence of weak initialization, since for our theoretical analysis a warm start is crucial for the convergence. It turns out that our algorithm is not very sensitive to the warm start; even if there is a lot of noise in the initialization, it still produces reasonable results. This allows it to be used in a wide arrange of applications where a strong warm start is hard to achieve.\nThe second set of experiments in Section B.2 evaluates the performance of the algorithm when the weight x has large sparsity. Note that our current bounds have a slightly strong dependency on the `1 norm of x. We believe that this is only because we want to make our statement as general as possible, making only assumptions on the first two moments of x. If in addition, for example, x is assumed to have nice third moments, then our bound can be greatly improved. Here we show that empirically, our algorithm indeed works for typical distributions with large sparsity.\nThe final set of experiments in Section B.3 applies our algorithm on typical real world applications of NMF. In particular, we consider topic modeling on text data and component analysis for image data, and compare our method to popular existing methods."}, {"heading": "B.1. Robustness to Initializations", "text": "In all the experiments in the main text, the initialization matrix A0 is set to A0 = A\u2217(I + U) where I is the identity matrix and U is a matrix whose entries are i.i.d. samples from the uniform distribution on [\u22120.05, 0.05]. Note that this is a very weak initialization, since [A0]i = (1 + Ui,i)[A\u2217]i + \u2211 j 6=i Uj,i[A\n\u2217]j and the magnitude of the noise component\u2211 j 6=i Uj,i[A \u2217]j can be larger than the signal part (1 + Ui,i)[A\u2217]i.\nHere, we further explore even worse initializations: A0 = A\u2217(I + U) + N where I is the identity matrix, U is a matrix whose entries are i.i.d. samples from the uniform distribution on [\u22120.05, 0.05] \u00d7 rl for a scalar rl, N is an additive error matrix whose entries are i.i.d. samples from the uniform distribution on [\u22120.05, 0.05] \u00d7 rn for a scalar rn. Here, we call U the in-span noise and N the out-of-span noise, since they introduce noise in or out of the span of A\u2217.\nWe varied the values of rl or rn, and found that even when U violates our assumptions strongly, or the column norm of N becomes as large as the column norm of the signal A\u2217, the algorithm can still recover the ground-truth up to small relative error. Figure 3(a) shows the results for different values of rl. Note that when rl = 1, the in-span noise already violates our assumptions, but as shown in the figure, even when rl = 2, the ground-truth can still be recovered, though at a slower yet exponential rate. Figure 3(b) shows the results for different values of rn. For these noise values, the column norm of the noise matrix N is comparable or even larger than the column norm of the signal A\u2217, but as shown in the figure, such noise merely affects on the convergence."}, {"heading": "B.2. Robustness to Sparsity", "text": "We performed experiments on the DIR data with different sparsity. In particular, construct a 100\u00d7 5000 matrix X, where each column is drawn from a Dirichlet prior D(\u03b1) on d = 100 dimension, where \u03b1 = (\u03b1/d, \u03b1/d, . . . , \u03b1/d) for a scalar \u03b1. Then the dataset is Y = A\u2217X. We varied the \u03b1 parameter of the prior to control the expected support sparsity, and ran the algorithm on the data generated.\nFigure 4 shows the results. For \u03b1 as large as 20, the algorithm still converges to the ground-truth in exponential rate. When \u03b1 = 80 meaning that the weight vectors (columns in X) have almost full support, the algorithm still produces good results, stabilizing to a small relative error at the end. This demonstates that the algorithm is not sensitive to the support sparsity of the data."}, {"heading": "B.3. Qualitative Results on Some Real World Applications", "text": "We applied our algorithm to two popular applications with real world data to demonstrate the applicability of the method to real world scenarios. Note that the evaluations here are qualitative, due to that the guarantees for our algorithm is the convergence to the ground-truth, while there are no predefined ground-truth for these datasets in practice. Quantitative studies using other criteria computable in practice are left for future work."}, {"heading": "B.3.1. TOPIC MODELING", "text": "Here our method is used to compute 10 topics on the 20newsgroups dataset, which is a standard dataset for the topic modeling setting. Our algorithm is initialized with 10 random documents from the dataset, and the hyperparameters like learning rate are from the experiments in the main text. Note that better initialization is possible, while here we keep things simple to demonstrate the power of the method.\nTable 1 shows the results of the NMF method and the LDA method in the sklearn package,10 and the result of our AND method. It shows that our method indeed leads to reasonable topics, with quality comparable to well implemented popular methods tuned to this task.\n10http://scikit-learn.org/\nB.3.2. IMAGE DECOMPOSITION\nHere our method is used to compute 6 components on the Olivetti faces dataset, which is a standard dataset for image decomposition. Our algorithm is initialized with 6 random images from the dataset, and the hyperparameters like learning rate are from the experiments in the main text. Again, note that better initialization is possible, while here we keep things simple to demonstrate the power of the method.\nFigure 5 shows some examples from the dataset, the result of our AND method, and 6 other methods using the implementation in the sklearn package. It can be observed that our method can produce meaningful component images, and the non-negative matrix factorization implementation from sklearn produces component images of similar quality. The results of these two methods are generally better than those by the other methods."}], "references": [{"title": "Two svds suffice: Spectral decompositions for probabilistic topic modeling and latent dirichlet allocation", "author": ["A. Anandkumar", "S. Kakade", "D. Foster", "Y. Liu", "D. Hsu"], "venue": "Technical report,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "Learning latent bayesian networks and topic models under expansion constraints", "author": ["A. Anandkumar", "D. Hsu", "A. Javanmard", "S. Kakade"], "venue": "In ICML,", "citeRegEx": "Anandkumar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2013}, {"title": "Learning topic models \u2013 going beyond svd", "author": ["S. Arora", "R. Ge", "A. Moitra"], "venue": "In FOCS,", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "A practical algorithm for topic modeling with provable guarantees", "author": ["S. Arora", "R. Ge", "Y. Halpern", "D. Mimno", "A. Moitra", "D. Sontag", "Y. Wu", "M. Zhu"], "venue": "In ICML,", "citeRegEx": "Arora et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2013}, {"title": "Simple, efficient, and neural algorithms for sparse coding", "author": ["S. Arora", "R. Ge", "T. Ma", "A. Moitra"], "venue": "In COLT,", "citeRegEx": "Arora et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2015}, {"title": "Computing a nonnegative matrix factorization\u2013 provably", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Kannan", "Ravindran", "Moitra", "Ankur"], "venue": "In STOC, pp. 145\u2013162", "citeRegEx": "Arora et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2012}, {"title": "Provable algorithms for inference in topic models", "author": ["Arora", "Sanjeev", "Ge", "Rong", "Koehler", "Frederic", "Ma", "Tengyu", "Moitra", "Ankur"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Arora et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arora et al\\.", "year": 2016}, {"title": "On some provably correct cases of variational inference for topic models", "author": ["Awasthi", "Pranjal", "Risteski", "Andrej"], "venue": "In NIPS, pp. 2089\u20132097,", "citeRegEx": "Awasthi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2015}, {"title": "Non-negative matrix factorization under heavy noise", "author": ["Bhattacharyya", "Chiranjib", "Goyal", "Navin", "Kannan", "Ravindran", "Pani", "Jagdeep"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Bhattacharyya et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bhattacharyya et al\\.", "year": 2016}, {"title": "Correlated topic models", "author": ["Blei", "David", "Lafferty", "John"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Blei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2006}, {"title": "Probabilistic topic models", "author": ["Blei", "David M"], "venue": "Communications of the ACM,", "citeRegEx": "Blei and M.,? \\Q2012\\E", "shortCiteRegEx": "Blei and M.", "year": 2012}, {"title": "Hierarchical als algorithms for nonnegative matrix and 3d tensor factorization", "author": ["Cichocki", "Andrzej", "Zdunek", "Rafal", "Amari", "Shun-ichi"], "venue": "In International Conference on Independent Component Analysis and Signal Separation,", "citeRegEx": "Cichocki et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2007}, {"title": "Topic discovery through data dependent and random projections", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "arXiv preprint arXiv:1303.3664,", "citeRegEx": "Ding et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2013}, {"title": "Efficient distributed topic modeling with provable guarantees", "author": ["W. Ding", "M.H. Rohban", "P. Ishwar", "V. Saligrama"], "venue": "In AISTAT, pp", "citeRegEx": "Ding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2014}, {"title": "Online learning for latent dirichlet allocation. In advances in neural information processing", "author": ["Hoffman", "Matthew", "Bach", "Francis R", "Blei", "David M"], "venue": null, "citeRegEx": "Hoffman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2010}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method", "author": ["Kim", "Hyunsoo", "Park", "Haesun"], "venue": "SIAM journal on matrix analysis and applications,", "citeRegEx": "Kim et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2008}, {"title": "Unsupervised learning by convex and conic coding", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1997}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Algorithms for non-negative matrix factorization", "author": ["Lee", "Daniel D", "Seung", "H Sebastian"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2001}, {"title": "Recovery guarantee of non-negative matrix factorization via alternating updates", "author": ["Li", "Yuanzhi", "Liang", "Yingyu", "Risteski", "Andrej"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Overlapping community detection at scale: a nonnegative matrix factorization approach", "author": ["Yang", "Jaewon", "Leskovec", "Jure"], "venue": "In Proceedings of the sixth ACM international conference on Web search and data mining,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "(Anandkumar et al., 2012)), which undermine their applicability in practice.", "startOffset": 0, "endOffset": 25}, {"referenceID": 19, "context": "This problem is poorly understood, with only a few provable guarantees known (Awasthi & Risteski, 2015; Li et al., 2016).", "startOffset": 77, "endOffset": 120}, {"referenceID": 19, "context": "Most of the previous theoretical approaches for analyzing alternating between decoding and encoding, such as (Awasthi & Risteski, 2015; Li et al., 2016; Arora et al., 2015), require the coordinates of x to be pairwiseindependent, or almost pairwise-independent (meaning E\u03bc[xixj ] \u2248 E\u03bc[xi]E\u03bc[xj ]).", "startOffset": 109, "endOffset": 172}, {"referenceID": 4, "context": "Most of the previous theoretical approaches for analyzing alternating between decoding and encoding, such as (Awasthi & Risteski, 2015; Li et al., 2016; Arora et al., 2015), require the coordinates of x to be pairwiseindependent, or almost pairwise-independent (meaning E\u03bc[xixj ] \u2248 E\u03bc[xi]E\u03bc[xj ]).", "startOffset": 109, "endOffset": 172}, {"referenceID": 19, "context": "Inverse decoding was also used in (Li et al., 2016; Arora et al., 2015; 2016).", "startOffset": 34, "endOffset": 77}, {"referenceID": 4, "context": "Inverse decoding was also used in (Li et al., 2016; Arora et al., 2015; 2016).", "startOffset": 34, "endOffset": 77}, {"referenceID": 6, "context": "It was also observed in (Arora et al., 2016) that Moore-Penrose", "startOffset": 24, "endOffset": 44}, {"referenceID": 11, "context": ", (Kim & Park, 2008; Lee & Seung, 2001; Cichocki et al., 2007; Ding et al., 2013; 2014).", "startOffset": 2, "endOffset": 87}, {"referenceID": 12, "context": ", (Kim & Park, 2008; Lee & Seung, 2001; Cichocki et al., 2007; Ding et al., 2013; 2014).", "startOffset": 2, "endOffset": 87}, {"referenceID": 8, "context": ", 2012b) also studied NMF under separability assumptions about the features, and (Bhattacharyya et al., 2016) studied NMF under related assumptions.", "startOffset": 81, "endOffset": 109}, {"referenceID": 19, "context": "The most related work is (Li et al., 2016), which analyzed an alternating minimization type algorithm.", "startOffset": 25, "endOffset": 42}, {"referenceID": 1, "context": "Recently, there is a line of theoretical work analyzing tensor decomposition (Arora et al., 2012a; 2013; Anandkumar et al., 2013) or combinatorial methods (Awasthi & Risteski, 2015).", "startOffset": 77, "endOffset": 129}, {"referenceID": 19, "context": "Since A and A\u2217 are in the same subspace, inspired by (Li et al., 2016) we can write A\u2217 as A(\u03a3 + E) for a diagonal matrix \u03a3 and an off-diagonal matrix E, and thus the decoding becomes z = \u03c6\u03b1(\u03a3x + Ex).", "startOffset": 53, "endOffset": 70}, {"referenceID": 3, "context": "In particular, we used the matrix with 100 topics computed by the algorithm in (Arora et al., 2013) on the NIPS papers dataset (about 1500 documents, average length about 1000).", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "We compare the algorithm AND to the following popular methods: Alternating Non-negative Least Square (ANLS (Kim & Park, 2008)), multiplicative update (MU (Lee & Seung, 2001)), LDA (online version (Hoffman et al., 2010)),7 and Hierarchical Alternating Least Square (HALS (Cichocki et al.", "startOffset": 196, "endOffset": 218}, {"referenceID": 11, "context": ", 2010)),7 and Hierarchical Alternating Least Square (HALS (Cichocki et al., 2007)).", "startOffset": 59, "endOffset": 82}], "year": 2017, "abstractText": "Non-negative matrix factorization is a basic tool for decomposing data into the feature and weight matrices under non-negativity constraints, and in practice is often solved in the alternating minimization framework. However, it is unclear whether such algorithms can recover the groundtruth feature matrix when the weights for different features are highly correlated, which is common in applications. This paper proposes a simple and natural alternating gradient descent based algorithm, and shows that with a mild initialization it provably recovers the ground-truth in the presence of strong correlations. In most interesting cases, the correlation can be in the same order as the highest possible. Our analysis also reveals its several favorable features including robustness to noise. We complement our theoretical results with empirical studies on semi-synthetic datasets, demonstrating its advantage over several popular methods in recovering the groundtruth.", "creator": "LaTeX with hyperref package"}}}