{"id": "1706.04125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Online Learning for Structured Loss Spaces", "abstract": "others consider statistical with expert advice when the loss vectors are assumed consistently lie in a set described by alternating sum of atomic tensor calculations. statistics pose a bounded bound for binary generic version of the elastic mirror distribution ( spp ) algorithm below uses a collection of regularizers, each quadrant properly compute constituent fault norms. the general result recovers standard omd regret bounds, and avoids smooth bounds for additive structured settings where smooth loss vectors sampled ( i ) noisy versions of points comprising a th - rank subspace, ( ii ) compact dir corrupted smoothing noise, and ( iii ) sparse perturbations of low - rank domains. unlike the indifference graph effectively learning optimal structured losses, we also show lower taxes on regret constraint properly combining risk and sparsity with the parameter stream between the optimal vectors, moreover avoids lower bounds for sampling above additive exit strategies as well.", "histories": [["v1", "Tue, 13 Jun 2017 15:31:22 GMT  (26kb)", "http://arxiv.org/abs/1706.04125v1", "23 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["siddharth barman", "aditya gopalan", "aadirupa saha"], "accepted": false, "id": "1706.04125"}, "pdf": {"name": "1706.04125.pdf", "metadata": {"source": "CRF", "title": "Online Learning for Structured Loss Spaces", "authors": ["Siddharth Barman", "Aditya Gopalan", "Aadirupa Saha"], "emails": ["barman@csa.iisc.ernet.in", "aditya@ece.iisc.ernet.in", "aadirupa.saha@csa.iisc.ernet.in"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n04 12\n5v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n17\nWe consider prediction with expert advice when the loss vectors are assumed to lie in a set described by the sum of atomic norm balls. We derive a regret bound for a general version of the online mirror descent (OMD) algorithm that uses a combination of regularizers, each adapted to the constituent atomic norms. The general result recovers standard OMD regret bounds, and yields regret bounds for new structured settings where the loss vectors are (i) noisy versions of points from a low-rank subspace, (ii) sparse vectors corrupted with noise, and (iii) sparse perturbations of low-rank vectors. For the problem of online learning with structured losses, we also show lower bounds on regret in terms of rank and sparsity of the source set of the loss vectors, which implies lower bounds for the above additive loss settings as well."}, {"heading": "1 Introduction", "text": "Online learning problems, such as predictionwith expert advice [Cesa-Bianchi and Lugosi, 2006] and online convex optimization [Zinkevich, 2003], involve a learner who sequentially makes decisions from a decision set. The learner seeks to minimize her total loss over a sequence of loss functions, unknown at the beginning, but which is revealed causally. Specifically, she attempts to achieve low regret, for each sequence in a class of loss sequences, with respect to the best single decision point in hindsight.\nThe theory of online learning, by now, has yielded flexible and elegant algorithmic techniques that enjoy provably sublinear regret in the time horizon of plays. Regret bounds for online learning algorithms typically hold across inputs (loss function sequences) that have little or no structure. For instance, for the prediction with experts problem, the exponentially weighted forecaster [Cesa-Bianchi and Lugosi, 2006] is known to achieve an expected regret of O( \u221a T lnN) over any sequence of N -dimensional loss vectors with coordinates bounded in [0, 1]; here T is the number of rounds of play.\nThere is often, however, more geometric structure in the input in online learning problems, beyond elementary \u2113\u221e-type constraints, which a learner, with a priori knowledge, can hope to exploit and improve her performance. A notable example is when the loss vectors for the prediction with experts problem come from a low-dimensional subspace [Hazan et al., 2016]. This is often the case in recommender systems which are based on latent factor models [Koren et al., 2009], where users and items are represented in terms of their features or attribute vectors, typically of small dimension.\n\u2217Indian Institute of Science. barman@csa.iisc.ernet.in \u2020Indian Institute of Science. aditya@ece.iisc.ernet.in \u2021Indian Institute of Science. aadirupa.saha@csa.iisc.ernet.in\nUnder a bilinear model for the utility of a user-item pair, each user\u2019s utility across all items becomes a vector from a subspace with dimension at most that of the feature vectors. Hazan et al. [2016] show that the learner can limit her regret to O( \u221a dT ) when each loss vector comes from a d-dimensional subspace of RN . If d \u226a N (in fact, d \u2264 lnN ), then, this confers an advantage over using a more general best experts algorithm like Exponential Weights.\nThis example is interesting not only because it shows that geometric/structural properties known in advance can help the learner achieve order-wise better regret, but also because it opens up the possibility of studying whether other, arguably more realistic, forms of structure can be exploited, such as sparsity in the input (or more generally small norm) and, more importantly, \u201cadditive\u201d combinations of such structures, e.g., low-rank losses added with losses of small \u21132-norm, which expresses losses that are noisy perturbations of a low-dimensional subspace. In this paper, we take a step in this direction and develop a framework for online learning problems with structured losses.\nOur Results and Techniques: We consider the prediction with experts problemwith loss sequences in which each element (loss vector) belongs to a set that respects structural constraints. Specifically, we assume that the loss vectors belong to a sum of atomic norm balls1 [Chandrasekaran et al., 2012], say A + B, where the sum of sets is in the Minkowski sense.2 For this setup\u2014which we call online learning with additive loss spaces\u2014we show a general regret guarantee for an online mirror descent (OMD) algorithm that uses a combination of regularizer functions, each of which is adapted to a constituent atomic norms of A and B, respectively.\nSpecializing this result for a variety of loss function sets recovers standard OMD regret guarantees for strongly convex regularizers [Shalev-Shwartz, 2012b], and subsumes a result of Hazan et al. [2016] for the online low-rank problem. But more importantly, this allows us to obtain \u201cnew results from old\u201d\u2014regret guarantees for settings such as noisy low rank (where losses are perturbations from a low-dimensional subspace), noisy sparse (where losses are perturbations of sparse vectors), and sparse low-rank (where losses are sparse perturbations from a low-dimensional subspace); see Tables 1 and 2.\nAnother contribution of this work is to show lower bounds on regret for the online learning problem with structured losses. We derive a generic lower bound on regret, for any algorithm for the prediction with experts problem, using structured (in terms of sparsity and dimension) loss vectors. This result allows us to derive regret lower bounds in a variety of individual and additive loss space settings including sparse, noisy, low rank, noisy low-rank, and noisy sparse losses.\nRelated work. The work that is perhaps closest in spirit to ours is that of Hazan et al. [2016], who study the best experts problem when the loss vectors all come from a low-dimensional subspace of the ambient space. A key result of theirs is that the online mirror descent (OMD) algorithm, used with a suitable regularization, improves the regret to depend only on the low rank and not the ambient dimension. More broadly, OMD theory provides regret bounds depending on properties of the regularizer and the geometry of the loss and decision spaces [Shalev-Shwartz, 2012a]. In this work, we notably generalize this to the setup of additive losses.\nStructured online learning has been studied in the recent past from the point of view of overall sequence complexity or \u201chardness,\u201d also informally called learning with \u201ceasy data.\u201d This includes\n1centrally symmetric, convex, compact sets with their centroids at the origin. 2A+B = {a+ b : a \u2208 A, b \u2208 B}.\nwork that shows algorithms enjoying first- and second-order regret bounds [Cesa-Bianchi et al., 2007] and quadratic variation-based regret bounds [Hazan and Kale, 2010; Steinhardt and Liang, 2014]. There is also recent work on achieving regret scaling with the covering number of the sequence of observed loss vectors [Cohen and Mannor, 2017], which is another measure of easy data.\nOur problem formulation, it can be argued, explores a different formulation of learningwith \u201ceasy data,\u201d in which the adversary, instead of being constrained to choose loss sequences with low total magnitude or variation, is limited to choosing loss vectors from sets with enough geometric structure (e.g., from particular atomic norm balls)."}, {"heading": "2 Notation and Preliminaries", "text": "For an integer n \u2208 Z+, we use [n] to denote the set {1, 2, . . . n}. For a vector x \u2208 Rn, xi denotes the ith component of x. The p-norm of x is defined as \u2016x\u2016p = ( \u2211n i=1 |xi|p)\n1/p, 0 \u2264 p < \u221e. Write \u2016x\u2016\u221e := maxni=1 |xi| and \u2016x\u20160 := |{i | xi 6= 0}|. If \u2016\u00b7 \u2016 is a norm defined on a closed convex set \u2126 \u2286 Rn, then its corresponding dual norm is defined as\n\u2016u\u2016\u2217 = sup x\u2208\u2126:\u2016x\u2016\u22641 x \u00b7 u ,\nwhere x \u00b7 u = \u2211i xiui is the standard inner product in Euclidean space. It follows that the dual of the standard p-norm (p \u2265 1) is the q-norm, where q is the Ho\u0308lder conjugate of p, i.e., 1p + 1q = 1. The n-probability simplex is defined as \u2206n = {x \u2208 [0, 1]n | \u2211n i=1 xi = 1}. Given any setA \u2286 Rn, we denote the convex hull ofA as conv(A). Clearly, when A = {e1, e2, . . . en}, conv (A) = \u2206n, where ei \u2208 [0, 1]n denotes ith standard basis vector of Rn."}, {"heading": "2.1 Atomic Norm and its Dual [Chandrasekaran et al., 2012]", "text": "Next we define the notion of an atomic norm along with its dual. These concepts will provide us with a unified framework for addressing structured loss spaces, and will be used extensively in the paper. Let A \u2286 Rn be a set which is convex, compact, and centrally symmetric about the origin (i.e., a \u2208 A if and only if \u2212a \u2208 A).\nThe atomic norm induced by the set A is defined as\n||x||A := inf{t > 0 | x \u2208 tA}, for x \u2208 Rn.\nThe dual of the atomic norm induced byA becomes the support function ofA [Boyd and Vandenberghe, 2004]; formally,\n||x||\u2217A := sup{x.z | z \u2208 A}, for x \u2208 Rn.\nFor example, if the setA is the convex hull of all unit-norm one-sparse vectors, i.e.,A := conv ({\u00b1ei}ni=1), then the corresponding atomic norm is the standard \u21131-norm \u2016\u00b7 \u20161."}, {"heading": "2.2 Problem setup", "text": "We consider the online learning problem of learning with expert advice from a collection ofN experts [Cesa-Bianchi and Lugosi, 2006]. In each round t = 1, 2, . . . , T , the learner receives advice from each\nof the N experts, following which the learner selects an expert from a distribution pt \u2208 \u2206N , maintained over the experts, whose advice is to be followed. Upon this, the adversary reveals the losses incurred by the N experts, lt = (lt(1), lt(2), . . . lt(N)) \u2208 [0, 1]N , lt(i) being the loss incurred by the ith expert. The learner suffers an expected loss of EIt\u223cpt [lt(It)] = \u2211N i=1 pt(i)lt(i). If the game is played for a total of T rounds, then the objective of the learner is to minimize the expected cumulative regret defined as:\nE [ RegretT ] = T\u2211\nt=1\npt.lt \u2212 min i\u2208[N ]\nT\u2211\nt=1\nlt(i).\nIt is well-known that without any further assumptions over the losses lt, the best achievable\nregret for this problem is \u0398( \u221a T lnN) \u2013 the exponential weights algorithm or the Hedge algorithm achieves regret O( \u221a T lnN) [Arora et al., 2012, Theorem 2.3], and a matching lower bound exists as well [Cesa-Bianchi and Lugosi, 2006, Theorem 3.7].\nNow, a very natural question to ask is: can a better (smaller) regret be achieved if the loss sequence has more structure? Suppose the loss vectors (lt) T t=1 all belong to a common structured loss space L \u2286 [0, 1]N , such as: 1. Sparse loss space: L = {l \u2208 [0, 1]N | \u2016l\u20160 = s}. Here, s \u2208 [N ] is the sparsity parameter.\n2. Spherical loss space: L = {l \u2208 [0, 1]N | \u2016l\u2016A = l\u22a4Al \u2264 \u01eb}, where A is a positive definite matrix and \u01eb > 0.\n3. Noisy loss space: L = {l \u2208 [0, 1]N | \u2016l\u201622 = \u01eb}, \u01eb > 0}. Note that noisy losses are a special class of spherical losses whereA = IN , the identity matrix.\n4. Low-rank loss space: L = {l \u2208 [0, 1]N | l = Uv }, where the rank of matrix U \u2208 RN\u00d7d is equal to d \u2208 [N ] and vector v \u2208 Rd (as mentioned previously, such loss vectors were considered by Hazan et al. [2016]).\n5. Additive loss space: L = L1 + L2 (Minkowski Sum). More formally, L = {l = l1 + l2 | l1 \u2208 L1 and l2 \u2208 L2}, where L1 \u2286 [0, 1]N and L2 \u2286 [0, 1]N are structured loss spaces themselves.3 Examples include any combination of the previously described loss spaces, such as the low-\nrank + noisy space.\nClearly, using the Exponential Weight or Hedge algorithm, one can always achieve O( \u221a T lnN) regret in the above settings. The relevant question is whether the geometry of such loss spaces can be exploited, in a principled fashion, to achieve improved regret guarantees (possibly independent of lnN )? In other words, can we come up with algorithms for above cases such that the regret is O( \u221a \u03c9T ), where \u03c9 < lnN?\nWe will show that, for all of the above loss spaces, we can obtain a regret factor \u03c9 which is orderwise better than lnN . In particular, we will establish these regret bounds by employing the Online Mirror Descent algorithm (described below) with a right choice of atomic norms. Furthermore, using this algorithm, we will also develop a framework to obtain new regret bounds from old. That is, we show that if we have an online mirror descent setup for L1 and L2, then we can in fact obtain a low-regret algorithm for the additive loss space L1 + L2.\n3Note that, in the problem setup at hand the learner observes only the loss vectors lt, and does not have access to the\nloss components l1t or l2t."}, {"heading": "2.3 Online Mirror Descent", "text": "In this section, we give a brief introduction to the Online Mirror Descent (OMD) algorithm [Bubeck, 2011; Shalev-Shwartz, 2012a], which is a subgradient descent based method for online convex optimization with a suitably chosen regularizer. A reader well-versed with the analysis of OMD may skip to the statement of Theorem 3 and proceed to Section 3.\nOMD generalizes the basic mirror descent algorithm used for offline optimization problems (see, e.g., Beck and Teboulle [2003]). Before detailing the algorithm, we will recall a few relevant definitions:\nDefinition 1. Bregman Divergence. Let \u2126 \u2208 Rn be a convex set, and f : \u2126\u2192R be a strictly convex and differentiable function. Then the Bregman divergence associated with f , denoted by Bf : \u2126 \u00d7 \u2126\u2192R, is defined as\nBf (u,v) := f(u)\u2212 f(v)\u2212 (u\u2212 v) \u00b7 \u2207f(v), for u,v \u2208 \u2126 .\nDefinition 2. Strong Convexity (see, e.g., Shalev-Shwartz [2012a] and Bubeck [2011]) Let \u2126 \u2208 Rn be a convex set, and f : \u2126\u2192R be a differentiable function. Then f is called \u03b1-strongly convex over \u2126 with respect to the norm \u2016 \u00b7 \u2016 iff for all x,y \u2208 \u2126,\nf(x)\u2212 f(y)\u2212 (\u2207f(y))T (x\u2212 y) \u2265 \u03b1 2 \u2016x\u2212 y\u20162.\nEquivalently, a continuous twice differentiable function, f , over \u2126 is said to be \u03b1-strongly convex iff for all x,w \u2208 \u2126 we have\nxT\u22072f(w)x \u2265 \u03b1\u2016x\u20162.\nWe now describe the OMD algorithm instantiated to the problem setup given in Section 2.2.\nAlgorithm 1 Online Mirror Descent (OMD)\n1: Parameters: Learning rate \u03b7 > 0. 2: Convex set \u2126 \u2286 RN , such that\u2206N \u2286 \u2126 3: Strictly convex, differentiable function R : \u2126\u2192R 4: Initialize: p1 = argmin\np\u2208\u2206N R(p)\n5: for t = 1, 2, \u00b7 \u00b7 \u00b7 T do 6: Play pt \u2208 \u2206N 7: Receive loss vector lt \u2208 [0, 1]N 8: Incur loss pt.lt 9: Update:\n10: \u2207R(p\u0303t+1) \u2190 \u2207R(pt)\u2212 \u03b7lt (Assume this yields p\u0303t+1 \u2208 \u2126) 11: pt+1 \u2190 argmin\np\u2208\u2206N BR(p, p\u0303t+1)\n12: end for\nThe regret guarantee of the above algorithm is as follows:\nTheorem 3 (OMD regret bound (Theorem 5.2, Bubeck [2011])). Let the loss vectors, {lt}Tt=1, belong to a loss space L \u2286 [0, 1]N , which is bounded with respect to a (arbitrary) norm \u2016\u00b7 \u2016; in particular, for any l \u2208 L we have \u2016l\u2016 \u2264 G. Furthermore, let \u2126 \u2287 \u2206N be a convex set, and R : \u2126\u2192R be a strictly convex, differentiable function that satisfies R(p) \u2212 R(p1) \u2264 D2 for parameter D \u2208 R and all p \u2208 \u2206N ; where p1 := argminp\u2208\u2206N R(p). Also, let the restriction of R to \u2206N be \u03b1-strongly convex with respect to \u2016\u00b7 \u2016\u2217, the dual norm of \u2016\u00b7 \u2016.\nThen, the regret of OMD algorithm with set \u2126, regularizer function R, and learning rate \u03b7 > 0, for T\nrounds satisfies\nRegretT (OMD(\u03b7 \u2217)) =\nT\u2211\nt=1\npt.lt \u2212 N min i=1\nT\u2211\nt=1\nlt(i) \u2264 1\n\u03b7\n( D2 + \u03b72G2T\n2\u03b1\n) ,\nwhere p1,p2, . . .pT denotes the sequential predictions of the algorithm in T rounds. Moreover, setting \u03b7 \u2217 = D G \u221a 2\u03b1 T (i.e., minimizing the right-hand-side of the above bound), we have\nRegretT (OMD(\u03b7 \u2217)) \u2264 DG\n\u221a 2T\n\u03b1 .\nFor completeness, a proof of the above theorem appears in Appendix A."}, {"heading": "3 Online Mirror Descent for Structured Losses", "text": "This section shows that, for specific structured loss spaces, instantiating the OMD algorithm\u2014with a right choice of the norm \u2016 \u00b7 \u2016 and regularizer R\u2014leads to improved (over the standard O( \u221a 2T lnN) bound) regret guarantees. The proofs of these results appear in Appendix B.\n1. Sparse loss space: L = {l \u2208 [0, 1]N | \u2016l\u20160 = s}, s \u2208 [N ] being the loss sparsity parameter. Then\nusing q-norm, R(x) = \u2016x\u20162q = (\u2211N i=1(x q i ) ) 2 q , where q = ln s \u2032 ln s\u2032\u22121 , s \u2032 = (s + 1)2, as the regularizer, we get,\nRegretT \u2264 2 \u221a ln(s+ 1)T .\n2. Spherical loss space: L = {l \u2208 [0, 1]N | \u2016l\u20162A = l\u22a4Al \u2264 \u01eb}, whereA is a positive definite matrix, \u01eb > 0. Then using the square of the ellipsoidal norm as the regularizer, R(x) = \u01ebx\u22a4A\u22121x, we get,\nRegretT \u2264 \u221a \u03bbmax(A\u22121)\u01ebT ,\nwhere \u03bbmax(A \u22121) denotes the maximum eigenvalue ofA\u22121.\n3. Noisy loss sapce: L = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb}, \u01eb > 0. Then using the square of the standard Euclidean norm as the regularizer, R(x) = \u01eb\u2016x\u201622, we get,\nRegretT \u2264 \u221a \u01ebT .\nNote that noisy loss is a special case of spherical loss whereA = A\u22121 = IN .\nHazan et al. [2016] have also usedOMD to address the loss vectors that belong to a low-dimensional\nsubspace. Specifically, if the loss space L = {l \u2208 [0, 1]N | l = Uv }, where matrix U \u2208 RN\u00d7d is of rank d and vector v \u2208 Rd, with 1 \u2264 d \u2264 lnN . Then, Hazan et al. [2016] show that the regularizer R(x) = \u2016x\u20162H = x\u22a4Hx (whereH = IN +U\u22a4MU,M is the matrix corresponding to the Lo\u0308wner-John ellipsoid of L and IN is the identity matrix) leads to the following regret bound:\nRegretT \u2264 4 \u221a dT .\nIn addition, for the standard loss space L = [0, 1]n, one can execute the OMD algorithm with the unnormalized negative entropy, R(x) = \u2211N i=1 xi log xi \u2212 \u2211N i=1 xi, as the regularizer, to obtain:\nRegretT \u2264 \u221a 2T lnN .\nNote that the above regret bound is same as that given by Hedge algorithm. In fact, it can be verified that, with the above choice of regularizer, the OMD algorithm exactly reduces down to standard Hedge algorithm (see, e.g., Bubeck [2011])."}, {"heading": "4 Online Learning for Additive Loss Spaces", "text": "We now present a key result of this paper, which enables us to obtain new regret bounds from old. In particular, we will develop a framework that provides a low-regret OMD algorithm for an additive loss space L = L1+L2, using the OMD setup of the constituent loss spacesL1 andL2. Specifically, we detail how to choose an appropriate regularizer for losses from L and, hence, construct a low-regret OMD algorithm.\nTheorem 4. (Main Result) Let L1,L2 \u2286 [0, 1]N be two loss spaces, such that L1 \u2286 A1, L2 \u2286 A2, where A1, A2 \u2208 RN are two centrally symmetric, convex, compact sets. We observe a sequence of loss vectors {lt}Tt=1, such that in any round t \u2208 [T ], lt = l1t+l2t, where l1t \u2208 L1 and l2t \u2208 L2. Consider two differentiable, strictly convex functions R1 : \u21261 7\u2192 R, R2 : \u21262 7\u2192 R, where \u21261,\u21262 \u2287 \u2206N are two convex sets. The restrictions of R1 and R2 to\u2206N are, respectively, \u03b11- and \u03b12-strongly convex with respect to the norms || \u00b7 ||\u2217A1 and || \u00b7 || \u2217 A2 .\nAlso, let parameters D1 and D2 be such that R1(p) \u2212 R1(p1) \u2264 D21 and R2(p) \u2212 R2(p1) \u2264 D22 for all p \u2208 \u2206N ; where p1 := argminp\u2208\u2206N (R1(p) +R2(p)).\nThen (with learning rate \u03b7\u2217 = \u221a (D2 1 +D2\n2 )min(\u03b11,\u03b12) T , regularizer R := R1 + R2, and p1 as the initial\nprediction) the regret of the OMD algorithm is bounded as\nRegretT \u2264 2\n\u221a( D21 +D 2 2 ) T\nmin(\u03b11, \u03b12) .\nA proof of the above theorem appears in Section 4.1.\nRemark 5. In general, the regret guarantee of Theorem 4 is essentially tight. That is, there exist loss spaces L1 and L2 such that OMD algorithm obtained via Theorem 4 provides an order-wise optimal regret bound for the additive loss space L = L1 + L2; see Appendix E for specific examples.\nThe above theorem immediately leads to the following corollary.\nCorollary 6. (New Regret Bounds from Old) Suppose L1,L2 \u2286 [0, 1]N are two loss spaces such that \u2016l\u2016A1 \u2264 1, \u2200l \u2208 L1, and \u2016l\u2016A2 \u2264 1, \u2200l \u2208 L2, where A1,A2 \u2208 RN are two centrally symmetric, convex, compact sets. Also, suppose there exists two strictly convex, differentiable functions R1 : \u21261 7\u2192 R and R2 : \u21262 7\u2192 R, (\u21261,\u21262 \u2287 \u2206N , convex) such that OMD with regularizer functions R1 and R2 gives the regret bounds ofD1 \u221a 2T \u03b11 andD2 \u221a 2T \u03b12\nover loss spaces L1 and L2, respectively. Here, \u03b11 (\u03b12) is the strong convexity parameter of R1 (R2) over \u2206N , with respect to the atomic norm || \u00b7 ||\u2217A1 (|| \u00b7 || \u2217 A2).\nIn addition let, D1 andD2 are parameters such that, for all p \u2208 \u2206N ,\nR1(p)\u2212R1(p\u20321) \u2264 D21 with p\u20321 := argminq\u2208\u2206N R1(q) and R2(p)\u2212R2(p\u20322) \u2264 D22 with p\u20322 = argminq\u2208\u2206N R2(q).\nThen, for the additive loss space L = L1+L2, the OMD algorithm with regularizer functionR = R1+R2, initial prediction p1 = argminp\u2208\u2206N (R1(p) +R2(p)) (and learning rate \u03b7 \u2217 = \u221a (D2 1 +D2 2 )min(\u03b11,\u03b12) T ) leads to the following regret bound:\nRegretT \u2264 2 \u221a (D21 +D 2 2)T\nmin(\u03b11, \u03b12) .\nNote that we can prove this corollary\u2014using Theorem 4\u2014by simlply verifying the following in-\nequalities: R1(p)\u2212R1(p1) \u2264 D21 andR2(p)\u2212R2(p1) \u2264 D22 , for all p \u2208 \u2206N and p1 := argminq\u2208\u2206N (R1(q) +R2(q)). This follows, since R1(p \u2032 1) \u2264 R1(p1) and R2(p\u20322) \u2264 R2(p1); recall that p\u20321 := argminq\u2208\u2206N R1(q) and p\u20322 := argminq\u2208\u2206N R2(q)."}, {"heading": "4.1 Proof of Theorem 4", "text": "Before proceeding to prove the theorem, we will establish the following useful lemmas. Let A1,A2 be any two convex, compact, centrally symmetric subsets of Rn and A = A1 +A2 (Minkowski Sum). Then, note that A is also convex, compact, and centrally symmetric. This follows from the fact that conv(X ) + conv(Y) = conv(X + Y) for any X ,Y \u2282 Rn. In addition, we have\nLemma 7. ||x||A \u2264 max{||x1||A1 , ||x2||A2}, where x = x1 + x2, x1 \u2208 A1, x2 \u2208 A2.\nProof. Recall the definition of atomic norm \u2016 \u00b7 \u2016A from Section 2.1. Suppose for any x = (x1 + x2) \u2208 R n, t1 = ||x1||A1 and t2 = ||x2||A2 . Clearly, x = x1 + x2 \u2208 (t1A1 + t2A2) \u2286 t(A1 + A2), where t = max{t1, t2}. The proof now follows directly from the definition of atomic norm, \u2016x\u2016A.\nLemma 8. ||x||\u2217A \u2264 ||x||\u2217A1 + ||x|| \u2217 A2 , for all x \u2208 R n.\nProof. Consider any x \u2208 Rn,\n||x||\u2217A = sup{x.z | z \u2208 A} = sup{x.(z1 + z2) | z1 \u2208 A1, z2 \u2208 A2} \u2264 sup{x.z1 | z1 \u2208 A1}+ sup{x.z2 | z2 \u2208 A2} \u2264 ||x||\u2217A1 + ||x|| \u2217 A2 .\nLemma 9. Suppose \u2126\u0303 \u2208 Rn be a convex set. Consider two differentiable functions R1 : Rn 7\u2192 R and R2 : R n 7\u2192 R, that are respectively \u03b11 and \u03b12-strongly convex with respect to || \u00b7 ||\u2217A1 and || \u00b7 || \u2217 A2 over \u2126\u0303. Then R = R1 +R2 is \u03b1 = 1 2 min(\u03b11, \u03b12)-strongly convex with respect to || \u00b7 ||\u2217A over \u2126\u0303.\nProof. For any x,y \u2208 \u2126\u0303,\nR(x)\u2212R(y)\u2212\u2207R(y)(y)(x \u2212 y) = R1(x)\u2212R1(y) \u2212\u2207R1(y)(y)(x \u2212 y) +R2(x)\u2212R2(y) \u2212\u2207R2(y)(x \u2212 y)\n= \u03b11 2 \u2016x\u2212 y\u2016\u22172A1 + \u03b12 2 \u2016x\u2212 y\u2016\u22172A2 \u2265 \u03b1 2 (2\u2016x\u2212 y\u2016\u22172A1 + 2\u2016x\u2016 \u22172 A2), (\u03b1 = 1 2 min(\u03b11, \u03b12)) \u2265 \u03b1 2 (\u2016x\u2212 y\u2016\u2217A1 + \u2016x\u2212 y\u2016 \u2217 A2) 2 (since 2(a2 + b2) > (a+ b)2, \u2200a, b \u2208 R) \u2265 \u03b1 2 (\u2016x\u2212 y\u2016\u22172A ) (via Lemma 8) .\nHence, R = R1 + R2 is \u03b1 = 1 2 min(\u03b11, \u03b12)-strongly convex with respect to || \u00b7 ||\u2217A over \u2126\u0303. Similarly, if R1 and R2 are twice continuously differentiable, then for any x,w \u2208 \u2126\u0303.\nxT\u22072R(w)x = xT\u22072(R1 +R2)(w)x = xT\u22072R1(w)x+ xT\u22072R2(w)x \u2265 \u03b11\u2016x\u2016\u22172A1 + \u03b12\u2016x\u2016 \u22172 A2\n\u2265 \u03b1(2\u2016x\u2016\u22172A1 + 2\u2016x\u2016 \u22172 A2), (\u03b1 =\n1 2 min(\u03b11, \u03b12))\n\u2265 \u03b1(\u2016x\u2016\u2217A1 + \u2016x\u2016 \u2217 A2) 2 (Since 2(a2 + b2) > (a+ b)2, \u2200a, b \u2208 R) \u2265 \u03b1(\u2016x\u2016\u2217A)2 (via Lemma 8) .\nThus R is 12 min(\u03b11, \u03b12)-strongly convex with respect to || \u00b7 ||\u2217A over \u2126\u0303.\nProof. of Theorem 4 Consider the norm \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u2016A, and its dual norm \u2016 \u00b7 \u2016\u2217 = \u2016 \u00b7 \u2016\u2217A. Note that:\n1. Lemma 7 along with the bounds \u2016l1\u2016A1 \u2264 1 and \u2016l2\u2016A2 \u2264 1 imply that \u2016l\u2016A \u2264 1, for any l = l1 + l1 \u2208 L. Hence, L \u2286 A.\n2. For any p \u2208 \u2206N , R(p) \u2212 R(p1) = (R1(p) \u2212 R1(p1)) + (R2(p) \u2212 R2(p1)) \u2264 D21 + D22. Hence, D = \u221a D21 +D 2 2. 3. R(x) = R1(x) +R2(x) is min{\u03b11,\u03b12}\n2 -strongly convex with respect to \u2016 \u00b7 \u2016\u2217A, \u2200x \u2208 \u2206N (Lemma 9). Hence, \u03b1 = min{\u03b11,\u03b12}2 .\nThe result now follows by applying Theorem 3."}, {"heading": "4.2 Applications of Theorem 4", "text": "In this section, we will derive novel regret bounds for additive loss spaces (L = L1 + L2) wherein the individual components (L1 and L2) are the loss spaces which were considered in Section 3. These results are derived by applying Theorem 4; details of the proofs appear in Appendix C.\nCorollary 10 (Noisy Low Rank). Suppose L1 = {l \u2208 [0, 1]N | l = Uv } is a d rank loss space (1 \u2264 d \u2264 lnN), perturbed with noisy losses L2 = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb, \u01eb > 0}. Then, the regret of the OMD algorithm over the loss space L = L1 + L2\u2014with regularizer R(x) = x\u22a4Hx + \u01eb\u2016x\u201622 and learning rate \u03b7\u2217 = \u221a 2(16d+\u01eb)\nT \u2014is upper bounded as follows\nRegretT \u2264 \u221a 2(16d + \u01eb)T .\nCorollary 11 (Noisy Sparse). Suppose L1 = {l \u2208 [0, 1]N | \u2016l\u20160 = s} is an s-sparse loss space (s \u2208 [N ]), perturbed with noisy losses from L2 = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb, \u01eb > 0}. Then, the regret of the OMD algorithm over the loss space L = L1 + L2\u2014with regularizer R(x) = \u2016x\u20162q + \u01eb\u2016x\u201622 and learning rate \u03b7\u2217 = \u221a 1+\u01eb\n(2 ln(s+1)\u22121)T \u2014is upper bounded as follows\nRegretT \u2264 2 \u221a 2(1 + \u01eb) ln(s+ 1)T .\nCorollary 12 (Low Rank with Sparse). Suppose L1 = {l \u2208 [0, 1]N | l = Uv } is a d rank loss space (1 \u2264 d \u2264 lnN), perturbed with s-sparse losses L2 = {l \u2208 [0, 1]N | \u2016l\u20160 = s}, s \u2208 [N ]. Then, the regret of the OMD algorithm over the loss space L = L1 + L2\u2014with regularizer R(x) = x\u22a4Hx+ \u2016x\u20162q and learning rate \u03b7\u2217 = \u221a 16d+1\n(2 ln(s+1)\u22121)T \u2014is upper bounded as follows\nRegretT \u2264 2 \u221a 2(16d + 1) ln(s + 1)T ."}, {"heading": "5 Lower Bounds", "text": "In this section we will derive lower bounds for online learning with experts\u2019 advice problem for different structured loss spaces. We first state the lower bound for a general loss space L \u2286 RN ; see Theorem 13. The proof of this theorem appears in Appendix D and is based on a lower-bound result of Ben-David et al. [2009] for online learning of binary hypotheses classes in terms of its Littlestone\u2019s dimension.\nTheorem 13 (Generic Lower Bound). Given parameters V > 0 and s > 0 along with any online learning algorithm, there exists a sequence of V -dimensional loss vectors l1, l2, . . . , lT \u2208 {0,\u00b1s}N of sparsity 2V \u2264 N (i.e., rank ([l1, l2, . . . , lT ]) = V and \u2016lt\u20160 = 2V , for all t \u2208 [T ]) such that\nRegretT \u2265 2s \u221a V T\n8 .\nProof of the above theorem is deferred to Appendix D.1. The following corollary is a direct con-\nsequence of Theorem 13.\nCorollary 14. Given parameters V \u2208 [lnN ] and s > 0 along with any online learning algorithm, there exists a sequence of loss vectors l1, l2, . . . , lT \u2208 [\u2212s, s]N of VC-dimension V (i.e., V C({l1, l2, . . . , lT }) = V ), such that\nRegretT \u2265 2s \u221a V T\n8 .\nProof. Consider the set of loss vectors L = {l \u2208 {0,\u00b1s}N | \u2016l\u20160 \u2264 2V }. From the definition of VC dimension (see Definition 23, Appendix D), it follows that V C(L) = V . Hence, Theorem 13 implies the stated claim.\nNext we instantiate Theorem 13 to derive the regret lower bounds for the structured loss spaces\nintroduced in Section 3. In particular, we begin by stating a lower bound for sparse loss vectors.\nCorollary 15. (Lower Bound for Sparse losses) Given k \u2208 [N ] and s > 0 along with any online learning algorithm, there exists a sequence of loss vectors l1, l2, . . . , lT \u2208 [\u2212s, s]N of sparsity k \u2208 N (i.e. \u2016lt\u20160 = k for all t \u2208 [T ]) such that\nRegretT \u2265 2s \u221a \u230aln k\u230bT 8 .\nAlong the same lines, Theorem 13 leads to a lower bound for losses with small \u2113p norm.\nCorollary 16. (Lower Bound for \u2113p losses) Given p \u2264 [lnN ] and s > 0 along with any online learning algorithm, there exists a sequence of loss vectors l1, l2, . . . , lT \u2208 [\u2212s, s]N of \u2113p norm at most s (i.e., \u2016lt\u2016p \u2264 s) such that\nRegretT \u2265 s \u221a pT\n8 .\nProof. Consider the set of all 2p-sparse loss vectors in [\u2212 s2 , s2 ]N . Any such loss vector l \u2208 [\u2212 s2 , s2 ]N has \u2016l\u2016p \u2264 s. The stated claim now follows by applying Theorem 13 with parameters s2 and V = p.\nCorollary 17. (Lower Bound for Noisy Losses) Given \u01eb > 0 and any online learning algorithm, there exists a sequence of \u01eb-noisy loss vectors l1, l2, . . . , lT \u2208 [\u2212\u01eb, \u01eb]N (i.e., \u2016lt\u201622 \u2264 \u01eb) such that\nRegretT \u2265 \u221a \u01ebT\n4 .\nProof. Consider the set of all 2-sparse loss vectors in [\u2212 \u221a \u01eb 2 , \u221a \u01eb 2 ]\nN . Clearly any such loss vector l \u2208 [\u2212 \u221a \u01eb 2 , \u221a \u01eb 2 ] N has \u2016l\u201622 \u2264 \u01eb. Hence with parameters s = \u221a \u01eb 2 and V = 1, the result follows directly from theorem 13.\nRemark 18. Note that Theorem 13 (with parameter V = d, s = 1) recovers the lower bound for low rank loss spaces as established by Hazan et al. [2016]: given 1 \u2264 d \u2264 lnN and any online learning algorithm, there exists a sequence of d-rank loss vectors l1, l2, . . . , lT \u2208 [\u22121, 1]N such that\nRegretT \u2265 2 \u221a dT\n8 .\nWe next derive the regret lower bounds for few instances of additive loss spaces.\nCorollary 19. (Lower Bound for Noisy Low Rank) Given parameters \u01eb > 0 and d \u2208 [lnN ] along with any online learning algorithm, there exists a sequence of loss vectors l1, l2, . . . , lT \u2208 [\u2212(1 + \u01eb), (1 + \u01eb)]N , where lt = lt1 + lt2, with lt1 \u2208 {l \u2208 [\u22121, 1]N | l = Uv} (U \u2208 RN\u00d7d is a rank d matrix), and \u2016lt2\u201622 \u2264 \u01eb, such that\nRegretT \u2265 2 ( 1 + \u221a \u01eb\n2d\n)\u221a dT\n8 .\nProof. Let N = 2d. Consider the matrix H \u2208 {\u22121, 1}N\u00d7d where 2d rows of H represent 2d vertices of the d-hypercube in [\u22121, 1]N . Let,L1 = {H(:, 1), . . .H(:, d)}, andL2 = { l \u2208 { \u2212 \u221a \u01eb 2d , \u221a \u01eb 2d }N | \u2016l\u201622 = \u01eb } . Note that any loss vectors in L2 is 2d-sparse. Consider L = L1+L2. The result now follows from Theorem 13, noting that\u2014with s = ( 1 +\n\u221a \u01eb 2d ) and V = d\u2014the lowering-bounding loss vectors assured\nin Theorem 13, l1, . . . , lT , are contained in L.\nCorollary 20. (Lower Bound for Noisy Sparse) Given parameters \u01eb > 0 and k \u2208 [N ] along with any online learning algorithm, there exists a sequence of loss vectors l1, l2, . . . , lT \u2208 [\u2212(1 + \u01eb), (1 + \u01eb)]N , where lt = lt1 + lt2, with lt1 \u2208 {l \u2208 [\u22121, 1]N | \u2016l\u20160 \u2264 k}, and \u2016lt2\u201622 = \u01eb, such that\nRegretT \u2265 2 ( 1 + \u221a \u01eb\nk )\u221a\u230aln k\u230bT 8 .\nProof. Consider the following set of loss vectors: L1 = {l \u2208 {\u22121, 1}N | \u2016l\u20160 = k}, and L2 ={ l \u2208 { \u2212 \u221a \u01eb k , \u221a \u01eb k }N | \u2016l\u201622 = \u01eb } . Note that any loss vectors in L2 is k-sparse. Write L = L1 + L2. The corollary now follows from Theorem 13, noting that\u2014with s = ( 1 + \u221a \u01eb k ) and V = \u230aln k\u230b\u2014the lowering-bounding loss vectors assured in Theorem 13, l1, . . . , lT , are contained in L."}, {"heading": "6 Conclusion", "text": "In this paper, we have developed a theoretical framework for online learning with structured losses, namely the broad class of problems with additive loss spaces. The framework yields both algorithms that generalize standard online mirror descent and also novel regret upper bounds for relevant settings such as noisy + sparse, noisy + low-rank, and sparse + low-rank losses. In addition, we have derived lower bounds\u2014i.e., fundamental limits\u2014on regret for a variety of online learning problems with structured loss spaces. In light of these results, tightening the gap between the upper and lower bounds for structured loss spaces is a natural, open problem.\nAnother relevant thread of research is to study settings wherein the learner knows that the loss space is structured, but is oblivious to the exact instantiation of the loss space, e.g., the losses might be perturbations of vectors from a low-dimensional subspace, but, a priori, the learning algorithm might not know the underlying subspace.4 Addressing structured loss spaces in bandit settings also remains an interesting direction for future work.\n4The result of Hazan et al. [2016] address the noiseless version of this problem."}, {"heading": "A Proof of Theorem 3", "text": "Theorem 3 (OMD regret bound (Theorem 5.2, Bubeck [2011])). Let the loss vectors, {lt}Tt=1, belong to a loss space L \u2286 [0, 1]N , which is bounded with respect to a (arbitrary) norm \u2016\u00b7 \u2016; in particular, for any l \u2208 L we have \u2016l\u2016 \u2264 G. Furthermore, let \u2126 \u2287 \u2206N be a convex set, and R : \u2126\u2192R be a strictly convex, differentiable function that satisfies R(p) \u2212 R(p1) \u2264 D2 for parameter D \u2208 R and all p \u2208 \u2206N ; where p1 := argminp\u2208\u2206N R(p). Also, let the restriction of R to \u2206N be \u03b1-strongly convex with respect to \u2016\u00b7 \u2016\u2217, the dual norm of \u2016\u00b7 \u2016.\nThen, the regret of OMD algorithm with set \u2126, regularizer function R, and learning rate \u03b7 > 0, for T\nrounds satisfies\nRegretT (OMD(\u03b7 \u2217)) =\nT\u2211\nt=1\npt.lt \u2212 N min i=1\nT\u2211\nt=1\nlt(i) \u2264 1\n\u03b7\n( D2 + \u03b72G2T\n2\u03b1\n) ,\nwhere p1,p2, . . .pT denotes the sequential predictions of the algorithm in T rounds. Moreover, setting \u03b7 \u2217 = D G \u221a 2\u03b1 T (i.e., minimizing the right-hand-side of the above bound), we have\nRegretT (OMD(\u03b7 \u2217)) \u2264 DG\n\u221a 2T\n\u03b1 .\nProof. Consider p \u2208 \u2206N . We have for all t \u2208 [T ]:\nlt.pt \u2212 lt.p \u2264 lt.(pt \u2212 p) ,\n= (\u2207R(pt)\u2212\u2207R(p\u0303t+1) \u03b7 ) \u00b7 (pt \u2212 p) = 1\n\u03b7\n( BR(p,pt)\u2212BR(p, p\u0303t+1) +BR(pt, p\u0303t+1) )\n\u2264 1 \u03b7\n( BR(p,pt)\u2212BR(p,pt+1)\u2212BR(pt+1, p\u0303t+1) +BR(pt, p\u0303t+1) ) ,\nwhere the last inequality follows from generalized pythagorean inequality (see Lemma 5.4, [Bubeck, 2011]). Summing over t = 1 . . . T , we thus get\nT\u2211\nt=1\n( lt.pt \u2212 lt.p ) \u2264 1\n\u03b7\n( BR(p,p1)\u2212BR(p,pT+1) ) + 1\n\u03b7\nT\u2211\nt=1\n( BR(pt, p\u0303t+1)\u2212BR (pt+1, p\u0303t+1) ) .\nNow, it can be shown that\nBR(p,p1) = R(p)\u2212R(p1)\u2212\u2207R(p1).(p \u2212 p1) \u2264 R(p)\u2212R(p1) \u2264 D2 .\nThe last inequality holds since p1 := argminpin\u2206N R(p), which implies \u2207R(p1).(p \u2212 p1) > 0 for all p \u2208 \u2206N . Also, we have T\u2211\nt=1\n( BR(pt, p\u0303t+1)\u2212BR(pt+1, p\u0303t+1) )\n=\nT\u2211\nt=1\n( R(pt)\u2212R(pt+1)\u2212\u2207R(p\u0303t+1) \u00b7 (pt \u2212 pt+1) )\n\u2264 T\u2211\nt=1\n(( \u2207R(pt) \u00b7 (pt \u2212 pt+1)\u2212 \u03b1\n2 \u2016pt \u2212 pt+1\u2016\u22172\n) \u2212\u2207R(p\u0303t+1).(pt \u2212 pt+1) ) , (by strong convexity of R)\n=\nT\u2211\nt=1\n( \u2212 \u03b7lt(pt) \u00b7 (pt \u2212 pt+1)\u2212 \u03b1\n2 \u2016pt \u2212 pt+1\u2016\u22172\n)\n\u2264 T\u2211\nt=1\n( \u03b7G\u2016pt \u2212 pt+1\u2016\u2217 \u2212 \u03b1\n2 \u2016pt \u2212 pt+1\u2016\u22172\n) , (by Ho\u0308lders inequality)\n\u2264 T\u2211\nt=1\n\u03b72G2\n2\u03b1 , (since \u03b7 2G2 2\u03b1 + \u03b1 2 \u2016pt \u2212 pt+1\u2016\u22172 \u2212 \u03b7G\u2016pt \u2212 pt+1\u2016\u2217 = ( \u03b7G\u221a 2\u03b1 \u2212 \u221a \u03b1 2 \u2016pt \u2212 pt+1\u2016\u2217 )2 \u2265 0)\n= \u03b72G2T\n2\u03b1 .\nThus, we get \u2211T\nt=1\n( lt.pt \u2212 lt.p ) \u2264 1\u03b7 ( D2 + \u03b7 2G2T 2\u03b1 ) . Note that above bound holds for any p \u2208 \u2206N .\nTherefore,\nT\u2211\nt=1\n( lt.pt \u2212 inf\np\u2208\u2206N lt.p\n) = T\u2211\nt=1\n( lt.pt \u2212 N min i=1 lt(i) ) \u2264 1 \u03b7\n( D2 + \u03b72G2T\n2\u03b1\n) .\nWhere the first equality holds since argminp\u2208\u2206N l.p \u2208 {e1, e2, . . . , eN}. Note that the right-handside is minimized at \u03b7\u2217 = DG \u221a 2\u03b1 T ; substituting this back in the above inequality gives the desired result."}, {"heading": "B Proofs from Section 3", "text": "1. Sparse loss space: L = {l \u2208 [0, 1]N | \u2016l\u20160 = s}, s being the loss sparsity, 1 \u2264 s \u2264 N . Then using\nq-norm, R(x) = \u2016x\u20162q = (\u2211N i=1(x q i ) ) 2 q , where q = ln s \u2032 ln s\u2032\u22121 , s \u2032 = (s + 1)2, as the regularizer, we get,\nRegretT \u2264 2 \u221a ln(s+ 1)T .\nProof. Note that 4 \u2264 (s+1)2 \u2264 (N +1)2. Let p = ln s\u2032 = 2 ln(s+1). Clearly, 2 \u2264 p \u2264 2 ln(N +1). Consider the norm \u2016 \u00b7 \u2016 = 1\u221a\n2 \u2016 \u00b7 \u2016p, and it dual \u2016 \u00b7 \u2016\u2217 = \u221a 2\u2016 \u00b7 \u2016q, where 1q = 1\u2212 1p , or q = ln s \u2032\nln s\u2032\u22121 \u2208 (1, 2]. Note that:\n(a) For l \u2208 L, \u2016l\u2016 \u2264 1 (since l is at most s-sparse). Hence G = 1. (b) For any p \u2208 \u2206N , R(p)\u2212R(p1) \u2264 R(p) \u2264 1. HenceD = 1. (c) R(x) = \u2016x\u20162q is (q\u2212 1)-strongly convex with respect to \u221a 2\u2016 \u00b7 \u2016q , for all x \u2208 \u2206N (see Lemma\n17 of Appendix A in [Shalev-Shwartz, 2007]). Hence \u03b1 = (q \u2212 1) = 1(2 ln(s+1)\u22121) .\nThe result now follows from an application of Theorem 3.\n2. Spherical loss space: L = {l \u2208 [0, 1]N | \u2016l\u2016A = l\u22a4Al \u2264 \u01eb}, whereA is a positive definite matrix, \u01eb > 0. Then using the ellipsoidal norm R(x) = \u01ebx\u22a4A\u22121x, as the regularizer, we get,\nRegretT \u2264 \u221a \u03bbmax(A\u22121)\u01ebT ,\nwhere \u03bbmax(A \u22121) denotes the maximum eigenvalue ofA\u22121.\nProof. Consider the norm \u2016 \u00b7 \u2016 = 1\u221a \u01eb \u2016 \u00b7 \u2016A, where for any x \u2208 RN , \u2016x\u2016A =\n\u221a x\u22a4Ax. Note that\nthe dual norm is \u2016 \u00b7 \u2016\u2217 = \u221a\u01eb\u2016 \u00b7 \u2016A\u22121 . In addition, we have\n(a) For any l \u2208 L, \u2016l\u2016 \u2264 1. Hence,G = 1. (b) For any p \u2208 \u2206N ,R(p)\u2212R(p1) \u2264 \u01ebp\u22a4A\u22121p \u2264 \u01eb\u03bbmax(A\u22121); here, the first inequality follows\nfrom the fact thatA\u22121 is positive definite. HenceD = \u221a \u01eb \u03bbmax(A\u22121).\n(c) R(x) = \u01ebx\u22a4A\u22121x is 2-strongly convex with respect to \u221a \u01eb\u2016 \u00b7 \u2016A\u22121 , for all x \u2208 \u2206N . Hence\n\u03b1 = 2.\nAs before, an application of Theorem 3 gives us the result.\n3. Noisy loss: L = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb}, \u01eb > 0. Then using the standard euclidean norm R(x) = \u01eb\u2016x\u201622, as the regularizer, we get,\nRegretT \u2264 \u221a \u01ebT .\nProof. Consider the norm \u2016 \u00b7 \u2016 = 1\u221a \u01eb \u2016 \u00b7 \u20162, and it dual \u2016 \u00b7 \u2016\u2217 = \u221a \u01eb\u2016 \u00b7 \u20162. Note that:\n(a) For any l \u2208 L, \u2016l\u2016 \u2264 1. Hence G = 1. (b) For any p \u2208 \u2206N , R(p)\u2212R(p1) = \u01eb\u2016p\u201622 \u2212 \u01eb\u2016p1\u201622 \u2264 \u01eb. HenceD = \u221a \u01eb. (c) R(x) = \u01eb\u2016x\u201622 is 2-strongly convex with respect to \u221a \u01eb\u2016 \u00b7 \u20162, \u2200x \u2208 \u2206N . Hence \u03b1 = 2.\nAs before the result now follows applying Theorem 3. One can also recover this regret bound of noisy loss as a special case of spherical loss withA = A\u22121 = IN , since \u03bbmax(IN ) = 1.\n4. Standard loss space: L = [0, 1]n. Then using unnormalized negative entropy,R(x) = \u2211Ni=1 xi log xi\u2212\u2211N i=1 xi, as the regularizer, we get\nRegretT \u2264 \u221a 2T lnN .\nProof. Consider the norm \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u2016\u221e, and its dual norm \u2016 \u00b7 \u2016\u2217 = \u2016 \u00b7 \u20161. Note that:\n(a) For any l \u2208 L, \u2016l\u2016 \u2264 1. Hence G = 1. (b) For any p \u2208 \u2206N , R(p)\u2212R(p1) = \u2211N i=1 pi ln ( pi p1i ) \u2212\u2211Ni=1(pi \u2212 p1i) \u2264 lnN (since p1 = 1N ,\nassuming 0 ln 0 = 0). HenceD = \u221a lnN .\n(c) R(x) = \u2211N i=1 xi log xi \u2212 \u2211N\ni=1 xi is 1-strongly convex with respect to \u2016 \u00b7 \u20161, \u2200x \u2208 \u2206N (see example 2.5, Shalev-Shwartz [2012a]). Hence \u03b1 = 1.\nThe result now follows via Theorem 3."}, {"heading": "C Proofs from Section 4.2", "text": "The proofs given in this section are based on the results given in Section B, which establish regret guarantees of the OMD algorithm for specific structured loss spaces.\nProof of Corollary 10\nCorollary 10 (Noisy Low Rank). Suppose L1 = {l \u2208 [0, 1]N | l = Uv } is a d rank loss space (1 \u2264 d \u2264 lnN), perturbed with noisy losses L2 = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb, \u01eb > 0}. Then, the regret of the OMD algorithm over the loss space L = L1 + L2\u2014with regularizer R(x) = x\u22a4Hx + \u01eb\u2016x\u201622 and learning rate \u03b7\u2217 = \u221a 2(16d+\u01eb)\nT \u2014is upper bounded as follows\nRegretT \u2264 \u221a 2(16d + \u01eb)T .\nProof. Consider the following two convex, compact, bounded and centrally symmetric sets\nA1 = { x \u2208 RN | \u221a x\u22a4H\u22121x \u2264 1 } , and A2 = { x \u2208 RN | 1\u221a\n\u01eb\n\u221a xTx \u2264 1 } .\nwhere H = IN + U \u22a4MU, M being the matrix corresponding to the Lo\u0308wner-John ellipsoid of L\n[Hazan et al., 2016]. We have \u2016x\u2016A1 = \u221a x\u22a4H\u22121x, and \u2016x\u2016A2 = 1\u221a\u01eb \u221a xTx, for any x \u2208 RN . Clearly, L1 \u2286 A1, and L2 \u2286 A2. Consider the norm \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u2016A, and its dual norm \u2016 \u00b7 \u2016\u2217 = \u2016 \u00b7 \u2016\u2217A, where A = A1 + A2. Note that, for any l \u2208 L, \u2016l\u2016A \u2264 1, since L \u2286 A. Let us choose R1(x) = x\u22a4Hx, and R2(x) = \u01eb\u2016x\u201622. Recall from Appendix B,\n1. R1(p)\u2212R1(p1) \u2264 D21 = 16d, and R2(p)\u2212R2(p1) \u2264 D22 = \u01eb. HenceD = \u221a 16d + \u01eb. 2. BothR1 andR2 are 2-strongly convex w.r.t. \u2016x\u2016\u2217A1 = \u221a x\u22a4Hx, and \u2016x\u2016\u2217A2 = \u221a \u01ebxTx respectively.\nHence \u03b11 = \u03b12 = 2, and \u03b1 = min{\u03b11,\u03b12}\n2 = 1.\nThe result now follows applying Theorem 4.\nProof of Corollary 11\nCorollary 11 (Noisy Sparse). Suppose L1 = {l \u2208 [0, 1]N | \u2016l\u20160 = s} is an s-sparse loss space (s \u2208 [N ]), perturbed with noisy losses from L2 = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb, \u01eb > 0}. Then, the regret of the OMD algorithm over the loss space L = L1 + L2\u2014with regularizer R(x) = \u2016x\u20162q + \u01eb\u2016x\u201622 and learning rate \u03b7\u2217 = \u221a 1+\u01eb\n(2 ln(s+1)\u22121)T \u2014is upper bounded as follows\nRegretT \u2264 2 \u221a 2(1 + \u01eb) ln(s+ 1)T .\nProof. Let s\u2032 = (s+ 1)2, p = ln s\u2032 = 2 ln(s+ 1). Note that 2 \u2264 p \u2264 2 ln(N + 1). Consider the following two convex, compact, and centrally symmetric sets\nA1 = { x \u2208 RN | 1\u221a\n2 \u2016x\u2016p \u2264 1\n} , and A2 = { x \u2208 RN | 1\u221a\n\u01eb\n\u221a xTx \u2264 1 } .\nWe have \u2016x\u2016A1 = 1\u221a2\u2016x\u2016p, and \u2016x\u2016A2 = 1\u221a \u01eb\n\u221a xTx, for any x \u2208 RN . We have L1 \u2286 A1 and L2 \u2286 A2.\nConsider the norm \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u2016A, and its dual norm \u2016 \u00b7 \u2016\u2217 = \u2016 \u00b7 \u2016\u2217A, where A = A1 +A2. Note that, for any l \u2208 L, \u2016l\u2016A \u2264 1, since L \u2286 A. Let us chooseR1(x) = \u2016x\u20162q , where 1q = 1\u2212 1p , or q = ln s \u2032\nln s\u2032\u22121 \u2208 (1, 2], and R2(x) = \u01eb\u2016x\u201622. Recall from Appendix B,\n1. R1(p)\u2212R1(p1) \u2264 D21 = 1, and R2(p)\u2212R2(p1) \u2264 D22 = \u01eb. HenceD = \u221a 1 + \u01eb. 2. R1 is (q\u2212 1)-strongly convex w.r.t. \u2016x\u2016\u2217A1 = \u221a 2\u2016x\u2016q , and R2 is 2-strongly convex w.r.t. \u2016x\u2016\u2217A2 =\u221a\n\u01ebxTx. Hence \u03b11 = (q \u2212 1), \u03b12 = 2, and \u03b1 = min{\u03b11,\u03b12}2 = (q \u2212 1), since (q \u2212 1) \u2208 (0, 1].\nUsing Theorem 4, we get the desired claim.\nProof of Corollary 12\nCorollary 12 (Low Rank with Sparse). Suppose L1 = {l \u2208 [0, 1]N | l = Uv } is a d rank loss space (1 \u2264 d \u2264 lnN), perturbed with s-sparse losses L2 = {l \u2208 [0, 1]N | \u2016l\u20160 = s}, s \u2208 [N ]. Then, the regret of the OMD algorithm over the loss space L = L1 + L2\u2014with regularizer R(x) = x\u22a4Hx+ \u2016x\u20162q and learning rate \u03b7\u2217 = \u221a 16d+1\n(2 ln(s+1)\u22121)T \u2014is upper bounded as follows\nRegretT \u2264 2 \u221a 2(16d + 1) ln(s + 1)T .\nProof. Let s\u2032 = (s+1)2, p = ln s\u2032 = 2 ln(s+1). Clearly, 2 \u2264 p \u2264 2 ln(N+1). Also letH = IN +U\u22a4MU, M being the matrix corresponding to the Lo\u0308wner-John ellipsoid of L; this regularizer was used in [Hazan et al., 2016]. Now consider the following two convex, compact, and centrally symmetric sets\nA1 = { x \u2208 RN | \u221a x\u22a4H\u22121x \u2264 1 } and A2 = { x \u2208 RN | 1\u221a\n\u01eb\n\u221a xTx \u2264 1 } .\nWe have \u2016x\u2016A1 = 1\u221a2\u2016x\u2016p and \u2016x\u2016A2 = 1\u221a \u01eb\n\u221a xTx, for any x \u2208 RN . Clearly, L1 \u2286 A1, and L2 \u2286 A2.\nConsider the norm \u2016 \u00b7 \u2016 = \u2016 \u00b7 \u2016A, and its dual norm \u2016 \u00b7 \u2016\u2217 = \u2016 \u00b7 \u2016\u2217A, where A = A1 +A2. Note that, for any l \u2208 L, \u2016l\u2016A \u2264 1, since L \u2286 A. Let us choose R1(x) = x\u22a4Hx, and R2(x) = \u2016x\u20162q , where 1q = 1\u2212 1p , or q = ln s \u2032\nln s\u2032\u22121 \u2208 (1, 2]. Recall from Appendix B,\n1. R1(p)\u2212R1(p1) \u2264 D21 = 16d, and R2(p)\u2212R2(p1) \u2264 D22 = 2\u01eb. HenceD = \u221a 16d+ 2\u01eb. 2. R1 is 2-strongly convex w.r.t. \u2016x\u2016\u2217A1 = \u221a x\u22a4Hx, andR2 is (q\u22121)-strongly convex w.r.t. \u2016x\u2016\u2217A1 =\u221a\n2\u2016x\u2016q . Hence \u03b11 = 2, \u03b12 = (q \u2212 1), and \u03b1 = min{\u03b11,\u03b12}2 = (q \u2212 1), since (q \u2212 1) \u2208 (0, 1].\nAs in the previous corollaries, the claim follows by applying Theorem 4."}, {"heading": "D Proofs from Section 5", "text": "This section provides a simple generalization of a lower-bound result of Ben-David et al. [2009] for online learning of binary hypotheses classes. Then, using this generalization, it establishes Theorem 13.\nWe begin by defining the binary hypothesis learning problem and Littlestone\u2019s dimension of a set\nof binary hypotheses.\nDefinition 21. Online Binary Hypothesis Learning Problem: For a given instance space X , binary label space Y = {0, 1}, and a class of binary hypothesis functions H = {h1, . . . , hn}, hi : X 7\u2192 {0, 1} \u2200i \u2208 [n], the problem of online binary hypothesis learning is a sequential prediction game between the environment and a learner. At each iteration, the environment provides an instance xt \u2208 X , and the learner\u2019s objective is to predict its class label y\u0302t \u2208 Y\u0302 = {0, 1}. At the end of T iterations, the performance of the learner is measured in terms of its number of mispredictions with respect to the best hypothesis in theH, termed as the regret of the learner, defined as follows:\nRegretT = T\u2211\nt=1\n|y\u0302t \u2212 yt| \u2212min h\u2208H\nT\u2211\nt=1\n|h(xt)\u2212 yt|\nDefinition 22. Littlestone\u2019s-dimension of a set of binary hypotheses (Ben-David et al. [2009]): For a binary hypothesis learning problem, letH be a non-empty class of binary hypotheses such that h : X 7\u2192 {0, 1} for all h \u2208 H, where X is the instance space. An instance-labeled tree is said to be shattered by the class H if for any root-to-leaf path (x1, y1), . . . , (xd, yd) there is some h \u2208 H such that for all i \u2264 d, h(xi) = yi for all i. The Littlestone\u2019s dimension of H, Ldim(H) is the largest integer d such that there exist a full binary tree of depth d (i.e., any of its branch contains d-many non-leaf nodes) that is shattered byH.\nDefinition 23. VC-dimension of a set (Alon et al. [2014]): Let A = {a1,a2, . . . ,am} be a set of m, ddimensional vectors, ai \u2208 Rd, \u2200i \u2208 [m]. Let C = {c1, . . . , ck} \u2286 [m] be a subset of vectors of set A. We say that A shatters C with respect to the real numbers (tc1 , . . . , tck), iff for anyD \u2286 C , there is a coordinate i \u2208 [d] with A(i, c) < tc for all c \u2208 D, and A(i, c) > tc for all c \u2208 C \\D. Then the VC dimension of the set of vectors A, denoted by V C(A), is defined as the maximal size subset of vectors shattered by A. Clearly, V C(A) \u2264 ln d.\nLet us first recall the lower-bound result of Ben-David et al. [2009] for online learning of binary\nhypotheses classes in terms of its Littlestone\u2019s dimension.\nLemma 24. [Ben-David et al., 2009] Let X and Y = {0, 1} respectively denote the instance and label space for an online binary hypothesis learning problem, and H be a class of binary hypotheses such that h : X 7\u2192 {0, 1} for all h \u2208 H. Then for any (possibly randomized) algorithm for the classification problem, there exists a sequence of labeled instances (x1, y1), . . . , (xT , yT ) \u2208 (X \u00d7 Y) such that\nE\n[ T\u2211\nt=1\n|y\u0302t \u2212 yt| ] \u2212min\nh\u2208H\nT\u2211\nt=1\n|h(xt)\u2212 yt| \u2265 \u221a\nLdim(H)T 8 ,\nwhere y\u0302t \u2208 {0, 1} is the algorithm\u2019s output at iteration t.\nNow suppose in the problem of binary hypothesis learning, the learner is allowed to make pre-\ndictions in { 0, 12 , 1 } , i.e. Y\u0302 = { 0, 12 , 1 } . We show that the lower bound guarantee of Lemma 24 holds as it is even for this problem. Formally,\nLemma 25. Let X and Y = {0, 1} respectively denote the instance and label space for an online binary hypothesis learning problem, andH be a class of binary hypotheses such that h : X 7\u2192 { 0, 12 , 1 } for all h \u2208 H. Then for any (possibly randomized) algorithm for the problem, there exists a sequence of labeled instances (x1, y1), . . . , (xT , yT ) \u2208 (X \u00d7 Y), such that\nE\n[ T\u2211\nt=1\n|y\u0302t \u2212 yt| ] \u2212min\nh\u2208H\nT\u2211\nt=1\n|h(xt)\u2212 yt| \u2265 \u221a\nLdim(H)T 8 ,\nwhere y\u0302t \u2208 { 0, 12 , 1 } is the algorithm\u2019s output at iteration t.\nProof. Let d = Ldim(H) and, for simplicity, assume that T is an integer multiple of d, say, T = kd for some non-negative integer k. Consider a full binary H-shattered tree of depth d. We construct the sequence (x1, y1), (x2, y2), . . . , (xT , yT ) by following a root-to-leaf path (u1, z1), (u2, z2), . . . , (ud, zd) in the shattered tree. We pick the path in a top-down fashion starting at the root. The label zi \u2208 {0, 1} determines whether the path moves to the left or to the right subtree of ui and it thus determines ui+1. Each node ui on the path, i \u2208 [d], corresponds to a block (x(i\u22121)k+1, y(i\u22121)k+1), . . . , (xik, yik) of k examples. We define x(i\u22121)k+1 = x(i\u22121)k+2 = \u00b7 \u00b7 \u00b7 = xik = ui, and we choose y(i\u22121)k+1, . . . , yik \u2208 {0, 1}, independently uniformly at random. For each block, let Ti = {(i\u2212 1)k + 1, ..., ik} be the time indices of the ith block. Denote r =\n\u2211 t\u2208Ti yt. Note that since yt \u2208 {0, 1},\nmin zi\u2208{0, 12 ,1}\n\u2211 t\u2208Ti |zi \u2212 yt| = min zi\u2208{0,1} \u2211 t\u2208Ti |zi \u2212 yt| = { k \u2212 r, if r \u2265 k2 r, otherwise\nNote that the expected loss incurred by the learner in the ith block is k/2. Hence, k/2\u2212minzi\u2208{0,1} \u2211\nt\u2208Ti |zi\u2212 yt| = |r\u2212k/2|. Taking expectations over the y\u2019s and usingKhinchine\u2019s inequality [Cesa-Bianchi and Lugosi, 2006], we obtain\nk/2\u2212E   min zi\u2208{0,1} \u2211\nt\u2208Ti |zi \u2212 yt|\n  = E[|r \u2212 k/2|] \u2265 \u221a k\n8 .\nSince Ldim(H) = d, note that there exists h \u2208 H, such that within each block we have h(ui) = zi. Thus by summing over the d blocks we get\ndk/2 \u2212E [ min h\u2208H \u2211\nt\u2208T |h(xt)\u2212 yt|\n] \u2265 d \u221a k\n8 .\nFinally, since dk2 = T 2 = E[ \u2211T t=1 |y\u0302t \u2212 yt|], we conclude that the expected regret, w.r.t. the randomness\nof choosing the labels, is at least d \u221a\nk 8 = \u221a dT 8 . Therefore, there exists a particular sequence for which\nthe regret is at least \u221a\ndT 8 , which concludes the proof.\nNote that the following corollary follows directly from Lemma 25:\nCorollary 26. Consider any s > 0. Let X and Y = {0, s} respectively denote the instance and label space for an online binary hypothesis learning problem, and H be a class of binary hypotheses such that h : X 7\u2192\n{ 0, s2 , s } for all h \u2208 H. Then for any (possibly randomized) algorithm for the problem, there exists a sequence of labeled instances (x1, y1), . . . , (xT , yT ), (xt, yt) \u2208 (X \u00d7 Y), for t \u2208 [T ], such that\nE\n[ T\u2211\nt=1\n|y\u0302t \u2212 yt| ] \u2212min\nh\u2208H\nT\u2211\nt=1\n|h(xt)\u2212 yt| \u2265 s \u221a\nLdim(H)T 8 ,\nwhere y\u0302t \u2208 { 0, s2 , s } is the algorithm\u2019s output at iteration t.\nD.1 Proof of Theorem 13\nTheorem 13 (Generic Lower Bound). Given parameters V > 0 and s > 0 along with any online learning algorithm, there exists a sequence of V -dimensional loss vectors l1, l2, . . . , lT \u2208 {0,\u00b1s}N of sparsity 2V \u2264 N (i.e., rank ([l1, l2, . . . , lT ]) = V and \u2016lt\u20160 = 2V , for all t \u2208 [T ]) such that\nRegretT \u2265 2s \u221a V T\n8 .\nProof. We first construct a problem instance of an online binary hypothesis learning problem (see Definition 21) as follows: Let X = {e1, e2, . . . , eV } and Y = {0, s} respectively denote the instance and label space. Consider the 2V vertices of the V -dimensional binary hypercube u1,u2, . . .u2V , where ui \u2208 {\u00b11}V represents the ith vertex of the hypercube. We define a matrix U \u2208 {\u00b11, 0}N\u00d7V such that\nU(i, j) = { ui(j), \u2200i \u2208 [2V ], j \u2208 [V ] 0, otherwise\nThat is, the first 2V rows of U are the uis and the remaining (N \u2212 2V ) rows are all zeros. Now, let us consider the following class of hypothesis functionsH = {h1, h2, . . . hN} hi : X 7\u2192 { 0, s2 , s } \u2200i \u2208 [N ], such that\nhi(ej) =    0, if U(i, j) = \u22121 s 2 , if U(i, j) = 0\ns, if U(i, j) = +1\nNote that Ldim(H) = V . Thus using Corollary 26 we get\nEhi\u223cpt\n[ T\u2211\nt=1\n|hi(vt)\u2212 yt| ] \u2212min\nh\u2208H\nT\u2211\nt=1\n|h(vt)\u2212 yt| \u2265 s \u221a V T\n8 ,\nwhere vt \u2208 X is the observed instance, yt \u2208 Y is the true label of vt, and pt \u2208 \u2206N is the distribution maintained by the algorithm over the set of N classifiers H, at iteration t. Let us denote algorithms expected prediction at time t as y\u0302t = \u2211N i=1 pt(i)hi(vt). Note that for any y \u2208 {0, s}, and v \u2208 X , |hi(v)\u2212 y| = s\u2212(2y\u2212s).U(i,:).v2 . Now let us construct the sequence of loss vectors lt = y\u0304tUvt, where y\u0304t = (s \u2212 2yt) \u2208 {\u2212s, s}. Note that lt \u2208 { 0, s2 , s } . Since the columns of matrix U is 2V sparse and each instance vector vt \u2208 {e1, . . . , eV }, we have \u2016lt\u20160 = 2V , for all t \u2208 [T ]. In addition, since the rank of matrix U\nis V , the dimensionality constraint on the loss vectors is satisfied as well: rank ([l1, l2, . . . , lT ]) = V . Note that the regret incurred by the learner in this case is\nEhi\u223cpt\n[ T\u2211\nt=1\ns\u2212 (2yt \u2212 s)U(i, :)vt 2\n] \u2212 min\nhi\u2208H\nT\u2211\nt=1\ns\u2212 (2yt \u2212 s)U(i, :)vt 2\n\u2265 s \u221a V T\n8 .\nEquivalently,\nT\u2211\nt=1\nN\u2211\ni=1\npt(i)y\u0304tU(i, :)vt \u2212min i\u2208N\nT\u2211\nt=1\ny\u0304tU(i, :)vt \u2265 2s \u221a V T\n8 .\nSince lt = y\u0304tUvt, this further gives\nT\u2211\nt=1\nptlt \u2212min i\u2208N\nT\u2211\nt=1\nlt(i) \u2265 2s \u221a V T\n8 .\nHence the desired result follows."}, {"heading": "E Tight Examples for Theorem 4", "text": "In this section we provide examples in which the regret guarantee of Theorem 4 is essentially tight. That is, we present loss spaces L1 and L2 such that OMD algorithm obtained via Theorem 4 provides an order-wise optimal regret guarantee for the additive loss space L = L1 + L2.\nComposition of LowRanks: LetL1 = {l \u2208 [0, 1]N | l = U1v } and L2 = {l \u2208 [0, 1]N | l = U2v} be loss spaces of rank d1 and d2, respectively (i.e., rank of the matricesU1 andU2 are respectively d1 and d2). Here (d1 + d2) \u2264 lnN . Consider the regularizer R(x) = x\u22a4(H1 +H2)x, whereH1 = IN +U\u22a41 M1U1, and H2 = IN +U \u22a4 2 M2U2, M1 and M2 being the Lo\u0308wner John ellipsoid matrix for L1 and L2. That is, R(x) = R1(x) +R2(x), where R1(x) and R2(x) are the regularizers for L1 and L2 respectively. Theorem 4 assets that the OMD algorithm, with regularizer R, for the loss space L = L1 + L2 achieves the following regret bound:\nRegretT \u2264 4 \u221a 2(d1 + d2)T .\nThis regret guarantee is tight, since Rank(L) can be as high as (d1 + d2) and, hence, we get a nearly matching lower bound by applying the result of Hazan et al. [2016]; see also Remark 18 in Section 5.\nComposition of Noise Let loss spaces L1 = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb1} and L2 = {l \u2208 [0, 1]N | \u2016l\u201622 \u2264 \u01eb2}. Then, via an instantiation of Theorem 4, we get that the regret of the OMD algorithm over the loss space L = L1 + L2, with regularizer R(x) = (\u01eb1 + \u01eb2)\u2016x\u201622 (and \u03b7\u2217 = \u221a 2(\u01eb1+\u01eb2) T ) is upper bounded as follows:\nRegretT \u2264 \u221a 2(\u01eb1 + \u01eb2)T .\nAgain, modulo constants, this is the best possible regret guarantee for L; see Corollary 17."}], "references": [{"title": "Agnostic online learning", "author": ["Shai Ben-David", "D\u00e1vid P\u00e1l", "Shai Shalev-Shwartz"], "venue": "In COLT,", "citeRegEx": "Ben.David et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2003}, {"title": "Introduction to online optimization", "author": ["NY York"], "venue": "Lecture Notes, Princeton University,", "citeRegEx": "York,? \\Q2004\\E", "shortCiteRegEx": "York", "year": 2004}, {"title": "Online learning with low rank experts", "author": ["Elad Hazan", "Tomer Koren", "Roi Livni", "Yishay Mansour"], "venue": null, "citeRegEx": "Hazan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2010}, {"title": "Online learning and online convex optimization", "author": ["Shai Shalev-Shwartz"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Shalev.Shwartz.,? \\Q2012\\E", "shortCiteRegEx": "Shalev.Shwartz.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Hazan et al. [2016] show that the learner can limit her regret to O( \u221a dT ) when each loss vector comes from a d-dimensional subspace of RN .", "startOffset": 0, "endOffset": 20}, {"referenceID": 2, "context": "Specializing this result for a variety of loss function sets recovers standard OMD regret guarantees for strongly convex regularizers [Shalev-Shwartz, 2012b], and subsumes a result of Hazan et al. [2016] for the online low-rank problem.", "startOffset": 184, "endOffset": 204}, {"referenceID": 2, "context": "Specializing this result for a variety of loss function sets recovers standard OMD regret guarantees for strongly convex regularizers [Shalev-Shwartz, 2012b], and subsumes a result of Hazan et al. [2016] for the online low-rank problem. But more importantly, this allows us to obtain \u201cnew results from old\u201d\u2014regret guarantees for settings such as noisy low rank (where losses are perturbations from a low-dimensional subspace), noisy sparse (where losses are perturbations of sparse vectors), and sparse low-rank (where losses are sparse perturbations from a low-dimensional subspace); see Tables 1 and 2. Another contribution of this work is to show lower bounds on regret for the online learning problem with structured losses. We derive a generic lower bound on regret, for any algorithm for the prediction with experts problem, using structured (in terms of sparsity and dimension) loss vectors. This result allows us to derive regret lower bounds in a variety of individual and additive loss space settings including sparse, noisy, low rank, noisy low-rank, and noisy sparse losses. Related work. The work that is perhaps closest in spirit to ours is that of Hazan et al. [2016], who study the best experts problem when the loss vectors all come from a low-dimensional subspace of the ambient space.", "startOffset": 184, "endOffset": 1183}, {"referenceID": 2, "context": "Low-rank loss space: L = {l \u2208 [0, 1]N | l = Uv }, where the rank of matrix U \u2208 RN\u00d7d is equal to d \u2208 [N ] and vector v \u2208 Rd (as mentioned previously, such loss vectors were considered by Hazan et al. [2016]).", "startOffset": 186, "endOffset": 206}, {"referenceID": 3, "context": "In this section, we give a brief introduction to the Online Mirror Descent (OMD) algorithm [Bubeck, 2011; Shalev-Shwartz, 2012a], which is a subgradient descent based method for online convex optimization with a suitably chosen regularizer. A reader well-versed with the analysis of OMD may skip to the statement of Theorem 3 and proceed to Section 3. OMD generalizes the basic mirror descent algorithm used for offline optimization problems (see, e.g., Beck and Teboulle [2003]).", "startOffset": 106, "endOffset": 479}, {"referenceID": 3, "context": ", Shalev-Shwartz [2012a] and Bubeck [2011]) Let \u03a9 \u2208 Rn be a convex set, and f : \u03a9\u2192R be a differentiable function.", "startOffset": 2, "endOffset": 25}, {"referenceID": 3, "context": ", Shalev-Shwartz [2012a] and Bubeck [2011]) Let \u03a9 \u2208 Rn be a convex set, and f : \u03a9\u2192R be a differentiable function.", "startOffset": 2, "endOffset": 43}, {"referenceID": 0, "context": "The proof of this theorem appears in Appendix D and is based on a lower-bound result of Ben-David et al. [2009] for online learning of binary hypotheses classes in terms of its Littlestone\u2019s dimension.", "startOffset": 88, "endOffset": 112}, {"referenceID": 2, "context": "Note that Theorem 13 (with parameter V = d, s = 1) recovers the lower bound for low rank loss spaces as established by Hazan et al. [2016]: given 1 \u2264 d \u2264 lnN and any online learning algorithm, there exists a sequence of d-rank loss vectors l1, l2, .", "startOffset": 119, "endOffset": 139}, {"referenceID": 2, "context": "The result of Hazan et al. [2016] address the noiseless version of this problem.", "startOffset": 14, "endOffset": 34}], "year": 2017, "abstractText": "We consider prediction with expert advice when the loss vectors are assumed to lie in a set described by the sum of atomic norm balls. We derive a regret bound for a general version of the online mirror descent (OMD) algorithm that uses a combination of regularizers, each adapted to the constituent atomic norms. The general result recovers standard OMD regret bounds, and yields regret bounds for new structured settings where the loss vectors are (i) noisy versions of points from a low-rank subspace, (ii) sparse vectors corrupted with noise, and (iii) sparse perturbations of low-rank vectors. For the problem of online learning with structured losses, we also show lower bounds on regret in terms of rank and sparsity of the source set of the loss vectors, which implies lower bounds for the above additive loss settings as well.", "creator": "LaTeX with hyperref package"}}}