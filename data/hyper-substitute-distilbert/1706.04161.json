{"id": "1706.04161", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Lost Relatives of the Gumbel Trick", "abstract": "the gumbel challenge is a ruse to devi from a normal probability distribution, determined to estimate rapidly normalizing partition behavior. the method relies on repeatedly applying a random subset to the region if such special way, each criterion solving accurately the particular likely product. maps derive an entire family representing related technologies, of which the actual specification is one member, and show each the new methods capture superior properties in dynamic manners producing minimal additional added cost. in particular, for the gumbel scaling to yield computational results enabling constructing graphical models, gumbel perturbations on consistent configurations are typically distributed with moderate - far low - gradient perturbations. maps show how a sample of our new methods operate outside this setting, proving new weaker and lower procedures on the log partition scheme often doing a complexity encoding sequential tables for either resulting distribution. similarly, we balance the problem towards showing how suitable simpler analytical form of selective gumbel apparatus enables inconsistent theoretical results.", "histories": [["v1", "Tue, 13 Jun 2017 17:01:54 GMT  (144kb,D)", "http://arxiv.org/abs/1706.04161v1", "34th International Conference on Machine Learning (ICML 2017)"]], "COMMENTS": "34th International Conference on Machine Learning (ICML 2017)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["matej balog", "nilesh tripuraneni", "zoubin ghahramani", "adrian weller"], "accepted": true, "id": "1706.04161"}, "pdf": {"name": "1706.04161.pdf", "metadata": {"source": "META", "title": "Lost Relatives of the Gumbel Trick", "authors": ["Matej Balog", "Nilesh Tripuraneni", "Zoubin Ghahramani", "Adrian Weller"], "emails": ["<first.last@gmail.com>."], "sections": [{"heading": "1. Introduction", "text": "In this work we are concerned with the fundamental problem of sampling from a discrete probability distribution and evaluating its normalizing constant. A probability distribution p on a discrete sample space X is provided in terms of its potential function \u03c6 : X \u2192 [\u2212\u221e,\u221e), corresponding to log-unnormalized probabilities via p(x) = e\u03c6(x)/Z, where the normalizing constant Z is the partition function. In this context, p is the Gibbs distribution on X associated with the potential function \u03c6. The challenges of sampling from such a discrete probability distribution and estimating the partition function are fundamental problems with ubiq-\n1University of Cambridge, UK 2MPI-IS, Tu\u0308bingen, Germany 3UC Berkeley, USA 4Uber AI Labs, USA 5Alan Turing Institute, UK. Correspondence to: Matej Balog <first.last@gmail.com>. Code: https://github.com/matejbalog/gumbel-relatives.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nuitous applications in machine learning, classical statistics and statistical physics (see, e.g., Lauritzen, 1996).\nPerturb-and-MAP methods (Papandreou & Yuille, 2010) constitute a class of randomized algorithms for estimating partition functions and sampling from Gibbs distributions, which operate by randomly perturbing the corresponding potential functions and employing maximum a posteriori (MAP) solvers on the perturbed models to find a maximum probability configuration. This MAP problem is NP-hard in general; however, substantial research effort has led to the development of solvers which can efficiently compute or estimate the MAP solution on many problems that occur in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009). Evaluating the partition function is a harder problem, containing for instance #P-hard counting problems. The general aim of perturb-and-MAP methods is to reduce the problem of partition function evaluation, or the problem of sampling from the Gibbs distribution, to repeated instances of the MAP problem (where each instance is on a different random perturbation of the original model).\nThe Gumbel trick (Papandreou & Yuille, 2011) relies on adding Gumbel-distributed noise to each configuration\u2019s potential \u03c6(x). We derive a wider family of perturb-andMAP methods that can be seen as perturbing the model in different ways \u2013 in particular using the Weibull and Fre\u0301chet distributions alongside the Gumbel. We show that the new methods can be implemented with essentially no additional computational cost by simply averaging existing Gumbel MAP perturbations in different spaces, and that they can lead to more accurate estimators of the partition function.\nEvaluating or perturbing each configuration\u2019s potential with i.i.d. Gumbel noise can be computationally expensive. One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014; Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d. Gumbel noise with a \u201clow-rank\u201d approximation. Hazan & Jaakkola (2012); Hazan et al. (2013) showed that from such an approximation, upper and lower bounds on the partition function and a sequential sampler for the Gibbs distribution can still be recovered. We show that a subfamily of our new methods, consisting of Fre\u0301chet, Exponential and Weibull tricks, can also be used with low-\nar X\niv :1\n70 6.\n04 16\n1v 1\n[ st\nat .M\nL ]\n1 3\nJu n\n20 17\nrank perturbations, and use these tricks to derive new upper and lower bounds on the partition function, and to construct new sequential samplers for the Gibbs distribution.\nOur main contributions are as follows:\n1. A family of tricks that can be implemented by simply averaging Gumbel perturbations in different spaces, and which can lead to more accurate or more sample efficient estimators of Z (Section 2). 2. New upper and lower bounds on the partition function of a discrete graphical model computable using low-rank perturbations, and a corresponding family of sequential samplers for the Gibbs distribution (Section 3). 3. Discussion of advantages of the simpler analytical form of the Gumbel trick including new links between the errors of estimating Z, sampling, and entropy estimation using low-rank Gumbel perturbations (Section 4).\nBackground and Related work The idea of perturbing the potential function of a discrete graphical model in order to sample from its associated Gibbs distribution was introduced by Papandreou & Yuille (2011), inspired by their previous work on reducing the sampling problem for Gaussian Markov random fields to the problem of finding the mean, using independent local perturbations of each Gaussian factor (Papandreou & Yuille, 2010). Tarlow et al. (2012) extended this perturb-and-MAP approach to sampling, in particular by considering more general structured prediction problems. Hazan & Jaakkola (2012) pointed out that MAP perturbations are useful not only for sampling the Gibbs distribution (considering the argmax of the perturbed model), but also for bounding and approximating the partition function (by considering the value of the max).\nAfterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation.\nPerturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling. Maddison (2016) cast this work into a unified framework together with adaptive rejection sampling techniques, based on the notion of exponential races. This recent view generally brings together perturb-\nand-MAP and accept-reject samplers, exploiting the connection between the Gumbel distribution and competing exponential clocks that we also discuss in Section 2.1.\nInspired by A* sampling, Kim et al. (2016) proposed an exact sampler for discrete graphical models based on lazilyinstantiated random perturbations, which uses linear programming relaxations to prune the optimization space. Further recent applications of perturb-and-MAP include structured prediction in computer vision (Bertasius et al., 2017) and turning the discrete sampling problem into an optimization task that can be cast as a multi-armed bandit problem (Chen & Ghahramani, 2016), see Section 5.2 below.\nIn addition to perturb-and-MAP methods, we are aware of three other approaches to estimate the partition function of a discrete graphical model via MAP solver calls. The WISH method (weighted-integrals-and-sums-by-hashing, Ermon et al., 2013) relies on repeated MAP inference calls applied to the model after subjecting it to random hash constraints. The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015). Weller & Jebara (2014a) instead use just one MAP call over a discretized mesh of marginals to approximate the Bethe partition function, which itself is an estimate (which often performs well) of the true partition function."}, {"heading": "2. Relatives of the Gumbel Trick", "text": "In this section, we review the Gumbel trick and state the mechanism by which it can be generalized into an entire family of tricks. We show how these tricks can equivalently be viewed as averaging standard Gumbel perturbations in different spaces, instantiate several examples, and compare the various tricks\u2019 properties.\nNotation Throughout this paper, let X be a finite sample space of size N := |X |. Let p\u0303 : X \u2192 [0,\u221e) be an unnormalized mass function over X and let Z := \u2211 x\u2208X p\u0303(x) be its normalizing partition function. Write p(x) := p\u0303(x)/Z for the normalized version of p\u0303, and \u03c6(x) := ln p\u0303(x) for the log-unnormalized probabilities, i.e. the potential function.\nWe write Exp(\u03bb) for the exponential distribution with rate (inverse mean) \u03bb and Gumbel(\u00b5) for the Gumbel distribution with location \u00b5 and scale 1. The latter has mean \u00b5+ c, where c \u2248 0.5772 is the Euler-Mascheroni constant."}, {"heading": "2.1. The Gumbel Trick", "text": "Similarly to the connection between the Gumbel trick and the Poisson process established by Maddison (2016), we introduce the Gumbel trick for discrete probability distributions using a simple and elegant construction via competing exponential clocks. ConsiderN independent clocks,\nstarted simultaneously, such that the j-th clock rings after a random time Tj \u223c Exp(\u03bbj). Then it is easy to show that (1) the time until some clock rings has Exp( \u2211N j=1 \u03bbj) distribution, and (2) the probability of the j-th clock ringing first is proportional to its rate \u03bbj . These properties are also widely used in survival analysis (Cox & Oakes, 1984).\nConsider N competing exponential clocks {Tx}x\u2208X , indexed by elements of X , with respective rates \u03bbx = p\u0303(x). Property (1) of competing exponential clocks tells us that\nmin x\u2208X {Tx} \u223c Exp(Z). (1)\nProperty (2) says that the random variable argminx Tx, taking values in X , is distributed according to p:\nargmin x\u2208X\n{Tx} \u223c p. (2)\nThe Gumbel trick is obtained by applying the function g(x) = \u2212 lnx \u2212 c to the equalities in distribution (1) and (2). When g is applied to an Exp(\u03bb) random variable, the result follows the Gumbel(\u2212c + ln\u03bb) distribution, which can also be represented as ln\u03bb + \u03b3, where \u03b3 \u223c Gumbel(\u2212c). Defining {\u03b3(x)}x\u2208X\ni.i.d.\u223c Gumbel(\u2212c) and noting that g is strictly decreasing, applying the function g to equalities in distribution (1) and (2), we obtain:\nmax x\u2208X {\u03c6(x) + \u03b3(x)} \u223c Gumbel(\u2212c+ lnZ), (1\u2019)\nargmax x\u2208X\n{\u03c6(x) + \u03b3(x)} \u223c p, (2\u2019)\nwhere we have recalled that \u03c6(x) = ln\u03bbx = ln p\u0303(x). The distribution Gumbel(\u2212c + lnZ) has mean lnZ, and thus the log partition function can be estimated by averaging samples (Hazan & Jaakkola, 2012)."}, {"heading": "2.2. Constructing New Tricks", "text": "Given the equality in distribution (1), we can treat the problem of estimating the partition function Z as a parameter estimation problem for the exponential distribution. Applying the function g(x) = \u2212 lnx \u2212 c as in the Gumbel trick to obtain a Gumbel(\u2212c+ lnZ) random variable, and\nestimating its mean to obtain an unbiased estimator of lnZ, is just one way of inferring information about Z.\nWe consider applying different functions g to (1); particularly those functions g that transform the exponential distribution to another distribution with known mean. As the original exponential distribution has rate Z, the transformed distribution will have mean f(Z), where f will in general no longer be the logarithm function. Since we often are interested in estimating various transformations f(Z) of Z, this provides us a with a collection of unbiased estimators from which to choose. Moreover, further transforming these estimators yields a collection of (biased) estimators for other transformations of Z, including Z itself. Example 1 (Weibull tricks). For any \u03b1 > 0, applying the function g(x) = x\u03b1 to an Exp(\u03bb) random variable yields a random variable with the Weibull(\u03bb\u2212\u03b1, \u03b1\u22121) distribution with scale \u03bb\u2212\u03b1 and shape \u03b1\u22121, which has mean \u03bb\u2212\u03b1\u0393(1 + \u03b1) and can be also represented as \u03bb\u2212\u03b1W , where W \u223c Weibull(1, \u03b1\u22121). Defining {W (x)}x\u2208X\ni.i.d.\u223c Weibull(1, \u03b1\u22121) and noting that g is increasing, applying g to the equality in distribution (1) gives\nmin x\u2208X {p\u0303\u2212\u03b1W (x)} \u223cWeibull(Z\u2212\u03b1, \u03b1\u22121). (1\u201d)\nEstimating the mean of Weibull(Z\u2212\u03b1, \u03b1\u22121) yields an unbiased estimator of Z\u2212\u03b1\u0393(1 + \u03b1). The special case \u03b1 = 1 corresponds to the identity function g(x) = x; we call the resulting trick the Exponential trick.\nTable 1 lists several examples of tricks derived this way. As Example 1 shows, these tricks may not involve additive perturbation of the potential function \u03c6(x); the Weibull tricks multiplicatively perturb exponentiated unnormalized probabilities p\u0303\u2212\u03b1 with Weibull noise. As models of interest are often specified in terms of potential functions, to be able to reuse existing MAP solvers in a black-box manner with the new tricks, we seek an equivalent formulation in terms of the potential function. The following Proposition shows that by not passing the function g through the minimization in equation (1), the new tricks can be equivalently formulated as averaging additive Gumbel perturbations of the potential function in different spaces.\nProposition 2. For any function g : [0,\u221e)\u2192 R such that f(Z) = ET\u223cExp(Z)[g(T )] exists, we have\nf(Z) = E\u03b3 [ g ( e\u2212c exp ( \u2212max x\u2208X {\u03c6(x) + \u03b3(x)} ))] ,\nwhere {\u03b3(x)}x\u2208X i.i.d.\u223c Gumbel(\u2212c).\nProof. As maxx{\u03c6(x) + \u03b3(x)} \u223c Gumbel(\u2212c + lnZ), we have e\u2212c exp(maxx{\u03c6(x) +\u03b3(x)}) \u223c Exp(Z) and the result follows by the assumption relating f and g.\nProposition 2 shows that the new tricks can be implemented by solving the same MAP problems maxx{\u03c6(x)+\u03b3(x)} as in the Gumbel trick, and then merely passing the solutions through the function x 7\u2192 g(e\u2212c exp(x)) before averaging them to approximate the expectation."}, {"heading": "2.3. Comparing Tricks", "text": ""}, {"heading": "2.3.1. ASYMPTOTIC EFFICIENCY", "text": "The Delta method (Casella & Berger, 2002) is a simple technique for assessing the asymptotic variance of estimators that are obtained by a differentiable transformation of an estimator with known variance. The last column in Table 1 lists asymptotic variances of corresponding tricks when unbiased estimators of f(Z) are passed through the function f\u22121 to yield (biased, but consistent and non-negative) estimators of Z itself. It is interesting to examine the constants that multiply Z2 in some of the obtained asymptotic variance expressions for the different tricks. For example, it can be shown using Gurland\u2019s ratio (Gurland, 1956) that this constant is at least 1 for the Weibull and Fre\u0301chet tricks, which is precisely the value achieved by the Exponential trick (which corresponds to \u03b1 = 1). Moreover, the Gumbel trick constant \u03c02/6 can be shown to be the limit as \u03b1 \u2192 0 of the Weibull and Fre\u0301chet trick constants. In particular, the constant of the Exponential trick is strictly better than that of the standard Gumbel trick: 1 < \u03c02/6 \u2248 1.65. This motivates us to compare the Gumbel and Exponential tricks in more detail."}, {"heading": "2.3.2. MEAN SQUARED ERROR (MSE)", "text": "For estimators Y , their MSE(Y ) = var(Y ) + bias(Y )2 is a commonly used comparison metric. When the Gumbel or Exponential tricks are used to estimate either Z or lnZ, the biases, variances, and MSEs of the estimators can be computed analytically using standard methods (Appendix A).\nFor example, the unbiased estimator of lnZ from the Gumbel trick can be turned into a consistent non-negative estimator of Z by exponentiation: Y = exp( 1M \u2211M m=1Xm), where X1, . . . , XM i.i.d.\u223c Gumbel(\u2212c + lnZ) are obtained using equation (1\u2019). The bias and variance of Y can be computed using independence and the moment generating functions of the Xm\u2019s, see Appendix A for details.\nPerhaps surprisingly, all estimator properties only depend on the true value of Z and not on the structure of the model (distribution p), since the estimators rely only on i.i.d. samples of a Gumbel(\u2212c + lnZ) random variable. Figure 1 shows the analytically computed estimator variances and MSEs. For estimating Z itself (left), the Exponential trick outperforms the Gumbel trick in terms of MSE for all sample sizes M \u2265 3 (for M \u2208 {1, 2}, both estimators have infinite variance and MSE). The ratio of MSEs quickly approaches \u03c02/6, and in this regime the Exponential trick requires 1 \u2212 6/\u03c02 \u2248 39% fewer samples than the Gumbel trick to reach the same MSE. Also, for estimating lnZ, (Figure 1, right), the Exponential trick provides a lower MSE estimator for sample sizes M \u2265 2; only for M = 1 the Gumbel trick provides a better estimator.\nNote that as biases are available analytically, the estimators can be easily debiased (by subtracting their bias). One then obtain estimators with MSEs equal to the variances of the original estimators, shown dashed in Figure 1. The Exponential trick would then always outperform the Gumbel trick when estimating lnZ, even with sample size M = 1.\nFor Weibull tricks with \u03b1 6= 1 and Fre\u0301chet tricks, we estimated the biases and variances of estimators of Z and lnZ by constructing K = 100, 000 estimators in each case and evaluating their bias and variance. Figure 2 shows the results for varying \u03b1 and several sample sizesM . We plot the\nanalytically computed value for the Gumbel trick at \u03b1 = 0, as we observe that the Weibull trick interpolates between the Gumbel trick and the Exponential trick as \u03b1 increases from 0 to 1. We note that the minimum MSE estimator is obtained by choosing a value of \u03b1 that is close to 1, i.e. the Exponential trick. This agrees with the finding from Section 2.3.1 that \u03b1 = 1 is optimal as M \u2192\u221e."}, {"heading": "2.4. Bayesian Perspective", "text": "A Bayesian approach exposes two choices when constructing estimators of Z, or of its transformations f(Z):\n1. A choice of prior distribution p0(Z), encoding prior beliefs about the value of Z before any observations. 2. A choice of how to summarize the posterior distribution pM (Z|X1, . . . , XM ) given M samples.\nTaking the Jeffrey\u2019s prior p0(Z) \u221d Z\u22121, an improper prior that it is invariant under reparametrization, observing M samples X1, . . . , XM i.i.d.\u223c Exp(Z) yields the posterior:\npM (Z|X1, . . . , XM ) \u221d ZM\u22121e\u2212Z \u2211M m=1 Xm .\nRecognizing the density of a Gamma(M, \u2211M m=1Xm) random variable, the posterior mean is\nE[Z|X1, . . . , XM ] = M\u2211M\nm=1Xm =\n( 1\nM M\u2211 m=1 Xm\n)\u22121 ,\ncoinciding with the Exponential trick estimator of Z."}, {"heading": "3. Low-rank Perturbations", "text": "One way of exploiting perturb-and-MAP to yield computational savings is to replace independent perturbations of each configuration\u2019s potential with an approximation. Such approximations are available e.g. in discrete graphical models, where the sampling space X has a product space structure X = X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xn, with Xi the state space of the i-th variable. Definition 3 ( (Hazan & Jaakkola, 2012)). The sum-unary perturbation MAP value is the random variable\nU := max x\u2208X\n{ \u03c6(x) + n\u2211 i=1 \u03b3i(xi) } ,\nwhere {\u03b3i(xi) | xi \u2208 Xi, 1 \u2264 i \u2264 n} i.i.d\u223c Gumbel(\u2212c).\nThis definition involves |X1|+ \u00b7 \u00b7 \u00b7+ |Xn| i.i.d. Gumbel random variables, rather than |X |. (With n = 1 this coincides with full-rank perturbations andU \u223c Gumbel(\u2212c+lnZ).) For n > 2 the distribution of U is not available analytically. One can similarly define the pairwise (or higher-order) perturbations, where independent Gumbel noise is added to each pairwise (or higher-order) potential.\nUnary perturbations provide the upper bound lnZ \u2264 E[U ] on the log partition function (Hazan & Jaakkola, 2012), can be used to construct a sequential sampler for the Gibbs distribution (Hazan et al., 2013), and, if the perturbations are scaled down by a factor of n, a lower bound on lnZ can also be recovered (Hazan et al., 2013). In this section we show that a subfamily of tricks introduced in Section 2, consisting of Fre\u0301chet and Weibull (and Exponential) tricks, is applicable in the low-rank perturbation setting and use them to derive new families of upper and lower bounds on lnZ and sequential samplers for the Gibbs distribution. Please note full proofs are deferred to Appendix B and C."}, {"heading": "3.1. Upper Bounds on the Partition Function", "text": "The following family of upper bounds on lnZ can be derived from the Fre\u0301chet and Weibull tricks.\nProposition 4. For any \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e), the upper bound lnZ \u2264 U(\u03b1) holds with\nU(\u03b1) := n ln \u0393(1 + \u03b1) \u03b1 + nc\u2212 1 \u03b1\nlnE\u03b3 [ e\u2212\u03b1U ] .\nProof. (Sketch.) By induction on n, with the induction step provided by our Clamping Lemma (Lemma 7) below.\nTo evaluate these bounds in practice, E[e\u2212\u03b1U ] is estimated using samples of U . Corollary 9 of Hazan et al. (2016) can be used to show that var(e\u2212\u03b1U ) is finite for \u03b1 > \u2212 1\n2 \u221a n , and so then the estimation is well-behaved.\nA natural question is how these new bounds relate to the Gumbel trick upper bound lnZ \u2264 E[U ] by Hazan & Jaakkola (2012). The following result aims to answers this:\nProposition 5. The limit of U(\u03b1) as \u03b1 \u2192 0 exists and equals U(0) := E[U ], i.e. the Gumbel trick upper bound.\nThe question remains: When is it advantageous to use a value \u03b1 6= 0 to obtain a tighter bound on lnZ than the Gumbel trick bound? The next result can provide guidance:\nProposition 6. The function U(\u03b1) is differentiable at \u03b1 = 0 and the derivative equals\nd\nd\u03b1 U(\u03b1) \u2223\u2223\u2223\u2223 \u03b1=0 = 1 2 ( n \u03c02 6 \u2212 var(U) ) .\nWhile the variance of U is generally not tractable, in practice one obtains samples fromU to estimate the expectation in U(\u03b1) and these samples can be reused to assess var(U). Interestingly, var(U) equals n\u03c02/6 for both the uniform distribution and the distribution concentrated on a single configuration, and in our empirical investigations always var(U) \u2264 n\u03c02/6. Then the derivative at 0 is non-negative and Fre\u0301chet tricks provide tighter bounds on lnZ. However, as U(\u03b1) is estimated with samples, the question of\nestimator variance arises. We investigate the trade-off between tightness of the bound lnZ \u2264 U(\u03b1) and the variance incurred in estimating U(\u03b1) empirically in Section 5.3."}, {"heading": "3.2. Clamping", "text": "Consider the partial sum-unary perturbation MAP values, where the values of the first j\u22121 variables have been fixed, and only the rest are perturbed:\nUj(x1, . . . , xj\u22121) := max xj ,...,xn \u03c6(x) + n\u2211 i=j \u03b3i(xi)  . The following lemma involving the Uj\u2019s serves three purposes: (I.) it provides the induction step for Proposition 4, (II.) it shows that clamping never hurts partition function estimation with Fre\u0301chet and Weibull tricks, and (III.) it will be used to show that a sequential sampler constructed in Section 3.3 below is well-defined.\nLemma 7 (Clamping Lemma). For any j \u2208 {1, . . . , n} and (x1, . . . , xj\u22121) \u2208 X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xj\u22121, the following inequality holds with any \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e):\u2211\nxj\u2208Xj\nE\u03b3 [ e\u2212(n\u2212j) ln \u0393(1+\u03b1)\u2212\u03b1(n\u2212j)c)e\u2212\u03b1Uj+1 ]\u22121/\u03b1 \u2264 E\u03b3 [ e\u2212(n\u2212(j\u22121)) ln \u0393(1+\u03b1)\u2212\u03b1(n\u2212(j\u22121))c)e\u2212\u03b1Uj\n]\u22121/\u03b1 Proof. This follows directly from the Fre\u0301chet trick (\u03b1 \u2208 (\u22121, 0)) or the Weibull trick (\u03b1 > 0) and representing the Fre\u0301chet resp. Weibull random variables in terms of Gumbel random variables. See Appendix B.1 for more details.\nCorollary 8. Clamping never hurts lnZ estimation using any of the Fre\u0301chet or Weibull upper bounds U(\u03b1).\nProof. Applying the function x 7\u2192 ln(x) to both sides of the Clamping Lemma 7 with j = 1, the right-hand side equals U(\u03b1), while the left-hand side is the estimate of lnZ after clamping variable x1.\nThis was shown previously in restricted settings (Hazan et al., 2013; Zhao et al., 2016). Similar results showing that clamping improves partition function estimation have been obtained for the mean field and TRW approximations (Weller & Domke, 2016), and in certain settings for the Bethe approximation (Weller & Jebara, 2014b) and LFIELD (Zhao et al., 2016)."}, {"heading": "3.3. Sequential Sampling", "text": "Hazan et al. (2013) derived a sequential sampling procedure for the Gibbs distribution by exploiting the U(0) Gumbel trick upper bound on lnZ. In the same spirit, one\ncan derive sequential sampling procedures from the Fre\u0301chet and Weibull tricks, leading to the following algorithm.\nAlgorithm 1 Sequential sampler for Gibbs distribution Input: \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e), potential function \u03c6 on X Output: a sample x from the Gibbs distribution \u221d e\u03c6(x)\n1: for j = 1 to n do 2: for xj \u2208 Xj do\n3: pj(xj)\u2190 e \u2212c \u0393(1+\u03b1)1/\u03b1 E\u03b3 [e\u2212\u03b1Uj+1(x1,...,xj)]\n\u22121/\u03b1\nE\u03b3 [e\u2212\u03b1Uj(x1,...,xj\u22121)] \u22121/\u03b1 4: pj(reject)\u2190 1\u2212 \u2211 xj\u2208Xj pj(xj) 5: xj \u2190 sample according to pj 6: if xj == reject then 7: RESTART (goto 1)\nThis algorithm is well-defined if pj(reject) \u2265 0 for all j, which can be shown by canceling terms in the Clamping Lemma 7. We discuss correctness in Appendix B.2. As for the Gumbel sequential sampler of Hazan et al. (2013), the expected number of restarts (and hence the running time) only depend on the quality of the upper bound (U(\u03b1) \u2212 lnZ), and not on the ordering of variables."}, {"heading": "3.4. Lower Bounds on the Partition Function", "text": "Similarly as in the Gumbel trick case (Hazan et al., 2013), one can derive lower bounds on lnZ by perturbing an arbitrary subset S of variables.\nProposition 9. Let X = X1 \u00d7 \u00b7 \u00b7 \u00b7 Xn be a product space and \u03c6 a potential function on X . Let \u03b1 \u2208 (\u22121, 0)\u222a (0,\u221e). For any subset S \u2286 {1, . . . , n} of the variables x1, . . . , xn we have lnZ \u2265\nc+ ln \u0393(1 + \u03b1) \u03b1 \u2212 1 \u03b1\nlnE [ e\u2212\u03b1maxx{\u03c6(x)+\u03b3S(xS)} ] ,\nwhere xS := {xi : i \u2208 S} and \u03b3S(xS) \u223c Gumbel(\u2212c) independently for each setting of xS .\nBy averaging n such lower bounds corresponding to singleton sets S = {i} together, we obtain a lower bound on lnZ that involves the average-unary perturbation MAP value\nL := max x\u2208X\n{ \u03c6(x) + 1\nn n\u2211 i=1 \u03b3i(xi)\n} .\nCorollary 10. For any \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e), we have the lower bound lnZ \u2265 L(\u03b1), where\nL(\u03b1) := c+ ln \u0393(1 + \u03b1) \u03b1 \u2212 1 n\u03b1 lnE [exp (\u2212n\u03b1L)] .\nAgain, L(0) := E[L] can be defined by continuity, where E[L] \u2264 lnZ is the Gumbel trick lower bound by Hazan et al. (2013)."}, {"heading": "4. Advantages of the Gumbel Trick", "text": "We have seen how the Gumbel trick can be embedded into a continuous family of tricks, consisting of Fre\u0301chet, Exponential, and Weibull tricks. We showed that the new tricks can provide more efficient estimators of the partition function in the full-rank perturbation setting (Section 2), and in the low-rank perturbation setting lead to sequential samplers and new bounds on lnZ, which can be also more efficient, as we investigate in Section 5.3. To balance the discussion of merits of different tricks, in this section we briefly highlight advantages of the Gumbel trick that stem from its simpler analytical form.\nFirst, by consulting Table 1 we see that the function g(x) = \u2212 lnx\u2212c has the property that the variance of the resulting estimator (of lnZ) does not depend on the value of Z; the function g is a variance stabilizing transformation for the Exponential distribution.\nSecond, exploiting the fact that the logarithm function leads to additive perturbations, Maji et al. (2014) showed that the entropy of x\u2217, the configuration with maximum potential after sum-unary perturbation in the sense of Definition 3, can be bounded as H(x\u2217) \u2264 B(p) := \u2211n i=1 E\u03b3i [\u03b3i(x\u2217i )]. We extend this result to show how the errors of bounding lnZ, sampling, and entropy estimation are related: Proposition 11. Writing p for the Gibbs distribution and B(p) := E\u03b3i [\u03b3i(x\u2217i )] for the entropy bound, we have\n(U(0)\u2212 lnZ)\ufe38 \ufe37\ufe37 \ufe38 error in lnZ bound + KL(x\u2217 \u2016 p)\ufe38 \ufe37\ufe37 \ufe38 sampling error = B(p)\u2212H(x\u2217)\ufe38 \ufe37\ufe37 \ufe38 error in entropy estimation .\nThird, the additive character of the Gumbel perturbations can also be used to derive a new result relating the error of the lower bound L(0) and of sampling x\u2217\u2217 as the configuration achieving the maximum average-unary perturbation value L, instead of sampling from the Gibbs distribution p: Proposition 12. Writing p for the Gibbs distribution,\nlnZ \u2212 L(0)\ufe38 \ufe37\ufe37 \ufe38 error in lnZ bound \u2265 KL(x\u2217\u2217 \u2016 p)\ufe38 \ufe37\ufe37 \ufe38 sampling error \u2265 0.\nRemark. While we knew from Hazan et al. (2013) that lnZ \u2212 L(0) \u2265 0, this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the approximate sampling distribution of x\u2217\u2217 and the Gibbs distribution p.\nProofs of the new results appear in Appendix B.3 and C.2.\nFourth, viewed as a function of the Gumbel perturbations \u03b3, the random variable U has a bounded gradient, allowing earlier measure concentration results (Orabona et al., 2014; Hazan et al., 2016). Proving similar measure concentration results for the expectations E[e\u2212\u03b1U ] appearing in U(\u03b1) for \u03b1 6= 0 may be more challenging."}, {"heading": "5. Experiments", "text": "We conducted experiments with the following aims:\n1. To show that the higher efficiency of the Exponential trick in the full-rank perturbation setting is useful in practice, we compared it to the Gumbel trick in A* sampling (Maddison et al., 2014) (Section 5.1) and in the large-scale discrete sampling setting of Chen & Ghahramani (2016) (Section 5.2). 2. To show that non-zero values of \u03b1 can lead to better estimators of lnZ in the low-rank perturbation setting as well, we compare the Fre\u0301chet and Weibull trick bounds U(\u03b1) to the Gumbel trick bound U(0) on a common discrete graphical model with different coupling strengths; see Section 5.3."}, {"heading": "5.1. A* Sampling", "text": "A* sampling (Maddison et al., 2014) is a sampling algorithm for continuous distributions that perturbs the logunnormalized density \u03c6 with a continuous generalization of the Gumbel trick, called the Gumbel process, and uses a variant of A* search to find the location of the maximum of the perturbed \u03c6. Returning the location yields an exact sample from the original distribution, as in the discrete Gumbel trick. Moreover, the corresponding maximum value also has the Gumbel(\u2212c + lnZ) distribution (Maddison et al., 2014). Our analysis in Section 2.3 tells us that the Exponential trick yields an estimator with lower MSE than the Gumbel trick; we briefly verified this on the Robust Bayesian Regression experiment of Maddison et al. (2014). We constructed estimators of lnZ from the Gumbel and Exponential tricks (debiased version, see Section 2.3.2), and assessed their variances by constructing each estimator K = 1000 times and looking at the sample variance. Figure 3a shows that the Exponential trick requires up to 40% fewer samples to reach a given MSE."}, {"heading": "5.2. Scalable Partition Function Estimation", "text": "Chen & Ghahramani (2016) considered sampling from a discrete distribution of the form p(x) \u221d f0(x) \u220fS s=1 fs(x) when the number of factors S is large relative to the sample space size |X |. Computing i.i.d. Gumbel perturbations \u03b3(x) for each x \u2208 X is then relatively cheap compared to evaluating all potentials \u03c6(x) = f0(x) + \u2211S s=1 ln fs(x). Chen & Ghahramani (2016) observed that each (perturbed) potential can be estimated by subsampling the factors, and potentials that appear unlikely to yield the MAP value can be pruned off from the search early on. The authors formalized the problem as a Multi-armed bandit problem with a finite reward population and derived approximate algorithms for efficiently finding the maximum perturbed potential with a probabilistic guarantee.\nWhile Chen & Ghahramani (2016) considered sampling, by modifying their procedure to return the value of the maximum perturbed potential rather than the argmax (cf equations (1) and (2)), we can estimate the partition function instead. However, the approximate algorithm only guarantees to find the MAP configuration with a probability 1 \u2212 \u03b4. Figure 3b shows the results of running the Racing-Normal algorithm of Chen & Ghahramani (2016) on the synthetic dataset considered by the authors with the \u201cvery hard\u201d noise setting \u03c3 = 0.1. For low error bounds \u03b4 the Exponential trick remained close to optimal, but for a larger error bound the Weibull trick interpolation between the Gumbel and Exponential tricks proved useful to provide an estimator with lower MSE.\n5.3. Low-rank Perturbation Bounds on lnZ\nHazan & Jaakkola (2012) evaluated tightness of the Gumbel trick upper bound U(0) \u2265 lnZ on 10\u00d7 10 binary spin glass models. We show one can obtain more accurate estimates of lnZ on such models by choosing \u03b1 6= 0. To account for the fact that in practice an expectation in U(\u03b1) is replaced with a sample average, we treat U(\u03b1) as an estimator of lnZ with asymptotic bias equal to the bound gap (U(\u03b1)\u2212 lnZ), and estimate its MSE.\nFigure 4 shows the MSEs of U(\u03b1) as estimators of lnZ on 10\u00d7 10 (n = 100) binary pairwise grid models with unary potentials sampled uniformly from [\u22121, 1] and pairwise potentials from [0, C] (attractive models) or from [\u2212C,C] (mixed models), for varying coupling strengths C. We replaced the expectations in U(\u03b1)\u2019s with sample averages of size M = 100, using libDAI (Mooij, 2010) to solve the MAP problems yielding these samples. We constructed each estimator 1000 times to assess its variance."}, {"heading": "6. Discussion", "text": "By casting partition function evaluation as a parameter estimation problem for the exponential distribution, we derived a family of methods of which the Gumbel trick is a special case. These methods can be equivalently seen as (1) perturbing models using different distributions, or as (2) averaging standard Gumbel perturbations in different spaces, allowing implementations with little additional cost.\nWe showed that in the full-rank perturbation setting, the new Exponential trick provides an estimator with lower MSE, or instead allows using up to 40% fewer samples than the Gumbel trick estimator to reach the same MSE.\nIn the low-rank perturbation setting, we used our Fre\u0301chet, Exponential and Weibull tricks to derive new bounds on lnZ and sequential samplers for the Gibbs distribution, and showed that these can also behave better than the corresponding Gumbel trick results. However, the optimal trick to use (as specified by \u03b1) depends on the model, sample size, and MAP solver used (if approximate). Since in practice the dominant computational cost is carried by solving repeated instances of the MAP problem, one can try and assess different values of \u03b1 on the problem at hand. That said, we believe that investigating when different tricks yield better results is an interesting avenue for future work.\nFinally, we balanced the discussion by pointing out that the Gumbel trick has a simpler analytical form which can be exploited to derive more interesting theoretical statements in the low-rank perturbation setting. Beyond existing results, we derived new connections between errors of different procedures using low-rank Gumbel perturbations."}, {"heading": "Acknowledgements", "text": "The authors thank Tamir Hazan for helpful discussions, and Mark Rowland, Maria Lomeli, and the anonymous reviewers for helpful comments. AW acknowledges support by the Alan Turing Institute under EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI."}, {"heading": "APPENDIX: Lost Relatives of the Gumbel Trick", "text": "Here we provide proofs for the results stated in the main text, together with additional supporting lemmas required for these proofs."}, {"heading": "A. Comparison of Gumbel and Exponential tricks", "text": "In Section 2.3.1 we analyzed the asymptotic efficiency of different estimators of Z by measuring their asymptotic variance. (As all our estimators in the full-rank perturbation setting are consistent, their bias is 0 in the limit of infinite data, and so this asymptotic variance equals the asymptotic MSE.) In the non-asymptotic regime, where an estimator Z\u0302 is constructed from a finite set of M samples, we can analyze both the variance var(Z\u0302) and the bias (E[Z\u0302]\u2212 Z) of the estimator. While in most cases these cannot be obtained analytically and there we can resort to an empirical evaluation, for the estimators stemming from the Gumbel and Exponential tricks analytical treatment turns out to be possible using standard methods.\nA.1. Estimating Z\nGumbel trick The Gumbel trick yields an unbiased estimator for lnZ, and we can turn it into a consistent estimator of Z by exponentiating it:\nZ\u0302 := exp\n( 1\nM M\u2211 m=1 Xm\n) where X1, . . . , XM iid\u223c Gumbel(\u2212c+ lnZ).\nRecalling that the moment generating function of a Gumbel(\u00b5) distribution is G(t) = \u0393(1\u2212 t)e\u00b5t, we can obtain by using independence of the samples:\nE[Z\u0302] = M\u220f m=1 E[eXm/M ] = ( \u0393(1\u2212 1/M)e(lnZ\u2212c)/M )M = \u0393(1\u2212 1/M)Me\u2212cZ,\nE[Z\u03022] = M\u220f m=1 E[e2Xm/M ] = ( \u0393(1\u2212 2/M)e2(lnZ\u2212c)/M )M = \u0393(1\u2212 2/M)Me\u22122cZ2.\nTherefore the squared bias, variance and MSE of the estimator Z\u0302 are, respectively:\nbias(Z\u0302)2 = (E[Z\u0302]\u2212 Z)2 = Z2 ( \u0393(1\u2212 1/M)Me\u2212c \u2212 1 ) ,\nvar(Z\u0302) = E[Z\u03022]\u2212 E[Z\u0302]2 = Z2 ( \u0393(1\u2212 2/M)Me\u22122c \u2212 \u0393(1\u2212 1/M)2Me\u22122c ) ,\nMSE(Z\u0302) = bias(Z\u0302)2 + var(Z\u0302) = Z2 ( \u0393(1\u2212 2/M)Me\u22122c \u2212 2\u0393(1\u2212 1/M)Me\u2212c + 1 ) .\nThese formulas hold for M > 2 where the moment generating functions are defined. For M = 1 the estimator has infinite bias (and infinite variance), and for M = 2 it has infinite variance. Figure 1 (left) shows the functional dependence of MSE(Z\u0302) on the number of samples M \u2265 3, in units of Z2.\nExponential trick The Exponential trick yields an unbiased estimator of 1/Z, and we can turn it into a consistent estimator of Z by inverting it:\nZ\u0302 :=\n( 1\nM M\u2211 m=1 Xm\n)\u22121 where X1, . . . , XM iid\u223c Exp(Z).\nAs X1, . . . , XM are independent and exponentially distributed with identical rates Z, their sum follows the Gamma distribution with shape M and rate Z. Therefore the estimator Z\u0302 can be written as Z\u0302 = MY , where Y \u223c InvGamma(M,Z).\nRecalling the mean and variance of the Inverse-Gamma distribution, we obtain: bias(Z\u0302)2 = (E[Z\u0302]\u2212 Z)2 = Z2 ( M M \u2212 1 \u2212 1 ) = Z2 1 M \u2212 1 ,\nvar(Z\u0302) = Z2M2 1\n(M \u2212 1)2(M \u2212 2) ,\nMSE(Z\u0302) = bias(Z\u0302)2 + var(Z\u0302) = Z2 M \u2212 2 +M2\n(M \u2212 1)2(M \u2212 2) = Z2\nM + 2\n(M \u2212 1)(M \u2212 2) .\nAgain these formulas hold for M > 2 where the relevant expectations are defined: for M = 1 the estimator has infinite bias, and for M \u2208 {1, 2} it has infinite variance. Figure 1 (left) shows the functional dependence of MSE(Z\u0302) on the number of samples M \u2265 3, in units of Z2. By inspecting the curves we observe that the Gumbel trick estimator requires roughly 45% more samples to yield the same MSE as the Exponential trick estimator.\nA.2. Estimating lnZ\nA similar analysis can be performed for estimating lnZ rather than Z. In that case the Gumbel trick estimator of lnZ is unbiased and has variance (and thus MSE) equal to 1M \u03c02 6 . On the other hand, the Exponential trick estimator is\nl\u0302nZ = \u2212 ln\n( 1\nM M\u2211 m=1 Xm\n) where X1, . . . , XM iid\u223c Exp(Z).\nAgain \u2211M m=1Xm \u223c Gamma(M,Z) and by reference to properties of the Gamma distribution,\nbias(l\u0302nZ)2 = (E[Z\u0302]\u2212 Z)2 = (lnM \u2212 (\u03c8(M)\u2212 lnZ)\u2212 lnZ)2 = (lnM \u2212 \u03c8(M))2 ,\nvar(l\u0302nZ) = \u03c81(M),\nMSE(l\u0302nZ) = bias(l\u0302nZ)2 + var(l\u0302nZ) = (lnM \u2212 \u03c8(M))2 + \u03c81(M),\nwhere \u03c8(\u00b7) is the digamma function and \u03c81(\u00b7) is the trigamma function. Note that the estimator can be debiased by subtracting its bias (lnM \u2212 \u03c8(M)). Figure 1 (right) compares the MSE of the Gumbel and Exponential trick estimators of lnZ. We observe that the Gumbel trick estimator performs better only for M = 1, and even in that case the Exponential trick estimator is better when debiased."}, {"heading": "B. Sum-unary perturbations", "text": "Recall that sum-unary perturbations refer to the setting where each variable\u2019s unary potentials are perturbed with Gumbel noise, and the perturbed potential of a configuration sums the perturbations from all variables (see Definition 3 in the main text). Using sum-unary perturbations we can derive a family U(\u03b1) of upper bounds on the log partition function (Proposition 4) and construct sequential samplers for the Gibbs distribution (Algorithm 1). Here we provide proofs for the related results stated in Sections 3.1 and 3.2.\nNotation We will write pow\u03b2 x for x\u03b2 , where x, \u03b2 \u2208 R, when we find this increases clarity of our exposition. Lemma 13 (Weibull and Fre\u0301chet tricks). For any finite set Y and any function h, we have\npow \u2212\u03b1 \u2211 y\u2208Y pow \u22121/\u03b1 h(y) = EW [ min y { h(y) W (y) \u0393(1 + \u03b1) }] where {W (y)}y\u2208Y i.i.d.\u223c Weibull(1, \u03b1\u22121) for \u03b1 \u2208 (0,\u221e),\npow \u2212\u03b1 \u2211 y\u2208Y pow \u22121/\u03b1 h(y) = EF [ max y { h(y) F (y) \u0393(1 + \u03b1) }] where {F (y)}y\u2208Y i.i.d.\u223c Fre\u0301chet(1,\u2212\u03b1\u22121) for \u03b1 \u2208 (\u22121, 0).\nProof. This follows from setting up competing exponential clocks with rates \u03bby = h(y)\u22121/\u03b1 and then applying the function g(x) = x\u03b1 as in Example 1 for the case of the Weibull trick. The case of the Fre\u0301chet trick is similar, except that g is strictly decreasing for \u03b1 \u2208 (\u22121, 0), hence the maximization in place of the minimization.\nB.1. Upper bounds on the partition function\nProposition 4. For any \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e), the upper bound lnZ \u2264 U(\u03b1) holds with\nU(\u03b1) := n ln \u0393(1 + \u03b1) \u03b1 + nc\u2212 1 \u03b1\nlnE\u03b3 [ e\u2212\u03b1U ] .\nProof. We show the result for \u03b1 \u2208 (0,\u221e) using the Weibull trick; the case of \u03b1 \u2208 (\u22121, 0) can be proved similarly using the Fre\u0301chet trick. The idea is to prove by induction on n that Z\u2212\u03b1 \u2265 e\u2212\u03b1U(\u03b1), so that the claimed result follows by applying the monotonically decreasing function x 7\u2192 \u2212 ln(x)/\u03b1.\nThe base case n = 1 is the Clamping Lemma 7 below with j = n = 1. Now assume the claim for n \u2212 1 \u2265 1 and for xn \u2208 Xn define\nUn\u22121(\u03b1, x1) := (n\u2212 1) ln \u0393(1 + \u03b1) \u03b1 + (n\u2212 1)c\u2212 1 \u03b1 lnE\u03b3\n[ exp ( \u2212\u03b1 max\nx2,...,xn\n{ \u03c6(x) +\nn\u2211 i=2 \u03b3i(xi)\n})] .\nWith this definition, the Clamping Lemma with j = 1 states that \u2211 x1 pow\u22121/\u03b1 e \u2212\u03b1Un\u22121(\u03b1,x1) \u2264 pow\u22121/\u03b1 e\u2212\u03b1U(\u03b1), so:\nZ\u2212\u03b1 \u2265 pow \u2212\u03b1 \u2211 x1\u2208X1 pow \u22121/\u03b1 e\u2212\u03b1Un\u22121(\u03b1,x1) [inductive hypothesis]\n\u2265 pow \u2212\u03b1 pow \u22121/\u03b1\ne\u2212\u03b1U(\u03b1) [Clamping Lemma]\n= e\u2212\u03b1U(\u03b1),\nas required to complete the inductive step.\nProposition 5. The limit of U(\u03b1) as \u03b1\u2192 0 exists and equals U(0) := E[U ], i.e. the Gumbel trick upper bound.\nProof. Recall that U(\u03b1) = n ln \u0393(1+\u03b1)\u03b1 + nc \u2212 1 \u03b1 lnE\n[ e\u2212\u03b1U ] . The first term tends to n\u03c8(1) = \u2212cn as \u03b1 \u2192 0 by\nL\u2019Ho\u0302pital\u2019s rule, where \u03c8 is the digamma function. The second term is constant in \u03b1. In the last term, E [ e\u2212\u03b1U ] is the moment generating function of U evaluated at \u2212\u03b1, and as such its derivative at \u03b1 = 0 exists and equals the negative of the mean of U . Hence by L\u2019Ho\u0302pital\u2019s rule,\n\u2212 lim \u03b1\u21920\n1 \u03b1 lnE\n[ e\u2212\u03b1U ] = \u2212 lim\n\u03b1\u21920 \u2212E[U ] E [e\u2212\u03b1U ] = E[U ] = U(0).\nThe claimed result then follows by the Algebra of Limits, as the contributions of the first two terms cancel.\nProposition 6. The function U(\u03b1) is differentiable at \u03b1 = 0 and the derivative equals\nd\nd\u03b1 U(\u03b1) \u2223\u2223\u2223\u2223 \u03b1=0 = n \u03c02 12 \u2212 var(U) 2 .\nProof. First we show that U(\u03b1) is differentiable on (\u22121, 0) \u222a (0,\u221e), and that the limit of the derivative as \u03b1 \u2192 0 exists and equals n\u03c02/12\u2212 var(U)/2.\nThe first term of U(\u03b1) is n ln \u0393(1+\u03b1)\u03b1 , which is differentiable for \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e) by the Quotient Rule, and its derivative equals\nd\nd\u03b1 n\nln \u0393(1 + \u03b1)\n\u03b1 = n \u03c8(1 + \u03b1)\u03b1\u2212 ln \u0393(1 + \u03b1) \u03b12 ,\nwhere \u03c8 is the digamma function (logarithmic derivative of the gamma function). Applying L\u2019Ho\u0302pital\u2019s rule we note that\nlim \u03b1\u21920\nd\nd\u03b1 n\nln \u0393(1 + \u03b1)\n\u03b1 = n lim \u03b1\u21920 \u03c8(1 + \u03b1) + \u03b1\u03c8(1)(1 + \u03b1)\u2212 \u03c8(1 + \u03b1) 2\u03b1 = n \u03c8(1)(1) 2 = n \u03b6(2) 2 = n \u03c02 12 ,\nwhere \u03c8(1) is the trigamma function (derivative of the digamma function), whose value at 1 is known to be \u03b6(2) = \u03c02/6, the Riemann zeta function evaluated at 2.\nThe second term of U(\u03b1) is constant in \u03b1. The last term can be written as K(\u2212\u03b1)/(\u2212\u03b1), where K is the cumulant generating function (logarithm of the moment generating function) of the random variable U . The cumulant generating function is differentiable, and by the Quotient rule\nd\nd\u03b1 K(\u2212\u03b1) \u2212\u03b1 = \u2212\u03b1K \u2032(\u2212\u03b1)\u2212K(\u2212\u03b1) \u03b12 .\nApplying L\u2019Ho\u0302pital\u2019s rule we note that\nlim \u03b1\u21920\nd\nd\u03b1 K(\u2212\u03b1) \u2212\u03b1 = lim \u03b1\u21920 K \u2032(\u2212\u03b1) + \u03b1K \u2032\u2032(\u2212\u03b1)\u2212K \u2032(\u2212\u03b1) 2\u03b1 = K \u2032\u2032(0) 2 = var(U) 2 ,\nwhere we have used that the second derivative of the cumulant generating function is the variance.\nAs U(\u03b1) is continuous at 0 by construction, the above implies that it has left and right derivatives at 0. As the values of these derivatives coincide, the function is differentiable at 0 and the derivative has the stated value.\nRecall that for a variable index j \u2208 {1, . . . , n} we also defined partial sum-unary perturbations\nUj(x1, . . . , xj\u22121) := max xj ,...,xn \u03c6(x) + n\u2211 i=j \u03b3i(xi)  , which fix the variables x1, . . . , xj\u22121 and perturb the remaining ones. Lemma 7 (Clamping Lemma). For any j \u2208 {1, . . . , n} and any fixed partial variable assignment (x1, . . . , xj\u22121) \u2208 X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xj\u22121, the following inequality holds with any trick parameter \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e):\u2211\nxj\u2208Xj\nE\u03b3 [ e\u2212(n\u2212j) ln \u0393(1+\u03b1)\u2212\u03b1(n\u2212j)c)e\u2212\u03b1Uj+1(x1,...,xj) ]\u22121/\u03b1 \u2264 E\u03b3 [ e\u2212(n\u2212(j\u22121)) ln \u0393(1+\u03b1)\u2212\u03b1(n\u2212(j\u22121))c)e\u2212\u03b1Uj(x1,...,xj\u22121) ]\u22121/\u03b1 .\nProof. For \u03b1 > 0, from the Weibull trick (Lemma 13), using independence of the perturbations and Jensen\u2019s inequality,\npow \u2212\u03b1 \u2211 xj\u2208Xj pow \u22121/\u03b1 EW  min xj+1,...,xn p\u0303(x)\u2212\u03b1 n\u220f i=j+1 W (xi) \u0393(1 + \u03b1)  = EW  min xj\u2208Xj EW  min xj+1,...,xn p\u0303(x)\u2212\u03b1 n\u220f\ni=j+1\nW (xi)\n\u0393(1 + \u03b1)  W (xj) \u0393(1 + \u03b1)  \n\u2264 EW  min xj ,...,xn p\u0303(x)\u2212\u03b1 n\u220f i=j W (xi) \u0393(1 + \u03b1)  Representing the Weibull random variables in terms of Gumbel random variables using the transformation W = e\u2212(\u03b3+c)\u03b1, where \u03b3 \u223c Gumbel(\u2212c), and manipulating the obtained expressions yields the claimed result.\nB.2. Sequential samplers for the Gibbs distribution\nThe family of sequential samplers for the Gibbs distribution presented in the main text as Algorithm 1 has the same overall structure as the sequential sampler derived by Hazan et al. (2013) from the Gumbel trick upper bound U(0), and hence correctness can be argued similarly. Conditioned on accepting the sample, the probability that x = (x1, . . . , xn) is returned is\nn\u220f i=1 pi(xi) = n\u220f i=1\ne\u2212c \u0393(1 + \u03b1)1/\u03b1 E\u03b3 [ e\u2212\u03b1Ui+1(x1,...,xi) ]\u22121/\u03b1 E\u03b3 [ e\u2212\u03b1Ui(x1,...,xi\u22121) ]\u22121/\u03b1 = e\u2212nc\u0393(1 + \u03b1)n/\u03b1 ( e\u2212\u03b1\u03c6(x1,...,xn) )\u22121/\u03b1 E[e\u2212\u03b1U ]\u22121/\u03b1 \u221d p(x),\nas required to show that the produced samples follow the Gibbs distribution p. Note, however, that in practice one introduces an approximation by replacing expectations with sample averages.\nB.3. Relationship between errors of sum-unary Gumbel perturbations\nWe write x\u2217 for the (random) MAP configuration after sum-unary perturbation of the potential function, i.e.,\nx\u2217 := argmax x\u2208X\n{ \u03c6(x) +\nn\u2211 i=1 \u03b3i(xi)\n} .\nLet qsum(x) := P[x = x\u2217] be the probability mass function of x\u2217.\nThe following results links together the errors acquired when using summed unary perturbations to upper bound the log partition function lnZ \u2264 U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum using the bound due to Maji et al. (2014). Proposition 11. Writing p for the Gibbs distribution, we have\n(U(0)\u2212 lnZ)\ufe38 \ufe37\ufe37 \ufe38 error in lnZ bound + KL(qsum \u2016 p)\ufe38 \ufe37\ufe37 \ufe38 sampling error = E\u03b3i [\u03b3i(x\u2217i )]\u2212H(qsum)\ufe38 \ufe37\ufe37 \ufe38 error in entropy estimation .\nProof. By conditioning on the maximizing configuration x\u2217, we can rewrite the Gumbel trick upper bound U(0) as follows:\nU(0) = E\u03b3 [ max x\u2208X { \u03b8(x) + n\u2211 i=1 \u03b3i(xi) }]\n= \u2211 x\u2208X qsum(x)\n( \u03b8(x) + E\u03b3 [ n\u2211 i=1 \u03b3i(xi) | x = x\u2217 ])\n= \u2211 x\u2208X qsum(x)\u03b8(x) + n\u2211 i=1 E\u03b3i [\u03b3i(x\u2217i )] .\nAt the same time, the KL divergence between qsum and the Gibbs distribution p generally expands as\nKL(qsum \u2016 p) = \u2212H(qsum)\u2212 \u2211 x\u2208X qsum(x) ln exp (\u03b8(x))\u2211 x\u0303\u2208X exp (\u03b8(x\u0303))\n= \u2212H(qsum)\u2212 \u2211 x\u2208X qsum(x)\u03b8(x) + lnZ.\nAdding the two equations together and rearranging yields the claimed result."}, {"heading": "C. Averaged unary perturbations", "text": "C.1. Lower bounds on the partition function\nIn the main text we stated the following two lower bounds on the log partition function lnZ. Proposition 9. Let \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e). For any subset S \u2286 {1, . . . , n} of the variables x1, . . . , xn we have lnZ \u2265\nc+ ln \u0393(1 + \u03b1) \u03b1 \u2212 1 \u03b1\nlnE [ e\u2212\u03b1maxx{\u03c6(x)+\u03b3S(xS)} ] ,\nwhere xS := {xi : i \u2208 S} and \u03b3S(xS) \u223c Gumbel(\u2212c) independently for each setting of xS .\nProof. Let S\u0304 := {1, . . . , n} \\ S. First we handle the case \u03b1 > 0. We have trivially that pow\u2212\u03b1 Z = pow\u2212\u03b1 \u2211 xS \u2211 xS\u0304 e\u03c6(xS ,xS\u0304) \u2264 pow\u2212\u03b1 \u2211 xS max xS\u0304 e\u03c6(xS ,xS\u0304).\nThe Weibull trick tells us that pow\u2212\u03b1 \u2211 y pow\u22121/\u03b1 h(y) = EW [miny h(y) \u0393(1+\u03b1)W (y)] where {W (y)}y\niid\u223c Weibull(1, \u03b1\u22121). Applying this to the summation over xS on the right-hand side of the above inequality, we obtain\npow\u2212\u03b1 Z \u2264 EW [ min xS pow\u2212\u03b1 maxxS\u0304 e \u03c6(xS ,xS\u0304) \u0393(1 + \u03b1) W (xS) ] .\nExpressing the Weibull random variable W (xS) as e\u2212\u03b1(\u03b3S(xS)+c) with \u03b3S(xS) \u223c Gumbel(\u2212c), the right-hand side can be simplified as follows:\npow\u2212\u03b1 Z \u2264 1 \u0393(1 + \u03b1) E\u03b3 [ pow\u2212\u03b1 max xS max xS\u0304 e\u03c6(xS ,xS\u0304)e\u03b3S(xS)+c ]\n= e\u2212\u03b1c \u0393(1 + \u03b1) E\u03b3 [ exp ( \u2212\u03b1max x {\u03c6(x) + \u03b3S(xS)} )] .\nTaking the logarithm and dividing by \u2212\u03b1 < 0 yields the claimed result for positive \u03b1. For \u03b1 \u2208 (\u22121, 0) we proceed similarly, obtaining that\npow\u2212\u03b1 Z \u2265 pow\u2212\u03b1 \u2211 xS max xS\u0304 e\u03c6(xS ,xS\u0304)\n= EF [ min xS pow\u2212\u03b1 maxxS\u0304 e \u03c6(xS ,xS\u0304) \u0393(1 + \u03b1) F (xS) ] ,\nwhere F (x(S)) \u223c Fre\u0301chet(1,\u2212\u03b1\u22121). Representing these random variables as e\u2212\u03b1(\u03b3S(xS)+c) with \u03b3S(xS) \u223c Gumbel(\u2212c), simplifying as in the previous case and finally dividing the inequality by \u2212\u03b1 > 0 yields the claimed result for \u03b1 \u2208 (\u22121, 0).\nCorollary 10. For any \u03b1 \u2208 (\u22121, 0) \u222a (0,\u221e), we have the lower bound lnZ \u2265 L(\u03b1), where\nL(\u03b1) := c+ ln \u0393(1 + \u03b1) \u03b1 \u2212 1 n\u03b1 lnE [exp (\u2212n\u03b1L)] ,\nProof. Applying Proposition 9 n times with all singleton sets S = {i} and averaging the obtained lower bounds yields\nlnZ \u2265 c+ ln \u0393(1 + \u03b1) \u03b1 \u2212 1 n n\u2211 i=1 1 \u03b1 lnE [ exp ( \u2212\u03b1max x {\u03c6(x) + \u03b3i(xi)} )] = c+ ln \u0393(1 + \u03b1)\n\u03b1 \u2212 1 n\u03b1 lnE\n[ exp ( \u2212\nn\u2211 i=1 \u03b1max x {\u03c6(x) + \u03b3i(xi)}\n)]\n= c+ ln \u0393(1 + \u03b1) \u03b1 \u2212 1 n\u03b1 lnE\n[ exp ( \u2212n\u03b1 1\nn n\u2211 i=1 max x {\u03c6(x) + \u03b3i(xi)}\n)] ,\nwhere the first equality used the fact that the perturbations \u03b3i(xi) are mutually independent for different indices i to replace the product of expectations with the expectation of the product. The claimed result follows by applying Jensen\u2019s inequality to swap the summation and the convex maxx function, noting that the inequality works out the right way for both positive and negative \u03b1.\nJensen\u2019s inequality can be used to relate the general lower bound L(\u03b1) to the Gumbel trick lower bound L(0), showing that the former cannot be arbitrarily worse than the latter:\nProposition 14. For all \u03b1 \u2208 (\u22121, 0), the lower bound L(\u03b1) on lnZ satisfies\nL(\u03b1) \u2265 L(0) + ln \u0393(1 + \u03b1) \u03b1 + c\nProof. Apply Jensen\u2019s inequality with the convex function x 7\u2192 e\u2212n\u03b1 to the last term in the definition of L(\u03b1), noting that the inequality works out the stated way for \u03b1 < 0.\nNote that ln \u0393(1+\u03b1)\u03b1 + c \u2264 0 for \u03b1 \u2208 (\u22121, 0) so this result does not imply that the Fre\u0301chet lower bounds are tighter than the Gumbel lower bound L(0); it merely says that they cannot be arbitrarily worse than L(0).\nC.2. Relationship between errors of averaged-unary Gumbel perturbations\nIn this section we write x\u2217 for the (random) MAP configuration after average-unary perturbation of the potential function, i.e.,\nx\u2217 := argmax x\u2208X\n{ \u03c6(x) + 1\nn n\u2211 i=1 \u03b3i(xi)\n} .\nwhere {\u03b3i(xi) | xi \u2208 Xi, 1 \u2264 i \u2264 n} i.i.d.\u223c Gumbel(\u2212c). Let qavg(x) := P[x = x\u2217] be the probability mass function of x\u2217. The Gumbel trick lower bound on the log partition function lnZ due to Hazan et al. (2013) is:\nlnZ \u2265 L(0) = L\u03c6(0) := E\u03b3 [ min x\u2208X { \u03c6(x) + 1 n n\u2211 i=1 \u03b3i(xi) }] . (3)\nWe show that the gap of this Gumbel trick lower bound on lnZ upper bounds the KL divergence between the approximate distribution qavg and the Gibbs distribution p. To this end, we first need an entropy bound for qavg analogous to Theorem 1 of (Maji et al., 2014).\nTheorem 15. The entropy of qavg can be lower bounded using expected values of max-perturbations as follows:\nH(qavg) \u2265 1\nn n\u2211 i=1 E\u03b3i [\u03b3i(x\u2217i )]\nRemark. Theorem 1 of (Maji et al., 2014) and this Theorem 15 differ in three aspects: (1) the former is an upper bound and the latter is a lower bound, (2) the former sums the expectations while the latter averages them, and (3) the distributions qsum and qavg of x\u2217 in the two theorems are different.\nProof. By the duality relation between negative entropy and the log partition function (Wainwright & Jordan, 2008), the entropy H(qavg) of the unary-avg perturb-max distribution qavg can be expressed as\nH(qavg) = inf \u03d5\n{ lnZ\u03d5 \u2212\n\u2211 x\u2208X qavg(x)\u03d5(x)\n} ,\nwhere the variable \u03d5 ranges over all potential functions on X , and Z\u03d5 = \u2211\nx\u2208X exp\u03d5(x). Applying the Gumbel trick lower bound on the log partition function gives\nH(qavg) \u2265 inf \u03d5\n{ L\u03d5(0)\u2212\n\u2211 x\u2208X qavg(x)\u03d5(x)\n} ,\nProposition 16 in Appendix D shows that L\u03d5(0) is a convex function of \u03d5. The expression \u2212 \u2211\nx\u2208X q(x)\u03d5(x) is a linear function of \u03d5, so also convex, and thus as a sum of two convex functions, the quantity L\u03d5(0) \u2212 \u2211 x\u2208X q(x)\u03d5(x) within the infimum is a convex function of \u03d5. Moreover, Proposition 17 in Appendix D tells us that the partial derivatives can be computed as\n\u2202\n\u2202\u03d5(x)\n( L\u03d5(0)\u2212\n\u2211 x\u2208X qavg(x)\u03d5(x)\n) = q\u03d5(x)\u2212 qavg(x)\nwhere q\u03d5(x) is the unary-avg perturb-max distribution associated with the potential function \u03d5. Proposition 18 in Appendix D confirms that these partial derivatives are continuous, so we observe that as a function of \u03d5, the expression L\u03d5(0)\u2212 \u2211 x\u2208X qavg(x)\u03d5(x) is a convex function with continuous partial derivatives, so it is a differentiable convex function. This is sufficient to establish that the point \u03d5 = \u03c6 is a global minimum of this function (Wright & Nocedal, 1999). Hence\nH(qavg) \u2265 inf \u03d5\n{ L\u03d5(0)\u2212\n\u2211 x\u2208X qavg(x)\u03d5(x) } = L\u03c6(0)\u2212\n\u2211 x\u2208X qavg(x)\u03c6(x)\n= \u2211 x\u2208X qavg(x)E\u03b3\n[ \u03c6(x) + 1\nn n\u2211 i=1 \u03b3i(xi) | x = x\u2217 ] \u2212 \u2211 x\u2208X qavg(x)\u03c6(x)\n= 1\nn n\u2211 i=1 E\u03b3i [\u03b3i(x\u2217i )]\nwhere we conditioned on the maximizing configuration x\u2217 when expanding L\u03c6(0).\nRemark. This proof proceeded in the same way as the proof of Maji et al. (2014) for the upper bound, except that establishing the minimizing configuration of the infimum is a non-trivial step that is actually required in this case. The second revision of (Hazan et al., 2016) computes the derivative of U\u03d5(0) \u2212 \u2211 x\u2208X qsum(x)\u03d5(x), which is similar to our\nL\u03d5(0)\u2212 \u2211 x\u2208X qavg(x)\u03d5(x), by differentiating under the expectation.\nEquipped with Theorem 15, we can now show a link between the approximation \u201cerrors\u201d of the averaged-unary perturbation MAP configuration distribution qavg (to the Gibbs distribution p) and estimate L(0) (to lnZ). Proposition 12. Let p be the Gibbs distribution on X . Then\nlnZ \u2212 L(0)\ufe38 \ufe37\ufe37 \ufe38 error in lnZ bound \u2265 KL(qavg \u2016 p)\ufe38 \ufe37\ufe37 \ufe38 sampling error \u2265 0\nRemark. While we knew from Hazan et al. (2013) that lnZ \u2212 L(0) \u2265 0 (i.e. that L(0) is a lower bound on lnZ), this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the average-unary perturbation MAP distribution qavg and the Gibbs distribution p.\nProof. The Kullback-Leibler divergence in question expands as\nKL(qavg \u2016 p) = \u2212H(qavg)\u2212 \u2211 x\u2208X qavg(x) ln exp\u03c6(x)\u2211 x\u0303\u2208X exp\u03c6(x\u0303) = \u2212H(qavg)\u2212 \u2211 x\u2208X qavg(x)\u03c6(x) + lnZ.\nFrom the proof of Theorem 15 we know that H(qavg) \u2265 L(0)\u2212 \u2211 x\u2208X qavg(x)\u03c6(x), so\nKL(qavg \u2016 p) \u2264 \u2212L(0) + \u2211 x\u2208X qavg(x)\u03c6(x)\u2212 \u2211 x\u2208X qavg(x)\u03c6(x) + lnZ = lnZ \u2212 L(0)."}, {"heading": "D. Technical results", "text": "In this section we write L(\u03c6) instead of L\u03c6(0) for the Gumbel trick lower bound on lnZ associated with the potential function \u03c6, see equation (3).\nProposition 16. The Gumbel trick lower bound L(\u03c6), viewed as a function of the potentials \u03c6, is convex.\nProof. Convexity can be proved directly from definition. Let \u03c61 and \u03c62 be two arbitrary potential functions on a discrete product space X , and let \u03bb \u2208 [0, 1]. Then\nL(\u03bb\u03c61 + (1\u2212 \u03bb)\u03c62)\n= E\u03b3 [ max x\u2208X { \u03bb\u03c61(x) + (1\u2212 \u03bb)\u03c62(x) + 1 n n\u2211 i=1 \u03b3i(xi) }]\n= E\u03b3 [ max x\u2208X { \u03bb ( \u03c61(x) + 1 n n\u2211 i=1 \u03b3i(xi) ) + (1\u2212 \u03bb) ( \u03c62(x) + 1 n n\u2211 i=1 \u03b3i(xi) )}]\n\u2264 E\u03b3 [ \u03bbmax\nx\u2208X\n{ \u03c61(x) + 1\nn n\u2211 i=1 \u03b3i(xi)\n} + (1\u2212 \u03bb) max\nx\u2208X\n{ \u03c62(x) + 1\nn n\u2211 i=1 \u03b3i(xi) }] = \u03bbL(\u03c61) + (1\u2212 \u03bb)L(\u03c62),\nwhere we have used convexity of the max function to obtain the inequality, and linearity of expectation to arrive at the final equality.\nRemark. This convexity proof goes through for other (low-dimensional) perturbations as well, e.g. it also works for U\u03c6(0). Proposition 17. The Gumbel trick lower bound L(\u03c6), viewed as a function of the potentials \u03c6, has partial derivatives\n\u2202\n\u2202\u03c6(x\u0303) L(\u03c6) = q\u03c6(x\u0303)\nwhere q\u03c6 is the probability mass function of the average-unary perturbation MAP configuration\u2019s distribution associated with the potential function \u03c6.\nProof. Let x\u0303 \u2208 X , so that \u03c6(x\u0303) is a general component of \u03c6, and let ex\u0303 be the indicator vector of x\u0303. For any \u03b4 \u2208 R, the change in the lower bound L due to replacing \u03c6(x\u0303) with \u03c6(x\u0303) + \u03b4 is\nL(\u03c6+ \u03b4ex\u0303)\u2212 L(\u03c6) = E\u03b3 [ max x\u2208X { \u03c6(x) + \u03b41{x = x\u0303}+ 1 n n\u2211 i=1 \u03b3i(xi) }] \u2212 E\u03b3 [ max x\u2208X { \u03c6(x) + 1 n n\u2211 i=1 \u03b3i(xi) }]\n= E\u03b3 [ max x\u2208X { \u03c6(x) + \u03b41{x = x\u0303}+ 1 n n\u2211 i=1 \u03b3i(xi) } \u2212max x\u2208X { \u03c6(x) + 1 n n\u2211 i=1 \u03b3i(xi) }] = E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3)]\nby linearity of expectation, where we have denoted by \u2206(\u03c6, \u03b4, x\u0303, \u03b3) the change in maximum due to replacing the potential \u03c6(x\u0303) with \u03c6(x\u0303) + \u03b4. Let\u2019s condition on the argmax before modifying \u03c6:\nL(\u03c6+ \u03b4ex\u0303)\u2212 L(\u03c6) = E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3)] = \u2211 x\u2208X q\u03c6(x)E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax]\nNow let\u2019s condition on the size of the gap G between the maximum and the runner-up:\nE\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax] = P(G \u2264 |\u03b4|)E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax, G \u2264 |\u03b4|] + P(G > |\u03b4|)E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax, G > |\u03b4|]\nLet\u2019s examine all four terms on the right-hand side one by one:\n1. P(G \u2264 |\u03b4|)\u2192 P(G = 0) = 0 as \u03b4 \u2192 0 by monotonicity of measure. 2. E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax, G \u2264 |\u03b4|] \u2264 \u03b4 since |\u2206(\u03c6, \u03b4, x\u0303, \u03b3)| \u2264 |\u03b4| always holds. 3. P(G > |\u03b4|)\u2192 P(G \u2265 0) = 1 as \u03b4 \u2192 0 by monotonicity of measure. 4. E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax, G > |\u03b4|] = \u03b41{x = x\u0303} since in this case both maximizations in the\ndefinition of \u2206(\u03c6, \u03b4, x\u0303, \u03b3) are maximized at x.\nTherefore, as \u03b4 \u2192 0,\nE\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax] = o(1)o(\u03b4) + (1 + o(1))\u03b41{x = x\u0303}\nPutting things together, we have\nlim \u03b4\u21920 L(\u03c6+ \u03b4ex\u0303)\u2212 L(\u03c6) \u03b4 = \u2211 x\u2208X q\u03c6(x) lim \u03b4\u21920 1 \u03b4 E\u03b3 [\u2206(\u03c6, \u03b4, x\u0303, \u03b3) | x is the original argmax]\n= \u2211 x\u2208X q\u03c6(x)1{x = x\u0303}\n= q\u03c6(x\u0303),\nwhich proves the stated claim directly from definition of a partial derivative.\nProposition 18. The probability mass function q\u03c6 of the average-unary perturbation MAP configuration\u2019s distribution associated with a potential function \u03c6 is continuous in \u03c6.\nProof. For any x\u2217 \u2208 X we have from definition\nq\u03c6(x \u2217) = P [ x\u2217 = argmax\nx\u2208X\n{ \u03c6(x) + 1\nn n\u2211 i=1 \u03b3i(xi)\n}]\n= P [ \u03c6(x\u2217) + 1\nn n\u2211 i=1 \u03b3i(x \u2217 i ) > max x\u2208X\\{x\u2217}\n{ \u03c6(x) + 1\nn n\u2211 i=1 \u03b3i(xi)\n}]\n= E [ 1 { \u03c6(x\u2217) + 1\nn n\u2211 i=1 \u03b3i(x \u2217 i ) > max x\u2208X\\{x\u2217}\n{ \u03c6(x) + 1\nn n\u2211 i=1 \u03b3i(xi)\n}}]\nwhich is continuous in \u03c6 by continuity of max, of 1 {\u00b7 > \u00b7} (as a function of \u03c6) and by the Bounded Convergence Theorem.\nRemark. The results above show that the Gumbel trick lower bound L(\u03c6), viewed as a function of the potentials \u03c6, is convex and has continuous partial derivatives."}], "references": [{"title": "Marginal inference in MRFs using Frank-Wolfe", "author": ["D. Belanger", "D. Sheldon", "A. McCallum"], "venue": "In NIPS Workshop on Greedy Optimization, Frank-Wolfe and Friends,", "citeRegEx": "Belanger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Belanger et al\\.", "year": 2013}, {"title": "Local Perturband-MAP for Structured Prediction", "author": ["G. Bertasius", "Q. Liu", "L. Torresani", "J. Shi"], "venue": "In AISTATS,", "citeRegEx": "Bertasius et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bertasius et al\\.", "year": 2017}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "Boykov et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Boykov et al\\.", "year": 2001}, {"title": "Scalable discrete sampling as a multi-armed bandit problem", "author": ["Y. Chen", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "Chen and Ghahramani,? \\Q2016\\E", "shortCiteRegEx": "Chen and Ghahramani", "year": 2016}, {"title": "Analysis of survival data, volume 21", "author": ["D. Cox", "D. Oakes"], "venue": "CRC Press,", "citeRegEx": "Cox and Oakes,? \\Q1984\\E", "shortCiteRegEx": "Cox and Oakes", "year": 1984}, {"title": "Global optimization for first order Markov random fields with submodular priors", "author": ["J. Darbon"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "Darbon,? \\Q2009\\E", "shortCiteRegEx": "Darbon", "year": 2009}, {"title": "Taming the curse of dimensionality: Discrete integration by hashing and optimization", "author": ["S. Ermon", "A. Sabharwal", "B. Selman"], "venue": "In ICML,", "citeRegEx": "Ermon et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ermon et al\\.", "year": 2013}, {"title": "An inequality satisfied by the Gamma function", "author": ["J. Gurland"], "venue": "Scandinavian Actuarial Journal,", "citeRegEx": "Gurland,? \\Q1956\\E", "shortCiteRegEx": "Gurland", "year": 1956}, {"title": "On the partition function and random maximum a-posteriori perturbations", "author": ["T. Hazan", "T. Jaakkola"], "venue": "In ICML,", "citeRegEx": "Hazan and Jaakkola,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Jaakkola", "year": 2012}, {"title": "On sampling from the Gibbs distribution with random maximum a-posteriori perturbations", "author": ["T. Hazan", "S. Maji", "T. Jaakkola"], "venue": "In NIPS", "citeRegEx": "Hazan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2013}, {"title": "High dimensional inference with random maximum aposteriori perturbations", "author": ["T. Hazan", "F. Orabona", "A. Sarwate", "S. Maji", "T. Jaakkola"], "venue": null, "citeRegEx": "Hazan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2016}, {"title": "Exact sampling with integer linear programs and random perturbations", "author": ["C. Kim", "A. Sabharwal", "S. Ermon"], "venue": "In AAAI,", "citeRegEx": "Kim et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Kolmogorov,? \\Q2006\\E", "shortCiteRegEx": "Kolmogorov", "year": 2006}, {"title": "Barrier Frank-Wolfe for Marginal Inference", "author": ["Krishnan", "Rahul G", "Lacoste-Julien", "Simon", "Sontag", "David"], "venue": "In NIPS", "citeRegEx": "Krishnan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Krishnan et al\\.", "year": 2015}, {"title": "Graphical models. Oxford statistical science series", "author": ["S. Lauritzen"], "venue": "Autre", "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "A Poisson process model for Monte Carlo", "author": ["C. Maddison"], "venue": null, "citeRegEx": "Maddison,? \\Q2014\\E", "shortCiteRegEx": "Maddison", "year": 2014}, {"title": "Active boundary annotation using random MAP perturbations", "author": ["S. Maji", "T. Hazan", "T. Jaakkola"], "venue": "In AISTATS,", "citeRegEx": "Maji et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Maji et al\\.", "year": 2014}, {"title": "libDAI: A free and open source C++ library for discrete approximate inference in graphical models", "author": ["J. Mooij"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mooij,? \\Q2010\\E", "shortCiteRegEx": "Mooij", "year": 2010}, {"title": "On measure concentration of random maximum a-posteriori perturbations", "author": ["F. Orabona", "T. Hazan", "A. Sarwate", "T. Jaakkola"], "venue": "In ICML,", "citeRegEx": "Orabona et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Orabona et al\\.", "year": 2014}, {"title": "Gaussian sampling by local perturbations", "author": ["G. Papandreou", "A. Yuille"], "venue": "In NIPS", "citeRegEx": "Papandreou and Yuille,? \\Q2010\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2010}, {"title": "Perturb-and-MAP random fields: Using discrete optimization to learn and sample from energy models", "author": ["G. Papandreou", "A. Yuille"], "venue": "In Proc. IEEE Int. Conf. on Computer Vision (ICCV),", "citeRegEx": "Papandreou and Yuille,? \\Q2011\\E", "shortCiteRegEx": "Papandreou and Yuille", "year": 2011}, {"title": "Randomized optimum models for structured prediction", "author": ["D. Tarlow", "R. Adams", "R. Zemel"], "venue": "In AISTATS,", "citeRegEx": "Tarlow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Tarlow et al\\.", "year": 2012}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M. Wainwright", "M. Jordan"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Clamping improves TRW and mean field approximations", "author": ["A. Weller", "J. Domke"], "venue": "In AISTATS,", "citeRegEx": "Weller and Domke,? \\Q2016\\E", "shortCiteRegEx": "Weller and Domke", "year": 2016}, {"title": "Approximating the Bethe partition function", "author": ["A. Weller", "T. Jebara"], "venue": "In UAI,", "citeRegEx": "Weller and Jebara,? \\Q2014\\E", "shortCiteRegEx": "Weller and Jebara", "year": 2014}, {"title": "Clamping variables and approximate inference", "author": ["A. Weller", "T. Jebara"], "venue": "In NIPS,", "citeRegEx": "Weller and Jebara,? \\Q2014\\E", "shortCiteRegEx": "Weller and Jebara", "year": 2014}, {"title": "Variable clamping for optimization-based inference", "author": ["J. Zhao", "J. Djolonga", "S. Tschiatschek", "A. Krause"], "venue": "In NIPS Workshop on Advances in Approximate Bayesian Inference,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "P[x = x\u2217] be the probability mass function of x\u2217. The Gumbel trick lower bound on the log partition function lnZ due to Hazan et al", "author": ["\u223c Gumbel(\u2212c"], "venue": null, "citeRegEx": "Gumbel.\u2212c..,? \\Q2013\\E", "shortCiteRegEx": "Gumbel.\u2212c..", "year": 2013}, {"title": "lnZ \u2212 L(0) \u2265 0 (i.e. that L(0) is a lower bound on lnZ), this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the average-unary perturbation MAP distribution qavg and the Gibbs distribution", "author": ["Hazan"], "venue": null, "citeRegEx": "Hazan,? \\Q2013\\E", "shortCiteRegEx": "Hazan", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "This MAP problem is NP-hard in general; however, substantial research effort has led to the development of solvers which can efficiently compute or estimate the MAP solution on many problems that occur in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009).", "startOffset": 214, "endOffset": 273}, {"referenceID": 5, "context": "This MAP problem is NP-hard in general; however, substantial research effort has led to the development of solvers which can efficiently compute or estimate the MAP solution on many problems that occur in practice (e.g., Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009).", "startOffset": 214, "endOffset": 273}, {"referenceID": 2, "context": ", Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009). Evaluating the partition function is a harder problem, containing for instance #P-hard counting problems. The general aim of perturb-and-MAP methods is to reduce the problem of partition function evaluation, or the problem of sampling from the Gibbs distribution, to repeated instances of the MAP problem (where each instance is on a different random perturbation of the original model). The Gumbel trick (Papandreou & Yuille, 2011) relies on adding Gumbel-distributed noise to each configuration\u2019s potential \u03c6(x). We derive a wider family of perturb-andMAP methods that can be seen as perturbing the model in different ways \u2013 in particular using the Weibull and Fr\u00e9chet distributions alongside the Gumbel. We show that the new methods can be implemented with essentially no additional computational cost by simply averaging existing Gumbel MAP perturbations in different spaces, and that they can lead to more accurate estimators of the partition function. Evaluating or perturbing each configuration\u2019s potential with i.i.d. Gumbel noise can be computationally expensive. One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014; Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d. Gumbel noise with a \u201clow-rank\u201d approximation. Hazan & Jaakkola (2012); Hazan et al.", "startOffset": 2, "endOffset": 1495}, {"referenceID": 2, "context": ", Boykov et al., 2001; Kolmogorov, 2006; Darbon, 2009). Evaluating the partition function is a harder problem, containing for instance #P-hard counting problems. The general aim of perturb-and-MAP methods is to reduce the problem of partition function evaluation, or the problem of sampling from the Gibbs distribution, to repeated instances of the MAP problem (where each instance is on a different random perturbation of the original model). The Gumbel trick (Papandreou & Yuille, 2011) relies on adding Gumbel-distributed noise to each configuration\u2019s potential \u03c6(x). We derive a wider family of perturb-andMAP methods that can be seen as perturbing the model in different ways \u2013 in particular using the Weibull and Fr\u00e9chet distributions alongside the Gumbel. We show that the new methods can be implemented with essentially no additional computational cost by simply averaging existing Gumbel MAP perturbations in different spaces, and that they can lead to more accurate estimators of the partition function. Evaluating or perturbing each configuration\u2019s potential with i.i.d. Gumbel noise can be computationally expensive. One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014; Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d. Gumbel noise with a \u201clow-rank\u201d approximation. Hazan & Jaakkola (2012); Hazan et al. (2013) showed that from such an approximation, upper and lower bounds on the partition function and a sequential sampler for the Gibbs distribution can still be recovered.", "startOffset": 2, "endOffset": 1516}, {"referenceID": 1, "context": "Further recent applications of perturb-and-MAP include structured prediction in computer vision (Bertasius et al., 2017) and turning the discrete sampling problem into an optimization task that can be cast as a multi-armed bandit problem (Chen & Ghahramani, 2016), see Section 5.", "startOffset": 96, "endOffset": 120}, {"referenceID": 0, "context": "The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015).", "startOffset": 119, "endOffset": 165}, {"referenceID": 13, "context": "The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015).", "startOffset": 119, "endOffset": 165}, {"referenceID": 11, "context": "Tarlow et al. (2012) extended this perturb-and-MAP approach to sampling, in particular by considering more general structured prediction problems.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Tarlow et al. (2012) extended this perturb-and-MAP approach to sampling, in particular by considering more general structured prediction problems. Hazan & Jaakkola (2012) pointed out that MAP perturbations are useful not only for sampling the Gibbs distribution (considering the argmax of the perturbed model), but also for bounding and approximating the partition function (by considering the value of the max).", "startOffset": 0, "endOffset": 171}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities.", "startOffset": 12, "endOffset": 32}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al.", "startOffset": 12, "endOffset": 406}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability.", "startOffset": 12, "endOffset": 430}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation.", "startOffset": 12, "endOffset": 583}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation. Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling.", "startOffset": 12, "endOffset": 829}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation. Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling. Maddison (2016) cast this work into a unified framework together with adaptive rejection sampling techniques, based on the notion of exponential races.", "startOffset": 12, "endOffset": 951}, {"referenceID": 6, "context": "Afterwards, Hazan et al. (2013) derived new lower bounds on the partition function and proposed a new sampler for the Gibbs distribution that samples variables of a discrete graphical model sequentially, using expected values of lowrank MAP perturbations to construct the conditional probabilities. Due to the low-rank approximation, this algorithm has the option to reject a sample. Orabona et al. (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a Bayesian active learning framework for interactive image boundary annotation. Perturb-and-MAP was famously generalized to continuous spaces by Maddison et al. (2014), replacing the Gumbel distribution with a Gumbel process and calling the resulting algorithm A* sampling. Maddison (2016) cast this work into a unified framework together with adaptive rejection sampling techniques, based on the notion of exponential races. This recent view generally brings together perturband-MAP and accept-reject samplers, exploiting the connection between the Gumbel distribution and competing exponential clocks that we also discuss in Section 2.1. Inspired by A* sampling, Kim et al. (2016) proposed an exact sampler for discrete graphical models based on lazilyinstantiated random perturbations, which uses linear programming relaxations to prune the optimization space.", "startOffset": 12, "endOffset": 1344}, {"referenceID": 0, "context": "The Frank-Wolfe method may be applied by iteratively updating marginals using a constrained MAP solver and line search (Belanger et al., 2013; Krishnan et al., 2015). Weller & Jebara (2014a) instead use just one MAP call over a discretized mesh of marginals to approximate the Bethe partition function, which itself is an estimate (which often performs well) of the true partition function.", "startOffset": 120, "endOffset": 191}, {"referenceID": 15, "context": "The Gumbel Trick Similarly to the connection between the Gumbel trick and the Poisson process established by Maddison (2016), we introduce the Gumbel trick for discrete probability distributions using a simple and elegant construction via competing exponential clocks.", "startOffset": 109, "endOffset": 125}, {"referenceID": 7, "context": "For example, it can be shown using Gurland\u2019s ratio (Gurland, 1956) that this constant is at least 1 for the Weibull and Fr\u00e9chet tricks, which is precisely the value achieved by the Exponential trick (which corresponds to \u03b1 = 1).", "startOffset": 51, "endOffset": 66}, {"referenceID": 9, "context": "Unary perturbations provide the upper bound lnZ \u2264 E[U ] on the log partition function (Hazan & Jaakkola, 2012), can be used to construct a sequential sampler for the Gibbs distribution (Hazan et al., 2013), and, if the perturbations are scaled down by a factor of n, a lower bound on lnZ can also be recovered (Hazan et al.", "startOffset": 185, "endOffset": 205}, {"referenceID": 9, "context": ", 2013), and, if the perturbations are scaled down by a factor of n, a lower bound on lnZ can also be recovered (Hazan et al., 2013).", "startOffset": 112, "endOffset": 132}, {"referenceID": 9, "context": "Corollary 9 of Hazan et al. (2016) can be used to show that var(e\u2212\u03b1U ) is finite for \u03b1 > \u2212 1 2 \u221a n , and so then the estimation is well-behaved.", "startOffset": 15, "endOffset": 35}, {"referenceID": 9, "context": "Corollary 9 of Hazan et al. (2016) can be used to show that var(e\u2212\u03b1U ) is finite for \u03b1 > \u2212 1 2 \u221a n , and so then the estimation is well-behaved. A natural question is how these new bounds relate to the Gumbel trick upper bound lnZ \u2264 E[U ] by Hazan & Jaakkola (2012). The following result aims to answers this: Proposition 5.", "startOffset": 15, "endOffset": 266}, {"referenceID": 9, "context": "This was shown previously in restricted settings (Hazan et al., 2013; Zhao et al., 2016).", "startOffset": 49, "endOffset": 88}, {"referenceID": 26, "context": "This was shown previously in restricted settings (Hazan et al., 2013; Zhao et al., 2016).", "startOffset": 49, "endOffset": 88}, {"referenceID": 26, "context": "Similar results showing that clamping improves partition function estimation have been obtained for the mean field and TRW approximations (Weller & Domke, 2016), and in certain settings for the Bethe approximation (Weller & Jebara, 2014b) and LFIELD (Zhao et al., 2016).", "startOffset": 250, "endOffset": 269}, {"referenceID": 9, "context": "Sequential Sampling Hazan et al. (2013) derived a sequential sampling procedure for the Gibbs distribution by exploiting the U(0) Gumbel trick upper bound on lnZ.", "startOffset": 20, "endOffset": 40}, {"referenceID": 9, "context": "As for the Gumbel sequential sampler of Hazan et al. (2013), the expected number of restarts (and hence the running time) only depend on the quality of the upper bound (U(\u03b1) \u2212 lnZ), and not on the ordering of variables.", "startOffset": 40, "endOffset": 60}, {"referenceID": 9, "context": "Lower Bounds on the Partition Function Similarly as in the Gumbel trick case (Hazan et al., 2013), one can derive lower bounds on lnZ by perturbing an arbitrary subset S of variables.", "startOffset": 77, "endOffset": 97}, {"referenceID": 9, "context": "Again, L(0) := E[L] can be defined by continuity, where E[L] \u2264 lnZ is the Gumbel trick lower bound by Hazan et al. (2013).", "startOffset": 102, "endOffset": 122}, {"referenceID": 16, "context": "Second, exploiting the fact that the logarithm function leads to additive perturbations, Maji et al. (2014) showed that the entropy of x\u2217, the configuration with maximum potential after sum-unary perturbation in the sense of Definition 3, can be bounded as H(x\u2217) \u2264 B(p) := \u2211n i=1 E\u03b3i [\u03b3i(xi )].", "startOffset": 89, "endOffset": 108}, {"referenceID": 9, "context": "While we knew from Hazan et al. (2013) that lnZ \u2212 L(0) \u2265 0, this is a stronger result showing that the size of the gap is an upper bound on the KL divergence between the approximate sampling distribution of x\u2217\u2217 and the Gibbs distribution p.", "startOffset": 19, "endOffset": 39}, {"referenceID": 18, "context": "Fourth, viewed as a function of the Gumbel perturbations \u03b3, the random variable U has a bounded gradient, allowing earlier measure concentration results (Orabona et al., 2014; Hazan et al., 2016).", "startOffset": 153, "endOffset": 195}, {"referenceID": 10, "context": "Fourth, viewed as a function of the Gumbel perturbations \u03b3, the random variable U has a bounded gradient, allowing earlier measure concentration results (Orabona et al., 2014; Hazan et al., 2016).", "startOffset": 153, "endOffset": 195}, {"referenceID": 9, "context": ", 2014; Hazan et al., 2016). Proving similar measure concentration results for the expectations E[e\u2212\u03b1U ] appearing in U(\u03b1) for \u03b1 6= 0 may be more challenging. 5. Experiments We conducted experiments with the following aims: 1. To show that the higher efficiency of the Exponential trick in the full-rank perturbation setting is useful in practice, we compared it to the Gumbel trick in A* sampling (Maddison et al., 2014) (Section 5.1) and in the large-scale discrete sampling setting of Chen & Ghahramani (2016) (Section 5.", "startOffset": 8, "endOffset": 513}, {"referenceID": 15, "context": "A* Sampling A* sampling (Maddison et al., 2014) is a sampling algorithm for continuous distributions that perturbs the logunnormalized density \u03c6 with a continuous generalization of the Gumbel trick, called the Gumbel process, and uses a variant of A* search to find the location of the maximum of the perturbed \u03c6. Returning the location yields an exact sample from the original distribution, as in the discrete Gumbel trick. Moreover, the corresponding maximum value also has the Gumbel(\u2212c + lnZ) distribution (Maddison et al., 2014). Our analysis in Section 2.3 tells us that the Exponential trick yields an estimator with lower MSE than the Gumbel trick; we briefly verified this on the Robust Bayesian Regression experiment of Maddison et al. (2014). We constructed estimators of lnZ from the Gumbel and Exponential tricks (debiased version, see Section 2.", "startOffset": 25, "endOffset": 753}, {"referenceID": 17, "context": "We replaced the expectations in U(\u03b1)\u2019s with sample averages of size M = 100, using libDAI (Mooij, 2010) to solve the MAP problems yielding these samples.", "startOffset": 90, "endOffset": 103}, {"referenceID": 27, "context": "Low-rank Perturbation Bounds on lnZ Hazan & Jaakkola (2012) evaluated tightness of the Gumbel trick upper bound U(0) \u2265 lnZ on 10\u00d7 10 binary spin glass models.", "startOffset": 36, "endOffset": 60}, {"referenceID": 9, "context": "Sequential samplers for the Gibbs distribution The family of sequential samplers for the Gibbs distribution presented in the main text as Algorithm 1 has the same overall structure as the sequential sampler derived by Hazan et al. (2013) from the Gumbel trick upper bound U(0), and hence correctness can be argued similarly.", "startOffset": 218, "endOffset": 238}, {"referenceID": 27, "context": "The following results links together the errors acquired when using summed unary perturbations to upper bound the log partition function lnZ \u2264 U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum using the bound due to Maji et al.", "startOffset": 186, "endOffset": 210}, {"referenceID": 16, "context": "The following results links together the errors acquired when using summed unary perturbations to upper bound the log partition function lnZ \u2264 U(0) using the Gumbel trick upper bound by Hazan & Jaakkola (2012), to approximately sample from the Gibbs distribution by using qsum instead, and to upper bound the entropy of the approximate distribution qsum using the bound due to Maji et al. (2014). Proposition 11.", "startOffset": 377, "endOffset": 396}, {"referenceID": 9, "context": "The Gumbel trick lower bound on the log partition function lnZ due to Hazan et al. (2013) is:", "startOffset": 70, "endOffset": 90}, {"referenceID": 16, "context": "To this end, we first need an entropy bound for qavg analogous to Theorem 1 of (Maji et al., 2014).", "startOffset": 79, "endOffset": 98}, {"referenceID": 16, "context": "Theorem 1 of (Maji et al., 2014) and this Theorem 15 differ in three aspects: (1) the former is an upper bound and the latter is a lower bound, (2) the former sums the expectations while the latter averages them, and (3) the distributions qsum and qavg of x\u2217 in the two theorems are different.", "startOffset": 13, "endOffset": 32}, {"referenceID": 10, "context": "The second revision of (Hazan et al., 2016) computes the derivative of U\u03c6(0) \u2212 \u2211 x\u2208X qsum(x)\u03c6(x), which is similar to our L\u03c6(0)\u2212 \u2211 x\u2208X qavg(x)\u03c6(x), by differentiating under the expectation.", "startOffset": 23, "endOffset": 43}, {"referenceID": 14, "context": "This proof proceeded in the same way as the proof of Maji et al. (2014) for the upper bound, except that establishing the minimizing configuration of the infimum is a non-trivial step that is actually required in this case.", "startOffset": 53, "endOffset": 72}, {"referenceID": 9, "context": "While we knew from Hazan et al. (2013) that lnZ \u2212 L(0) \u2265 0 (i.", "startOffset": 19, "endOffset": 39}], "year": 2017, "abstractText": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.", "creator": "LaTeX with hyperref package"}}}