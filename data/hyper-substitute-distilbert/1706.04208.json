{"id": "1706.04208", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2017", "title": "Hybrid Reward Architecture for Reinforcement Learning", "abstract": "one of the general challenges in reinforcement learning ( rl ) is generalisation. in typical weighted rl methods this is characterized by approximating optimal optimal prize function while a low - resource iteration plus a deep computational. while this approach arises highly in many domains, structured domains and the optimal value tree cannot easily reach inverted to purely 5 - dimensional representation, learning can always very slow yet unstable. original paper extends towards tackling such challenging considerations, by proving a popular terminology, called hybrid structure conditioning ( pas ). hra takes their efficiently generated decomposed reward function as learns a higher value function coding each component reward function. at each simulation typically cannot binds at individual subset dividing all categories, improved overall cost function is generally brighter and can be mapped away from a low - dimensional representation, offering more effective learning. we improved, on a toy - robot and the atari game workshop. pac - man, where instruction achieves above - human performance.", "histories": [["v1", "Tue, 13 Jun 2017 18:05:48 GMT  (1018kb,D)", "http://arxiv.org/abs/1706.04208v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["harm van seijen", "mehdi fatemi", "joshua romoff", "romain laroche", "tavian barnes", "jeffrey tsang"], "accepted": true, "id": "1706.04208"}, "pdf": {"name": "1706.04208.pdf", "metadata": {"source": "CRF", "title": "Hybrid Reward Architecture for Reinforcement Learning", "authors": ["Harm van Seijen", "Mehdi Fatemi", "Jeffrey Tsang"], "emails": ["harm.vanseijen@microsoft.com", "mehdi.fatemi@microsoft.com", "joshua.romoff@mail.mcgill.ca", "romain.laroche@microsoft.com", "tavian.barnes@microsoft.com", "tsang.jeffrey@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In reinforcement learning (RL) (Sutton & Barto, 1998; Szepesv\u00e1ri, 2009), the goal is to find a behaviour policy that maximises the return\u2014the discounted sum of rewards received over time\u2014in a data-driven way. One of the main challenges of RL is to scale methods such that they can be applied to large, real-world problems. Because the state-space of such problems is typically massive, strong generalisation is required to learn a good policy efficiently.\nMnih et al. (2015) achieved a big breakthrough in this area: by combining standard RL techniques with deep neural networks, they outperformed humans on a large number of Atari 2600 games, by learning a policy from pixels. The generalisation properties of their DeepQ-Networks (DQN) method is performed by approximating the optimal value function. A value function plays an important role in RL, because it predicts the expected return, conditioned on a state or state-action pair. Once the optimal value function is known, an optimal policy can easily be derived. By modelling the current estimate of the optimal value function with a deep neural network, DQN carries out a strong generalisation on the value function, and hence on the policy.\nar X\niv :1\n70 6.\n04 20\n8v 1\n[ cs\nThe generalisation behaviour of DQN is achieved by regularisation on the model for the optimal value function. However, if the optimal value function is very complex, then learning an accurate low-dimensional representation can be challenging or even impossible. Therefore, when the optimal value function cannot easily be reduced to a low-dimensional representation, we argue to apply a new, complementary form of regularisation on the target side. Specifically, we propose to replace the reward function with an alternative reward function that has a smoother optimal value function, but still yields a reasonable\u2014but not necessarily optimal\u2014policy, when acting greedily.\nA key observation behind regularisation on the target function is the difference between the performance objective, which specifies what type of behaviour is desired, and the training objective, which provides the feedback signal that modifies an agent\u2019s behaviour. In RL, a single reward function often takes on both roles. However, the reward function that encodes the performance objective might be awful as a training objective, resulting in slow or unstable learning. At the same time, a training objective can differ from the performance objective, but still do well with respect to it.\nIntrinsic motivation (Stout et al., 2005; Schmidhuber, 2010) uses the above observation to improve learning in sparse-reward domains. It achieves this by adding a domain-specific intrinsic reward signal to the reward coming from the environment. Typically, the intrinsic reward function is potential-based, which maintains optimality of the resulting policy. In our case, we define a training objective based on a different criterion: smoothness of the value function, such that it can easily be represented by a low-dimensional representation. Because of this different goal, adding a potential-based reward function to the original reward function is not a good strategy in our case, since this typically does not reduce the complexity of the optimal value function.\nOur main strategy for constructing a training objective is to decompose the reward function of the environment into n different reward functions. Each of them is assigned to a separate reinforcementlearning agent. Similar to the Horde architecture (Sutton et al., 2011), all these agents can learn in parallel on the same sample sequence by using off-policy learning. For action selection, each agent gives its values for the actions of the current state to an aggregator, which combines them into a single action-value for each action (for example, by averaging over all agents). Based on these action-values the current action is selected (for example, by taking the greedy action).\nWe test our approach on two domains: a toy-problem, where an agent has to eat 5 randomly located fruits, and Ms. Pac-Man, a hard game from the ALE benchmark set (Bellemare et al., 2013)."}, {"heading": "2 Related Work", "text": "Horde architecture. Our HRA method builds upon the Horde architecture (Sutton et al., 2011). The Horde architecture consists of a large number of \u2018demons\u2019 that learn in parallel via off-policy learning. Each demon trains a separate general value function (GVF) based on its own policy and pseudoreward function. A pseudo-reward can be any feature-based signal that encodes useful information. The Horde architecture is focused on building up general knowledge about the world, encoded via a large number of GVFs. HRA focusses on training separate components of the environment-reward function, in order to achieve a smoother value function to efficiently learn a control policy.\nLearning with respect to multiple reward functions is also a topic of multi-objective learning (Roijers et al., 2013). So alternatively, HRA can also be viewed as applying multi-objective learning in order to smooth the value function of a single reward function.\nOptions / Hierarchical Learning. This work is also related to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al., 2016). Options are temporally-extended actions that, like HRA\u2019s heads, can be trained in parallel based on their own (intrinsic) reward functions. However, once an option has been trained, the role of its intrinsic reward function is over. A higher-level agent that uses an option sees it as just another action and evaluates it using its own reward function. This can yield great speed-ups in learning and help substantially with better exploration, but they do not directly make the value function of the higher-level agent less complex. The heads of HRA represent values, trained with components of the environment reward. Even after training, these values stay relevant, because the aggregator uses the values of all heads to select its action."}, {"heading": "3 Model", "text": "Consider a Markov decision process (MDP), which models an agent interacting with an environment at discrete time steps t. It has a state set S , action setA, environment reward functionRenv : S\u00d7A \u2192 R, and transition probability function P : S \u00d7 A \u00d7 S \u2192 [0, 1]. At time step t, the agent observes state st \u2208 S and takes action at \u2208 A. The agent observes the next state st+1, drawn from the the transition probability function P (st, at), and a reward rt = Renv(st, at). The behaviour is defined by a policy \u03c0 : S \u00d7 A \u2192 [0, 1], which represents the selection probabilities over actions. The goal of an agent is to find a policy that maximises the expected return, which is the discounted sum of rewards: Gt := \u2211\u221e i=0 \u03b3\ni rt+i, where the discount factor \u03b3 \u2208 [0, 1] controls the importance of immediate rewards versus future rewards. Each policy \u03c0 has a corresponding action-value function that gives the expected return, conditioned on the state and action, when acting according to that policy: Q\u03c0(s, a) = E[Gt|st = s, at = a, \u03c0] (1) Model-free methods improve their policy by iteratively improving an estimate of the optimal actionvalue function Q\u2217(s, a) = max\u03c0 Q\u03c0(s, a), using sample-based updates. By acting greedily with respect to Q\u2217 (i.e., taking the action with the highest Q\u2217-value in every state), the optimal policy \u03c0\u2217 is obtained."}, {"heading": "3.1 Hybrid Reward Architecture", "text": "Because a Q-value function is high-dimensional, it is typically approximated with a deep network with parameters \u03b8: Q(s, a; \u03b8). DQN estimates the optimal Q-value function by minimising the sequence of loss functions:\nLi(\u03b8i) = Es,a,r,s\u2032 [(yDQNi \u2212Q(s, a; \u03b8i)) 2] , (2)\nwith yDQNi = r + \u03b3a\u2032Q(s \u2032, a\u2032; \u03b8\u2212i ), (3)\nwhere \u03b8\u2212i are the parameters of a target network that gets frozen for a number of iterations, while the online network Q(s, a; \u03b8i) is updated.\nLet the reward function of the environment be Renv . We propose to regularise the target function of the deep network, by splitting the reward function into n reward functions, weighted by wi:\nRenv(s, a) = n\u2211 k=1 wkRk(s, a) , for all s, a, s\u2032, (4)\nand training a separate reinforcement-learning agent on each of these reward functions. There are infinitely many different decompositions of a reward function possible, but to achieve smooth optimal value functions the decomposition should be such that each reward function is mainly affected by only a small number of state variables.\nBecause each agent has its own reward function, each agent i also has its own Q-value function associated with it: Qi(s, a; \u03b8). To derive a policy from these multiple action-value functions, an aggregator receives the action-values for the current state from the different agents and combines them into a single set of action-values (i.e., a single value for each action), using the same linear combination as used in the reward decomposition (Equation 4).\nQHRA(s, a; \u03b8) = n\u2211 k=1 wkQk(s, a; \u03b8) . (5)\nBecause the different agents can share multiple lower-level layers of a deep Q-network, the collection of agents can be viewed alternatively as a single agent with multiple heads, with each head producing the action-values of the current state under a different reward function. A single vector \u03b8 can be used for the parameters of this network. Each head is associated with a different reward function. This is the view that we adopt in this paper. We refer to it as a Hybrid Reward Architecture (HRA). Figure 1 illustrates the architecture. The loss function for HRA is:\nLi(\u03b8i) = Es,a,r,s\u2032 [\nn\u2211 k=1\n(yk,i \u2212Qk(s, a; \u03b8i))2 ] , (6)\nwith yk,i = Rk(s, a, s\u2032) + \u03b3max a\u2032 Qk(s \u2032, a\u2032; \u03b8\u2212i ) . (7)\nBy minimising this loss function, the different heads of HRA approximate the optimal action-value functions under the different reward functions: Q\u22171, . . . , Q \u2217 n. Furthermore, we define Q \u2217 HRA as follows:\nQ\u2217HRA(s, a) := n\u2211 k=1 wkQ \u2217 k(s, a) for all s, a .\nTherefore, with the update target above, the aggregator\u2019s Q-values approximate Q\u2217HRA. In general, Q\u2217HRA is not equal to Q \u2217 env, the optimal value function corresponding to Renv. If HRA\u2019s policy performs badly with respect to Renv, a different aggregation scheme can be used, for example, instead of the mean over heads, an aggregator action-value could be defined as the max over heads, or a voting-based aggregation scheme could be used. Alternatively, an update target based on the expected Sarsa update rule (van Seijen et al., 2009) can be used:\nyk,i = Rk(s, a, s \u2032) + \u03b3 \u2211 a\u2032 \u03c0(s\u2032, a\u2032)Qk(s \u2032, a\u2032; \u03b8\u2212i ) . (8)\nIn this case, minimisation of the loss function results in the heads approximating the action-values for \u03c0 under the different reward functions: Q\u03c01 , . . . , Q \u03c0 n. We define Q \u03c0 HRA(s, a) := \u2211n k=1 wkQ \u03c0 k (s, a). In contrast to Q\u2217HRA, Q \u03c0 HRA is equal to Q \u03c0 env , as shown in the following theorem:\nTheorem 1. With an aggregator that implements Equation (5), for any reward decomposition and stationary policy \u03c0 the following holds:\nQ\u03c0HRA(s, a) = Q \u03c0 env(s, a) for all s, a.\nProof. Substituting (4) in (1) gives:\nQ\u03c0env(s, a) = E [ \u221e\u2211 i=0 \u03b3i n\u2211 k=1 wkRk(st+i, at+i)|st = s, at = a, \u03c0 ] ,\n= n\u2211 k=1 wk \u00b7 E [ \u221e\u2211 i=0 \u03b3iRk(st+i, at+i)|st = s, at = a, \u03c0 ] ,\n= n\u2211 k=1 wkQ \u03c0 k (s, a) = Q \u03c0 HRA(s, a) ."}, {"heading": "3.2 Improving Performance further by using high-level domain knowledge.", "text": "In its basic setting, the only domain knowledge applied to HRA is in the form of the decomposed reward function. However, one of the strengths of HRA is that it can easily exploit more domain knowledge, if available. Domain knowledge can be exploited in one of the following ways:\n3. Using pseudo-reward functions. Instead of updating a head of HRA using a component of the environment reward, it can be updated using a pseudo-reward. In this scenario, a set of GVFs is trained in parallel using pseudo-rewards. Each head of HRA uses (an) appropriate GVF(s). This can often result in more efficient learning.\nThe first two types of domain knowledge can be used by any method, not just HRA. However, because HRA can apply this knowledge to each head individually, it can exploit domain knowledge to a much greater extent. We show this empirically in Section 4.1."}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Fruit Collection task", "text": "In our first domain, we consider an agent that has to collect fruits as quickly as possible in a 10\u00d7 10 grid. There are 10 possible fruit locations, spread out across the grid. For each episode, a fruit is randomly placed on 5 of those 10 locations. The agent starts at a random position. The episode ends after 300 steps or when all 5 fruits have been eaten, whichever comes first.\nWe compare the performance of DQN with HRA using the same network. The training objective for DQN gives +1 reward for each fruit and uses \u03b3 = 0.95. For HRA, we decompose this reward function into 10 different reward functions, one per possible fruit location. The network consists of a binary input layer of length 110, encoding the agent\u2019s position and whether there is a fruit on each location. This is followed by a fully connected hidden layer of length 250. This layer is connected to 10 heads consisting of 4 linear nodes each, representing the action-values of the 4 actions under the different reward functions. Finally, the mean of all nodes across heads is computed using a final linear layer of length 4 that connects the output of corresponding nodes in each head. This layer has fixed weights with value 1/10 (i.e., it implements Equation 5). The difference between HRA and DQN is that DQN updates the network from the fourth layer using loss function (2), whereas HRA updates the network from the third layer using loss function (6).\nBesides the full network, we test using different levels of domain knowledge, as outlined in Section 3.2: 1) removing the irrelevant features for each head (providing only the position of the agent + the corresponding fruit feature); 2) the above plus identifying terminal states; 3) the above plus using pseudo rewards for learning GVFs to go to each of the 10 locations (instead of learning a value function associated to the fruit at each location). The advantage is that these GVFs can be trained even if there is no fruit (anymore) at a location. The head for a particular fruit copies the Q-values of the GVF corresponding to the fruit\u2019s location, or outputs 0s if there is currently no fruit at the location. We refer to these as HRA+1, HRA+2 and HRA+3, respectively. For DQN, we also tested a version that was applied to the same network as HRA+1; we refer to this version as DQN+1.\nWe performed experiments with update targets that estimate an optimal policy (Equation 7) and update targets that evaluate a uniformly random policy (using Equation 8). Acting greedily with respect to the Q-values of a uniformly random policy evaluated under reward function Renv, can in some domains yields a very good performance with respect to Renv. We optimise the step-size for each method separately.\nThe results are shown in Figure 3 for the best settings. Interestingly, for DQN estimating the optimal policy performed better, while for HRA estimating the random policy performed better. Overall, HRA shows a clear performance boost over DQN, even though the network is identical. Furthermore, adding different forms of domain knowledge causes further large improvements. Whereas using a network structure enhanced by domain knowledge causes large improvements for HRA, using that same network for DQN results in not learning anything at all. The big boost in performance that occurs when the the terminal states are identified is due to the representation becoming a one-hot vector. Hence, we removed the hidden layer and directly fed this one-hot vector into the different heads. Because the heads are linear, this representation reduces to an exact, tabular representation. For the tabular representation, we used the same step-size as the optimal step-size for the deep network version.\n4.2 ATARI game: Ms. Pac-Man\nOur second domain is the Atari 2600 game Ms. Pac-Man (see Figure 4). Points are obtained by eating pellets, while avoiding ghosts (contact with one causes Ms. Pac-Man to lose a life). Eating one of the special power pellets turns the ghosts blue for a small duration, allowing them to be eaten for extra points. Bonus fruits can be eaten for further points, twice per level. When all pellets have been eaten, a new level is started. There are a total of 4 different maps and 7 different fruit types, each with a different point value. We provide full details on the domain in the supplementary material.\nBaselines. While our version of Ms. Pac-Man is the same as used in literature, we use different preprocessing. Hence, to test the effect of our preprocessing, we implement the A3C method (Mnih et al., 2016) and run it with our preprocessing. We refer to the version with our preprocessing as \u2018A3C(channels)\u2019, the version with the standard preprocessing \u2018A3C(pixels)\u2019, and A3C\u2019s score reported in literature \u2018A3C(reported)\u2019.\nPreprocessing. Each frame from ALE is 210\u00d7 160 pixels. We cut the bottom part and the top part of the screen to end up with 160 \u00d7 160 pixels. From this, we extract the position of the different objects and create for each object a separate input channel, encoding its location with an accuracy of 4 pixels. This results in 11 binary channels of size 40\u00d7 40. Specifically, there is a channel for: Ms. Pac-Man, each of the four ghosts, each of the four blue ghosts (these are treated as different objects), the fruit plus one channel with all the pellets (including power pellets). For A3C, we combine the 4 channels of the ghosts into a single channel, to allow it to generalise better across ghosts. We do the same with the 4 channels of the blue ghosts. Instead of giving the history of the last 4 frames as done in literature, we give the orientation of Ms. Pac-Man as a 1-hot vector of length 4 (representing the 4 compass directions).\nHRA architecture. We use a HRA architecture with one head for each pellet, one head for each ghost, one head for each blue ghost and one head for the fruit. Similar to the fruit collection task, HRA uses GVFs that learn the Q-values for getting to a particular location in the map (it learns separate GVFs for each of the four maps). The agent learns part of its representation during training: it starts of with 0 GVFs and 0 heads for the pellets. By wandering around the maze, it discovers new map locations it can reach, resulting in new GVFs being created. Whenever the agent finds a pellet at a new location it creates a new head corresponding to the pellet.\nThe Q-values of the head of an object (pellet/fruit/ghost/blue ghost) are simply the Q-values of the GVF that corresponds with the object\u2019s location (i.e., moving objects use a different GVF each time). If an object is not on the screen, all its Q-values are 0. Each head i is assigned a weight wi, which can be positive or negative. For the head of a pellet/blue ghost/fruit the weight corresponds to the reward received when the object is eaten. For the regular ghosts (contact with one causes Ms. Pac-Man to lose a life), the weights are set to -1,000.\nWe test two aggregator types. The first one is a linear one that sums the Q-values of all heads multiplied with the weights (see Equation 5). For the second one, we take the weighted sum of all the heads that produce points, and normalise the resulting Q-values; then, we add the weighted Q-values of the heads of the regular ghosts.\nFor exploration, we test two complementary types of exploration. Each type adds an extra exploration head to the architecture. The first type, which we call diversification, produces random Q-values, drawn from a uniform distribution over [0, 20]. We find that it is only necessary during the first 50 steps, to ensure starting each episode randomly. The second type, which we call count-based, adds a bonus for state-action pairs that have not been explored a lot. It is inspired by upper confidence bounds (Auer et al., 2002). Full details can be found in the supplementary material.\nFor our final experiment, we implement a special head inspired by executive-memory literature (Fuster, 2003; Gluck et al., 2013). When a human game player reaches the maximum of his cognitive and physical ability, he starts to look for favourable situations or even glitches and memorises them. This cognitive process is indeed memorising a sequence of actions (also called habit), and is not necessarily optimal. Our executive-memory head records every sequence of actions that led to pass a level without any kill. Then, when facing the same level, the head gives a very high value to the recorded action, in order to force the aggregator\u2019s selection. Note that our simplified version of executive memory does not generalise.\nEvaluation metrics. There are two different evaluation methods used across literature which result in very different scores. Because ALE is ultimately a deterministic environment (it implements pseudo-randomness using a random number generator that always starts with the same seed), both evaluation metrics aim to create randomness in the evaluation in order to rate methods with more generalising behaviour higher. The first metric introduces a mild form of randomness by taking a random number of no-op actions before control is handed over to the learning algorithm. In the case of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum number of no-op steps, resulting in the game having a fixed start after all. The second metric selects random starting points along a human trajectory and results in much stronger randomness, and does result in the intended random start evaluation. We refer to these metrics as \u2018fixed start\u2019 and \u2018random start\u2019.\nResults. Figure 5 shows the training curves; Table 1 the final score after training. The best reported fixed start score comes from STRAW (Vezhnevets et al., 2016); the best reported random start score comes from the Dueling network architecture (Wang et al., 2016). The human fixed start score comes from Mnih et al. (2015); the human random start score comes from Nair et al. (2015). We train A3C for 800 million frames. Because HRA learns fast, we train it only for 5,000 episodes, corresponding with about 150 million frames (note that better policies result\nin more frames per episode). The score shown for HRA uses normalisation and both exploration types. We try different combinations (with/without normalisation and with/without each type of exploration) for HRA. The score shown for HRA uses the best combination: with normalisation and\n1.5\u00d7 108\n3.0\u00d7 108\n4.1\u00d7 107\n2.6\u00d7 105\n9.3\u00d7 106 1.0\u00d7 108\n8.4\u00d7 108\n0 500 1000 1500 2000 2500 3000\n200k\n400k\n600k\n800k\n1M\n0\nLevel 1\nLevel 5 Level 10\nLevel 32\nLevel 50\nLevel 100\nLevel 180\nframes\nframes frames\nframes\nframes\nframes\nframes\nHuman high-scoreSc\nor e\nEpisodes\nFigure 6: Training with trajectory memorisation.\nwith both exploration types. All of the combinations achieved over 10,000 points in training, except the combination with no exploration at all, which performed very poorly. With the best combination, HRA not only outperforms the state-of-the-art on both metrics, it also significantly outperforms the human score, convincingly demonstrating the strength of HRA.\nComparing A3C(pixels) and A3C(channels) in Table 1 reveals a surprising result: while we use advanced preprocessing by separating the screen image into relevant object channels, this did not significantly change the performance of A3C.\nIn our final experiment, we test how well HRA does if it exploits the weakness of the fixed-start evaluation metric by using a simplified version of executive memory. Using this version, we not only surpass the human high-score,1, we achieve the maximum possible score of 999,990 points in less than 3,000 episodes. The curve is slow in the first stages because the model has to be trained, but even though the further levels get more and more difficult, the level passing speeds up by taking advantage of already knowing the maps. Obtaining more points is impossible, not because the game ends, but because the score overflows to 0 when reaching a million points. 2"}, {"heading": "5 Discussion", "text": "One of the strengths of HRA is that it can exploit domain knowledge to a much greater extent than single-head methods. This is clearly shown by the fruit collection task: while removing irrelevant features causes a large improvement in performance for HRA, for DQN no effective learning occurred when provided with the same network architecture. Furthermore, separating the pixel image into multiple binary channels only makes a small improvement in the performance of A3C over learning directly from pixels. This demonstrates that the reason that modern deep RL struggle with Ms. Pac-Man is not related to learning from pixels; the underlying issue is that the optimal value function for Ms. Pac-Man cannot easily be mapped to a low-dimensional representation.\nHRA solves Ms. Pac-Man by learning close to 1,800 general value functions. This results in an exponential breakdown of the problem size: whereas the input state-space corresponding with the binary channels is in the order of 1077, each GVF has a state-space in the order of 103 states, small enough to be represented without any function approximation. While we could have used a deep network for representing each GVF, using a deep network for such small problems hurts more than it helps, as evidenced by the experiments on the fruit collection domain.\nWe argue that many real-world tasks allow for reward decomposition. Even if the reward function can only be decomposed in two or three components, this can already help a lot, due to the exponential decrease of the problem size that decomposition might cause.\n1highscore.com reports oyamafamily as the world record holder with 266,330 points. 2For a video of HRA\u2019s final trajectory reaching this point, see: https://youtu.be/VeXNw0Owf0Y"}, {"heading": "A Ms. Pac-Man - experimental details", "text": "A.1 General information about Atari 2600 Ms. Pac-Man\nThe second domain is the Atari 2600 game Ms. Pac-Man. Points are obtained by eating pellets, while avoiding ghosts (contact with one causes Ms. Pac-Man to lose a life). Eating one of the special power pellets turns the ghost blue for a small duration, allowing them to be eaten for extra points. Bonus fruits can be eaten for further increasing points, twice per level. When all pellets have been eaten, a new level is started. There are a total of 4 different maps (see Figure 7 and Table 2) and 7 different fruit types, each with a different point value (see Table 3).\nMs. Pac-Man is considered one of the hard games from the ALE benchmark set. When comparing performance, it is important to realise that there are two different evaluation methods for ALE games used across literature which result in hugely different scores (see Table 4). Because ALE is ultimately a deterministic environment (it implements pseudo-randomness using a random number generator that always starts with the same seed), both evaluation metrics aim to create randomness in the evaluation in order to discourage methods from exploiting this deterministic property and rate methods with more generalising behaviour higher. The first metric introduces a mild form of randomness by taking a random number of no-op actions before control is handed over to the learning algorithm. In the case of Ms. Pac-Man, however, the game starts with a certain inactive period that exceeds the maximum number of random no-op steps, resulting in the game having a fixed start after all. The second metric selects random starting points along a human trajectory and results in much stronger randomness, and does result in the intended random start evaluation.\nThe best method with fixed start evaluation is STRAW with 6,673 points (Vezhnevets et al., 2016); the best with random start evaluation is the dueling network architecture with 2,251 points (Wang et al., 2016). The human baseline, as reported by Mnih et al. (2015), is 15,693 points. The highest reported score by a human is 266,330.3 All reported scores are shown in Table 4.\n3See highscore.com: \u2018Ms. Pac-Man (Atari 2600 Emulated)\u2019.\nA.2 HRA architecture\nGVF heads. Ms. Pac-Man state is defined as its position on the map and its direction (heading North, East, South or West). Depending on the map, there are about 400 positions and 950 states (not all directions are possible for each position). A GVF is created online for each visited Ms. Pac-Man position. Each GVF is then in charge of determining the value of the random policy of Ms. Pac-Man state for getting the pseudo-reward placed on the GVF\u2019s associated position. The GVFs are trained online with off-policy 1-step bootstrapping with \u03b1 = 1 and \u03b3 = 0.99. Thus, the full tabular representation of the GVF grid contains nbmaps\u00d7nbpositions\u00d7nbstates\u00d7nbactions \u2248 14M entries.\nAggregator. For each object of the game: pellets, ghosts and fruits, the GVF corresponding to its position is activated with a multiplier depending on the object type. Edible objects\u2019 multipliers are consistent with the number of points they grant: pellets\u2019 multiplier is 10, power pellets\u2019 50, fruits\u2019 200, and blue and blue (edible) ghosts\u2019 1,000. Initial tests showed that a ghosts\u2019 multiplier of -1,000 is a fair balance between gaining points and not being killed. Finally, the aggregator sums up all the activated and multiplied GVFs to compute a global score for each of the nine actions and chooses the action that maximises it.\nDiversification head. The blue curve on Figure 8 reveals that this na\u00efve setting performs badly because it tends to deterministically repeat a bad trajectory like a robot hitting a wall continuously. In order to avoid this pitfall, we need to add an exploratory mechanism. An -greedy exploration is not suitable for this problem since it might unnecessarily put Ms. Pac-Man in danger. A Boltzmann distributed exploration is more suitable because it favours exploring the safe actions. It would be\npossible to apply this on top of the aggregator, but we chose here to instead add a diversification head that generates for each action a random value. This random value is drawn according to a uniform distribution in [0,20]. We found that it is only necessary during the 50 first steps, to ensure each episode starts randomly.\nScore heads normalisation. The orange curve on Figure 8 shows that the diversification head solves the determinism issue. The so-built architecture progresses fast, up to 10,000 points, but then starts regressing. The analysis of the generated trajectories reveals that the system has difficulty in finishing levels: indeed, when only a few pellets remain on the screen, the aggregator gets overwhelmed by the ghost avoider values. The regression in score is explained by the fact that the more the system learns the more it gets easily scared by the ghosts, and therefore the more difficult it is for it to finish the levels. We solve this issue by modifying the additive aggregator with a normalisation over the score heads between 0 and 1. To fit this new value scale, the ghost multiplier is modified to -10.\nTargeted exploration head. The green curve on Figure 8 grows over time as expected. It might be surprising at first look that the orange curve grows faster, but it is because the episodes without normalisation tend to last much longer, which allows more GVF updates per episode. In order to speed up the learning, we decide to use a targeted exploration head (teh), that is motivated by trying out the less explored state-action couples. The value of this agent is computed as follows:\nvalueteh(s, a) = \u03ba\n\u221a 4 \u221a N\nn(s, a) , (9)\nwhere N is the number of actions taken until now and n(s, a) the number of times action a has been performed in state s. This formula is inspired from upper confidence bounds (Auer et al., 2002), but replacing the stochastically motivated logarithmic function by a less drastic one is more compliant with our need for bootstrapping propagation. Note that this targeted exploration head is not a replacement for the diversification head. They are complementary: diversification for making\neach trajectory unique, and targeted exploration for prioritised exploration. The red curve on Figure 8 reveals that the new targeted exploration head helps exploration and makes the learning faster. This setting constitutes the HRA architecture that is used in every experiment.\nExecutive memory head. When human game players reach the maximum of their cognitive and physical ability, they start to look for favourable situations or even glitches and memorise them. This cognitive process is referred as executive memory in cognitive science literature (Fuster, 2003; Gluck et al., 2013). The executive memory head records every sequence of actions that led to passing a level without losing any life. Then, when facing the same level, the head gives a high value to the recorded action, in order to force the aggregator\u2019s selection. Nota bene: since it does not allow generalisation, this head is only employed for the level-passing experiment.\nA.3 A3C baselines\nSince we use low-level features for the HRA architecture, we implement A3C and evaluate it both on the pixel-based environment and on the low-level features. The implementation is performed in a way to reproduce the results of Mnih et al. (2015).\nThey are both trained similarly as in Mnih et al. (2016) on 8\u00d7 108 frames, with \u03b3 = 0.99, entropy regularisation of 0.01, n-step return of 5, 16 threads, gradient clipping of 40, and \u03b1 is set to take the maximum performance over the following values: [0.0001, 0.00025, 0.0005, 0.00075, 0.001]. The pixel-based environment is a reproduction of the preprocessing and the network, except we only use a history of 2, because our steps are twice as long.\nWith the low features, five channels of a 40 \u00d7 40 map are used embedding the positions of Ms. Pac-Man, the pellets, the ghosts, the blue ghosts, and the special fruit. The input space is therefore 5\u00d7 40\u00d7 40 plus the direction appended after convolutions: 2 of them with 16 (respectively 32) filters of size 6\u00d7 6 (respectively 4\u00d7 4) and subsampling of 2\u00d7 2 and ReLU activation (for both). Then, the network uses a hidden layer of 256 fully connected units with ReLU activation. Finally, the policy\nhead has nbactions = 9 fully connected units with softmax activation, and the value head has 1 unit with a linear activation. All weights are uniformly initialised He et al. (2015).\nA.4 Results\nTraining curves. Most of the results are already presented in the main document. For more completeness, we present here the results of the gridsearch over \u03b3 values for both with and without the executive memory. Values [0.95, 0.97, 0.99] have been tried independently for \u03b3score and \u03b3ghosts.\nFigure 9 compares the training curves without executive memory. We can notice the following:\n\u2022 all \u03b3 values turn out to yield very good results, \u2022 those good results generalise over random human starts, \u2022 high \u03b3 values for the ghosts tend to be better, \u2022 and the \u03b3 value for the score is less impactful.\nFigure 10 compares the training curves with executive memory. We can notice the following:\n\u2022 the comments on Figure 9 are still holding, \u2022 and it looks like that there is a bit more randomness in the level passing efficiency."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "The option-critic architecture", "author": ["P. Bacon", "J. Harb", "D. Precup"], "venue": "In Proceedings of the Thirthy-first AAAI Conference On Artificial Intelligence (AAAI),", "citeRegEx": "Bacon et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Bacon et al\\.", "year": 2017}, {"title": "Recent advances in hierarchical reinforcement learning", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "Barto and Mahadevan,? \\Q2003\\E", "shortCiteRegEx": "Barto and Mahadevan", "year": 2003}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bellemare et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2013}, {"title": "Cortex and mind: Unifying cognition", "author": ["J.M. Fuster"], "venue": "Oxford university press,", "citeRegEx": "Fuster,? \\Q2003\\E", "shortCiteRegEx": "Fuster", "year": 2003}, {"title": "Learning and memory: From brain to behavior", "author": ["M.A. Gluck", "E. Mercado", "C.E. Myers"], "venue": "Palgrave Macmillan,", "citeRegEx": "Gluck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gluck et al\\.", "year": 2013}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Proceedings of the IEEE international conference on computer vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K.R. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Kulkarni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "Kumaran", "H. King D", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Harley", "T.P. Lillicrap", "D. Silver", "K. Kavukcuoglu"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "Maria", "A. De", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "Deep Learning Workshop,", "citeRegEx": "Nair et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2015}, {"title": "A survey of multi-objective sequential decision-making", "author": ["D.M. Roijers", "P. Vamplew", "S. Whiteson", "R. Dazeley"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Roijers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roijers et al\\.", "year": 2013}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "Schaul et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2016}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J. Schmidhuber"], "venue": "In IEEE Transactions on Autonomous Mental Development", "citeRegEx": "Schmidhuber,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning: A promising framework for developmental robotics", "author": ["A. Stout", "G. Konidaris", "A.G. Barto"], "venue": "In The AAAI Spring Symposium on Developmental Robotics,", "citeRegEx": "Stout et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Stout et al\\.", "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "Precup", "Doina"], "venue": "In Proceedings of 10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS),", "citeRegEx": "Sutton et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S.P. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Algorithms for reinforcement learning", "author": ["C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Szepesv\u00e1ri,? \\Q2009\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2009}, {"title": "Learning values across many orders of magnitude", "author": ["H. van Hasselt", "A. Guez", "M. Hessel", "V. Mnih", "D. Silver"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI, pp. 2094\u20132100,", "citeRegEx": "Hasselt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hasselt et al\\.", "year": 2016}, {"title": "A theoretical and empirical analysis of expected sarsa", "author": ["H. van Seijen", "H. van Hasselt", "S. Whiteson", "M. Wiering"], "venue": "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "Seijen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Seijen et al\\.", "year": 2009}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["A. Vezhnevets", "V. Mnih", "S. Osindero", "A. Graves", "O. Vinyals", "J. Agapiou", "K. Kavukcuoglu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "T. Schaul", "M. Hessel", "H. van Hasselt", "M. Lanctot", "N. Freitas"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "The best method with fixed start evaluation is STRAW with 6,673 points (Vezhnevets et al., 2016); the best with random start evaluation is the dueling network architecture with 2,251 points (Wang et al., 2016)", "author": ["Mnih"], "venue": "The human baseline,", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}, {"title": "A3C baselines Since we use low-level features for the HRA architecture, we implement A3C and evaluate it both on the pixel-based environment and on the low-level features. The implementation is performed in a way to reproduce the results", "author": ["Mnih"], "venue": "Mnih et al", "citeRegEx": "Mnih,? \\Q2015\\E", "shortCiteRegEx": "Mnih", "year": 2015}], "referenceMentions": [{"referenceID": 18, "context": "In reinforcement learning (RL) (Sutton & Barto, 1998; Szepesv\u00e1ri, 2009), the goal is to find a behaviour policy that maximises the return\u2014the discounted sum of rewards received over time\u2014in a data-driven way.", "startOffset": 31, "endOffset": 71}, {"referenceID": 8, "context": "Mnih et al. (2015) achieved a big breakthrough in this area: by combining standard RL techniques with deep neural networks, they outperformed humans on a large number of Atari 2600 games, by learning a policy from pixels.", "startOffset": 0, "endOffset": 19}, {"referenceID": 14, "context": "Intrinsic motivation (Stout et al., 2005; Schmidhuber, 2010) uses the above observation to improve learning in sparse-reward domains.", "startOffset": 21, "endOffset": 60}, {"referenceID": 13, "context": "Intrinsic motivation (Stout et al., 2005; Schmidhuber, 2010) uses the above observation to improve learning in sparse-reward domains.", "startOffset": 21, "endOffset": 60}, {"referenceID": 16, "context": "Similar to the Horde architecture (Sutton et al., 2011), all these agents can learn in parallel on the same sample sequence by using off-policy learning.", "startOffset": 34, "endOffset": 55}, {"referenceID": 3, "context": "Pac-Man, a hard game from the ALE benchmark set (Bellemare et al., 2013).", "startOffset": 48, "endOffset": 72}, {"referenceID": 16, "context": "Our HRA method builds upon the Horde architecture (Sutton et al., 2011).", "startOffset": 50, "endOffset": 71}, {"referenceID": 11, "context": "Learning with respect to multiple reward functions is also a topic of multi-objective learning (Roijers et al., 2013).", "startOffset": 95, "endOffset": 117}, {"referenceID": 17, "context": "This work is also related to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al.", "startOffset": 37, "endOffset": 78}, {"referenceID": 1, "context": "This work is also related to options (Sutton et al., 1999; Bacon et al., 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al.", "startOffset": 37, "endOffset": 78}, {"referenceID": 7, "context": ", 2017), and more generally hierarchical learning (Barto & Mahadevan, 2003; Kulkarni et al., 2016).", "startOffset": 50, "endOffset": 98}, {"referenceID": 9, "context": "Hence, to test the effect of our preprocessing, we implement the A3C method (Mnih et al., 2016) and run it with our preprocessing.", "startOffset": 76, "endOffset": 95}, {"referenceID": 0, "context": "It is inspired by upper confidence bounds (Auer et al., 2002).", "startOffset": 42, "endOffset": 61}, {"referenceID": 4, "context": "For our final experiment, we implement a special head inspired by executive-memory literature (Fuster, 2003; Gluck et al., 2013).", "startOffset": 94, "endOffset": 128}, {"referenceID": 5, "context": "For our final experiment, we implement a special head inspired by executive-memory literature (Fuster, 2003; Gluck et al., 2013).", "startOffset": 94, "endOffset": 128}, {"referenceID": 22, "context": "The best reported fixed start score comes from STRAW (Vezhnevets et al., 2016); the best reported random start score comes from the Dueling network architecture (Wang et al.", "startOffset": 53, "endOffset": 78}, {"referenceID": 23, "context": ", 2016); the best reported random start score comes from the Dueling network architecture (Wang et al., 2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 8, "context": "The human fixed start score comes from Mnih et al. (2015); the human random start score comes from Nair et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 8, "context": "The human fixed start score comes from Mnih et al. (2015); the human random start score comes from Nair et al. (2015). We train A3C for 800 million frames.", "startOffset": 39, "endOffset": 118}], "year": 2017, "abstractText": "One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the overall value function is much smoother and can be easier approximated by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.", "creator": "LaTeX with hyperref package"}}}