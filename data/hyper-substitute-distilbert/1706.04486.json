{"id": "1706.04486", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Learning and Evaluating Musical Features with Deep Autoencoders", "abstract": "beginning this work we describe and evaluate methods including learn evolutionary patterns. each constant is additive number that chooses fixed contiguous beats of value and is derived whenever a symbolic value. we consider logic - based methods defining symbolic databases, inverse context searching, combining evaluate predicted sampling values on a forward prediction and a classification calculation.", "histories": [["v1", "Wed, 14 Jun 2017 13:35:46 GMT  (101kb,D)", "http://arxiv.org/abs/1706.04486v1", null], ["v2", "Thu, 15 Jun 2017 20:09:52 GMT  (101kb,D)", "http://arxiv.org/abs/1706.04486v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["mason bretan", "sageev oore", "doug eck", "larry heck"], "accepted": false, "id": "1706.04486"}, "pdf": {"name": "1706.04486.pdf", "metadata": {"source": "CRF", "title": "A Unit Selection Methodology for Music Generation Using Deep Neural Networks", "authors": ["Mason Bretan", "Sageev Oore", "Douglas Eck", "Larry Heck"], "emails": [], "sections": [{"heading": null, "text": "vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task."}, {"heading": "1 Introduction", "text": "Music is a high dimensional and complex domain. Many of the tasks involving music analysis and information retrieval focus on reducing this dimensionality by categorizing music with discrete labels describing elements such as genre, composer, instrumentation, or era [5, 4]. However, new music is constantly being recorded, new genres are being identified/created, and so it is unreasonable to expect a musical dataset to have correct and comprehensive labelled examples for any of the above classifications. Any approach that depends on this will therefore not naturally scale.\nIn this article we focus on finding and leveraging the inherent continuity of the musical space. Instead of partitioning music based on a discrete labeling scheme we use deep neural networks to project music into a continuous, low dimensional space that captures meaningful characteristics. To assess the efficacy of this learned space, known as the \u201cembedding\u201d, we first determine the properties that such an embedding should comprise.\nTwo common tasks associated with music are listening and composing. Thus, a useful musical embedding should have properties that allow it to successfully complete corresponding tasks: (1) \u201cmake sense of what is heard\u201d, i.e. extract meaningful features that correlate with human perceptions, and (2) \u201ccompose\u201d, i.e. complete a defined task related to music generation. We thus use two separate tests to measure the efficacy related to these tasks: composer identification, and generative prediction. Note that in fact, listening and composing are not at all independent of each other: real-world human musical activities such as performing, improvising, and playing together require significant integration of listening and composing, among other skills.\nIn this work we describe and evaluate a multitude of methods to learn the musical embeddings. In our context, each embedding is a vector that represents four contiguous beats of music (each of quarter note duration) and is derived from a symbolic representation. That is, our raw input data are MIDI piano rolls with accurate tempo information. The methods include networks trained to denoise, reconstruct context, predict forward, and discriminate between composers. In addition to the common training methods for completing these tasks we introduce an architecture and training methodology that influences the network to learn parameters that provide the embedding with a desired property."}, {"heading": "2 Related Work", "text": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9]. For example, Huang et al [2] describe the use of a deep structured semantic model\nar X\niv :1\n70 6.\n04 48\n6v 1\n[ cs\n.S D\n] 1\n4 Ju\nn 20\nto new latent semantic models in order to project queries and documents into a common, lower-dimensional space in which cosine similarity provides a good estimate of the relevance between pairs of original items (i.e. before their projection).\nOne clear intuition for finding such embeddings is that two textual excerpts can be very related even if they do not use the exact same keywords. The hope is that the mapping to a lower-dimensional, continuous, embedding does indeed take two such excerpts to nearby points. Similarly, two musical passages can be very related without using the exact same harmonic, melodic or rhythmic structure. We are thus interested in finding good embeddings that will achieve the same goal, within the context of music. Indeed, while music and language have significant structural parallels[1], it is generally only recently that researchers have begun to explore how this may give rise to cross-pollination between the respective fields.\nIn ChordRipple[3], Huang et al create a tool that helps composing students select a next chord in a sequence or replace a subsequence of chords within a longer progression. To achieve this they use an underlying semantic representation derived by learning embeddings of chords, so that chords are mapped to similar vectors when they occur near each other in the data. Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords."}, {"heading": "3 Data and Representation", "text": ""}, {"heading": "3.1 Dataset & Augmentation", "text": "In order to test the various networks and training procedures we used a collection of piano compositions from 25 artists. The distribution of compositions among artists is depicted in Table 1. At least two songs from each artist were held out for testing. Only songs with a time signature in which the note value of one beat is equal to a quarter note were used (i.e. the denominator of the time signature must have a value of 4).\nWe augmented the data by transposing each piece into all keys. This also prevented the networks from simply learning the bias any composers might have had for specific key signatures."}, {"heading": "3.2 Representation", "text": "A piano roll representation of music can be thought of as an mxn matrix where m is the number of possible pitches and n represents the total time. In this work we allow for 60 total pitches ranging from midi note 36 (two octaves below middle \u2019C\u2019) to 96 (three octaves above it). Any notes that fall outside of this range are transposed into either the lowest or highest octaves available.\nInstead of using a standard decimal representation of time portraying seconds or milliseconds. Time is represented as \u201cticks-per-beat\u201d (or \u201cpulses per quarter note\u201d) which illustrate the smallest unit of time used for sequencing note events based on the musical notion of a beat. For example, if a value of four ticks-perbeat were used then the shortest possible note that could be accurately portrayed would be a sixteenth note. Triplets could not be accurately portrayed with this resolution. Typically 96 ticks-per-beat is considered adequate for capturing the temporal nuances of performance data and is the default resolution. The data used in this work come from quantized representations of a score (they are not performed) so capturing such nuances is not possible. Therefore, we use 24 ticks-per-beat which sufficiently captures the rhythms present in our data.\nEach of the models described in the following section takes as input a 60x96 matrix, corresponding to four beats (i.e. 96 ticks) worth of data. In the experiments only the note onsets are considered. The duration of each note is not addressed such that a matrix representing the beat sequence consisting of a quarter note middle \u2018C\u2019 followed by three quarter note rests is identical to the matrix representing a beat sequence containing only a whole note middle \u2018C\u2019. Note onsets are represented with a \u20181\u2019 in the pitch-time location they occur and all other values in the matrix are \u20180\u2019.\nIn order to train the following models a database of all segments of four contiguous beats in the training dataset are extracted. After the transposition augmentation process there are 4,682,293 four beat units available for training. We remove all duplicates and use 3,997,873 unique units for training the models described in the next section."}, {"heading": "4 Models", "text": "In this section several methods for learning embeddings are presented. While the architecture and number of learnable parameters remain roughly the same for each model, the inputs and outputs are modified depending on the task. Each model involves a series of convolutional layers followed by fully connected layers leading to a 100-dimensional vector depicting the embedding. This size was chosen because it is small enough to quickly compute nearest neighbor search over a database consisting of over a million possible data points for real-time applications, yet large enough to embed many important musical features."}, {"heading": "4.1 Denoising Autoencoders", "text": "A single layer in a neural network takes the output from the previous layer, hi\u22121, and applies an affine transformation followed by a nonlinearity to get hi = f\u03b8i(hi\u22121) = \u03c3i(Wihi\u22121+bi). We write \u03b8i = {Wi,bi} to denote the parameters for layer i.\nThe first half of an autoencoder, the \u201cencoder\u201d, consists of n \u2265 1 such layers, f\u03b81 , . . . , f\u03b8n , where the input to f\u03b81 is simply x, the input to the autoencoder. The output so far is given by\nhn = f\u03b8n(f\u03b8n\u22121(. . . f\u03b81(x) . . .))\n. We also write h (i.e. with no subscript) to denote the output of the final (i.e. nth) layer of this encoder. While the dimensionality of the intermediate layers may be larger than that of x, the dimensionality of\nthe final embedding, h is strictly (and usually considerably) less than that of x. That is, if x \u2208 Rd and h \u2208 Rp, then p < d.\nThe second half of the autoencoder, the \u201cdecoder\u201d consists of the inverse mapping,\nx\u0302n = f\u03b8\u20321(f\u03b8\u20322(. . . f\u03b8\u2032n(h) . . .))\nwhere f\u03b8\u2032i = \u03c3(W \u2032 i) + b \u2032). The network is trained to optimize the parameters \u03b8 = {W, b} by minimizing a given distance function. In order to capture local features that may repeat themselves, convolution is used so that the latent representation of the k-th feature map becomes hk = \u03c3(W k + bk).\nTo avoid simply learning the identity function the model learns to denoise a corrupted version of the input. In this work we test three methods of corruption:\n1. Random note drop \u2014 A random subset containing 50% of the note onsets within a given input matrix are zeroed out and trained to be reconstructed.\n2. Random beat drop \u2014 All the note onsets within a randomly selected beat (a 60x24 space in the input matrix) are zeroed out and trained to be reconstructed.\n3. Octave split \u2014 Either the two lower octaves (24x96 space) or three upper octaves (36x96 space) are zeroed out and reconstructed. This is a rough estimate of splitting the piano roll into left and right hand parts, thus, the task for the network is to successfully reconstruct the right hand part given the left hand part and vice versa.\nTo measure the distance between the reconstruction and original (non-corrupted) input, cosine similarity is used:\nsim(X\u0303 , Y\u0303 ) = X\u0303T \u00b7 Y\u0303 |X\u0303 ||Y\u0303 |\n(1)\nwhere ~X and ~Y are two equal length vectors derived by flattening the reconstruction and ground truth matrices.\nNegative examples are included using the following softmax function:\nP(R\u0303|Q\u0303) = exp(sim(Q\u0303 , R\u0303))\u2211 d\u0303 D exp(sim(Q\u0303 , d\u0303))\n(2)\nwhere ~R is the flattened reconstructed matrix and ~Q is the non-corrupted input. D is the set of five reconstructed matrices that includes ~R and four candidate reconstructed matrices derived from four randomly selected samples in the training set. The network then minimizes the following differentiable loss function using gradient descent:\n\u2212log \u220f\n(Q,R)\nP(R\u0303|Q\u0303) (3)\nThe architecture of the network is depicted in Figure 1. The first layer convolves a 12x6 feature map (one octave of pitches and a sixteenth note duration worth of ticks) and has a stride rate of the same size. The subsequent convolutional layers use feature maps with stride rates that are half the size. Each layer uses exponential linear units (elu) and batch normalization is performed on each layer. The batch size is 100. The parameters of the encoder, \u03b8 = {W, b} and decoder, \u03b8\u2032 = {W \u2032, b\u2032} are constrained such that W \u2032 =WT . A fully connected layer is used for the last layer of the encoder such that the embedding is a one-dimensional vector describing the entire measure."}, {"heading": "4.2 Context Prediction", "text": "Predicting context has proven to be a powerful method for learning effective word embeddings. Similarly, we use the surrounding music of a specific four beat unit to learn the music embeddings. If Ui denotes the ith four beat unit in a sequence of four beat units from a single composition, the idea is to utilize the information provided by Ui+1 and Ui\u22121. Using an identical architecture as the previous autoencoder (including tied weights between encoder and decoder) we train two models with the tasks, respectively, of\n1. Forward prediction \u2014 The input to the network is Ui and the output is Ui+1; and\n2. Contextual prediction\u2014The input to the network isUi and the output isUi\u22121+Ui+1. By summing the matrices of the surrounding units we maintain symmetry between the encoder and decoder, while obtaining 8 beats of context in a 60x96 space."}, {"heading": "4.3 Composer Classification", "text": "Another task is to learn to classify four-beat units based on their composer. One rationale behind this task is that in order to be successful at predicting composer, the learned set of features has necessarily captured some important information about the musical passage. This is analogous to transfer learning methods used in computer vision tasks in which a network is first trained to label images and then, assuming relevant features have been learned, its parameters are used for additional tasks.\nThe architecture for the classification network is identical to the encoder portion of the previously described autoencoder. However, instead of a decoder, the 100-sized embedding vector is attached to a single output layer of size 27 (one for each composer). For the loss function we use softmax with cross-entropy."}, {"heading": "4.4 Regularized Training", "text": "In the last model we use the task of composer classification as a means to regularizing the embedding of the previously described contextual prediction network. The general premise is depicted in Figure 2. Given the original encoder/decoder network an additional network is attached to the embedding layer. In particular, given a unit Ui, we use its 100-dimensional embedding representation h(U) as the (fully connected) input to a hidden layer with 50 hidden units, which in turn is fully connected to a 27-unit softmax layer.\nThe main objective of the network remains the same, given Ui then reconstruct Ui\u22121 + Ui+1. The auxiliary task of composer classification is imposed on the embedding such that network\u2019s parameters are optimized to not only reconstruct context, but also predict composer."}, {"heading": "5 Experiments and Results", "text": "We use several techniques and measures to assess the efficacy of our embeddings.\n5.0.1 Forward Prediction & Ranking\nWe train 3 stacked LSTM cells, each with 400 units, to predict the next step of a sequence. Specifically, at each time step, the input to the network is a the 100-dimensional embedding vector of a 4-beat unit Ui. We train this network to predict the 8th unit, Ui+7 given the previous 7 units Ui,Ui+1, . . .Ui+6. This means that 28 beats of context (in four beat chunks) are provided before the prediction is made.\nFor each given target Ui+7 as described above, we create a set of 1000 embedding vectors: one is h(Ui+7), the true embedding for the target. The other 999 vectors are embeddings of randomly selected units in the data set.\nCosine similarity is measured between the output of the LSTM and the encodings of each unit. The similarities are then sorted and ranked from highest to lowest similarity (so the lower the rank the better).\nThe LSTM was trained with sequences provided by compositions in the training set and tested on 325,219 sequences from compositions in the test set. Embeddings from each of the previously described models were used resulting in seven prediction networks.\n5.0.2 Composer Classification\nIn this experiment we test how useful the features learned in the 100-sized embedding vector are for discriminating between composers. To do this we train a neural network that takes in the embedding as an input, has a single elu activated hidden layer with 50 units, and an output layer of size 27 representing each artist. Given an input U, the softmax classification layer outputs a probability distribution P (Cj |U) over the set of composers C1, . . . , C27. The composer with the highest probability, f(x) = argmaxi\u22081...27 pi(x), is used to predict the composer."}, {"heading": "5.1 Results", "text": "For the prediction task we collected the ranks provided by each of the seven prediction LSTM networks for all 325,219 runs. We used these collection of ranks to compare models. The median, spread, and skew are reported in Table 2 and Figure 3 shows a boxplot of the results. Given that ranking distributions skewed considerably toward zero (the highest possible rank) we used the median instead of the mean.\nTo test for significance between the ranking distributions of each model t-tests comparing each possible model combination (21 possible comparisons) were performed. After a Bonferroni correction statistical significance is indicated by p < 0.00037. All distributions are significantly different from each other.\nTo compare the spreads of each distribution we used Levene\u2019s test, which tests the null hypothesis that all input samples are from populations with equal variances. For all seven distributions the test statistic\n[W=7398.17] indicated significant differences at the p < .001 level.\nThe performance of the embeddings in the classification task are reported in Table 3. For multi-class classification we computed two measures. The Micro-F1 score calculates metrics globally by counting the total true positives, false negatives and false positives. The Macro-F1 score calculates metrics for each composer separately and takes their average, weighted by the number of true instances for each composer. Thus, the Macro-F1 score accounts for label imbalance."}, {"heading": "6 Conclusions", "text": "We have demonstrated and compared a variety of deep autoencoder architectures and objective functions to learn embeddings of note-based musical data. The network trained to reconstruct context performed the best at the LSTM prediction task. Unsurprisingly, the network trained to identify composers performed much better at this task than the features learned by the resulting embeddings of the denoising autoencoders and context prediction networks. However, by regularizing the contextual prediction network with this auxiliary classification task performance was improved. This network learned parameters that found a performance balance between the two experimental tasks."}], "references": [{"title": "Comparison between language and music", "author": ["Mireille Besson", "Daniele Sch\u00f6n"], "venue": "Annals of the New York Academy of Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Chordripple: Recommending chords to help novice composers go beyond the ordinary", "author": ["Cheng-Zhi Anna Huang", "David Duvenaud", "Krzysztof Z. Gajos"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Music type classification by spectral contrast feature", "author": ["Dan-Ning Jiang", "Lie Lu", "Hong-Jiang Zhang", "Jian-Hua Tao", "Lian-Hong Cai"], "venue": "In Multimedia and Expo,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "A comparative study on content-based music genre classification", "author": ["Tao Li", "Mitsunori Ogihara", "Qi Li"], "venue": "In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Chord2vec: Learning Musical Chord Embeddings", "author": ["Sephora Madjiheurem", "Lizhen Qu", "Christian Walder"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G Hinton"], "venue": "In Proc. SIGIR Workshop Information Retrieval and Applications of Graphical Models,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Many of the tasks involving music analysis and information retrieval focus on reducing this dimensionality by categorizing music with discrete labels describing elements such as genre, composer, instrumentation, or era [5, 4].", "startOffset": 219, "endOffset": 225}, {"referenceID": 3, "context": "Many of the tasks involving music analysis and information retrieval focus on reducing this dimensionality by categorizing music with discrete labels describing elements such as genre, composer, instrumentation, or era [5, 4].", "startOffset": 219, "endOffset": 225}, {"referenceID": 1, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 6, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 7, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 8, "context": "There is a considerable body of work that uses deep learning to extract semantic content from languagebased data[2, 7, 8, 9].", "startOffset": 112, "endOffset": 124}, {"referenceID": 1, "context": "For example, Huang et al [2] describe the use of a deep structured semantic model", "startOffset": 25, "endOffset": 28}, {"referenceID": 0, "context": "Indeed, while music and language have significant structural parallels[1], it is generally only recently that researchers have begun to explore how this may give rise to cross-pollination between the respective fields.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "In ChordRipple[3], Huang et al create a tool that helps composing students select a next chord in a sequence or replace a subsequence of chords within a longer progression.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords.", "startOffset": 18, "endOffset": 21}, {"referenceID": 7, "context": "Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords.", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": "Madjiheurem et al [6] compare the application of skip-gram-based models[8] and sequence-to-sequence models[10] for learning embeddings of chords.", "startOffset": 106, "endOffset": 110}], "year": 2017, "abstractText": "In this work we describe and evaluate methods to learn musical embeddings. Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task.", "creator": "LaTeX with hyperref package"}}}