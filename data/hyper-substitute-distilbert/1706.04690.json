{"id": "1706.04690", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2017", "title": "Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP", "abstract": "online fast linear regression proves an online problem where its algorithm algorithm identifying a subset matrix coordinates its observe in an adversarially independent feature x, writes a basis - valued matrix, knows maximum true constraint, and incurs the squared integral. the goal resides to identify an easier estimation option and sublinear expectation to the query sparse linear predictor in hindsight. without null correlation, this problem is known to be computationally incorrect. citing this paper, respondents make the assumption that approximation extraction satisfies statistical isometry property, to establish that this assumption continues to computationally efficient algorithms given sublinear expectation reflecting new generations of the problem. except the first variant, thus true label is generated additionally to arbitrary strict linear model bearing additive gaussian derivatives. in the second, his true derivative is discarded adversarially.", "histories": [["v1", "Wed, 14 Jun 2017 23:03:54 GMT  (25kb)", "http://arxiv.org/abs/1706.04690v1", "Appearing in 34th International Conference on Machine Learning (ICML), 2017"]], "COMMENTS": "Appearing in 34th International Conference on Machine Learning (ICML), 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["satyen kale", "zohar s karnin", "tengyuan liang", "d\u00e1vid p\u00e1l"], "accepted": true, "id": "1706.04690"}, "pdf": {"name": "1706.04690.pdf", "metadata": {"source": "CRF", "title": "Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP", "authors": ["Satyen Kale"], "emails": ["satyenkale@google.com", "zkarnin@gmail.com", "Tengyuan.Liang@chicagobooth.edu", "dpal@yahoo-inc.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 6.\n04 69\n0v 1\n[ cs\n.L G\n] 1\nOnline sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially."}, {"heading": "1 Introduction", "text": "In modern real-world sequential prediction problems, samples are typically high dimensional, and construction of the features may itself be a computationally intensive task. Therefore in sequential prediction, due to the computation and resource constraints, it is preferable to design algorithms that compute only a limited number of features for each new data example. One example of this situation, from [Cesa-Bianchi et al., 2011], is medical diagnosis of a disease, in which each feature is the result of a medical test on the patient. Since it is undesirable to subject a patient to a battery of medical tests, we would like to adaptively design diagnostic procedures that rely on only a few, highly informative tests.\nOnline sparse linear regression (OSLR) is a sequential prediction problem in which an algorithm is allowed to see only a small subset of coordinates of each feature vector. The problem is parameterized by 3 positive integers: d, the dimension of the feature vectors, k, the sparsity of the linear regressors we compare the algorithm\u2019s performance to, and k0, a budget on the number of features that can be queried in each round by the algorithm. Generally we have k \u226a d and k0 \u2265 k but not significantly larger (our algorithms need1 k0 = O\u0303(k)).\nIn the OSLR problem, the algorithm makes predictions over a sequence of T rounds. In each round t, nature chooses a feature vector xt \u2208 Rd, the algorithm chooses a subset of {1, 2, . . . , d} of size at most k\u2032 and observes the corresponding coordinates of the feature vector. It then makes a prediction y\u0302t \u2208 R based on the observed features, observes the true label yt, and suffers loss (yt \u2212 y\u0302t)2. The goal of the learner is\n\u2217This work was done while the author was at Yahoo Research, New York. 1In this paper, we use the O\u0303(\u00b7) notation to suppress factors that are polylogarithmic in the natural parameters of the problem.\nto make the cumulative loss comparable to that of the best k-sparse linear predictor w in hindsight. The performance of the online learner is measured by the regret, which is defined as the difference between the two losses:\nRegretT = T\u2211\nt=1\n(yt \u2212 y\u0302t)2 \u2212 min w: \u2016w\u20160\u2264k\nT\u2211\nt=1\n(yt \u2212 \u3008xt, w\u3009)2 .\nThe goal is to construct algorithms that enjoy regret that is sub-linear in T , the total number of rounds. A sub-linear regret implies that in the asymptotic sense, the average per-round loss of the algorithm approaches the average per-round loss of the best k-sparse linear predictor.\nSparse regression is in general a computationally hard problem. In particular, given k, x1, x2, . . . , xT and y1, y2, . . . , yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w\u2217 such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only O\u0303(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all \u03b4 > 0 there exists no polynomial-time algorithm with regret O(T 1\u2212\u03b4) unless NP \u2286 BPP .\nFoster et al. [2016] posed the open question of what additional assumptions can be made on the data to make the problem tractable. In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005]. It has been shown that if RIP holds and there exists a sparse linear predictor w\u2217 such that yt = \u3008xt, w\u2217\u3009 + \u03b7t where \u03b7t is independent noise, the offline sparse linear regression problem admits computationally efficient algorithms, e.g., Candes and Tao [2007]. RIP and related Restricted Eigenvalue Condition [Bickel et al., 2009] have been widely used as a standard assumption for theoretical analysis in the compressive sensing and sparse regression literature, in the offline case. In the online setting, it is natural to ask whether sparse regression avoids the computational difficulty under an appropriate form of the RIP condition. In this paper, we answer this question in a positive way, both in the realizable setting and in the agnostic setting. As a by-product, we resolve the adaptive feature selection problem as the efficient algorithms we propose in this paper adaptively choose a different \u201csparse\u201d subset of features to query at each round. This is closely related to attribute-efficient learning (see discussion in Section 1.2) and online model selection."}, {"heading": "1.1 Summary of Results", "text": "We design polynomial-time algorithms for online sparse linear regression for two models for the sequence (x1, y1), (x2, y2), . . . , (xT , yT ). The first model is called the realizable and the second is called agnostic. In both models, we assume that, after proper normalization, for all large enough t, the matrix Xt formed from the first t feature vectors x1, x2, . . . , xt satisfies the restricted isometry property. The two models differ in the assumptions on yt. The realizable model assumes that yt = \u3008xt, w\u2217\u3009 + \u03b7t where w\u2217 is k-sparse and \u03b7t is an independent noise. In the agnostic model, yt can be arbitrary, and therefore, the regret bounds we obtain are worse than in the realizable setting. The models and corresponding algorithms are presented in Sections 2 and 3 respectively. Interestingly enough, the algorithms and their corresponding analyses are completely different in the realizable and agnostic case.\nOur algorithms allow for somewhat more flexibility than the problem definition: they are designed to work with a budget k0 on the number of features that can be queried that may be larger than the sparsity parameter k of the comparator. The regret bounds we derive improve with increasing values of k0. In the case when k0 \u2248 k, the dependence on d in the regret bounds is polynomial, as can be expected in limited feedback settings (this is analogous to polynomial dependence on d in bandit settings). In the extreme case when k0 = d, i.e. we have access to all the features, the dependence on the dimension d in the regret bounds we prove is only logarithmic. The interpretation is that if we have full access to the features, but the goal is to compete with just k sparse linear regressors, then the number of data points that need to be seen to achieve good predictive accuracy has only logarithmic dependence on d. This is analogous to the (offline)\ncompressed sensing setting where the sample complexity bounds, under RIP, only depend logarithmically on d.\nA major building block in the solution for the realizable setting (Section 2) consists of identifying the best k-sparse linear predictor for the past data at any round in the prediction problem. This is done by solving a sparse regression problem on the observed data. The solution of this problem cannot be obtained by a simple application of say, the Dantzig selector [Candes and Tao, 2007] since we do not observe the data matrix X , but rather a subsample of its entries. Our algorithm is a variant of the Dantzig selector that incorporates random sampling into the optimization, and computes a near-optimal solution by solving a linear program. The resulting algorithm has a regret bound of O\u0303(logT ). This bound has optimal dependence on T , since even in the full information setting where all features are observed there is a lower bound of \u2126(logT ) [Hazan and Kale, 2014].\nThe algorithm for the agnostic setting relies on the theory of submodular optimization. The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity. Weak supermodularity is a relaxation of standard supermodularity that is still strong enough to show performance bounds for the standard greedy feature selection algorithm for solving the sparse regression problem. We then employ a technique developed by Streeter and Golovin [2008] to construct an online learning algorithm that mimics the greedy feature selection algorithm. The resulting algorithm has a regret bound of O\u0303(T 2/3). It is unclear if this bound has the optimal dependence on T : it is easy to prove a lower bound of \u2126( \u221a T ) on the regret using standard arguments for the multiarmed bandit problem."}, {"heading": "1.2 Related work", "text": "A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers.\nWithout any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound. This question was answered in the negative by Foster et al. [2016], who showed that efficiency can only be obtained under additional assumptions on the data. This paper shows that the RIP assumption yields tractability in the online setting just as it does in the batch setting.\nIn the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein). In fact, these papers solve a more general problem where we observe a matrix Z rather than X that is an unbiased estimator of X . While we can use their results in a black-box manner, they are tailored for the setting where the variance of each Zij is constant and it is difficult to obtain the exact dependence on this variance in their bounds. In our setting, this variance can be linear in the dimension of the feature vectors, and hence we wish to control the dependence on the variance in the bounds. Thus, we use an algorithm that is similar to the one in Belloni et al. [2016], and provide an analysis for it (in the appendix). As an added bonus, our algorithm results in solving a linear program rather than a conic or general convex program, hence admits a solution that is more computationally efficient.\nIn the agnostic setting, the computationally efficient algorithm we propose is motivated from (online) supermodular optimization [Natarajan, 1995, Boutsidis et al., 2015, Streeter and Golovin, 2008]. The algorithm is computationally efficient and enjoys sublinear regret under an RIP-like condition, as we will show in Section 3. This result can be contrasted with the known computationally prohibitive algorithms for online sparse linear regression [Zolghadr et al., 2013, Foster et al., 2016], and the hardness result without RIP\n[Foster et al., 2015, 2016]."}, {"heading": "1.3 Notation and Preliminaries", "text": "For d \u2208 N, we denote by [d] the set {1, 2, . . . , d}. For a vector in x \u2208 Rd, denote by x(i) its i-th coordinate. For a subset S \u2286 [d], we use the notation RS to indicate the vector space spanned by the coordinate axes indexed by S (i.e. the set of all vectors w supported on the set S). For a vector x \u2208 Rd, denote by x(S) \u2208 Rd the projection of x on RS . That is, the coordinates of x(S) are\nx(S)(i) = { x(i) if i \u2208 S, 0 if i 6\u2208 S, for i = 1, 2, . . . , d.\nLet \u3008u, v\u3009 = \u2211i u(i) \u00b7 v(i) be the inner product of vectors u and v. For p \u2208 [0,\u221e], the \u2113p-norm of a vector x \u2208 Rd is denoted by \u2016x\u2016p. For p \u2208 (0,\u221e), \u2016x\u2016p = ( \u2211 i |xi|p)1/p, \u2016x\u2016\u221e = maxi |xi|, and \u2016x\u20160 is the number of non-zero coordinates of x. The following definition will play a key role:\nDefinition 1 (Restricted Isometry Property Candes and Tao [2007]). Let \u01eb \u2208 (0, 1) and k \u2265 0. We say that a matrix X \u2208 Rn\u00d7d satisfies restricted isometry property (RIP) with parameters (\u01eb, k) if for any w \u2208 Rd with \u2016w\u20160 \u2264 k we have\n(1\u2212 \u01eb) \u2016w\u20162 \u2264 1\u221a n \u2016Xw\u20162 \u2264 (1 + \u01eb) \u2016w\u20162 .\nOne can show that RIP holds with overwhelming probability if n = \u2126(\u01eb\u22122k log(ed/k)) and each row of the matrix is sampled independently from an isotropic sub-Gaussian distribution. In the realizable setting, the sub-Gaussian assumption can be relaxed to incorporate heavy tail distribution via the \u201csmall ball\u201d analysis introduced in Mendelson [2014], since we only require one-sided lower isometry property."}, {"heading": "1.4 Proper Online Sparse Linear Regression", "text": "We introduce a variant of online sparse regression (OSLR), which we call proper online sparse linear regression (POSLR). The adjective \u201cproper\u201d is to indicate that the algorithm is required to output a weight vector in each round and its prediction is computed by taking an inner product with the feature vector.\nWe assume that there is an underlying sequence (x1, y1), (x2, y2), . . . , (xT , yT ) of labeled examples in R\nd \u00d7 R. In each round t = 1, 2, . . . , T , the algorithm behaves according to the following protocol: 1. Choose a vector wt \u2208 Rd such that \u2016wt\u20160 \u2264 k.\n2. Choose St \u2286 [d] of size at most k0.\n3. Observe xt(St) and yt, and incur loss (yt \u2212 \u3008xt, wt\u3009)2. Essentially, the algorithm makes the prediction y\u0302t := \u3008xt, wt\u3009 in round t. The regret after T rounds of an algorithm with respect to w \u2208 Rd is\nRegretT (w) =\nT\u2211\nt=1\n(yt \u2212 \u3008xt, wt\u3009)2 \u2212 T\u2211\nt=1\n(yt \u2212 \u3008xt, w\u3009)2 .\nThe regret after T rounds of an algorithm with respect to the best k-sparse linear regressor is defined as\nRegretT = max w: \u2016w\u20160\u2264k RegretT (w) .\nNote that any algorithm for POSLR gives rise to an algorithm for OSLR. Namely, if an algorithm for POSLR chooseswt and St, the corresponding algorithm for OSLR queries the coordinates St\u222a{i : wt(i) 6= 0}.\nThe algorithm for OSLR queries at most k0 + k coordinates and has the same regret as the algorithm for POSLR.\nAdditionally, POSLR allows parameters settings which do not have corresponding counterparts in OSLR. Namely, we can consider the sparse \u201cfull information\u201d setting where k0 = d and k \u226a d.\nWe denote by Xt the t\u00d7d matrix of first t unlabeled samples i.e. rows of Xt are xT1 , xT2 , . . . , xTt . Similarly, we denote by Yt \u2208 Rt the vector of first t labels y1, y2, . . . , yt. We use the shorthand notation X , Y for XT and YT respectively.\nIn order to get computationally efficient algorithms, we assume that that for all t \u2265 t0, the matrix Xt satisfies the restricted isometry condition. The parameter t0 and RIP parameters k, \u01eb will be specified later."}, {"heading": "2 Realizable Model", "text": "In this section we design an algorithm for POSLR for the realizable model. In this setting we assume that there is a vector w\u2217 \u2208 Rd such that \u2016w\u2217\u20160 \u2264 k and the sequence of labels y1, y2, . . . , yT is generated according to the linear model yt = \u3008xt, w\u2217\u3009+ \u03b7t , (1) where \u03b71, \u03b72, . . . , \u03b7T are independent random variables fromN(0, \u03c3\n2). We assume that the standard deviation \u03c3, or an upper bound of it, is given to the algorithm as input. We assume that \u2016w\u2217\u20161 \u2264 1 and \u2016xt\u2016\u221e \u2264 1 for all t.\nFor convenience, we use \u03b7 to denote the vector (\u03b71, \u03b72, . . . , \u03b7T ) of noise variables."}, {"heading": "2.1 Algorithm", "text": "The algorithmmaintains an unbiased estimate X\u0302t of the matrixXt. The rows of X\u0302t are vectors x\u0302 T 1 , x\u0302 T 2 , . . . , x\u0302 T t which are unbiased estimates of xT1 , x T 2 , . . . , x T t . To construct the estimates, in each round t, the set St \u2286 [d] is chosen uniformly at random from the collection of all subsets of [d] of size k0. The estimate is\nx\u0302t = d\nk0 \u00b7 xt(St). (2)\nTo compute the predictions of the algorithm, we consider the linear program\nminimize \u2016w\u20161 s.t. \u2225\u2225\u2225\u2225 1\nt X\u0302Tt\n( Yt \u2212 X\u0302tw ) + 1\nt D\u0302tw \u2225\u2225\u2225\u2225 \u221e\n\u2264 C \u221a d log(td/\u03b4)\ntk0\n( \u03c3 + d\nk0\n) .\n(3)\nHere, C > 0 is a universal constant, and \u03b4 \u2208 (0, 1) is the allowed failure probability. D\u0302t, defined in equation (5), is a diagonal matrix that offsets the bias on the diag(X\u0302Tt X\u0302t).\nThe linear program (3) is called the Dantzig selector. We denote its optimal solution by w\u0302t+1. (We define w\u03021 = 0.)\nBased on w\u0302t, we construct w\u0303t \u2208 Rd. Let |w\u0302t(i1)| \u2265 |w\u0302t(i2)| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |w\u0302t(id)| be the coordinates sorted according to the their absolute value, breaking ties according to their index. Let S\u0303t = {i1, i2, . . . , ik} be the top k coordinates. We define w\u0303t as\nw\u0303t = w\u0302t(S\u0303t). (4)\nThe actual prediction wt is either zero if t \u2264 t0 or w\u0303s for some s \u2264 t and it gets updated whenever t is a power of 2.\nThe algorithm queries at most k + k0 features each round, and the linear program can be solved in polynomial time using simplex method or interior point method. The algorithm solves the linear program only \u2308log2 T \u2309 times by using the same vector in the rounds 2s, . . . , 2s+1\u2212 1. This lazy update improves both the computational aspects of the algorithm and the regret bound.\nAlgorithm 1 Dantzig Selector for POSLR\nRequire: T , \u03c3, t0, k, k0 1: for t = 1, 2, . . . , T do 2: if t \u2264 t0 then 3: Predict wt = 0 4: else if t is a power of 2 then 5: Let w\u0302t be the solution of linear program (3) 6: Compute w\u0303t according to (4) 7: Predict wt = w\u0303t 8: else 9: Predict wt = wt\u22121 10: end if 11: Let St \u2286 [d] be a random subset of size k0 12: Observe xt(St) and yt 13: Construct estimate x\u0302t according to (2) 14: Append x\u0302Tt to X\u0302t\u22121 to form X\u0302t \u2208 Rt\u00d7d 15: end for"}, {"heading": "2.2 Main Result", "text": "The main result in this section provides a logarithmic regret bound under the following assumptions 2\n\u2022 The feature vectors have the property that for any t \u2265 t0, the matrix Xt satisfies the RIP condition with (15 , 3k), with t0 = O(k log(d) log(T )).\n\u2022 The underlying POSLR online prediction problem has a sparsity budget of k and observation budget k0.\n\u2022 The model is realizable as defined in equation (1) with i.i.d unbiased Gaussian noise with standard deviation \u03c3 = O(1).\nTheorem 2. For any \u03b4 > 0, with probability at least 1\u2212 \u03b4, Algorithm 1 satisfies\nRegretT = O ( k2 log(d/\u03b4)(d/k0) 3 log(T ) ) .\nThe theorem asserts that an O(logT ) regret bound is efficiently achievable in the realizable setting. Furthermore when k0 = \u2126(d) the regret scales as log(d) meaning that we do not necessarily require T \u2265 d to obtain a meaningful result. We note that the complete expression for arbitrary t0, \u03c3 is given in (13) in the appendix.\nThe algorithm can be easily understood via the error-in-variable equation\nyt = \u3008xt, w\u2217\u3009+ \u03b7t , x\u0302t = xt + \u03bet.\nwith E[\u03bet] = E[x\u0302t \u2212 xt] = 0, where the expectation is taken over random sampling introduced by the algorithm when performing feature exploration. The learner observes yt as well as the \u201cnoisy\u201d feature vector x\u0302t, and aims to recover w\n\u2217. As mentioned above, we (implicitly) need an unbiased estimator of XTt Xt. By taking X\u0302 T t X\u0302t it is easy to verify that the off-diagonal entries are indeed unbiased however this is not the case for the diagonal. To this end we define Dt \u2208 Rd\u00d7d as the diagonal matrix compensating for the sampling bias on the diagonal elements of X\u0302Tt X\u0302t\nDt =\n( d\nk0 \u2212 1\n) \u00b7 diag ( XTt Xt )\n2A more precise statement with the exact dependence on the problem parameters can be found in the appendix.\nand the estimated bias from the observed data is\nD\u0302t =\n( 1\u2212 k0\nd\n) \u00b7 diag ( X\u0302Tt X\u0302t ) . (5)\nTherefore, program (1) can be viewed as Dantzig selector with plug-in unbiased estimates for XTt Yt and XTt Xt using limited observed features."}, {"heading": "2.3 Sketch of Proof", "text": "The main building block in proving Theorem 2 is stated in Lemma 3. It proves that the sequence of solutions w\u0302t converges to the optimal response w\n\u2217 based on which the signal yt is created. More accurately, ignoring all second order terms, it shows that \u2016w\u0302t\u2212w\u2217\u20161 \u2264 O(1/ \u221a t). In Lemma 4 we show that the same applies for the sparse approximation wt of w\u0302t. Now, since \u2016xt\u2016\u221e \u2264 1 we get that the difference between our response \u3008xt, wt\u3009 and the (almost) optimal response \u3008xt, w\u2217\u3009 is bounded by 1/ \u221a t. Given this, a careful calculation of the difference of losses leads to a regret bound w.r.t. w\u2217. Specifically, an elementary analysis of the loss expression leads to the equality\nRegretT (w \u2217) =\nT\u2211\nt=1\n2\u03b7t \u3008xt, w\u2217 \u2212 wt\u3009+ (\u3008xt, w\u2217 \u2212 wt\u3009)2\nA bound on both summands can clearly be expressed in terms of | \u3008xt, w\u2217 \u2212 wt\u3009 | = O(1/ \u221a t). The right summand requires a martingale concentration bound and the left is trivial. For both we obtain a bound of O(log(T )).\nWe are now left with two technicalities. The first is that w\u2217 is not necessarily the empirically optimal response. To this end we provide, in Lemma 16 in the appendix, a constant (independent of T ) bound on the regret of w\u2217 compared to the empirical optimum. The second technicality is the fact that we do not solve for w\u0302t in every round, but in exponential gaps. This translates to an added factor of 2 to the bound \u2016wt \u2212 w\u2217\u20161 that affects only the constants in the O(\u00b7) terms. Lemma 3 (Estimation Rates). Assume that the matrix Xt \u2208 Rt\u00d7d satisfies the RIP condition with (\u01eb, 3k) for some \u01eb < 1/5. Let w\u0302n+1 \u2208 Rd be the optimal solution of program (3). With probability at least 1\u2212 \u03b4,\n\u2016w\u0302t+1 \u2212 w\u2217\u20162 \u2264 C \u00b7 \u221a d\nk0 \u00b7 k log(d/\u03b4) t\n( \u03c3 + d\nk0\n) ,\n\u2016w\u0302t+1 \u2212 w\u2217\u20161 \u2264 C \u00b7 \u221a d\nk0\nk2 log(d/\u03b4)\nt\n( \u03c3 + d\nk0\n) .\nHere C > 0 is some universal constant and \u03c3 is the standard deviation of the noise.\nNote the w\u0302t may not be sparse; it can have many non-zero coordinates that are small in absolute value. However, we take the top k coordinates of w\u0302t in absolute value. Thanks to the Lemma 4 below, we lose only a constant factor \u221a 3.\nLemma 4. Let w\u0302 \u2208 Rd be an arbitrary vector and let w\u2217 \u2208 Rd be a k-sparse vector. Let S\u0303 \u2286 [d] be the top k coordinates of w\u0302 in absolute value. Then,\n\u2225\u2225\u2225w\u0302(S\u0303)\u2212 w\u2217 \u2225\u2225\u2225 2 \u2264 \u221a 3 \u2016w\u0302 \u2212 w\u2217\u20162 ."}, {"heading": "3 Agnostic Setting", "text": "In this section we focus on the agnostic setting, where we don\u2019t impose any distributional assumption on the sequence. In this setting, there is no \u201ctrue\u201d sparse model, but the learner \u2014 with limited access to features \u2014 is competing with the best k-sparse model defined using full information {(xt, yt)}Tt=1.\nAs before, we do assume that xt and yt are bounded. Without loss of generality, \u2016xt\u2016\u221e \u2264 1, and |yt| \u2264 1 for all t. Once again, without any regularity condition on the design matrix, Foster et al. [2016] have shown that achieving a sub-linear regret O(T 1\u2212\u03b4) is in general computationally hard, for any constant \u03b4 > 0 unless NP \u2286 BPP.\nWe give an efficient algorithm that achieves sub-linear regret under the assumption that the design matrix of any (sufficiently long) block of consecutive data points has bounded restricted condition number, which we define below:\nDefinition 5 (Restricted Condition Number). Let k \u2208 N be a sparsity parameter. The restricted condition number for sparsity k of a matrix X \u2208 Rn\u00d7d is defined as\nsup v,w: \u2016v\u2016=\u2016w\u2016=1,\n\u2016v\u20160,\u2016w\u20160\u2264k\n\u2016Xv\u2016 \u2016Xw\u2016 .\nIt is easy to see that if a matrixX satisfies RIP with parameters (\u01eb, k), then its restricted condition number for sparsity k is at most 1+\u01eb1\u2212\u01eb . Thus, having bounded restricted condition number is a weaker requirement than RIP.\nWe now define the Block Bounded Restricted Condition Number Property (BBRCNP):\nDefinition 6 (Block Bounded Restricted Condition Number Property). Let \u03ba > 0 and k \u2208 N. A sequence of feature vectors x1, x2, . . . , xT satisfies BBRCNP with parameters (\u03ba,K) if there is a constant t0 such that for any sequence of consecutive time steps T with |T | \u2265 t0, the restricted condition number for sparsity k of X, the design matrix of the feature vectors xt for t \u2208 T , is at most \u03ba.\nNote that in the random design setting where xt, for t \u2208 [T ], are isotropic sub-Gaussian vectors, t0 = O(log T +k log d) suffices to satisfy BBRCNP with high probability, where the O(\u00b7) notation hides a constant depending on \u03ba.\nWe assume in this section that the sequence of feature vectors satisfies BBRCNP with parameters (\u03ba,K) for some K = O(k log(T )) to be defined in the course of the analysis."}, {"heading": "3.1 Algorithm", "text": "The algorithm in the agnostic setting is of distinct nature from that in the stochastic setting. Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al. [2015] cast the sparse linear regression as maximization of weakly supermodular function. We will introduce an algorithm that blends various ideas from referred literature, to attack the online sparse regression with limited features.\nFirst, let\u2019s introduce the notion of a weakly supermodular function.\nDefinition 7. For parameters k \u2208 N and \u03b1 \u2265 1, a set function g : [d] \u2192 R is (k, \u03b1)-weakly supermodular if for any two sets S \u2286 T \u2286 [d] with |T | \u2264 k, the following two inequalities hold:\n1. (monotonicity) g(T ) \u2264 g(S), and 2. (approximately decreasing marginal gain)\ng(S)\u2212 g(T ) \u2264 \u03b1 \u2211\ni\u2208T\\S\n[g(S)\u2212 g(S \u222a {i})].\nThe definition is slightly stronger than that in Boutsidis et al. [2015]. We will show that sparse linear regression can be viewed as weakly supermodular minimization in Definition 7 once the design matrix has bounded restricted condition number.\nNow we outline the algorithm (see Algorithm 2). We divide the rounds 1, 2, . . . , T into mini-batches of size B each (so there are T/B such batches). The b-th batch thus consists of the examples (xt, yt) for t \u2208 Tb := {(b\u2212 1)B+1, (b\u2212 1)B+1, . . . , bB}. Within the b-th batch, our algorithm queries the same subset of features of size at most k0.\nThe algorithm consists of few key steps. First, one can show that under BBRCNP, as long as B is large enough, the loss within batch b defines a weakly supermodular set function\ngt(S) = 1\nB inf w\u2208RS\n\u2211\nt\u2208Tb\n(yt \u2212 \u3008xt, w\u3009)2.\nTherefore, we can formulate the original online sparse regression problem into online weakly supermodular minimization problem. For the latter problem, we develop an online greedy algorithm along the lines of [Streeter and Golovin, 2008]. We employ k1 = O\u2217(k) budgeted experts algorithms [Amin et al., 2015], denoted BEXP, with budget parameter3 k0k1 . The precise characteristics of BEXP are given in Theorem 8 (adapted from Theorem 2 in [Amin et al., 2015]).\nTheorem 8. For the problem of prediction from expert advice, let there be d experts, and let k \u2208 [d] be a budget parameter. In each prediction round t, the BEXP algorithm chooses an expert jt and a set of experts Ut containing jt of size at most k, obtains as feedback the losses of all the experts in Ut, suffers the loss of\nexpert jt, and guarantees an expected regret bound of 2 \u221a d log(d) k T over T prediction rounds.\nAt the beginning of each mini-batch b, the BEXP algorithms are run. Each BEXP algorithm outputs a set of coordinates of size k0k1 as well as a special coordinate in that set. The union of all of these sets is then used as the set of features to query throughout the subsequent mini-batch. Within the mini-batch, the algorithm runs the standard Vovk-Azoury-Warmuth algorithm for linear prediction with square loss restricted to set of special coordinates output by all the BEXP algorithms.\nAt the end of the mini-batch, every BEXP algorithm is provided carefully constructed losses for each coordinate that was output as feedback. These losses ensure that the set of special coordinates chosen by the BEXP algorithms mimic the greedy algorithm for weakly supermodular minimization."}, {"heading": "3.2 Main Result", "text": "In this section, we will show that Algorithm 2 achieves sublinear regret under BBRCNP.\nTheorem 9. Suppose the sequence of feature vectors satisfies BBRCNP with parameters (\u03ba, k1 + k) for k1 = 1 3\u03ba\n2k log(T ), and assume that T is large enough so that t0 \u2264 ( k0T\u03ba2dk )1/3. Then if Algorithm 2 is run with parameters B = ( k0T\u03ba2dk ) 1/3 and k1 as specified above, its expected regret is at most O\u0303(( \u03ba8dk4 k0 )1/3T 2/3).\nProof. The proof relies on a number of lemmas whose proofs can be found in the appendix. We begin with the connection between sparse linear regression, weakly supermodular function and RIP, formally stated in Lemma 10. This lemma is a direct consequence of Lemma 5 in [Boutsidis et al., 2015].\nLemma 10. Consider a sequence of examples (xt, yt) \u2208 Rd \u00d7 R for t = 1, 2, . . . , B, and let X be the design matrix for the sequence. Consider the set function associated with least squares optimization:\ng(S) = inf w\u2208RS\n1\nB\nB\u2211\nt=1\n(yt \u2212 \u3008xt, w\u3009)2.\nSuppose the restricted condition number of X for sparsity k is bounded by \u03ba. Then g(S) is (k, \u03ba2)-weakly supermodular.\nEven though minimization of weakly supermodular functions is NP-hard, the greedy algorithm provides a good approximation, as shown in the next lemma.\n3We assume, for convenience, that k0 is divisible by k1.\nAlgorithm 2 Online Greedy Algorithm for POSLR\nRequire: Mini-batch size B, sparsity parameters k0 and k1 1: Set up k1 budgeted prediction algorithms BEXP\n(i) for i \u2208 [k1], each using the coordinates in [d] as \u201cexperts\u201d with a per-round budget of k0k1 .\n2: for b = 1, 2, . . . , T/B do 3: For each i \u2208 [k1], obtain a coordinate j(i)b and subset of coordinates U (i) b from BEXP (i) such that\nj (i) b \u2208 U (i) b .\n4: Define V (0) b = \u2205 and for each i \u2208 [k1] define V (i) b = {j (i\u2032) b | i\u2032 \u2264 i}. 5: Set up the Vovk-Azoury-Warmuth (VAW) algorithm for predicting using the features in V (k1) b . 6: for t \u2208 Tb do 7: Set St = \u22c3 i\u2208[k1] U (i) b , obtain xt(St), and pass xt(V (k1) b ) to VAW. 8: Set wt to be the weight vector output by VAW. 9: Obtain the true label yt and pass it to VAW.\n10: end for 11: Define the function\ngb(S) = 1\nB inf w\u2208RS\n\u2211\nt\u2208Tb\n(yt \u2212 \u3008xt, w\u3009)2. (6)\n12: For each j \u2208 U (i)b , compute gb(V (i\u22121) b \u222a {j}) and pass it BEXP(i) as the loss for expert j. 13: end for\nLemma 11. Consider a (k, \u03b1)-weakly supermodular set function g(\u00b7). Let j\u2217 := argminj g({j}). Then, for any subset V of size at most k, we have\ng({j\u2217})\u2212 g(V ) \u2264 ( 1\u2212 1\u03b1|V | ) [g(\u2205)\u2212 g(V )].\nThe BEXP algorithms essentially implement the greedy algorithm in an online fashion. Using the properties of the BEXP algorithm, we have the following regret guarantee:\nLemma 12. Suppose the sequence of feature vectors satisfies BBRCNP with parameters (\u01eb, k1 + k). Then for any set V of coordinates of size at most k, we have\nE\n  T/B\u2211\nb=1\ngb(V (k1) b )\u2212 gb(V )\n \n\u2264 T/B\u2211\nb=1\n( 1\u2212 1\u03ba2|V | )k1 [gb(\u2205)\u2212 gb(V )] + 2\u03ba2k \u221a dk1 log(d)T k0B .\nFinally, within every mini-batch, the VAW algorithm guarantees the following regret bound, an immediate consequence of Theorem 11.8 in Cesa-Bianchi and Lugosi [2006]:\nLemma 13. Within every batch b, the VAW algorithm generates weight vectors wt for t \u2208 Tb such that \u2211\nt\u2208Tb\n(yt \u2212 \u3008xt, wt\u3009)2 \u2212Bgb(V (k1)b ) \u2264 O(k1 log(B)).\nWe can now prove Theorem 9. Combining the bounds of lemma 12 and 13, we conclude that for any\nsubset of coordinates V of size at most k, we have\nE\n[ T\u2211\nt=1\n(yt \u2212 \u3008xt, wt\u3009)2 ]\n(7)\n\u2264 T/B\u2211\nb=1\nBgb(V ) +B(1\u2212 1\u03ba2|V | )k1 [gb(\u2205)\u2212 gb(V )] (8)\n+O ( \u03ba2k \u221a dk1 log(d)BT\nk0 +\nT B k1 log(B)\n) . (9)\nFinally, note that T/B\u2211\nb=1\nBgb(V ) \u2264 inf w\u2208RV\nT\u2211\nt=1\n(yt \u2212 \u3008xt, w\u3009)2,\nand T/B\u2211\nb=1\nB(1\u2212 1\u03ba2|V |)k1 [gb(\u2205)\u2212 gb(V )] \u2264 T \u00b7 exp(\u2212 k1\u03ba2k ),\nbecause gb(\u2205) \u2264 1. Using these bounds in (9), and plugging in the specified values of B and k1, we get the stated regret bound."}, {"heading": "4 Conclusions and Future Work", "text": "In this paper, we gave computationally efficient algorithms for the online sparse linear regression problem under the assumption that the design matrices of the feature vectors satisfy RIP-type properties. Since the problem is hard without any assumptions, our work is the first one to show that assumptions that are similar to the ones used to sparse recovery in the batch setting yield tractability in the online setting as well.\nSeveral open questions remain in this line of work and will be the basis for future work. Is it possible to improve the regret bound in the agnostic setting? Can we give matching lower bounds on the regret in various settings? Is it possible to relax the RIP assumption on the design matrices and still have efficient algorithms? Some obvious weakenings of the RIP assumption we have made don\u2019t yield tractability. For example, simply assuming that the final matrix XT satisfies RIP rather than every intermediate matrix Xt for large enough t is not sufficient; a simple tweak to the lower bound construction of Foster et al. [2016] shows this. This tweak consists of simply padding the construction with enough dummy examples which are well-conditioned enough to overcome the ill-conditioning of the original construction so that RIP is satisfied by XT . We note however that in the realizable setting, our analysis can be easily adapted to work under weaker conditions such as irrepresentability [Zhao and Yu, 2006, Javanmard and Montanari, 2013]."}, {"heading": "A Proofs for Realizable Setting", "text": "Proof of Lemma 3. Let \u2206 := w\u0302 \u2212 w\u2217 be the difference between the true answer and solution to the optimization problem. Let S to be the support of w\u2217 and let Sc = [d] \\ S be the complements of S. Consider the permutation i1, . . . , id\u2212k of S\nc for which |\u2206(ij)| \u2265 |\u2206(ij+1)| for all j. That is, the permutation dictated by the magnitude of the entries of \u2206 outside of S. We split Sc into subsets of size k according to this permutation: Define Sj , for j \u2265 1 as {i(j\u22121)k+1, . . . , ijk}. For convenience we also denote by S01 the set S \u222a S1.\nNow, consider the matrix XS01 \u2208 Rt\u00d7|S01| whose columns are those of X with indices S01. The Restricted Isometry Property of X dictates that for any vector c \u2208 RS01 ,\n(1\u2212 \u01eb) \u2016c\u20162 \u2264 1\u221a n \u2016XS01c\u20162 \u2264 (1 + \u01eb) \u2016c\u20162 .\nLet V \u2286 Rt be the subspace of dimension |S01| that is the image of the linear operator XS01 , and let PV \u2208 Rt\u00d7t be the projection matrix onto that subspace. We have, for any vector z \u2208 Rt that\n(1\u2212 \u01eb) \u2016PV z\u2016 \u2264 1\u221a n \u2225\u2225XTS01z \u2225\u2225 \u2264 (1 + \u01eb) \u2016PV z\u2016\nWe apply this to z = X\u2206 and conclude that\n\u2016PV X\u2206\u2016 \u2264 1\u221a t(1\u2212 \u01eb) \u2225\u2225XTS01X\u2206 \u2225\u2225 (10)\nWe continue to lower bound the quantity of \u2016PV X\u2206\u2016. We decompose PV X\u2206 as\nPV X\u2206 = PV X\u2206(S01) + \u2211\nj\u22652\nPV X\u2206(Sj) (11)\nNow, according to the definition of V we that there exist vectors {cj}j\u22652 in R|S01| for which\nPV X\u2206(Sj) = XS01cj\nWe now invoke Lemma 1.1 from Candes and Tao [2005] stating that for any S\u2032, S\u2032\u2032 with |S\u2032|+ |S\u2032\u2032| \u2264 3k it holds that\n\u2200c, c\u2032 1 n \u3008XS\u2032c,XS\u2032\u2032c\u2032\u3009 \u2264 (2\u01eb\u2212 \u01eb2) \u2016c\u20162 \u2016c\u2032\u20162\nWe apply this for S01, Sj, j \u2265 2 and conclude that\n\u2016PV X\u2206(Sj)\u201622 = \u3008PV X\u2206(Sj), X\u2206(Sj)\u3009 \u2264 2\u01ebt \u2016cj\u20162 \u00b7 \u2016\u2206(Sj)\u2016 \u2264 2\u01eb \u221a t\n1\u2212 \u01eb \u2016PV X\u2206(Sj)\u20162 \u00b7 \u2016\u2206(Sj)\u20162 .\nDividing through by \u2016PV X\u2206(Sj)\u20162, we get\n\u2016PV X\u2206(Sj)\u2016 \u2264 2\u01eb \u221a t\n1\u2212 \u01eb \u2016\u2206(Sj)\u2016 . (12)\nLet us now bound the sum \u2016\u2206(Sj)\u2016. By the definition of Sj we know that any element i \u2208 Sj has the property \u2206(i) \u2264 (1/k) \u2016\u2206(Sj\u22121)\u20161. Hence\n\u2211\nj\u22652\n\u2016\u2206(Sj)\u2016 \u2264 (1/ \u221a k) \u2211\nj\u22651\n\u2016\u2206(Sj)\u20161 = (1/ \u221a k) \u2016\u2206(Sc)\u20161\nWe now combine this inequality with Equations (10), (11) and (12)\n1\nt\n\u2225\u2225XTS01X\u2206 \u2225\u2225 \u2265 1\u2212 \u01eb\u221a\nt \u2016PV X\u2206\u2016\n\u2265 1\u2212 \u01eb\u221a t \u2016PV X\u2206(S01)\u2016 \u2212 1\u2212 \u01eb\u221a n\n\u2211\nj\u22652\n\u2016PV X\u2206(Sj)\u2016\n\u2265 1\u2212 \u01eb\u221a t \u2016X\u2206(S01)\u2016 \u2212 2\u01eb \u2211\nj\u22652\n\u2016\u2206(Sj)\u2016\n\u2265 1\u2212 \u01eb\u221a t \u2016X\u2206(S01)\u2016 \u2212 2\u01eb\u221a k \u2016\u2206(Sc)\u20161\nThe third inequality holds since X\u2206(S01) \u2208 V hence PV X\u2206(S01) = X\u2206(S01). We continue to bound the expression by claiming that \u2016\u2206(S)\u20161 \u2265 \u2016\u2206(Sc)\u20161. This holds since in Sc, w\u0302Sc = \u2206(Sc) hence\n\u2016w\u2217\u20161 = \u2016w\u0302 \u2212\u2206(Sc)\u2212\u2206(S)\u20161 \u2264 \u2016w\u0302\u20161 + (\u2016\u2206(S)\u20161 \u2212 \u2016\u2206(Sc)\u20161) Now, the optimality of w\u0302 implies \u2016w\u0302\u20161 \u2264 \u2016w\u2217\u20161, hence indeed \u2016\u2206(S)\u20161 \u2265 \u2016\u2206(Sc)\u20161.\n\u2016\u2206(Sc)\u20161 \u2264 \u2016\u2206(S)\u20161 \u2264 \u221a k \u2016\u2206(S)\u20162 \u2264 \u2016\u2206(S01)\u20162 \u2264\n\u221a k\n(1\u2212 \u01eb) \u221a t \u2016X\u2206(S01)\u2016\nWe continue the chain of inequalities\n1\nt\n\u2225\u2225XTS01X\u2206 \u2225\u2225 \u2265 1\u2212 \u01eb\u221a\nn \u2016X\u2206(S01)\u2016 \u2212 2\u01eb\u221a k \u2016\u2206(Sc)\u20161\n\u2265 \u2016X\u2206(S01)\u2016 ( 1\u2212 \u01eb\u221a\nn \u2212 2\u01eb\u221a k \u00b7\n\u221a k\n(1 \u2212 \u01eb)\u221an\n)\n= (1\u2212 \u01eb)2 \u2212 2\u01eb (1\u2212 \u01eb) \u221a t \u2016X\u2206(S01)\u2016\nRearranging we conclude that\n\u2016\u2206(S01)\u2016 \u2264 1\n(1\u2212 \u01eb) \u221a t \u2016X\u2206(S01)\u2016 (RIP of X)\n\u2264 1 ((1 \u2212 \u01eb)2 \u2212 2\u01eb)t \u2225\u2225XTS01X\u2206 \u2225\u2225 \u2264 \u221a 2k\n(1\u2212 4\u01eb)t \u2225\u2225XTX\u2206 \u2225\u2225 \u221e\n(since for any z \u2208 R2k, \u2016z\u20162 \u2264 \u221a 2k \u2016z\u2016\u221e)\n\u2264 C \u221a dk log(d/\u03b4)\ntk0\n( \u03c3 + d\nk0 \u2016w\u2217\u20161\n) (Lemma 14 and \u01eb < 1/5)\nfor some constant C. We continue our bound on \u2016\u2206\u2016 by showing that \u2016\u2206(Sc01)\u2016 \u2264 \u2016\u2206(S01)\u2016\n\u2016\u2206(Sc01)\u201622 (i) \u2264 \u2016\u2206(Sc)\u201621 \u00b7 \u2211\nj\u2265k+1\n1 j2 \u2264 1 k \u2016\u2206(Sc)\u201621 \u2264 1 k \u2016\u2206(S)\u201621 \u2264 \u2016\u2206(S)\u2016 2 2 .\nInequality (i) holds due to the following: Let \u03b1i be the absolute value of the i\u2019th largest (in absolute value) element of \u2206(Sc). It obviously holds that \u03b1i \u2264 \u2016\u2206(Sc)\u20161 /i. Now, according to the definition of S01 we have that \u2016\u2206(Sc01)\u201622 = \u2211 j\u2265k+1 \u03b1 2 i and the inequality follows. Hence,\n\u2016\u2206(Sc01)\u20162 \u2264 \u2016\u2206(S)\u20162 \u2264 \u2016\u2206(S01)\u20162 .\nWe conclude that\n\u2016\u2206\u20162 \u2264 \u221a 2 \u2016\u2206(S01)\u20162 \u2264 C\n\u221a dk log(d/\u03b4)\ntk0\n( \u03c3 + d\nk0 \u2016w\u2217\u20161\n)\nfor some universal constant C > 0. Since \u2016\u2206(S)\u20161 \u2265 \u2016\u2206(Sc)\u20161 and |S| \u2264 k we get that\n\u2016\u2206\u20161 \u2264 2 \u2016\u2206(S)\u20161 \u2264 2 \u221a k \u2016\u2206(S)\u20162 \u2264 2 \u221a k \u2016\u2206\u20162\nand the claim follows.\nProof of Lemma 4. Let S be the support of w\u2217. We can decompose the square of the left hand side as\n\u2225\u2225\u2225w\u0302(S\u0303)\u2212 w\u2217 \u2225\u2225\u2225 2\n2 =\n\u2211\ni\u2208S\u2229S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + \u2211\ni\u2208S\u0303\\S\n(w\u0302(i))2 + \u2211\ni\u2208S\\S\u0303\n(w\u2217(i))2.\nWe upper bound the last sum on the right hand side as\n\u2211\ni\u2208S\\S\u0303\n(w\u2217(i))2 = \u2211\ni\u2208S\\S\u0303\n[(w\u0302(i)\u2212 w\u2217(i)) + (w\u0302(i))]2\n\u2264 2 \u2211\ni\u2208S\\S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + (w\u0302(i))2\n\u2264 2 \u2211\ni\u2208S\\S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + 2 \u2211\ni\u2208S\u0303\\S\n(w\u0302(i))2 ,\nwhere first inequality follows from the elementary inequality (a+ b)2 \u2264 2a2 + 2b2 and the second inequality is due to the fact that S\u0303 contains top k entries of w\u0302 in absolute value and |S \\ S\u0303| = |S\u0303 \\ S|. Hence,\n\u2225\u2225\u2225w\u0302(S\u0303)\u2212 w\u2217 \u2225\u2225\u2225 2\n2 =\n\u2211\ni\u2208S\u2229S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + \u2211\ni\u2208S\u0303\\S\n(w\u0302(i))2 + \u2211\ni\u2208S\\S\u0303\n(w\u2217(i))2\n\u2264 \u2211\ni\u2208S\u2229S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + 2 \u2211\ni\u2208S\\S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + 3 \u2211\ni\u2208S\u0303\\S\n(w\u0302(i))2\n\u2264 2 \u2211\ni\u2208S\u2229S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + 2 \u2211\ni\u2208S\\S\u0303\n(w\u0302(i)\u2212 w\u2217(i))2 + 3 \u2211\ni\u2208S\u0303\\S\n(w\u0302(i))2\n= 2 \u2211\ni\u2208S\n(w\u0302(i)\u2212 w\u2217(i))2 + 3 \u2211\ni\u2208S\u0303\\S\n(w\u0302(i))2\n\u2264 3 d\u2211\ni=1\n(w\u0302(i)\u2212 w\u2217(i))2\n= 3 \u2016w\u0302 \u2212 w\u2217\u201622 .\nTaking square root finishes the proof.\nLemma 14. There exists a universal constant C > 0 such that, with probability at least 1 \u2212 \u03b4, the convex program (3) is feasible and its optimal solution w\u0302 satisfies\n\u2225\u2225\u2225\u2225 1\nt XTt Xt(w\u0302 \u2212 w\u2217) \u2225\u2225\u2225\u2225 \u221e \u2264 C \u221a d log(d/\u03b4) tk0 ( \u03c3 + d k0 \u2016w\u2217\u20161 ) .\nWe note that the above lemma is beyond simple triangle inequality on the feasibility constraints, as the left hand side depends on actual design matrix Xt which we do not observe, instead of X\u0302t.\nProof. To simplify notation, we drop subscript t. Namely, let X = Xt, X\u0302 = Xt and D\u0302 = D\u0302t, and also let \u03b7 = (\u03b71, \u03b72, . . . , \u03b7t) be the vector of noise variables.\nFirst, we show that w\u2217 satisfies the constraint of (3) with probability at least 1\u2212 \u03b4. We upper bound \u2225\u2225\u2225\u2225 1 t X\u0302T (Y \u2212 X\u0302w\u2217) + 1 t D\u0302w\u2217 \u2225\u2225\u2225\u2225 \u221e = \u2225\u2225\u2225\u2225 [ 1 t X\u0302T (X \u2212 X\u0302) + 1 t D\u0302 ] w\u2217 + 1 t X\u0302T\u03b7 \u2225\u2225\u2225\u2225 \u221e\n\u2264 \u2225\u2225\u2225\u2225 [ 1 t X\u0302T (X \u2212 X\u0302) + 1 t D\u0302 ] w\u2217 \u2225\u2225\u2225\u2225 \u221e + 1 t \u2225\u2225\u2225X\u0302T \u03b7 \u2225\u2225\u2225 \u221e\nWe first bound the left summand. By Lemma 15, we have\n\u2225\u2225\u2225\u2225 [ 1 t X\u0302T (X \u2212 X\u0302) + 1 t D\u0302 ] w\u2217 \u2225\u2225\u2225\u2225 \u221e \u2264 \u2016w\u2217\u20161 \u00b7 \u2225\u2225\u2225\u2225 1 t X\u0302T (X \u2212 X\u0302) + 1 t D\u0302 \u2225\u2225\u2225\u2225 \u221e\n\u2264 \u2016w\u2217\u20161 (\u2225\u2225\u2225\u2225 1\nt XT (X\u0302 \u2212X) \u2225\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225\u2225 1 t (X\u0302 \u2212X)T (X\u0302 \u2212X)\u2212 1 t D\u0302 \u2225\u2225\u2225\u2225 \u221e )\n\u2264 \u2016w\u2217\u20161 C \u00b7 \u221a d3 log(d/\u03b4)\ntk0 3 .\nFor the right summand, since \u03b7 is vector of i.i.d Gaussians with variance \u03c32, with probability at least 1\u2212 \u03b4,\n1\nt \u2225\u2225\u2225X\u0302T\u03b7 \u2225\u2225\u2225 \u221e \u2264 C\u03c3 t \u221a log(d/\u03b4) \u00b7max i\u2208[d] \u2225\u2225\u2225X\u0302(i) \u2225\u2225\u2225 2\nwhere X\u0302(1), X\u0302(2), . . . , X\u0302(d) are the columns of X\u0302. Since the absolute value of the entries of X\u0302 is at most d/k0,\nwe have \u2225\u2225\u2225X\u0302(i) \u2225\u2225\u2225 2 \u2264 \u221a td/k0 and thus\n1\nt \u2225\u2225\u2225X\u0302T\u03b7 \u2225\u2225\u2225 \u221e\n\u2264 C\u03c3 \u221a d log(d/\u03b4)\ntk0 .\nCombining the inequalities so far provides\n\u2225\u2225\u2225\u2225 1 t X\u0302T (Y \u2212 X\u0302w\u2217) + 1 t D\u0302w\u2217 \u2225\u2225\u2225\u2225 \u221e \u2264 C \u221a d log(d/\u03b4) tk0 ( \u03c3 + d k0 \u2016w\u2217\u20161 )\nand hence conclude the constraint of the optimization problem (3) is satisfied (at least) by w\u2217 and thus the optimization problem is feasible.\nNow consider the vector \u2206 := w\u0302 \u2212 w\u2217, we have \u2225\u2225\u2225\u2225 1\nt XTX\u2206 \u2225\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225\u2225 1 t (X\u0302T X\u0302 \u2212 D\u0302)\u2206 \u2225\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225\u2225 1 t (X\u0302T X\u0302 \u2212 D\u0302 \u2212XTX)\u2206 \u2225\u2225\u2225\u2225 \u221e\n\u2264 \u2225\u2225\u2225\u2225 1\nt (X\u0302T X\u0302 \u2212 D\u0302)\u2206 \u2225\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225\u2225 1 t (X\u0302 \u2212X)TX\u2206 \u2225\u2225\u2225\u2225 \u221e\n+ \u2225\u2225\u2225\u2225 1\nt XT (X\u0302 \u2212X)\u2206 \u2225\u2225\u2225\u2225 \u221e + \u2225\u2225\u2225\u2225 ( 1 t (X\u0302 \u2212X)T (X\u0302 \u2212X)\u2212 1 t D\u0302 ) \u2206 \u2225\u2225\u2225\u2225 \u221e .\nAccording to Lemma 15 we have\n\u2225\u2225\u2225\u2225 1\nt XT (X\u0302 \u2212X)\u2206 \u2225\u2225\u2225\u2225 \u221e \u2264 \u2225\u2225\u2225\u2225 1 t XT (X\u0302 \u2212X) \u2225\u2225\u2225\u2225 \u221e \u2016\u2206\u20161 \u2264 C \u221a d log(d/\u03b4) tk0 (\u2016w\u2217\u20161 + \u2016w\u0302\u20161) \u2264 2C \u221a d log(d/\u03b4) tk0 \u00b7 \u2016w\u2217\u20161\nwhere the last inequality is by the optimality of w\u0302. The same argument provides an identical bound for\u2225\u2225\u2225 1t (X\u0302 \u2212X)TX\u2206 \u2225\u2225\u2225 \u221e . The last summand can also be bounded by using Lemma 15 and the optimality of w\u0302.\n\u2225\u2225\u2225\u2225 ( 1 t (X\u0302 \u2212X)T (X\u0302 \u2212X)\u2212 1 t D\u0302 ) \u2206 \u2225\u2225\u2225\u2225 \u221e \u2264 2C \u221a d3 log(d/\u03b4) tk0 3 \u00b7 \u2016w\u2217\u20161\nFinally, according to the feasibility of w\u0302 and w\u2217 we may bound the first summand\n\u2225\u2225\u2225\u2225 ( 1 t X\u0302T X\u0302 \u2212 1 t D\u0302 ) \u2206 \u2225\u2225\u2225\u2225 \u221e \u2264 2C \u221a d log(d/\u03b4) tk0 ( \u03c3 + d k0 \u2016w\u2217\u20161 ) ,\nand reach the final bound.\nLemma 15. For any t \u2265 t0, with probability at least 1\u2212 \u03b4, the following two inequalities hold \u2225\u2225\u2225\u2225 1\nt (X\u0302t \u2212Xt)T (X\u0302t \u2212Xt)\u2212\n1 t D\u0302t \u2225\u2225\u2225\u2225 \u221e \u2264 C \u221a d3 log(d/\u03b4) tk0 3 ,\n\u2225\u2225\u2225\u2225 1\nt XTt (X\u0302t \u2212Xt) \u2225\u2225\u2225\u2225 \u221e \u2264 C \u221a d log(d/\u03b4) tk0 ,\nwhere \u2016\u00b7\u2016\u221e denotes the maximum of the absolute values of the entries of a matrix.\nProof. Throughout we use that |xs(i)| \u2264 1 for all i \u2208 [d] and all s \u2208 [t], and (2) (x\u0302s(i) \u2212 xs(i))2 \u2212 1tDii is unbiased with absolute value of at most (d/k0) 2 and variance of at most (d/k0) 3. For the first term, let\u2019s bound [ 1\nt (X\u0302 \u2212X)T (X\u0302 \u2212X)\u2212 1 t D\u0302\n]\nij\n= 1\nt\nt\u2211\ns=1\n(x\u0302s(i)\u2212 xs(i))(x\u0302s(j)\u2212 xs(j))\u2212 1\nt D\u0302ij\nFor i = j, we have\nE [( (x\u0302s(i)\u2212 xs(i))2 \u2212 1\nt Dii\n)2] \u2264 E [ (x\u0302s(i)\u2212 xs(i))4 ] \u2264 (d/k0)3\n(x\u0302s(i)\u2212 xs(i))2 \u2212 1\nt Dii \u2264 (d/k0)2, E\n[ (x\u0302s(i)\u2212 xs(i))2 \u2212 1\nt Dii\n] = 0\nHence, by Bernstein\u2019s inequality, for any v > 0,\nPr [\u2223\u2223\u2223\u2223\u2223 1 t t\u2211\ns=1\n(x\u0302s(i)\u2212 xs(i))2 \u2212 1\nt Dii \u2223\u2223\u2223\u2223\u2223 > v ] \u2264 2 exp ( \u2212 v 2t (d/k0)3 + (d/k0)2v/3 ) .\nIt follows that for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 it holds for all i \u2208 [d] that, \u2223\u2223\u2223\u2223\u2223 1 t t\u2211\ns=1\n(x\u0302s(i)\u2212 xs(i))2 \u2212 1\nt Dii \u2223\u2223\u2223\u2223\u2223 \u2264 O ( log(d/\u03b4)d2 tk0 2 + \u221a log(d/\u03b4)d3 tk0 3 ) .\nSimilarly we have 1t (D\u0302ii \u2212Dii) \u2264 O ( log(d/\u03b4)d2 tk02 + \u221a log(d/\u03b4)d3 tk03 ) .\nFor i 6= j we use an analogous argument, only now the variance term in Bernstein\u2019s inequality is (d/k0)2 rather than (d/k0) 3, hence only reach a tighter bound.\nFor the second term, we again bound via Bernstein\u2019s inequality as\n[ 1\nt XT (X\u0302 \u2212X)\n]\nij\n= 1\nt\nt\u2211\ns=1\nxs(i)(x\u0302s(j)\u2212 xs(j)) \u2264 O\n  \u221a d log(d/\u03b4)\ntk0 +\nd log(d/\u03b4)\ntk0\n \nThe claim now follows by noticing that for large enough t, the dominating terms are those that scale as 1/ \u221a t.\nProof of Theorem 2. By Lemma 3,\n\u2016wt+1 \u2212 w\u2217\u20162 \u2264 O\n  \u221a d\nk0\nk log(d/\u03b4)\nt (\u03c3 +\nd\nk0 \u2016w\u2217\u20161)\n  .\nWe have\nRegretT (w \u2217)\u2212 Regrett0(w\u2217) =\nT\u2211\nt=t0+1\n(yt \u2212 \u3008xt, wt\u3009)2 \u2212 (yt \u2212 \u3008xt, w\u2217\u3009)2\n= T\u2211\nt=t0+1\n(\u3008xt, w\u2217 \u2212 wt\u3009+ \u03b7t)2 \u2212 \u03b72t\n=\nT\u2211\nt=t0+1\n(\u3008xt, w\u2217 \u2212 wt\u3009+ 2\u03b7t) \u3008xt, w\u2217 \u2212 wt\u3009\n=\nT\u2211\nt=t0+1\n2\u03b7t \u3008xt, w\u2217 \u2212 wt\u3009+ (\u3008xt, w\u2217 \u2212 wt\u3009)2 ,\nwhere we used that yt = \u3008xt, wt\u3009 + \u03b7t. To bound the regret we require the upper bound, that occurs with probability of at least 1\u2212 \u03b4,\n\u2200t \u2265 t0 |\u3008xt, w\u2217 \u2212 wt\u3009| (i) \u2264 \u2016xt\u2016\u221e \u221a \u2016wt \u2212 w\u2217\u20160 \u00b7 \u2016wt \u2212 w\u2217\u20162 (ii) \u2264 O  k \u00b7 \u221a d\nk0\nlog(log(T )d/\u03b4)\nt\n( \u03c3 + d\nk0\n)  .\nInequality (i) holds since \u3008a, b\u3009 \u2264 \u2016a(S)\u20162 \u00b7 \u2016b\u20162 with S being the support of b and \u2016a(S)\u20162 \u2264 \u2016a\u2016\u221e \u221a |S|. Inequality (ii) follows from Lemma 3 and Lemma 4, and a union bound over the \u2308log(T )\u2309 many times the vector wt is updated. Now, for the left summand of the regret bound we have by Martingale concentration inequality that w.p. 1\u2212 \u03b4\nT\u2211\nt=t0+1\n2\u03b7t \u3008xt, wt \u2212 w\u2217\u3009 \u2264 O  \u03c3 \u221a\u221a\u221a\u221alog(1/\u03b4) T\u2211\nt=t0+1\n\u3008xt, wt \u2212 w\u2217\u30092  \n= O  \u03c3 \u221a\nlog(1/\u03b4) log(T )k2 \u00b7 d log(d log(T )/\u03b4) k0\n( \u03c3 + d\nk0\n)2   .\nThe right summand is bounded as\nT\u2211\nt=t0+1\n\u3008xt, w\u2217 \u2212 wt\u30092 = O ( k2 \u00b7 d log(d log(T )/\u03b4)\nk0\n( \u03c3 + d\nk0\n)2 \u00b7 log(T ) ) .\nClearly, the right summand dominates the left one.\nIt remains to bound the regret in first t0 rounds. Since wt = 0 for t \u2264 t0, we have\nRegrett0(w \u2217) =\nt0\u2211\nt=1\n2\u03b7t \u3008xt, w\u2217\u3009+ (\u3008xt, w\u2217\u3009)2 \u2264 O ( \u03c3 \u221a t0 log(1/\u03b4) + t0 ) .\nHere, we used that | \u3008xt, w\u2217\u3009 | \u2264 1 since \u2016xt\u2016\u221e \u2264 1 and \u2016w\u2217\u20161 \u2264 1. We also used that \u03b7t \u3008xt, w\u2217\u3009 \u223c N(0, \u03c32 \u3008xt, w\u2217\u30092) and \u03b71 \u3008x1, w\u2217\u3009 , \u03b72 \u3008x2, w\u2217\u3009 , . . . , \u03b7t0 \u3008xt0 , w\u2217\u3009 are independent. Thus their sum is a Gaussian with variance at most \u03c32t0.\nCollecting all the terms along with Lemma 16, bounding the difference RegretT \u2212RegretT (w\u2217), gives\nRegretT \u2264 ( t0 + \u221a t0 log(1/\u03b4) + k\n2 \u00b7 d log(d log(T )/\u03b4) k0\n( \u03c3 + d\nk0\n)2 \u00b7 log(T ) ) (13)\nLemma 16. In the realizable case, w.p. at least 1\u2212\u03b4 we have for any sequence of wt that RegretT \u2212RegretT (w\u2217) = O(\u03c32k log(d/\u03b4)).\nProof. It is an easy exercise to show that RegretT \u2212RegretT (w\u2217) is equal to the regret on an algorithm that always plays w\u2217. We thus continue to bound the regret of w\u2217.\nLet \u2206 \u2208 Rd be the difference between w\u2217 and w\u0303, the empirical optimal solution for the sparse regression problem. The loss associated with w\u2217 is clearly \u2016\u03b7\u20162, where \u03b7 is the noise term y = Xw\u2217 + \u03b7. The loss associated with w\u0303 is\n\u2016X(w\u2217 +\u2206)\u2212Xw\u2217 \u2212 \u03b7\u20162 = \u2016\u03b7 \u2212X\u2206\u20162 = \u2016\u03b7 \u2212XS\u0303\u2206\u20162\nwhere S\u0303 is the support of \u2206, having a cardinality of at most 2k. The closed form solution for the least-squares problem dictates that\n\u2016\u03b7 \u2212XS\u0303\u2206\u20162 \u2265 \u2016\u03b7 \u2212XS\u0303X \u2020 S\u0303 \u03b7\u20162 = \u2016\u03b7\u20162 \u2212 \u2016XS\u0303X \u2020 S\u0303 \u03b7\u20162 .\nHere, A\u2020 is the pseudo inverse of a matrix A and XS is the matrix obtained from the columns of X whose indices are in S. It follows that the regret of w\u2217 is bounded by\n\u2016XS\u0303X \u2020 S\u0303 \u03b7\u20162\nfor some subset S\u0303 of size at most 2k. To bound this quantity we use a high probability bound for \u2016XSX\u2020S\u03b7\u20162 for a fixed set S, and take a union bound over all possible sets of cardinality 2k. For a fixed set S we have that \u2016XSX\u2020S\u03b7\u20162/\u03c32 is distributed according to the \u03c722k distribution. The tail bounds of this distribution suggest that\nPr [ \u2016XSX\u2020S\u03b7\u20162 > 2k\u03c32 + 2\u03c32 \u221a 2kx+ 2\u03c32x ] \u2264 exp(\u2212x)\nmeaning that with probability at least 1\u2212 \u03b4/d2k we have\n\u2016XSX\u2020S\u03b7\u20162 < 2k\u03c32 + 2\u03c32 \u221a 2k \u00b7 2k \u00b7 log(d/\u03b4) + 2\u03c32 \u00b7 2k \u00b7 log(d/\u03b4) = O(\u03c32k log(d/\u03b4))\nTaking a union bound over all possible subsets of size \u2264 2k we get that w.p. at least 1\u2212 \u03b4 the regret of w\u2217 is at most O(\u03c32k log(d/\u03b4))."}, {"heading": "B Proofs for Agnostic Setting", "text": "We begin with an auxiliary lemma for Lemma 10, informally proving that for any matrix X\u0304 with BBRCNP (Definition 6) and vector y, the set function\ng(S) = inf w\u2208RS\n\u2016y \u2212 X\u0304w\u20162\nis weakly supermodular. Its proof can be found in [Boutsidis et al., 2015], yet for completeness we provide it here as well.\nLemma 17. [Lemma 5 in [Boutsidis et al., 2015]] Let X\u0304 be a matrix whose columns have 2-norm at most 1 and y be a vector with \u2016y\u2016\u221e \u2264 1 of dimension matching the number of rows in X. the set function\ng(S) = inf w\u2208RS\n\u2016y \u2212Xw\u20162\nis \u03b1-weakly supermodular for sparsity k for \u03b1 = maxS:|S|\u2264k 1/\u03c3min(XS) 2, where XS is the submatrix of X obtained by choosing the columns indexed by S, and \u03c3min(A) is the smallest singular value of A.\nProof. Firstly, the well known closed form solution for the least-squares problem informs us that\ng(S) = inf w\u2208RS\n\u2016y \u2212Xw\u20162,\n= yT [I \u2212 (XTS )\u2020XTS ]y.\nWe use the notation A\u2020 for the pseudoinverse of a matrix A. That is, if the singular value decomposition of A is A = \u2211 i \u03c3iuiv T i with \u03c3i > 0 then A \u2020 = \u2211 i \u03c3 \u22121 i viu T i .\nLet us first estimate g(S) \u2212 g(T ), for sets S \u2282 T . For brevity, define HS as the projection matrix XSX \u2020 S projecting onto the column space of XS . Denote by ZT\\S the matrix whose columns are those of XT\\S projected away from the span of XS , and normalized. Namely, writing xi as the i\u2019th column of X , \u03b6i = \u2016(I \u2212HS)xi\u2016, zi = (I \u2212HS)xi/\u03b6i, and ZT\\S\u2019s columns are {zi}i\u2208T\\S . Notice that the columns of ZT\\S and XS are orthogonal, hence according to the Pythagorean theorem it holds that\ng(S) = \u2016y\u20162 \u2212 \u2016HSy\u20162, g(T ) = \u2016y\u20162 \u2212 \u2016HSy\u20162 \u2212 \u2016ZT\\SZ\u2020T\\Sy\u20162\nmeaning that g(S) \u2212 g(T ) = \u2016ZT\\SZ\u2020T\\Sy\u20162. In particular, this implies that for any j /\u2208 S it holds that g(S)\u2212 g(S \u222a {j}) = (zTj y)2, since zj is a unit vector. Let us now decompose g(S)\u2212 g(T ).\ng(S)\u2212 g(T ) = \u2016ZT\\SZ\u2020T\\Sy\u20162 = \u2016(ZTT\\S)\u2020ZTT\\Sy\u20162 \u2264 \u2016(ZTT\\S)\u2020\u20162 \u00b7 \u2016ZTT\\Sy\u20162\nThe norm used in the last inequality is the matrix operator norm. We now bound both factors of the product on the RHS separately. For the first factor, we claim that \u2016(ZTT\\S)\u2020\u2016 = \u2016Z \u2020 T\\S\u2016 \u2264 \u2016X \u2020 T\u2016. To see this, consider a vector w \u2208 R|T\\S|, for convenience denote its entries by {w(i)}i\u2208T\\S , and write zi = (xi \u2212 \u2211\nj\u2208S \u03b1ijxj)/\u03b6i. We have\nZT\\Sw = \u2211\ni\u2208T\\S\nziw(i) = \u2211\ni\u2208T\\S\nxiw(i)/\u03b6i \u2212 \u2211\nj\u2208S\nxj \u2211\ni\u2208T\\S\nw(i)\u03b1ij/\u03b6i = XTw \u2032\nfor the vector w\u2032 \u2208 R|T | defined as w\u2032(i) = w(i)/\u03b6i for i \u2208 T \\ S and w\u2032(j) = \u2212 \u2211 i\u2208T\\S w(i)\u03b1ij/\u03b6i for j \u2208 S. Since \u03b6i \u2264 \u2016xi\u2016 \u2264 1 we must have \u2016w\u2032\u2016 \u2265 \u2016w\u2016. Consider now the unit vector w for which \u2016ZT\\Sw\u2016 = \u2016Z\u2020T\\S\u2016\u22121, that is, the unit norm singular vector corresponding to the smallest non-zero singular value of ZT\\S . For this w, and its corresponding vector w \u2032, we have\n\u2016Z\u2020T\\S\u2016\u22121 = \u2016ZT\\Sw\u2016 = \u2016XTw\u2032\u2016 \u2265 \u03c3min(XT )\u2016w\u2032\u2016 \u2265 \u03c3min(XT )\u2016w\u2016 = \u03c3min(XT ).\nIt follows that \u2016(ZTT\\S)\u2020\u20162 = \u2016Z\u2020T\\S\u20162 \u2264 1/\u03c3min(XT )2\nWe continue to bound the right factor of product.\n\u2016ZTT\\Sy\u20162 = \u2211\ni\u2208T\\S\n(zTi y) 2 =\n\u2211\ni\u2208T\\S\ng(S)\u2212 g(S \u222a {i}).\nBy combining the inequalities we obtained the required result:\ng(S)\u2212 g(T ) \u2264 ( 1/\u03c3min(XT ) 2 ) \u2211\ni\u2208T\\S\ng(S)\u2212 g(S \u222a {i}).\nProof of Lemma 10. We would like to apply Lemma 17 on the design matrix X . The only catch is that the columns of X may not be bounded by 1 in norm. To remedy this, let j be the index of the column with the maximum norm and consider the matrix X\u0304 = 1\u2016Xj\u2016X instead (here, Xj is the j-th column of X ; note that Xj = Xej for the j-th standard basis vector ej). Now, for any subset S of coordinates,\ninf w\u2208RS \u2016y \u2212 X\u0304w\u20162 = inf w\u2208RS \u2016y \u2212Xw\u20162.\nThus, we conclude that the set function of interest, g(S) = infw\u2208RS \u2016y\u2212Xw\u20162, is \u03b1-weakly supermodular for sparsity k for \u03b1 = maxS:|S|\u2264k \u2016X\u0304\u2020S\u201622. For any subset of coordinates S of size at most k, let w be a unit norm right singular vector of X\u0304S corresponding to the smallest singular value, so that \u2016X\u0304\u2020S\u20162 = 1\u2016X\u0304Sw\u2016 . But\n1 \u2016X\u0304Sw\u2016 = \u2016Xej\u2016 \u2016Xw\u2032\u2016 , where w \u2032 is the vector w extended to all coordinates by padding with zeros.\nSince the restricted condition number of X for sparsity k is bounded by \u03ba we conclude that \u2016Xej\u2016 \u2016Xw\u2032\u2016 \u2264 \u03ba.\nSince this bound holds for any subset S of size at most k, we conclude that \u03b1 \u2264 \u03ba2.\nProof of Lemma 11. By the \u03b1-weak supermodularity of g, we have\ng(\u2205)\u2212 g(V ) \u2264 \u03b1 \u00b7 \u2211\nj\u2208V\n[g(\u2205)\u2212 g({j})]\n\u2264 \u03b1|V | \u00b7 [(g(\u2205)\u2212 g(V ))\u2212 (g({j\u2217})\u2212 g(V ))].\nRearranging, we get the claimed bounds.\nThe following lemma gives a useful property of weakly supermodular functions.\nLemma 18. Let g(\u00b7) be a (k, \u03b1)-weakly supermodular set function and U be a subset with |U | < k. Then g\u2032(S) := g(U \u222a S) is (k \u2212 |U |, \u03b1)-weakly supermodular. Proof. For any two subsets S \u2286 T with |T | \u2264 k \u2212 |U |, we have\ng\u2032(S)\u2212 g\u2032(T ) = g(U \u222a S)\u2212 g(U \u222a T ) \u2264 \u03b1 \u2211\nj\u2208(T\u222aU)\\(S\u222aU)\n[g(U \u222a S)\u2212 g(U \u222a S \u222a {j})]\n\u2264 \u03b1 \u2211\nj\u2208T\\S\n[g(U \u222a S)\u2212 g(U \u222a S \u222a {j})] = \u03b1 \u2211\nj\u2208T\\S\n[g\u2032(S)\u2212 g\u2032(S \u222a {j})].\nProof of Lemma 12. For i \u2208 {0, 1, . . . , k1}, define the set function g(i)b as g (i) b (S) = gb(S \u222a V (i) b ).\nFirst, we analyze the performance of the BEXP algorithms. Fix any i \u2208 [k1] and consider BEXP(i). Conceptually, for any j \u2208 [d], the loss of expert j at the end of mini-batch b is gb(V (i\u22121)b \u222a j) (note that this loss is only evaluated for j \u2208 U (i)b in the algorithm). To bound the regret, we need to bound the magnitude of the losses. Note that for any subset S, we have 0 \u2264 gb(S) \u2264 1B \u2211 t\u2208Tb\ny2t \u2264 1. Thus, the regret guarantee of BEXP (Theorem 8) implies that for any i \u2208 [k1] and any j \u2208 [d], we have\nE\n  T/B\u2211\nb=1\ngb(V (i\u22121) b \u222a {j (i) b })\n  \u2264 T/B\u2211\nb=1\ngb(V (i\u22121) b \u222a {j}) + 2\n\u221a dk1 log(d)T\nk0B .\nThe expectation above is conditioned on the randomness in V (i\u22121) b , for b \u2208 [T/B]. Rewriting the above inequality using the g(i\u22121) and g(i) functions, and using the fact that V (i\u22121) b \u222a {j (i) b } = V (i) b , we get\nE\n  T/B\u2211\nb=1\ng (i) b (\u2205)\n  \u2264 T/B\u2211\nb=1\ng (i\u22121) b ({j}) + 2\n\u221a dk1 log(d)T\nk0B . (14)\nNext, since we assumed that the sequence of feature vectors satisfies BBRCNP with parameters (\u01eb, k1+k), Lemma 10 implies that the set function gb defined in (6) is (k1 + k, \u03ba 2)-weakly supermodular for \u03ba = 1+\u01eb1\u2212\u01eb . By Lemma 18, the set function g (i) b is (k, \u03ba\n2)-weakly supermodular (since |V (i)b | \u2264 k1). It is easy to check that the sum of weakly supermodular functions is also weakly supermodular (with\nthe same parameters), and hence \u2211T/B\nb=1 g (i\u22121) b is also (k, \u03ba 2)-weakly supermodular. Hence, by Lemma 11, if\nj\u2217 = argminj \u2211T/B b=1 g (i\u22121) b ({j}), we have, for any subset V of size at most k,\nT/B\u2211\nb=1\ng (i\u22121) b ({j\u2217})\u2212 g (i\u22121) b (V ) \u2264 (1\u2212 1\u03ba2|V | )[\nT/B\u2211\nb=1\ng (i\u22121) b (\u2205)\u2212 g (i\u22121) b (V )].\nSince gb(V ) \u2265 gb(V \u222a V (i\u22121)b ) = g (i\u22121) b (V ), the above inequality implies that\nT/B\u2211\nb=1\ng (i\u22121) b ({j\u2217})\u2212 gb(V ) \u2264 (1\u2212 1\u03ba2|V | )[\nT/B\u2211\nb=1\ng (i\u22121) b (\u2205)\u2212 gb(V )].\nCombining this bound with (14) for j = j\u2217, we get\nE\n  T/B\u2211\nb=1\ng (i) b (\u2205)\u2212 gb(V )   \u2264 (1\u2212 1\u03ba2|V |)[ T/B\u2211\nb=1\ng (i\u22121) b (\u2205)\u2212 gb(V )] + 2\n\u221a dk1 log(d)T\nk0B .\nApplying this bound recursively for i \u2208 [k1] and simplifying, we get\nE\n  T/B\u2211\nb=1\ng (k1) b (\u2205)\u2212 gb(V )   \u2264 (1\u2212 1\u03ba2|V |)k1 [ T/B\u2211\nb=1\ng (0) b (\u2205)\u2212 gb(V )] + 2\u03ba2|V |\n\u221a dk1 log(d)T\nk0B .\nUsing the definitions of g (k1) b and g (0) b , and the fact that |V | \u2264 k, we get the claimed bound."}], "references": [{"title": "Budgeted prediction with expert advice", "author": ["Kareem Amin", "Satyen Kale", "Gerald Tesauro", "Deepak S. Turaga"], "venue": "In AAAI,", "citeRegEx": "Amin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Amin et al\\.", "year": 2015}, {"title": "Linear and conic programming estimators in high dimensional errors-in-variables models", "author": ["Alexandre Belloni", "Mathieu Rosenbaum", "Alexandre B. Tsybakov"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Belloni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belloni et al\\.", "year": 2016}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "author": ["Peter J Bickel", "Ya\u2019acov Ritov", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Bickel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bickel et al\\.", "year": 2009}, {"title": "Greedy minimization of weakly supermodular set functions", "author": ["Christos Boutsidis", "Edo Liberty", "Maxim Sviridenko"], "venue": "arXiv preprint arXiv:1502.06528,", "citeRegEx": "Boutsidis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2015}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "author": ["Emmanuel Candes", "Terence Tao"], "venue": "The Annals of Statistics,", "citeRegEx": "Candes and Tao.,? \\Q2007\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2007}, {"title": "Decoding by linear programming", "author": ["Emmanuel J Candes", "Terence Tao"], "venue": "IEEE transactions on information theory,", "citeRegEx": "Candes and Tao.,? \\Q2005\\E", "shortCiteRegEx": "Candes and Tao.", "year": 2005}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Efficient learning with partially observed attributes", "author": ["Nicol\u00f2 Cesa-Bianchi", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2011}, {"title": "Variable selection is hard", "author": ["Dean Foster", "Howard Karloff", "Justin Thaler"], "venue": "In COLT,", "citeRegEx": "Foster et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2015}, {"title": "Online sparse linear regression", "author": ["Dean Foster", "Satyen Kale", "Howard Karloff"], "venue": "In COLT,", "citeRegEx": "Foster et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2016}, {"title": "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale.,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2014}, {"title": "Linear regression with limited observation", "author": ["Elad Hazan", "Tomer Koren"], "venue": "In ICML,", "citeRegEx": "Hazan and Koren.,? \\Q2012\\E", "shortCiteRegEx": "Hazan and Koren.", "year": 2012}, {"title": "Model selection for high-dimensional regression under the generalized irrepresentability condition", "author": ["Adel Javanmard", "Andrea Montanari"], "venue": "In NIPS,", "citeRegEx": "Javanmard and Montanari.,? \\Q2013\\E", "shortCiteRegEx": "Javanmard and Montanari.", "year": 2013}, {"title": "Open problem: Efficient online sparse regression", "author": ["Satyen Kale"], "venue": "In COLT, pages 1299\u20131301,", "citeRegEx": "Kale.,? \\Q2014\\E", "shortCiteRegEx": "Kale.", "year": 2014}, {"title": "Attribute efficient linear regression with distribution-dependent sampling", "author": ["Doron Kukliansky", "Ohad Shamir"], "venue": "In ICML,", "citeRegEx": "Kukliansky and Shamir.,? \\Q2015\\E", "shortCiteRegEx": "Kukliansky and Shamir.", "year": 2015}, {"title": "Learning without concentration", "author": ["Shahar Mendelson"], "venue": "In COLT, pages", "citeRegEx": "Mendelson.,? \\Q2014\\E", "shortCiteRegEx": "Mendelson.", "year": 2014}, {"title": "Sparse approximate solutions to linear systems", "author": ["Balas Kausik Natarajan"], "venue": "SIAM journal on computing,", "citeRegEx": "Natarajan.,? \\Q1995\\E", "shortCiteRegEx": "Natarajan.", "year": 1995}, {"title": "Sparse recovery under matrix uncertainty", "author": ["Mathieu Rosenbaum", "Alexandre B. Tsybakov"], "venue": "The Annals of Statistics,", "citeRegEx": "Rosenbaum and Tsybakov.,? \\Q2010\\E", "shortCiteRegEx": "Rosenbaum and Tsybakov.", "year": 2010}, {"title": "An online algorithm for maximizing submodular functions", "author": ["Matthew J. Streeter", "Daniel Golovin"], "venue": "In NIPS,", "citeRegEx": "Streeter and Golovin.,? \\Q2008\\E", "shortCiteRegEx": "Streeter and Golovin.", "year": 2008}, {"title": "On model selection consistency of lasso", "author": ["Peng Zhao", "Bin Yu"], "venue": "Journal of Machine learning research,", "citeRegEx": "Zhao and Yu.,? \\Q2006\\E", "shortCiteRegEx": "Zhao and Yu.", "year": 2006}, {"title": "Online learning with costly features and labels", "author": ["Navid Zolghadr", "G\u00e1bor Bart\u00f3k", "Russell Greiner", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri"], "venue": "In NIPS,", "citeRegEx": "Zolghadr et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zolghadr et al\\.", "year": 2013}, {"title": "X\u0304w\u2016 is weakly supermodular. Its proof can be found in [Boutsidis et al., 2015], yet for completeness we provide it here as well", "author": ["\u2016y"], "venue": null, "citeRegEx": "\u2212,? \\Q2015\\E", "shortCiteRegEx": "\u2212", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "One example of this situation, from [Cesa-Bianchi et al., 2011], is medical diagnosis of a disease, in which each feature is the result of a medical test on the patient.", "startOffset": 36, "endOffset": 63}, {"referenceID": 5, "context": "In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005].", "startOffset": 205, "endOffset": 227}, {"referenceID": 2, "context": "RIP and related Restricted Eigenvalue Condition [Bickel et al., 2009] have been widely used as a standard assumption for theoretical analysis in the compressive sensing and sparse regression literature, in the offline case.", "startOffset": 48, "endOffset": 69}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t.", "startOffset": 185, "endOffset": 206}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only \u00d5(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details.", "startOffset": 185, "endOffset": 515}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only \u00d5(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all \u03b4 > 0 there exists no polynomial-time algorithm with regret O(T ) unless NP \u2286 BPP .", "startOffset": 185, "endOffset": 613}, {"referenceID": 5, "context": ", yT as inputs, the offline problem of finding a k-sparsew that minimizes the error \u2211T t=1(yt\u2212\u3008xt, w\u3009)2 does not admit a polynomial time algorithm under standard complexity assumptions Foster et al. [2015]. This hardness persists even under the assumption that there exists a k-sparse w such that yt = \u3008xt, w\u2217\u3009 for all t. Furthermore, the computational hardness is present even when the solution is required to be only \u00d5(k)-sparse solution and has to minimize the error only approximately; see Foster et al. [2015] for details. The hardness result was extended to online sparse regression by Foster et al. [2016]. They showed that for all \u03b4 > 0 there exists no polynomial-time algorithm with regret O(T ) unless NP \u2286 BPP . Foster et al. [2016] posed the open question of what additional assumptions can be made on the data to make the problem tractable.", "startOffset": 185, "endOffset": 744}, {"referenceID": 3, "context": "In this paper, we answer this open question by providing efficient algorithms with sublinear regret under the assumption that the matrix of feature vectors satisfies the restricted isometry property (RIP) [Candes and Tao, 2005]. It has been shown that if RIP holds and there exists a sparse linear predictor w such that yt = \u3008xt, w\u2217\u3009 + \u03b7t where \u03b7t is independent noise, the offline sparse linear regression problem admits computationally efficient algorithms, e.g., Candes and Tao [2007]. RIP and related Restricted Eigenvalue Condition [Bickel et al.", "startOffset": 206, "endOffset": 488}, {"referenceID": 4, "context": "The solution of this problem cannot be obtained by a simple application of say, the Dantzig selector [Candes and Tao, 2007] since we do not observe the data matrix X , but rather a subsample of its entries.", "startOffset": 101, "endOffset": 123}, {"referenceID": 10, "context": "This bound has optimal dependence on T , since even in the full information setting where all features are observed there is a lower bound of \u03a9(logT ) [Hazan and Kale, 2014].", "startOffset": 151, "endOffset": 173}, {"referenceID": 3, "context": "The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity.", "startOffset": 16, "endOffset": 40}, {"referenceID": 3, "context": "The analysis in [Boutsidis et al., 2015] shows that the RIP assumption implies that the set function defined as the minimum loss achievable by a linear regressor restricted to the set in question satisfies a property called weak supermodularity. Weak supermodularity is a relaxation of standard supermodularity that is still strong enough to show performance bounds for the standard greedy feature selection algorithm for solving the sparse regression problem. We then employ a technique developed by Streeter and Golovin [2008] to construct an online learning algorithm that mimics the greedy feature selection algorithm.", "startOffset": 17, "endOffset": 529}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al.", "startOffset": 66, "endOffset": 746}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound.", "startOffset": 66, "endOffset": 768}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound.", "startOffset": 66, "endOffset": 781}, {"referenceID": 3, "context": "2 Related work A related setting is attribute-efficient learning [Cesa-Bianchi et al., 2011, Hazan and Koren, 2012, Kukliansky and Shamir, 2015]. This is a batch learning problem in which the examples are generated i.i.d., and the goal is to simply output a linear regressor using only a limited number of features per example with bounded excess risk compared to the optimal linear regressor, when given full access to the features at test time. Since the goal is not prediction but simply computing the optimal linear regressor, efficient algorithms exist and have been developed by the aforementioned papers. Without any assumptions, only inefficient algorithms for the online sparse linear regression problem are known Zolghadr et al. [2013], Foster et al. [2016]. Kale [2014] posed the open question of whether it is possible to design an efficient algorithm for the problem with a sublinear regret bound. This question was answered in the negative by Foster et al. [2016], who showed that efficiency can only be obtained under additional assumptions on the data.", "startOffset": 66, "endOffset": 978}, {"referenceID": 2, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al.", "startOffset": 112, "endOffset": 134}, {"referenceID": 2, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al.", "startOffset": 112, "endOffset": 197}, {"referenceID": 1, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al.", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein).", "startOffset": 198, "endOffset": 386}, {"referenceID": 1, "context": "In the realizable setting, the linear program at the heart of the algorithm is motivated from Dantzig selection Candes and Tao [2007] and error-in-variable regression Rosenbaum and Tsybakov [2010], Belloni et al. [2016]. The problem of finding the best sparse linear predictor when only a sample of the entries in the data matrix is available is also discussed by Belloni et al. [2016] (see also the references therein). In fact, these papers solve a more general problem where we observe a matrix Z rather than X that is an unbiased estimator of X . While we can use their results in a black-box manner, they are tailored for the setting where the variance of each Zij is constant and it is difficult to obtain the exact dependence on this variance in their bounds. In our setting, this variance can be linear in the dimension of the feature vectors, and hence we wish to control the dependence on the variance in the bounds. Thus, we use an algorithm that is similar to the one in Belloni et al. [2016], and provide an analysis for it (in the appendix).", "startOffset": 198, "endOffset": 1005}, {"referenceID": 4, "context": "The following definition will play a key role: Definition 1 (Restricted Isometry Property Candes and Tao [2007]).", "startOffset": 90, "endOffset": 112}, {"referenceID": 4, "context": "The following definition will play a key role: Definition 1 (Restricted Isometry Property Candes and Tao [2007]). Let \u01eb \u2208 (0, 1) and k \u2265 0. We say that a matrix X \u2208 R satisfies restricted isometry property (RIP) with parameters (\u01eb, k) if for any w \u2208 R with \u2016w\u20160 \u2264 k we have (1\u2212 \u01eb) \u2016w\u20162 \u2264 1 \u221a n \u2016Xw\u20162 \u2264 (1 + \u01eb) \u2016w\u20162 . One can show that RIP holds with overwhelming probability if n = \u03a9(\u01ebk log(ed/k)) and each row of the matrix is sampled independently from an isotropic sub-Gaussian distribution. In the realizable setting, the sub-Gaussian assumption can be relaxed to incorporate heavy tail distribution via the \u201csmall ball\u201d analysis introduced in Mendelson [2014], since we only require one-sided lower isometry property.", "startOffset": 90, "endOffset": 665}, {"referenceID": 8, "context": "Once again, without any regularity condition on the design matrix, Foster et al. [2016] have shown that achieving a sub-linear regret O(T ) is in general computationally hard, for any constant \u03b4 > 0 unless NP \u2286 BPP.", "startOffset": 67, "endOffset": 88}, {"referenceID": 3, "context": "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine.", "startOffset": 133, "endOffset": 323}, {"referenceID": 3, "context": "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al.", "startOffset": 133, "endOffset": 445}, {"referenceID": 3, "context": "Our algorithm is motivated from literature on maximization of sub-modular set function [Natarajan, 1995, Streeter and Golovin, 2008, Boutsidis et al., 2015]. Though the problem being NP-hard, greedy algorithm on sub-modular maximization provides provable good approximation ratio. Specifically, Streeter and Golovin [2008] considered online optimization of super/sub-modular set functions using expert algorithm as sub-routine. Natarajan [1995], Boutsidis et al. [2015] cast the sparse linear regression as maximization of weakly supermodular function.", "startOffset": 133, "endOffset": 470}, {"referenceID": 3, "context": "The definition is slightly stronger than that in Boutsidis et al. [2015]. We will show that sparse linear regression can be viewed as weakly supermodular minimization in Definition 7 once the design matrix has bounded restricted condition number.", "startOffset": 49, "endOffset": 73}, {"referenceID": 18, "context": "For the latter problem, we develop an online greedy algorithm along the lines of [Streeter and Golovin, 2008].", "startOffset": 81, "endOffset": 109}, {"referenceID": 0, "context": "We employ k1 = O\u2217(k) budgeted experts algorithms [Amin et al., 2015], denoted BEXP, with budget parameter k0 k1 .", "startOffset": 49, "endOffset": 68}, {"referenceID": 0, "context": "The precise characteristics of BEXP are given in Theorem 8 (adapted from Theorem 2 in [Amin et al., 2015]).", "startOffset": 86, "endOffset": 105}, {"referenceID": 3, "context": "This lemma is a direct consequence of Lemma 5 in [Boutsidis et al., 2015].", "startOffset": 49, "endOffset": 73}, {"referenceID": 6, "context": "8 in Cesa-Bianchi and Lugosi [2006]: Lemma 13.", "startOffset": 5, "endOffset": 36}, {"referenceID": 8, "context": "For example, simply assuming that the final matrix XT satisfies RIP rather than every intermediate matrix Xt for large enough t is not sufficient; a simple tweak to the lower bound construction of Foster et al. [2016] shows this.", "startOffset": 197, "endOffset": 218}], "year": 2017, "abstractText": "Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.", "creator": "LaTeX with hyperref package"}}}