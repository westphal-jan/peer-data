{"id": "1706.04815", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "S-Net: From Answer Extraction to Answer Generation for Machine Reading Comprehension", "abstract": "most seminal works investigating analytic error surveys are built under the answer formation approach which predicts meta - portions their passages to probing questions. in this research, scholars apply an extraction - event - generation framework for mechanical reading comprehension, interpreting which the answer appears repeated from her correct transcript. essentially, we build the answer extraction technique to understand the most important sub - spans from the passage as recorded, fully develop the answer design process which takes the evidence as additional calculations assisting with the question and passage to further elaborate the extraction answers. developers build the answer extraction model covering was - of - the - art survey applications demonstrating reading comprehension, and the answer generation simulation with sequence - root - sequence neural computation. experiments on the ms - marco network show that the generation based tree holds par success than pure test finding.", "histories": [["v1", "Thu, 15 Jun 2017 11:10:33 GMT  (725kb,D)", "http://arxiv.org/abs/1706.04815v1", null], ["v2", "Tue, 5 Sep 2017 11:55:01 GMT  (666kb,D)", "http://arxiv.org/abs/1706.04815v2", null], ["v3", "Mon, 25 Sep 2017 01:41:07 GMT  (664kb,D)", "http://arxiv.org/abs/1706.04815v3", null], ["v4", "Mon, 9 Oct 2017 06:58:31 GMT  (664kb,D)", "http://arxiv.org/abs/1706.04815v4", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chuanqi tan", "furu wei", "nan yang", "weifeng lv", "ming zhou"], "accepted": false, "id": "1706.04815"}, "pdf": {"name": "1706.04815.pdf", "metadata": {"source": "CRF", "title": "S-NET: FROM ANSWER EXTRACTION TO ANSWER GENERATION FOR MACHINE READING COMPREHEN- SION", "authors": ["Chuanqi Tan", "Furu Wei", "Nan Yang", "Weifeng Lv", "Ming Zhou"], "emails": ["tanchuanqi@nlsde.buaa.edu.cn", "lwf@buaa.edu.cn", "fuwei@microsoft.com", "nanya@microsoft.com", "mingzhou@microsoft.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Machine reading comprehension (Nguyen et al., 2016), which attempts to enable machines to answer questions after reading a passage or a set of passages, attracts great attentions from both research and industry communities in recent years. The release of the Stanford Question Answering Dataset (SQuAD) and the Microsoft MAchine Reading COmprehension Dataset (MS-MARCO) provides the large-scale manually created datasets for model training and testing of machine learning (esp. deep learning) algorithms for this task. There are two settings in existing machine reading comprehension datasets. The SQuAD dataset constrains the answer to be an exact sub-span in the passage, while the words in the answer are not necessary in the passages as in the MS-MARCO dataset.\nExisting methods for machine reading comprehension usually follow the extraction based approach. It formulates the task as predicting the start and end positions of the answer in the passage. The work on the SQuAD dataset mostly follows this direction according to the task definition. However, as defined in the MS-MARCO dataset, the answer may come from multiple spans, and the system needs to elaborate the answer using words in the passages or not, which is more natural as it is how human answers questions after reading a passage.\nTable 1 shows the examples from the MS-MARCO dataset organized in different synthetic categories, in which the answer needs to be synthesized or generated from the question and passage. The answer may consist of the answer text spans (hereafter evidence snippets) from the passage and words from the question or even words that are not found in the passages or question. In the first example, the final answer is further refined from the evidence snippet in the passage. In the second example, the answer is composed through combining the evidence snippets and words in the question. In the third and fourth examples, the answers are from multiple evidence in the same passage or in different passages. In the last example, one should infer the answer as the words in it do not exist in the passage and question.\n\u2217 Contribution during internship at Microsoft Research.\nar X\niv :1\n70 6.\n04 81\n5v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\nIn this paper, we present an extraction-then-generation framework for machine reading comprehension, in which the answer is generated from the extraction results. We build the answer extraction model to predict the most important sub-spans from the passage as evidence, and then develop the answer generation model which takes the evidence as additional features along with the question and passage to further elaborate the final answers.\nSpecifically, the extraction model is based on the R-Net (Wang et al., 2017). We use the bidirectional recurrent networks (RNN) for word-level representation, and then apply the attention mechanism to incorporate matching information from question to passage. Next, we predict start and end positions of the evidence snippet by the Pointer Networks (Vinyals et al., 2015a). Then, we apply the sequence-to-sequence model to synthesize the final answer by extracted evidence. The question and passage are encoded by a bi-directional RNN. The start and end positions of extracted snippet are\nlabeled as features. Lastly, we combine the question and passage information as the input to feed the decoder, and use an attention-equipped decoder to generate the final answer.\nWe conduct experiments on the MS-MARCO dataset. The results show our extraction-thengeneration framework outperforms our baselines and all other existing methods in terms of ROUGEL and BLEU-1."}, {"heading": "2 RELATED WORK", "text": "Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Richardson et al. (2013) release MCTest whose goal is to select the best answer from four options given the question and the passage. CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages.\nTo the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al. (2016) solve SQuAD by ranking continuous text spans within passage. Yang et al. (2016) present a fine-grained gating mechanism to dynamically combine word-level and character-level representation and model the interaction between questions and passages.\nThe sequence-to-sequence model is widely-used in many tasks such as machine translation (Luong et al., 2015), parsing (Vinyals et al., 2015b), summarization generation (Zhou et al., 2017) and response generation (Gu et al., 2016). In this work, we use it to generate the synthetic answer based on a encoder incorporated with evidence features and an attention-based decoder."}, {"heading": "3 OUR APPROACH", "text": "Following the flowchart in Figure 1, our approach consists of two parts as evidence extraction1 and answer generation. The evidence extraction part aims to extract evidence snippets related to the question and passage. We use the gated attention recurrent neural network to match the question and passage, and then use pointer networks to predict the boundary of evidence snippets. The answer generation part is to generate the final answer based on the evidence snippets using a sequence-tosequence model. We annotate the boundary of extracted evidence snippets as the additional feature in the encoding part and apply an attention-equipped decoder for the final answer."}, {"heading": "3.1 GATED RECURRENT UNIT", "text": "We use Gated Recurrent Unit (GRU) (Cho et al., 2014) instead of basic RNN. Equation 1 describes the mathematical model of the GRU. rt and zt are the gates and ht is the hidden state.\nzt = \u03c3(Whzht\u22121 +Wxzxt + bz)\nrt = \u03c3(Whrht\u22121 +Wxrxt + br)\nh\u0302t = \u03a6(Wh(rt ht\u22121) +Wxxt + b) ht = (1\u2212 zt) ht\u22121 + z h\u0302t (1a)\n1In our model, we use \u201cevidence extraction\u201d to represent the pure \u201canswer extraction\u201d in previous work."}, {"heading": "3.2 EVIDENCE EXTRACTION", "text": "Consider a question Q = {wQt }mt=1 and a passage P = {wPt }nt=1, we first convert the words to their respective word-level embeddings and character-level embeddings. The character-level embeddings are generated by taking the final hidden states of a bi-directional GRU applied to embeddings of characters in the token. We then use a bi-directional GRU to produce new representation uQ1 , . . . , u Q m and uP1 , . . . , u P n of all words in the question and passage respectively:\nuQt = BiGRUQ(u Q t\u22121, [e Q t , c Q t ])\nuPt = BiGRUP (u P t\u22121, [e P t , c P t ]) (2)\nGiven question and passage representation {uQt }mt=1 and {uPt }nt=1, Rockta\u0308schel et al. (2015) propose generating sentence-pair representation {vPt }nt=1 via soft-alignment of words in the question and passage as follows:\nvPt = GRU(v P t\u22121, ct) (3)\nwhere ct = att(uQ, [uPt , v P t\u22121]) is an attention-pooling vector of the whole question (u Q):\nstj = v Ttanh(WQu u Q j +W P u u P t +W P v v P t\u22121)\nati = exp(s t i)/\u03a3 m j=1exp(s t j)\nct = \u03a3 m i=1a t iu Q i (4)\nEach passage representation vPt dynamically incorporates aggregated matching information from the whole question.\nWang & Jiang (2016a) introduce match-LSTM, which takes uPj as an additional input into the recurrent network. Wang et al. (2017) add another gate to the input ([uPt , ct]) of RNN to determine the importance of passage parts.\ngt = sigmoid(Wg[u P t , ct]) [uPt , ct] \u2217 = gt [uPt , ct]\nvPt = GRU(v P t\u22121, [u P t , ct]) (5)\nWe follow the mainstream approaches that answer the question by predicting the start and end positions in the passage using pointer networks (Vinyals et al., 2015a). Given the passage representation {vPt }nt=1, the attention mechanism is utilized as a pointer to select the start position (p1) and end position (p2) from the passage, which can be formulated as follows:\nstj = v Ttanh(WPh v P j +W a h h a t\u22121)\nati = exp(s t i)/\u03a3 n j=1exp(s t j)\npt = argmax(at1, . . . , a t n) (6)\nHere hat\u22121 represents the last hidden state of the answer recurrent network (pointer network). The input of the answer recurrent network is the attention-pooling vector based on current predicted probability at:\nct = \u03a3 n i=1a t iv P i\nhat = GRU(h a t\u22121, ct) (7)\nWhen predicting the start position, hat\u22121 represents the initial hidden state of the answer recurrent network. We utilize the question vector rQ as the initial state of the answer recurrent network. rQ = att(uQ, V Qr ) is an attention-pooling vector of the question based on the parameter V Q r :\nsj = v Ttanh(WQu u Q j +W Q v V Q r )\nai = exp(si)/\u03a3 m j=1exp(sj)\nrQ = \u03a3mi=1aiu Q i (8)\nFor this part, the objective function is to minimize the following cross entropy:\nLAP = \u2212\u03a32t=1\u03a3Ni=1[yti log ati + (1\u2212 yti) log(1\u2212 ati)] (9) where yti \u2208 {0, 1} denotes a label. yti = 1 means i is a correct position, otherwise yti = 0."}, {"heading": "3.3 ANSWER GENERATION", "text": "We first produce the representation hPt and h Q t of all words in the passage and question respectively. When producing the answer representation, we combine the basic word embedding with an additional feature to indicate the start and end positions of the evidence snippet predicted by evidence extraction model.\nhPt = BiGRU(h P t\u22121, [e p t , f p t ])\nhQt = BiGRU(h Q t\u22121, e Q t ) (10)\nOn top of the encoder, we use GRU with attention as the decoder to produce the answer. At each decoding time step t, the GRU reads the previous word embeddingwt\u22121 and previous context vector ct\u22121 as inputs to compute the new hidden state dt. To initialize the GRU hidden state, we use a linear layer with the last backward encoder hidden state ~h P 1 and ~h Q 1 as input:\ndt = GRU(wt\u22121, ct\u22121, dt\u22121)\nd0 = tanh(Wd[ ~h P 1 , ~h Q 1 ] + b) (11)\nwhere Wd is the weight matrix and b is the bias vector.\nThe context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state dt with each encoder hidden state\nht to get an importance score. Here hi consists of the answer representation hPt and the question representation hPt . The importance scores are then normalized to get the current context vector by weighted sum:\nstj = v T a tanh(Wadt\u22121 + Uahj)\nati = exp(s t i)/\u03a3 n j=1exp(s t j)\nct = \u03a3 n i=1a t ihi (12)\nWe then combine the previous word embedding wt\u22121, the current context vector ct, and the decoder state dt to construct the readout state rt. The readout state is then passed through a maxout hidden layer (Goodfellow et al., 2013) to predict the next word with a softmax layer over the decoder vocabulary.\nrt = Wrwt\u22121 + Urct + Vrdt\nmt = [max{rt,2j\u22121, rt,2j}]T\np(yt|y1, . . . , yt\u22121) = softmax(Womt) (13)\nwhere Wa, Ua, Wr, Ur, Vr and Wo are parameters to be learned. Readout state rt is a 2ddimensional vector, and the maxout layer (Equation 13) picks the max value for every two numbers in rt and produces a d-dimensional vector mt.\nOur goal is to maximize the output probability given the input sentence. Therefore, we optimize the negative log-likelihood loss function:\nLAG = \u2212 1\n|D| \u03a3(x,y)\u2208D log p(y|x) (14)\nwhere D denotes the gold answer."}, {"heading": "4 EXPERIMENT", "text": "We conduct our experiments on the MS-MARCO dataset (Nguyen et al., 2016). We compare our extraction-then-generation framework with single extraction model and other competing methods. Experimental results show that our model achieves better results in official evaluation metrics. We then use a few examples to show the superiority of our model."}, {"heading": "4.1 DATASET AND EVALUATION METRICS", "text": "For the MS-MARCO dataset, the questions are user queries issued to the Bing search engine, the context passages are real Web documents and the answers are human-generated. The data has been split into a training set (82,326 pairs), a development set (10,047 pairs) and a test set (9,650 pairs). The answers are not necessarily sub-spans of the passages so that the metrics in the official tool of MS-MARCO evaluation are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in many domains."}, {"heading": "4.2 IMPLEMENTATION DETAILS", "text": ""}, {"heading": "4.2.1 TRAINING DATA", "text": "For the evidence extraction, since the answers are not necessarily sub-spans of the passages, we choose the span with the highest ROUGE-L score with the reference answer as the gold span in the training. Moreover, we only use the data whose ROUGE-L score of answer span is higher than 0.7, therefore we only use 71,417 training pairs in our experiments.\nFor the answer generation, the training data consists of two parts. First, for all passages in the training data, we treat the passage as the input where the annotated boundary depends on the span with the highest ROUGE-L score, and the reference answer as the output. We only use the data whose ROUGE-L score of answer span is higher than 0.5. Second, we apply our evidence extraction model to all training data to obtain the extracted span. Then we treat the passage to which this span belongs as the input. We collect 233,670 pairs to train the generation model."}, {"heading": "4.2.2 PARAMETER", "text": "For the answer extraction, we use 300-dimensional uncased pre-trained GloVe embeddings (Pennington et al., 2014)2 for both question and passage without update during training. We use zero vectors to represent all out-of-vocabulary words. Hidden vector length is set to 150 for all layers. We also apply dropout (Srivastava et al., 2014) between layers, with dropout rate 0.1.\nFor the answer generation, we use an identical vocabulary set for the input and output collected from the training data. We set the vocabulary size to 100,000 according to the frequency and the other words are set to <unk>. All word embeddings are updated during the training. We set the word embedding size to 300, set the feature embedding of start and end positions of the extracted snippet to 50, and set all GRU hidden state sizes to 150."}, {"heading": "4.2.3 DECODING", "text": "When decoding, we first run our extraction model to obtain the extracted span, and run our generation model with the passage that contains the span for the answer sequence. After generating the final sequence by the sequence-to-sequence model, we post-process the sequence with following rules:\n\u2022 We only keep once if the sequence-to-sequence model may generate duplicated words or phrases.\n\u2022 For all \u201c<unk>\u201d and the word as well as phrase which is not existed in the extracted answer, we try to refine it by finding a word or phrase with the same adjacent words in the extracted span and passage.\n\u2022 If the generated answer only contains a single word \u201c<unk>\u201d, we use the extracted span as the final answer."}, {"heading": "4.3 BASELINE METHODS", "text": "We conduct experiments with following settings:\nAE: the model that only has the answer extraction part.\nAE+AG (S-Net): the model that consists of the evidence extraction part and the answer generation part.\nWe also compare with other methods which report results in MS-MARCO dataset, including FastQAExt (Weissenborn et al., 2017), Prediction (Wang & Jiang, 2016b), and ReasoNet (Shen et al., 2016)."}, {"heading": "4.4 RESULT", "text": "Table 2 shows ROUGE-L and BLEU-1 scores on the development set of our model and competing approaches3. As we can see, our method outperforms the baseline and several strong state-of-the-art systems for both ROUGE-L and BLEU-1.\n2http://nlp.stanford.edu/data/glove.6B.zip. 3Extracted from MS-MARCO leaderboard http://www.msmarco.org/leaders.aspx"}, {"heading": "4.5 DISCUSSION", "text": "We compare the result of answer extraction and answer generation in different synthetic categories in Table 4 with several examples in Table 5. For the questions whose answers can be exactly matched in the passage, our answer generation model performs slightly worse because the sequence-to-sequence model makes some deviation when copying extracted evidence. However, in other categories, our generation model achieves more or less improvement. We can observe that the largest category in the rest questions is to refine the extracted evidence. In the first example in Table 5, our generation model adds \u201cit\u201d to the extracted snippet to synthesize the answer, which makes the syntax of the sentence complete. In the next two examples, we find that the generation model is good at controlling the boundary of the answer. In the fourth example, the generation model refines the extracted answer by considering words in the question. The last example focuses on the inference question. The extraction model cannot answer the question because there is no \u201cyes\u201d in the passage even though it successfully detects the related snippets. Our generation model can easily solve this problem and give the correct answer through the extracted evidence. We do not handle the questions that contain multiple evidence and leave it to the future work.\nMoreover, we observe that our method only achieves significant improvement in terms of ROUGE-L compared with our baseline. In the official evaluation tool, the ROUGE-L is calculated by averaging the scores per question, however, the BLEU is normalized with all questions. We argue that the answer should be evaluated case-by-case in the scenario of question answering. By analyzing our answer generation model, we well solve the questions in the category of inference. However, in these questions the answers are too short to influence the final score in the official evaluation in terms of BLEU."}, {"heading": "5 CONCLUSION", "text": "In this paper, we introduce S-Net, an extraction-then-generation framework for machine reading comprehension. The extraction model aims to match the question and passage to predict most important sub-spans in the passage related to the question as evidence. Then, the generation model synthesizes the final answer from the question information and the evidence snippet. We use gated attention recurrent neural networks and pointer networks for evidence extraction, and use the sequence-tosequence model for answer generation. We conduct experiments on the MS-MARCO dataset. The results demonstrate that our approach outperforms pure answer extraction model and other existing methods. This work can be advanced from different perspectives. Specifically, we are working on models to improve the other synthetic categories with the special focus on the category of refining evidence snippets.\nQ: what term is tranquility P : tranquillity (also spelled tranquility) is the quality or state of being tranquil; meaning calmness, serenity, and worry-free. maps have been produced for the whole of england that show the tranquillity score of ordnance survey grid derived 500m x 500m squares. Extracted Evidence: is the quality or state of being tranquil; meaning calmness, serenity, and worry-free. Synthetic Answer: it is the quality or state of being tranquil; meaning calmness, serenity and worry-free. Q: which vitamins is effective in post inflammatory pigmentation P : vitamin c iontophoresis is very effective for removing fine wrinkles and treating post - inflammatory hyper - pigmentation and melasma. it has become one of the major methods of maintaining healthy skin. Extracted Evidence: vitamin c iontophoresis Synthetic Answer: vitamin c Q: what is transesophageal echocardiogram P : a transesophageal echocardiogram (tee) is a diagnostic procedure that uses echocardiography to assess the hearts function. echocardiography is a procedure used to assess the heart\u2019s function and structures. tee may be used during surgery to assess the cardiac status of patients with known cardiac disease who are undergoing noncardiac procedures, and during heart surgery to evaluate the effects of surgical intervention to the heart, such as bypass surgery or valve repair or replacement. Extracted Evidence: a transesophageal echocardiogram (tee) is a diagnostic procedure that uses echocardiography to assess the hearts function. Synthetic Answer: a diagnostic procedure that uses echocardiography to assess the heart s function. Q: what causes bowel obstruction P : a bowel obstruction happens when either your small or large intestine is partly or completely blocked. other causes include hernias and crohn\u2019s disease, which can twist or narrow the intestine, and tumors, which can block the intestine. a blockage also can happen if one part of the intestine folds like a telescope into another part, which is called intussusception. Extracted Evidence: hernias and crohn\u2019s disease, which can twist or narrow the intestine, and tumors, which can block the intestine. Synthetic Answer: it is a bowel obstruction happens when either your small or large intestine is partly and crohn\u2019s disease, which can twist or narrow the intestine, and tumors, which can block the intestine. Q: is influenza a contagious P : influenza a is considered to be contagious between people. generally the infectious agent may be transmitted by saliva, air, cough, fecal - oral route, surfaces, blood, needles, blood transfusions, sexual contact, mother to fetus, etc. influenza a, although infectious, is not a genetic disease. Extracted Evidence: influenza a is considered to be contagious between people. Synthetic Answer: yes\nTable 5: Case Study"}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank the MS-MARCO organizers for help in submissions."}], "references": [{"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1631\u20131640", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "O.K. Victor Li"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the International Conference on Learning Representations,", "citeRegEx": "Hill et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2016}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das"], "venue": "arXiv preprint arXiv:1611.01436,", "citeRegEx": "Lee et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "Lin.,? \\Q2004\\E", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "D. Christopher Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "MS MARCO: A human generated machine reading comprehension", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng"], "venue": "dataset. CoRR,", "citeRegEx": "Nguyen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["Matthew Richardson", "Christopher J.C. Burges", "Erin Renshaw"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Reasoning about entailment with neural attention", "author": ["Tim Rockt\u00e4schel", "Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Phil Blunsom"], "venue": null, "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2015}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "venue": "arXiv preprint arXiv:1611.01603,", "citeRegEx": "Seo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Learning natural language inference with LSTM", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "In NAACL HLT", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang"], "venue": "arXiv preprint arXiv:1608.07905,", "citeRegEx": "Wang and Jiang.,? \\Q2016\\E", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Wang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Fastqa: A simple and efficient neural architecture for question answering", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe"], "venue": "arXiv preprint arXiv:1703.04816,", "citeRegEx": "Weissenborn et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "venue": "arXiv preprint arXiv:1611.01604,", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Words or characters? fine-grained gating for reading", "author": ["Zhilin Yang", "Bhuwan Dhingra", "Ye Yuan", "Junjie Hu", "William W. Cohen", "Ruslan Salakhutdinov"], "venue": "comprehension. CoRR,", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "End-to-end reading comprehension with dynamic answer chunk ranking", "author": ["Yang Yu", "Wei Zhang", "Kazi Hasan", "Mo Yu", "Bing Xiang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1610.09996,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Selective encoding for abstractive sentence summarization", "author": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Ming Zhou"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Zhou et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 7, "context": "Machine reading comprehension (Nguyen et al., 2016), which attempts to enable machines to answer questions after reading a passage or a set of passages, attracts great attentions from both research and industry communities in recent years.", "startOffset": 30, "endOffset": 51}, {"referenceID": 18, "context": "Specifically, the extraction model is based on the R-Net (Wang et al., 2017).", "startOffset": 57, "endOffset": 76}, {"referenceID": 2, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 3, "context": ", 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage.", "startOffset": 15, "endOffset": 34}, {"referenceID": 10, "context": "Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging.", "startOffset": 49, "endOffset": 73}, {"referenceID": 7, "context": "Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages.", "startOffset": 31, "endOffset": 52}, {"referenceID": 6, "context": "The sequence-to-sequence model is widely-used in many tasks such as machine translation (Luong et al., 2015), parsing (Vinyals et al.", "startOffset": 88, "endOffset": 108}, {"referenceID": 23, "context": ", 2015b), summarization generation (Zhou et al., 2017) and response generation (Gu et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 1, "context": ", 2017) and response generation (Gu et al., 2016).", "startOffset": 32, "endOffset": 49}, {"referenceID": 4, "context": "Richardson et al. (2013) release MCTest whose goal is to select the best answer from four options given the question and the passage.", "startOffset": 0, "endOffset": 25}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer.", "startOffset": 15, "endOffset": 801}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al.", "startOffset": 15, "endOffset": 900}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually.", "startOffset": 15, "endOffset": 922}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer.", "startOffset": 15, "endOffset": 1024}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset.", "startOffset": 15, "endOffset": 1110}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al.", "startOffset": 15, "endOffset": 1437}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al. (2016) solve SQuAD by ranking continuous text spans within passage.", "startOffset": 15, "endOffset": 1459}, {"referenceID": 1, "context": "CNN/Daily-Mail(Hermann et al., 2015) and CBT(Hill et al., 2016) are the cloze-style datasets in which the goal is to predict the missing word (often a named entity) in a passage. Different from above datasets, the SQuAD dataset (Rajpurkar et al., 2016) whose answer can be much longer phrase is more challenging. The answer in SQuAD is a segment of text, or span, from the corresponding reading passage. Similar to the SQuAD, MS-MARCO (Nguyen et al., 2016) is the reading comprehension dataset which aims to answer the question given a set of passages. The answer in MS-MARCO is generated by human after reading all related passages and not necessarily sub-spans of the passages. To the best of our knowledge, the existing works on MS-MARCO dataset follow their methods on SQuAD. Wang & Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant co-attention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Wang et al. (2017) apply an additional gate to the attention-based recurrent networks and propose a self-matching mechanism for aggregating evidence from the whole passage, which achieves the state-of-the-art result on SQuAD dataset. Other works which only focus on the SQuAD dataset can also be applied on the MS-MARCO dataset. Yu et al. (2016) and Lee et al. (2016) solve SQuAD by ranking continuous text spans within passage. Yang et al. (2016) present a fine-grained gating mechanism to dynamically combine word-level and character-level representation and model the interaction between questions and passages.", "startOffset": 15, "endOffset": 1539}, {"referenceID": 0, "context": "We use Gated Recurrent Unit (GRU) (Cho et al., 2014) instead of basic RNN.", "startOffset": 34, "endOffset": 52}, {"referenceID": 12, "context": "Given question and passage representation {uQt }t=1 and {ut }t=1, Rockt\u00e4schel et al. (2015) propose generating sentence-pair representation {v t }t=1 via soft-alignment of words in the question and passage as follows: v t = GRU(v P t\u22121, ct) (3) where ct = att(u, [ut , v P t\u22121]) is an attention-pooling vector of the whole question (u ): sj = v tanh(W u u Q j +W P u u P t +W P v v P t\u22121) ai = exp(s t i)/\u03a3 m j=1exp(s t j) ct = \u03a3 m i=1a t iu Q i (4) Each passage representation v t dynamically incorporates aggregated matching information from the whole question.", "startOffset": 66, "endOffset": 92}, {"referenceID": 12, "context": "Given question and passage representation {uQt }t=1 and {ut }t=1, Rockt\u00e4schel et al. (2015) propose generating sentence-pair representation {v t }t=1 via soft-alignment of words in the question and passage as follows: v t = GRU(v P t\u22121, ct) (3) where ct = att(u, [ut , v P t\u22121]) is an attention-pooling vector of the whole question (u ): sj = v tanh(W u u Q j +W P u u P t +W P v v P t\u22121) ai = exp(s t i)/\u03a3 m j=1exp(s t j) ct = \u03a3 m i=1a t iu Q i (4) Each passage representation v t dynamically incorporates aggregated matching information from the whole question. Wang & Jiang (2016a) introduce match-LSTM, which takes uj as an additional input into the recurrent network.", "startOffset": 66, "endOffset": 585}, {"referenceID": 12, "context": "Given question and passage representation {uQt }t=1 and {ut }t=1, Rockt\u00e4schel et al. (2015) propose generating sentence-pair representation {v t }t=1 via soft-alignment of words in the question and passage as follows: v t = GRU(v P t\u22121, ct) (3) where ct = att(u, [ut , v P t\u22121]) is an attention-pooling vector of the whole question (u ): sj = v tanh(W u u Q j +W P u u P t +W P v v P t\u22121) ai = exp(s t i)/\u03a3 m j=1exp(s t j) ct = \u03a3 m i=1a t iu Q i (4) Each passage representation v t dynamically incorporates aggregated matching information from the whole question. Wang & Jiang (2016a) introduce match-LSTM, which takes uj as an additional input into the recurrent network. Wang et al. (2017) add another gate to the input ([ut , ct]) of RNN to determine the importance of passage parts.", "startOffset": 66, "endOffset": 692}, {"referenceID": 6, "context": "The context vector ct for current time step t is computed through the concatenate attention mechanism (Luong et al., 2015), which matches the current decoder state dt with each encoder hidden state", "startOffset": 102, "endOffset": 122}, {"referenceID": 7, "context": "We conduct our experiments on the MS-MARCO dataset (Nguyen et al., 2016).", "startOffset": 51, "endOffset": 72}, {"referenceID": 8, "context": "The answers are not necessarily sub-spans of the passages so that the metrics in the official tool of MS-MARCO evaluation are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004), which are widely used in many domains.", "startOffset": 131, "endOffset": 154}, {"referenceID": 5, "context": ", 2002) and ROUGE-L (Lin, 2004), which are widely used in many domains.", "startOffset": 20, "endOffset": 31}, {"referenceID": 9, "context": "For the answer extraction, we use 300-dimensional uncased pre-trained GloVe embeddings (Pennington et al., 2014)2 for both question and passage without update during training.", "startOffset": 87, "endOffset": 112}, {"referenceID": 14, "context": "We also apply dropout (Srivastava et al., 2014) between layers, with dropout rate 0.", "startOffset": 22, "endOffset": 47}, {"referenceID": 19, "context": "We also compare with other methods which report results in MS-MARCO dataset, including FastQAExt (Weissenborn et al., 2017), Prediction (Wang & Jiang, 2016b), and ReasoNet (Shen et al.", "startOffset": 97, "endOffset": 123}], "year": 2017, "abstractText": "Most existing works on machine reading comprehension are built under the answer extraction approach which predicts sub-spans from passages to answer questions. In this paper, we develop an extraction-then-generation framework for machine reading comprehension, in which the answer is generated from the extraction results. Specifically, we build the answer extraction model to predict the most important sub-spans from the passage as evidence, and develop the answer generation model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. We build the answer extraction model with state-of-the-art neural networks for reading comprehension, and the answer generation model with sequence-to-sequence neural networks. Experiments on the MS-MARCO dataset show that the generation based approach achieves better results than pure answer extraction.", "creator": "LaTeX with hyperref package"}}}