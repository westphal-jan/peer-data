{"id": "1706.04902", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2017", "title": "A survey of cross-lingual embedding models", "abstract": "cross - word embedding strategies allow us to project words from different contexts into arbitrary large embedding template. modelling facilitated the thus apply models dwelling on those with a lot in experience, f. cal. english to low - resource languages. in the cases, we will draw models. seek to learn para - lingual comprehension. we would check them based on possible meaning of approach and the nature our parallel data, they employ. subsequently, surveys will present challenges and describe how many test cross - lingual linguistic models.", "histories": [["v1", "Thu, 15 Jun 2017 14:46:56 GMT  (1827kb,D)", "http://arxiv.org/abs/1706.04902v1", "23 pages, 17 figures, 2 tables"], ["v2", "Wed, 18 Oct 2017 10:44:06 GMT  (2430kb,D)", "http://arxiv.org/abs/1706.04902v2", "Very heavily improved and revised version"]], "COMMENTS": "23 pages, 17 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder"], "accepted": false, "id": "1706.04902"}, "pdf": {"name": "1706.04902.pdf", "metadata": {"source": "CRF", "title": "A survey of cross-lingual embedding models\u2217", "authors": ["Sebastian Ruder"], "emails": ["ruder.sebastian@gmail.com"], "sections": [{"heading": null, "text": "In recent years, driven by the success of word embeddings, many models that learn accurate representations of words haven been proposed [Mikolov et al., 2013a, Pennington et al., 2014]. However, these models are generally restricted to capture representations of words in the language they were trained on. The availability of resources, training data, and benchmarks in English leads to a disproportionate focus on the English language and a negligence of the plethora of other languages that are spoken around the world. In our globalised society, where national borders increasingly blur, where the Internet gives everyone equal access to information, it is thus imperative that we do not only seek to eliminate bias pertaining to gender or race [Bolukbasi et al., 2016] inherent in our representations, but also aim to address our bias towards language.\nTo remedy this and level the linguistic playing field, we would like to leverage our existing knowledge in English to equip our models with the capability to process other languages. Perfect machine translation (MT) would allow this. However, we do not need to actually translate examples, as long as we are able to project examples into a common subspace such as the one in Figure 1.\nar X\niv :1\n70 6.\n04 90\n2v 1\n[ cs\n.C L\n] 1\n5 Ju\nn 20\n17\nUltimately, our goal is to learn a shared embedding space between words in all languages. Equipped with such a vector space, we are able to train our models on data in any language. By projecting examples available in one language into this space, our model simultaneously obtains the capability to perform predictions in all other languages (we are glossing over some considerations here; for these, refer to Section 7. This is the promise of cross-lingual embeddings.\nOver the course of this survey, we will give an overview of models and algorithms that have been used to come closer to the elusive goal of capturing the relations between words in multiple languages in a common embedding space.\nNote that while neural MT approaches implicitly learn a shared cross-lingual embedding space by optimizing for the MT objective, we will focus on models that explicitly learn cross-lingual word representations throughout this blog post. These methods generally do so at a much lower cost than MT and can be considered to be to MT what word embedding models [Mikolov et al., 2013a, Pennington et al., 2014] are to language modelling."}, {"heading": "1 Types of cross-lingual embedding models", "text": "In recent years, various models for learning cross-lingual representations have been proposed. In the following, we will order them by the type of approach that they employ. Note that while the nature of the parallel data used is equally discriminatory and has been shown to account for inter-model performance differences [Levy et al., 2017], we consider the type of approach more conducive to understanding the assumptions a model makes and \u2013 consequently \u2013 its advantages and deficiencies. Cross-lingual embedding models generally use four different approaches:\n1. Monolingual mapping: These models initially train monolingual word embeddings on large monolingual corpora. They then learn a linear mapping between monolingual representations in different languages to enable them to map unknown words from the source language to the target language.\n2. Pseudo-cross-lingual: These approaches create a pseudo-cross-lingual corpus by mixing contexts of different languages. They then train an off-the-shelf word embedding model on the created corpus. The intuition is that the cross-lingual contexts allow the learned representations to capture cross-lingual relations.\n3. Cross-lingual training: These models train their embeddings on a parallel corpus and optimize a cross-lingual constraint between embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space.\n4. Joint optimization: These approaches train their models on parallel (and optionally monolingual data). They jointly optimise a combination of monolingual and cross-lingual losses.\nIn terms of parallel data, methods may use different supervision signals that depend on the type of data used. These are, from most to least expensive:\n1. Word-aligned data: A parallel corpus with word alignments that is commonly used for machine translation; this is the most expensive type of parallel data to use.\n2. Sentence-aligned data: A parallel corpus without word alignments. If not otherwise specified, the model uses the Europarl corpus2 consisting of sentence-aligned text from the proceedings of the European parliament that is generally used for training Statistical Machine Translation models.\n3. Document-aligned data: A corpus containing documents in different languages. The documents can be topic-aligned (e.g. Wikipedia) or label/class-aligned (e.g. sentiment analysis and multi-class classification datasets).\n4. Lexicon: A bilingual or cross-lingual dictionary with pairs of translations between words in different languages.\n5. No parallel data: No parallel data whatsoever. Learning cross-lingual representations from only monolingual resources would enable zero-shot learning across languages.\n2http://www.statmt.org/europarl/\nTo make these distinctions clearer, we order cross-lingual embedding models by their approach and the type of parallel data they use in Table 1.\nAfter the discussion of cross-lingual embedding models, we will additionally look into how to incorporate visual information into word representations in Section 6, discuss the challenges that still remain in learning cross-lingual representations in Section 7, and finally summarize which models perform best and how to evaluate them in Section 7.6."}, {"heading": "2 Monolingual mapping", "text": "Methods that employ monolingual mapping train monolingual word representations independently on large monolingual corpora. They then seek to learn a transformation matrix that maps representations in one language to the representations of the other language. They usually employ a set of source word-target word pairs that are translations of each other, which are used as anchor words for learning the mapping.\nNote that all of the following methods presuppose that monolingual embedding spaces have already been trained. If not stated otherwise, these embedding spaces have been learned using the word2vec variants, skip-gram with negative sampling (SGNS) or continuous bag-of-words (CBOW) on large monolingual corpora."}, {"heading": "2.1 Linear projection", "text": "Mikolov et al. have popularised the notion that vector spaces can encode meaningful relations between words. In addition, they notice that the geometric relations that hold between words are similar across languages [Mikolov et al., 2013b], e.g. numbers and animals in English show a similar geometric constellation as their Spanish counterparts in Figure 2.\nThis suggests that it might be possible to transform one language\u2019s vector space into the space of another simply by utilising a linear projection with a transformation matrix W .\nIn order to achieve this, they translate the 5,000 most frequent words from the source language and use these 5,000 translations pairs as bilingual dictionary. They then learn W using stochastic gradient descent by minimising the distance between the previously learned monolingual representations of the source word wi that is transformed using W and its translation zi in the bilingual dictionary:\nmin W n\u2211 i=1 \u2016Wxi \u2212 zi\u20162."}, {"heading": "2.2 Projection via CCA", "text": "[Faruqui and Dyer, 2014] propose to use another technique to learn the linear mapping. They use canonical correlation analysis (CCA) to project words from two languages into a shared embedding space. Different to linear projection, CCA learns a transformation matrix for every language, as can be seen in Figure 3, where the transformation matrix V is used to project word representations from the embedding space \u03a3 to a new space \u03a3\u2217, while W transforms words from \u2126 to \u2126\u2217. Note that \u03a3\u2217 and \u2126\u2217 can be seen as the same shared embedding space.\nSimilar to linear projection, CCA also requires a number of translation pairs in \u03a3\u2032 and \u2126\u2032 whose correlation can be maximised. Faruqui and Dyer obtain these pairs by selecting for each source word the target word to which it has been aligned most often in a parallel corpus. Alternatively, they could have also used a bilingual dictionary. As CCA sorts the correlation vectors in V and W in descending order, Faruqui and Dyer perform experiments using only the top k correlated projection vectors and find that using the 80 % projection vectors with the highest correlation generally yields the highest performance.\nInterestingly, they find that using multilingual projection helps to separate synonyms and antonyms in the source language, as can be seen in Figure 4, where the unprotected antonyms of \u201cbeautiful\" are in two clusters in the top, whereas the CCA-projected vectors of the synonyms and antonyms form two distinct clusters in the bottom."}, {"heading": "2.3 Normalisation and orthogonal transformation", "text": "[Xing et al., 2015] notice inconsistencies in the linear projection method by [Mikolov et al., 2013b], which they set out to resolve. Recall that Mikolov et al. initially learn monolingual word embeddings. For this, they use the skip-gram objective, which is the following:\n1\nN N\u2211 i=1 \u2211 \u2212C\u2264j\u2264C,j 6=0 logP (wi+j | wi)\nwhere C is the context length and P (wi+j | wi) is computed using the softmax: P (wi+j | wi) = exp(cTwi+jcwi)\u2211 w exp(cTwcwi) .\nThey then learn a linear transformation between the two monolingual vector spaces with: min \u2211 i \u2016Wxi \u2212 zi\u20162\nwhere W is the projection matrix that should be learned and xi and zi are word vectors in the source and target language respectively that are similar in meaning.\nXing et al. argue that there is a mismatch between the objective function used to learn word representations (maximum likelihood based on inner product), the distance measure for word vectors (cosine similarity), and the objective function used to learn the linear transformation (mean squared error), which may lead to degradation in performance.\nThey subsequently propose a method to resolve each of these inconsistencies: In order to fix the mismatch between the inner product similarity measure cTwcw\u2032 during training and the cosine similarity measure cTwc \u2032 w\n\u2016cw\u2016\u2016cw\u2032\u2016 for testing, the inner product could also be used for testing. Cosine similarity,\nhowever, is used conventionally as an evaluation measure in NLP and generally performs better than the inner product. For this reason, they propose to normalise the word vectors to be unit length during training, which makes the inner product the same as cosine similarity and places all word vectors on a hypersphere as a side-effect, as can be seen in Figure 5.\nThey resolve the inconsistency between the cosine similarity measure now used in training and the mean squared error employed for learning the transformation by replacing the mean squared error with cosine similarity for learning the mapping, which yields:\nmax W \u2211 i (Wxi) T zi.\nFinally, in order to also normalise the projected vector Wxi to be unit length, they constrain W to be an orthogonal matrix by solving a separate optimisation problem."}, {"heading": "2.4 Max-margin and intruders", "text": "[Lazaridou et al., 2015a] identify another issue with the linear transformation objective of [Mikolov et al., 2013b]: They discover that using least-squares as objective for learning a projection matrix leads to hubness, i.e. some words tend to appear as nearest neighbours of many other words. To resolve this, they use a margin-based (max-margin) ranking loss [Collobert and Weston, 2008] to train the model to rank the correct translation vector yi of a source word xi that is projected to y\u0302i higher than any other target words yj : k\u2211 j 6=i max{0, \u03b3 + cos(y\u0302i, yi)\u2212 cos(y\u0302i, yj)}\nwhere k is the number of negative examples and \u03b3 is the margin.\nThey show that selecting max-margin over the least-squares loss consistently improves performance and reduces hubness. In addition, the choice of the negative examples, i.e. the target words compared to which the model should rank the correct translation higher, is important. They hypothesise that an informative negative example is an intruder (\u201ctruck\" in the example), i.e. it is near the current projected vector y\u0302i but far from the actual translation vector yi (\u201ccat\") as depicted in Figure 6.\nThese intruders should help the model identify cases where it is failing considerably to approximate the target function and should thus allow it to correct its behaviour. At every step of gradient descent, they compute sj = cos(y\u0302i, yj) \u2212 cos(yi, yj) for all vectors yt in the target embedding space with j 6= i and choose the vector with the largest sj as negative example for xi. Using intruders instead of random negative examples yields a small improvement of 2 percentage points on their comparison task."}, {"heading": "2.5 Multilingual CCA", "text": "[Ammar et al., 2016] extend the bilingual CCA projection method of [Faruqui and Dyer, 2014] to the multi-lingual setting using the English embedding space as the foundation for their multilingual embedding space.\nThey learn the two projection matrices for every other language with English. The transformation from each target language space \u2126 to the English embedding space \u03a3 can then be obtained by projecting the vectors in \u2126 into the CCA space \u2126\u2217 using the transformation matrix W as in Figure 3. As \u2126\u2217 and \u03a3\u2217 lie in the same space, vectors in \u03a3\u2217 can be projected into the English embedding space \u03a3 using the inverse of V ."}, {"heading": "2.6 Alignment-based projection", "text": "[Guo et al., 2015] propose another projection method that solely relies on word alignments. They count the number of times each word in the source language is aligned with each word in the target language in a parallel corpus and store these counts in an alignment matrix A. In order to project a word wi from its source representation v(wSi ) to its representation in the target embedding space v(wi)T in the target embedding space, they simply take the average of the embeddings of its translations v(wj)T weighted by their alignment probability with the source word:\nv(wi) T = \u2211 i,j\u2208A ci,j\u2211 j ci,j \u00b7 v(wj)T\nwhere ci,j is the number of times the ith source word has been aligned to the jth target word.\nThe problem with this method is that it only assigns embeddings for words that are aligned in the reference parallel corpus. Gou et al. thus propagate alignments from in-vocabulary to OOV words by using edit distance as a metric for morphological similarity. They set the projected vector of an OOV source word v(wTOOV ) as the average of the projected vectors of source words that are similar to it in edit distance:\nv(wTOOV ) = Avg(v(wT ))\nwhere C = {w |EditDist(wTOOV , w) \u2264 \u03c4}. They set the threshold \u03c4 empirically to 1. Even though this approach seems simplistic, they actually observe significant improvements over projection via CCA in their experiments."}, {"heading": "2.7 Orthogonal transformation, normalisation, and mean centering", "text": "The previous approaches have introduced models that imposed different constraints for mapping monolingual representations of different languages to each other. The relation between these methods and constraints, however, is not clear.\n[Artetxe et al., 2016] thus propose to generalise previous work on learning a linear transformation between monolingual vector spaces: Starting with the basic optimisation objective, they propose several constraints that should intuitively help to improve the quality of the learned cross-lingual representations. Recall that the linear transformation learned by [Mikolov et al., 2013b] aims to find a parameter matrix W that satisfies:\nargmin W \u2211 i \u2016Wxi \u2212 zi\u20162\nwhere xi and zi are similar words in the source and target language respectively.\nIf the performance of the embeddings on a monolingual evaluation task should not be degraded, the dot products need to be preserved after the mapping. This can be guaranteed by requiring W to be an orthogonal matrix.\nSecondly, in order to ensure that all embeddings contribute equally to the objective, embeddings in both languages can be normalised to be unit vectors:\nargmin W \u2211 i \u2016W xi \u2016xi\u2016 \u2212 zi \u2016zi\u2016 \u20162.\nAs the norm of an orthogonal matrix is 1, if W is orthogonal, we can add it to the denominator and move W to the numerator:\nargmin W \u2211 i \u2016 Wxi \u2016Wxi\u2016 \u2212 zi \u2016zi\u2016 \u20162.\nThrough expansion of the above binomial, we obtain:\nargmin W \u2211 i \u2016 Wxi \u2016Wxi\u2016 \u20162 + \u2016 zi \u2016zi|| \u20162 \u2212 2 Wxi \u2016Wxi\u2016 T zi \u2016zi\u2016 .\nAs the norm of a unit vector is 1 the first two terms reduce to 1, which leaves us with the following:\nargmin W \u2211 i 2\u2212 2 Wxi \u2016Wxi\u2016 T zi \u2016zi\u2016 ).\nThe latter term now is just the cosine similarity of Wxi and zi:\nargmin W \u2211 i 2\u2212 2 cos(Wxi, zi).\nAs we are interested in finding parameters W that minimise our objective, we can remove the constants above:\nargmin W \u2211 i \u2212 cos(Wxi, zi).\nMinimising the sum of negative cosine similarities is then equal to maximising the sum of cosine similarities, which gives us the following:\nargmax W \u2211 i cos(Wxi, zi).\nThis is equal to the objective by [Xing et al., 2015], although they motivated it via an inconsistency of the objectives.\nFinally, Artetxe et al. argue that two randomly selected words are generally expected not to be similar. For this reason, the cosine of their embeddings in any dimension \u2013 as well as their cosine similarity \u2013 should be zero. They capture this intuition by performing dimension-wise mean centering with a centering matrix Cm:\nargmin W \u2211 i ||CmWxi \u2212 Cmzi||2.\nThis reduces to maximizing the sum of dimension-wise covariance as long as W is orthogonal similar as above:\nargmax W \u2211 i cov(Wxi, zi).\nInterestingly, the method by [Faruqui and Dyer, 2014] is similar to this objective, as CCA maximizes the dimension-wise covariance of both projections. This is equivalent to the single projection here, as it is constrained to be orthogonal. The only difference is that, while CCA changes the monolingual embeddings so that different dimensions have the same variance and are uncorrelated \u2013 which might degrade performance \u2013 Artetxe et al. enforce monolingual invariance."}, {"heading": "2.8 Hybrid mapping with symmetric seed lexicon", "text": "The previous mapping approaches used a bilingual dictionary as inherent component of their model, but did not pay much attention to the quality of the dictionary entries, using either automatic translations of frequent words or word alignments of all words.\n[Vulic\u0301 and Korhonen, 2016] in turn emphasise the role of the seed lexicon that is used for learning the projection matrix. They propose a hybrid model that initially learns a first shared bilingual embedding space based on an existing cross-lingual embedding model. They then use this initial vector space to obtain translations for a list of frequent source words by projecting them into the space and using the nearest neighbour in the target language as translation. With these translation pairs as seed words, they learn a projection matrix analogously to [Mikolov et al., 2013b]. In addition, they propose a symmetry constraint, which enforces that words are only included if their projections are neighbours of each other in the first embedding space. Additionally, one can retain pairs whose second nearest neighbours are less similar than the first nearest neighbours up to some threshold. They run experiments showing that their model with the symmetry constraint outperforms comparison models and that a small threshold of 0.01 or 0.025 leads to slightly improved performance."}, {"heading": "2.9 Adversarial auto-encoder", "text": "All previous approaches to learning a transformation matrix between monolingual representations in different languages require either a dictionary or word alignments as a source of parallel data.\n[Barone, 2016], in contrast, seeks to get closer to the elusive goal of creating cross-lingual representations without parallel data. He proposes to use an adversarial auto-encoder to transform source embeddings into the target embedding space. The auto-encoder is then trained to reconstruct the source embeddings, while the discriminator is trained to differentiate the projected source embeddings from the actual target embeddings as in Figure 7.\nWhile intriguing, learning a transformation between languages without any parallel data at all seems unfeasible at this point. However, future approaches that aim to learn a mapping with fewer and fewer parallel data may bring us closer to this goal.\nMore generally, however, it remains unclear if a projection can reliably transform the embedding space of one language into the embedding space of another language. Additionally, the reliance on lexicon data or word alignment information is expensive."}, {"heading": "3 Pseudo-cross-lingual", "text": "The second type of cross-lingual models seeks to construct a pseudo-cross-lingual corpus that captures interactions between the words in different languages. Most approaches aim to identify words that can be translated to each other in monolingual corpora of different languages and replace these with placeholders to ensure that translations of the same word have the same vector representation."}, {"heading": "3.1 Mapping of translations to same representation", "text": "[Xiao and Guo, 2014] propose the first pseudo-cross-lingual method that leverages translation pairs: They first translate all words that appear in the source language corpus into the target language using Wiktionary. As these translation pairs are still very noisy, they filter them by removing polysemous words in the source and target language and translations that do not appear in the target language corpus. From this bilingual dictionary, they now create a joint vocabulary, in which each translation pair has the same vector representation.\nFor training, they use the margin-based ranking loss of [Collobert and Weston, 2008] to rank correct word windows higher than corrupted ones, where the middle word is replaced by an arbitrary word. In contrast to the subsequent methods, they do not construct a pseudo-cross-lingual corpus explicitly. Instead, they feed windows of both the source and target corpus into the model during training, thereby essentially interpolating source and target language. It is thus most likely that, for ease of training, the authors replace translation pairs in source and target corpus with a placeholder to ensure a common vector representation, similar to the procedure of subsequent models."}, {"heading": "3.2 Random translation replacement", "text": "[Gouws and S\u00f8gaard, 2015] in turn explicitly create a pseudo-cross-lingual corpus: They leverage translation pairs of words in the source and in the target language obtained via Google Translate. They concatenate the source and target corpus and replace each word that is part of a translation pair with its translation equivalent with a probability of 50%. They then train CBOW on this corpus. It is interesting to note that they also experiment with replacing words not based on translation but part-of-speech equivalence, i.e. words with the same part-of-speech in different languages will be replaced with one another. While replacement based on part-of-speech leads to small improvements for cross-lingual part-of-speech tagging, replacement based on translation equivalences yields even better performance for the task."}, {"heading": "3.3 On-the-fly replacement and polysemy handling", "text": "[Duong et al., 2016] propose a similar approach to [Gouws and S\u00f8gaard, 2015]. They also use CBOW, which predicts the centre word in a window given the surrounding words. Instead of randomly replacing every word in the corpus with its translation during pre-processing, they replace each centre word with a translation on-the-fly during training.\nIn addition to past approaches, they also seek to handle polysemy explicitly by proposing an EMinspired method that chooses as replacement the translation w\u0304i whose representation is most similar to the combination of the representations of the source word vwi and the context vector hi:\nw\u0304i = argmaxw \u2208 dict(wi) cos(vwi + hi, vw)\nwhere dict(wi) contains the translations of wi.\nThey then jointly learn to predict both the words and their appropriate translations. They use PanLex as bilingual dictionary, which covers around 1,300 language with about 12 million expressions. Consequently, translations are high coverage but often noisy."}, {"heading": "3.4 Multilingual cluster", "text": "[Ammar et al., 2016] propose another approach that is similar to the previous method by [Gouws and S\u00f8gaard, 2015]: They use bilingual dictionaries to find clusters of synonymous words in different languages. They then concatenate the monolingual corpora of different languages and\nreplace tokens in the same cluster with the cluster ID. They then train SGNS on the concatenated corpus."}, {"heading": "3.5 Document merge and shuffle", "text": "The previous methods all use a bilingual dictionary or a translation tool as a source of translation pairs that can be used for replacement.\n[Vulic\u0301 and Moens, 2016] present a model that does without translation pairs and learns cross-lingual embeddings only from document-aligned data. In contrast to the previous methods, the authors propose not to merge two monolingual corpora but two aligned documents of different languages into a pseudo-bilingual document.\nThey concatenate the documents and then shuffle them by randomly permutating the words. The intuition is that as most methods rely on learning word embeddings based on their context, shuffling the documents would lead to bilingual contexts for each word that will enable the creation of a robust embedding space. As shuffling is necessarily random, however, it might lead to sub-optimal configurations. For this reason, they propose another merging strategy that assumes that the structures of the document are similar: They then alternatingly insert words from each language into the pseudo-bilingual document in the order in which they appear in their monolingual document and based on the mono-lingual documents\u2019 length ratio.\nWhile pseudo-cross-lingual approaches are attractive due to their simplicity and ease of implementation, relying on naive replacement and permutation does not allow them to capture more sophisticated facets of cross-lingual relations."}, {"heading": "4 Cross-lingual training", "text": "Cross-lingual training approaches focus exclusively on optimising the cross-lingual objective. These approaches typically rely on sentence alignments rather than a bilingual lexicon and require a parallel corpus for training."}, {"heading": "4.1 Bilingual compositional sentence model", "text": "The first approach that optimizes only a cross-lingual objective is the bilingual compositional sentence model by [Hermann and Blunsom, 2013]. They train two models to produce sentence representations of aligned sentences in two languages and use the distance between the two sentence representations as objective. They minimise the following loss:\nEdist(a, b) = \u2016aroot \u2212 broot\u20162\nwhere aroot and broot are the representations of two aligned sentences from different languages. They compose aroot and broot simply as the sum of the embeddings of the words in the corresponding sentence. The full model is depicted in Figure 8.\nThey train the model then to output a higher score for correct translations than for randomly sampled incorrect translations using the max-margin hinge loss of [Collobert and Weston, 2008]."}, {"heading": "4.2 Bilingual bag-of-words autoencoder", "text": "Instead of minimising the distance between two sentence representations in different languages, [Lauly et al., 2013] aim to reconstruct the target sentence from the original source sentence. They start with a monolingual autoencoder that encodes an input sentence as a sum of its word embeddings and tries to reconstruct the original source sentence. For efficient reconstruction, they opt for a tree-based decoder that is similar to a hierarchical softmax. They then augment this autoencoder with a second decoder that reconstructs the aligned target sentence from the representation of the source sentence as in Figure 9.\nEncoders and decoders have language-specific parameters. For an aligned sentence pair, they then train the model with four reconstruction losses: for each of the two sentences, they reconstruct from the sentence to itself and to its equivalent in the other language."}, {"heading": "4.3 Distributed word alignment", "text": "While the previous approaches required word alignments as a prerequisite for learning cross-lingual embeddings, [Koc\u030cisk\u00fd et al., 2014] simultaneously learn word embeddings and alignments. Their model, Distributed Word Alignment, combines a distributed version of FastAlign [Dyer et al., 2013] with a language model. Similar to other bilingual approaches, they use the word in the source language sentence of an aligned sentence pair to predict the word in the target language sentence.\nThey replace the standard multinomial translation probability of FastAlign with an energy function that tries to bring the representation of a target word f close to the sum of the context words around the word ei in the source sentence:\nE(f, ei) = \u2212( k\u2211\ns=\u2212k rTei+sTs)rf \u2212 b T r rf \u2212 bf\nwhere rei+s and rf are vector representations for source and target words, Ts is a projection matrix, and br and bf are representation and target biases respectively. For calculating the translation probability p(f |ei), we then simply need to apply the softmax to the translation probabilities between the source word and all words in the target language.\nIn addition, the authors speed up training by using a class factorisation strategy similar to the hierarchical softmax and predict frequency-based class representations instead of word representations. For training, they also use EM but fix the alignment counts learned by FastAlign that was initially trained for 5 epochs during the E-step and optimise the translation probabilities in the M-step only."}, {"heading": "4.4 Bilingual compositional document model", "text": "[Hermann and Blunsom, 2014] extend their approach [Hermann and Blunsom, 2013] to documents, by applying their composition and objective function recursively to compose sentences into documents. First, sentence representations are computed as in Section 4.1. These sentence representations are then fed into a document-level compositional vector model, which integrates the sentence representations in the same way as can be seen in Figure 10.\nThe advantage of this method is that weaker supervision in the form of document-level alignment can be used instead of or in conjunction with sentence-level alignment. The authors run experiments both on Europarl as well as on a newly created corpus of multilingual aligned TED talk transcriptions and find that the document signal helps considerably.\nIn addition, they propose another composition function that \u2013 instead of summing the representations \u2013 applies a non-linearity to bigram pairs: f(x) = n\u2211\ni=1\ntanh(xi\u22121 + xi)\nThey find that this composition slightly outperforms addition, but underperforms it on smaller training datasets."}, {"heading": "4.5 Bag-of-words autoencoder with correlation", "text": "[Chandar et al., 2014] extend the approach by [Lauly et al., 2013] in two ways: Instead of using a tree-based decoder for calculating the reconstruction loss, they reconstruct a sparse binary vector of word occurrences as in Figure 11. Due to the high-dimensionality of the binary bag-of-words vector, reconstruction is slower. As they perform training using mini-batch gradient descent, where each mini-batch consists of adjacent sentences, they propose to merge the bags-of-words of the mini-batch into a single bag-of-words and to perform updates based on the merged bag-of-words. They find that this yields good performance and even outperforms the tree-based decoder.\nSecondly, they propose to add a term cor(a(x), a(y)) to the objective function that encourages correlation between the representations a(x) , a(y) of the source and target language respectively by summing the scalar correlations between all dimensions of the two vectors."}, {"heading": "4.6 Bilingual paragraph vectors", "text": "Similar to the previous methods, [Pham et al., 2015] learn sentence representations as a means for learning cross-lingual word embeddings. They extend paragraph vectors [Le and Mikolov, 2014] to the multilingual setting by forcing aligned sentences of different languages to share the same vector representation as in Figure 12 where sent is the shared sentence representation. The shared sentence representation is concatenated with the sum of the previous N words in the sentence and the model is trained to predict the next word in the sentence.\nThe authors use a hierarchical softmax to speed-up training. As the model only learns representations for the sentences it has seen during training, at test time for an unknown sentence, the sentence representation is randomly initialised and the model is trained to predict only the words in the sentence. Only the sentence vector is updated, while the other model parameters are frozen."}, {"heading": "4.7 Translation-invariant LSA", "text": "Besides word embedding models such as skip-gram, matrix factorisation approaches have historically been used successfully to learn representations of words. One of the most popular methods is LSA, which [Gardner et al., 2015] extend as translation-invariant LSA to to learn cross-lingual word embeddings. They factorise a multilingual co-occurrence matrix with the restriction that it should be invariant to translation, i.e. it should stay the same if multiplied with the respective word or context dictionary."}, {"heading": "4.8 Inverted indexing on Wikipedia", "text": "All previous approaches to learn cross-lingual representations have been based on some form of language model or matrix factorisation. In contrast, [S\u00f8gaard et al., 2015] propose an approach that does without any of these methods, but instead relies on the structure of the multilingual knowledge base Wikipedia, which they exploit by inverted indexing. Their method is based on the intuition that similar words will be used to describe the same concepts across different languages.\nIn Wikipedia, articles in multiple languages deal with the same concept. We would typically represent every concept with the terms that are used to describe it across different languages. To learn crosslingual word representations, we can now simply invert the index and instead represent a word by the Wikipedia concepts it is used to describe. This way, we are directly provided with cross-lingual representations of words without performing any optimisation whatsoever. As a post-processing step, we can perform dimensionality reduction on the produced word representations.\nWhile the previous methods are able to make effective use of parallel sentence and documents to learn cross-lingual word representations, they neglect the monolingual quality of the learned\nrepresentations. Ultimately, we do not only want to embed languages into a shared embedding space, but also want the monolingual representations do well on the task at hand."}, {"heading": "5 Joint optimisation", "text": "Models that use joint optimisation aim to do exactly this: They not only consider a cross-lingual constraint, but jointly optimize mono-lingual and cross-lingual objectives.\nIn practice, for two languages l1 and l2, these models optimize a monolingual loss M for each language and one or multiple terms \u2126 that regularize the transfer from language l1 to l2 (and vice versa):\nMl1 +Ml2 + \u03bb(\u2126l1\u2192l2 + \u2126l2\u2192l1) where \u03bb is an interpolation parameter that adjusts the impact of the cross-lingual regularization."}, {"heading": "5.1 Multi-task language model", "text": "The first jointly optimised model for learning cross-lingual representations was created by [Klementiev et al., 2012]. They train a neural language model for each language and jointly optimise the monolingual maximum likelihood objective of each language model with a word-alignment based MT regularization term as the cross-lingual objective. The monolingual objective is thus to maximise the probability of the current word wt given its n surrounding words:\nM = log P (wt | wt\u2212n+1:t\u22121). This is optimised using the classic language model of [Bengio et al., 2003]. The cross-lingual regularisation term in turn encourages the representations of words that are often aligned to each other to be similar:\n\u2126 = 1\n2 cT (A\u2297 I)c\nwhere A is the matrix capturing alignment scores, I is the identity matrix,\u2297 is the Kronecker product, and c is the representation of word wt."}, {"heading": "5.2 Bilingual matrix factorisation", "text": "[Zou et al., 2013] use a matrix factorisation approach in the spirit of GloVe [Pennington et al., 2014] to learn cross-lingual word representations for English and Chinese. They create two alignment matrices Aen\u2192zh and Azh\u2192en using alignment counts automatically learned from the Chinese Gigaword corpus. In Aen\u2192zh, each element aij contains the number of times the i-th Chinese word was aligned with the j-th English word, with each row normalised to sum to 1. Intuitively, if a word in the source language is only aligned with one word in the target language, then those words should have the same representation. If the target word is aligned with more than one source word, then its representation should be a combination of the representations of its aligned words. Consequently, the authors represent the embeddings in the target language as the product of the source embeddings Ven and their corresponding alignment counts Aen\u2192zh. They then minimise the squared difference between these two terms:\n\u2126en\u2192zh = ||Vzh \u2212Aen\u2192zhVen||2\n\u2126zh\u2192en = ||Ven \u2212Azh\u2192enVzh||2\nwhere Ven and Vzh are the embedding matrices of the English and Chinese word embeddings respectively.\nThey employ the max-margin hinge loss objective by [Collobert and Weston, 2008] as monolingual objectiveM and train the English and Chinese word embeddings to minimise the corresponding objective above together with a monolingual objective. For instance, for English, the training objective is:\nMen + \u03bb\u2126zh\u2192en.\nIt is interesting to observe that the authors learn embeddings using a curriculum, training different frequency bands of the vocabulary at a time. The entire training process takes 19 days."}, {"heading": "5.3 Bilingual skip-gram", "text": "[Luong et al., 2015] in turn extend skip-gram to the cross-lingual setting and use the skip-gram objectives as monolingual and cross-lingual objectives. Rather than just predicting the surrounding words in the source language, they use the words in the source language to additionally predict their aligned words in the target language as in Figure 13.\nFor this, they require word alignment information. They propose two ways to predict aligned words: For their first method, they automatically learn alignment information; if a word is unaligned, the alignments of its neighbours are used for prediction. In their second method, they assume that words in the source and target sentence are monotonically aligned, with each source word at position i being aligned to the target word at position i \u00b7 T/S where S and T are the source and target sentence lengths. They find that a simple monotonic alignment is comparable to the unsupervisedly learned alignment in performance."}, {"heading": "5.4 Bilingual bag-of-words without word alignments", "text": "[Gouws et al., 2015] propose a Bilingual Bag-of-Words without Word Alignments (BilBOWA) that leverages additional monolingual data. They use the skip-gram objective as a monolingual objective and a novel sampled l2 loss as cross-lingual regularizer as in Figure 14.\nMore precisely, instead of relying on expensive word alignments, they simply assume that each word in a source sentence is aligned with every word in the target sentence under a uniform alignment model. Thus, instead of minimising the distance between words that were aligned to each other, they minimise the distance between the means of the word representations in the aligned sentences, which is shown in Figure 15, where se and sf are the sentences in source and target language respectively.\nThe cross-lingual objective in the BilBOWA model is thus:\n\u2126 = \u2016 1 m m\u2211 wi\u2208sl1 rl1i \u2212 1 n n\u2211 wj\u2208sl2 rl2j \u20162\nwhere ri and rj are the word embeddings of word wi and wj in each sentence sl1 and sl2 of length m and n in languages l1 and l2 respectively."}, {"heading": "5.5 Bilingual skip-gram without word alignments", "text": "Another extension of skip-gram to learning cross-lingual representations is proposed by [Coulmance et al., 2015]. They also use the regular skip-gram objective as monolingual objective. For the cross-lingual objective, they make a similar assumption as [Gouws et al., 2015] by supposing that every word in the source sentence is uniformly aligned to every word in the target sentence.\nUnder the skip-gram formulation, they treat every word in the target sentence as context of every word in the source sentence and thus train their model to predict all words in the target sentence with the following skip-gram objective:\n\u2126e,f = \u2211\n(sl1 ,sl2 )\u2208Cl1,l2 \u2211 wl1\u2208sl1 \u2211 cl2\u2208sl2 \u2212log \u03c3(wl1 , cl2)\nwhere s is the sentence in the respective language, C is the sentence-aligned corpus, w are word and c are context representations respectively, and \u2212 log \u03c3(\u00b7) is the standard skip-gram loss function.\nAs the cross-lingual objective is asymmetric, they use one cross-lingual objective for the sourceto-target and another one for the target-to-source direction. The complete Trans-gram objective including two monolingual and two cross-lingual skip-gram objectives is displayed in Figure 16."}, {"heading": "5.6 Joint matrix factorisation", "text": "[Shi et al., 2015] use a joint matrix factorisation model to learn cross-lingual representations. In contrast to [Zou et al., 2013], they also take into account additional monolingual data. Similar to the former, they also use the GloVe objective [Pennington et al., 2014] as monolingual objective:\nMli = \u2211 j,k f(X lijk)(w li j \u00b7 c li k + b li wj + b li ck + bli \u2212M lijk)\nwherewlij and c li k are the embeddings andM li jk the PMI value of a word-context pair (j, k) in language li, while bliwj and b li ck and bli are the word-specific and language-specific bias terms respectively.\nThey then place cross-lingual constraints on the monolingual representations as can be seen in Figure 17. The authors propose two cross-lingual regularisation objectives: The first one is based on calculating cross-lingual co-occurrence counts. These co-occurrences can be calculated without alignment information using a uniform alignment model as in [Gouws et al., 2015]. Alternatively, co-occurrence counts can also be calculated by leveraging automatically learned word alignments. The co-occurrence counts are then stored in a matrix Xbi where every entry Xbijk contains the number of times the source word j occurred with the target word k in an aligned sentence pair in the parallel corpus. For optimisation, a PMI matrix M bijk can be calculated based on the co-occurrence counts in Xbi. This matrix can again be factorised as in the GloVe objective, where now the context word representation clik is replaced with the representation of the word in the target language w l2 k :\n\u2126 = \u2211\nj\u2208V l1 ,k\u2208V l2 f(X l1jk)(w l1 j \u00b7 w l2 k + b l1 wj + b l2 wk + bbi \u2212M bijk).\nThe second cross-lingual regularisation term they propose leverages the translation probabilities produced by a machine translation system and involves minimising the distances of the representations of related words in the two languages weighted by their similarities:\n\u2126 = \u2211\nj\u2208V l1 ,k\u2208V l2 sim(j, k) \u00b7 ||wl1j \u2212 w l2 k ||2\nwhere j and k are words in the source and target language respectively and sim(j, k) is their translation probability."}, {"heading": "5.7 Bilingual sparse representations", "text": "[Vyas and Carpuat, 2016] propose another method based on matrix factorisation that \u2013 in contrast to previous approaches \u2013 allows learning sparse cross-lingual representations. They first independently train two monolingual word representations Xe and Xf in two different languages using GloVe [Pennington et al., 2014] on two large monolingual corpora.\nThey then learn monolingual sparse representations from these dense representations by decomposing X into two matrices A and D such that the l2 reconstruction error is minimised, with an additional constraint on A for sparsity: Mli = vli\u2211 i=1 \u2016AliiDTli \u2212Xlii\u2016+ \u03bbli\u2016Alii\u20161\nwhere vli is the number of dense word representations in language li.\nThe above equation, however, only creates sparse monolingual embeddings. To learn bilingual embeddings, they add another constraint based on automatically learned word alignment that minimises the l2 reconstruction error between words that were strongly aligned to each other: \u2126 = vl1\u2211 i=1 vl2\u2211 j=1 1 2 \u03bbxSij\u2016Al1i \u2212Al2j\u201622\nwhere S is the alignment matrix where each entry Sij contains the alignment score of source word Xl1i with target word Xl2j .\nThe complete objective function is thus the following:\nMl1 +Ml2 + \u2126."}, {"heading": "5.8 Bilingual paragraph vectors (without parallel data)", "text": "[Mogadala and Rettinger, 2016] use an approach similar to [Pham et al., 2015], but extend it to also work without parallel data. They use the paragraph vectors objective as monolingual objectiveM. They jointly optimise this objective together with a cross-lingual regularization function \u2126 that encourages the representations of words in languages l1 and l2 to be close to each other.\nTheir main innovation is that the cross-lingual regularizer \u2126 is adjusted based on the nature of the training corpus. In addition to regularising the mean of word vectors in a sentence to be close to the mean of word vectors in the aligned sentence similar to [Gouws et al., 2015] (the second term in the below equation), they also regularise the paragraph vectors SP l1 and SP l2 of aligned sentences in languages l1 and l2 to be close to each other. The complete cross-lingual objective then uses elastic net regularization to combine both terms:\n\u2126 = \u03b1||SP l1j \u2212 SP l2 j ||2 + (1\u2212 \u03b1)\n1\nm m\u2211 wi\u2208s l1 j W l1i \u2212 1 n n\u2211 wk\u2208s l2 j W l2k\nwhere W l1i and W l2 k are the word embeddings of word wi and wk in each sentence sj of length m and n in languages l1 and l2 respectively.\nTo leverage data that is not sentence-aligned, but where an alignment is still present on the document level, they propose a two-step approach: They use Procrustes analysis3, a method for statistical shape analysis, to find for each document in language l1 the most similar document in language l2. This is done by first learning monolingual representations of the documents in each language using paragraph vectors on each corpus. Subsequently, Procrustes analysis aims to learn a transformation between the two vector spaces by translating, rotating, and scaling the embeddings in the first space until they most closely align to the document representations in the second space. In the second step, they then simply use the previously described method to learn cross-lingual word representations from the alignment documents, this time treating the entire documents as paragraphs."}, {"heading": "6 Incorporating visual information", "text": "A recent branch of research proposes to incorporate visual information to improve the performance of monolingual [Lazaridou et al., 2015b] or cross-lingual [Vulic\u0301 et al., 2016] representations. These methods show good performance on comparison tasks. They additionally demonstrate application for zero-shot learning and might thus ultimately be helpful in learning cross-lingual representations without (linguistic) parallel data."}, {"heading": "7 Challenges", "text": ""}, {"heading": "7.1 Functional modeling", "text": "Models for learning cross-linguistic representations share weaknesses with other vector space models of language: While they are very good at modeling the conceptual aspect of meaning evaluated in\n3https://en.wikipedia.org/wiki/Procrustes_analysis\nword similarity tasks, they fail to properly model the functional aspect of meaning, e.g. to distinguish whether one remarks \u201cGive me a pencil\" or \u201cGive me that pencil\"."}, {"heading": "7.2 Word order", "text": "Secondly, due to the reliance on bag-of-words representations, current models for learning crosslingual word embeddings completely ignore word order. Models that are oblivious to word order, for instance, assign to the following sentence pair [Landauer and Dumais, 1997] the exact same representation as they contain the same set of words, even though they are completely different in meaning:\n- \u201cThat day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.\" - \u201cIt was not the sales manager, who hit the bottle that day, but the office worker with a serious drinking problem\"."}, {"heading": "7.3 Compositionality", "text": "Most approaches for learning cross-lingual representations focus on word representations. These approaches are not able to easily compose word representations to form representations of sentences and documents. Even approaches that learn jointly learn word and sentence representations do so by via simple summation of words in the sentence. In the future, it will be interesting to see if LSTMs or CNNs that can form more composable sentence representations can be applied efficiently to learn cross-lingual representations."}, {"heading": "7.4 Polysemy", "text": "While conflating multiple senses of a word is already problematic for learning mono-lingual word representations, this issue is amplified in a cross-lingual embedding space: Monosemous words in one language might align with polysemous words in another language and thus fail to capture the entirety of the cross-lingual relations. There has already been promising work on learning monolingual multi-sense embeddings. We hypothesize that learning cross-lingual multi-sense embeddings will become increasingly relevant, as it enables us to capture more fine-grained cross-lingual meaning."}, {"heading": "7.5 Feasibility", "text": "The final challenge pertains to the feasibility of the venture of learning cross-lingual embeddings itself: Languages are incredibly complex, human artefacts. Learning a monolingual embedding space is already difficult; sharing such a vector space between two languages and expecting that inter-language and intra-language relations are reliably reflected then seems utopian. Additionally, some languages show linguistic features, which other languages lack. The ease of constructing a shared embedding space between languages and consequently the success of cross-lingual transfer is intuitively proportional to the similarity of the languages: An embedding space shared between Spanish and Portuguese tends to capture more linguistic nuances of meaning than an embedding space populated with English and Chinese representations. Furthermore, if two languages are too dissimilar, cross-linguistic transfer might not be possible at all \u2013 similar to the negative transfer that occurs in domain adaptation between very dissimilar domains."}, {"heading": "7.6 Evaluation", "text": "Having surveyed models to learn cross-lingual word representations, we would now like to know which is the best method to use for the task we care about. Cross-lingual representation models have been evaluated on a wide range of tasks such as cross-lingual document classification (CLDC), Machine Translation (MT), word similarity, as well as cross-lingual variations of the following tasks: named entity recognition, part-of-speech tagging, super sense tagging, dependency parsing, and dictionary induction. In the context of the CLDC evaluation setup by [Klementiev et al., 2012] 40-dimensional cross-lingual word embeddings are learned to classify documents in one language and evaluated on the documents of another language. As CLDC is among the most widely used, we show below exemplarily the evaluation table of [Mogadala and Rettinger, 2016] for this task:\nThe results in Table 2, however, should not be considered as representative of the general performance of cross-lingual embedding models as different methods tend to well on different tasks depending on the type of approach and the type of data used. [Upadhyay et al., 2016]] evaluate cross-lingual embedding models that require different forms of supervision on various tasks. They find that on word similarity datasets, models that require cheaper forms of supervision (sentence-aligned and document-aligned data) are almost as good as models with more expensive supervision in the form of word alignments. For cross-lingual classification and dictionary induction, more informative supervision is better. Finally, for parsing, models with word-level alignment are able to capture syntax more accurately and thus perform better overall.\nThe findings by Upadhyay et al. are further proof for the intuition that the choice of the data is important. [Levy et al., 2017] go even further than this in comparing models for learning cross-lingual word representations to traditional alignment models on dictionary induction and word alignment tasks. They argue that whether or not an algorithm uses a particular feature set is more important than the choice of the algorithm. In their experiments, using sentence ids, i.e. creating a sentence\u2019s language-independent representation (for instance with doc2vec) achieves better results than just using the source and target words.\nFinally, to facilitate evaluation of cross-lingual word embeddings, [Ammar et al., 2016] make a website4 available where learned representations can be uploaded and automatically evaluated on a wide range of tasks."}, {"heading": "8 Conclusion", "text": "Models that allow us to learn cross-lingual representations have already been useful in a variety of tasks such as Machine Translation (decoding and evaluation), automated bilingual dictionary generation, cross-lingual information retrieval, parallel corpus extraction and generation, as well as cross-language plagiarism detection. It will be interesting to see what further progress the future will bring."}], "references": [{"title": "Massively Multilingual Word Embeddings", "author": ["Ammar et al", "W. 2016] Ammar", "G. Mulcaire", "Y. Tsvetkov", "G. Lample", "C. Dyer", "N.A. Smith"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Learning principled bilingual mappings of word embeddings while preserving monolingual invariance", "author": ["Artetxe et al", "M. 2016] Artetxe", "G. Labaka", "E. Agirre"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Towards cross-lingual distributed representations without parallel text trained with adversarial autoencoders", "author": ["Barone", "A.V.M. 2016] Barone"], "venue": "Proceedings of the 1st Workshop on Representation Learning for NLP,", "citeRegEx": "Barone and Barone,? \\Q2016\\E", "shortCiteRegEx": "Barone and Barone", "year": 2016}, {"title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "author": ["Bolukbasi et al", "T. 2016] Bolukbasi", "Chang", "K.-W", "J. Zou", "V. Saligrama", "A. Kalai"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "An Autoencoder Approach to Learning Bilingual Word Representations", "author": ["Chandar et al", "S. 2014] Chandar", "S. Lauly", "H. Larochelle", "M.M. Khapra", "B. Ravindran", "V. Raykar", "A. Saha"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A unified architecture for natural language processing", "author": ["Collobert", "Weston", "R. 2008] Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning ICML", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Trans-gram, Fast Cross-lingual Word-embeddings", "author": ["Coulmance et al", "J. 2015] Coulmance", "Marty", "J.-M", "G. Wenzek", "A. Benhalloum"], "venue": "EMNLP 2015,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning Crosslingual Word Embeddings without Bilingual Corpora", "author": ["Duong et al", "L. 2016] Duong", "H. Kanayama", "T. Ma", "S. Bird", "T. Cohn"], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16)", "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "A simple, fast, and effective parameterization of IBM model 2", "author": ["Dyer et al", "C. 2013] Dyer", "V. Chahuneau", "N. Smith"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Improving Vector Space Word Representations Using Multilingual Correlation", "author": ["Faruqui", "Dyer", "M. 2014] Faruqui", "C. Dyer"], "venue": "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Translation Invariant Word Embeddings", "author": ["Gardner et al", "M. 2015] Gardner", "K. Huang", "E. Paplexakis", "X. Fu", "P. Talukdar", "C. Faloutsos", "N. Sidiropoulos", "T. Mitchell"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "BilBOWA: Fast Bilingual Distributed Representations without Word Alignments", "author": ["Gouws et al", "S. 2015] Gouws", "Y. Bengio", "G. Corrado"], "venue": "Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard", "S. 2015] Gouws", "A. S\u00f8gaard"], "venue": "Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Cross-lingual Dependency Parsing Based on Distributed Representations", "author": ["Guo et al", "J. 2015] Guo", "W. Che", "D. Yarowsky", "H. Wang", "T. Liu"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Multilingual Distributed Representations without Word Alignment", "author": ["Hermann", "Blunsom", "K.M. 2013] Hermann", "P. Blunsom"], "venue": "arXiv preprint arXiv:1312.6173", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Multilingual Models for Compositional Distributed Semantics", "author": ["Hermann", "Blunsom", "K.M. 2014] Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "Hermann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2014}, {"title": "Inducing Crosslingual Distributed Representations of Words", "author": ["Klementiev et al", "A. 2012] Klementiev", "I. Titov", "B. Bhattarai"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2012\\E", "shortCiteRegEx": "al. et al\\.", "year": 2012}, {"title": "Learning Bilingual Word Representations by Marginalizing Alignments", "author": ["Ko\u010disk\u00fd et al", "T. 2014] Ko\u010disk\u00fd", "K.M. Hermann", "P. Blunsom"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.", "author": ["Landauer", "Dumais", "T.K. 1997] Landauer", "S.T. Dumais"], "venue": "Psychological review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Learning Multilingual Word Representations using a Bag-of-Words Autoencoder", "author": ["Lauly et al", "S. 2013] Lauly", "A. Boulanger", "H. Larochelle"], "venue": "NIPS WS on Deep Learning,", "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning", "author": ["Lazaridou et al", "A. 2015a] Lazaridou", "G. Dinu", "M. Baroni"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Combining Language and Vision with a Multimodal Skip-gram Model", "author": ["Lazaridou et al", "A. 2015b] Lazaridou", "T.P. Nghia", "M. Baroni"], "venue": "Proceedings of Human Language Technologies:", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Mikolov", "Q.V. 2014] Le", "T. Mikolov"], "venue": "International Conference on Machine Learning - ICML", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A Strong Baseline for Learning Cross-Lingual Word Embeddings from Sentence Alignments", "author": ["Levy et al", "O. 2017] Levy", "A. S\u00f8gaard", "Y. Goldberg"], "venue": "EACL", "citeRegEx": "al. et al\\.,? \\Q2017\\E", "shortCiteRegEx": "al. et al\\.", "year": 2017}, {"title": "Bilingual Word Representations with Monolingual Quality in Mind", "author": ["Luong et al", "2015] Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "Workshop on Vector Modeling for NLP,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Mikolov et al", "T. 2013a] Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Exploiting Similarities among Languages for Machine Translation", "author": ["Mikolov et al", "T. 2013b] Mikolov", "Q.V. Le", "I. Sutskever"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}, {"title": "Bilingual Word Embeddings from Parallel and Non-parallel Corpora for Cross-Language Text Classification", "author": ["Mogadala", "Rettinger", "A. 2016] Mogadala", "A. Rettinger"], "venue": null, "citeRegEx": "Mogadala et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mogadala et al\\.", "year": 2016}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Pennington et al", "J. 2014] Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "al. et al\\.,? \\Q2014\\E", "shortCiteRegEx": "al. et al\\.", "year": 2014}, {"title": "Learning Distributed Representations for Multilingual Text Sequences", "author": ["Pham et al", "H. 2015] Pham", "Luong", "M.-T", "C.D. Manning"], "venue": "Workshop on Vector Modeling for NLP,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Learning Cross-lingual Word Embeddings via Matrix Co-factorization", "author": ["Shi et al", "T. 2015] Shi", "Z. Liu", "Y. Liu", "M. Sun"], "venue": "Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Inverted indexing for cross-lingual NLP. The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL-IJCNLP", "author": ["S\u00f8gaard et al", "A. 2015] S\u00f8gaard", "Z. Agic", "H.M. Alonso", "B. Plank", "B. Bohnet", "A. Johannsen"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Cross-lingual Models of Word Embeddings: An Empirical Comparison", "author": ["Upadhyay et al", "S. 2016] Upadhyay", "M. Faruqui", "C. Dyer", "D. Roth"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "Multi-Modal Representations for Improved Bilingual Lexicon Learning", "author": ["Vuli\u0107 et al", "I. 2016] Vuli\u0107", "D. Kiela", "S. Clark", "Moens", "M.-F"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2016\\E", "shortCiteRegEx": "al. et al\\.", "year": 2016}, {"title": "On the Role of Seed Lexicons in Learning Bilingual Word Embeddings", "author": ["Vuli\u0107", "Korhonen", "I. 2016] Vuli\u0107", "A. Korhonen"], "venue": "Proceedings of ACL,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Bilingual Distributed Word Representations from Document-Aligned Comparable Data", "author": ["Vuli\u0107", "Moens", "I. 2016] Vuli\u0107", "M.-F"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment", "author": ["Vyas", "Carpuat", "Y. 2016] Vyas", "M. Carpuat"], "venue": null, "citeRegEx": "Vyas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vyas et al\\.", "year": 2016}, {"title": "Distributed Word Representation Learning for Cross-Lingual Dependency Parsing", "author": ["Xiao", "Guo", "M. 2014] Xiao", "Y. Guo"], "venue": null, "citeRegEx": "Xiao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2014}, {"title": "Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation", "author": ["Xing et al", "C. 2015] Xing", "C. Liu", "D. Wang", "Y. Lin"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "al. et al\\.", "year": 2015}, {"title": "Bilingual Word Embeddings for Phrase-Based Machine Translation", "author": ["Zou et al", "W.Y. 2013] Zou", "R. Socher", "D. Cer", "C.D. Manning"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2013\\E", "shortCiteRegEx": "al. et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Pseudocrosslingual (\u00a73) Mapping to same representation [Xiao and Guo, 2014] Lexicon Random translation replacement [Gouws et al., 2015] On-the-fly replacement with polysemy [Duong et al.", "startOffset": 115, "endOffset": 135}, {"referenceID": 12, "context": ", 2015] Bilingual bag-of-words, not word-aligned [Gouws et al., 2015] Sentence-aligned Bilingual skip-gram, not word-aligned [Coulmance et al.", "startOffset": 49, "endOffset": 69}, {"referenceID": 12, "context": "[Gouws et al., 2015] propose a Bilingual Bag-of-Words without Word Alignments (BilBOWA) that leverages additional monolingual data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "Figure 14: The BilBOWA model [Gouws et al., 2015]", "startOffset": 29, "endOffset": 49}, {"referenceID": 12, "context": "Figure 15: Approximating word alignments with uniform alignments [Gouws et al., 2015]", "startOffset": 65, "endOffset": 85}, {"referenceID": 12, "context": "For the cross-lingual objective, they make a similar assumption as [Gouws et al., 2015] by supposing that every word in the source sentence is uniformly aligned to every word in the target sentence.", "startOffset": 67, "endOffset": 87}, {"referenceID": 12, "context": "These co-occurrences can be calculated without alignment information using a uniform alignment model as in [Gouws et al., 2015].", "startOffset": 107, "endOffset": 127}, {"referenceID": 12, "context": "In addition to regularising the mean of word vectors in a sentence to be close to the mean of word vectors in the aligned sentence similar to [Gouws et al., 2015] (the second term in the below equation), they also regularise the paragraph vectors SP l1 and SP l2 of aligned sentences in languages l1 and l2 to be close to each other.", "startOffset": 142, "endOffset": 162}, {"referenceID": 34, "context": ", 2015b] or cross-lingual [Vuli\u0107 et al., 2016] representations.", "startOffset": 26, "endOffset": 46}, {"referenceID": 12, "context": "4 [Gouws et al., 2015] 86.", "startOffset": 2, "endOffset": 22}], "year": 2017, "abstractText": "Cross-lingual embedding models allow us to project words from different languages into a shared embedding space. This allows us to apply models trained on languages with a lot of data, e.g. English to low-resource languages. In the following, we will survey models that seek to learn cross-lingual embeddings. We will discuss them based on the type of approach and the nature of parallel data that they employ. Finally, we will present challenges and summarize how to evaluate cross-lingual embedding models. In recent years, driven by the success of word embeddings, many models that learn accurate representations of words haven been proposed [Mikolov et al., 2013a, Pennington et al., 2014]. However, these models are generally restricted to capture representations of words in the language they were trained on. The availability of resources, training data, and benchmarks in English leads to a disproportionate focus on the English language and a negligence of the plethora of other languages that are spoken around the world. In our globalised society, where national borders increasingly blur, where the Internet gives everyone equal access to information, it is thus imperative that we do not only seek to eliminate bias pertaining to gender or race [Bolukbasi et al., 2016] inherent in our representations, but also aim to address our bias towards language. To remedy this and level the linguistic playing field, we would like to leverage our existing knowledge in English to equip our models with the capability to process other languages. Perfect machine translation (MT) would allow this. However, we do not need to actually translate examples, as long as we are able to project examples into a common subspace such as the one in Figure 1. Figure 1: A shared embedding space between two languages [Luong et al., 2015] \u2217This article originally appeared as a blog post at http://sebastianruder.com/ cross-lingual-embeddings/index.html on 28 November 2016. ar X iv :1 70 6. 04 90 2v 1 [ cs .C L ] 1 5 Ju n 20 17 Ultimately, our goal is to learn a shared embedding space between words in all languages. Equipped with such a vector space, we are able to train our models on data in any language. By projecting examples available in one language into this space, our model simultaneously obtains the capability to perform predictions in all other languages (we are glossing over some considerations here; for these, refer to Section 7. This is the promise of cross-lingual embeddings. Over the course of this survey, we will give an overview of models and algorithms that have been used to come closer to the elusive goal of capturing the relations between words in multiple languages in a common embedding space. Note that while neural MT approaches implicitly learn a shared cross-lingual embedding space by optimizing for the MT objective, we will focus on models that explicitly learn cross-lingual word representations throughout this blog post. These methods generally do so at a much lower cost than MT and can be considered to be to MT what word embedding models [Mikolov et al., 2013a, Pennington et al., 2014] are to language modelling. 1 Types of cross-lingual embedding models In recent years, various models for learning cross-lingual representations have been proposed. In the following, we will order them by the type of approach that they employ. Note that while the nature of the parallel data used is equally discriminatory and has been shown to account for inter-model performance differences [Levy et al., 2017], we consider the type of approach more conducive to understanding the assumptions a model makes and \u2013 consequently \u2013 its advantages and deficiencies. Cross-lingual embedding models generally use four different approaches: 1. Monolingual mapping: These models initially train monolingual word embeddings on large monolingual corpora. They then learn a linear mapping between monolingual representations in different languages to enable them to map unknown words from the source language to the target language. 2. Pseudo-cross-lingual: These approaches create a pseudo-cross-lingual corpus by mixing contexts of different languages. They then train an off-the-shelf word embedding model on the created corpus. The intuition is that the cross-lingual contexts allow the learned representations to capture cross-lingual relations. 3. Cross-lingual training: These models train their embeddings on a parallel corpus and optimize a cross-lingual constraint between embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. 4. Joint optimization: These approaches train their models on parallel (and optionally monolingual data). They jointly optimise a combination of monolingual and cross-lingual losses. In terms of parallel data, methods may use different supervision signals that depend on the type of data used. These are, from most to least expensive: 1. Word-aligned data: A parallel corpus with word alignments that is commonly used for machine translation; this is the most expensive type of parallel data to use. 2. Sentence-aligned data: A parallel corpus without word alignments. If not otherwise specified, the model uses the Europarl corpus2 consisting of sentence-aligned text from the proceedings of the European parliament that is generally used for training Statistical Machine Translation models. 3. Document-aligned data: A corpus containing documents in different languages. The documents can be topic-aligned (e.g. Wikipedia) or label/class-aligned (e.g. sentiment analysis and multi-class classification datasets). 4. Lexicon: A bilingual or cross-lingual dictionary with pairs of translations between words in different languages. 5. No parallel data: No parallel data whatsoever. Learning cross-lingual representations from only monolingual resources would enable zero-shot learning across languages. http://www.statmt.org/europarl/", "creator": "LaTeX with hyperref package"}}}