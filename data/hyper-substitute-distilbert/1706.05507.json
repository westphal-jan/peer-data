{"id": "1706.05507", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2017", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "abstract": "chaotic gradient functions must become recently very popular, to particular as they consistently been consistently very be viable in simulator training describing static hierarchical network. recognizing this paper we have analyzed rmsprop, albeit crucial for the training involving strong neural networks, in 2002 papers on online convex modeling and show $ \\ sqrt { t } $ - partial regret estimates. moreover, others obtained two variants sc - mps and sc - rmsprop for which we include logarithmic regret bounds | strongly convex functions. so, we demonstrate in the experiments characterized these expression types evolve through adaptive algorithms extensions or stochastic cascade descent in the optimization of strongly adaptive functions exceptionally well as in training of pure neural function.", "histories": [["v1", "Sat, 17 Jun 2017 09:48:55 GMT  (9014kb,D)", "http://arxiv.org/abs/1706.05507v1", "ICML 2017, 16 pages, 23 figures"]], "COMMENTS": "ICML 2017, 16 pages, 23 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE stat.ML", "authors": ["mahesh chandra mukkamala", "matthias hein"], "accepted": true, "id": "1706.05507"}, "pdf": {"name": "1706.05507.pdf", "metadata": {"source": "META", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "authors": ["Mahesh Chandra Mukkamala", "Matthias Hein"], "emails": ["<mmahesh.chandra873@gmail.com>."], "sections": [{"heading": null, "text": "\u221a T -type re-\ngret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.\n1. Introduction There has recently been a lot of work on adaptive gradient algorithms such as Adagrad (Duchi et al., 2011), RMSProp (Hinton et al., 2012), ADADELTA (Zeiler, 2012), and Adam (Kingma & Bai, 2015). The original idea of Adagrad to have a parameter specific learning rate by analyzing the gradients observed during the optimization turned out to be useful not only in online convex optimization but also for training deep neural networks. The original analysis of Adagrad (Duchi et al., 2011) was limited to the case of all convex functions for which it obtained a datadependent regret bound of order O( \u221a T ) which is known to be optimal (Hazan, 2016) for this class. However, a lot of learning problems have more structure in the sense that one optimizes over the restricted class of strongly convex functions. It has been shown in (Hazan et al., 2007) that one can achieve much better logarithmic regret bounds for the class of strongly convex functions.\n1Department of Mathematics and Computer Science, Saarland University, Germany 2IMPRS-CS, Max Planck Institute for Informatics, Saarbru\u0308cken, Germany . Correspondence to: Mahesh Chandra Mukkamala <mmahesh.chandra873@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nThe goal of this paper is twofold. First, we propose SCAdagrad which is a variant of Adagrad adapted to the strongly convex case. We show that SC-Adagrad achieves a logarithmic regret bound for the case of strongly convex functions, which is data-dependent. It is known that such bounds can be much better in practice than data independent bounds (Hazan et al., 2007),(McMahan, 2014). Second, we analyze RMSProp which has become one of the standard methods to train neural networks beyond stochastic gradient descent. We show that under some conditions on the weighting scheme of RMSProp, this algorithm achieves a data-dependent O( \u221a T ) regret bound. In fact, it turns out that RMSProp contains Adagrad as a special case for a particular choice of the weighting scheme. Up to our knowledge this is the first theoretical result justifying the usage of RMSProp in online convex optimization and thus can at least be seen as theoretical support for its usage in deep learning. Similarly, we then propose the variant SCRMSProp for which we also show a data-dependent logarithmic regret bound similar to SC-Adagrad for the class of strongly convex functions. Interestingly, SC-Adagrad has been discussed in (Ruder, 2016), where it is said that \u201cit does not to work\u201d. The reason for this is that SC-Adagrad comes along with a damping factor which prevents potentially large steps in the beginning of the iterations. However, as our analysis shows this damping factor has to be rather large initially to prevent large steps and should be then monotonically decreasing as a function of the iterations in order to stay adaptive. Finally, we show in experiments on three datasets that the new methods are competitive or outperform other adaptive gradient techniques as well as stochastic gradient descent for strongly convex optimization problem in terms of regret and training objective but also perform very well in the training of deep neural networks, where we show results for different networks and datasets.\n2. Problem Statement We first need some technical statements and notation and then introduce the online convex optimization problem.\nar X\niv :1\n70 6.\n05 50\n7v 1\n[ cs\n.L G\n] 1\n7 Ju\nn 20\n17\n2.1. Notation and Technical Statements\nWe denote by [T ] the set {1, . . . , T}. Let A \u2208 Rd\u00d7d be a symmetric, positive definite matrix. We denote as \u3008x, y\u3009A = \u3008x,Ay\u3009 = d\u2211\ni,j=1\nAijxiyj , \u2016x\u2016A = \u221a \u3008x, x\u3009A\nNote that the standard Euclidean inner product becomes \u3008x, y\u3009 = \u2211 i xiyi = \u3008x, y\u3009I While we use here the general notation for matrices for comparison to the literature. All positive definite matrices A in this paper will be diagonal matrices, so that the computational effort for computing inner products and norms is still linear in d. The CauchySchwarz inequality becomes, \u3008x, y\u3009A \u2264 \u2016x\u2016A \u2016y\u2016A . We further introduce the element-wise product a b of two vectors. Let a, b \u2208 Rd, then (a b)i = aibi for i = 1, . . . , d.\nLet A \u2208 Rd\u00d7d be a symmetric, positive definite matrix, z \u2208 Rd and C \u2282 Rd a convex set. Then we define the weighted projection PAC (z) of z onto the set C as\nPAC (z) = argmin x\u2208C \u2016x\u2212 z\u20162A . (1)\nIt is well-known that the weighted projection is unique and non-expansive.\nLemma 2.1 Let A \u2208 Rd\u00d7d be a symmetric, positive definite matrix and C \u2282 Rd be a convex set. Then\u2225\u2225PAC (z)\u2212 PAC (y)\u2225\u2225A \u2264 \u2016z \u2212 y\u2016A . Proof: The first order optimality condition for the weighted projection in (1) is given as\nA(x\u2212 z) \u2208 NC(x),\nwhere NC(x) denotes the normal cone of C at x. This can be rewritten as\n\u3008z \u2212 x, y \u2212 x\u3009A \u2264 0 \u2200y \u2208 C.\nThis yields\u2329 z \u2212 PAC (z), PAC (y)\u2212 PAC (z) \u232a A \u2264 0,\u2329\ny \u2212 PAC (y), PAC (z)\u2212 PAC (y) \u232a A \u2264 0.\nAdding these two inequalities yields\u2329 z \u2212 PAC (z)\u2212 y + PAC (y), PAC (y)\u2212 PAC (z) \u232a A \u2264 0\n=\u21d2 \u2225\u2225PAC (y)\u2212 PAC (z)\u2225\u22252A \u2264 \u2329z \u2212 y, PAC (y)\u2212 PAC (z)\u232aA .\nThe result follows from the application of the weighted Cauchy-Schwarz inequality.\nLemma 2.2 For any symmetric, positive semi-definite matrix A \u2208 Rd\u00d7d we have\n\u3008x,Ax\u3009 \u2264 \u03bbmax(A) \u3008x, x\u3009 \u2264 tr(A) \u3008x, x\u3009 (2)\nwhere \u03bbmax(A) is the maximum eigenvalue of matrix A and tr(A) denotes the trace of matrix A .\n2.2. Problem Statement\nIn this paper we analyze the online convex optimization setting, that is we have a convex setC and at each round we get access to a (sub)-gradient of some continuous convex function ft : C \u2192 R. At the t-th iterate we predict \u03b8t \u2208 C and suffer a loss ft(\u03b8t). The goal is to perform well with respect to the optimal decision in hindsight defined as\n\u03b8\u2217 = argmin \u03b8\u2208C T\u2211 t=1 ft(\u03b8).\nThe adversarial regret at time T \u2208 N is then given as\nR(T ) = T\u2211 t=1 (ft(\u03b8t)\u2212 ft(\u03b8\u2217)).\nWe assume that the adversarial can choose from the class of convex functions on C, for some parts we will specialize this to the set of strongly convex functions.\nDefinition 2.1 Let C be a convex set. We say that a function f : C \u2192 R is \u00b5-strongly convex, if there exists \u00b5 \u2208 Rd with \u00b5i > 0 for i = 1, . . . , d such that for all x, y \u2208 C,\nf(y) \u2265 f(x) + \u3008\u2207f(x), y \u2212 x\u3009+ \u2016y \u2212 x\u20162diag(\u00b5)\n= f(x) + \u3008\u2207f(x), y \u2212 x\u3009+ d\u2211 i=1 \u00b5i(yi \u2212 xi)2.\nLet \u03b6 = mini=1,...,d \u00b5i, then this function is \u03b6-strongly convex (in the usual sense), that is\nf(y) \u2265 f(x) + \u3008\u2207f(x), y \u2212 x\u3009+ \u03b6 \u2016x\u2212 y\u20162 .\nNote that the difference between our notion of componentwise strong convexity and the usual definition of strong convexity is indicated by the bold font versus normal font. We have two assumptions:\n\u2022 A1: It holds supt\u22651 \u2016gt\u20162 \u2264 G which implies the existence of a constantG\u221e such that supt\u22651 \u2016gt\u2016\u221e \u2264 G\u221e.\n\u2022 A2: It holds supt\u22651 \u2016\u03b8t \u2212 \u03b8\u2217\u20162 \u2264 D which implies the existence of a constant D\u221e such that supt\u22651 \u2016\u03b8t \u2212 \u03b8\u2217\u2016\u221e \u2264 D\u221e.\nAlgorithm 1 Adagrad Input: \u03b81 \u2208 C , \u03b4 > 0, v0 = 0 \u2208 Rd for t = 1 to T do gt \u2208 \u2202ft(\u03b8t) vt = vt\u22121 + (gt gt) At = diag( \u221a vt) + \u03b4I\n\u03b8t+1 = P At C ( \u03b8t \u2212 \u03b1A\u22121t gt ) end for\nOne of the first methods which achieves the optimal regret bound of O( \u221a T ) for convex problems is online projected gradient descent (Zinkevich, 2003), defined as\n\u03b8t+1 = PC(\u03b8t \u2212 \u03b1tgt) (3)\nwhere \u03b1t = \u03b1\u221at is the step-size scheme and gt is a (sub)gradient of ft at \u03b8t. With \u03b1t = \u03b1t , online projected gradient descent method achieves the optimal O(log(T )) regret bound for strongly-convex problems (Hazan et al., 2007). We consider Adagrad in the next subsection which is one of the popular adaptive alternative to online projected gradient descent.\n2.3. Adagrad for convex problems\nIn this section we briefly recall the main result for the Adagrad. The algorithm for Adagrad is given in Algorithm 1. If the adversarial is allowed to choose from the set of all possible convex functions on C \u2282 Rd, then Adagrad achieves the regret bound of orderO( \u221a T ) as shown in (Duchi et al., 2011). This regret bound is known to be optimal for this class, see e.g. (Hazan, 2016). For better comparison to our results for RMSProp, we recall the result from (Duchi et al., 2011) in our notation. For this purpose, we introduce the notation, g1:T,i = (g1,i, g2,i, .., gT,i)T , where gt,i is the i-th component of the gradient gt \u2208 Rd of the function ft evaluated at \u03b8t.\nTheorem 2.1 (Duchi et al., 2011) Let Assumptions A1, A2 hold and let \u03b8t be the sequence generated by Adagrad in Algorithm 1, where gt \u2208 \u2202ft(\u03b8t) and ft : C \u2192 R is an arbitrary convex function, then for stepsize \u03b1 > 0 the regret is upper bounded as\nR(T ) \u2264 D 2 \u221e\n2\u03b1 d\u2211 i=1 \u2016g1:T,i\u20162 + \u03b1 d\u2211 i=1 \u2016g1:T,i\u20162 .\nThe effective step-length of Adagrad is on the order of \u03b1\u221a t . This can be seen as follows; first note that vT,i = \u2211T t=1 g 2 t,i and thus (At)\u22121 is a diagonal matrix with entries 1\u221avt,i+\u03b4 . Then one has\n\u03b1(A\u22121T )ii = \u03b1\u221a\u2211T\nt=1 g 2 t,i + \u03b4\n= \u03b1\u221a T 1\u221a 1 T \u2211T t=1 g 2 t,i + \u03b4\u221a T\n(4)\nThus an alternative point of view of Adagrad, is that it has a decaying stepsize \u03b1\u221a\nt but now the correction term becomes\nthe running average of the squared derivatives plus a vanishing damping term. However, the effective stepsize has to decay faster to get a logarithmic regret bound for the strongly convex case. This is what we analyze in the next section, where we propose SC-Adagrad for strongly convex functions.\n3. Strongly convex Adagrad (SC-Adagrad) The modification SC-Adagrad of Adagrad which we propose in the following can be motivated by the observation that the online projected gradient descent (Hazan et al., 2007) uses stepsizes of order \u03b1 = O( 1T ) in order to achieve the logarithmic regret bound for strongly convex functions. In analogy with the derivation in the previous section, we still have vT,i = \u2211T t=1 g 2 t,i. But now we modify (At) \u22121 and set it as a diagonal matrix with entries 1vt,i+\u03b4t . Then one has\n\u03b1(A\u22121T )ii = \u03b1\u2211T\nt=1 g 2 t,i + \u03b4t\n= \u03b1\nT 1 1 T \u2211T t=1 g 2 t,i + \u03b4T T .\n(5)\nAgain, we have in the denominator a running average of the observed gradients and a decaying damping factor. In this way, we get an effective stepsize of order O( 1T ) in SCAdagrad. The formal method is presented in Algorithm 2. As just derived the only difference of Adagrad and SCAdagrad is the definition of the diagonal matrix At. Note\nAlgorithm 2 SC-Adagrad Input: \u03b81 \u2208 C , \u03b40 > 0, v0 = 0 \u2208 Rd for t = 1 to T do gt \u2208 \u2202ft(\u03b8t) vt = vt\u22121 + (gt gt) Choose 0 < \u03b4t \u2264 \u03b4t\u22121 element wise At = diag(vt) + diag(\u03b4t) \u03b8t+1 = P At C ( \u03b8t \u2212 \u03b1A\u22121t gt\n) end for\nalso that we have defined the damping factor \u03b4t as a function of t which is also different from standard Adagrad. The constant \u03b4 in Adagrad is mainly introduced due to numerical reasons in order to avoid problems when gt,i is very small for some components in the first iterations and is typically chosen quite small e.g. \u03b4 = 10\u22128. For SCAdagrad the situation is different. If the first components g1,i, g2,i, . . . are very small, say of order , then the update is 2+\u03b4t which can become extremely large if \u03b4t is chosen\nto be small. This would make the method very unstable and would lead to huge constants in the bounds. This is probably why in (Ruder, 2016), the modification of Adagrad where one \u201cdrops the square-root\u201d did not work. A good choice of \u03b4t should be initially roughly on the order of 1 and it should decay as vt,i = \u2211T t=1 g 2 t,i starts to grow. This is why we propose to use\n\u03b4t,i = \u03be2e \u2212\u03be1vt,i , i = 1, . . . , d,\nfor \u03be1 > 0, \u03be2 > 0 as a potential decay scheme as it satisfies both properties for sufficiently large \u03be1 and \u03be2 chosen on the order of 1. Also, one can achieve a constant decay scheme for \u03be1 = 0 , \u03be2 > 0. We will come back to this choice after the proof. In the following we provide the regret analysis of SC-Adagrad and show that the optimal logarithmic regret bound can be achieved. However, as it is data-dependent it is typically significantly better in practice than data-independent bounds.\n3.1. Analysis\nFor any two matrices A,B \u2208 Rd\u00d7d, we use the notation \u2022 to denote the inner product i.e A \u2022 B = \u2211 i \u2211 j AijBij . Note that A \u2022B = tr(ATB).\nLemma 3.1 [Lemma 12 (Hazan et al., 2007)] Let A,B be positive definite matrices, let A B 0 then\nA\u22121 \u2022 (A\u2212B) \u2264 log ( |A| |B| ) (6)\nwhere |A| denotes the determinant of the matrix A\nLemma 3.2 Let Assumptions A1, A2 hold, then for T \u2265 1 and At, \u03b4t as defined in the SC-Adagrad algorithm we have,\nT\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a \u2264 d\u2211 i=1 log\n( \u2016g1:T,i\u20162 + \u03b4T,i\n\u03b41,i\n)\n\u2212 d\u2211 i=1 T\u2211 t=2 \u03b4t,i \u2212 \u03b4t\u22121,i \u2016g1:t,i\u20162 + \u03b4t,i\nProof: Consider the following summation, T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a \u2264 T\u2211 t=1 A\u22121t \u2022 diag(gtgTt )\n= A\u221211 \u2022 (A1 \u2212 diag(\u03b41))\n+ T\u2211 t=2 A\u22121t \u2022 (At \u2212At\u22121 \u2212 diag(\u03b4t) + diag(\u03b4t\u22121))\n\u2264 log ( |A1| |diag(\u03b41)| ) + T\u2211 t=2 log ( |At| |At\u22121| )\n\u2212 T\u2211 t=2 A\u22121t \u2022 (diag(\u03b4t)\u2212 diag(\u03b4t\u22121))\n= log ( |AT | |diag(\u03b41)| ) \u2212 T\u2211 t=2 A\u22121t \u2022 (diag(\u03b4t)\u2212 diag(\u03b4t\u22121))\n\u2264 d\u2211 i=1 log\n( \u2016g1:T,i\u20162 + \u03b4T,i\n\u03b41,i\n)\n\u2212 T\u2211 t=2 A\u22121t \u2022 (diag(\u03b4t)\u2212 diag(\u03b4t\u22121))\n\u2264 d\u2211 i=1 log\n( \u2016g1:T,i\u20162 + \u03b4T,i\n\u03b41,i\n) \u2212\nd\u2211 i=1 T\u2211 t=2 \u03b4t,i \u2212 \u03b4t\u22121,i \u2016g1:t,i\u20162 + \u03b4t,i\nIn the first step we use \u3008x,Ax\u3009 = A \u2022 diag(xxT ) where A is a diagonal matrix and subsequently we use\u2200t > 1 , diag(gtg T t ) = At \u2212 At\u22121 \u2212 diag(\u03b4t) + diag(\u03b4t\u22121), and for t = 1 we have diag(g1gT1 ) = A1 \u2212 diag(\u03b41). In the first inequality we use Lemma 3.1 also see for Lemma 12 of (Hazan et al., 2007). Note that for T = 1, the upper bound results in 0.\nTheorem 3.1 Let Assumptions A1, A2 hold and let \u03b8t be the sequence generated by the SC-Adagrad in Algorithm 2, where gt \u2208 \u2202ft(\u03b8t) and ft : C \u2192 R is an arbitrary \u00b5-strongly convex function (\u00b5 \u2208 Rd+) where the stepsize fulfills \u03b1 \u2265 maxi=1,...,d G 2 \u221e\n2\u00b5i . Furthermore, let \u03b4t > 0 and\n\u03b4t,i \u2264 \u03b4t\u22121,i\u2200t \u2208 [T ],\u2200i \u2208 [d], then the regret of SCAdagrad can be upper bounded for T \u2265 1 as\nR(T ) \u2264 D 2 \u221e tr(diag(\u03b41)) 2\u03b1 + \u03b1 2 d\u2211 i=1 log (\u2016g1:T,i\u20162 + \u03b4T,i \u03b41,i ) + 1\n2 d\u2211 i=1 inf t\u2208[T ] ( (\u03b8t,i \u2212 \u03b8\u2217i )2 \u03b1 \u2212 \u03b1 \u2016g1:t,i\u20162 + \u03b4t,i ) (\u03b4T,i \u2212 \u03b41,i)\nFor constant \u03b4t i.e \u03b4t,i = \u03b4 > 0 \u2200t \u2208 [T ] and\u2200i \u2208 [d] then the regret of SC-Adagrad is upper bounded as\nR(T ) \u2264 D 2 \u221ed\u03b4 2\u03b1 + \u03b1 2 d\u2211 i=1 log (\u2016g1:T,i\u20162 + \u03b4 \u03b4 ) (7)\nFor \u03b6-strongly convex function choosing \u03b1 \u2265 G 2 \u221e\n2\u03b6 we obtain the above mentioned regret bounds.\nProof: We rewrite the regret bound with the definition of \u00b5-strongly convex functions as\nR(T ) = T\u2211 t=1 (ft(\u03b8t)\u2212 ft(\u03b8\u2217))\n\u2264 T\u2211 t=1 \u3008gt, \u03b8t \u2212 \u03b8\u2217\u3009 \u2212 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\nUsing the non-expansiveness we have\n\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162At = \u2225\u2225\u2225PAtC (\u03b8t \u2212 \u03b1A\u22121t gt)\u2212 \u03b8\u2217\u2225\u2225\u22252\nAt \u2264 \u2225\u2225\u03b8t \u2212 \u03b1A\u22121t gt \u2212 \u03b8\u2217\u2225\u22252At \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 2\u03b1 \u3008gt, \u03b8t \u2212 \u03b8 \u2217\u3009+ \u03b12 \u2329 gt, A \u22121 t gt\n\u232a This yields\n\u3008gt, \u03b8t \u2212 \u03b8\u2217\u3009 \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t+1 \u2212 \u03b8\n\u2217\u20162At 2\u03b1 + \u03b1 2\n\u2329 gt, A \u22121 t gt \u232a Hence we can upper bound the regret as follows\nR(T )\n\u2264 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t+1 \u2212 \u03b8 \u2217\u20162At 2\u03b1\n+ \u03b1\n2 T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a \u2212 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\n\u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162A1\n2\u03b1 + T\u2211 t=2 (\u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At\u22121 2\u03b1 ) \u2212 \u2016\u03b8T+1 \u2212 \u03b8\u2217\u20162AT\n2\u03b1 + \u03b1 2 T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a \u2212\nT\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\n\u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162A1\u22122\u03b1diag(\u00b5)\n2\u03b1\n+ T\u2211 t=2 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At\u2212At\u22121\u22122\u03b1 diag(\u00b5) 2\u03b1 + \u03b1 2 T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a In the last step we use the equality \u2200x \u2208 Rn \u2016x\u20162A \u2212 \u2016x\u20162B = \u2016x\u20162A\u2212B where A,B \u2208 Rn xn and both are diagonal matrices. Now, we choose \u03b1 such that At \u2212 At\u22121 \u2212 2\u03b1 diag(\u00b5) 4 diag(\u03b4t) \u2212 diag(\u03b4t\u22121) \u2200t \u2265 2 and A1 \u2212 2\u03b1 diag(\u00b5) 4 diag(\u03b41) Since At \u2212 At\u22121 4 G2\u221eI +diag(\u03b4t)\u2212 diag(\u03b4t\u22121) and A1 4 G2\u221eI +diag(\u03b41) because at any round the difference between subsequent squares of sub-gradients is bounded by G2\u221e. Also by Algorithm 2, \u03b4t,i \u2264 \u03b4t\u22121,i\u2200t > 1,\u2200i \u2208 [d] hence diag(\u03b4t) \u2212 diag(\u03b4t\u22121) 0. Hence by choosing \u03b1 \u2265 maxi=1,...,d G2\u221e 2\u00b5i\nwe have At \u2212 At\u22121 \u2212 2\u03b1 diag(\u00b5) 4 diag(\u03b4t) \u2212 diag(\u03b4t\u22121)\u2200t \u2265 2 and A1 \u2212 2\u03b1diag(\u00b5) 4 diag(\u03b41) which yields\nR(T )\n\u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162diag(\u03b41)\n2\u03b1 + T\u2211 t=2 \u2016\u03b8t \u2212 \u03b8\u2217\u2016diag(\u03b4t)\u2212diag(\u03b4t\u22121) 2\u03b1\n+ \u03b1\n2 T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a = \u2016\u03b81 \u2212 \u03b8\u2217\u20162diag(\u03b41)\n2\u03b1 + T\u2211 t=2 d\u2211 i=1 (\u03b8t,i \u2212 \u03b8\u2217i )2(\u03b4t,i \u2212 \u03b4t\u22121,i) 2\u03b1\n+ \u03b1\n2 T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a \u2264 D 2 \u221e tr(diag(\u03b41))\n2\u03b1 + \u03b1 2 T\u2211 t=1 \u2329 gt, A \u22121 t gt \u232a +\nT\u2211 t=2 d\u2211 i=1 (\u03b8t,i \u2212 \u03b8\u2217i )2(\u03b4t,i \u2212 \u03b4t\u22121,i) 2\u03b1\n\u2264 D 2 \u221e tr(diag(\u03b41)) 2\u03b1 + \u03b1 2 d\u2211 i=1 log (\u2016g1:T,i\u20162 + \u03b4T,i \u03b41,i ) + 1\n2 T\u2211 t=2 d\u2211 i=1\n( (\u03b8t,i \u2212 \u03b8\u2217i )2(\u03b4t,i \u2212 \u03b4t\u22121,i)\n\u03b1 \u2212 \u03b1(\u03b4t,i \u2212 \u03b4t\u22121,i) \u2016g1:t,i\u20162 + \u03b4t,i\n)\n\u2264 D 2 \u221e tr(diag(\u03b41)) 2\u03b1 + \u03b1 2 d\u2211 i=1 log (\u2016g1:T,i\u20162 + \u03b4T,i \u03b41,i ) + 1\n2 d\u2211 i=1 inf t\u2208[T ] ( (\u03b8t,i \u2212 \u03b8\u2217i )2 \u03b1 \u2212 \u03b1 \u2016g1:t,i\u20162 + \u03b4t,i ) (\u03b4T,i \u2212 \u03b41,i)\nIn the second inequality we bounded \u2016\u03b81 \u2212 \u03b8\u2217\u20162diag(\u03b41) \u2264 D2\u221e tr(diag(\u03b41)). In the second last step we use the Lemma 3.2. So under a constant \u03b4t i.e \u03b4t,i = \u03b4 > 0, \u2200t \u2208 [T ],\u2200i \u2208 [d] we have tr(diag(\u03b41)) = d\u03b4 hence proving the result (7). For \u03b6-strongly convex functions choosing \u03b1 \u2265 G2\u221e,i 2\u03b6 we obtain the the same results as \u00b5-strongly convex functions. This can be seen by setting \u00b5i = \u03b6, \u2200i \u2208 [d].\nNote that the first and the last term in the regret bound can be upper bounded by constants. Only the second term depends on T . Note that \u2016g1:T,i\u20162 \u2264 TG2 and as \u03b4t is monotonically decreasing, the second term is on the order of O(log(T )) and thus we have a logarithmic regret bound. As the bound is data-dependent, in the sense that it depends on the observed sequence of gradients, it is much tighter than a data-independent bound.\nThe bound includes also the case of a non-decaying damping factor \u03b4t = \u03b4 = \u03be2 (\u03be1 = 0). While a rather large constant damping factor can work well, we have noticed that the best results are obtained with the decay scheme\n\u03b4t,i = \u03be2e \u2212\u03be1vt,i , i = 1, . . . , d.\nwhere \u03be1 > 0 , \u03be2 > 0 , which is what we use in the experiments. Note that this decay scheme for \u03be1, \u03be2 > 0 is\nadaptive to the specific dimension and thus increases the adaptivity of the overall algorithm. For completeness we also give the bound specialized for this decay scheme.\nCorollary 3.1 In the setting of Theorem 3.1 choose \u03b4t,i = \u03be2e \u2212\u03be1vt,i for i = 1, . . . , d for some \u03be1 > 0, \u03be2 > 0 . Then the regret of SC-Adagrad can be upper bounded for T \u2265 1 as\nR(T ) \u2264 dD 2 \u221e\u03be2 2\u03b1 \u2212 \u03b1 2 log(\u03be2e \u2212\u03be1G2\u221e)\n+ \u03b1\n2 d\u2211 i=1 log ( \u2016g1:T,i\u20162 + \u03be2 e\u2212\u03be1\u2016g1:T,i\u2016 2 )\n+ \u03b1\u03be1\u03be2 2 ( log(\u03be2 \u03be1) + 1 ) d\u2211 i=1 ( 1\u2212 e\u2212\u03be1\u2016g1:T,i\u2016 2 )\nProof: Note that \u03b4T,i = \u03be2e\u2212\u03be1vT,i = \u03be2e\u2212\u03be1\u2016g1:T,i\u2016 2\n. Plugging this into Theorem 3.1 for \u03be1, \u03be2 > 0 yields the results for the first three terms. Using (\u03b8t,i \u2212 \u03b8\u2217i )2 \u2265 0 we have\ninf t\u2208[T ] ( (\u03b8t,i \u2212 \u03b8\u2217i )2 \u03b1 \u2212 \u03b1 \u2016g1:t,i\u20162 + \u03b4t,i ) \u2265 \u2212\u03b1\ninfj\u2208[1:T ] \u2016g1:j,i\u20162 + \u03b4j,i\nNote that \u2016g1:j,i\u20162 + \u03b4j,i = vj,i + \u03be2e\u2212\u03be1vj,i , in order to find the minimum of this term we thus analyze the function f : R+ \u2192 R, f(x) = x + \u03be2e\u2212\u03be1x. and a straightforward calculation shows that the minimum is attained at x\u2217 = 1\u03be1 log(\u03be1\u03be2) and f(x\n\u2217) = 1\u03be1 (log(\u03be1\u03be2) + 1). This yields the fourth term.\nUnfortunately, it is not obvious that the regret bound for our decaying damping factor is better than the one of a constant damping factor. Note, however that the third term in the regret bound of Theorem 3.1 can be negative. It thus remains an interesting question for future work, if there exists an optimal decay scheme which provably works better than any constant one.\n4. RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015). It has been used frequently in computer vision (Karpathy & Fei-Fei, 2016) e.g. to train the latest InceptionV4 network (Szegedy et al., 2016a;b). Note that RMSProp outperformed other adaptive methods like Adagrad order Adadelta as well as SGD with momentum in a large number of tests in (Schaul et al., 2014). It has been argued that if the changes in the parameter update\nare approximately Gaussian distributed, then the matrix At can be seen as a preconditioner which approximates the diagonal of the Hessian (Daniel et al., 2016). However, it is fair to say that despite its huge empirical success in practice and some first analysis in the literature, there is so far no rigorous theoretical analysis of RMSProp. We will analyze RMSProp given in Algorithm 3 in the framework of of online convex optimization.\nAlgorithm 3 RMSProp Input: \u03b81 \u2208 C , \u03b4 > 0, \u03b1 > 0, v0 = 0 \u2208 Rd for t = 1 to T do gt \u2208 \u2202ft(\u03b8t) vt = \u03b2tvt\u22121 + (1\u2212 \u03b2t)(gt gt) Set t = \u03b4\u221at and \u03b1t = \u03b1\u221a t\nAt = diag( \u221a vt) + tI \u03b8t+1 = P At C ( \u03b8t \u2212 \u03b1tA\u22121t gt ) end for\nFirst, we will show that RMSProp reduces to Adagrad for a certain choice of its parameters. Second, we will prove for the general convex case a regret bound of O( \u221a T ) similar to the bound given in Theorem 2.1. It turns out that the convergence analysis requires that in the update of the weighted cumulative squared gradients (vt) , it has to hold\n1\u2212 1 t \u2264 \u03b2t \u2264 1\u2212 \u03b3 t ,\nfor some 0 < \u03b3 \u2264 1. This is in contrast to the original suggestion of (Hinton et al., 2012) to choose \u03b2t = 0.9. It will turn out later in the experiments that the constant choice of \u03b2t leads sometimes to divergence of the sequence, whereas the choice derived from our theoretical analysis always leads to a convergent scheme even when applied to deep neural networks. Thus we think that the analysis in the following is not only interesting for the convex case but can give valuable hints how the parameters of RMSProp should be chosen in deep learning.\nBefore we start the regret analysis we want to discuss the sequence vt in more detail. Using the recursive definition of vt, we get the closed form expression\nvt,i = t\u2211 j=1 (1\u2212 \u03b2j) t\u220f k=j+1 \u03b2kg 2 j,i.\nWith \u03b2t = 1\u2212 1t one gets, vt,i = \u2211t j=1 1 j \u220ft k=j+1 k\u22121 k g 2 j,i,\nand using the telescoping product ones gets\u220ft k=j+1 k\u22121 k = j t and thus\nvt,i = 1 t \u2211t j=1 g 2 j,i.\nIf one uses additionally the stepsize scheme \u03b1t = \u03b1\u221at and\nt = \u03b4\u221a T , then we recover the update scheme of Adagrad,\nsee (4), as a particular case of RMSProp. We are not aware of that this correspondence of Adagrad and RMSProp has been observed before.\nThe proof of the regret bound for RMSProp relies on the following lemma.\nLemma 4.1 Let Assumptions A1 and A2 and suppose that 1\u2212 1t \u2264 \u03b2t \u2264 1\u2212 \u03b3 t for some 0 < \u03b3 \u2264 1, and t \u2265 1. Also\nfor t > 1 suppose \u221a (t\u2212 1) t\u22121 \u2264 \u221a t t, then\nT\u2211 t=1 g2t,i\u221a t vt,i + \u221a t t \u2264 2(2\u2212 \u03b3) \u03b3 (\u221a T vT,i + \u221a T T ) .\nProof: The lemma is proven via induction. For T = 1 we have v0 = 0 and thus v1,i = (1\u2212 \u03b21)g21,i and thus\ng21,i ( \u221a v1,i + 1) = (1\u2212 \u03b21)g2t,i (1\u2212 \u03b21) (\u221a (1\u2212 \u03b21)g21,i + 1 )\n\u2264\n\u221a (1\u2212 \u03b21)g21,i + 1\n1\u2212 \u03b21 \u2264\n( \u221a v1,i + 1)\n\u03b3 .\nNote that 1\u03b3 \u2264 2(2\u2212\u03b3) \u03b3 since 2(2\u2212 \u03b3) > 1 for \u03b3 \u2264 1 hence the bound holds for T = 1. For T > 1 we suppose that the bound is true for T \u2212 1 and get\nT\u22121\u2211 t=1 g2t,i\u221a t vt,i + \u221a t t\n\u2264 2(2\u2212 \u03b3) \u03b3\n(\u221a (T \u2212 1) vT\u22121,i + \u221a (T \u2212 1) T\u22121 ) .\nWe rewrite vT,i = \u03b2T vT\u22121,i + (1 \u2212 \u03b2T )g2T,i as vT\u22121,i = 1 \u03b2T vT,i\u2212 1\u2212\u03b2T\u03b2T g 2 T,i and with \u221a (t\u2212 1) t\u22121 \u2264\n\u221a t t we get\u221a\n(T \u2212 1)vT\u22121,i + \u221a (T \u2212 1) T\u22121\n\u2264 \u221a T \u2212 1 \u03b2T vT,i \u2212 (T \u2212 1)(1\u2212 \u03b2T ) \u03b2T g2T,i + \u221a T T\n= \u221a T \u2212 1 T\u03b2T TvT,i \u2212 (T \u2212 1)(1\u2212 \u03b2T ) \u03b2T g2T,i + \u221a T T\n\u2264 \u221a TvT,i \u2212\n(T \u2212 1)(1\u2212 \u03b2T ) \u03b2T g2T,i + \u221a T T\nNote that in the last step we have used that T\u22121T\u03b2T \u2264 1 and the fact that \u221a x is concave and thus \u221a x\u2212 c \u2264 \u221a x\u2212 c\n2 \u221a x\ngiven that x \u2212 c \u2265 0, along with \u22121\u221a TvT,i \u2264 \u22121\u221a TvT,i+ \u221a T t for vt,i 6= 0 we have\u221a (T \u2212 1)vT\u22121,i + \u221a (T \u2212 1) T\u22121\n\u2264 \u221a TvT,i + \u221a T T \u2212\n(T \u2212 1)(1\u2212 \u03b2T ) 2\u03b2T (\u221a TvT,i + \u221a T T )g2T,i (8) Using the above bound we have the following\nT\u2211 t=1 g2t,i\u221a t vt,i + \u221a t t\n= T\u22121\u2211 t=1 g2t,i\u221a t vt,i + \u221a t t + g2T,i\u221a TvT,i + \u221a T T \u2264 2(2\u2212 \u03b3) \u03b3 (\u221a (T \u2212 1) vT\u22121,i + \u221a (T \u2212 1) T\u22121\n) +\ng2T,i\u221a T vT,i + \u221a T t\n\u2264 2(2\u2212 \u03b3) \u03b3\n(\u221a TvT,i + \u221a T T ) + ( 1\u2212 2(2\u2212 \u03b3)\n\u03b3\n(T \u2212 1)(1\u2212 \u03b2T ) 2\u03b2T ) g2T,i\u221a TvT,i + \u221a T T\nIn the last step we use (8) and since for T > 1 the term( 1\u2212 2(2\u2212\u03b3)\u03b3 (T\u22121)(1\u2212\u03b2T ) 2\u03b2T ) \u2264 0 for 1\u2212 1t \u2264 \u03b2t \u2264 1\u2212 \u03b3 t\nCorollary 4.1 Let Assumptions A1, A2 hold and suppose that 1 \u2212 1t \u2264 \u03b2t \u2264 1 \u2212 \u03b3 t for some 0 < \u03b3 \u2264 1, and\nt \u2265 1. Also for t > 1 suppose \u221a (t\u2212 1) t\u22121 \u2264 \u221a t t, and set \u03b1t = \u03b1\u221at , then T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2264 \u03b1(2\u2212 \u03b3) \u03b3 d\u2211 i=1 (\u221a T vT,i + \u221a T T )\nProof: Using the definition of At = diag( \u221a vt) + tI , \u03b1t = \u03b1\u221a t along with Lemma 4.1 we get T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a = T\u2211 t=1 \u03b1t 2 d\u2211 i=1 g2t,i\u221a vt,i + t\n= \u03b1\n2 T\u2211 t=1 d\u2211 i=1 g2t,i\u221a t vt,i + \u221a t t\n\u2264 \u03b1 2 d\u2211 i=1 2(2\u2212 \u03b3) \u03b3 (\u221a T vT,i + \u221a T T )\nWith the help of Lemma 4.1 and Corollary 4.1 we can now state the regret bound for RMSProp.\nTheorem 4.1 Let Assumptions A1, A2 hold and let \u03b8t be the sequence generated by RMSProp in Algorithm 3, where gt \u2208 \u2202ft(\u03b8t) and ft : C \u2192 R is an arbitrary convex function and \u03b1t = \u03b1\u221at for some \u03b1 > 0 and 1\u2212 1 t \u2264 \u03b2t \u2264 1\u2212 \u03b3 t\nfor some 0 < \u03b3 \u2264 1. Also for t > 1 let \u221a (t\u2212 1) t\u22121 \u2264\u221a\nt t, then the regret of RMSProp can be upper bounded for T \u2265 1 as\nR(T ) \u2264 (D2\u221e 2\u03b1 + \u03b1(2\u2212 \u03b3) \u03b3 ) d\u2211 i=1 (\u221a TvT,i + \u221a T T ) Proof: Note that for every convex function ft : C \u2192 R it holds for all x, y \u2208 C and gt \u2208 \u2202ft(x),\nft(y) \u2265 ft(x) + \u3008gt, y \u2212 x\u3009 .\nWe use this to upper bound the regret as\nR(T ) = T\u2211 t=1 (ft(\u03b8t)\u2212 ft(\u03b8\u2217)) \u2264 T\u2211 t=1 \u3008gt, \u03b8t \u2212 \u03b8\u2217\u3009\nUsing the non-expansiveness of the weighted projection, we have\n\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162At = \u2225\u2225\u2225PAtC (\u03b8t \u2212 \u03b1tA\u22121t gt)\u2212 \u03b8\u2217\u2225\u2225\u22252\nAt \u2264 \u2225\u2225\u03b8t \u2212 \u03b1tA\u22121t gt \u2212 \u03b8\u2217\u2225\u22252At \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 2\u03b1t \u3008gt, \u03b8t \u2212 \u03b8 \u2217\u3009+ \u03b12t \u2329 gt, A \u22121 t gt\n\u232a This yields\n\u3008gt, \u03b8t \u2212 \u03b8\u2217\u3009 \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t+1 \u2212 \u03b8\n\u2217\u20162At 2\u03b1t + \u03b1t 2\n\u2329 gt, A \u22121 t gt \u232a Hence we can upper bound the regret as follows\nR(T )\n\u2264 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t+1 \u2212 \u03b8 \u2217\u20162At 2\u03b1t\n+ T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162A1\n2\u03b11 + T\u2211 t=2 (\u2016\u03b8t \u2212 \u03b8\u2217\u20162At 2\u03b1t \u2212 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At\u22121 2\u03b1t\u22121 ) \u2212 \u2016\u03b8T+1 \u2212 \u03b8\u2217\u20162AT\n2\u03b1t + T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162A1\n2\u03b1\n+ T\u2211 t=2 \u2016\u03b8t \u2212 \u03b8\u2217\u20162\u221atAt\u2212\u221at\u22121At\u22121 2\u03b1 + T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a In the last step we used \u03b1t = \u03b1\u221at . We show that\n\u221a tAt \u2212 \u221a t\u2212 1At\u22121 0 \u2200t \u2208 [T ] (9)\nNote that At are diagonal matrices for all t \u2265 1. We note that\n(At)ii = \u221a vt,i + t,\nand vt,i = \u03b2tvt\u22121,i + (1 \u2212 \u03b2t)g2t,i as well as \u03b2t \u2265 1 \u2212 1t which implies t\u03b2t \u2265 t\u2212 1. We get\n\u221a t(At)ii = \u221a tvt,i + \u221a t t\n= \u221a t\u03b2tvt\u22121,i + t(1\u2212 \u03b2t)g2t,i + \u221a t t\n\u2265 \u221a (t\u2212 1)vt\u22121,i + \u221a t\u2212 1 t\u22121,\nwhere we used in the last inequality that \u221a t t \u2265\u221a\nt\u2212 1 t\u22121. Note that T\u2211 t=2 \u2016\u03b8t \u2212 \u03b8\u2217\u20162\u221atAt\u2212\u221at\u22121At\u22121\n= T\u2211 t=2 d\u2211 i=1\n(\u03b8t,i \u2212 \u03b8\u2217i )2(\u221a tvt,i + \u221a t t \u2212 \u221a (t\u2212 1)vt\u22121,i + \u221a t\u2212 1 t\u22121\n) \u2264\nd\u2211 i=1 D2\u221e\nT\u2211 t=2 (\u221a tvt,i + \u221a t t \u2212 \u221a (t\u2212 1)vt\u22121,i \u2212 \u221a t\u2212 1 t\u22121 ) =\nd\u2211 i=1 D2\u221e (\u221a TvT,i + \u221a T T \u2212 \u221a v1,i \u2212 1 ) where the inequality could be done as we showed before that the difference of the terms in vt,i is non-negative for all i \u2208 [d] and t \u2265 1. As (A1)ii = \u221a v1,i + 1 we get\n\u2016\u03b81 \u2212 \u03b8\u2217\u20162A1 2\u03b1 \u2264 D 2 \u221e 2\u03b1 d\u2211 i=1 (\u221a v1,i + 1 ) .\nThus in total we have\nR(T ) \u2264 D2\u221e\n\u2211d i=1 (\u221a TvT,i + \u221a T T ) 2\u03b1\n+ T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a .\nFinally, with Corollary 4.1 we get the result.\nNote that for \u03b2t = 1\u2212 1t , that is \u03b3 = 1, and t = \u03b4 T where RMSProp corresponds to Adagrad we recover the regret bound of Adagrad in the convex case, see Theorem 2.1, up to the damping factor. Note that in this case\n\u221a TvT,i = \u221a\u221a\u221a\u221a T\u2211 j=1 g2j,i = \u2016g1:T,i\u20162 .\nAlgorithm 4 SC-RMSProp Input: \u03b81 \u2208 C , \u03b40 = 1 , v0 = 0 \u2208 Rd for t = 1 to T do gt \u2208 \u2202ft(\u03b8t) vt = \u03b2tvt\u22121 + (1\u2212 \u03b2t)(gt gt) Set t = \u03b4tt where \u03b4t,i \u2264 \u03b4t\u22121,i for i \u2208 [d] and \u03b1t = \u03b1 t\nAt = diag(vt + t) \u03b8t+1 = P At C ( \u03b8t \u2212 \u03b1tA\u22121t gt ) end for\n4.1. SC-RMSProp\nSimilar to the extension of Adagrad to SC-Adagrad, we present in this section SC-RMSProp which achieves a logarithmic regret bound.\nNote that again there exist choices for the parameters of SC-RMSProp such that it reduces to SC-Adagrad. The correspondence is given by the choice\n\u03b2t = 1\u2212 1\nt , \u03b1t =\n\u03b1 t , t = \u03b4t t ,\nfor which again it follows vt,i = 1t \u2211t j=1 g 2 j,i with the same argument as for RMSProp. Please see Equation (5) for the correspondence. Moreover, with the same argument as for SC-Adagrad we use a decay scheme for the damping factor\nt,i = \u03be2 e\u2212\u03be1 t vt,i\nt , i = 1, . . . , d. for \u03be1 \u2265 0 , \u03be2 > 0\nThe analysis of SC-RMSProp is along the lines of SCAdagrad with some overhead due to the structure of vt.\nLemma 4.2 Let \u03b1t = \u03b1t , 1 \u2212 1 t \u2264 \u03b2t \u2264 1 \u2212 \u03b3 t and At as defined in SC-RMSProp, then it holds for all T \u2265 1,\nT\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2264 \u03b1 2\u03b3 d\u2211 i=1 log (T (vT,i + T,i) 1,i ) + \u03b1\n2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i tvt,i + t t,i\n+ \u03b1\n2\u03b3 d\u2211 i=1 (1\u2212 \u03b3)(1 + log T ) infj\u2208[1,T ] jvj,i + j j,i\nProof: Using \u3008x,Ax\u3009 = A \u2022 diag(xxT ) and with g2t,i = vt,i\u2212\u03b2tvt\u22121,i\n1\u2212\u03b2t and using that At is diagonal with (At)ii = vt,i + t,i, we get\nT\u2211 t=1 \u03b1 2 ((tAt) \u22121 \u2022 diag(gtgTt )) = \u03b1\n2(1\u2212 \u03b21) ((A1)\n\u22121 \u2022 (A1 \u2212 diag( 1)))\n+ T\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (At \u2212 diag( t) (1\u2212 \u03b2t) )) \u2212\nT\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (\u03b2tAt\u22121 \u2212 \u03b2tdiag( t\u22121) (1\u2212 \u03b2t) )) \u2264 \u03b1\n2\u03b3 ((A1)\n\u22121 \u2022 (A1 \u2212 diag( 1)))\n+ T\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 ( tAt \u2212 t\u03b2tAt\u22121 \u03b3 )) +\nT\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (\u2212diag( t) + \u03b2tdiag( t\u22121) (1\u2212 \u03b2t) )) \u2264 \u03b1\n2\u03b3 log ( |A1| |diag( 1)| ) + \u03b1 2\u03b3 T\u2211 t=2 log ( |tAt| |t\u03b2tAt\u22121| ) +\nT\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (\u2212diag( t) + \u03b2tdiag( t\u22121) (1\u2212 \u03b2t) )) \u2264 \u03b1\n2\u03b3 log ( |A1| |diag( 1)| ) + \u03b1 2\u03b3 T\u2211 t=2 log ( |tAt| |(t\u2212 1)At\u22121| ) +\nT\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (\u2212diag( t) + \u03b2tdiag( t\u22121) (1\u2212 \u03b2t) )) In the first inequality we use 11\u2212\u03b2t \u2264 t \u03b3 . In the last step we use, that \u2200t > 1, 1t\u03b2t \u2264 1 t\u22121 . Finally, by upper bounding the last term with Lemma 4.3 T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2264 \u03b1 2\u03b3 log ( |TAT | |diag( 1)|\n) + \u03b1\n2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i tvt,i + t t,i\n+ \u03b1\n2\u03b3 d\u2211 i=1 (1\u2212 \u03b3)(1 + log T ) infj\u2208[1,T ] jvj,i + j j,i\nWe note that\nlog(|TAT |) = log ( d\u220f i=1 (T (vT,i + T,i)) )\n= d\u2211 i=1 log(T (vT,i + T,i)),\nand similar, log(|diag( 1)|) = \u2211d i=1 log( 1,i).\nNote that for \u03b3 = 1 and the choice t = \u03b4tt this reduces to the result of Lemma 3.2.\nLemma 4.3 Let t \u2264 1t and 1\u2212 1 t \u2264 \u03b2t \u2264 1\u2212 \u03b3 t for some 1 \u2265 \u03b3 > 0. Then it holds, T\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (\u2212diag( t) + \u03b2tdiag( t\u22121) (1\u2212 \u03b2t) ))\n\u2264 \u03b1 2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i tvt,i + t t,i\n+ \u03b1\n2\u03b3 d\u2211 i=1 (1\u2212 \u03b3)(1 + log T ) infj\u2208[1,T ] jvj,i + j j,i\nProof: Using \u03b3t \u2264 1\u2212 \u03b2t \u2264 1 t , we get T\u2211 t=2 \u03b1 2 ( (tAt) \u22121 \u2022 (\u2212diag( t) + \u03b2tdiag( t\u22121) (1\u2212 \u03b2t)\n)) \u2264 \u03b1\n2 T\u2211 t=2 ( (tAt) \u22121 \u2022 (\u2212tdiag( t) + t\u03b2tdiag( t\u22121) \u03b3 )) = \u03b1\n2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + t\u03b2t t\u22121,i tvt,i + t t,i\n= \u03b1\n2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i + (t\u03b2t \u2212 (t\u2212 1)) t\u22121,i tvt,i + t t,i\n\u2264 \u03b1 2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i tvt,i + t t,i\n+ \u03b1\n2\u03b3 T\u2211 t=2 d\u2211 i=1 (1\u2212 \u03b3) t\u22121,i tvt,i + t t,i\n\u2264 \u03b1 2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i tvt,i + t t,i\n+ \u03b1\n2\u03b3 d\u2211 i=1 1\u2212 \u03b3 infj\u2208[2,T ] jvj,i + j j,i T\u2211 t=2 1 t\n\u2264 \u03b1 2\u03b3 T\u2211 t=2 d\u2211 i=1 \u2212t t,i + (t\u2212 1) t\u22121,i tvt,i + t t,i\n+ \u03b1\n2\u03b3 d\u2211 i=1 1\u2212 \u03b3 infj\u2208[2,T ] jvj,i + j j,i (1 + log T )\nTheorem 4.2 Let Assumptions A1, A2 hold and let \u03b8t be the sequence generated by SC-RMSProp in Algorithm 4, where gt \u2208 \u2202ft(\u03b8t) and ft : C \u2192 R is an arbitrary \u00b5strongly convex function (\u00b5 \u2208 Rd+) with \u03b1t = \u03b1t for some \u03b1 \u2265 (2\u2212\u03b3)G 2 \u221e\n2mini \u00b5i and 1 \u2212 1t \u2264 \u03b2t \u2264 1 \u2212 \u03b3 t for some 0 <\n\u03b3 \u2264 1. Furthermore, set t = \u03b4tt and assume 1 \u2265 \u03b4t,i > 0 and \u03b4t,i \u2264 \u03b4t\u22121,i\u2200t \u2208 [T ],\u2200i \u2208 [d], then the regret of SCRMSProp can be upper bounded for T \u2265 1 as\nR(T ) \u2264 D 2 \u221e tr(diag(\u03b41))\n2\u03b1 +\n\u03b1\n2\u03b3 d\u2211 i=1 log (TvT,i + \u03b4T,i \u03b41,i )\n+ 1\n2 d\u2211 i=1 inf t\u2208[T ] ( (\u03b8t,i \u2212 \u03b8\u2217i )2 \u03b1 \u2212 \u03b1 \u03b3(tvt,i + t t,i) ) (\u03b4T,i \u2212 \u03b41,i)\n+ \u03b1\n2\u03b3 d\u2211 i=1 (1\u2212 \u03b3)(1 + log T ) infj\u2208[1,T ] jvj,i + j j,i\nProof: We rewrite the regret bound with the definition of \u00b5-strongly convex functions as\nR(T ) = T\u2211 t=1 (ft(\u03b8t)\u2212 ft(\u03b8\u2217))\n\u2264 T\u2211 t=1 \u3008gt, \u03b8t \u2212 \u03b8\u2217\u3009 \u2212 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\nUsing the non-expansiveness of the weighted projection, we get\n\u2016\u03b8t+1 \u2212 \u03b8\u2217\u20162At = \u2225\u2225\u2225PAtC (\u03b8t \u2212 \u03b1tA\u22121t gt)\u2212 \u03b8\u2217\u2225\u2225\u22252\nAt \u2264 \u2225\u2225\u03b8t \u2212 \u03b1tA\u22121t gt \u2212 \u03b8\u2217\u2225\u22252At \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 2\u03b1t \u3008gt, \u03b8t \u2212 \u03b8 \u2217\u3009+ \u03b12t \u2329 gt, A \u22121 t gt\n\u232a This yields\n\u3008gt, \u03b8t \u2212 \u03b8\u2217\u3009 \u2264 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t+1 \u2212 \u03b8\n\u2217\u20162At 2\u03b1t + \u03b1t 2\n\u2329 gt, A \u22121 t gt \u232a Hence we can upper bound the regret as follows\nR(T )\n\u2264 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At \u2212 \u2016\u03b8t+1 \u2212 \u03b8 \u2217\u20162At 2\u03b1t\n+ T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2212 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\n\u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162A1\n2\u03b11 + T\u2211 t=2 (\u2016\u03b8t \u2212 \u03b8\u2217\u20162At 2\u03b1t \u2212 \u2016\u03b8t \u2212 \u03b8\u2217\u20162At\u22121 2\u03b1t\u22121 ) \u2212 \u2016\u03b8T+1 \u2212 \u03b8\u2217\u20162AT\n2\u03b1t + T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2212\nT\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\n\u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162A1\n2\u03b11 + T\u2211 t=2 \u2016\u03b8t \u2212 \u03b8\u2217\u20162tAt\u2212(t\u22121)At\u22121 2\u03b1\n+ T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2212 T\u2211 t=1 \u2016\u03b8t \u2212 \u03b8\u2217\u20162diag(\u00b5)\nNow on imposing the following condition\ntAt \u2212 (t\u2212 1)At\u22121 \u2212 2\u03b1 diag(\u00b5) diag(\u03b4t)\u2212 diag(\u03b4t\u22121)\u2200t \u2265 2\n(10)\nA1 \u2212 2\u03b1diag(\u00b5) diag(\u03b41) (11)\nNote that with t,i = \u03b4t,i t and \u03b4t,i \u2264 \u03b4t\u22121,i for all t \u2265 1 and i \u2208 [d], it holds t t,i \u2264 (t \u2212 1) t\u22121,i. We show regarding the inequality in (10)\ntvt,i + t t,i \u2212 (t\u2212 1)vt\u22121,i \u2212 (t\u2212 1) t\u22121,i \u2212 2\u03b1\u00b5i = t\u03b2t,ivt\u22121,i + t t,i + t(1\u2212 \u03b2t)g2t,i \u2212 (t\u2212 1) t\u22121,i \u2212 (t\u2212 1)vt\u22121,i \u2212 2\u03b1\u00b5i = (t\u03b2t,i \u2212 (t\u2212 1))vt\u22121,i + t(1\u2212 \u03b2t)g2t,i \u2212 2\u03b1\u00b5i + t t,i \u2212 (t\u2212 1) t\u22121,i \u2264 (1\u2212 \u03b3)vt\u22121,i + g2t,i \u2212 2\u03b1\u00b5i + t t,i \u2212 (t\u2212 1) t\u22121,i \u2264 (1\u2212 \u03b3)G2\u221e +G2\u221e \u2212 2\u03b1\u00b5i + t t,i \u2212 (t\u2212 1) t\u22121,i = (2\u2212 \u03b3)G2\u221e \u2212 2\u03b1\u00b5i + t t,i \u2212 (t\u2212 1) t\u22121,i \u2264 t t,i \u2212 (t\u2212 1) t\u22121,i,\nwhere the last inequality follows by choosing \u03b1 \u2265 G2\u221e 2min i \u00b5i (2\u2212 \u03b3) and we have used that\nvt,i = t\u2211 j=1 (1\u2212 \u03b2j) t\u220f k=j+1 \u03b2kg 2 j,i\n\u2264 G2\u221e t\u2211\nj=1 ( t\u220f k=j+1 \u03b2k \u2212 t\u220f k=j \u03b2k )\n\u2264 G2\u221e ( 1\u2212 t\u220f k=1 \u03b2k ) \u2264 G2\u221e\nThe second inequality (11) holds easily with the given choice of \u03b1. Choosing some \u03b2t = 1\u2212 \u03b3t\ntvt,i = t\u03b2t,ivt\u22121,i + t(1\u2212 \u03b2t)g2t,i \u2265 (t\u2212 1)vt\u22121,i + t(1\u2212 \u03b2t)g2t,i \u2265 (t\u2212 1)vt\u22121,i\nHence \u03b4t,i \u2264 \u03b4t\u22121,i where \u03b4t,i = e\u2212 t vt,i for > 0. With t,i = \u03b4t,i t we have t t \u2264 (t\u2212 1) t\u22121.\nR(T )\n\u2264 \u2016\u03b81 \u2212 \u03b8\u2217\u20162diag(\u03b41)\n2\u03b1 + T\u2211 t=2 \u2016\u03b8t \u2212 \u03b8\u2217\u2016diag(\u03b4t)\u2212diag(\u03b4t\u22121) 2\u03b1\n+ T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a = \u2016\u03b81 \u2212 \u03b8\u2217\u20162diag(\u03b41)\n2\u03b1 + T\u2211 t=2 d\u2211 i=1 (\u03b8t,i \u2212 \u03b8\u2217i )2(\u03b4t,i \u2212 \u03b4t\u22121,i) 2\u03b1\n+ T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a \u2264 D 2 \u221e tr(diag(\u03b41))\n2\u03b1 + T\u2211 t=1 \u03b1t 2 \u2329 gt, A \u22121 t gt \u232a +\nT\u2211 t=2 d\u2211 i=1 (\u03b8t,i \u2212 \u03b8\u2217i )2(\u03b4t,i \u2212 \u03b4t\u22121,i) 2\u03b1\n\u2264 D 2 \u221e tr(diag(\u03b41))\n2\u03b1 +\n\u03b1\n2\u03b3 d\u2211 i=1 log (TvT,i + \u03b4T,i \u03b41,i ) + 1\n2 T\u2211 t=2 d\u2211 i=1\n( (\u03b8t,i \u2212 \u03b8\u2217i )2(\u03b4t,i \u2212 \u03b4t\u22121,i)\n\u03b1 \u2212 \u03b1(\u03b4t,i \u2212 \u03b4t\u22121,i)\n\u03b3(tvt,i + t t,i)\n)\n+ \u03b1\n2\u03b3 d\u2211 i=1 (1\u2212 \u03b3)(1 + log T ) infj\u2208[1,T ] jvj,i + j j,i\n\u2264 D 2 \u221e tr(diag(\u03b41))\n2\u03b1 +\n\u03b1\n2\u03b3 d\u2211 i=1 log (TvT,i + \u03b4T,i \u03b41,i ) + 1\n2 d\u2211 i=1 inf t\u2208[T ] ( (\u03b8t,i \u2212 \u03b8\u2217i )2 \u03b1 \u2212 \u03b1 \u03b3(tvt,i + t t,i) ) (\u03b4T,i \u2212 \u03b41,i)\n+ \u03b1\n2\u03b3 d\u2211 i=1 (1\u2212 \u03b3)(1 + log T ) infj\u2208[1,T ] jvj,i + j j,i\nwhere we have used Lemma 4.2 in the last inequality and t,i = \u03b4t,i t for i \u2208 [d] and t \u2265 1.\nNote that the regret bound reduces for \u03b3 = 1 to that of SCAdagrad. For 0 < \u03b3 < 1 a comparison between the bounds is not straightforward as the vt,i terms cannot be compared. It is an interesting future research question whether it is possible to show that one scheme is better than the other one potentially dependent on the problem characteristics.\n5. Experiments The idea of the experiments is to show that the proposed algorithms are useful for standard learning problems in both online and batch settings. We are aware of the fact that in the strongly convex case online to batch conversion is not tight (Hazan & Kale, 2014), however that does not necessarily imply that the algorithms behave generally suboptimal. We compare all algorithms for a strongly convex problem and present relative suboptimality plots, log10 ( f(xt)\u2212p\u2217 p\u2217 ) , where p\u2217 is the global optimum, as well as separate regret plots, where we compare to the best optimal parameter in hindsight for the fraction of training points seen so far. On the other hand RMSProp was originally developed by (Hinton et al., 2012) for usage in deep learning. As discussed before the fixed choice of \u03b2t is\nnot allowed if one wants to get the optimal O( \u221a T ) regret bound in the convex case. Thus we think it is of interest to the deep learning community, if the insights from the convex optimization case transfer to deep learning. Moreover, Adagrad and RMSProp are heavily used in deep learning and thus it is interesting to compare their counterparts SCAdagrad and SC-RMSProp developed for the strongly convex case also in deep learning. For the deep learning experiments we optimize the learning rate once for smallest training objective as well as for best test performance after a fixed number of epochs (typically 200 epochs).\nDatasets: We use three datasets where it is easy, difficult and very difficult to achieve good test performance, just in order to see if this influences the performance. For this purpose we use MNIST (60000 training samples, 10 classes), CIFAR10 (50000 training samples, 10 classes) and CIFAR100 (50000 training samples, 100 classes). We refer to (Krizhevsky, 2009) for more details on the CIFAR datasets.\nAlgorithms: We compare 1) Stochastic Gradient Descent (SGD) (Bottou, 2010) with O(1/t) decaying step-size for the strongly convex problems and for non-convex problems we use a constant learning rate, 2) Adam (Kingma & Bai, 2015) , is used with step size decay of \u03b1t = \u03b1\u221at for strongly\nconvex problems and for non-convex problems we use a constant step-size. 3) Adagrad, see Algorithm 1, remains the same for strongly convex problems and non-convex problems. 4) RMSProp as proposed in (Hinton et al., 2012) is used for both strongly convex problems and non-convex problems with \u03b2t = 0.9 \u2200t \u2265 1. 5) RMSProp (Ours) is used with step-size decay of \u03b1t = \u03b1\u221at and \u03b2t = 1 \u2212 \u03b3 t . In order that the parameter range is similar to the original RMSProp ((Hinton et al., 2012)) we fix as \u03b3 = 0.9 for all experiment (note that for \u03b3 = 1 RMSProp (Ours) is equivalent to Adagrad), 6) SC-RMSProp is used with stepsize \u03b1t = \u03b1 t and \u03b3 = 0.9 as RMSProp (Ours) 7) SC-Adagrad is used with a constant stepsize \u03b1. The decaying damping factor for both SC-Adagrad and SC-RMSProp is used with \u03be1 = 0.1, \u03be2 = 1 for convex problems and we use \u03be1 = 0.1, \u03be2 = 0.1 for non-convex deep learning problems. Finally, the numerical stability parameter \u03b4 used in Adagrad, Adam, RMSProp is set to 10\u22128 as it is typically recommended for these algorithms.\nSetup: Note that all methods have only one varying parameter: the stepsize \u03b1 which we choose from the set of {1, 0.1, 0.01, 0.001, 0.0001} for all experiments. By this setup no method has an advantage just because it has more hyperparameters over which it can optimize. The optimal\nrate is always chosen for each algorithm separately so that one achieves either best training objective or best test performance after a fixed number of epochs.\nStrongly Convex Case - Softmax Regression: Given the training data (xi, yi)i\u2208[m] and let yi \u2208 [K]. we fit a linear model with cross entropy loss and use as regularization the squared Euclidean norm of the weight parameters. The objective is then given as\nJ(\u03b8) = \u2212 1 m m\u2211 i=1 log\n( e\u03b8\nT yi xi+byi\u2211K\nj=1 e \u03b8Tj xi+bj\n) + \u03bb\nK\u2211 k=1 \u2016\u03b8k\u20162\nAll methods are initialized with zero weights. The regularization parameter was chosen so that one achieves the best prediction performance on the test set. The results are shown in Figure 1. We also conduct experiments in an online setting, where we restrict the number of iterations to the number of training samples. Here for all the algorithms, we choose the stepsize resulting in best regret value at the end. We plot the Regret ( in log scale ) vs dataset proportion seen, and as expected SC-Adagrad and SC-RMSProp outperform all the other methods across all the considered datasets. Also, RMSProp (Ours) has a lower regret values than the original RMSProp as shown in Figure 2.\nConvolutional Neural Networks: Here we test a 4-layer CNN with two convolutional (32 filters of size 3 \u00d7 3) and one fully connected layer (128 hidden units followed by 0.5 dropout). The activation function is ReLU and after the last convolutional layer we use max-pooling over a 2 \u00d7 2 window and 0.25 dropout. The final layer is a softmax layer and the final objective is cross-entropy loss. This is a pretty simple standard architecture and we use it for all datasets. The results are shown in Figures 3, 6. SC-RMSProp is competitive in terms of training objective on all datasets though SGD achieves the best performance. SC-Adagrad is not very competitive and the reason seems to be that the numerical stability parameter is too small. RMSProp diverges on CIFAR10 dataset whereas RMSProp (Ours) converges on all datasets and has similar performance as Adagrad in terms of training objective. Both RMSProp (Ours) and SCAdagrad perform better than all the other methods in terms of test accuracy for CIFAR10 dataset. On both CIFAR100 and MNIST datasets SC-RMSProp is very competitive.\nMulti-Layer Perceptron: We also conduct experiments for a 3-layer Multi-Layer perceptron with 2 fully connected hidden layers and a softmax layer according to the number of classes in each dataset. For the first two hidden layers we\nhave 512 units in each layer with ReLU activation function and 0.2 dropout. The final layer is a softmax layer. We report the results in Figures 7, 8. On all the datasets, SCAdagrad and SC-RMSProp perform better in terms of Test accuracy and also have the best training objective performance on CIFAR10 dataset. On MNIST dataset, Adagrad and RMSProp(Ours) achieves best training objective performance however SC-Adagrad and SC-RMSProp eventually performs as good as Adagrad. Here, the performance is not as competitive as Adagrad, because the numerical stability decay parameter of SC-Adagrad and SC-RMSProp are too prohibitive.\nResidual Network: We also conduct experiments for ResNet-18 network proposed in (He et al., 2016a) where the residual blocks are used with modifications proposed in (He et al., 2016b) on CIFAR10 dataset. We report the results in Figures 4. SC-Adagrad, SC-RMSProp and RMSProp (Ours) have the best performance in terms of test Accuracy and RMSProp (Ours) has the best performance in terms of training objective along with Adagrad.\nGiven these experiments, we think that SC-Adagrad, SCRMSProp and RMSProp (Ours) are valuable new adaptive gradient techniques for deep learning.\n6. Conclusion We have analyzed RMSProp originally proposed in the deep learning community in the framework of online convex optimization. We show that the conditions for convergence of RMSProp for the convex case are different than what is used by (Hinton et al., 2012) and that this leads to better performance in practice. We also propose variants SC-Adagrad and SC-RMSProp which achieve logarithmic regret bounds for the strongly convex case. Moreover, they perform very well for different network models and datasets and thus they are an interesting alternative to existing adaptive gradient schemes. In the future we want to explore why these algorithms perform so well in deep learning tasks even though they have been designed for the strongly convex case.\nAcknowledgements We would like to thank Shweta Mahajan and all the reviewers for their insightful comments.\nReferences Bottou, L. Large-scale machine learning with stochastic\ngradient descent. In Proceedings of COMPSTAT\u20192010, pp. 177\u2013186. Springer, 2010.\nDaniel, C., Taylor, J., and Nowozin, S. Learning step size controllers for robust neural network training. In AAAI, 2016.\nDauphin, Y., de Vries, H., and Bengio, Y. Equilibrated adaptive learning rates for non-convex optimization. In NIPS, 2015.\nDuchi, J., Hazan, E., and Singer, Y. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nHazan, E. Introduction to online convex optimization. Foundations and Trends in Optimization, 2:157\u2013325, 2016.\nHazan, E. and Kale, S. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. Journal of Machine Learning Research, 15(1):2489\u20132512, 2014.\nHazan, E., Agarwal, A., and Kale, S. Logarithmic regret algorithms for online convex optimization. Machine Learning, 69(2-3):169\u2013192, 2007.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016a.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Identity mappings in deep residual networks. In European Conference on Computer Vision, pp. 630\u2013645. Springer, 2016b.\nHinton, G., Srivastava, N., and Swersky, K. Lecture 6d - a separate, adaptive learning rate for each connection. Slides of Lecture Neural Networks for Machine Learning, 2012.\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2016.\nKingma, D. P. and Bai, J. L. Adam: a method for stochastic optimization. ICLR, 2015.\nKoushik, Jayanth and Hayashi, Hiroaki. Improving stochastic gradient descent with feedback. arXiv preprint arXiv:1611.01505, 2016.\nKrizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\nMcMahan, H Brendan. A survey of algorithms and analysis for adaptive online learning. arXiv preprint arXiv:1403.3465, 2014.\nRuder, S. An overview of gradient descent optimization algorithms. preprint, arXiv:1609.04747v1, 2016.\nSchaul, T., Antonoglou, I., and Silver, D. Unit tests for stochastic optimization. In ICLR, 2014.\nSchmidhuber, J. Deep learning in neural networks: An overview. Neural Networks, 61:85 \u2013 117, 2015.\nSrivastava, Nitish, Hinton, Geoffrey E, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1): 1929\u20131958, 2014.\nSzegedy, C., Ioffe, S., and Vanhoucke, V. Inception-v4, inception-resnet and the impact of residual connections on learning. In ICLR Workshop, 2016a.\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In CVPR, 2016b.\nZeiler, M. D. ADADELTA: An adaptive learning rate method. preprint, arXiv:1212.5701v1, 2012.\nZinkevich, M. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003."}], "references": [{"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "In Proceedings of COMPSTAT\u20192010,", "citeRegEx": "Bottou,? \\Q2010\\E", "shortCiteRegEx": "Bottou", "year": 2010}, {"title": "Learning step size controllers for robust neural network training", "author": ["C. Daniel", "J. Taylor", "S. Nowozin"], "venue": "In AAAI,", "citeRegEx": "Daniel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2016}, {"title": "Equilibrated adaptive learning rates for non-convex optimization", "author": ["Y. Dauphin", "H. de Vries", "Y. Bengio"], "venue": "In NIPS,", "citeRegEx": "Dauphin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Introduction to online convex optimization", "author": ["E. Hazan"], "venue": "Foundations and Trends in Optimization,", "citeRegEx": "Hazan,? \\Q2016\\E", "shortCiteRegEx": "Hazan", "year": 2016}, {"title": "Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Hazan and Kale,? \\Q2014\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2014}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Identity mappings in deep residual networks", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Lecture 6d - a separate, adaptive learning rate for each connection", "author": ["G. Hinton", "N. Srivastava", "K. Swersky"], "venue": "Slides of Lecture Neural Networks for Machine Learning,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "In CVPR,", "citeRegEx": "Karpathy and Fei.Fei,? \\Q2016\\E", "shortCiteRegEx": "Karpathy and Fei.Fei", "year": 2016}, {"title": "Adam: a method for stochastic optimization", "author": ["D.P. Kingma", "J.L. Bai"], "venue": "ICLR,", "citeRegEx": "Kingma and Bai,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Bai", "year": 2015}, {"title": "Improving stochastic gradient descent with feedback", "author": ["Koushik", "Jayanth", "Hayashi", "Hiroaki"], "venue": "arXiv preprint arXiv:1611.01505,", "citeRegEx": "Koushik et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Koushik et al\\.", "year": 2016}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, University of Toronto,", "citeRegEx": "Krizhevsky,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky", "year": 2009}, {"title": "A survey of algorithms and analysis for adaptive online learning", "author": ["McMahan", "H Brendan"], "venue": "arXiv preprint arXiv:1403.3465,", "citeRegEx": "McMahan and Brendan.,? \\Q2014\\E", "shortCiteRegEx": "McMahan and Brendan.", "year": 2014}, {"title": "An overview of gradient descent optimization", "author": ["S. Ruder"], "venue": "algorithms. preprint,", "citeRegEx": "Ruder,? \\Q2016\\E", "shortCiteRegEx": "Ruder", "year": 2016}, {"title": "Unit tests for stochastic optimization", "author": ["T. Schaul", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "Schaul et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Schaul et al\\.", "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber", "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Srivastava", "Nitish", "Hinton", "Geoffrey E", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Inception-v4, inception-resnet and the impact of residual connections on learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "In ICLR Workshop,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "Rethinking the inception architecture for computer vision", "author": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "venue": "In CVPR,", "citeRegEx": "Szegedy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2016}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "preprint, arXiv:1212.5701v1,", "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Introduction There has recently been a lot of work on adaptive gradient algorithms such as Adagrad (Duchi et al., 2011), RMSProp (Hinton et al.", "startOffset": 99, "endOffset": 119}, {"referenceID": 9, "context": ", 2011), RMSProp (Hinton et al., 2012), ADADELTA (Zeiler, 2012), and Adam (Kingma & Bai, 2015).", "startOffset": 17, "endOffset": 38}, {"referenceID": 21, "context": ", 2012), ADADELTA (Zeiler, 2012), and Adam (Kingma & Bai, 2015).", "startOffset": 18, "endOffset": 32}, {"referenceID": 3, "context": "The original analysis of Adagrad (Duchi et al., 2011) was limited to the case of all convex functions for which it obtained a datadependent regret bound of order O( \u221a T ) which is known to be optimal (Hazan, 2016) for this class.", "startOffset": 33, "endOffset": 53}, {"referenceID": 4, "context": ", 2011) was limited to the case of all convex functions for which it obtained a datadependent regret bound of order O( \u221a T ) which is known to be optimal (Hazan, 2016) for this class.", "startOffset": 154, "endOffset": 167}, {"referenceID": 6, "context": "It has been shown in (Hazan et al., 2007) that one can achieve much better logarithmic regret bounds for the class of strongly convex functions.", "startOffset": 21, "endOffset": 41}, {"referenceID": 6, "context": "It is known that such bounds can be much better in practice than data independent bounds (Hazan et al., 2007),(McMahan, 2014).", "startOffset": 89, "endOffset": 109}, {"referenceID": 15, "context": "Interestingly, SC-Adagrad has been discussed in (Ruder, 2016), where it is said that \u201cit does not to work\u201d.", "startOffset": 48, "endOffset": 61}, {"referenceID": 22, "context": "One of the first methods which achieves the optimal regret bound of O( \u221a T ) for convex problems is online projected gradient descent (Zinkevich, 2003), defined as \u03b8t+1 = PC(\u03b8t \u2212 \u03b1tgt) (3) where \u03b1t = \u03b1 t is the step-size scheme and gt is a (sub)gradient of ft at \u03b8t.", "startOffset": 134, "endOffset": 151}, {"referenceID": 6, "context": "With \u03b1t = \u03b1t , online projected gradient descent method achieves the optimal O(log(T )) regret bound for strongly-convex problems (Hazan et al., 2007).", "startOffset": 130, "endOffset": 150}, {"referenceID": 3, "context": "If the adversarial is allowed to choose from the set of all possible convex functions on C \u2282 R, then Adagrad achieves the regret bound of orderO( \u221a T ) as shown in (Duchi et al., 2011).", "startOffset": 164, "endOffset": 184}, {"referenceID": 4, "context": "(Hazan, 2016).", "startOffset": 0, "endOffset": 13}, {"referenceID": 3, "context": "For better comparison to our results for RMSProp, we recall the result from (Duchi et al., 2011) in our notation.", "startOffset": 76, "endOffset": 96}, {"referenceID": 3, "context": "1 (Duchi et al., 2011) Let Assumptions A1, A2 hold and let \u03b8t be the sequence generated by Adagrad in Algorithm 1, where gt \u2208 \u2202ft(\u03b8t) and ft : C \u2192 R is an arbitrary convex function, then for stepsize \u03b1 > 0 the regret is upper bounded as R(T ) \u2264 D 2 \u221e 2\u03b1 d \u2211", "startOffset": 2, "endOffset": 22}, {"referenceID": 6, "context": "Strongly convex Adagrad (SC-Adagrad) The modification SC-Adagrad of Adagrad which we propose in the following can be motivated by the observation that the online projected gradient descent (Hazan et al., 2007) uses stepsizes of order \u03b1 = O( 1 T ) in order to achieve the logarithmic regret bound for strongly convex functions.", "startOffset": 189, "endOffset": 209}, {"referenceID": 15, "context": "This is probably why in (Ruder, 2016), the modification of Adagrad where one \u201cdrops the square-root\u201d did not work.", "startOffset": 24, "endOffset": 37}, {"referenceID": 6, "context": "1 [Lemma 12 (Hazan et al., 2007)] Let A,B be positive definite matrices, let A B 0 then A\u22121 \u2022 (A\u2212B) \u2264 log ( |A| |B| ) (6)", "startOffset": 12, "endOffset": 32}, {"referenceID": 6, "context": "1 also see for Lemma 12 of (Hazan et al., 2007).", "startOffset": 27, "endOffset": 47}, {"referenceID": 16, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 2, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 1, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 17, "context": "RMSProp and SC-RMSProp RMSProp is one of the most popular adaptive gradient algorithms used for the training of deep neural networks (Schaul et al., 2014; Dauphin et al., 2015; Daniel et al., 2016; Schmidhuber, 2015).", "startOffset": 133, "endOffset": 216}, {"referenceID": 16, "context": "Note that RMSProp outperformed other adaptive methods like Adagrad order Adadelta as well as SGD with momentum in a large number of tests in (Schaul et al., 2014).", "startOffset": 141, "endOffset": 162}, {"referenceID": 1, "context": "It has been argued that if the changes in the parameter update are approximately Gaussian distributed, then the matrix At can be seen as a preconditioner which approximates the diagonal of the Hessian (Daniel et al., 2016).", "startOffset": 201, "endOffset": 222}, {"referenceID": 9, "context": "This is in contrast to the original suggestion of (Hinton et al., 2012) to choose \u03b2t = 0.", "startOffset": 50, "endOffset": 71}, {"referenceID": 9, "context": "On the other hand RMSProp was originally developed by (Hinton et al., 2012) for usage in deep learning.", "startOffset": 54, "endOffset": 75}, {"referenceID": 13, "context": "We refer to (Krizhevsky, 2009) for more details on the CIFAR datasets.", "startOffset": 12, "endOffset": 30}, {"referenceID": 0, "context": "Algorithms: We compare 1) Stochastic Gradient Descent (SGD) (Bottou, 2010) with O(1/t) decaying step-size for the strongly convex problems and for non-convex problems we use a constant learning rate, 2) Adam (Kingma & Bai, 2015) , is used with step size decay of \u03b1t = \u03b1 t for strongly convex problems and for non-convex problems we use a constant step-size.", "startOffset": 60, "endOffset": 74}, {"referenceID": 9, "context": "4) RMSProp as proposed in (Hinton et al., 2012) is used for both strongly convex problems and non-convex problems with \u03b2t = 0.", "startOffset": 26, "endOffset": 47}, {"referenceID": 9, "context": "In order that the parameter range is similar to the original RMSProp ((Hinton et al., 2012)) we fix as \u03b3 = 0.", "startOffset": 70, "endOffset": 91}, {"referenceID": 9, "context": "We show that the conditions for convergence of RMSProp for the convex case are different than what is used by (Hinton et al., 2012) and that this leads to better performance in practice.", "startOffset": 110, "endOffset": 131}], "year": 2017, "abstractText": "Adaptive gradient methods have become recently very popular, in particular as they have been shown to be useful in the training of deep neural networks. In this paper we have analyzed RMSProp, originally proposed for the training of deep neural networks, in the context of online convex optimization and show \u221a T -type regret bounds. Moreover, we propose two variants SC-Adagrad and SC-RMSProp for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the experiments that these new variants outperform other adaptive gradient techniques or stochastic gradient descent in the optimization of strongly convex functions as well as in training of deep neural networks.", "creator": "LaTeX with hyperref package"}}}