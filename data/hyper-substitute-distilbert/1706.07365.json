{"id": "1706.07365", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2017", "title": "Pixels to Graphs by Associative Embedding", "abstract": "graphs allow a useful abstraction of image content. not only can graphs represent ideas about individual objects in a scene but humans better capture potential interactions between pairs of objects. we present a method manually training a distributed behavior network such that it benefits from original input image and creates a full graph. this should done end - to - base until good single stage with the improvement of network diagrams. the network learns neural thoroughly recall all of the images that draw up sensor representation ; piece them together. kelley benchmark on the visual interaction presentation, and report a red @ accuracy of 33. 37 % found as later google state - eighty - one - art at 89. 4 %, a later promising improvement : the challenging task dynamic scene graph processing.", "histories": [["v1", "Thu, 22 Jun 2017 15:20:25 GMT  (4886kb,D)", "http://arxiv.org/abs/1706.07365v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["alejandro newell", "jia deng"], "accepted": true, "id": "1706.07365"}, "pdf": {"name": "1706.07365.pdf", "metadata": {"source": "CRF", "title": "Pixels to Graphs by Associative Embedding", "authors": ["Alejandro Newell", "Jia Deng"], "emails": ["jiadeng}@umich.edu", "Recall@50"], "sections": [{"heading": "1 Introduction", "text": "Extracting semantics from images is one of the main goals of computer vision. Recent years have seen rapid progress in the classification and localization of objects [7, 24, 10]. But a bag of labeled and localized objects is an impoverished representation of image semantics: it tells us what and where the objects are (\u201cperson\u201d and \u201ccar\u201d), but does not tell us about their relations and interactions (\u201cperson next to car\u201d). A necessary step is thus to not only detect objects but to identify the relations between them. We consider this problem in the context of graph prediction: given an image, producing a semantic graph whose vertices represent objects and whose edges represent relationships.\nEnd-to-end training of convolutional networks has proven to be a highly effective strategy for image understanding tasks. It is therefore natural to ask whether the same strategy would be viable for predicting graphs from pixels. Existing approaches, however, tend to break the problem down into more manageable steps. For example, one might run an object detection system to propose all of the objects in the scene, then evaluate on pairs of objects to identify the relationships between them. [18] This breakdown often restricts the visual features used in later steps and limits reasoning over the full graph and over the full contents of the image.\nIn this paper we propose a new approach that trains a network to directly predict a complete graph from a raw set of pixels. Our proposed supervision allows end-to-end training of a single network such that all factors that might determine a graph can be considered internally in the network architecture. The entire image context can be used to decide what vertices are present and how they connect to each other.\nTo do this, we treat all graph elements\u2014vertices and edges\u2014as visual entities to be detected as in a standard object detection pipeline. Specifically, a vertex is an instance of an object (\u201cperson\u201d), and an edge is an instance of an object-object relation (\u201cperson next to car\u201d). Since the graph is derived from pixels, each vertex or edge is naturally grounded in the image and has spatial support. Detecting vertices and edges therefore reduces to the problem of scoring candidate detections as in established object detection approaches [24].\nObjects and object relations are detected separately, but to form a graph, we need to know which objects belong to which relations, that is, which detected vertices are connected by a detected edge.\nar X\niv :1\n70 6.\n07 36\n5v 1\n[ cs\n.C V\n] 2\n2 Ju\nn 20\nThe network needs a way for detections to refer to each other, and for that we draw inspiration from associative embeddings [20], a method originally proposed for joint detection and grouping in the context of multiperson pose estimation. For pose, the idea is to predict an embedding vector for each detection of a body joint such that detections with similar embeddings can be grouped to form an individual person. In other words, the embeddings serve as the identifier of groups and each detection uses an embedding to point to a group.\nWe adapt this idea towards more general graph prediction. We train the network to predict associative embeddings with each detection\u2014for every vertex the network predicts a unique identifier in the form of a vector embedding; for every edge the network predicts two vector embeddings which serve as pointers to the source and destination vertices. Once the network is trained it is straightforward to match the embeddings from detected edges to each vertex and construct a final graph.\nThere is one further issue that we address in this work. That is, how to deal with detections grounded at the same location in the image. Frequently in graph prediction, multiple vertices or edges may appear in the same place. Supervision of this is difficult as training a network traditionally requires telling it exactly what appears and where. With an unordered set of overlapping detections there may not be a direct mapping to explicitly lay this out. Consider a set of object relations grounded at the same pixel location. Assume the network has some fixed output space consisting of discrete \u201cslots\u201d in which detections can appear. It is unclear how to define a mapping so that the network has a consistent rule for organizing its relation predictions into these slots. We address this problem by not enforcing any explicit mapping at all, and instead provide supervision such that it does not matter how the network chooses to fill its output, a correct loss can still be applied.\nOur contributions are a novel use of associative embeddings for connecting the vertices and edges of a graph, and a technique for supervising an unordered set of network outputs. Together these form the building blocks of our system for direct graph prediction from pixels. We apply our method to the task of generating a semantic graph of objects and relations and test on the Visual Genome dataset [14]. We achieve state-of-the-art results improving performance over prior work by nearly a factor of three on the most difficult task setting."}, {"heading": "2 Related Work", "text": "Relationship detection: There are many ways to frame the task of identifying objects and the relationships between them. This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12]. In all of these settings, the aim is to correctly determine the relationships between pairs of objects and ground this in the image with accurate object bounding boxes.\nVisual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23]. The open-ended and challenging nature of the task lends itself to a variety of diverse approaches and solutions. For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];\nlearning to project proposed objects into a vector space such that the difference between two object vectors is informative of the relationship between them [27].\nMost of these approaches rely on generated bounding boxes from a Region Proposal Network (RPN) [24]. Our method does not require proposed boxes and can produce detections directly from the image. However proposals can be incorporated as additional input to improve performance. Furthermore, many methods process pairs of objects in isolation whereas we train a network to process the whole image and produce all object and relationship detections at once.\nAssociative Embedding: Vector embeddings are used in a variety of contexts. For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13]. Recent work uses vector embeddings to group together body joints for multiperson pose estimation [20]. These are referred to as associative embeddings since supervision does not require the network to output a particular vector value, and instead uses the distances between pairs of embeddings to calculate a loss. What is important is not the exact value of the vector but how it relates to the other embeddings produced by the network.\nMore specifically, in [20] a network is trained to detect body joints of the various people in an image. In addition, it must produce a vector embedding for each of its detections. The embedding is used to identify which person a particular joint belongs to. This is done by ensuring that all joints that belong to a single individual produce the same output embedding, and that the embeddings across individuals are sufficiently different to separate detections out into discrete groups. In a certain sense, this approach does define a graph, but it is restricted in that it can only represent clusters of nodes. For the purposes of our work, we will take a different perspective on the associative embedding loss in order to express any arbitrary graph as defined by a set of vertices and directed edges."}, {"heading": "3 Pixels\u2192 Graph", "text": "Our goal is to construct a graph from a set of pixels. In particular, we want to construct a graph grounded in the space of these pixels. Meaning that in addition to identifying vertices of the graph, we want to know their precise locations. A vertex in this case can refer to any object of interest in the scene including people, cars, clothing, and buildings. The relationships between these objects is then captured by the edges of the graph. These relationships may include verbs (eating, riding), spatial relations (on the left of, behind), and comparisons (smaller than, same color as).\nMore formally we consider a directed graph G = (V,E). A given vertex vi \u2208 V is grounded at a location (xi, yi) and defined by its class and bounding box. Each edge e \u2208 E takes the form ei = (vs, vt, ri) defining a relationship of type ri from vs to vt. We train a network to explicitly define V and E. This training is done end-to-end on a single network, allowing the network to reason fully over the image and all possible components of the graph when making its predictions.\nWhile production of the graph occurs all at once, it helps to think of the process in two main steps: detecting individual elements of the graph, and connecting these elements together. For the first step, the network indicates where vertices and edges are likely to exist and predicts the properties of these detections. For the second, we determine which two vertices are connected by a detected edge. We describe these two steps in detail in the following subsections."}, {"heading": "3.1 Detecting graph elements", "text": "First, the network must find all of the vertices and edges that make up a graph. Each graph element is grounded at a pixel location which the network must identify. In a scene graph where vertices correspond to object detections, the center of the object bounding box will serve as the grounding location. We ground edges at the midpoint of the source and target vertices: (bxs+xt2 c, b ys+yt 2 c). With this grounding in mind, we can detect individual elements by using a network that produces per-pixel features at a high output resolution. The feature vector at a pixel determines if an edge or vertex is present at that location, and if so is used to predict the properties of that element.\nA convolutional neural network is used to process the image and produce a feature tensor of size h x w x f . All information necessary to define a vertex or edge is thus encoded at particular pixel in a feature vector of length f . Note that even at a high output resolution, multiple graph elements may\nbe grounded at the same location. The following discussion assumes up to one vertex and edge can exist at a given pixel, and we elaborate on how we accommodate multiple detections in Section 3.3.\nWe use a stacked hourglass network [21] to process an image and produce the output feature tensor. While our method has no strict dependence on network architecture, there are some properties that are important for this task. The hourglass design combines global and local information to reason over the full image and produce high quality per-pixel predictions. This is originally done for human pose prediction which requires global reasoning over the structure of the body, but also precise localization of individual joints. Similar logic applies to scene graphs where the context of the whole scene must be taken into account, but we wish to preserve the local information of individual elements.\nAn important design choice here is the output resolution of the network. It does not have to match the full input resolution, but there are a few details worth considering. First, it is possible for elements to be grounded at the exact same pixel. The lower the output resolution, the higher the probability of overlapping detections. Our approach allows this, but the fewer overlapping detections, the better. All information necessary to define these elements must be encoded into a single feature vector of length f which gets more difficult to do as more elements occupy a given location. Another detail is that increasing the output resolution aids in performing better localization.\nTo predict the presence of graph elements we take the final feature tensor and apply a 1x1 convolution and sigmoid activation to produce two heatmaps (one for vertices and another for edges). Each heatmap indicates the likelihood that a vertex or edge exists at a given pixel. Supervision is a binary cross-entropy loss on the heatmap activations, and we threshold on the result to produce a candidate set of detections.\nNext, for each of these detections we must predict their properties such as their class label. We extract the feature vector from the corresponding location of a detection, and use the vector as input to a set of fully connected networks. A separate network is used for each property we wish to predict, and each consists of a single hidden layer with f nodes. This is illustrated above in Figure 2. During training we use the ground truth locations of vertices and edges to extract features. A softmax loss is used to supervise labels like object class and relationship predicate. And to predict bounding box information we use anchor boxes and regress offsets based on the approach in Faster-RCNN [24].\nIn summary, the detection pipeline works as follows: We pass the image through a network to produce a set of per-pixel features. These features are first used to produce heatmaps identifying vertex and edge locations. Individual feature vectors are extracted from the top heatmap locations to predict the appropriate vertex and edge properties. The final result is a pool of vertex and edge detections that together will compose the graph."}, {"heading": "3.2 Connecting elements with associative embeddings", "text": "Next, the various pieces of the graph need to be put together. This is made possible by training the network to produce additional outputs in the same step as the class and bounding box prediction. For every vertex, the network produces a unique identifier in the form of a vector embedding, and for every edge, it must produce the corresponding embeddings to refer to its source and destination\nvertices. The network must learn to ensure that embeddings are different across different vertices, and that all embeddings that refer to a single vertex are the same.\nThese embeddings are critical for explicitly laying out the definition of a graph. For instance, while it is helpful that edge detections are grounded at the midpoint of two vertices, this ultimately does not address a couple of critical details for correctly constructing the graph. The midpoint does not indicate which vertex serves as the source and which serves as the destination, nor does it disambiguate between pairs of vertices that happen to share the same midpoint.\nTo train the network to produce a coherent set of embeddings we build off of the loss penalty used in [20]. During training, we have a ground truth set of annotations defining the unique objects in the scene and the edges between these objects. This allows us to enforce two penalties: that an edge points to a vertex by matching its output embedding as closely as possible, and that the embedding vectors produced for each vertex are sufficiently different. We think of the first as \u201cpulling together\u201d all references to a single vertex, and the second as \u201cpushing apart\u201d the references to different individual vertices.\nWe consider an embedding hi \u2208 Rd produced for a vertex vi \u2208 V . All edges that connect to this vertex produce a set of embeddings h\u2032ik, k = 1, ...,Ki where Ki is the total number of references to that vertex. Given an image with n objects the loss to \u201cpull together\u201d these embeddings is:\nLpull = 1\u2211n\ni=1 Ki n\u2211 i=1 Ki\u2211 k=1 (hi \u2212 h\u2032ik)2\nTo \u201cpush apart\u201d embeddings across different vertices we use a slightly different penalty from what is described in [20], instead applying a margin-based penalty similar to [9]:\nLpush = n\u22121\u2211 i=1 n\u2211 j=i+1 max(0,m\u2212 ||hi \u2212 hj ||)\nIntuitively, Lpush is at its highest the closer hi and hj are to each other. The penalty drops off sharply as the distance between hi and hj grows, eventually hitting zero once the distance is greater than a given margin m. On the flip side, for some edge connected to a vertex vi, the loss Lpull will quickly grow the further its reference embedding h\u2032i is from hi.\nThe two penalties are weighted equally leaving a final associative embedding loss of Lpull + Lpush. In this work, we use m = 8 and d = 8. Convergence of the network improves greatly after increasing the dimension d of tags up from 1 as used in [20].\nOnce the network is trained with this loss, full construction of the graph can be performed with a trivial postprocessing step. The network produces a pool of vertex and edge detections. For every edge, we look at the source and destination embeddings and match them to the closest embedding amongst the detected vertices. Multiple edges may have the same source and target vertices, vs and vt, and it is also possible for vs to equal vt."}, {"heading": "3.3 Support for overlapping detections", "text": "In scene graphs, there are going to be many cases where multiple vertices or multiple edges will be grounded at the same pixel location, for example, person wearing shirt \u2014 shirt on person. For that matter, people and their clothing are often centered at the same location. So, the detection pipeline must be extended to support multiple detections at the same pixel.\nOne way of dealing with this is to define an extra axis that allows for discrete separation of detections at a given x, y location. For example, one could split up objects along a third spatial dimension assuming the z-axis were annotated, or perhaps separate them by bounding box anchors. In either of these cases there is a visual cue guiding the network so that it can learn a consistent rule for assigning new detections to a correct slot in the third dimension. Unfortunately this idea cannot be applied as easily to relationship detections. It is unclear how to define a third axis such that there is a reliable and consistent bin assignment for each relationship.\nIn our approach, we still separate detections out into several discrete bins, but address the issue of assignment by not enforcing any specific assignment at all. This means that for a given detection we strictly supervise the x, y location in which it is to appear, but allow it to show up in one of several \u201cslots\u201d. We have no way of knowing ahead of time in which slot it will be placed by the network, so this means an extra step must be taken at training time to identify where we think the network has placed its predictions and then enforce the loss at those slots.\nWe define so and sr to be the number of slots available for objects and relationships respectively. We modify the network pipeline so that instead of producing predictions for a single object and relationship at a pixel, a feature vector is used to produce predictions for a set of so objects and sr relationships. That is, given a feature vector f from a single pixel, the network will for example output so object class labels, so bounding box predictions, and so embeddings. This is done with separate fully connected layers predicting the various object and relationship properties for each available slot. No weights are shared amongst these layers. Furthermore, we add an additional output to serve as a score indicating whether or not a detection exists at each slot.\nDuring training, we have some number of ground truth objects, between 1 and so, grounded at a particular pixel. We do not know which of the so outputs of the network will correspond to which objects, so we must perform a matching step. The network produces distributions across possible object classes and bounding box sizes, so we try to best match the outputs to the ground truth information we have available. We construct a reference vector by concatenating one-hot encodings of the class and bounding box anchor for a given object. Then we compare these reference vectors to the output distributions produced at each slot. The Hungarian method is used to perform a maximum matching step such that ground truth annotations are assigned to the best possible slot, but no two annotations are assigned to the same slot.\nMatching for relationships is similar. The ground truth reference vector is constructed by concatenating a one-hot encoding of its class with the output embeddings hs and ht from the source and destination vertices, vs and vt. Once the best matching has been determined we have a correspondence between the network predictions and the set of ground truth annotations and can now apply the various losses. We also supervise the score for each slot depending on whether or not it is matched up to a ground truth detection - thus teaching the network to indicate a \u201cfull\u201d or \u201cempty\u201d slot.\nThis matching process is only used during training. At test time, we extract object and relationship detections from the network by first thresholding on the heatmaps to find a set of candidate pixel locations, and then thresholding on individual slot scores to see which slots have produced detections."}, {"heading": "4 Implementation details", "text": "We train a stacked hourglass architecture [21] in TensorFlow [1]. The input to the network is a 512x512 image, with an output resolution of 64x64. The output feature length f is 256. We make a slight modification to the orginal hourglass design: doubling the number of features to 512 at the two lowest resolutions of the hourglass. All losses - classification, bounding box regression, associative embedding - are weighted equally throughout the course of training. We set so = 3 and sr = 6 which is sufficient to completely accommodate the detection annotations for all but a small fraction of cases.\nIncorporating prior detections: In some problem settings, a prior set of object detections may be made available either as ground truth annotations or as proposals from an independent system. It is good to have some way of incorporating these into the network. We do this by formatting an object detection as a two channel input with one channel activating at the center of the object detection and another serving as a binary mask of the object bounding box.\nIf provided with a large set of detections, a set of input channels is used. These channels are either separated by class, or if no class information is available, by bounding box size. To reduce computational cost this additional input is incorporated after several layers of convolution and pooling have been applied to the input image.\nSparse supervision: It is important to note that it is almost impossible to exhaustively annotate images for scene graphs. A large number of possible relationships can be described between pairs of objects in a real-world scene. The network is likely to generate many reasonable predictions that are not covered in the ground truth. We want to reduce the penalty associated with these detections and\nencourage the network to produce as many detections as possible. There are a few properties of our training pipeline that are conducive to this.\nFor example, we do not need to supervise the entire heatmap for object and relationship detections. Instead, we apply a loss at the pixels we know correspond to positive detections, and then randomly sample some fraction from the rest of the image to serve as negatives. This balances the proportion of positive and negative samples, and reduces the chance of falsely penalizing unannotated detections."}, {"heading": "5 Experiments", "text": "Dataset: We evaluate the performance of our method on the Visual Genome dataset [14]. Visual Genome consists of 108,077 images annotated with object detections and object-object relationships, and it serves as a challenging benchmark for scene graph generation on real world images. Some processing has to be done before using the dataset as objects and relationships are annotated with natural language not with discrete classes, and many redundant bounding box detections are provided for individual objects. To make a direct comparison to prior work we use the preprocessed version of the set made available by Xu et al. [26]. Their network is trained to predict the 150 most frequent object classes and 50 most frequent relationship predicates in the dataset. We use the same categories, as well as the same training and test split as defined by the authors.\nTask: The scene graph task is defined as the production of a set of subject-predicate-object tuples. A proposed tuple is composed of two objects defined by their class and bounding box and the relationship between them. A tuple is correct if the object and relationship classes match those of a ground truth annotation and the two objects have at least a 0.5 IoU overlap with the corresponding ground truth objects. To avoid penalizing extra detections that may be correct but missing an annotation, the standard evaluation metric used for scene graphs is Recall@k which measures the fraction of ground\ntruth tuples to appear in a set of k proposals. Following [26], we report performance on three problem settings:\nSGGen: Detect and classify all objects and determine the relationships between them.\nSGCls: Ground truth object boxes are provided, classify them and determine their relationships.\nPredCls: Boxes and classes are provided for all objects, predict their relationships.\nSGGen corresponds to the full scene graph task while PredCls allows us to focus exclusively on predicate classification. Example predictions on the SgGen and PredCls tasks are shown in Figure 3. It can be seen in Table 1 that on all three settings, we achieve a significant improvement in performance over prior work. It is worth noting that prior approaches to this problem require a set of object proposal boxes in order to produce their predictions. For the full scene graph task (SGGen) these detections are provided by a Region Proposal Network (RPN) [24]. We evaluate performance with and without the use of RPN boxes, and achieve promising results even without the use of proposal boxes - using nothing but the raw image as input. Furthermore, the network is trained from scratch, and does not rely on pretraining on other datasets.\nDiscussion: There are a few interesting results that emerge from our trained model. The network exhibits a number of biases in its predictions. For one, the vast majority of predicate predictions correspond to a small fraction of the 50 predicate classes. Relationships like \u201con\u201d and \u201cwearing\u201d tend to completely dominate the network output, and this is in large part a function of the distribution of ground truth annotations of Visual Genome. There are several orders of magnitude more examples for \u201con\u201d than most other predicate classes. This discrepancy becomes especially apparent when looking at the performance per predicate class in Table 2. The poor results on the worst classes do not have much effect on final performance since there are so few instances of relationships labeled with those predicates.\nWe do some additional analysis to see how the network fills its \u201cslots\u201d for relationship detection. Remember, there is no explicit mapping telling the network which slots it should put particular detections. From Figure 4, we can see that the network ends up dividing slots up to correspond to subsets of predicates. For example, any detection for the predicates behind, has, in, of, and on will exclusively fall into three of the six available slots. This pattern emerges for most classes, with the exception of wearing/wears where detections are distributed uniformly across all six slots."}, {"heading": "6 Conclusion", "text": "The qualities of a graph that allow it to capture so much information about the semantic content of an image come at the cost of additional complexity for any system that wishes to predict them. We show how to supervise a network such that all of the reasoning about a graph can be abstracted away into a single network. The use of associative embeddings and unordered output slots offer the network the flexibility necessary to making training of this task possible. Our results on Visual Genome clearly demonstrate the effectiveness of our approach."}, {"heading": "7 Acknowledgements", "text": "This publication is based upon work supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) under Award No. OSR-2015-CRG42639."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": "arXiv preprint arXiv:1603.04467,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Learning to generalize to new compositions in image understanding", "author": ["Yuval Atzmon", "Jonathan Berant", "Vahid Kezami", "Amir Globerson", "Gal Chechik"], "venue": "arXiv preprint arXiv:1608.07639,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Learning to detect human-object interactions", "author": ["Yu-Wei Chao", "Yunfan Liu", "Xieyang Liu", "Huayi Zeng", "Jia Deng"], "venue": "arXiv preprint arXiv:1702.05448,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2017}, {"title": "Detecting visual relationships with deep relational networks", "author": ["Bo Dai", "Yuqi Zhang", "Dahua Lin"], "venue": "arXiv preprint arXiv:1704.03114,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2017}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Learning globally-consistent local distance functions for shape-based image retrieval and classification", "author": ["Andrea Frome", "Yoram Singer", "Fei Sha", "Jitendra Malik"], "venue": "IEEE 11th International Conference on Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Improving image-sentence embeddings using large weakly annotated photo collections", "author": ["Yunchao Gong", "Liwei Wang", "Micah Hodosh", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["Raia Hadsell", "Sumit Chopra", "Yann LeCun"], "venue": "In Computer vision and pattern recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Modeling relationships in referential expressions with compositional modular networks", "author": ["Ronghang Hu", "Marcus Rohrbach", "Jacob Andreas", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1611.09978,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Image retrieval using scene graphs", "author": ["Justin Johnson", "Ranjay Krishna", "Michael Stark", "Li-Jia Li", "David Shamma", "Michael Bernstein", "Li Fei- Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations. 2016", "author": ["Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Vip-cnn: A visual phrase reasoning convolutional neural network for visual relationship detection", "author": ["Yikang Li", "Wanli Ouyang", "Xiaogang Wang"], "venue": "arXiv preprint arXiv:1702.07191,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2017}, {"title": "Deep variation-structured reinforcement learning for visual relationship and attribute detection", "author": ["Xiaodan Liang", "Lisa Lee", "Eric P Xing"], "venue": "arXiv preprint arXiv:1703.03054,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2017}, {"title": "On support relations and semantic scene graphs", "author": ["Wentong Liao", "Michael Ying Yang", "Hanno Ackermann", "Bodo Rosenhahn"], "venue": "arXiv preprint arXiv:1609.05834,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Visual relationship detection with language priors", "author": ["Cewu Lu", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Beyond holistic object recognition: Enriching image understanding with part states", "author": ["Cewu Lu", "Hao Su", "Yongyi Lu", "Li Yi", "Chikeung Tang", "Leonidas Guibas"], "venue": "arXiv preprint arXiv:1612.07310,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Associative embedding: End-to-end learning for joint detection and grouping", "author": ["Alejandro Newell", "Jia Deng"], "venue": "arXiv preprint arXiv:1611.05424,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["Alejandro Newell", "Kaiyu Yang", "Jia Deng"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Phrase localization and visual relationship detection with comprehensive linguistic cues", "author": ["Bryan A Plummer", "Arun Mallya", "Christopher M Cervantes", "Julia Hockenmaier", "Svetlana Lazebnik"], "venue": "arXiv preprint arXiv:1611.06641,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Discovering objects and their relations from entangled scene representations", "author": ["David Raposo", "Adam Santoro", "David Barrett", "Razvan Pascanu", "Timothy Lillicrap", "Peter Battaglia"], "venue": "arXiv preprint arXiv:1702.05068,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2017}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing", "author": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q Weinberger", "John Blitzer", "Lawrence K Saul"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Scene graph generation by iterative message passing", "author": ["Danfei Xu", "Yuke Zhu", "Christopher B Choy", "Li Fei-Fei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2017}, {"title": "Visual translation embedding network for visual relation detection", "author": ["Hanwang Zhang", "Zawlin Kyaw", "Shih-Fu Chang", "Tat-Seng Chua"], "venue": "arXiv preprint arXiv:1702.08319,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2017}, {"title": "Towards context-aware interaction recognition", "author": ["Bohan Zhuang", "Lingqiao Liu", "Chunhua Shen", "Ian Reid"], "venue": "arXiv preprint arXiv:1703.06246,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2017}], "referenceMentions": [{"referenceID": 6, "context": "Recent years have seen rapid progress in the classification and localization of objects [7, 24, 10].", "startOffset": 88, "endOffset": 99}, {"referenceID": 22, "context": "Recent years have seen rapid progress in the classification and localization of objects [7, 24, 10].", "startOffset": 88, "endOffset": 99}, {"referenceID": 16, "context": "[18] This breakdown often restricts the visual features used in later steps and limits reasoning over the full graph and over the full contents of the image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Detecting vertices and edges therefore reduces to the problem of scoring candidate detections as in established object detection approaches [24].", "startOffset": 140, "endOffset": 144}, {"referenceID": 18, "context": "The network needs a way for detections to refer to each other, and for that we draw inspiration from associative embeddings [20], a method originally proposed for joint detection and grouping in the context of multiperson pose estimation.", "startOffset": 124, "endOffset": 128}, {"referenceID": 12, "context": "We apply our method to the task of generating a semantic graph of objects and relations and test on the Visual Genome dataset [14].", "startOffset": 126, "endOffset": 130}, {"referenceID": 9, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 101, "endOffset": 104}, {"referenceID": 16, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 171, "endOffset": 175}, {"referenceID": 10, "context": "This includes localization from referential expressions [11], detection of human-object interactions [3], or the more general tasks of visual relationship detection (VRD) [18] and scene graph generation [12].", "startOffset": 203, "endOffset": 207}, {"referenceID": 16, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 26, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 25, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 1, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 15, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 17, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 20, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 21, "context": "Visual relationship detection has drawn much recent attention [18, 28, 27, 2, 17, 19, 22, 23].", "startOffset": 62, "endOffset": 93}, {"referenceID": 16, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 85, "endOffset": 89}, {"referenceID": 24, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 275, "endOffset": 279}, {"referenceID": 14, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 398, "endOffset": 402}, {"referenceID": 3, "context": "For example: incorporating vision and language when reasoning over a pair of objects [18]; using message-passing RNNs to process a set of proposed object boxes [26]; predicting over triplets of bounding boxes that corresponding to proposals for a subject, phrase, and object [15]; using reinforcement learning to sequentially evaluate on pairs of object proposals and determine their relationships [16]; comparing the visual features and relative spatial positions of pairs of boxes [4];", "startOffset": 483, "endOffset": 486}, {"referenceID": 25, "context": "learning to project proposed objects into a vector space such that the difference between two object vectors is informative of the relationship between them [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 22, "context": "Most of these approaches rely on generated bounding boxes from a Region Proposal Network (RPN) [24].", "startOffset": 95, "endOffset": 99}, {"referenceID": 5, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 23, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 63, "endOffset": 70}, {"referenceID": 4, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 132, "endOffset": 142}, {"referenceID": 7, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 132, "endOffset": 142}, {"referenceID": 11, "context": "For example, to measure the similarity between pairs of images [6, 25], or to map visual and text features to a shared vector space [5, 8, 13].", "startOffset": 132, "endOffset": 142}, {"referenceID": 18, "context": "Recent work uses vector embeddings to group together body joints for multiperson pose estimation [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "More specifically, in [20] a network is trained to detect body joints of the various people in an image.", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "We use a stacked hourglass network [21] to process an image and produce the output feature tensor.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "And to predict bounding box information we use anchor boxes and regress offsets based on the approach in Faster-RCNN [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "To train the network to produce a coherent set of embeddings we build off of the loss penalty used in [20].", "startOffset": 102, "endOffset": 106}, {"referenceID": 18, "context": "To \u201cpush apart\u201d embeddings across different vertices we use a slightly different penalty from what is described in [20], instead applying a margin-based penalty similar to [9]:", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "To \u201cpush apart\u201d embeddings across different vertices we use a slightly different penalty from what is described in [20], instead applying a margin-based penalty similar to [9]:", "startOffset": 172, "endOffset": 175}, {"referenceID": 18, "context": "Convergence of the network improves greatly after increasing the dimension d of tags up from 1 as used in [20].", "startOffset": 106, "endOffset": 110}, {"referenceID": 19, "context": "We train a stacked hourglass architecture [21] in TensorFlow [1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "We train a stacked hourglass architecture [21] in TensorFlow [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 16, "context": "[18] \u2013 \u2013 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] \u2013 \u2013 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Dataset: We evaluate the performance of our method on the Visual Genome dataset [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 24, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Following [26], we report performance on three problem settings: SGGen: Detect and classify all objects and determine the relationships between them.", "startOffset": 10, "endOffset": 14}, {"referenceID": 22, "context": "For the full scene graph task (SGGen) these detections are provided by a Region Proposal Network (RPN) [24].", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and report a Recall@50 of 9.7% compared to the prior state-of-the-art at 3.4%, a nearly threefold improvement on the challenging task of scene graph generation.", "creator": "LaTeX with hyperref package"}}}