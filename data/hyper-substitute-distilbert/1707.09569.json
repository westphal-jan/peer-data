{"id": "1707.09569", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jul-2017", "title": "Learning Language Representations for Typology Prediction", "abstract": "one underlying mystery of neural nlp is communicating neural artifacts \" know \" about their origin matter. when a data machine delivery service develops to translate from external language hilbert space, helping it learn the objects or relating to the languages? can evolutionary knowledge be done from computational cortex to fill holes in human physiological knowledge? existing typological databases contain theoretically full standard lengths for only a whole hundred monkeys. challenging the accuracy of parallel characters in louder than half millennium languages, we build a massive point - to - one genetic machine machine ( nmt ) construct from 1017 monkeys teaching english, and distribute this to predict information missing from typological databases. advantages require that the coding protocol is able not infer not only incorrect, but reverse phonological and phonetic inventory errors, and improves over linguistic nation that has access to information toward the items'textual and geographical neighbors.", "histories": [["v1", "Sat, 29 Jul 2017 23:38:25 GMT  (2505kb,D)", "http://arxiv.org/abs/1707.09569v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["chaitanya malaviya", "graham neubig", "patrick littell"], "accepted": true, "id": "1707.09569"}, "pdf": {"name": "1707.09569.pdf", "metadata": {"source": "CRF", "title": "Learning Language Representations for Typology Prediction", "authors": ["Chaitanya Malaviya"], "emails": ["cmalaviy@cs.cmu.edu", "gneubig@cs.cmu.edu", "pwl@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "Linguistic typology is the classification of human languages according to syntactic, phonological, and other classes of features, and the investigation of the relationships and correlations between these classes/features. This study has been a scientific pursuit in its own right since the 19th century (Greenberg, 1963; Comrie, 1989; Nichols, 1992), but recently typology has borne practical fruit within various subfields of NLP, particularly on problems involving lower-resource languages.\n1Code and learned vectors are available at http:// github.com/chaitanyamalaviya/lang-reps\nTypological information from sources like the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), has proven useful in many NLP tasks (O\u2019Horan et al., 2016), such as multilingual dependency parsing (Ammar et al., 2016), generative parsing in low-resource settings (Naseem et al., 2012; Ta\u0308ckstro\u0308m et al., 2013), phonological language modeling and loanword prediction (Tsvetkov et al., 2016), POStagging (Zhang et al., 2012), and machine translation (Daiber et al., 2016).\nHowever, the needs of NLP tasks differ in many ways from the needs of scientific typology, and typological databases are often only sparsely populated, by necessity or by design.2 In NLP, on the other hand, what is important is having a relatively full set of features for the particular group of languages you are working on. This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daume\u0301 III and Campbell, 2007; Daume\u0301 III, 2009; Coke et al., 2016; Littell et al., 2017).\nIn this study, we examine whether we can 2For example, each chapter of WALS aims to provide a statistically balanced set of languages over language families and geographical areas, and so many languages are left out in order to maintain balance.\nar X\niv :1\n70 7.\n09 56\n9v 1\n[ cs\n.C L\n] 2\n9 Ju\nl 2 01\n7\ntackle the problem of inferring linguistic typology from parallel corpora, specifically by training a massively multi-lingual neural machine translation (NMT) system and using the learned representations to infer typological features for each language. This is motivated both by prior work in linguistics (Bugarski, 1991; Garc\u0131\u0301a, 2002) demonstrating strong links between translation studies and tools for contrastive linguistic analysis, work in inferring typology from bilingual data (O\u0308stling, 2015) and English as Second Language texts (Berzak et al., 2014), as well as work in NLP (Shi et al., 2016; Kuncoro et al., 2017; Belinkov et al., 2017) showing that syntactic knowledge can be extracted from neural nets on the word-by-word or sentence-by-sentence level. This work presents a more holistic analysis of whether we can discover what neural networks learn about the linguistic concepts of an entire language by aggregating their representations over a large number of the sentences in the language.\nWe examine several methods for discovering feature vectors for typology prediction, including those learning a language vector specifying the language while training multilingual neural language models (O\u0308stling and Tiedemann, 2017) or neural machine translation (Johnson et al., 2016) systems. We further propose a novel method for aggregating the values of the latent state of the encoder neural network to a single vector representing the entire language. We calculate these feature vectors using an NMT model trained on 1017 languages, and use them for typlogy prediction both on their own and in composite with feature vectors from previous work based on the genetic and geographic distance between languages (Littell et al., 2017). Results show that the extracted representations do in fact allow us to learn about the typology of languages, with particular gains for syntactic features like word order and the presence of case markers."}, {"heading": "2 Dataset and Experimental Setup", "text": "Typology Database: To perform our analysis, we use the URIEL language typology database (Littell et al., 2017), which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and Kayne, 2011), PHOIBLE (Moran et al., 2014), Ethnologue (Lewis et al., 2015), and\nGlottolog (Hammarstro\u0308m et al., 2015). These features are divided into separate classes regarding syntax (e.g. whether a language has prepositions or postpositions), phonology (e.g. whether a language has complex syllabic onset clusters), and phonetic inventory (e.g. whether a language has interdental fricatives). There are 103 syntactical features, 28 phonology features and 158 phonetic inventory features in the database.\nBaseline Feature Vectors: Several previous methods take advantage of typological implicature, the fact that some typological traits correlate strongly with others, to use known features of a language to help infer other unknown features of the language (Daume\u0301 III and Campbell, 2007; Takamura et al., 2016; Coke et al., 2016). As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have proposed a method for inferring typological features directly from the language\u2019s k nearest neighbors (k-NN) according to geodesic distance (distance on the Earth\u2019s surface) and genetic distance (distance according to a phylogenetic family tree). In our experiments, our baseline uses this method by taking the 3-NN for each language according to normalized geodesic+genetic distance, and calculating an average feature vector of these three neighbors.\nTypology Prediction: To perform prediction, we trained a logistic regression classifier3 with the baseline k-NN feature vectors described above and the proposed NMT feature vectors described in the next section. We train individual classifiers for predicting each typological feature in a class (syntax etc). We performed 10-fold crossvalidation over the URIEL database, where we train on 9/10 of the languages to predict 1/10 of the languages for 10 folds over the data."}, {"heading": "3 Learning Representations for Typology Prediction", "text": "In this section we describe three methods for learning representations for typology prediction with multilingual neural models.\nLM Language Vector Several methods have been proposed to learn multilingual language\n3We experimented with a non-linear classifier as well, but the logistic regression classifier performed better.\nmodels (LMs) that utilize vector representations of languages (Tsvetkov et al., 2016; O\u0308stling and Tiedemann, 2017). Specifically, these models train a recurrent neural network LM (RNNLM; Mikolov et al. (2010)) using long short-term memory (LSTM; Hochreiter and Schmidhuber (1997)) with an additional vector representing the current language as an input. The expectation is that this vector will be able to capture the features of the language and improve LM accuracy. O\u0308stling and Tiedemann (2017) noted that, intriguingly, agglomerative clustering of these language vectors results in something that looks roughly like a phylogenetic tree, but stopped short of performing typological inference. We train this vector by appending a special token representing the source language (e.g. \u201c\u3008fra\u3009\u201d for French) to the beginning of the source sentence, as shown in Fig. 1, then using the word representation learned for this token as a representation of the language. We will call this first set of feature vectors LMVEC, and examine their utility for typology prediction.\nNMT Language Vector In our second set of feature vectors, MTVEC, we similarly use a language embedding vector, but instead learn a multilingual neural MT model trained to translate from many languages to English, in a similar fashion to Johnson et al. (2016); Ha et al. (2016). In contrast to LMVEC, we hypothesize that the alignments to an identical sentence in English, the model will have a stronger signal allowing it to more accurately learn vectors that reflect the syntactic, phonetic, or semantic consistencies of various languages. This has been demonstrated to some extent in previous work that has used specifically engineered alignment-based models (Lewis and Xia, 2008; O\u0308stling, 2015; Coke et al., 2016), and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well.\nNMT Encoder Mean Cell States Finally, we propose a new vector representation of a language (MTCELL) that has not been investigated in previous work: the average hidden cell state of the encoder LSTM for all sentences in the language. Inspired by previous work that has noted that the hidden cells of LSTMs can automatically capture salient and interpretable information such as syntax (Karpathy et al., 2015; Shi et al., 2016) or\nsentiment (Radford et al., 2017), we expect that the cell states will represent features that may be linked to the typology of the language. To create vectors for each language using LSTM hidden states, we obtain the mean of cell states (c in the standard LSTM equations) for all time steps of all sentences in each language.4"}, {"heading": "4 Experiments", "text": ""}, {"heading": "4.1 Multilingual Data and Training Regimen", "text": "To train a multilingual neural machine translation system, we used a corpus of Bible translations that was obtained by scraping a massive online Bible database at bible.com.5 This corpus contains data for 1017 languages. After preprocessing the corpus, we obtained a training set of 20.6 million sentences over all languages.\nThe implementation of both the LM and NMT models described in \u00a73 was done in the DyNet toolkit (Neubig et al., 2017). In order to obtain a manageable shared vocabulary for all languages, we divided the data into subwords using joint byte-pair encoding of all languages (Sennrich et al., 2016) with 32K merge operations. We\n4We also tried using the mean of final hidden cell states of the encoder LSTM, but the mean cell state over all words in the sentence gave improved performance. Additionally, we tried using the hidden states h, but we found that these had significantly less information and lesser variance, due to being modulated by the output gate at each time step.\n5A possible concern is that Bible translations may use archaic language not representative of modern usage. However, an inspection of the data did not turn up such archaisms, likely because the bulk of world Bible translation was done in the late 19th and 20th centuries. In addition, languages that do have antique Bibles are also those with many other Bible translations, so the effect of the archaisms is likely limited.\nused LSTM cells in a single recurrent layer with 512-dimensional hidden state and input embedding size. The Adam optimizer was used with a learning rate of 0.001 and a dropout of 0.5 was enforced during training."}, {"heading": "4.2 Results and Discussion", "text": "The results of the experiments can be found in Tab. 1. First, focusing on the \u201c-Aux\u201d results, we can see that all feature vectors obtained by the neural models improve over the chance rate, demonstrating that indeed it is possible to extract information about linguistic typology from unsupervised neural models. Comparing LMVEC to MTVEC, we can see a convincing improvement of 2-3% across the board, indicating that the use of bilingual information does indeed provide a stronger signal, allowing the network to extract more salient features. Next, we can see that MTCELL further outperforms MTVEC, indicating that the proposed method of investigating the hidden cell dynamics is more effective than using a statically learned language vector. Finally, combining both feature vectors as MTBOTH leads to further improvements. To measure statistical significance of the results, we performed a paired bootstrap test to measure the gain between NONE+AUX and MTBOTH+AUX and found that the gains for syntax and inventory were significant (p=0.05), but phonology was not, perhaps because the number of phonological features was fewer than the other classes (only 28).\nWhen further using the geodesic/genetic distance neighbor feature vectors, we can see that these trends largely hold although gains are much smaller, indicating that the proposed method is still useful in the case where we have a-priori knowledge about the environment in which the language exists. It should be noted, however, that the gains of LMVEC evaporate, indicating that access to aligned data may be essential when inferring the typology of a new language. We also noted that the accuracies of certain features decreased from NONE-AUX to MTBOTH-AUX, particularly gender markers, case suffix and negative affix, but these decreases were to a lesser extent in magnitude than the improvements.\nInterestingly, and in contrast to previous methods for inferring typology from raw text, which have been specifically designed for inducing word order or other syntactic features (Lewis and Xia,\n2008; O\u0308stling, 2015; Coke et al., 2016), our proposed method is also able to infer information about phonological or phonetic inventory features. This may seem surprising or even counterintuitive, but a look at the most-improved phonology/inventory features (Tab. 2) shows a number of features in which languages with the \u201cnondefault\u201d option (e.g. having uvular consonants or initial velar nasals, not having lateral consonants, etc.) are concentrated in particular geographical regions. For example, uvular consonants are not common world-wide, but are common in particular geographic regions like the North American Pacific Northwest and the Caucasus (Maddieson, 2013b), while initial velar nasals are common in Southeast Asia (Anderson, 2013), and lateral consonants are uncommon in the Amazon Basin (Maddieson, 2013a). Since these are also regions with a particular and sometimes distinct syntactic character, we think the model may be find-\ning regional clusters through syntax, and seeing an improvement in regionally-distinctive phonology/inventory features as a side effect.\nFinally, given that MTCELL uses the feature vectors of the latent cell state to predict typology, it is of interest to observe how these latent cells behave for typologically different languages. In Fig. 2 we examine the node that contributed most to the prediction of \u201cS OBJ BEFORE VERB\u201d (the node with maximum weight in the classifier) for German and Korean, where the feature is active, and Portuguese and Catalan, where the feature is inactive. We can see that the node trajectories closely track each other (particularly at the beginning of the sentence) for Portuguese and Catalan, and in general the languages where objects precede verbs have higher average values, which would be expressed by our mean cell state features. The similar trends for languages that share the value for a typological feature (S OBJ BEFORE VERB) indicate that information stored in the selected hidden node is consistent across languages with similar structures."}, {"heading": "5 Conclusion and Future Work", "text": "Through this study, we have shown that neural models can learn a range of linguistic concepts, and may be used to impute missing features in typological databases. In particular, we have demonstrated the utility of learning representations with parallel text, and results hinted at the importance of modeling the dynamics of the representation as models process sentences. We hope that this study will encourage additional use of typological features in downstream NLP tasks, and inspire further techniques for missing knowledge prediction in under-documented languages."}, {"heading": "Acknowledgments", "text": "We thank Lori Levin and David Mortensen for their useful comments and also thank the reviewers for their feedback about this work."}], "references": [{"title": "Many languages, one parser", "author": ["Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith."], "venue": "Transactions of the Association for Computational Linguistics, 4:431\u2013444.", "citeRegEx": "Ammar et al\\.,? 2016", "shortCiteRegEx": "Ammar et al\\.", "year": 2016}, {"title": "The velar nasal", "author": ["Gregory D.S. Anderson."], "venue": "Matthew S. Dryer and Martin Haspelmath, editors,", "citeRegEx": "Anderson.,? 2013", "shortCiteRegEx": "Anderson.", "year": 2013}, {"title": "What do neural machine translation models learn about morphology", "author": ["Yonatan Belinkov", "Nadir Durrani", "Fahim Dalvi", "Hassan Sajjad", "James Glass"], "venue": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Belinkov et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2017}, {"title": "Reconstructing native language typology from foreign language usage", "author": ["Yevgeni Berzak", "Roi Reichart", "Boris Katz."], "venue": "Eighteenth Conference on Computational Natural Language Learning (CoNLL).", "citeRegEx": "Berzak et al\\.,? 2014", "shortCiteRegEx": "Berzak et al\\.", "year": 2014}, {"title": "Contrastive analysis of terminology and the terminology of contrastive analysis", "author": ["Ranko Bugarski."], "venue": "Languages in Contact und Contrast. Essays in Contact Linguistics/Edited by Vladimir Ivir and Damir Kalogjera.\u2013Berlin, pages 73\u201382.", "citeRegEx": "Bugarski.,? 1991", "shortCiteRegEx": "Bugarski.", "year": 1991}, {"title": "Classifying syntactic regularities for hundreds of languages", "author": ["Reed Coke", "Ben King", "Dragomir Radev."], "venue": "arXiv preprint arXiv:1603.08016.", "citeRegEx": "Coke et al\\.,? 2016", "shortCiteRegEx": "Coke et al\\.", "year": 2016}, {"title": "Syntactic Structures of the World\u2019s Languages", "author": ["Chris Collins", "Richard Kayne."], "venue": "New York University, New York.", "citeRegEx": "Collins and Kayne.,? 2011", "shortCiteRegEx": "Collins and Kayne.", "year": 2011}, {"title": "Language Universals and Linguistic Typology: Syntax and Morphology", "author": ["Bernard Comrie."], "venue": "Blackwell, Oxford.", "citeRegEx": "Comrie.,? 1989", "shortCiteRegEx": "Comrie.", "year": 1989}, {"title": "Universal reordering via linguistic typology", "author": ["Joachim Daiber", "Milo\u0161 Stanojevi\u0107", "Khalil Sima\u2019an"], "venue": "In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Daiber et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2016}, {"title": "Non-parametric Bayesian areal linguistics", "author": ["Hal Daum\u00e9 III."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 593\u2013601, Boul-", "citeRegEx": "III.,? 2009", "shortCiteRegEx": "III.", "year": 2009}, {"title": "A Bayesian model for discovering typological implications", "author": ["Hal Daum\u00e9 III", "Lyle Campbell."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 65\u201372, Prague, Czech Republic. Association for Computa-", "citeRegEx": "III and Campbell.,? 2007", "shortCiteRegEx": "III and Campbell.", "year": 2007}, {"title": "Contrastive linguistics and translation studies interconnected: The corpusbased approach", "author": ["Noelia Ram\u00f3n Garc\u0131\u0301a"], "venue": "Linguistica Antverpiensia, New Series\u2013Themes in Translation Studies,", "citeRegEx": "Garc\u0131\u0301a.,? \\Q2002\\E", "shortCiteRegEx": "Garc\u0131\u0301a.", "year": 2002}, {"title": "Some universals of grammar with particular reference to the order of meaningful elements", "author": ["Joseph Greenberg."], "venue": "Joseph Greenberg, editor, Universals of Language, pages 110\u2013113. MIT Press, London.", "citeRegEx": "Greenberg.,? 1963", "shortCiteRegEx": "Greenberg.", "year": 1963}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "author": ["Thanh-Le Ha", "Jan Niehues", "Alexander Waibel."], "venue": "arXiv preprint arXiv:1611.04798.", "citeRegEx": "Ha et al\\.,? 2016", "shortCiteRegEx": "Ha et al\\.", "year": 2016}, {"title": "Glottolog 2.6. Max Planck Institute for the Science of Human History, Jena", "author": ["Harald Hammarstr\u00f6m", "Robert Forkel", "Martin Haspelmath", "Sebastian Bank"], "venue": null, "citeRegEx": "Hammarstr\u00f6m et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hammarstr\u00f6m et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation", "author": ["Melvin Johnson", "Mike Schuster", "Quoc V Le", "Maxim Krikun", "Yonghui Wu", "Zhifeng Chen", "Nikhil Thorat", "Fernanda Vi\u00e9gas", "Martin Wattenberg", "Greg Corrado"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei."], "venue": "arXiv preprint arXiv:1506.02078.", "citeRegEx": "Karpathy et al\\.,? 2015", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "What do recurrent neural network grammars learn about syntax", "author": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith"], "venue": "In Proceedings of the 15th Conference of the European Chapter", "citeRegEx": "Kuncoro et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Kuncoro et al\\.", "year": 2017}, {"title": "Ethnologue: Languages of the World, Eighteenth edition", "author": ["M. Paul Lewis", "Gary F. Simons", "Charles D. Fennig."], "venue": "SIL International, Dallas, Texas.", "citeRegEx": "Lewis et al\\.,? 2015", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "Automatically identifying computationally relevant typological features", "author": ["William D Lewis", "Fei Xia."], "venue": "Proceedings of the Third International Joint Conference on Natural Language Processing, Volume II, pages 685\u2013690.", "citeRegEx": "Lewis and Xia.,? 2008", "shortCiteRegEx": "Lewis and Xia.", "year": 2008}, {"title": "Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors", "author": ["Patrick Littell", "David R. Mortensen", "Ke Lin", "Katherine Kairis", "Carlisle Turner", "Lori Levin."], "venue": "Proceedings of the 15th Conference of the Euro-", "citeRegEx": "Littell et al\\.,? 2017", "shortCiteRegEx": "Littell et al\\.", "year": 2017}, {"title": "Lateral consonants", "author": ["Ian Maddieson."], "venue": "Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online. Max Planck Institute for Evolutionary Anthropology, Leipzig.", "citeRegEx": "Maddieson.,? 2013a", "shortCiteRegEx": "Maddieson.", "year": 2013}, {"title": "Uvular consonants", "author": ["Ian Maddieson."], "venue": "Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online. Max Planck Institute for Evolutionary Anthropology, Leipzig.", "citeRegEx": "Maddieson.,? 2013b", "shortCiteRegEx": "Maddieson.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Interspeech, volume 2, page 3.", "citeRegEx": "Mikolov et al\\.,? 2010", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "PHOIBLE Online", "author": ["Steven Moran", "Daniel McCloy", "Richard Wright."], "venue": "Max Planck Institute for Evolutionary Anthropology, Leipzig.", "citeRegEx": "Moran et al\\.,? 2014", "shortCiteRegEx": "Moran et al\\.", "year": 2014}, {"title": "Selective sharing for multilingual dependency parsing", "author": ["Tahira Naseem", "Regina Barzilay", "Amir Globerson."], "venue": "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 629\u2013637. Asso-", "citeRegEx": "Naseem et al\\.,? 2012", "shortCiteRegEx": "Naseem et al\\.", "year": 2012}, {"title": "Dynet: The dynamic neural network toolkit", "author": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn"], "venue": null, "citeRegEx": "Neubig et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2017}, {"title": "Linguistic Diversity in Space and Time", "author": ["Joanna Nichols."], "venue": "University of Chicago Press, Chicago.", "citeRegEx": "Nichols.,? 1992", "shortCiteRegEx": "Nichols.", "year": 1992}, {"title": "Survey on the use of typological information in natural language processing", "author": ["Helen O\u2019Horan", "Yevgeni Berzak", "Ivan Vulic", "Roi Reichart", "Anna Korhonen"], "venue": "In Proceedings of COLING 2016,", "citeRegEx": "O.Horan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "O.Horan et al\\.", "year": 2016}, {"title": "Word order typology through multilingual word alignment", "author": ["Robert \u00d6stling."], "venue": "The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 205\u2013211.", "citeRegEx": "\u00d6stling.,? 2015", "shortCiteRegEx": "\u00d6stling.", "year": 2015}, {"title": "Continuous multilinguality with language vectors", "author": ["Robert \u00d6stling", "J\u00f6rg Tiedemann."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 644\u2013649, Valencia,", "citeRegEx": "\u00d6stling and Tiedemann.,? 2017", "shortCiteRegEx": "\u00d6stling and Tiedemann.", "year": 2017}, {"title": "Learning to generate reviews and discovering sentiment", "author": ["Alec Radford", "Rafal Jozefowicz", "Ilya Sutskever."], "venue": "arXiv preprint arXiv:1704.01444.", "citeRegEx": "Radford et al\\.,? 2017", "shortCiteRegEx": "Radford et al\\.", "year": 2017}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u2013", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Does string-based neural MT learn source syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526\u2013 1534, Austin, Texas", "author": ["Xing Shi", "Inkit Padhi", "Kevin Knight."], "venue": "Association for Computational", "citeRegEx": "Shi et al\\.,? 2016", "shortCiteRegEx": "Shi et al\\.", "year": 2016}, {"title": "Target language adaptation of discriminative transfer parsers", "author": ["Oscar T\u00e4ckstr\u00f6m", "Ryan McDonald", "Joakim Nivre."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2013", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Discriminative analysis of linguistic features for typological study", "author": ["Hiroya Takamura", "Ryo Nagata", "Yoshifumi Kawasaki."], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016),", "citeRegEx": "Takamura et al\\.,? 2016", "shortCiteRegEx": "Takamura et al\\.", "year": 2016}, {"title": "Polyglot neural language models: A case study in cross-lingual phonetic", "author": ["Yulia Tsvetkov", "Sunayana Sitaram", "Manaal Faruqui", "Guillaume Lample", "Patrick Littell", "David Mortensen", "Alan W Black", "Lori Levin", "Chris Dyer"], "venue": null, "citeRegEx": "Tsvetkov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2016}, {"title": "Learning to map into a universal POS tagset", "author": ["Yuan Zhang", "Roi Reichart", "Regina Barzilay", "Amir Globerson."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-", "citeRegEx": "Zhang et al\\.,? 2012", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "This study has been a scientific pursuit in its own right since the 19th century (Greenberg, 1963; Comrie, 1989; Nichols, 1992), but recently typology has borne practical fruit within various subfields of NLP, particularly on problems involving lower-resource languages.", "startOffset": 81, "endOffset": 127}, {"referenceID": 7, "context": "This study has been a scientific pursuit in its own right since the 19th century (Greenberg, 1963; Comrie, 1989; Nichols, 1992), but recently typology has borne practical fruit within various subfields of NLP, particularly on problems involving lower-resource languages.", "startOffset": 81, "endOffset": 127}, {"referenceID": 28, "context": "This study has been a scientific pursuit in its own right since the 19th century (Greenberg, 1963; Comrie, 1989; Nichols, 1992), but recently typology has borne practical fruit within various subfields of NLP, particularly on problems involving lower-resource languages.", "startOffset": 81, "endOffset": 127}, {"referenceID": 29, "context": "World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), has proven useful in many NLP tasks (O\u2019Horan et al., 2016), such as multilingual dependency parsing (Ammar et al.", "startOffset": 108, "endOffset": 130}, {"referenceID": 0, "context": ", 2016), such as multilingual dependency parsing (Ammar et al., 2016), generative parsing in low-resource", "startOffset": 49, "endOffset": 69}, {"referenceID": 26, "context": "settings (Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013), phonological language modeling and loanword prediction (Tsvetkov et al.", "startOffset": 9, "endOffset": 54}, {"referenceID": 35, "context": "settings (Naseem et al., 2012; T\u00e4ckstr\u00f6m et al., 2013), phonological language modeling and loanword prediction (Tsvetkov et al.", "startOffset": 9, "endOffset": 54}, {"referenceID": 37, "context": ", 2013), phonological language modeling and loanword prediction (Tsvetkov et al., 2016), POStagging (Zhang et al.", "startOffset": 64, "endOffset": 87}, {"referenceID": 38, "context": ", 2016), POStagging (Zhang et al., 2012), and machine translation (Daiber et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 8, "context": ", 2012), and machine translation (Daiber et al., 2016).", "startOffset": 33, "endOffset": 54}, {"referenceID": 5, "context": "This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum\u00e9 III and Campbell, 2007; Daum\u00e9 III, 2009; Coke et al., 2016; Littell et al., 2017).", "startOffset": 135, "endOffset": 223}, {"referenceID": 21, "context": "This mismatch of needs has motivated various proposals to reconstruct missing entries, in WALS and other databases, from known entries (Daum\u00e9 III and Campbell, 2007; Daum\u00e9 III, 2009; Coke et al., 2016; Littell et al., 2017).", "startOffset": 135, "endOffset": 223}, {"referenceID": 4, "context": "This is motivated both by prior work in linguistics (Bugarski, 1991; Garc\u0131\u0301a, 2002) demon-", "startOffset": 52, "endOffset": 83}, {"referenceID": 11, "context": "This is motivated both by prior work in linguistics (Bugarski, 1991; Garc\u0131\u0301a, 2002) demon-", "startOffset": 52, "endOffset": 83}, {"referenceID": 30, "context": "strating strong links between translation studies and tools for contrastive linguistic analysis, work in inferring typology from bilingual data (\u00d6stling, 2015) and English as Second Language texts (Berzak et al.", "startOffset": 144, "endOffset": 159}, {"referenceID": 3, "context": "strating strong links between translation studies and tools for contrastive linguistic analysis, work in inferring typology from bilingual data (\u00d6stling, 2015) and English as Second Language texts (Berzak et al., 2014), as well as work in NLP (Shi", "startOffset": 197, "endOffset": 218}, {"referenceID": 31, "context": "We examine several methods for discovering feature vectors for typology prediction, including those learning a language vector specifying the language while training multilingual neural language models (\u00d6stling and Tiedemann, 2017) or", "startOffset": 202, "endOffset": 231}, {"referenceID": 16, "context": "neural machine translation (Johnson et al., 2016) systems.", "startOffset": 27, "endOffset": 49}, {"referenceID": 21, "context": "Typology Database: To perform our analysis, we use the URIEL language typology database (Littell et al., 2017), which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and Kayne, 2011), PHOIBLE (Moran et al.", "startOffset": 88, "endOffset": 110}, {"referenceID": 6, "context": ", 2017), which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and Kayne, 2011), PHOIBLE (Moran et al.", "startOffset": 178, "endOffset": 203}, {"referenceID": 25, "context": ", 2017), which is a collection of binary features extracted from multiple typological, phylogenetic, and geographical databases such as WALS (World Atlas of Language Structures) (Collins and Kayne, 2011), PHOIBLE (Moran et al., 2014), Ethnologue (Lewis et al.", "startOffset": 213, "endOffset": 233}, {"referenceID": 19, "context": ", 2014), Ethnologue (Lewis et al., 2015), and Glottolog (Hammarstr\u00f6m et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 14, "context": ", 2015), and Glottolog (Hammarstr\u00f6m et al., 2015).", "startOffset": 23, "endOffset": 49}, {"referenceID": 36, "context": "tures of the language (Daum\u00e9 III and Campbell, 2007; Takamura et al., 2016; Coke et al., 2016).", "startOffset": 22, "endOffset": 94}, {"referenceID": 5, "context": "tures of the language (Daum\u00e9 III and Campbell, 2007; Takamura et al., 2016; Coke et al., 2016).", "startOffset": 22, "endOffset": 94}, {"referenceID": 5, "context": ", 2016; Coke et al., 2016). As an alternative that does not necessarily require pre-existing knowledge of the typological features in the language at hand, Littell et al. (2017) have", "startOffset": 8, "endOffset": 178}, {"referenceID": 37, "context": "of languages (Tsvetkov et al., 2016; \u00d6stling and Tiedemann, 2017).", "startOffset": 13, "endOffset": 65}, {"referenceID": 31, "context": "of languages (Tsvetkov et al., 2016; \u00d6stling and Tiedemann, 2017).", "startOffset": 13, "endOffset": 65}, {"referenceID": 23, "context": "Specifically, these models train a recurrent neural network LM (RNNLM; Mikolov et al. (2010)) using long short-term memory (LSTM; Hochreiter and Schmidhuber (1997))", "startOffset": 71, "endOffset": 93}, {"referenceID": 15, "context": "(2010)) using long short-term memory (LSTM; Hochreiter and Schmidhuber (1997))", "startOffset": 44, "endOffset": 78}, {"referenceID": 30, "context": "\u00d6stling and Tiedemann (2017) noted that, intriguingly, agglomerative clustering of these language vectors", "startOffset": 0, "endOffset": 29}, {"referenceID": 15, "context": "feature vectors, MTVEC, we similarly use a language embedding vector, but instead learn a multilingual neural MT model trained to translate from many languages to English, in a similar fashion to Johnson et al. (2016); Ha et al.", "startOffset": 196, "endOffset": 218}, {"referenceID": 13, "context": "(2016); Ha et al. (2016). In contrast", "startOffset": 8, "endOffset": 25}, {"referenceID": 20, "context": "This has been demonstrated to some extent in previous work that has used specifically engineered alignment-based models (Lewis and Xia, 2008; \u00d6stling, 2015; Coke et al., 2016), and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well.", "startOffset": 120, "endOffset": 175}, {"referenceID": 30, "context": "This has been demonstrated to some extent in previous work that has used specifically engineered alignment-based models (Lewis and Xia, 2008; \u00d6stling, 2015; Coke et al., 2016), and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well.", "startOffset": 120, "endOffset": 175}, {"referenceID": 5, "context": "This has been demonstrated to some extent in previous work that has used specifically engineered alignment-based models (Lewis and Xia, 2008; \u00d6stling, 2015; Coke et al., 2016), and we examine whether these results apply to neural network feature extractors and expand beyond word order and syntax to other types of typology as well.", "startOffset": 120, "endOffset": 175}, {"referenceID": 17, "context": "Inspired by previous work that has noted that the hidden cells of LSTMs can automatically capture salient and interpretable information such as syntax (Karpathy et al., 2015; Shi et al., 2016) or Syntax Phonology Inventory -Aux +Aux -Aux +Aux -Aux +Aux", "startOffset": 151, "endOffset": 192}, {"referenceID": 34, "context": "Inspired by previous work that has noted that the hidden cells of LSTMs can automatically capture salient and interpretable information such as syntax (Karpathy et al., 2015; Shi et al., 2016) or Syntax Phonology Inventory -Aux +Aux -Aux +Aux -Aux +Aux", "startOffset": 151, "endOffset": 192}, {"referenceID": 32, "context": "sentiment (Radford et al., 2017), we expect that the cell states will represent features that may be", "startOffset": 10, "endOffset": 32}, {"referenceID": 27, "context": "The implementation of both the LM and NMT models described in \u00a73 was done in the DyNet toolkit (Neubig et al., 2017).", "startOffset": 95, "endOffset": 116}, {"referenceID": 33, "context": "In order to obtain a manageable shared vocabulary for all languages, we divided the data into subwords using joint byte-pair encoding of all languages (Sennrich et al., 2016) with 32K merge operations.", "startOffset": 151, "endOffset": 174}, {"referenceID": 23, "context": "For example, uvular consonants are not common world-wide, but are common in particular geographic regions like the North American Pacific Northwest and the Caucasus (Maddieson, 2013b), while initial velar nasals are common in Southeast Asia (Anderson, 2013), and lateral consonants are uncommon in the Amazon Basin (Maddieson, 2013a).", "startOffset": 165, "endOffset": 183}, {"referenceID": 1, "context": "For example, uvular consonants are not common world-wide, but are common in particular geographic regions like the North American Pacific Northwest and the Caucasus (Maddieson, 2013b), while initial velar nasals are common in Southeast Asia (Anderson, 2013), and lateral consonants are uncommon in the Amazon Basin (Maddieson, 2013a).", "startOffset": 241, "endOffset": 257}, {"referenceID": 22, "context": "For example, uvular consonants are not common world-wide, but are common in particular geographic regions like the North American Pacific Northwest and the Caucasus (Maddieson, 2013b), while initial velar nasals are common in Southeast Asia (Anderson, 2013), and lateral consonants are uncommon in the Amazon Basin (Maddieson, 2013a).", "startOffset": 315, "endOffset": 333}], "year": 2017, "abstractText": "One central mystery of neural NLP is what neural models \u201cknow\u201d about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one neural machine translation (NMT) system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages\u2019 geographic and phylogenetic neighbors.1", "creator": "TeX"}}}