{"id": "1708.03271", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Aug-2017", "title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search", "abstract": "in this paper, we introduce a targeting technique with attention - based multimedia vector translation ( sts ). a target selection learned involving experimental mt scanning demonstrates a term representing the nmt beam pattern whenever the reader of the nmt model encounters most repeated sentence words translated by this segment. comparisons arranged in this way are scored with the encoded language, interpreted also with smt features including sample - class translation characteristics and simplified target language model. experimental observations on german - & gt ; british global domain and english - & gt ; russian e - commerce domain temporal mapping show feasibility using intensity - valued models in nmt search improves multimedia retrieval by up to 2. 3 % the absolute as improvement to basic numerical nmt baseline.", "histories": [["v1", "Thu, 10 Aug 2017 15:48:33 GMT  (97kb,D)", "http://arxiv.org/abs/1708.03271v1", "To appear in Proceedings of EMNLP 2017"]], "COMMENTS": "To appear in Proceedings of EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["leonard dahlmann", "evgeny matusov", "pavel petrushkov", "shahram khadivi"], "accepted": true, "id": "1708.03271"}, "pdf": {"name": "1708.03271.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation Leveraging Phrase-based Models in a Hybrid Search", "authors": ["Leonard Dahlmann", "Shahram Khadivi"], "emails": ["skhadivi}@ebay.com"], "sections": [{"heading": "1 Introduction", "text": "Neural machine translation has become state-ofthe-art in recent years, reaching higher translation quality than statistical phrase-based machine translation (PBMT) on many tasks. Human analysis (Bentivogli et al., 2016) showed that NMT makes significantly fewer reordering errors, and also is able to select correct word forms more often than PBMT in the case of morphologically rich target languages. Overall, the fluency of the MT output improves when NMT is used, and the number of lexical choice errors is also reduced. However, state-of-the-art NMT approaches based on an encoder-decoder architecture with an attention mechanism as introduced by (Bahdanau et al., 2014) exhibit weaknesses that sometimes lead to MT errors which a phrase-based MT system does not make. In particular, PBMT usually can better translate rare words (e.g. singletons), as well as\nmemorize and use phrasal translations. NMT has problems translating rare words because of limitations on the vocabulary size, as well as the fact that word embeddings are used to represent both source and target words. A rare word\u2019s embedding can not be trained reliably.\nAnother handicap of NMT is a general difficulty of fixing errors made by a neural MT system. Since NMT does not explicitly use or save word-to-word or phrase-to-phrase mappings, and its search is a target word beam search with almost no constraints, it is difficult to fix errors by an NMT system. It is important to quickly fix certain errors in real-life applications of MT systems to avoid negative user feedback or other (e.g. legal) consequences. An error identified in the output of a PBMT system can be fixed by tracing which phrase pair was used that resulted in the error, and down-weighting or even removing the phrase pair. Also, in PBMT it is easy to add an \u201coverride\u201d translation.\nIn this work, we combine the strengths of NMT and PBMT approaches by introducing a novel hybrid search algorithm. In this algorithm, the standard NMT beam search is extended with phrase translation hypotheses from a statistical phrase table. The decision on when to use what phrasal translations is taken based on the attention mechanism of the NMT model, which provides a soft coverage of the source sentence words. All partial phrasal translations are scored with the NMT decoder and can be continued with a word-based NMT translation candidate or another phrasal translation candidate.\nThe proposed search algorithm uses a log-linear model in which the NMT translation score is combined with standard phrase translation scores, including a target n-gram language model (LM) score. Thus, a LM trained on additional monolingual data can be used. The decisions on the word\nar X\niv :1\n70 8.\n03 27\n1v 1\n[ cs\n.C L\n] 1\n0 A\nug 2\n01 7\norder in the produced target translation are taken based only on the states of the NMT decoder.\nThis paper is structured as follows. We review related work in Section 1.1. The baseline NMT model we use is described in Section 2, where we also recap the log-linear model combination used in PBMT. Section 3 presents the details of the proposed hybrid search. Experimental results are presented in Section 4, followed by conclusions and outlook in Section 5."}, {"heading": "1.1 Related Work", "text": "In the line of research closely related to our approach, neural models are used as additional features in vanilla phrase-based systems. Examples include the work of (Devlin et al., 2014), (JunczysDowmunt et al., 2016), etc. Such approaches have certain limitations: first, the search space of the model is still restricted by what can be produced using a phrase table extracted from parallel data based on word alignments. Second, the organization of the search, in which only a limited target word history (e.g. 4 last target words) is available for each partial hypothesis, makes it difficult to integrate recurrent neural network LMs and translation models which take all previously generated target words into account. That is why, for instance, the attention-based NMT models were usually applied only in rescoring (Peter et al., 2016).\nIn (Stahlberg et al., 2017), a two-step translation process is used, where in the first step a SMT translation lattice is generated, and in the second step the NMT decoder combines NMT scores with the Bayes-risk of the translations according to the lattice. In contrast, we explicitly use phrasal translations and language model scores in an integrated search.\nIn (Arthur et al., 2016), a statistical word lexicon is used to influence NMT hypotheses, also based on the attention mechanism. (Gu\u0308lc\u0327ehre et al., 2015) combine target n-gram LM scores with NMT scores to find the best translation. (He et al., 2016) also use a target LM, but add further SMT features such as word penalty and word lexica to the NMT beam search. To the best of our knowledge, no previous work extends the beam search with phrasal translation hypotheses of PBMT, like we propose in this paper.\nIn (Tang et al., 2016), the NMT decoder is modified to switch between using externally de-\nfined phrases and standard NMT word hypotheses. However, only one target phrase per source phrase is considered, and the reported improvements are significant only when manually selected phrase pairs (mostly for rare named entities) are used.\nSomewhat related to our work is the concept of coverage-based NMT (Tu et al., 2016), where the model architecture is changed to explicitly account for source coverage. In our work, we use a standard NMT architecture, but track coverage with accumulated attention weights."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Neural MT", "text": "Neural MT proposed by (Bahdanau et al., 2014) maximizes the conditional log-likelihood of the target sentence E : e1, . . . , eI given the source sentence F : f1, . . . , fJ :\nHD = \u2212 1\nN N\u2211 n=1 log p\u03b8(En|Fn)\nwhere (En, Fn) refers to the n-th training sentence pair in a dataset D, and N denotes the total number of sentence pairs in the training corpus. When using the encoder-decoder architecture by (Cho et al., 2014), the conditional probability can be written as: p(e1 \u00b7 \u00b7 \u00b7 eI |f1 \u00b7 \u00b7 \u00b7 fJ) =\nI\u220f i=1 p(ei|ei\u22121 \u00b7 \u00b7 \u00b7 e1, c)\nwith p(ei|ei\u22121 \u00b7 \u00b7 \u00b7 e1, c) = g(si, ei\u22121, c), where I is the length of the target sentence and J is the length of source sentence, c is a fixed-length vector to encode the source sentence, si is a hidden state of RNN at time step i, and g(\u00b7) is a nonlinear function to approximate the word probability. When the attention mechanism is used, the vector c in each sentence is replaced by a timevariant representation ci that is a weighted summary over a sequence of annotations (h1, . . . , hJ), and hj contains information about the whole input sentence, but with a strong focus on the parts surrounding the j-th word (Bahdanau et al., 2014). Then, the context vector can be defined as:\nci = J\u2211 j \u03b1ijhj where \u03b1ij = exp(rij)\u2211J j=1 exp(rij) .\nTherefore, \u03b1ij is normalized over all source positions j. Also, rij = a(si\u22121, hj) is the attention model used to calculate the log-likelihood of\naligning the i-th target word to the j-th source word."}, {"heading": "2.2 Phrase-based MT", "text": "The log-linear model, as introduced in (Och and Ney, 2002), allows decomposing the translation probability Pr(eI1|fJ1 ) by using an arbitrary number of features hm(fJ1 , e I 1). Each feature is multiplied by a corresponding scaling factor \u03bbm:\nPr(eI1|fJ1 ) = exp\n(\u2211M m=1 \u03bbmhm(f J 1 , e I 1) )\n\u2211 e\u0303I\u03031 exp (\u2211M m=1 \u03bbmhm(f J 1 , e\u0303 I\u0303 1) ) .\nThe standard PBMT approach uses a log-linear model in which bidirectional phrasal and lexical scores, language model scores, distortion scores, word penalties and phrase penalties are combined as features."}, {"heading": "3 Hybrid Approach", "text": "In this section we describe our proposed hybrid NMT approach. The algorithm allows translations to be generated partially by phrases1 and partially by words. Section 3.1 describes the models we use to score hypotheses. The search algorithm is presented in Section 3.2."}, {"heading": "3.1 Log-linear Combination", "text": "We use a log-linear model combination to introduce SMT models into the NMT search. Since translations can be partially generated by phrases, we introduce the phrase segmentation sK1 as a hidden variable into the models similarly to (Zens and Ney, 2008), where K is the number of phrases used in the translation. Note that, unlike standard PBMT, sK1 does not need to cover the whole source sentence, as parts of the translation can be generated by words. Using the maximum approximation, the search criterion then is\ne\u0302I\u03021 = argmax I,eI1 { max sK1 M\u2211 m=1 \u03bbmhm(f J 1 , e I 1, s K 1 ) } .\n(1) Let f\u0303k, e\u0303k be the chosen phrase pairs in the segmentation sK1 for k = 1, . . . ,K. In our experiments with the proposed hybrid search, we use the following features:\n1. The NMT feature hNMT.\n1As in SMT, phrases can consist of only a single token.\n2. The word penalty feature hWP counts the number of target words. This feature can help control the length of translations. 3. The source word coverage feature hSWC counts the number of source words translated by phrases:\nhSWC(f J 1 , e I 1, s K 1 ) = K\u2211 k=1 |f\u0303k|.\nThe purpose of this feature is to control the usage of phrases. 4. The phrase penalty feature hPP counts the number of phrases used. Together with the word penalty and the source word coverage feature, the phrase penalty can control the length of chosen phrases. 5. The n-gram language model feature hLM. 6. The bidirectional phrase features hPhr and hiPhr. Note that these features are only applied for those parts of the translation that are generated by phrases. The other parts get a phrase score of zero.\nThe scaling factors \u03bbm are tuned with minimum error rate training (MERT) (Och, 2003) on n-best lists of the development set."}, {"heading": "3.2 Search", "text": "The algorithm is based on the beam search for NMT, which generates translations one word per time step in a left-to-right fashion. We modify this search to allow hypothesizing phrases in addition to normal word hypotheses. The phrases are suggested based on the neural attention, starting from the source position with the maximal current attention. We only suggest phrases if a source position is focused. We check that suggested phrases do not overlap with already translated source words by keeping track of the sum of attention in previous time steps for each source position. Thus, the problem of global reordering is left entirely to the NMT model and we follow the attention when hypothesizing phrases.\nHypotheses are scored by NMT and SMT models. The beam is divided into two parts of fixed size: the word beam and the phrase beam. The phrase beam is used to score target phrases which were hypothesized from an entry in a previous word beam. In order to score a target phrase consisting of k words with the NMT model, we use k time steps, allowing us to keep the efficiency of batched NMT scoring. Once a target phrase has been fully scored (and if the hypothesis has\nnot been pruned), the hypothesis is returned to the word beam. Both beams are generated and pruned independently in each time step.\nThe algorithm has some hyper-parameters that need to be set manually. First, we have the beam size Np for phrase hypotheses and the beam size Nw for word hypotheses. Second, \u03c4focus is the minimum attention that needs to be on a source position to consider it for extending with a phrase translation candidate whose source phrase starts on that position. Third, \u03c4cov is the minimum sum of attention of a source position over previous time steps at which it is considered to be covered. We do not hypothesize phrases that overlap with covered positions.\nIn the following, we describe the search in detail. Let fJ1 be the source sentence. Before search, we run the standard phrase matching algorithm on the source sentence to retrieve the translation options E(j, j\u2032) for source positions 1 \u2264 j < j\u2032 \u2264 J from a given SMT phrase table. With each hypothesis h, we associate the following items: \u2022 C(h, j) is the sum of the NMT attention to\nsource position j involved in generating the target words of h. This can be considered as a soft coverage vector for h. \u2022 Q(h) is the partial log-linear score of h ac-\ncording to Equation 1. \u2022 E(h) is the n-gram target word history of h. \u2022 If h is a phrase hypothesis with target phrase e\u0303, of which k words already have been scored by NMT, then P (h) := (e\u0303, k) is the phrase state. Also, each hypothesis is associated with its corresponding NMT hidden state. We initialize the beam to consist of an empty word hypothesis. Each step of the beam search proceeds as follows:\n1. Let B = [Bw, Bp] be the previous beam with word/phrase hypotheses, respectively. First, we generate the attention vector \u03b1h,j and the distribution over target words p\u0302h(e) for each hypothesis h \u2208 B and word e in the NMT target vocabulary VT using the NMT model in batched scoring 2. 2. Initialize new beam [B\u2032w, B \u2032 p] = [\u2205, \u2205]. 3. Generate new word hypotheses: find the maximal Nw pairs (h, e) with h \u2208 Bw and e \u2208 VT according to the score Q(h)+\u03bbNMT \u00b7\n2If a target word e is not in VT , set p\u0302h(e) = p\u0302h(UNK) where UNK is a special token denoting unknowns. Note that this almost never happens when using a word segmentation like BPE (Sennrich et al., 2016b).\nlog p\u0302h(e). For the top pairs h\u2032 = (h, e), set\nQ(h\u2032) = Q(h) + \u03bbNMT \u00b7 log p\u0302h(e) + \u03bbLM \u00b7 log pLM(e|E(h)) + \u03bbWP\nand insert h\u2032 into B\u2032w. Update the soft coverageC(h\u2032, j) = C(h, j)+\u03b1h,j for 1 \u2264 j \u2264 J . 4. Generate new phrase hypotheses: for each previous word hypothesis h \u2208 Bw, convert the soft attention C(h, \u00b7) into a binary coverage set C, such that j \u2208 C iff. C(h, j) > \u03c4cov. Identify the current NMT focus as\nj\u0302 = argmax 1\u2264j\u2264J, \u03b1h,j>\u03c4focus \u03b1h,j .\nIf there is no such j with \u03b1h,j > \u03c4focus, no phrase hypotheses are generated from h in this step. Otherwise, for each source phrase length l with C\u2229{j\u0302, j\u0302+1, . . . , j\u0302+ l\u22121} = \u2205 and each target phrase e\u0303 \u2208 E(j\u0302, j\u0302+ l), create a new hypothesis h\u2032 = (h, e\u03031) with the score\nQ(h\u2032) = Q(h) + \u03bbNMT \u00b7 log p\u0302h(e1) + \u03bbLM \u00b7 log pLM(e\u0303|E(h)) + |e\u0303| \u00b7 \u03bbWP + \u03bbPP + l \u00b7 \u03bbSWC. (2)\nNote that, in this step, the full target phrase is scored using the language model, while only the first target word is scored using NMT. Initialize the phrase state of h\u2032: P (h\u2032) = (e\u0303, 1). As in step 3, update the soft coverage. If |e\u0303| = 1, insert h\u2032 into B\u2032w, otherwise insert into B\u2032p. 5. Advance previous phrase hypotheses: for each h \u2208 Bp, with phrase state P (h) = (e\u0303, k), score the (k + 1)-th target word of e\u0303 using NMT, setting h\u2032 = (h, e\u0303k+1) and\nQ(h\u2032) = Q(h) + \u03bbNMT \u00b7 log p\u0302h(e\u0303k+1).\nAs in step 3, update the soft coverage. Set the new phrase state as P (h\u2032) = (e\u0303, k + 1). If k + 1 = |e\u0303|, we are finished scoring the phrase and h\u2032 is inserted into B\u2032w. Otherwise, h\u2032 is inserted in B\u2032p. 6. Prune B\u2032w to Nw entries and B \u2032 p to Np entries\naccording to Q(\u00b7). 7. Insert all hypotheses from the prunedB\u2032w and B\u2032p where the last word is the sentence end token into the set of finished hypotheses Bf . 8. B := [B\u2032w, B \u2032 p].\nIf phrase scores from a phrase table are to be included in the search, Equation 2 needs to be modified by adding \u03bbPhr log p(f\u0303 |e\u0303) and \u03bbiPhr log p(e\u0303|f\u0303).\nAs in the pure NMT beam search, this procedure is repeated until either the last word of all hypotheses in a step is the sentence end token, or 2 \u00b7 J many beam steps have been performed. Finally, the best translation is chosen as the one in Bf with the highest score.\nNote that the same target sequence can be generated with different phrasal segmentations. During search, if two hypotheses have the same full target history in a beam, we recombine them and discard the hypothesis with the lower score."}, {"heading": "4 Experiments", "text": "We perform experiments comparing the translation quality of our hybrid approach to phrasebased and pure end-to-end NMT baselines. We present results on two tasks: an inhouse English\u2192Russian e-commerce task (translation of real product/item descriptions from an e-commerce site), and the WMT 2016 German\u2192English task (news domain). The corpus statistics are shown in Table 1.\nFor the English\u2192Russian task, the parallel training data consists of an in-domain part (ca. 5.5M running words) of product/item titles and descriptions and other e-commerce content. The rest is out-of-domain data (UN, subtitles, TAUS data collections, etc.) sampled to have significant ngram overlap with the in-domain description data. Item descriptions are provided by private sellers and, like any user-generated content, may contain ungrammatical sentences, spelling errors, and other noise. Product descriptions usually originate from product catalogs and are more \u201cclean\u201d, but on the other hand, are difficult to translate because of rare domain-specific terminology. Both types\nof text contain itemizations, measurement units, and other structures which are usually not found in normal sentences. We tune the system on a development set that is a mix of product and item descriptions, and evaluate on separate product/item description test sets. For development and test sets, two reference translations are used.\nThe German\u2192English system is trained on parallel corpora provided for the constrained WMT 2017 evaluation (Europarl, Common Crawl, and others). We use the WMT 2015 evaluation data as development set, and the evaluation is performed on two sets from the WMT evaluations in 2014 and 2016. Only a single human reference translation is provided.\nFor the phrase-based baselines, we use an inhouse phrase-decoder (Matusov and Ko\u0308pru\u0308, 2010) which is similar to the Moses decoder (Koehn et al., 2007). We use standard SMT features, including word-level and phrase-level translation probabilities, the distortion model, 5-gram LMs, and a 7-gram joint translation and reordering model reimplemented based on the work of (Guta et al., 2015). The language model for the ecommerce task is trained on additional monolingual Russian item description data containing 28.2M words. For the WMT task, we use the English News Crawl data containing 3.8B words for additional language model data. The tuning is performed using MERT (Och, 2003) to increase the BLEU score on the development set. To stabilize the optimization on the English\u2192Russian task, we detach Russian morphological suffixes from the word stems both in hypotheses and references using a context-independent \u201cpoor man\u2019s\u201d morphological analysis. We prefix each suffix with a special symbol and treat them as separate tokens.\nWe have implemented our NMT model in\nPython using the TensorFlow3 deep learning library. We use the embedding size of 620, RNN size of 1000 and GRU cells. The model is trained with maximum likelihood loss for 15 epochs using Adam optimizer (Kingma and Ba, 2014) on complete data in batches of 100 sentences. The learning rate is initialized to 0.0002, decaying by 0.9 each epoch. For regularization we use L2 loss with weight 10\u22127 and dropout following Gal and Ghahramani (2016). We set the dropout probability for input and recurrent connections of the RNN to 0.2 and word embedding dropout probability to 0.1. On the English\u2192Russian task, the model is then fine-tuned on in-domain data for 10 epochs. The vocabulary is limited using byte pair encoding (BPE) (Sennrich et al., 2016b) with 40K splits separately for each language. To speed up training we use approximate loss as described in (Jean et al., 2015). For pure NMT experiments, we employ length normalization (Wu et al., 2016), as otherwise short translations would be favored.\nFor the hybrid approach, we use the same trained end-to-end model as in the NMT baseline. We use all the phrase-based model features plus the NMT score and run MERT as described in Section 3.1. Language models are trained on the level of BPE tokens. We consider at most 100 translation options for each source phrase. If not specified otherwise, we use a beam size of 96 for phrase hypotheses and a beam size of 32 for word hypotheses, resulting in a combined beam size of 128. Furthermore, we set the focus threshold \u03c4focus = 0.3 and the coverage threshold \u03c4cov = 0.7 by default. We also perform experiments where these hyper-parameters are varied.\n3http://tensorflow.org"}, {"heading": "4.1 E-commerce English\u2192Russian", "text": "The results on the e-commerce English\u2192Russian task are summarized in Table 2.\nNMT vs. phrase-based SMT The pure NMT system exhibits large improvements over the phrase-based baseline4. These improvements are also significantly larger than when we use the NMT model to rescore PBMT 1000- best lists. NMT results are not improved when the beam size is increased from 12 to 128.\nHybrid search vs. pure NMT search For the hybrid approach, we train a phrase-table on the in-domain data and split the source and target phrases with BPE afterwards for compatibility with the NMT vocabulary. With the hybrid approach, when using a LM trained only on the target side of bilingual data, we get an improvement of 0.3% BLEU on item descriptions and 1.4% BLEU on product descriptions over the pure NMT system. When we use the LM trained on extra monolingual data, we get total improvements of 1.0% BLEU and 2.3% BLEU with the hybrid approach. In contrast, when we add this language model and a word penalty on top of the pure NMT system and tune scaling factors with MERT, we get small improvements (last row of Table 2) only on product descriptions. This shows that the hybrid approach can exploit the LM better than a purely word-based NMT approach. We have also performed experiments utilizing the additional monolingual data for synthetic training data for NMT as in (Sennrich et al., 2016a), but did not get improvements.\nTo analyze the improvements of the hybrid system, we perform experiments in which we either\n4The significance of these improvements was also confirmed by an in-house human evaluation with 3 judges.\ndisable or limit some of the SMT models. The results are shown in Table 3. Without the language model, the hybrid approach has almost no improvements over the NMT baseline. This indicates that the language model is crucial in selecting appropriate phrase candidates. Similarly, when we disable the source word coverage feature, the translation quality is degraded, suggesting that this feature helps choose between phrase hypotheses and word hypotheses during the search. Next, we do not use phrase-level scores. Here, we observe only a small degradation of translation quality. Finally, we limit the source length of phrases used in the search, allowing only one-word source phrases in one experiment and only source phrases with two or more words in another experiment. In both cases, the translation quality decreases. Thus, both one-word phrases and longer phrases are necessary to obtain the best results.\nTuning the beam size Next, we study the effect of different beam sizes on translation quality. The results are shown in Table 4. Note that we retune the system for each choice. With a total beam size of 128, we get the best results by using a phrase beam size of 96 and a word beam size of 32. When we use a phrase beam size of 116 or 64 instead, the translation quality worsens. In another experiment, we decrease the total beam size to 64. The translation quality degrades only slightly, which means that we can still expect MT quality improvements with hybrid search even if we optimize the system for speed. To further test this, we reduce the beam sizes to Nw = 12 and Np = 4 after tuning with Nw = 32 and Np = 96. We get BLEU scores of 27.1% on item descriptions and 30.1% on product descriptions, losing 0.3% and 0.7% BLEU respectively compared to the full beam size.\nTuning the attention focus/coverage thresholds Table 5 shows results with different values for the coverage threshold \u03c4cov. Again, we retune the system for each choice. Setting the coverage threshold to 1.0 or even disabling the coverage check (by setting \u03c4cov = \u221e) has little effect on the translation scores on this task. This can be explained by the fact that translation from English to Russian is mostly monotonic. We also tried varying the focus threshold \u03c4focus between 0.0 and 0.3 but did not notice any significant effect on this task.\nAnalysis To understand the behavior of the hybrid search, we count the number of source words that are\ntranslated by phrases in the product descriptions test set. Of the 9320 source words, 7109 (76.3%) are covered by phrase hypotheses. 78.3% of the source phrases are unigrams, 19.5% are bigrams and 2.2% are trigrams or longer. Among the many one-word phrases used, almost all (99.2%) are also within the top 3 predictions of word-based NMT, and 90.3% are equal to the top NMT prediction.\nFurther human analysis by a native Russian speaker of the pure NMT vs. hybrid search translations shows that hybrid search is often able to correct the following known NMT handicaps:\n\u2022 incorrect translation of rare words (among other reasons, due to incorrect sub-word unit translation in which rare words are aggressively segmented). \u2022 repetition of same or similar words as a result\nof multiple attention to the same source word, as well as untranslated words that received no attention. \u2022 incorrect or partially correct word-by-word\ntranslation when a phrasal (non-literal) translation should be used instead.\nIn all of these cases, the usage of phrasal translations is able to better enforce the coverage, and this, in turn, leads to improved lexical choice. The fact that not many long phrase pairs are selected indicates, in our opinion, that the search and modeling problem in NMT is far from being solved: with the right, diverse model scores, the proposed hybrid search is able to select and extend better hypotheses with words, most of which already had a high NMT probability. Yet they are not always selected in the pure NMT beam search, among other reasons, due to competition from words erroneously placed near them in the embedding space."}, {"heading": "4.2 WMT 2016 German\u2192English", "text": "The results on the WMT German\u2192English task are shown in Table 6. The initial phrase-based baseline uses the 5-gram language model estimated on the target side of bilingual data. By adding the News Crawl LM data, we gain 2.5% and 2.3% BLEU on the test sets, but PBMT still is behind NMT.\nFor the hybrid approach, we use a beam size of 64 and a maximal number of beam steps of 1.5 \u00b7 J (instead of 2 \u00b7 J) to speed up experiments. We use separate word penalty features, one for word-based hypotheses and one for phrase-based hypotheses to allow for more control of translation lengths. With the hybrid approach, using the 5-gram language model estimated on the target side of bilingual data, and phrase scores, we get small improvements in BLEU over the NMT baseline. However, the TER increases. We experiment with different thresholds, setting \u03c4focus = 0.1 and \u03c4cov = 1.0. With this hybrid system, we get improvements of 1.0% and 1.1% BLEU over pure NMT. Finally, we add the News Crawl LM data on top. This significantly improves the results by 1.7% and 2.0% BLEU. In total, we gain 2.7% and 3.1% BLEU over pure NMT. These results reinforce the fact that, similar to PBMT, language model quality is important for the proposed hybrid search. In contrast, we have also tried applying only the LM (including News Crawl data) with a word penalty on top of NMT, but did not get consistent improvements.\nFigure 1 shows an example for the phrase pairs chosen by the hybrid system on top of the NMT attention. The hybrid approach correctly translates the German idiom \u201cnach und nach\u201d as \u201cgradually\u201d, while the pure NMT system incorrectly translates it word-by-word as \u201cafter and after\u201d."}, {"heading": "5 Conclusion", "text": "In this work, we proposed a novel hybrid search that extends NMT with phrase-based models. The NMT beam search was modified to insert phrasal translations based on the current and accumulated attention weights of the NMT decoder RNN. The NMT model score was used in a log-linear model with standard phrase-based scores as well as an n-gram language model. We described the algorithm in detail, in which we keep separate beams for NMT word hypotheses and hypotheses with an incomplete phrasal translation, as well as introduce parameters which control the source sentence coverage. Numerous experiments on two large vocabulary translation tasks showed that the hybrid search improves BLEU scores significantly as compared to a strong NMT baseline that already outperforms phrase-based SMT by a large margin.\nIn the future, we plan to focus on integration of phrasal components into NMT training, including better coverage constraints, as well as methods for context-dependent translation override within our hybrid search algorithm."}, {"heading": "Acknowledgments", "text": "We would like to thank Tamer Alkhouli and JanThorsten Peter for helpful discussions. We thank the anonymous reviewers for their suggestions."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation", "author": ["Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "CoRR abs/1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Neural versus phrasebased machine translation quality: a case study", "author": ["Luisa Bentivogli", "Arianna Bisazza", "Mauro Cettolo", "Marcello Federico"], "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Bentivogli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2016}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M. Schwartz", "John Makhoul"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Yarin Gal", "Zoubin Ghahramani"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Gal and Ghahramani.,? \\Q2016\\E", "shortCiteRegEx": "Gal and Ghahramani.", "year": 2016}, {"title": "On using monolingual corpora in neural machine translation", "author": ["\u00c7aglar G\u00fcl\u00e7ehre", "Orhan Firat", "Kelvin Xu", "Kyunghyun Cho", "Lo\u0131\u0308c Barrault", "Huei-Chi Lin", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR abs/1503.03535", "citeRegEx": "G\u00fcl\u00e7ehre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "G\u00fcl\u00e7ehre et al\\.", "year": 2015}, {"title": "A comparison between count and neural network models based on joint translation and reordering sequences", "author": ["Andreas Guta", "Tamer Alkhouli", "Jan-Thorsten Peter", "Joern Wuebker", "Hermann Ney"], "venue": "In Proceedings of the 2015 Conference on Empiri-", "citeRegEx": "Guta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Guta et al\\.", "year": 2015}, {"title": "Improved neural machine translation with SMT features", "author": ["Wei He", "Zhongjun He", "Hua Wu", "Haifeng Wang"], "venue": "In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "KyungHyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "The AMU-UEDIN submission to the WMT16 news translation task: Attention-based NMT models as feature functions in phrase-based SMT", "author": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Rico Sennrich"], "venue": "In Proceedings of the First Conference", "citeRegEx": "Junczys.Dowmunt et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Junczys.Dowmunt et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR abs/1412.6980", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens"], "venue": null, "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "AppTek\u2019s APT Machine Translation System for IWSLT", "author": ["Evgeny Matusov", "Sel\u00e7uk K\u00f6pr\u00fc"], "venue": "International Workshop on Spoken Language Translation,", "citeRegEx": "Matusov and K\u00f6pr\u00fc.,? \\Q2010\\E", "shortCiteRegEx": "Matusov and K\u00f6pr\u00fc.", "year": 2010}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics", "citeRegEx": "Och.,? \\Q2003\\E", "shortCiteRegEx": "Och.", "year": 2003}, {"title": "Discriminative training and maximum entropy models for statistical machine translation", "author": ["Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Och and Ney.,? \\Q2002\\E", "shortCiteRegEx": "Och and Ney.", "year": 2002}, {"title": "The rwth aachen machine translation system for iwslt 2016", "author": ["Jan-Thorsten Peter", "Andreas Guta", "Nick Rossenbach", "Miguel Graa", "Hermann Ney"], "venue": "In International Workshop on Spoken Language", "citeRegEx": "Peter et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Peter et al\\.", "year": 2016}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation by minimising the Bayes-risk with respect to syntactic translation lattices", "author": ["Felix Stahlberg", "Adri\u00e0 de Gispert", "Eva Hasler", "Bill Byrne"], "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association", "citeRegEx": "Stahlberg et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Stahlberg et al\\.", "year": 2017}, {"title": "Neural machine translation with external phrase memory", "author": ["Yaohua Tang", "Fandong Meng", "Zhengdong Lu", "Hang Li", "Philip L.H. Yu"], "venue": "CoRR abs/1606.01792", "citeRegEx": "Tang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2016}, {"title": "Modeling coverage for neural machine translation", "author": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In Proceedings of the 54th Annual Meeting of the Association", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Improvements in dynamic programming beam search for phrasebased statistical machine translation", "author": ["Richard Zens", "Hermann Ney"], "venue": "In International Workshop on Spoken Language Translation,", "citeRegEx": "Zens and Ney.,? \\Q2008\\E", "shortCiteRegEx": "Zens and Ney.", "year": 2008}], "referenceMentions": [{"referenceID": 2, "context": "Human analysis (Bentivogli et al., 2016) showed that NMT makes significantly fewer reordering errors, and also is able to select correct word forms more often than PBMT in the case of morphologically rich target languages.", "startOffset": 15, "endOffset": 40}, {"referenceID": 1, "context": "However, state-of-the-art NMT approaches based on an encoder-decoder architecture with an attention mechanism as introduced by (Bahdanau et al., 2014) exhibit weaknesses that sometimes lead to MT errors which a phrase-based MT system does not make.", "startOffset": 127, "endOffset": 150}, {"referenceID": 4, "context": "include the work of (Devlin et al., 2014), (JunczysDowmunt et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 16, "context": "That is why, for instance, the attention-based NMT models were usually applied only in rescoring (Peter et al., 2016).", "startOffset": 97, "endOffset": 117}, {"referenceID": 19, "context": "In (Stahlberg et al., 2017), a two-step translation process is used, where in the first step a SMT translation lattice is generated, and in the second step the NMT decoder combines NMT scores with the Bayes-risk of the translations according to the lattice.", "startOffset": 3, "endOffset": 27}, {"referenceID": 0, "context": "In (Arthur et al., 2016), a statistical word lexicon is used to influence NMT hypotheses, also based on the attention mechanism.", "startOffset": 3, "endOffset": 24}, {"referenceID": 6, "context": "(G\u00fcl\u00e7ehre et al., 2015) combine target n-gram LM scores with NMT scores to find the best translation.", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "(He et al., 2016) also use a target LM, but add further SMT features such as word penalty and word lexica to the NMT beam search.", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "In (Tang et al., 2016), the NMT decoder is modified to switch between using externally defined phrases and standard NMT word hypotheses.", "startOffset": 3, "endOffset": 22}, {"referenceID": 21, "context": "Somewhat related to our work is the concept of coverage-based NMT (Tu et al., 2016), where the model architecture is changed to explicitly account for source coverage.", "startOffset": 66, "endOffset": 83}, {"referenceID": 1, "context": "Neural MT proposed by (Bahdanau et al., 2014)", "startOffset": 22, "endOffset": 45}, {"referenceID": 3, "context": "When using the encoder-decoder architecture by (Cho et al., 2014), the conditional probability can be written as:", "startOffset": 47, "endOffset": 65}, {"referenceID": 1, "context": ", hJ), and hj contains information about the whole input sentence, but with a strong focus on the parts surrounding the j-th word (Bahdanau et al., 2014).", "startOffset": 130, "endOffset": 153}, {"referenceID": 15, "context": "The log-linear model, as introduced in (Och and Ney, 2002), allows decomposing the translation probability Pr(e1|f 1 ) by using an arbitrary number of features hm(f 1 , e I 1).", "startOffset": 39, "endOffset": 58}, {"referenceID": 22, "context": "Since translations can be partially generated by phrases, we introduce the phrase segmentation s1 as a hidden variable into the models similarly to (Zens and Ney, 2008), where K is the number of phrases used in the translation.", "startOffset": 148, "endOffset": 168}, {"referenceID": 14, "context": "The scaling factors \u03bbm are tuned with minimum error rate training (MERT) (Och, 2003) on n-best lists of the development set.", "startOffset": 73, "endOffset": 84}, {"referenceID": 13, "context": "For the phrase-based baselines, we use an inhouse phrase-decoder (Matusov and K\u00f6pr\u00fc, 2010) which is similar to the Moses decoder (Koehn et al.", "startOffset": 65, "endOffset": 90}, {"referenceID": 12, "context": "For the phrase-based baselines, we use an inhouse phrase-decoder (Matusov and K\u00f6pr\u00fc, 2010) which is similar to the Moses decoder (Koehn et al., 2007).", "startOffset": 129, "endOffset": 149}, {"referenceID": 7, "context": "including word-level and phrase-level translation probabilities, the distortion model, 5-gram LMs, and a 7-gram joint translation and reordering model reimplemented based on the work of (Guta et al., 2015).", "startOffset": 186, "endOffset": 205}, {"referenceID": 14, "context": "The tuning is performed using MERT (Och, 2003) to increase the BLEU score on the development set.", "startOffset": 35, "endOffset": 46}, {"referenceID": 11, "context": "The model is trained with maximum likelihood loss for 15 epochs using Adam optimizer (Kingma and Ba, 2014) on complete data in batches of 100 sentences.", "startOffset": 85, "endOffset": 106}, {"referenceID": 5, "context": "For regularization we use L2 loss with weight 10\u22127 and dropout following Gal and Ghahramani (2016). We set the dropout probability for input and recurrent connections of the RNN to 0.", "startOffset": 73, "endOffset": 99}, {"referenceID": 9, "context": "use approximate loss as described in (Jean et al., 2015).", "startOffset": 37, "endOffset": 56}], "year": 2017, "abstractText": "In this paper, we introduce a hybrid search for attention-based neural machine translation (NMT). A target phrase learned with statistical MT models extends a hypothesis in the NMT beam search when the attention of the NMT model focuses on the source words translated by this phrase. Phrases added in this way are scored with the NMT model, but also with SMT features including phrase-level translation probabilities and a target language model. Experimental results on German\u2192English news domain and English\u2192Russian ecommerce domain translation tasks show that using phrase-based models in NMT search improves MT quality by up to 2.3% BLEU absolute as compared to a strong", "creator": "LaTeX with hyperref package"}}}