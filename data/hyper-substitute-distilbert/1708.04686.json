{"id": "1708.04686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2017", "title": "VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation", "abstract": "a text structured texture labeled datasets is clearly the main possible areas for aiding recent realization on conversation - language understanding. many seemingly distant insights ( la. f., symbolic segmentation and visual narrative answering ( vqa ) ) both inherently versatile in that they reveal different accurately defining perspectives of human understandings employing the same visual scenes - - - and offer the same varieties of images ( e. col., of coco ). gaining popularity of coco correlates large annotations and tasks. explicitly linking matching up did significantly benefit both cooperative investigations plus the unified vision and scenario modeling. theorists present the technical work reports linking the component segmentations addressed by extension to the sentences and puzzles ( qas ) in paradigm resource tree, and particularly the tentative bilingual visual questions and segmentation answers ( cdp ). they perform scientific supervision between the previously separate languages, offer authors substantial leverage throughout existing findings, these also bridge the door providing new research simulations and models. experiments connect two applications of the vqs data in this development : conceptual processing index vqa and a novel space - focused objective reference task. beneath the former, theories obtain state - wide - \u2010 - art techniques between comparing supposedly real multiple - choice task by simply augmenting the multilayer perceptrons satisfying some attention features, are computed using additional ip - qa links like explicit points. to put their knowledge in effectiveness, we study internally plausible methods and test them requires an oracle method for that the associated segmentations stay ready at the test stage.", "histories": [["v1", "Tue, 15 Aug 2017 20:47:02 GMT  (3730kb,D)", "http://arxiv.org/abs/1708.04686v1", "To appear on ICCV 2017"]], "COMMENTS": "To appear on ICCV 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.LG", "authors": ["chuang gan", "yandong li", "haoxiang li", "chen sun", "boqing gong"], "accepted": false, "id": "1708.04686"}, "pdf": {"name": "1708.04686.pdf", "metadata": {"source": "CRF", "title": "VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation", "authors": ["Chuang Gan", "Yandong Li", "Haoxiang Li", "Chen Sun", "Boqing Gong"], "emails": [], "sections": [{"heading": null, "text": "We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage."}, {"heading": "1. Introduction", "text": "Connecting visual understanding with natural language has received extensive attentions in recent years. We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks. However, image captions tend to be diverse and sub-\nCode and data: https://github.com/Cold-Winter/vqs.\njective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities. In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.g., scene, object, activity, attribute, context, relationships, etc.).\nRich and dense human annotated datasets are arguably the main \u201cenabler\u201d, among others, for this line of exciting works on vision-language understanding. COCO [25] is especially noticeable among them. It contains mainly classical labels (e.g., segmentations, object categories and instances, key points, etc.) and image captions. Many research groups have then collected additional labels of the COCO images for a variety of tasks. Agrawal et al. crowdsourced questions and answers (QAs) about a subset of the COCO images and abstract scenes [3]. Zhu et al. collected seven types of QAs in which the object mentions are as-\n1\nar X\niv :1\n70 8.\n04 68\n6v 1\n[ cs\n.C V\n] 1\n5 A\nug 2\n01 7\nsociated with bounding boxes in the images [51]. Mao et al. [28] and Yu et al. [50] have users to give referring expressions that each pinpoints a unique object in an image. The Visual Genome dataset [22] also intersects with COCO in terms of images and provides dense human annotations, especially scene graphs.\nThese seemingly distant annotations are inherently connected in the sense that they reveal different perspectives of human understandings about the same COCO images. The popularity of COCO could strongly correlate those annotations \u2014 and even tasks. Explicitly linking them up, as we envision, can significantly benefit both individual tasks and unified vision-language understanding, as well as the corresponding approaches and models. One of our contributions in this paper is to initiate the preliminary work on this. In particular, we focus on linking the segmentations provided by COCO [25] to the QAs in the VQA dataset [3]. Displaying an image and a QA pair about the image, we ask the participant to choose the segmentation(s) of the image in order to visually answer the question.\nFigure 1 illustrates some of the collected \u201cvisual answers\u201d. For the question \u201cWhat is next to the dog?\u201d, the output is supposed to be the segmentation mask over the man. For the question \u201cWhat time is it?\u201d, the clock should be segmented out. Another intriguing example is that the cars are the desired segmentations to answer \u201cIs this street empty?\u201d, providing essential visual evidence for the simple text answer \u201cno\u201d. Note that while many visual entities could be mentioned in a question, we only ask the participants to choose the target segmentation(s) that visually answer the question. This simplifies the annotation task and results in higher agreement between participants. Section 2 details the annotation collection process and statistics.\nTwo related datasets. Das et al. have collected some human attention maps for the VQA task [5]. They blur the images and then ask users to scratch them to seek visual cues that help answer the questions. The obtained attention maps are often small, revealing meaningful parts rather than complete objects. The object parts are also mixed with background areas and with each other. As a result, the human attention maps are likely less accurate supervision for the attention based approaches to VQA than the links we built between segmentations and QAs. Our experiments verify this hypothesis (cf. Section 3). While bounding boxes are provided in Visual7W [51] for object mentions in QAs, they do not serve for the purpose of directly answering the questions except for the \u201cpointing\u201d type of questions. In contrast, we provide direct visual answers in the form of segmentations to more question types."}, {"heading": "1.1. Applications of the segmentation-QA links", "text": "We call the collected links between the COCO segmentations [25] and QA pairs in the VQA dataset [3] visual ques-\ntions and segmentation answers (VQS). Such links transfer human supervision between the previously separate tasks, i.e., semantic segmentation and VQA. They enable us to tackle existing problems with more effective leverage than before and also open the door for new research problems and models for the vision-language understanding. We study two applications of our VQS dataset in this paper: supervised attention for VQA and a novel question-focused semantic segmentation (QFSS) task. For the former, we obtain state-of-the-art results on the VQA real multiplechoice task by simply augmenting the multilayer perceptrons (MLP) of [17] with attention features."}, {"heading": "1.1.1 Supervised attention for VQA", "text": "VQA is designed to answer natural language questions about images in the form of short texts. The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27]. However, lacking explicit attention annotations, the existing methods opt for latent variables and use indirect cues (e.g., text answers) for inference. As a result, the machine-generated attention maps are poorly correlated with human attention maps [5]. This is not surprising since latent variables hardly match semantic interpretations due to the lack of explicit training signals; similar observations exist in other studies, e.g., object detection [8], video recognition [11] and text processing [49].\nThese phenomena highlight the need for explicit links between the visual and text answers, realized in this work as VQS. We show that, by supervised learning to attend different image regions using the collected segmentationQA links, we can boost the simple MLP model [17] to very compelling performance on the VQA real multi-choice task."}, {"heading": "1.1.2 Question-focused semantic segmentation (QFSS)", "text": "In addition to the supervised attention for better tackling VQA, VQS also enables us to explore a novel questionfocused semantic segmentation (QFSS) task.\nSince VQA desires only text answers, there exist potential shortcuts for the learning agent, e.g., to generate correct answers without accurately reasoning the locations and relations of different visual entities. While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image. In order to bring together the best of VQA and VG, we propose the QFSS task, whose objective is to produce pixel-wise segmentations in order to visually answer the questions about images. It effectively borrows the versatile questions from VQA and meanwhile resembles the design of VG in terms of the pixel-wise segmentations as the desired output.\nGiven an image and a question about the image, we propose a mask aggregation approach to generating a segmentation mask as the visual answer. Since QFSS is a new task, to put it in perspective, we not only compare the proposed approach to competing baselines but also study an upperbound method by assuming all instance segmentations are given as oracles at the test stage.\nHu et al.\u2019s work [15] is the most related to QFSS. They learn to ground text expressions in the form of image segmentations. Unlike the questions used in this work that are flexible to incorporate commonsense and knowledge bases, the expressive scope of the text phrases in [15] is often limited to the visual entities in the associated images.\nThe rest of this paper is organized as follows. Section 2 details the collection process and analyses of our VQS data. In section 3, we show how to use the collected segmentation-QA links to learn supervised attention features and to augement the existing VQA methods. In section 3.2, we study a few potential frameworks to address the new question-focused semantic segmentation task. Section 4 concludes the paper."}, {"heading": "2. Linking image segmentations to text QAs", "text": "In this section, we describe in detail how we collect the links between the semantic image segmentations and text questions and answers (QAs). We build our work upon the images and instance segmentation masks in COCO [25] and the QAs in the VQA dataset [3]. The COCO images are mainly about everyday scenes that contain common objects in their natural contexts, accommodating complex interactions and relationships between different visual entities. To\navoid trivial links between the segmentations and QA pairs, we only keep the images that contain at least three instance segmentations in this work. The questions in VQA [3] are diverse and comprehensively cover various parts of an image, different levels of semantic interpretations, as well as commonsense and knowledge bases.\nNext, we elaborate the annotation instructions and provide some analyses about the collected dataset."}, {"heading": "2.1. Annotation instructions", "text": "We display to the annotators an image, its instance segmentations from the COCO dataset, and a QA pair about the image from the VQA dataset. The textual answer is given in addition to the question, to facilitate the participants to choose the right segmentations as the visual answer. Here are the instructions we give to the annotators (cf. the supplementary materials for the GUI): \u2022 Please choose the right segmentation(s) in the image\nto answer the question. Note that the text answer is shown after the question. \u2022 A question about the target entities may use other enti-\nties to help refer to the target. Choose the target entities only and nothing else (e.g., the purse for \u201cWhat is on the bench next to woman?\u201d in Figure 2(g)). \u2022 A question may be about an activity. Choose all visual\nentities involved in the activity. Taking Figure 2(j) for instance, choose both the person and motorcycle for the question \u201cwhat is the person doing?\u201d. \u2022 Sometimes, in addition to the image regions covered\nby the segmentation masks, you may need other regions to answer the question. To include them, please\ndraw tight bounding box(es) over the region(s). \u2022 For the \u201cHow many\u201d type of questions, the number of\nselected segments (plus bounding boxes) must match the answer. If the answer is greater than three, it is fine to put one bounding box around the entities being asked in the question. \u2022 Please tick the black button under the question, if you\nthink the question has to be answered by the full image. \u2022 Please tick the gray button under the question, if you\nfeel the question is ambiguous, or if you are not sure which segment/region to select to answer the question. Occasionally, the visual answer is supposed to be only part of an instance segment given by COCO. For instance, the McDonald logo answers \u201cWhat fast food restaurant can be seen?\u201d in Figure 2(o) but there is no corresponding segmentation for the logo in COCO. Another example is the region of the ring that answers \u201cIs the woman wearing ring?\u201d (cf. Figure 2(c)). For these cases, we ask the participants to draw tight bounding boxes around them. If we segment them out instead, a learning agent for QFSS may never be able to produce the right segmentation for them unless we include more training images in the future, since these regions (e.g., McDonald logo, ring) are very fine-grained visual entities and show up only a few times in our data collection process. Quality control. We tried AMTurk to collect the annotations at the beginning. While the inter-annotator agreement is high on the questions about objects and people, there are many inconsistent annotations for the questions referring to activities (e.g., \u201cWhat sport is played?\u201d). Besides, the AMTurk workers tend to frequently tick the black button, which says the full image is the visual answer, and the gray button, which tells the question is ambiguous. To obtain higher-quality annotations, we instead invited 10 undergraduate and graduate volunteers and trained them in person (we include some slides used for the training in the supplementary materials). To further control the annotation quality, each annotator was asked to finish an assignment of 100 images (around 300 question-answer pairs) before we met with them again to look over their annotations together \u2014\nall the volunteers were asked to participate the discussion and jointly decide the expected annotations for every question. We also gradually increased the hourly payment rate from $12/hr to $14/hr as incentives for high-quality work."}, {"heading": "2.2. Tasks addressed by the participants", "text": "Thanks to the rich set of questions collected by Agrawal et al. [3] and the complex visual scenes in COCO [25], the participants have to parse the question, understand the visual scene and context, infer the interactions between visual entities, and then pick up the segmentations that answer the questions. We find that many vision tasks may play roles in this process. Figure 2 shows some typical examples to facilitate the following discussion.\nObject detection. Many questions directly ask about the properties of some objects in the images. In Figure 2(b), the participants are supposed to identify the cup in the cluttered scene for the question \u201cWhat color is the coffee cup?\u201d.\nSemantic segmentation. For some questions, the visual evidence to answers is best represented by semantic segmentations. Take Figures 2(j) and (k) for instance. Simply detecting the rider and/or the bike would be inadequate in expressing their spatial interactions.\nSpatial relationship reasoning. A question like \u201cWhat is on the bench next to the woman?\u201d (Figure 2(g)) poses a challenge to the participants through the spatial relationship between objects including bench, woman, and the answer purse. Figure 2(i) is another example in this realm.\nFine-grained activity recognition. When the question is about an activity (e.g., \u201cWhat sport is being played?\u201d in Figure 2(l)), we ask the participants to label all the visual entities (e.g., person, tennis racket , and tennis ball) involved. In other words, they are expected to spot the finegrained details of the activity.\nCommonsense reasoning. Commonsense knowledge can help the participants significantly reduce the search space for the visual answers, e.g., the clock to answer \u201cWhat time is it?\u201d in Figure 1, and the McDonald logo to answer \u201cWhat fast food restaurant can be seen?\u201d in Figure 2(o)."}, {"heading": "2.3. Data statistics", "text": "After collecting the annotations, we remove the question-image pairs for which the users selected the black buttons (full image) or gray buttons (unknown) to avoid trivial and ambiguous segmentation-QA links, respectively. In total, we keep 37,868 images, 96,508 questions, 108,537 instance segmentations, and 43,725 bounding boxes. In the following, we do not differentiate the segmentations from the bounding boxes for the ease of presentation and also for the sake that the bounding boxes are tight, small, and much fewer than the segmentations.\nFigure 3 counts the distribution of the possible number of instance segmentations selected per image in response to a question. Over 70% of questions are answered by one segmentation. On average, each question-image pair has 6.7 candidate segmentations, among which 1.6 are selected by the annotators as the visual answers.\nIn Figure 4, we visualize the distribution of question types. The most popular type is the \u201cWhat\u201d questions (46%). There are 31,135 \u201cis/are\u201d and \u201cdoes/do\u201d questions (32.1%). Note that although the textual answers to them are simply yes or no, in VQS, we ask the participants to explicitly demonstrate their understanding about the visual content by producing the semantic segmentation masks. In the third column of Table 3, we show the average number of segmentations chosen by the users out of the average number of candidates for each of the question types."}, {"heading": "3. Applications of VQS", "text": "The user linked visual questions and segmentations, where the latter visually answers the former, are quite versatile. They offer better leverage than before for at least two problems, i.e., supervised attention for VQA and questionfocused semantic segmentation (QFSS)."}, {"heading": "3.1. Supervised attention for VQA", "text": "VQA is designed to answer natural language questions about an image in the form of short texts. We conjecture that a learning agent can produce more accurate text answers given the privileged access to the segmentations that are user linked to the QAs in training. To verify this point, we design a simple experiment to augment the MLP model in [17]. The augmented MLP significantly improves upon the plain version and gives rise to state-of-the-art results on the VQA real multiple-choice task [3].\nExperiment setup. We conduct experiments on the VQA Real Multiple Choices [3]. The dataset contains 248,349 questions for training, 121,512 for validation, and 244,302 for testing. Each question has 18 candidate answer choices and the learning agent is required to figure out the correct answer among them. We evaluate our results following the metric suggested in [3].\nMLP for VQA Multiple Choice. Since the VQA multiple-choice task supplies candidate answers to each question, Jabri et al. propose to transform the problem to a stack of binary classification problems [17] and solve them by the multilayer perceptrons (MLP) model:\ny = \u03c3(W2 max(0,W1xiqa) + b) (1)\nwhere xiqa is the concatenation of the feature representations of an image, a question about the image, and a candidate answer, and \u03c3(\u00b7) is the sigmoid function. The hidden layer has 8,096 units and a ReLU activation. This model is very competitive, albeit simple."}, {"heading": "3.1.1 Augmenting MLP by supervised attention", "text": "We propose to augment the MLP model by richer feature representations of the questions, answers, images, and es-\npecially by the supervised attention features detailed below.\nQuestion and answer features xq&xa. For a question or answer, we represent it by averaging the 300D word2vec [30] vectors of the constituent words, followed by the l2 normalization. This is the same as in [17].\nImage features xi. We extract two types of features from an input image: ResNet [14] pool5 activation and attribute features [44], where the latter is the attribute detection scores. We implement an attribute detector by revising the output layer of ResNet. Particularly, given C = 256 attributes, we impose a sigmoid function for each attribute and then train the network using the binary cross-entropy loss. The training data is obtained from the COCO image captions [25]. We keep the most frequent 256 words as the attributes after removing the stop words.\nAttention features xatt. We further concatenate attention features xatt to the original input xiqa. The attention features are motivated by the weighted combination of image regional features and question features in [48, eq. (22)], where the the non-negative weight pi = f(Q, {ri}) for each image region is a function of the question Q and regional features {ri}. We borrow the network architecture as well as code implementation from Yang et al. [48, Section 3.3] for this function, except that we train this network by a cross-entropy loss to match the weights {pi} to the \u201cgroundtruth\u201d attentions derived from the segmentations in our VQS dataset. In particular, we down-sample the segmentation map associated with each question-image pair to the same size as the number of image regions, and then l1 normalize it to a valid probability distribution. By training the network to match the weights pi = f(Q, {ri}) toward such attentions, we enforce larger weights for the regions that correspond to the user selected segmentations.\nThe upper panel of Figure 5 illustrates the process of extracting the attention features, and the bottom panel shows the MLP model [17] augmented with our attention features for the VQA real multiple-choice task."}, {"heading": "3.1.2 Experimental results", "text": "Table 1 reports the comparison results of the attention features augmented MLP with several state-of-the-art methods on the VQA real multiple-choice task. We mainly use the Test Dev for comparison. After determining our best single and ensemble models, we also submit them to the evaluation server to acquire the results on Test Standard.\nFirst of all, we note that there is an 1.5% absolute improvement over the plain MLP model (MLP + ResNet) by simply augmenting it using the learned attention features (MLP + ResNet + Atten.). Second, the attribute features for the images are actually quite effective. We gain 1.0% improvement over the plain MLP by replacing the ResNet image features with the attribute features (cf. the row of MLP + Attri. vs. MLP + ResNet). Nonetheless, by appending attention features to MLP + Attri., we can still observe 1.1% absolute gain. Finally, with an ensemble of five MLP + ResNet + Atten. models and five MLP + Attri. + Atten. models, our submission to the evaluation server was ranked to the second on Test Standard for the VQA real multiplechoice task, as of the paper submission date."}, {"heading": "3.1.3 What is good supervision for attention in VQA?", "text": "In this section, we contrast the VQS data to the human attention maps (HAT) [5] and bounding boxes that are placed tightly around the segmentations in VQS. The comparison results, reported in Table 2, are evaluated on the TestDev dataset of VQA Real Multiple Choice. We can see that the segmentaitons linked to QAs give rise to a little better results than bounding boxes, which further outperform HAT. These confirm our conjecture that HAT might be suboptimal for the supervised learning of attentions in VQA, since they reveal usually small parts of objects and contain large proportions of background. However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].\nIn the supplementary materials, we describe the detailed implementation for the ensemble model. We also present additional results studying how different resolutions of the segmentation masks influence the VQA results."}, {"heading": "3.2. Question-focused semantic segmentation", "text": "This section explores a new task, question-focused semantic segmentation (QFSS), which is feasible thanks to the collected VQS that connects two previously separate tasks (i.e., segmentations and VQA). Given a question about\nan image, QFSS expects the learning agent to output a visual answer by semantically segment the right visual entities out of the image. It is designed in a way similarly to the segmentation from natural language expressions [15], with possible applications to robot vision, photo editing, etc.\nIn order to put the new task in perspective, we propose a mask aggregation approach to QFSS, study a baseline, and also investigate an upper bound method by assuming all instance segmentations are given as oracles at the test stage."}, {"heading": "3.2.1 Mask aggregation for QFSS", "text": "We propose a mask aggregation approach to tackling QFSS. The modeling hypothesis is that the desired output segmentation mask can be composed from high-quality segmentation proposals. In particular, we use N = 25 segmentation proposals e1, e2, \u00b7 \u00b7 \u00b7 , eN generated by SharpMask [34] given an image. Each proposal is a binary segmentation mask of the same size as the image.\nWe then threshold a convex combination of these masks E = \u2211 i siei as the final output in response to a questionimage pair, where the i-th combination coefficient si is determined by the question features xq and the representations zi of the i-th segmentation proposal through a softmax function, i.e., si = softmax(xTq Azi). We learn the model parameters A by minimizing an l2 loss between the the user selected segmentationsE? and the model generated segmentation maskE. Our current model is \u201cshallow\u201d but it is straightforward to make it deep, e.g., by stacking its output with the original input following the prior practice (e.g., memory network [45] and stacked attention network [48]).\nAn oracle upper bound. We devise an upper bound to the proposed method by 1) replacing the segmentation proposals with all the instance segmentations released by MS\nInput Inage Ground truthAggregation Deconvoulution\nCOCO, assuming they are available as oracles at testing, and 2) using a binary classifier to determine whether or not an instance segmentation should be included into the visual answer. The results can be considered an upper bound for our approach because the segmentations are certainly more accurate than the machine generated proposals, and the binary classification is arguably easier to solve than aggregating multiple masks. We re-train the MLP (eq. 1) for the binary classifier here; it now takes as input the concatenated features of a segmentation and a question.\nFigure 6 depicts the proposed approach and the upperbound method with a concrete question-image example.\nA baseline using deconvolutional network. Finally, we study a competitive baseline which is motivated by the textconditioned FCN [15]. As Figure 7 shows, it contains three components, a convolutional neural network (CNN) [23], a deconvolutional neural network (DeconvNet) [31], and a question embedding to attend the feature maps in CNN. All the images are resized to 224 \u00d7 224. The convolutional and deconvolutional nets follow the specifications in [31]. Namely, a VGG-16 [39] is trimmed till the last convolutional layer, followed by two fully connected layers, and then mirrored by DeconvNet. For the input question, we use an embedding matrix to map it to the same size as the feature map of the last convolutional layer. The question embedding is then element-wsie multiplied with the feature map. We train the network with an l2 loss between the output mask and the groundtruth segmentation mask."}, {"heading": "3.2.2 Experiments on QFSS", "text": "Features. In addition to representing the questions using the word embedding features xq as in Section 3.1.1, we also test the bag-of-words features. For each instance segmentation or proposal, we mask out all the other pixels in the image with 0\u2019s and then extract its features from the last pooling layer of a ResNet-152 [14].\nDataset Split. The SharpMask we use is learned from the training set of MS COCO. Hence, we split our VQS data in such a way that our test set does not intersect with the training set for SharpMask. Particularly, we use 26,995 images and correspondingly 68,509 questions as our training set. We split the remaining images and questions to two parts: 5,000 images and associated questions for validation, and 5,873 images with 14,875 questions as the test set.\nResults. Table 3 reports the comparison results on QFSS, evaluated by intersection-over-union (IOU). In addition, the first three columns are about the number of different types of questions and the average numbers of user selected segmentations per question type. On average, more than one segmentations are selected for any of the question types.\nFirst of all, we note that the proposed mask aggregation outperforms the baseline DeconvNet and yet is significantly worse than its upper bound method. The mask aggregation is superior over DeconvNet partially because it has actually used extra supervised information beyond our VQS data; namely, the SharpMask is trained using all the instance segmentations in the training set of MS COCO. The upper bound results indicate there is still large room for the mask aggregation framework to improve; one possibility is make it deep in the future work.\nBesides, we find that the two question representations, bag-of-wrods (B) and word embedding (W), give rise to distinguishable results for either mask aggregation or DeconvNet. This observation is intriguing since it implies that the QFSS task is responsive to the question representation schemes. It is thus reasonable to expect that QFSS will both benefit from and advance the progress on joint vision and\nlanguage modeling methods. Finally, Figure 8 shows some qualitative segmentation results. Note the two separate instance segmentations in the first row that visually answer the \u201cHow many\u201d question."}, {"heading": "4. Conclusion", "text": "In this paper, we propose to link the instance segmentations provided by COCO [25] to the questions and answers in VQA [3]. The collected links, named visual questions and segmentation answers (VQS), transfer human supervision between the individual tasks of semantic segmentation and VQA, thus enabling us to study at least two problems with better leverage than before: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting multilayer perceptrons with some attention features. For the latter, we propose a new approach based on mask aggregation. To put it in perspective, we study a baseline method and an upper-bound method by assuming the instance segmentations are given as oracles.\nOur work is inspired upon observing the popularity of COCO [25]. We suspect that the existing and seemingly distinct annotations about MSCOCO images are inherently connected. They reveal different levels and perspectives of human understandings about the same visual scenes. Explicitly linking them up can significantly benefit not only individual tasks but also the overarching goal of unified vision-language understanding. This paper just scratches the surface. We will explore more types of annotations and richer models in the future work.\nAcknowledgement This work is supported in part by the NSF award IIS #1566511, a gift from Adobe Systems Inc., and a GPU from NVIDIA. C. Gan is partially supported by the National Basic Research Program of China 2011CBA00300 & 2011CBA00301, and the National Natural Science Foundation of China 61033001 & 61361136003."}, {"heading": "A. Annotation Interface", "text": "Figure 10 shows the annotation user interface we used to collect the VQS dataset. Given a question about an image, the participants are asked to tick the colors of the corresponding segmentations to visually answer the question. The participants can also click the \u201cAdd\u201d button to draw bounding box(es) over the image in order to answer the question, in addition to choosing the segments. For more information please see the attached slides which we used to train the annotators.\nB. VQS vs. VQA-HAT Figure 9 contrasts the human attention maps in VAQHAT [5] with our collected image segmentations that are linked by the participants to the questions and answers. We observe that the HAT maps are rough comparing to the segmentation masks. For example, to answer the question \u201cwhat color is the ball?\u201d, our VQS dataset will provide a very accurate segmentation mask of the ball without including any background. We expect that such accurate annotations are more suitable for visual grounding tasks. Moreover, while segmentation is the desired final output in VQS, the HAT maps mainly serve to analyze and potentially improve VQA models that output/choose text answers."}, {"heading": "C. The influence of VQS segmentation mask", "text": "resolution on the supervised attention in VQA\nThe attention features we studied in Section 3.1.1 of the main text weigh the feature representations of different regions according to the question about the image. The number of regions per image indicate the attention resolutions. The more regions (the higher resolution) we consider, the more accurate the attention model could be. Of course, too small regions would also result in trivial solutions since the visual cues in each region would be too subtle then.\nIn the Table 4, we report the VQA Real Multiple-Choice results on the Test-Dev by using different resolutions of the segmentation masks. We can observe that higher resolution leads to better VQA results. In some spirit, this implies the necessity of the accurate segmentation annotations for the supervised attention in VQA."}, {"heading": "D. Some implementation details in the VQA", "text": "We use an ensemble of 10 models in our experiments for the VQA Real Multiple-Choice task (cf. Table 1 of the main text). Among them, five are trained using the attribute feature representations of the images and the other five are based on the ResNet features. We use the validation set to select the best 10 models as well as how to combine them by a convex combination of their decision values. After that, we test the ensemble on Test-Dev and Test-Standard, respectively.\nFor the VQS experiments, we use the ADAM [21] gradient descent to train the whole network with the learning\nrate 0.001 and batch size 16. It takes about one week on one Titan X GPU machine to converge after 15 epochs. We also report some additional results in Table 5 for our exploration of the LSTM language embedding in the DeconvNet approach. We observe that the LSTM language embedding model (L) gives rise to about 0.02 improvement over the bag-of-words (B) and word2vec embedding (W) on the challenging VQS task."}], "references": [{"title": "SPICE: Semantic propositional image caption evaluation", "author": ["P. Anderson", "B. Fernando", "M. Johnson", "S. Gould"], "venue": "ECCV, pages 382\u2013398,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "K. Dan"], "venue": "CVPR, 27:55\u201356,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, pages 2425\u20132433,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "CVPR, pages 2422\u20132431,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh", "D. Batra"], "venue": "arXiv preprint arXiv:1606.03556,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, pages 2625\u20132634,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing automatic evaluation measures for image description", "author": ["D. Elliott", "F. Keller"], "venue": "ACL, 452(457):457,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "T-PAMI, 32(9):1627\u20131645,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "EMNLP,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Stylenet: Generating attractive visual captions with styles", "author": ["C. Gan", "Z. Gan", "X. He", "J. Gao", "L. Deng"], "venue": "CVPR,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2017}, {"title": "DevNet: A deep event network for multimedia event detection and evidence recounting", "author": ["C. Gan", "N. Wang", "Y. Yang", "D.-Y. Yeung", "A.G. Hauptmann"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2568\u20132577,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic compositional networks for visual captioning", "author": ["Z. Gan", "C. Gan", "X. He", "Y. Pu", "K. Tran", "J. Gao", "L. Carin", "L. Deng"], "venue": "CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2017}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, pages 2296\u20132304,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Segmentation from natural language expressions", "author": ["R. Hu", "M. Rohrbach", "T. Darrell"], "venue": "ECCV,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural language object retrieval", "author": ["R. Hu", "H. Xu", "M. Rohrbach", "J. Feng", "K. Saenko", "T. Darrell"], "venue": "CVPR,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "In ECCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, pages 3128\u20133137,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D. Kwak", "M.-O. Heo", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": "NIPS, pages 361\u2013369,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Hadamard product for low-rank bilinear pooling", "author": ["J.-H. Kim", "K.-W. On", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": "ICLR,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2017}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.-J. Li", "D.A. Shamma"], "venue": "arXiv preprint arXiv:1602.07332,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Visual question answering with question representation update (qru)", "author": ["R. Li", "J. Jia"], "venue": "Advances in Neural Information Processing Systems, pages 4655\u20134663,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, pages 740\u2013755,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "NIPS,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Generation and comprehension of unambiguous object descriptions", "author": ["J. Mao", "J. Huang", "A. Toshev", "O. Camburu", "A. Yuille", "K. Murphy"], "venue": "arXiv preprint arXiv:1511.02283,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-RNN)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "NIPS, 26:3111\u20133119,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "ICCV, pages 1520\u20131528,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P. Hongsuck Seo", "B. Han"], "venue": "CVPR, pages 30\u201338,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Y. Pan", "T. Mei", "T. Yao", "H. Li", "Y. Rui"], "venue": "CVPR, pages 4594\u20134602,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to refine object segments", "author": ["P.O. Pinheiro", "T.Y. Lin", "R. Collobert", "P. Doll\u00e1r"], "venue": "ECCV,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "author": ["B.A. Plummer", "L. Wang", "C.M. Cervantes", "J.C. Caicedo", "J. Hockenmaier", "S. Lazebnik"], "venue": "ICCV, pages 2641\u20132649,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS, pages 2953\u20132961,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2015}, {"title": "Grounding of textual phrases in images by reconstruction", "author": ["A. Rohrbach", "M. Rohrbach", "R. Hu", "T. Darrell", "B. Schiele"], "venue": "ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR, pages 4613\u20134621,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic concept discovery from parallel text and visual corpora", "author": ["C. Sun", "C. Gan", "R. Nevatia"], "venue": "ICCV,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "CVPR, pages 4566\u20134575,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, pages 3156\u20133164,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deep structurepreserving image-text embeddings", "author": ["L. Wang", "Y. Li", "S. Lazebnik"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5005\u20135013,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2016}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Q. Wu", "C. Shen", "L. Liu", "A. Dick", "A. van den Hengel"], "venue": "In CVPR,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "ICML,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "ECCV,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A. Courville", "R. Salakhudinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, pages 2048\u20132057,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A. Smola"], "venue": "CVPR,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning structural svms with latent variables", "author": ["C.-N.J. Yu", "T. Joachims"], "venue": "ICML, pages 1169\u20131176. ACM,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "Modeling context in referring expressions", "author": ["L. Yu", "P. Poirson", "S. Yang", "A.C. Berg", "T.L. Berg"], "venue": "European Conference on Computer Vision, pages 69\u201385. Springer,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 41, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 28, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 17, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 5, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 39, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 3, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 46, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 11, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 32, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 9, "context": "We have witnessed the resurgence of image captioning [42, 29, 18, 6, 40, 4, 47, 12, 33, 10] which is often addressed by jointly modeling visual and textual content with deep neural networks.", "startOffset": 53, "endOffset": 91}, {"referenceID": 6, "context": "jective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities.", "startOffset": 91, "endOffset": 101}, {"referenceID": 40, "context": "jective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities.", "startOffset": 91, "endOffset": 101}, {"referenceID": 0, "context": "jective \u2014 it is hard to evaluate the quality of captions generated by different algorithms [7, 41, 1], and tend to miss subtle details \u2014 in training, the models may be led to capturing the scene-level gist rather than fine-grained entities.", "startOffset": 91, "endOffset": 101}, {"referenceID": 2, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 50, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 35, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 12, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 91, "endOffset": 106}, {"referenceID": 34, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 14, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 36, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 27, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 15, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 42, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 49, "context": "In light of the premises and demerits of image captioning, visual question answering (VQA) [3, 51, 36, 13] and visual grounding [35, 15, 37, 28, 16, 43, 50] are proposed, in parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.", "startOffset": 128, "endOffset": 156}, {"referenceID": 24, "context": "COCO [25] is especially noticeable among them.", "startOffset": 5, "endOffset": 9}, {"referenceID": 2, "context": "crowdsourced questions and answers (QAs) about a subset of the COCO images and abstract scenes [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 50, "context": "sociated with bounding boxes in the images [51].", "startOffset": 43, "endOffset": 47}, {"referenceID": 27, "context": "[28] and Yu et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[50] have users to give referring expressions that each pinpoints a unique object in an image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The Visual Genome dataset [22] also intersects with COCO in terms of images and provides dense human annotations, especially scene graphs.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "In particular, we focus on linking the segmentations provided by COCO [25] to the QAs in the VQA dataset [3].", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "In particular, we focus on linking the segmentations provided by COCO [25] to the QAs in the VQA dataset [3].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "have collected some human attention maps for the VQA task [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 50, "context": "While bounding boxes are provided in Visual7W [51] for object mentions in QAs, they do not serve for the purpose of directly answering the questions except for the \u201cpointing\u201d type of questions.", "startOffset": 46, "endOffset": 50}, {"referenceID": 24, "context": "We call the collected links between the COCO segmentations [25] and QA pairs in the VQA dataset [3] visual questions and segmentation answers (VQS).", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "We call the collected links between the COCO segmentations [25] and QA pairs in the VQA dataset [3] visual questions and segmentation answers (VQS).", "startOffset": 96, "endOffset": 99}, {"referenceID": 16, "context": "For the former, we obtain state-of-the-art results on the VQA real multiplechoice task by simply augmenting the multilayer perceptrons (MLP) of [17] with attention features.", "startOffset": 144, "endOffset": 148}, {"referenceID": 47, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 45, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 44, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 25, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 23, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 97, "endOffset": 117}, {"referenceID": 1, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 151, "endOffset": 158}, {"referenceID": 26, "context": "The attention scheme is often found useful for VQA, by either attending particular image regions [48, 46, 45, 26, 24] or modeling object relationships [2, 27].", "startOffset": 151, "endOffset": 158}, {"referenceID": 4, "context": "As a result, the machine-generated attention maps are poorly correlated with human attention maps [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": ", object detection [8], video recognition [11] and text processing [49].", "startOffset": 19, "endOffset": 22}, {"referenceID": 10, "context": ", object detection [8], video recognition [11] and text processing [49].", "startOffset": 42, "endOffset": 46}, {"referenceID": 48, "context": ", object detection [8], video recognition [11] and text processing [49].", "startOffset": 67, "endOffset": 71}, {"referenceID": 16, "context": "We show that, by supervised learning to attend different image regions using the collected segmentationQA links, we can boost the simple MLP model [17] to very compelling performance on the VQA real multi-choice task.", "startOffset": 147, "endOffset": 151}, {"referenceID": 34, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 36, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 27, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 15, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 72, "endOffset": 88}, {"referenceID": 14, "context": "While visual grounding (VG) avoids the caveat by placing bounding boxes [35, 37, 28, 16] or segmentations [15] over the target visual entities, the scope of the text expressions in existing VG works is often limited to the visual entities present in the image.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "\u2019s work [15] is the most related to QFSS.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Unlike the questions used in this work that are flexible to incorporate commonsense and knowledge bases, the expressive scope of the text phrases in [15] is often limited to the visual entities in the associated images.", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "We build our work upon the images and instance segmentation masks in COCO [25] and the QAs in the VQA dataset [3].", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "We build our work upon the images and instance segmentation masks in COCO [25] and the QAs in the VQA dataset [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "The questions in VQA [3] are diverse and comprehensively cover various parts of an image, different levels of semantic interpretations, as well as commonsense and knowledge bases.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "[3] and the complex visual scenes in COCO [25], the participants have to parse the question, understand the visual scene and context, infer the interactions between visual entities, and then pick up the segmentations that answer the questions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "[3] and the complex visual scenes in COCO [25], the participants have to parse the question, understand the visual scene and context, infer the interactions between visual entities, and then pick up the segmentations that answer the questions.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "After that, we augment the MLP model [17] by the attention features.", "startOffset": 37, "endOffset": 41}, {"referenceID": 16, "context": "To verify this point, we design a simple experiment to augment the MLP model in [17].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The augmented MLP significantly improves upon the plain version and gives rise to state-of-the-art results on the VQA real multiple-choice task [3].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "We conduct experiments on the VQA Real Multiple Choices [3].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "We evaluate our results following the metric suggested in [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "propose to transform the problem to a stack of binary classification problems [17] and solve them by the multilayer perceptrons (MLP) model:", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "Two-layer LSTM [3] 62.", "startOffset": 15, "endOffset": 18}, {"referenceID": 37, "context": "1 Region selection [38] 62.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "4 DPPNet [32] 62.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "7 MCB [9] 65.", "startOffset": 6, "endOffset": 9}, {"referenceID": 25, "context": "4 \u2212 Co-Attention [26] 65.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "1 MRN [19] 66.", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "3 MLB [20] \u2212 68.", "startOffset": 6, "endOffset": 10}, {"referenceID": 16, "context": "9 MLP + ResNet [17] 67.", "startOffset": 15, "endOffset": 19}, {"referenceID": 29, "context": "For a question or answer, we represent it by averaging the 300D word2vec [30] vectors of the constituent words, followed by the l2 normalization.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "This is the same as in [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 13, "context": "We extract two types of features from an input image: ResNet [14] pool5 activation and attribute features [44], where the latter is the attribute detection scores.", "startOffset": 61, "endOffset": 65}, {"referenceID": 43, "context": "We extract two types of features from an input image: ResNet [14] pool5 activation and attribute features [44], where the latter is the attribute detection scores.", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "The training data is obtained from the COCO image captions [25].", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "The upper panel of Figure 5 illustrates the process of extracting the attention features, and the bottom panel shows the MLP model [17] augmented with our attention features for the VQA real multiple-choice task.", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Plain MLP [17] 80.", "startOffset": 10, "endOffset": 14}, {"referenceID": 4, "context": "HAT [5] 80.", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "In this section, we contrast the VQS data to the human attention maps (HAT) [5] and bounding boxes that are placed tightly around the segmentations in VQS.", "startOffset": 76, "endOffset": 79}, {"referenceID": 47, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 45, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 44, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 25, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 23, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 1, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 26, "context": "However, we believe it remains interesting to examine VQS for more generic attention-based VQA models [48, 46, 45, 26, 24, 2, 27].", "startOffset": 102, "endOffset": 129}, {"referenceID": 14, "context": "It is designed in a way similarly to the segmentation from natural language expressions [15], with possible applications to robot vision, photo editing, etc.", "startOffset": 88, "endOffset": 92}, {"referenceID": 33, "context": "In particular, we use N = 25 segmentation proposals e1, e2, \u00b7 \u00b7 \u00b7 , eN generated by SharpMask [34] given an image.", "startOffset": 94, "endOffset": 98}, {"referenceID": 44, "context": ", memory network [45] and stacked attention network [48]).", "startOffset": 17, "endOffset": 21}, {"referenceID": 47, "context": ", memory network [45] and stacked attention network [48]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Finally, we study a competitive baseline which is motivated by the textconditioned FCN [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "As Figure 7 shows, it contains three components, a convolutional neural network (CNN) [23], a deconvolutional neural network (DeconvNet) [31], and a question embedding to attend the feature maps in CNN.", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "As Figure 7 shows, it contains three components, a convolutional neural network (CNN) [23], a deconvolutional neural network (DeconvNet) [31], and a question embedding to attend the feature maps in CNN.", "startOffset": 137, "endOffset": 141}, {"referenceID": 30, "context": "The convolutional and deconvolutional nets follow the specifications in [31].", "startOffset": 72, "endOffset": 76}, {"referenceID": 38, "context": "Namely, a VGG-16 [39] is trimmed till the last convolutional layer, followed by two fully connected layers, and then mirrored by DeconvNet.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "For each instance segmentation or proposal, we mask out all the other pixels in the image with 0\u2019s and then extract its features from the last pooling layer of a ResNet-152 [14].", "startOffset": 173, "endOffset": 177}, {"referenceID": 24, "context": "In this paper, we propose to link the instance segmentations provided by COCO [25] to the questions and answers in VQA [3].", "startOffset": 78, "endOffset": 82}, {"referenceID": 2, "context": "In this paper, we propose to link the instance segmentations provided by COCO [25] to the questions and answers in VQA [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 24, "context": "Our work is inspired upon observing the popularity of COCO [25].", "startOffset": 59, "endOffset": 63}], "year": 2017, "abstractText": "Rich and dense human labeled datasets are among the main enabling factors for the recent advance on visionlanguage understanding. Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal different levels and perspectives of human understandings about the same visual scenes \u2014 and even the same set of images (e.g., of COCO). The popularity of COCO correlates those annotations and tasks. Explicitly linking them up may significantly benefit both individual tasks and the unified vision and language modeling. We present the preliminary work of linking the instance segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected links visual questions and segmentation answers (VQS). They transfer human supervision between the previously separate tasks, offer more effective leverage to existing problems, and also open the door for new research problems and models. We study two applications of the VQS data in this paper: supervised attention for VQA and a novel question-focused semantic segmentation task. For the former, we obtain state-of-the-art results on the VQA real multiple-choice task by simply augmenting the multilayer perceptrons with some attention features that are learned using the segmentation-QA links as explicit supervision. To put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that the instance segmentations are given at the test stage.", "creator": "LaTeX with hyperref package"}}}