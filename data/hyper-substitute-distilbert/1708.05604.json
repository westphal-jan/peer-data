{"id": "1708.05604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2017", "title": "Accelerating recurrent neural network training using sequence bucketing and multi-GPU data parallelization", "abstract": "an overall algorithm for recurrent structured network training technically useful. the complexity increases the training speed for just saying any length of the input sequence may differ significantly. so proposed approach is plotted on the optimal batch bounded by input queue iteration algorithms improves parallelization on multiple graphical processing units. the baseline sampling performance optimal sample bucketing is compared with the proposed solution of performing different number or patterns. an example is given for each online packet recognition task using an lstm recurrent genetic nets. second evaluation scenario considered approximately terms of the local clock time, trends and epochs, random signal loss value.", "histories": [["v1", "Fri, 18 Aug 2017 13:36:30 GMT  (981kb)", "http://arxiv.org/abs/1708.05604v1", "4 pages, 5 figures, Comments, 2016 IEEE First International Conference on Data Stream Mining &amp; Processing (DSMP), Lviv, 2016"]], "COMMENTS": "4 pages, 5 figures, Comments, 2016 IEEE First International Conference on Data Stream Mining &amp; Processing (DSMP), Lviv, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["viacheslav khomenko", "oleg shyshkov", "olga radyvonenko", "kostiantyn bokhan samsung r&d institute ukraine srk)"], "accepted": false, "id": "1708.05604"}, "pdf": {"name": "1708.05604.pdf", "metadata": {"source": "META", "title": "Accelerating Recurrent Neural Network Training", "authors": ["Viacheslav Khomenko", "Oleg Shyshkov", "Olga Radyvonenko", "Kostiantyn Bokhan"], "emails": ["v.khomenko@samsung.com"], "sections": [{"heading": null, "text": "network training is presented. The approach increases the training speed for tasks where a length of the input sequence may vary significantly. The proposed approach is based on the optimal batch bucketing by input sequence length and data parallelization on multiple graphical processing units. The baseline training performance without sequence bucketing is compared with the proposed solution for a different number of buckets. An example is given for the online handwriting recognition task using an LSTM recurrent neural network. The evaluation is performed in terms of the wall clock time, number of epochs, and validation loss value.\nKeywords \u2014 recurrent neural network; mini-batch;\nsequence bucketing; data parallelization; LSTM; GPU\nI. INTRODUCTION\nDeep neural networks have recently proven to be successful in pattern recognition tasks. The Recurrent Neural Network (RNN) is a subclass of neural networks defined by presence of feedback connections.\nLong Short-Term Memory (LSTM) [1] RNNs perform better on tasks involving long time lags compared to traditional RNNs. The gating mechanism permits LSTM to bridge long time lags between relevant events (103 time steps and more). Gated Recurrent Unit (GRU) networks [2] have similar ideology to an LSTM, but they speed up training due to architectural simplifications.\nThe ability of RNNs to memorize historical data makes them a powerful sequence-modeling tool. They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].\nHowever, RNN training on a big amount of data is still a challenging problem. The aim of the paper is to demonstrate an effective approach to accelerate RNN minibatch training on big amount of data.\nThis paper is organized as follows. The related works overview is given in section 2. Section 3 describes the training algorithm. The experimental evaluation is given in section 4. Then, the results are discussed, and the evaluation of the proposed sequence bucketing algorithm against the conventional sequence shuffling is presented.\nII. BACKGROUND AND RELATED WORK\nThe problem of accelerating the RNN mini-batch gradient descent training is widely discussed in the literature\nin the last years [8, 9]. Some works consider adaptive learning algorithms with heuristics for tuning of learning parameters (learning rate, weight decay, etc.) to improve convergence of model training [8].\nMany researchers have focused their efforts on experiments with different network architectures and parallelization techniques [6, 9 and 10].\nTraining parallelization and a two-stage network structure for RNN [9] allow to speed-up training. However, the two-stage architecture gives substantial acceleration mainly when the number of outputs of the network is sufficiently large (103 or more).\nThe BlackOut [11] approach allows to accelerate training for even larger vocabularies (106 outputs). It relies on weighted sampling strategy, employs a discriminative training loss and is applied only to the softmax output layer, in contrast to DropOut [12], which is typically applied to the input and hidden layers. The application of BlackOut is also limited to networks with large output size.\nIn its turn, the curriculum learning [13] consists in organizing training samples in a meaningful way rather than in purely random order. It improves LSTM training on the program evaluation and memorization tasks [14].\nIt is commonly known that labeling of unsegmented sequence input data is a ubiquitous problem in the realworld sequence learning. It is particularly common in perceptual tasks (e.g. handwriting, speech or gesture recognition), where noisy real-valued input sequences are annotated with non-aligned strings [15]. Since Connectionist Temporal Classification (CTC) networks gained use in RNN training as a sequence alignment technique, the problem of RNN training using input sequences of different length turned out to be more important and affecting training speed.\nUsually, sequences are grouped into mini-batches. The length of the longest sequence in the batch thus defines the computational complexity of the training. Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length. Batch grouping algorithms could be useful for organizing training data [19, 20]. However, the following two problems arise in this case:\n1. Finding the optimal batch clustering by sequence lengths.\n2. Balancing between input data streamlining and the need of shuffling training data sequences before RNN training.\nIn the next sections, we present and evaluate the RNN training approach with an effective sequence bucketing that solves problems mentioned above.\n\u201c\u00a9 20xx IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works,\nfor resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.\u201d\nIII. TRAINING ALGORITHM BASED ON SEQUENCE BUCKETING AND MULTI-GPU DATA PARALLELIZATION\nWe propose the RNN model training algorithm that runs in parallel on multiple Graphical Processing Units (GPUs). The developed solution uses a map-reduce approach for parallel computing of individual models by sub-partitioning training data. The training data is shuffled before every epoch and is equally redistributed between different GPU processes. Each training process applies batch bucketing optimization scheme by clustering training sequences considering input lengths. Final model parameters are obtained by reducing results of each training process.\nThe proposed training workflow is presented in the\ndiagram (Fig. 1)."}, {"heading": "A. Sequence bucketing algorithm", "text": "We accelerate the training on the individual GPU by sequence bucketing that deals with the problem of large variation of input sequence lengths. The empirical distribution of input sequence lengths and an example of clusterization for the number of buckets Q = 6 are shown in Fig. 2.\nThe bucketing can be described as an optimization\nproblem. Let },...,,{ 21 nsssS  be the set of sequences\nand ii sl  is the length of sequence i . Each GPU\nprocesses sequences in a mini-batch in a synchronized parallel manner, so processing time of a mini-batch },...,,{ 21 ksssB  is proportional to  iki lO ,,1max  and processing time of whole set is expressed as:\n   ini lknOST ,,1max  (1)\nThe minimum and maximum sequence length in a mini-batch might be very different if sequences were shuffled randomly before splitting. As a result, a GPU does additional work by processing empty tails of shorter sequences. To overcome this flaw and decrease processing time, we recommend an algorithm that optimizes batch clustering.\nLet's call bucketing a process when we cluster all sequences into Q buckets by their lengths, where Q is some small positive integer number. Let },...,{ 11  ii jji ssS be the i th bucket. For every bucket, we perform mini-batch training. The processing time of the whole set will become:\n                    QlknOSTOST Q i pp Q i i ijij 11 11 max)(  (2)\nThe dynamic programming algorithm is used to find optimal bucket sizes. The bucket sizes only depend on sequence lengths, so we use an array f that stores a number of sequences for each length.\nWe use the following notations:\n\u2022 Q is the desired number of buckets;\n\u2022 f[l] is the number of frequencies with input length l;\n\u2022 dp[i][k] is the best score of bucketing if first i elements were cut into k groups;\n\u2022 dp[0][0] is set to 0;\n\u2022 dp[i][0] is set to INF;\n\u2022         \n\n\ni\nit\nij\nj tfikjdpkidp\n1\n1\n1 ][]1][[min]][[ ;\n\u2022 prevDp[i][j] is the end index of the i \u2013 1 bucket when first j elements were split into i buckets.\nThe pseudo-code of the proposed algorithm for\nsequence bucketing is presented below:\nprocedure DYNAMICBUCKETING (Q, f)\nn \u2190 length(f) for q = 1 to Q do\nfor i = 1 to n do\ncurSum \u2190 f[i] for j = i - 1 downto 0 do\nval \u2190 curSum \u2219 i + dp[q - 1][j] if val < dp[q][i] then\ndp[q][i] \u2190 val prevDp[q][i] \u2190 j\nend if curSum \u2190 curSum + f[j]\nend for\nend for\nend for curId \u2190 n - 1 bests \u2190 [] for i = Q downto 1 do\nbests.push_front(curId) curId \u2190 prevDp[i][curId]\nend for\nreturn bests end procedure\nThe algorithm optimization result for the given distribution of input sequence lengths and desired number of buckets is presented in Fig. 2."}, {"heading": "B. Parallel training of recurrent neural networks", "text": "For the parallel training of the RNNs on GPU we propose the following algorithm which allows massive parallel model training and can scale up to a large number of GPUs:\n1. Initialize base model parameters. 2. Build Q models (for optimal input sequence lengths) and serialize models to storage, for example, the file system. Parameters of each model are initialized randomly.\n3. Generate training data (1st epoch) or re-generate training data by re-shuffling between portions of data for following epochs. The number of data portions is equal to the number of GPUs. This is possible under the assumption that the amount of training data is sufficient [9].\n4. For each of the training data portions, spawn individual training processes. The process iterates over the mini-batches in the data portion. The batches are formed considering input sequence lengths. The appropriate models are selected. The parameter update rule of the individual model is ADADELTA [21].\n5. Each of the training processes returns model parameters. The aggregation of parameters is done in the main process according to the model update rule proposed in [9]. We have found that setting parameter 1 gives the best results for our model and leads to the following equation:\n     1 tVtVtV  (3)\nwhere  tV are the parameter values at the current epoch;\n 1tV are the parameter values at the previous epoch (or\ninitial values for the first epoch);  tV are the mean parameter values over parallel models; 6101  is the\nregularization term that leads to weight decay proportionally to their values.\nIV. EMPIRICAL EVALUATION"}, {"heading": "A. Experimental setup", "text": "The system was evaluated on online handwriting recognition task. The raw data contain 1 Gb samples of Afrikaans and English languages in binary form.\nThe dataset was collected on Samsung Galaxy S-Note devices with stylus input. The validation set is created from 5% of randomly selected samples of different length. The dataset contains textual labels that serve as a reference to output sequences. However, these labels are not explicitly aligned with input handwriting stroke sequences. At every epoch, the system was first fed with shorter sequences (first bucket), and then gradually the bucket number increased.\nThe LSTM model was trained with CTC cost function\nusing Theano [22] and Lasagne [23] frameworks.\nThe recurrent neural network model training procedure\nwas evaluated on a rack of 6 NVidia Tesla K40m GPUs."}, {"heading": "B. Model training", "text": "The validation loss as a function of the wall clock time and epoch number is given in Fig. 3; the best acceleration with minimum validation loss was achieved for Q = 3.\nIn terms of the wall clock time, the system without bucketing (purely random split of sequences on minibatches) has the longest epoch time (4 hours per epoch, Fig. 4). The epoch time reduces as Q increases. For the value of Q = 3, the speed up factor is close to 4.\nFrom the comparison, we can observe the influence of\nthe sequence buckets on the training speed and loss.\nWe observed faster convergence of the validation loss\nespecially at the beginning of the training.\nThe validation loss as a function of wall clock time for different number of used GPUs is presented in Fig. 5a, and as a function of epoch number in Fig. 5b.\nThe experiment shows that the validation error for 30 hours of training is better by 23% for 6 GPU case (see Fig. 5a, 1 GPU vs 6 GPU comparison).\nCONCLUSION\nIn this paper, the algorithm of RNN training acceleration based on sequence bucketing and multi-GPU data parallelization was presented.\nPrevious work demonstrated approaches for accelerating RNN gradient descent training involving heuristics for tuning of learning parameters, different network architectures and parallelization techniques. Those solutions, however, did not take into account that the data for perceptual machine learning tasks usually contain input sequences of different length. At the same time, the computational complexity of the training is defined by the longest sequence in the data batch.\nThe proposed approach improves training speed by clustering sequences into buckets by their length, thus finding a compromise between data structuring and shuffling.\nAn example of application to online handwriting recognition task with LSTM RNN is given. We obtained the acceleration factor 4 for the number of buckets Q = 3. Due to data parallelization in its turn, we observed the reduction of the validation loss by 23% for the same wall clock time compared to the single GPU case. In future work, we plan to investigate different strategies of bucket ordering during training on the model generalization.\nThis approach may also be useful for LSTM and GRU training in speech recognition, language modelling and other perceptual machine learning tasks."}], "references": [{"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation, vol. 12(10), pp. 2451\u20132471, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint, arXiv:1412.3555, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Unconstrained on-line handwriting recognition with recurrent neural networks", "author": ["A. Graves", "M. Liwicki", "H. Bunke", "J. Schmidhuber", "S. Fern\u00e1ndez"], "venue": "Advances in Neural Information Processing Systems, 2008, pp. 577\u2013584.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Combination of global and local contexts for text/non-text classification in heterogeneous online handwritten documents", "author": ["T. Van Phan", "M. Nakagawa"], "venue": "Pattern Recognition, vol. 51, 2016, pp. 112\u2013124.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Opinion Mining with Deep Recurrent Neural Networks", "author": ["O. Irsoy", "C. Cardie"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, October, pp. 720\u2013728.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convex net: A scalable architecture for speech pattern classification", "author": ["D. Yu", "L. Deng"], "venue": "Proceedings of Interspeech, 2011, pp. 2285\u20132288.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for lowlatency speech synthesis", "author": ["H. Zen", "H. Sak"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference, 2015, pp. 4470\u20134474.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speed Up of Recurrent Neural Network Language Models With Sentence Independent Subsampling Stochastic Gradient Descent", "author": ["Y. Shi", "M.-Y. Hwang", "K. Yao", "M. Larson"], "venue": "INTERSPEECH, 2013. pp. 1203\u20131207.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Accelerating recurrent neural network training via two stage classes and parallelization", "author": ["H. Zhiheng"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop, 2013.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A. Smola", "L. Li"], "venue": "Advances in Neural Information Processing Systems, vol. 23, 2010, pp. 2595\u20132603.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "BlackOut: Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies", "author": ["S. Ji", "S.V.N. Vishwanathan", "N.N. Satish", "M.J. Anderson", "P. Dubey"], "venue": "arXiv preprint, arXiv:1511.06909, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["V. Pham", "T. Bluche", "C. Kermorvant", "J. Louradour"], "venue": "Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference, pp. 285\u2013290.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Curriculum learning", "author": ["Y. Bengio", "J. Louradour", "R. Collobert"], "venue": "In Proceedings of the 26th annual international conference on machine learning", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Learning to execute", "author": ["W. Zaremba", "I. Sutskever"], "venue": "arXiv preprint,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez"], "venue": "Proceedings of the International Conference on Machine Learning, ICML-2006, pp. 369\u2013376.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM", "author": ["J.S. Garofolo"], "venue": "National Institute of Standards and Technology, NISTIR 4930, 1993.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1993}, {"title": "Unipen project of on-line data exchange and recognizer benchmarks", "author": ["I. Guyon"], "venue": "Pattern Recognition, vol. 2- Conference B: Computer Vision & Image Processing, Proceedings of the 12th IAPR International. Conference on, vol. 2, IEEE, 1994, pp. 29\u201333.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "IAMonDo-database: an online handwritten document database with non-uniform contents", "author": ["E. Inderm\u00fchle", "M. Liwicki", "H. Bunke"], "venue": "Proceedings of the 9th IAPR International Workshop on Document Analysis Systems, ACM, 2010, pp. 97\u2013104.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Recurrent Memory Network for Language Modeling.", "author": ["K. Tran", "A. Bisazza", "C. Monz"], "venue": "arXiv preprint,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1223\u20131231.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint, arXiv:1212.5701, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["T.T.D. Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "A. Belopolsky"], "venue": "arXiv preprint, arXiv:1605.02688, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Long Short-Term Memory (LSTM) [1] RNNs perform better on tasks involving long time lags compared to traditional RNNs.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "Gated Recurrent Unit (GRU) networks [2] have similar ideology to an LSTM, but they speed up training due to architectural simplifications.", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 148, "endOffset": 151}, {"referenceID": 3, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 4, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 195, "endOffset": 198}, {"referenceID": 5, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 219, "endOffset": 222}, {"referenceID": 6, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 237, "endOffset": 240}, {"referenceID": 7, "context": "They have found applications in pattern recognition and classification tasks where inputs and outputs are sequences: online handwriting recognition [3], document analysis [4], sentiment analysis [5], speech recognition [6] and synthesis [7], language modeling [8].", "startOffset": 260, "endOffset": 263}, {"referenceID": 7, "context": "The problem of accelerating the RNN mini-batch gradient descent training is widely discussed in the literature in the last years [8, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 8, "context": "The problem of accelerating the RNN mini-batch gradient descent training is widely discussed in the literature in the last years [8, 9].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": ") to improve convergence of model training [8].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "Training parallelization and a two-stage network structure for RNN [9] allow to speed-up training.", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "The BlackOut [11] approach allows to accelerate training for even larger vocabularies (10 outputs).", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "It relies on weighted sampling strategy, employs a discriminative training loss and is applied only to the softmax output layer, in contrast to DropOut [12], which is typically applied to the input and hidden layers.", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "In its turn, the curriculum learning [13] consists in organizing training samples in a meaningful way rather than in purely random order.", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "It improves LSTM training on the program evaluation and memorization tasks [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "handwriting, speech or gesture recognition), where noisy real-valued input sequences are annotated with non-aligned strings [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length.", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length.", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "Most of benchmark datasets for perceptual machine learning tasks (TIMIT [16], UNIPEN [17], IAMonDo [18]) contain recordings of different length.", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "Batch grouping algorithms could be useful for organizing training data [19, 20].", "startOffset": 71, "endOffset": 79}, {"referenceID": 19, "context": "Batch grouping algorithms could be useful for organizing training data [19, 20].", "startOffset": 71, "endOffset": 79}, {"referenceID": 8, "context": "This is possible under the assumption that the amount of training data is sufficient [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 20, "context": "The parameter update rule of the individual model is ADADELTA [21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 8, "context": "The aggregation of parameters is done in the main process according to the model update rule proposed in [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 21, "context": "The LSTM model was trained with CTC cost function using Theano [22] and Lasagne [23] frameworks.", "startOffset": 63, "endOffset": 67}], "year": 2017, "abstractText": "An efficient algorithm for recurrent neural network training is presented. The approach increases the training speed for tasks where a length of the input sequence may vary significantly. The proposed approach is based on the optimal batch bucketing by input sequence length and data parallelization on multiple graphical processing units. The baseline training performance without sequence bucketing is compared with the proposed solution for a different number of buckets. An example is given for the online handwriting recognition task using an LSTM recurrent neural network. The evaluation is performed in terms of the wall clock time, number of epochs, and validation loss value.", "creator": "Microsoft\u00ae Word 2016"}}}