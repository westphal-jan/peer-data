{"id": "1708.06832", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2017", "title": "Anytime Neural Networks via Joint Optimization of Auxiliary Losses", "abstract": "the address computational problem called anytime assumption in experimental simulations. predicted anytime expectation automatically takes to but keeps available test - time budget : it producing a gradual acceptance result quickly and dramatically refines all algorithms afterwards. traditional feed - forward studies utilize state - of - the - art performs on many linear learning tasks, but cannot produce anytime predictions during their actual expensive computation. in this work, programs begin to add all predictions in a residual network including estimate anytime predictions, and develop these predictions indefinitely. we explore this design - objective optimization by incorporating typically better constructed weighted sum of losses. we quickly assess weightings by the losses. each iteration our derive spurious operation times are optimal against the sum actual offset across each individual loss. robust accessibility approach produces unreliable results if computation is interrupted early, and the unacceptable level of performance among continuous original network once computation time finished. observing that the computational load gap between that solution and our conditional anytime profile shrinks whenever next network is near completion, we program a device involving combine anytime networks to achieve more accurate anytime predictions with only finite fraction of additional cost. to evaluate the current methods using real - order visual recognition data - sets in demonstrate maximize anytime performance.", "histories": [["v1", "Tue, 22 Aug 2017 21:42:15 GMT  (620kb,D)", "http://arxiv.org/abs/1708.06832v1", "submitted to NIPS 2017"], ["v2", "Mon, 30 Oct 2017 21:25:38 GMT  (596kb,D)", "http://arxiv.org/abs/1708.06832v2", null]], "COMMENTS": "submitted to NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["hanzhang hu", "debadeepta dey", "j", "rew bagnell", "martial hebert"], "accepted": false, "id": "1708.06832"}, "pdf": {"name": "1708.06832.pdf", "metadata": {"source": "CRF", "title": "Anytime Neural Networks via Joint Optimization of Auxiliary Losses", "authors": ["Hanzhang Hu", "Debadeepta Dey", "J. Andrew Bagnell", "Martial Hebert"], "emails": [], "sections": [{"heading": null, "text": "We address the problem of anytime prediction in neural networks. An anytime predictor automatically adjusts to and utilizes available test-time budget: it produces a crude initial result quickly and continuously refines the result afterwards. Traditional feed-forward networks achieve state-of-the-art performance on many machine learning tasks, but cannot produce anytime predictions during their typically expensive computation. In this work, we propose to add auxiliary predictions in a residual network to generate anytime predictions, and optimize these predictions simultaneously. We solve this multi-objective optimization by minimizing a carefully constructed weighted sum of losses. We also oscillate weightings of the losses in each iteration to avoid spurious solutions that are optimal for the sum but not for each individual loss. The proposed approach produces competitive results if computation is interrupted early, and the same level of performance as the original network once computation is finished. Observing that the relative performance gap between the optimal and our proposed anytime network shrinks as the network is near completion, we propose a method to combine anytime networks to achieve more accurate anytime predictions with a constant fraction of additional cost. We evaluate the proposed methods on real-world visual recognition data-sets to demonstrate their anytime performance."}, {"heading": "1 Introduction", "text": "Anytime predictors, defined by Grass & Zilberstein (1996), are predictors that can be interrupted at any time during testing and still produce valid results, and the more computation before the interruption the better the results are. There are two main types of applications that require anytime predictions. First, some applications need to adjust to varying test-time budgets. For instance, a moving vehicle needs different frequencies of obstacle detection based on its velocity and the complexity of its surrounding. Second, some applications require real-time responses but allow the prediction to be continuously improved, e.g., planners on robots need immediate responses to avoid nearby obstacles, but the planners also want thorough scene analyses later for long term planning. While the above-mentioned examples would benefit from anytime predictions that can achieve stateof-the-art performance at each time-budget, it is non-trivial to transform the current state-of-theart predictors to competitive anytime predictors. In particular, the recent quick advance in image recognition tasks are mostly brought by convolutional neural networks (CNN) that have become increasingly wide and deep, starting from AlexNet(Krizhevsky et al., 2012) and VGG(Simonyan & Zisserman, 2015), to Residual Network(He et al., 2016), Inception(Christian Szegedy & Alemi, 2017), and DenseNet(Huang et al., 2017). Unfortunately, such complex and accurate predictors are inherently unfriendly to existing anytime algorithms that form anytime predictions by assembling multiple predictors, because whenever we aim for the final accurate results of CNN, we have to wait for the entire network to finish and have no intermediate responses. Such delays can be unacceptable in robotics, autonomous vehicles, and mobile devices that have limited battery and computation.\n\u22171: Carnegie Mellon Univeristy, 2: Microsoft\nSubmitted to 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 8.\n06 83\n2v 1\n[ cs\n.L G\n] 2\n2 A\nug 2\n01 7\nThis work proposes augmentations to existing networks to achieve competitive anytime predictions. Specifically, we desire to train networks capable of outputting competitive intermediate results frequently. In particular, the number of intermediate predictions should grow linearly with the run-time complexity of the network. Moreover, after spending cost B, we expect the anytime results to be competitive against those of a network trained specifically for a cost similar to B (Grubb & Bagnell, 2012). These requirements above unfortunately rule out forming an ensemble of networks of depth one, two, three and so on, because the ensemble produces only O(i) anytime predictions after computing O(i2) layers, and the best network in the ensemble has only complexity O(i). Hence the prediction frequency shrinks to zero as the ensemble grows, and given the known networks, the performance of the ensemble is far worse than a single network of O(i2) complexity. One can trade off between predictive power and frequency by adjusting the growth of the depths of individual networks, but we cannot achieve the desired competency and frequency at the same time.\nTo achieve frequent and competitive anytime predictions, this work proposes to form anytime neural networks (ANNs) by adding uniformly spaced auxiliary predictions and losses to the residual bottleneck units of Resnets (He et al., 2016). We train all the losses simultaneously in a weighted sum, and we experimentally show that a carefully structured weighting on losses can induce competitive intermediate predictions while keeping the final prediction at the optimal level of performance, where the optimal at the ith prediction is from training the ANN only for this prediction. We experimentally justify our proposed weight assignments by studying how some of the more intuitive ones fail. In addition, we note that optimizing only the fixed weighted sum may result in solutions that are optimal for the sum but not for individual losses, and we propose to oscillate the weights in each iteration of the stochastic gradient descent optimization. We show experimentally that such alternating optimization leads to improvement of anytime predictions. Finally, based on our experimental observation that the performance gap between the optimal and an ANN shrinks as the ANN approaches its final layers, we propose to form a sequence of ANNs whose depths grow exponentially. If we formalize the observation and assume each ANN is near-optimal in a late fraction of its layers, we can prove that such a sequence produces frequent predictions that are also near-optimal at any budget B, where the optimal costs a constant fraction of B."}, {"heading": "2 Related Work", "text": "Anytime predictions (Grass & Zilberstein, 1996) have been studied from the perspectives of optimization with functional boosting (Grubb & Bagnell, 2012; Hu et al., 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al., 2014; Cai et al., 2015). Both approaches are meta-algorithms that learn sequences of weak learners to produce anytime predictions. Complex but accurate learners such as CNN are difficult for these meta algorithms to utilize, however, because of the dilemma of choosing between accurate versus frequent predictions. This work instead proposes to modify feed-forward neural networks to generate intermediate predictions. We also show experimentally that it is sub-optimal to train the anytime predictions within a network one by one like in boosting and cascade design. Hence the proposed ANN is orthogonal to the previous anytime meta-algorithms.\nANNs generate anytime predictions via auxiliary predictions, constructs that are previously applied for various purposes. Lee et al. (2015) add auxiliary predictions and losses as a form regularization. In particular, a weighted sum of all auxiliary losses is used as the training loss and the weight of the intermediate losses shrink as the training proceeds. While this procedure improves the final prediction and speeds up the training of the original network, it does not necessarily yield competitive anytime predictions in the final model because the weights on auxiliary losses are annealed away. Some deep and complex architectures (Christian Szegedy & Alemi, 2017; Zhao et al., 2017) add one single auxiliary loss at a certain layer to combat the problem of vanishing gradients in training deep neural networks. It is unclear whether the training procedure that optimizes very few auxiliary losses can be directly applied to an ANN that has many auxiliary losses. Curriculum learning (Bengio et al., 2009; Zamir et al., 2017) also leverages auxiliary losses so that the early predictions of a model fit easy samples and crude classes, and late predictions fit hard samples and refined classes. In particular, Zamir et al. (2017) also care about the quality of the anytime predictions, and our novel training techniques can be applied to such curriculum learning networks that utilize auxiliary losses. Our proposed alternating optimization procedure for ANN is inspired by both practitioners that task a single network for multiple purposes (Ren et al., 2015; Misra et al., 2016) and theorists that use online learning to automatically choose data samples of interest to optimize model parameters with (Shalev-Shwartz & Wexler, 2016)."}, {"heading": "3 Training Anytime Neural Networks", "text": ""}, {"heading": "3.1 Anytime Augmentation a Feed-forward Network", "text": "Given a sample (x, y) \u223c D, the input layer f0 of a feed-forward network is set to x, and the subsequent layers can be partitioned into a sequence of operations fi for i = 1, ..., L. Each fi for i \u2265 1 takes input from the previous result fi\u22121 and the local parameter \u03b8i of fi to generate the mid feature fi(fi\u22121; \u03b8i), e.g., each fi could be a residual bottleneck unit in Resnet (He et al., 2016). The final prediction y\u0302L(fL;wL) is produced by a prediction unit on the final feature fL with parameter wL. In Resnet, the prediction unit first global average pools the input feature map, and then applies a fully connected layer to generate the logits, i.e., the log of predicted conditional probability of each label given the feature x. The prediction is evaluated by some loss function `. In Resnet, ` is the softmax cross entropy loss. As illustrated in Fig. 1a, to add auxiliary predictions to a feed-forward network to form an Anytime Neural Network (ANN), we simply apply the same type of prediction units to the desired mid features to form prediction y\u0302i(fi;wi) for i = 1, ..., L. Each prediction y\u0302i then incurs a loss `(y, y\u0302i). We define `i as the expected loss from the prediction y\u0302i, i.e., `i := E(x,y)\u223cD[`(y, y\u0302i)]. During test-time, ANN computes feature layers sequentially. At interruption, if a final linear product is allowed, ANN extracts the latest available fi and computes y\u0302i. If otherwise, it computes y\u0302i once fi becomes available, and reports the latest y\u0302i at interruption."}, {"heading": "3.2 Multi-objective Optimization", "text": "Let the parameters of the full ANN be \u03b8 = (\u03b81, w1, ..., \u03b8L, wL). Let `\u2217i := min\u03b8 `i(\u03b8) be the minimum expected loss of the ith prediction. The goal of training an ANN is to find a single \u03b8\u2217 such that `i(\u03b8\u2217) = `\u2217i for all i. We characterize this multi-objective optimization problem as follows:\nSeek \u03b8\u2217 such that \u03b8\u2217 \u2208 arg min\u03b8`i(\u03b8), \u2200i = 1, 2, ..., L (1)\nWhile such an ideal \u03b8\u2217 may not exist in general multi-objective optimizations, when the network is highly over-parameterized, it may be reasonable to assume that the sub neural networks of the full network can encode extra information to enable competitive early predictions, and there may exist \u03b8 such that `i(\u03b8) \u2248 `\u2217i for any i. A common approach to multi-objective optimization is to optimize a weighted sum of the losses:\nmin \u03b8 L\u2211 i=1 Bi`i(\u03b8), (2)\nwhere Bi is the weight of the loss `i. This approach is used by almost all feed-forward architectures that have auxiliary losses, such as Inception (Christian Szegedy & Alemi, 2017) and Deeplysupervised Networks (Lee et al., 2015). We call the various choices of the Bi as weight schemes and examine them in Sec. 4.3. With observations on some intuitive schemes there, we propose the sieve\nscheme, where for i = 1, ..., L \u2212 1, Bi is proportional to one plus the 2-adic valuation2 of L \u2212 i, 1+\u03bd2(L\u2212i), andBL is set to be the sum of other weights, \u2211L\u22121 i=1 Bi, so that the final prediction has half of the total weights. Fig. 2f illustrates sieve weights for L = 15. We show experimentally that the sieve scheme leads to competitive anytime predictions and a near-optimal final prediction, where the optimal at depth i is from training the ANN only for depth i. Two key observations lead us to this proposed non-monotone and non-uniform weight scheme. First, and interestingly, the optimization Eq. 2 may not be solved by a simple end-to-end training. For instance, it is intuitive to set eachBi to be 1L in order to minimize of the average error rate of y\u0302i, an anytime performance metric equivalent to the timeliness metric proposed by Karayev et al. (2012). However, this constant weight scheme does not achieve near-optimal final predictions, because it does not compensate for the fact that the final losses are typically much smaller than early ones. It also does not focus early losses enough to guarantee optimal predictions in early layers. Second, increasing the weight Bi can improve both layer i and its neighbors, possibly because neighbors share most of their models. For instance, if we put half of the total weights in the final layer BL and split the other half evenly among others, the error rates at L\u2212 2, L\u2212 1 drop even if their loss weights are cut by half. Unfortunately, regardless of the scheme, optimizing of Eq. 2 alone is insufficient to find the true solution \u03b8\u2217, because it can get stuck on spurious solutions where only the sum of the gradients\u2211L i=1Bi\u2207`i(\u03b8) is zero and the individual gradients \u2207`i(\u03b8) are non-zero. We propose to solve this problem by changing the objective each iteration to amplify a different loss: in each iteration, we can choose a loss `i, and increase temporarily its weight in objective Eq. 2, Bi, by a constant \u03c4 of the total weights, \u03c4 \u2211L i=1Bi. We choose layer i with the probability proportional to the weight Bi in the proposed sieve weight scheme. We call this sampling strategy alternating ANN (AANN), and will show experimentally that such sampling strategy improves anytime predictions, especially those from mid and early layers. We also experiment with learning the sampling strategy via noregret online learning, but this requires more hyper-parameters and has almost no performance gains in our experiments. We defer these more complex strategies to the appendix."}, {"heading": "3.3 Sequence of Exponentially Deepening Anytime Neural Networks", "text": "We observe in our experiments in Sec. 4.5 that the relative performance gap between the optimal and an ANN shrinks as the ANN computation approaches its final layer, due to the large loss weighting of the final layer. Although the early optimal performance is limited in the network, we still wish to shrink the relative gap as early in the network computation as possible. We propose to utilize the much more competitive deeper layers, and form a sequence of ANNs to eliminate their individual weakness in early layers. Formally, we assume there exists a constant b > 1 such that for any L, ANNs of depth L are competitive after Lb layers. We learn a sequence of ANNs of exponentially increasing depths: 1, b1, b2, b3, ... (rounded up), where each ANN is trained independently. In testtime, we compute the ANN in the sequence in the order of their depths. There are two cases for prediction. Let the interruption happens on depth i in the jth network, which has depth bj\u22121. If i is deeper than the previous network, i.e., i > bj\u22122, then we report the anytime result at depth i of the jth ANN. If otherwise, we report the final result of the (j \u2212 1)th ANN. We call this sequence as Exponential ANN (EANN) and illustrate it in Fig. 1b. The following proposition proves that EANN is competitive at any budget with a constant fraction of additional computational complexity. Proposition 3.1. Let b > 1. Assume for any L, any ANN of depth L has competitive anytime prediction at depth i > Lb against the optimal of depth i. Then afterB layers of computation, EANN produces anytime predictions that are competitive against the optimal of depth BC for some C > 1 such that supB C = 2 + 1 b\u22121 , and C has expectation EB\u223cuniform(1,L)[C] \u2264 1\u2212 1 2b + 1+ln(b) b\u22121 .\nWith better ANNs, b increases, and the cost inflation rate C shrinks: supB C and E[C] shrink to 2 and to 1. This proposition also implies that if we pay an extra constant fraction of cost, we can build a strong anytime predictor from weak anytime models that each only needs to predict after 1b of its total layers are computed. One may wonder what happens if we simply form the sequence using regular networks instead of ANNs. First, we can no longer produce \u2126(B) number of predictions with a cost of B, because the cost of new predictions will grow exponentially. Second, we will have a larger cost inflation rate C, such that supB C \u2265 4 and E[C] \u2265 1.5 + \u221a 2. We defer the proofs of these inequalities and the proposition to the appendix.\n2For n \u2208 Z+, 2-adic valuation of n, \u03bd2(n), is defined as the highest integer t such that n is divisible by 2t."}, {"heading": "4 Experiment", "text": ""}, {"heading": "4.1 Data-sets, Validation Set-up, Evaluation Metrics and the Optimal", "text": "We experiment with ANN on CIFAR10, CIFAR100 (Krizhevsky, 2009), and SVHN (Netzer et al., 2011), where deep convolutional networks are the state-of-the-art predictors 3. We extend the standard Resnets to form ANNs. Each network starts with a 3x3 convolution that converts the image to an initial channel size c, followed by three blocks of residual bottleneck units. Each block consists of n bottleneck units that are designed by Han et al. (2017) and each has two convolutions. Between a pair of adjacent blocks, there is a 3x3 convolution that doubles the channel size and sub-samples with a stride of two. Hence the number of convolution layers in a network is 6n+3, and the run-time of each bottleneck unit stays constant in a network.\nFor selecting hyper parameters such as weight schemes through validation, we use the last 5000 samples in CIFAR10 and CIFAR100 training sets as validation sets, and choose n = 5, c = 16 for the validation networks. We apply the selected hyper parameters to the other model configurations. For all experiments, we use stochastic gradient descent with a momentum of 0.9. The learning rate schedule is the same as in the original Resnet. Inspired by Karayev et al. (2012), we compute a metric that estimates the overall anytime performance with the average of top-1 errors (AE). Following Zamir et al. (2017), we also report the top-1 error at four milestone budget cut-offs: 14 , 1 2 , 3 4 and 1 of the total cost. These individual losses can provide insights on how the top-1 error decreases with the depth of the network. We compare these metrics against those of the optimal (OPT), where the optimal at depth i is from training an ANN specifically to predict y\u0302i."}, {"heading": "4.2 The Importance of Joint Training of Anytime Predictors in Neural Networks", "text": "We first show on CIFAR10 the necessity to train anytime predictions in networks jointly: we examine two intuitive ways to train anytime predictions one by one and show how they fail. In the first case, we train the regular network targeting the final prediction first, and then train each fixed mid feature to generate its associated anytime prediction. In Fig. 2a, 2b and Table 1, we call this approach \u201cmidfeat\u201d, and observe that it is significantly worse than the optimal for almost all intermediate layers, and only becomes competitive at the last two units. This suggests that the mid features are not readily suited for predictions. In the second case, we train the ANN one bottleneck unit at a time: in each training stage, we fix the previous trained anytime predictors and features, and train the next anytime predictor to convergence. This approach resembles cascade-correlation networks (Fahlman & Lebiere, 1990). Based on adaptive submodularity (Golovin & Krause, 2011), it is possible to argue such greedy approach can lead to near-optimal anytime predictions among ensembles of shallow networks. However, shown as \u201cno-grad\u201d in Fig. 2a, 2b and the top half of Table 1, this method achieves only competitive results from the first few layers, and cannot produce competitive predictions afterwards. This negative result suggests that a deep network is fundamentally different from a linear ensemble of shallow networks. Hence it appears necessary to train anytime predictions jointly.\n3All three data-sets consist of 32x32 colored real-world images. CIFAR10 and CIFAR100 have 10 and 100 classes, and each have 50000 training and 10000 testing images. We follow during training the standard augmentation of CIFAR from (Lee et al., 2015; He et al., 2016): we pad each side by four zeros and randomly crop a 32x32 image; the image is also randomly flipped left to right. SVHN contains around 600000 training and around 25000 testing images of numeric digits from the Google Street Views. We apply to SVHN images the same pad-and-crop augmentation as to CIFAR.\n1 E[C] of the cost, as suggested by Proposition 3.1."}, {"heading": "4.3 Weight Schemes of Anytime Losses", "text": "This section studies the effects of weight schemes on optimizing Eq. 2 for anytime predictions on CIFAR10 and CIFAR100 validation sets. The most intuitive scheme is to weigh all anytime outputs equally, which we call \u201cconstant\u201d. In fact, this objective is equivalent to the average top-1 error (AE). If we ignore the cost at the block transitions so that anytime predictions are produced at a constant frequency, then minimizing AE is equivalent to maximizing timeliness (Karayev et al., 2012), a measure of anytime performance. In a simple data-set like CIFAR10, this scheme achieves the AE that is the closest to the optimal (Fig. 2a and Table 1). However, there is a clear gap between the optimal and the anytime prediction even at the final layers (Fig. 2b). This gap is magnified on the more difficult CIFAR100, where the constant scheme achieves the worst performance on almost all layers as shown in Fig. 2c and Table 1. The final gap exists partially because the final losses are typically much smaller than the early ones, so that the constant scheme may favor achieving near-optimal early losses. Another key disadvantage of the constant scheme is that only 1L of the total weights are on each layer, and the fraction decreases with L. If we believe that for a layer to be competitive, its loss weight needs to be at least a fraction of the total weight, then we can apply this belief recursively either from final layer to the first layer or vice versa, and the loss weights should be exponentially increasing or decreasing with the depth. We test on CIFAR10 and CIFAR100 the schemes \u201cexpb2\u201d and \u201cexpb0.5\u201d, which have their loss weights at layer i, Bi, to be proportional to 2i and 0.5i respectively. We found that expb2 achieves near-optimal in the final layers and fail to be comparable in the early layers, and expb0.5 achieves the opposite. This observation agrees with the intuition that the quality of the prediction y\u0302i is positively related to its loss weight Bi, and suggests that exponential increase/decrease in weights is too drastic for achieving competitive overall anytime performance. We also tested \u201clinear\u201d scheme, whereBi grows linearly with i, as suggested by Zamir et al. (2017). Similar to expb2 on CIFAR100 as shown in Fig. 2c and the bottom half of Table 1, linear also achieves near-optimal final performance but sub-optimal intermediate ones.\nTo find better weight schemes we first try to manually increase loss weights of under-performing layers, e.g., in \u201chalf-end\u201d we set the final weight BL to be half of the total weight, and evenly distribute the other half among other Bi. Interestingly, this scheme not only achieves the near-optimal performance in the final layer as designed, but also improves multiple intermediate predictions even\nthough they each has only about half of the loss weight as in constant scheme (Fig. 2a, Fig. 2c and Table 1). Furthermore, the improvement is more drastic near the final layer. As we manually add weights at 1/4, 1/2, and 3/4 of the total costs, we observe similar improvements of neighborhoods. A plausible explanation is that because the neighboring predictions are highly correlated, a good solution for one loss is also competitive for its neighboring losses. Hence it could be both easier and better to focus on a few spaced out losses than to optimize all losses evenly. Based on these observations, we develop the heuristic \u201csieve\u201d weight scheme as described in Sec. 3. On CIFAR100 validation network, sieve scheme achieves the best average error among listed weight schemes and near-optimal final error. The advantage of sieve over half-end is demonstrated in the plot of training soft-max cross entropy loss versus depth in Fig. 2d, where the more concentrated weights in sieve reduce the loss of almost all layers, including those whose relative weights decreased. We use sieve in all remaining experiments."}, {"heading": "4.4 Evaluate Alternating Anytime Neural Networks (AANN)", "text": "As explained in Sec. 3, AANN samples layer i with the probability proportional to Bi in each iteration and increase the weight Bi temporarily by \u03c4 \u2211L i=1Bi. We choose \u03c4 = 0.5 from the set {0.25, 0.5, 1.0, 2.0} by experiments on the validation network. Since we evaluate anytime performance at milestone costs, 1/4, 1/2, 3/4 and 1 of the total cost, we also consider the sampling strategy, called milestone, where half of the samples are split evenly among the milestones, and the other half are split evenly among the others. Table 3 showcases some performances of AANN on networks of various complexity, and a more comprehensive table is in the appendix. In total we experimented these methods on 13 network models and on all three data-sets. Using these 39 experiments, we compare these sampling strategy against the vanilla ANN by computing the percentages of experiments in which each strategy is better than ANN in each of the five metrics. We list these percentages in Table 2a. We apply hypothesis testing to better understand these percentages. For each entry in Table 2a, let X be the probability listed in the entry and p \u2208 [0, 1] be the probability that ANN outperforms the associated method on the associated metric. Let the null hypothesis H0 be \u201cp \u2264 0.5\u201d and the alternative hypothesis H1 be \u201cp > 0.5\u201d. Using Hoeffding\u2019s inequality4, we have Pr(X|H0) \u2264 0.05 for X \u2265 0.0598, and Pr(X|H0) \u2264 0.001 for X \u2265 0.65. Thus, we accept that AANN improves over ANN at the first three milestone costs and the average top-1 error. On the other hand, though the \u201cmilestone\u201d strategy focuses directly on the four milestones, it generally degrades the overall performance, an observation similar to the previous observations on constant scheme. It would be interesting for future work to further understand why end-to-end training is not ideal for learning anytime predictions in neural networks. The average of the relative increases in error rate from OPT in the 39 experiments are shown in Table 2b, where we observe the relative performance gaps between OPT and ANNs drop exponentially from the early 50% extra mistakes to almost none (2%) at the final layer. AANN also has a smaller relative performance gap everywhere than ANN has, an evidence of the uniform improvement of AANN over ANN."}, {"heading": "4.5 Evaluation on Multiple Models", "text": "In this section, we experiment with ANNs of various model complexity. An ANN is specified by the number of units in each of its three residual blocks, n, and the initial channel size, c. An ANN can also decide the prediction period s, i.e., the number of residual bottleneck units between consecutive predictions. We defer a detailed study on the prediction frequency to the appendix, and set s = 1 unless specified otherwise. Table 3 lists performances of some of the 13 models we experiment on. A full table is in the appendix. In all experiments, ANN and AANN are competitive against OPT in the intermediate predictions, and improve these predictions to near-optimal at the final layer. As expected, as the model becomes deeper and wider, the performance generally improves. However,\n4Using Hoeffding\u2019s inequality on the 39 experiments, we know Pr(X|H0) \u2264 Pr(X \u2265 p( + 1)) \u2264 exp(\u221278 2). Under H0, for each probability X Table 2a, = Xp \u2212 1 \u2265 X 2 \u2212 1.\nin the more complex models such as (n = 9, c = 128) we found the optimal (OPT) has difficulty to converge well, so that the final performance may not even be better than the optimal at 3/4 of the total cost. Anytime neural networks have no trouble converging in these cases and can sometimes outperform OPT, possibly due to the regularizing effect suggested by Lee et al. (2015). We also observe that the relative performance difference between ANN and OPT shrinks when we hold n fixed and increase the initial channel size from 16 to up to 128. We somewhat expected this result, because a network that is more over-parameterized has more parameters to spare for improving anytime predictions. This result is also encouraging for ANN, because it means when the base Resnet is wide and complex so that anytime predictions are more appreciated, ANN is closer to the optimal. In practice, wide networks are advocated by Zagoruyko & Komodakis (2016) and are used in some state-of-the-art semantic segmentation networks (Zhao et al., 2017)."}, {"heading": "4.6 Demonstration of EANN", "text": "In this section, we experimentally analyze sequences of exponentially deepening ANNs (EANN) as described in Sec. 3.3. We train individually AANN models that have c = 32 and n = 1, 3, 7, 13, 25 in order to form an EANN with exponential base b \u2248 2. By proposition 3.1, E[C] \u2248 2.44, and the EANN should compete against the optimal with n = (1 + 3 + 7 + 13 + 25)/2.44 \u2248 20. In our comparison, we instead compete against the optimal from n = 25, a more complex and more competitive model. To set an even more challenging target performance, called OPT+, we also collect all previously computed optimal performances with c = 32, and the error of OPT+ at depth i is set to be the minimum error among the available OPT of depth no greater than i. Fig. 2e and last rows of Table 3 illustrate the performance of EANN against OPT+. Unlike individual AANNs, EANN, with its constant fraction of additional computational cost, has almost no performance gap from the OPT+ at every milestone. The performance of EANN can potentially be further improved, if we train the contained ANNs so that they only predict after 1b of their layers are computed."}, {"heading": "5 Conclusion", "text": "In this work, we propose weighting techniques to achieve competitive anytime predictions in deep neural networks without degrading the final performance. We further improve the anytime performance to be near-optimal at any test-time budget using an ensemble technique with a constant fraction of additional computational cost."}, {"heading": "A Sketch of Proof of Proposition 3.1", "text": "Proof. For each budget consumed x, we compute the cost x\u2032 of the optimal that EANN is competitive against. The goal is then to analyze the ratio C = xx\u2032 . The first ANN in EANN has depth 1. The optimal and the result of EANN are the same. Now assume EANN is on depth z of ANN number n+ 1 for n \u2265 0, which has depth bn. (Case 1) For z \u2264 bn\u22121, EANN reuse the result from the end of ANN number n. The cost spent is x = z + \u2211n\u22121 i=0 b i = z + b n\u22121 b\u22121 . The optimal we compete has cost of the last ANN, which is b n\u22121 The ratio satisfies:\nC = x/x\u2032 = z\nbn\u22121 + 1 +\n1 b\u2212 1 \u2212 1 bn\u22121(b\u2212 1) \u2264 2 + 1 b\u2212 1 + 1 bn\u22121(b\u2212 1) n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 2 + 1 b\u2212 1 .\nFurthermore, since C increases with z, Ez\u223cUniform(0,bn\u22121)[C] \u2264 b1\u2212n \u222b bn\u22121\n0\nzb1\u2212n + 1 + 1\nb\u2212 1 dz = 1.5 +\n1\nb\u2212 1 .\n(Case 2) For bn\u22121 < z \u2264 bn, EANN outputs anytime results from ANN number n + 1 at depth z. The cost is still x = z + b\nn\u22121 b\u22121 . The optimal competitor has cost x \u2032 = z. Hence the ratio is\nC = x/x\u2032 = 1 + bn \u2212 1 z(b\u2212 1) \u2264 2 + 1 b\u2212 1 + 1 bn\u22121(b\u2212 1) n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 2 + 1 b\u2212 1 .\nFurthermore, since C decreases with z, Ez\u223cUniform(bn\u22121,bn)[C] \u2264 (b\u2212 1)\u22121b1\u2212n [ (2 + 1\nb\u2212 1 ) + \u222b bn bn\u22121 2 + 1 b\u2212 1 + bn \u2212 1 z(b\u2212 1) dz ] n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 1 + b ln b\n(b\u2212 1)2\nFinally, since case 1 and case 2 happen with probability 1b and (1\u2212 1 b ), we have\nsup B C = 2 +\n1\nb\u2212 1 and EB\u223cUniform(0,L)[C] \u2264 1\u2212\n1\n2b +\n1\nb\u2212 1 +\nln b\nb\u2212 1 .\nWe also note that with large b, supB C \u2192 2 and E[C]\u2192 1 from above.\nIf we form a sequence of regular networks that grow exponentially in depth instead of ANN, then the worst case happen right before a new prediction is produced. Hence the ratio between the consumed budget and the cost of the optimal that the current anytime prediction can compete, C, right before the number n+ 1 network is completed, is\u2211n\ni=1 b i\nbn\u22121 n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 b\n2\nb\u2212 1 = 2 + (b\u2212 1) + 1 b\u2212 1 .\nNote that (b\u2212 1) + 1b\u22121 \u2265 4 and the inequality is tight at b = 2. Hence we know supB C is at least 4. Furthermore, the expected value of C, assume B is uniformly sampled such that the interruption happens on the (n+ 1)th network, is:\nE[C] = 1\nbn \u222b bn 0 x+ b n\u22121 b\u22121 bn\u22121 dx n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 1.5 + b\u2212 1 2 + 1 b\u2212 1 \u2265 1.5 + \u221a 2 \u2248 2.91.\nThe inequality is tight at b = 1+ \u221a\n2. With large n, since almost all budgets are consumed by the last few networks, we know the overall expectation EB\u223cUniform(0,L)[C] approaches 1.5 + b\u221212 + 1 b\u22121 ,\nwhich is at least 1.5 + \u221a 2."}, {"heading": "B Frequency of Anytime Prediction", "text": "We note that not every residual bottleneck unit is required to compute an anytime output, and it is possible that allowing more layers to be computed without interruption can lead to better predictions. This section studies the relationship between frequency and performance of anytime predictions. We expect that a model with more frequent anytime predictions to perform worse, because optimizing the extra auxiliary losses limits the freedom of the network. Additionally, with more anytime outputs, the average weight of anytime losses decreases, assuming the total loss weight is a constant. Then according to our previous observations in Sec. 4.3, the less relative weight a layer has, the less optimized the layer will be. Following (Zamir et al., 2017), we call an ANN that makes an anytime prediction every i units of feature transformations (residual bottlenecks) as stack-i (i.e., the prediction period is i). The layers that predict bottlenecks whose index can be written as L \u2212 ci, where c is a non-negative integer, so that the last layer always predict. Fig. 3 and Table 4 demonstrate the performances of ANNs on CIFAR100 with different stack-i on networks with n = 7, 9, 13. In all cases all anytime predictions are able to achieve near-optimal final predictions due to the final large weights from the sieve scheme. From the slightly larger network n = 9 and n = 13, we observe the general trend of improvement of anytime performances in all metrics as the period s increases. However, such increase is not without a limit. For instance, when n = 7, if have a relatively large period, s = 5, then it is possible that a milestone cost ( 12 in this case) is right before a new prediction, so that the anytime performance at such a milestone suffers. In applications, we should choose large period s as long as it does not cause much waste of budgets right before milestones.\nC In Relation to the Evaluation of Feedback Networks\nFeedback networks uses recurrent neural network structures to generate early predictions that can be used for anytime predictions. Recent work by Zamir et al. (2017) shows it is possible to use convLSTM (Shi et al., 2015) to train feedback networks for recognition tasks, and show that such network can produce competitive anytime predictions against the optimal. However, each cell or\ndepth of convLSTM, requires four gates, and each gate contains two convolutions layers. Given that Feedback Networks uses the same channel size as the original Resnet, each depth of Feedback Network costs as much as eight convolutional layers, or four residual units. Furthermore, because each convLSTM cell outputs one single anytime prediction, and Zamir et al. (2017) advise to output every two cells, feedback network effectively produces an anytime prediction every 16 convolutional layers, or equivalently every eight residual units. In this work, the cost of network is determined by the computational costs, e.g., FLOPS, instead of the depth of the computational dependencies, which is used by Zamir et al. (2017). If our anytime networks are allowed to have four times the cost of the optimal that we compete against, then we suggest applying EANN, which has been shown in Sec. 4.6 to be able to achieve competitive performance with an expectation cost inflation E[C], which is less than four if the exponential base b is no smaller than 1.4."}, {"heading": "D No-regret Online learners for Choosing Layers to Amplify", "text": "D.1 Learn to Choose Layers via Min-max Optimization\nThis section consider to replace the heuristic and static strategy in AANN with strategies that are learned during training. Inspired by Shalev-Shwartz & Wexler (2016), we choose Bi to amplify in each iteration by applying no-regret online learner on the maximization of the following min-max optimization:\nmin \u03b8 max v\u2208\u2206L L\u2211 i=1 viLi(\u03b8), (3)\nwhere \u2206L is the L-dimensional probability simplex, and Li(\u03b8) is a heuristic loss objective of choosing layer i at parameter \u03b8. Note that L may not be the same as the loss of layer i, `i. Intuitively, this min-max optimization can be considered as a two player zero-sum game, where the max player chooses layer i using v to generate the maximal loss, and the min player updates \u03b8 to reduce the chosen loss. As suggested by Shalev-Shwartz & Wexler (2016), applying no-regret online learner on the maximization w.r.t. v leads to a no-regret strategy of choosing which layers against any static strategies. There are multiple options for no-regret online learners. For instance, EXP3(Auer et al., 2002) and Random Weighted Majority (RWM)(Littlestone & Warmuth, 1994) both exponential gradient ascend on variable v to find layers that consistently results in high losses. Ideally, these no-regret online learners will discover high loss layer for optimization to focus on, so that eventually all loss Li will be equal. We also consider another heuristic sampling strategy called AVGL: we sample layer i with probability proportional to the exponential average of Li.\nD.2 Maximization Objective\nNow we explain our choice of heuristic loss objective Li so that minimization of Eq. 3 leads to low loss `i. Ideally, we would like to set Li = max( `i\u2212` \u2217 i\n`\u2217i , 0) to measure the relative difference\nof `i from its optimal `\u2217i . In fact Lee et al. (2015) apply intermediate loss of this very form, and set the `\u2217i to be a hyper parameter to tune. However, in general tuning this parameter is difficult, and we cannot compute `\u2217i efficiently without training a model specifically for each i. To avoid the dependency on `\u2217i , we consider the relative loss difference between neighboring layers, and use the following min-max optimization to select losses `i to optimize,\nmin \u03b8 max v\u2208\u2206L L\u22121\u2211 i=1 vi `i(\u03b8)\u2212 `i+1(\u03b8) `i(\u03b8) + \u03b7vL max i=1,...,L\u22121 `i(\u03b8)\u2212 `i+1(\u03b8) `i(\u03b8) , (4)\nwhere \u03b7 is a constant chosen by cross-validation. Intuitively, for i < L, Li(\u03b8) = `i(\u03b8)\u2212`i+1(\u03b8)`i(\u03b8) represents the relative reduction in loss from layer i to i + 1, and if this value is high, layer i is performing much worse than its successor while their structures are relative the same. This suggests layer i could be improved significantly with more optimization focus. The final loss objective is set to be LL = \u03b7maxi=1,...,L\u22121 `i(\u03b8)\u2212`i+1(\u03b8)`i(\u03b8) , a constant fraction of the maximal Li of the previous L\u2212 1 layers, because the final layer does not have a successor and we desire a relative high probability of choosing the final layer according to experiments in Sec. 4.3.\nD.3 No-regret Online Learning Algorithms\nAs suggested by Shalev-Shwartz & Wexler (2016), we let the max player use EXP3, a no-regret online learning algorithm, to update v in the min-max optimization in Eq. 3 as follows. In each iteration, we first sample a loss `i using EXP3 according to the probability distribution pi = (1 \u2212 \u03b3)vi + \u03b3 L , where \u03b3 is a hyper parameter of EXP3 that represents the probability that the algorithm\nshould explore a loss at random. Then the chosen `i is added to a fixed sum objective \u2211L j=1Bj`j\nto form the total loss `total = \u2211L j=1Bj`j + \u03c4 \u2211L j=1Bj`i, where \u03c4 is a hyper parameter. The\nextra weight of `i is \u03c4 \u2211L j=1Bj , a fraction of the total weight of the sum objective, so that the chosen `i has significant influence on the total loss. After computing `total, we next update network parameter \u03b8 gradient of `total. Finally, we apply EXP3 update rules to update vi to be viexp(\n\u03b3Li(\u03b8) Lpi )\nand normalize vector v onto the probability simplex \u2206L. If we use Random Weighted Majority (RWM) (Littlestone & Warmuth, 1994) instead of EXP3, the procedure is almost identical: the only difference is that in each iteration we update all vj , j = 1, ..., L, by exponential gradient descent using the signal Lj .\nD.4 Hyper Parameter Set-up\nThere are three hyper parameters for applying EXP3 or RWM to alternating optimization: a probability \u03b3 for the online learner to explore uniformly at random, a ratio \u03b7 between the reward for the final layer and the maximum reward among previous layers, and the additional weight of the chosen layer as \u03c4 \u2211L i=1Bi. We run grid search on CIFAR10 and CIFAR100 validation network with \u03b3 from {0.1, 0.2, 0.3, 0.4}, \u03b7 from {0.7, 0.8, 0.9, 1.0}, and \u03c4 from {2, 1, 0.5, 0.25}, and then sort the settings by average error and final error. We choose the setting that first appears on both ranking: \u03b3 = 0.3, \u03b7 = 0.8, and \u03c4 = 0.5. A true optimal parameter setting may be hard to determine due\nto the random nature of SGD and the randomized strategy itself, but luckily we found on validation set that the performance is not overly sensitive to parameters. We set the momentum constant in the exponential gradient average of the AVGL strategy to be 0.9\nD.5 Experimental Results of Various Sampling Strategies\nIn Fig. 4, we plot the probability that each bottleneck unit i is chosen by various strategies on model with n = 7, and c = 32. The overall behavior of the probabilities are similar across models, even the performance ranking are not the same. We plot the stack graph versus the training epochs, so that we can view how the strategy evolve over time. We found that both no-regret algorithms, EXP3 and RWM, learned to pick the final layer with more than half of the probability. The remaining probability are roughly split evenly among layers. AVGL learns to put more weight on the final layer than the other layers as well, but since AVGL update its probabilities with gradient descent instead of exponentiated gradient descent in EXP3 and RWM, AVGL does not select the final layer with too high a probability. Moreover, AVGL learns to focus layers around layer 7 and 14, which are around the transition layers that subsample the feature maps and double the channel sizes via 1x1 convolutions. AANN based on sieve weight scheme naturally focus half of the weights on the final layer, and it does not spread the remaining weights evenly. Instead, we see intermittent large weights and small weights, so that every layer is somewhat focused on due to its correlation with its neighbors, and no neighborhood other than the final layer is overly focused on.\nTable 5 lists for each sampling strategy and each anytime evaluation metric the percentage of the 39 experiments in Table 6 such that the corresponding strategy is better than the vanilla ANN in the corresponding metric. Missing experiments are ignored. We see that RWM degrades performance in every metric. EXP3 is only able to improve at the 3/4 milestone, and degrades performance at all other milestones. AVGL is able to somewhat improve performance at 1/2 and 3/4 milestones, and improves the average performance measured in AE. However, this is at the cost of the final prediction performance, even though we note the difference is usually small. In contrast, AANN is able to improve performances in every metric, except in the final prediction, which cannot be improved because ANN is already at the optimal level in this metric. Overall, all of dynamic strategies, EXP3, RWM and AVGL are not as effective as the proposed simple AANN strategy that samples layers according to the normalized static weights, Bi. Since each of these dynamic strategies has extra parameters to tune, and the chosen heuristic loss objective L is probably sub-optimal, our experiments cannot fully prove that they are not helpful for improving performance of ANN. However, we can conclude that even if they can be helpful, they require extensive parameter tuning."}, {"heading": "E Full Table for Evaluating Sampling Strategies", "text": "Table 6 lists the 13 different models that we experiment with sampling strategies on. The table also contains the performance of the sampling strategy milestone, which we show to be inferior to the vanilla ANN. These models are specified by pairs (n, c), which are their number of bottleneck units n per residual block and their initial channel size c. The 13 models we tried are: (7, 16), (7, 32), (9, 16), (9, 32), (9, 64), (9, 128), (13, 16), (13, 32), (13, 64), (17, 16), (17, 32), (25, 16), (25, 32). Every model produces anytime predictions on every bottleneck unit, except n = 25, where we set the model to produce an anytime output every three units, because having 75 anytime predictions seems excessive and slow down the training due to the extra GPU memory and computation for storing gradients for the additional auxiliary losses. We examine more in detail about the frequency of the anytime prediction in the next appendix section Sec. B, where we show that we can trade off between frequency and anytime prediction performance."}], "references": [{"title": "The nonstochastic multiarmed bandit problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicolo", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "In SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Learning Complexity-Aware Cascades for Deep Pedestrian Detection", "author": ["Cai", "Zhaowei", "Saberian", "Mohammad J", "Vasconcelos", "Nuno"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "Cai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2015}, {"title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alemi", "Alex"], "venue": "In Association for the Advancement of Artificial Intelligence (AAAI),", "citeRegEx": "Szegedy et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2017}, {"title": "The cascade-correlation learning architecture", "author": ["Fahlman", "Scott E", "Lebiere", "Christian"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Fahlman et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Fahlman et al\\.", "year": 1990}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "author": ["Golovin", "Daniel", "Krause", "Andreas"], "venue": "In Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Golovin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2011}, {"title": "Anytime Algorithm Development Tools", "author": ["Grass", "Joshua", "Zilberstein", "Shlomo"], "venue": "SIGART Bulletin,", "citeRegEx": "Grass et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Grass et al\\.", "year": 1996}, {"title": "SpeedBoost: Anytime Prediction with Uniform NearOptimality", "author": ["Grubb", "Alexander", "Bagnell", "J. Andrew"], "venue": "In the 15th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Grubb et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Grubb et al\\.", "year": 2012}, {"title": "Deep pyramidal residual networks", "author": ["Han", "Dongyoon", "Kim", "Jiwhan", "Junmo"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Han et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Han et al\\.", "year": 2017}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Efficient feature group sequencing for anytime linear prediction", "author": ["Hu", "Hanzhang", "Grubb", "Alexander", "Hebert", "Martial", "Bagnell", "J. Andrew"], "venue": "In UAI,", "citeRegEx": "Hu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2016}, {"title": "Densely connected convolutional networks", "author": ["Huang", "Gao", "Liu", "Zhuang", "Weinberger", "Kilian Q", "van der Maaten", "Laurens"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Huang et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2017}, {"title": "Timely Object Recognition", "author": ["Karayev", "Sergey", "Baumgartner", "Tobias", "Fritz", "Mario", "Darrell", "Trevor"], "venue": "In Conference and Workshop on Neural Information Processing Systems (NIPS),", "citeRegEx": "Karayev et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Karayev et al\\.", "year": 2012}, {"title": "Learning multiple layers of features from tiny images", "author": ["Krizhevsky", "Alex"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Alex.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Joint Cascade Optimization Using a Product of Boosted Classifiers", "author": ["Lefakis", "Leonidas", "Fleuret", "Francois"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Lefakis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lefakis et al\\.", "year": 2010}, {"title": "The weighted majority algorithm", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Littlestone and Warmuth,? \\Q1994\\E", "shortCiteRegEx": "Littlestone and Warmuth", "year": 1994}, {"title": "Cross-stitch networks for multi-task learning", "author": ["Misra", "Ishan", "Shrivastava", "Abhinav", "Gupta", "Hebert", "Martial"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Misra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Misra et al\\.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Netzer", "Yuval", "Wang", "Tao", "Coates", "Adam", "Bissacco", "Alessandro", "Wu", "Bo", "Ng", "Andrew Y"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["Ren", "Shaoqing", "He", "Kaiming", "Girshick", "Ross B", "Sun", "Jian"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Minimizing the maximal loss: How and why", "author": ["Shalev-Shwartz", "Shai", "Wexler", "Yonatan"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2016}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["Shi", "Xingjian", "Chen", "Zhourong", "Wang", "Hao", "Yeung", "Dit-Yan", "kin Wong", "Wai", "chun Woo"], "venue": "In International Conference on Neural Information Processing Systems Pages,", "citeRegEx": "Shi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Karen", "Zisserman", "Andrew"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Rapid Object Detection using a Boosted Cascade of Simple Features", "author": ["Viola", "Paul A", "Jones", "Michael J"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Viola et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Viola et al\\.", "year": 2001}, {"title": "Classifier cascades and trees for minimizing feature evaluation cost", "author": ["Z. Xu", "M.J. Kusner", "K.Q. Weinberger", "M. Chen", "O. Chapelle"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2014}, {"title": "Wide residual networks", "author": ["Zagoruyko", "Sergey", "Komodakis", "Nikos"], "venue": "In British Machine Vision Conference (BMVC),", "citeRegEx": "Zagoruyko et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko et al\\.", "year": 2016}, {"title": "Pyramid scene parsing network", "author": ["Zhao", "Hengshuang", "Shi", "Jianping", "Qi", "Xiaojuan", "Wang", "Xiaogang", "Jia", "Jiaya"], "venue": "In Computer Vision and Pattern Recognition", "citeRegEx": "Zhao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2017}, {"title": "2017) shows it is possible to use convLSTM (Shi et al., 2015) to train feedback networks for recognition tasks, and show that such network can produce competitive anytime predictions", "author": ["Zamir"], "venue": null, "citeRegEx": "Zamir,? \\Q2015\\E", "shortCiteRegEx": "Zamir", "year": 2015}], "referenceMentions": [{"referenceID": 13, "context": "In particular, the recent quick advance in image recognition tasks are mostly brought by convolutional neural networks (CNN) that have become increasingly wide and deep, starting from AlexNet(Krizhevsky et al., 2012) and VGG(Simonyan & Zisserman, 2015), to Residual Network(He et al.", "startOffset": 191, "endOffset": 216}, {"referenceID": 8, "context": ", 2012) and VGG(Simonyan & Zisserman, 2015), to Residual Network(He et al., 2016), Inception(Christian Szegedy & Alemi, 2017), and DenseNet(Huang et al.", "startOffset": 64, "endOffset": 81}, {"referenceID": 10, "context": ", 2016), Inception(Christian Szegedy & Alemi, 2017), and DenseNet(Huang et al., 2017).", "startOffset": 65, "endOffset": 85}, {"referenceID": 8, "context": "To achieve frequent and competitive anytime predictions, this work proposes to form anytime neural networks (ANNs) by adding uniformly spaced auxiliary predictions and losses to the residual bottleneck units of Resnets (He et al., 2016).", "startOffset": 219, "endOffset": 236}, {"referenceID": 9, "context": "Anytime predictions (Grass & Zilberstein, 1996) have been studied from the perspectives of optimization with functional boosting (Grubb & Bagnell, 2012; Hu et al., 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al.", "startOffset": 129, "endOffset": 169}, {"referenceID": 23, "context": ", 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al., 2014; Cai et al., 2015).", "startOffset": 29, "endOffset": 110}, {"referenceID": 1, "context": ", 2016), and cascades design (Viola & Jones, 2001; Lefakis & Fleuret, 2010; Xu et al., 2014; Cai et al., 2015).", "startOffset": 29, "endOffset": 110}, {"referenceID": 25, "context": "Some deep and complex architectures (Christian Szegedy & Alemi, 2017; Zhao et al., 2017) add one single auxiliary loss at a certain layer to combat the problem of vanishing gradients in training deep neural networks.", "startOffset": 36, "endOffset": 88}, {"referenceID": 18, "context": "Our proposed alternating optimization procedure for ANN is inspired by both practitioners that task a single network for multiple purposes (Ren et al., 2015; Misra et al., 2016) and theorists that use online learning to automatically choose data samples of interest to optimize model parameters with (Shalev-Shwartz & Wexler, 2016).", "startOffset": 139, "endOffset": 177}, {"referenceID": 16, "context": "Our proposed alternating optimization procedure for ANN is inspired by both practitioners that task a single network for multiple purposes (Ren et al., 2015; Misra et al., 2016) and theorists that use online learning to automatically choose data samples of interest to optimize model parameters with (Shalev-Shwartz & Wexler, 2016).", "startOffset": 139, "endOffset": 177}, {"referenceID": 23, "context": "Some deep and complex architectures (Christian Szegedy & Alemi, 2017; Zhao et al., 2017) add one single auxiliary loss at a certain layer to combat the problem of vanishing gradients in training deep neural networks. It is unclear whether the training procedure that optimizes very few auxiliary losses can be directly applied to an ANN that has many auxiliary losses. Curriculum learning (Bengio et al., 2009; Zamir et al., 2017) also leverages auxiliary losses so that the early predictions of a model fit easy samples and crude classes, and late predictions fit hard samples and refined classes. In particular, Zamir et al. (2017) also care about the quality of the anytime predictions, and our novel training techniques can be applied to such curriculum learning networks that utilize auxiliary losses.", "startOffset": 70, "endOffset": 634}, {"referenceID": 8, "context": ", each fi could be a residual bottleneck unit in Resnet (He et al., 2016).", "startOffset": 56, "endOffset": 73}, {"referenceID": 11, "context": "For instance, it is intuitive to set eachBi to be 1 L in order to minimize of the average error rate of \u0177i, an anytime performance metric equivalent to the timeliness metric proposed by Karayev et al. (2012). However, this constant weight scheme does not achieve near-optimal final predictions, because it does not compensate for the fact that the final losses are typically much smaller than early ones.", "startOffset": 186, "endOffset": 208}, {"referenceID": 17, "context": "We experiment with ANN on CIFAR10, CIFAR100 (Krizhevsky, 2009), and SVHN (Netzer et al., 2011), where deep convolutional networks are the state-of-the-art predictors 3.", "startOffset": 73, "endOffset": 94}, {"referenceID": 7, "context": "Each block consists of n bottleneck units that are designed by Han et al. (2017) and each has two convolutions.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "Inspired by Karayev et al. (2012), we compute a metric that estimates the overall anytime performance with the average of top-1 errors (AE).", "startOffset": 12, "endOffset": 34}, {"referenceID": 11, "context": "Inspired by Karayev et al. (2012), we compute a metric that estimates the overall anytime performance with the average of top-1 errors (AE). Following Zamir et al. (2017), we also report the top-1 error at four milestone budget cut-offs: 1 4 , 1 2 , 3 4 and 1 of the total cost.", "startOffset": 12, "endOffset": 171}, {"referenceID": 8, "context": "We follow during training the standard augmentation of CIFAR from (Lee et al., 2015; He et al., 2016): we pad each side by four zeros and randomly crop a 32x32 image; the image is also randomly flipped left to right.", "startOffset": 66, "endOffset": 101}, {"referenceID": 11, "context": "If we ignore the cost at the block transitions so that anytime predictions are produced at a constant frequency, then minimizing AE is equivalent to maximizing timeliness (Karayev et al., 2012), a measure of anytime performance.", "startOffset": 171, "endOffset": 193}, {"referenceID": 11, "context": "If we ignore the cost at the block transitions so that anytime predictions are produced at a constant frequency, then minimizing AE is equivalent to maximizing timeliness (Karayev et al., 2012), a measure of anytime performance. In a simple data-set like CIFAR10, this scheme achieves the AE that is the closest to the optimal (Fig. 2a and Table 1). However, there is a clear gap between the optimal and the anytime prediction even at the final layers (Fig. 2b). This gap is magnified on the more difficult CIFAR100, where the constant scheme achieves the worst performance on almost all layers as shown in Fig. 2c and Table 1. The final gap exists partially because the final losses are typically much smaller than the early ones, so that the constant scheme may favor achieving near-optimal early losses. Another key disadvantage of the constant scheme is that only 1 L of the total weights are on each layer, and the fraction decreases with L. If we believe that for a layer to be competitive, its loss weight needs to be at least a fraction of the total weight, then we can apply this belief recursively either from final layer to the first layer or vice versa, and the loss weights should be exponentially increasing or decreasing with the depth. We test on CIFAR10 and CIFAR100 the schemes \u201cexpb2\u201d and \u201cexpb0.5\u201d, which have their loss weights at layer i, Bi, to be proportional to 2 and 0.5 respectively. We found that expb2 achieves near-optimal in the final layers and fail to be comparable in the early layers, and expb0.5 achieves the opposite. This observation agrees with the intuition that the quality of the prediction \u0177i is positively related to its loss weight Bi, and suggests that exponential increase/decrease in weights is too drastic for achieving competitive overall anytime performance. We also tested \u201clinear\u201d scheme, whereBi grows linearly with i, as suggested by Zamir et al. (2017). Similar to expb2 on CIFAR100 as shown in Fig.", "startOffset": 172, "endOffset": 1909}, {"referenceID": 25, "context": "In practice, wide networks are advocated by Zagoruyko & Komodakis (2016) and are used in some state-of-the-art semantic segmentation networks (Zhao et al., 2017).", "startOffset": 142, "endOffset": 161}], "year": 2017, "abstractText": "We address the problem of anytime prediction in neural networks. An anytime predictor automatically adjusts to and utilizes available test-time budget: it produces a crude initial result quickly and continuously refines the result afterwards. Traditional feed-forward networks achieve state-of-the-art performance on many machine learning tasks, but cannot produce anytime predictions during their typically expensive computation. In this work, we propose to add auxiliary predictions in a residual network to generate anytime predictions, and optimize these predictions simultaneously. We solve this multi-objective optimization by minimizing a carefully constructed weighted sum of losses. We also oscillate weightings of the losses in each iteration to avoid spurious solutions that are optimal for the sum but not for each individual loss. The proposed approach produces competitive results if computation is interrupted early, and the same level of performance as the original network once computation is finished. Observing that the relative performance gap between the optimal and our proposed anytime network shrinks as the network is near completion, we propose a method to combine anytime networks to achieve more accurate anytime predictions with a constant fraction of additional cost. We evaluate the proposed methods on real-world visual recognition data-sets to demonstrate their anytime performance.", "creator": "LaTeX with hyperref package"}}}