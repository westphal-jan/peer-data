{"id": "1708.07050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "Capturing Long-term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition", "abstract": "the goal of continuous emotion recognition isn to tie an emotion value to every frame in ample suite of acoustic features. we estimate but incorporating long - continuously timed observations is required with continuous emotion recognition tasks. finding this end, we first investigate regions specifically reflect unpredictable triggers. more show that \u2014 though this sequences use powerful intuitive predictions, the feeling representations returning from such tasks undergo erratic changes between consecutive selection steps. time means inconsistent with the systematically moving ground - nesting emotion labels there are obtained used human annotators. to deal with this problem, manual model generated downsampled version showing the input recording and then generate the output back through upsampling. which only does spontaneous resulting downsampling / upsampling paradigm achieve responsive performance, it also reduces steady speech trajectories. automated method yields the current stable audio - only performance in visually analyzed dataset.", "histories": [["v1", "Wed, 23 Aug 2017 15:27:00 GMT  (1701kb,D)", "http://arxiv.org/abs/1708.07050v1", "5 pages, 5 figures, 2 tables, Interspeech 2017"]], "COMMENTS": "5 pages, 5 figures, 2 tables, Interspeech 2017", "reviews": [], "SUBJECTS": "cs.SD cs.AI", "authors": ["soheil khorram", "zakaria aldeneh", "dimitrios dimitriadis", "melvin mcinnis", "emily mower provost"], "accepted": false, "id": "1708.07050"}, "pdf": {"name": "1708.07050.pdf", "metadata": {"source": "CRF", "title": "Capturing Long-term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition", "authors": ["Soheil Khorram", "Zakaria Aldeneh", "Dimitrios Dimitriadis", "Melvin McInnis", "Emily Mower Provost"], "emails": ["emilykmp}@umich.edu,", "dbdimitr@us.ibm.com"], "sections": [{"heading": "1. Introduction", "text": "Emotion recognition has many potential applications including building more natural human-computer interfaces. Emotion can be quantified using categorical classes (e.g., neutral, happy, sad, etc.) or using dimensional values (e.g., valence-arousal). In addition, emotional labels can be quantified statically, over units of speech (e.g., utterances), or continuously in time.\nIn this work, we focus on problems where the goal is to recognize emotions in the valence-arousal space, continuously in time. The valence-arousal space is a psychologically grounded method for describing emotions [1]. Valence ranges from negative to positive, while activation ranges from calm to excited. Research has demonstrated that it is critical to incorporate longterm temporal information for making accurate emotion predictions. For instance, Valstar et al. [2] showed that it was necessary to consider larger windows when making frame-level emotion predictions (four seconds for arousal and six seconds for valence). Le et al. [3] and Cardinal et al. [4] found that increasing the number of contextual frames when training a deep neural network (DNN) for making frame-level emotion predictions is helpful but only to a certain point. Bidirectional long short-term memory networks (BLSTMs) can naturally incorporate longterm temporal dependencies between features; explaining their success in continuous emotion recognition tasks (e.g., [5]).\nIn this work, we investigate two convolutional network architectures, dilated convolutional networks and downsampling/upsampling networks, that capture long-term temporal dependencies. We interpret the two architectures in the context of\n\u2217These authors contributed equally to this work\ncontinuous emotion recognition and show that these architectures can be used to build accurate continuous emotion recognition systems."}, {"heading": "2. Related Work", "text": "Even though the problem of emotion recognition has been extensively studied in the literature, we only focus on works that predicted dimensional values, continuously in time. Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6]. With the exception of BLSTMs, such approaches do not incorporate longterm dependencies unless coupled with feature engineering. In this work, we show that purely convolutional neural networks can be used to incorporate long-term dependencies and achieve good emotion recognition performance, and are more efficient to train than their recurrent counterparts.\nIn their winning submission to the AVEC 2016 challenge, Brady et al. [6] extracted a set of audio features (Mel-frequency cepstral coefficients, shifted delta cepstral, prosody) and then learned higher-level representations of the features using sparse coding. The higher-level audio features were used to train linear SVRs. Povolny et al. [7] used eGeMAPS [8] features along with a set of higher-level bottleneck features extracted from a DNN trained for automatic speech recognition (ASR) to train linear regressors. The higher level features were produced from an initial set of 24 Mel filterbank (MFB) features and four different estimates of the fundamental frequency (F0). Povolny et al. used all features to train linear regressors to predict a value for each frame, and considered two methods for incorporating contextual information: simple frame stacking and temporal content summarization by applying statistics to local windows. In contrast, in this work we show that considering temporal dependencies that are longer than those presented in [6, 7] is critical to improve continuous emotion recognition performance.\nHe et al. [5] extracted a comprehensive set of 4, 684 features, which included energy, spectral, and voicing-related features, and used them to train BLSTMs. The authors introduced delay to the input to compensate for human evaluation lag and then applied feature selection. The authors ran the predicted time series through a Gaussian smoothing filter to produce the final output. In this work, we show that it is sufficient to use 40 MFBs to achieve state-of-the-art performance, without the need for special handling of human evaluation lag.\nTrigeorgis et al. [9] trained a convolutional recurrent network for continuous emotion recognition using the time domain signal directly. The authors split the utterances into five-second segments for batch training. Given an output from a the trained model, the authors applied a chain of post-processing steps (median filtering, centering, scaling, time shifting) to get the final\nar X\niv :1\n70 8.\n07 05\n0v 1\n[ cs\n.S D\n] 2\n3 A\nug 2\n01 7\noutput. In contrast, we show that convolutional networks make it possible to efficiently process full utterances without the need for segmenting. Further, since our models work on full-length utterances, we show that it is not necessary to apply any postprocessing steps as described in [9].\nOn the ASR end, Sercu et al. [10] proposed viewing ASR problems as dense prediction tasks, where the goal is to assign a label to every frame in a given sequence, and showed that this view provides a set of tools (e.g., dilated convolutions, batch normalization, efficient processing) that can improve ASR performance. The authors argued that ASR approaches required practitioners to splice their input sequences into independent windows, making the training and evaluation procedures cumbersome and computationally inefficient. In contrast, the authors\u2019 proposed approach allows practitioners to efficiently process full sequences without requiring splicing or processing frames independently. The authors showed that their approach obtained the best published single model results on the switchboard-2000 benchmark dataset.\nIn this work, we treat the problem of continuous emotion recognition as a dense prediction task and show that, given this view of the problem, we can utilize convolutional architectures that can efficiently incorporate long-term temporal dependencies and provide accurate emotion predictions."}, {"heading": "3. Problem Setup", "text": "We focus on the RECOLA database [11] following the AVEC 2016 guidelines [2]. The RECOLA database consists of spontaneous interactions in French and provides continuous, dimensional (valence and arousal) ground-truth descriptions of emotions. Even though the AVEC 2016 challenge is multi-modal in nature, we only focus on the speech modality in this work. The RECOLA database contains a total of 27 five-minute utterances, each from a distinct speaker (9 train; 9 validation; 9 test). Ground-truth continuous annotations were computed, using audio-visual cues, on a temporal granularity of 40ms from six annotators (three females).\nFeatures. We use the Kaldi toolkit [12] to extract 40- dimensional log MFB features, using a window length of 25ms with a hop size of 10ms. Previous work showed that MFB features are better than conventional MFCCs for predicting emotions [13]. We perform speaker-specific z-normalization on all extracted features. RECOLA provides continuous labels at a granularity of 40ms. Thus, we stack four subsequent MFB frames to ensure correspondence between hop sizes in the input and output sequences.\nProblem Setup. Given a sequence of stacked acoustic features X = [x1,x2, . . . ,xT ], where xt \u2208 Rd, the goal is to produce a sequence of continuous emotion labels y = [y1, y2, . . . , yT ], where yt \u2208 R.\nEvaluation Metrics. Given a sequence of ground-truth labels y = [y1, y2, . . . , yT ] and a sequence of predicted labels y\u0302 = [y\u03021, y\u03022, . . . , y\u0302T ], we evaluate the performance using the root mean squared error (RMSE) and the Concordance Correlation Coefficient (CCC) to be consistent with previous work. The CCC is computed as follows: CCC = 2\u03c32yy\u0302/(\u03c3 2 y + \u03c3 2 y\u0302 + (\u00b5y \u2212 \u00b5y\u0302)2), where \u00b5y = E(y), \u00b5y\u0302 = E(y\u0302), \u03c32y = var(y), \u03c32y\u0302 = var(y\u0302), and \u03c3 2 yy\u0302 = cov(y, y\u0302)."}, {"heading": "4. Preliminary Experiment", "text": "We first study the effect of incorporating temporal dependencies of different lengths. The network that we use in the preliminary\nexperiments consists of a convolutional layer with one filter of variable length from 2 to 2048 frames, followed by a tanh nonlinearity, followed by a linear regression layer. We vary the length of the filter and validate the performance using CCC. We train our model on the training partition and evaluate on the development partition. We report the results of our preliminary experiment in Figure 1. The results show that incorporating long-term temporal dependencies improves the performance on the validation set up to a point.\nThe observed diminishing gains in performance past 512 (20.48 seconds) frames may occur either due to the increased number of parameters or because contextual information becomes irrelevant after 512 frames. Covering contexts as large as 512 frames still provided improvements in performance compared to results obtained from covering smaller contexts. The utility of contexts spanning 512 frames (20.48 seconds) is contrary to previous work that considered much smaller time scales. For instance, Valstar et al. [2] only covered six seconds worth of features and Povolny et al. [7] considered a maximum of eight seconds worth of features. Results from the preliminary experiment suggest that continuous emotion prediction systems could benefit from incorporating long-term temporal dependencies. This acts as a motivation for using architectures that are specifically designed for considering long-term dependencies."}, {"heading": "5. Methods", "text": "In this section, we describe the two architectures that we propose to use to capture long-term temporal dependencies in continuous emotion prediction tasks."}, {"heading": "5.1. Dilated Convolutions", "text": "Dilated convolutions provide an efficient way to increase the receptive field without causing the number of learnable parameters to vastly increase. Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].\nvan den Oord et al. [15] recently showed that it is possible to use convolutions with various dilation factors to allow the receptive field of a generative model to grow exponentially in order to cover thousands of time steps and synthesize highquality speech. Sercu et al. [10] showed that ASR could benefit from dilated convolutions since they allow larger regions to be covered without disrupting the length of the input signals. Continuous emotion recognition could benefit from such properties.\nWhen compared to filters of regular convolutions, those of dilated convolutions touch the input signal every k time steps,\nC on\nv. +\nM ax\np oo l\nC on\nv. +\nM ax\np oo l\nD ec\non v\nD ec\non v\nDownsampling UpsamplingDilated Convolution\n(b)(a)\nInput PredictionsStack of dilated convolutions\nFigure 2: A visualization of our dilated convolution network. We use convolutions with a different dilation factor for different layers. We use a 1\u00d7 1 convolution for the last layer to produce the final output. C on\nv. +\nM ax\np oo\nl\nC on\nv. +\nM ax\np oo\nl\nD ec\non v\nD ec\non v\nDownsampling UpsamplingDilated Convolution (b)(a) Input Predictions\nFigure 3: A visualization of our downsampling/upsampling network. Downsampling compresses the input signal into shorter signal which is then used to reconstruct a signal of the same length by the upsampling sub-network. We use the transpose convolution operation to perform upsampling.\nwhere k is the dilation factor. If [w1, w2, w3] is a filter with a dilation factor of zero, then [w1, 0, w2, 0, w3] is the filter with a dilation factor of one and [w1, 0, 0, w2, 0, 0, w3] is the filter with a dilation factor of two, and so on. We build a network that consists of stacked convolution layers, where the convolution functions in each layer use a dilation factor of 2n, where n is the layer number. This causes the dilation factors to grow exponentially with depth while the number of parameters grows linearly with depth. Figure 2 shows a diagram of our dilated convolution network."}, {"heading": "5.2. Downsampling/Upsampling", "text": "The emotion targets in the RECOLA database are sampled at a frequency of 25 Hz. Using Fourier analysis, we find that more than 95 percent of the power of these trajectories lies in frequency bands that are lower than 1 Hz. In other words, the output signals are smooth and they have considerable time dependencies. This finding is not surprising because we do not expect rapid reactions from human annotators. Networks that use dilated convolutions do not take this fact into account while making predictions, causing them to generate output signals whose variance is not consistent with the continuous ground truth contours (Section 6.2). To deal with this problem, we propose the use of a network architecture that compresses the input signal into a low-resolution signal through downsampling and then reconstructs the output signal through upsampling. Not only does the downsampling/upsampling architecture capture long-term temporal dependencies, it also generates a smooth output trajectory.\nWe conduct an experiment to investigate the effect of down-\nDownsampling Rate 0 50 100\nC C\nC\n0.85\n0.9\n0.95\n1 Arousal Valence\nFigure 4: Effect of downsampling/upsampling on CCC.\nsampling/upsampling on continuous emotion labels. First, we convert the ground truth signals to low-resolution signals using standard uniform downsampling. Given the downsampled signals, we then generate the original signals using spline interpolation. We vary the downsampling factor exponentially from 2 to 128 and compute the CCC between the original signals and the reconstructed ones. The results that we show in Figure 4 demonstrate that distortions caused by downsampling with factors up to 64 are minor (< 5% loss in CCC relative to original).\nThe network that we use contains two subnetworks: (1) a downsampling network; (2) an upsampling network. The downsampling network consists of a series of convolutions and maxpooling operations. The max-pooling layers reduce the resolution of the signal and increase the effective receptive field of the convolution layers. Initial experiments showed that maxpooling was more effective than other pooling techniques.\nThe upsampling function can be implemented in a number of ways [16]. In this work we use the transposed convolution1 [17, 18] operation to perform upsampling. Transposed convolutions provide a learnable map that can upsample a lowresolution signal to a high-resolution one. In contrast to standard convolution filters that connect multiple input samples to a single output sample, transposed convolution filters generate multiple outputs samples from just one input sample. Since it generates multiple outputs simultaneously, the transposed convolution can be thought of as a learnable interpolation function.\nDownsampling/upsampling architectures have been used in many computer vision tasks (e.g., [16, 19, 20]). For instance, Noh et al. [16] showed that transposed convolution operations can be effectively applied to image segmentation tasks. In addition to vision applications, downsampling/upsampling architectures have been successfully applied to speech enhancement problems [21], where the goal is to learn a mapping between noisy speech spectra and their clean counterparts. Park et al. [21] demonstrated that downsampling/upsampling convolutional networks can be 12\u00d7 smaller (in terms of the number of learnable parameters) than their recurrent counterparts and yet yield better performance on speech enhancement tasks.\nThe main goal of a transposed convolution is to take an nx-dimensional low-resolution vector x and generate an nydimensional high-resolution vector y using an nw-dimensional filter w (where ny > nx). Similar to other linear transforms, y can be expressed as: y = Tx, where T is the linear ny-by-nx transform matrix that is given by T = [T1,T2, ...,Tnx ]. Ti is the i-th column of T and can be written as:\nTi = [0, ..., 0\ufe38 \ufe37\ufe37 \ufe38 s(i\u22121) , wT\ufe38\ufe37\ufe37\ufe38 nw , 0, ..., 0\ufe38 \ufe37\ufe37 \ufe38 s(nx\u2212i) ]T\nwhere s is the upsampling factor. This linear interpolator is\n1Other names in literature include deconvolution, upconvolution, backward strided convolution and fractionally strided convolution.\nable to expand the input vector x to the output vector y with the length of ny = s(nx \u2212 1) + nw. Note that the matrix T is nothing but the transposed version of the standard strided convolution transform matrix. Our experiments confirm that the proposed downsampling/upsampling network generates smooth trajectories."}, {"heading": "6. Results and Discussion", "text": ""}, {"heading": "6.1. Experimental Setup", "text": "We build our models using Keras [22] with a Theano backend [23]. We train our models on the training partition of the dataset and use the development partition for early stopping and hyper-parameter selection (e.g., learning rate, number of layers layer size, filter width, l2 regularization, dilation factors, downsampling factors). We optimize CCC directly in all setups. We repeat each experiment five times to account for the effect of initialization. The final test evaluation is done by the AVEC 2016 organizers (i.e., we do not have access to test labels). Our test submissions were created by averaging the predictions produced from the five runs.\nWe report published results from the literature as baselines. Almost all previous works only report their final test results based on multi-modal features. We only show results that are reported on the audio modality in the results tables. We also compare our performance to that of an optimized BLSTM regression model, described in [24]. Our final dilated convolution structure has a depth of 10 layers, each having a width of 32. Our final downsampling/upsampling network contains four downsampling layers, one intermediate layer, and four transposed convolution layers, each having width of 32 for arousal and 128 for valence. We use a downsampling factor of three. We do not splice the input utterances into segments. Instead, we train on full length utterances and use a batch size of one."}, {"heading": "6.2. Results", "text": "Tables 1 and 2 show the development and test results for arousal and valence, respectively. Each row shows the results for one setup. We only include results from the literature that are based on the speech modality and use \u201c\u2013\u201d to show unreported results.\nBoth proposed systems show improvements over baseline results by Valstar et al. [2]. Our dilated convolution based system provides improvements of 5.6% and 19.5% over baseline systems for arousal and valence, respectively. Our downsampling/upsampling system provides improvements of 5.1% and 33.9% over baseline systems for arousal and valence, respectively. We report the results we obtain from our BLSTM system to provide a reference point. Our BLSTM system performs well when compared to the baseline results.\nThe proposed methods outperform BLSTMs and are more efficient to train on long utterances. For instance, given a con-\nvolutional network and a BLSTM network with approximately equal number of learnable parameters, one epoch of training on the AVEC dataset takes about 13 seconds on the convolutional network while one epoch of training takes about 10 minutes on the BLSTM network. This suggests that convolutional architectures can act as replacement for recurrent ones for continuous emotion recognition problems.\nWe show an example 40-second segment of the predictions made by our two networks along with the ground-truth predictions in Figure 5. The figure shows that the predictions produced by the downsampling/upsampling network are much smoother than those produced by the dilated convolution networks. We believe that the structure of the downsampling/upsampling network forces the output to be smooth by generating the output from a compressed signal. The compressed signal only stores essential information that is necessary for generating trajectories, removing any noise components."}, {"heading": "7. Conclusion", "text": "We investigated two architectures that provide different means for capturing long-term temporal dependencies in a given sequence of acoustic features. Dilated convolutions provides a method for incorporating long-term temporal information without disrupting the length of the input signal by using filters with varying dilation factors. Downsampling/upsampling networks incorporate long-term dependencies by applying a series of convolutions and max-poolings to downsample the signal and get a global view of the features. The downsampled signal is then used to reconstruct an output with a length that is equal to the uncompressed input. Our methods achieve the best known audio-only performance on the AVEC 2016 challenge."}, {"heading": "8. Acknowledgement", "text": "This work was partially supported by IBM under the Sapphire project. We would like to thank Dr. David Nahamoo and Dr. Lazaros Polymenakos, IBM Research, Yorktown Heights, for their support.\n\u2217Unpublished test results, courtesy of the authors."}, {"heading": "9. References", "text": "[1] M. Lewis, J. M. Haviland-Jones, and L. F. Barrett, Handbook of\nemotions. Guilford Press, 2010.\n[2] M. Valstar, J. Gratch, B. Schuller, F. Ringeval, D. Lalanne, M. Torres Torres, S. Scherer, G. Stratou, R. Cowie, and M. Pantic, \u201cAVEC 2016: Depression, mood, and emotion recognition workshop and challenge,\u201d in Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 3\u201310.\n[3] D. Le and E. M. Provost, \u201cEmotion recognition from spontaneous speech using hidden markov models with deep belief networks,\u201d in workshop on automatic speech recognition and understanding (ASRU). IEEE, pp. 216\u2013221.\n[4] P. Cardinal, N. Dehak, A. L. Koerich, J. Alam, and P. Boucher, \u201cETS system for AVEC 2015 challenge,\u201d in Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. ACM, 2015, pp. 17\u201323.\n[5] L. He, D. Jiang, L. Yang, E. Pei, P. Wu, and H. Sahli, \u201cMultimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks,\u201d in Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. ACM, 2015, pp. 73\u201380.\n[6] K. Brady, Y. Gwon, P. Khorrami, E. Godoy, W. Campbell, C. Dagli, and T. S. Huang, \u201cMulti-modal audio, video and physiological sensor learning for continuous emotion prediction,\u201d in Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 97\u2013104.\n[7] F. Povolny, P. Matejka, M. Hradis, A. Popkova\u0301, L. Otrusina, P. Smrz, I. Wood, C. Robin, and L. Lamel, \u201cMultimodal emotion recognition for AVEC 2016 challenge,\u201d in Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 75\u201382.\n[8] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andre\u0301, C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan et al., \u201cThe Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing,\u201d IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.\n[9] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou, \u201cAdieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200\u20135204.\n[10] T. Sercu and V. Goel, \u201cDense prediction on sequences with time-dilated convolutions for speech recognition,\u201d arXiv preprint arXiv:1611.09288, 2016.\n[11] F. Ringeval, A. Sonderegger, J. Sauer, and D. Lalanne, \u201cIntroducing the RECOLA multimodal corpus of remote collaborative and affective interactions,\u201d in International Conference and Workshops on Automatic Face and Gesture Recognition (FG). IEEE, 2013, pp. 1\u20138.\n[12] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe Kaldi speech recognition toolkit,\u201d in workshop on automatic speech recognition and understanding (ASRU). IEEE, 2011.\n[13] C. Busso, S. Lee, and S. S. Narayanan, \u201cUsing neutral speech models for emotional speech analysis.\u201d in Interspeech, 2007, pp. 2225\u20132228.\n[14] F. Yu and V. Koltun, \u201cMulti-scale context aggregation by dilated convolutions,\u201d arXiv preprint arXiv:1511.07122, 2015.\n[15] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d CoRR abs/1609.03499, 2016.\n[16] H. Noh, S. Hong, and B. Han, \u201cLearning deconvolution network for semantic segmentation,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015, pp. 1520\u20131528.\n[17] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, \u201cDeconvolutional networks,\u201d in Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 2528\u20132535.\n[18] V. Dumoulin and F. Visin, \u201cA guide to convolution arithmetic for deep learning,\u201d arXiv preprint arXiv:1603.07285, 2016.\n[19] V. Badrinarayanan, A. Kendall, and R. Cipolla, \u201cSegnet: A deep convolutional encoder-decoder architecture for image segmentation,\u201d arXiv preprint arXiv:1511.00561, 2015.\n[20] W. Chen, Z. Fu, D. Yang, and J. Deng, \u201cSingle-image depth perception in the wild,\u201d in Advances in Neural Information Processing Systems (NIPS), 2016, pp. 730\u2013738.\n[21] S. R. Park and J. Lee, \u201cA fully convolutional neural network for speech enhancement,\u201d arXiv preprint arXiv:1609.07132, 2016.\n[22] F. Chollet, \u201cKeras,\u201d https://github.com/fchollet/keras, 2015.\n[23] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio, \u201cTheano: a CPU and GPU math expression compiler,\u201d in Proceedings of the Python for Scientific Computing Conference (SciPy), 2010.\n[24] D. Le, Z. Aldeneh, and E. Mower Provost, \u201cDiscretized continuous speech emotion recognition with multi-task deep recurrent neural network,\u201d in Interspeech, 2017 (to apear), 2017."}], "references": [{"title": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge", "author": ["M. Valstar", "J. Gratch", "B. Schuller", "F. Ringeval", "D. Lalanne", "M. Torres Torres", "S. Scherer", "G. Stratou", "R. Cowie", "M. Pantic"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 3\u201310.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Emotion recognition from spontaneous speech using hidden markov models with deep belief networks", "author": ["D. Le", "E.M. Provost"], "venue": "workshop on automatic speech recognition and understanding (ASRU). IEEE, pp. 216\u2013221.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 0}, {"title": "ETS system for AVEC 2015 challenge", "author": ["P. Cardinal", "N. Dehak", "A.L. Koerich", "J. Alam", "P. Boucher"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. ACM, 2015, pp. 17\u201323.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. He", "D. Jiang", "L. Yang", "E. Pei", "P. Wu", "H. Sahli"], "venue": "Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge. ACM, 2015, pp. 73\u201380.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-modal audio, video and physiological sensor learning for continuous emotion prediction", "author": ["K. Brady", "Y. Gwon", "P. Khorrami", "E. Godoy", "W. Campbell", "C. Dagli", "T.S. Huang"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 97\u2013104.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal emotion recognition for AVEC 2016 challenge", "author": ["F. Povolny", "P. Matejka", "M. Hradis", "A. Popkov\u00e1", "L. Otrusina", "P. Smrz", "I. Wood", "C. Robin", "L. Lamel"], "venue": "Proceedings of the 6th International Workshop on Audio/Visual Emotion Challenge. ACM, 2016, pp. 75\u201382.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing", "author": ["F. Eyben", "K.R. Scherer", "B.W. Schuller", "J. Sundberg", "E. Andr\u00e9", "C. Busso", "L.Y. Devillers", "J. Epps", "P. Laukka", "S.S. Narayanan"], "venue": "IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "author": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5200\u20135204.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Dense prediction on sequences with time-dilated convolutions for speech recognition", "author": ["T. Sercu", "V. Goel"], "venue": "arXiv preprint arXiv:1611.09288, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions", "author": ["F. Ringeval", "A. Sonderegger", "J. Sauer", "D. Lalanne"], "venue": "International Conference and Workshops on Automatic Face and Gesture Recognition (FG). IEEE, 2013, pp. 1\u20138.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "workshop on automatic speech recognition and understanding (ASRU). IEEE, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Using neutral speech models for emotional speech analysis.", "author": ["C. Busso", "S. Lee", "S.S. Narayanan"], "venue": "in Interspeech,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Wavenet: A generative model for raw audio", "author": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "CoRR abs/1609.03499, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015, pp. 1520\u20131528.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Deconvolutional networks", "author": ["M.D. Zeiler", "D. Krishnan", "G.W. Taylor", "R. Fergus"], "venue": "Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2010, pp. 2528\u20132535.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "A guide to convolution arithmetic for deep learning", "author": ["V. Dumoulin", "F. Visin"], "venue": "arXiv preprint arXiv:1603.07285, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Single-image depth perception in the wild", "author": ["W. Chen", "Z. Fu", "D. Yang", "J. Deng"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2016, pp. 730\u2013738.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "A fully convolutional neural network for speech enhancement", "author": ["S.R. Park", "J. Lee"], "venue": "arXiv preprint arXiv:1609.07132, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras, 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "Proceedings of the Python for Scientific Computing Conference (SciPy), 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Discretized continuous speech emotion recognition with multi-task deep recurrent neural network", "author": ["D. Le", "Z. Aldeneh", "E. Mower Provost"], "venue": "Interspeech, 2017 (to apear), 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "[2] showed that it was necessary to consider larger windows when making frame-level emotion predictions (four seconds for arousal and six seconds for valence).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] and Cardinal et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] found that increasing the number of contextual frames when training a deep neural network (DNN) for making frame-level emotion predictions is helpful but only to a certain point.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": ", [5]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 2, "context": "Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6].", "startOffset": 89, "endOffset": 92}, {"referenceID": 3, "context": "Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "Successful attempts to solving the continuous emotion recognition problem relied on DNNs [4], BLSTMs [5], and more commonly, support vector regression (SVR) classifiers [6].", "startOffset": 169, "endOffset": 172}, {"referenceID": 4, "context": "[6] extracted a set of audio features (Mel-frequency cepstral coefficients, shifted delta cepstral, prosody) and then learned higher-level representations of the features using sparse coding.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] used eGeMAPS [8] features along with a set of higher-level bottleneck features extracted from a DNN trained for automatic speech recognition (ASR) to train linear regressors.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] used eGeMAPS [8] features along with a set of higher-level bottleneck features extracted from a DNN trained for automatic speech recognition (ASR) to train linear regressors.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "In contrast, in this work we show that considering temporal dependencies that are longer than those presented in [6, 7] is critical to improve continuous emotion recognition performance.", "startOffset": 113, "endOffset": 119}, {"referenceID": 5, "context": "In contrast, in this work we show that considering temporal dependencies that are longer than those presented in [6, 7] is critical to improve continuous emotion recognition performance.", "startOffset": 113, "endOffset": 119}, {"referenceID": 3, "context": "[5] extracted a comprehensive set of 4, 684 features, which included energy, spectral, and voicing-related features, and used them to train BLSTMs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] trained a convolutional recurrent network for continuous emotion recognition using the time domain signal directly.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Further, since our models work on full-length utterances, we show that it is not necessary to apply any postprocessing steps as described in [9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 8, "context": "[10] proposed viewing ASR problems as dense prediction tasks, where the goal is to assign a label to every frame in a given sequence, and showed that this view provides a set of tools (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "We focus on the RECOLA database [11] following the AVEC 2016 guidelines [2].", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "We focus on the RECOLA database [11] following the AVEC 2016 guidelines [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 10, "context": "We use the Kaldi toolkit [12] to extract 40dimensional log MFB features, using a window length of 25ms with a hop size of 10ms.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "Previous work showed that MFB features are better than conventional MFCCs for predicting emotions [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "[2] only covered six seconds worth of features and Povolny et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] considered a maximum of eight seconds worth of features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "Networks that use dilated convolutions have shown success in a number of tasks, including image segmentation [14], speech synthesis [15] and ASR [10].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "[15] recently showed that it is possible to use convolutions with various dilation factors to allow the receptive field of a generative model to grow exponentially in order to cover thousands of time steps and synthesize highquality speech.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10] showed that ASR could benefit from dilated convolutions since they allow larger regions to be covered without disrupting the length of the input signals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The upsampling function can be implemented in a number of ways [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "In this work we use the transposed convolution [17, 18] operation to perform upsampling.", "startOffset": 47, "endOffset": 55}, {"referenceID": 16, "context": "In this work we use the transposed convolution [17, 18] operation to perform upsampling.", "startOffset": 47, "endOffset": 55}, {"referenceID": 14, "context": ", [16, 19, 20]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 17, "context": ", [16, 19, 20]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 18, "context": ", [16, 19, 20]).", "startOffset": 2, "endOffset": 14}, {"referenceID": 14, "context": "[16] showed that transposed convolution operations can be effectively applied to image segmentation tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In addition to vision applications, downsampling/upsampling architectures have been successfully applied to speech enhancement problems [21], where the goal is to learn a mapping between noisy speech spectra and their clean counterparts.", "startOffset": 136, "endOffset": 140}, {"referenceID": 19, "context": "[21] demonstrated that downsampling/upsampling convolutional networks can be 12\u00d7 smaller (in terms of the number of learnable parameters) than their recurrent counterparts and yet yield better performance on speech enhancement tasks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We build our models using Keras [22] with a Theano backend [23].", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "We build our models using Keras [22] with a Theano backend [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "We also compare our performance to that of an optimized BLSTM regression model, described in [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[2] \u2013 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7]\u2217 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "682 BLSTM [24] .", "startOffset": 10, "endOffset": 14}, {"referenceID": 0, "context": "[2] \u2013 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] .", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7]\u2217 .", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "349 BLSTM [24] .", "startOffset": 10, "endOffset": 14}], "year": 2017, "abstractText": "The goal of continuous emotion recognition is to assign an emotion value to every frame in a sequence of acoustic features. We show that incorporating long-term temporal dependencies is critical for continuous emotion recognition tasks. To this end, we first investigate architectures that use dilated convolutions. We show that even though such architectures outperform previously reported systems, the output signals produced from such architectures undergo erratic changes between consecutive time steps. This is inconsistent with the slow moving ground-truth emotion labels that are obtained from human annotators. To deal with this problem, we model a downsampled version of the input signal and then generate the output signal through upsampling. Not only does the resulting downsampling/upsampling network achieve good performance, it also generates smooth output trajectories. Our method yields the best known audioonly performance on the RECOLA dataset.", "creator": "LaTeX with hyperref package"}}}