{"id": "1708.07738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2017", "title": "A Function Approximation Method for Model-based High-Dimensional Inverse Reinforcement Learning", "abstract": "this raises just the inverse reinforcement learning problem in high - dimensional state spaces, which relies toward an analytical solution of model - based high - mass reinforcement learning problems. to finish the computationally expensive reinforcement expectation problems, we propose robust function design : to ensure that the bellman optimality measure always holds, and eventually chooses a threshold based after estimated true human size for inverse vector learning problems. the time complexity of where proposed method is linearly related to average variance behind the parameters set, are unable to handle high - scale objective continuous state spaces efficiently. we test the proposed method in having simulated test to allow its accuracy, helping intensive clinical tasks currently show how users can be interpreted to know we doctor's behaviour.", "histories": [["v1", "Wed, 23 Aug 2017 20:25:01 GMT  (674kb)", "http://arxiv.org/abs/1708.07738v1", "arXiv admin note: substantial text overlap witharXiv:1707.09394"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1707.09394", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["kun li", "joel w burdick"], "accepted": false, "id": "1708.07738"}, "pdf": {"name": "1708.07738.pdf", "metadata": {"source": "CRF", "title": "A Function Approximation Method for Model-based High-Dimensional Inverse Reinforcement Learning", "authors": ["Kun Li", "Joel W. Burdick"], "emails": ["kunli@caltech.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n70 8.\n07 73\n8v 1\n[ cs\n.L G\n] 2\n3 A\nug 2\n01 7\nI. INTRODUCTION\nRecently, surgical robots, like Da Vinci Surgical System, have been applied to many tasks, due to its reliability and accuracy. In these systems, a doctor operates the robot manipulator remotely, and gets the visual feedback during a surgery. With a sophisticated control system and highresolution images, the surgery can be done with higher precision and less accidents. However, this requires the doctor to concentrate on robot operations and visual feedbacks during the whole surgery, which may lead to fatigue and errors.\nTo solve the problem, some level of automation can be introduced, considering that many surgeries contain repeating atomic operations. For example, knot tying is a typical procedure after many surgeries, as shown in Figure 1, and it can be decomposed into a sequence of pre-trained standard operations for the robot. The automation can also be used to avoid possible mistakes committed by an inexperienced doctor during a surgery, where alarm signal can be triggered when an unusual action is taken by the doctor, and the amount of alarm signals can be used to evaluate the doctor as well.\nThe core of the automation system is a control policy, predicting which action to take under each state for typical surgical robots. The control policy can be defined manually, but it is difficult due to the possible number of states occurring during a surgery. Another solution is estimating the policy by solving a Markov decision process, but it needs an accurate reward function, depending on too many factors to be defined manually.\nAn alternative solution is learning the control policy from experts\u2019 demonstrations through imitation learning. Many\n*This work was supported by the National Institutes of Health, NIBIB. 1Kun Li and Joel W. Burdick are with Department of Mechanical and Civil Engineering, California Institute of Technology, Pasadena, CA 91125, USA kunli@caltech.edu\nalgorithms try to learn the policy from the state-action pair directly in a supervised way, but the learned policy usually does not indicate how good a state-action pair is, which is useful for online doctor action evaluation. This problem can be solved by inverse reinforcement learning algorithms, which learns a reward function from the observed demonstrations, and the optimality of a control policy can be estimated based on the reward function.\nExisting solutions of the inverse reinforcement learning problem mainly work on small-scale problems, by collecting a set of observations for reward estimation and using the estimated reward afterwards. For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy. The method in [5] collects a set of trajectories of the agent, and estimates a reward function that maximizes the likelihood of the trajectories. This strategy works for applications in small state spaces. However, the state space of sensory feedback is huge for surgical evaluation, and these method cannot handle it well due to the reinforcement learning problem in each iteration of reward estimation.\nSome existing methods can be scaled to high-dimensional state spaces and solve the problem without learning the transition model. While they improve the learning efficiency, they cannot utilize unsupervised data, or data from the demonstrations of non-experts. These data cannot be used to learn the reward function, but they provide information about the environment dynamics.\nIn this work, we find that inverse reinforcement learning in high-dimensional space can be simplified under the condition that the transition model and the set of action remain\nunchanged for the subject, where each reward function leads to a unique optimal value function. Based on this assumption, we propose a function approximation method that learns the reward function and the optimal value function, but without the computationally expensive reinforcement learning steps, thus it can be scaled to high dimensional state spaces. This method can also solve model-based high-dimensional reinforcement learning problems, although it is not our main focus.\nThe paper is organized as follows. We review existing work on inverse reinforcement learning in Section II, and formulate the function approximation inverse reinforcement learning method for high-dimensional problems in III. A simulated experiment and a clinical experiment are shown in Section IV, with conclusions in Section V."}, {"heading": "II. RELATED WORKS", "text": "Approximate dynamic programming for reinforcement learning is a well-researched topic in Markov decision process. A good introduction is given in [6]. Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc. But in many robotic applications, reward values are not available for all robot actions, and those data is wasted in model-free learning. Common modelbased approximation methods use a function to approximate the value function or the Q function, and the performance depends on the selected features.\nInverse Reinforcement Learning problem is firstly formulated in [2], where the agent observes the states resulting from an assumingly optimal policy, and tries to learn a reward function that makes the policy better than all alternatives. Since the goal can be achieved by multiple reward functions, this paper tries to find one that maximizes the difference between the observed policy and the second best policy. This idea is extended by [10], in the name of max-margin learning for inverse optimal control. Another extension is proposed in [3], where the purpose is not to recover the real reward function, but to find a reward function that leads to a policy equivalent to the observed one, measured by the amount of rewards collected by following that policy.\nSince a motion policy may be difficult to estimate from observations, a behavior-based method is proposed in [5], which models the distribution of behaviors as a maximumentropy model on the amount of reward collected from each behavior. This model has many applications and extensions. For example, [11] considers a sequence of changing reward functions instead of a single reward function. [12] and [13] consider complex reward functions, instead of linear one, and use Gaussian process and neural networks, respectively, to model the reward function. [14] considers complex environments, instead of a well-observed Markov Decision Process, and combines partially observed Markov Decision Process with reward learning. [15] models the behaviors based on the local optimality of a behavior, instead of the summation of\nrewards. [16] uses a multi-layer neural network to represent nonlinear reward functions.\nAnother method is proposed in [17], which models the probability of a behavior as the product of each state-action\u2019s probability, and learns the reward function via maximum a posteriori estimation. However, due to the complex relation between the reward function and the behavior distribution, the author uses computationally expensive Monte-Carlo methods to sample the distribution. This work is extended by [4], which uses sub-gradient methods to simplify the problem. Another extensions is shown in [18], which tries to find a reward function that matches the observed behavior. For motions involving multiple tasks and varying reward functions, methods are developed in [19] and [20], which try to learn multiple reward functions.\nMost of these methods need to solve a reinforcement learning problem in each step of reward learning, thus practical large-scale application is computationally infeasible. Several methods are applicable to large-scale applications. The method in [2] uses a linear approximation of the value function, but it requires a set of manually defined basis functions. The methods in [13], [21] update the reward function parameter by minimizing the relative entropy between the observed trajectories and a set of sampled trajectories based on the reward function, but they require a set of manually segmented trajectories of human motion, where the choice of trajectory length will affect the result. The method in [22] only learns an optimal value function, instead of the reward function."}, {"heading": "III. HIGH-DIMENSIONAL INVERSE REINFORCEMENT LEARNING", "text": ""}, {"heading": "A. Markov Decision Process", "text": "A Markov Decision Process is described with the follow-\ning variables:\n\u2022 S= {s}, a set of states \u2022 A= {a}, a set of actions \u2022 Pa\nss\u2032 , a state transition function that defines the probabil-\nity that state s becomes s\u2032 after action a.\n\u2022 R= {r(s)}, a reward function that defines the immediate reward of state s. \u2022 \u03b3 , a discount factor that ensures the convergence of the MDP over an infinite horizon.\nAn agent\u2019s motion can be represented as a sequence of\nstate-action pairs:\n\u03b6 = {(si,ai)|i= 0, \u00b7 \u00b7 \u00b7 ,N\u03b6 },\nwhere N\u03b6 denotes the length of the motion, varying in different observations. Given the observed sequence, inverse reinforcement learning algorithms try to recover a reward function that explains the motion.\nOne key problem is how to model the action in each state, or the policy, \u03c0(s) \u2208 A, a mapping from states to actions. This problem can be handled by reinforcement learning algorithms, by introducing the value function V (s) and the\nQ-function Q(s,a), described by the Bellman Equation [23]:\nV \u03c0(s) = \u2211 s\u2032|s,\u03c0(s) P \u03c0(s) ss\u2032 [r(s\u2032)+ \u03b3 \u2217V \u03c0(s\u2032)], (1) Q\u03c0(s,a) = \u2211 s\u2032|s,a Pass\u2032 [r(s \u2032)+ \u03b3 \u2217V \u03c0(s\u2032)], (2)\nwhere V \u03c0 and Q\u03c0 define the value function and the Qfunction under a policy \u03c0 . For an optimal policy \u03c0\u2217, the value function and the Q-function should be maximized on every state. This is described by the Bellman Optimality Equation [23]:\nV \u2217(s) =max a\u2208A\n\u2211 s\u2032|s,a\nPass\u2032 [r(s \u2032)+ \u03b3 \u2217V \u2217(s\u2032)], (3)\nQ\u2217(s,a) = \u2211 s\u2032|s,a Pass\u2032 [r(s \u2032)+ \u03b3 \u2217max a\u2032\u2208A Q\u2217(s\u2032,a\u2032)]. (4)\nIn typical inverse reinforcement learning algorithms, the Bellman Optimality Equation needs to be solved once for each parameter updating of the reward function, thus it is computationally infeasible in high-dimensional state spaces. While several existing approaches solve the problem at the expense of the optimality, we propose an approximation method to avoid the problem."}, {"heading": "B. Function Approximation Framework", "text": "Given the set of actions and the transition probability, a reward function leads to a unique optimal value function. To learn the reward function from the observed motion, instead of directly learning the reward function, we use a parameterized function, named as VR function, to represent the summation of the reward function and the discounted value function:\nf (s,\u03b8 ) = r(s)+ \u03b3 \u2217V \u2217(s). (5)\nThe function value of a state is named as VR value.\nSubstituting Equation (5) into Bellman Optimality Equa-\ntion, the optimal Q function is given as:\nQ\u2217(s,a) = \u2211 s\u2032|s,a Pass\u2032 f (s \u2032 ,\u03b8 ), (6)\nthe optimal value function is given as:\nV \u2217(s) =max a\u2208A Q\u2217(s,a)\n=max a\u2208A \u2211 s\u2032|s,a\nPass\u2032 f (s \u2032 ,\u03b8 ), (7)\nand the reward function can be computed as:\nr(s) = f (s,\u03b8 )\u2212 \u03b3 \u2217V \u2217(s)\n= f (s,\u03b8 )\u2212 \u03b3 \u2217max a\u2208A \u2211 s\u2032|s,a\nPass\u2032 f (s \u2032 ,\u03b8 ). (8)\nNote that this formulation can be generalized to other extensions of Bellman Optimality Equation by replacing the max operator with other types of Bellman backup operators. For example, V \u2217(s) = loga\u2208A expQ \u2217(s,a) is used in the maximum-entropy method[5]; V \u2217(s) = 1 k loga\u2208A expk \u2217 Q\u2217(s,a) is used in Bellman Gradient Iteration [24].\nFor any VR function f and any parameter \u03b8 , the optimal Q function Q\u2217(s,a), optimal value function V \u2217(s), and reward function r(s) constructed with Equation (6), (7), and (8) always meet the Bellman Optimality Equation. Under this condition, we try to recover a parameterized function f (s,\u03b8 ) that best explains the observed rewards for reinforcement learning problems, and expert demonstrations \u03b6 for inverse reinforcement learning problems.\nFor reinforcement learning problems, the Bellman backup operator should be a differentiable one, thus the function parameter can be updated based on the observed rewards.\nFor inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v\n\u2217(k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a). The main limitation is the assumption of a known transition model Pa\nss\u2032 , but it only requires a partial\nmodel on the visited states rather than a full environment model, and it can be learned independently in an unsupervised way."}, {"heading": "C. High-dimensional Reinforcement Learning", "text": "Although it is not our main focus, we briefly show how the proposed method solves high-dimensional reinforcement learning problems. Assuming the approximation function is a neural network, the parameter \u03b8 = {w,b}-weights and biasesin Equation (5) can be estimated from the observed sequence of rewards R\u0302s via least-square estimation, where the objective function is:\nLSE(\u03b8 ) = \u2211 s\n||R\u0302s\u2212 r(s)|| 2 .\nThe reward function r(s) in Equation (8) is nondifferentiable with the max function as the Bellman backup operator. By approximating it with the generalized softmax function [24], the gradient of the objective function is:\n\u2207\u03b8LSE(\u03b8 ) = \u2211 s 2 \u2217 ||R\u0302s\u2212 r(s)|| \u2217 (\u2212\u2207\u03b8 r(s)),\nwhere\n\u2207\u03b8 r(s)=\u2207\u03b8 f (s,\u03b8 )\u2212\u03b3 \u2217 \u2211 a\u2208A\nexp(kQ\u2217(s,a))\n\u2211a\u2032\u2208A exp(kQ \u2217(s,a)) \u2211\ns\u2032|s,a\nPass\u2032\u2207\u03b8 f (s,\u03b8 ),\nand k is the approximation level.\nThe parameter \u03b8 can be learned with gradient methods. The algorithm is shown in Algorithm 1. With the learned parameter, the optimal value function and a control policy can be estimated."}, {"heading": "D. High-dimensional Inverse Reinforcement Learning", "text": "For IRL problems, this work chooses max as the Bellman backup operator and a motion model p(a|s) based on the optimal Q function Q\u2217(s,a) [17]:\nP(a|s) = expb \u2217Q\u2217(s,a)\n\u2211a\u0303\u2208A expb \u2217Q\u2217(s, a\u0303) , (9)\nAlgorithm 1 Function Approximation RL with Neural Network\n1: Data: R,S,A,P,\u03b3,b,\u03b1 2: Result: optimal value V \u2217[S], optimal action value\nQ\u2217[S,A] 3: create variable \u03b8 = {W,b} for a neural network 4: build f [S,\u03b8 ] as the output of the neural network 5: build Q\u2217[S,A], V \u2217[S], and R[S] based on Equation (5),\n(6), (7), and (8).\n6: build objective function LSE[\u03b8 ] based on R[S] 7: compute gradient \u2207\u03b8LSE[\u03b8 ] 8: initialize \u03b8 9: while not converging do\n10: \u03b8 = \u03b8 +\u03b1 \u2217\u2207\u03b8LSE[\u03b8 ] 11: end while 12: evaluate optimal value V \u2217[S], optimal action value Q\u2217[S,A] 13: return Q\u2217[S,A]\nwhere b is a parameter controlling the degree of confidence in the agent\u2019s ability to choose actions based on Q values. In the remaining sections, we use Q\u2217(s,a) to denote the optimal Q values for simplified notations.\nAssuming the approximation function is a neural network, the parameter \u03b8 = {w,b}-weights and biases-in Equation (5) can be estimated from the observed sequence of state-action pairs \u03b6 via maximum-likelihood estimation:\n\u03b8 = argmax \u03b8 logP(\u03b6 |\u03b8 ), (10)\nwhere the log-likelihood of P(\u03b6 |\u03b8 ) is given by:\nL(\u03b8 ) = logP(\u03b6 |\u03b8 )\n= log \u220f (s,a)\u2208\u03b6 P(a|\u03b8 ;s)\n= log \u220f (s,a)\u2208\u03b6\nexpb \u2217Q\u2217(s,a)\n\u2211a\u0302\u2208A expb \u2217Q \u2217(s, a\u0302)\n= \u2211 (s,a)\u2208\u03b6 (b \u2217Q\u2217(s,a)\u2212 log \u2211 a\u0302\u2208A expb \u2217Q\u2217(s, a\u0302)), (11)\nand the gradient of the log-likelihood is given by:\n\u2207\u03b8L(\u03b8 ) = \u2211 (s,a)\u2208\u03b6\n(b \u2217\u2207\u03b8Q \u2217(s,a)\n\u2212 b \u2217 \u2211 a\u0302\u2208A\nP((s, a\u0302)|r(\u03b8 ))\u2207\u03b8Q \u2217(s, a\u0302)). (12)\nWith a differentiable approximation function,\n\u2207\u03b8Q \u2217(s,a) = \u2211\ns\u2032|s,a\nPass\u2032\u2207\u03b8 f (s \u2032 ,\u03b8 ),\nand\n\u2207\u03b8L(\u03b8 ) = \u2211 (s,a)\u2208\u03b6 (b \u2217 \u2211 s\u2032|s,a\nPass\u2032\u2207\u03b8 f (s \u2032 ,\u03b8 )\n\u2212 b \u2217 \u2211 a\u0302\u2208A P((s, a\u0302)|r(\u03b8 )) \u2211 s\u2032|s,a\nPass\u2032\u2207\u03b8 f (s \u2032 ,\u03b8 )), (13)\nAlgorithm 2 Function Approximation IRL with Neural Network\n1: Data: \u03b6 ,S,A,P,\u03b3,b,\u03b1 2: Result: optimal value V \u2217[S], optimal action value\nQ\u2217[S,A], reward value R[S] 3: create variable \u03b8 = {W,b} for a neural network 4: build f [S,\u03b8 ] as the output of the neural network 5: build Q\u2217[S,A], V \u2217[S], and R[S] based on Equation (5),\n(6), (7), and (8).\n6: build loglikelihood L[\u03b8 ] based on \u03b6 and Q\u2217[S,A] 7: compute gradient \u2207\u03b8L[\u03b8 ] 8: initialize \u03b8 9: while not converging do\n10: \u03b8 = \u03b8 +\u03b1 \u2217\u2207\u03b8L[\u03b8 ] 11: end while 12: evaluate optimal value V \u2217[S], optimal action value Q\u2217[S,A], reward value R[S] 13: return R[S]\nwhere \u2207\u03b8 f (s \u2032 ,\u03b8 ) denotes the gradient of the neural network output with respect to neural network parameter \u03b8 = {w,b}.\nIf the VR function f (s,\u03b8 ) is linear, the objective function in Equation (11) is concave, and a global optimum exists. However, a multi-layer neural network works better to handle the non-linearity in approximation and the high-dimensional state space data.\nA gradient ascent method is used to learn the parameter\n\u03b8 :\n\u03b8 = \u03b8 +\u03b1 \u2217\u2207\u03b8L(\u03b8 ), (14)\nwhere \u03b1 is the learning rate.\nWhen the method converges, we can compute the optimal Q function, the optimal value function, and the reward function based on Equation (5), (6), (7), and (8). The algorithm under a neural network-based approximation function is shown in Algorithm 2.\nThis method does not involve solving the MDP problem for each updated parameter \u03b8 , and large-scale state space can be easily handled by an approximation function based on a multi-layer neural network.\nObviously, the approximation function is not unique, but all of them will generate the same optimal values and rewards for the observed state-action pairs after convergence. By choosing a neural network with higher capacity, we may overfit the observed state-action distribution, and do not generalize well. Therefore, the choice of the approximation function depends on how well the observed motion matches the ground truth one."}, {"heading": "IV. EXPERIMENTS", "text": "We first test the proposed method in a simulated environment, to compare its accuracy under different approximation functions, and then apply the proposed method to surgical data in JIGSAW dataset [1]."}, {"heading": "A. Simulated Environment", "text": "We create a four-dimensional grids, with 10 grid in each dimension, thus 10000 states are generated. Several rewardemitting objects are put randomly in the grid, and each of them generates an exponentially decaying negative or positive reward value to all the grid based on the distances. The true reward value of each grid is the summation of the generated rewards in the grid. An agent moves in the grids, and it can choose to move up, down, or stay still in each dimension, described by an action set of 34 = 81 actions. The observable feature of a grid is the grid\u2019s distances to the reward-generating objects.\nTo test the application of the proposed method to reinforcement learning problems, we assume that the reward value of each state is available for the robot, and it has to learn an optimal value function from it. We compare the ground truth value function, computed through value iteration, and the value function recovered by the robot based on the mean error of the optimal Q values.\nWe choose neural networks as the approximation function, and compare the errors under different neural net configurations. We choose the configuration by firstly fixing the number of nodes in each hidden layer and increasing the number of layers, and then fixing the number of hidden layers and increasing the number of nodes in each layer. Stochastic gradient descent is used in optimization, with batch size 50 and learning rate 0.00001. The result is shown in Figure 2 and 3.\nTo test the application of the proposed method to inverse reinforcement learning problems, we generate 200000 trajectories with random initial position and length 10 based on the true reward function, and try to recover a reward function based on the trajectories. We compute the accuracy based on the correlation coefficient between the ground truth reward function and the recovered reward function. Similarly, we compare the accuracy under different neural network configurations. The result is shown in Figure 4 and 5.\nThe results show that the accuracies of learned value function and reward function improve as the capacity of network increases, and increasing network width works better."}, {"heading": "B. Surgical Robot Operator", "text": "We apply the proposed method to surgical robot operators in JIGSAW data set [1]. This data set describes three tasks, knot tying, needling passing, and suturing. An illustration of the tasks is shown in Figure 6. Each task is conducted by multiple robot operators, whose skills range from expert, intermediate to novice.\nThe data includes videos from two stereo cameras and robot states synchronized to the images. We assume the operator\u2019s actions change the linear and angular acceleration of the robot, and then we use k-means clustering to identify 10000 actions from the dataset. The state set includes the robot manipulator\u2019s positions and velocities, represented by a length-38 vector with continuous values. The transition probability is computed based on physical law.\nWe apply the model to surgical operator evaluation on three tasks by training on all experts and testing on novice and intermediate operators. The results are shown in Figure 7, 8 and 9.\nThe results show that the proposed method successfully identifies the difference between inexperienced operators and experienced operators, thus it can be used in evaluation tasks."}, {"heading": "V. CONCLUSIONS", "text": "This work deals with the problem of high-dimensional inverse reinforcement learning, where the state space is usually\ntoo large for many existing solutions. We solve the problem with a function approximation framework by approximating the reinforcement learning solution. The method is firstly tested in a simulated environment, and then applied to the evaluation of surgical robot operators in three clinical tasks.\nIn current settings, each task has one reward function, associated with an optimal value function. In future work, we will extend this method for a robot to learn multiple reward functions. Besides, we will try to integrate transition model learning into the framework."}], "references": [{"title": "Jhu-isi gesture and skill assessment working set (jigsaws): A surgical activity dataset for human motion modeling", "author": ["Y. Gao", "S.S. Vedula", "C.E. Reiley", "N. Ahmidi", "B. Varadarajan", "H.C. Lin", "L. Tao", "L. Zappella", "B. B\u00e9jar", "D.D. Yuh"], "venue": "MICCAI Workshop: M2CAI, vol. 3, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S. Russell"], "venue": "in Proc. 17th International Conf. on Machine Learning, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "Proceedings of the twenty-first international conference on Machine learning. ACM, 2004, p. 1.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Apprenticeship learning using inverse reinforcement learning and gradient methods", "author": ["G. Neu", "C. Szepesv\u00e1ri"], "venue": "arXiv preprint arXiv:1206.5264, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximum entropy inverse reinforcement learning", "author": ["B.D. Ziebart", "A. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "Proc. AAAI, 2008, pp. 1433\u20131438.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep reinforcement learning with double q-learning", "author": ["H. v. Hasselt", "A. Guez", "D. Silver"], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. AAAI Press, 2016, pp. 2094\u20132100.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Advances in neural information processing systems, 2000, pp. 1057\u20131063.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 729\u2013736.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Inverse reinforcement learning with locally consistent reward functions", "author": ["Q.P. Nguyen", "B.K.H. Low", "P. Jaillet"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1747\u20131755.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "Advances in Neural Information Processing Systems 24, J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2011, pp. 19\u201327.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["C. Finn", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1603.00448, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Inverse reinforcement learning in partially observable environments", "author": ["J. Choi", "K.-E. Kim"], "venue": "Journal of Machine Learning Research, vol. 12, no. Mar, pp. 691\u2013730, 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Continuous inverse optimal control with locally optimal examples", "author": ["S. Levine", "V. Koltun"], "venue": "arXiv preprint arXiv:1206.4617, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep inverse reinforcement learning", "author": ["M. Wulfmeier", "P. Ondruska", "I. Posner"], "venue": "arXiv preprint arXiv:1507.04888, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": "Proceedings of the 20th International Joint Conference on Artifical Intelligence, ser. IJCAI\u201907. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2007, pp. 2586\u20132591.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "From human to humanoid locomotionan inverse optimal control approach", "author": ["K. Mombaur", "A. Truong", "J.-P. Laumond"], "venue": "Autonomous robots, vol. 28, no. 3, pp. 369\u2013383, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian multitask inverse reinforcement learning", "author": ["C. Dimitrakakis", "C.A. Rothkopf"], "venue": "European Workshop on Reinforcement Learning. Springer, 2011, pp. 273\u2013284.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonparametric bayesian inverse reinforcement learning for multiple reward functions", "author": ["J. Choi", "K.-E. Kim"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 305\u2013313.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Relative entropy inverse reinforcement learning", "author": ["A. Boularias", "J. Kober", "J.R. Peters"], "venue": "International Conference on Artificial Intelligence and Statistics, 2011, pp. 182\u2013189.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Linearly-solvable markov decision problems", "author": ["E. Todorov"], "venue": "Proceedings of the 19th International Conference on Neural Information Processing Systems. MIT Press, 2006, pp. 1369\u20131376.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Bellman Gradient Iteration for Inverse Reinforcement Learning", "author": ["K. Li", "J.W. Burdick"], "venue": "ArXiv e-prints, Jul. 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Linearly-solvable markov decision problems", "author": ["E. Todorov"], "venue": "Advances in neural information processing systems, 2007, pp. 1369\u2013 1376.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "1: Knot tying with Da Vinci robot: the photo is grabbed from JIGSAW dataset [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy.", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "For example, the methods in [2], [3], [4] estimate the agent\u2019s policy from a set of observations, and estimate a reward function that leads to the policy.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "The method in [5] collects a set of trajectories of the agent, and estimates a reward function that maximizes the likelihood of the trajectories.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "A good introduction is given in [6].", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Some model-free methods produce many promising results in recent years, like deep Q network [7], double Q learning [8], advantage learning [9], etc.", "startOffset": 139, "endOffset": 142}, {"referenceID": 1, "context": "Inverse Reinforcement Learning problem is firstly formulated in [2], where the agent observes the states resulting from an assumingly optimal policy, and tries to learn a reward function that makes the policy better than all alternatives.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "This idea is extended by [10], in the name of max-margin learning for inverse optimal control.", "startOffset": 25, "endOffset": 29}, {"referenceID": 2, "context": "extension is proposed in [3], where the purpose is not to recover the real reward function, but to find a reward function that leads to a policy equivalent to the observed one, measured by the amount of rewards collected by following that policy.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "Since a motion policy may be difficult to estimate from observations, a behavior-based method is proposed in [5], which models the distribution of behaviors as a maximumentropy model on the amount of reward collected from each behavior.", "startOffset": 109, "endOffset": 112}, {"referenceID": 10, "context": "For example, [11] considers a sequence of changing reward functions instead of a single reward function.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "[12] and [13] consider complex reward functions, instead of linear one, and use Gaussian process and neural networks, respectively, to model the reward function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] and [13] consider complex reward functions, instead of linear one, and use Gaussian process and neural networks, respectively, to model the reward function.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "[14] considers complex environments, instead of a well-observed Markov Decision Process, and combines partially observed Markov Decision Process with reward learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] models the behaviors based on the local optimality of a behavior, instead of the summation of rewards.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] uses a multi-layer neural network to represent nonlinear reward functions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Another method is proposed in [17], which models the probability of a behavior as the product of each state-action\u2019s probability, and learns the reward function via maximum a posteriori estimation.", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "This work is extended by [4], which uses sub-gradient methods to simplify the problem.", "startOffset": 25, "endOffset": 28}, {"referenceID": 17, "context": "Another extensions is shown in [18], which tries to find a reward function that matches the observed behavior.", "startOffset": 31, "endOffset": 35}, {"referenceID": 18, "context": "For motions involving multiple tasks and varying reward functions, methods are developed in [19] and [20], which try to learn multiple reward functions.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "For motions involving multiple tasks and varying reward functions, methods are developed in [19] and [20], which try to learn multiple reward functions.", "startOffset": 101, "endOffset": 105}, {"referenceID": 1, "context": "The method in [2] uses a linear approximation of the value function, but it requires a set of manually defined basis functions.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "The methods in [13], [21] update the reward function parameter by minimizing the relative entropy between the observed trajectories and a set of sampled trajectories based on the reward function, but they require a set of manually segmented trajectories of human motion, where the choice of trajectory length will affect the result.", "startOffset": 15, "endOffset": 19}, {"referenceID": 20, "context": "The methods in [13], [21] update the reward function parameter by minimizing the relative entropy between the observed trajectories and a set of sampled trajectories based on the reward function, but they require a set of manually segmented trajectories of human motion, where the choice of trajectory length will affect the result.", "startOffset": 21, "endOffset": 25}, {"referenceID": 21, "context": "The method in [22] only learns an optimal value function, instead of the reward function.", "startOffset": 14, "endOffset": 18}, {"referenceID": 22, "context": "Q-function Q(s,a), described by the Bellman Equation [23]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "This is described by the Bellman Optimality Equation [23]:", "startOffset": 53, "endOffset": 57}, {"referenceID": 4, "context": "For example, V \u2217(s) = loga\u2208A expQ \u2217(s,a) is used in the maximum-entropy method[5]; V \u2217(s) = 1 k loga\u2208A expk \u2217 Q\u2217(s,a) is used in Bellman Gradient Iteration [24].", "startOffset": 78, "endOffset": 81}, {"referenceID": 23, "context": "For example, V \u2217(s) = loga\u2208A expQ \u2217(s,a) is used in the maximum-entropy method[5]; V \u2217(s) = 1 k loga\u2208A expk \u2217 Q\u2217(s,a) is used in Bellman Gradient Iteration [24].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "For inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v (k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a).", "startOffset": 197, "endOffset": 201}, {"referenceID": 4, "context": "For inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v (k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a).", "startOffset": 264, "endOffset": 267}, {"referenceID": 16, "context": "For inverse reinforcement learning problems, combined with different Bellman backup operators, this formulation can extend many existing methods to high-dimensional space, like the motion model in [25], p(a|s) = \u2212v\u2217(s)\u2212 log\u2211k ps,k exp(\u2212v (k)), the motion model in [5], p(a|s) = expQ\u2217(s,a)\u2212V \u2217(s), and the motion model in [17], p(a|s) \u221d expQ\u2217(s,a).", "startOffset": 321, "endOffset": 325}, {"referenceID": 23, "context": "By approximating it with the generalized softmax function [24], the gradient of the objective function is:", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "For IRL problems, this work chooses max as the Bellman backup operator and a motion model p(a|s) based on the optimal Q function Q\u2217(s,a) [17]:", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "We first test the proposed method in a simulated environment, to compare its accuracy under different approximation functions, and then apply the proposed method to surgical data in JIGSAW dataset [1].", "startOffset": 197, "endOffset": 200}, {"referenceID": 0, "context": "We apply the proposed method to surgical robot operators in JIGSAW data set [1].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 19, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 28, "endOffset": 47}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 48, "endOffset": 71}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 72, "endOffset": 99}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 0, "context": "[10, 10, 1] [10, 10, 10, 1] [10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 1] [10, 10, 10, 10, 10, 10, 10, 1]", "startOffset": 100, "endOffset": 131}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 19, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 12, "endOffset": 27}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 28, "endOffset": 43}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 44, "endOffset": 59}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 9, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}, {"referenceID": 0, "context": "[10, 10, 1] [10, 20, 10, 1] [10, 30, 10, 1] [10, 40, 10, 1] [10, 50, 10, 1]", "startOffset": 60, "endOffset": 75}], "year": 2017, "abstractText": "This works handles the inverse reinforcement learning problem in high-dimensional state spaces, which relies on an efficient solution of model-based high-dimensional reinforcement learning problems. To solve the computationally expensive reinforcement learning problems, we propose a function approximation method to ensure that the Bellman Optimality Equation always holds, and then estimate a function based on the observed human actions for inverse reinforcement learning problems. The time complexity of the proposed method is linearly proportional to the cardinality of the action set, thus it can handle high-dimensional even continuous state spaces efficiently. We test the proposed method in a simulated environment to show its accuracy, and three clinical tasks to show how it can be used to evaluate a doctor\u2019s proficiency.", "creator": "LaTeX with hyperref package"}}}