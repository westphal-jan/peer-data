{"id": "1708.09086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "A Deep Learning Approach for Population Estimation from Satellite Imagery", "abstract": "informing such people remain reveals a pivotal component of ecosystem rule making process such as personal health, technological change analysis, countryside planning, risk management, conservation planning, hopefully more. while truth - runner, survey driven reports can provide a comprehensive view into the population landscape of another country, they are poor researchers realize, are infrequently updated, and only provide population scales affect geographic areas. population disaggregation techniques when population inference methods individually address these shortcomings, ensuring neither refuse accuracy nor most own. to sufficiently answer the questions of \" globally divided populations say \" and \" assuming many realities fall alike, \" we propose a deep learning model. creating high - utility population estimations behind satellite signals. annually, we implement convolutional neural devices to identify population in the classroom at a $ 79. 38 ^ { \\ circ } \\ times 0. 99 ^ { \\ circ } $ > matrices display 1 - gram solar landsat imagery. we discuss these models including two ways : quantitatively, by coordinating all population's customized topography grid distributed over high statistical - level to several us topographic county - level population projections, and qualitatively, manually directly interpreting the model's predictions in terms using common satellite image inputs. this assure that aggregating neural scales'statistical estimates provided superior results with the census county - unit models plotted and does the predictions made by neither model therefore be directly interpreted, but give it advantages among traditional grid disaggregation methods. through general, our mapping cannot specifically expose of how expert learning techniques advances be an influential tool for extracting information from intentionally unstructured, remotely sensed data to provide effective solutions, social deprivation.", "histories": [["v1", "Wed, 30 Aug 2017 02:05:16 GMT  (5138kb,D)", "http://arxiv.org/abs/1708.09086v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["caleb robinson", "fred hohman", "bistra dilkina"], "accepted": false, "id": "1708.09086"}, "pdf": {"name": "1708.09086.pdf", "metadata": {"source": "CRF", "title": "A Deep Learning Approach for Population Estimation from Satellite Imagery", "authors": ["Caleb Robinson", "Fred Hohman", "Bistra Dilkina"], "emails": ["dcrobins@gatech.edu", "fredhohman@gatech.edu", "bdilkina@cc.gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Many countries around the world conduct censuses to gather rich information about their population\u2019s size, composition, and demographics. While these censuses only happen every 5 to 10 years depending on the country, they are highly important for government policymakers and planners who use population projections to gauge future demand for food, water, energy, and services. In the United States sub-national population estimates between census dates are used extensively. County level population estimates are used in: \u201cfederal and state funds allocation\u201d, \u201cdenominators for vital rates and per capita time series\u201d, \u201csurvey controls\u201d, \u201cadministrative planning and marketing guidance\u201d, and \u201cdescriptive and analytical studies\u201d, according to Long, 1996 (1). Population projections also impact the economy and may result is large governmental spending. For example, according to the US General Accounting Office, more than \u201c70 federal programs distribute tens\nar X\niv :1\n70 8.\n09 08\n6v 1\n[ cs\n.A I]\n3 0\nA ug\nof billions of dollars annually on the basis of population estimates\u201d, and \u201c[e]ven more money was distributed indirectly on the basis of indicators which used population estimates for denominators or controls\u201d (1). Unfortunately, censuses in many other countries are non-representative due to limited civil registration systems (2).\nPopulation projection accuracy has also been gaining more attention due to the consequences of long-term health effects such as aging and infectious diseases such as HIV/AIDS. Traditionally, population predictions rely on the interaction between three factors: fertility, mortality, and migration1. To project population characteristics at a future date, demographers make assumptions about fertility and mortality in a current population and further assume how many people will move into or out of an area before that date, i.e., migration. But since population projections carry inherent uncertainty, demographers often times can use previous projections and projection errors to better inform future projections.\nGiven an administrative area, the spatial distribution of the population in that area can be determined by answering two questions: \u201chow many people live in the area?\u201d, and \u201cwhere, specifically, in the area do people live?\u201d. These two questions can be cast as the following two problems: population projection, and population disaggregation. Traditionally, these questions are addressed independently of one another using population projection methods and population disaggregation methods, respectively. In the population projection task, the goal is to estimate the number of people that live in a particular administrative area based on historical data. Methods such as regression models, and non-comprehensive supplemental census surveys (like the American Community Survey) belong to this category. In the population disaggregation task, the goal is to distribute a population estimate for a given administrative area within that area, i.e., at a higher spatial resolution than the population estimate was originally made for.\nOur proposed method performs both of these tasks jointly. Using recent techniques from deep learning, which has shown remarkable state-of-the-art results in many computer vision tasks (3; 4), we train convolutional neural networks (CNNs) to directly predict the population of a given 0.01\u25e6 \u00d7 0.01\u25e6 area using only satellite imagery, then summarize the predictions at different administrative area resolutions. These high-level predictions provide greater confidence in the accuracy of our model\u2019s predictions at the finer resolution. We perform two types of model validation. Quantitatively, we compare our model\u2019s grid cell estimates aggregated at a county level to several US Census county level population projections. Qualitatively, we directly interpret the model\u2019s predictions in terms of the satellite image inputs."}, {"heading": "2 Related Work", "text": "Deep learning is being used with increasing frequency to solve problems in the domain of computational sustainability and urban planning. At a broader level, CNNs have been extensively used in computer vision applications in recent years, and have achieved state of the art results in image classification and object recognition (4; 3; 5). New types of network layers, such as batch normalization and dropout, have also been developed to improve the accuracy of CNNs (6; 7). Convolutional neural networks have been used to predict the spatial distribution of poverty in developing countries by using nighttime lights as a data rich target for a transfer learning task (8; 9). Pre-trained CNNs have recently been shown to be effective at the problem of remote sensing image scenes classification through the tuning a small number of layers (10; 11). Similarly, deep learning\n1Public Reference Bureau: http://www.prb.org/Publications/Reports/\nhas been shown to be effective in the task of classifying land cover type, with recent work that has achieved high classification accuracy on new large land cover datasets using mixed CNN based approaches (12; 13).\nThe most similar work to ours also uses CNNs to estimate population from satellite imagery (14). The motivation of this paper is similar to ours, as we both attempt to create high-resolution gridded population counts for use in planning applications. This paper estimates population in Kenya at a 8km2 resolution with a CNN trained on data from Tanzania at a 250m2 satellite pixel resolution. The author\u2019s propose a way to use their CNN\u2019s output as a weighted surface for population disaggregation, and compare this method to other methods for disaggregating population counts in Kenya. Our work differs in several important ways. First, we focus on validating our model\u2019s predictions as raw population projections and do not consider using our model\u2019s prediction as a weighted surface for distributing population counts. If the population (or projected population) of an area is known a priori, then any population assignment method can degrade into a weighting scheme. Secondly, we focus on interpreting the results of our model as a way of validating its ability to generalize. Thirdly, we apply our method to the entire US using census block derived training and testing data.\nOther related work is divided between the two problems we aim to address jointly with our method: population projection and population disaggregation. In the following paragraphs we address each of these problems to give context to our methodology.\nOn average, county population can be reliably extrapolated over short time horizons with simple linear models, however if some counties experience disproportionally higher or lower growth rates, more complicated models are needed (15). The US Census has led research into population and demographic projections, and uses a variety of different population and demographic projection methods to create sub-national projections broken down by age, sex, and race (16; 1). Census postcensal projections, projections done in between census years, are created with a method known as the ratio-correlation method (17; 18; 1). This method uses the current year\u2019s estimated population, number of live births, registered vehicles, public school enrollment, registered voters, deaths, and other information to determine the estimated population change at the next census date. More recently, the American Community Survey has been used as annual supplemental surveys to update the demographics profiles of a variety of sub-national areas in between census years (19; 20).\nPopulation disaggregation methods, and the creation of high resolution population grids have been studied for decades (21; 22). The most basic method in this class is areal interpolation, whereby the known population of an administrative zone is distributed uniformly across its area (23). This process happens on a discretized grid over an administrative zone, where each cell in the grid is assigned a population value equal to the total population over the total number of cells that cover an administrative zone. Dasymetric weighting schemes extend this idea of distributing the known population of an area by creating a weighted surface to distribute the known population, instead of doing so uniformly. The weighting schemes are determined by combining different spatial layers (e.g., slope, average rainfall, land/water masks) according to some set of rules. While some weighting schemes are completely ad-hoc, recently, machine learning methods have been used to improve upon this approach (24; 25; 26). These methodologies are similar to traditional supervised machine learning problems (27), but since actual ground truth data does not exist to compare against, validating the results of dasymetric models is challenging. Finally, there are many existing gridded population datasets created using a variety of the previously mentioned disaggregation techniques. Briefly, these include: Gridded Population of the World (28), GRUMP (29), Landscan (30; 31), as well as the AfriPop, AsiaPop, and AmeriPop databases."}, {"heading": "3 Methods", "text": "The goal of this research is to make high-resolution gridded population estimates from satellite imagery. To do this we train CNNs that take satellite imagery of some area as input, and output a population estimate for that area. We train our models on the continental United States using US Census population counts and Landsat 7 1-year composite imagery from the year 2000. We test our models using the 2010 versions of the same datasets, and evaluate the population estimates in two ways: (1) aggregating our model\u2019s estimates at the county geography level, then comparing them to projected county population counts; and (2) showing why our model makes predictions in terms of input image features.\nAs described in Section 3.1, we let Pt be a grid of target population values covering the continental United States, Ct be a grid of target population class values, and \u03b8t be a grid of satellite images, where for every target value P i,jt and C i,j t there is an associated satellite image, \u03b8 i,j t . Using this notation, we can express our learning task as estimating two functions: one in a regression format, f(\u03b8i,jt ) = P i,j t , and one in a classification format, g(\u03b8 i,j t ) = C i,j t . For the purpose of this study we will focus on the classification version of this problem. We use CNNs to approximate this function, as the mapping from image to population counts will be highly non-linear, noisy, and depend strongly on the semantic content of the input image, e.g., on the quantity and type of buildings visible in an input image. Once we have approximated g on a training year, i.e. for t = 2000, we can use it to create population projections for a future year, in which a census has not been taken, but satellite imagery exists for. We validate this modeling methodology by training CNNs using data from C2000 and \u03b82000, then running our model with all of \u03b82010 to create a predicted population surface for 2010. To evaluate our predictions, we compare our predicted population values aggregated at the county level to other county level population predictions, we show the errors our models makes, and we use interpretation techniques to uncover why our models are making such predictions.\nWe describe the data and the preprocessing steps that we use in Section 3.1, the CNN model architecture choices in Section 3.2, and the experimental methodology that we follow to train, validate, and test our models in Section 3.3. Note that we perform all model training, testing, and experiments using a single desktop workstation containing an NVIDIA Titan GPU."}, {"heading": "3.1 Data", "text": "We use three datasets in this work: the Center for International Earth Science Information Networks\u2019 (CIESIN) US Census Summary Grids for 2000 and 2010 (32; 33), Landsat 7 1-year composite images for 2000 and 2010 (courtesy of the U.S. Geological Survey)2 downloaded from Google Earth Engine, and county level population data for 2000 and 2010 from the US Census.\nThe US Census Summary Grids are raster files with a resolution of 30 arc-seconds (\u2248 1km2) where the raster cell values are population counts from their respective census. The per cell counts are created by disaggregating census survey data from census block geographies, while taking into account various geographic features, such as bodies of water, where people won\u2019t be living. In general, a raster cell will contain an area-weighted combination of the populations from the census block shapes that it intersects with. Since census block geographies are smaller than the 30 arcsecond grid in heavily populated areas, these maps represent the closest \u201cground truth\u201d values for population that are available to use as training data for our machine learning models. As a\n2Landsat: https://landsat.usgs.gov/\npre-processing step, we re-project these two rasters into a slightly coarser grid with a resolution of 0.01\u25e6 \u00d7 0.01\u25e6 (\u2248 1105m2 at the equator), where the northwest corner is at 124.849\u25e6W, 49.3844\u25e6N .\nWe represent each of these grids as a matrix, Pt \u2208 Z2499\u00d75796+ , where an entry P i,j t represents the population of the cell in the ith row and jth column from year t (in this case t \u2208 {2000, 2010}). We further pre-process the data by creating an additional, binned version of each population raster, where a cell takes on a value representing which bin its population count falls in. Specifically, we create matrices Ct, where an entry C i,j t = 0 if 0 \u2264 P i,j t < 1, 1 if 2\n1 \u2264 P i,jt < 22, ..., k if 2k \u2264 P i,jt < 2k+1 where k \u2208 N. This process discretizes the target population values which simplifies our learning tasks by creating a classification problem. For C2000 the highest class value is k = 17, representing a cell that has a population in the range [65, 536, 131, 072). For the rest of the study, we will use these population class values instead of the raw population count values when discussing estimating population.\nLandsat 7 1-year composite data is available through Google Earth Engine for the years of 1999 through 20143. The 1-year composites are made by taking the median pixel values from a sample of the least cloudy images from the given year. We use data from the 2000 and 2010 sets, with bands 1 through 7, at a 15m2 resolution. This data is downsampled from the native resolution of 30m2 recorded by the Landsat 7 satellite using nearest neighbor interpolation. As a pre-processing step, for every 0.01\u25e6 \u00d7 0.01\u25e6 cell in the population matrices, we take the grid of Landsat imagery that it covers. We resize the grid of Landsat imagery covered by a single population cell into a square volume with a height and width of 74 pixels, as the number of actual satellite imagery pixels that cover a 0.01\u25e6\u00d70.01\u25e6 area will vary with latitude. We choose a height and width of 74, because at a latitude of 45\u25e6N (approximately the center of the US), a 0.01\u25e6\u00d7 0.01\u25e6 cell is \u2248 1, 111m2, and with a height and width of 74 pixels of 15x15 meters, our satellite images will represent a similarly sized 1, 110m2 area. We let the grids of Landsat images be represented as \u03b8t, where by for every P i,j t cell from the population matrices, we have an associated satellite image volume, \u03b8i,jt \u2208 Z 74\u00d774\u00d77 + .\nThe county level population data from the US Census includes the ground truth population values for each county in 2000, and 2010, the postcensal population estimates for each county in 2010, and the ACS 5-year 2006-2010 population estimates for each county in 2010. We use this data evaluate our models\u2019 aggregate estimates, and refer to the ground truth 2010 county population counts as \u201cActual 2010\u201d in Section 4."}, {"heading": "3.2 Model Architecture", "text": "We experimented with different CNN architectures and hyperparameters using training and validation sets sampled from the 2000 datasets over a 1\u25e6\u00d71\u25e6 area in the southeast United States. Our assumption is that a model architecture/hyperparameter set which can perform well on this subset of the entire US will be able to perform equally well throughout the entire study area. The training and validation set sampling was performed through the methodology described in Section 3.3.\nWe considered the 5 well-known \u2018VGG\u2019 model architectures, VGG-A through VGG-E from (3), and variations of each of the 5 VGG architectures that included dropout and batch normalization layers. We adapt the VGG architectures to use our input images of size (74,74,7). Since we have discretized our target values into 17 different classes, we resize the output layer to 17 and use a softmax activation function. For all experiments we use a batch size of 512 samples, the Adam optimization method (34) from the Python Keras library (35) (with default parameters), the categorical cross entropy loss function, and we train all networks for 30 epochs (with consideration\n3Google Earth Engine: https://earthengine.google.com/\nto overfitting through observing the training/validation loss curves). We found that a VGG-A architecture results in the best top-1 and top-3 accuracy on both the training and validation sets over 30 training epochs and therefore use this architecture for the remainder of the study. See Figure 1 for a diagram showing the structure of our model. We chose 30 epochs as a cut off as the best models do not show any improvements in terms of validation loss after this point."}, {"heading": "3.3 Experimental Setup", "text": "Our study area consists of a 2, 499 by 5, 796 grid covering the continental United States that contains \u2248 8 million target values. As using all of these samples to train with presents a significant computational challenge, we divide up the study area into 15, 1, 000 by 1, 000 (1\u25e6 \u00d7 1\u25e6) chunks, and train an independent model for each chunk according to the methods described in Section 3.2. Recent work using random forest models for population mapping suggests that, \u201cmore accurate population maps can be produced by using regionally-parameterized models where more spatially refined data exists\u201d (24), which we follow with this methodology. Within each chunk we sample 1/10th of the available data to use as training samples, and 1/100th of the data to use as validation samples. As there is a class imbalance problem in the population data, with many more samples in the lower population classes than in the higher population classes, we perform a weighted sampling to select training and validation points. We let ci represent the number of points in class i over the entire training set, then the probability of selecting a point Ci,jt = x is given as 1\u2212 cx/ \u221117 i=1 ci. This sampling methodology serves to undersample the higher frequency classes more often than the lower frequency ones, while still resulting in a representative sample of all classes from the study area. Figure 2 shows the results of this sampling methodology.\nAn important component of any machine learning or modeling application is validating that the models are able to generalize well to unseen data, and that the models are able to make reasonable predictions. It is important to note that because there does not exist any true \u201cground truth\u201d gridded population data, it is not possible to truly evaluate population disaggregation techniques.\nAs the purpose of our models is to predict population values from only satellite imagery, they should (a) be able to make reasonable population predictions when compared to other population prediction techniques, (b) be interpretable, where population predictions are able to be explained in terms of semantic features of the input images, and (c) should have explainable errors. We address each of these three points in the following three paragraphs.\nWe first evaluate our results by comparing our model\u2019s aggregate population estimates at the county level with US Census Postcensal county level estimates for 2010 (POSTCENSAL) (1), and American Community Survey 5-year estimates for 2006-2010 (ACS5YR) (20) in terms of accuracy when evaluated against the actual 2010 Census (33). We convert our per grid cell population class predictions, C\u0302i,j , into county level population estimates, P\u0302 i,j , in two ways. The first method (CONVRAW), involves converting the class values directly into population values as described in Equation 1.\nP\u0302 i,j = { 0 C\u0302i,j = 0 1 2(2 C\u0302i,j\u22121 + 2C\u0302 i,j ) otherwise (1)\nThis formula is equivalent to predicting the middle point of each class bin as the population estimate. We sum the predicted population values for each cell whose centroid falls within a particular county to get the aggregate county predictions. The second method, (CONVAUG), uses the values from the softmax activations in the last layer of each CNN as \u201cfeatures\u201d into a secondary machine learning model. Specifically, the last layer of our CNN models has a width of 17, where the output values represent the probability that the input image belongs to each of the 17 population classes. We run our CNN models for each cell in the training dataset (covering the entire US), and record the output vector at each location. We aggregate the output vectors by county by summing the vectors\nof all pixels that are covered by each county. This process gives us a feature vector for each county which contains information about the composition of the population classes of the cells that make up that county. We then use these feature vectors to train a gradient boosting model to predict the ground truth county population values from the training set year. We perform the same process on the test set to create feature vectors with our trained CNN models and use the trained gradient boosting model to make county level population estimates. While this methodology is somewhat orthogonal to the main points of this paper, it shows how our trained CNN models can be used as a mechanism for feature extraction, and that the features the model learns are indeed valid signals of population numbers. We show the results from this county level evaluation in Section 4.1.\nAs described in the previous paragraph, for each input cell our model outputs a probability distribution over the possible population class values. Using this, we create maps that show the probability that each cell belongs to a given class. Similarly, we show which input images maximally activate every given output class. We show these interpretability results in Section 4.2\nFinally, we interpret the largest errors that our model makes. Because our model is limited to using satellite imagery data, it will become \u201cconfused\u201d in cases where there are signs of human settlements that do not manifest as populated in the census datasets. This confusion is evidence that our models are able to learn the higher-order features as to what constitutes \u201cpopulated areas\u201d, however do not have enough data to discriminate between different types of human activities. The results and discussion of this are shown in Section 4.3."}, {"heading": "4 Results and Discussion", "text": "Our results focus on validating the modeling methodology, and are broken down into three sections: evaluating how good our model\u2019s population estimates are when aggregated at the county level in Section 4.1, interpreting why our models make the predictions that they do in 4.2, and evaluating and explaining our model\u2019s per pixel errors when compared with ground truth in Section 4.3."}, {"heading": "4.1 County level Estimates", "text": "Here we compare 4 different methods for predicting county level population counts for the continental US in 2010. The four methods are as described in Section 3.3: POSTCENSAL, ACS5YR, CONVRAW, and CONVAUG. None of these methods contain information about the true population counts for the target year, 2010, therefore must infer the population either from detailed historical population and demographic data in the case of POSTCENSAL, supplemental survey information in the case of ACS5YEAR, or a combination of satellite and historical population data in the case of our methods CONVRAW and CONVAUG. We compare the predicted populations for all counties with each method to the ground truth population taken from the US 2010 Census and record the mean absolute error (Mean AE), median absolute error (Median AE), r2 score, and mean absolute percentage error (MAPE). The results for this comparison can be found in Table 1, and the per county errors for each method are visualized in Figure 3.\nThe two statistical methods used by the US Census provide more accurate predictions of county level population for 2010, and have lower median and mean absolute errors than our two methods. This result is expected, as the predictions made by these methods take many more historical features into account, while our methods only use the previous census\u2019 population counts and satellite imagery to make predictions. Our model\u2019s mean and median errors fall within an order of magnitude of the census model\u2019s errors, and our model\u2019s MAPE is similar to the ACS5YR results. We perform\nthis comparison to validate that our model\u2019s unaided population estimates are not wildly off, which suggests that our model is able to capture the true signal in determining population values from satellite imagery. Considering the evaluation of how well our model captures the locations of populations, we argue that because our aggregate estimates at the county level are not wildly off, our model\u2019s individual cell predictions must be approximately valid. Similar to population disaggregation methodology, our model\u2019s individual cell predictions will be the most accurate when they are scaled to match the true population value, or a trusted population estimate. While these county level estimates should not be used in place of the more accurate census estimation methods in the US, they could be used to create continuously updated population maps for developing countries that do not have the detailed data required to run population projection models."}, {"heading": "4.2 Prediction Interpretability", "text": "Interpretability is an important aspect of any modeling process. As we cover in Section 2, some population disaggregation methods rely on ad-hoc rules to assign the population of an administrative area to the grid cells that cover the same area. In some applications, the methods for determining these rules, or the rules themselves, are available, while in other products, such as Landsat (30; 31), the methodology is not public, and therefore, subsequent years of predictions are not comparable. Additionally, while some basic dasymetric heuristics, such as \u201chumans do not live on land where the slope is over 45\u25e6\u201d, can be globally applied, more detailed heuristics might be region specific. Our methodology bypasses these potential problems by generating the probability that a section of satellite imagery belongs to each population class, which allows us to show how confident our models are about a certain classification decision. Similarly, because our model only considers satellite imagery as input, all of the predictions made by our model will be able to be explained in terms of the features of the input image. We show these two components of our methodology in Figures 4 and 5 respectively.\nIn Figure 4 we show maps for several of the output population classes that show the estimated probability of each pixel belonging to the respective class. From these we observe that our model makes confident predictions about the 0 population class (Layer 0), and the higher population classes. The lack of confidence in the lower population classes (Layers 2 and 4) makes sense as we do not expect the visual difference between 1km2 areas in which 4 and 16 people live to be large. To compound this, census block geographies are larger in low population rural areas, meaning that our disaggregated \u201cground truth\u201d training data will be noisier in lower population areas. In Figure 5 we show, for each class, the top 8 satellite image inputs from the testing set, that maximize the softmax output for that class. These images give us an insight into what types of features our model is learning. There are clear patterns moving from the lower classes, which represent sparsely populated areas, to very the upper classes which represent more urbanized areas. In the lower classes, most of the images contain some sort of roadway or distinctively marked fields. In classes 6 through 9 there are several buildings and developments visible, while finally in classes 10 through 14 there are dense suburban and urban developments with gridded patterns visible."}, {"heading": "4.3 Prediction Errors", "text": "Here we show some of the errors of our model. Through inspecting the pixel class errors, i.e., the true population class value in 2010 (disaggregated from the Census population counts) minus the predicted population class values, we noticed that our model is systematically over-predicting some large areas. In Figure 6 we show three of these cases: Oak Ridge National Laboratory in\nOak Ridge, TN, Anniston Army Depot in Anniston, AL, and Walt Disney World in Orlando, FL. These locations all share the property of having many man-made structures and signals of human activity, without the \u201cground truth\u201d labeling of a population count from the Census data. Walt Disney World has many structures that look similar to those in high population residential areas, and therefore will always be mis-classified by a model that only relies on satellite imagery as input. In these cases, a traditional dasymetric modeling approach to disaggregating population will have an advantage over our model, as such an augmented approach could easily incorporate layers describing army bases, amusement parks, and other large spatial structures that will not have populations living within their borders. Finally, these observations are further evidence that our model is generalizing and learning useful semantic content about the input images with which to make its prediction."}, {"heading": "5 Future Work and Conclusion", "text": "Our goal in this work is to train convolutional neural networks to create high-resolution gridded population maps using only satellite imagery, then validate our model\u2019s predictions both quantitatively and qualitatively. We predict population counts in the continental US at a 0.01\u25e6 \u00d7 0.01\u25e6 (\u2248 1km2) resolution for 2010, after training on data from 2000. To evaluate and validate our models, we first aggregate the population predictions at the county level, and compare them to ground truth county population counts from the 2010 census. Our models perform well on the task of projecting county population, with the best model having a median absolute error of 4,642, and although they are not better than traditional county population projection methods used by the US Census, they are able to make reasonable predictions. Secondly, we show what the models have learned by creating maps that show the estimated probability of each cell belonging to a given class, and by visualizing the satellite image inputs for each class that our model is most confidently classifying. We observe that the most confident images for each class follow an expected pattern, whereby images of rural areas with small roads and fields are classified as low population cells, and gridded urban areas with dense housing are classified as high population cells. Finally we qualitatively explain some of the errors that our model is making in terms of noisy input data; for example, our model predicts that an army base in Anniston, Alabama is a high population area, even though the \u201cground truth\u201d census data says that the area is unpopulated.\nFor future work we plan on extending our current methodology in several different ways. In terms of the CNN training process, there are several changes and experiments that we would like to try: experimenting with different loss functions and loss function weighting schemes that could take the ordinal nature of our classification problem into account. Currently we optimize the categorical cross entropy, which will not discriminate between \u201csmall\u201d and \u201clarge\u201d errors, i.e., the loss will not penalize misclassifying a label with true class 11, as a 10, more than it would penalize misclassifying the 11 as a 1. We also would like to try training a model on the entire US; as this task has the potential to use over 8 million samples, this will bring entirely different challenges to the deep learning process. In terms of applying and evaluating the models, we would like to use these models to predict population counts in countries where censuses are not taken as often, and are not taken at as fine of a resolution as in the US. Similarly, we want to experiment with the trade-offs between ground truth data resolution and model accuracy to determine the limits of the applicability of these models. Finally, we would like to apply transfer learning methods to this problem such as investigating whether pre-training models on land-use classification tasks result in better predictions or whether directly predicting nighttime light intensities helps."}], "references": [{"title": "Postcensal population estimates: States, counties, and places", "author": ["John F Long"], "venue": "In Indirect Estimators in US Federal Programs,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Determining global population distribution: methods, applications and data", "author": ["DL Balk", "U Deichmann", "G Yetman", "F Pozzi", "SI Hay", "A Nelson"], "venue": "Advances in parasitology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1929}, {"title": "Transfer learning from deep features for remote sensing and poverty mapping", "author": ["Michael Xie", "Neal Jean", "Marshall Burke", "David Lobell", "Stefano Ermon"], "venue": "arXiv preprint arXiv:1510.00098,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Combining satellite imagery and machine learning to predict poverty", "author": ["Neal Jean", "Marshall Burke", "Michael Xie", "W Matthew Davis", "David B Lobell", "Stefano Ermon"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery", "author": ["Fan Hu", "Gui-Song Xia", "Jingwen Hu", "Liangpei Zhang"], "venue": "Remote Sensing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Towards better exploiting convolutional neural networks for remote sensing scene classification", "author": ["Keiller Nogueira", "Ot\u00e1vio AB Penatti", "Jefersson A dos Santos"], "venue": "Pattern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}, {"title": "Deepsat: a learning framework for satellite imagery", "author": ["Saikat Basu", "Sangram Ganguly", "Supratik Mukhopadhyay", "Robert DiBiano", "Manohar Karki", "Ramakrishna Nemani"], "venue": "In Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Using convolutional networks and satellite imagery to identify patterns in urban environments at a large scale", "author": ["Adrian Albert", "Jasleen Kaur", "Marta Gonzalez"], "venue": "arXiv preprint arXiv:1704.02965,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2017}, {"title": "Equitable development through deep learning: The case of sub-national population density estimation", "author": ["Patrick Doupe", "Emilie Bruzelius", "James Faghmous", "Samuel G Ruchman"], "venue": "In Proceedings of the 7th Annual Symposium on Computing for Development,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Tests of forecast accuracy and bias for county population projections", "author": ["Stanley K Smith"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1987}, {"title": "A survey of census bureau population projection methods", "author": ["John F Long", "David Byron McMillen"], "venue": "Climatic Change,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1987}, {"title": "Accuracy of the ratio-correlation method for estimating postcensal population", "author": ["Robert C Schmitt", "Albert H Crosetti"], "venue": "Land Economics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1954}, {"title": "A new short-term county population projection method", "author": ["David A Swanson", "Donald M Beck"], "venue": "Journal of Economic and Social Measurement,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "The american community survey", "author": ["Mark Mather", "Kerri L Rivers", "Linda A Jacobsen"], "venue": "Population Bulletin,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "A review of spatial population database design and modeling", "author": ["Uwe Deichmann"], "venue": "National Center for Geographic Information and Analysis,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "The accuracy of human population maps for public health application", "author": ["SI Hay", "AM Noor", "A Nelson", "AJ Tatem"], "venue": "Tropical Medicine & International Health,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "A framework for the areal interpolation of socioeconomic data", "author": ["Michael F Goodchild", "Luc Anselin", "Uwe Deichmann"], "venue": "Environment and planning A,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Exploring nationally and regionally defined models for large area population", "author": ["AE Gaughan", "Forrest R Stevens", "Catherine Linard", "Nirav N Patel", "Andrew J Tatem"], "venue": "mapping. International Journal of Digital Earth,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "High-resolution gridded population datasets for latin america and the caribbean", "author": ["Alessandro Sorichetta", "Graeme M Hornby", "Forrest R Stevens", "Andrea E Gaughan", "Catherine Linard", "Andrew J Tatem"], "venue": "Scientific data,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Disaggregating census data for population mapping using random forests with remotely-sensed and ancillary data", "author": ["Forrest R Stevens", "Andrea E Gaughan", "Catherine Linard", "Andrew J Tatem"], "venue": "PloS one,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Assessing the use of global land cover data for guiding large area population distribution modelling", "author": ["Catherine Linard", "Marius Gilbert", "Andrew J Tatem"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Taking advantage of the improved availability of census data: a first look at the gridded population of the world, version 4", "author": ["Erin Doxsey-Whitfield", "Kytt MacManus", "Susana B Adamo", "Linda Pistolesi", "John Squires", "Olena Borkovska", "Sandra R Baptista"], "venue": "Papers in Applied Geography,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "A new map of global urban extent from modis satellite data", "author": ["Annemarie Schneider", "Mark A Friedl", "David Potere"], "venue": "Environmental Research Letters,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Landscan: a global population database for estimating populations at risk", "author": ["Jerome E Dobson", "Edward A Bright", "Phillip R Coleman", "Richard C Durfee", "Brian A Worley"], "venue": "Photogrammetric engineering and remote sensing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}], "referenceMentions": [], "year": 2017, "abstractText": "Knowing where people live is a fundamental component of many decision making processes such as urban development, infectious disease containment, evacuation planning, risk management, conservation planning, and more. While bottom-up, survey driven censuses can provide a comprehensive view into the population landscape of a country, they are expensive to realize, are infrequently performed, and only provide population counts over broad areas. Population disaggregation techniques and population projection methods individually address these shortcomings, but also have shortcomings of their own. To jointly answer the questions of \u201cwhere do people live\u201d and \u201chow many people live there,\u201d we propose a deep learning model for creating high-resolution population estimations from satellite imagery. Specifically, we train convolutional neural networks to predict population in the USA at a 0.01\u25e6 \u00d7 0.01\u25e6 resolution grid from 1-year composite Landsat imagery. We validate these models in two ways: quantitatively, by comparing our model\u2019s grid cell estimates aggregated at a county-level to several US Census county-level population projections, and qualitatively, by directly interpreting the model\u2019s predictions in terms of the satellite image inputs. We find that aggregating our model\u2019s estimates gives comparable results to the Census county-level population projections and that the predictions made by our model can be directly interpreted, which give it advantages over traditional population disaggregation methods. In general, our model is an example of how machine learning techniques can be an effective tool for extracting information from inherently unstructured, remotely sensed data to provide effective solutions to social problems.", "creator": "LaTeX with hyperref package"}}}