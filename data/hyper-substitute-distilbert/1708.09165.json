{"id": "1708.09165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Tensor Networks for Dimensionality Reduction and Large-Scale Optimizations. Part 2 Applications and Future Perspectives", "abstract": "clause viii of flow chart sets here the introduction related vector networks and differential operations presented about part 36. it focuses purely tensor network or a super - precision higher - order representation of data / parameters and related cost functions, in providing an outline of their applications in machine learning applied learning analytics. taking particular emphasis is on the tensor train ( tt ) defining intrinsic tucker ( ht ) decompositions, whilst when physically precise interpretations commonly respect the principles of the tensor network approach. rather a dual approach, we also emphasize how, typically matching or proving underlying spatial - rank tensor approximations and matching contractions inside core objects, constraint networks have the ability to maximize distributed data whilst otherwise prohibitively large boundaries among data / parameters, consequently alleviating or not proving problems curse intrinsic dimensionality. the bearing on this notation is illustrated over what number of applied areas, including generalized regression and classification ( support tensor algorithms, dimensional correlation analysis, superior order vector least squares ), generalized eigenvalue decomposition, riemannian optimization, and in the optimization of virtual edge networks. between 1 and + 2 which empirical work does be used both as leave - alone performance resources, or also as a theoretically comprehensive catalogue of the expressive limitations of arbitrary - rank tensor representation via dimensional nodes.", "histories": [["v1", "Wed, 30 Aug 2017 08:37:36 GMT  (4847kb,D)", "http://arxiv.org/abs/1708.09165v1", "232 pages"]], "COMMENTS": "232 pages", "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["a cichocki", "a-h phan", "q zhao", "n lee", "i v oseledets", "m sugiyama", "d mandic"], "accepted": false, "id": "1708.09165"}, "pdf": {"name": "1708.09165.pdf", "metadata": {"source": "CRF", "title": "Tensor Networks for Dimensionality Reduction and Large-Scale Optimizations", "authors": ["A. Cichocki", "A-H. Phan", "Q. Zhao", "N. Lee", "I.V. Oseledets", "M. Sugiyama", "D. Mandic"], "emails": ["cia@brain.riken.jp", "phan@brain.riken.jp", "zhao@brain.riken.jp", "namgil.lee@riken.jp", "i.oseledets@skolkovotech.ru", "sugi@k.u-tokyo.ac.jp", "d.mandic@imperial.ac.uk"], "sections": [{"heading": null, "text": "Tensor Networks for Dimensionality Reduction and Large-Scale Optimizations\nPart 2 Applications and Future Perspectives 1\nA. Cichocki, A-H. Phan, Q. Zhao, N. Lee,\nI.V. Oseledets, M. Sugiyama, D. Mandic\nar X\niv :1\n70 8.\n09 16\n5v 1\n[ cs\n.N A\n] 3\n0 A\nug 2\n01 7\nAndrzej CICHOCKI Riken BSI, Japan,\nSkolkovo Institute of Science and Technology (Skoltech), Russia Systems Research Institute, Polish Academy of Science, Poland\ncia@brain.riken.jp\nAnh-Huy PHAN Riken BSI, Japan\nphan@brain.riken.jp\nQibin ZHAO Riken AIP, Japan\nzhao@brain.riken.jp\nNamgil LEE Kangwon National University\nKorea, namgil.lee@riken.jp\nIvan OSELEDETS Skolkovo Institute of Science and Technology (SKOLTECH), and\nInstitute of Numerical Mathematics of Russian Academy of Sciences, Russia\ni.oseledets@skolkovotech.ru\nMasashi SUGIYAMA Riken AIP, Japan\nUniversity of Tokyo sugi@k.u-tokyo.ac.jp\nDanilo P. MANDIC Imperial College, UK\nd.mandic@imperial.ac.uk\n1Copyright A.Cichocki et al. Please make reference to: A. Cichocki, A.H. Phan, Q. Zhao, N. Lee, I. Oseledets, and D.P. Mandic (2017), \u201cTensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 2 Applications and Future perspectives\u201d, Foundations and Trends in Machine Learning: Vol. 9: No. 6, pp 431-673.\nAbstract\nPart 2 of this monograph builds on the introduction to tensor networks and their operations presented in Part 1. It focuses on tensor network models for super-compressed higher-order representation of data/parameters and related cost functions, while providing an outline of their applications in machine learning and data analytics.\nA particular emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions, and their physically meaningful interpretations which reflect the scalability of the tensor network approach. Through a graphical approach, we also elucidate how, by virtue of the underlying low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volumes of data/parameters, thereby alleviating or even eliminating the curse of dimensionality.\nThe usefulness of this concept is illustrated over a number of applied areas, including generalized regression and classification (support tensor machines, canonical correlation analysis, higher order partial least squares), generalized eigenvalue decomposition, Riemannian optimization, and in the optimization of deep neural networks.\nPart 1 and Part 2 of this work can be used either as stand-alone separate texts, or indeed as a conjoint comprehensive review of the exciting field of low-rank tensor networks and tensor decompositions.\n1\nChapter 1\nTensorization and Structured Tensors\nThe concept of tensorization refers to the generation of higher-order structured tensors from the lower-order data formats (e.g., vectors, matrices or even low-order tensors), or the representation of very large scale system parameters in low-rank tensor formats. This is an essential step prior to multiway data analysis, unless the data itself is already collected in a multiway format; examples include color image sequences where the R, G and B frames are stacked into a 3rd-order tensor, or multichannel EEG signals combined into a tensor with modes, e.g., channel \u02c6 time \u02c6 epoch. For any given original data format, the tensorization procedure may affect the choice and performance of a tensor decomposition in the next stage.\nEntries of the so constructed tensor can be obtained through: i) a particular rearrangement, e.g., reshaping of the original data to a tensor, ii) alignment of data blocks or epochs, e.g., slices of a third-order tensor are epochs of multi-channel EEG signals, or iii) data augmentation through, e.g., Toeplitz and Hankel matrices/tensors. In addition, tensorization of fibers of a lower-order tensor will yield a tensor of higher order. A tensor can also be generated using transform-domain methods, for example, by a time-frequency transformation via the short time Fourier transform or wavelet transform. The latter procedure is most common for multichannel data, such as EEG, where, e.g., S channels of EEG are recorded over T time samples, to produce S matrices of F \u02c6 T dimensional time-frequency spectrograms stacked together into an F \u02c6 T \u02c6 S dimensional third-order tensor. A tensor can also represent the\n2\ndata at multi-scale and orientation levels by using, e.g., the Gabor, countourlet, or pyramid steerable transformations. When exploiting statistical independence of latent variables, tensors can be generated by means of higher-order statistics (cumulants) or by partial derivatives of the Generalised Characteristic Functions (GCF) of the observations. Such tensors are usually partially or fully symmetric, and their entries represent mutual interaction between latent variables. This kind of tensorization is commonly used in ICA, BSS and blind identification of a mixing matrix. In a similar way, a symmetric tensor can be generated through measures of distances between observed entities, or their information exchange. For example, a third-order tensor, created to analyse common structures spread over EEG channels, can comprise distance matrices of pair-wise correlation or other metrics, such as causality over trials. A symmetric third-order tensor can involve three-way similarities. For such a tensorization, symmetric tensor decompositions with nonnegativity constraints are particularly well-suited.\nTensorization can also be performed through a suitable representation of the estimated parameters in some low-rank tensor network formats. This method is often used when the number of estimated parameters is huge, e.g., in modelling system response in a nonlinear system, in learning weights in a deep learning network. In this way, computation on the parameters, e.g., multiplication, convolution, inner product, Fourier transform, can be performed through core tensors of smaller scale.\nOne of the main motivations to develop various types of tensorization is to take advantage of data super-compression inherent in tensor network formats, especially in quantized tensor train (QTT) formats. In general, the type of tensorization depends on a specific task in hand and the structure presented in data. The next sections introduce some common tensorization methods employed in blind source separation, harmonic retrieval, system identification, multivariate polynomial regression, and nonlinear feature extraction."}, {"heading": "1.1 Reshaping or Folding", "text": "The simplest way of tensorization is through the reshaping or folding operations, also known as segmentation [Bousse\u0301 et al., 2015, Debals and De Lathauwer, 2015]. This type of tensorization preserves the number of original data entries and their sequential ordering, as it only rearranges a vector to a matrix or tensor. Hence, folding does not require additional\n3\nmemory space. Folding. A tensor Y of size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN is considered a folding of a vector y of length I1 I2 \u00a8 \u00a8 \u00a8 IN , if\nY(i1, i2, . . . , iN) = y(i) , (1.1)\nfor all 1 \u010f in \u010f In, where i = 1 + \u0159N n=1(in \u00b4 1) \u015bn\u00b41\nk=1 Ik is a linear index of (i1, i2, . . . , i2).\nIn other words, the vector y is vectorization of the tensor Y, while Y is a tensorization of y.\nAs an example, the arrangement of elements in a matrix of size I\u02c6 L/I, which is folded from a vector y of length L is given by\nY =  y(1) y(I + 1) \u00a8 \u00a8 \u00a8 y(L\u00b4 I + 1) y(2) y(I + 2) \u00a8 \u00a8 \u00a8 y(L\u00b4 I + 2) ... ... . . . ...\ny(I) y(2I) \u00a8 \u00a8 \u00a8 y(L)  . (1.2) Higher-order folding/reshaping refers to the application of the folding procedure several times, whereby a vector y P RI1 I2\u00a8\u00a8\u00a8IN is converted into an Nth-order tensor of size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN . Application to BSS. It is important to notice that a higher-order folding (quantization) of a vector of length qN (q = 2, 3, . . .), sampled from an exponential function yk = azk\u00b41, yields an Nth-order tensor of rank 1. Moreover, wide classes of functions formed by products and/or sums of trigonometric, polynomial and rational functions can be quantized in this way to yield (approximate) low-rank tensor train (TT) network formats [Khoromskij, 2011a,b, Oseledets, 2012]. Exploitation of such low-rank representations allows us to separate the signals from a single or a few mixtures, as outlined below.\nConsider a single mixture, y(t), which is composed of J component signals, xj(t), j = 1, . . . , J, and corrupted by additive Gaussian noise, n(t), to give\ny(t) = a1x1(t) + a2x2(t) + \u00a8 \u00a8 \u00a8+ aJ xJ(t) + n(t). (1.3)\nThe aim is to extract the unknown sources (components) xj(t) from the observed signal y(t). Assume that higher-order foldings, Xj, of the component signals, xj(t), have low-rank representations in, e.g., the CP or Tucker format, given by\nXj = JGj; U (1) j , U (2) j , . . . , U (N) j K ,\n4\nor in the TT format\nXj = xxG (1) j , G (2) j , . . . , G (N) j yy,\nor in any other tensor network format. Because of the multi-linearity of this tensorization, the following relation between the tensorization of the mixture, Y, and the tensorization of the hidden components, Xj, holds\nY = a1X1 + a2X2 + \u00a8 \u00a8 \u00a8+ aJXJ + N , (1.4) where N is the tensorization of the noise n(t).\nNow, by a decomposition of Y into J blocks of tensor networks, each corresponding to a tensor network (TN) representation of a hidden component signal, we can find approximations of Xj and the separate component signals up to a scaling ambiguity. The separation method can be used in conjunction with the Toeplitz and Hankel foldings. Example 9 illustrates the separation of damped sinusoid signals."}, {"heading": "1.2 Tensorization through a Toeplitz/Hankel Tensor", "text": ""}, {"heading": "1.2.1 Toeplitz Folding", "text": "The Toeplitz matrix is a structured matrix with constant entries in each diagonal. Toeplitz matrices appear in many signal processing applications, e.g., through covariance matrices in prediction, estimation, detection, classification, regression, harmonic analysis, speech enhancement, interference cancellation, image restoration, adaptive filtering, blind deconvolution and blind equalization [Bini, 1995, Gray, 2006].\nBefore introducing a generalization of a Toeplitz matrix to a Toeplitz tensor, we shall first consider the discrete convolution between two vectors x and y of respective lengths I and L \u0105 I, given by\nz = x \u02da y . (1.5)\nNow, we can write the entries zI:L = [z(I), z(I + 1), . . . , z(L)] T in a linear algebraic form as\nzI:L =  y(I) y(I \u00b4 1) y(I \u00b4 2) \u00a8 \u00a8 \u00a8 y(1) y(I + 1) y(I) y(I \u00b4 1) \u00a8 \u00a8 \u00a8 y(2) y(I + 2) y(I + 1) y(I) \u00a8 \u00a8 \u00a8 y(3) ... ... ... . . .\n... y(L) y(L\u00b4 1) y(L\u00b4 2) \u00a8 \u00a8 \u00a8 y(J)\n  x(1) x(2) x(3)\n... x(I)  = YTx = Y \u00af\u0302 1x,\n5\nwhere J = L \u00b4 I + 1. With this representation, the convolution can be computed through a linear matrix operator, Y, which is called the Toeplitz matrix of the generating vector y. Toeplitz matrix. A Toeplitz matrix of size I \u02c6 J, which is constructed from a vector y of length L = I + J \u00b4 1, is defined as\nY = TI,J(y) =  y(I) y(I + 1) \u00a8 \u00a8 \u00a8 y(L) y(I \u00b4 1) y(I) \u00a8 \u00a8 \u00a8 y(L\u00b4 1) ... ... . . .\n... y(1) y(2) \u00a8 \u00a8 \u00a8 y(L\u00b4 I + 1)  . (1.6) The first column and first row of the Toeplitz matrix represent its entire generating vector.\nIndeed, all (L + I \u00b4 1) entries of y in the above convolution (1.5) can be expressed either by: (i) using a Toeplitz matrix formed from a zero-padded generating vector [0TI\u00b41, y T, 0TI\u00b41] T, with [yT, 0TI\u00b41] being the first row of this Toeplitz matrix, to give\nz = TI,L+I\u00b41([0TI\u00b41, yT, 0TI\u00b41]T)T x , (1.7)\nor (ii) through a Toeplitz matrix of the generating vector [0TL\u00b41, x T, 0TL\u00b41] T, to yield\nz = TL,L+I\u00b41([0TL\u00b41, xT, 0TL\u00b41]T)T y . (1.8)\nThe so expanded Toeplitz matrix is a circulant matrix of [yT, 0TI\u00b41] T.\nConsider now a convolution of three vectors, x1, x2 and y of respective lengths I1, I2 and (L \u011b I1 + I2), given by\nz = x1 \u02da x2 \u02da y .\nFor its implementation, we first construct a Toeplitz matrix, Y, of size I1 \u02c6 (L\u00b4 I1 + 1) from the generating vector y. Then, we use the rows Y(k, :) to generate Toeplitz matrices, Yk of size I2\u02c6 I3. Finally, all I1 Toeplitz matrices, Y1, . . . , YI1 , are stacked as horizontal slices of a third-order tensor Y, i.e., Y(k, :, :) = Yk, k = 1, . . . , I1. It can be verified that entries [z(I1 + I2 \u00b4 1), . . . , z(L)]T can be computed as z(I1 + I2 \u00b4 1)...\nz(L)\n = [x1 \u02da x2 \u02da y]I1+I2\u00b41:L = Y \u00af\u0302 1 x1 \u00af\u0302 2 x2. 6\nThe tensor Y is referred to as the Toeplitz tensor of the generating vector y. Toeplitz tensor. An Nth-order Toeplitz tensor of size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN , which is represented by Y = TI1,...,IN (y), is constructed from a generating vector y of length L = I1 + I2 + \u00a8 \u00a8 \u00a8+ IN \u00b4 N + 1, such that its entries are defined as\nY(i1, . . . , iN\u00b41, iN) = y(i\u03041 + \u00a8 \u00a8 \u00a8+ i\u0304N\u00b41 + iN) , (1.9)\nwhere i\u0304n = In \u00b4 in. An example of the Toeplitz tensor is illustrated in Figure 1.1.\nExample 1 Given a 3\u02c6 3\u02c6 3 dimensional Toeplitz tensor of a sequence 1, 2, . . . , 7, the horizontal slices are Toeplitz matrices of sizes 3\u02c6 3 given by\nT3,3,3(1, . . . , 7) =  T3,3(3, . . . , 7)T3,3(2, . . . , 6) T3,3(1, . . . , 5)  =   5 6 74 5 6 3 4 5   4 5 63 4 5 2 3 4   3 4 52 3 4\n1 2 3\n  .\n7\nRecursive generation. An Nth-order Toeplitz tensor of a generating vector y is of size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN , can be constructed from an (N \u00b4 1)th-order Toeplitz tensor of size I1\u02c6 I2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 (IN\u00b41 + IN \u00b4 1) of the same generating vector, by a conversion of mode-(N \u00b4 1) fibers to Toeplitz matrices of size IN\u00b41 \u02c6 IN .\nFollowing the definition of the Toeplitz tensor, the convolution of (N \u00b4 1) vectors, xn of respective lengths In, and a vector y of length L, can be represented as a tensor-vector product of an Nth-order Toeplitz tensor and vectors xn, that is\n[x1 \u02da x2 \u02da \u00a8 \u00a8 \u00a8 \u02da xN\u00b41 \u02da y]J:L = Y \u00af\u0302 1 x1 \u00af\u0302 2 x2 \u00a8 \u00a8 \u00a8 \u00af\u0302 N\u00b41 xN\u00b41 ,\nwhere Y = TI1,...,IN\u00b41,L\u00b4J(y) is a Toeplitz tensor of size I1\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN\u00b41\u02c6 (L\u00b4 J) generated from y, and J =\n\u0159N\u00b41 n=1 In \u00b4 N + 1, or\nx1 \u02da x2 \u02da \u00a8 \u00a8 \u00a8 \u02da xN\u00b41 \u02da y = rY \u00af\u0302 1 x1 \u00af\u0302 2 x2 \u00a8 \u00a8 \u00a8 \u00af\u0302 N\u00b41 xN\u00b41 ,\nwhere rY = TI1,...,IN\u00b41,L+J([0TJ , yT, 0TJ ]T) is a Toeplitz tensor, of the zeropadded vector of y, is of size I1 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN\u00b41 \u02c6 (L + J)."}, {"heading": "1.2.2 Hankel Folding", "text": "The Hankel matrix and Hankel tensor have similar structures to the Toeplitz matrix and tensor and can also be used as linear operators in the convolution. Hankel matrix. An I \u02c6 J Hankel matrix of a vector y, of length L = I + J \u00b4 1, is defined as\nY = HI,J(y) =  y(1) y(2) \u00a8 \u00a8 \u00a8 y(J) y(2) y(3) \u00a8 \u00a8 \u00a8 y(J + 1) ... ... . . . ...\ny(I) y(I + 1) \u00a8 \u00a8 \u00a8 y(L)  . (1.10) Hankel tensor. [Papy et al., 2005] An Nth-order Hankel tensor of size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN , which is represented by Y = HI1,...,IN (y), is constructed from a generating vector y of length L = \u0159\nn In \u00b4 N + 1, such that its entries are defined as\nY(i1, i2, . . . , iN) = y(i1 + i2 + \u00a8 \u00a8 \u00a8+ iN \u00b4 N + 1) . (1.11)\nRemark 1 (Properties of a Hankel tensor)\n8\n\u2022 The generating vector y can be reconstructed by a concatenation of fibers of the Hankel tensor Y(I1, . . . , In\u00b41, :, 1, . . . , 1), where n = 1, . . . , N \u00b4 1, and\ny =  Y(1 : I1 \u00b4 1, 1, . . . , 1) ... Y(I1, . . . , In\u00b41, 1 : In \u00b4 1, 1, . . . , 1)\n... Y(I1, . . . , IN\u00b41, 1 : IN)\n . (1.12)\n\u2022 Slices of a Hankel tensor Y, i.e., any subset of the tensor produced by fixing (N \u00b4 2) indices of its entries and varying the two remaining indices, are also Hankel matrices.\n\u2022 An Nth-order Hankel tensor, HI1,...,IN\u00b41,IN (y), can be constructed from an (N \u00b4 1)th-order Hankel tensor HI1,...,IN\u00b42,IN\u00b41+IN\u00b41(y) of size I1 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN\u00b42 \u02c6 (IN\u00b41 + IN \u00b4 1) by converting its mode-(N \u00b4 1) fibers to Hankel matrices of size IN\u00b41 \u02c6 IN .\n\u2022 Similarly to the Toeplitz tensor, the convolution of (N \u00b4 1) vectors, xn of lengths In, and a vector y of length L, can be represented as\n[x1 \u02da x2 \u02da \u00a8 \u00a8 \u00a8 \u02da xN\u00b41 \u02da y]J:L = Y \u00af\u0302 1 x\u03031 \u00af\u0302 2 x\u03032 \u00a8 \u00a8 \u00a8 \u00af\u0302 N\u00b41 x\u0303N\u00b41,\nor\nx1 \u02da x2 \u02da \u00a8 \u00a8 \u00a8 \u02da xN\u00b41 \u02da y = rY \u00af\u0302 1 x\u03031 \u00af\u0302 2 x\u03032 \u00a8 \u00a8 \u00a8 \u00af\u0302 N\u00b41 x\u0303N\u00b41,\nwhere x\u0303n = [xn(In), . . . , xn(2), xn(1)], J = \u0159 n In \u00b4 N + 1, Y = HI1,...,IN\u00b41,L\u00b4J(y) is the Nth-order Hankel tensor of y, whereas rY = HI1,...,IN\u00b41,L+J([0TJ , yT, 0TJ ]T) is the Hankel tensor of a zero-padded version of y.\n\u2022 A Hankel tensor with identical dimensions In = I, for all n, is a symmetric tensor.\nExample 2 A 3\u02c63\u02c63 \u2013 dimensional Hankel tensor of a sequence 1, 2, . . . , 7 is a symmetric tensor, and is given by\nH3,3,3(1 : 7) =  1 2 32 3 4 3 4 5  ,  2 3 43 4 5 4 5 6  ,  3 4 54 5 6 5 6 7  .\n9"}, {"heading": "1.2.3 Quantized Tensorization", "text": "It is important to notice that the tensorizations into the Toeplitz and Hankel tensors typically enlarge the number of data samples (in the sense that the number of entries of the corresponding tensor is larger than the number of original samples). For example, when the dimensions In = 2 for all n, the so generated tensor to be a quantized tensor of order (L \u00b4 1), while the number of entries of a such tensor increases from the original size L to 2L\u00b41. Therefore, quantized tensorizations are suited to analyse signals of short-length, especially in multivariate autoregressive modelling."}, {"heading": "1.2.4 Convolution Tensor", "text": "Consider again the convolution x \u02da y of two vectors of respective lengths I and L. We can then rewrite the expression for the entries-(I, I + 1, . . . , L) as\n[x \u02da y]I:L = C \u00af\u0302 1 x \u00af\u0302 3 y ,\nwhere C is a third-order tensor of size I \u02c6 J \u02c6 L, J = L\u00b4 I + 1, for which the (l \u00b4 I)-th diagonal elements of l-th slices are ones, and the remaining entries are zeros, for l = 1, 2, . . . , L. For example, the slices C(:, :, l), for l \u010f I, are given by\nC(:, :, l) =   0 0 1 . . .\n. . . 0 1 0\nl\n.\nThe tensor C is called the convolution tensor. Illustration of a convolution tensor of size I \u02c6 I \u02c6 (2I \u00b4 1) is given in Figure 1.2.\nNote that a product of this tensor with the vector y yields the Toeplitz matrix of the generating vector y, which is of size I \u02c6 J, in the form\nC \u00af\u0302 3 y = TI,J(y) ,\nwhile the tensor-vector product C \u00af\u0302 1x yields a Toeplitz matrix of the generating vector [0TL\u00b4I , x T, 0TJ\u00b41] T, or a circulant matrix of [0TL\u00b4I , x T]T\nC \u00af\u0302 1 x = TL,J([0TL\u00b4I , xT, 0TJ\u00b41]T) .\n10\nIn general, for a convolution of (N \u00b4 1) vectors, x1, . . . , xN\u00b41, of respective lengths I1, . . . , IN\u00b41 and a vector y of length L\nz = x1 \u02da x2 \u02da \u00a8 \u00a8 \u00a8 \u02da xN\u00b41 \u02da y , (1.13)\nthe entries of z can be expressed through a multilinear product of a convolution tensor, C, of (N + 1)th-order and size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN \u02c6 L, IN = L\u00b4 \u0159N\u00b41 n=1 In + N \u00b4 1, and the N input vectors\nzL\u00b4IN+1:L = C \u00af\u0302 1 x1 \u00af\u0302 2 x2 \u00a8 \u00a8 \u00a8 \u00af\u0302 N\u00b41 xN\u00b41 \u00af\u0302 N+1 y . (1.14)\nMost entries of C are zeros, except for those located at (i1, i2, . . . , iN+1), such that\nN\u00b41 \u00ff\nn=1\ni\u0304n + iN \u00b4 iN+1 = 0 , (1.15)\nwhere i\u0304n = In \u00b4 in, in = 1, 2, . . . , In.\n11\nThe tensor product C \u00af\u0302 N+1 y yields the Toeplitz tensor of the generating vector y, shown below\nC \u00af\u0302 N+1 y = TI1,...,IN (y). (1.16)"}, {"heading": "1.2.5 QTT Representation of the Convolution Tensor", "text": "An important property of the convolution tensor is that it has a QTT representation with rank no larger than the number of inputs vectors, N. To illustrate this property, for simplicity, we consider an Nth-order Toeplitz tensor of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I generated from a vector of length (N I\u00b4N + 1), where I = 2D. The convolution tensor of this Toeplitz tensor is of (N + 1)th-order and of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I \u02c6 (N I \u00b4 N + 1). Zero-padded convolution tensor. By appending (N \u00b4 1) zero tensors of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I before the convolution tensor, we obtain an (N + 1)thorder convolution tensor, C, of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I \u02c6 IN. QTT representation. The zero-padded convolution tensor can be represented in the following QTT format\nC = rC (1) |b| rC (2) |b| \u00a8 \u00a8 \u00a8 |b| rC (D) |b| rC (D+1) , (1.17)\nwhere \u201c |b| \u201d represents the strong Kronecker product between block tensors1 rC (n) = [rC (n) r,s ] defined from the (N + 3)th-order core tensors C (n) as rC (n) r,s = C\n(n)(r, :, . . . , :, s). The last core tensor C(D+1) represents an exchange (backward identity) matrix of size N \u02c6 N which can represented as an (N + 3)th-order tensor of size N \u02c6 1\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 1\u02c6 N \u02c6 1. The first D core tensors C(1), C(2), . . . , C(D) are expressed based on the so-called elementary core tensor S of size N \u02c6 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 looooooomooooooon\n(N + 1) dimensions\n\u02c6N, as\nC(1) = S(1, :, . . . , :), C(2) = \u00a8 \u00a8 \u00a8 = C(D) = S . (1.18)\nThe rigorous definition of the elementary core tensor is provided in Appendix 3.\n1A \u201cblock tensor\u201d represents a multilevel matrix, the entries of which are matrices or tensors.\n12\nTable 1.1 provides ranks of the QTT representation for various order of convolution tensors. The elementary core tensor S can be further reexpressed in a (tensor train) TT-format with (N + 1) sparse TT cores, as\nS = xxG(1), G(2), . . . , G(N+1)yy ,\nwhere G(k) is of size (N + k \u00b4 1)\u02c6 2\u02c6 (N + k), for k = 1, . . . , N, and the last core tensor G(N+1) is of size 2N \u02c6 2\u02c6 N.\nExample 3 Convolution tensor of 3rd-order. For the vectors x of length 2D and y of length (2D+1 \u00b4 1), the expanded convolution tensor has size of 2D \u02c6 2D \u02c6 2D+1. The elementary core tensor S is then of size 2\u02c6 2\u02c6 2\u02c6 2\u02c6 2 and its sub-tensors, S(i, :, :, :, :), are given in a 2\u02c6 2 block form of the last two indices through four matrices, S1, S2, S3 and S4, of size 2\u02c6 2, that is\nS(1, :, :, :, :) = [\nS1 S3 S2 S4\n] , S(2, :, :, :, :) = [ S2 S4 S3 S1 ] ,\nwhere S1 = [\n1 0 0 1\n] , S2 = [ 0 1 0 0 ] , S3 = [ 0 0 0 0 ] , S4 = [ 0 0 1 0 ] .\nThe convolution tensor can then be represented in a QTT format of rank-2 [Kazeev et al., 2013] with core tensors C(2) = \u00a8 \u00a8 \u00a8 = C(D) = S,\nC(1) = S(1, :, :, :, :), and the last core tensor C(D+1) = [\n0 1 1 0\n] which is\nof size 2\u02c6 1\u02c6 1\u02c6 2\u02c6 1. This QTT representation is useful to generate a\n13\n12 Tensorization and Structured Tensors\nR RR\n2\n2 2 2 2\nR\nI1 J1 I2 J2 ID JD\nI D\nJ D\nI 1\nJ 1\nR2\nI2\nJ 2\nD+1D21\n1 R2 2 R2 D\n2 2 2\nT (y)\nC\ny\nC(1) C(2) C(D) C(D+1)\nY(1) Y(2) Y(D) Y(D+1)\nT(1) T(2) T(D)\nFigure 1.3: Representation of the convolution tensor in QTT format. (Top) Illustration of a convolution tensor of size I \u00d7 I \u00d7 2I in a QTT format, where I = 2D, with the first core tensor of size 1 \u00d7 2 \u00d7 2 \u00d7 2 \u00d7 2, the last core tensor represents a backward identity matrix, and the remaining (D \u2212 1) core tensors are identical and of size 2 \u00d7 2 \u00d7 2 \u00d7 2 \u00d7 2 \u00d7 2; (Centre) a vector y of length 2D+1 in a QTT format; (Bottom) Generation of the Toeplitz matrix, T (y), of the vector y from the convolution tensor, and its representation in the QTT format, Id = Jd = 2 for d = 1, . . . , D.\ntensors1 C\u0303(n) = [C\u0303(n)r,s ] defined from the (N + 3)th-order core tensors C(n) as C\u0303(n)r,s = C(n)(r, :, . . . , :, s). The last core tensorC(D+1) represents an exchange (backward identity) matrix of size N \u00d7 N but as an (N + 3)th-order tensor of size N \u00d7 1\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 1\u00d7N \u00d7 1. The first D core tensors C(1), C(2), . . . , C(D) are expressed based on the so-called elementary core tensor S of size N \u00d7 2\u00d7 2\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 2\ufe38 \ufe37\ufe37 \ufe38\n(N + 1) dimensions\n\u00d7N\nC(1) = S(1, :, . . . , :), C(2) = \u00b7 \u00b7 \u00b7 = C(D) = S . (1.16)\nDefinition of the elementary core tensor is provided in Appendix 1.12.3. In Table 1.1, we provide ranks of the QTT representation of some\nconvolution tensors. The elementary core tensor S can even be further 1A \u201cblock tensor\u201d represents a multilevel matrix, entries of which are matrices or tensors.\nFigure 1.3: Representation of the convolution tensor in QTT format. (Top) Distributed representation of a convolution tensor C of size I \u02c6 J \u02c6 2I in a QTT format, where I = J = 2D. The first core tensor C(1) is of size 1\u02c6 2\u02c6 2\u02c6 2\u02c6 2, the last core tensor C(D+1) represents a backward identity matrix, and the remaining 5th-order core tensors of size 2\u02c6 2\u02c6 2\u02c6 2\u02c6 2 are identical. A vector y is of length 2D+1 in a QTT format. (Bottom) Generation of the Toeplitz matrix, T (y), of the vector y from the convolution tensor and its represent tion n th QTT format, Id = Jd = 2 for d = 1, . . . , D.\nToeplitz matrix when its generating vector is given in the QTT format. An illustration of the convolution tensor C is provided in Figure 1.3.\nExample 4 Convolution tensor of fourth-order. For the convolution tensor of fourth order, i.e., Toeplitz order N = 3, the elementary core tensor S is of size 3\u02c6 2\u02c6 2\u02c6 2\u02c6 2\u02c6 3, and is given in a 2\u02c6 3 block form of the last two indices as\nS(1, :, . . . , :) = [\nS1 S3 S5 S2 S4 S6\n] , S(2, :, . . . , :) = [ S2 S4 S6 S5 S1 S3 ] ,\nS(3, :, . . . , :) = [\nS5 S1 S3 S6 S2 S4\n] .\n14\nwhere Sn are of size 2\u02c6 2\u02c6 2, S5, S6 are zero tensors, and\nS1 = [[ 1 0 0 0 ] [ 0 1 1 0 ]] , S2 = [[ 0 0 0 0 ] [ 1 0 0 0 ]] ,\nS3 = [[ 0 0 0 1 ] [ 0 0 0 0 ]] , S4 = [[ 0 1 1 0 ] [ 0 0 0 1 ]] .\nFinally, the zero-padded convolution tensor of size 2D \u02c6 2D \u02c6 2D \u02c6 3 \u00a8 2D has a QTT representation in (1.17) with C(1) = S(1, :, :, :, :, [1, 2]), C(2) = S([1, 2], :, :, :, :, :), C(3) = \u00a8 \u00a8 \u00a8 = C(D) = S, and the last core tensor CD+1 = 0 0 10 1 0\n1 0 0  which is of size 3\u02c6 1\u02c6 1\u02c6 3\u02c6 1."}, {"heading": "1.2.6 Low-rank Representation of Hankel and Toeplitz Matrices/Tensors", "text": "The Hankel and Toeplitz foldings are multilinear tensorizations, and can be applied to the BSS problem, as in (1.4). When the Hankel and Toeplitz tensors of the hidden sources are of low-rank in some tensor network representation, the tensor of the mixture is expressed as a sum of low rank tensor terms.\nFor example, the Hankel and Toeplitz matrices/tensors of an exponential function, vk = azk\u00b41, are rank-1 matrices/tensors, and consequently Hankel matrices/tensors of sums and/or products of exponentials, sinusoids, and polynomials will also be of low-rank, which is equal to the degree of the function being considered.\nHadamard Product. More importantly, when Hankel/Toeplitz tensors of two vectors u and v have low-rank CP/TT representations, the Hankel/Toeplitz tensor of their element-wise product, w = uf v, can also be represented in the same CP/TT tensor format\nH(u)fH(v) = H(uf v) T (u)f T (v) = T (uf v) .\nThe CP/TT rank of H(uf v) or T (uf v) is not larger than the product of the CP/TT ranks of the tensors of u and v.\nExample 5\n15\nThe third-order Hankel tensor of u(t) = sin(\u03c9t) is a rank-3 tensor, and the third-order Hankel tensor of v(t) = t is of rank-2; hence the Hankel tensor of the w(t) = t sin(\u03c9t) has at most rank-6. Symmetric CP and Vandermonde decompositions. It is important to notice that a Hankel tensor Y of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I can always be represented by a symmetric CP decomposition\nY = I\u02c61 A\u02c62 A \u00a8 \u00a8 \u00a8 \u02c6N A .\nMoreover, the tensor Y also admits a symmetric CP decomposition with Vandermonde structured factor matrix [Qi, 2015]\nY = diagN(\u03bb)\u02c61 V T \u02c62 VT \u00a8 \u00a8 \u00a8 \u02c6N VT , (1.19)\nwhere \u03bb comprises R non-zero coefficients, and V is a Vandermonde matrix generated from R distinct values v = [v1, v2, . . . , vR]\nV =  1 v1 v21 . . . v I\u00b41 1 1 v2 v22 . . . v I\u00b41 2 ... ... ... . . .\n... 1 vR v2R . . . v I\u00b41 R  . (1.20) By writing the decomposition in (1.19) for the entries Y(I1, . . . , In\u00b41, : , 1, . . . , 1) (see (1.12)), the Vandermonde decomposition of the Hankel tensor Y becomes a Vandermonde factorization of y [Chen, 2016], given by\ny =  1 1 . . . 1 v1 v2 . . . vR v21 v 2 2 . . . v 2 R ... ... . . . ...\nvL\u00b411 v L\u00b41 2 . . . v L\u00b41 R\n \u03bb .\nObserve that various Vandermonde decompositions of the Hankel tensors of the same vector y, but of different tensor orders N, have the same generating Vandermonde vector v. Moreover, the Vandemonde rank, i.e, the minimum of R in the decomposition (1.19), therefore cannot exceed the length L of the generating vector y.\nQTT representation of Toeplitz/Hankel tensor. As mentioned previously, the zero-padded convolution tensor of (N + 1)th-order can be represented\n16\nin a QTT format of rank of at most N. Hence, if a vector y of length 2D N has a QTT representation of rank-(R1, . . . , RD), given by\ny = rY(1) |b| rY(2) |b| \u00a8 \u00a8 \u00a8 |b| rY(D+1) , (1.21)\nwhere rY(d) is an Rd\u00b41\u02c6Rd block matrix of the core tensor Y(d) of size Rd\u00b41\u02c6 2\u02c6 Rd, for d = 1, . . . , D, or of Y(D+1) of size RD \u02c6 N \u02c6 1, then following the relation between the convolution tensor and the Toeplitz tensor of the generating vector y, we have\nT (y) = C \u00af\u0302 N+1 y . (1.22)\nThis Nth-order Toeplitz tensor can also be represented by a QTT tensor with rank of at most N(R1, . . . , RD), as\nT (y) = rT(1) |b| rT(2) |b| \u00a8 \u00a8 \u00a8 |b| rT(D) , (1.23)\nwhere rT (d)\nis a block tensor of the core tensor T(d). The core T(1) is of size 1 \u02c6 2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 \u02c6 N R1, and cores T(2), . . . , T(D\u00b41) are of size NRd\u00b41 \u02c6 2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 \u02c6 NRd, while the last core tensor T(D) is of size NRD\u00b41 \u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2\u02c6 1. These core tensors are core contractions between the two core tensors C(d) and Y(d). Figure 1.3 illustrates the generation of a Toeplitz matrix as a tensor-vector product of a third-order convolution tensor C and a generating vector, x, of length 2D+1, both in QTT-formats. The core tensors of C are given in Example 3.\nRemarks:\n\u2022 Because of zero-padding within the convolution tensor, the Toeplitz tensor of y, generated in (1.22) and (1.23), takes only entries y(N), y(N + 1), . . . , y(2D N), i.e., it corresponds to the Toeplitz tensor of the generating vector y(N), y(N + 1), . . . , y(2D N).\n\u2022 The Hankel tensor also admits a QTT representation in the similar form to a Toeplitz tensor (cf. (1.23)).\n\u2022 Low-rank TN representation of the Toeplitz and Hankel tensors has been exploited, e.g., in blind source separation and harmonic retrieval. By verifying a low-rank TN representation of the signal in hand, we can confirm the existence of a low-rank TN representation of Toeplitz/Hankel tensors of the signal.\n17\n\u2022 QTT rank of the Toeplitz tensor in (1.23) is at most N times the QTT rank of the generating vector y. The rank may not be minimal. For example, the sinusoid signal is of rank-2 in QTT format, and its Toeplitz tensor also has a rank-2 QTT representation.\n\u2022 Fast convolution of vectors in QTT formats. A straightforward consequences is that when vectors xn are given in their QTT formats, their convolution x1 \u02da x2 \u02da \u00a8 \u00a8 \u00a8 \u02da xN can be computed through core contractions between the core tensors of the convolution tensor and those of the vectors."}, {"heading": "1.3 Tensorization by Means of Lo\u0308wner Matrix (Lo\u0308wner Folding)", "text": "A Lo\u0308wner matrix of a vector v P RI+J is formed from a function f (t) sampled at (I + J) distinct points tx1, . . . , xI , y1, . . . , yJu, to give\nv = [ f (x1), . . . , f (xI), f (y1), . . . , f (yJ)] T P RI+J ,\nso that the entries of v are partitioned into two disjoint sets, t f (xi)uIi=1 and t f (yj)uJj=1. The vector v is then converted into the Lo\u0308wner matrix, L P R\nI\u02c6J , defined by\nL = [\nf (xi)\u00b4 f (yj) xi \u00b4 yj ] ij P RI\u02c6J .\nLo\u0308wner matrices appear as a powerful tool in fitting a model to data in the form of a rational (Pade form) approximation, that is f (x) = A(x)/B(x). When considered as transfer functions, such type of approximations are much more powerful than the polynomial approximations, as in this way it is also possible to model discontinuities and spiky data. The optimal order of such a rational approximation is given by the rank of the Lo\u0308wner matrix. In the context of tensors, this allows us to construct a model of the original dataset which is amenable to higherorder tensor representation, has minimal computational complexity, and for which the accuracy is governed by the rank of the Lo\u0308wner matrix. An example of Lo\u0308wner folding of a vector [1/3, 1/4, 1/5, 1/6, 1/8, 1/9, 1/10] is\n18\ngiven below 1/3\u00b41/8 3\u00b48 1/3\u00b41/9 3\u00b49 1/3\u00b41/10 3\u00b410 1/4\u00b41/8 4\u00b48 1/4\u00b41/9 4\u00b49 1/4\u00b41/10 4\u00b410 1/5\u00b41/8\n5\u00b48 1/5\u00b41/9 5\u00b49 1/5\u00b41/10\n5\u00b410 1/6\u00b41/8\n6\u00b48 1/6\u00b41/9 6\u00b49 1/6\u00b41/10 6\u00b410\n = \u00b4 \n1/3 1/4 1/5 1/6  [1/8 1/9 1/10] . More applications of this tensorization can be found in [Debals et al., 2016a].\n1.4 Tensorization based on Cumulant and Derivatives of the Generalised Characteristic Functions\nThe use of higher-order statistics (cumulants) or partial derivatives of the Generalised Characteristic Functions (GCF) as a means of tensorization is useful in the identification of a mixing matrix in a blind source separation.\nConsider linear mixtures of R stationary sources, S, received by an array of I sensors in the presence of additive noise, N (see Figure 1.4 for a general principle). The task is to estimate a mixing matrix H P RI\u02c6R from only the knowledge of the noisy observations\nX = HS + N , (1.24)\nunder some mild assumptions, i.e., the sources are statistically independent and non-Gaussian, their number is known, and the matrix H has no pairwise collinear columns (see also [Comon and Rajih, 2006, Yeredor, 2000])\nA well-known approach to this problem is based on the decomposition of a high dimensional structured tensor, Y, generated from the observations, X, by means of partial derivatives of the second GCFs of the observations at multiple processing points. Derivatives of the GCFs. More specifically, we next show how to generate the tensor Y from the observation, X. We shall denote the first and second GCFs of the observations evaluated at a vector u of length I, respectively by\n\u03c6x(u) = E [ exp(uTx) ] , \u03a6x(u) = log \u03c6x(u) . (1.25)\nSimilarly, \u03c6s(v) and \u03a6s(v) designate the first and second GCFs of the sources, where v is of length R. Because the sources are statistically\n19\nindependent, the following holds\n\u03a6s(v) = \u03a6s1(v1) + \u03a6s2(v2) + \u00a8 \u00a8 \u00a8+ \u03a6sR(vR) , (1.26)\nwhich implies that Nth-order derivatives of \u03a6s(v) with respect to v result in Nth-order diagonal tensors of size R\u02c6 R\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 R, where N = 2, 3, . . ., that is\n\u03a8s(v) = BN\u03a6s(v) BvN = diagN \" dN\u03a6s1 dvN1 , dN\u03a6s2 dvN2 , . . . , dN\u03a6sR dvNR * . (1.27)\nIn addition, for the noiseless case x(t) = H s(t), and since \u03a6x(u) = \u03a6s(HTu), the Nth-order derivative of \u03a6x(u) with respect to u yields a symmetric tensor of Nth-order which admits a CP decomposition of rankR with N identical factor matrices H, to give\n\u03a8x(u) = \u03a8s(H Tu)\u02c61 H\u02c62 H \u00a8 \u00a8 \u00a8 \u02c6N H . (1.28)\n20\nIn order to improve the identification accuracy, the mixing matrix H should be estimated as a joint factor matrix in decompositions of various derivative tensors, evaluated at distinct processing points u1, u2, . . . , uK. This is equivalent to a decomposition of an (N + 1)th-order tensor Y of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I \u02c6 K concatenated from the K derivative tensors as\nY(:, . . . , :, k) = \u03a8x(uk), k = 1, 2, . . . , K. (1.29)\nThe CP decomposition of the tensor Y can be written in form of\nY = I\u02c61 H\u02c62 H \u00a8 \u00a8 \u00a8 \u02c6N H\u02c6N+1 D , (1.30)\nwhere the last factor matrix D is of size K\u02c6 R, and each row comprises the diagonal of the symmetric tensor \u03a8s(HTuk).\nIn the presence of statistically independent, additive and stationary Gaussian noise, we can eliminate the derivatives of the noise terms in the derivative tensor \u03a8x(u) by subtracting any other derivative tensor \u03a8x(u\u0303), or by an average of derivative tensors. Estimation of Derivatives of GCF. In practice, the GCF of the observation and its derivatives are unknown, but can be estimated from the sample first GCF [Yeredor, 2000]. Detailed expression and the approximation of the derivative tensor \u03a8x(u) for some low orders N = 2, 3, . . . , 7, are given in Appendix 1. Cumulants. When the derivative is taken at the origin, u = [0, . . . , 0]T, the tensor K(N)x = \u03a8(N)x (0) is known as the Nth-order cumulant of x, and a joint diagonalization or the CP decomposition of higher-order cumulants is a well-studied method for the estimation of the mixing matrix H.\nFor the sources with symmetric probabilistic distributions, their oddorder cumulants, N = 3, 5, . . ., are zero, and the cumulants of the mixtures are only due to noise. Hence, a decomposition of such tensors is not able to retrieve the mixing matrix. However, the odd-order cumulant tensors can be used to subtract the noise term in the derivative tensors evaluated at other processing points.\nExample 6 Blind identification (BI) in a system of 2 mixtures and R binary signals.\nTo illustrate the efficiency of higher-order derivatives of the second GCF in blind identification we considered a system of two mixtures, I = 2, linearly composed by R signals of length T = 100\u02c6 2R, the entries of which can take the values 1 or \u00b41, i.e., sr,t = 1 or \u00b41. The mixing matrix H of size 2\u02c6 R was randomly generated, where R = 4, 6, 8. The signal-to-noise ratio\n21\nwas SNR = 20 dB. The main purpose of BI is to estimate the mixing matrix H.\nWe constructed 50 tensors Yi (i = 1, . . . , 50) of size R \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 R \u02c6 3, which comprise three derivative tensors evaluated at the two leading left singular vectors of X, and a unit-length processing point, generated such that its collinearity degree with the first singular vector uniformly distributed over a range of [\u00b40.99, 0.99]. The average derivative tensor was used to eliminate the noise term in Yi.\nCP decomposition of derivative tensors. The tensors Yi were decomposed by CP decompositions of rank-R to retrieve the mixing matrix H. The mean of Squared Angular Errors SAE(hr, h\u0302r) = \u00b420 log10 arccos( hTr h\u0302r |hr|2|h\u0302r|2\n) over all columns hr was computed as a performance index for one estimation of the mixing matrix.\nThe averages of the mean and best MSAEs over 100 independent runs for the number of the unknown sources R = 4, 6, 8 are plotted in Figure 1.5. The results indicate that with a suitably chosen processing point, the decomposition of the derivative tensors yielded good estimation of the mixing matrix. Of more importance is that higher-order derivative tensors, e.g., 7th and 8th orders, yielded better performance than lowerorder tensors, while the estimation accuracy deteriorated with the number of sources.\nCP decomposition of cumulant tensors. Because of symmetric pdfs, the odd order cumulants of the sources are zero. Only decompositions of cumulants of order 6 or 8 were able to retrieve the mixing matrix H. For all the test cases, better performances could be obtained by a decomposition of three derivative tensors.\nTensor train decomposition of derivative tensors. The estimation of the mixing matrix H can be performed in a two-stage decomposition\n\u2022 A tensor train decomposition of high-order derivative tensors, e.g., tensor order exceeds 5.\n\u2022 A CP decomposition of the tensor in TT-format, to retrieve the mixing matrix.\nExperimental results confirmed that the performances with prior TTdecomposition were more stable and yielded an approximately 2 dB higher mean SAE than those using only CP decomposition for derivative tensors of orders 7 and 8 and a relatively high number of unknown sources.\n22"}, {"heading": "1.4.1 Tensor Structures in Constant Modulus Signal Separation", "text": "Another method to generate tensors of relatively high order in BSS is through modelling modulus of the estimated signals as roots of a polynomial.\nConsider a linear mixing system X = HS with R sources of length K, and I mixtures, where the modulus of the sources S is drawn from a set of given moduli. For simplicity, we assume I = R. For example, the binary phase-shift keying (BPSK) signal in telecommunication consists of a sequence of 1 and \u00b41, hence, it has a constant modulus of unity. The quadrature phase shift keying (QPSK) signal takes one of the values \u02d81\u02d8 1i, i.e., it has a constant modulus ? 2. The 16-QAM signal has three squared moduli of 2, 10 and 18. For this BSS problem for single constant modulus signals, Lathauwer [2004] linked the problem to CP decomposition of a fourth-order tensor. For multi-constant modulus signals, Debals et al. [2016b] established a link to a coupled CP decomposition.\n23\nA common method to extract the original sources S is to use a demixing matrix W of size I \u02c6 R or a vector w of length I such that y = wTX is an estimate of one of the source signals. The constant modulus constraints require that each entry, |yk|, must be one of given moduli, c1, c2, . . . , cM. This means that for all entries of y the following holds\nf (yk) = M \u017a\nm=1\n(|yk|2 \u00b4 cm) = 0 . (1.31)\nIn other words, |yk|2 are roots of an Mth-degree polynomial, given by\npM + \u03b1m pM\u00b41 + \u00a8 \u00a8 \u00a8+ \u03b12 p + \u03b11 ,\nwith coefficients \u03b1M+1 = 1, and \u03b11, \u03b12, . . . , \u03b1M, given by\n\u03b1m = (\u00b41)m\u00b41 \u00ff\ni1,i2,...,im\nci1 ci2 \u00a8 \u00a8 \u00a8 cim . (1.32)\nBy expressing |yk|2 = (wbw\u02da)T(xk b x\u02dak ), and\n|yk|2m = (wbm b (wbm)\u02da)T(xbmk b (x bm k ) \u02da) ,\nwhere the symbol \u201c\u02da\u201d represents the complex conjugate, xbm = x b x b \u00a8 \u00a8 \u00a8 b x denotes the Kronecker product of m vectors x, and bearing in mind that the rank-1 tensors w\u02ddm = w \u02ddw \u02dd \u00a8 \u00a8 \u00a8 \u02ddw are symmetric, and in general have only (R+m\u00b41)!m!(R\u00b41)! distinct coefficients, the rank-1 tensors w \u02ddm \u02dd (w\u02ddm)\u02da\nhave at least ( (R+m\u00b41)! m!(R\u00b41)! )2 distinct entries. We next introduce the operator K which keeps only distinct entries of the symmetric tensor w\u02ddm \u02dd (w\u02ddm)\u02da or of the vector wbm b (wbm)\u02da. The constant modulus constraint of yk can then be rewritten as\nf (yk) = \u03b11 + M+1 \u00ff\nm=2\n\u03b1m(wbm b (wbm)\u02da)T(xbmk b (x bm k ) \u02da)\n= \u03b11 + M+1 \u00ff\nm=2\n\u03b1m(K(wbm b (wbm)\u02da))T diag(dm)K(xbmk b (x bm k ) \u02da)\n= \u03b11 + [ . . . , (K(wbm b (wbm)\u02da))T, . . . ] [. . . ,K(xbmk b (x bm k ) \u02da)T diag(\u03b1mdm), . . .]T ,\n24\nwhere dm(i) represents the number of occurrences of an entry of K(xbmk b (xbmk ) \u02da) in xbmk b (x bm k )\n\u02da. The vector of the constant modulus constraints of y is now given by\nf = [. . . , f (yk), . . .]T = \u03b111 + Qv , (1.33)\nwhere\nv =  ...\nK(wbm b (wbm)\u02da) ...\n , Q =  ... diag(\u03b1mdm)K(Xdm d (Xdm)\u02da)\n...\n T .\nThe constraint vector is zero for the exact case, and should be small for the noisy case. For the exact case, from (1.33) and f (yk+1) \u00b4 f (yk) = 0, this leads to\nLQv = 0 ,\nwhere L is the first-order Laplacian implying that the vector v is in the null space of the matrix Q\u0303 = LQ. The above condition holds for other demixing vectors w, i.e., Q\u0303V = 0, where V = [v1, . . . , vR], and each vr is constructed from a corresponding demixing vector wr.\nWith the assumption I = R, and that the sources have complex values, and the mixing matrix does not have collinear columns, it can be shown that the kernel of the matrix Q\u0303 has the dimension of R [Debals et al., 2016b]. Therefore, the basis vectors, zr, r = 1, . . . , R, of the kernel of Q\u0303 can be represented as linear combination of V, that is\nzr = V\u03bbr .\nNext we partition zr into M parts, zr = [zrm], each of the length( (R+m\u00b41)! m!(R\u00b41)! )2 , which can be expressed as\nzrm = R \u00ff\ns=1\n\u03bbrsK(wbms b (wbms )\u02da) = K ( R \u00ff\ns=1\n\u03bbrswbms b (wbms )\u02da ) ,\nthus implying that W and W\u02da are factor matrices of a symmetric tensor Zrm of (2m)th-order, constructed from the vector zrm, i.e., K(vec(Zrm)) = zrm, in the form\nZrm = Jdiag2m(\u03bbr); W, . . . , Wloooomoooon m terms , W\u02da, . . . , W\u02da loooooomoooooon m terms K . (1.34)\n25\nBy concatenating all R tensors Z1m, . . . , ZRm into one (2m + 1)th-order tensor Zm, the above R CP decompositions become\nZm = JI; W, . . . , Wloooomoooon m terms , W\u02da, . . . , W\u02da loooooomoooooon m terms , \u039bK . (1.35)\nAll together, the M CP decompositions of Z1, . . . , ZM form a coupled CP tensor decomposition to find the two matrices W and \u039b.\nExample 7 [Separation of QAM signals.] We performed the separation of two rectangular 32- or 64-QAM signals of length 1000 from two mixture signals corrupted by additive Gaussian noise with SNR = 15 dB. Columns of the real-valued mixing matrix had unit-length, and a pair-wise collinearity of 0.4. The 32-QAM signal had M = 5 constant moduli of 2, 10, 18, 26 and 34, whereas the 64-QAM signal had M = 9 squared constant moduli of 2, 10, 18, 26, 34, 50, 58, 74 and 98. Therefore, for the first case (32-QAM), the demixing matrix was estimated from 5 tensors of size 2 \u02c6 2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 and of respective orders 3, 5, 7, 9 and 11, while for the later case (64-QAM), we decomposed 9 quantized tensors of orders 3, 5, . . . , 19. The estimated QAM signals for the two cases were perfectly reconstructed with zero bit error rates. Scatter plots of the recovered signals are shown in Figure 1.6."}, {"heading": "1.5 Tensorization by Learning Local Structures", "text": "Different from the previous tensorizations, this tensorization approach generates tensors from local blocks (patches) which are similar or closely related. For the example of an image, given that the intensities of pixels in a small window are highly correlated, hidden structures which represent relations between small patches of pixels can be learnt in local areas. These structures can then be used to reconstruct the image as a whole in, e.g., an application of image denoising [Phan et al., 2016].\nFor a color RGB image Y of size I \u02c6 J \u02c6 3, each block of pixels of size h\u02c6w\u02c6 3 is denoted as\nYr,c = Y(r : r + h\u00b4 1, c : c + w\u00b4 1, :).\nA small tensor, Zr,c, of size h \u02c6 w \u02c6 3 \u02c6 (2d + 1) \u02c6 (2d + 1), comprising (2d + 1)2 blocks centered around Yr,c, with d denoting the neighbourhood width, can be constructed in the form\nZr,c(:, :, :, d + 1 + i, d + 1 + j) = Yr+i,c+j,\n26\nwhere i, j = \u00b4d, . . . , 0, . . . , d, as illustrated in Figure 1.7. Every (r, c)-th block Zr,c is then approximated through a constrained tensor decomposition\n}Zr,c \u00b4 Z\u0302r,c}2F \u010f \u03b52 , (1.36)\n27\nwhere the noise level \u03b52 can be determined by inspecting the coefficients of the image in the high-frequency bands. A pixel is then reconstructed as the average of all its approximations which cover that pixel.\nExample 8 Image denoising. The principle of tensorization from learning the local structures is next demonstrated in an image denoising application for the benchmark \u201cpeppers\u201d color image of size 256\u02c6 256\u02c6 3, which was corrupted by white Gaussian noise at SNR = 10 dB. Latent structures were learnt for patches of sizes 8\u02c6 8\u02c6 3 (i.e., h = w = 8) in the search area of width d = 3. To the noisy image, we applied the DCT spatial filtering before their block reconstruction. The results are shown in Figure 1.8, and illustrate the advantage of the tensor network approach over a CP decomposition approach."}, {"heading": "1.6 Tensorization based on Divergences, Similarities or Information Exchange", "text": "For a set of I data points xi, i = 1, 2, . . . , I, this type of tensorization generates an Nth-order nonnegative symmetric tensor of size I\u02c6 I\u02c6\u00a8 \u00a8 \u00a8\u02c6 I, the entries of which represent N-way similarities or dissimilarities between xi1 , xi2 , . . . , xiN , where in = 1, . . . , I, so that\nY(i1, i2, . . . , iN) = d(xi1 , xi2 , . . . , xiN ) . (1.37)\n28\nSuch metric function can express pair-wise distances between the two observations xi and xj. In a general case, d(xi1 , xi2 , . . . , xiN ) can compute the volume of a convex hull formed by N data points.\nThe so generated tensor can be expanded to (N + 1)th-order tensor, where the last mode expresses the change of data points over e.g., time or trials. Tensorizations based on divergences and similarities are useful for the analysis of interaction between observed entities, and for their clustering or classification."}, {"heading": "1.7 Tensor Structures in Multivariate Polynomial Regression", "text": "The Multivariate Polynomial Regression (MPR) is an extension of the linear and multilinear regressions which allows us to model nonlinear interaction between independent variables [Billings, 2013, Chen and Billings, 1989, Vaccari, 2003]. For illustration, consider a simple example of fitting a curve to data with two independent variables x1 and x2, in the form\ny = w0 + w1x1 + w2x2 + w12x1x2 . (1.38)\nThe term w12 then quantifies the strength of interaction between the two independent variables in the data, x1 and x2. Observe that the model is still linear with respect to the variables x1 and x2, while involving the crossterm w12x1x2. The above model can also have more terms, e.g., x21, x1x 2 2, to describe more complex functional behaviours. For example, the full quadratic polynomial regression for two independent variables, x1 and x2, can have up to 9 terms, given by\ny = w0 + w1x1 + w2x2 + w12x1x2 +w11x21 + w22x 2 2 + w112x 2 1x2 + w122x1x 2 2 + w1122x 2 1x 2 2 . (1.39)\nTensor representation of the system weights. The simple model for two independent variables in (1.38) can be rewritten in a bilinear form as\ny = [ 1 x1 ] [ w0 w2\nw1 w12 ] [ 1 x2 ] ,\nwhereas the full model in (1.39) has an equivalent bilinear expression\ny = [ 1 x1 x21 ]  w0 w2 w22w1 w12 w122\nw11 w112 w1122  1x2 x22  , 29\nor a tensor-vector product representation\ny = W \u00af\u0302 1 [ 1 x1 ] \u00af\u0302 2 [ 1 x1 ] \u00af\u0302 3 [ 1 x2 ] \u00af\u0302 4 [ 1 x2 ] , (1.40)\nwhere the 4th-order weight tensor W is of size 2\u02c6 2\u02c6 2\u02c6 2, and is given by\nW(:, :, 1, 1) = [\nw0 12 w1 1 2 w1 w11\n] , W(:, :, 2, 2) = [ w22 12 w122\n1 2 w122 w1122\n] ,\nW(:, :, 1, 2) = W(:, :, 2, 1) = 1 2\n[ w2 12 w12\n1 2 w12 w112\n] .\nIt is now obvious that for a generalised system with N independent variables, x1, . . . , xN , the MPR can be written as a tensor-vector product as [Chen and Billings, 1989]\ny = N \u00ff\ni1=0\nN \u00ff\ni2=0\n\u00a8 \u00a8 \u00a8 N \u00ff\niN=0\nwi1,i2,...,iN x i1 1 x i2 2 \u00a8 \u00a8 \u00a8 x iN N\n= W \u00af\u0302 1 VN(x1) \u00af\u0302 2 VN(x2) \u00a8 \u00a8 \u00a8 \u00af\u0302 N VN(xN) , (1.41)\nwhere W is an Nth-order tensor of size (N + 1)\u02c6 (N + 1)\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 (N + 1), and VN(x) is the length-(N + 1) Vandermonde vector of x, given by\nVN(x) = [ 1 x x2 . . . xN ]T . (1.42)\nSimilarly to the representation in (1.40), the MPR model in (1.41) can be equivalently expressed as a product of a tensor of N2th-order and size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 with N vectors of length-2, to give\ny = \u0102W \u00af\u0302 1:N [ 1 x1 ] \u00af\u0302 N+1:2N [ 1 x2 ] \u00a8 \u00a8 \u00a8 \u00af\u0302 N(N\u00b41)+1:N2 [ 1 xN ] . (1.43)\nAn illustration of the MPR is given in Figure 1.9, where the input units are scalars.\nThe MPR has found numerous applications, owing to its ability to model any smooth, continuous nonlinear input-output system , see e.g. [Vaccari, 2003]. However, since the number of parameters in the model in (1.41) grows exponentially with the number of variables, N, the MPR demands a huge amount of data in order to yield a good model, and therefore, it is computationally intensive in a raw tensor format, and thus\n30\nnot suitable for very high-dimensional data. To this end, low-rank tensor network representation emerges as a viable approach to accomplishing MPR. For example, the weight tensor W can be constrained to be in low rank TT-format [Chen et al., 2016]. An alternative approach would be to consider a truncated model which takes only two entries along each mode of W in (1.41). In other words, this truncated model becomes linear with respect to each variable xn [Novikov et al., 2016], leading to\ny = Wt \u00af\u0302 1 [ 1 x1 ] \u00af\u0302 2 [ 1 x2 ] \u00a8 \u00a8 \u00a8 \u00af\u0302 N [ 1 xN ] , (1.44)\nwhere Wt is a tensor of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 in the QTT-format. Both (1.43) and (1.44) represent the weight tensors in the QTT-format, however, the tensor \u0102W in (1.43) has N2 core tensors of the full MPR, whereas Wt in (1.44) has N core tensors for the truncated model."}, {"heading": "1.8 Tensor Structures in Vector-variate Regression", "text": "The MPR in (1.41) is formulated for scalar data. When the observations are vectors or tensors, the model can be extended straightforwardly. For\n31\nillustration, consider a simple case of two independent vector inputs x1 and x2. Then, the nonlinear function which maps the input to the output y = h(x1, x2) can be approximated in a linear form as\ny = h(x1, x2) = w0 + wT1 x1 + w T 2 x2 + x T 1 W12x2 (1.45) = [1, xT1 ] [\nw0 wT2 w1 W12 ] [ 1 x2 ] ,\nor in a quadratic with 9 terms, including one bias, two vectors, three matrices, two third-order tensors and one fourth-order tensor, given by\nh(x1, x2) = w0 + wT1 x1 + w T 2 x2 + x T 1 W12x2 + x T 1 W11x1 + x T 2 W22x2\n+ W112 \u00af\u0302 1 x1 \u00af\u0302 2 x1 \u00af\u0302 3 x2 + W122 \u00af\u0302 1 x1 \u00af\u0302 2 x2 \u00af\u0302 3 x2 + W1122 \u00af\u0302 1 x1 \u00af\u0302 2 x1 \u00af\u0302 3 x2 \u00af\u0302 4 x2\n= [1, xT1 , (x1 b x1)T] W  1x2 x2 b x2  , where the matrix W is given\nW =  w0 wT2 vec(W22)Tw1 W12 [W122](1) vec(W11) [W112](1,2) [W1122](1,2)  . (1.46) and [W112](1,2) represents the mode-(1,2) unfolding of the tensor W112. Similarly to (1.40), the above model has an equivalent expression of through the tensor-vector product of a fourth-order tensor W, in the form\ny = W \u00af\u0302 1 [ 1 x1 ] \u00af\u0302 2 [ 1 x1 ] \u00af\u0302 3 [ 1 x2 ] \u00af\u0302 4 [ 1 x2 ] . (1.47)\nIn general, the regression for a system with N input vectors, xn of lengths In, can be written as\nh(x1, . . . , xN) = w0 + N2 \u00ff\nd=1\nN \u00ff\ni1,i2,...,id=1\nWi1,i2,...,id \u00af\u0302 (xi1 \u02dd xi2 \u02dd \u00a8 \u00a8 \u00a8 \u02dd xid) , (1.48)\nwhere \u00af\u0302 represents the inner product between two tensors, and the tensors Wi1,...,id are of d-th order, and of size Ii1 \u02c6 Ii2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 Iid , d = 1, . . . , N\n2. The representation of the generalised model as a tensor-vector product of an\n32\n(a) (b)\nW ...\nW~\n. . . . ... . .\n...\nvN (x )1\nvN (x )2 vN (x )n\nvN (x )N\nJ1 J2\nJn\nJN\n[ 1x 1 [ [ 1x n [\n[ 1x N [ I +1N\nI +11 I +1n\nI +1nI +11\nI +1N\n...\n...\nFigure 1.10: Graphical illustration of the vector-variate regression. (a) The vector-variate regression for multiple input units x1, . . . , xN , where the nonlinear function h(x1, . . . , xN) is expressed as a tensor-vector product of an Nth-order core tensor, W, of size J1 \u02c6 J2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 JN , and Vandermondelike vectors vN(xn) of length Jn, where Jn = IN+1n \u00b41\nIn\u00b41 . (b) An equivalent\nregression model but with an N2th-order tensor of size (I1 + 1)\u02c6\u00a8 \u00a8 \u00a8\u02c6 (I1 + 1) \u02c6 (I2 + 1) \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 (IN + 1) \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 (IN + 1). When the input units are scalars, the tensor \u0102W is of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2.\nNth-order tensor of size J1 \u02c6 J2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 JN , where Jn = I N+1 n \u00b41 In\u00b41 , comprising all the weights, is given by\nh(x1, . . . , xN) = W \u00af\u0302 1 vN(x1) \u00af\u0302 2 vN(x2) \u00a8 \u00a8 \u00a8 \u00af\u0302 N vN(xN), (1.49)\nwhere\nvN(x) = [ 1 xT (xb x)T . . . (xb \u00a8 \u00a8 \u00a8 b x)T ]T , (1.50)\nor, in a more compact form, with a very high-order tensor \u0102W of N2th-order and of size (I1 + 1)\u02c6\u00a8 \u00a8 \u00a8\u02c6 (I1 + 1)\u02c6 (I2 + 1)\u02c6\u00a8 \u00a8 \u00a8\u02c6 (IN + 1)\u02c6\u00a8 \u00a8 \u00a8\u02c6 (IN + 1), as\nh(x1, . . . , xN) = \u0102W \u00af\u0302 1:N [ 1 x1 ] \u00a8 \u00a8 \u00a8 \u00af\u0302 N(N\u00b41)+1:N2 [ 1 xN ] . (1.51)\nThe illustration of this generalized model is given in Figure 1.10. Tensor-variate model. When the observations are matrices, Xn, or higherorder tensors, Xn, the models in (1.48), (1.49) and (1.51) are still applicable\n33\nand operate by replacing the original vectors, xn, by the vectorization of the higher-order inputs. This is because the inner product between two tensors can be expressed as a product of their two vectorizations. Separable representation of the weights. Similar to the MPR, the challenge in the generalised tensor-variate regression is the curse of dimensionality of the weight tensor W in (1.49), or of the tensor \u0102W in (1.51).\nA common method to deal with the problem is to restrict the model to some low order, i.e., to the first order. The weight tensor is now only of size (I1 + 1)\u02c6 (I2 + 1)\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 (IN + 1). The large weight tensor can then be represented in the canonical form [Nguyen et al., 2015, Qi et al., 2016], the TT/MPS tensor format [Stoudenmire and Schwab, 2016], or the hierarchical Tucker tensor format [Cohen and Shashua, 2016]."}, {"heading": "1.9 Tensor Structure in Volterra Models of Nonlinear Systems", "text": ""}, {"heading": "1.9.1 Discrete Volterra Model", "text": "System identification is a paradigm which aims to provide a mathematical description of a system from the observed system inputs and outputs [Billings, 2013]. In practice, tensors are inherently present in Volterra operators which model the system response of a nonlinear system which maps an input signal x(t) to an output signal y(t) in the form\ny(t) = V(x(t)) = h0 + H1(x(t)) + H2(x(t)) + \u00a8 \u00a8 \u00a8+ Hn(x(t)) + \u00a8 \u00a8 \u00a8\nwhere h0 is a constant and Hn(x(t)) is the nth-order Volterra operator, defined as a generalised convolution of the integral Volterra kernels h(n)(\u03c41, . . . , \u03c4n) and the input signal, that is\nHn(x(t)) = \u017c h(n)(\u03c41, . . . , \u03c4n)x(t\u00b4 \u03c41) \u00a8 \u00a8 \u00a8 x(t\u00b4 \u03c4n)d\u03c41 \u00a8 \u00a8 \u00a8 d\u03c4n . (1.52)\nThe system, which is assumed to be time-invariant and continuous, is treated as a black box, and needs to be represented by appropriate Volterra operators.\nIn practice, for a finite duration sample input data, x, the discrete system can be modelled using truncated Volterra kernels of size M\u02c6 M\u02c6\n34"}, {"heading": "H (3)", "text": "\u00a8 \u00a8 \u00a8 \u02c6M, given by\nHn(x) = I \u00ff\ni1=1\n\u00a8 \u00a8 \u00a8 I \u00ff\nin=1\nh(n)i1,...,in xi1 . . . xi2\n= H(n) \u00af\u0302 1x \u00af\u0302 2x \u00a8 \u00a8 \u00a8 \u00af\u0302 nx. (1.53)\nFor simplicity, the Volterra kernels H(n) = [h(n)i1,...,in ] are assumed to have the same size M in each mode, and, therefore, to yield a symmetric tensor. Otherwise, they can be symmetrized. Curse of dimensionality. The output which corresponds to the input x is\n35\nwritten as a sum of N tensor products (see in Figure 1.11), given by\ny = h0 + N \u00ff\nn=1\nH(n) \u00af\u0302 1 x \u00af\u0302 2 x \u00a8 \u00a8 \u00a8 \u00af\u0302 n x. (1.54)\nDespite the symmetry of the Volterra kernels, H(n), the number of actual coefficients of the nth-order kernel to be estimated is still huge, especially for higher-order kernels, and is given by (M+n\u00b41)!n!(M\u00b41)! . As a consequence, the estimation requires a large number of measures (data samples), so that the method for a raw tensor format is only feasible for systems with a relatively small memory and low-dimensional input signals."}, {"heading": "1.9.2 Separable Representation of Volterra Kernel", "text": "In order to deal with the curse of dimensionality in Volterra kernels, we consider the kernel H(n) to be separable, i.e., it can be expressed in some low rank tensor format, e.g., as a CP tensor or in any other suitable tensor network format (for the concept of general separability of variables, see Part 1). Volterra-CP model. The first and simplest separable Volterra model, proposed in [Favier et al., 2012], represents the kernels by symmetric tensors of rank Rn in the CP format, that is\nH(n) = I\u02c61 An \u02c62 An \u00a8 \u00a8 \u00a8 \u02c6n An . (1.55)\nFor this tensor representation, the identification problem simplifies into the estimation of N factor matrices, An, of size M \u02c6 Rn and an offset, h0, so that the number of parameters reduces to M \u0159\nn Rn + 1 (note that R1 = 1). Moreover, the implementation of the Volterra model becomes\nyk = h0 + N \u00ff\nn=1\n(xTk An) \u201an 1Rn , (1.56)\nwhere xk = [xk\u00b4M+1, . . . , xk\u00b41, xk]T comprises M samples of the input signal, and (\u201a)\u201an represents the element-wise power operator. The entire output vector y can be computed in a simpler way through the convolution of the input vector x and the factor matrices An, as [Batselier et al., 2016a]\ny = h0 + N \u00ff\nn=1\n(x \u02daAn)\u201an 1Rn . (1.57)\n36\nVolterra-TT model. Alternatively, the Volterra kernels, H(n), can be represented in the TT-format, as\nH(n) = xxG(1)n , G (2) n , . . . , G (n) n yy . (1.58)\nBy exploiting the fast contraction over all modes between a TT-tensor and xk, we have\nH(n) \u00af\u0302 xk = (G (1) n \u00af\u0302 2xk)(G (2) n \u00af\u0302 2xk) \u00a8 \u00a8 \u00a8 (G (n) n \u00af\u0302 2xk) .\nThe output signal, can be then computed through the convolution of the core tensors and the input vector, as\nyk = h0 + N \u00ff\nn=1\nZn,1(1, k, :)Zn,2(:, k, :) \u00a8 \u00a8 \u00a8 Zn,n\u00b41(:, k, :)Zn,n(:, k) ,\nwhere Zn,m = G (m) n \u02da2 x is a mode-2 partial convolution of the input signal x and the core tensor G(m)n , for m = 1, . . . , n. A similar method, but with only one TT-tensor, is considered in [Batselier et al., 2016b]."}, {"heading": "1.9.3 Volterra-based Tensorization for Nonlinear Feature Extraction", "text": "Consider nonlinear feature extraction in a supervised learning system, such that the extracted features maximize the Fisher score [Kumar et al., 2009]. In other words, for a data sample xk, which can be a recorded signal in one trial or a vectorization of an image, a feature extracted from xk by a nonlinear process is denoted by yk = f (xk). Such constrained (discriminant) feature extraction can be treated as a maximization of the Fisher score\nmax \u0159 c(y\u0304c \u00b4 y\u0304)2 \u0159\nk(yk \u00b4 y\u0304ck)2 , (1.59)\nwhere y\u0304ck is the mean feature of the samples in class-k, and y\u0304 the mean feature of all the samples.\nNext, we model the nonlinear system f (x) by a truncated Volterra series representation\nyk = N \u00ff\nn=1\nH(n) \u00af\u0302 (xk \u02dd xk \u02dd \u00a8 \u00a8 \u00a8 \u02dd xk) = hT zk , (1.60)\n37\nwhere h and xk are vectors comprising all coefficients of the Volterra kernels and\nh = [vec ( H(1) )T , vec ( H(2) )T , . . . , vec ( H(N) )T ]T ,\nzk = [xTk , (x b 2 k ) T, . . . , (xbNk ) T]T .\nThe shorthand xb n = xb xb \u00a8 \u00a8 \u00a8 b x represents the Kronecker product of n vectors x. The offset coefficient, h0, is omitted in the above Volterra model because it will be eliminated in the objective function (1.59). The vector h can be shortened by keeping only distinct coefficients, due to symmetry of the Volterra kernels. The augmented sample zk needs a similar adjustment but multiplied with the number of occurrences.\nObserve that the nonlinear feature extraction, f (xk), becomes a linear mapping, as in (1.60) after xk is tensorized into zk. Hence, the nonlinear discriminant in (1.59) can be rewritten in the form of a standard linear discriminant analysis\nmax hTSbh hTSwh , (1.61)\nwhere Sb = \u0159 c(z\u0304c \u00b4 z\u0304)(z\u0304c \u00b4 z\u0304)T and Sw = \u0159 k(zk \u00b4 z\u0304ck)(zk \u00b4 z\u0304ck)T are respectively between- and within-scattering matrices of zk. The problem then boils down to finding generalised principal eigenvectors of Sb and Sw. Efficient implementation. The problem with the above analysis is that the length of eigenvectors, h, in (1.61) grows exponentially with the data size, especially for higher-order Volterra kernels. To this end, Kumar et al. [2009] suggested to split the data into small patches. Alternatively, we can impose low rank-tensor structures, e.g., the CP or TT format, onto the Volterra kernels, H(n), or the entire vector h."}, {"heading": "1.10 Low-rank Tensor Representations of Sinusoid Signals and their Applications to BSS and Harmonic Retrieval", "text": "Harmonic signals are fundamental in many practical applications. This section addresses low-rank structures of sinusoid signals under several tensorization methods. These properties can then be exploited in the\n38\nblind separation of sinusoid signals or their modulated variants, e.g., the exponentially decaying signals, the examples of which are\nx(t) = sin(\u03c9 t + \u03c6) , x(t) = t sin(\u03c9 t + \u03c6) , (1.62) x(t) = exp(\u00b4\u03b3t) sin(\u03c9 t + \u03c6) , x(t) = t exp(\u00b4\u03b3t) , (1.63)\nfor t = 1, 2, . . . , L, \u03c9 \u2030 0."}, {"heading": "1.10.1 Folding - Reshaping of Sinusoid", "text": "Harmonic matrix. The harmonic matrix U\u03c9,I is a matrix of size I \u02c6 2 defined over the two variables, the angular frequency \u03c9 and the folding size I, as\nU\u03c9,I =  1 0 ... ... cos(k\u03c9) sin(k\u03c9) ... ...\ncos((I \u00b4 1)\u03c9) sin((I \u00b4 1)\u03c9)\n . (1.64)\nTwo-way folding. A matrix of size I\u02c6 J, folded from a sinusoid signal x(t) of length L = I J, is of rank-2, and can be decomposed as\nY = U\u03c9,I S UT\u03c9I,J , (1.65)\nwhere S is invariant to the folding size I, depends only on the phase \u03c6, and takes the form\nS = [\nsin(\u03c6) cos(\u03c6) cos(\u03c6) \u00b4 sin(\u03c6)\n] . (1.66)\nThree-way folding. A third-order tensor of size I \u02c6 J \u02c6 K, where I, J, K \u0105 2, reshaped from a sinusoid signal of length L, can take the form of a multilinear rank-(2,2,2) or rank-3 tensor\nY = JH; U\u03c9,I , U\u03c9I,J , U\u03c9I J,KK , (1.67)\nwhere H = G\u02c63 S is a small-scale tensor of size 2\u02c6 2\u02c6 2, and\nG(:, :, 1) = [\n1 0 0 \u00b41\n] , G(:, :, 2) = [ 0 1 1 0 ] . (1.68)\n39\nThe above expression can be derived by folding the signal y(t) two times. We can prove by contradiction that the so-created core tensor G does not have rank-2, but has the following rank-3 tensor representation\nG = 1 2 [ 1 1 ] \u02dd [ 1 1 ] \u02dd [ 1 1 ] \u00b4 1 2 [ \u00b41 1 ] \u02dd [ \u00b41 1 ] \u02dd [ \u00b41 1 ] + 2 [ 0 1 ] \u02dd [ 0 1 ] \u02dd [ \u00b41 0 ] .\nHence, Y is also a rank-3 tensor. Note that Y does not have a unique rank-3 decomposition.\nRemark 2 The Tucker-3 decomposition in (1.67) has a fixed core tensor G, while the factor matrices are identical for signals of the same frequency.\nHigher-order folding - TT-representation. An Nth-order tensor of size I1 \u02c6 I2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 IN , where In \u011b 2, which is reshaped from a sinusoid signal, can be represented by a multilinear rank-(2,2,. . . , 2) tensor\nY = JH; U\u03c9,I1 , U\u03c9 J1,I2 , . . . , U\u03c9 JN\u00b41,INK , (1.69)\nwhere H = xxG, G, . . . , G loooooomoooooon\n(N\u00b42)terms\n, Syy is an Nth-order tensor of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2,\nand Jn = \u015bn k=1 Ik.\nRemark 3 (TT-representation) Since the tensor H has TT-rank of (2,2,. . . ,2), the folding tensor Y is also a tensor in TT-format of rank-(2,2,. . . ,2), that is\nY = xxA1, A2, . . . , ANyy , (1.70)\nwhere A1 = U\u03c9,I1 , AN = SU T \u03c9 JN\u00b41,IN and An = G \u02c62 U\u03c9 Jn\u00b41,In for n = 2, . . . , N \u00b4 1.\nRemark 4 (QTT-Tucker representation) When the folding sizes In = 2, for n = 1, . . . , N, the representation of the folding tensor Y in (1.69) is also known as the QTT-Tucker format, given by\nY = JH; A1, . . . , AN\u00b41, ANK, (1.71)\nwhere An = [\n1 0 cos(2n\u00b41\u03c9) sin(2n\u00b41\u03c9)\n] .\nExample 9 Separation of damped sinusoid signals.\n40\nThis example demonstrates the use of multiway folding in a single channel separation of damped sinusoids. We considered a vector composed of P damped sinusoids,\ny(t) = P \u00ff\np=1\nap xp(t) + n(t) , (1.72)\nwhere\nxp(t) = exp( \u00b45t Lp ) sin( 2\u03c0 fp fs t + (p\u00b4 1)\u03c0 P ) ,\nwith frequencies fp = 10, 12 and 14 Hz, and the sampling frequency fs = 10 fP. Additive Gaussian noise, n(t), was generated at a specific signal-noise-ratio (SNR). The weights, ap, were set such that the component sources were equally contributing to the mixture, i.e., a1}x1} = \u00a8 \u00a8 \u00a8 = aP}xP}, and the signal length was L = 2d P2.\nIn order to separate the three signals xp(t) from the mixture y(t), we tensorized the mixture to a dth-order tensor of size 2R \u02c6 2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 \u02c6 2R. Under this tensorization, the exponentially decaying signals exp(\u03b3t) yielded rank-1 tensors, while according to (1.69) the sinusoids have TTrepresentations of rank-(2, 2, . . . , 2). Hence, the tensors of x(t) can also be represented by tensors in the TT-format of rank-(2, 2, . . . , 2). We were,\n41\ntherefore, able to approximate Y as a sum of P TT-tensors Xr of rank(2, 2, . . . , 2), that is, through the minimization [Phan et al., 2016]\nmin }Y\u00b4 X1 \u00b4 X2 \u00b4 \u00a8 \u00a8 \u00a8 \u00b4 XP}2F . (1.73)\nFor this purpose, a tensor Xp in a TT-format was fitted sequentially to the residual Yp = Y\u00b4 \u0159\ns\u2030p Xs, calculated by the difference between the data tensor Y and its approximation by the other TT-tensors Xs where s \u2030 p, that is,\narg min Xp\n}Yp \u00b4 Xp}2F , (1.74)\nfor p = 1, . . . , P. Figure 1.12 illustrates the mean SAEs (MSAE) of the estimated signals for various noise levels SNR = 0, 10, . . . , 50 dB, and different signal lengths K = 9\u02c6 2d, where d = 12, 14, 16, 18.\nOn average, an improvement of 2 dB SAE is achieved if the signal is two times longer. If the signal has less than L = 9\u02c6 26 = 576 samples, the estimation quality will deteriorate by about 12 dB compared to the case when signal length of L = 9\u02c6 212. For such cases, we suggest to augment the signals using other tensorizations before performing the source extraction, e.g., by construction of multiway Toeplitz or Hankel tensors. Example 10 further illustrates the separation of short length signals."}, {"heading": "1.10.2 Toeplitz Matrix and Toeplitz Tensors of Sinusoidal Signals", "text": "Toeplitz matrix of sinusoid. The Toeplitz matrix, Y, of a sinusoid signal, y(t) = sin(\u03c9 t + \u03c6), is of rank-2 and can be decomposed as\nY =  y(1) y(2) y(2) y(3) ... ...\ny(I) y(I + 1)\nQT [ y(I) \u00a8 \u00a8 \u00a8 y(L) y(I \u00b4 1) \u00a8 \u00a8 \u00a8 y(L\u00b4 1) ] , (1.75)\nwhere QT is invariant to the selection of folding length I, and has the form\nQT = 1\nsin2(\u03c9)\n[ \u00b4y(3) y(2) y(2) \u00b4y(1) ] . (1.76)\nThe above expression follows from the fact that\n[ y(i) y(i + 1) ] [\u00b4y(3) y(2) y(2) \u00b4y(1) ] [ y(j) y(j\u00b4 1)] ] = sin2(\u03c9) y(j\u00b4 i + 1) .\n42\nToeplitz tensor of sinusoid. An Nth-order Toeplitz tensor, tensorized from a sinusoidal signal, has a TT-Tucker representation\nY = JG; U1, . . . , UN\u00b41, UNK (1.77)\nwhere the factor matrices Un are given by\nU1 =  y(1) y(2)... ... y(J1) y(J1 + 1)  , UN = y(JN\u00b41 \u00b4 1) y(JN\u00b41 \u00b4 2)... ... y(L) y(L\u00b4 1)  , Un =\n y(Jn\u00b41) y(Jn\u00b41 + 1)... ... y(Jn \u00b4 1) y(Jn)  , n = 2, . . . , N \u00b4 1 , (1.78) in which Jn = I1 + I2 + \u00a8 \u00a8 \u00a8+ In. The core tensor G is an Nth-order tensor of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2, in a TT-format, given by\nG = xxG(1), G(2), . . . , G(N\u00b41)yy, (1.79)\nwhere G(1) = T(1) is a matrix of size 1\u02c6 2\u02c6 2, while the core tensors G(n), for n = 2, . . . , N \u00b4 1, are of size 2\u02c6 2\u02c6 2 and have two horizontal slices, given by\nG(n)(1, :, :) = T(Jn\u00b41 \u00b4 n + 2) , G(n)(2, :, :) = T(Jn\u00b41 \u00b4 n + 1) ,\nwith\nT(I) = 1\nsin2(\u03c9)\n[ \u00b4y(I + 2) y(I + 1) y(I + 1) \u00b4y(I) ] . (1.80)\nFollowing the two-stage Toeplitz tensorization, and upon applying (1.75), we can deduce the decomposition in (1.77) from that for the (N\u00b41)th-order Toeplitz tensor.\nRemark 5 For second-order tensorization, the core tensor G in (1.79) comprises only G(1), which is identical to the matrix QT in (1.76).\nQuantized Toeplitz tensor. An (L \u00b4 1)th-order Toeplitz tensor of a sinusoidal signal of length L and size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8\u02c6 2 has a TT-representation with (L\u00b4 3) identical core tensors G, in the form\nY = xxG, G, . . . , G, [\ny(L\u00b4 1) y(L) y(L\u00b4 2) y(L\u00b4 1)\n] yy ,\n43\nwhere\nG(1, :, :) = [\n1 0 0 1\n] , G(2, :, :) = [ 0 1 \u00b41 2 cos(\u03c9) ] .\nExample 10 Separation of short-length damped sinusoid signals. This example illustrates the use of Toeplitz-based tensorization in the separation of damped sinusoid signals from a short-length observation. We considered a single signal composed by P = 3 damped sinusoids of length L = 66, given by\ny(t) = P \u00ff\np=1\nap xp(t) + n(t) , (1.81)\nwhere\nx(t) = exp( \u00b4pt 30 ) sin( 2\u03c0 fp fs t + p\u03c0 7 ) (1.82)\nwith frequencies fp = 10, 11 and 12 Hz, the sampling frequency fs = 300 Hz, and the mixing factors ap = p. Additive Gaussian noise n(t) was generated at a specific signal-noise-ratio.\nIn order to separate the three signals, xp(t), from the mixture y(t), we first tensorized the observed signal to a 7th-order Toeplitz tensor of size\n44\n16\u02c6 8\u02c6 8\u02c6 8\u02c6 8\u02c6 8\u02c6 16, then folded this tensor to a 23th-order tensor of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2. With this tensorization, according to (1.77) and (1.69), each damped sinusoid xp(t) had a TT-representation of rank-(2, 2, . . . , 2). The result produced by minimizing the cost function (1.73), annotated by TT-SEPA, is shown in Figure 1.13 as a solid line with star marker. The so obtained performance was much better than in Example 9, even for the signal length of only 66 samples.\nWe note that the parameters of the damped signals can be estimated using linear self-prediction (auto-regression) methods, e.g., singular value decomposition of the Hankel-type matrix as in the Kumaresan-Tufts (KT) method [Kumaresan and Tufts, 1982]. As shown in Figure 1.13, the obtained results based on the TT-decomposition were slightly better than those using the KT method. For this particular problem, the estimation performance can even be higher when applying self-prediction algorithms, which exploit the low-rank structure of damped signals, e.g., TT-KT, and TT-linear prediction methods based on SVD. For a detailed derivation of these algorithms, see [Phan et al., 2017]."}, {"heading": "1.10.3 Hankel Matrix and Hankel Tensor of Sinusoidal Signal", "text": "Hankel tensor of sinusoid. The Hankel tensor of a sinusoid signal y(t) is a TT-Tucker tensor,\nY = JG; U1, U2, . . . , UNK , (1.83)\nfor which the factor matrices are defined in (1.78). The core tensor G is an Nth-order tensor of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2, in the TT-format, given by\nG = xxG(1), G(2), . . . , G(N\u00b41)yy , (1.84)\nwhere G(1) = H(J1) is a matrix of size 1 \u02c6 2 \u02c6 2, while the core tensors G(n), for n = 2, . . . , N \u00b4 1, are of size 2\u02c6 2\u02c6 2 and have two horizontal slices, given by\nG(n)(1, :, :) = H(Jn \u00b4 n + 1) , G(n)(2, :, :) = H(Jn \u00b4 n + 2) ,\nwith\nH(I) = 1\nsin2(\u03c9)\n[ y(I) \u00b4y(I + 1)\n\u00b4y(I \u00b4 1) y(I)\n] . (1.85)\nRemark 6 The two TT-Tucker representations of the Toeplitz and Hankel tensors of the same sinusoid have similar factor matrices Un, but their core tensors are different.\n45\nQuantized Hankel tensor. An (L\u00b4 1)th-order Hankel tensor of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 of a sinusoid signal of length L has a TT-representation with (N\u00b4 2) identical core tensors G, in the form\nY = xxG, G, . . . , G, [\ny(L\u00b4 2) y(L\u00b4 1) y(L\u00b4 1) y(L)\n] yy ,\nwhere\nG(1, :, :) = [\n2 cos(\u03c9) \u00b41 1 0\n] , G(2, :, :) = [ 1 0 0 1 ] .\nFinally, representations of the sinusoid signal in various tensor format of size are summarised in Figure 1.14."}, {"heading": "1.11 Summary", "text": "This chapter has introduced several common tensorization methods, together with their properties and illustrative applications in blind source separation, blind identification, denoising, and harmonic retrieval. The\n46\nmain criterion for choosing a suitable tensorization is that the tensor generated from lower-order original data must reveal the underlying lowrank tensor structure in some tensor format. For example, the folded tensors of mixtures of damped sinusoid signals have low-rank QTT representation, while the derivative tensors in blind identification admit the CP decomposition. The Toeplitz and Hankel tensor foldings augment the number of signal entries, through the replication of signal segments (redundancy), and in this way become suited to modeling of signals of short length. A property crucial to the solution via the tensor networks shown in this chapter, is that the tensors can be generated in the TT/QTT format, if the generating vector admits a low-rank QTT representation.\nIn modern data analytics problems, such as regression and deep learning, the number of model parameters can be huge, which renders the model intractable. Tensorization can then serve as a remedy, by representing the parameters in some low-rank tensor format. For further discussion on tensor representation of parameters in tensor regression, we refer to Chapter 2. A wide class of optimization problems including of solving linear systems, eigenvalue decomposition, singular value decomposition, Canonical Correlation Analysis (CCA) are addressed in Chapter 3. The tensor structures for Boltzmann machines and convolutional deep neural networks (CNN) are provided in Chapter 4.\n47\nChapter 2\nSupervised Learning with Tensors Learning a statistical model that formulates a hypothesis for the data distribution merely from multiple input data samples, x, without knowing the corresponding values of the response variable, y, is refereed to as unsupervised learning. In contrast, supervised learning methods, when seen from a probabilistic perspective, model either the joint distribution p(x, y) or the conditional distribution p(y|x), for given training data pair tx, yu. Supervised learning can be categorized into regression, if y is continuous, or classification, if y is categorical (see also Figure 2.1).\nRegression models can be categorized into linear regression and nonlinear regression. In particular, multiple linear regression is associated with multiple smaller-order predictors, while multivariate regression corresponds to a single linear regression model but with multiple predictors and multiple responses. Normally, multivariate regression tasks are encountered when the predictors are arranged as vectors, matrices or tensors of variables. A basic linear regression model in the vector form is defined as\ny = f (x; w, b) = xx, wy+ b = wTx + b, (2.1)\nwhere x P RI is the input vector of independent variables, w P RI the vector of regression coefficients or weights, b the bias, and y the regression output or a dependent/target variable.\nSuch simple linear models can be applied not only for regression but also for feature selection and classification. In all the cases, those models approximate the target variable y by a weighted linear combination of input variables, wTx + b.\nTensor representations are often very useful in mitigating the small sample size problem in discriminative subspace selection, because the\n48\nNo data labels\ninformation about the structure of objects is inherent in tensors and is a natural constraint which helps reduce the number of unknown parameters in the description of a learning model. In other words, when the number of training measurements is limited, tensor-based learning machines are expected to perform better than the corresponding vector-based learning machines, as vector representations are associated with several problems, such as loss of information for structured data and over-fitting for highdimensional data."}, {"heading": "2.1 Tensor Regression", "text": "Regression is at the very core of signal processing and machine learning, whereby the output is typically estimated based on a linear combination of regression coefficients and the input regressor, which can be a vector, matrix, or tensor. In this way, regression analysis can be used to predict dependent variables (responses, outputs, estimates), from a set of independent\n49\nvariables (predictors, inputs, regressors), by exploring the correlations among these variables as well as explaining the inherent factors behind the observed patterns. It is also often convenient, especially regarding ill-posed cases of matrix inversion, which is inherent to regression to jointly perform regression and dimensionality reduction through, e.g., principal component regression (PCR) [Jolliffe, 1982], whereby regression is performed on a well-posed low-dimensional subspace defined through most significant principal components. With tensors being a natural generalization of vectors and matrices, tensor regression can be defined in an analogous way.\nA well established and important supervised learning technique is linear or nonlinear Support Vector Regression (SVR) [Smola and Vapnik, 1997], which allows for the modeling of streaming data and is quite closely related to Support Vector Machines (SVM) [Cortes and Vapnik, 1995]. The model produced by SVR only depends on a subset of training data (support vectors), because the cost function for building the model ignores any training data that is close (within a threshold \u03b5) to the model prediction.\nStandard support vector regression techniques have been naturally extended to Tensor Regression (TR) or Support Tensor Machine (STM) methods [Tao et al., 2005]. In the (raw) tensor format, the TR/STM can be formulated as\ny = f (X; W, b) = xX, Wy+ b, (2.2)\nwhere X P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN is the input tensor regressor, W P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN the tensor of weights (also called regression tensor or model tensor), b the bias, and y the regression output, xX, Wy = vec(X)T vec(W) is the inner product of two tensors.\nWe shall denote input samples of multiple predictor variables (or features) by X1, . . . , XM (tensors), and the actual continuous or categorical response variables by y1, y2, . . . , yM (usually scalars). The training process, that is the estimation of the weight tensor W and bias b, is carried out based on the set of available training samples tXm, ymu for m = 1, . . . , M. Upon arrival of a new training sample, the TR model is used to make predictions for that sample.\nThe problem is usually formulated as a minimization of the following squared cost function, given by\nJ(X, y | W, b) = M \u00ff\nm=1\n( ym \u00b4 ( xW, Xmy+ b ))2 (2.3)\n50\nor the logistic loss function (usually employed in classification problems), given by\nJ(X, y | W, b) = M \u00ff\nm=1\nlog (\n1 1 + e\u00b4ym(xW,Xmy+b)\n) . (2.4)\nIn practice, for very large scale problems, tensors are expressed approximately in tensor network formats, especially using Canonical Polyadic (CP), Tucker or Tensor Train (TT)/Hierarchical Tucker (HT) models [Grasedyck, 2010, Oseledets, 2011a]. In this case, a suitable representation of the weight tensor, W, plays a key role in the model performance. For example, the CP representation of the weight tensor, in the form\nW = R \u00ff\nr=1\nu(1)r \u02dd u (2) r \u02dd \u00a8 \u00a8 \u00a8 \u02dd u (N) r\n= I\u02c61 U(1) \u02c62 U(2) \u00a8 \u00a8 \u00a8 \u02c6N U(N), (2.5)\nwhere \u201c\u02dd\u201d denotes the outer product of vectors, leads to a generalized linear model (GLM), called the CP tensor regression [Zhou et al., 2013].\nAnalogously, upon the application of Tucker multilinear rank tensor representation\nW = G\u02c61 U(1) \u02c62 U(2) \u00a8 \u00a8 \u00a8 \u02c6N U(N), (2.6)\nwe obtain Tucker tensor regression [Hoff, 2015, Li et al., 2013, Yu et al., 2016]. An alternative form of the multilinear Tucker regression model, proposed by Hoff [2015], assumes that the replicated observations tXm, YmuMm=1 are stacked in concatenated tensors X P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6M and Y P RJ1\u02c6\u00a8\u00a8\u00a8\u02c6JN\u02c6M, which admit the following model\nY = X\u02c61 W1 \u02c62 W2 \u00a8 \u00a8 \u00a8 \u02c6N WN \u02c6N+1 DM + E, (2.7)\nwhere DM is an M \u02c6 M diagonal matrix, Wn P RJn\u02c6In are the weight matrices within the Tucker model, and E is a zero-mean residual tensor of the same order as Y. The unknown regression coefficient matrices, Wn can be found using the procedure outlined in Algorithm 1.\nIt is important to highlight that the Tucker regression model offers several benefits over the CP regression model, which include:\n1. A more parsimonious modeling capability and a more compact model, especially for a limited number of available samples;\n51\nAlgorithm 1: Multilinear Tucker Regression Input: X P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6M and Y P RJ1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6JN\u02c6M. Output: tWnu, n = 1, . . . , N.\n1: Initialize randomly Wn for n = 1, . . . , N. 2: while not converged or iteration limit is not reached do 3: for n = 1 to N do 4: X(n) = X\u02c61 W1 \u00a8 \u00a8 \u00a8 \u02c6n\u00b41 Wn\u00b41 \u02c6n+1 Wn+1 \u00a8 \u00a8 \u00a8 \u02c6N WN 5: Matricize tensors X(n) and Y into their respective unfolded matrices\nX(n) (n) and Y(n)\n6: Compute Wn = Y(n)(X (n) (n))\nT (\nX(n) (n) (X (n) (n))\nT )\u00b41\n7: end for 8: end while\n2. Ability to fully exploit multi-linear ranks, through the freedom to choose a different rank for each mode, which is essentially useful when data is skewed in dimensions (different sizes in modes);\n3. Tucker decomposition explicitly models the interaction between factor matrices in different modes, thus allowing for a finer grid search over a larger modeling space.\nBoth the CP and Tucker tensor regression models can be solved by alternating least squares (ALS) algorithms which sequentially estimate one factor matrix at a time while keeping other factor matrices fixed. To deal with the curse of dimensionality, various tensor network decompositions can be applied, such as the TT/HT decomposition for very high-order tensors [Grasedyck, 2010, Oseledets, 2011a]. When the weight tensor W in (2.2) is represented by a low-rank HT decomposition, this is referred to as the H-Tucker tensor regression [Hou, 2017].\nRemark 1. In some applications, the weight tensor, W, is of a higherorder than input tensors, Xm, this yields a more general tensor regression model\nYm = xXm|Wy+ Em, m = 1, . . . , M, (2.8)\nwhere xXm|Wy denotes a tensor contraction along the first N modes of an Nth-order input (covariate) tensor, Xm P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN , and a Pth-order weight tensor, W P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IP , with P \u0105 N, while E P RIP+1\u02c6\u00a8\u00a8\u00a8\u02c6IP is the residual tensor and Y P RIP+1\u02c6\u00a8\u00a8\u00a8\u02c6IP the response tensor.\n52\nObserve that the tensor inner product is equivalent to a contraction of two tensors of the same order (which is a scalar) while a contraction of two tensors of different orders, X P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN and W P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IP , with P \u0105 N, is defined as a tensor Y P RIN+1\u02c6\u00a8\u00a8\u00a8\u02c6IP of (P\u00b4 N)th-order with entries\nxX|WyiN+1,...,iP = I1 \u00ff\ni1=1\n\u00a8 \u00a8 \u00a8 IN \u00ff\niN=1\nxi1,...,iN wi1,...,iN ,iN+1,...,iP . (2.9)\nMany regression problems are special cases of the general tensor regression model in (2.8), including the multi-response regression, vector autoregressive model and pair-wise interaction tensor model (see [Raskutti and Yuan, 2015] and references therein).\nIn summary, the aim of tensor regression is to estimate the entries of a weight tensor, W, based on available input-output observations tXm, Ymu. In a more general scenario, support tensor regression (STR) aims to identify a nonlinear function, f : RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN \u00d1 R, from a collection of observed input-output data pairs tXm, ymuMm=1 generated from the model\nym = f (Xm) + \u03b5, (2.10)\nwhere the input Xm has a tensor structure, the output ym is a scalar, and \u03b5 is also a scalar which represents the error.\nUnlike linear regression models, nonlinear regression models have the ability to characterize complex nonlinear dependencies in data, in which the responses are modeled through a combination of nonlinear parameters and predictors, with the nonlinear parameters usually taking the form of an exponential function, trigonometric function, power function, etc. The nonparametric counterparts, so called nonparametric regression models, frequently appear in machine learning, such as Gaussian Processes (GP) (see [Hou, 2017, Zhao et al., 2013b] and references therein)."}, {"heading": "2.2 Regularized Tensor Models", "text": "Regularized tensor models aims to reduce the complexity of tensor regression models through constraints (restrictions) on the model parameters, W. This is particularly advantageous for problems with a large number of features but a small number of data samples.\nRegularized linear tensor models can be generally formulated as\nmin W,b f (X, y) = J(X, y | W, b) + \u03bbR(W), (2.11)\n53\nwhere J(X, y | W, b) denotes a loss (error) function, R(W) is a regularization term, while the parameter \u03bb \u0105 0 controls the trade-off between the contributions of the original loss function and regularization term.\nOne such well-known regularized linear model is the Frobeniusnorm regularized model for support vector regression, called Grouped LASSO [Tibshirani, 1996a], which employs logistic loss and `1-norm regularization for simultaneous classification (or regression) and feature selection. Another very popular model of this kind is the support vector machines (SVM) [Cortes and Vapnik, 1995] which uses a hinge loss in the form l(y\u0302) = max(0, 1\u00b4 yy\u0302), where y\u0302 is the prediction and y the true label.\nIn regularized tensor regression, when the regularization is performed via the Frobenius norm, }W}2F = xW, Wy, we arrive at the standard Tikhonov regularization, while using the `1 norm, } \u00a8 }`1 , we impose classical sparsity constraints. The advantage of tensors over vectors and matrices is that we can exploit the flexibility in the choice of sparsity profiles. For example, instead of imposing global sparsity for the whole tensor, we can impose sparsity for slices or fibers if there is a need to enforce some fibers or slices to have most of their entries zero. In such a case, similarly to group LASSO, we can apply group-based `1-norm regularizers, such as the `2,1 norm, i.e., }X}2,1 = \u0159J j=1 }xj}2.\nSimilarly to matrices, the various rank properties can also be employed for tensors, and are much richer and more complex due to the multidimensional structure of tensors. In addition to sparsity constraints, a low-rank of a tensor can be exploited as a regularizer, such as the canonical rank or multilinear (Tucker) rank, together with various more sophisticated tensor norms like the nuclear norm or latent norm of the weight tensor W.\nThe low-rank regularization problem can be formulated as\nmin W J(X, y | W, b), s.t. multilinear rank(W) \u010f R. (2.12)\nSuch a formulation allows us to estimate a weight tensor, W P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN , that minimizes the empirical loss J, subject to the constraint that the multilinear rank of W is at most R. Equivalently, this implies that the weight tensor, W, admits a low-dimensional factorization in the form W = G\u02c61 U(1) \u02c62 U(2) \u00a8 \u00a8 \u00a8 \u02c6N U(N).\nLow rank structure of tensor data has been successfully utilized in applications including missing data imputation [Liu et al., 2013, Zhao et al., 2015], multilinear robust principal component analysis [Inoue et al., 2009], and subspace clustering [Zhang et al., 2015a]. Instead of low-rank\n54\nproperties of data itself, low-rank regularization can be also applied to learning coefficients in regression and classification [Yu et al., 2016].\nSimilarly, for very large-scale problems, we can also apply the tensor train (TT) decomposition as the constraint, to yield\nmin W\nJ(X, y | W, b), s.t. W = xxG(1), . . . , G(N)yy. (2.13)\nIn this way, the weight tensor W is approximated by a low-TT rank tensor of the TT-rank (R1, . . . , RN\u00b41) of at most R.\nThe low-rank constraint for the tensor W can also be formulated through the tensor norm, in the form [Wimalawarne et al., 2016]\nmin W,b (J(X, y | W, b) + \u03bb}W}), (2.14)\nwhere } \u00a8 } is a suitably chosen tensor/matrix norm. One of the important and useful tensor norms is the tensor nuclear norm [Liu et al., 2013] or the (overlapped) trace norm [Wimalawarne et al., 2014], which can be defined for a tensor W P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN as\n}W}\u02da = N \u00ff\nn=1\n}W(n)}\u02da, (2.15)\nwhere }W(n)}\u02da = \u0159 i \u03c3i, \u03c3i denotes the ith singular value of W(n). The overlapped tensor trace norm can be viewed as a direct extension of the matrix trace norm, since it uses unfolding matrices of a tensor in all of its modes, and then computes the sums of trace norms of those unfolding matrices. Regularization based on the overlapped trace norm can also be viewed as an overlapped group regularization, due to the fact that the same tensor is unfolded over all modes and regularized using the trace norm.\nRecently Tomioka and Suzuki [2013] proposed the latent trace norm of a tensor, which takes a mixture of N latent tensors, W(n), and regularizes each of them separately, as in (2.15), to give\n}W}latent = inf W\nN \u00ff\nn=1\n}W(n) (n)}\u02da, (2.16)\nwhere W = W(1) + W(2) + \u00a8 \u00a8 \u00a8+ W(N) and W(n) (n) denotes the unfolding of W(n) in its nth mode. In contrast to the nuclear norm, the latent tensor trace norm effectively regularizes different latent tensors in each unfolded mode,\n55\nwhich makes it possible for the latent tensor trace norm to identify (in some cases) a latent tensor with the lowest possible rank. In general, the content of each latent tensor depends on the rank of its unfolding matrices.\nRemark 2. A major drawback of the latent trace norm approach is its inability to identify the rank of a tensor, when the rank value is relatively close to its dimension (size). In other words, if a tensor has a mode with a dimension (size) much smaller than other modes, the latent trace norm may be incorrectly estimated. To deal with this problem, the scaled latent norm was proposed by Wimalawarne et al. [2014] which is defined as\n}W}scaled = inf W\nN \u00ff\nn=1\n1? In }W(n) (n)}\u02da, (2.17)\nwhere W = W(1) + W(2) + \u00a8 \u00a8 \u00a8+ W(N). Owing to the normalization by the mode size, in the form of (2.17), the scaled latent trace norm scales with mode dimension, and thus estimates more reliably the rank, even when the sizes of various modes are quite different.\nThe state-of-the-art tensor regression models therefore employ the scaled latent trace norm, and are solved by the following optimization problem\nP(W, b) = min W,b\n( M \u00ff\nm=1\n( ym \u00b4 ( xW, Xmy+ b ))2 + N \u00ff\nn=1\n\u03bbn}W(n)(n)}\u02da ) , (2.18)\nwhere W = W(1) + W(2) + \u00a8 \u00a8 \u00a8+ W(N), for n = 1, . . . , N, and for any given regularization parameter \u03bb, where \u03bbn = \u03bb in the case of latent trace norm and \u03bbn = \u03bb?In in the case of the scaled latent trace norm, while W (n) (n) denotes the unfolding of W(n) in its nth mode. Remark 3. Note that the above optimization problems involving the latent and scaled latent trace norms require a large number (N \u015b\nn In) of variables in N latent tensors.\nMoreover, the existing methods based on SVD are infeasible for very large-scale applications. Therefore, for very large scale high-order tensors, we need to use tensor network decompositions and perform all operations in tensor networks formats, taking advantage of their lower-order core tensors.\n56"}, {"heading": "2.3 Higher-Order Low-Rank Regression (HOLRR)", "text": "Consider a full multivariate regression task in which the response has a tensor structure. Let f : RI0 \u00d1 RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN be a function we desire to learn from input-output data, txm, YmuMm=1, drawn from the model Y = W \u00af\u0302 1 x + E, where E is an error tensor, x P RI0 , Y P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN , and W P RI0\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN is a tensor of regression coefficients. The extension of lowrank regression methods to tensor-structured responses can be achieved by enforcing low multilinear rank of the regression tensor, W, to yield the socalled higher-order low rank regression (HOLRR) [Rabusseau and Kadri, 2016, Sun and Li, 2016]. The aim is to find a low-rank regression tensor, W, which minimizes the loss function based on the training data.\nTo avoid numerical instabilities and to prevent overfitting, it is convenient to employ a ridge regularization of the objective function, leading to the following minimization problem\nmin W\nM \u00ff\nm=1\n}W \u00af\u0302 1 xm \u00b4 Ym}2F + \u03b3}W}2F\ns.t. multilinear rank(W) \u010f (R0, R1, . . . , RN),\n(2.19)\nfor some given integers R0, R1, . . . , RN . Taking into account the fact that input vectors, xm, can be concatenated into an input matrix, X = [x1, . . . , xM]T P RM\u02c6I0 , the above optimization problem can be reformulated in the following form\nmin W\n}W\u02c61 X\u00b4 Y}2F + \u03b3}W}2F\ns.t. W = G\u02c61 U(0) \u02c62 U(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N), U(n)TU(n) = I for n = 1, . . . , N,\n(2.20)\nwhere the output tensor Y is obtained by stacking the output tensors Ym along the first mode Y(m, :, . . . , :) = Ym. The regression function can be rewritten as\nf : x \u00de\u00d1 G\u02c61 xTU(0) \u02c62 U(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N). (2.21)\nThis minimization problem can be reduced to finding (N + 1) projection matrices onto subspaces of dimensions, R0, R1, . . . , RN , where the core tensor G is given by\nG = Y\u02c61 (U(0)T(XTX + \u03b3I)U(0))\u00b41U(0)TXT \u02c62 U(1)T \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N)T, (2.22)\n57\nAlgorithm 2: Higher-Order Low-Rank Regression (HOLRR) Input: X P RM\u02c6I0 , Y P RM\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN , rank (R0, R1, . . . , RN)\nand a regularization parameter \u03b3. Output: W = G\u02c61 U(0) \u02c62 U(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N)\n1: U(0) \u00d0 top R0 eigenvectors of (XTX + \u03b3I)\u00b41XTY(1)YT(1)X 2: for n = 1 to N do 3: U(n) \u00d0 top Rn eigenvectors of Y(n)YT(n) 4: end for 5: T = ( U(0)T(XTX + \u03b3I)U(0) )\u00b41 U(0)TXT\n6: G \u00d0 Y\u02c61 T\u02c62 U(1)T \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N)T\nand the orthogonal matrices U(n) for n = 0, 1, . . . , N can be computed by the eigenvectors of\n#\n(XTX + \u03b3I)\u00b41XTY(1)YT(1)X, n = 0, Y(n)YT(n), otherwise.\n(2.23)\nIn other words, by the computation of Rn largest eigenvalues and the corresponding eigenvectors. The HOLRR procedure is outlined in Algorithm 2."}, {"heading": "2.4 Kernelized HOLRR", "text": "The HOLRR can be extended to its kernelized version, the kernelized HOLRR (KHOLRR). Let \u03c6 : RI0 \u00d1 RL be a feature map and let \u03a6 P RM\u02c6L be a matrix with row vectors \u03c6(xTm) for m P t1, . . . , Mu. The HOLRR problem in the feature space boils down to the ridge regularized minimization problem\nmin W\n}W\u02c61 \u03a6\u00b4 Y}2F + \u03b3}W}2F\ns.t. W = G\u02c61 U(0) \u02c62 U(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N), and U(n)TU(n) = I for n = 0, . . . , N.\n(2.24)\nThe tensor W is represented in a Tucker format. Then, the core tensor G is given by\nG = Y\u02c61 (U(0)TK(K + \u03b3I)U(0))\u00b41U(0)TK\u02c62 U(1)T \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N)T, (2.25)\n58\nAlgorithm 3: Kernelized Higher-Order Low-Rank Regression (KHOLRR)\nInput: Gram matrix K P RM\u02c6M, Y P RM\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN , rank (R0, R1, . . . , RN) and a regularization parameter \u03b3. Output: W = G\u02c61 U(0) \u02c62 U(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N) 1: U(0) \u00d0 top R0 eigenvectors of (K + \u03b3I)\u00b41Y(1)YT(1)K 2: for n = 1 to N do 3: U(n) \u00d0 top Rn eigenvectors of Y(n)YT(n) 4: end for 5: T = ( U(0)TK(K + \u03b3I)U(0) )\u00b41 U(0)TK\n6: G \u00d0 Y\u02c61 T\u02c62 U(1)T \u00a8 \u00a8 \u00a8 \u02c6N+1 U(N)T\nwhere K is the kernel matrix and the orthogonal matrices U(n), n = 0, 1, . . . , N, can be computed via the eigenvectors of\n#\n(K + \u03b3I)\u00b41Y(1)YT(1)K, n = 0, Y(n)YT(n), otherwise,\n(2.26)\nwhich corresponds to the computation of Rn largest eigenvalues and the associated eigenvectors, where K = \u03a6\u03a6T is the kernel matrix. The KHOLRR procedure is outlined in Algorithm 3.\nNote that the kernel HOLRR returns the weight tensor, W P RM\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN , which is used to define the regression function\nf : x \u00de\u00d1 W \u00af\u0302 1kx = M \u00ff\nm=1\nk(x, xm)Wm, (2.27)\nwhere Wm = W(m, :, . . . , :) P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN and the mth component of kx is k(x, xm)."}, {"heading": "2.5 Higher-Order Partial Least Squares (HOPLS)", "text": "In this section, Partial Least Squares (PLS) method is briefly presented followed by its generalizations to tensors."}, {"heading": "2.5.1 Standard Partial Least Squares", "text": "The principle behind the PLS method is to search for a common set of latent vectors in the independent variable X P RI\u02c6J and the dependent\n59\nvariable Y P RI\u02c6M by performing their simultaneous decomposition, with the constraint that the components obtained through such a decomposition explain as much as possible of the covariance between X and Y. This problem can be formulated as (see also Figure 2.2)\nX = TPT + E = R \u00ff\nr=1\ntrpTr + E, (2.28)\nY = TDCT + F = R \u00ff\nr=1\ndrrtrcTr + F, (2.29)\nwhere T = [t1, t2, . . . , tR] P RI\u02c6R consists of R orthonormal latent variables from X, and a matrix U = TD = [u1, u2, . . . , uR] P RI\u02c6R represents latent variables from Y which have maximum covariance with the latent variables, T, in X. The matrices P and C represent loadings (PLS regression coefficients), and E, F are respectively the residuals for X and Y, while D is a scaling diagonal matrix.\nThe PLS procedure is recursive, so that in order to obtain the set of first latent components in T, the standard PLS algorithm finds the two sets of weight vectors, w and c, through the following optimization problem\nmax tw,cu\n( wTXTYc )2 , s.t. wTw = 1, cTc = 1. (2.30)\nThe obtained latent variables are given by t = Xw/}Xw}22 and u = Yc. In doing so, we have made the following two assumptions: i) the latent\n60\nvariables ttruRr=1 are good predictors of Y; ii) the linear (inner) relation between the latent variables t and u does exist, that is U = TD + Z, where Z denotes the matrix of Gaussian i.i.d. residuals. Upon combining with the decomposition of Y, in (2.29), we have\nY = TDCT + (ZCT + F) = TDCT + F\u02da, (2.31)\nwhere F\u02da is the residual matrix. Observe from (2.31) that the problem boils down to finding common latent variables, T, that best explain the variance in both X and Y. The prediction of new dataset X\u02da can then be performed by Y\u02da \u00ab X\u02daWDCT.\n2.5.2 The N-way PLS Method\nThe multi-way PLS (called N-way PLS) proposed by Bro [1996] is a simple extension of the standard PLS. The method factorizes an Nthorder tensor, X, based on the CP decomposition, to predict response variables represented by Y, as shown in Figure 2.3. For a 3rd-order tensor, X P RI\u02c6J\u02c6K, and a multivariate response matrix, Y P RI\u02c6M, with the respective elements xijk and yim, the tensor of independent variables, X, is decomposed into one latent vector t P RI\u02c61 and two loading vectors, p P RJ\u02c61 and q P RK\u02c61, i.e., one loading vector per mode. As shown in Figure 2.3, the 3-way PLS (the N-way PLS for N = 3) performs the following simultaneous tensor and matrix decompositions\nX = R \u00ff\nr=1\ntr \u02dd pr \u02dd qr + E, Y = R \u00ff\nr=1\ndrr trcTr + F. (2.32)\nThe objective is to find the vectors pr, qr and cr, which are the solutions of the following optimization problem\ntpr, qr, cru = arg max pr ,qr ,cr cov(tr, ur),\ns.t. tr = X \u00af\u0302 1pr \u00af\u0302 2qr, ur = Ycr, }pr}22 = }qr}22 = }cr}22 = 1.\nAgain, the problem boils down to finding the common latent variables, tr, in both X and Y, that best explain the variance in X and Y. The prediction of a new dataset, X\u02da, can then be performed by Y\u02da(1) \u00ab X \u02da (1)(Q \u00c4\nP)\u00b41DCT, where P = [p1, . . . , pR], Q = [q1, . . . , qR], and D = diag(drr).\n61"}, {"heading": "2.5.3 HOPLS using Constrained Tucker Model", "text": "An alternative, more flexible and general multilinear regression model, termed the higher-order partial least squares (HOPLS) [Zhao et al., 2011, 2013a], performs simultaneously constrained Tucker decompositions for an (N + 1)th-order independent tensor, X P RM\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN , and an (N + 1)thorder dependent tensor, Y P RM\u02c6J1\u02c6\u00a8\u00a8\u00a8\u02c6JN , which have the same size in the first mode, i.e., M samples. Such a model allows us to find the optimal subspace approximation of X, in which the independent and dependent variables share a common set of latent vectors in one specific mode (i.e., samples mode). More specifically, we assume that X is decomposed as a sum of rank-(1, L1, . . . , LN) Tucker blocks, while Y is decomposed as a sum of rank-(1, K1, . . . , KN) Tucker blocks, which can be expressed as\nX = R \u00ff\nr=1\nGxr\u02c61 tr\u02c62 P (1) r \u00a8 \u00a8 \u00a8\u02c6N+1 P (N) r +ER,\nY = R \u00ff\nr=1\nGyr\u02c61 tr\u02c62 Q (1) r \u00a8 \u00a8 \u00a8\u02c6N+1Q (N) r +FR,\n(2.33)\n62\nwhere R is the number of latent vectors, tr P RM is the r-th latent vector, !\nP(n)r )N\nn=1 P RIn\u02c6Ln and\n! Q(n)r )N\nn=1 P RJn\u02c6Kn are the loading matrices in\nmode-n, and Gxr P R1\u02c6L1\u02c6\u00a8\u00a8\u00a8\u02c6LN and Gyr P R1\u02c6K1\u02c6\u00a8\u00a8\u00a8\u02c6KN are core tensors. By defining a latent matrix T = [t1, . . . , tR], mode-n loading matrix P (n) = [P(n)1 , . . . , P (n) R ], mode-n loading matrix Q (n) = [Q(n)1 , . . . , Q (n) R ] and core tensors\nGx = blockdiag(Gx1, . . . , GxR) P RR\u02c6RL1\u02c6\u00a8\u00a8\u00a8\u02c6RLN , Gy = blockdiag(Gy1, . . . , GyR) P RR\u02c6RK1\u02c6\u00a8\u00a8\u00a8\u02c6RKN ,\n(2.34)\nthe HOPLS model in (2.33) can be rewritten as\nX = Gx \u02c61 T\u02c62 P (1) \u00a8 \u00a8 \u00a8 \u02c6N+1 P (N) + ER, Y = Gy \u02c61 T\u02c62 Q (1) \u00a8 \u00a8 \u00a8 \u02c6N+1 Q (N) + FR,\n(2.35)\nwhere ER and FR are the residuals obtained after extracting R latent components. The core tensors, Gx and Gy, have a special blockdiagonal structure (see Figure 2.4) and their elements indicate the level of local interactions between the corresponding latent vectors and loading matrices.\nBenefiting from the advantages of Tucker decomposition over the CP model, HOPLS generates approximate latent components that better model the data than N-way PLS. Specifically, the HOPLS differs substantially from the N-way PLS model in the sense that the sizes of loading matrices are controlled by a hyperparameter, providing a tradeoff between the model fit and model complexity. Note that HOPLS simplifies into N-way PLS if we impose the rank-1 constraints for the Tucker blocks, that is @n : tLnu = 1 and @m : tKmu = 1.\nThe optimization of subspace transformation according to (2.33) can be formulated as a problem of determining a set of orthogonal loadings and latent vectors. Since these terms can be optimized sequentially, using the same criteria and based on deflation, in the following, we shall illustrate the procedure based on finding the first latent vector, t, and two sequences of loading matrices, P(n) and Q(n). Finally, we shall denote tensor contraction in mode one by xX, Yyt1;1u, so that the cross-covariance tensor is defined by\nC = COVt1;1u(X, Y) P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6J1\u02c6\u00a8\u00a8\u00a8\u02c6JN . (2.36)\n63\n64\nAlgorithm 4: Higher-Order Partial Least Squares (HOPLS) Input: X P RM\u02c6I1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN and Y P RM\u02c6J1\u02c6J2\u02c6\u00a8\u00a8\u00a8\u02c6JN . Output: tP(n)r u, tQ\n(n) r u, tGxru, tGyru, T, n = 1, . . . , N, r = 1, . . . , R.\n1: for r = 1 to R do 2: C \u00d0 COV1;1(X, Y) P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6J1\u02c6\u00a8\u00a8\u00a8\u02c6JN 3: Compute tP(n)r u and tQ (n) r u by HOOI decomposition of C. 4: tr \u00d0 the principal eigenvector of X\u02c62 P (1) r \u00a8 \u00a8 \u00a8 \u02c6N+1 P (N) r 5: Gxr \u00d0 X \u00af\u0302 1tr \u02c62 P (1)T r \u00a8 \u00a8 \u00a8 \u02c6N+1 P (N)T r 6: Gyr \u00d0 Y \u00af\u0302 1tr \u02c62 Q (1)T r \u00a8 \u00a8 \u00a8 \u02c6N+1 Q (N)T r 7: Deflation: X \u00d0 X\u00b4Gxr \u02c61 tr \u02c62 P (1) r \u00a8 \u00a8 \u00a8 \u02c6N+1 P (N) r 8: Deflation: Y \u00d0 Y\u00b4Gyr \u02c61 tr \u02c62 Q (1) r \u00a8 \u00a8 \u00a8 \u02c6N+1 Q (N) r 9: end for\nThe above optimization problem can now be formulated as\nmax tP(n),Q(n)u\n\u203a \u203a \u203a [[C; P(1)T,. . . ,P(N)T, Q(1)T,. . ., Q(N)T]] \u203a \u203a \u203a\n2\nF\ns.t. P(n)TP(n) = ILn , Q (n)TQ(n) = IKn , (2.37)\nwhere [[. . .]] denotes the multilinear products between a tensor and a set of matrices, P(n) and Q(n), n = 1, . . . , N, comprise the unknown parameters. This is equivalent to finding the best subspace approximation of C which can be obtained by rank-(L1, . . . , LN , K1, . . . , KN) HOSVD of tensor C. The higher-order orthogonal iteration (HOOI) algorithm, which is known to converge fast, can be employed to find the parameters P(n) and Q(n) by orthogonal Tucker decomposition of C. The detailed procedure of HOPLS is shown in Algorithm 4.\nThe prediction for a new sample set X\u02da is then performed as Y\u02da(1) \u00ab\nX\u02da(1)W xWy, where the rth column of Wxr is given by wxr = (P (N) r b \u00a8 \u00a8 \u00a8 b P(1)r )Gx : r(1) and the rth row of W y r is given by w y r = Gy : r(1)(Q (N) r b \u00a8 \u00a8 \u00a8 b Q(1)r )T."}, {"heading": "2.6 Kernel HOPLS", "text": "We next briefly introduce the concept of kernel-based tensor PLS (KTPLS or KHOPLS) [Hou et al., 2016b, Zhao et al., 2013c], as a natural extension\n65\nof the HOPLS to possibly infinite dimensional and nonlinear kernel spaces. Consider N pairs of tensor observations t(Xm, Ym)uMm=1, where Xm denotes an Nth-order independent tensor and Ym an Lth-order dependent tensor. Note that the data tensors, Xm and Ym, can be concatenated to form an (N + 1)th-order tensor X P RM\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN and (L + 1)th-order tensor Y P RM\u02c6J1\u02c6\u00a8\u00a8\u00a8\u02c6JL . Within the kernel framework, the data tensors X and Y are mapped onto the Hilbert space by a reproducing kernel mapping \u03c6 : Xm \u00de\u00dd\u00d1 \u03c6 (Xm). For simplicity, we shall denote \u03c6(X) by \u03a6 and \u03c6(Y) by \u03a8. The KTPLS then aims to perform the tensor decompositions, corresponding to those in (2.35), such that\n\u03a6 = GX \u02c61 T\u02c62 P(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 P(N) + EX, \u03a8 = GY \u02c61 U\u02c62 Q(1) \u00a8 \u00a8 \u00a8 \u02c6L+1 Q(L) + EY, U = TD + EU .\n(2.38)\nSince within the kernel framework, the tensors rGX = GX \u02c62 P(1) \u00a8 \u00a8 \u00a8 \u02c6N+1 P(N) and rGY = GY \u02c62 Q(1) \u00a8 \u00a8 \u00a8 \u02c6L+1 Q(L) can be respectively represented as a linear combination of t\u03c6(Xm)u and t\u03c6(Ym)u, i.e., rGX = \u03a6\u02c61 TT and rGY = \u03a8\u02c61 UT, for the KTPLS solution we only need to find the latent vectors of T = [t1, . . . , tR] and U = [u1, . . . , uR], such that they exhibit maximum pairwise covariance, through solving sequentially the following optimization problems\nmaxtw(n)r ,v(l)r u [cov(tr, ur)] 2, (2.39)\nwhere\ntr = \u03a6 \u00af\u0302 2w (1) r \u00a8 \u00a8 \u00a8 \u00af\u0302 N+1w (N) r ,\nur = \u03a8 \u00af\u0302 2v (1) r \u00a8 \u00a8 \u00a8 \u00af\u0302 L+1v (L) r .\n(2.40)\nUpon rewriting (2.40) in a matrix form, this yields tr = \u03a6(1) rwr where rwr = w (N) r b \u00a8 \u00a8 \u00a8 b w (1) r and ur = \u03a8(1)rvr where rvr = v (N) r b \u00a8 \u00a8 \u00a8 b v (1) r , which can be solved by a kernelized version of the eigenvalue problem, i.e., \u03a6(1)\u03a6 T (1)\u03a8(1)\u03a8 T (1)tr = \u03bbtr and ur = \u03a8(1)\u03a8 T (1)tr [Rosipal and Trejo, 2002]. Note that since the term \u03a6(1)\u03a6 T (1) contains only the inner products between vectorized input tensors, it can be replaced by an M\u02c6M kernel matrix KX, to give KXKYtr = \u03bbtr and ur = KYtr.\nIn order to exploit the rich multilinear structure of tensors, kernel matrices should be computed using the kernel functions for tensors, i.e.,\n66\nAlgorithm 5: Kernel Higher-Order Partial Least Squares (KHOPLS) Input: X P RM\u02c6I1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN and Y P RM\u02c6J1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6JL . Output: T, U, KX.\n1: Compute kernel matrix KX and KY by using tensor kernel functions 2: Compute tr by solving KXKYtr = \u03bbtr, r = 1, . . . , R 3: ur = KYtr, r = 1, . . . , R 4: T = [t1, . . . , tR] and U = [u1, . . . , uR]\n(KX)mm1 = k (Xm, Xm1) and (KY)mm1 = k (Ym, Ym1) (for more details see Section 2.7).\nFinally, for a new data sample, X\u02da, the prediction of y, denoted by y\u02da is performed by computing the vector\ny\u02daT = k\u02daTU(TTKXU)\u00b41TTY(1), (2.41)\nwhere (k\u02da)m = k (Xm, X \u02da) and the vector y\u02da can be reshaped to a tensor Y\u02da. The detailed procedure of KHOPLS is shown in Algorithm 5. The intuitive interpretation of (2.41) can take several forms: i) as a linear combination of M observations tYmu and the coefficients k\u02daTU(TTKXU)\u00b41TT; ii) as a linear combination of M kernels, each centered around a training point, i.e., y\u02daj = \u0159M m=1 \u03b1mk (Xm, X\n\u02da), where \u03b1m =( U(TTKXU)\u00b41TTY(1) ) mj ; iii) the vector t\u02da is obtained by nonlinearly projecting the tensor X\u02da onto the latent space spanned by t\u02daT = k\u02daTU(TTKXU)\u00b41, then y\u02daT is predicted by a linear regression against t\u02da, i.e., y\u02daT = t\u02daTC, where the regression coefficients are given by the matrix C = TTY(1).\nNote that, in general, to ensure a strict linear relationship between the latent vectors and output in the original spaces, the kernel functions for data Y are restricted to linear kernels."}, {"heading": "2.7 Kernel Functions in Tensor Learning", "text": "Kernel functions can be considered as a means for defining a new topology which implies a priori knowledge about the invariance in the input space [Scho\u0308lkopf and Smola, 2002] (see Figure 2.5). Kernel algorithms can also be used for estimation of vector-valued nonlinear and nonstationary signals [Tobar et al., 2014]. In this section, we discuss the kernels for tensor-valued\n67\ninputs, which should exploit multi-way structures while maintaining the notion of similarity measures.\nMost straightforward tensor-valued reproducing kernels are a direct generalization from vectors to Nth-order tensors, such as the following kernel functions k : X\u02c6 X \u00d1 R, given by\nLinear kernel: k(X, X1) = xvec(X), vec(X1)y, Gaussian-RBF kernel: k(X, X1) = exp ( \u00b4 1\n2\u03b22 }X\u00b4 X1}2F\n) .\n(2.42)\nIn order to define a similarity measure that directly exploits the multilinear algebraic structure of input tensors, Signoretto et al. [2011, 2012] proposed a tensorial kernel which both exploits the algebraic geometry of tensors spaces and provides a similarity measure between the different subspaces spanned by higher-order tensors. Another such kernel is the product kernel which can be defined by N factor kernels, as k(X, X1) = \u015bN\nn=1 k ( X(n), X1(n) )\n, where each factor kernel represents a similarity measure between mode-n matricizations of two tensors, X and X1.\nOne similarity measure between matrices is the so called Chordal distance, which is a projection of the Frobenius norm on a Grassmannian manifolds [Signoretto et al., 2011]. For example, for an Nth-order tensor, X, upon the applications of SVD to its mode-n unfolding, that is, X(n) = U(n)X \u03a3 (n) X V (n)T X , the Chordal distance-based kernel for tensorial data is\n68\ndefined as\nk(X, X1) = N \u017a\nn=1\nexp ( \u00b4 1\n2\u03b22\n\u203a \u203a\n\u203a V(n)X V (n)T X \u00b4V (n) X1 V (n)T X1\n\u203a \u203a \u203a\n2\nF\n) , (2.43)\nwhere \u03b2 is a kernel parameter. It should be emphasized that such a tensor kernel ensures (provides) rotation and reflection invariance for elements on the Grassmann manifold [Signoretto et al., 2011].\nZhao et al. [2013c] proposed a whole family of probabilistic product kernels based on generative models. More specifically, an Nth-order tensor-valued observation is first mapped onto an N-dimensional model space, then information divergence is applied as a similarity measure in such a model space (see also [Cichocki et al., 2011, 2015]).\nThe advantage of probabilistic tensor kernels is that they can deal with multiway data with missing values and with variable data length. Given that probabilistic kernels provide a way to model one tensor from N different viewpoints which correspond to different lower-dimensional vector spaces, this makes it possible for multiway relations to be captured within a similarity measure.\nA general similarity measure between two tensors, X and X1, in mode-n can be defined as\nSn(X}X1) = D ( p(x|\u2126Xn )}q(x|\u2126X 1 n ) ) , (2.44)\nwhere p and q are distributions which respectively represent the probability density function for X and X1, D(p}q) is an information divergence between two distributions, while \u2126Xn denotes the parameters of a moden distribution of X, which depends on the model assumption.\nFor simplicity, we usually assume Gaussian models for all modes of tensor X, so that \u2126 can be expressed by the mean values and covariance matrices of that distribution. One simple and very useful information divergence is the standard symmetric Kullback-Leibler (sKL) divergence [Moreno et al., 2003], expressed as\nDsKL ( p(x|\u03bb)}q(x|\u03bb1) ) =\n1 2\n\u017c +8\n\u00b48 p(x|\u03bb) log p(x|\u03bb) q(x|\u03bb1) dx\n+ 1 2\n\u017c +8\n\u00b48 q(x|\u03bb1) log q(x|\u03bb\n1) p(x|\u03bb) dx, (2.45)\nwhere \u03bb and \u03bb1 are parameters of the probability distributions. An alternative simple divergence is the symmetric Jensen-Shannon (JS)\n69\ndivergence [Chan et al., 2004, Cichocki et al., 2011, Endres and Schindelin, 2003], given by\nDJS(p}q) = 1 2 KL(p}r) + 1 2 KL(q}r), (2.46)\nwhere KL(\u00a8}\u00a8) denotes the Kullback-Leibler divergence and r(x) = p(x)+q(x)\n2 . In summary, a probabilistic product kernel for tensors can be defined as\nk(X, X1) = \u03b12 N \u017a\nn=1\nexp ( \u00b4 1\n2\u03b22n Sn(X}X1)\n) , (2.47)\nwhere Sn(X}X1) is a suitably chosen probabilistic divergence, \u03b1 denotes a magnitude parameter and [\u03b21, . . . , \u03b2N ] are length-scale parameters.\nAn intuitive interpretation of the kernel function in (2.47) is that Nthorder tensors are assumed to be generated from N generative models, while the similarity of these models, measured by information divergence, is employed to provide a multiple kernel with well defined properties and conditions. This kernel can then effectively capture the statistical properties of tensors, which promises to be a powerful tool for multidimensional structured data analysis, such as video classification and multichannel feature extraction from brain electrophysiological responses.\nThere are many possibilities to define kernel functions for tensors, as outlined in Table 2.1 and Figure 2.6.\n70\n71"}, {"heading": "2.8 Tensor Variate Gaussian Processes (TVGP)", "text": "Gaussian processes (GP) can be considered as a class of probabilistic models which specify a distribution over a function space, where the inference is performed directly in the function space [Rasmussen and Williams, 2006]. The GP model for tensor-valued input spaces, called the Tensor-based Gaussian Processes (Tensor-GP), is designed so as to take into account the tensor structure of data [Hou et al., 2015, Zhao et al., 2013c, 2014].\nGiven a paired dataset of M observations D = t(Xm, ym)|m = 1, . . . , Mu, the tensor inputs for all M instances (cases) are aggregated into an (N + 1)th-order design concatenated tensor X P RM\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN , while the targets are collected in the vector y = [y1, . . . , yM]T. After observing the training data D = tX, yu, the aim is to make inferences about the relationship between the inputs X and targets (output y), i.e., to perform estimation of the conditional distribution of the targets given the inputs, and in doing so to perform the prediction based on a new input, X\u02da, which has not been seen in the training set.\nThe distribution of observations can be factored over cases in the training set by y \u201e\n\u015bM m=1N (ym| fm, \u03c32), where fm denotes the latent\nfunction f (Xm), and \u03c32 denotes noise variance. A Gaussian process prior can be placed over the latent function, which implies that any finite subset of latent variables has a multivariate Gaussian distribution, denoted by\nf (X) \u201e GP(m(X), k(X, X1) | \u03b8), (2.48)\nwhere m(X) is the mean function which is usually set to zero for simplicity, and k(X, X1) is the covariance function (e.g., kernel function) for tensor data, with a set of hyper-parameters denoted by \u03b8.\nThe hyper-parameters from the observation model and the GP prior are collected in \u0398 = t\u03c3, \u03b8u. The model is then hierarchically extended to the third level by also giving priors over the hyperparameters in \u0398.\nTo incorporate the knowledge that the training data provides about the function f(X), we can use the Bayes rule to infer the posterior of the latent function f(X) = [ f (X1), . . . , f (XM)]T by\np (f|D, \u0398) = p (y|f, \u03c3) p (f|X, \u03b8)\u015f p (y|f, \u03c3) p (f|X, \u03b8) df , (2.49)\nwhere the denominator in (2.49) can be interpreted as the marginal\n72\nlikelihood obtained by the integration over f, to yield\ny|X, \u03b8, \u03c32 \u201e N (y|0, K + \u03c32I), (2.50)\nwhere (K)ij = k(Xi, Xj) denotes the covariance matrix or kernel matrix. Note that, since the Gaussian observation model is analytically tractable and so it avoids the approximate inference, the conditional posterior of the latent function f is Gaussian, and the posterior of f\u02da is also Gaussian, together with the observation y\u02da.\nFinally, the predictive distribution of y\u02da corresponding to X\u02da can be inferred as y\u02da|X\u02da, X, y, \u0398 \u201e N (y\u02da, cov(y\u02da)), where\ny\u02da = k(X\u02da, X)(k(X, X) + \u03c3 2 nI) \u00b41y,\ncov(y\u02da) = k(X\u02da, X\u02da)\u00b4 k(X\u02da, X)(k(X, X) + \u03c32I)\u00b41k(X, X\u02da). (2.51)\nThe classification problem consisting of Nth-order tensors Xm P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN which are associated with target classes ym P t1, . . . , Cu, where C \u0105 2 and m = 1, . . . , M, was investigated by Zhao et al. [2013b]. All class labels are collected in the M\u02c6 1 target vector y, and all tensors are concatenated in an (N + 1)th-order tensor X of size M\u02c6 I1\u02c6\u00a8 \u00a8 \u00a8\u02c6 IN . Given the latent function fm = [ f 1m, f 2m, . . . , f Cm ]T = f(Xm) at the observed input location Xm, the class labels ym are assumed to be independent and identically distributed, as defined by a multinomial probit likelihood model p(ym|f). The latent vectors from all observations are denoted by f = [ f 11 , . . . , f 1 M, f 2 1 , . . . , f 2 M, . . . , f C1 , . . . , f C M]\nT. The objective of TVGP is to predict the class membership for a new input tensor, X\u02da, given the observed data, D = tX, yu. Gaussian process priors are placed on the latent function related to each class, which is the common assumption in multi-class GP classification (see [Rasmussen and Williams, 2006, Riihima\u0308ki et al., 2012]).\nSuch a specification results in the zero-mean Gaussian prior for f, given by p(f|X) = N (0, K), (2.52) where K is a CM\u02c6CM block-diagonal covariance matrix and every matrix K1, . . . , KC (of size M\u02c6 M) on its diagonal corresponds to each class. The element Kci,j in a cth class covariance matrix defines the prior covariance between f ci and f c j , which is governed by a kernel function k(Xi, Xj), i.e., Kci,j = k(Xi, Xj) = Cov( f c i , f c j ) within the class c. Note that since the kernel function should be defined in tensor-variate input space, hence commonly used kernel functions, such as Gaussian RBF, are infeasible. Therefore, the\n73\nframework of probabilistic product kernel for tensors, described in Sec.2.7, should be applied. In a kernel function, the hyperparameters are defined so as to control the smoothness properties and overall variance of latent functions, and are usually combined into one vector \u03b8. For simplicity, we use the same \u03b8 for all classes. For the likelihood model, we consider the multinomial probit, which is a generalization of the probit model, given by\np(ym|fm) = Ep(um)\n$\n&\n%\nC \u017a\nc=1,c\u2030ym\n\u03a6(um + f ym m \u00b4 f cm)\n,\n.\n-\n, (2.53)\nwhere \u03a6 denotes the cumulative distribution function of the standard normal distribution, and the auxiliary variable um is distributed as p(um) = N (0, 1).\nBy applying the Bayes theorem, the posterior distribution of the latent function is given by\np(f|D, \u03b8) = 1 Z\np(f|X, \u03b8) M \u017a\nm=1\np(ym|fm), (2.54)\nwhere Z = \u015f p(f|X, \u03b8) \u015bM\nm=1 p(ym|fm)df is known as the marginal likelihood.\nThe inference for a test input, X\u02da, is performed in two steps. First, the posterior distribution of the latent function, f\u02da, is given as p(f\u02da|D, X\u02da, \u03b8) = \u015f\np(f\u02da|f, X\u02da, \u03b8)p(f|D, \u03b8)df. Then, we compute the posterior predictive probability of X\u02da, which is given by p(y\u02da|D, X\u02da, \u03b8) = \u015f\np(y\u02da|f\u02da)p(f\u02da|D, X\u02da, \u03b8)df\u02da. Since the non-Gaussian likelihood model results in an analytically intractable posterior distribution, variational approximative methods can be used for approximative inference. The additive multiplicative nonparametric regression (AMNR) model constructs f as the sum of local functions which take the components of a rank-one tensor as inputs [Imaizumi and Hayashi, 2016]. In this approach, the function space and the input space are simultaneously decomposed.\nFor example, upon applying the CP decomposition, to give X = \u0159R\nr=1 u (1) r \u02dd \u00a8 \u00a8 \u00a8 \u02dd u (N) r , function f is decomposed into a set of local functions\nas\nf (X) = f (U(1), . . . , U(N)) = R \u00ff\nr=1\nN \u017a\nn=1\nf (n)r (u (n) r ). (2.55)\n74\nFor each local function, f (n)r , consider the GP prior GP( f (n) r ), which is represented as a multivariate Gaussian distribution N (0In , K (n) r ). The likelihood function is then \u015b\nmN (ym| f (Xm), \u03c32). By employing the Gaussian processes regression, we can obtain the posterior distribution p( f |X, y), and the predictive distribution of p( f \u02da|X, y, X\u02da)."}, {"heading": "2.9 Support Tensor Machines", "text": "In this section, Support Vector Machine (SVM) is briefly reviewed followed by the generalizations of SVM to matrices and tensors."}, {"heading": "2.9.1 Support Vector Machines", "text": "The classical SVM [Cortes and Vapnik, 1995] aims to find a classification hyperplane which maximizes the margin between the \u2018positive\u2019 measurements and the \u2018negative\u2019 measurements, as illustrated in Figure 2.7. Consider M training measurements, xm P RL(m = 1, . . . , M), associated with one of the two class labels of interest ym P t+1,\u00b41u. The standard SVM, that is, the soft-margin SVM, finds a projection vector w P RL and a bias b P R through the minimization of the cost function\nmin w,b,\u03be J(w, b, \u03be) = 1 2 }w}2 + C\nM \u00ff\nm=1\n\u03bem,\ns.t. ym(wTxm + b) \u011b 1\u00b4 \u03bem, \u03be \u011b 0, m = 1, . . . , M,\n(2.56)\nwhere \u03be = [\u03be1, \u03be2, . . . , \u03beM]T P RM is the vector of all slack variables1 required to deal with this linearly nonseparable problem. The parameter \u03bem, m = 1, . . . , M, is also called the marginal error for the mth training measurement, while the margin is 2}w}2 . When the classification problem is linearly separable, we can set \u03be = 0. The decision function for such classification is the binary y(xm) = sign[wTxm + b].\nThe soft-margin SVM can be simplified into the least squares SVM [Suykens and Vandewalle, 1999, Zhao et al., 2014b], given by\nmin w,b,\u03b5 J(w, b, \u03b5) = 1 2 }w}2 + \u03b3 2 \u03b5T\u03b5,\ns.t. ym(wTxm + b) = 1\u00b4 \u03b5m, m = 1, . . . , M, (2.57)\n1In an optimization problem, a slack variable is a variable that is added to an inequality constraint to transform it to an equality.\n75\nwhere the penalty coefficient \u03b3 \u0105 0. The two differences between the soft-margin SVM and the least squares SVM are: 1) the inequality constraints in (2.56) are replaced by equality constraints in (2.57); and 2) the loss\n\u0159M m=1 \u03bem(\u03bem \u011b 0) is replaced by a\nsquared loss. These two modifications enable the solution of the leastsquares SVM to be more conveniently obtained, compared to the softmargin SVM.\nRemark 4: According to the statistical learning theory [Vapnik and Vapnik, 1998], SVM-based learning performs well when the number of training measurements is larger than the complexity of the model. Moreover, the complexity of the model and the number of parameters to describe the model are always in a direct proportion."}, {"heading": "2.9.2 Support Matrix Machines (SMM)", "text": "Recall that the general SVM problem in (2.56) deals with data expressed in a vector form, x. If, on the other hand, the data are collected as matrices, X, these are typically first vectorized by vec(X), and then fed to the SVM. However, in many classification problems, such as EEG classification, data is naturally expressed by matrices, the structure information could be exploited for a better solution. For matrices, we have xW, Wy = tr(WTW), and so intuitively we could consider the following formulation for the soft\n76\nmargin support matrix machine [Luo et al., 2015a]\nmin W,b,\u03be 1 2\ntr(WTW) + C M \u00ff\nm=1\n\u03bem\ns.t. ym(tr(WTXm) + b) \u011b 1\u00b4 \u03bem, \u03be \u011b 0, m = 1, . . . , M.\n(2.58)\nHowever, observe that when w = vec(WT) the above formulation is essentially equivalent to (2.56) and does not exploit the correlation between the data channels inherent to a matrix structure, since\ntr(WTXm) = vec(WT)Tvec(XTm) = w Txm,\ntr(WTW) = vec(WT)Tvec(WT) = wTw, (2.59)\nIn order to include correlations between the rows or columns of data matrices, Xm, the nuclear norm can be introduced so that the problem becomes\narg min W,b,\u03bem 1 2\ntr(WTW) + \u03c4}W}\u02da + C m \u00ff\nm=1\n\u03bem\ns.t. ym(tr(WTXm) + b) \u011b 1\u00b4 \u03bem. (2.60)\nNote that this is a generalization of the standard SVM, which is obtained for \u03c4 = 0. The solution to (2.60) is obtained as\nW\u0303 = D\u03c4\n( M \u00ff\nm=1\n\u03b2\u0303mymXm ) , (2.61)\nwhere D\u03c4(\u00a8) is the singular value thresholding operator, which suppresses singular values below \u03c4 to zeros. Denote by \u2126 =\n\u0159M m=1 \u03b2\u0303mymXm a\ncombination of Xm associated to non-zero \u03b2\u0303m, which are the so called support matrices [Luo et al., 2015a].\nThe solution to (2.60) can be obtained by rewriting the problem as\nargmin W,b 1 2\ntr(WTW) + \u03c4}W}\u02da + C M \u00ff\nm=1\n[1\u00b4 ym(tr(WTXm) + b)]+, (2.62)\nwhich is equivalent to\nargmin W,b,S H(W, b) + G(S) s.t. S\u00b4W = 0, (2.63)\n77\nand\nH(W, b) = 1 2\ntr(WTW) + C M \u00ff\nm=1\n[1\u00b4 ym(tr(WTXm) + b)]+\nG(S) = \u03c4}S}\u02da,\n(2.64)\nwhere [x]+ = maxtx, 0u. The solution is obtained using an augmented Lagrangian form\nL(W, b, S, \u039b) = H(W, b) + G(S) + tr(\u039bT(S\u00b4W)) + \u03c1 2 }S\u00b4W}2F, (2.65)\nwhere \u03c1 \u0105 0 is a hyperparameter."}, {"heading": "2.9.3 Support Tensor Machines (STM)", "text": "Consider now a typical problem in computer vision, where the objects are represented by data tensors and the number of the training measurement is limited. This naturally leads to a tensor extension of SVM, called the support tensor machine (STM). Consider a general supervised learning scenario with M training measurements, tXm, ymu, m = 1, . . . , M, represented by Nthorder tensors, Xm P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN , which are associated with the scalar variable ym. There are two possible scenarios: 1) ym takes a continuous set of values, which leads to the tensor regression problem, and 2) ym P t+1,\u00b41u that is, it takes categorical values, which is a standard classification problem. For the classification case, STM [Biswas and Milanfar, 2016, Hao et al., 2013, Tao et al., 2005] can be formulated through the following minimization problem\nmin wn,b,\u03be J(wn, b, \u03be) = 1 2 \u203a \u203a \u203a bNn=1wn \u203a \u203a \u203a 2 + C\nM \u00ff\nm=1\n\u03bem\ns.t. ym (Xm \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 NwN + b) \u011b 1\u00b4 \u03bem, \u03be \u011b 0, m = 1, . . . , M.\n(2.66)\nHere, \u03be = [\u03be1, \u03be2, . . . , \u03beM]T P RM is the vector of all slack variables which helps to deal with linearly nonseparable problems.\nThe STM problem is therefore composed of N quadratic programming (QP)sub-problems with inequality constraints, where the nth QP sub-\n78\nproblem can be formulated, as follows [Hao et al., 2013]:\nmin wn,b,\u03be 1 2 }wn}2\ni\u2030n \u017a\n1\u010fi\u010fN (}wi}2) + C\nM \u00ff\nm=1\n\u03bem\ns.t. ym ( wTn (Xm \u00af\u0302 i\u2030nwi) + b ) \u011b 1\u00b4 \u03bem, \u03be \u011b 0,\nm = 1, . . . , M.\n(2.67)\nBased on the least squares SVM in (2.57), its tensor extension, referred to as the least squares STM (LS-STM), can be formulated as\nmin wn,b,\u03b5 J(wn, b, \u03b5) = 1 2 }w1 bw2 b \u00a8 \u00a8 \u00a8 bwN}2 + \u03b3 2 \u03b5T\u03b5\ns.t. ym (Xm \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 NwN + b) = 1\u00b4 \u03b5m, m = 1, . . . , M. (2.68)\nThen, the LS-STM solution can be obtained by the following alternating least squares optimization problem (see Algorithm 6)\nmin wn,b,\u03b5 1 2 }wn}2\ni\u2030n \u017a\n1\u010fi\u010fN (}wi}2) +\n\u03b3 2 \u03b5T\u03b5\ns.t. ym ( wTn (Xm \u00af\u0302 i\u2030nwi) + b ) = 1\u00b4 \u03b5m, m = 1, . . . , M.\n(2.69)\nOnce the STM solution has been obtained, the class label of a test example, X\u02da, can be predicted by a nonlinear transformation\ny(X) = sign (X\u02da \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 NwN + b) . (2.70)\nIn practice, it is often more convenient to solve the optimization problem (2.67) by considering the dual problem, given by\nmax t\u03b1iuMi=1\nM \u00ff\nm=1\n\u03b1m \u00b4 1 2\nM \u00ff\ni,j=1\n\u03b1i\u03b1jyiyjxXi, Xjy,\ns.t. M \u00ff\nm=1\n\u03b1mym = 0, 0 \u010f \u03b1m \u010f C, m = 1, . . . , M,\n(2.71)\nwhere \u03b1m are the Lagrange multipliers and xXi, Xjy the inner products of Xi and Xj.\nIt is obvious that when the input samples, Xi, are vectors, this optimization model simplifies into the standard vector SVM. Moreover, if\n79\nAlgorithm 6: Least Squares Support Tensor Machine (LS-STM) Input: tXm, ymu, m = 1, . . . , M where Xm P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN ,\nym P t+1,\u00b41u and the penalty coefficient \u03b3. Output: twnu, n = 1, . . . , N and b.\n1: Initialization: Random vectors twnu, n = 1, . . . , N. 2: while not converged or iteration limit is not reached do 3: for n = 1 to N do 4: \u03b7 \u00d0\n\u015bi\u2030n 1\u010fi\u010fN }wi}2,\n5: xm = Xm \u00af\u0302 i\u2030nwi 6: Compute wn by solving the optimization problem\nmin wn ,b,\u03b5\n\u03b7 2 }wn}2 + \u03b3 2 \u03b5T\u03b5\ns.t. ym ( wTnxm + b ) = 1\u00b4 \u03b5m,\nm = 1, . . . , M.\n7: end for 8: end while\nthe original input tensors are used to compute xXi, Xjy, then the optimal solutions are the same as those produced by the SVM. Consider now the rank-one decompositions of Xi and Xj in the form Xi \u00ab \u0159R r=1 x (1) ir \u02dd x (2) ir \u02dd \u00a8 \u00a8 \u00a8 \u02dd x(N)ir and Xj \u00ab \u0159R r=1 x (1) jr \u02dd x (2) jr \u02dd \u00a8 \u00a8 \u00a8 \u02dd x (N) jr . Then, the inner product of Xi and Xj is given by Hao et al. [2013]\nxXi, Xjy \u00ab R \u00ff\np=1\nR \u00ff\nq=1\nA\nx(1)ip , x (1) jq\nEA\nx(2)ip , x (2) jq\nE \u00a8 \u00a8 \u00a8 A x(N)ip , x (N) jq E , (2.72)\nand (2.71) can be solved by a sequential QP optimization algorithm. The class label of a test example X\u02da is then predicted as\ny(X\u02da) = sign  M\u00ff m=1 R \u00ff p=1 R \u00ff q=1 \u03b1mym N \u017a n=1 A x(n)mp , x (n) \u02daq E + b  . (2.73)"}, {"heading": "2.10 Higher Rank Support Tensor Machines (HRSTM)", "text": "Higher Rank STMs (HRSTM) aim to estimate a set of parameters in the form of the sum of rank-one tensors [Kotsia et al., 2012], which defines a\n80\nseparating hyperplane between classes of data. The benefits of this scheme are twofold:\n1. The use of a direct CP representation is intuitively closer to the idea of properly processing tensorial input data, as the data structure is more efficiently retained;\n2. The use of simple CP decompositions allows for multiple projections of the input tensor along each mode, leading to considerable improvements in the discriminative ability of the resulting classifier.\nThe corresponding optimization problems can be solved in an iterative manner utilizing, e.g., the standard CP decomposition, where at each iteration the parameters corresponding to the projections along a single tensor mode are estimated by solving a typical STM-type optimization problem.\nThe aim of the so formulated STM is therefore to learn a multilinear decision function, g : RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN \u00d1 [\u00b41, 1], which classifies a test tensor X P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN , by using the nonlinear transformation g(X) = sign(xX, Wy+ b).\nThe weight tensor, W, is estimated by solving the following soft STMs optimization problem\nmin W,b,\u03be 1 2 xW, Wy+ C\nM \u00ff\nm=1\n\u03bem,\ns.t. ym (xW, Xmy+ b) \u011b 1\u00b4 \u03bem, \u03bem \u011b 0, m = 1, . . . , M,\n(2.74)\nwhere b is the bias term, \u03be = [\u03be1, . . . , \u03bem]T is the vector of slack variables and C is the term which controls the relative importance of penalizing the training errors. The training is performed in such a way that the margin of the support tensors is maximized while the upper bound on the misclassification errors in the training set is minimized.\nConsider, as an example, the case where the weight parameter, W, is represented via a CP model, to give\nW = R \u00ff\nr=1\nu(1)r \u02dd u (2) r \u02dd \u00a8 \u00a8 \u00a8 \u02dd u (N) r , (2.75)\nwhere u(n)r P RIn , n = 1, 2, . . . , N. Then, the n-th mode matricization of W can be written as\nW(n) = U (n)(U(N) d \u00a8 \u00a8 \u00a8 dU(n+1) dU(n\u00b41) d \u00a8 \u00a8 \u00a8 dU(1))T = U(n)(U(\u00b4n))T,\n(2.76)\n81\nwhere d denotes the Khatri-Rao product. Note that the inner product in (2.74) can be computed very efficiently e.g., in the form\nxW, Wy = tr[W(n)WT(n)] = vec(W(n)) Tvec(W(n)). (2.77)\nThe above optimization problem, however, is not convex with respect to all sets of parameters in W. For this reason, we adopt an iterative scheme in which at each iteration we solve only for the parameters that are associated with the n-th mode of the parameter tensor W, while keeping all the other parameters fixed, similarly to the ALS algorithm. More specifically, for the n-th mode, at each iteration we solve the following optimization problem\nmin W(n),b,\u03be 1 2\ntr (\nW(n)W T (n)\n) + C M \u00ff\nm=1\n\u03bem,\ns.t. ym ( tr ( W(n)X T m(n) ) + b ) \u011b 1\u00b4 \u03bem, \u03bem \u011b 0,\nm = 1, . . . , M, n = 1, . . . , N,\n(2.78)\nUnder the assumption that the tensor W is represented as a sum of rank one tensors, we can replace the above matrices by W(n) = U(n)(U(\u00b4n))T, and thus the above equation becomes\nmin U(n),b,\u03be 1 2\ntr(U(n)(U(\u00b4n))T(U(\u00b4n))(U(n))T) + C M \u00ff\nm=1\n\u03bem,\ns.t. ym ( tr ( U(n)(U(\u00b4n))TXTm(n) ) + b ) \u011b 1\u00b4 \u03bem, \u03bem \u011b 0,\nm = 1, . . . , M, n = 1, . . . , N.\n(2.79)\nAt each iteration the optimization problem is solved for only one set of matrices U(n) for the mode n, while keeping the other matrices U(k) for k \u2030 n, fixed. Note that the optimization problem defined in (2.79) can also be solved using a classic vector-based SVM implementation.\nLet us define A = U(\u00b4n) T U(\u00b4n), which is a positive definite matrix, and\nlet rU(n) = U(n)A 1 2 . Then,\ntr[U(n)(U(\u00b4n))T(U(\u00b4n))(U(n))T] = tr[rU(n)(rU(n))T]\n= vec(rU(n))Tvec(rU(n)). (2.80)\nBy letting rXm(n) = Xm(n)U(\u00b4n)A\u00b4 1 2 , we have\ntr[U(n)(U(\u00b4n))TXTm(n)] = tr[rU (n) rXTm(n)]\n= vec(rU(n))Tvec(rXm(n)). (2.81)\n82\nThen, the optimization problem (2.79) can be simplified as\nmin U(n),b,\u03be 1 2\nvec(rU(n))Tvec(rU(n)) + C M \u00ff\nm=1\n\u03bem,\ns.t. ym ( vec(rU(n))Tvec(rXm(n)) + b ) \u011b 1\u00b4 \u03bem, \u03bem \u011b 0,\nm = 1, . . . , M, n = 1, . . . , N.\n(2.82)\nObserve that now the STM optimization problem for the n-th mode in (2.79) can be formulated as a standard vector SVM problem with respect to rU(n). In other words, such a procedure leads to a straightforward implementation of the algorithm by solving (2.82) with respect to rU(n) using a standard SVM implementation, and then solving for U(n) as U(n) = rU(n)A\u00b4 1 2 ."}, {"heading": "2.11 Kernel Support Tensor Machines", "text": "The class of support tensor machines has been recently extended to the nonlinear case by using the kernel framework, and is referred to as kernel support tensor regression (KSTR) [Gao and Wu, 2012]. After mapping each row of every original tensor (or every tensor converted from original vector) onto a high-dimensional space, we obtain the associated points in a new high-dimensional feature space, and then compute the regression function. Suppose we are given a set of training samples tXm, ymu, m = 1, . . . , M, where each training sample, Xm, is a data point in RI1 bRI2 , where RI1 and RI2 are two vector spaces, and ym is the target value associated with Xm. Denote by zmp the p-th row of Xm, we can then use a nonlinear mapping function \u03d5(Xm) to map Xm onto a high-dimensional tensor feature space, and define a nonlinear mapping function for tensor Xm, given by\n\u03c6(Xm) =  \u03d5(zm1) \u03d5(zm2) ...\n\u03d5(zmI1)\n . (2.83)\n83\nIn this way, we can obtain the new kernel function\nK(Xm, Xn) = \u03c6(Xm)\u03c6(Xn) T =  \u03d5(zm1) \u03d5(zm2) ...\n\u03d5(zmI1)\n  \u03d5(zn1) \u03d5(zn2) ...\n\u03d5(znI1)\n T\n=  \u03d5(zm1)\u03d5(zn1)T \u00a8 \u00a8 \u00a8 \u03d5(zm1)\u03d5(znI1)T ... . . . ...\n\u03d5(zmI1)\u03d5(zn1) T \u00a8 \u00a8 \u00a8 \u03d5(zmI1)\u03d5(znI1)T\n . (2.84)\nNote that such a kernel function is quite different from the function used in the SVR - the output of this kernel function is a matrix as opposed to a scalar in SVR.\nFor instance, if we use an RBF kernel function within KSTR, then the mn-th element of the kernel matrix is expressed as\n\u03d5(zmp1)\u03d5(znp2) T = e\u00b4\u03b3}zmp1\u00b4znp2} 2 . (2.85)\nThe support tensor regression with an \u03b5-insensitive loss function is similar to standard support tensor regression, and the regression function in such a case can be defined as\nf (X) = uT\u03c6(X)v + b. (2.86)\nThis function can be estimated by solving the following quadratic programming problem\nmin u,v,b,\u03bem,\u03be\u02dam 1 2 \u203a \u203a \u203a uvT \u203a \u203a \u203a 2 + C\nM \u00ff\nm=1\n(\u03bem + \u03be \u02da m)\ns.t.\n$\n\u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 %\nym \u00b4 uT\u03c6(Xm)v\u00b4 b \u010f \u03b5\u00b4 \u03bem,\nuT\u03c6(Xm)v + b\u00b4 ym \u010f \u03b5 + \u03be\u02dam, m = 1, . . . , M,\n\u03bem, \u03be\u02dam \u011b 0,\n(2.87)\nwhere C is a pre-specified constant, \u03b5 is a user-defined scalar, and \u03bem, \u03be\u02dam are slack variables which represent the upper and lower constraints on the outputs of the classification system.\n84\nConsider again (2.87), and let u be a column vector of the same dimension as the row number of samples; we can then calculate the vector v using the Lagrange multiplier method, where the Lagrangian is constructed according to\nmax \u03b1m,\u03b1\u02dam,\u03b7m,\u03b7\u02dam min v,b,\u03bem,\u03be\u02dam\n$\n\u2019 \u2019 \u2019 \u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 \u2019 \u2019 \u2019 %\nL = 12}uv T}2 + C \u0159M m=1(\u03bem + \u03be \u02da m) \u00b4 \u0159M\nm=1(\u03b7m\u03bem + \u03b7 \u02da m\u03be \u02da m)\u00b4\n\u0159M m=1 \u03b1m(\u03b5 + \u03bem \u00b4 ym + uT\u03c6(Xm)v + b) \u00b4 \u0159M\nm=1 \u03b1 \u02da m(\u03b5 + \u03be \u02da m \u00b4 ym + uT\u03c6(Xm)v + b)\n,\n/ / / / / / .\n/ / / / / / -\ns.t. \u03b1m, \u03b1\u02dam, \u03b7m, \u03b7 \u02da m \u011b 0, m = 1, . . . , M,\n(2.88)\nand \u03b1m, \u03b1\u02dam, \u03b7m, \u03b7\u02dam are the Lagrange multipliers. In next step, we determine the Lagrange multipliers \u03b1m, \u03b1\u02dam and v, }v}2.\nAlternatively, let x1n = vT\u03c6(Xn) = 1 }u}4 \u0159M m=1(\u03b1m \u00b4 \u03b1\u02dam)K(Xm, Xn) be the\nnew training samples. Then, we construct another Lagrangian\nmax \u03b1m,\u03b1\u02dam,\u03b7m,\u03b7\u02dam min u,b,\u03bem,\u03be\u02dam\n$\n\u2019 \u2019 \u2019 \u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 \u2019 \u2019 \u2019 %\nL = 12}uv T}2 + C \u0159M m=1(\u03bem + \u03be \u02da m) \u00b4 \u0159M\nm=1(\u03b7m\u03bem + \u03b7 \u02da m\u03be \u02da m)\u00b4\n\u0159M m=1 \u03b1m(\u03b5 + \u03bem \u00b4 ym + x1mu + b) \u00b4 \u0159M\nm=1(\u03b1 \u02da m(\u03b5 + \u03be \u02da m + ym \u00b4 x1mu\u00b4 b))\n,\n/ / / / / / .\n/ / / / / / -\ns.t. \u03b1m, \u03b1\u02dam, \u03b7m, \u03b7 \u02da m \u011b 0, m = 1, . . . , M,\n(2.89)\nand obtain u and b. These two steps are performed iteratively to compute v, u, b.\nThe KSTR method has the following advantages over the STR method: i) it has a strong ability to learn and a superior generalization ability; ii) the KSTR is able to solve nonlinearly separable problems more efficiently. A disadvantage of KSTR is that the computational load of KSTR is much higher than that of STR."}, {"heading": "2.12 Tensor Fisher Discriminant Analysis (FDA)", "text": "Fisher discriminant analysis (FDA) has been widely applied for classification. It aims to find a direction which separates the class means well while minimizing the variance of the total training measurements.\n85\nThis procedure is equivalent to maximizing the symmetric KullbackLeibler divergence (KLD) between positive and negative measurements with identical covariances, so that the positive measurements are separated from the negative measurements. To extend the FDA to tensors, that is, to introduce Tensor FDA (TFDA), consider M training measurements, Xm P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN , m = 1, . . . , M, associated with the class labels ym P t+1,\u00b41u. The mean of positive training measurements is L+ = (1/M+) \u0159M m=1(I(ym = +1)Xm), the mean of the negative training measurements is L\u00b4 = (1/M\u00b4) \u0159M\nm=1(I(ym = \u00b41)Xm), the mean of all training measurements is L = (1/M)\n\u0159M m=1 Xm, and M+(M\u00b4) are the\nnumbers of the positive (negative) measurements. The decision function is a multilinear function y(X) = sign(X \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 NwN + b), while the projection vectors, wn P RIn , n = 1, . . . , N, and the bias b in TFDA are obtained from\nmax wn|Nn=1\nJ(wn) = }(L+ \u00b4 L\u00b4) \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 wN}2\n\u0159M m=1 }(Xm \u00b4 L) \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 wN}2\n. (2.90)\nUnfortunately, there is no closed-form solution for TFDA, however, alternating projection algorithms can be applied to iteratively obtain the desired solution using the TFDA framework. For the projection vectors wn|Nn=1, we can obtain the bias b using\nb = M\u00b4 \u00b4M+ \u00b4 (M+L+ + M\u00b4L\u00b4) \u00af\u0302 1w1 \u00a8 \u00a8 \u00a8 \u00af\u0302 NwN\nM\u00b4 + M+ . (2.91)\nThe regularized Multiway Fisher Discriminant Analysis (MFDA) [Lechuga et al., 2015] aims to impose the structural constraints in such a way that the weight vector w will be decomposed as w = wN b \u00a8 \u00a8 \u00a8 bw1. Let X(1) P RM\u02c6I1 I2\u00a8\u00a8\u00a8IN be an unfolded matrix of observed samples and Y P RM\u02c6C the matrix of dummy variables indicating the group memberships. The optimization problem of MFDA can then be formulated as\narg max w wTSBw wTSTw + \u03bbwTRw\n(2.92)\nwhere w denotes wN b \u00a8 \u00a8 \u00a8 bw1, SB = XT(1)Y(Y TY)\u00b41YTX(1) is the between covariance matrix, ST = XT(1)X(1) is the total covariance matrix, and R is usually an identity matrix. The regularization term \u03bbwTRw is added to improve the numerical stability when computing the inverse of ST in highdimensional settings (M ! I1 I2 \u00a8 \u00a8 \u00a8 IN). This optimization problem can be solved in an alternate fashion by fixing all wn except one.\n86\nThe Linear Discriminant Analysis (LDA) can also be extended to tensor data, which is referred to as Higher Order Discriminant Analysis (HODA) [Phan and Cichocki, 2010] and Multilinear Discriminant Analysis (MDA) [Li and Schonfeld, 2014]. Suppose that we have M tensor samples Xm P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN , m = 1, . . . , M belonging to one of the C classes, and Mc is the number of samples in class c such that M =\n\u0159C c=1 Mc. ym denotes\nthe associated class label of Xm. The class mean tensor for class c is computed by Lc = 1 Mc \u0159Mc m=1 Xm I(ym = c), and the total mean tensor is L = 1M \u0159M\nm=1 Xm. The goal is to find the set of optimal projection matrices W1, W2, . . . , WN (Wn P RIn\u02c6Jn for n = 1, . . . , N) that lead to the most accurate classification in the projected tensor subspace where\nZm = Xm \u02c61 WT1 \u00a8 \u00a8 \u00a8 \u02c6N WTN P RJ1\u02c6J2\u02c6\u00a8\u00a8\u00a8\u02c6JN . (2.93)\nThe mean tensor of class c in the projected subspace is given by\nrLc = 1\nMc\nMc \u00ff\nm=1\nZm I(ym = c) = Lc \u02c61 WT1 \u00a8 \u00a8 \u00a8 \u02c6N WTN , (2.94)\nwhich is simply the projection of Lc. Similarly, the total mean tensor of projected samples is L\u02c61 WT1 \u00a8 \u00a8 \u00a8 \u02c6N WTN .\nThe mode-n between-class scatter matrix in the projected tensor subspace can be derived, as follows\nBn = C \u00ff\nc=1\nMc (Lc N\u017a n=1 \u02c6nWTn ) (n) \u00b4 ( L N \u017a n=1 \u02c6nWTn ) (n)  (Lc N\u017a\nn=1\n\u02c6nWTn ) (n) \u00b4 ( L N \u017a n=1 \u02c6nWTn ) (n) T\n=WTn\n$\n&\n%\nC \u00ff\nc=1\nMc [ (Lc \u00b4 L) N \u017a\nk=1,k\u2030n \u02c6kWTn ] (n)[\n(Lc \u00b4 L) N \u017a\nk=1,k\u2030n \u02c6kWTn ]T (n) , . - Wn\n=WTnB\u00b4nWn.\n(2.95)\nNote that L \u015bN n=1\u02c6nWTn = L\u02c61 WT1 \u02c62 WT2 \u00a8 \u00a8 \u00a8 \u02c6N WTN . Here, B\u00b4n denotes the mode-n between-class scatter matrix in the partially projected tensor\n87\nsubspace (by all tensor modes except for mode n). The mode-n between class scatter matrix characterizes the separation between C classes in terms of mode-n unfolding of the tensor samples.\nSimilarly, the mode-n within-class scatter matrix is\nSn =WTn\n$\n&\n%\nC \u00ff\nc=1\nMc \u00ff\nm=1\n[ (Xcm \u00b4 Lc) N \u017a\nk=1,k\u2030n \u02c6kWTk ] (n)[\n(Xcm \u00b4 Lc) N \u017a\nk=1,k\u2030n \u02c6kWTk ]T (n) , . - Wn\n=WTnS\u00b4nWn,\n(2.96)\nwhere S\u00b4n represents the mode-n within-class scatter matrix in the partially projected tensor subspace (in all tensor modes except for n). Finally, the MDA objective function can be described as\nJ(Wn) = \u0159C c=1 Mc}(Lc \u00b4 L) \u015bN n=1\u02c6nWTn}2F \u0159C\nc=1 \u0159Mc m=1 }(X c m \u00b4 Lc) \u015bN n=1\u02c6nWTn}2F\n= Tr(WTnB\u00b4nWn) Tr(WTnS\u00b4nWn) ,\n(2.97)\nand the set of optimal projection matrices should maximize J(Wn) for n = 1, . . . , N simultaneously, to best preserve the given class structure.\nIn this section, we have discussed SVM and STM for medium and large scale problems. For big data classification problems quite perspective and promising is a quantum computing approach [Biamonte et al., 2016, Chatterjee and Yu, 2016, Li et al., 2015, Rebentrost et al., 2014, Schuld et al., 2015] and a tensor network approach discussed in the next Chapter 3.\n88\nChapter 3\nTensor Train Networks for Selected Huge-Scale Optimization Problems\nFor extremely large-scale, multidimensional datasets, due to curse of dimensionality, most standard numerical methods for computation and optimization problems are intractable. This chapter introduces feasible solutions for several generic huge-scale dimensionality reduction and related optimization problems, whereby the involved optimized cost functions are approximated by suitable low-rank TT networks. In this way, a very large-scale optimization problem can be converted into a set of much smaller optimization sub-problems of the same kind [Cichocki, 2014, Dolgov et al., 2014, Holtz et al., 2012a, Kressner et al., 2014a, Lee and Cichocki, 2016b, Schollwo\u0308ck, 2011, 2013], which can be solved using standard methods.\nThe related optimization problems often involve structured matrices and vectors with over a billion entries (see [Dolgov, 2014, Garreis and Ulbrich, 2017, Grasedyck et al., 2013, Hubig et al., 2017, Stoudenmire and Schwab, 2016] and references therein). In particular, we focus on Symmetric Eigenvalue Decomposition (EVD/PCA) and Generalized Eigenvalue Decomposition (GEVD) [Dolgov et al., 2014, Hubig et al., 2015, Huckle and Waldherr, 2012, Kressner and Uschmajew, 2016, Kressner et al., 2014a, Zhang et al., 2016], SVD [Lee and Cichocki, 2015], solutions of overdetermined and undetermined systems of linear algebraic equations [Dolgov and Savostyanov, 2014, Oseledets and Dolgov, 2012], the Moore\u2013 Penrose pseudo-inverse of structured matrices [Lee and Cichocki, 2016b], and LASSO regression problems [Lee and Cichocki, 2016a]. Tensor\n89\nnetworks for extremely large-scale multi-block (multi-view) data are also discussed, especially TN models for orthogonal Canonical Correlation Analysis (CCA) and related Higher-Order Partial Least Squares (HOPLS) problems [Hou, 2017, Hou et al., 2016b, Zhao et al., 2011, 2013a]. For convenience, all these problems are reformulated as constrained optimization problems which are then, by virtue of low-rank tensor networks, reduced to manageable lower-scale optimization sub-problems. The enhanced tractability and scalability is achieved through tensor network contractions and other tensor network transformations.\nPrior to introducing solutions to several fundamental optimization problems for very large-scale data, we shall describe basic strategies for optimization with cost functions in TT formats."}, {"heading": "3.1 Tensor Train (TT/MPS) Splitting and Extraction of Cores", "text": ""}, {"heading": "3.1.1 Extraction of a Single Core and a Single Slice for ALS Algorithms", "text": "For an efficient implementation of ALS optimization algorithms, it is convenient to first divide a TT network which represents a tensor, X = xxG(1), G(2), . . . , G(N)yy P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN , into sub-trains, as illustrated in Figure 3.1(a). In this way, a large-scale task is replaced by easier-to-handle sub-tasks, whereby the aim is to extract a specific TT core or its slices from the whole TT network. For this purpose, the TT sub-trains can be defined as follows [Holtz et al., 2012a, Kressner et al., 2014a]\nG\u0103n = xxG(1), G(2), . . . , G(n\u00b41)yy P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6In\u00b41\u02c6Rn\u00b41 (3.1) G\u0105n = xxG(n+1), G(n+2), . . . , G(N)yy P RRn\u02c6In+1\u02c6\u00a8\u00a8\u00a8\u02c6IN (3.2)\nwhile their corresponding unfolding matrices, also called interface matrices, are given by:\nG\u0103n = [G\u0103n]\u0103n\u00b41\u0105 P RI1 I2\u00a8\u00a8\u00a8In\u00b41\u02c6Rn\u00b41 , G\u0105n = [G\u0105n](1) P RRn\u02c6In+1\u00a8\u00a8\u00a8IN . (3.3)\nFor convenience, the left and right unfoldings of the cores are defined as\nG(n)L = [G (n)]\u01032\u0105 P RRn\u00b41 In\u02c6Rn and G (n) R = [G (n)]\u01031\u0105 P RRn\u00b41\u02c6InRn\n90\nIt is important to mention here, that the orthogonalization of core tensors is an essential procedure in many algorithms for the TT formats (in order to reduce computational complexity of contraction of core tensors and improve robustness of the algorithms) [Dolgov, 2014, Dolgov et al., 2014, Kressner et al., 2014a, Oseledets, 2011a, Steinlechner, 2016a,b].\nWhen considering the nth TT core, it is usually assumed that all cores to its left are left-orthogonalized, and all cores to its right are rightorthogonalized.\nNotice that if a TT network, X, is n-orthogonal then the interface matrices are also orthogonal, i.e.,\n(G\u0103n)T G\u0103n = IRn\u00b41 , G\u0105n (G\u0105n)T = IRn .\nThrough basic multilinear algebra, we can construct a set of linear equations, referred to as the frame equation, given by\nx = G\u2030 n g(n), n = 1, 2, . . . , N, (3.4)\nwhere x = vec(X) P RI1 I2\u00a8\u00a8\u00a8IN , g(n) = vec(G(n)) P RRn\u00b41 InRn , while a tall-and-skinny matrix, G\u2030n, called the frame matrix, is formulated as\nG\u2030 n = G\u0103n bL IIn bL (G\u0105n)T P RI1 I2\u00a8\u00a8\u00a8IN\u02c6Rn\u00b41 InRn . (3.5)\nRemark. From Eqs. (3.4) and (3.5), observe that the frame and interface matrices indicate an important property of the TT format \u2013 its linearity with respect to each core G(n), when expressed in the vectorized form (see Figure 3.1(b)).\nAnother important advantage of this approach is that by splitting the TT cores, we can express the data tensor in the following matrix form (see Figure 3.2 for n = p)\nrXkn = G \u0103n G(n)kn G \u0105n P RI1\u00a8\u00a8\u00a8In\u00b41\u02c6In+1\u00a8\u00a8\u00a8IN , (3.6)\nwhere rXkn are lateral slices of a 3rd-order reshaped raw tensor, X\u0303 P RI1\u00a8\u00a8\u00a8In\u00b41 \u02c6 In \u02c6 In+1\u00a8\u00a8\u00a8IN , and n = 1, 2, . . . , N, in = 1, 2, . . . , In, (obtained by reshaping an Nth-order data tensor X P RI1 \u02c6I2 \u02c6 \u00a8\u00a8\u00a8 \u02c6IN ). Assuming that the columns of G\u0103n and the rows of G\u0105n are orthonormalized, the lateral slices of a core tensor G(n) can be expressed as\nG(n)in = (G \u0103n)T rXin (G \u0105n)T P RRn\u00b41 \u02c6 Rn . (3.7)\n91\n(a)\nI1 I2 In-1 INIn+1\nG(2) G( +1)n G( )N\nIn\nG( )nG( 1)n-G(1) G n G n\nRn-1 RnRn-2R1 R2 Rn+1 RN-1\nLeft-orthogonalized Right-orthogonalized\n(b)\nx Rn -1 Rn\n= In\ng( )n\n1 2 1( )NI I I 1 2 -1 -1( )n nI I I R +1 +2( )n n N nI I I R -1 1( )n nR( )n nI I\nn G\nInterface matrices\nFrame matrix\n( (TnG\nL L\nI Rn\nFigure 3.1: Extraction of a single core from a TT network. (a) Representation of a tensor train by the left- and right-orthogonalized sub-trains, with respect to the nth core. (b) Graphical illustration of a linear frame equation expressed via a frame matrix or, equivalently, via interface matrices (see Eqs. (3.4) and (3.5))."}, {"heading": "3.1.2 Extraction of Two Neighboring Cores for Modified ALS (MALS)", "text": "The Modified ALS algorithm, also called the two-site Density Matrix Renormalization Group (DMRG2) algorithm1 requires the extraction of two\n1The DMRG algorithm was first proposed by White [1992]. At that time, people did not know the relation between tensor network and the DMRG. In [O\u0308stlund and Rommer, 1995] pointed out that the wave function generated by the DMRG iteration is a matrix product state. The objective of the DMRG was to compute the ground states (minimum eigenpairs)\n92\n(a)\nneighboring cores [Holtz et al., 2012a]. Similarly to the previous section, the extraction of a block of two neighboring TT cores is based on the following linear equation\nx = G\u2030 n,n+1 g(n,n+1), n = 1, 2, . . . , N \u00b4 1, (3.8)\nwhere the frame (tall-and-skinny) matrix is formulated as\nG\u2030 n,n+1 = G\u0103n bL IIn bL IIn+1 bL (G \u0105n+1)T\nP RI1 I2\u00a8\u00a8\u00a8IN\u02c6Rn\u00b41 In In+1Rn+1 (3.9)\nof spin systems.\n93\n(a)\nand g(n,n+1) = vec(G(n) TL G (n+1) R ) = vec(G (n,n+1)) P RRn\u00b41 In In+1Rn+1 , for n = 1, 2, . . . , N \u00b4 1 (see Figure 3.3).\nSimple matrix manipulations now yield the following useful recursive formulas (see also Table 3.1)\nX(n) = G (n) (2) (G \u0103n (n) bL G \u0105n (1)), (3.10)\nG\u2030 n = G\u2030 n,n+1 (IRn\u00b41 In bL (G (n+1) (1) ) T), (3.11)\nG\u2030 n+1 = G\u2030 n,n+1 ((G (n) (3) ) T bL IIn+1Rn+1). (3.12)\nIf the cores are normalized in a such way that all cores to the left of the\n94\ncurrently considered (optimized) TT core G(n) are left-orthogonal2\nG(k) (3) G (k) T (3) = G (k) T \u01032\u0105 G (k) \u01032\u0105 = IRk , k \u0103 n, (3.13)\nand all the cores to the right of G(n) are right-orthogonal\nG(p) (1) G (p) T (1) = G (p) \u01031\u0105 G (p) T \u01031\u0105 = IRp\u00b41 , p \u0105 n, (3.14)\nthen the frame matrices have orthogonal columns [Dolgov et al., 2014, Kressner et al., 2014a, Piz\u030corn and Verstraete, 2012], that is\nGT\u2030 n G\u2030 n = IRn\u00b41 In Rn , (3.15)\nGT\u2030 n,n+1 G\u2030 n,n+1 = IRn\u00b41 In In+1 Rn+1 . (3.16)\nFigure 3.2 illustrates that the operation of splitting the TT cores into three sub-networks can be expressed in the following matrix form (for p = n + 1)\nrXkn,n+1 = G \u0103n G(n,n+1)kn,n+1 G \u0105n+2 P RI1\u00a8\u00a8\u00a8In\u00b41 \u02c6 In+2\u00a8\u00a8\u00a8IN , (3.17)\nfor n = 1, 2, . . . , N \u00b4 1, kn,n+1 = 1, 2, . . . , In In+1, where rXkn,n+1 are the frontal slices of a 3rd-order reshaped data tensor X\u0303 P RI1\u00a8\u00a8\u00a8In\u00b41 \u02c6 In In+1 \u02c6 In+2\u00a8\u00a8\u00a8IN .\nAssuming that the columns of G\u0103n and rows of G\u0105n+2 are orthonormalized, the frontal slices of a super-core tensor, G(n,n+1) P RRn\u00b41\u02c6In In+1\u02c6Rn+1 , can be expressed in the following simple form\nG(n,n+1)kn,n+1 = (G \u0103n)T rXkn,n+1 (G \u0105n+2)T P RRn\u00b41\u02c6Rn+1 . (3.18)"}, {"heading": "3.2 Alternating Least Squares (ALS) and Modified ALS (MALS)", "text": "Consider the minimization of a scalar cost (energy) function, J(X), of an Nth-order tensor variable expressed in the TT format as X \u2013 xxX(1), X(2), . . . , X(N)yy. The solution is sought in the form of a tensor train; however, the simultaneous minimization over all cores X(n) is usually\n2We recall here, that G(n) (3) P R Rn\u02c6Rn\u00b41 In and G(n) (1) P R Rn\u00b41\u02c6In Rn are the mode-3 and\nmode-1 matricization of the TT core tensor G(n) P RRn\u00b41\u02c6In\u02c6Rn , respectively.\n96\nAlgorithm 7: Alternating Least Squares (ALS) Input: Cost (energy) function J(X) and an initial guess for an\nNth-order tensor in the TT format X = xxX(1), X(2), . . . , X(N)yy Output: A tensor X in the TT format minimizes the cost\nfunction J(X) 1: while Stopping condition not fulfilled do 2: for n = 1 to N do 3: Find: X\u0302(n) = arg min\nX(n)\u02da\nJ(X\u0302(1), . . . , X\u0302(n\u00b41), X(n)\u02da , X(n+1), . . . , X(N))\n4: end for 5: end while\ntoo complex and nonlinear. For feasibility, the procedure is replaced by a sequence of optimizations carried out over one core at a time, that is X(n) = arg min J(X(1), . . . , X(n), . . . , X(N)).\nThe idea behind the Alternating Least Squares (ALS) optimization3 (also known as the Alternating Linear Scheme or one-site DMRG (DMRG1)) is that in each local optimization (called the micro-iteration step), only one core tensor, X(n), is updated, while all other cores are kept fixed. Starting from some initial guess for all cores, the method first updates, say, core X(1), while cores X(2), . . . X(N), are fixed, then it updates X(2) with X(1), X(3), . . . , X(n) fixed and so on, until X(N) is optimized. After completing this forward half-sweep, the algorithm proceeds backwards along the sequence of cores N, N\u00b4 1, . . . , 1, within the so-called backward half-sweep. The sequence of forward and backward iterations complete one full sweep4, which corresponds to one (global-) iteration. These iterations are repeated until some stopping criterion is satisfied, as illustrated in Algorithm 7 and Figure 3.4.\nAs in any iterative optimization based on gradient descent, the cost function can only decrease, however, there is no guarantee that a global minimum would be reached. Also, since the TT rank for the desired solution is unknown, the standard ALS relies on an initial guess for the TT rank; for some initial conditions the iteration process can therefore be very slow. To alleviate these problems, the modified ALS (MALS/DMRG2) scheme aims to merge two neighboring TT cores (blocks), optimize the resulting \u201csuper-node\u201d also called \u201csuper-core\u201d or \u201csuper-block\u201d, and split\n3The ALS algorithms can be considered as the block nonlinear Gauss-Seidel iterations. 4Note that this sweeping process operates in a similar fashion as the self-consistent\nrecursive loops, where the solution is improved iteratively and gradually.\n97\n98\nagain the result into separate factors by low-rank matrix factorizations, usually using truncated SVD 5. Some remarks:\n\u2022 If the global cost (loss) function J(X) is quadratic, so too are the local cost functions Jn(X(n)), and the problem reduces to solving a small-scale optimization at each micro-iteration. In addition, local problems in the micro-iterations are usually better conditioned, due to the orthogonalization of TT cores.\n\u2022 In most optimization problems considered in this monograph the TT decomposition is assumed to be in an orthogonalized form, where all cores are left\u2013 or right\u2013orthogonal with respect to the core X(n), which is being optimized (see also Figure 3.4 and Figure 3.5) [Dolgov and Khoromskij, 2013, Holtz et al., 2012a]."}, {"heading": "3.3 Tensor Completion for Large-Scale Structured Data", "text": "The objective of tensor completion is to reconstruct a high-dimensional structured multiway array for which a large proportion of entries is missing or noisy [Grasedyck et al., 2015, Steinlechner, 2016a,b, Yokota et al., 2016, Zhao et al., 2015, 2016].\nWith the assumption that a good low-rank TN approximation for an incomplete data tensor Y P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN does exist, the tensor completion problem can be formulated as the following optimization problem\nmin X\nJ(X) = }P\u2126(Y)\u00b4 P\u2126(X)}2F (3.19)\nsubject to X P MR := tX P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN | rankTT(X) = rTTu, where X is the reconstruction tensor in TT format and rTT = tR1, R2, . . . , RN\u00b41u. The symbol P\u2126 denotes the projection onto the sampling set \u2126 which corresponds to the indices of the known entries of Y, i.e., P\u2126(Y(i1, i2, . . . , iN)) = Y(i1, i2, . . . , iN) if (i1, i2, . . . , iN) P \u2126, otherwise zero.\n5Although DMRG2 and MALS are equivalent, the SVD was not used in original DMRG2 algorithm. As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Oru\u0301s [2014], Orus and Vidal [2008], Vidal [2003].)\n99\nFor a large-scale high-order reconstruction tensor represented in the TT format, that is, X = xxX(1), X2), . . . , X(N)yy, the above optimization problem can be converted by the ALS approach to a set of smaller optimization problems. These can be represented in a scalar form through slices of core\n100\ntensors, as follows\npX (n)\n= arg min X(n)\n\u00ff\niP\u2126\n( Y(i1, . . . , iN)\u00b4 X\n(1) i1 \u00a8 \u00a8 \u00a8X(n)in X (n+1) in \u00a8 \u00a8 \u00a8X (N) iN\n)2 .\nFor convergence, it is necessary to iterate through all cores, X(n), n = 1, 2, . . . , N, and over several sweeps.\nFor efficiency, all the cores are orthogonalized after each sweep, and the cost function is minimized in its vectorized form by sequentially minimizing each individual slice, X(n)in , of the cores, X\n(n) P RRn\u00b41\u02c6In\u02c6Rn (see also formulas (3.6) and (3.7)), that is\npX(n)in = arg min X(n)in }vec(rYin)\u00b4 vec(X \u0103n X(n)in X \u0105n)}2\u2126 (3.20)\n= arg min X(n)in\n}vec(rYin \u00b4 [X \u0103n bL (X\u0105n)T]vec(X (n) in )} 2 \u2126,\nfor all in = 1, 2, . . . , In and n = 1, 2, . . . , N, where }v}2\u2126 = \u0159 iP\u2126 v 2 i .\nThe above optimization problem for vec(X(n)in ) can be considered as a standard linear least squares (LS) problem, which can be efficiently solved even for huge-scale datasets.\nIn general, TT ranks are not known beforehand and must be estimated during the optimization procedure. These can be estimated by, for example, starting with a maximum rank Rmax and reducing gradually ranks by TT rounding. Alternatively, starting from a minimum rank, Rmin, the ranks could be gradually increased. The rank increasing procedure can start with rTT = t1, 1, . . . , 1u, and the so obtained result is used for another run (sweep), but with rTT = t1, 2, . . . , 1u. In other words, the rank R2 between the second and third core is increased, and so on until either the prescribed residual tolerance \u03b5 or Rmax is reached. As the rank value is a key factor which determines the complexity of the ALS, the second approach is often more efficient and is computationally comparable with rank-adaptive approaches [Grasedyck et al., 2015, Steinlechner, 2016a,b].\n101"}, {"heading": "3.4 Computing a Few Extremal Eigenvalues and Eigenvectors", "text": ""}, {"heading": "3.4.1 TT Network for Computing the Single Smallest Eigenvalue and the Corresponding Eigenvector", "text": "Machine learning applications often require the computation of extreme (minimum or maximum) eigenvalues and the corresponding eigenvectors of large-scale structured symmetric matrices. This problem can be formulated as the standard symmetric eigenvalue decomposition (EVD), in the form\nA xk = \u03bbkxk, k = 1, 2, . . . , K, (3.21)\nwhere xk P RI are the orthonormal eigenvectors and \u03bbk the corresponding eigenvalues of a symmetric matrix A P RI\u02c6I .\nIterative algorithms for extreme eigenvalue problems often exploit the Rayleigh Quotient (RQ) of a symmetric matrix as the following cost function\nJ(x) = R(x, A) = xTAx xTx = xAx, xy xx, xy , x \u2030 0. (3.22)\nBased on the Rayleigh Quotient (RQ), the largest, \u03bbmax, and smallest, \u03bbmin, eigenvalue of the matrix A can be computed as\n\u03bbmax = max x R(x, A), \u03bbmin = minx R(x, A), (3.23)\nwhile the critical points and critical values of R(x, A) are the corresponding eigenvectors and eigenvalues of A.\nThe traditional methods for solving eigenvalue problems for a symmetric matrix, A P RI\u02c6I , are prohibitive for very large values of I, say I = 1015 or higher. This computational bottleneck can be very efficiently dealt with through low-rank tensor approximations, and the last 10 years have witnessed the development of such techniques for several classes of optimization problems, including EVD/PCA and SVD [Dolgov et al., 2014, Kressner et al., 2014a, Lee and Cichocki, 2015]. The principle is to represent the cost function in a tensor format; under certain conditions, such as that tensors can be often quite well approximated in a low-rank TT format, thus allowing for low-dimensional parametrization.\nIn other words, if a structured symmetric matrix A and its eigenvector x admit low-rank TT approximations, a large-scale eigenvalue problem\n102\ncan be converted into a set of smaller optimization problems by representing the eigenvector, x P RI , and the matrix, A P RI\u02c6I , in TT (MPS/MPO) formats, i.e., a TT/MPS for an Nth-order tensor X \u2013 xxX(1), . . . , X(N)yy P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN and a matrix TT/MPO for a 2Nth-order tensor A \u2013 xxA(1), . . . , A(N)yy P RI1\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6IN , where I = I1 I2 \u00a8 \u00a8 \u00a8 IN .\nFigure 3.6 illustrates this conversion into a set of much smaller sub-\n103\nproblems by employing tensor contractions and the frame equations (see section 3.1)\nx = X\u2030n x(n), n = 1, 2, . . . , N, (3.24)\nwith the frame matrices\nX\u2030n = X\u0103n bL IIn bL (X\u0105n)T P RI1 I2\u00a8\u00a8\u00a8IN\u02c6Rn\u00b41 InRn . (3.25)\nSince the cores X(m) for m \u2030 n are constrained to be left- or rightorthogonal, the RQ can be minimized (or maximized), as follows\nmin x J(x) = min x(n)\nJ(X\u2030nx(n)) (3.26)\n= min x(n)\nx(n) T A(n) x(n)\nxx(n), x(n)y , n = 1, 2, . . . , N,\nwhere x(n) = vec(X(n)) P RRn\u00b41 InRn and the matrix A(n) (called the effective Hamiltonian) can be expressed as\nA(n) = (X\u2030n)TAX\u2030n P RRn\u00b41 InRn\u02c6Rn\u00b41 InRn (3.27)\nfor n = 1, 2, . . . , N, under the condition (X\u2030n)TX\u2030n = I. For relatively small TT ranks, the matrices A(n) are usually much smaller than the original matrix A, so that a large-scale EVD problem can be converted into a set of much smaller EVD sub-problems, which requires solving the set of equations\nA(n) x(n) = \u03bbx(n), n = 1, 2, . . . , N. (3.28)\nIn practice, the matrices A(n) are never computed directly by (3.27). Instead, we compute matrix-by-vector multiplication6, A(n) x(n), iteratively via optimized contraction of TT cores within a tensor network. The concept of optimized contraction is illustrated in Figure 3.7.\nIt should be noted that the contraction of the matrix A in TT/MPO format, with corresponding TT cores of the vectors x and xT in TT/MPS format, leads to the left- and right contraction tensors L\u0103n and R\u0105n, as illustrated in Figure 3.6. In other words, efficient solution of the matrix\n6Such local matrix-by-vectors multiplications can be incorporated to standard iterative methods, such as Arnoldi, Lanczos, Jacobi-Davidson and LOBPCG.\n104\n(a)\n(b)\n(c)\nequation (3.28) requires computation of the blocks L\u0103n and R\u0105n; these can be built iteratively so as to best reuse available information, which involves an optimal arrangement of a tensor network contraction. In a practical implementation, the full network contraction is never carried out globally, but through iterative sweeps from right to left or vice-versa, to build up L\u0103n and R\u0105n from the previous steps. The left and right orthogonalization of the cores can also be exploited to simplify the tensor contraction process [Dolgov et al., 2014, Kressner et al., 2014a, Lee and Cichocki, 2015, 2016b, Schollwo\u0308ck, 2011].\n105\nRemark. Before we construct the tensor network shown in the Figure 3.6, we need to construct approximate distributed representation of the matrix A in the TT/MPO format. For ways to construct efficient representations of huge-scale matrix in TT/MPO format while simultaneously performing compression see [August et al., 2016, Hubig et al., 2017] and also Part 1 [Cichocki et al., 2016]."}, {"heading": "3.4.2 TT Network for Computing Several Extremal Eigenvalues and Corresponding Eigenvectors for Symmetric EVD", "text": "In a more general case, several (say, K) extremal eigenvalues and the corresponding K eigenvectors of a symmetric structured matrix A can be computed through the following trace minimization (or maximization) with orthogonality constraints\nmin X\ntr(XTAX), s.t. XTX = IK, (3.29)\nwhere X = [x1, x2, . . . , xK] P RI1 I2\u00a8\u00a8\u00a8IN \u02c6 K is a matrix composed of eigenvectors xk, and A P RI\u02c6I is a huge-scale symmetric matrix.\nNote that the global minimum of this cost function is equivalent to a sum of K smallest eigenvalues, \u03bb1 + \u03bb2 + \u00a8 \u00a8 \u00a8+ \u03bbK.\nFor a very large-scale symmetric matrix A P RI\u02c6I with say, I = 1015 or higher, the optimization problem in (3.29) cannot be solved directly. A feasible TT solution would be to first tensorize the matrices A P RI\u02c6I and X P RI\u02c6K into the respective tensors A P RI1\u02c6I1\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6IN and X P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6K, where I = I1 I2 \u00a8 \u00a8 \u00a8 IN and the value of every In is much smaller than I; then, a tensor network structure can be exploited to allow a good low-rank TN approximation of the underlying cost function.\nInstead of representing each eigenvector xk individually in the TT format, we can parameterize them jointly in a block TT format (see Figure 3.8) [Dolgov et al., 2014, Piz\u030corn and Verstraete, 2012]. In general, the block TT format is suitable for approximate representation of tall and skinny structured matrices using almost the same number of data samples as for representation a single vector/matrix. Within the block-n TT, all but one cores are 3rd-order tensors, the remaining core is a 4th-order tensor and has an additional physical index K which represents the number of eigenvectors, as shown in Figure 3.9. The position of this 4th-order core X(n) which carries the index K is not fixed; during sequential optimization, it is moved backward and forward from position 1 to N [Dolgov et al., 2014,\nHoltz et al., 2012a]. Observe that this 4th-order core X(n) = [ X(n)k ]K k=1 =\n106\nX(n,k)\n]\nP RRn\u00b41\u02c6In\u02c6Rn\u02c6K comprises the K different eigenvectors, while all\nremaining cores retain their original structure. During the optimization process, the neighbouring core tensors need to be appropriately reshaped, as explained in detail in Algorithm 8 and Algorithm 9.\nWhen using the block-n TT model to represent the K mutually orthogonal vectors, the matrix frame equation takes a slightly modified form\nX = X\u2030n X(n) P RI\u02c6K, n = 1, 2, . . . , N, (3.30)\nwhere X(n) = X(n)\u01033\u0105 = X (n) T (4) P R Rn\u00b41 InRn\u02c6K. Hence, we can express the trace in (3.29), as follows\ntr(XTAX) = tr((X\u2030nX(n))TAX\u2030nX(n))\n= tr((X(n))T[XT\u2030nAX\u2030n]X (n)) = tr(X(n)T A(n) X(n)), (3.31)\nwhere A(n) = XT\u2030nAX\u2030n. Assuming that the frame matrices have orthogonal columns, the optimization problem in (3.29) can be converted into a set of linked relatively small-scale optimization problems\nmin X(n)\ntr(X(n)T A(n) X(n)), s.t. X(n) TX(n) = IK (3.32)\nfor n = 1, 2, . . . , N, where A(n) is computed recursively by tensor network contractions, as illustrated in Figure 3.9. In other words, the EVD problem can be solved iteratively via optimized recursive contraction of the TT network, whereby the active block (i.e., currently optimized core X(n) =\n107\nAlgorithm 8: Transformation of the block-n TT into the block-(n + 1) TT [Dolgov et al., 2014, Kressner et al., 2014a]\nInput: Nth-order tensor, X, in block-n TT format with n \u0103 N Output: The tensor X in block-(n + 1) TT format, with new cores\nX(n) P RRn\u00b41 \u02c6In \u02c6 Rn and X(n+1) P RRn \u02c6 In+1 \u02c6 Rn+1 \u02c6K 1: Reshape the core X(n) = [ X(n)k ]K k=1 = [ X(n,k) ] P RRn\u00b41\u02c6In\u02c6Rn\u02c6K\ninto a 3rd-order tensor X(n)L P R Rn\u00b41 In \u02c6 Rn \u02c6K\n2: Perform a minimum rank decomposition (using, e.g., QR or SVD) X(n)L (:, :, k) = Q Pk P R\nRn\u00b41 In \u02c6 Rn , k = 1, . . . , K where Q P RRn\u00b41 In\u02c6R, Pk P RR \u02c6 Rn\n3: Update the rank Rn \u00d0 R 4: Update new cores\nX(n)L \u00d0 Q, X (n+1) R (:, :, k)\u00d0 PkX (n+1) R P R Rn\u02c6In+1Rn+1 , @k 5: return X = xxX(1), . . . , X(n), X(n+1), . . . , X(N)yy\nAlgorithm 9: Transformation of the block-n TT into the block-(n\u00b4 1) TT [Kressner et al., 2014a]\nInput: Nth-order tensor, X, in the block-n TT format with n \u0105 1 Output: The tensor X in block-(n\u00b4 1) TT format, with new cores\nX(n\u00b41) P RRn\u00b42 \u02c6 In\u00b41 \u02c6Rn\u00b41 \u02c6 K and X(n) P RRn\u00b41 \u02c6 In\u02c6 Rn 1: Reshape the core X(n) = [ X(n,k) ] P RRn\u00b41 \u02c6 In \u02c6 Rn \u02c6 K\ninto a 3rd-order tensor for the TT core X(n)R P R Rn\u00b41 \u02c6 InRn\u02c6K\n2: Perform a minimum rank decomposition (using, e.g., RQ/LQ) X(n)R (:, :, k) = QkP P R\nRn\u00b41 \u02c6 InRn , k = 1, . . . , K where Qk P RRn\u00b41 \u02c6 R, P P RR \u02c6 InRn\n3: Update the rank Rn\u00b41 \u00d0 R 4: Update new cores\nX(n)R \u00d0 P, X (n\u00b41) L (:, :, k)\u00d0 X (n\u00b41) L Qk P R Rn\u00b42 In\u00b41 \u02c6 Rn\u00b41 , @k 5: return X = xxX(1), . . . , X(n\u00b41), X(n), . . . , X(N)yy\n108\nX(n,k)\n]\n) is sequentially optimized by sweeping from left to right and\nback from right to left, and so on, until convergence as illustrated in Algorithm 10 [Dolgov et al., 2014, Kressner et al., 2014a].\nDuring the optimization, the global orthogonality constraint, XTX = IK, is equivalent to the set of local orthogonality constraints, (X(n))TX(n) = IK, @n, so that due to left- and right-orthogonality of the TT cores, we can write\nXTX = X(n) T XT\u2030n X\u2030nX (n) (3.33)\n= X(n)TX(n), @n.\n109\nwhich allows for a dramatic reduction in computational complexity."}, {"heading": "3.4.3 Modified ALS (MALS/DMRG2) for Symmetric EVD", "text": "In order to accelerate the convergence speed and improve performance of ALS/DMRG1 methods, the MALS (two-site DMRG2) scheme optimizes, instead of only one, simultaneously two neighbouring TT-cores at each micro-iteration, while the other TT-cores remain fixed (assumed to be known). This reduces a large-scale optimization problem to a set of smaller-scale optimization problems, although the MALS sub-problems are essentially of bigger size than those for the ALS.\nAfter local optimization (micro-iteration) on a merged core tensor, X(n,n+1) = X(n) \u02c61 X(n+1), it is in the next step decomposed (factorized) into two separate TT-cores via a \u03b4-truncated SVD of the unfolded matrix,\n110\nAlgorithm 10: One full sweep of the ALS algorithm for symmetric EVD [Dolgov and Savostyanov, 2014]\nInput: A symmetric matrix A P RI\u02c6I , and initial guesses for X P RI1 I2\u00a8\u00a8\u00a8IN\u02c6K in block-1 TT format and with right orthogonal cores X(2), . . . , X(N) Output: Approximative solution X in the TT format 1: for n = 1 to N \u00b4 1 do 2: Perform tensor network contractions (Figure 3.9) and solve a\nreduced EVD problem (3.31), for the TT core X(n)\n3: Transform block-n TT to block-(n + 1) TT by applying Algorithm 8, such that the updated core X(n) is left-orthogonal 4: end for 5: for n = N to 2 do 6: Perform tensors network contraction and solve a reduced EVD problem\n(3.31) for the TT core X(n)\n7: Transform block-n TT to block-(n\u00b4 1) TT by applying Algorithm 9, so that the updated core X(n) is right-orthogonal 8: end for 9: return X = xxX(1), X(2), . . . , X(N)yy\nX(n,n+1)\u01032\u0105 P RRn\u00b41 In \u02c6 In+1Rn+1 , as\n[U, S, V] = tSVD ( X(n,n+1)\u01032\u0105 ) , (3.34)\nwhich allows us to both update the cores, X(n)\u01032\u0105 = U and X (n+1) \u01031\u0105 = SVT, and to estimate the TT rank between the cores as Rn = min(size(U, 2), Rmax).\nIn this way, the TT-ranks can be adaptively determined during the iteration process, while the TT-cores can be left- or right-orthogonalized, as desired. The truncation parameter, \u03b4 \u0105 0, can be selected heuristically or adaptively (see e.g. [Lee and Cichocki, 2015, Oseledets and Dolgov, 2012]).\nThe use of truncated SVD allows us to: (1) Estimate the optimal rank, Rn, and to (2) quantify the error associated with this splitting. In other words, the power of MALS stems from the splitting process which allows for a dynamic control of TT ranks during the iteration process, as well as from the ability to control the level of error of low-rank TN approximations.\n111"}, {"heading": "3.4.4 Alternating Minimal Energy Method for EVD (EVAMEn)", "text": "Recall that the standard ALS method cannot adaptively change the TT ranks during the iteration process, whereas the MALS method achieves this in intermediate steps, through the merging and then splitting of two neighboring TT cores, as shown in the previous section. The cost function J(X) is not convex with respect to all the elements of the core tensors of X and as consequence the ALS is prone to getting stuck in local minima and suffers from a relatively slow convergence speed. On the other hand, the intermediate steps of the MALS help to alleviate these problems and improve the convergence speed, but there is still not guarantee that the algorithm will converge to a global minimum.\nThe Alternating Minimal Energy (AMEn) algorithm aims to avoid the problem of convergence to non-global minimum by exploiting the information about the gradient of a cost function or information about the value of a current residual by \u201cenriching\u201d the TT cores with additional information during the iteration process [Dolgov and Savostyanov, 2014]. In that sense, the AMEn can be considered as a subspace correction technique. Such an enrichment was efficiently implemented first by Dolgov and Savostyanov to solve symmetric as well as nonsymmetric linear systems [Dolgov et al., 2016] and was later extended to symmetric EVD in [Kressner et al., 2014a] and [Hubig et al., 2015].\nSimilar to the ALS method, at each iteration step, the AMEn algorithm updates a single core tensor (see Figure 3.11). Then, it concatenates the updated TT core X(n) with a core tensor Z(n) obtained from the residual vector. At each micro-iteration, only one core tensor of the solution is enriched by the core tensor computed based on the gradient vector. By concatenating two core tensors X(n) and Z(n), the AMEn can achieve global convergence, while maintaining the computational and storage complexities as low as those of the standard ALS [Dolgov and Khoromskij, 2015, Dolgov and Savostyanov, 2014].\nThe concatenation step of the AMEn is also called the (core) enrichment, the basis expansion, or the local subspace correction. The concept was proposed by White [2005] as a corrected one-side DMRG1 method, while [Dolgov and Savostyanov, 2014] proposed a significantly improved AMEn algorithm for solving large scale systems of linear equations together with theoretical convergence analysis. [Kressner et al., 2014a] and [Hubig et al., 2015] developed AMEn type methods for solving large scale eigenvalue problems.\nIn this case, the goal is to compute a single or only a few (K \u011b 1) extreme\n112\n113\neigenvalues and corresponding eigenvectors of a very large symmetric matrix A P RI\u02c6I , with I = I1 I2 \u00a8 \u00a8 \u00a8 IN . To this end, we exploit a cost function in the form of the block Rayleigh quotient, J(X) = tr(XTAX), where the matrix X = [x1, x2, . . . , xK] P RI\u02c6K is assumed to be represented in a block TT format.\nIn the AMEn algorithm for the EVD, each core tensor, X(n) P RRn\u00b41\u02c6In\u02c6Rn\u02c6K of X, is updated at each micro-iteration, similarly to the ALS algorithm. Next, a core tensor Z(n) P RRn\u00b41\u02c6In\u02c6Qn\u02c6K is constructed and is concatenated with the core X(n) as\nrX (n) \u00d0 X(n) \u20183 Z(n) P RRn\u00b41\u02c6In\u02c6(Rn+Qn)\u02c6K, (3.35)\nor equivalently using tensor slices\nrX(n):,in,:,k \u00d0 [X (n) :,in,:,k , Z(n):,in,:,k] P R Rn\u00b41\u02c6(Rn+Qn), @in,@k.\nFor the consistency of sizes of TT cores, the neighboring core tensor is concatenated with a tensor consisting all zero entries, to yield\nrX(n+1):,in+1,: \u00d0 [ X(n+1):,in+1,: 0 ] P R(Rn+Qn)\u02c6Rn+1 .\nAlgorithm 11 describes the AMEn for computing several extreme eigenvalues, \u039b = diag(\u03bb1, . . . , \u03bbK), and the corresponding eigenvectors, X.\nIn principle, the TT cores Z(n) are estimated from the residual7 as\nR \u2013 AX\u00b4 X\u039b P RI\u02c6K (3.36)\nwith \u039b = XTAX P RK\u02c6K. This corresponds to an orthogonal projection of the gradient of J(X) = tr(XTAX) onto the tangent space of the Stiefel manifold [Absil et al., 2008]. [Kressner et al., 2014a] formulated the local residual of the MALS method for EVD as\nRn,n+1 = A (n,n+1)X(n,n+1) \u00b4 X(n,n+1)\u039b\n= XT\u2030n,n+1(AX\u00b4 X\u039b) P RRn\u00b41 In In+1Rn+1\u02c6K, (3.37)\nwhere A(n,n+1) = XT\u2030n,n+1AX\u2030n,n+1 and X (n,n+1)(:, k) = vec(X(n):,:,:,k \u02c6 1 X(n+1)), k = 1, . . . , K.\n7In order to improve convergence, the residual R is often defined as R \u2013 M\u00b41(AX\u00b4 X\u039b), where M is a suitably designed preconditioning matrix.\n114\nAlgorithm 11: One full sweep of the AMEn algorithm for EVD (EVAMEn) [Hubig et al., 2015, Kressner et al., 2014a]\nInput: A symmetric matrix A P RI1\u00a8\u00a8\u00a8IN\u02c6I1\u00a8\u00a8\u00a8IN , and an initial guess for X P RI1\u00a8\u00a8\u00a8IN\u02c6K in a block-1 TT format with right-orthogonal cores X(2), . . . , X(N) Output: Approximate solution X in the TT format 1: for n = 1 to N \u00b4 1 do 2: Perform tensor network contraction (Figure 3.9) and solve\nthe reduced EVD problem (3.31) for the TT core X(n)\n3: Compute the core Z(n) defined in (3.39) by a contraction of the block L\u0103n and the cores A(n), X(n) (See Figure 3.11(a)) 4: Augment the two cores X(n) and X(n+1) with Z(n) and 0 of compatible sizes, to give the slices\nX(n):,in,:,k \u00d0 [ X(n):,in,:,k, Z (n) :,in,:,k ] , X(n+1):,in+1,: \u00d0 X(n+1):,in+1,: 0  5: Transform block-n TT into block-(n + 1) TT by applying Algorithm 8, so that the updated core X(n) is left-orthogonal 6: end for 7: for n = N to 2 do 8: Perform tensors network contraction and solve the reduced EVD problem (3.31) for the TT core X(n). 9: Compute the core Z(n) by a contraction of the block R\u0105n\nand the cores A(n), X(n) (See Figure 3.11(b)). 10: Augment the two cores X(n\u00b41) and X(n) with 0 and Z(n) of\ncompatible sizes, to give the slices\nX(n\u00b41):,in\u00b41,: \u00d0 [ X(n\u00b41):,in\u00b41,:, 0 ] , X(n):,in,:,k \u00d0 X(n):,in,:,k Z(n):,in,:,k  11: Transform block-n TT into block-(n\u00b4 1) TT by applying Algorithm 9 such that the updated core X(n) is right-orthogonal. 12: end for 13: return X = xxX(1), X(2), . . . , X(N)yy\n115\nProvided that the matrices A and X are given in a TT format, it is possible to build two core tensors Z(n) P RRn\u00b41\u02c6In\u02c6Qn\u02c6K and Z(n+1) P RQn\u02c6In+1\u02c6Rn+1 separately, such that\nRn,n+1(:, k) \u201d vec(Z (n) :,:,:,k \u02c6 1 Z(n+1)), (3.38)\nwhere the size Qn is equal to that of the residual, Qn = PnRn + Rn, unless preconditioned.\nAlternatively, [Hubig et al., 2015] developed a more efficient algorithm, whereby instead of using the exact residual AX \u00b4 X\u039b to compute the enrichment term, which is computationally expensive, they show that it is sufficient to exploit only the AX term. Note that if R = AX \u00b4 X\u039b, then the column space is preserved as range([X, R]) = range([X, AX]). See also the discussion regarding the column space of X\u2030n+1 in Section 3.8.2. Through this the simplified form, the expressions (3.37) and (3.38) can be written as Rn,n+1 = XT\u2030n,n+1AX, Rn,n+1(:, k) \u201d vec(Z (n) :,:,:,k \u02c6 1 Z(n+1)). (3.39)\nIt is important to note that the TT cores Z(n) and Z(n+1) can be estimated independently without explicitly computing A(n,n+1) or X(n,n+1). Specifically, Z(n) can be computed by a sequential contraction of the block L\u0103n and the cores A(n) and X(n). Figures 3.11(a) and (b) illustrate the core enrichment procedure in the AMEn algorithm during the left-to-right or the right-to-left half-sweep. In the figure, for simplicity Z(n) is computed based on the expression (3.39) for eigenvalue problems, with K = 1 eigenvalue [Hubig et al., 2015]. Furthermore, it has been found that even a rough approximation to the residual / gradient for the enrichment Z(n) will lead to a faster convergence of the algorithm [Dolgov and Savostyanov, 2014]. See also Section 3.8.2 for another way of computing TT cores Z(n).\nIn general, the core tensor Z(n) is computed in a way dependent on a specific optimization problem and an efficient approximation strategy.\n116"}, {"heading": "3.5 TT Networks for Tracking a Few Extreme Singular Values and Singular Vectors in SVD", "text": "Similarly to the symmetric EVD problem described in the previous section, the block TT concept can be employed to compute only K largest singular values and the corresponding singular vectors of a given matrix A P RI\u02c6J , by performing the maximization of the following cost function [Cichocki, 2013, 2014, Lee and Cichocki, 2015]:\nJ(U, V) = tr(UTAV), s.t. UTU = IK, VTV = IK, (3.40)\nwhere U P RI\u02c6K and V P RJ\u02c6K. Conversely, the computation of K smallest singular values and the corresponding left and right singular vectors of a given matrix, A P RI\u02c6J , can be formulated as the following optimization problem:\nmax U,V\ntr(VTA:U), s.t. UTU = IK, VTV = IK, (3.41)\nwhere A: P RJ\u02c6I is the Moore-Penrose pseudo-inverse of the matrix A. Now, after tensorizing the involved huge-scale matrices, an asymmetric tensor network can be constructed for the computation of K extreme (minimal or maximal) singular values, as illustrated in Figure 3.12 (see [Lee and Cichocki, 2015] for detail and computer simulation experiments).\nThe key idea behind the solutions of (3.40) and (3.41) is to perform TT core contractions to reduce the unfeasible huge-scale optimization problem to relatively small-scale optimization sub-problems, as follows:\n\u2022 For the problem (3.40)\nmax U(n),V(n)\ntr((U(n))T A(n) V(n)), (3.42)\n\u2022 For the problem (3.41)\nmax U(n),V(n)\ntr((V(n))T (A(n)): U(n)), (3.43)\nsubject to the orthogonality constraints\n(U(n))TU(n) = IK, (V(n))TV(n) = IK, n = 1, 2, . . . , N,\n117\n118\nwhere U(n) P RR\u0303n\u00b41 In R\u0303n\u02c6K and V(n) P RRn\u00b41 JnRn\u02c6K, while the contracted matrices are formally expressed as\nA(n) = UT\u2030nAV\u2030n P RR\u0303n\u00b41 In R\u0303n \u02c6 Rn\u00b41 Jn Rn (3.44)\n(A(n)): = VT\u2030nA :U\u2030n P RRn\u00b41 Jn Rn \u02c6 R\u0303n\u00b41 In R\u0303n . (3.45)\nIn this way, the problem is reduced to the computation of the largest or smallest singular values of a relatively small matrix A(n), for which any efficient SVD algorithm can be applied.\nNote that for a very large number of, say, thousands or more singular values K = K1K2 \u00a8 \u00a8 \u00a8KN , the block-n TT formats may not be efficient, and\n119\nalternative tensor networks should be employed, as shown in Figure 3.13, where each TT core is a 4th-order tensor. One problem with such tensor networks with many loops is the computationally complex contraction of core tensors. However, all loops can be easily eliminated by vectorizing the orthogonal matrices\nU P RI\u02c6K \u00d1 u P RIK, V P RJ\u02c6K \u00d1 u P RJK (3.46)\nand by rewriting the cost function as (see bottom part of Figure 3.13)\nJ(U, V) = tr(UTAV)\u00d1 J(u, v) = uT rAv = uT(Ab IK)v, (3.47)\nwhere rA = Ab IK P RIK\u02c6JK."}, {"heading": "3.6 GEVD and Related Problems using TT Networks", "text": "Table 3.2 illustrates the rich scope of the Generalized Eigenvalue Decomposition (GEVD), a backbone of many standard8 methods for linear dimensionality reduction, classification, and clustering [Benson et al., 2015, Cunningham and Ghahramani, 2015, Gleich et al., 2015, Kokiopoulou et al., 2011, Wu et al., 2016].\nFor example, the standard PCA, PLS, orthonormalized PLS (OPLS) and CCA can be formulated as the following EVD/GEVD problems\nPCA : Cxv = \u03bbv (3.48)\nOPLS : CxyCTxyv = \u03bbCxv (3.49)\nPLS :\n 0 Cxy CTxy 0 v u  = \u03bb v u  (3.50) CCA :\n 0 Cxy CTxy 0 v u  = \u03bb Cx 0 0 CTy v u  , (3.51) 8In some applications, we need to compute only very few eigenvectors. For example, in spectral co-clustering, only one eigenvector (called Fiedler eigenvector) needs to be computed, which corresponds to the second smallest generalized eigenvalue [Wu et al., 2016].\n120\nwhere for given data matrices X and Y, Cx = XTX and Cy = YTY are the sample covariance matrices, while Cxy = XTY is the sample crosscovariance matrix.\nIn general, the GEVD problem can be formulated as the following trace optimization (maximization or minimization) problem\nmax VPRI\u02c6K\ntr(VTAV), s.t. VTBV = IK, (3.52)\nwhere the symmetric matrix, A P RI\u02c6I , and symmetric positive-definite matrix, B P RI\u02c6I , are known, while the objective is to estimate K largest (or the smallest in the case of minimization) eigenvalues and the corresponding eigenvectors represented by the orthonormal matrix V P RI\u02c6K, with I = I1 I2 \u00a8 \u00a8 \u00a8 IN and K = K1K2 \u00a8 \u00a8 \u00a8KN . For the estimation of the K smallest eigenvalues, the minimization of the cost function is performed instead of the maximization.\nIt is sometimes more elegant to incorporate the constraint, VTBV = IK, into (3.52) into the trace operator, to give the following optimization problem [Absil et al., 2008]\nmin VPRI\u02c6K\ntr(VTAV(VTBV)\u00b41). (3.53)\nor alternatively, to represent GEVD approximately as an unconstrained optimization problem\nmin VPRI\u02c6K\ntr(VTAV) + \u03b3 }VTBV\u00b4 IK}2F, (3.54)\nwhere the parameter \u03b3 \u0105 0 controls the orthogonality level of B. Also, by a change in the variables, W = B1/2V, the GEVD reduces to the standard symmetric EVD [Cunningham and Ghahramani, 2015]\nmin WPRI\u02c6K\ntr(WTB\u00b41/2AB\u00b41/2W), s.t. WTW = IK.\nFinally, the GEVD is closely related but not equivalent, to the trace ratio optimization problem, given by\nmin VPRI\u02c6K tr(VTAV) tr(VTBV) , s.t. VTV = IK. (3.55)\nFigure 3.14 illustrates the trace ratio optimization for huge-scale matrices, whereby the two TT networks represent approximately tr(VTAV) and tr(VTBV). These sub-networks are simultaneously optimized in the sense\n122\nthat one is being minimized while the other is being maximized, subject to orthogonality constraints. The objective is to estimate the matrix V P RI\u02c6K in the TT format, assuming that huge-scale structured matrices A, B and V admit good low-rank TT approximations.\nFigure 3.14 illustrates that a recursive contraction of TT cores reduces the trace ratio optimization problem (3.55) to a set of following smaller scale trace ratio minimizations\nmin V(n)PRIn\u02c6Kn\ntr((V(n))T A(n) V(n)) tr((V(n))T B(n) V(n)) , s.t. (V(n))TV(n) = IKn ,\n(3.56)\nwhere the relatively small-size matrices, A(n) and B(n), are formally defined\n123\nas\nA(n) = [VT\u2030nAV\u2030n] P RRn\u00b41 InRn\u02c6Rn\u00b41 InRn (3.57)\nand\nB(n) = [VT\u2030nBV\u2030n] P RRn\u00b41 InRn\u02c6Rn\u00b41 InRn . (3.58)"}, {"heading": "3.6.1 TT Networks for Canonical Correlation Analysis (CCA)", "text": "The Canonical Correlation Analysis (CCA) method, introduced by Hotelling in 1936, can be considered as a generalization of the PCA, and is a classical method for determining the relationship between two sets of variables (for modern approaches to CCA, see [Bach and Jordan, 2005, Chu et al., 2013, Cunningham and Ghahramani, 2015] and references therein).\nGiven two zero-mean (i.e., centered) datasets, X P RI\u02c6J and Y P RL\u02c6J , recorded from the same set of J observations, the CCA seeks linear combinations of the latent variables in X and Y that are maximally mutually correlated. Formally, the classical CCA computes two projection vectors, wx = w (1) x P RI and wy = w (1) y P RL, so as to maximize the correlation coefficient\n\u03c1 = wTx XYTwy b\n(wTx XXTwx)(wTy YYTwy) . (3.59)\nIn a similar way, kernel CCA can be formulated by replacing the inner product of matrices by the kernel matrices, Kx P RJ\u02c6J and Ky P RJ\u02c6J , to give\n\u03c1 = max \u03b1x ,\u03b1y\n\u03b1Tx KxKy\u03b1y b\n(\u03b1Tx KxKx\u03b1x)(\u03b1Ty KyKy\u03b1y) . (3.60)\nSince the correlation coefficient \u03c1 is invariant to the scaling of the vectors wx and wy, the standard CCA can be equivalently formulated as a constrained optimization problem\nmax wx ,wy\nwTx XY Twy (3.61)\ns.t. wTx XX Twx = wTy Y TYwy = 1.\nThe vectors t1 = XTwx and u1 = YTwy are referred to as the canonical variables.\n124\nMultiple canonical vectors for the classical CCA can be computed by reformulating the optimization problem in (3.61), as follows\nmax Wx , Wy\ntr(WTx X Y T Wy) (3.62)\ns.t. WTx XX T Wx = IK, WTy YY T Wy = IK, WTx XY T Wy = \u039b,\nwhere Wx = [w (1) x , w (2) x , . . . , w (K) x ] P RI\u02c6K and Wy = [w(1)y , w (2) y , . . . , w (K) y ] P RL\u02c6K and \u039b is any diagonal matrix. However, this may become computationally very expensive due to orthogonality constraints for large-scale matrices.\nAn alternative approach is to use the orthogonal CCA model, which can be formulated as [Cunningham and Ghahramani, 2015]\nmax Wx ,Wy\n tr(WTx XYTWy) b\ntr(WTx XXTWx) tr(WTy YYTWy)  (3.63) s.t. WTx Wx = IK, W T y Wy = IK.\nThis model can be relatively easily implemented using the TT network approach, as illustrated in Figure 3.15. By using contractions of TT sub-networks, the huge-scale optimization problem in (3.64) can be transformed into a set of smaller optimization problems\nmax W(n)x ,W (n) y\n tr(W(n) Tx C(n)xy W(n)y ) b\ntr(W(n) Tx C (n) xx Wx) tr(W (n) T y C (n) yy W (n) y )  (3.64) s.t. W(n) Tx W (n) x = IK, W (n) T y W (n) y = IK,\nfor n = 1, 2, . . . , N, where C(n)xy = WTx, \u2030nXYTWy, \u2030n, C (n) xx = WTx, \u2030nXXTWx, \u2030n and C (n) yy = WTy, \u2030nYYTWy, \u2030n.\nFor computational tractability of huge-scale CCA problems and for physical interpretability of latent variables, it is often useful to impose sparsity constraints, to yield the sparse CCA. Sparsity constraints can be imposed, for example, by applying the `1-norm penalty terms to the matrices Wx and Wy, and assuming that the cross-product matrices XXT and YYT can be roughly approximated by identity matrices [Witten, 2010, Witten et al., 2009]. Under such assumptions, a simplified optimization\n125\nproblem can be formulated as\nmin Wx ,wy\n( \u00b4 tr(WTx XYTWy) + \u03b31}Wx}1 + \u03b32}Wy}1 ) (3.65)\ns.t. (Wx)T Wx = IK (Wy)T Wy = IK.\nA basic optimization scheme for an approximate huge sparse CCA in the TT format is illustrated in Figure 3.15, whereby the recursive contraction of TT networks reduces the problem to a set of smaller-dimensional sparse\n126\nCCA problems in the forms9\nmin W(n)x ,W (n) y\n( \u00b4 tr((W(n)x )T C (n) xy W (n) y ) + \u03b31 }W (n) x }1 + \u03b32 }W (n) y }1 ) (3.66)\ns.t. (W(n)x )T W (n) x = IK (W (n) y ) T W(n)y = IK,\nfor n = 1, 2, . . . , N, where C(n)xy = WTx, \u2030nXYTWy, \u2030n are the contracted crosscovariance matrices.\nNote that in the TT/MPO format, the sparsification penalty terms can be expressed as the `1 norm }W}1, e.g., as a sum of `1 norms of fibers of each TT-core,\n\u0159N n=1 \u0159RN\u00b41,RN ,K rn\u00b41,rn,k }w (n,k) rn\u00b41,rn}1, or equivalently as a sum of\nthe `1 norms of slices, \u0159N n=1 }W (n) (2)}1 (see Section 3.8.4 for more detail). In such cases, the orthogonalization of TT cores is replaced by a simple normalization of fibers to unit length."}, {"heading": "3.6.2 Tensor CCA for Multiview Data", "text": "The standard matrix CCA model has been generalized to tensor CCA, which in its simplest form can be formulated as the following optimization problem\nmax tw(n)u\n(C \u00af\u0302 1 w(1) \u00af\u0302 2 w(2) \u00a8 \u00a8 \u00a8 \u00af\u0302 N w(N)) (3.67)\ns.t. w(n) T Cnn w(n) = 1 (n = 1, 2, . . . , N),\nwhere w(n) P RIn are canonical vectors, Cnn P RIn\u02c6In are covariance matrices and C P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN is an Nth-order data tensor [Kim and Cipolla, 2009, Kim et al., 2007, Luo et al., 2015b].\nSimilar to the standard CCA, the objective is to find the canonical vectors which maximize the correlation function in (3.67). Depending on the application, the matrices Cnn and tensor C can be constructed in many different ways. For example, for multi-view data with N different views tXnuNn=1, expressed as Xn = [xn1, xn2, . . . , xnJ ] P RIn\u02c6J , the following\n9In quantum physics TT/MPOs X and YT are often called transfer matrices and the procedure to maximize the cost function WTx XYTWy is called Transfer Matrix Renormalization Group (TMRG) algorithm [Bursill et al., 1996, Wang and Xiang, 1997, Xiang and Wang, 1999].\n127\nsampled covariance matrices and the tensor can be constructed as [Luo et al., 2015b]\nCnn = 1 J\nJ \u00ff\nj=1\n(xnjxTnj) = 1 J XnXTn , (3.68)\nC = 1 J\nJ \u00ff\nj=1\n(x1j \u02dd x2j \u02dd \u00a8 \u00a8 \u00a8 \u02dd xNj). (3.69)\nThen the normalized data tensor, C, can be expressed as\npC = C \u02c61 pC\u00b41/211 \u02c62 pC \u00b41/2 22 \u00a8 \u00a8 \u00a8 \u02c6N pC \u00b41/2 NN (3.70)\nwhere the regularized covariance matrices are expressed as pCnn = Cnn + \u03b5IIn , with small regularization parameters \u03b5 \u0105 0 (see Figure 3.16(a)).\nIn order to find canonical vectors, the optimization problem (3.67) can be reformulated in a simplified form as\nmax tu(n)u\n(pC \u00af\u0302 1 u(1) \u00af\u0302 2 u(2) \u00a8 \u00a8 \u00a8 \u00af\u0302 N u(N)) (3.71)\ns.t. u(n) T u(n) = 1 (n = 1, 2, . . . , N),\nwhere u(n) = pC\u00b41/2nn w(n), as illustrated in Figure 3.16(b). Note that the problem is equivalent to finding the rank-one approximation, for which many efficient algorithms exist. However, if the number of views is relatively large (N \u0105 5) the core tensor becomes too large and we need to represent the core tensor in a distributed tensor network format (e.g., in TC format, as illustrated in Figure 3.16(b))."}, {"heading": "3.7 Two-Way Component Analysis in TT Formats", "text": "Low-rank matrix factorizations with specific constraints can be formulated in the following standard optimization setting [Cichocki, 2011, Cichocki and Zdunek, 2006, 2007, Cichocki et al., 1995, 2009]\nmin A,B\nJ(A, B) = }X\u00b4ABT}2F, (3.72)\nwhere a large-scale data matrix, X P RI\u02c6J , is given and the objective is to estimate the factor matrices, A P RI\u02c6R and B P RJ\u02c6R (with the assumption\n128\n(a)\nI1\nI2\nIN\nX1\nX2\nXN\nJ 1\nJ 1\nJ 1\nX1\nX2\nXN\nX1 T\nX2 T\nXT\n+\n+\n+\n\u03b5\n\u03b5\n\u03b5\nI I1=\n=\n=\nI2\nIN\nC11 I1\nC22\nCNN\nIN\nI2 \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb \u30fb\n^\n^\n^ N\nI1\nII2\nIIN\nJ\n(b)\nthat R ! tI, Ju), subject to suitable constraints being imposed on one or both factor matrices.\nIf such a structured data matrix can be represented by a low-rank TT or TC network (e.g., through a cheap matrix cross-approximation the factor matrices can be represented in a TT/TC format, as illustrated in the top panel of Figure 3.17), then in the first step no constraint on the TT core tensors, A(n) and B(m), need to be imposed to represent X in a distributed form. However, such a factorization is not unique and the so obtained\n129\nI JX RA B\nI=I ...1 I2 IN J=J ...1J2 JM\nTensorization and TT decomposition of X\n= RA B1\nI J\nR0\n130\ncomponents (in TT format) do not have any physical meaning. With a matrix X already in a compressed TT format, in the second step, we may impose the desired constraints. For example, by imposing the leftorthogonality on TT cores of A P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6R and right-orthogonality on the TT cores of B P RR\u02c6J1\u02c6\u00a8\u00a8\u00a8\u02c6JN , a truncated SVD can be computed for the R largest singular values and the corresponding singular vectors, as illustrated in the bottom panel of Figure 3.17. In a similar way, we can compute the pseudo-inverse of a huge matrix."}, {"heading": "3.8 Solving Huge-Scale Systems of Linear Equations", "text": "Solving linear systems of large scale equations arises throughout science and engineering; e.g, convex optimization, signal processing, finite elements and machine learning all rely partially on approximately solving linear equations, typically using some additional criteria like sparseness or smoothes.\nConsider a huge system of linear algebraic equations in a TT format (see also Part 1 [Cichocki et al., 2016]), given by\nAx \u2013 b (3.73)\nwhere A P RI\u02c6J , b P RI and x P RJ . The objective is to find an approximative solution, x P RJ , in a TT format, by imposing additional constraints (regularization terms) such as smoothness, sparseness and/or nonnegativity on the vector x. Several innovative TT/HT network solutions to this problem do exist (see [Dolgov and Savostyanov, 2014, Oseledets and Dolgov, 2012] and references therein), however, most focus on only symmetric square matrices. We shall next consider several more general cases."}, {"heading": "3.8.1 Solutions for a Large-scale Linear Least Squares Problem using ALS", "text": "The Least Squares (LS) solution (often called the ridge regression) minimizes the following regularized cost function\nJ(x) = }Ax\u00b4 b}22 + \u03b3}Lx}22 = xTATAx\u00b4 2xTATb + bTb + \u03b3xTLTLx, (3.74)\n131\nwith the Tikhonov regularization term on the right hand side, while L is the so-called smoothing matrix, typically in the form of the discrete first-order or second-order derivative matrix, and rL = LTL P RJ\u02c6J .\nUpon neglecting the constant factor, bTb, we arrive at a simplified form\nJ(x) = xTATAx\u00b4 2xTATb + \u03b3xTrLx. (3.75)\nThe approximative TT representation of the matrix A and vectors x and b allows for the construction of three tensor sub-networks, as shown in Figure 3.18. Upon simultaneous recursive contractions of all the three sub-networks for each node n = 1, 2, . . . , N, a large-scale infeasible optimization problem in (3.74) can be converted into a set of much smaller optimization problems based on the minimization of cost functions\nJ(x) = J(X\u2030nx(n)) (3.76)\n= (x(n))TXT\u2030nA TAX\u2030nx(n) \u00b4 2(x(n))TXT\u2030nATb\n+\u03b3(x(n))TXT\u2030nL TLX\u2030nx(n), n = 1, 2, . . . , N.\nThese can be expressed in the following equivalent compact forms\nJn(x(n)) = (x(n))T A (n) x(n) \u00b4 2(x(n))T b(n) + \u03b3(x(n))T L(n) x(n).\n(3.77)\nwhere x(n) P RRn\u00b41 JnRn , A(n) = XT\u2030nATAX\u2030n P RRn\u00b41 JnRn\u02c6Rn\u00b41 JnRn , b (n) = XT\u2030nATb P RRn\u00b41 JnRn , and L (n)\n= XT\u2030nLTLX\u2030n P RRn\u00b41 JnRn\u02c6Rn\u00b41 JnRn . In this way, a large-scale system of linear equations is converted into a\nset of smaller-size systems, which can be solved by any standard method\nA(n) x(n) \u2013 b(n), n = 1, 2, . . . , N, (3.78)\nIt is important to note that if the TT cores are left- and rightorthonormalized, the symmetric semi-positive definite matrix A(n) is better conditioned than the original huge matrix A.\nThe matrix multiplications in (3.77) is not performed explicitly, however, the TT format alleviates the curse of dimensionality via a recursive contraction of cores, as shown in Figure 3.18.\nRemark. The usual assumption that all data admits low-rank TT/QTT approximations is a key condition for successfully employing this approach. However, for data with a weak structure, TT approximations\n132\n133\nmay have relatively large ranks which makes the calculation difficult or even impossible. The way in which the TT ranks are chosen and adapted is therefore the key factor in efficiently solving huge-scale structured linear systems."}, {"heading": "3.8.2 Alternating Minimum Energy (AMEn) Algorithm for Solving a Large-scale Linear Least Squares Problems", "text": "The AMEn approach, introduced in Section 3.4.4 for the EVD problem, has been developed first historically as an efficient solution to large-scale least squares problems [Dolgov and Savostyanov, 2014]. The main difference from the EVAMEn method for solving eigenvalue problems is in the definition of the gradient of the cost function and in the computation of enrichment cores Z(n).\nAlgorithm 12 summarizes the AMEn algorithm for solving huge systems of linear equations Ax \u2013 b, for A P RI\u02c6J and b P RI given in the TT format, with I = I1 I2 \u00a8 \u00a8 \u00a8 IN and J = J1 J2 \u00a8 \u00a8 \u00a8 JN . In the first preliminary step, we assume, that the core X(n) P RRn\u00b41\u02c6Jn\u02c6Rn has been updated at the nth micro-iteration by solving the reduced linear system A(n) x(n) \u2013 b(n), similarly to the ALS method described in the previous section. Note that solving this reduced linear system is equivalent to minimizing the cost function J(x) = }Ax \u00b4 b}2 with respect to the core X(n). Similarly, for a residual vector defined as r = AT(Ax \u00b4 b) P RJ in the TT format (i.e., rR = xxrR (1) , rR (2) , . . . , rR (N)\n, yy, each of its TT cores rR (n) P RQn\u00b41\u02c6Jn\u02c6Qn can be updated via the ALS scheme by r(n) = RT\u2030n r = arg min rR (n) }r\u00b4 (ATAx\u00b4ATb)}2. In other words, the vectorrapproximates the gradient vector, \u2207J(x)9ATAx\u00b4ATb, and it can be efficiently updated via the contractions of core tensors.\nNext, for building an enrichment, Z(n) P RRn\u00b41\u02c6Jn\u02c6Qn , [Dolgov and Savostyanov, 2014] considered an approximation to the partially projected gradient\n(X\u0103n bL IJn\u00a8\u00a8\u00a8JN ) T(ATAx\u00b4ATb) \u2013 vec ( xxZ(n), R(n+1), . . . , R(N)yy ) P RRn\u00b41 Jn\u00a8\u00a8\u00a8JN (3.79)\nwith respect to Z(n) in the TT format. Since vec(xxZ(n), R(n+1), . . . , R(N)yy) = (IRn\u00b41 Jn bL (R\u0105n)T) z(n), the following closed form representation can be\n134\nAlgorithm 12: AMEn for linear systems Ax \u2013 b [Dolgov and Savostyanov, 2014]\nInput: Matrix A P RI\u02c6J , with I \u011b J, vector b P RI , initial guesses for x P RJ and residual pr = AT(Ax\u00b4 b) P RJ in the TT format Output: Approximate solution x in the TT format X = xxX(1), X(2), . . . , X(N)yy\n1: while not converged or iteration limit is not reached do 2: Right-orthogonalize cores X(n), pR (n)\nfor n = N, N \u00b4 1, . . . , 2 3: for n = 1 to N do 4: Update the core X(n) by solving A(n) x(n) \u2013 b(n) 5: Update the core pR (n)\nby solving R\u2030npr(n) \u2013 pr 6: if n \u0103 N then 7: Compute the core Z(n) of the partially projected\ngradient by solving (X\u0103n bL IJn bL (R\u0105n)T) z(n) \u2013 pr\n8: Enrich the cores X(n)jn \u00d0 [X (n) jn , Z (n) jn ] and X (n+1) jn+1 \u00d0 X(n+1)jn+1 0  9: Left-orthogonalize cores X(n), R(n)\n10: end if 11: end for 12: end while 13: return X = xxX(1), X(2), . . . , X(N)yy\nobtained\nz(n) = (X\u0103n bL IJn bL (R\u0105n)T)T r (3.80)\nwhich can be efficiently computed via recursive contractions of core tensors, similarly to the standard ALS method.\nNote that due to the partial gradient projection, the sizes of the TT cores X(n) and Z(n) now become consistent, which allows us to perform the concatenation\nrX (n) \u00d0 X(n) \u2018 3 Z(n) P RRn\u00b41\u02c6Jn\u02c6(Rn+Qn), (3.81)\nor equivalently, rX(n)jn \u00d0 [X (n) jn , Z (n) jn ] P R Rn\u00b41\u02c6(Rn+Qn). After the enrichment and the subsequent orthogonalization of the TT cores, the column space (i.e., the subspace spanned by the columns) of the\n135\nframe matrix, say X\u2030n+1, is expanded (see [Kressner et al., 2014a] for more detail).\nFor rigor, it can be shown that\nrange ( pX\u2030n+1 ) \u0104 range (X\u2030n+1) ,\nwhere range(A) denotes the column space of a matrix A, whereas X\u2030n+1 and pX\u2030n+1 are the frame matrices before and after the enrichment and orthogonalization. It should be noted that range(X\u2030n+1) = range(X\u0103n+1) b RJn+1 b range((X\u0105n+1)T), and that X\u0103n+1 = (X\u0103n bL IJn)X (n) L P RJ1 J2\u00a8\u00a8\u00a8Jn\u02c6Rn , where X (n) L is the left unfolding of X(n). The left-orthogonalization of the enriched core rX (n)\nin (3.81) can be written as\nrX(n)L = [ X(n)L Z (n) L ] = pX(n)L P P R Rn\u00b41 Jn\u02c6(Rn+Qn),\nwhere pX (n)\nP RRn\u00b41\u02c6Jn\u02c6R\u0302n is the left-orthogonalized core and P P RR\u0302n\u02c6(Rn+Qn) is a full row rank matrix. From these expressions, it follows that range(pX\u0103n+1) \u0104 range(X\u0103n+1)."}, {"heading": "3.8.3 Multivariate Linear Regression and Regularized Approximate Estimation of Moore-Penrose Pseudo-Inverse", "text": "The approaches described in the two previous sections can be straightforwardly extended to regularized multivariate regression, which can be formulated in a standard way as the minimization of the cost function\nJ(X) = }AX\u00b4 B}2F + \u03b3}LX}2F (3.82) = tr(XTATAX)\u00b4 2 tr(BTAX) + tr(BTB) + \u03b3 tr(XTLTLX),\nwhere A P RI\u02c6J (with I \u011b J), B P RI\u02c6K, X P RJ\u02c6K and L P RJ\u02c6J (see Figure 3.19). The objective is to find an approximative solution, X P RJ\u02c6K, in a TT format, by imposing additional constraints (e.g., via Tikhonov regularization). In a special case when B = II , for K = I the problem is equivalent to the computation of Moore-Penrose pseudo inverse in an appropriate TT format (see [Lee and Cichocki, 2016b] for computer simulation experiments).\n136\n137\nThe approximative TT representation of matrices A, B and X generates TT sub-networks shown in Figure 3.19, which can be optimized by the ALS or MALS (DMRG2) approaches.\nBy minimizing all three sub-networks simultaneously using recursive contractions of TT cores and performing ALS sequentially for each node n = 1, 2, . . . , N, a large-scale infeasible optimization problem in (3.83) can be converted into a set of much smaller optimization problems expressed by the set of (linked) cost functions\nJn(X(n)) = tr ( (X(n))T A(n) X(n) ) \u00b4 2 tr ( (B(n))T X(n) ) +\n+\u03b3 tr ( (X(n))T L(n) X(n) ) , n = 1, . . . , N, (3.83)\nwhere\nA(n) = XT\u2030nA TAX\u2030n P RJn\u02c6Jn , B(n) = XT\u2030nA TB P RJn\u02c6Kn , L(n) = XT\u2030nL TLX\u2030n P RJn\u02c6Jn\nand X(n) P RJn\u02c6Kn , with Jn = Rn\u00b41 JnRn and Kn = Rn\u00b41KnRn. The regularization term, \u03b3 tr ( (X(n))T L(n) X(n) ) , in (3.83) helps not only\nto alleviate the ill-posedness of the problem, but also to considerably reduce computational complexity and improve convergence by avoiding overestimation of the TT ranks. In other words, regularization terms, especially those that impose smoothness or sparsity, lead to smaller TT ranks with fewer parameters, thus yielding a better approximation.\nAlternatively, the MALS (DMRG2) procedure produces somewhat larger-scale optimization sub-problems\nJn(X(n)) = tr ( (X(n,n+1))T A(n,n+1) X(n,n+1) ) + (3.84)\n\u00b42 tr ( (B(n,n+1))T X(n,n+1) ) + \u03b3 tr ( (X(n,n+1))T L(n,n+1) X(n,n+1) ) where X(n,n+1) P RJn,n+1\u02c6Kn,n+1 and\nA(n,n+1) = XT\u2030n,n+1A TAX\u2030n,n+1 P RJn,n+1\u02c6Jn,n+1 , B(n,n+1) = XT\u2030n,n+1A TB P RJn,n+1\u02c6Kn,n+1 , (3.85) L(n,n+1) = XT\u2030n,n+1L TLX\u2030n,n+1 P RJn,n+1\u02c6Jn,n+1 (3.86)\n138\nwith Jn,n+1 = Rn\u00b41 Jn Jn+1Rn+1 and Kn,n+1 = Rn\u00b41KnKn+1Rn+1. One direction of future work is to select an optimal permutation of data tensor which gives an optimal tree for a TT approximation, given the available data samples."}, {"heading": "3.8.4 Solving Huge-scale LASSO and Related Problems", "text": "One way to impose sparsity constraints on a vector or a matrix is through the LASSO approach, which can be formulated as the following optimization problem [Boyd et al., 2011, Kim et al., 2015, Tan et al., 2015, Tibshirani, 1996a]:\nmin x t}Ax\u00b4 b}22 + \u03b3}x}1u (3.87)\nwhich is equivalent to\nmin x }Ax\u00b4 b}22, s.t. }x}1 \u010f t (3.88)\nand is closely related to the Basis Pursuits (BP) and/or compressed sensing (CS) problems, given by\nmin }x}1 s.t. Ax = b, min }Lx}1 s.t. }Ax\u00b4 b}22 \u010f \u03b5.\nIn many applications, it is possible to utilize a priori information about a sparsity profile or sparsity pattern [El Alaoui et al., 2016] in data. For example, components of the vector x may be clustered in groups or blocks, so-called group sparse components. In such a case, to model the group sparsity, it is convenient to replace the `1-norm with the `2,1-norm, given by\n}x}2,1 = \u00ff\nk\n}xgk}2, k = 1, 2, . . . , K, (3.89)\nwhere the symbol xgk denotes the components in the k-th group and K is the total number of groups.\nWhen the sparsity structures are overlapping, the problem can be reformulated as the Group LASSO problem [Chen et al., 2014]\nmin x }Ax\u00b4 b}22 + \u03b3}G\u03a6x}2,1, (3.90)\nwhere \u03a6 is the optional sparse basis and G a binary matrix representing the group configuration.\n139\nSimilarly, a multivariate regression problem with sparsity constraints can be formulated as\nJ(X) = }AX\u00b4 B}2F + \u03b3}X}Sq (3.91)\n= tr(XTATAX)\u00b4 2 tr(BTAX) + tr(BTB) + \u03b3 tr(XTX)q/2,\nwhere }X}Sq = tr(XTX)q/2 = \u0159 r \u03c3 q r (X) is the Schatten q-norm and \u03c3r(X) is a singular value of the matrix X. Other generalizations of the standard LASSO include the Block LASSO, Fused LASSO, Elastic Net and Bridge regression algorithms [Ogutu and Piepho, 2014]. However, these models have not yet been deeply investigated or experimentally tested in the tensor network setting, where the challenge is to efficiently represent/optimize in a TT format nonsmooth penalty terms in the form of the `1-norm }x}1, or more generally `q-norm }Lx} q q and }X}Sq with 0 \u0103 q \u010f 1.\nA simple approach in this direction would be to apply Iteratively Reweighted Least Squares (IRLS) methods10 [Candes et al., 2008], whereby the `1-norm is replaced by the reweighted `2-norm (see Figure 3.20) [Lee and Cichocki, 2016a], that is\n}x}1 = xTWx = J \u00ff\nj=1\nwjx2j , (3.92)\n10 The IRLS approach for the `1-norm is motivated by the variational representation of the norm }x}1 = minwj\u01050(0.5) \u0159 j(x 2 j /wj + wj).\n140\nwhere W(x) = diag(w1, w2, . . . , wJ), with the diagonal elements wj = |xj|\u00b41. For a slightly more general scenario with the `q-norm (q \u010f 1), we have\n}x}qq = xTW(x)x, (3.93)\nwhere the diagonal elements are wj = [|xj|2 + \u03b52]q/2\u00b41, and \u03b5 \u0105 0 is a very small number needed to avoid divergence for a small xj [Candes et al., 2008].\nSimilarly, for the non-overlapping group LASSO [Chen et al., 2014]\n}x}qq,1 = \u00ff\nk\n}xgk} q q = xTW(x)x, (3.94)\nwhere W is a block diagonal matrix W = diag(W1, W2, . . . , WK), with every diagonal sub-matrix Wk, given by [Wk]jj = (|x (gk) j |\n2 + \u03b52)q/2\u00b41. In practice, it is sufficient to approximate the diagonal matrix W(x) in a tensorized form by a rank one tensor, that is, with the TT rank of unity.\nIn the simplest scenario, upon dividing the datasets into an appropriate number of sub-groups, the huge-scale standard LASSO problem can be converted into a set of low-dimension group LASSO sub-problems, given\n141\nby (see also the first sub-networks in Figure 3.21)\nmin x(n)PRRn\u00b41 Jn Rn\n( x(n) TA(n)x(n) \u00b4 2[x(n)]Tb(n) + \u03b3 Jn \u00ff\njn=1\n}X(n)(:, jn, :)}F )\n(3.95)\nwhich can be solved by any efficient algorithm for the group LASSO, e.g., by ADMM methods [Boyd et al., 2011]."}, {"heading": "3.9 Truncated Optimization Approach in TT Format", "text": "A wide class of optimization problems can be solved by iterative algorithms which in general can be written as\nxk+1 = xk + \u2206xk = xk + zk, (3.96)\nwhere x P RI is a vector of the cost function J(x) and vector z = \u2206x P RI is the update vector, which can take various forms, e.g.,\n1. zk = \u00b4\u03b7k\u2207J(xk), for gradient descent methods, where \u03b7k \u0105 0 is a learning rate, and \u2207J(xk) is the gradient vector of the cost function J(x) at the current iteration;\n2. zk = \u00b4\u03b7kM\u00b41k \u2207J(xk), for preconditioned gradient descent, e.g., preconditioned conjugate gradient (CG), where M P RI\u02c6I is the preconditioned matrix;\n3. zk = \u00b4\u03b7kH\u00b41k \u2207J(xk) for quasi-Newton methods, where H P RI\u02c6I is an approximate Hessian;\n4. zk = \u00b4\u03b7kgk, for subgradient methods, where g is subgradient.\nTo date, the following cost functions and corresponding gradients have been investigated in connection with tensor networks [Ballani and Grasedyck, 2013, Dolgov, 2013, Lebedeva, 2011]\n\u2022 \u2207J(xk)9ATAxk \u00b4ATb for the squared residual J(x) = }Ax\u00b4 b}2;\n\u2022 \u2207J(xk)9Axk \u00b4 b for J(x) = xTAx \u00b4 2bTx. In this case the process (3.96) is often referred to as a preconditioned (non-stationary) Richardson iteration;\n142\n\u2022 \u2207J(xk)9Axk \u00b4 xk\u03bbk with \u03bbk = xTk Axk, for the Rayleigh quotient J(x) = xTAx with }x}2 = 1, which is an orthogonal projection of the gradient of J onto the tangent space of a sphere, or a Stiefel manifold in general [Absil et al., 2008].\nSince huge scale optimization problems are intractable, we need to represent vectors and corresponding matrices in distributed tensor formats. In a simplest scenario we can construct two TT networks, one representing vector x and the other representing approximately gradient vector z, as illustrated in Figure 3.22. Assuming that both huge vectors x and z are approximately represented in TT formats, the summation of two TT networks leads to a new TT network, for which the slices of the cores are given by (cf. Part 1 [Cichocki et al., 2016])\nX(n)in \u00d0 X(n)in 0 0 Z(n)in  , (n = 2, 3 . . . , N \u00b4 1) (3.97) with the border cores\nX(1)i1 \u00d0 [ X(1)i1 , Z (1) i1 ] , X(N)iN \u00d0  X(N)iN ) h Z(N)iN  , (3.98) while for the right to left sweep we have\nX(1)i1 \u00d0 [X (1) i1 , Z(1)i1 ], X (n) in \u00d0 X(n)in 0 0 Z(n)in  , (n = 1, 2, . . . , N \u00b4 1) X(N)iN \u00d0  X(N)iN ) h Z(N)iN\n . (3.99) After the enrichment of the core X(n), it needs to be orthogonalized in order to keep slice matrices orthogonal. Figure 3.22 illustrates the preconditioned gradient step (3.96), where the solution vector xk and the preconditioned gradient zk = \u00b4\u03b1kM\u00b41k \u2207J(xk) are represented in TT formats. Note that the TT ranks tSnu of the preconditioned gradient zk are typically quite large due to the operations involved in the computation of the gradient.\nRecall that the basic linear algebraic operations such as the matrixby-vector multiplication can be performed efficiently, with a logarithmic\n143\ncomputational complexity, via the TT representations for large matrices and vectors. Since the algebraic operations on TT formats usually result in increased TT ranks, truncation (e.g., TT-rounding [Oseledets, 2011a]) needs to be subsequently performed. It should be emphasized that truncated iteration methods are not limited to the TT format but also apply to any low-rank tensor formats.\nLet Te denote a truncation operator which truncates ranks of a tensor format with a tolerance e \u0105 0. Truncated iteration methods can be described as a preconditioned gradient step combined with truncation, as\nxk+1 = Te ( xk \u00b4 \u03b1kT\u03b7(M\u00b41k \u2207J(xk)) ) , e, \u03b7 \u0105 0.\nThe rank truncation can be carried out by a hard thresholding scheme or\n144\na soft thresholding scheme [Bachmayr et al., 2016]. In the iterative hard thresholding scheme, singular values below some threshold are set to zero as in the truncated SVD or TT rounding [Oseledets, 2011a]. In the iterative soft thresholding scheme, on the other hand, each of the singular values is shrunk to zero by the function s\u03ba(x) = sign(x)max(|x| \u00b4 \u03ba, 0) for some \u03ba \u011b 0. The soft thresholding is equivalent to minimizing the nuclear norm of a matrix, which is a convex relaxation of rank. Both types of thresholding schemes adaptively change the ranks during iteration process.\nThe truncated iteration approach has been already investigated and extensively tested for several very large-scale data applications using various low-rank tensor formats, especially for:\n\u2022 Solving huge systems of linear equations and discretization of PDEs by iterative algorithms (e.g., Richardson, CG GMRES) and combined with CP format [Beylkin and Mohlenkamp, 2005, Khoromskij and Schwab, 2011], CP and Tucker formats [Billaud-Friess et al., 2014], TT format [Dolgov, 2013, Khoromskij and Oseledets, 2010], HT format [Bachmayr and Dahmen, 2015, Bachmayr and Schneider, 2016, Kressner and Tobler, 2011a], and a subspace projection method combined with HT format [Ballani and Grasedyck, 2013];\n\u2022 A subspace projection method combined with HT format [Ballani and Grasedyck, 2013];\n\u2022 Computing extreme eigenvalues and corresponding eigenvectors by Lanczos method combined with TT format [Huang et al., 2016, Huckle and Waldherr, 2012], preconditioned inverse iteration with TT format [Mach, 2013], block CG method (LOBPCG) combined with TT format [Lebedeva, 2011] and with HT format [Kressner and Tobler, 2011b];\n\u2022 Iterative hard thresholding for low-rank matrix/tensor completion [Foucart and Rauhut, 2013, Tanner and Wei, 2013].\nAn advantage of truncated iteration methods is that we do not need to construct tensor networks for a specific cost (loss) function. Moreover, theoretical global convergence properties hold under some restricted conditions (see [Bachmayr et al., 2016] for more detail). In addition, due to the analogy between low-rank truncation and sparse signal estimation techniques, truncated iteration-type algorithms are also suitable for largescale compressed sensing [Blumensath and Davies, 2009].\nHowever, truncated iteration methods have a few drawbacks. They require the estimation of several auxiliary vectors. In addition to\n145\nthe desired vector, x, right-hand vector, z, need to have a low-rank representation in the specific tensor format. For example, in the GMRES method [Dolgov, 2013], even if the solution vector and the right-hand side vector are well approximated by the TT format, the residuals and Krylov vectors involved in intermediate iterations usually have high TT ranks. Moreover, all the core tensors and factor matrices of the solution vector have to be truncated at each iteration step, which often incurs high computational costs, especially when ranks are large."}, {"heading": "3.10 Riemannian Optimization for Low-Rank Tensor Manifolds", "text": "Methods of Riemannian Optimization (RO) have been recently a subject of great interest in data analytics communities; see, for example, [Absil et al., 2007, 2008, Bento et al., 2016, Bonnabel, 2013, Cambier and Absil, 2016]. Some optimization problems discussed in previous sections, can be naturally formulated on Riemannian manifolds, so as to directly benefit from the underlying geometric structures that can be exploited to significantly reduce the cost of obtaining solutions. Moreover, from a Riemannian geometry point of view, constrained optimization problems can often be viewed as unconstrained ones. It is therefore natural to ask whether Riemannian optimization can also help open up new research directions in conjunction with tensor networks; see for example [Ishteva et al., 2011, Kasai and Mishra, 2015, Sato et al., 2017, Steinlechner, 2016a, Zhou et al., 2016].\nRiemannian optimization for tensors can be formulated in the following generalized form\nmin XPMr J(X), (3.100)\nwhere J(X) is a scalar cost function of tensor variables and the search space, Mr, is smooth, in the sense of a differentiable manifold structure. Note that to perform optimization, the cost function needs to be defined for points on the manifoldMr. The optimization problem in the form (3.100) is quite general, and most of the basic optimization problems can be cast into this form.\nIn our applications of RO, Mr is a certain low-rank tensor manifold. One of the potential advantages of using Riemannian optimization tools for tensors is therefore that the intrinsic geometric and algebraic properties of\n146\nthe manifold allow us to convert a constrained optimization problems to an unconstrained one, in other words, to perform unconstrained optimization on a constrained space.\nManifold structure for Tucker model. The Riemannian optimization (also called the differential geometric optimization) has been successfully applied to the computation of the best Tucker approximation [Ishteva et al., 2009, 2011, Steinlechner, 2016a], whereby the Tucker manifold structure can be formulated through a set of Tucker tensors of fixed multilinear rank rML = tR1, . . . , RNu\nMr := tX P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN | rankML(X) = rMLu,\nwhich forms a smooth embedded submanifold of RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN of dimension\ndimMr = N \u017a\nn=1\nRn + N \u00ff\nn=1\n(Rn In \u00b4 R2n). (3.101)\nManifold structure for TT model. The set of TT tensors of fixed TTrank, rTT = tR1, . . . , RN\u00b41u, given by\nMr := tX P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN | rank(X)TT = rTT],\nforms a smooth embedded submanifold of RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN of dimension\ndimMr = N \u00ff\nn=1\nRn\u00b41 InRn \u00b4 N\u00b41 \u00ff\nn=1\nR2n, (3.102)\nwith RN = 1. If the inner product of two tensors induces a Euclidean metric on the embedding space RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN , then the above submanifold Mr is a Riemannian manifold [Holtz et al., 2012b, Steinlechner, 2016a, Uschmajew and Vandereycken, 2013].\nSimilar results are also available for the more general hierarchical Tucker models. For example, [Uschmajew and Vandereycken, 2013] developed the manifold structure for the HT tensors, while Lubich et al. [2013] developed the concept of dynamical low-rank approximation for both HT and TT formats. Moreover, Riemannian optimization in the Tucker and TT/HT formats has been successfully applied to large-scale tensor completion problems [Da Silva and Herrmann, 2013, Kasai and Mishra, 2015, Zhou et al., 2016].\nIt is important to emphasize that the condition of full TN rank is a pre-requisite for the sets Mr to be smooth manifolds. Otherwise, a set of\n147\ntensors for which the ranks are only bounded from above by Rmax would no longer be a smooth manifold, but an algebraic variety, for which special optimization methods are needed, at least for theoretical justification.\nThe fundamental and basic algorithms for Riemannian optimization [Absil et al., 2008] are quite attractive, but are still not widely used as they are more technically complicated than, e.g., standard gradient descent or conjugate gradient (CG) used in the classical optimization algorithms of low-rank tensor manifolds. However, in many cases Riemannian manifolds can be relatively easily implemented and often provide much better performance than the standard algorithms [Kasai and Mishra, 2015, Kressner et al., 2014b].\nThe MANOPT package [Boumal et al., 2014] (for a Python version see [Koep and Weichwald, 2016]) provides a useful interface for many standard matrix Riemannian optimization techniques, however, extensions for tensors (especially in high dimensions) are not easy and should be performed very carefully. The key idea is to work in the full space, but with the restrictions: i) the solution lies on a non-linear smooth manifold, and ii) the manifold condition is enforced implicitly (i.e., the derivatives with respect to the parametrization are not explicitly used).\nIn order to apply optimization algorithms based on line search, we must consider a direction on a manifold. The tangent space at X P Mr is the set of all tangent vectors (directions) at X, denoted by TXMr. The tangent space is a linear space, which, at a point on the manifold, provides us with a vector space of tangent vectors that allow us to find a search direction on the manifold. A Riemannian metric allows us to compute the angle and length of directions (tangent vectors). Roughly speaking, the optimization is performed on the tangent space, by either performing linear search or building a local model, in order to estimate the tangent vector and perform the next iteration on the manifold. The retraction operator provides a method to map the tangent vector to the next iterate (see the next section).\nThe smoothness condition of the manifold is crucial, since this allows us to define a tangent space, TXMr, for each point on the manifold, which locally approximates the manifold with second-order accuracy. Such a linearization of the manifold allows us to compute the next approximation restricted to the tangent space, which is a linear subspace. We shall next describe the general setting in which Riemannian optimization resides.\n148"}, {"heading": "3.10.1 Simplest Riemannian Optimization Method: Riemannian Gradient Descent", "text": "Consider a smooth manifold,Mr, in a vector space of tensors. The simplest optimization method is gradient descent which has the form\nXk+1 = Xk \u00b4 \u03b1k\u2207J(Xk),\nwhere \u03b1k \u0105 0 is the step size. For a sufficiently small \u03b1, the value of the cost function J(X) will decrease. However, this does not provide a restriction that Xk+1 should lie on a manifold; although Xk is on the manifold, the step in the direction of the negative gradient will leave the manifold. If the manifold is a linear subspace, then the solution is quite simple \u2014 we just need to take a projected gradient step, i.e., project \u2207J(Xk) to this subspace. For a general smooth manifold we can do the same, but the projection is computed onto the tangent space of the manifold at the current iteration point, that is\nXk+1 = Xk \u00b4 \u03b1kPTXkMr(\u2207J(Xk)). (3.103)\nThe continuous version of (3.103) will have the following projected gradient flow\ndX dt = \u00b4PTXMr(\u2207J(X)), (3.104)\nand it is quite easy to see that if X(0) (initial conditions) is on the manifold, then the whole trajectory X(t) will be on the manifold. This is, however, not guaranteed for the discretized version in (3.103), which can be viewed as a forward Euler scheme applied to (3.104). This also has important computational consequences.\nAn iterative optimization algorithm involves the computation of a search direction and then performing a step in that direction. Note that all iterations on a manifoldMr should be performed by following geodesics11. However, in general geodesics may be either expensive to compute or even not available in a closed form. To this end, in practice, we relax the constraint of moving along geodesics by applying the concept of Retraction, which is any map RX : TXMr \u00d1Mr that locally approximates a geodesic, up to first order, thereby reducing the computational cost of the update and ensuring convergence of iterative algorithms.\nMore precisely the Retraction can be defined, as follows\n11A geodesic is the shortest path on the manifold, which generalizes the concept of straight lines in Euclidean space.\n149\nDefinition 1 [Adler et al., 2002] A mapping RX is called the retraction, if\n1. RX(X, Z) PMr for Z P TXMr,\n2. RX is defined and smooth in a neighborhood of the zero section in TXMr,\n3. RX(X, 0) = X for all X PMr,\n4. ddt RX(X, tZ) |t=0= Z, for all X PMr and Z P TXMr.\nFor low-rank matrices and tensors with fixed TT-ranks, the simplest retraction is provided by respective SVD and TT-SVD algorithms, however, there are many other types of retractions; for more detail we refer to the survey [Absil and Oseledets, 2015].\nFinally, the Riemannian gradient descent method has the form\nXk+1 = RX(Xk, \u00b4\u03b1kPTXkMr(\u2207J(Xk))). (3.105)\nFigure 3.23 illustrates the three-step procedure within the Riemannian gradient descent iteration given in (3.105). From the implementation viewpoint, the main difficulty is to compute the projection of the gradient PTXkMr(\u2207J(Xk)). For the optimization problems mentioned above, this projection can be computed without forming the full gradient.\n150"}, {"heading": "3.10.2 Advanced Riemannian Optimization Methods", "text": "Vector transport. More advanced optimization techniques, such as conjugate-gradient (CG)-type methods and quasi-Newton methods, use directions from previous iteration steps. However, these directions lie in different tangent spaces which are different from the corresponding optimization in Euclidean spaces. To this end, we need to employ the concept of vector transport, which plays a crucial role in Riemannian optimization. The idea of vector transport is quite simple: it represents a mapping from one tangent space TXMr, to another tangent space, TYMr. For low-rank matrix/tensor manifolds, the orthogonal projection to a tangent space PTYMr is an example of a vector transport.\nRiemannian CG Method. Figure 3.24 illustrates the vector transport during a Riemannian CG iteration, which transforms a tangent vector in TXMr at a certain point X P Mr to another tangent space TYMr at Y P Mr. In the Riemannian CG iteration, the previous update direction \u03b7k\u00b41 P TXk\u00b41Mr is mapped to a tangent vector in TXkMr, and the transformed vector is combined with the Riemannian (projected) gradient vector to complete the CG iteration. Now, by using the projection onto the tangent space, retraction, and vector transport, we can immediately implement a general nonlinear CG method (Riemannian CG method) (see Algorithm 13).\n151\nAlgorithm 13: Riemannian CG method 1: Initial guess X0 PMr 2: \u03b70 := \u00b4PTX0Mr (\u2207J(X0))\n\u03b10 = arg min\u03b1 J(X0 + \u03b1\u03b70) 3: X1 = RX(X0, \u03b10\u03b70). 4: for k = 1, . . . do 5: \u03bek := PTXkMr (\u2207J(Xk)) 6: \u03b7k := \u00b4\u03bek + \u03b2kTXk\u00b41\u00d1Xk \u03b7k\u00b41 7: \u03b1k = arg min\u03b1 J(Xk + \u03b1\u03b7k) 8: Find the smallest integer m \u011b 0 such that J(Xk)\u00b4 J(RX(Xk, 2\u00b4m\u03b1k\u03b7k)) \u011b \u03b4 x\u03b7k, 2\u00b4m\u03b1k\u03b7ky 9: Xk+1 := RX(Xk, 2\u00b4m\u03b1k\u03b7k).\n10: end for\nThe main problem in Riemannian CG methods is the computation of the parameter \u03b2k within the conjugate gradient direction. To this end, as suggested in Kressner et al. [2016] the Polak-Ribiere update formula [Nocedal and Wright, 2006] can be adapted to Riemannian optimization [Absil et al., 2008]."}, {"heading": "3.10.3 Riemannian Newton Method", "text": "The implementation of Riemannian Newton-type algorithms is much more complicated, but follows the standard concept of Newton optimization methods. The local search direction, \u03bek, is determined from the correction equation [Absil et al., 2008] as\nHXk \u03bek = \u00b4PTXkMr\u2207J(Xk), (3.106)\nwhere the Riemannian Hessian, HXk : TXkMr \u00d1 TXkMr, is defined as [Absil et al., 2013, Kressner et al., 2016]\nHXk = PTXkMr ( \u22072 J(Xk) + P1Xk\u2207J(Xk) ) PTXkMr , (3.107)\nand P1Xk is the derivative of the projector PTXMr with respect to X. The second term in the sum corresponds to the curvature of the manifold. If the true minimizer is on the manifold, then \u2207J(X\u02da) = 0, and at least in the vicinity of the solution the second term plays no role. However, this term can cause problems with stability if the solution is close to a singular point, where the tangent space can exhibit considerable variation. In many\n152\ncases, only the first term in (3.107) is used, leading to the approximate Newton method, also known as the Gauss-Newton method in constrained optimization, given by\npHXk = PTXkMr \u2207 2 J PTXkMr . (3.108)\nIf \u22072 J is positive definite (e.g., for strongly convex J(X)) the matrix pHX is also nonnegative definite.\nThe Gauss-Newton method has a simple interpretation: we approximate the manifold by a tangent space, and seek for the minimum of a specific cost function J(X) on the tangent space. However, the theoretical justification of such methods, even for relatively simple low-rank matrix cases, is still an open problem. Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al. [2011].\nRetraction and vector transport are critical operations to the success of sophisticated Riemannian optimization algorithms, such as Riemannian CG, Newton and/or quasi-Newton methods. Retraction is used to obtain the next iterate and vector transport is used to compare tangent vectors in different tangent spaces and to transport operators from one tangent space to another tangent space.\nParametrization. When considering the tensor case, the whole concept above applies to the matrix-based tensor formats, such as the Tucker format and TT format. However, the concept does not apply to the CP decomposition, since the set of all tensors with a given canonical rank does not form a manifold (CP decomposition is unique)."}, {"heading": "3.10.4 Low-Rank TT-Manifold", "text": "Consider a set of Nth-order tensors in TT formats, with all TT-ranks equal to (R1, . . . , RN\u00b41). This set can also be thought of as an intersection of (N\u00b4 1) low-rank matrix manifolds, since the nth TT-rank of a tensor X is equal to the matrix rank of its nth mode unfolding X\u0103n\u0105.\nA tensor in the TT-format can be parameterized in the form\nX(i1, . . . , iN) = X (1) i1 \u00a8 \u00a8 \u00a8X(N)iN ,\nwhere X(n)in P R Rn\u00b41\u02c6Rn are slices of TT-cores X(n) .\n153\nTangent space of TT. The tangent space at a point X in the TT-manifold is defined as a set of tensors of the form\n\u03b4X(i1, . . . , iN) = \u03b4X (1) i1 X(2)i2 \u00a8 \u00a8 \u00a8X (N) iN + \u00a8 \u00a8 \u00a8+ X(1)i1 X (2) i2 \u00a8 \u00a8 \u00a8 \u03b4X(N)iN , (3.109)\nand is parameterized by the variations of the TT-cores \u03b4X(n). We denote the QR decompositions of unfoldings X\u010fn and X\u011bn+1 as\nX\u010fn = Q\u010fnR\u010fn, (X\u011bn+1)T = Q\u011bn+1R\u011bn+1,\nand the orthogonal projectors onto the column spaces of these matrices can be introduced in the form\nP\u010fn = Q\u010fnQT\u010fn, P\u011bn = Q\u011bnQ T \u011bn.\nUsing these projectors the orthogonal projector onto the tangent space, PTXMr(Z), can be written as [Holtz et al., 2012b, Lubich et al., 2015]\nPTXMr(Z) = N \u00ff\nn=1\nP\u010fnX (P \u011bn+1 X (Z)) , (3.110)\nwhere\nP\u010fnX (Z) = Ten(P\u010fnZ\u0103n\u0105) , (3.111) P\u011bn+1X (Z) = Ten(Z\u0103n\u0105P\u011bn+1), (3.112)\nand Ten(Z) is an inverse operator of the matricization of the tensor X, i.e., creating a tensor of the same size as the tensor X.\nIt should be noted that any point on a tangent space has TTranks bounded by (2R1, . . . , 2RN\u00b41), thus the computation of the TTrank (R1, . . . , RN\u00b41) approximation is possible by running the rounding procedure (or the TT-SVD) at the O(NIR3) cost, where we assumed that In = I, Rn = R. Therefore, many Riemannian optimization algorithms for low rank matrices remain formally the same, making the Riemannian optimization a promising and powerful tool for optimization over TTmanifolds."}, {"heading": "3.10.5 Dynamical Low-Rank Matrix/Tensor Approximation", "text": "Riemannian optimization is intricately connected with the concept of dynamical low-rank approximation, and although it was introduced in\n154\nmathematics by Koch and Lubich [2007], its roots are in quantum mechanics and can be traced back to the so-called Dirac-Frenkel principle. Indeed, some fundamental results can be found in the quantum molecular dynamics community which has used the so-called Multi Configurational Time-Dependent Hartree (MCTDH) method already since the 1990s [Manthe et al., 1992]. As an example, consider a time-varying matrix A(t) which we wish to approximate by a low-rank matrix X(t). Of course, this can be achieved on a sample-by-sample basis, by computing SVD for each t, but this does not involve any \u201cmemory\u201d about the previous steps.\nA Dirac-Frenkel principle states that the optimal trajectory should not minimize the distance }A(t)\u00b4 Y(t)} over all matrices of rank R, but instead the local dynamics given by\n}dA dt \u00b4 dY dt }. (3.113)\nFor the Euclidean norm, the minimization of the cost function (3.113) leads the following differential equations on manifoldMr\ndY dt = PTYMr dA dt , Y(0) PMr. (3.114)\nIn other words, the approximated trajectory Y(t) is such that the velocity dYdt always lies in the tangent space, and thus the trajectory always stays on the manifold. The dynamical approximation in (3.114) allows us to construct (and actually strictly define) low-rank approximation to the dynamical systems. If A(t) admits the representation\ndA dt = F(A, t), (3.115)\nwhich is a typical case in quantum mechanics, where the idea was first proposed, we obtain a dynamical system for the point on a low-rank manifold in the form\ndY dt = PTYMr F(Y, t). (3.116)\nSuch a system can be efficiently treated using only the parametrization of a point on a such manifold, thereby greatly reducing the cost (and if the manifold approximates the solution well,with a sufficiently good accuracy). Note that the continuous variant of the Riemannian gradient in (3.103) can be written exactly in the form (3.116), with\nF(Y, t) = \u00b4\u2207J.\n155\nRetraction is needed only in the discrete case, since in the continuous case the exact solution lies exactly on the manifold. Thus, if we have an efficient way to solve (3.116) numerically, that would give an efficient way of solving Riemannian optimization problems.\nThere are two straightforward ways of solving (3.116). The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form\nY(t) = U(t)S(t)VT(t).\nThe second approach is also straightforward: apply any time integration scheme to the equation (3.116). In this case, a standard method will yield the solution which is not on the manifold, and a retraction would be needed. In Lubich and Oseledets [2014] a simple and efficient solution to this problem, referred to as the projector-splitting scheme, was proposed, based on the special structure of the manifold. This stable and explicit second-order scheme for the integration of (3.116) examines the projector onto the tangent space\nPTYMr Z = UU TZ + ZVVT \u00b4UUTZVVT = PU + PV \u00b4 PUV ,\nand represents it as a sum of three projectors, in order to apply the classical Strang-Marchuk splitting scheme. For each projector, the equations of dynamical low-rank approximations can now be easily integrated. Indeed, the equation\ndY dt = dA dt VVT,\nhas a simple solution; the V component does not change and the new point is just\nY1 = Y0 + (A1 \u00b4A0)V, (3.117)\nwhere A1 = A(t + h), A0 = A(t). Similar formulas are valid for PV and PUV . What is not trivial is the order in which the splitting steps are taken: we should make a step in PU , then in PUV , and then in PV . This leads to a much better convergence, and moreover an exactness result [Lubich and Oseledets, 2014] can be proven if A1 and A0 are on the manifold, and Y0 = A0, then this scheme is exact.\nThrough the link with the Riemannian optimization, the projectorsplitting scheme can be also used here, which can be viewed as a secondorder retraction [Absil and Oseledets, 2015]. A simple form can be obtained\n156\nfor a given X and a step Z, where for the matrix case the retraction is\nU1S 1 2 = QR((X + Z)V0), V1ST1 = QR((X + Z) TU1).\nThis is just one step of a block power method, applied to (X+Z). Although this is only a crude approximation of the SVD of (X + Z) which is the standard retraction, the projector-splitting retraction is also a second-order retraction.\nThe generalization to the TT network was proposed by Lubich et al. [2015], and can be implemented within the framework of sweeping algorithms, allowing for the efficient TT-approximation of dynamical systems and solution of optimization problems with non-quadratic functionals."}, {"heading": "3.10.6 Convergence of Riemannian Optimization Algorithms", "text": "Convergence theory for the optimization over non-convex manifolds is much more complicated than the corresponding theory in Euclidean spaces, and far from complete. Local convergence results follow from the general theory [Absil et al., 2008], however the important problem of the curvature and singular points is not yet fully addressed. One way forward is to look for the desingularization [Khrulkov and Oseledets, 2016], another technique is to employ the concept of tangent bundle. Even if the final point is on the manifold, it is not clear whether it is better to take a trajectory on the manifold. An attempt to study the global convergence was presented in [Kolesnikov and Oseledets, 2016], and even in this case, convergence to the spurious local minima is possible in a carefully designed example. Also, convergence should be considered together with low-rank approximation to the solution itself. When we are far from having converged, a rankone approximation to the solution will be satisfactory, then the rank should gradually increase. This was observed experimentally in [Kolesnikov and Oseledets, 2016] in the form of a \u201cstaircase\u201d convergence. So, both the local convergence, in the case when the solution is only approximately of low-rank, and the global convergence of Riemannian optimization have to be studied, but numerous applications to machine learning problems (like matrix/tensor completion) confirm its efficiency, while the applications to tensor network optimization have just started to appear.\n157"}, {"heading": "3.10.7 Exponential Machines for Learning Multiple Interactions", "text": "Riemannian optimization methods have been recently demonstrated as a powerful approach for solving a wide variety of standard machine learning problems. In this section, we will briefly describe one such promising application [Novikov et al., 2016]. Consider machine learning tasks with categorical data, where for improved performances it is important to properly model complex interactions between the multiple features. Consider the training data, t(xm, ym)uMm=1, where xm = [x1,m, . . . , xN,m]T is an N-dimensional feature vector and ym the target value of the m-th object.\nIn the special case of N = 3, the linear model which incorporates interactions between the features x1, x2 and x3, can be described by\npy(x) = w000 + w100x1 + w010x2 + w001x3 + w110x1x2 + w101x1x3 + w011x2x3 + w111x1x2x3,\nwhere the parameters wijk denote the strength of interactions between the features xi, xj and xk (see also Chapter 1).\nNote that in the general case of N features, the number of pairwise interactions is O(N2), and the number of all possible interactions grows exponentially as O(2N). Such interactions occur, for example, in language/text analysis or in sentiment analysis where each individual word can interact with other words. To deal with the exponentially large number of parameters, Novikov et al. [2016], Stoudenmire and Schwab [2016] proposed the so-called Exponential Machines (ExM), where a large tensor of parameters is represented compactly in the TT format in order to provide low-rank regularization of the model and a reduction of the problem to a manageable scale. The so obtained linear model can be described in a general tensor form as\npy(X) = xW, Xy = 1 \u00ff\ni1=0\n\u00a8 \u00a8 \u00a8 1 \u00ff\niN=0\nwi1,...,iN N \u017a\nn=1\nxinn ,\nwhere W P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN (In = 2 ,@n) is the tensor of parameters and X = [1, x1]T \u02dd \u00a8 \u00a8 \u00a8 \u02dd [1, xN ]T is the rank-1 tensor of all interaction terms.\nThe TT representation of the weight tensor is given by\nW = W(1) \u02c61 W(2) \u02c61 \u00a8 \u00a8 \u00a8 \u02c61 W(N)\n= xxW(1), W(2), . . . , W(N)yy,\nwhere W(n) P RRn\u00b41\u02c6In\u02c6Rn are third-order core tensors for n = 1, . . . , N with R0 = RN = 1.\n158\nSuch a linear model can be applied to various machine learning problems for which the parameters are learned by minimizing the cost function\nJ(W) = M \u00ff\nm=1\nl (xW, Xmy , ym) + \u03bb}W}2F, (3.118)\nwhere l(py, y) is a squared loss function (in least squares), hinge loss (in support vector machines), and logistic loss (in logistic regression), and }W}2F = xW, Wy.\nRiemannian gradient descent can be applied to solve very efficiently the problem in (3.118), where the gradient of the cost function is computed as\n\u2207J(W) = M \u00ff\nm=1\nBl Bpy Xm + \u03bbW. (3.119)\nDue to its linearity, the projection of the gradient to the tangent space is expressed as the sum\nPTWMr(\u2207J(W)) = M \u00ff\nm=1\nBl Bpy PTWMr(Xm) + \u03bbW. (3.120)\nMoreover, a stochastic version of the Riemannian gradient descent, summarized in Algorithm 14, can be derived by sampling randomly a mini-batch (subset) of data points from the total of M training data, which makes the method more useful for big data applications. See [Novikov et al., 2016] for more technical details, such as the initialization, step-size selection and dropout.\nThis Riemannian optimization algorithm has been successfully applied to analyze formally a tensor with 2160 entries (i.e. to analyze a text with 160 words, where interactions between all words have been exploited). Furthermore, the Exponential Machine has been applied to a recommender system datasets MovieLens with 105 users [Novikov et al., 2016]."}, {"heading": "3.10.8 Future Perspectives of Riemannian Optimization", "text": "There has been a steady growth of the interest in Riemannian optimization for machine learning problems, especially in the context of low-rank matrix constraints [Boumal and Absil, 2011, Hosseini and Sra, 2015]. As an alternative, the nuclear norm regularization can be used, since it\n159\nAlgorithm 14: Stochastic Riemannian optimization for learning interactions (ExM) [Novikov et al., 2016]\nInput: Training data t(xm, ym)uMm=1, desired TT rank tR1, . . . , RN\u00b41u, number of iterations K, mini-batch size P, 0 \u0103 c1 \u0103 0.5, 0 \u0103 \u03c1 \u0103 1 Output: Weight tensor W in TT format which approximately minimizes the loss function (3.118)\n1: Initialize \u03b10 = 1 2: for k = 1, . . . , K do 3: Sample P data objects randomly, denote their indices by h1, . . . , hP P t1, . . . , Mu 4: Gk = \u0159P p=1 Bl Bpy PTWk\u00b41Mr (Xhp) + \u03bbWk\u00b41 5: Find the smallest integer s \u011b 0 such that J(Wk\u00b41)\u00b4 J(RW(Wk\u00b41,\u00b4\u03c1s\u03b1kc1Gk)) \u011b \u03c1s\u03b1kc1xGk, Gky 6: Wk := RW(Wk\u00b41,\u00b4\u03c1s\u03b1kGk). 7: end for\noften leads to convex problems with provable guarantees of convergence. The disadvantage is that without additional tricks, the optimization is performed with full matrices, whereas the Riemannian approach still works in the low-parametric representation, which makes it more efficient especially in the tensor case. Attempts have been made to generalize the nuclear norm concept to the tensor case [Phien et al., 2016], but a recent negative result shows that it is not possible to provide a good convex surrogate for the TT-manifold [Rauhut et al., 2016], thus making the Riemannian optimization the most promising tool for low-rank constrained optimization. For enhanced efficiency it is desirable to create instruments for the construction of such methods in the spirit of modern deep learning frameworks that are based on automatic differentiation techniques. The Pymanopt [Koep and Weichwald, 2016] is the first step in this direction, but it is still quite far from big-data problems, since it works with full matrices even for a low-rank manifold, and in the tensor case that would be prohibitive. The stochastic variants of the Riemannian gradient can be readily derived [Bonnabel, 2013, Novikov et al., 2016, Shalit et al., 2010] and such methods are applicable to a large number of data points (similar to stochastic gradient descent). Derivation of methods which are more complex than Riemannian stochastic gradient descent is still work in progress.\n160"}, {"heading": "3.11 Software and Computer Simulation Experiments with TNs for Optimization Problems", "text": "Tensor decompositions and tensor network algorithms require sophisticated software libraries, which are being rapidly developed.\nThe TT Toolbox, developed by Oseledets and coworkers, (http:// github.com/oseledets/TT-Toolbox) for MATLAB and (http://github. com/oseledets/ttpy) for PYTHON is currently the most complete software for the TT (MPS/MPO) and QTT networks [Oseledets et al., 2012]. The TT toolbox supports advanced applications, which rely on solving sets of linear equations (including the AMEn algorithm), symmetric eigenvalue decomposition (EVD), and inverse/psudoinverse of huge matrices.\nRelated and complementary algorithms implemented by Kressner et al. [2014a] are available within the MATLAB TTeMPS Toolbox (http://anchp.epfl.ch/TTeMPS). This MATLAB toolbox is designed to accommodate various algorithms in the Tensor Train (TT) / Matrix Product States (MPS) format, making use of the object-oriented programming techniques in current MATLAB versions. It also provides an implementation of the efficient AMEn algorithm for calculating multiple extremal eigenvalues and eigenvectors of high-dimensional symmetric eigenvalue decompositions.\nFor standard TDs (CPD, Tucker models) the Tensor Toolbox for MATLAB, originally developed by Kolda and Bader, provides several general-purpose functions and special facilities for handling sparse, dense, and structured TDs [Bader and Kolda, 2006, 2015], while the N-Way Toolbox for MATLAB, by Andersson and Bro, was developed mostly for Chemometrics applications [Andersson and Bro, 2000].\nThe Tensorlab toolbox developed by Vervliet, Debals, Sorber, Van Barel and De Lathauwer builds upon a complex optimization framework and offers efficient numerical algorithms for computing the CP, BTD, and constrained Tucker decompositions. The toolbox includes a library of many constraints (e.g., nonnegativity, orthogonality) and offers the possibility to combine and jointly factorize dense, sparse and incomplete tensors [Vervliet et al., 2016]. The new release introduces a tensorization framework and offers enhanced support for handling large-scale and structured multidimensional datasets. Furthermore, sophisticated tools for the visualization of tensors of an arbitrary order are also available.\nOur own developments include the TDALAB (http://bsp.brain. riken.jp/TDALAB) and TENSORBOX Matlab toolboxes (http://www.bsp.\n161\nbrain.riken.jp/~phan), which provide a user-friendly interface and advanced algorithms for basic tensor decompositions (CP, Tucker, BTD) [Phan et al., 2012, Zhou and Cichocki, 2013].\nThe Hierarchical Tucker toolbox by Kressner and Tobler (http://www. sam.math.ethz.ch/NLAgroup/htucker_toolbox.html) focuses mostly on HT type of tensor networks [Kressner and Tobler, 2014], which avoid explicit computation of the SVDs when truncating a tensor which is already in a HT format [Espig et al., 2012, Grasedyck et al., 2013, Kressner and Tobler, 2014].\nIn quantum physics and chemistry, a number of software packages have been developed in the context of DMRG techniques for simulating quantum networks. One example is the intelligent Tensor (iTensor) by Stoudenmire and White [2014], an open source C++ library for rapid development of tensor network algorithms. The iTensor is competitive against other available software packages for basic DMRG calculations, and is especially well suited for developing next-generation tensor network algorithms.\nThe Universal Tensor Network Library (Uni10), developed in C++ by Ying-Jer Kao, provides algorithms for tensor network contraction and a convenient user interface (http://uni10.org/about.html). The library is aimed towards more complex tensor networks such as PEPS and MERA [Kao et al., 2015]. The Kawashima group in the University of Tokyo has developed a parallel C + + library for tensor calculations, \u201cmptensor\u201d (https://github.com/smorita/mptensor). This is the first library which can perform parallel tensor network computation on supercomputers.\nTensor Networks for huge-scale optimization problems are still in their infancy, however, comprehensive computer experiments over a number of diverse applications have validated the significant performance advantages, enabled by this technology. The following paradigms for large-scale structured datasets represented by TNs have been considered in the recent publications listed below:\n\u2022 Symmetric EVD, PCA, CCA, SVD and related problems [Dolgov et al., 2014, Kressner and Macedo, 2014, Kressner et al., 2014a, Lee and Cichocki, 2015],\n\u2022 Solution of huge linear systems of equations, together with the inverse/pseudoinverse of matrices and related problems [Bolten et al., 2016, Dolgov, 2013, Dolgov and Savostyanov, 2014, Lee and Cichocki, 2016b, Oseledets and Dolgov, 2012],\n162\n\u2022 Tensor completion problems [Grasedyck et al., 2015, Karlsson et al., 2016, Kressner et al., 2014b, Steinlechner, 2016b],\n\u2022 Low-rank tensor network methods for dynamical and parametric problems [Cho et al., 2016, Garreis and Ulbrich, 2017, Liao et al., 2015, Lubich et al., 2015],\n\u2022 TT/HT approximation of nonlinear functions and systems [Dolgov and Khoromskij, 2013, Khoromskij, 2011b, Khoromskij and Miao, 2014],\n\u2022 CP decomposition and its applications [Choi and Vishwanathan, 2014, Kamal et al., 2016, Kang et al., 2012, Shin et al., 2017, Vervliet and Lathauwer, 2016, Wang et al., 2015, Wetzstein et al., 2012],\n\u2022 Tucker decompositions and their applications [Caiafa and Cichocki, 2013, 2015, Jeon et al., 2015, 2016, Oseledets et al., 2008, Zhao et al., 2016, Zhe et al., 2016a],\n\u2022 TT/HT decompositions and their applications in numerical methods for scientific computing [Benner et al., 2016, Corona et al., 2015, Dolgov et al., 2016, Khoromskaia and Khoromskij, 2016, Khoromskij and Veit, 2016,?, Litsarev and Oseledets, 2016, Zhang et al., 2015b],\n\u2022 Classification and clustering problems using the TT format [Stoudenmire and Schwab, 2016, Sun et al., 2016, Zhe et al., 2016b].\nWhen it comes to the implementation, the recently introduced TensorFlow system is an open-source platform, developed by Google, which uses hardware tensor processing units and provides particularly strong support for deep neural networks applications."}, {"heading": "3.12 Open Problems and Challenges in Applying TNs for Optimization and Related Problems", "text": "In this chapter, we have demonstrated that tensor networks are promising tools for very large-scale optimization problems, numerical methods (scientific computing) and dimensionality reduction. As we move towards real-world applications, it is important to highlight that the TN approach requires the following two assumptions to be satisfied:\n1. Datasets admit sufficiently good low-rank TN approximations,\n163\n2. Approximate solutions are acceptable.\nChallenging issues that remain to be addressed in future research include the extension of the low-rank tensor network approach to complex constrained optimization problems with multiple constraints, non-smooth penalty terms, and/or sparsity and/or nonnegativity constraints. It would be particularly interesting to investigate low-rank tensor networks for the following optimization problems, for which efficient algorithms do exist for small- to medium-scale datasets:\n1. Sparse Nonnegative Matrix Factorization (NMF) [Cichocki et al., 2009], given by\nmin A,B\n}X\u00b4ABT}2F + \u03b31}A}1 + \u03b32}B}1, s.t. air \u011b 0, bjr \u011b 0,\n(3.121)\nwhere X P RI\u02c6J+ is a given nonnegative data matrix and the `1 norm } \u00a8 }1 is defined element-wise, as the sum of absolute values of the entries. The objective is to estimate huge nonnegative and sparse factor matrices A P RI\u02c6R+ , B P R J\u02c6R + .\n2. Linear and Quadratic Programming (LP/QP) and related problems in the form\nmint1 2 xTQx + cTxu, s.t. Ax = b, x \u011b 0,\nwhere x P RJ and the matrix Q P RJ\u02c6J is symmetric positive definite. When Q = 0 then the above QP problem reduces to the standard form of Linear Program (LP).\nIf the nonnegativity constraints, x \u011b 0, are replaced by conic constraints, x P K, then the standard QP becomes a quadratic conic program, while Semidefinite Programs (SDPs) are the generalization of linear programs to matrices. In the standard form, an SDP minimizes a linear function of a matrix, subject to linear equality constraints and a positive definite matrix constraint\nmin X\u013e0\ntr(CX) + \u03b3 tr(Xq), s.t. tr(AiX) = bi, i = 1, . . . , I,\nwhere Ai, C and X are N\u02c6N square matrices and X must be a positive definite matrix.\n164\nAnother important generalization of LPs is Semi-Infinite Linear Programs (SILPs), which are linear programs with infinitely many constraints. In other words, SILPs minimize a linear cost function subject to an infinite number of linear constraints.\n3. Sparse Inverse Covariance Selection [Friedman et al., 2008], given by\nmin XPSN\u02c6N++ tr(CxX)\u00b4 log det X + \u03b3}X}1, s.t. \u03b1IN \u013a X \u013a \u03b2IN ,\n(3.122)\nwhere Cx is the given sample covariance matrix, SN\u02c6N++ denotes the set of N\u02c6N square strictly positive definite matrices and parameters \u03b1, \u03b2 \u0105 0 impose bounds on the eigenvalues of the solution.\n4. Regressor Selection, whereby\nmin }Ax\u00b4 b}22, s.t. }x}0 = card(x) \u010f c, (3.123)\nand the operator card(x) denotes the cardinality of x, that is, the number of non-zero components in a vector x. The objective is to find the best fit to a given vector b in the form of a linear combination of no more than c columns of the matrix A.\n5. Sparse Principal Component Analysis (SPCA) using the Penalized Matrix Decomposition (PMD) [d\u2019Aspremont et al., 2007, Witten, 2010, Witten et al., 2009], given by\nmax u,v tuTAvu, (3.124) s.t. }u}22 \u010f 1, }v}22 \u010f 1, P1(u) \u010f c1, P2(v) \u010f c2,\nwhere the positive parameters, c1, c2, control the sparsity level, and the convex penalty functions P1, P2 can take a variety of forms. Useful examples of penalty functions which promote sparsity and smoothness are [Witten, 2010]\nP(v) = }v}1 = I \u00ff\ni=1\n|vi|, (LASSO) (3.125)\nP(v) = }v}0 = I \u00ff\ni=1\n| sign(vi)|, (`0 \u00b4 norm) (3.126)\nP(v) = I \u00ff\ni=1\n|vi|+ \u03b3 I \u00ff\ni=2\n|vi \u00b4 vi\u00b41|. (Fused-LASSO) (3.127)\n165\n6. Two-way functional PCA/SVD [Huang et al., 2009] in the form\nmax u,v tuTAv\u00b4 \u03b3 2 P1(u)P2(v)u, s.t. }u}22 \u010f 1, }v}22 \u010f 1. (3.128)\n7. Sparse SVD [Lee et al., 2010], given by\nmax u,v tuTAv\u00b4 1 2 uTuvTv\u00b4 \u03b31 2 P1(u)\u00b4 \u03b32 2 P2(v)u. (3.129)\n8. Generalized nonnegative SPCA [Allen and Maletic-Savatic, 2011] which solves\nmax u,v tuTARv\u00b4 \u03b1}v}1u, s.t. uTu \u010f 1, vTRv \u010f 1, v \u011b 0. (3.130)\nThe solution to these open problems would be important for a more wide-spread use of low-rank tensor approximations in practical applications.\nAdditional important and challenging issues that have to be addressed include:\n\u2022 Development of new parallel algorithms for wider classes of optimization problems, with the possibility to incorporate diverse cost functions and constraints. Most of algorithms described in this monograph are sequential; parallelizing such algorithms is important but not trivial.\n\u2022 Development of efficient algorithms for tensor contractions. Tensors in TT and HT formats can be relatively easily optimized, however, contracting a complex tensor network with cycles (loops) such as PEPS, PEPO, and MERA, is a complex task which requires large computational resources. Indeed, for complex tensor network structures, the time needed to perform an exact contraction may in some cases increase exponentially with the order of the tensor. The work in this direction is important because, although approximate contractions may be sufficient for certain applications, they may also brings difficulties in controlling the approximation error.\n166\n\u2022 Finding the \u201cbest\u201d tensor network structures or tensor network formats for specific datasets or specific applications, so as to provide low-rank TN approximations of a sufficiently low-order and with a relatively low computational complexity.\nThe term \u201cbest\u201d is used here in the sense of a tensor network which provides linear or sub-linear compression of large-scale datasets, so as to reduce storage cost and computational complexity to affordable levels.\nIn other words, the challenge is to develop more sophisticated and flexible tensor networks, whereby the paradigm shift is in a full integration of complex systems and optimization problems, e.g., a system which simulates the structure of biological molecule [Savostyanov et al., 2014].\n\u2022 Current implementations of tensor train decompositions and tensor contractions still require a number of tuning parameters, such as the approximation accuracy and the estimation of TN ranks. A step forward would be the development of improved and semiautomatic criteria for the control of approximation accuracy, and a priori errors bounds. In particular, the unpredictable accumulation of the rounding error and the problem of TT-rank explosion should be carefully addressed.\n\u2022 Convergence analysis tools for TN algorithms are still being developed but are critical in order to better understand convergence properties of such algorithms.\n\u2022 Theoretic and methodological approaches are needed to determine the kind of constraints imposed on factor matrices/cores, so as to be able to extract only the most significant features or only the desired hidden (latent) variables with meaningful physical interpretations.\n\u2022 Investigations into the uniqueness of various TN decompositions and their optimality properties (or lack thereof) are a prerequisite for the development of faster and/or more reliable algorithms.\n\u2022 Techniques to visualize huge-scale tensors in tensor network formats are still missing and are urgently needed.\n167\nChapter 4\nTensor Networks for Deep Learning\nRecent breakthroughs in the fields of Artificial Intelligence (AI) and Machine Learning (ML) have been largely triggered by the emergence of the class of deep convolutional neural networks (DCNNs), often simply called CNNs. The CNNs have become a vehicle for a large number of practical applications and commercial ventures in computer vision, speech recognition, language processing, drug discovery, biomedical informatics, recommender systems, robotics, gaming, and artificial creativity, to mention just a few.\nIt is important to recall that widely used linear classifiers, such as the SVM and STM discussed in Chapter 2, can be considered as shallow feedforward neural networks (NN). They therefore inherit the limitations of shallow NNs, when compared to deep neural networks1 (DNNs), which include:\n\u2022 Inability to deal with highly nonlinear data, unless sophisticated kernel techniques are employed;\n\u2022 Cumbersome and non-generic extensions to multi-class paradigms2;\n\u2022 Incompatibility of their shallow architectures to learn high-level features which can only be learnt through a multiscale approach, that is, via DNNs.\n1A deep neural network comprises two or more processing (hidden) layers in between the input and output layers, in contrast to a shallow NN, which has only one hidden layer.\n2The multi-class version of the linear STM model is essentially either a one-versus-rest or one-versus-one extension of the two-class version.\n168\nThe renaissance of deep learning neural networks [Goodfellow et al., 2016, LeCun et al., 2015, Schmidhuber, 2015, Schneider, 2017] has both created an active frontier of research in machine learning and has provided many advantages in applications, to the extent that the performance of DNNs in multi-class classification problems can be similar or even better than what is achievable by humans.\nDeep learning is also highly interesting in very large-scale data analytics, since:\n1. Regarding the degree of nonlinearity and multi-level representation ability of features, deep neural networks often significantly outperform their shallow counterparts;\n2. High-level representations learnt by deep NN models, that are easily interpretable by humans, can also help us to understand the complex information processing mechanisms and multi-dimensional interaction of neuronal populations in the human brain;\n3. In big data analytics, deep learning is very promising for mining structured data, e.g., for hierarchical multi-class classification of a huge number of images.\nRemark. It is well known that both shallow and deep NNs are universal function approximators3 in the sense of their ability to approximate arbitrarily well any continuous function of N variables on a compact domain; this is achieved under the condition that a shallow neural network has an unbounded width (i.e., the size of a hidden layer), that is, an unlimited number of parameters. In other words, a shallow NN may require a huge (intractable) number of parameters (curse of dimensionality), while DNNs can perform such approximations using a much smaller number of parameters.\nDespite recent advances in the theory of DNNs, for a continuing success and future perspectives of DNNs (especially DCNNs), the following fundamental challenges need to be further addressed:\n\u2022 Ability to generalize while avoiding overfitting in the learning process;\n3Universality refers to the ability of a neural network to approximate any function, when no restrictions are imposed on its size (width). On the other hand, depth efficiency refers to the phenomenon whereby a function realized by polynomially-sized deep neural network requires shallow neural networks to have super-polynomial (exponential) size for the same accuracy of approximation (curse of dimensionality). This is often referred to as the expressive power of depth.\n169\n\u2022 Fast learning and the avoidance of \u201cbad\u201d local and spurious minima, especially for highly nonlinear score (objective) functions;\n\u2022 Rigorous investigation of the conditions under which deep neural networks are \u201cmuch better\u201d than the shallow networks (i.e., NNs with one hidden layer);\n\u2022 Theoretical and practical bounds on the expressive power of a specific architecture, i.e., quantification of the ability to approximate or learn wide classes of unknown nonlinear functions;\n\u2022 Ways to reduce the number of parameters without a dramatic reduction in performance.\nThe aim of this chapter is to discuss the many advantages of tensor networks in addressing the last two of the above challenges and to build up both intuitive and mathematical links between DNNs and TNs. Revealing such inherent connections will both cross-fertilize the areas of deep learning and tensor networks and provide new insights. In addition to establishing the existing and developing new links, this will also help to optimize existing DNNs and/or generate new, more powerful, architectures.\nWe start with an intuitive account of DNNs based on a simplified hierarchical Tucker (HT) model, followed by alternative simple and efficient architectures. Finally, more sophisticated TNs, such as MERA and 2D tensor network models are briefly discussed in order to enable more flexibility, potentially improved performance, and/or higher expressive power of the next generation DCNNs."}, {"heading": "4.1 A Perspective of Tensor Networks for Deep Learning", "text": "Several research groups have recently investigated the application of tensor decompositions to simplify DNNs and to establish links between deep learning and low-rank tensor networks [Chen et al., 2017, Cohen et al., 2016, Lebedev and Lempitsky, 2015, Novikov et al., 2015, Poggio et al., 2016, Yang and Hospedales, 2016]. For example, Chen et al. [2017] presented a general and constructive connection between Restricted Boltzmann Machines (RBM) and TNs, together with the correspondence between\n170\ngeneral Boltzmann machines and the TT/MPS model (see detail in the next section).\nIn a series of research papers [Cohen and Shashua, 2016, Cohen and Shashua, 2016, Cohen et al., 2016, Sharir et al., 2016] the expressive power of a class of DCNNs was analyzed using simplified Hierarchal Tucker (HT) models (see the next sections). Particulary, Convolutional Arithmetic Circuits (ConvAC), also known as Sum-Product Networks, and Convolutional Rectifier Networks (CRN) have been theoretically analyzed as variants of the HT model. The authors claim that a shallow (single hidden layer) network corresponds to the CP (rank-1) tensor decomposition, whereas a deep network with log2 N hidden layers realizes or corresponds to the HT decomposition (see the next section). Some authors also argued that the \u201cunreasonable success\u201d of deep learning can be explained by inherent laws of physics within the theory of TNs, which often employ physical constraints of locality, symmetry, compositional hierarchical functions, entropy, and polynomial log-probability, that are imposed on the measurements or input training data [Chen et al., 2017, Lin and Tegmark, 2016, Poggio et al., 2016].\nThis all suggests that a very wide range of tensor networks can be potentially used to model and analyze some specific classes of DNNs, in order to obtain simpler and/or more efficient neural networks in the sense of enhanced expressive power or reduced complexity. In other words, consideration of tensor networks in this context may give rise to new NN architectures which could even be superior to the existing ones; this topic has so far been overlooked by practitioners.\nFurthermore, by virtue of redundancy present in both TNs and DNNs, methods used for the reduction or approximation of TNs may become a vehicle to achieve more efficient DNNs, and with a reduced number of parameters. Also, given that usually neither TNs nor DNNs are unique (two NNs with different connection weights and biases may be modeling the same nonlinear function), the knowledge about redundancy in TNs can help simplify DNNs.\nThe general concept of optimization of DNNs via TNs is illustrated in Figure 4.1. For a specific DNN, we first construct its equivalent or corresponding TN representation; then the TN is transformed into a reduced or canonical form by performing, e.g., truncated SVD. This will reduce the rank value to the minimal requirement determined by a desired accuracy of approximation. Finally, the reduced and optimized TN is mapped back to another DNN of similar universal approximation power to the original DNN, but with a considerably reduced number of parameters.\n171\nThis follows from the fact that a rounded (approximated) TN has a smaller number of parameters.\nIt should be noted that, in practice, the reduction of DNNs through lowrank TN approximations offers many potential advantages over a direct reduction of redundant DNNs, owing to the availability of many efficient optimization methods to reduce the number of parameters and achieve a pre-specified approximation error. Moreover, low-rank tensor networks are capable of avoiding the curse of dimensionality through low-order sparsely interconnected core tensors.\nOn the other hand, in the past two decades, quantum physicists and computer scientists have developed solid theoretical understanding and efficient numerical techniques for low-rank TN decompositions, especially the TT/MPS and TT/MPO. The entanglement entropy4, Renyi\u2019s\n4Entanglement is a physical phenomenon that occurs when pairs or groups of particles, such as photons, electrons, or qubits, are generated or interact in such way that the quantum state of each particle cannot be described independently of the others, so that a quantum state must be described for the system as a whole. Entanglement entropy is therefore a\n172\nentropy, entanglement spectrum and long-range correlations are the most widely used quantities (calculated from a spatial reduced density matrix) investigated in the theory of tensor networks. The spatial reduced density matrix is determined by splitting a TN into two parts, say, regions A and B, where a density matrix in region A is generated by integrating out all the degrees of freedom in region B. The entanglement spectra are determined by the eigenvalues of the reduced density matrix [Eisert et al., 2010, Zwanziger, 1994]. Entanglement entropy also characterizes the information content of a bipartition of a specific TN. Furthermore, the entanglement area law explains that the entanglement entropy increases only proportionally to the boundary between the two tensor sub-networks. Also, entanglement entropy characterizes the information content of the distribution of singular values of a matricized tensor, and can be viewed as a proxy for the correlations between the two partitions; uncorrelated data has zero entanglement entropy at any bipartition.\nNote that TNs are usually designed to efficiently represent large systems which exhibit a relatively low entanglement entropy. In practice, similar to deep neural networks, we often need to only care about a small fraction of the input measurements or training data among a huge number of possible inputs. This all suggest that certain guiding principles in DNNs correspond to the entanglement area law used in the theory of tensor networks. These may then be used to quantify the expressive power of a wide class of DCNNs. Note that long range correlations also typically increase with the entanglement. We therefore conjecture that realistic datasets in most successful machine learning applications have relatively low entanglement entropies [Calabrese and Cardy, 2004]. On the other hand, by exploiting the entanglement entropy bound of TNs, we can rigorously quantify the expressive power of a wide class of DNNs applied to complex and highly correlated datasets."}, {"heading": "4.2 Restricted Boltzmann Machines (RBM)", "text": "Restricted Boltzmann machines (RBMs), illustrated in Figure 4.2, are generative stochastic artificial neural networks, which have found a wide range of applications in dimensionality reduction, feature extractions, and recommender systems, by virtue of their inherent modeling of the\nmeasure for the amount of entanglement. Strictly speaking, entanglement entropy is a measure of how quantum information is stored in a quantum state and is mathematically expressed as the von Neumann entropy of the reduced density matrix.\n173\nprobability distributions of a variety of input data, including natural image and speech signals.\nThe RBMs are fundamental basic building blocks in a class of Deep (restricted) Boltzmann Machines (DBMs) and Deep Belief Nets (DBNs). In such deep neural networks, after training one RBM layer, the values of its hidden units can be treated as data for training higher-level RBMs (see Figure 4.3).\nTo create an RBM, we first perform a partition of variables into at least two sets: visible (observed) variables, v P RM, and hidden variables, h P RK. The goal of RBMs is then to learn a higher-order latent representation, h P RK, typically for binary variables5, within a bipartite undirected graphical model encoding these two layers, as illustrated in Figure 4.2(a).\nThe basic idea is that the hidden units of a trained RBM represent relevant features of observations. In other words, an RBM is a probabilistic graphical model based on a layer of hidden variables, which is used to build the probability distribution over input variables. The standard RBM assigns energy to a joint configuration (v, h) as (see Figure 4.2(b))\nE(v, h; \u03b8) = \u00b4(F (v) + vTWh + aTv + bTh), (4.1)\nwhere a P RM and b P RK are the biases corresponding to the visible and hidden variables, W P RM\u02c6K is the (coupling) matrix of mapping parameters, \u03b8 = ta, b, Wu is a set of the model parameters to be estimated, and F (v) is a type-specific function, e.g., F (v) = 0 for a binary input and F (v) = \u00b40.5 \u0159\nm v 2 m for Gaussian variables. Such an RBM model obeys the\nBoltzmann distribution for binary variables, as follows\np(v, h | \u03b8) = 1 Z(\u03b8) exp (\u00b4E(v, h; \u03b8)) , (4.2)\nwhere Z(\u03b8) is the normalization constant. The absence of connections between hidden binary variables within RBMs allows us to calculate relatively easily the marginal distribution of the visible variables, as follows\np(v | \u03b8) = \u00ff\nh\np(v, h | \u03b8) = 1 Z \u00ff\nh\nexp (\u00b4E(v, h; \u03b8)) (4.3)\n= 1 Z\nM \u017a\nm=1\nexp (amvm) K \u017a\nk=1\n( 1 + exp ( bk + M \u00ff\nm=1\nwmkvm\n)) ,\n5There are several extensions of RBMs for continuous data values.\n174\nand the distribution of the hidden variables\np(h | \u03b8) = \u00ff v p(v, h | \u03b8) = 1 Z \u00ff v exp(\u00b4E(v, h; \u03b8)) (4.4)\n= 1 Z exp\n( K \u00ff\nk=1\nbkhk\n) M \u017a\nm=1\n( 1 + exp ( am + K \u00ff\nk=1\nwmkhk\n)) .\nThe above formulas indicate that an RBM can be regarded as a product of\n176\n\u201cexperts\u201d, in which a number of experts for individual observations are combined multiplicatively [Fischer and Igel, 2012].\nThe RBM model can be converted into an equivalent (or corresponding) tensor network model. Figure 4.2(c) [Chen et al., 2017] shows that such a tensor network model comprises a set of interconnected diagonal core tensors: Kth-order tensors \u039b(m)v = diagK(1, e\nam) and Mth-order tensors \u039b\n(k) h = diagM(1, e bk), all of sizes 2 \u02c6 2 \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2. This means that the core tensors are extremely sparse since only the diagonal entries \u039b (m) v (1, 1, . . . , 1) = 1, \u039b (m) v (2, 2, . . . , 2) = eam , and \u039b (k) h (1, 1, . . . , 1) = 1 , \u039b(k)h (2, 2, . . . , 2) = e bk are non-zero. The diagonal core tensors in the visible and hidden layer are interconnected via the 2\u02c6 2 matrices Cmk = [1, 1; 1, ewmk ].\nThe bipartite structure of the RBM enables units in one layer to become conditionally independent of the other layer. Thus, the conditional distributions over the hidden and visible variables can be factorized as\np(v|h; \u03b8) = \u015bM m=1 p(vm|h), p(h|v; \u03b8) = K \u017a\nk=1\np(hk|v), (4.5)\nand\np(vm = 1|h) = \u03c3 ( \u00ff\nk\nwmkhk + am\n) ,\np(hk = 1|v) = \u03c3 ( \u00ff\nm wmkvm + bk\n) ,\n(4.6)\nwhere \u03c3(x) = 1/(1 + exp(\u00b4x)) is the sigmoid function. Higher-order RBMs. The modeling power of an RBM can be enhanced by increasing the number of hidden units, or by adding and splitting visible layers and/or adding conditional and activation layers. The standard RBM can be generalized and extended in many different ways. The most natural way is to jointly express the energy for more than two set of variables. For example, for four sets of binary variables, v, h, x, y, the energy function (with neglected biases) can be expressed in the following form (see Figure 4.4)\nE(v, h, x, y) = \u00b4 I \u00ff\ni=1\nJ \u00ff\nj=1\nM \u00ff\nm=1\nK \u00ff\nk=1\nwijmk xi yj vm hk\n= \u00b4W \u00af\u0302 1 x \u00af\u0302 2 y \u00af\u0302 3 v \u00af\u0302 4h. (4.7)\n177\nUnfortunately, higher-order RBM models suffer from a large number of parameters, which scales with the product of visible, hidden and conditioning variables. To alleviate this problem, we can perform dimensionality reduction via low-rank tensor network approximations."}, {"heading": "4.2.1 Matrix Variate Restricted Boltzmann Machines", "text": "The matrix-variate restricted Boltzmann machine (MVRBM) model was proposed as a generalization of the classic RBM to explicitly model matrix data [Qi et al., 2016], whereby both input and hidden variables are in their matrix forms which are connected by bilinear transforms. The MVRBM has much fewer model parameters than the standard RBM and thus admits\n178\nfaster training while retaining a comparable performance to the classic RBM.\nLet V P RJ\u02c6M be a binary matrix of visible variables, and H P RK\u02c6I a binary matrix of hidden variables. Given the parameters of a 4th-order tensor, W P RI\u02c6J\u02c6M\u02c6K, and bias matrices, A P RJ\u02c6M and B P RK\u02c6I , the energy function can be defined as (see Figure 4.5)\nE(V, H; \u03b8) = \u00b4 I \u00ff\ni=1\nJ \u00ff\nj=1\nM \u00ff\nm=1\nK \u00ff\nk=1\nvmjwijmkhki (4.8)\n\u00b4 M \u00ff\nm=1\nJ \u00ff\nj=1\nvjmajm \u00b4 K \u00ff\nk=1\nI \u00ff\ni=1\nhkibki,\nwhere \u03b8 = tW, A, Bu is the set of all model parameters. In order to reduce the number of free parameters, the higher-order weight tensor can be approximated by a suitable tensor network, e.g. a TT or HT network. In the particular case of a four-way weight tensor, W P RI\u02c6J\u02c6M\u02c6K, it can be represented via truncated SVD by two 3rd-order core tensors, W(1) P RM\u02c6K\u02c6R and W(2) P RI\u02c6J\u02c6R. For a very crude approximation with rank R = 1, these core tensors simplify to matrices W(1) P RM\u02c6K and W(2) P RI\u02c6J , while the energy function in (4.8) simplifies to the following form\nE(V, H; \u03b8) = \u00b4tr(W(1)HW(2)V)\u00b4 tr(VTA)\u00b4 tr(HTB). (4.9)\nBoth matrices W(1) and W(2) approximate the interaction between the input matrix V and the hidden matrix H. Note that in this way, the total number of free parameters is reduced from I JMK to I J + KM + JM + IK. For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al. [2016], Salakhutdinov and Hinton [2009]."}, {"heading": "4.2.2 Tensor-Variate Restricted Boltzmann Machines", "text": "The tensor-variate RMBs (TvRBMs) have the ability to capture multiplicative interactions between data modes and latent variables (see e.g. [Nguyen et al., 2015]). The TvRBMs are highly compact, in that the number of free parameters grows only linearly with the number of modes, while their multiway factoring of mapping parameters link data modes and hidden units. Modes interact in a multiplicative fashion gated by hidden units, and TvRBM uses tensor data and the hidden layer to construct an (N + 1)-mode tensor.\n179\n+ -A\nV\nJM + H\n-B IK\nV\nH\n-W (1) W (2)\nR\nI K\n-W\nJ M\nIn a TvRBM, the set of visible units is represented by an Nth-order tensor, V P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN , and the hidden units h are the same as in RBM. The goal is to model the joint distribution, p(V, h), by the optimization of the energy function\nE(V, h; \u03b8) = \u00b4F (V)\u00b4 xA, Vy \u00b4 bTh\u00b4 xW, V \u02dd hy, (4.10)\nwhere F (V) is a type-specific function, A P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN are visible biases, and W P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN\u02c6K are mapping parameters.\nThe hidden posterior is then given by\np(hk = 1|V; \u03b8) = \u03c3 (bk + xV, W(:, . . . , :, k)y) , (4.11)\nwhere \u03c3(x) is the sigmoid function. The generative distribution, p(V|h), on the other hand is type-specific.\n180\nThe most popular cases are the binary and Gaussian inputs. For a binary input, we have F (V) = 0, and the following generative distribution\np(vi1i2 ...iN |h) = \u03c3 ( ai1i2 ...iN + W(i1, . . . , iN , :) T h ) . (4.12)\nFor a Gaussian input, assuming unit variance, i.e. F (V) = \u00b40.5xV, Vy, the generative distribution becomes\np(vi1i2...iN |h) = N ( tai1i2...in + W(i1, . . . , iN , :) T hu; 1 ) , (4.13)\nwhere 1 P RI1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN is the tensor with unit elements. A major problem with the parameterizations of W is the excessively large number of free parameters, which scales with the product of data modes and hidden dimensions. In particular, the (N + 1)th-order weight tensor W has K \u015b\nn In elements, and its size quickly reaches billions of entries, even when the mode dimensionalities K, (I1, . . . , IN) and N are quite moderate. This makes learning in the raw tensor format extremely difficult or even impossible, since robust estimation of parameters would require a huge dataset. To this end, low-rank tensor network decompositions can be employed to construct approximate interactions between visible modes and hidden units.\nIn the simplest scenario, we can approximately represent W by a rank-R CP tensor\nW = R \u00ff\nr=1\n\u03bbr \u00a8 (w(1)r \u02dd \u00a8 \u00a8 \u00a8 \u02ddw (N) r \u02ddw (N+1) r ) (4.14)\n= \u039b\u02c61 W(1) \u00a8 \u00a8 \u00a8 \u02c6N W(N) \u02c6N+1 W(N+1),\nwhere the matrix W(n) = [w(n)1 , . . . , w (n) R ] P RIn\u02c6R for n = 1, . . . , N represents the mode-factor weights, and W(N+1) = W(h) P RK\u02c6R the hidden-factor matrix. Such factoring allows for multiplicative interactions between modes to be moderated by the hidden units through the hiddenfactor matrix W(h). Thus, the model captures the modes of variation through the new representation, enabled by h. By employing the CP decomposition, the number of mapping parameters may be reduced down to R(K + \u0159\nn In), which grows linearly rather than exponentially in N. Importantly, the conditional independence among intra-layer variables, i.e. p(h|V) =\n\u015bK k=1 p(hk|V) and p(V|h) = \u015b n \u015bIn in=1 p(vi1i2 ...iN |h), is not affected. Therefore, the model preserves the fast sampling and interface properties of RBM.\n181"}, {"heading": "4.3 Basic Features of Deep Convolutional Neural Networks", "text": "Basic DCNNs are characterized by at least three features: locality, weight sharing (optional) and pooling, as explained below:\n\u2022 Locality refers to the connection of a (artificial) neuron to only neighboring neurons in the preceding layer, as opposed to being fed by the entire layer (this is consistent with biological NNs).\n\u2022 Weight sharing reflects the property that different neurons in the same layer, connected to different neighborhoods in the preceding layer, often share the same weights. Note that weight sharing, when combined with locality, gives rise to standard convolution6.\n\u2022 Pooling is essentially an operator which gradually decimates (reduces) layer sizes by replacing the local population of neural activations in a spatial window by a single value (e.g., by taking their maxima, average values or their scaled products). In the context of images, pooling induces invariance to translation, which often does not affect semantic content, and is interpreted as a way to create a hierarchy of abstractions in the patterns that neurons respond to [Anselmi et al., 2015, Cohen et al., 2016].\nUsually, DCNNs perform much better when dealing with compositional function approximations7 and multi-class classification problems than shallow neural network architectures with one hidden layer. In fact, DCNNs can even efficiently and conveniently select a subset of features for multiple classes, while for efficient learning a DCNN model can be pre-trained by first learning each DCNN layer, followed by fine tuning of the parameter of the entire model, using e.g., stochastic gradient descent. To summarize, deep learning neural networks have the ability to arbitrarily well exploit and approximate the complexity of compositional hierarchical functions, a wide class of functions to which shallow networks are blind.\n6Although weight sharing may reduce the complexity of a deep neural network, this step is optional. However, the locality at each layer is a key factor which gives DCNNs an exponential advantage over shallow NNs in terms of the representation ability [Anselmi et al., 2015, Mhaskar and Poggio, 2016, Poggio et al., 2016].\n7A compositional function can take, for example, the following form h1(\u00a8 \u00a8 \u00a8 h3(h21(h11(x1, x2)h12(x3, x4)), h22(h13(x5, x6)h14(x7, x8)) \u00a8 \u00a8 \u00a8 ))).\n182"}, {"heading": "4.4 Score Functions for Deep Convolutional Neural Networks", "text": "Consider a multi-class classification task where the input training data, also called local structures or instances, e.g., input patches in images, are denoted by the set X = tx1, . . . , xNu, where xn P RS, (n = 1, . . . , N), belong to one of distinct categories (classes) denoted by c P t1, 2, . . . , Cu. Such a representation is quite natural for many high-dimensional data \u2013 in images, the local structures represent vectorization of patches consisting of S pixels, while in audio data can be represented through spectrograms.\nFor this kind of problems, DCNNs can be described by a set of multivariate score functions\ny = hc(x1, . . . , xN) = I1 \u00ff\ni1=1\n\u00a8 \u00a8 \u00a8 IN \u00ff\niN=1\nW c(i1, . . . , iN) N \u017a\nn=1\nf\u03b8in (xn),\n(4.15)\nwhere W c P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN is an Nth-order coefficient tensor, with all dimensions In = I, @n, N is the number of (overlapped) input patches txnu, In is the size (dimension) of each mode W c, and f\u03b81 , . . . , f\u03b8In are referred to as the representation functions (in the representation layer) selected from a parametric family of nonlinear functions8.\nIn general, the one-dimensional basis functions could be polynomials, splines or other sets of basis functions. Natural choices for this family of nonlinear functions are also radial basis functions (Gaussian RBFs), wavelets9, and affine functions followed by point-wise activations. Note that the representation functions in standard (artificial) neurons have the form\nf\u03b8i(x) = \u03c3(w\u0303 T i x + bi), (4.16)\nfor the set of parameters \u03b8i = tw\u0303i, biu, where \u03c3(\u00a8) is a suitably chosen activation function.\nThe representation layer play a key role to transform the inputs, by means of I nonlinear functions, f\u03b8i(xn) (i = 1, 2, . . . , I), to template input patches, thereby creating I feature maps [Cohen and Shashua, 2016]. Note\n8Note that the representation layer can be considered as a tensorization of the input patches xn.\n9Particularly interesting are Gabor wavelets, owing to their ability to induce features that resemble representations in the visual cortex of human brain.\n183\n(a) (b) (c)\nr ),\nwhere W(n) = [w(n)1 , . . . , w (n) R ] P RI\u02c6R. This CP model corresponds to a simple shallow neural network with one hidden layer comprising weights w(n)ir , and the output layer comprising weights \u03bb (c) r , r = 1, . . . , R. Note that the coefficient tensor Wc can be represented in a distributed form by any suitable tensor network.\nthat the representation layer can be described by a feature vector defined as\nf = f\u03b8(x1)b f\u03b8(x2)b \u00a8 \u00a8 \u00a8 b f\u03b8(xN) P R I1 I2\u00a8\u00a8\u00a8IN , (4.17)\nwhere f\u03b8(xn) = [ f\u03b81(xn), f\u03b82(xn), . . . , f\u03b8In (xn)] T P RIn for n = 1, 2, . . . , N and in = 1, 2, . . . , In. Equivalently, the representation layer can be described as a rank one tensor (see Figure 4.6(a))\nF = f\u03b8(x1) \u02dd f\u03b8(x2) \u02dd \u00a8 \u00a8 \u00a8 \u02dd f\u03b8(xN) P R I1\u02c6I2\u02c6\u00a8\u00a8\u00a8\u02c6IN . (4.18)\nThis allows us to represent the score function as an inner product of two tensors, as illustrated in Figure 4.6(a)\nhc(x1, . . . , xN) = xWc, Fy = Wc \u00af\u0302 1 f\u03b8(x1) \u00af\u0302 2 f\u03b8(x2) \u00a8 \u00a8 \u00a8 \u00af\u0302 N f\u03b8(xN). (4.19)\n184\nTo simplify the notations, assume that In = I, @n, then we can also construct a square matrix F P RI\u02c6I , with rows F(i, :) = [ f\u03b81(x (i)), f\u03b82(x (i)), . . . , f\u03b8I (x\n(i))] for i = 1, 2, . . . , I and entries taken from values of nonlinear basis functions t f\u03b81 , . . . , f\u03b8Iu on the selected templates tx(1), x(2), . . . , x(I)u [Cohen and Shashua, 2016].\nFor discrete data values, the score function can be represented by a grid tensor, as graphically illustrated in Figure 4.6(b). The grid tensor of the nonlinear score function hc(x1, , ..., xN) determined over all the templates x(1), x(2), ..., x(I) can be expressed, as follows\nW(hc) = Wc \u02c61 F\u02c62 F \u00a8 \u00a8 \u00a8 \u02c6N F. (4.20)\nOf course, since the order N of the coefficient (core) tensor is large, it cannot be implemented, or even saved on a computer due to the curse of dimensionality.\nTo this end, we represent the weight tensor in some low-rank tensor network format with a smaller number of parameters. A simple model is the CP representation, which leads to a shallow network as illustrated in Figure 4.6(c). However, this approach is associated with two problems: (i) the rank R of the coefficient tensor Wc can be very large (so compression ratio cannot be very high), (ii) the existing CP decomposition algorithms are not very stable for very high-order tensors, and so an alternative promising approach would be to apply tensor networks such as HT that enable us to avoid the curse of dimensionality.\nFollowing the representation layer, a DCNN may consists of a cascade of L convolutional hidden layers with pooling in-between, where the number of layers L should be at least two. In other words, each hidden layer performs 3D or 4D convolution followed by spatial window pooling, in order to reduce (decimate) feature maps by e.g., taking a product of the entries in sub-windows. The output layer is a linear dense layer.\nClassification can then be carried out in a standard way, through the maximization of a set of labeled score functions, hc for C classes, that is, the predicted label for the input instants X = (x1, . . . , xN) will be the index y\u0302 for which the score value attains a maximum, that is\ny\u0302 = argmaxtcuhc(x1, . . . , xN). (4.21)\nSuch score functions can be represented through their coefficient tensors which, in turn, can be approximated by low-rank tensor network decompositions.\n185\nOne restriction of the so formulated score functions (4.15) is that they allow for a straightforward implementation of only a particular class of DCNNs, called convolutional Arithmetic Circuit (ConvAC). However, the score functions can be approximated indirectly and almost equivalently using more popular CNNs (see the next section). For example, it was shown recently how NNs with a univariate rectified linear unit (ReLU) nonlinearity may perform multivariate function approximation [Poggio et al., 2016].\nAs discussed in Part 1 and in Chapter 1 the main idea is to employ a low-rank tensor network representation to approximate and interpolate a multivariate function hc(x1, . . . , xN) of N variables by a finite sum of separated products of simpler functions (i.e., via sparsely interconnected core tensors). Tensorial Mixture Model (TMM). Recently, the model in (4.15) has been extended to a corresponding probabilistic model, referred to as the Tensorial Mixture Model (TMM), given by Sharir et al. [2016]\nP(x1, x2, . . . , xN | y = c) (4.22)\n= I \u00ff\ni1=1\n\u00a8 \u00a8 \u00a8 I \u00ff\niN=1\nPy=c(i1, . . . , iN) N \u017a\nn=1\nP(xn|in; \u03b8in),\nwhere P(y=c)(i1, . . . , iN) (corresponding to W c(i1, . . . , iN)) are jointly decomposed prior tensors and tP(xn|in; \u03b8in)u (corresponding to f\u03b8in (xn)) are mixing components (marginal likelihoods) shared across all input samples xn (n = 1, . . . , N) and classes tcu, (c = 1, 2, . . . , C). Note that for computational tractability, suitable scaling and nonnegativity constraints must be imposed on the TMM model.\nThe TMM can be considered as a generalization of standard mixture models widely used in machine learning, such as the Gaussian Mixture Model (GMM). However, unlike the standard mixture models, in the TMM we cannot perform inference directly from (4.23), nor can we even store the prior tensor, P(y=c)(i1, . . . , iN) P RI\u02c6\u00a8\u00a8\u00a8\u02c6I , given its exponential size of IN entries. Therefore, the TMM presented by (4.23) is not tractable directly but only in a distributed form, whereby the coefficient tensor is approximated by low-rank tensor network decompositions with nonnegative cores. The TMM based on the HT decompositions is promising for multiple classification problems with partially missing training data, and potentially even regardless of the distribution of missing data [Sharir et al., 2016].\n186"}, {"heading": "4.5 Convolutional Arithmetic Circuits and HT Networks", "text": "Conceptually, the Convolutional Arithmetic Circuit (ConvAC) can be divided into three parts: (i) the first (input) layer is the representation layer which transforms input vectors (x1, . . . , xN) into N I real valued scalars t f\u03b8i(xn)u for n = 1, . . . , N and i = 1, . . . , I. In other words, the representation functions, f\u03b8i : R\nS \u00d1 R, i = 1, . . . , I, map each local patch xn into a feature space of dimension I; (ii) the second, a key part, is a convolutional arithmetic circuit consisting of many hidden layers that takes the N I measurements (training samples) generated by the representation layer; (iii) the output layer, which can be represented by a matrix, W(L) P RR\u02c6C, which computes C different score functions hc [Cohen et al., 2016].\nOnce the set of score functions has been formulated as (4.15), we can construct (design) a suitable multilayered or distributed representation for DCNN representation. The objective is to estimate the parameters \u03b81, . . . , \u03b8I and coefficient tensors10 W 1, . . . , W C. Since the tensors are of Nth-order and each with IN entries, in order to avoid the curse of dimensionality we employ low-rank tensor network representations. Note that a direct implementation of (4.15) or (4.23) is intractable owing to a huge number of parameters.\nThe simplified HT tensor network, shown in Figure 4.7, contains sparse 3rd-order core tensors W(l,j) P RR(l\u00b41,2j\u00b41)\u02c6R(l\u00b41,2j\u00b41)\u02c6R(l,j) for l = 1, . . . , L\u00b4 1 and matrices W(0,j) = [w(0,j)1 , . . . , w (0,j) R(0,j)\n] P RIj\u02c6R(0,j) for l = 0 and j = 1, . . . , N/2 l (for simplicity, we assumed that N = 2M = 2L). In order to mimic basic features of the standard ConvAC, we assumed that R(l,j) = R(l), @j and that frontal slices of the core tensors W(l,j) are diagonal matrices, with entries W(l,j)(r(l\u00b41), r(l\u00b41), r(l)) = w(l,j) r(l\u00b41),r(l)\n, as illustrated in Figure 4.7. The top (output) layer is represented by a 3rdorder tensor W(L) P RR(L\u00b41)\u02c6R(L\u00b41)\u02c6C with diagonal frontal slices W(L)(:, : , c) = diagt\u03bb(c)1 , . . . , \u03bb (c) R(L\u00b41)\nu, c = 1, . . . , C. Note that the sparse core tensors W(l,j), with diagonal frontal slices, can be represented by dense matrices defined as W(l,j) P RR(l\u00b41)\u02c6R(l) for l = 1, . . . , L\u00b4 1, and the top tensor can be also represented by a dense matrix W(L) of size R(L\u00b41) \u02c6 C, in which each column corresponds to the diagonal matrix \u03bb(c) = diagt\u03bb(c)1 , . . . , \u03bb (c) R(L\u00b41)\nu. The so simplified HT tensor network can be mathematically described\n10These tensors share the same entries, except for the parameters in the output layer.\n187\nin the following recursive form\nW(0,j) = [w(0,j)1 , . . . , w (0,j) R(0) ] P RIj\u02c6R(0)\nW(\u010f1,j) r(1)\n= R(0) \u00ff\nr(0)=1\nw(1,j) r(0),r(1)\n\u00a8 (\nw(0,2j\u00b41) r(0) \u02ddw(0,2j) r(0)\n) P RI2j\u00b41\u02c6I2j\n188\n\u00a8 \u00a8 \u00a8 (4.23)\nW(\u010fl,j) r(l)\n= R(l\u00b41) \u00ff\nr(l\u00b41)=1\nw(l,j) r(l\u00b41),r(l)\n\u00a8 (\nW(\u010fl\u00b41,2j\u00b41) r(l\u00b41) \u02ddW(\u010fl\u00b41,2j) r(l\u00b41) ) \u00a8 \u00a8 \u00a8\nW(\u010fL\u00b41,j) r(L\u00b41)\n= R(L\u00b42) \u00ff\nr(L\u00b42)=1\nw(L\u00b41,j) r(L\u00b42),r(L\u00b41)\n\u00a8 (\nW(\u010fL\u00b42,2j\u00b41) r(L\u00b42) \u02ddW(\u010fL\u00b42,2j) r(L\u00b42) ) for j = 1, . . . , 2L\u00b4l and\nWc = R(L\u00b41) \u00ff\nr(L\u00b41)=1\n\u03bb (c) r(L\u00b41)\n\u00a8 (\nW(\u010fL\u00b41,1) r(L\u00b41) \u02ddW(\u010fL\u00b41,2) r(L\u00b41)\n) P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN ,\n(4.24)\nwhere \u03bb(c) r(L\u00b41) = W(L)(r(L\u00b41), r(L\u00b41), c). In a special case, when the weights in each layer are shared, i.e., W(l,j) = W(l), @j, the above equation can be considerably simplified to\nW(\u010fl) r(l)\n= R(l\u00b41) \u00ff\nr(l\u00b41)=1\nw(l) r(l\u00b41),r(l) (W(\u010fl\u00b41) r(l\u00b41) \u02ddW(\u010fl\u00b41) r(l\u00b41) ) (4.25)\nfor the layers l = 1, . . . , L, where W(\u010fl) r(l) = W(\u010fl)(:, . . . , :, r(l)) P RI\u02c6\u00a8\u00a8\u00a8\u02c6I are sub-tensors of W(\u010fl), for each r(l) = 1, . . . , R(l), and w(l) r(l\u00b41),r(l) is the (r(l\u00b41), r(l))th entry of the weight matrix W(l) P RR(l\u00b41)\u02c6R(l) .\nHowever, it should be noted that the simplified HT model shown in Figure 4.7 has a limited ability to approximate an arbitrary grid tensor, W(hc), due to the strong constraints imposed on core tensors. A more flexible and powerful HT model is shown in Figure 4.8, in which the constraints imposed on 3rd-order cores have been completely removed. In fact this is the standard HT tensor network, which can be mathematically\n189\nexpressed as (with a slight abuse of notation), as follows\nw(\u010f0,j)r = w (0,j) r\nW(\u010fl,j)r = R(l\u00b41,2j\u00b41) \u00ff\nr1=1\nR(l\u00b41,2j) \u00ff\nr2=1\nw(l,j)r1,r2,r \u00a8 ( W(\u010fl\u00b41,2j\u00b41)r1 \u02ddW (\u010fl\u00b41,2j) r2 ) (4.26)\nW c = W (\u010fL,1),\nfor l = 1, . . . , L and j = 1, . . . , 2L\u00b4l . The HT model can be further extended by using more flexible and general Tree Tensor Networks States (TTNS). As illustrated in Figure 4.9, the use of the TTNS, instead HT tensor networks, allows for more flexibility in the choice of the size of pooling-window, as the pooling size in each hidden layer can be adjusted by applying core tensors with a suitable variable order in each layer. For example, if we use 5th-order (4rd-order)\n190\ncore tensors instead 3rd-order cores, then the pooling layer will employ a size-4 pooling window (size-3 pooling) instead of only size-2 pooling window when using 3rd-order core tensors in HT tensor networks. For more detail regrading HT networks and their generalizations to TTNS, see Part 1 of our monograph [Cichocki et al., 2016]."}, {"heading": "4.6 Convolutional Rectifier NN Using Nonlinear TNs", "text": "The convolutional arithmetic circuit (ConvACs) model employs the standard outer (tensor) products, which for two tensors, A P RI1\u02c6\u00a8\u00a8\u00a8\u02c6IN and B P RJ1\u02c6\u00a8\u00a8\u00a8\u02c6JM , are defined as\n(A \u02dd B)i1,...,iN ,j1,...,jM = ai1,...,iN \u00a8 bj1,...,jM .\nIn order to convert ConvAC tensor models to widely used convolutional rectifier networks, we need to employ the generalized (nonlinear) outer products, defined as [Cohen and Shashua, 2016]\n(A \u02dd\u03c1 B)i1,...,iN ,j1,...,jM = \u03c1(ai1,...,iN , bj1,...,jM), (4.27)\n191\nwhere the \u03c1 operator can take various forms, e.g.,\n\u03c1(a, b) = \u03c1\u03c3,P(a, b) = P[\u03c3(a), \u03c3(b)], (4.28)\nand is referred to as the activation-pooling function11, which meets the associativity and the commutativity requirements: \u03c1(\u03c1(a, b), c) = \u03c1(a, \u03c1(b, c)) and \u03c1(a, b) = \u03c1(b, a), @a, b, c P R). For two vectors, a P RJ and b P RJ , it is defined as a matrix C = a \u02dd\u03c1 b P RI\u02c6J , with entries cij = \u03c1(ai, bj). Note that the nonlinear function \u03c1 can also take special form: cij = maxtaibj, 0u or cij = maxtai, bj, 0u.\nFor a particular case of the convolutional rectifier network with max pooling, we can use the following activation-pooling operator\n\u03c1\u03c3,P(a, b) = maxt[a]+, [b]+u = maxta, b, 0u. (4.29)\nIn an analogous way, we can define the generalized Kronecker and the Khatri-Rao products.\nExample 11 Consider a generalized CP decomposition, which corresponds to a shallow rectifier network in the form [Cohen and Shashua, 2016]\nW c = R \u00ff\nr=1\n\u03bb (c) r \u00a8 (w (1) r \u02dd\u03c1 w (2) r \u02dd\u03c1 \u00a8 \u00a8 \u00a8 \u02dd\u03c1 w (N) r ), (4.30)\nwhere the coefficients \u03bb(c)r represent weights of the output layer, vectors w(n)r P RIn are weights in the hidden layer and the operator \u02dd\u03c1 denotes the nonlinear outer products defined above.\nThe generalized CP decomposition can be expressed in an equivalent vector form, as follows\nwc = vec(Wc) = [W (N) d\u03c1 W(N\u00b41) d\u03c1 \u00a8 \u00a8 \u00a8 d\u03c1 W(1)]\u03bb(c), (4.31)\nwhere d\u03c1 is the generalized Khatri-Rao product of two matrices. It should be noted that if we employ weight sharing, then all vectors w(n)r = wr, @n, and consequently the coefficient tensor, W c, must be a symmetric tensor which further limits the ability of this model to\n11The symbols \u03c3(\u00a8) and P(\u00a8) are respectively the activation and pooling functions of the network.\n192\napproximate a desired function.\nExample 12 Consider the simplified HT model as shown Figure 4.7, but with the generalized outer product defined above. Such nonlinear tensor networks can be rigorously described for I1 = I2 = \u00a8 \u00a8 \u00a8 = IN = I, as follows\nw(\u010f0,j) r(0) = w(0,j) r(0)\nW(\u010fl,j) r(l)\n= R(l\u00b41) \u00ff\nr(l\u00b41)=1\nw(l,j) r(l\u00b41),r(l)\n\u00a8 (\nW(\u010fl\u00b41,2j\u00b41) r(l\u00b41) \u02dd\u03c1 W (\u010fl\u00b41,2j) r(l\u00b41)\n) P RI\u02c6\u00a8\u00a8\u00a8\u02c6I\nWc = W (\u010fL,1),\nfor l = 1, . . . , L and j = 1, . . . , 2L\u00b4l , or in an equivalent matrix form as\nW(\u010f0,j) = W(0,j) P RI\u02c6R(0) W(\u010fl,j) = ( W(\u010fl\u00b41,2j\u00b41) d\u03c1 W(\u010fl\u00b41,2j) ) W(l,j) P RI2 l\u02c6R (l)\nvec(Wc) = ( W(\u010fL\u00b41,1) d\u03c1 W(\u010fL\u00b41,2) ) \u03bb(c) P RIN ,\n(4.32)\nfor l = 1, . . . , L\u00b4 1, where the core tensors are reduced to matrices W(l,j) P RR\n(l\u00b41)\u02c6R(l) with entries w(l,j) r(l\u00b41),r(l\u00b41),r(l)\n. We should emphasize that the HT/TTNS architectures are not the only suitable TN architectures which can be used to model DCNNs, and the whole family of powerful tensor networks can be employed for this purpose. Particularly attractive and simple are the TT/MPS, TT/MPO and TC models for DNNs, for which efficient decomposition and tensor completion algorithms already exist. The TT/MPS, TT/MPO and TC networks provide not only simplicity in comparison to HT, but also very deep TN structures, that is, with N hidden layers. Note that the standard HT model generates architectures of DCNNs with L = log2(N) hidden layers, while TT/TC networks may employ N hidden layers. Taking into account the current trend in deep leaning to use a large number of hidden layers, it would be quite attractive to employ QTT tensor networks with a relatively large number of hidden layers, L = N \u00a8 log2(I).\nTo summarize, deep convolutional neural networks may be considered as a special case of hierarchical architectures, which can be indirectly simulated and optimized via relatively simple and well understood tensor networks, especially HT, TTNS, TT/MPS, TT/MPO, and TC (i.e., using\n193\nunbalanced or balanced binary trees and graphical models). However, more sophisticated tensor network diagrams with loops, discussed in the next section may provide potentially better performance and the ability to generate novel architectures of DCNNs."}, {"heading": "4.7 MERA and 2D TNs for a Next Generation of DCNNs", "text": "The Multiscale Entanglement Renormalization Ansatz (MERA) tensor network was first introduced by Vidal [2008], and for this network numerical algorithms to minimize some specific cost functions or local Hamiltonians used in quantum physics already exist [Evenbly and Vidal, 2009]. The MERA is a relatively new tensor network, widely investigated in quantum physics based on variational Ansatz, since it is capable of capturing many of the key complex physical properties of strongly correlated ground states [Evenbly and Vidal, 2015]. The MERA also shares many relationships with the AdS/CFT (gauge-gravity) correspondence, through its complete holographic duality with the tensor networks framework. Furthermore, the MERA can be regarded as a TN realization of an orthogonal wavelet transform [Evenbly and White, 2016a,b, Matsueda, 2016].\nFor simplicity, this section focuses on 1D binary and ternary MERA architectures (see Figure 4.10(a) for basic binary MERA). Instead of writing complex mathematical formulas, it is more convenient to describe MERA tensor networks graphically, as illustrated in Figures 4.10(a), (b) and (c). Using the terminology from quantum physics, the standard binary MERA architecture contains three classes of core tensors: (i) disentanglers \u2013 4thorder cores; (ii) isometries, also called the coarse-grainers, which are typically 3rd-order cores for binary MERA and 4th-order cores for ternary MERA; and (iii) one output core which is usually a matrix or a 4th-order core. Each MERA layer is constructed of a row of disentanglers and a row of coarse-grainers or isometries. Disentanglers remove the short-scale entanglement between the adjacent modes, while isometries renormalise each pair of modes to a single mode. Each renormalisation layer performs these operations on a scale of different length.\nFrom the mapping perspective, the nodes (core tensors) can be considered as processing units, that is, the 4th-order cores map matrices to other matrices, while the coarse-grainers take matrices and map them to vectors. The key idea here is to realize that the \u201ccompression\u201d capability\n194\n195\narises from the hierarchy and the entanglement. As a matter of fact, the MERA network embodies the mutual information chain rule.\nIn other words, the main idea underlying MERA is that of disentangling the system at scales of various lengths, following coarse graining Renormalization Group (RG) flow in the system. The MERA is particularly effective for (scale invariant) critical points of physical systems. The key properties of MERA can be summarized, as follows [Evenbly and Vidal, 2015]:\n\u2022 MERA can capture scale-invariance in input data;\n\u2022 It reproduces a polynomial decay of correlations between inputs, in contrast to HT or TT networks which reproduce only exponential decay of correlations;\n\u2022 MERA has the ability to compress tensor data much better than TT/HT tensor networks;\n\u2022 It reproduces a logarithmic correction to the area law, therefore MERA is more powerful tensor network than HT/TTNS or TT/TC networks;\n\u2022 MERA can be efficiently contracted due to unitary constraints imposed on core tensors.\nMotivated by these features, we are currently investigating MERA tensor networks as powerful tools to model and analyze DCNNs. The main objective is to establish a precise connection between MERA tensor networks and extended models of DCNNs. This connection may provide exciting new insights about deep learning while also allowing for the construction of improved families of DCNNs, with potential application to more efficient data/image classification, clustering and prediction. In other words, we conjecture that the MERA and 2D TNs (see Figure 4.11) will lead to useful new results, potentially allowing not only for better characterization of expressive power of DCNNs, but also for new practical implementations. Conversely, the links and relations between TNs and DCNNs could lead to useful advances in the design of novel deep neural networks.\nThe MERA tensor networks, shown in Figure 4.10, may provide a much higher expressive power of deep learning in comparison to networks corresponding to HT/TT architectures, since this class of tensor networks can model more complex long-term correlations between input\n196\ninstances. This follows from the facts that for HT and TT tensor networks correlations between input variables decay exponentially, and the entanglement entropy saturates to a constant, while the more sophisticated MERA tensor networks provide polynomially decaying correlations.\nWe firmly believe that the insights into the theory of tensor networks and quantum many-body physics can provide better theoretical understanding of deep learning, together with the guidance for optimized DNNs design.\nTo summarize, the tensor network methodology and architectures discussed briefly in this section may be extended to allow analytic construction of new DCNNs. Moreover, systematic investigation of the correspondences between DNNs and a wide spectrum of TNs can provide a very fruitful perspective including verification of the existing conjectures and claims about operational similarities and correspondences between DNNs and TNs into a more rigorous and constructive framework.\n197\nChapter 5\nDiscussion and Conclusions Machine learning and data mining algorithms are becoming increasingly important in the analysis of large volume, multi-relational and multi-modal datasets, which are often conveniently represented as multiway arrays or tensors. The aim of this monograph has therefore been to provide the ML and data analytics communities with both a state-of-the-art review and new directions in tensor decompositions and tensor network approaches for fundamental problems of large-scale optimization. Our focus has been on supervised and unsupervised learning with tensors, tensor regression, support tensor machines, tensor canonical coloration analysis, higher-order and kernel partial least squares, and deep learning.\nIn order to demystify the concepts of tensor algebra and to provide a seamless transition from the flat view linear algebra to multi-view multilinear algebra, we have employed novel graphical representations of tensor networks to interpret mathematical expressions directly and intuitively on graphs rather than through tedious multiple-index relationships. Our main focus has been on the Tucker, Tensor Train (TT) and Hierarchical Tucker (HT) decompositions and their extensions or generalizations. To make the material self-contained, we have also addressed the important concept of tensorization and distributed representation of structured lower-order data in tensor network formats, as a tool to provide efficient higher-order representation of vectors, matrices and low-order tensors and the optimization of cost (loss) functions.\nIn order to combat the curse of dimensionality and possibly obtain a linear or even sub-linear complexity of storage and computational complexities, we have next addressed distributed representation of tensor functions through low-rank tensor networks. Finally, we have demonstrated how such approximations can be used to solve a wide class of huge-scale linear/multilinear dimensionality reduction and related\n198\noptimization problems that are far from being tractable when using classical machine learning methods.\nThe lynchpin of this work are low-rank tensor network approximations and the associated tensor contraction algorithms, and we have elucidated how these can be used to convert otherwise intractable huge-scale optimization problems into a set of much smaller linked and/or distributed sub-problems of affordable sizes and complexity. In doing so, we have highlighted the ability of tensor networks to account for the couplings between the multiple variables, and for multimodal, incomplete and/or noisy data.\nThe Part 2 finishes with a discussion of the potential of tensor networks in the design of improved and optimized deep learning neural network architectures. It is our firm conviction that the links between tensor networks and deep learning provide both exciting new insights into multi-layer neural networks and a platform for the construction of improved families of DNNs, with potential applications in highly efficient data/image classifications, clustering and prediction.\n199\nAppendices"}, {"heading": "1 Estimation of Derivatives of GCF", "text": "The GCF of the observation and its derivatives are unknown, but can be estimated from the sample first GCF\n\u03c6\u0302x(u) = 1 T\nT \u00ff\nt=1\nexp(uTxt) . (A.1)\nThe nth-order partial derivatives of \u03c6\u0302x(u) with respect to u are nth-order tensors of size I \u02c6 I \u02c6 \u00a8 \u00a8 \u00a8 \u02c6 I, given by\n\u03c8(n)x (u) = Bn\u03c6\u0302x(u) Bun = 1 T\nT \u00ff\nt=1\nexp(uTxt) (xt \u02dd xt \u02dd \u00a8 \u00a8 \u00a8 \u02dd xt) looooooooomooooooooon\nn terms xt\n= diagN( 1 T exp(uTxt))\u02c61 X\u02c62 X \u00a8 \u00a8 \u00a8 \u02c6N X.\nThe order-N derivatives of the second GCF are then given by\n\u03a8x(u) = N \u00ff\nk=1\n(\u00b41)k\u00b41 (k\u00b4 1)! \u03c6\u0302x(u)k\n\u00ff\nn1,n2,...,nk\nmn1,n2,...,nk S(\u03c8n1,n2,...,nk(u)) ,\n(A.2)\nwhere 1 \u010f n1 \u010f n2 \u010f \u00a8 \u00a8 \u00a8 \u010f nk \u010f N, n1 + n2 + \u00a8 \u00a8 \u00a8+ nk = N, \u03c8tn1,n2,...,nku(u) are Nth-order tensors constructed as an outer product of k njth-order derivative tensors \u03c8 (nj) x (u), that is\n\u03c8tn1,n2,...,nku(u) = \u03c8 (n1) x (u) \u02dd\u03c8 (n2) x (u) \u02dd \u00a8 \u00a8 \u00a8 \u02dd\u03c8 (nk) x (u). (A.3)\n200\nThe operator S(A) symmetrizes an Nth-order tensor A, i.e., it yields a symmetric tensor, defined as\nS(A) = 1 N! \u00ff\n\u03c0i\n\u03c0i(A) , (A.4)\nwhere \u03c0i runs through the list of all N! possible tensor permutations of the Nth-order tensor A. In the expression (A.2), this symmetrization can be simplified to operate only once on the resulting tensor \u03a8x(u), instead on each tensor \u03c8n1,n2,...,nk(u).\nThe number mn1,n2,...,nk represents the total number of partitions of t1, 2, . . . , Nu into k distinct parts, each having nj entries, j = 1, . . . , k. This number is given by\nmn1,n2,...,nk = N!\n\u015bk\u0304 j=1(n\u0304j!) lj lj! ,\nwhere t1 \u010f n\u03041 \u0103 n\u03042 \u0103 \u00a8 \u00a8 \u00a8 \u0103 n\u0304k\u0304 \u010f Nu denotes the set of distinct numbers of tn1, n2, . . . , nku, while 1 \u010f lj \u010f N represents the replication number of n\u0304j in tn1, n2, . . . , nku, for j = 1, . . . , k\u0304, i.e., l1n\u03041 + l2n\u03042 + \u00a8 \u00a8 \u00a8+ lk\u0304n\u0304k\u0304 = N.\nThe approximation of the derivative tensor \u03a8x(u) for some low orders N = 2, 3, . . . , 7, are given below\n\u03a8 (2) x (u) = \u03c8(2)(u) \u03c6\u0302x(u) \u00b4 \u03c8 (1)(u) \u02dd\u03c8(1)(u) \u03c6\u03022x(u) ,\n\u03a8 (3) x (u) = \u03c8(3)(u) \u03c6\u0302x(u) \u00b4 3S(\u03c8 (1)(u) \u02dd\u03c8(2)(u)) \u03c6\u03022x(u)\n+ 2 \u03c8(1)(u) \u02dd\u03c8(1)(u) \u02dd\u03c8(1)(u)\n\u03c6\u03023x(u) ,\n\u03a8 (4) x (u) = \u03c8(4)(u) \u03c6\u0302x(u) \u00b4 4S(\u03c8 (1)(u) \u02dd\u03c8(3)(u)) + 3S(\u03c8(2)(u) \u02dd\u03c8(2)(u)) \u03c6\u03022x(u)\n+ 2 6S(\u03c8(1)(u) \u02dd\u03c8(1)(u) \u02dd\u03c8(2)(u))\n\u03c6\u03023x(u)\n\u00b4 6 \u03c8 (1)(u) \u02dd\u03c8(1)(u) \u02dd\u03c8(1)(u) \u02dd\u03c8(1)(u)\n\u03c6\u03024x(u) ,\n201\n\u03a8 (5) x (u) = S ( \u03c8(5)(u) \u03c6\u0302x(u) \u00b4 5 \u03c81,4(u) + 10 \u03c82,3(u) \u03c6\u03022x(u)\n+ 2 10\u03c81,1,3(u) + 15\u03c81,2,2(u)\n\u03c6\u03023x(u) \u00b4 6\n15\u03c81,1,1,2(u)\n\u03c6\u03024x(u)\n+ 24 \u03c81,1,1,1,1(u)\n\u03c6\u03025x(u)\n) ,\n\u03a8 (6) x (u) = S ( \u03c8(6)(u) \u03c6\u0302x(u) \u00b4 6 \u03c81,5(u) + 15 \u03c82,4(u) + 10 \u03c83,3(u) \u03c6\u03022x(u)\n+ 2 15 \u03c81,1,4(u) + 60 \u03c81,2,3(u) + 15 \u03c82,2,2(u)\n\u03c6\u03023x(u)\n\u00b4 6 20 \u03c81,1,1,3(u) + 45 \u03c81,1,2,2(u)\n\u03c6\u03024x(u)\n+ 24 15 \u03c81,1,1,1,2(u)\n\u03c6\u03025x(u) \u00b4120 \u03c81,1,1,1,1,1 \u03c6\u03026x(u)\n) ,\n\u03a8 (7) x (u) = S ( \u03c8(7)(u) \u03c6\u0302x(u) \u00b4 7 \u03c81,6(u) + 21 \u03c82,5(u) + 35 \u03c83,4(u) \u03c6\u03022x(u)\n+ 2 21 \u03c81,1,5(u) + 105 \u03c81,2,4(u) + 70 \u03c81,3,3(u) + 105 \u03c82,2,3(u)\n\u03c6\u03023x(u)\n\u00b4 6 35 \u03c81,1,1,4(u) + 210 \u03c81,1,2,3(u) + 105 \u03c81,2,2,2(u)\n\u03c6\u03024x(u)\n+ 24 35 \u03c81,1,1,1,3(u) + 105 \u03c81,1,1,2,2(u)\n\u03c6\u03025x(u)\n\u00b4 120 21 \u03c81,1,1,1,1,2(u)\n\u03c6\u03026x(u) +720 \u03c81,1,1,1,1,1,1 \u03c6\u03027x(u)\n) .\nThe symmetrization in derivation of \u03a8(4)x (u) can be performed only once, as in the derivatives of orders-5, 6 and 7."}, {"heading": "2 Higher Order Cumulants", "text": "When the mixtures are centered to have zero-mean, then the first derivative \u03c8\n(1) x (0) = 1T \u0159\nt xt = 0, which leads to \u03c8tn1=1,n2,...,nku(0) being all zero for arbitrary partitions tn1 = 1, n2, . . . , nku. This simplifies the computation of the cumulant by ignoring the partitions tn1 = 1, n2, . . . , nku. For example,\n202\ncumulants of order 2, 3, . . . , 7 can be computed as\nK(2)x = \u03c8(2)(0) = 1 T I\u02c61 X\u02c62 X = 1 T XXT , K(3)x = \u03c8(3)(0) = 1 T I\u02c61 X\u02c62 X\u02c63 X , K(4)x = \u03c8(4)(0)\u00b4 3S(\u03c8(2)(0) \u02dd\u03c8(2)(0)) ,\nK(5)x = \u03c8(5)(0)\u00b4 10S(\u03c8(2)(0) \u02dd\u03c8(3)(0)) , K(6)x = S ( \u03c8(6)(0)\u00b4 15\u03c8(2)(0) \u02dd\u03c8(4)(0)\u00b4 10\u03c8(3)(0) \u02dd\u03c8(3)(0)\n+30\u03c8(2)(0) \u02dd\u03c8(2)(0) \u02dd\u03c8(2)(0) ) ,\nK(7)x = S ( \u03c8(7)(0)\u00b4 21\u03c8(2)(0) \u02dd\u03c8(5)(0)\u00b4 35\u03c8(3)(0) \u02dd\u03c8(4)(0)\n+210\u03c8(2)(0) \u02dd\u03c8(2)(0) \u02dd\u03c8(3)(0) ) ."}, {"heading": "3 Elementary Core Tensor for the Convolution Tensor", "text": "The elementary core tensor S of size N \u02c6 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2 looooooomooooooon\n(N + 1) dimensions\n\u02c6N is constructed\nfrom 2N Nth-order sparse tensors S1, S2, . . . , S2N of size 2\u02c6 2\u02c6 \u00a8 \u00a8 \u00a8 \u02c6 2, which take ones only at locations (i1, i2, . . . , iN) such that\ni\u03041 + i\u03042 + \u00a8 \u00a8 \u00a8+ i\u0304N\u00b41 + iN = # max(N \u00b4 n + 1, 0) odd n, max(N \u00b4 n + 3, 0) even n,\n(A.5)\nwhere i\u0304n = 2\u00b4 in. The structure of the tensor S based on Sn is explained, as follows\nS(1, :, . . . , :, 1, n) = S2n\u00b41 , S(1, :, . . . , :, 2, n) = S2n , (A.6)\nfor n = 1, . . . , N. The other sub-tensors S(m, :, . . . , :, n), m = 2, . . . , N, n = 1, . . . , N, are recursively defined as\nS(m, :, . . . , :, 1, n) = S(m\u00b4 1, :, . . . , :, 2, n), (A.7) S(m, :, . . . , :, 2, 1) = S(m\u00b4 1, :, . . . , :, 1, N), (A.8) S(m, :, . . . , :, 2, n) = S(m\u00b4 1, :, . . . , :, 1, n\u00b4 1). (A.9)\nAccording to definition of Sn, such tensors are orthogonal, i.e., their pair-wise inner products are zeros, and there are only (N + 1) such nonzero tensors, which are S1, . . . , SN and SN+2 for even N, and S1, . . . , SN and SN+1 for odd N; the remaining (N \u00b4 1) core tensors Sn are zero tensors.\n203"}, {"heading": "Acknowledgements", "text": "The authors wish to express their sincere gratitude to the anonymous reviewers for their constructive and helpful comments. We also appreciate the helpful comments of Claudius Hubig (Ludwig-MaximiliansUniversity, Munich) and Victor Lempitsky (Skolkovo Institute of Science and Technology, Moscow), and help with the artwork of Zhe Sun (Riken BSI). We acknowledge the insightful comments and rigorous proofreading of selected chapters by Anthony Constantinides, Ilia Kisil, Giuseppe Calvi, Sithan Kanna, Wilhelm von Rosenberg, Alex Stott, Thayne Thiannithi, and Bruno Scalzo Dees (Imperial College London).\nThis work was partially supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001), and by the EPSRC in the UK (grant EP/P008461).\n204"}], "references": [{"title": "Low-rank retractions: A survey and new results", "author": ["P.-A. Absil", "I.V. Oseledets"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Absil and Oseledets.,? \\Q2015\\E", "shortCiteRegEx": "Absil and Oseledets.", "year": 2015}, {"title": "Trust-region methods on Riemannian manifolds", "author": ["P.-A. Absil", "C.G. Baker", "K.A. Gallivan"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Absil et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2007}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2008}, {"title": "An extrinsic look at the Riemannian Hessian", "author": ["P.-A. Absil", "R. Mahony", "J. Trumpf"], "venue": "In Geometric Science of Information,", "citeRegEx": "Absil et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2013}, {"title": "Newton\u2019s method on Riemannian manifolds and a geometric model for the human spine", "author": ["R.L. Adler", "J.-P. Dedieu", "J.Y. Margulies", "M. Martens", "M. Shub"], "venue": "IMA Journal of Numerical Analysis,", "citeRegEx": "Adler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Adler et al\\.", "year": 2002}, {"title": "Sparse non-negative generalized PCA with applications to metabolomics", "author": ["G.I. Allen", "M. Maletic-Savatic"], "venue": null, "citeRegEx": "Allen and Maletic.Savatic.,? \\Q2011\\E", "shortCiteRegEx": "Allen and Maletic.Savatic.", "year": 2011}, {"title": "The N-way toolbox for MATLAB", "author": ["C.A. Andersson", "R. Bro"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Andersson and Bro.,? \\Q2000\\E", "shortCiteRegEx": "Andersson and Bro.", "year": 2000}, {"title": "Deep convolutional networks are hierarchical kernel machines", "author": ["F. Anselmi", "L. Rosasco", "C. Tan", "T. Poggio"], "venue": "arXiv preprint arXiv:1508.01084,", "citeRegEx": "Anselmi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Anselmi et al\\.", "year": 2015}, {"title": "On the approximation of functionals of very large hermitian matrices represented as matrix product operators", "author": ["M. August", "M. Ba\u00f1uls", "T. Huckle"], "venue": "CoRR, abs/1610.06086,", "citeRegEx": "August et al\\.,? \\Q2016\\E", "shortCiteRegEx": "August et al\\.", "year": 2016}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Technical report,", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Adaptive near-optimal rank tensor approximation for high-dimensional operator equations", "author": ["M. Bachmayr", "W. Dahmen"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Bachmayr and Dahmen.,? \\Q2015\\E", "shortCiteRegEx": "Bachmayr and Dahmen.", "year": 2015}, {"title": "Iterative methods based on soft thresholding of hierarchical tensors", "author": ["M. Bachmayr", "R. Schneider"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Bachmayr and Schneider.,? \\Q2016\\E", "shortCiteRegEx": "Bachmayr and Schneider.", "year": 2016}, {"title": "Tensor networks and hierarchical tensors for the solution of high-dimensional partial differential equations", "author": ["M. Bachmayr", "R. Schneider", "A. Uschmajew"], "venue": "Foundations of Computational Mathematics,", "citeRegEx": "Bachmayr et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bachmayr et al\\.", "year": 2016}, {"title": "Algorithm 862: MATLAB tensor classes for fast algorithm prototyping", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Bader and Kolda.,? \\Q2006\\E", "shortCiteRegEx": "Bader and Kolda.", "year": 2006}, {"title": "MATLAB tensor toolbox version", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Bader and Kolda.,? \\Q2015\\E", "shortCiteRegEx": "Bader and Kolda.", "year": 2015}, {"title": "A tensor-based volterra series black-box nonlinear system identification and simulation framework", "author": ["K. Batselier", "Z. Chen", "H. Liu", "N. Wong"], "venue": "In 2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),", "citeRegEx": "Batselier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Batselier et al\\.", "year": 2016}, {"title": "Tensor train alternating linear scheme for MIMO Volterra system identification", "author": ["K. Batselier", "Z. Chen", "N. Wong"], "venue": "CoRR, abs/1607.00127,", "citeRegEx": "Batselier et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Batselier et al\\.", "year": 2016}, {"title": "A reduced basis approach for calculation of the Bethe\u2013Salpeter excitation energies by using low-rank tensor factorisations", "author": ["P. Benner", "V. Khoromskaia", "B.N. Khoromskij"], "venue": "Molecular Physics,", "citeRegEx": "Benner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Benner et al\\.", "year": 2016}, {"title": "Tensor spectral clustering for partitioning higher-order network structures", "author": ["A.R. Benson", "D.F. Gleich", "J. Leskovec"], "venue": "In Proceedings of the 2015 SIAM International Conference on Data Mining,", "citeRegEx": "Benson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Benson et al\\.", "year": 2015}, {"title": "Iteration-complexity of gradient, subgradient and proximal point methods on Riemannian manifolds", "author": ["G.C. Bento", "O.P. Ferreira", "J.G. Melo"], "venue": "arXiv preprint arXiv:1609.04869,", "citeRegEx": "Bento et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bento et al\\.", "year": 2016}, {"title": "Algorithms for numerical analysis in high dimensions", "author": ["G. Beylkin", "M.J. Mohlenkamp"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Beylkin and Mohlenkamp.,? \\Q2005\\E", "shortCiteRegEx": "Beylkin and Mohlenkamp.", "year": 2005}, {"title": "Quantum machine learning", "author": ["J. Biamonte", "P. Wittek", "N. Pancotti", "P. Rebentrost", "N. Wiebe", "S. Lloyd"], "venue": "arXiv preprint arXiv:1611.09347,", "citeRegEx": "Biamonte et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Biamonte et al\\.", "year": 2016}, {"title": "A tensor approximation method based on ideal minimal residual formulations for the solution of highdimensional problems", "author": ["M. Billaud-Friess", "A. Nouy", "O. Zahm"], "venue": "ESAIM: Mathematical Modelling and Numerical Analysis,", "citeRegEx": "Billaud.Friess et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Billaud.Friess et al\\.", "year": 2014}, {"title": "Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains", "author": ["S.A. Billings"], "venue": null, "citeRegEx": "Billings.,? \\Q2013\\E", "shortCiteRegEx": "Billings.", "year": 2013}, {"title": "Toeplitz matrices, algorithms and applications", "author": ["D. Bini"], "venue": "ECRIM News Online Edition,,", "citeRegEx": "Bini.,? \\Q1995\\E", "shortCiteRegEx": "Bini.", "year": 1995}, {"title": "Linear support tensor machine: Pedestrian detection in thermal infrared images", "author": ["S.K. Biswas", "P. Milanfar"], "venue": "arXiv preprint arXiv:1609.07878,", "citeRegEx": "Biswas and Milanfar.,? \\Q2016\\E", "shortCiteRegEx": "Biswas and Milanfar.", "year": 2016}, {"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies.,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies.", "year": 2009}, {"title": "Multigrid Methods for Tensor Structured Markov Chains with Low Rank Approximation", "author": ["M. Bolten", "K. Kahl", "S. Sokolovi\u0107"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Bolten et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bolten et al\\.", "year": 2016}, {"title": "Stochastic gradient descent on Riemannian manifolds", "author": ["S. Bonnabel"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Bonnabel.,? \\Q2013\\E", "shortCiteRegEx": "Bonnabel.", "year": 2013}, {"title": "Manopt, a MATLAB toolbox for optimization on manifolds", "author": ["N. Boumal", "B. Mishra", "P.-A. Absil", "R. Sepulchre"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Boumal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Boumal et al\\.", "year": 2014}, {"title": "A tensor-based method for large-scale blind source separation using segmentation", "author": ["M. Bouss\u00e9", "O. Debals", "L. De Lathauwer"], "venue": "Technical Report Tech. Report 15-59,", "citeRegEx": "Bouss\u00e9 et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bouss\u00e9 et al\\.", "year": 2015}, {"title": "Multiway calibration. Multilinear PLS", "author": ["R. Bro"], "venue": "Journal of Chemometrics,", "citeRegEx": "Bro.,? \\Q1996\\E", "shortCiteRegEx": "Bro.", "year": 1996}, {"title": "The density matrix renormalization group for a quantum spin chain at non-zero temperature", "author": ["R.J. Bursill", "T. Xiang", "G.A. Gehring"], "venue": "Journal of Physics: Condensed Matter,", "citeRegEx": "Bursill et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bursill et al\\.", "year": 1996}, {"title": "Computing sparse representations of multidimensional signals using Kronecker bases", "author": ["C. Caiafa", "A. Cichocki"], "venue": "Neural Computaion,", "citeRegEx": "Caiafa and Cichocki.,? \\Q2013\\E", "shortCiteRegEx": "Caiafa and Cichocki.", "year": 2013}, {"title": "Stable, robust, and super\u2013fast reconstruction of tensors using multi-way projections", "author": ["C. Caiafa", "A. Cichocki"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Caiafa and Cichocki.,? \\Q2015\\E", "shortCiteRegEx": "Caiafa and Cichocki.", "year": 2015}, {"title": "Entanglement entropy and quantum field theory", "author": ["P. Calabrese", "J. Cardy"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "citeRegEx": "Calabrese and Cardy.,? \\Q2004\\E", "shortCiteRegEx": "Calabrese and Cardy.", "year": 2004}, {"title": "Robust low-rank matrix completion by Riemannian optimization", "author": ["L. Cambier", "P.-A. Absil"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Cambier and Absil.,? \\Q2016\\E", "shortCiteRegEx": "Cambier and Absil.", "year": 2016}, {"title": "Enhancing sparsity by reweighted `1 minimization", "author": ["E.J. Candes", "M.B. Wakin", "S.P. Boyd"], "venue": "Journal of Fourier Analysis and Applications,", "citeRegEx": "Candes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2008}, {"title": "A family of probabilistic kernels based on information divergence", "author": ["A.B. Chan", "N. Vasconcelos", "P.J. Moreno"], "venue": "Technical report,", "citeRegEx": "Chan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Chan et al\\.", "year": 2004}, {"title": "Generalized coherent states, reproducing kernels, and quantum support vector machines", "author": ["R. Chatterjee", "T. Yu"], "venue": "arXiv preprint arXiv:1612.03713,", "citeRegEx": "Chatterjee and Yu.,? \\Q2016\\E", "shortCiteRegEx": "Chatterjee and Yu.", "year": 2016}, {"title": "Preconditioning for accelerated iteratively reweighted least squares in structured sparsity reconstruction", "author": ["C. Chen", "J. Huang", "L. He", "H. Li"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Structured Tensors: Theory and Applications", "author": ["H. Chen"], "venue": "PhD thesis,", "citeRegEx": "Chen.,? \\Q2016\\E", "shortCiteRegEx": "Chen.", "year": 2016}, {"title": "On the equivalence of Restricted Boltzmann Machines and Tensor Network States", "author": ["J. Chen", "S. Cheng", "H. Xie", "L. Wang", "T. Xiang"], "venue": "ArXiv eprints,", "citeRegEx": "Chen et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Representations of non-linear systems: the narmax model", "author": ["S. Chen", "S.A. Billings"], "venue": "International Journal of Control,", "citeRegEx": "Chen and Billings.,? \\Q1989\\E", "shortCiteRegEx": "Chen and Billings.", "year": 1989}, {"title": "Parallelized tensor train learning of polynomial classifiers", "author": ["Z. Chen", "K. Batselier", "J.A.K. Suykens", "N. Wong"], "venue": "CoRR, abs/1612.06505,", "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Numerical methods for high-dimensional probability density function equations", "author": ["H. Cho", "D. Venturi", "G.E. Karniadakis"], "venue": "Journal of Computational Physics,", "citeRegEx": "Cho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2016}, {"title": "DFacTo: Distributed factorization of tensors", "author": ["J.H. Choi", "S. Vishwanathan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Choi and Vishwanathan.,? \\Q2014\\E", "shortCiteRegEx": "Choi and Vishwanathan.", "year": 2014}, {"title": "Sparse canonical correlation analysis: New formulation and algorithm", "author": ["D. Chu", "L.-Z. Liao", "M.K. Ng", "X. Zhang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Chu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chu et al\\.", "year": 2013}, {"title": "Tensor decompositions: New concepts in brain data analysis", "author": ["A. Cichocki"], "venue": "Journal of the Society of Instrument and Control Engineers,", "citeRegEx": "Cichocki.,? \\Q2011\\E", "shortCiteRegEx": "Cichocki.", "year": 2011}, {"title": "Era of big data processing: A new approach via tensor networks and tensor decompositions, (invited)", "author": ["A. Cichocki"], "venue": "In Proceedings of the International Workshop on Smart Info-Media Systems in Asia (SISA2013),", "citeRegEx": "Cichocki.,? \\Q2013\\E", "shortCiteRegEx": "Cichocki.", "year": 2013}, {"title": "Tensor networks for big data analytics and large-scale optimization problems", "author": ["A. Cichocki"], "venue": "arXiv preprint arXiv:1407.3124,", "citeRegEx": "Cichocki.,? \\Q2014\\E", "shortCiteRegEx": "Cichocki.", "year": 2014}, {"title": "Multilayer nonnegative matrix factorisation", "author": ["A. Cichocki", "R. Zdunek"], "venue": "Electronics Letters,", "citeRegEx": "Cichocki and Zdunek.,? \\Q2006\\E", "shortCiteRegEx": "Cichocki and Zdunek.", "year": 2006}, {"title": "Regularized alternating least squares algorithms for non-negative matrix/tensor factorization", "author": ["A. Cichocki", "R. Zdunek"], "venue": "In International Symposium on Neural Networks,", "citeRegEx": "Cichocki and Zdunek.,? \\Q2007\\E", "shortCiteRegEx": "Cichocki and Zdunek.", "year": 2007}, {"title": "Multi-layer neural networks with a local adaptive learning rule for blind separation of source signals", "author": ["A. Cichocki", "W. Kasprzak", "S. Amari"], "venue": "In Proc. Int. Symposium Nonlinear Theory and Applications (NOLTA),", "citeRegEx": "Cichocki et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 1995}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separation", "author": ["A. Cichocki", "R. Zdunek", "A.-H. Phan", "S. Amari"], "venue": null, "citeRegEx": "Cichocki et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2009}, {"title": "Generalized alpha-beta divergences and their application to rubust nonnegative matrix factorization", "author": ["A. Cichocki", "S. Cruces", "S. Amari"], "venue": "Entropy, 13:134\u2013170,", "citeRegEx": "Cichocki et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2011}, {"title": "Log-determinant divergences revisited: Alpha-beta and gamma log-det", "author": ["A. Cichocki", "S. Cruces", "S. Amari"], "venue": "divergences. Entropy,", "citeRegEx": "Cichocki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cichocki et al\\.", "year": 2015}, {"title": "Inductive bias of deep convolutional networks through pooling geometry", "author": ["N. Cohen", "A. Shashua"], "venue": "CoRR, abs/1605.06743,", "citeRegEx": "Cohen and Shashua.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Shashua.", "year": 2016}, {"title": "Convolutional rectifier networks as generalized tensor decompositions", "author": ["N. Cohen", "A. Shashua"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "Cohen and Shashua.,? \\Q2016\\E", "shortCiteRegEx": "Cohen and Shashua.", "year": 2016}, {"title": "On the expressive power of deep learning: A tensor analysis", "author": ["N. Cohen", "O. Sharir", "A. Shashua"], "venue": "In 29th Annual Conference on Learning Theory,", "citeRegEx": "Cohen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2016}, {"title": "Blind identification of under-determined mixtures based on the characteristic function", "author": ["P. Comon", "M. Rajih"], "venue": "Signal Processing,", "citeRegEx": "Comon and Rajih.,? \\Q2006\\E", "shortCiteRegEx": "Comon and Rajih.", "year": 2006}, {"title": "A Tensor-Train accelerated solver for integral equations in complex geometries", "author": ["E. Corona", "A. Rahimian", "D. Zorin"], "venue": "arXiv preprint arXiv:1511.06029,", "citeRegEx": "Corona et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Corona et al\\.", "year": 2015}, {"title": "Linear dimensionality reduction: Survey, insights, and generalizations", "author": ["J.P. Cunningham", "Z. Ghahramani"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Cunningham and Ghahramani.,? \\Q2015\\E", "shortCiteRegEx": "Cunningham and Ghahramani.", "year": 2015}, {"title": "Hierarchical Tucker tensor optimization \u2013 Applications to tensor completion", "author": ["C. Da Silva", "F.J. Herrmann"], "venue": "In Proc. 10th International Conference on Sampling Theory and Applications,", "citeRegEx": "Silva and Herrmann.,? \\Q2013\\E", "shortCiteRegEx": "Silva and Herrmann.", "year": 2013}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L. El Ghaoui", "M.I. Jordan", "G. Lanckriet"], "venue": "SIAM Review,", "citeRegEx": "d.Aspremont et al\\.,? \\Q2007\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2007}, {"title": "Stochastic and deterministic tensorization for blind signal separation", "author": ["O. Debals", "L. De Lathauwer"], "venue": "Proceedings of the 12th International Conference Latent Variable Analysis and Signal Separation,", "citeRegEx": "Debals and Lathauwer.,? \\Q2015\\E", "shortCiteRegEx": "Debals and Lathauwer.", "year": 2015}, {"title": "L\u00f6wner-based blind signal separation of rational functions with applications", "author": ["O. Debals", "M. Van Barel", "L. De Lathauwer"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Debals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Debals et al\\.", "year": 2016}, {"title": "Analytical multi-modulus algorithms based on coupled canonical polyadic decompositions", "author": ["O. Debals", "M. Sohail", "L. De Lathauwer"], "venue": "Technical report,", "citeRegEx": "Debals et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Debals et al\\.", "year": 2016}, {"title": "TT-GMRES: Solution to a linear system in the structured tensor format", "author": ["S.V. Dolgov"], "venue": "Russian Journal of Numerical Analysis and Mathematical Modelling,", "citeRegEx": "Dolgov.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov.", "year": 2013}, {"title": "Tensor Product Methods in Numerical Simulation of Highdimensional Dynamical Problems", "author": ["S.V. Dolgov"], "venue": "PhD thesis, Faculty of Mathematics and Informatics,", "citeRegEx": "Dolgov.,? \\Q2014\\E", "shortCiteRegEx": "Dolgov.", "year": 2014}, {"title": "Two-level QTT-Tucker format for optimized tensor calculus", "author": ["S.V. Dolgov", "B.N. Khoromskij"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Dolgov and Khoromskij.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov and Khoromskij.", "year": 2013}, {"title": "Simultaneous state-time approximation of the chemical master equation using tensor product formats", "author": ["S.V. Dolgov", "B.N. Khoromskij"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Dolgov and Khoromskij.,? \\Q2015\\E", "shortCiteRegEx": "Dolgov and Khoromskij.", "year": 2015}, {"title": "Alternating minimal energy methods for linear systems in higher dimensions. Part I: SPD systems", "author": ["S.V. Dolgov", "D.V. Savostyanov"], "venue": "arXiv preprint arXiv:1301.6068,", "citeRegEx": "Dolgov and Savostyanov.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov and Savostyanov.", "year": 2013}, {"title": "Alternating minimal energy methods for linear systems in higher dimensions. Part II: Faster algorithm and application to nonsymmetric systems", "author": ["S.V. Dolgov", "D.V. Savostyanov"], "venue": "arXiv preprint arXiv:1304.1222,", "citeRegEx": "Dolgov and Savostyanov.,? \\Q2013\\E", "shortCiteRegEx": "Dolgov and Savostyanov.", "year": 2013}, {"title": "Alternating minimal energy methods for linear systems in higher dimensions", "author": ["S.V. Dolgov", "D.V. Savostyanov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Dolgov and Savostyanov.,? \\Q2014\\E", "shortCiteRegEx": "Dolgov and Savostyanov.", "year": 2014}, {"title": "Computation of extreme eigenvalues in higher dimensions using block tensor train format", "author": ["S.V. Dolgov", "B.N. Khoromskij", "I.V. Oseledets", "D.V. Savostyanov"], "venue": "Computer Physics Communications,", "citeRegEx": "Dolgov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dolgov et al\\.", "year": 2014}, {"title": "Fast tensor product solvers for optimization problems with fractional differential equations as constraints", "author": ["S.V. Dolgov", "J.W. Pearson", "D.V. Savostyanov", "M. Stoll"], "venue": "Applied Mathematics and Computation,", "citeRegEx": "Dolgov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dolgov et al\\.", "year": 2016}, {"title": "Colloquium: Area laws for the entanglement entropy", "author": ["J. Eisert", "M. Cramer", "M.B. Plenio"], "venue": "Reviews of Modern Physics,", "citeRegEx": "Eisert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Eisert et al\\.", "year": 2010}, {"title": "Asymptotic behavior of `p-based Laplacian regularization in semisupervised learning", "author": ["A. El Alaoui", "X. Cheng", "A. Ramdas", "M.J. Wainwright", "M.I. Jordan"], "venue": "arXiv e-prints", "citeRegEx": "Alaoui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Alaoui et al\\.", "year": 2016}, {"title": "A new metric for probability distributions", "author": ["D.M. Endres", "J.E. Schindelin"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Endres and Schindelin.,? \\Q2003\\E", "shortCiteRegEx": "Endres and Schindelin.", "year": 2003}, {"title": "Algorithms for entanglement renormalization", "author": ["G. Evenbly", "G. Vidal"], "venue": "Physical Review B,", "citeRegEx": "Evenbly and Vidal.,? \\Q2009\\E", "shortCiteRegEx": "Evenbly and Vidal.", "year": 2009}, {"title": "Tensor network renormalization yields the multiscale entanglement renormalization Ansatz", "author": ["G. Evenbly", "G. Vidal"], "venue": "Physical Review Letters,", "citeRegEx": "Evenbly and Vidal.,? \\Q2015\\E", "shortCiteRegEx": "Evenbly and Vidal.", "year": 2015}, {"title": "Entanglement renormalization and wavelets", "author": ["G. Evenbly", "S.R. White"], "venue": "Physical Review Letters,", "citeRegEx": "Evenbly and White.,? \\Q2016\\E", "shortCiteRegEx": "Evenbly and White.", "year": 2016}, {"title": "Representation and design of wavelets using unitary circuits", "author": ["G. Evenbly", "S.R. White"], "venue": "arXiv e-prints,", "citeRegEx": "Evenbly and White.,? \\Q2016\\E", "shortCiteRegEx": "Evenbly and White.", "year": 2016}, {"title": "Nonlinear system modeling and identification using Volterra-PARAFAC models", "author": ["G. Favier", "A.Y. Kibangou", "T. Bouilloc"], "venue": "International Journal of Adaptive Control and Signal Processing,", "citeRegEx": "Favier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Favier et al\\.", "year": 2012}, {"title": "An introduction to restricted Boltzmann machines", "author": ["A. Fischer", "C. Igel"], "venue": "In Iberoamerican Congress on Pattern Recognition,", "citeRegEx": "Fischer and Igel.,? \\Q2012\\E", "shortCiteRegEx": "Fischer and Igel.", "year": 2012}, {"title": "A Mathematical Introduction to Compressive Sensing", "author": ["S. Foucart", "H. Rauhut"], "venue": null, "citeRegEx": "Foucart and Rauhut.,? \\Q2013\\E", "shortCiteRegEx": "Foucart and Rauhut.", "year": 2013}, {"title": "Sparse inverse covariance estimation with the graphical", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Lasso. Biostatistics,", "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Kernel support tensor regression", "author": ["C. Gao", "X.-J. Wu"], "venue": "Procedia Engineering,", "citeRegEx": "Gao and Wu.,? \\Q2012\\E", "shortCiteRegEx": "Gao and Wu.", "year": 2012}, {"title": "Ultimate tensorization: compressing convolutional and FC layers", "author": ["T. Garipov", "D. Podoprikhin", "A. Novikov", "D.P. Vetrov"], "venue": "alike. CoRR,", "citeRegEx": "Garipov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Garipov et al\\.", "year": 2016}, {"title": "Constrained optimization with low-rank tensors and applications to parametric problems with PDEs", "author": ["S. Garreis", "M. Ulbrich"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Garreis and Ulbrich.,? \\Q2017\\E", "shortCiteRegEx": "Garreis and Ulbrich.", "year": 2017}, {"title": "Multilinear PageRank", "author": ["D.F. Gleich", "L.-H. Lim", "Y. Yu"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Gleich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gleich et al\\.", "year": 2015}, {"title": "Deep Learning", "author": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "Goodfellow et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2016}, {"title": "Hierarchical singular value decomposition of tensors", "author": ["L. Grasedyck"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Grasedyck.,? \\Q2010\\E", "shortCiteRegEx": "Grasedyck.", "year": 2010}, {"title": "A literature survey of low-rank tensor approximation techniques", "author": ["L. Grasedyck", "D. Kessner", "C. Tobler"], "venue": "GAMM-Mitteilungen,", "citeRegEx": "Grasedyck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grasedyck et al\\.", "year": 2013}, {"title": "Variants of alternating least squares tensor completion in the tensor train format", "author": ["L. Grasedyck", "M. Kluge", "S. Kr\u00e4mer"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Grasedyck et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grasedyck et al\\.", "year": 2015}, {"title": "Quantum state tomography via compressed sensing", "author": ["D. Gross", "Y.-K. Liu", "S.T. Flammia", "S. Becker", "J. Eisert"], "venue": "Phys. Rev. Lett.,", "citeRegEx": "Gross et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gross et al\\.", "year": 2010}, {"title": "A linear support higher-order tensor machine for classification", "author": ["Z. Hao", "L. He", "B. Chen", "X. Yang"], "venue": "IEEE Transactions on Image Processing,", "citeRegEx": "Hao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hao et al\\.", "year": 2013}, {"title": "Multilinear tensor regression for longitudinal relational data", "author": ["P.D. Hoff"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "Hoff.,? \\Q2015\\E", "shortCiteRegEx": "Hoff.", "year": 2015}, {"title": "The alternating linear scheme for tensor optimization in the tensor train format", "author": ["S. Holtz", "T. Rohwedder", "R. Schneider"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Holtz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Holtz et al\\.", "year": 2012}, {"title": "On manifolds of tensors of fixed TT-rank", "author": ["S. Holtz", "T. Rohwedder", "R. Schneider"], "venue": "Numerische Mathematik,", "citeRegEx": "Holtz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Holtz et al\\.", "year": 2012}, {"title": "Matrix manifold optimization for Gaussian mixtures", "author": ["R. Hosseini", "S. Sra"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hosseini and Sra.,? \\Q2015\\E", "shortCiteRegEx": "Hosseini and Sra.", "year": 2015}, {"title": "Tensor-based Regression Models and Applications", "author": ["M. Hou"], "venue": "PhD thesis,", "citeRegEx": "Hou.,? \\Q2017\\E", "shortCiteRegEx": "Hou.", "year": 2017}, {"title": "Online local Gaussian processes for tensor-variate regression: Application to fast reconstruction of limb movements from brain signal", "author": ["M. Hou", "Y. Wang", "B. Chaib-draa"], "venue": "In Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2015}, {"title": "Common and discriminative subspace kernel-based multiblock tensor partial least squares regression", "author": ["M. Hou", "Q. Zhao", "B. Chaib-draa", "A. Cichocki"], "venue": "In Proc. of Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Hou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hou et al\\.", "year": 2016}, {"title": "The analysis of two-way functional data using two-way regularized singular value decompositions", "author": ["J.Z. Huang", "H. Shen", "A. Buja"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Huang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2009}, {"title": "A generalized Lanczos method for systematic optimization of tensor network states", "author": ["R.-Z. Huang", "H.-J. Liao", "Z.-Y. Liu", "H.-D. Xie", "Z.-Y. Xie", "H.-H. Zhao", "J. Chen", "T. Xiang"], "venue": "ArXiv e-prints,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Strictly singlesite DMRG algorithm with subspace expansion", "author": ["C. Hubig", "I.P. McCulloch", "U. Schollw\u00f6ck", "F.A. Wolf"], "venue": "Physical Review B,", "citeRegEx": "Hubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hubig et al\\.", "year": 2015}, {"title": "Generic construction of efficient matrix product operators", "author": ["C. Hubig", "I.P. McCulloch", "U. Schollw\u00f6ck"], "venue": "Phys. Rev. B,", "citeRegEx": "Hubig et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Hubig et al\\.", "year": 2017}, {"title": "Subspace iteration methods in terms of matrix product states. PAMM", "author": ["T. Huckle", "K. Waldherr"], "venue": null, "citeRegEx": "Huckle and Waldherr.,? \\Q2012\\E", "shortCiteRegEx": "Huckle and Waldherr.", "year": 2012}, {"title": "Doubly decomposing nonparametric tensor regression", "author": ["M. Imaizumi", "K. Hayashi"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "citeRegEx": "Imaizumi and Hayashi.,? \\Q2016\\E", "shortCiteRegEx": "Imaizumi and Hayashi.", "year": 2016}, {"title": "Robust multilinear principal component analysis", "author": ["K. Inoue", "K. Hara", "K. Urahama"], "venue": "In IEEE 12th International Conference on Computer Vision,", "citeRegEx": "Inoue et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Inoue et al\\.", "year": 2009}, {"title": "Best low multilinear rank approximation of higher-order tensors, based on the Riemannian trust-region scheme", "author": ["M. Ishteva", "P.A. Absil", "S. Van Huffel", "L. De Lathauwer"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Ishteva et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ishteva et al\\.", "year": 2011}, {"title": "Haten2: Billionscale tensor decompositions", "author": ["I. Jeon", "E. Papalexakis", "U. Kang", "C. Faloutsos"], "venue": "In Proceedings of the 31st IEEE International Conference on Data Engineering (ICDE),", "citeRegEx": "Jeon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2015}, {"title": "Mining billionscale tensors: Algorithms and discoveries", "author": ["I. Jeon", "E.E. Papalexakis", "C. Faloutsos", "L. Sael", "U. Kang"], "venue": "The VLDB Journal,", "citeRegEx": "Jeon et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jeon et al\\.", "year": 2016}, {"title": "A note on the use of principal components in regression", "author": ["I. Jolliffe"], "venue": "Applied Statistics,", "citeRegEx": "Jolliffe.,? \\Q1982\\E", "shortCiteRegEx": "Jolliffe.", "year": 1982}, {"title": "Tensor low-rank and sparse light field photography", "author": ["M.H. Kamal", "B. Heshmat", "R. Raskar", "P. Vandergheynst", "G. Wetzstein"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Kamal et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kamal et al\\.", "year": 2016}, {"title": "GigaTensor: Scaling tensor analysis up by 100 times \u2013 algorithms and discoveries", "author": ["U. Kang", "E.E. Papalexakis", "A. Harpale", "C. Faloutsos"], "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD", "citeRegEx": "Kang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kang et al\\.", "year": 2012}, {"title": "Uni10: An open-source library for tensor network algorithms", "author": ["Y.-J. Kao", "Y.-D. Hsieh", "P. Chen"], "venue": "In Journal of Physics: Conference Series,", "citeRegEx": "Kao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kao et al\\.", "year": 2015}, {"title": "Parallel algorithms for tensor completion in the CP format", "author": ["L. Karlsson", "D. Kressner", "A. Uschmajew"], "venue": "Parallel Computing,", "citeRegEx": "Karlsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Karlsson et al\\.", "year": 2016}, {"title": "Riemannian preconditioning for tensor completion", "author": ["H. Kasai", "B. Mishra"], "venue": "arXiv preprint arXiv:1506.02159,", "citeRegEx": "Kasai and Mishra.,? \\Q2015\\E", "shortCiteRegEx": "Kasai and Mishra.", "year": 2015}, {"title": "Multilevel Toeplitz matrices generated by tensor-structured vectors and convolution with logarithmic complexity", "author": ["V.A. Kazeev", "B.N. Khoromskij", "E.E. Tyrtyshnikov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Kazeev et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kazeev et al\\.", "year": 2013}, {"title": "Fast tensor method for summation of long-range potentials on 3D lattices with defects", "author": ["V. Khoromskaia", "B.N. Khoromskij"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Khoromskaia and Khoromskij.,? \\Q2016\\E", "shortCiteRegEx": "Khoromskaia and Khoromskij.", "year": 2016}, {"title": "O(d log N)-quantics approximation of N-d tensors in high-dimensional numerical modeling", "author": ["B.N. Khoromskij"], "venue": "Constructive Approximation,", "citeRegEx": "Khoromskij.,? \\Q2011\\E", "shortCiteRegEx": "Khoromskij.", "year": 2011}, {"title": "Tensors-structured numerical methods in scientific computing: Survey on recent advances", "author": ["B.N. Khoromskij"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Khoromskij.,? \\Q2011\\E", "shortCiteRegEx": "Khoromskij.", "year": 2011}, {"title": "Superfast wavelet transform using quanticsTT approximation. I. application to Haar wavelets", "author": ["B.N. Khoromskij", "S. Miao"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Khoromskij and Miao.,? \\Q2014\\E", "shortCiteRegEx": "Khoromskij and Miao.", "year": 2014}, {"title": "Quantics-TT collocation approximation of parameter-dependent and stochastic elliptic PDEs", "author": ["B.N. Khoromskij", "I. Oseledets"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Khoromskij and Oseledets.,? \\Q2010\\E", "shortCiteRegEx": "Khoromskij and Oseledets.", "year": 2010}, {"title": "Tensor-structured Galerkin approximation of parametric and stochastic elliptic PDEs", "author": ["B.N. Khoromskij", "C. Schwab"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Khoromskij and Schwab.,? \\Q2011\\E", "shortCiteRegEx": "Khoromskij and Schwab.", "year": 2011}, {"title": "Efficient computation of highly oscillatory integrals by using QTT tensor approximation", "author": ["B.N. Khoromskij", "A. Veit"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Khoromskij and Veit.,? \\Q2016\\E", "shortCiteRegEx": "Khoromskij and Veit.", "year": 2016}, {"title": "Desingularization of bounded-rank matrix sets", "author": ["V. Khrulkov", "I. Oseledets"], "venue": "arXiv preprint arXiv:1612.03973,", "citeRegEx": "Khrulkov and Oseledets.,? \\Q2016\\E", "shortCiteRegEx": "Khrulkov and Oseledets.", "year": 2016}, {"title": "New robust Lasso method based on ranks", "author": ["H.-J. Kim", "E. Ollila", "V. Koivunen"], "venue": "In 23rd European Signal Processing Conference (EUSIPCO),", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Canonical correlation analysis of video volume tensors for action categorization and detection", "author": ["T.K. Kim", "R. Cipolla"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kim and Cipolla.,? \\Q2009\\E", "shortCiteRegEx": "Kim and Cipolla.", "year": 2009}, {"title": "Tensor canonical correlation analysis for action classification", "author": ["T.K. Kim", "S.F. Wong", "R. Cipolla"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Dynamical low rank approximation", "author": ["O. Koch", "C. Lubich"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Koch and Lubich.,? \\Q2007\\E", "shortCiteRegEx": "Koch and Lubich.", "year": 2007}, {"title": "Dynamical tensor approximation", "author": ["O. Koch", "C. Lubich"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Koch and Lubich.,? \\Q2010\\E", "shortCiteRegEx": "Koch and Lubich.", "year": 2010}, {"title": "Pymanopt: A Python Toolbox for manifold optimization using automatic differentiation", "author": ["N. Koep", "S. Weichwald"], "venue": "arXiv preprint arXiv:1603.03236,", "citeRegEx": "Koep and Weichwald.,? \\Q2016\\E", "shortCiteRegEx": "Koep and Weichwald.", "year": 2016}, {"title": "Trace optimization and eigenproblems in dimension reduction methods", "author": ["E. Kokiopoulou", "J. Chen", "Y. Saad"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Kokiopoulou et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kokiopoulou et al\\.", "year": 2011}, {"title": "Convergence analysis of projected fixed-point iteration on a low-rank matrix manifold", "author": ["D. Kolesnikov", "I.V. Oseledets"], "venue": "arXiv preprint arXiv:1604.02111,", "citeRegEx": "Kolesnikov and Oseledets.,? \\Q2016\\E", "shortCiteRegEx": "Kolesnikov and Oseledets.", "year": 2016}, {"title": "Higher rank support tensor machines for visual recognition", "author": ["I. Kotsia", "W. Guo", "I. Patras"], "venue": "Pattern Recognition,", "citeRegEx": "Kotsia et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kotsia et al\\.", "year": 2012}, {"title": "Low-rank tensor methods for communicating Markov processes", "author": ["D. Kressner", "F. Macedo"], "venue": "In Quantitative Evaluation of Systems,", "citeRegEx": "Kressner and Macedo.,? \\Q2014\\E", "shortCiteRegEx": "Kressner and Macedo.", "year": 2014}, {"title": "Low-rank tensor Krylov subspace methods for parametrized linear systems", "author": ["D. Kressner", "C. Tobler"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Kressner and Tobler.,? \\Q2011\\E", "shortCiteRegEx": "Kressner and Tobler.", "year": 2011}, {"title": "Preconditioned low-rank methods for highdimensional elliptic PDE eigenvalue problems", "author": ["D. Kressner", "C. Tobler"], "venue": "Computational Methods in Applied Mathematics,", "citeRegEx": "Kressner and Tobler.,? \\Q2011\\E", "shortCiteRegEx": "Kressner and Tobler.", "year": 2011}, {"title": "Algorithm 941: HTucker\u2013A MATLAB toolbox for tensors in hierarchical Tucker format", "author": ["D. Kressner", "C. Tobler"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Kressner and Tobler.,? \\Q2014\\E", "shortCiteRegEx": "Kressner and Tobler.", "year": 2014}, {"title": "On low-rank approximability of solutions to high-dimensional operator equations and eigenvalue problems", "author": ["D. Kressner", "A. Uschmajew"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Kressner and Uschmajew.,? \\Q2016\\E", "shortCiteRegEx": "Kressner and Uschmajew.", "year": 2016}, {"title": "Low-rank tensor methods with subspace correction for symmetric eigenvalue problems", "author": ["D. Kressner", "M. Steinlechner", "A. Uschmajew"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Kressner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2014}, {"title": "Low-rank tensor completion by Riemannian optimization", "author": ["D. Kressner", "M. Steinlechner", "B. Vandereycken"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Kressner et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2014}, {"title": "Preconditioned lowrank Riemannian optimization for linear systems with tensor product structure", "author": ["D. Kressner", "M. Steinlechner", "B. Vandereycken"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Kressner et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kressner et al\\.", "year": 2016}, {"title": "Volterrafaces: Discriminant analysis using volterra kernels", "author": ["R. Kumar", "A. Banerjee", "B.C. Vemuri"], "venue": "In 2009 IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Estimating the parameters of exponentially damped sinusoids and pole-zero modelling in noise", "author": ["R. Kumaresan", "D.W. Tufts"], "venue": "IEEE Trans. Acoust. Speech Signal Processing,", "citeRegEx": "Kumaresan and Tufts.,? \\Q1982\\E", "shortCiteRegEx": "Kumaresan and Tufts.", "year": 1982}, {"title": "Algebraic techniques for the blind deconvolution of constant modulus signals", "author": ["L. De Lathauwer"], "venue": "In The 2004 12th European Signal Processing Conference,", "citeRegEx": "Lathauwer.,? \\Q2004\\E", "shortCiteRegEx": "Lathauwer.", "year": 2004}, {"title": "Fast convolutional neural networks using group-wise brain damage", "author": ["V. Lebedev", "V. Lempitsky"], "venue": "arXiv preprint arXiv:1506.02515,", "citeRegEx": "Lebedev and Lempitsky.,? \\Q2015\\E", "shortCiteRegEx": "Lebedev and Lempitsky.", "year": 2015}, {"title": "Tensor conjugate-gradient-type method for Rayleigh quotient minimization in block QTT-format", "author": ["O.S. Lebedeva"], "venue": "Russian Journal of Numerical Analysis and Mathematical Modelling,", "citeRegEx": "Lebedeva.,? \\Q2011\\E", "shortCiteRegEx": "Lebedeva.", "year": 2011}, {"title": "Discriminant analysis for multiway data", "author": ["G. Lechuga", "L. Le Brusquet", "V. Perlbarg", "L. Puybasset", "D. Galanaud", "A. Tenenhaus"], "venue": "Springer Proceedings in Mathematics and Statistics,", "citeRegEx": "Lechuga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lechuga et al\\.", "year": 2015}, {"title": "Biclustering via sparse singular value decomposition", "author": ["M. Lee", "H. Shen", "J.Z. Huang", "J.S. Marron"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2010}, {"title": "Estimating a few extreme singular values and vectors for large-scale matrices in Tensor Train format", "author": ["N. Lee", "A. Cichocki"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lee and Cichocki.,? \\Q2015\\E", "shortCiteRegEx": "Lee and Cichocki.", "year": 2015}, {"title": "Tensor train decompositions for higher-order regression with LASSO penalties", "author": ["N. Lee", "A. Cichocki"], "venue": "In Workshop on Tensor Decompositions and Applications (TDA2016),", "citeRegEx": "Lee and Cichocki.,? \\Q2016\\E", "shortCiteRegEx": "Lee and Cichocki.", "year": 2016}, {"title": "Regularized computation of approximate pseudoinverse of large matrices using low-rank tensor train decompositions", "author": ["N. Lee", "A. Cichocki"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lee and Cichocki.,? \\Q2016\\E", "shortCiteRegEx": "Lee and Cichocki.", "year": 2016}, {"title": "Multilinear discriminant analysis for higher-order tensor data classification", "author": ["Q. Li", "D. Schonfeld"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Li and Schonfeld.,? \\Q2014\\E", "shortCiteRegEx": "Li and Schonfeld.", "year": 2014}, {"title": "Tucker tensor regression and neuroimaging analysis", "author": ["X. Li", "H. Zhou", "L. Li"], "venue": "arXiv preprint arXiv:1304.5637,", "citeRegEx": "Li et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Li et al\\.", "year": 2013}, {"title": "Experimental realization of a quantum support vector machine", "author": ["Z. Li", "X. Liu", "N. Xu", "J. Du"], "venue": "Physical Review Letters,", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Tensor methods for parameter estimation and bifurcation analysis of stochastic reaction networks", "author": ["S. Liao", "T. Vejchodsk\u00fd", "R. Erban"], "venue": "Journal of the Royal Society Interface,", "citeRegEx": "Liao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liao et al\\.", "year": 2015}, {"title": "Why does deep and cheap learning work so well", "author": ["H.W. Lin", "M. Tegmark"], "venue": "ArXiv e-prints,", "citeRegEx": "Lin and Tegmark.,? \\Q2016\\E", "shortCiteRegEx": "Lin and Tegmark.", "year": 2016}, {"title": "A low-rank approach to the computation of path integrals", "author": ["M.S. Litsarev", "I.V. Oseledets"], "venue": "Journal of Computational Physics,", "citeRegEx": "Litsarev and Oseledets.,? \\Q2016\\E", "shortCiteRegEx": "Litsarev and Oseledets.", "year": 2016}, {"title": "Tensor completion for estimating missing values in visual data", "author": ["J. Liu", "P. Musialski", "P. Wonka", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Liu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Dynamical approximation of hierarchical Tucker and tensor-train tensors", "author": ["C. Lubich", "T. Rohwedder", "R. Schneider", "B. Vandereycken"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Lubich et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lubich et al\\.", "year": 2013}, {"title": "Time integration of tensor trains", "author": ["C. Lubich", "I.V. Oseledets", "B. Vandereycken"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "Lubich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lubich et al\\.", "year": 2015}, {"title": "A projector-splitting integrator for dynamical low-rank approximation", "author": ["C. Lubich", "I.V. Oseledets"], "venue": "BIT Numerical Mathematics,", "citeRegEx": "Lubich and Oseledets.,? \\Q2014\\E", "shortCiteRegEx": "Lubich and Oseledets.", "year": 2014}, {"title": "Support matrix machines", "author": ["L. Luo", "Y. Xie", "Z. Zhang", "W.-J. Li"], "venue": "In The International Conference on Machine Learning (ICML),", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Tensor canonical correlation analysis for multi-view dimension reduction", "author": ["Y. Luo", "D. Tao", "K. Ramamohanarao", "C. Xu", "Y. Wen"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Computing inner eigenvalues of matrices in tensor train matrix format", "author": ["T. Mach"], "venue": "In Numerical Mathematics and Advanced Applications", "citeRegEx": "Mach.,? \\Q2011\\E", "shortCiteRegEx": "Mach.", "year": 2011}, {"title": "Wave-packet dynamics within the multiconfiguration Hartree framework: General aspects and application to NOCl", "author": ["U. Manthe", "H.-D. Meyer", "L.S. Cederbaum"], "venue": "Journal of Chemical Physics,", "citeRegEx": "Manthe et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Manthe et al\\.", "year": 1992}, {"title": "Analytic optimization of a MERA network and its relevance to quantum integrability and wavelet", "author": ["H. Matsueda"], "venue": "arXiv preprint arXiv:1608.02205,", "citeRegEx": "Matsueda.,? \\Q2016\\E", "shortCiteRegEx": "Matsueda.", "year": 2016}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "author": ["H. Mhaskar", "T. Poggio"], "venue": "Analysis and Applications,", "citeRegEx": "Mhaskar and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar and Poggio.", "year": 2016}, {"title": "A Kullback-Leibler divergence based kernel for SVM classification in multimedia applications", "author": ["P.J. Moreno", "P. Ho", "N. Vasconcelos"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Moreno et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Moreno et al\\.", "year": 2003}, {"title": "Tensor-variate Restricted Boltzmann Machines", "author": ["T.D. Nguyen", "T. Tran", "D.Q. Phung", "S. Venkatesh"], "venue": "In AAAI,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Tensorizing neural networks", "author": ["A. Novikov", "D. Podoprikhin", "A. Osokin", "D.P. Vetrov"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Novikov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Novikov et al\\.", "year": 2015}, {"title": "Regularized group regression methods for genomic prediction: Bridge, MCP, SCAD, Group Bridge, Group Lasso, Sparse Group Lasso, Group MCP and Group SCAD", "author": ["J. Ogutu", "H. Piepho"], "venue": "In BMC Proceedings,", "citeRegEx": "Ogutu and Piepho.,? \\Q2014\\E", "shortCiteRegEx": "Ogutu and Piepho.", "year": 2014}, {"title": "A practical introduction to tensor networks: Matrix product states and projected entangled pair states", "author": ["R. Or\u00fas"], "venue": "Annals of Physics,", "citeRegEx": "Or\u00fas.,? \\Q2014\\E", "shortCiteRegEx": "Or\u00fas.", "year": 2014}, {"title": "Infinite time-evolving block decimation algorithm beyond unitary evolution", "author": ["R. Or\u00fas", "G. Vidal"], "venue": "Physical Review B,", "citeRegEx": "Or\u00fas and Vidal.,? \\Q2008\\E", "shortCiteRegEx": "Or\u00fas and Vidal.", "year": 2008}, {"title": "Tensor-train decomposition", "author": ["I.V. Oseledets"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets.,? \\Q2011\\E", "shortCiteRegEx": "Oseledets.", "year": 2011}, {"title": "Constructive representation of functions in low-rank tensor formats", "author": ["I.V. Oseledets"], "venue": "Constructive Approximation,", "citeRegEx": "Oseledets.,? \\Q2012\\E", "shortCiteRegEx": "Oseledets.", "year": 2012}, {"title": "Solution of linear systems and matrix inversion in the TT-format", "author": ["I.V. Oseledets", "S.V. Dolgov"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Oseledets and Dolgov.,? \\Q2012\\E", "shortCiteRegEx": "Oseledets and Dolgov.", "year": 2012}, {"title": "Tucker dimensionality reduction of three-dimensional arrays in linear time", "author": ["I.V. Oseledets", "D.V. Savostianov", "E.E. Tyrtyshnikov"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Oseledets et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Oseledets et al\\.", "year": 2008}, {"title": "URL https:// github.com/oseledets/TT-Toolbox", "author": ["I.V. Oseledets", "S.V. Dolgov", "V.A. Kazeev", "D. Savostyanov", "O. Lebedeva", "P. Zhlobich", "T. Mach", "L. Song"], "venue": "TT-Toolbox,", "citeRegEx": "Oseledets et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Oseledets et al\\.", "year": 2012}, {"title": "Thermodynamic limit of density matrix renormalization", "author": ["S. \u00d6stlund", "S. Rommer"], "venue": "Physical Review Letters,", "citeRegEx": "\u00d6stlund and Rommer.,? \\Q1995\\E", "shortCiteRegEx": "\u00d6stlund and Rommer.", "year": 1995}, {"title": "Exponential data fitting using multilinear algebra: the single-channel and multi-channel case", "author": ["J.M. Papy", "L. De Lathauwer", "S. Van Huffel"], "venue": "Numerical Linear Algebra with Applications,", "citeRegEx": "Papy et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Papy et al\\.", "year": 2005}, {"title": "Tensor networks for latent variable analysis. Part I: Algorithms for tensor train decomposition", "author": ["A.-H. Phan", "A. Cichocki", "A. Uschmajew", "P. Tichavsky", "G. Luta", "D. Mandic"], "venue": "ArXiv e-prints,", "citeRegEx": "Phan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2016}, {"title": "Tensor networks for latent variable analysis. Part 2: Blind source separation and hamonic retrieval, submitted to ArXiv e-prints, 2017", "author": ["A.-H. Phan", "A. Cichocki", "A. Uschmajew", "P. Tichavsk\u00fd", "G. Luta", "D.P. Mandic"], "venue": null, "citeRegEx": "Phan et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2017}, {"title": "Tensor decompositions for feature extraction and classification of high dimensional datasets", "author": ["A.H. Phan", "A. Cichocki"], "venue": "Nonlinear Theory and its Applications, IEICE,", "citeRegEx": "Phan and Cichocki.,? \\Q2010\\E", "shortCiteRegEx": "Phan and Cichocki.", "year": 2010}, {"title": "TENSORBOX: MATLAB package for tensor decomposition", "author": ["A.H. Phan", "P. Tichavsk\u00fd", "A. Cichocki"], "venue": "arXiv preprint arXiv:1601.01083,", "citeRegEx": "Phan et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2012}, {"title": "Variational numerical renormalization group: Bridging the gap between NRG and density matrix renormalization group", "author": ["I. Pi\u017eorn", "F. Verstraete"], "venue": "Physical Review Letters,", "citeRegEx": "Pi\u017eorn and Verstraete.,? \\Q2012\\E", "shortCiteRegEx": "Pi\u017eorn and Verstraete.", "year": 2012}, {"title": "Why and when can deep\u2013but not shallow\u2013networks avoid the curse of dimensionality: A review", "author": ["T. Poggio", "H. Mhaskar", "L. Rosasco", "B. Miranda", "Q.L. Liao"], "venue": "arXiv preprint arXiv:1611.00740,", "citeRegEx": "Poggio et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Poggio et al\\.", "year": 2016}, {"title": "Matrix variate RBM and its applications", "author": ["G. Qi", "Y. Sun", "J. Gao", "Y. Hu", "J. Li"], "venue": "arXiv preprint arXiv:1601.00722,", "citeRegEx": "Qi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Qi et al\\.", "year": 2016}, {"title": "Hankel tensors: Associated Hankel matrices and Vandermonde decomposition", "author": ["L. Qi"], "venue": "Commun Math Sci,", "citeRegEx": "Qi.,? \\Q2015\\E", "shortCiteRegEx": "Qi.", "year": 2015}, {"title": "Higher-order low-rank regression", "author": ["G. Rabusseau", "H. Kadri"], "venue": "arXiv preprint arXiv:1602.06863,", "citeRegEx": "Rabusseau and Kadri.,? \\Q2016\\E", "shortCiteRegEx": "Rabusseau and Kadri.", "year": 2016}, {"title": "Convex regularization for high-dimensional tensor regression", "author": ["G. Raskutti", "M. Yuan"], "venue": "arXiv preprint arXiv:1512.01215,", "citeRegEx": "Raskutti and Yuan.,? \\Q2015\\E", "shortCiteRegEx": "Raskutti and Yuan.", "year": 2015}, {"title": "Gaussian Processes for Machine Learning, volume 1", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen and Williams.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen and Williams.", "year": 2006}, {"title": "Low rank tensor recovery via iterative hard thresholding", "author": ["H. Rauhut", "R. Schneider", "Z. Stojanac"], "venue": "arXiv preprint arXiv:1602.05217,", "citeRegEx": "Rauhut et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rauhut et al\\.", "year": 2016}, {"title": "Quantum support vector machine for big data classification", "author": ["P. Rebentrost", "M. Mohseni", "S. Lloyd"], "venue": "Physical Review letters,", "citeRegEx": "Rebentrost et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rebentrost et al\\.", "year": 2014}, {"title": "Nested expectation propagation for Gaussian process classification with a multinomial probit likelihood", "author": ["J. Riihim\u00e4ki", "P. Jyl\u00e4nki", "A. Vehtari"], "venue": "arXiv preprint arXiv:1207.3649,", "citeRegEx": "Riihim\u00e4ki et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Riihim\u00e4ki et al\\.", "year": 2012}, {"title": "Kernel partial least squares regression in reproducing kernel Hilbert space", "author": ["R. Rosipal", "L.J. Trejo"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Rosipal and Trejo.,? \\Q2002\\E", "shortCiteRegEx": "Rosipal and Trejo.", "year": 2002}, {"title": "Deep Boltzmann Machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of The 12th International Conference on Artificial Intelligence and Statistics (AISTATS\u201909),", "citeRegEx": "Salakhutdinov and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Salakhutdinov and Hinton.", "year": 2009}, {"title": "Riemannian stochastic variance reduced gradient", "author": ["H. Sato", "H. Kasai", "B. Mishra"], "venue": "arXiv preprint arXiv:1702.05594,", "citeRegEx": "Sato et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Sato et al\\.", "year": 2017}, {"title": "Exact NMR simulation of protein-size spin systems using tensor train formalism", "author": ["D.V. Savostyanov", "S.V. Dolgov", "J.M. Werner", "I. Kuprov"], "venue": "Physical Review B,", "citeRegEx": "Savostyanov et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Savostyanov et al\\.", "year": 2014}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Deeper and cheaper machine learning [top tech 2017", "author": ["D. Schneider"], "venue": "IEEE Spectrum,", "citeRegEx": "Schneider.,? \\Q2017\\E", "shortCiteRegEx": "Schneider.", "year": 2017}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "The density-matrix renormalization group in the age of matrix product states", "author": ["U. Schollw\u00f6ck"], "venue": "Annals of Physics,", "citeRegEx": "Schollw\u00f6ck.,? \\Q2011\\E", "shortCiteRegEx": "Schollw\u00f6ck.", "year": 2011}, {"title": "Matrix product state algorithms: DMRG, TEBD and relatives", "author": ["U. Schollw\u00f6ck"], "venue": "In Strongly Correlated Systems,", "citeRegEx": "Schollw\u00f6ck.,? \\Q2013\\E", "shortCiteRegEx": "Schollw\u00f6ck.", "year": 2013}, {"title": "An introduction to quantum machine learning", "author": ["M. Schuld", "I. Sinayskiy", "F. Petruccione"], "venue": "Contemporary Physics,", "citeRegEx": "Schuld et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schuld et al\\.", "year": 2015}, {"title": "Higher-order Boltzmann machines", "author": ["T.J. Sejnowski"], "venue": "In AIP Conference Proceedings 151 on Neural Networks for Computing,", "citeRegEx": "Sejnowski.,? \\Q1987\\E", "shortCiteRegEx": "Sejnowski.", "year": 1987}, {"title": "Online learning in the manifold of low-rank matrices", "author": ["U. Shalit", "D. Weinshall", "G. Chechik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shalit et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalit et al\\.", "year": 2010}, {"title": "Tensorial mixture models", "author": ["O. Sharir", "R. Tamari", "N. Cohen", "A. Shashua"], "venue": "CoRR, abs/1610.04167,", "citeRegEx": "Sharir et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sharir et al\\.", "year": 2016}, {"title": "Higher order orthogonal iteration of tensors (HOOI) and its relation to PCA and GLRAM", "author": ["B. Sheehan", "Y. Saad"], "venue": "In Proceedings of the 2007 SIAM International Conference on Data Mining,", "citeRegEx": "Sheehan and Saad.,? \\Q2007\\E", "shortCiteRegEx": "Sheehan and Saad.", "year": 2007}, {"title": "Fully scalable methods for distributed tensor factorization", "author": ["K. Shin", "L. Sael", "U. Kang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "Shin et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shin et al\\.", "year": 2017}, {"title": "A kernel-based framework to tensorial data analysis", "author": ["M. Signoretto", "L. De Lathauwer", "J.A.K. Suykens"], "venue": "Neural Networks,", "citeRegEx": "Signoretto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2011}, {"title": "Classification of multichannel signals with cumulant-based kernels", "author": ["M. Signoretto", "E. Olivetti", "L. De Lathauwer", "J.A.K. Suykens"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Signoretto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Signoretto et al\\.", "year": 2012}, {"title": "Support vector regression machines", "author": ["A. Smola", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Smola and Vapnik.,? \\Q1997\\E", "shortCiteRegEx": "Smola and Vapnik.", "year": 1997}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "In Parallel Distributed Processing: Explorations in the Microstructure of Cognition,", "citeRegEx": "Smolensky.,? \\Q1986\\E", "shortCiteRegEx": "Smolensky.", "year": 1986}, {"title": "Riemannian Optimization for Solving High-Dimensional Problems with Low-Rank Tensor Structure", "author": ["M. Steinlechner"], "venue": "PhD thesis, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne,", "citeRegEx": "Steinlechner.,? \\Q2016\\E", "shortCiteRegEx": "Steinlechner.", "year": 2016}, {"title": "Riemannian optimization for high-dimensional tensor completion", "author": ["M. Steinlechner"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Steinlechner.,? \\Q2016\\E", "shortCiteRegEx": "Steinlechner.", "year": 2016}, {"title": "Supervised learning with quantuminspired tensor networks", "author": ["E.M. Stoudenmire", "D.J. Schwab"], "venue": "arXiv preprint arXiv:1605.05775,", "citeRegEx": "Stoudenmire and Schwab.,? \\Q2016\\E", "shortCiteRegEx": "Stoudenmire and Schwab.", "year": 2016}, {"title": "ITensor Library Release v0.2.5", "author": ["E.M. Stoudenmire", "S.R. White"], "venue": "Technical report,", "citeRegEx": "Stoudenmire and White.,? \\Q2014\\E", "shortCiteRegEx": "Stoudenmire and White.", "year": 2014}, {"title": "Sparse low-rank tensor response regression", "author": ["W.W. Sun", "L. Li"], "venue": "arXiv preprint arXiv:1609.04523,", "citeRegEx": "Sun and Li.,? \\Q2016\\E", "shortCiteRegEx": "Sun and Li.", "year": 2016}, {"title": "Heterogeneous tensor decomposition for clustering via manifold optimization", "author": ["Y. Sun", "J. Gao", "X. Hong", "B. Mishra", "B. Yin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Sun et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2016}, {"title": "Least squares support vector machine classifiers", "author": ["J.A.K. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters,", "citeRegEx": "Suykens and Vandewalle.,? \\Q1999\\E", "shortCiteRegEx": "Suykens and Vandewalle.", "year": 1999}, {"title": "Matching pursuit Lasso Part I and II: Sparse recovery over big dictionary", "author": ["M. Tan", "I. Tsang", "L. Wang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Tan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2015}, {"title": "Normalized iterative hard thresholding for matrix completion", "author": ["J. Tanner", "K. Wei"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Tanner and Wei.,? \\Q2013\\E", "shortCiteRegEx": "Tanner and Wei.", "year": 2013}, {"title": "Supervised tensor learning", "author": ["D. Tao", "X. Li", "W. Hu", "S. Maybank", "X. Wu"], "venue": "In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM\u201905),", "citeRegEx": "Tao et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tao et al\\.", "year": 2005}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Multikernel least mean squares algorithm", "author": ["F.A. Tobar", "S.-Y. Kung", "D. P Mandic"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Tobar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tobar et al\\.", "year": 2014}, {"title": "Convex tensor decomposition via structured Schatten norm regularization", "author": ["R. Tomioka", "T. Suzuki"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Tomioka and Suzuki.,? \\Q2013\\E", "shortCiteRegEx": "Tomioka and Suzuki.", "year": 2013}, {"title": "The geometry of algorithms using hierarchical tensors", "author": ["A. Uschmajew", "B. Vandereycken"], "venue": "Linear Algebra and its Applications,", "citeRegEx": "Uschmajew and Vandereycken.,? \\Q2013\\E", "shortCiteRegEx": "Uschmajew and Vandereycken.", "year": 2013}, {"title": "Statistical Learning Theory, volume 1", "author": ["V.N. Vapnik", "V. Vapnik"], "venue": null, "citeRegEx": "Vapnik and Vapnik.,? \\Q1998\\E", "shortCiteRegEx": "Vapnik and Vapnik.", "year": 1998}, {"title": "A randomized block sampling approach to Canonical Polyadic decomposition of large-scale tensors", "author": ["N. Vervliet", "L. De Lathauwer"], "venue": "IEEE Transactions on Selected Topics Signal Processing,", "citeRegEx": "Vervliet and Lathauwer.,? \\Q2016\\E", "shortCiteRegEx": "Vervliet and Lathauwer.", "year": 2016}, {"title": "Efficient classical simulation of slightly entangled quantum computations", "author": ["G. Vidal"], "venue": "Physical Review Letters,", "citeRegEx": "Vidal.,? \\Q2003\\E", "shortCiteRegEx": "Vidal.", "year": 2003}, {"title": "Class of quantum many-body states that can be efficiently simulated", "author": ["G. Vidal"], "venue": "Physical Review Letters,", "citeRegEx": "Vidal.,? \\Q2008\\E", "shortCiteRegEx": "Vidal.", "year": 2008}, {"title": "Transfer-matrix density-matrix renormalizationgroup theory for thermodynamics of one-dimensional quantum systems", "author": ["X. Wang", "T. Xiang"], "venue": "Physical Review B,", "citeRegEx": "Wang and Xiang.,? \\Q1997\\E", "shortCiteRegEx": "Wang and Xiang.", "year": 1997}, {"title": "Fast and guaranteed tensor decomposition via sketching", "author": ["Y. Wang", "H.-Y. Tung", "A. Smola", "A. Anandkumar"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Tensor displays: Compressive light field synthesis using multilayer displays with directional backlighting", "author": ["G. Wetzstein", "D. Lanman", "M. Hirsch", "R. Raskar"], "venue": "ACM Transaction on Graphics,", "citeRegEx": "Wetzstein et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wetzstein et al\\.", "year": 2012}, {"title": "Density matrix formulation for quantum renormalization groups", "author": ["S.R. White"], "venue": "Physical Review Letters,", "citeRegEx": "White.,? \\Q1992\\E", "shortCiteRegEx": "White.", "year": 1992}, {"title": "Density matrix renormalization group algorithms with a single center site", "author": ["S.R. White"], "venue": "Physical Review B,", "citeRegEx": "White.,? \\Q2005\\E", "shortCiteRegEx": "White.", "year": 2005}, {"title": "Multitask learning meets tensor factorization: Task imputation via convex optimization", "author": ["K. Wimalawarne", "M. Sugiyama", "R. Tomioka"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Wimalawarne et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2014}, {"title": "Theoretical and experimental analyses of tensor-based regression and classification", "author": ["K. Wimalawarne", "R. Tomioka", "M. Sugiyama"], "venue": "Neural Computation,", "citeRegEx": "Wimalawarne et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wimalawarne et al\\.", "year": 2016}, {"title": "A Penalized Matrix Decomposition, and its Applications", "author": ["D.M. Witten"], "venue": "PhD dissertation,", "citeRegEx": "Witten.,? \\Q2010\\E", "shortCiteRegEx": "Witten.", "year": 2010}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "R. Tibshirani", "T. Hastie"], "venue": null, "citeRegEx": "Witten et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Witten et al\\.", "year": 2009}, {"title": "General tensor spectral co-clustering for higher-order data", "author": ["T. Wu", "A.R. Benson", "D.F. Gleich"], "venue": "arXiv preprint arXiv:1603.00395,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Density-matrix renormalization", "author": ["T. Xiang", "X. Wang"], "venue": "Lecture Notes in Physics,", "citeRegEx": "Xiang and Wang.,? \\Q1999\\E", "shortCiteRegEx": "Xiang and Wang.", "year": 1999}, {"title": "Deep multi-task representation learning: A tensor factorisation approach", "author": ["Y. Yang", "T. Hospedales"], "venue": "arXiv preprint arXiv:1605.06391,", "citeRegEx": "Yang and Hospedales.,? \\Q2016\\E", "shortCiteRegEx": "Yang and Hospedales.", "year": 2016}, {"title": "Blind source separation via the second characteristic function", "author": ["A. Yeredor"], "venue": "Signal Processing,", "citeRegEx": "Yeredor.,? \\Q2000\\E", "shortCiteRegEx": "Yeredor.", "year": 2000}, {"title": "Smooth PARAFAC decomposition for tensor completion", "author": ["T. Yokota", "Q. Zhao", "A. Cichocki"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Yokota et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yokota et al\\.", "year": 2016}, {"title": "Learning from multiway data: Simple and efficient tensor regression", "author": ["R. Yu", "E.Y. Liu", "U.S.C. Edu"], "venue": "Proceedings of the 33rd International Conference on Mahince Learning (ICML),", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "Low-rank tensor constrained multiview subspace clustering", "author": ["C. Zhang", "H. Fu", "S. Liu", "G. Liu", "X. Cao"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Subspace methods with local refinements for eigenvalue computation using low-rank tensor-train format", "author": ["J. Zhang", "Z. Wen", "Y. Zhang"], "venue": "Journal of Scientific Computing,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Enabling high-dimensional hierarchical uncertainty quantification by ANOVA and tensor-train decomposition", "author": ["Z. Zhang", "X. Yang", "I.V. Oseledets", "G.E. Karniadakis", "L. Daniel"], "venue": "IEEE Transactions on ComputerAided Design of Integrated Circuits and Systems,", "citeRegEx": "Zhang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Multilinear subspace regression: An orthogonal tensor decomposition approach", "author": ["Q. Zhao", "C.F. Caiafa", "D.P. Mandic", "L. Zhang", "T. Ball", "A. Schulze-Bonhage", "A. Cichocki"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}, {"title": "Higher order partial least squares (HOPLS): A generalized multilinear regression method", "author": ["Q. Zhao", "C. Caiafa", "D.P. Mandic", "Z.C. Chao", "Y. Nagasaka", "N. Fujii", "L. Zhang", "A. Cichocki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "A tensor-variate Gaussian process for classification of multidimensional structured data", "author": ["Q. Zhao", "L. Zhang", "A. Cichocki"], "venue": "In Proceedings of the Twenty-Seven AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Kernelization of tensor-based models for multiway data analysis: Processing of multidimensional structured data", "author": ["Q. Zhao", "G. Zhou", "T. Adali", "L. Zhang", "A. Cichocki"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Zhao et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2013}, {"title": "Tensor-variate Gaussian processes regression and its application to video surveillance", "author": ["Q. Zhao", "G. Zhou", "L. Zhang", "A. Cichocki"], "venue": "In Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "Bayesian CP factorization of incomplete tensors with automatic rank determination", "author": ["Q. Zhao", "L. Zhang", "A. Cichocki"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}, {"title": "Bayesian robust tensor factorization for incomplete multiway data", "author": ["Q. Zhao", "G. Zhou", "L. Zhang", "A. Cichocki", "S.I. Amari"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Zhao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2016}, {"title": "Least squares twin support tensor machine for classification", "author": ["X.B. Zhao", "H.F. Shi", "M. Lv", "L. Jing"], "venue": "Journal of Information & Computational Science,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}, {"title": "DinTucker: Scaling up Gaussian process models on large multidimensional arrays", "author": ["S. Zhe", "Y. Qi", "Y. Park", "Z. Xu", "I. Molloy", "S. Chari"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence,", "citeRegEx": "Zhe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhe et al\\.", "year": 2016}, {"title": "Distributed flexible nonlinear tensor factorization", "author": ["S. Zhe", "P. Wang", "K.-C. Lee", "Z. Xu", "J. Yang", "Y. Park", "Y. Qi"], "venue": "arXiv preprint arXiv:1604.07928,", "citeRegEx": "Zhe et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhe et al\\.", "year": 2016}, {"title": "TDALAB: Tensor Decomposition Laboratory", "author": ["G. Zhou", "A. Cichocki"], "venue": "http://bsp.brain.riken.jp/TDALAB/,", "citeRegEx": "Zhou and Cichocki.,? \\Q2013\\E", "shortCiteRegEx": "Zhou and Cichocki.", "year": 2013}, {"title": "Tensor regression with applications in neuroimaging data analysis", "author": ["H. Zhou", "L. Li", "H. Zhu"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zhou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2013}, {"title": "Riemannian tensor completion with side information", "author": ["T. Zhou", "H. Qian", "Z. Shen", "C. Xu"], "venue": "arXiv preprint arXiv:1611.03993,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}, {"title": "Fundamental modular region, Boltzmann factor and area law in lattice theory", "author": ["D. Zwanziger"], "venue": "Nuclear Physics B,", "citeRegEx": "Zwanziger.,? \\Q1994\\E", "shortCiteRegEx": "Zwanziger.", "year": 1994}], "referenceMentions": [{"referenceID": 48, "context": "Cichocki et al. Please make reference to: A. Cichocki, A.H. Phan, Q. Zhao, N. Lee, I. Oseledets, and D.P. Mandic (2017), \u201cTensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 2 Applications and Future perspectives\u201d, Foundations and Trends in Machine Learning: Vol.", "startOffset": 0, "endOffset": 120}, {"referenceID": 185, "context": "[Papy et al., 2005] An Nth-order Hankel tensor of size I1 \u02c6 I2 \u02c6  \u0308  \u0308  \u0308 \u02c6 IN , which is represented by Y = HI1,.", "startOffset": 0, "endOffset": 19}, {"referenceID": 121, "context": "The convolution tensor can then be represented in a QTT format of rank-2 [Kazeev et al., 2013] with core tensors C(2) =  \u0308  \u0308  \u0308 = C(D) = S, C(1) = S(1, :, :, :, :), and the last core tensor C(D+1) = [ 0 1 1 0 ] which is of size 2\u02c6 1\u02c6 1\u02c6 2\u02c6 1.", "startOffset": 73, "endOffset": 94}, {"referenceID": 193, "context": "Moreover, the tensor Y also admits a symmetric CP decomposition with Vandermonde structured factor matrix [Qi, 2015] Y = diagN(\u03bb)\u02c61 V T \u02c62 VT  \u0308  \u0308  \u0308 \u02c6N VT , (1.", "startOffset": 106, "endOffset": 116}, {"referenceID": 41, "context": "12)), the Vandermonde decomposition of the Hankel tensor Y becomes a Vandermonde factorization of y [Chen, 2016], given by", "startOffset": 100, "endOffset": 112}, {"referenceID": 249, "context": "In practice, the GCF of the observation and its derivatives are unknown, but can be estimated from the sample first GCF [Yeredor, 2000].", "startOffset": 120, "endOffset": 135}, {"referenceID": 147, "context": "For this BSS problem for single constant modulus signals, Lathauwer [2004] linked the problem to CP decomposition of a fourth-order tensor.", "startOffset": 58, "endOffset": 75}, {"referenceID": 66, "context": "For multi-constant modulus signals, Debals et al. [2016b] established a link to a coupled CP decomposition.", "startOffset": 36, "endOffset": 58}, {"referenceID": 186, "context": ", an application of image denoising [Phan et al., 2016].", "startOffset": 36, "endOffset": 55}, {"referenceID": 43, "context": ", xN , the MPR can be written as a tensor-vector product as [Chen and Billings, 1989]", "startOffset": 60, "endOffset": 85}, {"referenceID": 44, "context": "For example, the weight tensor W can be constrained to be in low rank TT-format [Chen et al., 2016].", "startOffset": 80, "endOffset": 99}, {"referenceID": 221, "context": ", 2016], the TT/MPS tensor format [Stoudenmire and Schwab, 2016], or the hierarchical Tucker tensor format [Cohen and Shashua, 2016].", "startOffset": 34, "endOffset": 64}, {"referenceID": 57, "context": ", 2016], the TT/MPS tensor format [Stoudenmire and Schwab, 2016], or the hierarchical Tucker tensor format [Cohen and Shashua, 2016].", "startOffset": 107, "endOffset": 132}, {"referenceID": 23, "context": "1 Discrete Volterra Model System identification is a paradigm which aims to provide a mathematical description of a system from the observed system inputs and outputs [Billings, 2013].", "startOffset": 167, "endOffset": 183}, {"referenceID": 84, "context": "The first and simplest separable Volterra model, proposed in [Favier et al., 2012], represents the kernels by symmetric tensors of rank Rn in the CP format, that is H(n) = I\u02c61 An \u02c62 An  \u0308  \u0308  \u0308 \u02c6n An .", "startOffset": 61, "endOffset": 82}, {"referenceID": 147, "context": "3 Volterra-based Tensorization for Nonlinear Feature Extraction Consider nonlinear feature extraction in a supervised learning system, such that the extracted features maximize the Fisher score [Kumar et al., 2009].", "startOffset": 194, "endOffset": 214}, {"referenceID": 147, "context": "To this end, Kumar et al. [2009] suggested to split the data into small patches.", "startOffset": 13, "endOffset": 33}, {"referenceID": 186, "context": ", 2), that is, through the minimization [Phan et al., 2016] min }Y \u0301 X1  \u0301 X2  \u0301  \u0308  \u0308  \u0308  \u0301 XP}F .", "startOffset": 40, "endOffset": 59}, {"referenceID": 148, "context": ", singular value decomposition of the Hankel-type matrix as in the Kumaresan-Tufts (KT) method [Kumaresan and Tufts, 1982].", "startOffset": 95, "endOffset": 122}, {"referenceID": 187, "context": "For a detailed derivation of these algorithms, see [Phan et al., 2017].", "startOffset": 51, "endOffset": 70}, {"referenceID": 115, "context": ", principal component regression (PCR) [Jolliffe, 1982], whereby regression is performed on a well-posed low-dimensional subspace defined through most significant principal components.", "startOffset": 39, "endOffset": 55}, {"referenceID": 217, "context": "A well established and important supervised learning technique is linear or nonlinear Support Vector Regression (SVR) [Smola and Vapnik, 1997], which allows for the modeling of streaming data and is quite closely related to Support Vector Machines (SVM) [Cortes and Vapnik, 1995].", "startOffset": 118, "endOffset": 142}, {"referenceID": 228, "context": "Standard support vector regression techniques have been naturally extended to Tensor Regression (TR) or Support Tensor Machine (STM) methods [Tao et al., 2005].", "startOffset": 141, "endOffset": 159}, {"referenceID": 266, "context": "5) where \u201c \u030b\u201d denotes the outer product of vectors, leads to a generalized linear model (GLM), called the CP tensor regression [Zhou et al., 2013].", "startOffset": 127, "endOffset": 146}, {"referenceID": 98, "context": "6) we obtain Tucker tensor regression [Hoff, 2015, Li et al., 2013, Yu et al., 2016]. An alternative form of the multilinear Tucker regression model, proposed by Hoff [2015], assumes that the replicated observations tXm, Ymu m=1 are stacked in concatenated tensors X P RI1\u02c6 \u0308 \u0308 \u0308\u02c6IN\u02c6M and Y P RJ1\u02c6 \u0308 \u0308 \u0308\u02c6JN\u02c6M, which admit the following model Y = X\u02c61 W1 \u02c62 W2  \u0308  \u0308  \u0308 \u02c6N WN \u02c6N+1 DM + E, (2.", "startOffset": 39, "endOffset": 174}, {"referenceID": 102, "context": "2) is represented by a low-rank HT decomposition, this is referred to as the H-Tucker tensor regression [Hou, 2017].", "startOffset": 104, "endOffset": 115}, {"referenceID": 195, "context": "8), including the multi-response regression, vector autoregressive model and pair-wise interaction tensor model (see [Raskutti and Yuan, 2015] and references therein).", "startOffset": 117, "endOffset": 142}, {"referenceID": 111, "context": ", 2015], multilinear robust principal component analysis [Inoue et al., 2009], and subspace clustering [Zhang et al.", "startOffset": 57, "endOffset": 77}, {"referenceID": 251, "context": "properties of data itself, low-rank regularization can be also applied to learning coefficients in regression and classification [Yu et al., 2016].", "startOffset": 129, "endOffset": 146}, {"referenceID": 243, "context": "The low-rank constraint for the tensor W can also be formulated through the tensor norm, in the form [Wimalawarne et al., 2016] min W,b (J(X, y | W, b) + \u03bb}W}), (2.", "startOffset": 101, "endOffset": 127}, {"referenceID": 163, "context": "One of the important and useful tensor norms is the tensor nuclear norm [Liu et al., 2013] or the (overlapped) trace norm [Wimalawarne et al.", "startOffset": 72, "endOffset": 90}, {"referenceID": 242, "context": ", 2013] or the (overlapped) trace norm [Wimalawarne et al., 2014], which can be defined for a tensor W P RI1\u02c6 \u0308 \u0308 \u0308\u02c6IN as }W} \u030a = N", "startOffset": 39, "endOffset": 65}, {"referenceID": 231, "context": "Recently Tomioka and Suzuki [2013] proposed the latent trace norm of a tensor, which takes a mixture of N latent tensors, W(n), and regularizes each of them separately, as in (2.", "startOffset": 9, "endOffset": 35}, {"referenceID": 242, "context": "To deal with this problem, the scaled latent norm was proposed by Wimalawarne et al. [2014] which is defined as", "startOffset": 66, "endOffset": 92}, {"referenceID": 31, "context": "2 The N-way PLS Method The multi-way PLS (called N-way PLS) proposed by Bro [1996] is a simple extension of the standard PLS.", "startOffset": 72, "endOffset": 83}, {"referenceID": 200, "context": ", \u03a6(1)\u03a6 T (1)\u03a8(1)\u03a8 T (1)tr = \u03bbtr and ur = \u03a8(1)\u03a8 T (1)tr [Rosipal and Trejo, 2002].", "startOffset": 56, "endOffset": 81}, {"referenceID": 206, "context": "7 Kernel Functions in Tensor Learning Kernel functions can be considered as a means for defining a new topology which implies a priori knowledge about the invariance in the input space [Sch\u00f6lkopf and Smola, 2002] (see Figure 2.", "startOffset": 185, "endOffset": 212}, {"referenceID": 230, "context": "Kernel algorithms can also be used for estimation of vector-valued nonlinear and nonstationary signals [Tobar et al., 2014].", "startOffset": 103, "endOffset": 123}, {"referenceID": 215, "context": "One similarity measure between matrices is the so called Chordal distance, which is a projection of the Frobenius norm on a Grassmannian manifolds [Signoretto et al., 2011].", "startOffset": 147, "endOffset": 172}, {"referenceID": 215, "context": "It should be emphasized that such a tensor kernel ensures (provides) rotation and reflection invariance for elements on the Grassmann manifold [Signoretto et al., 2011].", "startOffset": 143, "endOffset": 168}, {"referenceID": 173, "context": "One simple and very useful information divergence is the standard symmetric Kullback-Leibler (sKL) divergence [Moreno et al., 2003], expressed as", "startOffset": 110, "endOffset": 131}, {"referenceID": 207, "context": "It should be emphasized that such a tensor kernel ensures (provides) rotation and reflection invariance for elements on the Grassmann manifold [Signoretto et al., 2011]. Zhao et al. [2013c] proposed a whole family of probabilistic product kernels based on generative models.", "startOffset": 144, "endOffset": 190}, {"referenceID": 196, "context": "8 Tensor Variate Gaussian Processes (TVGP) Gaussian processes (GP) can be considered as a class of probabilistic models which specify a distribution over a function space, where the inference is performed directly in the function space [Rasmussen and Williams, 2006].", "startOffset": 236, "endOffset": 266}, {"referenceID": 253, "context": ", M, was investigated by Zhao et al. [2013b]. All class labels are collected in the M\u02c6 1 target vector y, and all tensors are concatenated in an (N + 1)th-order tensor X of size M\u02c6 I1\u02c6 \u0308  \u0308  \u0308\u02c6 IN .", "startOffset": 25, "endOffset": 45}, {"referenceID": 110, "context": "The additive multiplicative nonparametric regression (AMNR) model constructs f as the sum of local functions which take the components of a rank-one tensor as inputs [Imaizumi and Hayashi, 2016].", "startOffset": 166, "endOffset": 194}, {"referenceID": 233, "context": "Remark 4: According to the statistical learning theory [Vapnik and Vapnik, 1998], SVM-based learning performs well when the number of training measurements is larger than the complexity of the model.", "startOffset": 55, "endOffset": 80}, {"referenceID": 97, "context": "problem can be formulated, as follows [Hao et al., 2013]:", "startOffset": 38, "endOffset": 56}, {"referenceID": 97, "context": "Then, the inner product of Xi and Xj is given by Hao et al. [2013] xXi, Xjy \u00ab R", "startOffset": 49, "endOffset": 67}, {"referenceID": 138, "context": "10 Higher Rank Support Tensor Machines (HRSTM) Higher Rank STMs (HRSTM) aim to estimate a set of parameters in the form of the sum of rank-one tensors [Kotsia et al., 2012], which defines a", "startOffset": 151, "endOffset": 172}, {"referenceID": 88, "context": "11 Kernel Support Tensor Machines The class of support tensor machines has been recently extended to the nonlinear case by using the kernel framework, and is referred to as kernel support tensor regression (KSTR) [Gao and Wu, 2012].", "startOffset": 213, "endOffset": 231}, {"referenceID": 152, "context": "91) The regularized Multiway Fisher Discriminant Analysis (MFDA) [Lechuga et al., 2015] aims to impose the structural constraints in such a way that the weight vector w will be decomposed as w = wN b  \u0308  \u0308  \u0308 bw1.", "startOffset": 65, "endOffset": 87}, {"referenceID": 188, "context": "The Linear Discriminant Analysis (LDA) can also be extended to tensor data, which is referred to as Higher Order Discriminant Analysis (HODA) [Phan and Cichocki, 2010] and Multilinear Discriminant Analysis (MDA) [Li and Schonfeld, 2014].", "startOffset": 142, "endOffset": 167}, {"referenceID": 157, "context": "The Linear Discriminant Analysis (LDA) can also be extended to tensor data, which is referred to as Higher Order Discriminant Analysis (HODA) [Phan and Cichocki, 2010] and Multilinear Discriminant Analysis (MDA) [Li and Schonfeld, 2014].", "startOffset": 212, "endOffset": 236}, {"referenceID": 154, "context": ", 2016], SVD [Lee and Cichocki, 2015], solutions of overdetermined and undetermined systems of linear algebraic equations [Dolgov and Savostyanov, 2014, Oseledets and Dolgov, 2012], the Moore\u2013 Penrose pseudo-inverse of structured matrices [Lee and Cichocki, 2016b], and LASSO regression problems [Lee and Cichocki, 2016a].", "startOffset": 13, "endOffset": 37}, {"referenceID": 184, "context": "In [\u00d6stlund and Rommer, 1995] pointed out that the wave function generated by the DMRG iteration is a matrix product state.", "startOffset": 3, "endOffset": 29}, {"referenceID": 239, "context": "2 Extraction of Two Neighboring Cores for Modified ALS (MALS) The Modified ALS algorithm, also called the two-site Density Matrix Renormalization Group (DMRG2) algorithm1 requires the extraction of two 1The DMRG algorithm was first proposed by White [1992]. At that time, people did not know the relation between tensor network and the DMRG.", "startOffset": 244, "endOffset": 257}, {"referenceID": 177, "context": "As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Or\u00fas [2014], Orus and Vidal [2008], Vidal [2003].", "startOffset": 176, "endOffset": 188}, {"referenceID": 177, "context": "As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Or\u00fas [2014], Orus and Vidal [2008], Vidal [2003].", "startOffset": 176, "endOffset": 211}, {"referenceID": 177, "context": "As a matter of fact, quantum physicists use rather the time-evolving block decimation (TEBD) algorithm in order to optimize matrix product states (TT/MPS) (for more detail see Or\u00fas [2014], Orus and Vidal [2008], Vidal [2003].)", "startOffset": 176, "endOffset": 225}, {"referenceID": 74, "context": "Algorithm 10: One full sweep of the ALS algorithm for symmetric EVD [Dolgov and Savostyanov, 2014] Input: A symmetric matrix A P RI\u02c6I , and initial guesses for X P RI1 I2 \u0308 \u0308 \u0308IN\u02c6K in block-1 TT format and with right orthogonal cores X(2), .", "startOffset": 68, "endOffset": 98}, {"referenceID": 74, "context": "The Alternating Minimal Energy (AMEn) algorithm aims to avoid the problem of convergence to non-global minimum by exploiting the information about the gradient of a cost function or information about the value of a current residual by \u201cenriching\u201d the TT cores with additional information during the iteration process [Dolgov and Savostyanov, 2014].", "startOffset": 317, "endOffset": 347}, {"referenceID": 76, "context": "Such an enrichment was efficiently implemented first by Dolgov and Savostyanov to solve symmetric as well as nonsymmetric linear systems [Dolgov et al., 2016] and was later extended to symmetric EVD in [Kressner et al.", "startOffset": 137, "endOffset": 158}, {"referenceID": 107, "context": ", 2014a] and [Hubig et al., 2015].", "startOffset": 13, "endOffset": 33}, {"referenceID": 74, "context": "The concept was proposed by White [2005] as a corrected one-side DMRG1 method, while [Dolgov and Savostyanov, 2014] proposed a significantly improved AMEn algorithm for solving large scale systems of linear equations together with theoretical convergence analysis.", "startOffset": 85, "endOffset": 115}, {"referenceID": 107, "context": ", 2014a] and [Hubig et al., 2015] developed AMEn type methods for solving large scale eigenvalue problems.", "startOffset": 13, "endOffset": 33}, {"referenceID": 68, "context": "The Alternating Minimal Energy (AMEn) algorithm aims to avoid the problem of convergence to non-global minimum by exploiting the information about the gradient of a cost function or information about the value of a current residual by \u201cenriching\u201d the TT cores with additional information during the iteration process [Dolgov and Savostyanov, 2014]. In that sense, the AMEn can be considered as a subspace correction technique. Such an enrichment was efficiently implemented first by Dolgov and Savostyanov to solve symmetric as well as nonsymmetric linear systems [Dolgov et al., 2016] and was later extended to symmetric EVD in [Kressner et al., 2014a] and [Hubig et al., 2015]. Similar to the ALS method, at each iteration step, the AMEn algorithm updates a single core tensor (see Figure 3.11). Then, it concatenates the updated TT core X(n) with a core tensor Z(n) obtained from the residual vector. At each micro-iteration, only one core tensor of the solution is enriched by the core tensor computed based on the gradient vector. By concatenating two core tensors X(n) and Z(n), the AMEn can achieve global convergence, while maintaining the computational and storage complexities as low as those of the standard ALS [Dolgov and Khoromskij, 2015, Dolgov and Savostyanov, 2014]. The concatenation step of the AMEn is also called the (core) enrichment, the basis expansion, or the local subspace correction. The concept was proposed by White [2005] as a corrected one-side DMRG1 method, while [Dolgov and Savostyanov, 2014] proposed a significantly improved AMEn algorithm for solving large scale systems of linear equations together with theoretical convergence analysis.", "startOffset": 318, "endOffset": 1453}, {"referenceID": 2, "context": "This corresponds to an orthogonal projection of the gradient of J(X) = tr(XTAX) onto the tangent space of the Stiefel manifold [Absil et al., 2008].", "startOffset": 127, "endOffset": 147}, {"referenceID": 107, "context": "Alternatively, [Hubig et al., 2015] developed a more efficient algorithm, whereby instead of using the exact residual AX  \u0301 X\u039b to compute the enrichment term, which is computationally expensive, they show that it is sufficient to exploit only the AX term.", "startOffset": 15, "endOffset": 35}, {"referenceID": 107, "context": "39) for eigenvalue problems, with K = 1 eigenvalue [Hubig et al., 2015].", "startOffset": 51, "endOffset": 71}, {"referenceID": 74, "context": "Furthermore, it has been found that even a rough approximation to the residual / gradient for the enrichment Z(n) will lead to a faster convergence of the algorithm [Dolgov and Savostyanov, 2014].", "startOffset": 165, "endOffset": 195}, {"referenceID": 154, "context": "12 (see [Lee and Cichocki, 2015] for detail and computer simulation experiments).", "startOffset": 8, "endOffset": 32}, {"referenceID": 246, "context": "For example, in spectral co-clustering, only one eigenvector (called Fiedler eigenvector) needs to be computed, which corresponds to the second smallest generalized eigenvalue [Wu et al., 2016].", "startOffset": 176, "endOffset": 193}, {"referenceID": 2, "context": "52) into the trace operator, to give the following optimization problem [Absil et al., 2008] min VPRI\u02c6K tr(VTAV(VTBV) \u03011).", "startOffset": 72, "endOffset": 92}, {"referenceID": 62, "context": "Also, by a change in the variables, W = B1/2V, the GEVD reduces to the standard symmetric EVD [Cunningham and Ghahramani, 2015] min WPRI\u02c6K tr(WTB \u03011/2AB \u03011/2W), s.", "startOffset": 94, "endOffset": 127}, {"referenceID": 62, "context": "An alternative approach is to use the orthogonal CCA model, which can be formulated as [Cunningham and Ghahramani, 2015]", "startOffset": 87, "endOffset": 120}, {"referenceID": 74, "context": "4 for the EVD problem, has been developed first historically as an efficient solution to large-scale least squares problems [Dolgov and Savostyanov, 2014].", "startOffset": 124, "endOffset": 154}, {"referenceID": 74, "context": "Next, for building an enrichment, Z(n) P RRn \u03011\u02c6Jn\u02c6Qn , [Dolgov and Savostyanov, 2014] considered an approximation to the partially projected gradient (X\u0103n bL IJn \u0308 \u0308 \u0308JN ) T(ATAx \u0301ATb) \u2013 vec ( xxZ(n), R(n+1), .", "startOffset": 56, "endOffset": 86}, {"referenceID": 74, "context": "Algorithm 12: AMEn for linear systems Ax \u2013 b [Dolgov and Savostyanov, 2014] Input: Matrix A P RI\u02c6J , with I \u011b J, vector b P RI , initial guesses for x P RJ and residual pr = AT(Ax \u0301 b) P RJ in the TT format Output: Approximate solution x in the TT format X = xxX(1), X(2), .", "startOffset": 45, "endOffset": 75}, {"referenceID": 40, "context": "When the sparsity structures are overlapping, the problem can be reformulated as the Group LASSO problem [Chen et al., 2014] min x }Ax \u0301 b}2 + \u03b3}G\u03a6x}2,1, (3.", "startOffset": 105, "endOffset": 124}, {"referenceID": 176, "context": "Other generalizations of the standard LASSO include the Block LASSO, Fused LASSO, Elastic Net and Bridge regression algorithms [Ogutu and Piepho, 2014].", "startOffset": 127, "endOffset": 151}, {"referenceID": 37, "context": "A simple approach in this direction would be to apply Iteratively Reweighted Least Squares (IRLS) methods10 [Candes et al., 2008], whereby the `1-norm is replaced by the reweighted `2-norm (see Figure 3.", "startOffset": 108, "endOffset": 129}, {"referenceID": 37, "context": "93) where the diagonal elements are wj = [|xj| + \u03b52]q/2 \u03011, and \u03b5 \u0105 0 is a very small number needed to avoid divergence for a small xj [Candes et al., 2008].", "startOffset": 135, "endOffset": 156}, {"referenceID": 40, "context": "Similarly, for the non-overlapping group LASSO [Chen et al., 2014] }x}q,1 = \u00ff", "startOffset": 47, "endOffset": 66}, {"referenceID": 2, "context": "\u2022 \u2207J(xk)9Axk  \u0301 xk\u03bbk with \u03bbk = xk Axk, for the Rayleigh quotient J(x) = xTAx with }x}2 = 1, which is an orthogonal projection of the gradient of J onto the tangent space of a sphere, or a Stiefel manifold in general [Absil et al., 2008].", "startOffset": 216, "endOffset": 236}, {"referenceID": 12, "context": "a soft thresholding scheme [Bachmayr et al., 2016].", "startOffset": 27, "endOffset": 50}, {"referenceID": 22, "context": ", Richardson, CG GMRES) and combined with CP format [Beylkin and Mohlenkamp, 2005, Khoromskij and Schwab, 2011], CP and Tucker formats [Billaud-Friess et al., 2014], TT format [Dolgov, 2013, Khoromskij and Oseledets, 2010], HT format [Bachmayr and Dahmen, 2015, Bachmayr and Schneider, 2016, Kressner and Tobler, 2011a], and a subspace projection method combined with HT format [Ballani and Grasedyck, 2013]; \u2022 A subspace projection method combined with HT format [Ballani and Grasedyck, 2013]; \u2022 Computing extreme eigenvalues and corresponding eigenvectors by Lanczos method combined with TT format [Huang et al.", "startOffset": 135, "endOffset": 164}, {"referenceID": 151, "context": ", 2016, Huckle and Waldherr, 2012], preconditioned inverse iteration with TT format [Mach, 2013], block CG method (LOBPCG) combined with TT format [Lebedeva, 2011] and with HT format [Kressner and Tobler, 2011b]; \u2022 Iterative hard thresholding for low-rank matrix/tensor completion [Foucart and Rauhut, 2013, Tanner and Wei, 2013].", "startOffset": 147, "endOffset": 163}, {"referenceID": 12, "context": "Moreover, theoretical global convergence properties hold under some restricted conditions (see [Bachmayr et al., 2016] for more detail).", "startOffset": 95, "endOffset": 118}, {"referenceID": 26, "context": "In addition, due to the analogy between low-rank truncation and sparse signal estimation techniques, truncated iteration-type algorithms are also suitable for largescale compressed sensing [Blumensath and Davies, 2009].", "startOffset": 189, "endOffset": 218}, {"referenceID": 68, "context": "For example, in the GMRES method [Dolgov, 2013], even if the solution vector and the right-hand side vector are well approximated by the TT format, the residuals and Krylov vectors involved in intermediate iterations usually have high TT ranks.", "startOffset": 33, "endOffset": 47}, {"referenceID": 232, "context": "For example, [Uschmajew and Vandereycken, 2013] developed the manifold structure for the HT tensors, while Lubich et al.", "startOffset": 13, "endOffset": 47}, {"referenceID": 98, "context": "If the inner product of two tensors induces a Euclidean metric on the embedding space RI1\u02c6 \u0308 \u0308 \u0308\u02c6IN , then the above submanifold Mr is a Riemannian manifold [Holtz et al., 2012b, Steinlechner, 2016a, Uschmajew and Vandereycken, 2013]. Similar results are also available for the more general hierarchical Tucker models. For example, [Uschmajew and Vandereycken, 2013] developed the manifold structure for the HT tensors, while Lubich et al. [2013] developed the concept of dynamical low-rank approximation for both HT and TT formats.", "startOffset": 158, "endOffset": 447}, {"referenceID": 2, "context": "The fundamental and basic algorithms for Riemannian optimization [Absil et al., 2008] are quite attractive, but are still not widely used as they are more technically complicated than, e.", "startOffset": 65, "endOffset": 85}, {"referenceID": 29, "context": "The MANOPT package [Boumal et al., 2014] (for a Python version see [Koep and Weichwald, 2016]) provides a useful interface for many standard matrix Riemannian optimization techniques, however, extensions for tensors (especially in high dimensions) are not easy and should be performed very carefully.", "startOffset": 19, "endOffset": 40}, {"referenceID": 135, "context": ", 2014] (for a Python version see [Koep and Weichwald, 2016]) provides a useful interface for many standard matrix Riemannian optimization techniques, however, extensions for tensors (especially in high dimensions) are not easy and should be performed very carefully.", "startOffset": 34, "endOffset": 60}, {"referenceID": 146, "context": "105) and [Kressner et al., 2016].", "startOffset": 9, "endOffset": 32}, {"referenceID": 4, "context": "Definition 1 [Adler et al., 2002] A mapping RX is called the retraction, if 1.", "startOffset": 13, "endOffset": 33}, {"referenceID": 0, "context": "For low-rank matrices and tensors with fixed TT-ranks, the simplest retraction is provided by respective SVD and TT-SVD algorithms, however, there are many other types of retractions; for more detail we refer to the survey [Absil and Oseledets, 2015].", "startOffset": 223, "endOffset": 250}, {"referenceID": 2, "context": "[2016] the Polak-Ribiere update formula [Nocedal and Wright, 2006] can be adapted to Riemannian optimization [Absil et al., 2008].", "startOffset": 109, "endOffset": 129}, {"referenceID": 141, "context": "To this end, as suggested in Kressner et al. [2016] the Polak-Ribiere update formula [Nocedal and Wright, 2006] can be adapted to Riemannian optimization [Absil et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 2, "context": "The local search direction, \u03bek, is determined from the correction equation [Absil et al., 2008] as HXk \u03bek = \u0301PTXkMr\u2207J(Xk), (3.", "startOffset": 75, "endOffset": 95}, {"referenceID": 1, "context": "Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al.", "startOffset": 162, "endOffset": 182}, {"referenceID": 1, "context": "Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al.", "startOffset": 162, "endOffset": 207}, {"referenceID": 1, "context": "Recently, in order to achieve a better global convergence, an alternative version of the Riemannian Newton method called the Trust-Region scheme was developed by Absil et al. [2007], Boumal and Absil [2011], Ishteva et al. [2011]. Retraction and vector transport are critical operations to the success of sophisticated Riemannian optimization algorithms, such as Riemannian CG, Newton and/or quasi-Newton methods.", "startOffset": 162, "endOffset": 230}, {"referenceID": 170, "context": "Indeed, some fundamental results can be found in the quantum molecular dynamics community which has used the so-called Multi Configurational Time-Dependent Hartree (MCTDH) method already since the 1990s [Manthe et al., 1992].", "startOffset": 203, "endOffset": 224}, {"referenceID": 133, "context": "mathematics by Koch and Lubich [2007], its roots are in quantum mechanics and can be traced back to the so-called Dirac-Frenkel principle.", "startOffset": 15, "endOffset": 38}, {"referenceID": 134, "context": "The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form Y(t) = U(t)S(t)VT(t).", "startOffset": 95, "endOffset": 118}, {"referenceID": 166, "context": "This leads to a much better convergence, and moreover an exactness result [Lubich and Oseledets, 2014] can be proven if A1 and A0 are on the manifold, and Y0 = A0, then this scheme is exact.", "startOffset": 74, "endOffset": 102}, {"referenceID": 0, "context": "Through the link with the Riemannian optimization, the projectorsplitting scheme can be also used here, which can be viewed as a secondorder retraction [Absil and Oseledets, 2015].", "startOffset": 152, "endOffset": 179}, {"referenceID": 132, "context": "The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form Y(t) = U(t)S(t)VT(t).", "startOffset": 25, "endOffset": 48}, {"referenceID": 132, "context": "The original approach of Koch and Lubich [2007] (later generalized to the Tucker and TT models [Koch and Lubich, 2010]) is to write down ordinary differential equations for the parameters U(t), S(t), V(t) of the SVD like decomposition in the form Y(t) = U(t)S(t)VT(t). The second approach is also straightforward: apply any time integration scheme to the equation (3.116). In this case, a standard method will yield the solution which is not on the manifold, and a retraction would be needed. In Lubich and Oseledets [2014] a simple and efficient solution to this problem, referred to as the projector-splitting scheme, was proposed, based on the special structure of the manifold.", "startOffset": 25, "endOffset": 524}, {"referenceID": 164, "context": "The generalization to the TT network was proposed by Lubich et al. [2015], and can be implemented within the framework of sweeping algorithms, allowing for the efficient TT-approximation of dynamical systems and solution of optimization problems with non-quadratic functionals.", "startOffset": 53, "endOffset": 74}, {"referenceID": 2, "context": "Local convergence results follow from the general theory [Absil et al., 2008], however the important problem of the curvature and singular points is not yet fully addressed.", "startOffset": 57, "endOffset": 77}, {"referenceID": 129, "context": "One way forward is to look for the desingularization [Khrulkov and Oseledets, 2016], another technique is to employ the concept of tangent bundle.", "startOffset": 53, "endOffset": 83}, {"referenceID": 137, "context": "An attempt to study the global convergence was presented in [Kolesnikov and Oseledets, 2016], and even in this case, convergence to the spurious local minima is possible in a carefully designed example.", "startOffset": 60, "endOffset": 92}, {"referenceID": 137, "context": "This was observed experimentally in [Kolesnikov and Oseledets, 2016] in the form of a \u201cstaircase\u201d convergence.", "startOffset": 36, "endOffset": 68}, {"referenceID": 174, "context": "To deal with the exponentially large number of parameters, Novikov et al. [2016], Stoudenmire and Schwab [2016] proposed the so-called Exponential Machines (ExM), where a large tensor of parameters is represented compactly in the TT format in order to provide low-rank regularization of the model and a reduction of the problem to a manageable scale.", "startOffset": 59, "endOffset": 81}, {"referenceID": 174, "context": "To deal with the exponentially large number of parameters, Novikov et al. [2016], Stoudenmire and Schwab [2016] proposed the so-called Exponential Machines (ExM), where a large tensor of parameters is represented compactly in the TT format in order to provide low-rank regularization of the model and a reduction of the problem to a manageable scale.", "startOffset": 59, "endOffset": 112}, {"referenceID": 197, "context": ", 2016], but a recent negative result shows that it is not possible to provide a good convex surrogate for the TT-manifold [Rauhut et al., 2016], thus making the Riemannian optimization the most promising tool for low-rank constrained optimization.", "startOffset": 123, "endOffset": 144}, {"referenceID": 135, "context": "The Pymanopt [Koep and Weichwald, 2016] is the first step in this direction, but it is still quite far from big-data problems, since it works with full matrices even for a low-rank manifold, and in the tensor case that would be prohibitive.", "startOffset": 13, "endOffset": 39}, {"referenceID": 183, "context": "com/oseledets/ttpy) for PYTHON is currently the most complete software for the TT (MPS/MPO) and QTT networks [Oseledets et al., 2012].", "startOffset": 109, "endOffset": 133}, {"referenceID": 6, "context": "For standard TDs (CPD, Tucker models) the Tensor Toolbox for MATLAB, originally developed by Kolda and Bader, provides several general-purpose functions and special facilities for handling sparse, dense, and structured TDs [Bader and Kolda, 2006, 2015], while the N-Way Toolbox for MATLAB, by Andersson and Bro, was developed mostly for Chemometrics applications [Andersson and Bro, 2000].", "startOffset": 363, "endOffset": 388}, {"referenceID": 140, "context": "Related and complementary algorithms implemented by Kressner et al. [2014a] are available within the MATLAB TTeMPS Toolbox (http://anchp.", "startOffset": 52, "endOffset": 76}, {"referenceID": 142, "context": "html) focuses mostly on HT type of tensor networks [Kressner and Tobler, 2014], which avoid explicit computation of the SVDs when truncating a tensor which is already in a HT format [Espig et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 118, "context": "The library is aimed towards more complex tensor networks such as PEPS and MERA [Kao et al., 2015].", "startOffset": 80, "endOffset": 98}, {"referenceID": 47, "context": ", 2012, Zhou and Cichocki, 2013]. The Hierarchical Tucker toolbox by Kressner and Tobler (http://www. sam.math.ethz.ch/NLAgroup/htucker_toolbox.html) focuses mostly on HT type of tensor networks [Kressner and Tobler, 2014], which avoid explicit computation of the SVDs when truncating a tensor which is already in a HT format [Espig et al., 2012, Grasedyck et al., 2013, Kressner and Tobler, 2014]. In quantum physics and chemistry, a number of software packages have been developed in the context of DMRG techniques for simulating quantum networks. One example is the intelligent Tensor (iTensor) by Stoudenmire and White [2014], an open source C++ library for rapid development of tensor network algorithms.", "startOffset": 17, "endOffset": 630}, {"referenceID": 54, "context": "Sparse Nonnegative Matrix Factorization (NMF) [Cichocki et al., 2009], given by min A,B }X \u0301AB}F + \u03b31}A}1 + \u03b32}B}1, s.", "startOffset": 46, "endOffset": 69}, {"referenceID": 87, "context": "Sparse Inverse Covariance Selection [Friedman et al., 2008], given by min XPSN\u02c6N ++ tr(CxX) \u0301 log det X + \u03b3}X}1, s.", "startOffset": 36, "endOffset": 59}, {"referenceID": 244, "context": "Useful examples of penalty functions which promote sparsity and smoothness are [Witten, 2010]", "startOffset": 79, "endOffset": 93}, {"referenceID": 105, "context": "Two-way functional PCA/SVD [Huang et al., 2009] in the form max u,v tuTAv \u0301 \u03b3 2 P1(u)P2(v)u, s.", "startOffset": 27, "endOffset": 47}, {"referenceID": 153, "context": "Sparse SVD [Lee et al., 2010], given by", "startOffset": 11, "endOffset": 29}, {"referenceID": 5, "context": "Generalized nonnegative SPCA [Allen and Maletic-Savatic, 2011] which solves max u,v tuTARv \u0301 \u03b1}v}1u, s.", "startOffset": 29, "endOffset": 62}, {"referenceID": 203, "context": ", a system which simulates the structure of biological molecule [Savostyanov et al., 2014].", "startOffset": 64, "endOffset": 90}, {"referenceID": 40, "context": "1 A Perspective of Tensor Networks for Deep Learning Several research groups have recently investigated the application of tensor decompositions to simplify DNNs and to establish links between deep learning and low-rank tensor networks [Chen et al., 2017, Cohen et al., 2016, Lebedev and Lempitsky, 2015, Novikov et al., 2015, Poggio et al., 2016, Yang and Hospedales, 2016]. For example, Chen et al. [2017] presented a general and constructive connection between Restricted Boltzmann Machines (RBM) and TNs, together with the correspondence between", "startOffset": 237, "endOffset": 408}, {"referenceID": 35, "context": "We therefore conjecture that realistic datasets in most successful machine learning applications have relatively low entanglement entropies [Calabrese and Cardy, 2004].", "startOffset": 140, "endOffset": 167}, {"referenceID": 42, "context": "The TN can be simplified through conversion to TT/MPS formats [Chen et al., 2017].", "startOffset": 62, "endOffset": 81}, {"referenceID": 169, "context": "3: Graphical illustrations of the construction of Deep (restricted) Boltzmann Machines (DBMs) and Standard Deep Belief Networks (DBNs). Both use initialization schemes based on greedy layer-wise training of Restricted Boltzmann Machines (RBMs) (left panel), i.e., they are both probabilistic graphical models consisting of stacked layers of RBMs. Although DBNs and DBMs look diagrammatically quite similar, they are qualitatively very different. The main difference is in how the hidden layers are connected. In a DBM, the connection between all layers is undirected, thus each pair of layers forms an RBM, while a DBN has undirected connections between its top two layers and downward directed connections between all its lower layers (see Salakhutdinov and Hinton [2009] for detail).", "startOffset": 78, "endOffset": 773}, {"referenceID": 85, "context": "\u201cexperts\u201d, in which a number of experts for individual observations are combined multiplicatively [Fischer and Igel, 2012].", "startOffset": 98, "endOffset": 122}, {"referenceID": 42, "context": "2(c) [Chen et al., 2017] shows that such a tensor network model comprises a set of interconnected diagonal core tensors: Kth-order tensors \u039b v = diagK(1, e am) and Mth-order tensors \u039b (k) h = diagM(1, e bk), all of sizes 2 \u02c6 2 \u02c6  \u0308  \u0308  \u0308 \u02c6 2.", "startOffset": 5, "endOffset": 24}, {"referenceID": 192, "context": "1 Matrix Variate Restricted Boltzmann Machines The matrix-variate restricted Boltzmann machine (MVRBM) model was proposed as a generalization of the classic RBM to explicitly model matrix data [Qi et al., 2016], whereby both input and hidden variables are in their matrix forms which are connected by bilinear transforms.", "startOffset": 193, "endOffset": 210}, {"referenceID": 85, "context": "For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al.", "startOffset": 46, "endOffset": 70}, {"referenceID": 85, "context": "For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al. [2016], Salakhutdinov and Hinton [2009].", "startOffset": 46, "endOffset": 88}, {"referenceID": 85, "context": "For the corresponding learning algorithms see Fischer and Igel [2012], Qi et al. [2016], Salakhutdinov and Hinton [2009].", "startOffset": 46, "endOffset": 121}, {"referenceID": 174, "context": "[Nguyen et al., 2015]).", "startOffset": 0, "endOffset": 21}, {"referenceID": 57, "context": ", I), to template input patches, thereby creating I feature maps [Cohen and Shashua, 2016].", "startOffset": 65, "endOffset": 90}, {"referenceID": 57, "context": ", x(I)u [Cohen and Shashua, 2016].", "startOffset": 8, "endOffset": 33}, {"referenceID": 191, "context": "For example, it was shown recently how NNs with a univariate rectified linear unit (ReLU) nonlinearity may perform multivariate function approximation [Poggio et al., 2016].", "startOffset": 151, "endOffset": 172}, {"referenceID": 191, "context": "For example, it was shown recently how NNs with a univariate rectified linear unit (ReLU) nonlinearity may perform multivariate function approximation [Poggio et al., 2016]. As discussed in Part 1 and in Chapter 1 the main idea is to employ a low-rank tensor network representation to approximate and interpolate a multivariate function hc(x1, . . . , xN) of N variables by a finite sum of separated products of simpler functions (i.e., via sparsely interconnected core tensors). Tensorial Mixture Model (TMM). Recently, the model in (4.15) has been extended to a corresponding probabilistic model, referred to as the Tensorial Mixture Model (TMM), given by Sharir et al. [2016] P(x1, x2, .", "startOffset": 152, "endOffset": 679}, {"referenceID": 212, "context": "The TMM based on the HT decompositions is promising for multiple classification problems with partially missing training data, and potentially even regardless of the distribution of missing data [Sharir et al., 2016].", "startOffset": 195, "endOffset": 216}, {"referenceID": 59, "context": ", I, map each local patch xn into a feature space of dimension I; (ii) the second, a key part, is a convolutional arithmetic circuit consisting of many hidden layers that takes the N I measurements (training samples) generated by the representation layer; (iii) the output layer, which can be represented by a matrix, W(L) P RR\u02c6C, which computes C different score functions hc [Cohen et al., 2016].", "startOffset": 377, "endOffset": 397}, {"referenceID": 57, "context": "In order to convert ConvAC tensor models to widely used convolutional rectifier networks, we need to employ the generalized (nonlinear) outer products, defined as [Cohen and Shashua, 2016] (A \u030b\u03c1 B)i1,.", "startOffset": 163, "endOffset": 188}, {"referenceID": 57, "context": "Example 11 Consider a generalized CP decomposition, which corresponds to a shallow rectifier network in the form [Cohen and Shashua, 2016]", "startOffset": 113, "endOffset": 138}, {"referenceID": 80, "context": "7 MERA and 2D TNs for a Next Generation of DCNNs The Multiscale Entanglement Renormalization Ansatz (MERA) tensor network was first introduced by Vidal [2008], and for this network numerical algorithms to minimize some specific cost functions or local Hamiltonians used in quantum physics already exist [Evenbly and Vidal, 2009].", "startOffset": 303, "endOffset": 328}, {"referenceID": 81, "context": "The MERA is a relatively new tensor network, widely investigated in quantum physics based on variational Ansatz, since it is capable of capturing many of the key complex physical properties of strongly correlated ground states [Evenbly and Vidal, 2015].", "startOffset": 227, "endOffset": 252}, {"referenceID": 230, "context": "7 MERA and 2D TNs for a Next Generation of DCNNs The Multiscale Entanglement Renormalization Ansatz (MERA) tensor network was first introduced by Vidal [2008], and for this network numerical algorithms to minimize some specific cost functions or local Hamiltonians used in quantum physics already exist [Evenbly and Vidal, 2009].", "startOffset": 146, "endOffset": 159}, {"referenceID": 81, "context": "The key properties of MERA can be summarized, as follows [Evenbly and Vidal, 2015]: \u2022 MERA can capture scale-invariance in input data; \u2022 It reproduces a polynomial decay of correlations between inputs, in contrast to HT or TT networks which reproduce only exponential decay of correlations; \u2022 MERA has the ability to compress tensor data much better than TT/HT tensor networks; \u2022 It reproduces a logarithmic correction to the area law, therefore MERA is more powerful tensor network than HT/TTNS or TT/TC networks; \u2022 MERA can be efficiently contracted due to unitary constraints imposed on core tensors.", "startOffset": 57, "endOffset": 82}], "year": 2017, "abstractText": "Part 2 of this monograph builds on the introduction to tensor networks and their operations presented in Part 1. It focuses on tensor network models for super-compressed higher-order representation of data/parameters and related cost functions, while providing an outline of their applications in machine learning and data analytics. A particular emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions, and their physically meaningful interpretations which reflect the scalability of the tensor network approach. Through a graphical approach, we also elucidate how, by virtue of the underlying low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volumes of data/parameters, thereby alleviating or even eliminating the curse of dimensionality. The usefulness of this concept is illustrated over a number of applied areas, including generalized regression and classification (support tensor machines, canonical correlation analysis, higher order partial least squares), generalized eigenvalue decomposition, Riemannian optimization, and in the optimization of deep neural networks. Part 1 and Part 2 of this work can be used either as stand-alone separate texts, or indeed as a conjoint comprehensive review of the exciting field of low-rank tensor networks and tensor decompositions.", "creator": "LaTeX with hyperref package"}}}