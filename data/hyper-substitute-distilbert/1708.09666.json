{"id": "1708.09666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Generating Video Descriptions with Topic Guidance", "abstract": "generating video diagrams in natural imagery ( a. etc. a. video maps ) is evidently more challenging task than goal management as the videos are intrinsically worse complicated than images in two aspects. first, videos generate obviously broader range of programming, such old news, music, sports and so much. later, related mechanisms actually apply in the same spectrum. among long paper, students anticipated enabling novel theoretical code, hypothesis - guided model ( cm ), meaning generate frequency - determined descriptions for persons in the wild via descriptive topic information. so addition to predefined sources, i. e., category tags generated from archaeological web, we design process citations in a data - saturated way fed on training conducted by an unsupervised topic mining tree. we show that data - supported topics reflect one culturally accessible grammar than objective source narrative. as i testing learning topic prediction, we treat the thematic control model as teacher to inspire the student, the role tracker model, by imposing the full frequency - modalities in analytic video covering the speech modality.... propose evolving database of training models to exploit topic description, furthermore implicitly identifying instructional topics as input features to generate charts related to the topic and explicitly altering the results in the query search wheels potentially function as an aid showing area - aware language frames. our comprehensive experimental framework accompanying the two largest internet research dataset 73 - 82 illustrated the effectiveness being our topic - guided model, whom significantly surpasses numerical winning results in the 2016 msr guides to language training.", "histories": [["v1", "Thu, 31 Aug 2017 11:17:53 GMT  (5086kb,D)", "http://arxiv.org/abs/1708.09666v1", "Appeared at ICMI 2017"], ["v2", "Mon, 4 Sep 2017 11:38:38 GMT  (5086kb,D)", "http://arxiv.org/abs/1708.09666v2", "Appeared at ICMR 2017"]], "COMMENTS": "Appeared at ICMI 2017", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["shizhe chen", "jia chen", "qin jin"], "accepted": false, "id": "1708.09666"}, "pdf": {"name": "1708.09666.pdf", "metadata": {"source": "META", "title": "Generating Video Descriptions with Topic Guidance", "authors": ["Shizhe Chen", "Jia Chen", "Qin Jin"], "emails": ["cszhe1@ruc.edu.cn", "jiac@cs.cmu.edu", "qjin@ruc.edu.cn"], "sections": [{"heading": null, "text": "Keywords Video Captioning; Data-driven Topics; Multi-modalities; Teacherstudent Learning"}, {"heading": "1 Introduction", "text": "It is an ultimate goal of video understanding to automatically generate natural language descriptions of video contents. A wide range of applications can bene t from it such as assisting blind people, video editing, indexing, searching or sharing. Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a ention to the video captioning task to translate videos to natural language.\nHowever, the open domain videos are quite diverse in topics which makes generating video descriptions more complicated than the image captioning. For various topics ranging from political news to edited movie trailers, the vocabularies and expression styles vary a lot in describing the video contents. For example, for political news videos, words from the political domain occur more\n\u2217Corresponding author.\nfrequently. Also political news descriptions are typically in the style of somebody reporting something, which are quite di erent from descriptions for other topics such as gaming, travel, movie, and animals etc. Besides the topic diversity across videos, even in the same video, its diversity in content and video structure can result in very di erent video descriptions capturing di erent topics in the video as shown in Figure 1. erefore, the topic information is important to guide the caption model to generate be er topicoriented language expression.\nIn our previous study [5], we have utilized the prede ned topics, the category tags crawled from video meta-data during data collection, to improve the captioning performance. However, the prede ned topics are suboptimal for video captioning because: 1) e crawled information contains labelling mistakes which harms the captioning performance; 2) e exclusive topic labels do not capture the topic diversity nature inside the video; 3) e prede ned topic schema is not specially designed for the video captioning task, which may not re ect the topic distributions well. In this work, we propose the data-driven topics to deal with the drawbacks of prede ned topics. We use the Latent Dirichlet Allocation (LDA) topic mining model to automatically generate topics from the annotated video captions in training set.\nIn order to use the mined topic information in the captioning task, two questions need to be addressed: 1) how to obtain the topics automatically for testing videos; and 2) how to e ectively\nar X\niv :1\n70 8.\n09 66\n6v 1\n[ cs\n.C V\n] 3\n1 A\nug 2\n01 7\nemploy the topic information to model the di erence of sentence descriptions in di erent video topics.\nFor the rst question, we take a teacher-student learning perspective [6] to train the data-driven topic prediction model. e LDA topic mining model is viewed as the teacher to guide the student topic prediction model to learn. A video generally contains multiple modalities including image, motion, aural and speech modalities. Image modality provides rich information for understanding the video\u2019s semantic contents such as object and scene. e motion modality presents actions of the objects and the temporal structure of videos. Aural and speech modalities provide additional information for understanding semantic topics from the sound perspective. Hence, we build one general topic prediction model that utilizes all the four modalities and another topic prediction model dedicated to speech modality as its feature representation is di erent from other modalities.\nFor the second question, we propose a novel topic-guided model (TGM) to employ the predicted topics, which is based on the encoderdecoder framework [7]. e TGM functions as an ensemble of topic-aware language decoders to learn speci c vocabularies and expressions for various video topics. We also compare the TGM with a series of caption models that we propose in this paper to exploit the topic information, including topic concatenation in encoder (TCE), topic concatenation in decoder (TCD), topic embedding addition/multiplication in decoder (TEAD/TEMD). ese compared models implicitly use topics to change the input features of the encoder or decoder, while our proposed TGM explicitly modi es the weights in the decoder according to the predicted topics to capture the sentence distributions within the topic more e ectively. e framework of the overall system for testing videos is shown in Figure 2. Experimental results on the MSR-VTT dataset demonstrate the e ectiveness of our proposed method, which can generate more comprehensive and accurate video descriptions.\nIn summary, our contributions in this work include: 1) we show that the data-driven topics are more suitable as the topic representation for video captioning than the prede ned topics, e.g. the category tags, with respect to the topic accuracy and schema; 2) to the best of our knowledge, we are the rst to use the full multimodalities especially the speech modality to successfully boost the\nvideo captioning performance; and 3) the proposed topic-guided model can exploit the topic information more e ectively to generate be er topic-oriented video descriptions.\ne rest of the paper is organized as follows: Section 2 introduces the related work. Section 3 compares the prede ned and the datadriven topics. Our proposed topic-guided model is described in Section 4. Section 5 presents experimental results and analysis. Section 6 draws some conclusions."}, {"heading": "2 Related Works", "text": "ere are mainly two directions in previous image/video captioning works. e rst is to build rule based systems, which rst detect words by object or action recognition and then generate sentences with prede ned language constrains. For example, Lebret et al. [1] predict phrases with a bilinear model and generate descriptions using simple syntax statistics. Rohrbach et al. [8] use the Conditional Random Field to learn object and activity labels from the video. Such systems su er from the expression accuracy and exibility.\nMore recently, researches have been focusing on the second direction of encoder-decoder framework [7] which generates sentences based on image/video features in an end-to-end manner. For example, Vinyals et al. [2] utilize the LSTM to generate sentences with CNN features extracted from the image. Venugopalan et al. [9] transfer knowledge from image caption models with the encoder to perform mean pooling over frame CNN features for video captioning. Pan et al. [10] explicitly embed the sentences and the videos into a common space in addition to the video description generation.\nere are also works considering to employ semantic concepts in the encoder-decoder framework. For example, Wu et al. [11] directly generate image captions based on the detected semantic concepts. You et al. [4] propose to selectively a end to concept proposals in the decoder. Gan et al. [12] propose the semantic compositional networks (SCN) which works as an ensemble of concept-dependent language decoder. Our topic-guided model is inspired by SCN but with di erent aims of producing topic-oriented descriptions to address the topic diversity in video captioning. e reasons of using topics rather than semantic concepts are as follows: 1) ere are much more objects in a video than in an image but many\nof them might be irrelevant to the video description. 2) e topics contain more additional information than semantic objects such as from the motion, aural and speech modalities; and 3) e prediction accuracy is very important for the model as shown in [12]. e topic classi cation is much easier than object classi cation.\nFor video captioning, various video topics result in quite diverse expressions compared with image captioning. Previous works have explored generating descriptions for narrow-domain videos such as YouCook [13] and TACoS [14], whose vocabularies and expressions are similar in the dataset. However, for open-domain videos with various topics such as the MSR-VTT dataset [15], Jin et al. [5] exploit the prede ned video categories in the encoder and signi cantly improve the captioning performance, which results in their winning of the MSR video to language challenge [16]. In our work, we further analyze the qualities of the prede ned categories and propose to mine topics in a data-driven approach that leads to be er accuracy and topic schema.\nMulti-modality nature is also emphasized in video captioning. For the motion modality in videos, Yao et al. [17] explore the temporal structure with local C3D features and global temporal a ention mechanism. Venugopalan et al. [18] propose the sequence to sequence structure which utilizes the LSTM as encoder to capture the temporal dynamics of videos. Pan et al. [19] further propose the hierarchical RNN encoder as well as the temporal-spatial a ention. Aural modality has also been explored for video captioning. Jin et al. [5, 20] and Ramanishka et al. [21] integrate the visual and aural features in the encoder by early fusion and show that the multimodal fusion was bene cial to improve captioning performance. In our work, we consider more modalities in videos especially for the use of speech content modality."}, {"heading": "3 Prede ned vs. Data-driven Topics", "text": "Our previous study [5] has shown that using prede ned topics such as category tags can signi cantly boost video captioning performance. In this section, we analyze the qualities of these prede ned topics, and propose a data-driven approach to develop be er topics for the captioning task."}, {"heading": "3.1 Prede ned Topics: Category Tags", "text": "Each video clip in the MSR-VTT dataset contains a prede ned category tag derived from the meta-data of the video. e distribution\nof the category tags is shown in Figure 3. e prede ned category tags re ect the variety of the video topics, but there are mainly three disadvantages of them:\n(1) Inaccurate category labels: e prede ned category labels contain a certain amount of labelling mistakes as shown in the examples in Figure 4, which greatly harm the captioning performance.\n(2) Exclusive topic distributions: e users can only assign one of the category labels. Such one-hot topic representation cannot re ect the topic diversity inside the video.\n(3) Suboptimal topic schema: a) Ambiguous category de nition. For example, the \u2018people\u2019 category is too general to classify. b) Overlap between di erent categories. For example, the \u2018food\u2019 and \u2018cooking\u2019 categories cover almost similar videos. c) Large mixed categories. Some categories contain much more videos than others and are mixed with many subclasses. d) Indirect connection with captioning task. e category tags are de ned to organize videos in the wild, they are not speci cally de ned for video captioning.\nerefore, although the prede ned category tags have bene ted video captioning a lot, there is still much room for improvement by de ning a be er topic schema to represent the diversity of videos for the video captioning task."}, {"heading": "3.2 Data-driven Topics", "text": "In order to overcome the drawbacks of the prede ned category tags, we propose a data-driven way to generate a more suitable set of video topics. e human generated groundtruth captions provide us with rich and accurate annotations about the videos, which also re ect a more task-related topic distributions of the videos. us, we propose to mine topics from the groundtruth video captions in the training set. We note that it requires no additional labelling e ort on the dataset.\nWe observe that the multiple human generated groundtruth captions sometimes do not agree with each other even for the same video. For example, Figure 1 shows an example video and its groundtruth captions which describe the video from di erent aspects including detailed frame contents, speech contents and general video contents. Such example indicates that a video usually\nTable 1: Examples of some data-driven topicswith their representativewords and their co-occurrencewith prede ned category tags.\ntopic id #videos co-occurrence with prede ned categories representative words 1 182 music:43%, people:21% people dancing group girls dance women music video dances 3 281 music:50% stage man playing singing band performing song guitar music 13 439 food:63%, cooking:29% food cooking kitchen dish person bowl ingredients pan preparing 8 864 news:15%, edu:14%, sci:13% man talking talks guy speaking person si ing giving camera\ncontains several topics, which are re ected in its multiple diverse groundtruth captions. e above observation aligns with the generation process of Latent Dirichlet Allocation (LDA) model [22]: 1. the model rst draws a topic index zdi \u223c Multinomial(\u03b8d ) from the video, where \u03b8d \u223c Dirichlet(\u03b1),d = 1, ...,D. 2. the model draws the observed word wi j \u223c Multinomial(\u03b2zdi ) from the selected topic, where \u03b2k \u223c Dirichlet(\u03b7),k = 1...K . Here, we group the multiple groundtruth captions of a video into one document to mine latent topics from the training data. Stopwords are removed and the the bag-of-words representation is used as our document feature."}, {"heading": "3.3 Relation betweenPrede nedTopics andData-driven Topics", "text": "We study the relation between the prede ned topics and data-driven topics based on their co-occurrence in videos. For each video in the training set, we have its corresponding prede ned category tag and the data-driven topic distribution calculated from the LDA model. To simplify the calculation of co-occurrence, we assign each video with the most likely topic. Table 1 shows co-occurrence between some prede ned categories and data-driven topics. We can see that some prede ned categories are split into di erent topics. For example, the music category is mainly separated into topic 1 of dancing and topic 3 of singing. Some content similar categories are combined together as one topic, i.e. topic 13 consisting of food and cooking categories. And categories that are di erent but express similar content are also merged. For example, news, educations and science categories are merged into one as most of the descriptions under these categories are \u201csomebody is talking about something\u201d. In summary, it shows that the data-driven topics are quite promising and re ect video content distributions be er than prede ned categories."}, {"heading": "4 Topic Guidance Model", "text": "In this section, we provide our solutions for the following two problems: 1) how to automatically predict topics for testing videos with multi-modalities; 2) how to maximize the e ects of the topic information for caption generation."}, {"heading": "4.1 Multimodal Features", "text": "We extract features from image, motion, aural and speech modalities to fully represent the content of videos.\nImagemodality: e image modality re ects the static content of videos. We extract activations from the penultimate layers of the inception-resnet [23] pre-trained on the ImageNet as image object features, and the penultimate layers of the resnet [24] pre-trained\nFigure 5: e framework for topic prediction. We treat the problem from a teacher-student perspective. e teacher topic mining model is used to guide the two student topic prediction models to learn based on general multimodalilties and speech modality respectively.\non the places365 [25] as image scene features, the dimensionality of which are 1536 and 2048 respectively.\nMotion modality: e motion modality captures the local temporal motion. We extract features from the C3D model [26] pretrained on the Sports-1M dataset. We extract activations from the last 3D convolution layer and max-pooling them along the spatial dimension (width and height) to obtain video features with dimensionality of 512. We then applied l2-norm on the C3D features.\nAural modality: Aural modality is complementary to visual modalities, especially for distinguishing scenes or events. We extract the Mel-Frequency Cepstral Coe cients (MFCCs) [27] as the basic low-level descriptors. Two encoding strategies, Bag-of-AudioWords [28] and Fisher Vector [29], are used to aggregate MFCC frames into one video-level feature vector, with dimensionality of 1024 and 624 respectively.\nSpeech modality: Speech modality provides semantic topics and content details of the video. We use the IBM Watson API [30] for speech recognition. Since the backgrounds of the videos are noisy, we clean the speech transcriptions by removing transcriptions with less than 10 words and out-of-vocabulary words based on the training caption vocabulary. Only about half portion of the videos contain speech transcriptions a erwards and a certain amount of transcription errors still exist in the transcriptions. We use the bag-of-words representation as the speech modality feature."}, {"heading": "4.2 Topic Prediction", "text": "For prede ned topics, i.e. category tag, we train a standard one hidden layer neural network with cross-entropy loss to predict. e inputs of this neural network are the multimodal features as described in the section 4.1.\nFor data-driven topics, as there is no direct topic class label, we leverage the topic distribution generated from the topic mining LDA model in section 3.2. We take a teacher-student learning perspective [6] to train the data-driven topic prediction model. To be speci c, the topic mining LDA model is viewed as the teacher and the topic prediction model is viewed as the student. e teacher, topic mining model, is trained in unsupervised style and it generates label, i.e. topic, to guide the student, topic prediction model, to learn. First, we design two topic prediction models: one is general for all multimodal features and the other one is dedicated to speech modality features. en, we ensemble predictions from these two models by averaging to get the nal prediction.\nGeneral Multimodal Topic Prediction Model: is model is designed to predict topic from the video content using all the multimodal features. In the teacher-student perspective, the student model usually learns the output distribution on labels, i.e. dark knowledge [31], rather than output label from the teacher model. Following this way, we choose to use KL-divergence as our loss function. e formulation of KL-divergence is as follows:\nDKL(P | |Q) = K\u2211 k=1 Pk lo\u0434 Pk Qk\n(1)\nwhere P ,Q are the probability distributions of the mined topics and predicted topics respectively. e DKL(P | |Q) is di erentiable and thus can be optimized via back-propagation.\nSpeechDedicatedTopic PredictionModel: As speech modality is very informative but not always available in videos, we build a dedicate topic prediction model using speech features. Di erent from other modality features, the speech text feature is of high dimensionality with noises and is sparse. Instead of using the same architecture of the general multimodal topic prediction model, we design a very di erent architecture for the speech dedicated topic prediction model. We make a simple choice of the architecture: reusing the topic mining model for topic prediction on speech text features. As the representation of speech text features is the same as that of features used in the topic mining model, we don\u2019t need to reinvent the wheel.\nAs to the problem of missing modalities, we only use videos that contain the corresponding speech modality in the speech dedicated topic prediction model. In general multimodal topic prediction model, the features of missing modalities are padded as zeros. In ensembling, predictions of the missing modalities are not considered in the average process."}, {"heading": "4.3 Caption Models with Topic Guidance", "text": "Suppose we have multiple video-sentence pairs (V , y) in video captioning dataset, where y = {w1, . . . ,wNw } is the sentence with Nw words. Assume the multimodal features of the video arem1, . . . ,mNm , where Nm is the number of modalities, the multimodal encoder is a neural network that fuses the multimodal features into a dense video representation x as follows:\nx =We [m1; . . . ;mNm ] + be (2)\nwhereWe ,be are the parameters in the encoder and [\u00b7] denotes the feature concatenation. Since the captioning output is the sequential words, we utilize the LSTM [32] recurrent neural networks as our\nlanguage decoder: ht = f (ht\u22121,wt\u22121;\u03b8d ) for t = 1, . . . ,Nw (3)\nwhere f is the LSTM update function, ht is the state of LSTM and \u03b8d is the parameter in LSTM. We initialize h0 as x to condition on the video representation and w0 as the sentence start symbol. en the probability of the correct word conditioned on the video content and previous words can be expressed as:\nPr(wt |x,w0, . . . ,wt\u22121) = So max(Wdht + bd ) (4) whereWd ,bd are parameters. e objective function is to maximize the log likelihood of the correct description:\nlog Pr(y|x) = Nw\u2211 t=1 log Pr(wt |x,w0, . . . ,wt\u22121) (5)\nWe denote the predicted topics in Section 4.2 as z \u2208 RK , where K is the number of topics. In order to e ectively exploit the topics z for video captioning, we propose the novel topic-guided model (TGM), and a series of simple yet strong caption models with topic guidance for comparison, called TCE, TCD and TEAD/TEMD for short. Detailed descriptions of these models are as follows.\nTopic Concatenation in Encoder (TCE): We fuse the topic distribution z together with multimodal features in the encoder as the topic-aware video representation x:\nx =We ([m1; . . . ;mNm ; z]) + be (6) e LSTM decoder then generates video description conditioning on the new topic-aware representation x.\nTopic Concatenation in Decoder (TCD): In the TCE model, the topic information only occurs at the rst step in the decoder, which could easily make the topic guidance \u201cdri away\u201d. To enhance the topic guidance, we concatenate the topic distribution z with the word embeddingwt\u22121 as the input to the LSTM every step, which is similar to the gLSTM proposed in [33]:\nht = f (ht\u22121, [wt\u22121; z];\u03b8d ) (7) e extra input of topic z can guide the language decoder to generate words related to the topic in every step.\nTopic EmbeddingAddition/Multiplication inDecoder (TEAD/TEMD): To generate a more comparable representation for the topic representation z compared to the word embedding, we embed each topic into a latent vector space with the same dimensionality as the word embedding:\nze =Wzz + bz (8) whereWz ,bz are topic embedding parameters. We perform addition or multiplication on the topic embedding and word embedding to generate the topic-aware input feature for the language decoder every step, which are expressed as:\nTEAD : ht = f (ht\u22121,wt\u22121 \u2295 ze ;\u03b8d ) (9) TEMD : ht = f (ht\u22121,wt\u22121 ze ;\u03b8d ) (10)\nwhere \u2295, are element-wise addition and multiplication respectively.\nTopic-GuidedModel (TGM): e above TCD and TEAD/TEMD models only implicitly using the topic information as the global guidance, which modify the inputs to the language decoder in every step and cannot take into account of the overall expressions\nwithin the topic. Inspired by Gan et al. [12], therefore, we further propose the topic-guided model (TGM) that explicitly functions as an ensemble of topic-aware language decoders to capture di erent sentence distributions for each topic. e structure of the TGM is shown in the right side in Figure 2, which can automatically modify the weight matrices in LSTM according to the topic distribution z.\nLet us take one of weight matrices in the LSTM as an example, and other parameters in LSTM cell are alike. We de ne weight W\u03c4 \u2208 Rnh\u00d7nw\u00d7K , where nh is the number of hidden units and nw is the dimension of word embedding. e W\u03c4 can be viewed as the ensemble of K topic-speci c LSTM weight matrices. e topic-related weight matrixW (z) \u2208 Rnh\u00d7nw can be speci ed as\nW (z) = K\u2211 k=1 zkW\u03c4 [k] (11)\nwhere zk is the k-th topic in z; W\u03c4 [k] denote the k-th matrix of W\u03c4 . In this way, the video topic z can automatically generate its corresponding LSTM decoders to produce the topic-oriented video descriptions. However, the parameters are increasing with K which may result in over- ing easily. So the ideas in [34] are used to share parameters by factorizingW (z) as follows:\nW (z) =Wa \u00b7 dia\u0434(Wbz) \u00b7Wc (12)\nwhere Wa \u2208 Rnh\u00d7nf , Wb \u2208 Rnf \u00d7K and Wc \u2208 Rnf \u00d7nw . nf is the number of factors. Wa andWc are shared among all topics, while Wb can be viewed as the latent topic embedding."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Experimental Setup", "text": "Dataset: e MSR-VTT corpus [15] is currently the largest video to language dataset with a wide variety of video contents. It consists of 10,000 video clips with 20 human generated captions per clip. Each video also contains a prede ned category tag, which is one of the 20 popular video categories in web videos. Following the standard data split, we use 6,513 videos for training, 497 videos for validation and the remained 2,990 for testing.\nData Preprocessing: We convert all descriptions to lower case and remove all the punctuations. We add begin-of-sentence tag <BOS> and end-of-sentence tag <EOS> to our vocabulary. Words which appear more than twice are selected, resulting in a vocabulary of size 10,868. e maximum length of a generated caption is set to be 30.\nTraining Settings: We empirically set the feed forward neural networks for topic prediction to have one hidden layer with 512 units. e dimension of LSTM hidden size is set to be 512. e output weights to predict the words are the transpose of the input word embedding matrix. We apply dropout with rate of 0.5 on the input and output of LSTM and use ADAM algorithm [35] with learning rate of 10\u22124. Beam search with beam width of 5 is used to generate sentences during testing process. e baseline system is the vanilla encoder-decoder framework with multimodal features (we call it the multimodal baseline).\nIn our experiments, we nd that the dimensionality of the features to the encoder should not be too high in order to avoid over- ing, so only the image object, video motion, and aural features are used as input to the encoder.\nEvaluation Metrics: We evaluate the caption results comprehensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39]."}, {"heading": "5.2 Evaluation", "text": "Table 2 presents captioning performances on testing set with prede ned category tags and the data-driven topics using their corresponding best caption model with topic guidance. We can see that the di erent topic guidances (the second to forth rows) all greatly improve the performance of the multimodal baseline (the rst row). Since the data-driven topics on testing set are predicted as shown in Figure 5, we also use the predicted category tags for a fair comparison. e guidance from predicted data-driven topics outperforms that from the predicted category tags on all four evaluation metrics, and the Student\u2019s t-test shows the improvement is signi cant with p-value< 0.002. Even compared with the category tags assigned by video uploaders, the predicted data-driven topics also slightly boost the captioning performance on multiple metrics with the Student\u2019s t-test p-value< 0.01 on BLEU@4 and CIDEr metrics, which shows the performance gain is robust. ese results suggest that the data-driven topics are more suitable as the topic representation than prede ned topics for video captioning.\nTo demonstrate the e ectiveness of the proposed topic-guided model (TGM), we further compare the TGM with other caption models with topic guidance. As shown in Table 3, the TGM achieves the best performance among all the caption models on all four metrics especially for the CIDEr score. It suggests that modifying the weights of the decoder according to the topic distributions can employ the topic guidance more e ectively to generate be er topicoriented descriptions. Our proposed TGM model also achieves be er performance than the winning performance in 2016 MSR video to language challenge [5], where we use multimodal features and select best models by the prede ned categories.\nFigure 6 presents some examples in the testing set. In addition to more accurate video descriptions, the TGM can also generate more novel concepts such as the llama. Our statistics on the generated sentence show that the number of unique caption words generated by TGM is 397, while it is 360 by the multimodal baseline model."}, {"heading": "5.3 Ablation Experiments", "text": "InteractiveCaptionwithManuallyAnnotatedTopic: Our model o ers the exibility of manually assigned topics to the video. It means that we could interactively annotate the topics for testing videos based on relevance between video contents and the representative words in topics to generate be er captions. Results in Table 4\nand examples in Figure 7 presents the captioning performance with the annotated topics. Both show that with more accurate topic information the captioning performance can be further improved.\nIn uence of Multi-Modalities: As an implicit evaluation, we can evaluate the topic prediction performance with the annotated topics on testing set. e prediction accuracies with di erent modalities are shown in Figure 8. e performance of the aural and speech modality are evaluated only on videos containing the corresponding modality. ough the aural modality alone do not perform well, the ensemble of aural with image and motion modalities improves the prediction signi cantly with an absolute 6% boost. By further\nusing the speech modality predictions, the accuracy is improved from 62.61% to 63.65% on testing set.\nTo explicitly explore the usefulness of speech modality in video captioning, we use two kinds of predicted topics which are obtained by the fusion with or without the predictions from speech modality. Results are presented in Table 5. We can see that the captioning performance achieves large gain in all metrics although the speech modality gets only 1.04% absolute prediction accuracy improvement as mentioned above. It mainly results from the similar topic probability distributions using the shared topic mining model for speech modality. So the speech modality is quite useful to generate topic proposals and thus boost the captioning performance.\nIn uence of the Number of Topics: We also explore the performance of TGM with di erent numbers of topics. As shown in Table 6, the number of topics 20 achieves the best performance, which is the balance between the topic prediction performance and the topic guidance performance. When there is fewer number of topics, the accuracy of topic prediction is higher but it provides less guidance to generate video descriptions. When there is more number of topics, though the topic guidance becomes strong, the captioning performance su ers from the low topic prediction accuracy. Since the more the topics are the more the topics resemble semantic concepts, it suggests that using a small number of topics is enough for the video captioning task and are superior to the large number of detected concepts."}, {"heading": "6 Conclusions", "text": "Descriptions of videos with diverse topics vary a lot in vocabularies and expression styles. In this paper, we propose a novel topicguided model to deal with the topic diversity nature of videos. It can generate be er topic related descriptions for videos in various topics. Our experimental results show that the topic information is very useful to guide the caption model for more topic appropriate description generation and topics automatically mined in data-driven way are superior to the prede ned topics as the topic guidance. Multimodal features especially the speech modality features are vital to predict topics for testing videos. Our proposed topic-guided model which functions as an ensemble of topic-aware language decoders can utilize the topic information more e ectively than other caption models. It signi cantly improves the multimodal baseline performance on the current largest video caption dataset MSR-VTT, outperforming the winning performance in the 2016 MSR video to language challenge. In the future work, we will continue to improve the topic prediction performance and jointly learn the topic representation and caption generation end-to-end."}, {"heading": "7 Acknowledgments", "text": "is work is supported by National Key Research and Development Plan under Grant No. 2016YFB1001202."}], "references": [{"title": "Phrase-based image captioning", "author": ["R\u00e9mi Lebret", "Pedro H.O. Pinheiro", "Ronan Collobert"], "venue": "In ICML, pages 2085\u20132094,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In CVPR,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Show, a\u008aend and tell: Neural image caption generation with visual a\u008aention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Image captioning with semantic a\u008aention", "author": ["\u008banzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Describing videos using multi-modal fusion", "author": ["Qin Jin", "Jia Chen", "Shizhe Chen", "Yifan Xiong", "Alexander Hauptmann"], "venue": "In ACM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Do deep nets really need to be deep", "author": ["Lei Jimmy Ba", "Rich Caruana"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Translating video content to natural language descriptions", "author": ["Marcus Rohrbach", "Wei Qiu", "Ivan Titov", "Stefan \u008cater", "Manfred Pinkal", "Bernt Schiele"], "venue": "In ICCV,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["Subhashini Venugopalan", "Huijuan Xu", "Je\u0082 Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": "Computer Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Jointly modeling embedding and translation to bridge video and language", "author": ["Yingwei Pan", "Tao Mei", "Ting Yao", "Houqiang Li", "Yong Rui"], "venue": "In CVPR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "What value do explicit high level concepts have in vision to language problems", "author": ["Qi Wu", "Chunhua Shen", "Lingqiao Liu", "Anthony Dick", "Anton van den Hengel"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pa\u0088ern Recognition,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Semantic compositional networks for visual captioning", "author": ["Zhe Gan", "Chuang Gan", "Xiaodong He", "Yunchen Pu", "Kenneth Tran", "Jianfeng Gao", "Lawrence Carin", "Li Deng"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2017}, {"title": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching", "author": ["Pradipto Das", "Chenliang Xu", "Richard F. Doell", "Jason J. Corso"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Msr-v\u008a: A large video description dataset for bridging video and language", "author": ["Jun Xu", "Tao Mei", "Ting Yao", "Yong Rui"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher J. Pal", "Hugo Larochelle", "Aaron C. Courville"], "venue": "In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Je\u0082rey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Hierarchical recurrent neural encoder for video representation with application to captioning", "author": ["Pingbo Pan", "Zhongwen Xu", "Yi Yang", "Fei Wu", "Yueting Zhuang"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Video description generation using audio and visual cues", "author": ["Qin Jin", "Junwei Liang"], "venue": "In ICMR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Multimodal video description", "author": ["Vasili Ramanishka", "Abir Das", "Dong Huk Park", "Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach", "Kate Saenko"], "venue": "In Proceedings of the 2016 ACM on Multimedia Conference,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Inceptionv4, inception-resnet and the impact of residual connections on learning", "author": ["Christian Szegedy", "Sergey Io\u0082e", "Vincent Vanhoucke", "Alex Alemi"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In CVPR,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Places: An image database for deep scene understanding", "author": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Antonio Torralba", "Aude Oliva"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Learning spatiotemporal features with 3d convolutional networks", "author": ["Du Tran", "Lubomir Bourdev", "Rob Fergus", "Lorenzo Torresani", "Manohar Paluri"], "venue": "In ICCV,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["Steven Davis", "Paul Mermelstein"], "venue": "IEEE transactions on acoustics, speech, and signal processing,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1980}, {"title": "So\u0089ening quantization in bag-of-audiowords", "author": ["Stephanie Pancoast", "Murat Akbacak"], "venue": "In ICASSP,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Image classi\u0080cation with the \u0080sher vector: \u008ceory and practice", "author": ["Jorge S\u00e1nchez", "Florent Perronnin", "\u008comas Mensink", "Jakob Verbeek"], "venue": "International journal of computer vision,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["Geo\u0082rey Hinton", "Oriol Vinyals", "Je\u0082 Dean"], "venue": "Computer Science,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Guiding the long-short term memory model for image caption generation", "author": ["Xu Jia", "Efstratios Gavves", "Basura Fernando", "Tinne Tuytelaars"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Unsupervised learning of image transformations", "author": ["R Memisevic", "G Hinton"], "venue": "In CVPR, pages", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "Association for Computational Linguistics,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Meteor universal: Language speci\u0080c translation evaluation for any target language", "author": ["Michael Denkowski", "Alon Lavie"], "venue": "Proceedings of the Ninth Workshop on Statistical Machine Translation. Citeseer,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Rouge: A package for automatic evaluation of summaries. In Text summarization branches out", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the ACL-04 workshop,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2004}, {"title": "Cider: Consensusbased image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In CVPR,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 1, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 2, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 3, "context": "Drawing on the recent success of image captioning [1\u20134], where a sentence is generated to describe the image content, more researchers are paying a\u008aention to the video captioning task to translate videos to natural language.", "startOffset": 50, "endOffset": 55}, {"referenceID": 4, "context": "In our previous study [5], we have utilized the prede\u0080ned topics, the category tags crawled from video meta-data during data collection, to improve the captioning performance.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "For the \u0080rst question, we take a teacher-student learning perspective [6] to train the data-driven topic prediction model.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "For the second question, we propose a novel topic-guided model (TGM) to employ the predicted topics, which is based on the encoderdecoder framework [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "[1] predict phrases with a bilinear model and generate descriptions using simple syntax statistics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] use the Conditional Random Field to learn object and activity labels from the video.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "More recently, researches have been focusing on the second direction of encoder-decoder framework [7] which generates sentences based on image/video features in an end-to-end manner.", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "[2] utilize the LSTM to generate sentences with CNN features extracted from the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] transfer knowledge from image caption models with the encoder to perform mean pooling over frame CNN features for video captioning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] explicitly embed the sentences and the videos into a common space in addition to the video description generation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] directly generate image captions based on the detected semantic concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] propose to selectively a\u008aend to concept proposals in the decoder.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[12] propose the semantic compositional networks (SCN) which works as an ensemble of concept-dependent language decoder.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "2) \u008ce topics contain more additional information than semantic objects such as from the motion, aural and speech modalities; and 3) \u008ce prediction accuracy is very important for the model as shown in [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 12, "context": "Previous works have explored generating descriptions for narrow-domain videos such as YouCook [13] and TACoS [14], whose vocabularies and expressions are similar in the dataset.", "startOffset": 94, "endOffset": 98}, {"referenceID": 13, "context": "However, for open-domain videos with various topics such as the MSR-VTT dataset [15], Jin et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 4, "context": "[5] exploit the prede\u0080ned video categories in the encoder and signi\u0080cantly improve the captioning performance, which results in their winning of the MSR video to language challenge [16].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[17] explore the temporal structure with local C3D features and global temporal a\u008aention mechanism.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] propose the sequence to sequence structure which utilizes the LSTM as encoder to capture the temporal dynamics of videos.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] further propose the hierarchical RNN encoder as well as the temporal-spatial a\u008aention.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5, 20] and Ramanishka et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "[5, 20] and Ramanishka et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "[21] integrate the visual and aural features in the encoder by early fusion and show that the multimodal fusion was bene\u0080cial to improve captioning performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Our previous study [5] has shown that using prede\u0080ned topics such as category tags can signi\u0080cantly boost video captioning performance.", "startOffset": 19, "endOffset": 22}, {"referenceID": 19, "context": "eration process of Latent Dirichlet Allocation (LDA) model [22]: 1.", "startOffset": 59, "endOffset": 63}, {"referenceID": 20, "context": "We extract activations from the penultimate layers of the inception-resnet [23] pre-trained on the ImageNet as image object features, and the penultimate layers of the resnet [24] pre-trained Figure 5: \u0087e framework for topic prediction.", "startOffset": 75, "endOffset": 79}, {"referenceID": 21, "context": "We extract activations from the penultimate layers of the inception-resnet [23] pre-trained on the ImageNet as image object features, and the penultimate layers of the resnet [24] pre-trained Figure 5: \u0087e framework for topic prediction.", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "on the places365 [25] as image scene features, the dimensionality of which are 1536 and 2048 respectively.", "startOffset": 17, "endOffset": 21}, {"referenceID": 23, "context": "We extract features from the C3D model [26] pretrained on the Sports-1M dataset.", "startOffset": 39, "endOffset": 43}, {"referenceID": 24, "context": "We extract the Mel-Frequency Cepstral Coe\u0081cients (MFCCs) [27] as the basic low-level descriptors.", "startOffset": 57, "endOffset": 61}, {"referenceID": 25, "context": "Two encoding strategies, Bag-of-AudioWords [28] and Fisher Vector [29], are used to aggregate MFCC frames into one video-level feature vector, with dimensionality of 1024 and 624 respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "Two encoding strategies, Bag-of-AudioWords [28] and Fisher Vector [29], are used to aggregate MFCC frames into one video-level feature vector, with dimensionality of 1024 and 624 respectively.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "We take a teacher-student learning perspective [6] to train the data-driven topic prediction model.", "startOffset": 47, "endOffset": 50}, {"referenceID": 27, "context": "dark knowledge [31], rather than output label from the teacher model.", "startOffset": 15, "endOffset": 19}, {"referenceID": 28, "context": "Since the captioning output is the sequential words, we utilize the LSTM [32] recurrent neural networks as our language decoder:", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "To enhance the topic guidance, we concatenate the topic distribution z with the word embeddingwt\u22121 as the input to the LSTM every step, which is similar to the gLSTM proposed in [33]:", "startOffset": 178, "endOffset": 182}, {"referenceID": 11, "context": "[12], therefore, we further propose the topic-guided model (TGM) that explicitly functions as an ensemble of topic-aware language decoders to capture di\u0082erent sentence distributions for each topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "So the ideas in [34] are used to share parameters by factorizingW (z) as follows:", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "Dataset: \u008ce MSR-VTT corpus [15] is currently the largest video to language dataset with a wide variety of video contents.", "startOffset": 27, "endOffset": 31}, {"referenceID": 31, "context": "5 on the input and output of LSTM and use ADAM algorithm [35] with learning rate of 10\u22124.", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 47, "endOffset": 51}, {"referenceID": 33, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 60, "endOffset": 64}, {"referenceID": 34, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "hensively on all major metrics, including BLEU [36], METEOR [37], ROUGE-L [38] and CIDEr [39].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Our proposed TGM model also achieves be\u008aer performance than the winning performance in 2016 MSR video to language challenge [5], where we use multimodal features and select best models by the prede\u0080ned categories.", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "v2t navigator [5] 0.", "startOffset": 14, "endOffset": 17}], "year": 2017, "abstractText": "Generating video descriptions in natural language (a.k.a. video captioning) is a more challenging task than image captioning as the videos are intrinsically more complicated than images in two aspects. First, videos cover a broader range of topics, such as news, music, sports and so on. Second, multiple topics could coexist in the same video. In this paper, we propose a novel caption model, topic-guided model (TGM), to generate topic-oriented descriptions for videos in the wild via exploiting topic information. In addition to prede\u0080ned topics, i.e., category tags crawled from the web, we also mine topics in a data-driven way based on training captions by an unsupervised topic mining model. We show that data-driven topics re\u0083ect a be\u008aer topic schema than the prede\u0080ned topics. As for testing video topic prediction, we treat the topic mining model as teacher to train the student, the topic prediction model, by utilizing the full multi-modalities in the video especially the speech modality. We propose a series of caption models to exploit topic guidance, including implicitly using the topics as input features to generate words related to the topic and explicitly modifying the weights in the decoder with topics to function as an ensemble of topic-aware language decoders. Our comprehensive experimental results on the current largest video caption dataset MSR-VTT prove the e\u0082ectiveness of our topic-guided model, which signi\u0080cantly surpasses the winning performance in the 2016 MSR video to language challenge.", "creator": "LaTeX with hyperref package"}}}