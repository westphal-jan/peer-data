{"id": "1709.02236", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2017", "title": "Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses", "abstract": "improper emergence of mental signals over time complicates their use to show highly articulated features. to address vision problem, studies currently claimed to combine surface areas with modalities when are emotionally comfortable maintaining the amputation and environment, such be accelerometry or gaze propagation. in the latter case, either hypothesis is once a subject looks at the object displayed commands she intends he manipulate and that knowing looking sensors'natural affordances while removing constrain targets set presenting weak grasps. evaluating this paper, we build an automated design to view fragile particles will show that gaze sensing is indeed helpful in designing hand movements. describing our multimodal program, we carefully detect stable surfaces and scan an object to interest around stated subject's fixation in the visual frame. the patch shown around indicates object is subsequently fed used an off - empty - bat deep convolutional neural network to obtain a high level protein representation, which remains then applied with traditional surface electromyography in diagnostic classification domain. tests have been performed entitled sibling cluster separately involving 13 primate males who performed certain types of recall examining various properties as models recorded in a preliminary examination. they verified that successful gathering of dark information triggered the classification accuracy process. focus demonstrations then demonstrate this improvement is consistent per all grasps and concentrated across the movement testing and offset.", "histories": [["v1", "Tue, 29 Aug 2017 16:59:19 GMT  (4584kb,D)", "http://arxiv.org/abs/1709.02236v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["andrea gigli", "arjan gijsberts", "valentina gregori", "matteo cognolato", "manfredo atzori", "barbara caputo"], "accepted": false, "id": "1709.02236"}, "pdf": {"name": "1709.02236.pdf", "metadata": {"source": "CRF", "title": "Visual Cues to Improve Myoelectric Control of Upper Limb Prostheses", "authors": ["Andrea Gigli", "Arjan Gijsberts", "Valentina Gregori", "Matteo Cognolato", "Manfredo Atzori", "Barbara Caputo"], "emails": ["surname@dis.uniroma1.it", "name.surname@hevs.ch"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nThe loss of a hand or an arm due to amputation has a drastic impact on the quality of life. Although advanced myoelectric prostheses have the potential to restore some of the lost functionality, their acceptance among amputees is very low [1]. Aside from high cost, one of the problems with these active prostheses is that their control is not robust and requires a long and painful training procedure. Myoelectric signals change over time, for instance due to electrode shift, user adaptation, and fatigue, and this hurts control robustness.\nAcademic efforts have therefore started to focus on how to make prosthetic control more stable and more intuitive. An interesting avenue is to reduce the dependency on surface electromyography (sEMG) by including other sources of contextual information, such as inertial sensors [2] or computer vision [3, 4]. The working principle is that this context is helpful in decoding the intent of the prosthesis user. This seems obvious in case of the orientation of the limb (cf. inertial sensors), but also the user\u2019s gaze behavior and the visual description of an object of interest may contain\n*This work was supported by the Swiss National Science Foundation Sinergia project #160837 \u201cMegane Pro\u201d\n1Department of Computer, Control, and Management Engineering, University of Rome La Sapienza, via Ariosto 25, 00185 Roma, Italy surname@dis.uniroma1.it\n2Department of Business Information Systems, University of Applied Sciences Western Switzerland (HES-SO Valais), Sierre, Switzerland name.surname@hevs.ch\nimportant side-information to determine the desired hand movement. For example, it seems more likely that a person that is fixating a pen lying on a table desires to perform a writing tripod than a power disk grasp.\nRecent studies have investigated the use of visual information to preshape a prosthesis based on the estimated object size and orientation. Rather than object size and orientation, we argue that also the object\u2019s affordances are relevant to determine the desired grasp type. We therefore extract highlevel features of the object of interest using a powerful, off-the-shelf convolution neural network. These features are highly discriminative for object identification and they will therefore also contain informative content on the object\u2019s functionality. Furthermore, in contrast to earlier studies we do not require users to trigger the visual recognition system, but instead use gaze tracking to automatically detect stable fixations and to segment the object of interest.\nThe proposed method was evaluated offline on data collected from five intact subjects performing ten grasps. All grasps were repeated both seated and standing, and with three different objects each. The chosen objects are associated with activities of daily living and thus representative of a home environment. To promote variability in the arm dynamics and visual scene, subjects also performed these grasps as part of 15 functional movements (e.g., open a zipper using a lateral grasp).\nThe remainder of this paper continues with an overview of related work in section II. In section III, we give a detailed description of our method to automatically detect fixations and how to integrate the object\u2019s visual representation with sEMG. We then describe the experimental setup of our evaluation in section IV and follow this with the results in section V. This paper is concluded in section VI."}, {"heading": "II. RELATED WORK", "text": "The difficulty of reliably measuring and interpreting sEMG has led to active research on the inclusion of other types of sensory modalities to control myoelectric prostheses, such as sonomyography, mechanomyography, and force myography (for detailed overviews, see [5, 6]). Besides those that measure muscular activity, also modalities that provide an informative context on the intended movement have been combined with sEMG. Several studies have shown that accelerometry of the relevant arm provides useful information on arm orientation and dynamics that is complementary to sEMG [2, 7].\nMore recently, also computer vision and gaze information have been considered to improve intent recognition. Their\nar X\niv :1\n70 9.\n02 23\n6v 1\n[ cs\n.C V\n] 2\n9 A\nug 2\n01 7\nrelevance has been shown in early studies, in which innovative systems were proposed for controlling the prehension of a transradial prosthesis. These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation. This approach has subsequently been integrated with a myoelectric control strategy by Markovic et al. [9, 10]. In their system, myoelectric activity is combined with computer vision and inertial sensing to provide artificial proprioceptive feedback on the grasp type and object size. Via sEMG-based sequential and proportional control, the user can override the automated preselections of the system. The use of computer vision in the context of prosthetics was also hinted at by Ghazaei et al. [11], who used deep learning to classify grasps based on the object\u2019s appearance.\nSlightly different from our application in prosthetics for amputees, sEMG and gaze information has also been used to operate a robot arm for tetraplegic patients. Corbett et al. [12] use the subject\u2019s gaze to help to determine the target position of a reaching movement, while McMullen et al. [13] combine this with computer vision to initiate and automatically perform the reach-grasp-drop motion of the robot arm."}, {"heading": "III. GAZE INTEGRATION", "text": "The basic idea behind this work is to extract a representation of the object that is observed during a prehension and use it as an auxiliary cue in support of a standard sEMG based grasp classifier. To do so, we designed a method to automatically detect stable gaze fixations, extract relevant visual information associated with those fixations, and subsequently integrate this information in the movement classifier."}, {"heading": "A. Fixation detection", "text": "The first step of the algorithm consists in finding fixations in the gaze tracking data. A fixation consists of a period of time (generally between 350ms to 450ms [14]) where the eye-gaze remains in a limited area of the visual field. Since we are only interested in fixations that precede a grasp, we attempt to identify an increase in muscle activity by looking at the Root Mean Square (RMS) of the myoelectric signals in a sliding window of length \u03c4rms. As can be seen in Figure 1, the average activity over all electrodes, denoted with AvgRmsEmg, increases drastically during the initial reach-to-grasp phase. We identify these increases in an online manner using Bollinger bands, which calculate the number of standard deviations that a current value xt of a signal is from a historical mean within a sliding window of length \u03c4boll\nb(x, t, \u03c4boll) = xt \u2212 \u00b5(xt:t\u2212\u03c4boll) \u03c3(xt:t\u2212\u03c4boll) , (1)\nwhere xt:t\u2212\u03c4boll denotes the sliding window, and \u00b5(\u00b7) and \u03c3(\u00b7) denote the window mean and standard deviation. Since we are interested in sudden increases in muscle activity, we limit\nour attention to when the value exceeds the upper Bollinger band\nb(x, t, \u03c4boll) \u2265 \u03b7, (2)\nwhere \u03b7 regulates the sensitivity of the method to the signal\u2019s variations. Figure 1 shows the identified time intervals associated to the grasp\u2019s onset (red) when AvgRmsEmg exceeds its upper Bollinger band (green).\nAfter identifying these regions where the hand has started reaching, we identify stable fixations on the basis of the gaze volatility. Since the gaze is represented as a 2-dimensional vector (both x and y coordinates in the image frame), we define multidimensional volatility of a sequence of gaze points X as Euclidean variance around the centroid\nv(X, t, \u03c4gaze) = 1\n\u03c4gaze t\u2212\u03c4gaze\u2211 i=t \u2225\u2225xi \u2212 \u00b5(Xt:t\u2212\u03c4gaze)\u2225\u22252 . (3) We use this quantity to define a fixation when volatility v(X, t, \u03c4gaze) falls below a threshold which is updated to the 40th percentile of the volatility every 0.5 s. Figure 1 shows the gaze volatility (magenta) along with its threshold (yellow) and the selected gaze fixations (black circles). Based on results during preliminary analyses, we have set the values of the parameters to \u03c4rms = \u03c4boll = \u03c4gaze = 300ms and \u03b7 = 2.\nB. Visual Feature Extraction\nFor each fixation, the gaze position will be used to obtain an image of the observed object from the first-person video recording of the scene. From this image, the object\u2019s affordances will be encoded into appropriate visual features.\nThe video recording and the gaze tracking data are synchronized and expressed in the same reference system, thus the gaze point always lies on the object on which the user is focusing. We isolate this object from the others in the image using the Active Segmentation algorithm by Mishra et al. [15]. This method uses brightness, colors, and textures to segment the object on which the gaze falls. The drawback of the fixation-guided segmentation is its sensitivity to noise in the gaze position estimate. On the other end, its substantial advantage over object-detection methods based on machine\nlearning is that it does not require any prior knowledge about the appearance of the objects of interest.\nThe object\u2019s affordances are extracted from its image and encoded into appropriate visual features using a Convolutional Neural Network (CNN) as a feature extractor. Deep visual features, indeed, are able to gather spatial and highlevel visual characteristic, like shapes and color gradients. The object image is fed into the VGG-16 CNN pre-trained on ImageNet [16] and the activation of the second-last fullyconnected layer is taken as the image visual feature.\nUnder the assumption that the object of interest remains the same during the whole prehension, the CNN feature associated with a certain fixation is maintained for all the subsequent samples, until the next fixation. A side effect of this choice is that each arm rest will be associated with the visual features of the object grasped in the previous prehension. However, this is inevitable because we do not know a priori the grasp\u2019s duration."}, {"heading": "C. Multimodal integration", "text": "At the end of the feature extraction process, the visual cue can be used alone or in conjunction with the myoelectric one to train a grasp classifier. Among the possible methods to integrate multimodal cues, we opt for mid-level integration [17], also known as integration at the kernel level. This method combines couples (x,y) of multimodal samples of the type x = {x1, \u00b7 \u00b7 \u00b7 ,xC} by computing their similarity via a weighted sum of cue-specific kernel functions\nkmc(x,y) = C\u2211 i=1 wiki(x i,yi) for wi > 0 . (4)\nThe weights wi of the kernel combination are free hyperparameters of the multi-cue kernel. Such similarities will be used by a kernel machine classifier to create the prediction model."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "We collected a custom dataset in which we recorded sEMG and gaze while subjects performed a set of grasps on different objects. In the following, we detail the dataset and how the data was used in our offline evalutation."}, {"heading": "A. Dataset", "text": "Five intact subjects (4M, 1F) participated in our study. We selected ten grasps based on relevant literature [18] and on their perceived importance for Activities of Daily Living (ADL). Each of the grasps was performed on three representative objects that could reasonably be manipulated using the respective grasp. In selecting these objects, we attempted to re-use them as much as possible for multiple grasps to enforce a many-to-many relationship: grasps can be used with multiple objects and objects can be used with multiple grasps. This avoids the risk that an object\u2019s identity alone is sufficient to unequivocally predict a grasp. During the acquisition, we made sure that there were always a minimum of five objects placed in front of the subject,\nto encourage realistic gaze behavior and to increase visual clutter.\nAside from multiple objects, the acquisition protocol was extended in two other manners to encourage variability in the myoelectric signals. One source of variability is given by the limb position effect, meaning that the signals will depend on the orientation of the limb. We took this into account by performing all movements both while seated and standing, which are likely the most common orientations in ADL. Second, we extended the protocol with either one or two functional tasks for each of the grasps. This introduces variability in the dynamic context of the hand, or more precisely crosstalk due to the added activity of muscles controlling the wrist and limb. Also in this case these functional movements were selected to represent ADL. The grasps, their respective objects, and the functional tasks are listed in Table I.\nDuring the exercise, the subject was in front of a table on which the objects were placed. Prior to each grasp, a screen showed short movies of the movements with the aim to clarify how each of the objects should be approached. The scope of this video was to help the subject become familiar with the procedure and to perform a training trial while the video was playing. After this initial phase, the subject was requested to repeat each movement-object combination or functional movement four times. The computer indicated when to start the grasp and when to release via audio instructions. As visual support, the required grasp was schematically shown on the screen for the entire duration of the exercise. Each repetition took approximately 8 s, containing the actual grasp (4 s to 5 s) and the subsequent transition back to the rest posture (3 s to 4 s).\nMuscular activity was recorded using twelve Delsys Trigno double differential sEMG electrodes placed in two rows around the forearm, where the upmost row contained eight electrodes and the remaining four were placed lower (see Figure 2). The myoelectric signals were sampled at 2 kHz. At the same time, the gaze and first-person scene video were recorded using the Tobii Pro Glasses II. These glasses record the subject\u2019s gaze at 100Hz with a theoretical accuracy and precision of 0.5\u25e6 and 0.3\u25e6 degrees RMS, respectively. The frame also contains a forward facing scene camera with a field of view of 90\u25e6 that records Full HD video at 25Hz. The onboard software of the Tobii glasses conveniently precomputes the gaze point in the reference frame of the gaze camera, which is what we will use in the remainder of the paper. Figure 2 gives an overview of the acquisition setup.\nThe acquisition laptop was used to assign timestamps to sEMG and gaze samples in a shared reference time. These timestamps were used during preprocessing to synchronize all modalities and to upsample them to the sampling rate of sEMG. Furthermore, we filtered powerline interference and corrected the labels using the relabeling method described by Gijsberts et al. [19]."}, {"heading": "B. Classifier", "text": "Also our classification setup was inspired by [19], based on a Kernel Reguralized Least Squares (KRLS) classifier [20]. This learning method is a so-called kernel method, meaning that it approaches nonlinear problems by using kernel functions that implicitly map the original input space into a high-dimensional feature space. This also means that it is straightforward to use the multicue kernel described in section III in this classifier.\nBased on reports in previous work [19], we opted to combine the marginal Discrete Wavelet Transform (mDWT) representation for sEMG in a sliding window of 200ms with the exp-\u03c72 kernel function\nk\u03c72(x,y) = exp ( \u2212\u03b3\u03c72\nn\u2211 i=1 (xi \u2212 yi)2 xi + yi\n) for \u03b3\u03c72 > 0 .\n(5)\nFor the visual cue, we chose the standard Radial Basis Function (RBF) kernel function\nkrbf(x,y) = exp ( \u2212\u03b3rbf\u2016x\u2212 y\u20162 ) for \u03b3rbf > 0 .\nA linear kernel is typically sufficient for the representation at high levels of a CNN, but we prefer an RBF kernel to ensure that the outputs of both kernels in the combination are in the range [0, 1]. The multi-cue kernel combining the myoelectric and the visual cues becomes therefore\nkmc(x,y) = wemgk\u03c72(x,y) + wcnnkrbf(x,y).\nThe KRLS algorithm and the multi-cue kernel require the optimization of the regularization parameter \u03bb, the kernel-specific parameters \u03b3\u03c72 and \u03b3rbf, and the weights used in kernel combination wemg and wcnn. The parameters are optimized using k-fold cross-validation on the training set, where each of the folds corresponds to one of the movement repetitions used for training. The parameter ranges that we considered with a dense grid search are \u03bb \u2208 {2\u221214, 2\u221212, \u00b7 \u00b7 \u00b7 , 2\u22124}, \u03b3\u03c72 \u2208 {2\u221214, 2\u221212, \u00b7 \u00b7 \u00b7 , 2\u22128}, \u03b3rbf \u2208 {2\u221220, 2\u221218, \u00b7 \u00b7 \u00b7 , 2\u221214}, and wemg, wcnn \u2208 {0, 0.1, 0.2, \u00b7 \u00b7 \u00b7 , 1} such that wemg +wcnn = 1. The grids have been determined during preliminary analyses.\nThe grasp classification is repeated over four possible training/test splits of the database, such that each of the four repetitions of a movement is used once to test the\nmodel while the remaining three are used as training set. Subsequently, the prediction accuracy is averaged over the four splits. For computational reasons, we subsampled the test data at a factor 20, meaning that effectively we predict a sliding window with interval of 10ms. The training data instead was subsampled with an additional factor of 10, while the data used for hyperparameter optimization was subsampled with a factor of 10 \u00b7 4. Besides our multimodal classifier, we also include single cue classifiers as reference and a baseline that predicts simply the most common class in the training data. In our specific case, this means predicting always the \u201crest\u201d class, since this is the most common class due to our acquisition protocol."}, {"heading": "V. RESULTS", "text": "The goal of this section is to determine if the standard sEMG approach would benefit from the integration of the visual cues found by our algorithm. Figure 3 reports the average classification accuracy of the four classifiers for each subject. The sole visual cue does not produce a considerable improvement in accuracy with respect to the baseline, as the average improvement is of the 2% and it is mainly due to two of the five subjects. Nevertheless, the integration of vision to the muscular cue increases the average accuracy of more than 4% over that of the EMG classifier, and this appears to be a common trend for all the subjects. This result confirms our initial guess that the visual cue conveys complementary information with respect to the muscular one and that their integration improves the performance of the grasp classification task.\nThe contribution of each of the two cues to the multimodal classification is indicated by the values of the weights wEMG and wCNN that the algorithm automatically choose (during hyperparameter optimization) to combine the cues at the kernel level. Figure 4 reports the values of the kernel weights\nfor each subject and demonstrates how the contribution of the two cues is balanced, being around the 65% for the muscular cue and the 35% for the visual one on average.\nWe also analyze the distribution of the prediction error during the different phases of the prehension. Each prehension of the experiment has a different duration after relabeling, but always consists of a grasp preceded by a rest phase. In Figure 5, we report the prediction error with respect to the normalized duration of the rest phase ([\u22121, 0]) and subsequent grasp ([0,+1]). The addition of visual cues consistently reduces the prediction error during the grasp (t \u2208 [0, 1]). Relevantly, the most consistent reduction in prediction error due to the visual cue (around the 10%) happens at the onset and at the offset of the grasp. This indicates that vision compensates for the increased level of noise in the myoelectric signals during movement transitions. At the same time, the visual cue causes a slightly higher prediction error during the rest phase. This is because, generally, the visual information related to arm rest comes from the previous grasp and is, in fact, misleading for the classification of the \u201crest\u201d movement. As already explained in subsection III-B, this side-effect of the visual feature propagation is inevitable because we do not know in advance the duration of the grasps.\nQualitatively, the classification improvement obtained by integrating CNN and EMG can be observed by subtracting the confusion matrix of EMG+CNN to that of EMG. This difference is shown in Figure 6. The positive values on the diagonal indicate a uniform improvement of the classification accuracy for all the 10 grasps. However, the negative value at location (1,1) indicates an increase in rest misclassification and confirms the considerations made about the effect of holding the visual cues also during rest.\nIn tasks where the classification predictions form a temporal sequence, it is advisable to define performance metrics\nthat distinguish if errors are caused by misclassifications or delays in the predictions. This is useful, for instance, to observe the effects of smoothing the series of predicted movements via a majority vote. When the dimension k of the majority vote window is increased, the number of misclassifications generally decreases at the expense of a higher prediction delay. Standard classification accuracy fails to catch these competing effects, hence we will analyze the classifier performances also using the Movement Error Rate (MER) and the prediction delay, proposed by Gijsberts et al. [19]. The MER measures the similarity between the true and the predicted series of movements rather than considering the accuracy of the classification sample by sample. This quantity is insensitive to delays in the prediction, which are instead measured via the prediction delay, defined as the average time interval between a label change and the first correct prediction. Figure 7 represents the values of MER and delay achieved by the EMG (blue) and the EMG+CNN (red) classifiers when varying the length k of a majority vote window (k \u2208 {1, 3, 5, 11, 25, 50, 100, 150, 250}). The integration of visual features to muscular ones proves to reduce the MER consistently for all the considered values of k. In particular, for k < 50, EMG+CNN shows a halved MER with an unchanged prediction delay. This shows that the reduction in error of the EMG+CNN classifier does not come at the cost of increased delay."}, {"heading": "VI. CONCLUSIONS", "text": "This work demonstrated how standard sEMG based grasp classification benefits from the integration of the affordances of the manipulated objects. We proposed a method to automatically extract the object affordances from a first-person video recording of the scene and an estimate of the gaze position. The method identifies relevant gaze fixations on the\nbase of ocular and muscular activity. The objects observed during such fixations are segmented and their affordances are encoded into high-level visual features, extracted by an off-the-shelf Convolutional Neural Network. Despite we only conducted an offline evaluation of the method, the fixation detection has been designed to follow an online execution paradigm.\nThe method was evaluated on the data collected from intact subjects performing several of the most common grasps in activities of daily living. The acquisition protocol has been designed to simulate the prosthesis usage in a realistic environment. To ensure variability, we considered\ngrasps both in a static setting as well as when used to perform a functional task, while we took the limb position effect into account by repeating the movements while seated and standing. Furthermore, the same objects were associated to multiple grasps to enforce a many-to-many relationship between grasps and objects, and multiple objects were placed in the user\u2019s field of view to encourage realistic gaze behavior.\nOur tests confirmed that the integration of object affordances to the muscular activity of the forearm is indeed useful for grasp classification. The average prediction accuracy went from 80%, when using only the EMG cue, to 84%, when integrating EMG and vision. This improvement was considerable, as it involved uniformly all the subjects and all the grasp types. As expected, the contribution of vision was higher at the onset and the offset of the grasp, when the myoelectric cue is affected by motion artifacts. Finally, the analysis of the Movement Error Rate suggested that the performances of the multimodal classifier can be further reduced with a majority vote of the predictions at no expense of the prediction delay."}], "references": [{"title": "Myoelectric forearm prostheses: State of the art from a user-centered perspective", "author": ["B. Peerdeman", "D. Boere", "H. Witteveen", "R.H. in \u2018t Veld", "H. Hermens", "S. Stramigioli", "H. Rietman", "P. Veltink", "S. Misra"], "venue": "Journal of Rehabilitation Research and Development, vol. 48, no. 6, pp. 719\u2013738, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "A multi-modal approach for hand motion classification using surface emg and accelerometers", "author": ["A. Fougner", "E. Scheme", "A.D.C. Chan", "K. Englehart", "\u00d8. Stavdahl"], "venue": "Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2011, pp. 4247\u20134250.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Cognitive vision system for control of dexterous prosthetic hands: Experimental evaluation", "author": ["S. Do\u0161en", "C. Cipriani", "M. Kosti\u0107", "M. Controzzi", "M.C. Carrozza", "D.B. Popovi\u0107"], "venue": "Journal of NeuroEngineering and Rehabilitation, vol. 7, no. 1, p. 42, 2010. [Online]. Available: http: //dx.doi.org/10.1186/1743-0003-7-42", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Transradial prosthesis: Artificial vision for control of prehension", "author": ["S. Do\u0161en", "D.B. Popovi\u0107"], "venue": "Artificial Organs, vol. 35, no. 1, pp. 37\u201348, 2011. [Online]. Available: http://dx.doi.org/10.1111/j.1525-1594.2010.01040.x", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-modal sensing techniques for interfacing hand prostheses: A review", "author": ["Y. Fang", "Nalinda", "D. Zhou", "H. Liu"], "venue": "IEEE Sensors Journal, vol. 15, no. 11, pp. 6065\u20136076, Nov 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Noninvasive control interfaces for intention detection in active movement-assistive devices", "author": ["J. Lobo-Prat", "P.N. Kooren", "A.H. Stienen", "J.L. Herder", "B.F. Koopman", "P.H. Veltink"], "venue": "Journal of NeuroEngineering and Rehabilitation, vol. 11, no. 1, p. 168, 2014. [Online]. Available: http://dx.doi.org/10.1186/1743-0003-11-168", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting accelerometers to improve movement classification for prosthetics", "author": ["A. Gijsberts", "B. Caputo"], "venue": "2013 IEEE 13th International Conference on Rehabilitation Robotics (ICORR), June 2013, pp. 1\u20135.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Controlling handassistive devices: utilizing electrooculography as a substitute for vision", "author": ["Y. Hao", "M. Controzzi", "C. Cipriani", "D.B. Popovi\u0107", "X. Yang", "W. Chen", "X. Zheng", "M.C. Carrozza"], "venue": "IEEE Robotics Automation Magazine, vol. 20, no. 1, pp. 40\u201352, March 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Stereovision and augmented reality for closedloop control of grasping in hand prostheses", "author": ["M. Markovic", "S. Do\u0161en", "C. Cipriani", "D. Popovi\u0107", "D. Farina"], "venue": "Journal of  Neural Engineering, vol. 11, no. 4, p. 046001, 2014. [Online]. Available: http://stacks.iop.org/1741-2552/11/i=4/a=046001", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Sensor fusion and computer vision for contextaware control of a multi degree-of-freedom prosthesis", "author": ["M. Markovic", "S. Do\u0161en", "D. Popovi\u0107", "B. Graimann", "D. Farina"], "venue": "Journal of Neural Engineering, vol. 12, no. 6, p. 066022, 2015. [Online]. Available: http://stacks.iop.org/1741-2552/12/ i=6/a=066022", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "An exploratory study on the use of convolutional neural networks for object grasp classification", "author": ["G. Ghazaei", "A. Alameer", "P. Degenaar", "G. Morgan", "K. Nazarpour"], "venue": "2nd IET International Conference on Intelligent Signal Processing 2015 (ISP), Dec 2015, pp. 1\u20135.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal decoding and congruent sensory information enhance reaching performance in subjects with cervical spinal cord injury", "author": ["E.A. Corbett", "N.A. Sachs", "K.P. K\u00f6rding", "E.J. Perreault"], "venue": "Frontiers in Neuroscience, vol. 8, p. 123, 2014. [Online]. Available: http://journal.frontiersin.org/ article/10.3389/fnins.2014.00123", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Demonstration of a semi-autonomous hybrid brain-machine interface using human intracranial eeg, eye tracking, and computer vision to control a robotic upper limb prosthetic", "author": ["D.P. McMullen", "G. Hotson", "K.D. Katyal", "B.A. Wester", "M.S. Fifer", "T.G. Mcgee", "A. Harris", "M.S. Johannes", "R.J. Vogelstein", "A.D. Ravitz", "W.S. Anderson", "N.V. Thakor", "N.E. Crone"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 22, no. 4, pp. 784\u2013796, July 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Eye\u2013hand coordination in object manipulation", "author": ["R.S. Johansson", "G. Westling", "A. B\u00e4ckstr\u00f6m", "J.R. Flanagan"], "venue": "Journal of Neuroscience, vol. 21, no. 17, pp. 6917\u20136932, 2001.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Active visual segmentation", "author": ["A.K. Mishra", "Y. Aloimonos", "L.-F. Cheong", "A.A. Kassim"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 4, pp. 639\u2013 653, April 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, vol. abs/1409.1556, 2014. [Online]. Available: http://arxiv.org/abs/ 1409.1556", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative cue integration for medical image annotation", "author": ["T. Tommasi", "F. Orabona", "B. Caputo"], "venue": "Pattern Recognition Letters, vol. 29, no. 15, pp. 1996\u20132002, 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "The grasp taxonomy of human grasp types", "author": ["T. Feix", "J. Romero", "H.B. Schmiedmayer", "A.M. Dollar", "D. Kragic"], "venue": "IEEE Transactions on Human-Machine Systems, vol. 46, no. 1, pp. 66\u201377, Feb 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Movement error rate for evaluation of machine learning methods for semg-based hand movement classification", "author": ["A. Gijsberts", "M. Atzori", "C. Castellini", "H. M\u00fcller", "B. Caputo"], "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 22, no. 4, pp. 735\u2013744, 7 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Regularized least squares classification", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": "Advances in Learning Theory: Methods, Model and Applications, vol. 190. VIOS Press, 2003, pp. 131\u2013154.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Although advanced myoelectric prostheses have the potential to restore some of the lost functionality, their acceptance among amputees is very low [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "An interesting avenue is to reduce the dependency on surface electromyography (sEMG) by including other sources of contextual information, such as inertial sensors [2] or", "startOffset": 164, "endOffset": 167}, {"referenceID": 2, "context": "computer vision [3, 4].", "startOffset": 16, "endOffset": 22}, {"referenceID": 3, "context": "computer vision [3, 4].", "startOffset": 16, "endOffset": 22}, {"referenceID": 4, "context": "ses, such as sonomyography, mechanomyography, and force myography (for detailed overviews, see [5, 6]).", "startOffset": 95, "endOffset": 101}, {"referenceID": 5, "context": "ses, such as sonomyography, mechanomyography, and force myography (for detailed overviews, see [5, 6]).", "startOffset": 95, "endOffset": 101}, {"referenceID": 1, "context": "on arm orientation and dynamics that is complementary to sEMG [2, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 6, "context": "on arm orientation and dynamics that is complementary to sEMG [2, 7].", "startOffset": 62, "endOffset": 68}, {"referenceID": 2, "context": "These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation.", "startOffset": 27, "endOffset": 33}, {"referenceID": 3, "context": "These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation.", "startOffset": 27, "endOffset": 33}, {"referenceID": 7, "context": "These either used a webcam [3, 4] or electro-oculography [8] to automatically preshape the prosthesis based on the estimated object size and orientation.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "[9, 10].", "startOffset": 0, "endOffset": 7}, {"referenceID": 10, "context": "[11], who used deep learning to classify grasps based on the object\u2019s appearance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] use the subject\u2019s gaze to help to determine the target position of a reaching movement, while McMullen et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] combine this with computer vision to initiate and automatically perform the reach-grasp-drop motion of the robot arm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "A fixation consists of a period of time (generally between 350ms to 450ms [14]) where the eye-gaze remains in a limited area of the visual field.", "startOffset": 74, "endOffset": 78}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The object image is fed into the VGG-16 CNN pre-trained on ImageNet [16] and the activation of the second-last fullyconnected layer is taken as the image visual feature.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Among the possible methods to integrate multimodal cues, we opt for mid-level integration [17], also known as integration at the kernel level.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "We selected ten grasps based on relevant literature [18] and on their perceived importance for Activities of Daily Living (ADL).", "startOffset": 52, "endOffset": 56}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Also our classification setup was inspired by [19], based on a Kernel Reguralized Least Squares (KRLS) classifier [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "Also our classification setup was inspired by [19], based on a Kernel Reguralized Least Squares (KRLS) classifier [20].", "startOffset": 114, "endOffset": 118}, {"referenceID": 18, "context": "Based on reports in previous work [19], we opted to combine the marginal Discrete Wavelet Transform (mDWT) representation for sEMG in a sliding window of 200ms with the exp-\u03c7 kernel function", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "A linear kernel is typically sufficient for the representation at high levels of a CNN, but we prefer an RBF kernel to ensure that the outputs of both kernels in the combination are in the range [0, 1].", "startOffset": 195, "endOffset": 201}, {"referenceID": 0, "context": "The addition of visual cues consistently reduces the prediction error during the grasp (t \u2208 [0, 1]).", "startOffset": 92, "endOffset": 98}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "The instability of myoelectric signals over time complicates their use to control highly articulated prostheses. To address this problem, studies have tried to combine surface electromyography with modalities that are less affected by the amputation and environment, such as accelerometry or gaze information. In the latter case, the hypothesis is that a subject looks at the object he or she intends to manipulate and that knowing this object\u2019s affordances allows to constrain the set of possible grasps. In this paper, we develop an automated way to detect stable fixations and show that gaze information is indeed helpful in predicting hand movements. In our multimodal approach, we automatically detect stable gazes and segment an object of interest around the subject\u2019s fixation in the visual frame. The patch extracted around this object is subsequently fed through an off-the-shelf deep convolutional neural network to obtain a high level feature representation, which is then combined with traditional surface electromyography in the classification stage. Tests have been performed on a dataset acquired from five intact subjects who performed ten types of grasps on various objects as well as in a functional setting. They show that the addition of gaze information increases the classification accuracy considerably. Further analysis demonstrates that this improvement is consistent for all grasps and concentrated during the movement onset and offset.", "creator": "LaTeX with hyperref package"}}}