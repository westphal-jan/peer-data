{"id": "1709.02842", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Combining LSTM and Latent Topic Modeling for Mortality Prediction", "abstract": "there is a great disregard for algorithms thereby extensively limit the mortality of groups undergoing intensive care units with continuously satisfactory accuracy and accountability. we present joint worst - to - end neural network architectures along implement long full - term memory ( lstm ) in a latent topic model to simultaneously propose a classifier for disease prediction and learn robust topics data of mortality from textual clinical notes. for topic interpretability, specific computational modeling layer has been carefully applied maintaining a single - layer network describing constraints inspired by lda. principles affecting biomedical mimic - iii architecture stand whilst our research significantly leverage prior models yet are based on lda topics interpreting general decisions. primarily, we achieve limited probability testing better method for forcing inputs into the trained model if looking read the global network weights.", "histories": [["v1", "Fri, 8 Sep 2017 19:30:09 GMT  (888kb,D)", "http://arxiv.org/abs/1709.02842v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yohan jo", "lisa lee", "shruti palaskar"], "accepted": false, "id": "1709.02842"}, "pdf": {"name": "1709.02842.pdf", "metadata": {"source": "META", "title": "Combining LSTM and Latent Topic Modeling for Mortality Prediction ", "authors": ["Yohan Jo", "Lisa Lee", "Shruti Palaskar"], "emails": ["YOHANJ@CS.CMU.EDU", "LSLEE@CS.CMU.EDU", "SPALASKA@CS.CMU.EDU"], "sections": [{"heading": "1. Introduction", "text": "Many intensive care units (ICUs) suffer from a shortage of nurses and doctors to care for patients in critical conditions. As caregivers inevitably have to prioritize patients based on the severity of their conditions, it is essential to leverage patient data\u2014collected from laboratory tests and clinical notes\u2014to help determine the most efficient ICU resource allocation, e.g., by estimating patient mortality. The problem of mortality prediction involves several challenges. (1) Mortality prediction requires dealing with time series data, where a progressive analysis of the patients\u2019 conditions is preferred to cross-sectional analysis. While prior work has conducted time series analysis of clinical notes (Jo & Rose\u0301, 2015; Grnarova et al., 2016) and other measurements (Lipton et al., 2015) for predicting mortality/diagnoses, there is no standard methodology that yields high accuracy in mortality prediction. (2) Algorithmic accountability is also critical, as doctors cannot blindly accept the machine\u2019s decisions without knowing the rationales behind them. While\nProject for 10-708: Probabilistic Graphical Models. Spring 2017.\nneural network techniques have demonstrated promising performance in prediction tasks (Grnarova et al., 2016; Lipton et al., 2015), they often lack interpretability and suffer from data sparsity (e.g., text vocabulary), especially in the case of insufficient data.\nWe present two-layer joint models for mortality prediction that combine the advantages of long short-term memory (LSTM) and latent topic modeling. The LSTM layer captures long-range dependencies in sequential data, trained for mortality prediction, and this information propagates back to the topic modeling layer to learn topics that are predictive of mortality. We tried three different structures for the topic modeling layer. The Encoder only structure has a single-layer network that encodes a bag-of-words to a latent document vector (topic distribution) using a trainable word-topic weight matrix. The Encoder+Decoder structure reconstructs the input vector from the document vector using a trainable topic-word weight matrix that aims to associate each latent dimension with cohesive words. The Encoder+Transcoder+Decoder structure has an intermediate Transcoder layer that converts a document representation to a sparse vector to mimic the sparsity constraints in LDA.\nWe evaluate our model on the MIMIC-III dataset. The prediction accuracy of our models is compared to that of LDAbased models. The learned topics are evaluated by examining top words associated with each latent dimension. We also analyze the quality of learned topics in terms of their similarity to LDA topics."}, {"heading": "2. Background & Related Work", "text": ""}, {"heading": "2.1. Clinical Outcome Prediction", "text": "The three main types of clinical data that have been used for clinical outcome prediction are textual notes (Ghassemi et al., 2014; Jo & Rose\u0301, 2015; Grnarova et al., 2016), realvalued measurements (e.g., laboratory/physiologic measurements) (Lipton et al., 2015), and categorical measurements (e.g., medical codes).\nTextual clinical notes contain qualitative information that cannot be found in numeric measurements, such as insights from nurses and doctors, patients\u2019 progress, and social context (e.g., relationships with family and friends). Clinical\nar X\niv :1\n70 9.\n02 84\n2v 1\n[ cs\n.C L\n] 8\nS ep\n2 01\n7\nnotes were found to be helpful for long-term prediction, but not as much for short-term prediction (Jo & Rose\u0301, 2015). Numeric measurements provide useful insight into the patients current health condition and health record.\nSome of the main challenges in analyzing textual clinical notes include unstructured text, incomplete sentences/phrases, irregular use of language, abundant abbreviations, and rich medical jargons and their variations. These characteristics make it difficult to apply NLP tools, such as part-of-speech taggers, dependency parsers, named-entity recognition, etc. Topic modeling is one of the techniques applied to tackle these problems. Ghassemi et al. (2014) did a cross-sectional analysis of topics to predict mortality, and later, Jo and Rose\u0301 (2015) did a time series analysis using a joint model of HMM and LDA to find patients\u2019 latent states and state transition patterns.\nBoth studies offer good interpretability due to the topics learned from the notes, but the time series analysis with Markovian assumption in the latter study achieved only a minor improvement from the former cross-sectional analysis. Our model can hopefully capture more complex time dependencies in patient trajectories by using an LSTM. Recently, a joint model of LSTM and convolutional neural network (CNN) was used to predict mortality and found phrases that are highly related to mortality (Grnarova et al., 2016). This model outperforms the previous models, but it is cross-sectional and has limited interpretability. Our work may improve this model by introducing progression analysis and interpretable topics.\nFor real-valued time series measurements, Chia and Syed (2014) predicted mortality using the variability of ECG signals for each patient measured by dynamic time warping between every pair of consecutive heart beats. Chia and Syed (2013) also used time series heart rate patterns for mortality prediction, by binning each heart rate (per minute), clustering subsequences of the bins, and choosing clusters that are indicative of mortality and survival respectively. Recently, LSTM without feature engineering was used for diagnosis prediction, where 13 types of time series data (e.g., blood pressure, blood oxygen saturation) were resampled to an hourly rate by taking the mean value and then put into an LSTM for prediction (Lipton et al., 2015). However, this study found evidence that even simple statistics (e.g., max, min, mean) of the measurements throughout the entire timeline of a patient achieve almost comparable accuracy with a simple MLP."}, {"heading": "2.2. Neural Network for Topic Modeling", "text": "Our goal is to design a neural network architecture, where the upper layer is LSTM for predicting mortality, and the lower layer feeds the LSTM at each time point with a latent topic distribution learned from clinical notes. In this\nsection, we review prior work on modeling topic distributions using neural networks.\nOne of the simplest and earliest approaches is a restricted Boltzmann machine (Srivastava et al., 2013; Hinton & Salakhutdinov, 2009). The activation probability of each hidden node is a sigmoid function on a weighted sum of the frequency of individual words. Hence, a hidden node can be interpreted as a latent topic that has a weight associated with each word. Inspired by these models, a deep sigmoid belief network has been proposed to learn topic distributions in a supervised way given a bag-of-words, and there has been an attempt to interpret the hidden layers (Zhang et al., 2016). In the same vein, we try to interpret topics from the hidden layers of our joint models LSTM+E and LSTM+E+D.\nIn a more recent model, TopicRNN, the topic distribution of text is assumed to be drawn from a normal distribution whose mean and variance are computed from the input BoW using a neural network (Dieng et al., 2016). This topic distribution serves as the global semantics of the text, and is fed into an RNN to help predict the following word.\nTopics learned by the above models are based on unigrams, but general n-grams may capture better topics for some tasks, as shown in (Zhai & Boyd-Graber, 2013; Hardisty et al., 2010; Wallach, 2006). A neural network model has been proposed that learns the association between an arbitrary n-gram and topics (Cao et al., 2015). This model, however, takes individual n-grams of interest as separate inputs, which makes training tricky. Recently, hierarchical LSTM has been proposed (Chung et al., 2016). Our other joint model, the hierarchical LSTM, exploits this architecture and explores the idea of using a lower LSTM layer for learning topic distributions from a sequence of words without limiting itself to unigrams."}, {"heading": "3. Methods", "text": "Our task is to build a classifier for predicting a patent\u2019s mortality given his/her clinical notes written so far (Figure 1). Throughout the report, we define time points as 12 hourlong time segments of a patient\u2019s timeline from admission (e.g., time point 1 is the first 12 hours, time point 2 is the next 12 hours, etc.) (Ghassemi et al., 2014). For each patient, we aggregate all notes in each time point t into a bagof-words representation xt, normalized to sum to 1.\nIn this section, we present each of our models (see Fig. 2 for an overview). First, we present and compare two LDA baseline models to analyze the benefit of using LSTM over linear SVM for capturing long-term dependencies (Section 3.1). Then we present three end-to-end models which jointly learn topic models and mortality prediction (Section 3.2)."}, {"heading": "3.1. LDA baselines", "text": "Here, we present two baseline methods that use LDA, which has been used to infer topic distributions in textual clinical notes. Note that these models are not a joint model of topic modeling and mortality prediction, because they use a separate LDA model to train topics.\nSVM+LDA is the model proposed by Ghassemi et al. (2014). This model builds a linear SVM for each time point that predicts a patient\u2019s mortality given the patient\u2019s clinical notes written up to that time point. To obtain the\ninput for the classifier for time point t, we first compute the topic distribution of each note using LDA and then aggregate all topic distributions up to time point t by simply averaging the topic distribution vectors.\nLSTM+LDA replaces the linear SVM with LSTM, allowing us to evaluate the ability of LSTM to capture time dependencies for mortality prediction. The LSTM inputs a sequence of topic distribution vectors z = (z(1), . . . , z(T )), generates a predicted outcome y\u0302(t) at each time point t, and is trained to minimize the prediction lossH(y\u0302(t), y) over all time points, where y \u2208 {0, 1} is the mortality label of the patient and H(p, q) is the weighted cross-entropy defined as\nH(p, q) = \u2212CFNq log p\u2212 (1\u2212 q) log(1\u2212 p), (1)\nwhere CFN is the cost for false negative classification.\nUnlike the SVM baseline, LSTM+LDA does not build separate classifiers for different time points, and instead is trained to predict the correct outcome at any time point. By comparing these baselines, we try to establish whether an LSTM can infer relevant semantic information based on topic distributions. (As shown in Fig. 3, LSTM+LDA is more robust than SVM+LDA in predicting the mortality of patients who stay longer in the hospital.)"}, {"heading": "3.2. Joint Models", "text": "Next, we present three end-to-end models which jointly learn topic modeling and mortality prediction.\nThe LSTM Encoder (LSTM+E) model replaces the pretrained LDA topic model in LSTM+LDA with an Encoder (E) network. The Encoder is a single-layer neural network that is expected to find latent topics. Due to the Encoder structure, the Encoder weights \u03b8E are reminiscent of LDA word-topic weights in that we can interpret the Encoder as a graphical model that relates words in x(t) to the latent topics in z(t) (see Section 5.2 for our analysis).\nThe Encoder of LSTM+E does not guarantee that cohesive words fall into the same hidden node, which may make hidden nodes difficult to interpret. Hence, LSTM EncoderDecoder (LSTM+E+D) adds a Decoder (D), a single-layer network with no biases that tries to reconstruct the original bag-of-words data by minimizing the reconstruction loss H(x\u0302(t), x(t)) across all time points. Our rationale behind the Decoder is that the words with the highest weights for each hidden node are likely to be generated together in the same document through the Decoder, thus finding cohesive topics like LDA topics. We can interpret topics from the Decoder weights \u03b8D in the same way we do from the Encoder weights.\nOne potential drawback of LSTM+E+D is that latent document vectors z are tied to both the LSTM network and the\nDecoder, thereby being optimized neither for mortality nor for topic interpretation. To relax this tie, LSTM EncoderTranscoder-Decoder (LSTM+E+T+D) adds an intermediate Transcoder (T) that maps the latent vector z(t) to a sparse topic vector z\u0302(t), before the Decoder (D) converts z\u0302(t) to a bag-of-words vector x\u0302(t).\nInspired by LDA\u2019s sparsity constraints on the document\u2019s probability distribution over topics and the topic\u2019s probability distribution over words, we impose L1 regularizations on the learned topic vector z\u0302(t) and the Decoder\u2019s weights \u03b8D, respectively. Thus, our final loss function for LSTM+E+T+D is T\u2211 t=1 H(y\u0302(t), y)\ufe38 \ufe37\ufe37 \ufe38 prediction +\u03bb1H(x\u0302 (t), x(t))\ufe38 \ufe37\ufe37 \ufe38 reconstruction +\u03bb2\u2016z\u0302(t)\u20161 +\u03bb3\u2016\u03b8D\u20161 (2) where \u03bb1, \u03bb2, \u03bb3 are hyperparameters to control the weight of each loss.\nUnlike LSTM+LDA, which uses a pre-trained topic model to generate topic vectors for documents at each time point, these joint LSTM models are end-to-end in that they jointly learn the topic models and the contexts between documents. As a result, they learn a better latent document representation z(t) for mortality prediction and outperform LSTM+LDA on all three mortality prediction tasks (see Fig. 3)."}, {"heading": "4. Experiments", "text": ""}, {"heading": "4.1. Evaluation Metrics", "text": "Following Ghassemi et al. (2014; 2016), we evaluate our model on three mortality prediction tasks: in-hospital mortality, 30-day post-discharge mortality, and 1-year postdischarge mortality. These three tasks cover both shortterm and long-term mortality. We measure accuracy via the Area Under ROC Curve (AUC) metric (Rakotomamonjy, 2004), which captures how well a trained classifier discriminates between positive instances and negative instances."}, {"heading": "4.2. Data and Preprocessing", "text": "We use the MIMIC-III dataset which contains data about patients admitted to critical care units at a large tertiary care hospital (Johnson et al., 2016). For data preparation, we followed Ghassemi et al. (2014)\u2019s work as closely as we could, as their setting has been widely used (Jo & Rose\u0301, 2015; Grnarova et al., 2016). We also excluded patients who are less than 18 years old; these patients (mostly infants) show very different trajectories from adult patients (Jo & Rose\u0301, 2015). We used all textual clinical notes except discharge summaries because they explicitly mention the patient\u2019s outcomes. We randomly split pa-\ntients into training, validation, and test sets with the ratio of 6:2:2. Since the classes are severely skewed toward negative (i.e., survival), the negative instances in the training set are downsampled such that negative instances constitute no more than 70% of the training set. The validation and test sets are not downsampled.\nFor preprocessing of the text, we first normalized some text into categories to cluster meaningful text. We replace deidentified information in text with the given category (e.g., \u201c[** First Name 3 **]\u201d \u2192 \u201c##firstname##\u201d). We also replace times and numbers with \u201c##time##\u201d and \u201c#\u201d using regular expressions. Next, we tokenized the text with nonalphanumeric letters. We removed stop words using the Onix stop word list1.\nTo reduce the vocabulary size, we retained at most 500 words that have the highest tf-idf among all documents for each patient in the training set, and only included these words into the vocabulary. We excluded subjects from the training set if the total length of the clinical notes is less than 100 words. The statistics of the final data after preprocessing are listed in Table 1.\nFor our LSTM models, we handle missing time points by using zero vectors for documents x(t). Note that this setting does not affect the prediction and reconstruction loss."}, {"heading": "4.3. Models and Parameters", "text": "For the LDA baseline, the number of topics is set to 50 (Ghassemi et al., 2014). The cost C and the weight w for the positive class (\u201cdied\u201d) in libsvm are explored on the grid of C = {2\u22125, 2\u22123, 2\u22121, 21, 23, 25, 27, 29, 211, 213, 215} and w = {1, 3, 5, 7, 9} and tuned on the validation set. The weight for the negative class (\u201csurvived\u201d) is fixed to 1.\nThe network configuration of our models is summarized in Table 2. The batch size is 10, the number of training steps is 100,000, and the learning rate is 0.001. As in the SVM, we explored different false negative costs (Eq. 1) CFN = {20, 21, 22, 23} and chose the optimal value based on the validation set.\n1http://www.lextek.com/manuals/onix/ stopwords1.html\nFor the loss function (Eq 2), we explored \u03bb1 = {10\u22122, 10\u22121, 100, 101} and \u03bb2, \u03bb3 \u2208 {0, 1}. Ultimately, we found that \u03bb1 = 10\u22122, \u03bb2 = 0, \u03bb3 = 1 resulted in the highest AUC score for mortality prediction on the validation set, and used these settings for our experiments."}, {"heading": "5. Results", "text": "We evaluate our models on both mortality prediction (Section 5.1) and the quality of learned topics (Sections 5.2 and 5.3)."}, {"heading": "5.1. Mortality Prediction Accuracy", "text": "In Fig. 3, we plot the AUC score of each model for each time point on the three mortality prediction tasks. The joint models LSTM+E(+T)(+D) outperform the LDA baselines on all three mortality prediction tasks. For example, LSTM+E outperforms LSTM+LDA by about +4% (Hospital), +2% (30Days), +7% (1Year) on average.\nThe LSTM-based approaches show less accuracy drop over time than the SVM+LDA baseline, notably for the inhospital prediction. One of the reasons for SVM+LDA\u2019s performance drop is having fewer training instances for later time points, as patients who have died or been discharged at a certain time point are excluded from the training set (Ghassemi et al., 2014). According to the inhospital prediction, LSTM seems to be able to relieve this problem. We suspect that the way we define our cost function\u2014the average of losses at previous time points\u2014 might have compensated for the data loss. Another reason for performance drop is that it is more difficult to predict the destiny of patients who stay in an ICU for long. Long-term time dependencies captured by LSTM might have help make stable prediction.\nWe introduced the cost CFN for false negative classification to compensate for the fewer number of positive instances, but this cost had inconsistent effects. For example, we found that a cost greater than 1 improved the accuracy for in-hospital and 1-year post-discharge mortality prediction, but decreased the accuracy for the 30-day postdischarge prediction.\nThe latent document vectors learned by our joint models have a better ability to separate documents by mortality rate. This is demonstrated in Fig. 4, where we visualize the latent document vectors z(t) for each model, and see that the joint models, LSTM+E(+D), have documents with the same mortality label clustered more closely."}, {"heading": "5.2. Topic Interpretation", "text": "A popular method for qualitatively analyzing latent topics is to examine the top words of each topic. We expected each hidden node in the Encoder to represent a cohesive topic that is indicative of mortality. We tried interpreting the hidden nodes by examining the words that have the highest weights for each hidden node. As shown in Table 3b, the top words consist largely of typos and infrequent words and thus hardly represent an interpretable notion of topics compared to LDA topics in Table 3a. We conclude that (I) mortality is related with certain words individually rather than as a group like an LDA topic, and (II) the Encoder weights cannot find cohesive topics partly because words in the same document are not encouraged to be tied to the same hidden node.\nWe introduced the Decoder of LSTM+E+D to alleviate (II). We tried interpreting topics from the Decoder weights in the same way we did from the Encoder weights. As shown in Table 3c, the top words consist of more frequent words than do the top words from the Encoder weights. Yet, there still appear many typos and infrequent words, and the topics are quite difficult to interpret. This does not necessarily mean that topics are not cohesive. Due to the limited time, we could not further investigate the learned topics, which consist of a lot of medical terms and jargons. We leave this for future work.\nThe topics interpreted from the Decoder weights of LSTM+E+T+D are shown in Table 3d. We expected that the sparsity constraints for z\u0302(t) and \u03b8D imposed on this model would produce more interpretable, cohesive, LDAlike topics, but the learned topics do not show significant difference from those from LSTM+E+D. Moreover, we found that L1 regularization on z\u0302(t) caused the AUC to fall, while L1 regularization on the Decoder weights\n\u03b8D increased the AUC. However, these results might be due to using a suboptimal setting for topic modeling. We leave further experiments on finetuning \u03bb1, \u03bb2, \u03bb3 in Eq. (2) to balance the tradeoff between better mortality prediction performance and better latent topic modeling as future work.\nInterpreting topics from single-layer encoder and decoder turns out to be extremely difficult. The Encoder seems to pick important words predictive of mortality, instead of finding cohesive topics. The structure of the Decoder is similar to probabilistic latent semantic indexing (PLSI), which computes the probability of word w in document d as follows:\np(w|d) = \u2211 t p(w|t)p(t|d),\nwhere p(w|t) and p(t|d) correspond to the input vector and\nthe weights of the Decoder, respectively. We imposed sparsity on p(w|t) and p(t|d) in an attempt to make topics more interpretable, but we could not attain satisfactory topics. In order to make sure that the Decoder works as expected, we may need to run PLSI on the data and compare the result topics with those obtained from our models. The learned topics may not be interpretable simply because documents are noisy; each document is a concatenation of multiple clinical notes in the same time point that may be of very different types."}, {"heading": "5.3. Topic Quality", "text": "We evaluate the quality of learned topics by analyzing the t-SNE plots of the latent document vectors in Fig. 5. We visualized the LDA topic vectors and the latent vectors of LSTM+E(+D) with colors representing patients (Fig. 5). We focused on the documents in the blue box in the left\npane and looked at whether these documents are clustered closely in the LSTM+E (middle pane) and LSTM+E+D (right pane) plots. LSTM+E+D seems to produce a more similar clustering to LDA than LSTM+E does, and thus LSTM+E+D may outperform LSTM+E with respect to the first method. Note that we did not observe any significant difference in the quality of topic clusters learned by LSTM+E+D vs. by LSTM+E+T+D, so we omitted the plot for LSTM+E+T+D."}, {"heading": "6. Conclusion", "text": "We presented three joint models of LSTM and topic modeling that combine the benefits of both (1) LSTMs, which can capture time dependencies for mortality prediction, and (2) topic modeling, which can interpret topics predictive of mortality. Our models improved the accuracy of mortality prediction significantly from the baseline LDA-based models and were able to learn latent document representations indicative of mortality. We also proposed a method for interpreting topics from our models based on the Encoder and Decoder weights. However, the words with the highest weights for each hidden node did not provide interpretable, cohesive topics as LDA topics do. This may imply that LDA-like topics are suboptimal as feature for mortality prediction, and more indicative information is conveyed rather by certain individual words. We tried to make the Decoder work in a similar manner to PLSI and LDA by imposing constraints, but it turned out to be extremely difficult to obtain interpretable LDA-like topics from the Decoder. This noise might come from our concatenating multiple clinical notes of different types in the same time point.\nFuture work\nTo understand which words are indicative of mortality and survival, we can investigate which words trigger each hidden node in the LSTM. To do this, we calculate the derivative of the value of each hidden node in the LSTM cell in terms of each input word, while fixing the weights of the whole network after they are trained. By training in this way, our model may be able to generate a bag-of-words vector that maximizes each hidden node in the LSTM.\nWe observed that most of the top words for each hidden node (ranked by the Encoder or Decoder weights) are typos (e.g., \u201crecomendations\u201d, \u201cevenign\u201d) or rare words. To make these top words more interpretable, we suggest preprocessing the data in the following ways: (1) Remove the n most frequent and infrequent words. (2) Use a spellchecker to fix typos. Since electronic health records have many medical jargons, we want to be careful and only replace 1-character typos (e.g., correct \u201ccooeprative\u201d to \u201ccooperative\u201d).\nIn Section 5.3, we evaluated the quality of learned topics by analyzing the t-SNE plots of the latent document vectors. For future work, we suggest a method to quantitatively evaluate the learned topics by comparing the clusters they form to a gold standard. More specifically, for each latent vector, we identify k nearest neighbors and compute the overlap with gold standard neighbors of the vector. Since no ground truth quality scores are available, we tried two gold standards: LDA topics and patient identity. In the first method, we identify the k nearest neighbors for each LDA latent vector and compute the overlap with the k nearest neighbors of the same document vector for LSTM+E and\nLSTM+E+D, respectively. Document representations similar to LDA topics may be considered good in that they represent cohesive, interpretable topics, but setting the standard to LDA topics is not optimal, as our ultimate goal is to find more indicative topics than LDA topics. In the second method, we assume that each patient has consistent topics, and for each latent vector, we compute the percentage of the k nearest neighbor vectors that are from the same patient. The assumption may not hold, however, if a patient\u2019s symptoms and condition are diverse and change significantly over time.\nSince we interpret topics from the Decoder weights \u03b8D in LSTM+E(+T)+D, we can use arbitrarily deep Encoder and Transcoder networks to increase the complexity of our model, which may allow the model to learn better hidden representations for both topic modeling and mortality prediction. However, keep in mind that a more complex network requires more training data in order for the model to saturate."}], "references": [{"title": "A novel neural topic model and its supervised extension", "author": ["Cao", "Ziqiang", "Li", "Sujian", "Liu", "Yang", "Wenjie", "Ji", "Heng"], "venue": "In Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence, January 25-30,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Scalable noise mining in longterm electrocardiographic time-series to predict death following heart attacks", "author": ["C C Chia", "Z. Syed"], "venue": "In Proceedings of the 20th ACM SIGKDD international ", "citeRegEx": "Chia and Syed,? \\Q2014\\E", "shortCiteRegEx": "Chia and Syed", "year": 2014}, {"title": "Computationally Generated Cardiac Biomarkers: Heart Rate Patterns to Predict Death Following Coronary Attacks", "author": ["Chia", "Chih-chun", "Syed", "Zeeshan"], "venue": "In Proceedings of the 2011 SIAM International Conference on Data Mining,", "citeRegEx": "Chia et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chia et al\\.", "year": 2011}, {"title": "Hierarchical multiscale recurrent neural networks", "author": ["Chung", "Junyoung", "Ahn", "Sungjin", "Bengio", "Yoshua"], "venue": "CoRR, abs/1609.01704,", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Topicrnn: A recurrent neural network with long-range semantic dependency", "author": ["Dieng", "Adji B", "Wang", "Chong", "Gao", "Jianfeng", "Paisley", "John William"], "venue": "CoRR, abs/1611.01702,", "citeRegEx": "Dieng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dieng et al\\.", "year": 2016}, {"title": "Neural Document Embeddings for Intensive Care Patient Mortality Prediction", "author": ["Grnarova", "Paulina", "Schmidt", "Florian", "Hyland", "Stephanie L", "Eickhoff", "Carsten"], "venue": "cs.CL,", "citeRegEx": "Grnarova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grnarova et al\\.", "year": 2016}, {"title": "Modeling perspective using adaptor grammars", "author": ["Hardisty", "Eric", "Boyd-Graber", "Jordan", "Resnik", "Philip"], "venue": "EMNLP \u201910: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Hardisty et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hardisty et al\\.", "year": 2010}, {"title": "Time Series Analysis of Nursing Notes for Mortality Prediction via a State Transition Topic Model", "author": ["Jo", "Yohan", "Ros\u00e9", "Carolyn Penstein"], "venue": "Proceedings of the 24th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Jo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jo et al\\.", "year": 2015}, {"title": "MIMIC-III, a freely accessible critical care database", "author": ["Johnson", "Alistair E W", "Pollard", "Tom J", "Shen", "Lu", "Lehman", "Li-wei H", "Feng", "Mengling", "Ghassemi", "Mohammad", "Moody", "Benjamin", "Szolovits", "Peter", "Anthony Celi", "Leo", "Mark", "Roger G"], "venue": null, "citeRegEx": "Johnson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Learning to Diagnose with LSTM Recurrent Neural Networks", "author": ["Lipton", "Zachary C", "Kale", "David C", "Elkan", "Charles", "Wetzell", "Randall"], "venue": null, "citeRegEx": "Lipton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lipton et al\\.", "year": 2015}, {"title": "Optimizing Area Under Roc Curve with SVMs", "author": ["Rakotomamonjy", "Alain"], "venue": "The 1st workshop on ROC analysis in artificial intelligence,", "citeRegEx": "Rakotomamonjy and Alain.,? \\Q2004\\E", "shortCiteRegEx": "Rakotomamonjy and Alain.", "year": 2004}, {"title": "Modeling Documents with Deep Boltzmann Machines", "author": ["Srivastava", "Nitish", "Salakhutdinov", "Ruslan R", "Hinton", "Geoffrey E"], "venue": null, "citeRegEx": "Srivastava et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2013}, {"title": "Topic modeling: beyond bag-of-words", "author": ["H. Wallach"], "venue": "Proceedings of the 23rd international conference on ", "citeRegEx": "Wallach,? \\Q2006\\E", "shortCiteRegEx": "Wallach", "year": 2006}, {"title": "Online Latent Dirichlet Allocation with Infinite Vocabulary", "author": ["Zhai", "Ke", "Boyd-Graber", "Jordan L"], "venue": "Proceedings of the 30th International Conference on Ma- chine Learning,", "citeRegEx": "Zhai et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2013}, {"title": "Learning from LDA using deep neural networks", "author": ["Zhang", "Dongxu", "Luo", "Tianyi", "Wang", "Dong"], "venue": "In Natural Language Understanding and Intelligent Applications 5th CCF Conference on Natural Language Processing and Chinese Computing,", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 5, "context": "While prior work has conducted time series analysis of clinical notes (Jo & Ros\u00e9, 2015; Grnarova et al., 2016) and other measurements (Lipton et al.", "startOffset": 70, "endOffset": 110}, {"referenceID": 9, "context": ", 2016) and other measurements (Lipton et al., 2015) for predicting mortality/diagnoses, there is no standard methodology that yields high accuracy in mortality prediction.", "startOffset": 31, "endOffset": 52}, {"referenceID": 5, "context": "neural network techniques have demonstrated promising performance in prediction tasks (Grnarova et al., 2016; Lipton et al., 2015), they often lack interpretability and suffer from data sparsity (e.", "startOffset": 86, "endOffset": 130}, {"referenceID": 9, "context": "neural network techniques have demonstrated promising performance in prediction tasks (Grnarova et al., 2016; Lipton et al., 2015), they often lack interpretability and suffer from data sparsity (e.", "startOffset": 86, "endOffset": 130}, {"referenceID": 5, "context": "The three main types of clinical data that have been used for clinical outcome prediction are textual notes (Ghassemi et al., 2014; Jo & Ros\u00e9, 2015; Grnarova et al., 2016), realvalued measurements (e.", "startOffset": 108, "endOffset": 171}, {"referenceID": 9, "context": ", laboratory/physiologic measurements) (Lipton et al., 2015), and categorical measurements (e.", "startOffset": 39, "endOffset": 60}, {"referenceID": 5, "context": "Recently, a joint model of LSTM and convolutional neural network (CNN) was used to predict mortality and found phrases that are highly related to mortality (Grnarova et al., 2016).", "startOffset": 156, "endOffset": 179}, {"referenceID": 9, "context": ", blood pressure, blood oxygen saturation) were resampled to an hourly rate by taking the mean value and then put into an LSTM for prediction (Lipton et al., 2015).", "startOffset": 142, "endOffset": 163}, {"referenceID": 1, "context": "For real-valued time series measurements, Chia and Syed (2014) predicted mortality using the variability of ECG signals for each patient measured by dynamic time warping between every pair of consecutive heart beats.", "startOffset": 42, "endOffset": 63}, {"referenceID": 1, "context": "For real-valued time series measurements, Chia and Syed (2014) predicted mortality using the variability of ECG signals for each patient measured by dynamic time warping between every pair of consecutive heart beats. Chia and Syed (2013) also used time series heart rate patterns for mortality prediction, by binning each heart rate (per minute), clustering subsequences of the bins, and choosing clusters that are indicative of mortality and survival respectively.", "startOffset": 42, "endOffset": 238}, {"referenceID": 11, "context": "One of the simplest and earliest approaches is a restricted Boltzmann machine (Srivastava et al., 2013; Hinton & Salakhutdinov, 2009).", "startOffset": 78, "endOffset": 133}, {"referenceID": 14, "context": "Inspired by these models, a deep sigmoid belief network has been proposed to learn topic distributions in a supervised way given a bag-of-words, and there has been an attempt to interpret the hidden layers (Zhang et al., 2016).", "startOffset": 206, "endOffset": 226}, {"referenceID": 4, "context": "In a more recent model, TopicRNN, the topic distribution of text is assumed to be drawn from a normal distribution whose mean and variance are computed from the input BoW using a neural network (Dieng et al., 2016).", "startOffset": 194, "endOffset": 214}, {"referenceID": 6, "context": "Topics learned by the above models are based on unigrams, but general n-grams may capture better topics for some tasks, as shown in (Zhai & Boyd-Graber, 2013; Hardisty et al., 2010; Wallach, 2006).", "startOffset": 132, "endOffset": 196}, {"referenceID": 12, "context": "Topics learned by the above models are based on unigrams, but general n-grams may capture better topics for some tasks, as shown in (Zhai & Boyd-Graber, 2013; Hardisty et al., 2010; Wallach, 2006).", "startOffset": 132, "endOffset": 196}, {"referenceID": 0, "context": "A neural network model has been proposed that learns the association between an arbitrary n-gram and topics (Cao et al., 2015).", "startOffset": 108, "endOffset": 126}, {"referenceID": 3, "context": "Recently, hierarchical LSTM has been proposed (Chung et al., 2016).", "startOffset": 46, "endOffset": 66}, {"referenceID": 8, "context": "We use the MIMIC-III dataset which contains data about patients admitted to critical care units at a large tertiary care hospital (Johnson et al., 2016).", "startOffset": 130, "endOffset": 152}, {"referenceID": 5, "context": "(2014)\u2019s work as closely as we could, as their setting has been widely used (Jo & Ros\u00e9, 2015; Grnarova et al., 2016).", "startOffset": 76, "endOffset": 116}, {"referenceID": 7, "context": "We use the MIMIC-III dataset which contains data about patients admitted to critical care units at a large tertiary care hospital (Johnson et al., 2016). For data preparation, we followed Ghassemi et al. (2014)\u2019s work as closely as we could, as their setting has been widely used (Jo & Ros\u00e9, 2015; Grnarova et al.", "startOffset": 131, "endOffset": 211}], "year": 2017, "abstractText": "There is a great need for technologies that can predict the mortality of patients in intensive care units with both high accuracy and accountability. We present joint end-to-end neural network architectures that combine long short-term memory (LSTM) and a latent topic model to simultaneously train a classifier for mortality prediction and learn latent topics indicative of mortality from textual clinical notes. For topic interpretability, the topic modeling layer has been carefully designed as a single-layer network with constraints inspired by LDA. Experiments on the MIMIC-III dataset show that our models significantly outperform prior models that are based on LDA topics in mortality prediction. However, we achieve limited success with our method for interpreting topics from the trained models by looking at the neural network weights.", "creator": "LaTeX with hyperref package"}}}