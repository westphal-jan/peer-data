{"id": "0809.1493", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2008", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning", "abstract": "for supervised and unsupervised collective learning, positive definite kernels allow to use large and potentially infinite maximum dimensional feature spaces with a variable computational cost that only depends on the number balance of observations. this is usually done through the penalization of predictor functions by euclidean or functional hilbertian norms. in this paper, theoretically we explore penalizing by integrating sparsity - inducing memory norms such as the l1 - norm or the block l1 - norm. we assume that the kernel decomposes into a large sum of minimal individual basis kernels which can uniformly be embedded in a directed acyclic graph ; thus we show that typically it is then physically possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number component of selected kernels. this framework is naturally directly applied to non linear variable partition selection ; our extensive simulations on synthetic datasets and datasets from the uci repository show cases that efficiently feasible exploring the large feature loss space identified through sparsity - inducing norms leads to state - of - the - art predictive performance.", "histories": [["v1", "Tue, 9 Sep 2008 06:48:10 GMT  (53kb)", "http://arxiv.org/abs/0809.1493v1", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["francis r bach"], "accepted": true, "id": "0809.1493"}, "pdf": {"name": "0809.1493.pdf", "metadata": {"source": "CRF", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning", "authors": ["Francis Bach"], "emails": ["francis.bach@mines.org"], "sections": [{"heading": null, "text": "ar X\niv :0\n80 9.\n14 93\nv1 [\ncs .L"}, {"heading": "1 Introduction", "text": "In the last two decades, kernel methods have been a prolific theoretical and algorithmic machine learning framework. By using appropriate regularization by Hilbertian norms, representer theorems enable to consider large and potentially infinite-dimensional feature spaces while working within an implicit feature space no larger than the number of observations. This has led to numerous works on kernel design adapted to specific data types and generic kernel-based algorithms for many learning tasks (see, e.g., [1, 2]).\nRegularization by sparsity-inducing norms, such as the \u21131-norm has also attracted a lot of interest in recent years. While early work has focused on efficient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework [4].\nIn this paper, we aim to bridge the gap between these two lines of research by trying to use \u21131-norms inside the feature space. Indeed, feature spaces are large and we expect the estimated predictor function to require only a small number of features, which is exactly the situation where\n1\n\u21131-norms have proven advantageous. This leads to two natural questions that we try to answer in this paper: (1) Is it feasible to perform optimization in this very large feature space with cost which is polynomial in the size of the input space? (2) Does it lead to better predictive performance and feature selection?\nMore precisely, we consider a positive definite kernel that can be expressed as a large sum of positive definite basis or local kernels. This exactly corresponds to the situation where a large feature space is the concatenation of smaller feature spaces, and we aim to do selection among these many kernels, which may be done through multiple kernel learning [5]. One major difficulty however is that the number of these smaller kernels is usually exponential in the dimension of the input space and applying multiple kernel learning directly in this decomposition would be intractable.\nIn order to peform selection efficiently, we make the extra assumption that these small kernels can be embedded in a directed acyclic graph (DAG). Following [6, 7], we consider in Section 2 a specific combination of \u21132-norms that is adapted to the DAG, and will restrict the authorized sparsity patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of selected kernels (Section 3). In simulations (Section 5), we focus on directed grids, where our framework allows to perform non-linear variable selection. We provide extensive experimental validation of our novel regularization framework; in particular, we compare it to the regular \u21132-regularization and shows that it is always competitive and often leads to better performance, both on synthetic examples, and standard regression and classification datasets from the UCI repository.\nFinally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufficient conditions for model consistency. In particular, we show that our framework is adapted to estimating consistently only the hull of the relevant variables. Hence, by restricting the statistical power of our method, we gain computational efficiency."}, {"heading": "2 Hierarchical multiple kernel learning (HKL)", "text": "We consider the problem of predicting a random variable Y \u2208 Y \u2282 R from a random variable X \u2208 X , where X and Y may be quite general spaces. We assume that we are given n i.i.d. observations (xi, yi) \u2208 X \u00d7 Y , i = 1, . . . , n. We define the empirical risk of a function f from X to R as 1 n \u2211n i=1 \u2113(yi, f(xi)), where \u2113 : Y \u00d7 R 7\u2192 R+ is a loss function. We only assume that \u2113 is convex with respect to the second parameter (but not necessarily differentiable). Typical examples of loss functions are the square loss for regression, i.e., \u2113(y, y\u0302) = 12(y\u2212 y\u0302)2 for y \u2208 R, and the logistic loss \u2113(y, y\u0302) = log(1+ e\u2212yy\u0302) or the hinge loss \u2113(y, y\u0302) = max{0, 1\u2212yy\u0302} for binary classification, where y \u2208 {\u22121, 1}, leading respectively to logistic regression and support vector machines. Other losses may be used for other settings (see, e.g., [2] or the Appendix)."}, {"heading": "2.1 Graph-structured positive definite kernels", "text": "We assume that we are given a positive definite kernel k : X \u00d7 X \u2192 R, and that this kernel can be expressed as the sum, over an index set V , of basis kernels kv, v \u2208 V , i.e, for all x, x\u2032 \u2208 X , k(x, x\u2032) = \u2211\nv\u2208V kv(x, x \u2032). For each v \u2208 V , we denote by Fv and \u03a6v the feature space and feature\nmap of kv, i.e., for all x, x\u2032 \u2208 X , kv(x, x\u2032) = \u3008\u03a6v(x),\u03a6v(x\u2032)\u3009. Throughout the paper, we denote\n2\nby \u2016u\u2016 the Hilbertian norm of u and by \u3008u, v\u3009 the associated dot product, where the precise space is omitted and can always be inferred from the context.\nOur sum assumption corresponds to a situation where the feature map \u03a6(x) and feature space F for k is the concatenation of the feature maps \u03a6v(x) for each kernel kv, i.e, F = \u220f\nv\u2208V Fv and \u03a6(x) = (\u03a6v(x))v\u2208V . Thus, looking for a certain \u03b2 \u2208 F and a predictor function f(x) = \u3008\u03b2,\u03a6(x)\u3009 is equivalent to looking jointly for \u03b2v \u2208 Fv, for all v \u2208 V , and f(x) = \u2211\nv\u2208V \u3008\u03b2v,\u03a6v(x)\u3009. As mentioned earlier, we make the assumption that the set V can be embedded into a directed acyclic graph. Directed acyclic graphs (referred to as DAGs) allow to naturally define the notions of parents, children, descendants and ancestors. Given a node w \u2208 V , we denote by A(w) \u2282 V the set of its ancestors, and by D(w) \u2282 V , the set of its descendants. We use the convention that any w is a descendant and an ancestor of itself, i.e., w \u2208 A(w) and w \u2208 D(w). Moreover, for W \u2282 V , we let denote sources(W ) the set of sources of the graph G restricted to W (i.e., nodes in W with no parents belonging to W ). Given a subset of nodes W \u2282 V , we can define the hull of W as the union of all ancestors of w \u2208 W , i.e., hull(W ) = \u22c3w\u2208W A(w). Given a set W , we define the set of extreme points of W as the smallest subset T \u2282 W such that hull(T ) = hull(W ) (note that it is always well defined, as \u22c2\nT\u2282V, hull(T )=hull(W ) T ). See Figure 1 for examples of these notions. The goal of this paper is to perform kernel selection among the kernels kv , v \u2208 V . We essentially use the graph to limit the search to specific subsets of V . Namely, instead of considering all possible subsets of active (relevant) vertices, we are only interested in estimating correctly the hull of these relevant vertices; in Section 2.2, we design a specific sparsity-inducing norms adapted to hulls.\nIn this paper, we primarily focus on kernels that can be expressed as \u201cproducts of sums\u201d, and on the associated p-dimensional directed grids, while noting that our framework is applicable to many other kernels. Namely, we assume that the input space X factorizes into p components X = X1 \u00d7 \u00b7 \u00b7 \u00b7\u00d7Xp and that we are given p sequences of length q+1 of kernels kij(xi, x\u2032i), i \u2208 {1, . . . , p}, j \u2208 {0, . . . , q}, such that k(x, x\u2032) =\n\u2211q j1,...,jp=0 \u220fp i=1 kiji(xi, x \u2032 i) = \u220fp i=1\n(\n\u2211q ji=0 kiji(xi, x \u2032 i) ) . We\nthus have a sum of (q+1)p kernels, that can be computed efficiently as a product of p sums. A natural DAG on V =\n\u220fp i=1{0, . . . , q} is defined by connecting each (j1, . . . , jp) to (j1+1, j2, . . . , jp),\n. . . , (j1, . . . , jp\u22121, jp+1). As shown in Section 2.2, this DAG will correspond to the constraint of selecting a given product of kernels only after all the subproducts are selected. Those DAGs are especially suited to nonlinear variable selection, in particular with the polynomial and Gaussian\n3\nkernels. In this context, products of kernels correspond to interactions between certain variables, and our DAG implies that we select an interaction only after all sub-interactions were already selected.\nPolynomial kernels We consider Xi = R, kij(xi, x\u2032i) = (q j ) (xix \u2032 i) j ; the full kernel is then\nequal to k(x, x\u2032) = \u220fp\ni=1 \u2211q j=0 (q j ) (xix \u2032 i) j = \u220fp i=1(1 + xix \u2032 i) q. Note that this is not exactly\nthe usual polynomial kernel (whose feature space is the space of multivariate polynomials of total degree less than q), since our kernel considers polynomials of maximal degree q.\nGaussian kernels We also consider Xi = R, and the Gaussian-RBF kernel e\u2212b(x\u2212x \u2032)2 . The following decomposition is the eigendecomposition of the non centered covariance operator for a normal distribution with variance 1/4a (see, e.g., [8]):\ne\u2212b(x\u2212x \u2032)2 = \u2211\u221e k=0\n(b/A)k\n2kk! [e\u2212\nb A (a+c)x2Hk( \u221a 2cx)][e\u2212 b A (a+c)(x\u2032)2Hk( \u221a 2cx\u2032)],\nwhere c2 = a2 + 2ab, A = a + b + c, and Hk is the k-th Hermite polynomial. By appropriately truncating the sum, i.e, by considering that the first q basis kernels are obtained from the first q single Hermite polynomials, and the (q + 1)-th kernel is summing over all other kernels, we obtain a decomposition of a uni-dimensional Gaussian kernel into q + 1 components (q of them are one-dimensional, the last one is infinite-dimensional, but can be computed by differencing). The decomposition ends up being close to a polynomial kernel of infinite degree, modulated by an exponential [2]. One may also use an adaptive decomposition using kernel PCA (see, e.g., [2, 1]), which is equivalent to using the eigenvectors of the empirical covariance operator associated with the data (and not the population one associated with the Gaussian distribution with same variance). In simulations, we tried both with no significant differences.\nFinally, by taking product over all variables, we obtain a decomposition of the p-dimensional Gaussian kernel into (q + 1)p components, that are adapted to nonlinear variable selection. Note that for q = 1, we obtain ANOVA-like decompositions [2].\nKernels or features? In this paper, we emphasize the kernel view, i.e., we are given a kernel (and thus a feature space) and we explore it using \u21131-norms. Alternatively, we could use the feature view, i.e., we have a large structured set of features that we try to select from; however, the techniques developed in this paper assume that (a) each feature might be infinite-dimensional and (b) that we can sum all the local kernels efficiently (see in particular Section 3.2). Following the kernel view thus seems slightly more natural."}, {"heading": "2.2 Graph-based structured regularization", "text": "Given \u03b2 \u2208 \u220fv\u2208V Fv, the natural Hilbertian norm \u2016\u03b2\u2016 is defined through \u2016\u03b2\u20162 = \u2211 v\u2208V \u2016\u03b2v\u20162. Penalizing with this norm is efficient because summing all kernels kv is assumed feasible in polynomial time and we can bring to bear the usual kernel machinery; however, it does not lead to sparse solutions, where many \u03b2v will be exactly equal to zero.\nAs said earlier, we are only interested in the hull of the selected elements \u03b2v \u2208 Fv, v \u2208 V ; the hull of a set I is characterized by the set of v, such that D(v) \u2282 Ic, i.e., such that all descendants of v are in the complement Ic: hull(I) = {v \u2208 V,D(v) \u2282 Ic}c. Thus, if we try to estimate hull(I), we need to determine which v \u2208 V are such that D(v) \u2282 Ic. In our context, we are hence looking at selecting vertices v \u2208 V for which \u03b2D(v) = (\u03b2w)w\u2208D(v) = 0.\nWe thus consider the following structured block \u21131-norm defined as \u2211\nv\u2208V\ndv\u2016\u03b2D(v)\u2016 = \u2211\nv\u2208V\ndv( \u2211\nw\u2208D(v)\n\u2016\u03b2w\u20162)1/2,\n4\nwhere (dv)v\u2208V are positive weights. Penalizing by such a norm will indeed impose that some of the vectors \u03b2D(v) \u2208 \u220f\nw\u2208D(v) Fw are exactly zero. We thus consider the following minimization problem1:\nmin\u03b2\u2208 Q v\u2208V Fv 1 n \u2211n i=1 \u2113(yi, \u2211 v\u2208V \u3008\u03b2v ,\u03a6v(xi)\u3009) + \u03bb2 ( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016 )2 . (1)\nOur Hilbertian norm is a Hilbert space instantiation of the hierarchical norms recently introduced by [6]. If all Hilbert spaces are finite dimensional, our particular choice of norms corresponds to an \u201c\u21131-norm of \u21132-norms\u201d. While with uni-dimensional groups or kernels, the \u201c\u21131-norm of \u2113\u221e-norms\u201d allows an efficient path algorithm for the square loss and when the DAG is a tree [6], this is not possible anymore with groups of size larger than one, or when the DAG is a not a tree. In Section 3, we propose a novel algorithm to solve the associated optimization problem in time polynomial in the number of selected groups or kernels, for all group sizes, DAGs and losses. Moreover, in Section 4, we show under which conditions a solution to the problem in Eq. (1) consistently estimates the hull of the sparsity pattern.\nFinally, note that in certain settings (finite dimensional Hilbert spaces and distributions with absolutely continuous densities), these norms have the effect of selecting a given kernel only after all of its ancestors [6]. This is another explanation why hulls end up being selected, since to include a given vertex in the models, the entire set of ancestors must also be selected."}, {"heading": "3 Optimization problem", "text": "In this section, we give optimality conditions for the problems in Eq. (1), as well as optimization algorithms with polynomial time complexity in the number of selected kernels. In simulations we consider total numbers of kernels larger than 1030, and thus such efficient algorithms are essential to the success of hierarchical multiple kernel learning (HKL)."}, {"heading": "3.1 Reformulation in terms of multiple kernel learning", "text": "Following [9, 10], we can simply derive an equivalent formulation of Eq. (1). Using CauchySchwarz inequality, we have that for all \u03b7 \u2208 RV such that \u03b7 > 0 and \u2211v\u2208V d2v\u03b7v 6 1,\n( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016)2 6 \u2211 v\u2208V \u2016\u03b2D(v)\u2016\n2\n\u03b7v =\n\u2211 w\u2208V ( \u2211 v\u2208A(w) \u03b7 \u22121 v )\u2016\u03b2w\u20162,\nwith equality if and only if \u03b7v = d\u22121v \u2016\u03b2D(v)\u2016( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016)\u22121. We associate to the vector \u03b7 \u2208 RV , the vector \u03b6 \u2208 RV such that \u2200w \u2208 V , \u03b6\u22121w = \u2211 v\u2208A(w) \u03b7 \u22121 v . We use the natural convention that if \u03b7v is equal to zero, then \u03b6w is equal to zero for all descendants w of v. We let denote H the set of allowed \u03b7 and Z the set of all associated \u03b6 . The set H and Z are in bijection, and we can interchangeably use \u03b7 \u2208 H or the corresponding \u03b6(\u03b7) \u2208 Z . Note that Z is in general not convex (unless the DAG is a tree, see the Appendix), and if \u03b6 \u2208 Z , then \u03b6w 6 \u03b6v for all w \u2208 D(v), i.e., weights of descendant kernels are smaller, which is consistent with the known fact that kernels should always be selected after all their ancestors.\nThe problem in Eq. (1) is thus equivalent to\nmin \u03b7\u2208H min \u03b2\u2208 Q\nv\u2208V Fv\n1 n \u2211n i=1 \u2113(yi, \u2211 v\u2208V \u3008\u03b2v ,\u03a6v(xi)\u3009) + \u03bb2 \u2211 w\u2208V \u03b6w(\u03b7) \u22121\u2016\u03b2w\u20162. (2)\n1Following [5], we consider the square of the norm, which does not change the regularization properties, but allow simple links with multiple kernel learning.\n5\nUsing the change of variable \u03b2\u0303v = \u03b2v\u03b6 \u22121/2 v and \u03a6\u0303(x) = (\u03b6 1/2 v \u03a6v(x))v\u2208V , this implies that given the optimal \u03b7 (and associated \u03b6), \u03b2 corresponds to the solution of the regular supervised learning problem with kernel matrix K = \u2211\nw\u2208V \u03b6wKw, where Kw is n \u00d7 n the kernel matrix associated with kernel kw. Moreover, the solution is then \u03b2w = \u03b6w \u2211n i=1 \u03b1i\u03a6w(xi), where \u03b1 \u2208 Rn are the dual parameters associated with the single kernel learning problem. Thus, the solution is entirely determined by \u03b1 \u2208 Rn and \u03b7 \u2208 RV (and its corresponding \u03b6 \u2208 R V ). More precisely, we have (see proof in the Appendix):\nProposition 1 The pair (\u03b1, \u03b7) is optimal for Eq. (1), with \u2200w, \u03b2w = \u03b6w \u2211n\ni=1 \u03b1i\u03a6w(xi), if and only if (a) given \u03b7, \u03b1 is optimal for the single kernel learning problem with kernel matrix K = \u2211\nw\u2208V \u03b6w(\u03b7)Kw , and (b) given \u03b1, \u03b7 \u2208 H maximizes \u2211\nw\u2208V\n( \u2211\nv\u2208A(w)\n\u03b7\u22121v ) \u22121\u03b1\u22a4Kw\u03b1.\nMoreover, the total duality gap can be upperbounded as the sum of the two separate duality gaps for the two optimization problems, which will be useful in Section 3.2 (see Appendix for more details). Note that in the case of \u201cflat\u201d regular multiple kernel learning, where the DAG has no edges, we obtain back usual optimality conditions [9, 10].\nFollowing a common practice for convex sparsity problems [11], we will try to solve a small problem where we assume we know the set of v such that \u2016\u03b2D(v)\u2016 is equal to zero (Section 3.3). We then \u201csimply\u201d need to check that variables in that set may indeed be left out of the solution. In the next section, we show that this can be done in polynomial time although the number of kernels to consider leaving out is exponential (Section 3.2)."}, {"heading": "3.2 Conditions for global optimality of reduced problem", "text": "We let denote J the complement of the set of norms which are set to zero. We thus consider the optimal solution \u03b2 of the reduced problem (on J), namely,\nmin\u03b2J\u2208 Q v\u2208JFv 1 n \u2211n i=1 \u2113(yi, \u2211 v\u2208J \u3008\u03b2v,\u03a6v(xi)\u3009) + \u03bb2 ( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2229J\u2016 )2 , (3)\nwith optimal primal variables \u03b2J , dual variables \u03b1 and optimal pair (\u03b7J , \u03b6J). We now consider necessary conditions and sufficient conditions for this solution (augmented with zeros for non active variables, i.e., variables in Jc) to be optimal with respect to the full problem in Eq. (1). We denote by \u03b4 = \u2211\nv\u2208J dv\u2016\u03b2D(v)\u2229J\u2016 the optimal value of the norm for the reduced problem.\nProposition 2 (NJ ) If the reduced solution is optimal for the full problem in Eq. (1) and all kernels in the extreme points of J are active, then we have\nmax t\u2208sources(Jc)\n\u03b1\u22a4Kt\u03b1/d 2 t 6 \u03b4 2.\nProposition 3 (SJ,\u03b5) If maxt\u2208sources(Jc) \u2211 w\u2208D(t) \u03b1 \u22a4Kw\u03b1/( \u2211 v\u2208A(w)\u2229D(t) dv) 2 6 \u03b42+\u03b5/\u03bb, then the total duality gap is less than \u03b5.\n6\nThe proof is fairly technical and can be found in the Appendix; this result constitutes the main technical contribution of the paper: it essentially allows to solve a very large optimization problem over exponentially many dimensions in polynomial time.\nThe necessary condition (NJ) does not cause any computational problems. However, the sufficient condition (SJ,\u03b5) requires to sum over all descendants of the active kernels, which is impossible in practice (as shown in Section 5, we consider V of cardinal often greater than 1030). Here, we need to bring to bear the specific structure of the kernel k. In the context of directed grids we consider in this paper, if dv can also be decomposed as a product, then \u2211\nv\u2208A(w)\u2229D(t) dv is also factorized, and we can compute the sum over all v \u2208 D(t) in linear time in p. Moreover we can cache the sums \u2211\nw\u2208D(t) Kw/( \u2211 v\u2208A(w)\u2229D(t) dv) 2 in order to save running time."}, {"heading": "3.3 Dual optimization for reduced or small problems", "text": "When kernels kv, v \u2208 V have low-dimensional feature spaces, we may use a primal representation and solve the problem in Eq. (1) using generic optimization toolboxes adapted to conic constraints (see, e.g., [12]). However, in order to reuse existing optimized supervised learning code and use high-dimensional kernels, it is preferable to use a dual optimization. Namely, we use the same technique as [9]: we consider for \u03b6 \u2208 Z , the function B(\u03b6) = min\u03b2\u2208Qv\u2208V Fv 1 n \u2211n i=1 \u2113(yi, \u2211\nv\u2208V \u3008\u03b2v,\u03a6v(xi)\u3009)+ \u03bb 2 \u2211 w\u2208V \u03b6 \u22121 w \u2016\u03b2w\u20162, which is the optimal value of the single kernel learning problem with kernel matrix \u2211\nw\u2208V \u03b6wKw. Solving Eq. (2) is equivalent to minimizing B(\u03b6(\u03b7)) with respect to \u03b7 \u2208 H . If a ridge (i.e., positive diagonal) is added to the kernel matrices, the function B is differentiable. Moreover, the function \u03b7 7\u2192 \u03b6(\u03b7) is differentiable on (R\u2217+)V . Thus, the function \u03b7 7\u2192 B[\u03b6((1 \u2212 \u03b5)\u03b7 + \u03b5|V |d\n\u22122)] , where d\u22122 is the vector with elements d\u22122v , is differentiable if \u03b5 > 0. We can then use the same projected gradient descent strategy as [9] to minimize it. The overall complexity of the algorithm is then proportional to O(|V |n2)\u2014to form the kernel matrices\u2014plus the complexity of solving a single kernel learning problem\u2014typically between O(n2) and O(n3)."}, {"heading": "3.4 Kernel search algorithm", "text": "We are now ready to present the detailed algorithm which extends the feature search algorithm of [11]. Note that the kernel matrices are never all needed explicitly, i.e., we only need them (a) explicitly to solve the small problems (but we need only a few of those) and (b) implicitly to compute the sufficient condition (SJ,\u03b5), which requires to sum over all kernels, as shown in Section 3.2.\n\u2022 Input: kernel matrices Kv \u2208 Rn\u00d7n, v \u2208 V , maximal gap \u03b5, maximal # of kernels Q \u2022 Algorithm\n1. Initialization: set J = sources(V ), compute (\u03b1, \u03b7) solutions of Eq. (3), obtained using Section 3.3\n2. while (NJ) and (SJ,\u03b5) are not satisfied and #(V ) 6 Q\n\u2013 If (NJ) is not satisfied, add violating variables in sources(Jc) to J else, add violating variables in sources(Jc) of (SJ,\u03b5) to J \u2013 Recompute (\u03b1, \u03b7) optimal solutions of Eq. (3)\n\u2022 Output: J , \u03b1, \u03b7\n7\nThe previous algorithm will stop either when the duality gap is less than \u03b5 or when the maximal number of kernels Q has been reached. In practice, when the weights dv increase with the depth of v in the DAG (which we use in simulations), the small duality gap generally occurs before we reach a problem larger than Q. Note that some of the iterations only increase the size of the active sets to check the sufficient condition for optimality; forgetting those does not change the solution, only the fact that we may actually know that we have an \u03b5-optimal solution.\nIn the directed p-grid case, the total running time complexity is a function of the number of observations n, and the number R of selected kernels; with proper caching, we obtain the following complexity, assuming O(n3) for the single kernel learning problem, which is conservative: O(n3R+n2Rp2+n2R2p), which decomposes into solving O(R) single kernel learning problems, caching O(Rp) kernels, and computing O(R2p) quadratic forms for the sufficient conditions. Note that the kernel search algorithm is also an efficient algorithm for unstructured MKL."}, {"heading": "4 Consistency conditions", "text": "As said earlier, the sparsity pattern of the solution of Eq. (1) will be equal to its hull, and thus we can only hope to obtain consistency of the hull of the pattern, which we consider in this section.\nFor simplicity, we consider the case of finite dimensional Hilbert spaces (i.e., Fv = Rfv ) and the square loss. We also hold fixed the vertex set of V , i.e., we assume that the total number of features is fixed, and we let n tend to infinity and \u03bb = \u03bbn decrease with n.\nFollowing [4], we make the following assumptions on the underlying joint distribution of (X,Y ): (a) the joint covariance matrix \u03a3 of (\u03a6(xv))v\u2208V (defined with appropriate blocks of size fv \u00d7 fw) is invertible, (b) E(Y |X) = \u2211\nw\u2208W \u3008\u03b2w,\u03a6w(x)\u3009 with W \u2282 V and var(Y |X) = \u03c32 > 0 almost surely. With these simple assumptions, we obtain (see proof in the Appendix):\nProposition 4 (Sufficient condition) If we have\nmax t\u2208sources(Wc)\n\u2211\nw\u2208D(t)\n\u2016\u03a3wW \u03a3 \u22121 W W Diag(dv\u2016\u03b2D(v)\u2016 \u22121)v\u2208W \u03b2W \u2016 2\n( P v\u2208A(w)\u2229D(t) dv) 2 < 1,\nthen \u03b2 and the hull of W are consistently estimated when \u03bbnn1/2 \u2192 \u221e and \u03bbn \u2192 0.\nProposition 5 (Necessary condition) If the \u03b2 and the hull of W are consistently estimated for some sequence \u03bbn, then\nmax t\u2208sources(Wc)\n\u2016\u03a3wW\u03a3\u22121WW Diag(dv/\u2016\u03b2D(v)\u2016)v\u2208W\u03b2W\u20162/d2t 6 1.\nNote that the last two propositions are not consequences of the similar results for flat MKL [4], because the groups that we consider are overlapping. Moreover, the last propositions show that we indeed can estimate the correct hull of the sparsity pattern if the sufficient condition is satisfied. In particular, if we can make the groups such that the between-group correlation is as small as possible, we can ensure correct hull selection. Finally, it is worth noting that if the ratios dw/maxv\u2208A(w) dv tend to infinity slowly with n, then we always consistently estimate the depth of the hull, i.e., the optimal interaction complexity. We are currently investigating extensions to the non parametric case [4], in terms of pattern selection and universal consistency.\n8"}, {"heading": "5 Simulations", "text": "Synthetic examples We generated regression data as follows: n = 1024 samples of p \u2208 [22, 27] variables were generated from a random covariance matrix, and the label y \u2208 R was sampled as a random sparse fourth order polynomial of the input variables (with constant number of monomials). We then compare the performance of our hierarchical multiple kernel learning method (HKL) with the polynomial kernel decomposition presented in Section 2 to other methods that use the same kernel and/or decomposition: (a) the greedy strategy of selecting basis kernels one after the other, a procedure similar to [13], and (b) the regular polynomial kernel regularization with the full kernel (i.e., the sum of all basis kernels). In Figure 2, we compare the two approaches on 40 replications in the following two situations: original data (left) and rotated data (right), i.e., after the input variables were transformed by a random rotation (in this situation, the generating polynomial is not sparse anymore). We can see that in situations where the underlying predictor function is sparse (left), HKL outperforms the two other methods when the total number of variables p increases, while in the other situation where the best predictor is not sparse (right), it performs only slightly better: i.e., in non sparse problems, \u21131-norms do not really help, but do help a lot when sparsity is expected.\nUCI datasets For regression datasets, we compare HKL with polynomial (degree 4) and Gaussian-RBF kernels (each dimension decomposed into 9 kernels) to the following approaches with the same kernel: regular Hilbertian regularization (L2), same greedy approach as earlier (greedy), regularization by the \u21131-norm directly on the vector \u03b1, a strategy which is sometimes used in the context of sparse kernel learning [14] but does not use the Hilbertian structure of the kernel (lasso-\u03b1), multiple kernel learning with the p kernels obtained by summing all kernels associated with a single variable, a strategy suggested by [5] (MKL). For all methods, the kernels were held fixed, while in Table 1, we report the performance for the best regularization parameters obtained by 10 random half splits.\nWe can see from Table 1, that HKL outperforms other methods, in particular for the datasets bank-32nm, bank-32nh, pumadyn-32nm, pumadyn-32nh, which are datasets dedicated to non linear regression. Note also, that we efficiently explore DAGs with very large numbers of vertices #(V ).\nFor binary classification datasets, we compare HKL (with the logistic loss) to two other methods (L2, greedy) in Table 2. For some datasets (e.g., spambase), HKL works better, but for some others, in particular when the generating problem is known to be non sparse (ringnorm, twonorm), it performs slightly worse than other approaches.\n9\n10"}, {"heading": "6 Conclusion", "text": "We have shown how to perform hierarchical multiple kernel learning (HKL) in polynomial time in the number of selected kernels. This framework may be applied to many positive definite kernels and we have focused on polynomial and Gaussian kernels used for nonlinear variable selection. In particular, this paper shows that trying to use \u21131-type penalties may be advantageous inside the feature space. We are currently investigating applications to other kernels, such as the pyramid match kernel [15], string kernels, and graph kernels [2]."}, {"heading": "A Optimization results", "text": "In this first section, we give proofs of all results related to the optimization problems. We first recall precisely how we obtained the relationships between \u03b7 and \u03b6 . Using Cauchy-Schwarz inequality, we know that for all \u03b7 \u2208 RV such that \u03b7 > 0 and \u2211\nv\u2208V d 2 v\u03b7v 6 1,\n(\n\u2211\nv\u2208V\ndv\u2016\u03b2D(v)\u2016 )2 = ( \u2211\nv\u2208V\n(dv\u03b7 1/2 v ) \u2016\u03b2D(v)\u2016 \u03b7 1/2 v\n)2\n6 \u2211\nv\u2208V\nd2v\u03b7v \u00d7 \u2211\nv\u2208V\n\u2016\u03b2D(v)\u20162 \u03b7v 6 \u2211\nw\u2208V\n\n\n\u2211\nv\u2208A(w)\n\u03b7\u22121v\n\n \u2016\u03b2w\u20162,\nwith equality if and only if \u03b7v = d\u22121v \u2016\u03b2D(v)\u2016( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016)\u22121.\nA.1 Set of weights for trees\nWhen the DAG is a tree (i.e., when each vertex has at most one parent), then, without loss of generality we may consider that only one vertex has no parent (the root r) while all others w have exactly one parent \u03c0(w). In this situation, we have for all v 6= r, \u03b6\u22121\u03c0(v) \u2212 \u03b6\u22121v = \u2212\u03b7 \u22121 \u03c0(v). Moreover, for all leaves v, \u03b6v = \u03b7v. This implies that the constraint \u03b7 > 0 is equivalent to \u03b6 > 0 and for all v 6= r, \u03b6\u03c0(v) > \u03b6v. The final constraint \u2211 v\u2208V \u03b7vd 2 v 6 1, may then be written as:\n\u2211\nv 6=r\nd2v 1\n\u03b6\u22121v \u2212 \u03b6\u22121\u03c0(v) +\n\u2211\nv leaf\n\u03b6vd 2 v 6 1,\nthat is, \u2211\nv 6=r\nd2v\n(\n\u03b6v + \u03b62v\n\u03b6\u03c0(v) \u2212 \u03b6v\n)\n+ \u2211\nv leaf\n\u03b6vd 2 v 6 1,\nwhich is clearly convex [12]. When the DAG is not a tree, we conjecture that the set Z is not convex.\nA.2 Fenchel conjugates\nFollowing [16, 17], in order to derive optimality conditions for all losses, we need to introduce Fenchel conjugates. Let \u03c8i : R 7\u2192 R, be the Fenchel conjugate [12] of the convex function \u03d5i : a 7\u2192 \u2113(yi, a), defined as\n\u03c8i(b) = max a\u2208R\nab\u2212 \u03d5i(a).\n11\nThe function \u03c8i is always convex and, because we have assumed that \u03d5i is convex and continuous, we can represent \u03d5i as the Fenchel conjugate of \u03c8i, i.e., for all a \u2208 R,\n\u03d5i(a) = max b\u2208R\nab\u2212 \u03c8i(b).\nIn particular, we have for the following standard examples:\n\u2022 for least-squares regression, we have \u03d5i(a) = 12(yi \u2212 a)2 and \u03c8i(b) = 12b2 + byi,\n\u2022 for logistic regression, we have \u03d5i(a) = log(1 + exp(\u2212yiai)), where yi \u2208 {\u22121, 1}, and \u03c8i(b) = (1 + byi) log(1 + byi)\u2212 byi log(\u2212byi) if byi \u2208 [\u22121, 0], +\u221e otherwise.\n\u2022 for support vector machine classification, we have \u03d5i(a) = max(0, 1 \u2212 yia), where yi \u2208 {\u22121, 1}, and \u03c8i(b) = yib if byi \u2208 [\u22121, 0], +\u221e otherwise.\nA.3 Preliminary propositions\nWe first recall the duality result for the regular \u21132-norm kernel learning problem:\nProposition 6 For all nonnegative \u03b6 \u2208 RV , the dual of the optimization problem\nmin \u03b2\u2208 Q\nv\u2208V Fv\n1 n \u2211n i=1 \u2113(yi, \u2211 v\u2208V \u3008\u03b2v,\u03a6v(xi)\u3009) + \u03bb2 \u2211 w\u2208V \u03b6 \u22121 w \u2016\u03b2w\u20162\nis\nmax \u03b1\u2208Rn \u2212 1 n\nn \u2211\ni=1\n\u03c8i(\u2212n\u03bb\u03b1i)\u2212 \u03bb\n2 \u03b1\u22a4\n(\n\u2211\nw\u2208V\n\u03b6wKw\n)\n\u03b1,\nand the optimal \u03b2 can be found from an optimal \u03b1 as \u03b2w = \u2211n i=1 \u03b1i\u03a6w(xi).\nProof We introduce auxiliary variables ui = \u2211 v\u2208V \u3008\u03b2v ,\u03a6v(xi)\u3009 and consider the Lagrangian:\nL = 1 n\nn \u2211\ni=1\n\u03d5i(ui) + \u03bb\n2\n\u2211\nw\u2208V\n\u03b6\u22121w \u2016\u03b2w\u20162 + \u03bb n \u2211\ni=1\n\u03b1i(ui \u2212 \u2211\nv\u2208V\n\u3008\u03b2v,\u03a6v(xi)\u3009)\nMinimizing with respect to the primal variables u, \u03b2, we get the dual problem.\nWe will use the following simple result, which implies that each component \u03b6w(\u03b7) is a concave function of \u03b7:\nLemma 1 The minimum of \u2211m j=1 ajx 2 j subject to \u2211m j=1 xj = 1 is equal to\n(\n\u2211m j=1 a \u22121 i\n)\u22121 and is\nattained at xi = a \u22121 i\n(\n\u2211m j=1 a \u22121 i\n)\u22121 .\nThe following proposition derives the dual of the problem in \u03b7:\n12\nProposition 7 Let L = {\u03ba \u2208 RV\u00d7V ,\u2200w \u2208 V, \u2211 v\u2208A(w) \u03bavw = 1}. The following optimization problems are dual to each other, and there is no duality gap :\nmin \u03ba\u2208L max v\u2208V\nd\u22122v \u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1\nmax \u03b7\u2208H\n\u2211\nw\u2208V\n\u03b1\u22a4\u03b6w(\u03b7)Kw\u03b1.\nProof We have the Lagrangian\nL = \u03b42 + \u2211\nv\u2208V\n\u03b7v\n\n\n\u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1\u2212 \u03b42d2v\n\n ,\nwhich can be minimized in closed form with respect to \u03b42 and \u03ba \u2208 L, and leads to (using Lemma 1):\nmin \u03ba\u2208L max v\u2208V\nd\u22122v \u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1 = max \u03b7 \u03b1\u22a4\n(\n\u2211\nw\u2208V\n\u03b6w(\u03b7)Kw\n)\n\u03b1.\nA.4 Duality gaps\nWe consider the following function of \u03b7 \u2208 H and \u03b1 \u2208 Rn:\nF (\u03b7, \u03b1) = \u2212 1 n\nn \u2211\ni=1\n\u03c8i(\u2212n\u03bb\u03b1i)\u2212 \u03bb\n2 \u03b1\u22a4\n\u2211\nw\u2208V\n\u03b6w(\u03b7)Kw\u03b1.\nThis function is convex in \u03b7 (because of Lemma 1) and concave in \u03b1, standard arguments (e.g., primal and dual strict feasibilities) show that there is no duality gap to the variational problems:\ninf \u03b7\u2208H sup \u03b1\u2208Rn F (\u03b7, \u03b1) = sup \u03b1\u2208Rn inf \u03b7\u2208H F (\u03b7, \u03b1).\n13\nWe can decompose the duality gap, given a pair (\u03b7, \u03b1) as\nsup \u03b1\u2032\u2208Rn F (\u03b7, \u03b1\u2032)\u2212 inf \u03b7\u2032\u2208H F (\u03b7\u2032, \u03b1)\n= min \u03b2\n{\n1\nn\nn \u2211\ni=1\n\u2113(yi, \u2211\nv\u2208V\n\u3008\u03b2v ,\u03a6v(xi)\u3009) + \u03bb\n2\n\u2211\nw\u2208V\n\u03b6w(\u03b7) \u22121\u2016\u03b2w\u20162\n}\n\u2212 inf \u03b7\u2032\u2208H F (\u03b7\u2032, \u03b1)\n6 1\nn\nn \u2211\ni=1\n\u2113(yi, \u2211\nw\u2208V\n\u03b6w(\u03b7)(Kw\u03b1)i) + \u03bb\n2\n\u2211\nw\u2208V\n\u03b6w\u03b1 \u22a4Kw\u03b1+\n1\nn\nn \u2211\ni=1\n\u03c8i(\u2212n\u03bb\u03b1i)\n+ sup \u03b7\u2032\u2208H\n\u03bb 2 \u03b1\u22a4 \u2211\nw\u2208V\n\u03b6w(\u03b7 \u2032)\u03b1\n= 1\nn\nn \u2211\ni=1\n\u2113(yi, \u2211\nw\u2208V\n\u03b6w(\u03b7)(Kw\u03b1)i) + 1\nn\nn \u2211\ni=1\n\u03c8i(\u2212n\u03bb\u03b1i) + \u03bb \u2211\nw\u2208V\n\u03b6w(\u03b7)\u03b1 \u22a4Kw\u03b1\n+ sup \u03b7\u2032\u2208H\n\u03bb 2 \u03b1\u22a4 \u2211\nw\u2208V\n\u03b6w(\u03b7 \u2032)\u03b1\u2212 \u03bb\n2\n\u2211\nw\u2208V\n\u03b6w(\u03b7)\u03b1 \u22a4Kw\u03b1.\nWe thus get the desired upper bound from which proposition 1 (of the main paper) follows, as well as the upper bound on the duality gap.\nA.5 Necessary and sufficient conditions - truncated problem\nWe assume that we know the optimal solution of a truncated problem where the entire set of decendants of some nodes have been removed. We let denote J the hull of the set of active variables. We now consider necessary conditions and sufficient conditions for this solution to be optimal with respect to the full problem. This will lead to Proposition 2 and 3 of the main paper.\nWe first use Proposition 2 of the Appendix, to get a set of \u03bavw for (v,w) \u2208 J for the reduced problem; the goal here is to get necessary conditions by relaxing the dual problem defining \u03ba \u2208 L and find an approximate solution, while for the sufficient condition, any candidate leads to a sufficient condition. It turns out that we will use the solution of the relaxed solution required for the necessary condition for the sufficient condition.\nIf we assume that all variables in J are indeed active, then any optimal \u03ba \u2208 L must be such that \u03bavw = 0 if v \u2208 J and w \u2208 Jc. We then let free \u03bavw for v,w in J . Our goal is to find good candidates for those free dual parameters.\nWe first derive necessary conditions by lowerbounding the sums by maxima:\nmax v\u2208V \u2229Jc\nd\u22122v \u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1 > max v\u2208V \u2229Jc d\u22122v max w\u2208D(v) \u03ba2vw\u03b1 \u22a4Kw\u03b1,\nwhich can be minimized in closed form with respect to \u03ba leading to\n\u03bavw = dv( \u2211\nv\u2032\u2208A(w)\u2229Jc\ndv\u2032) \u22121\n14\nand to the lower bound\nmin \u03ba\u2208L max v\u2208V \u2229Jc\nd\u22122v \u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1 > max\nw\u2208Jc\n\u03b1\u22a4Kw\u03b1\n( \u2211 v\u2208A(w)\u2229Jc dv) 2 . (4)\nFor sufficient conditions, we simply take the value obtained before for \u03ba, which leads to\nmin \u03ba\u2208L max v\u2208V \u2229Ic\nd\u22122v \u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1 6 max\nv\u2208V \u2229Jc\n\u2211\nw\u2208D(v)\n\u03b1\u22a4Kw\u03b1\n( \u2211 v\u2208A(w)\u2229Jc dv) 2\n= max v\u2208sources(Jc)\n\u2211\nw\u2208D(v)\n\u03b1\u22a4Kw\u03b1\n( \u2211 v\u2208A(w)\u2229Jc dv) 2 .\nWe have moreover \u2211\nv\u2208A(w)\ndv > \u2211\nv\u2208A(w)\u2229Jc\ndv > \u2211\nv\u2208A(w)\u2229D(t)\ndv,\nleading to the desired upper bound\nmin \u03ba\u2208L max v\u2208V \u2229Jc\nd\u22122v \u2211\nw\u2208D(v)\n\u03ba2vw\u03b1 \u22a4Kw\u03b1 6 max\nt\u2208sources(Jc)\n\u2211\nw\u2208D(t)\n\u03b1\u22a4Kw\u03b1\n( \u2211 v\u2208A(w)\u2229D(t) dv) 2 . (5)\nA.6 Optimality conditions for the primal formulation\nWe know derive optimality conditions for the problem in the paper, which we will need in Section B, i.e.:\nmin \u03b2\u2208 Q\nv\u2208V Fv\n1 n \u2211n i=1 \u03d5i( \u2211 v\u2208V \u3008\u03b2v ,\u03a6v(xi)\u3009) + \u03bb2 ( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016 )2 .\nLet \u03b2 \u2208 RV , with J being the hull of the active variables. The directional derivative in the direction \u2206 \u2208 RV is equal to\n1\nn\nn \u2211\ni=1\n\u2211\nw\u2208V\n\u03d5\u2032i( \u2211\nv\u2208J\n\u3008\u03b2v,\u03a6v(xi)\u3009)\u03a6w(xi)\u22a4\u2206w\n+\u03bb\n(\n\u2211\nv\u2208J\ndv\u2016\u03b2D(v)\u2016 )( \u2211\nv\u2208J\ndv \u03b2D(v)\u2229J \u2016\u03b2D(v)\u2229J\u2016 \u22a4 \u2206v + \u2211\nv\u2208Jc\ndv\u2016\u2206D(v)\u2016 )\nand thus \u03b2 if optimal if and ony if, we have, with \u03b4 = \u2211 v\u2208J dv\u2016\u03b2D(v)\u2229J\u2016:\n\u2200w \u2208 J, 1 n\nn \u2211\ni=1\n\u03d5\u2032i( \u2211\nv\u2208J\n\u3008\u03b2v,\u03a6v(xi)\u3009)\u03a6w(xi) + \u03bb\u03b4\n\n\n\u2211\nv\u2208A(w)\ndv \u2016\u03b2D(v)\u2229J\u2016\n\n\u03b2w = 0\n\u2200\u2206Jc \u2208 RJ c , 1\nn\nn \u2211\ni=1\n\u2211\nw\u2208Jc\n\u03d5\u2032i( \u2211\nv\u2208J\n\u3008\u03b2v ,\u03a6v(xi)\u3009)\u03a6w(xi)\u22a4\u2206w + \u03bb\u03b4 ( \u2211\nv\u2208Jc\ndv\u2016\u2206D(v)\u2016 ) > 0.\nNote that when regularizing by \u03bb \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016 instead of \u03bb2 ( \u2211 v\u2208V dv\u2016\u03b2D(v)\u2016 )2\n, we have the same optimality condition with \u03b4 = 1.\n15"}, {"heading": "B Consistency conditions", "text": "We assume that we are in the finite dimensional setting (i.e., each Fv has finite dimensions fv) with the square loss. For w \u2208 V , we let denote Xw \u2208 Rn\u00d7fw the matrix whose n-th row is \u03a6w(xi). We let denote \u03a3vw \u2208 Rfv\u00d7fw the population covariance between \u03a6v(x) and \u03a6w(x). The full covariance matrix, defined from the blocks \u03a3vw is assumed invertible. With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211\nv\u2208W dv\u2016\u03b2D(v)\u2016:\nmin \u03b3\u2208 Q\nw R fw\n1 2 \u03b3\u22a4\u03a3\u03b3 + \u03b4 \u2211\nv\u2208W\ndv \u03b2D(v)\u2229W\n\u2016\u03b2D(v)\u2229W\u2016\n\u22a4\n\u03b3v + \u03b4 \u2211\nv\u2208Wc\ndv\u2016\u03b3D(v)\u2016.\nThe consistency condition is then obtained by studying when the first order expansion indeed has the correct sparsity pattern (for more precise statements and arguments, see [19]). We let denote \u03b3W the solution of the previous problem, restricted to \u03b3Wc = 0. We have:\n\u03b3W = \u03b4\u03a3 \u22121 WW Diag\n(\n\u2211\nv\u2208A(w) dv\n\u2016\u03b2D(v)\u2229W \u2016\n)\nw\u2208W \u03b2W .\nFollowing the previous section, it is optimal if and only for all \u2206 \u2208 W c,\n\u2206\u22a4Wc\u03a3WcW\u03b3W + \u03b4\n(\n\u2211\nv\u2208Wc\ndv\u2016\u2206D(v)\u2016 ) > 0.\nWe let denote\nAWc = \u03b4 \u22121 \u03a3WcW\u03b3W = \u03a3WcW\u03a3 \u22121 WW Diag\n(\n\u2211\nv\u2208A(w) dv\n\u2016\u03b2D(v)\u2229W \u2016\n)\nw\u2208W \u03b2W .\nThe condition for good pattern selection is that for all \u2206 \u2208 W c,\n\u2206\u22a4WcAWc + \u2211\nv\u2208Wc\ndv\u2016\u2206D(v)\u2016 > 0,\nwhich is exactly equivalent to \u2016AWc\u2016\u2217 6 1, where x 7\u2192 \u2016x\u2016\u2217 is the dual norm of the norm \u2206Wc 7\u2192 \u2211\nv\u2208Wc dv\u2016\u2206D(v)\u2016. This dual norm may be computed in closed form in the unstructured case, where D(v) = v, and is equal to the \u2113\u221e-norm. In general, it cannot be computed in closed form. However, we can give the following lower and upper bounds that lead to the desired propositions of the main paper.\nWe have:\n\u2211\nv\u2208Wc\ndv\u2016\u2206D(v)\u2016 6 \u2211\nv\u2208Wc\n\u2211\nw\u2208D(v)\ndv\u2016\u2206w\u2016 = \u2211\nw\u2208Wc\n\n\n\u2211\nv\u2208A(v)\u2229W c\ndv\n\n \u2016\u2206w\u2016,\nwhich leads to the upper bound\n\u2016x\u2016\u2217 6 max w\u2208Wc\n\u2016xw\u2016 \u2211\nv\u2208A(v)\u2229W c dv\n16\nMoreover, we have:\n(\n\u2211\nv\u2208W c\ndv\u2016\u2206D(v)\u2016 )2 = \u2211\nv\u2208W c\n\u2211\nv\u2032\u2208Wc\ndvdv\u2032\u2016\u2206D(v)\u2016\u2016\u2206D(v\u2032)\u2016\n> \u2211\nv\u2208W c\n\u2211\nv\u2032\u2208Wc\ndvdv\u2032\u2016\u2206D(v)\u2229D(v\u2032)\u20162\n= \u2211\nv\u2208W c\n\u2211\nv\u2032\u2208Wc\n\u2016\u2206w\u20162 \u2211\nw\u2208D(v)\u2229D(v\u2032)\ndvdv\u2032\n= \u2211\nw\u2208Wc\n\u2016\u2206w\u20162 \u2211\nv\u2208A(w)\u2229W c\n\u2211\nv\u2032\u2208A(w)\u2229Wc\ndvdv\u2032\n= \u2211\nw\u2208Wc\n\u2016\u2206w\u20162  \n\u2211\nv\u2208A(w)\u2229Wc\ndv\n\n\n2\n.\nwhich leads to the lower bound:\n(\u2016x\u2016\u2217)2 > \u2211\nw\u2208Wc\n\u2016xw\u20162 (\n\u2211\nv\u2208A(v)\u2229W c dv\n)2 ."}], "references": [{"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Camb. U. P.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "On model selection consistency of Lasso", "author": ["P. Zhao", "B. Yu"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Consistency of the group Lasso and multiple kernel learning", "author": ["F.R. Bach"], "venue": "Technical Report 00164735,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "In Proc. ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Grouped and hierarchical model selection through composite absolute penalties", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, To appear,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Composite kernel learning", "author": ["M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy"], "venue": "In Proc. ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "The effect of the input density distribution on kernel-based classifiers", "author": ["C.K.I. Williams", "M. Seeger"], "venue": "In Proc. ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "More efficiency in multiple kernel learning", "author": ["A. Rakotomamonjy", "F.R. Bach", "S. Canu", "Y. Grandvalet"], "venue": "In Proc. ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Learning the kernel function via regularization", "author": ["M. Pontil", "C.A. Micchelli"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A. Ng"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Mark: A boosting algorithm for heterogeneous kernel models", "author": ["K. Bennett", "M. Momma", "J. Embrechts"], "venue": "In Proc. SIGKDD,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "The generalized Lasso", "author": ["V. Roth"], "venue": "IEEE Trans. on Neural Networks,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "The pyramid match kernel: Efficient learning with sets of features", "author": ["K. Grauman", "T. Darrell"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Computing regularization paths for learning multiple kernels", "author": ["F.R. Bach", "R. Thibaux", "M.I. Jordan"], "venue": "In Adv. NIPS", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "J. Mach. Learn. Res., 7:1531\u20131565,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "The adaptive Lasso and its oracle properties", "author": ["H. Zou"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Asymptotics for Lasso-type estimators", "author": ["W. Fu", "K. Knight"], "venue": "Annals of Statistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "On the non-negative garrotte estimator", "author": ["M. Yuan", "Y. Lin"], "venue": "Journal of The Royal Statistical Society Series B,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": ", [1, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": ", [1, 2]).", "startOffset": 2, "endOffset": 8}, {"referenceID": 2, "context": "While early work has focused on efficient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework [4].", "startOffset": 220, "endOffset": 223}, {"referenceID": 3, "context": "While early work has focused on efficient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework [4].", "startOffset": 273, "endOffset": 276}, {"referenceID": 4, "context": "This exactly corresponds to the situation where a large feature space is the concatenation of smaller feature spaces, and we aim to do selection among these many kernels, which may be done through multiple kernel learning [5].", "startOffset": 222, "endOffset": 225}, {"referenceID": 5, "context": "Following [6, 7], we consider in Section 2 a specific combination of l2-norms that is adapted to the DAG, and will restrict the authorized sparsity patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of selected kernels (Section 3).", "startOffset": 10, "endOffset": 16}, {"referenceID": 6, "context": "Following [6, 7], we consider in Section 2 a specific combination of l2-norms that is adapted to the DAG, and will restrict the authorized sparsity patterns; in our specific kernel framework, we are able to use the DAG to design an optimization algorithm which has polynomial complexity in the number of selected kernels (Section 3).", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufficient conditions for model consistency.", "startOffset": 112, "endOffset": 118}, {"referenceID": 3, "context": "Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufficient conditions for model consistency.", "startOffset": 112, "endOffset": 118}, {"referenceID": 1, "context": ", [2] or the Appendix).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": ", [8]): e\u2212b(x\u2212x \u2032)2 = \u2211\u221e k=0 (b/A) 2kk! [e b A 2Hk( \u221a 2cx)][e b A \u20322Hk( \u221a 2cx\u2032)], where c2 = a2 + 2ab, A = a + b + c, and Hk is the k-th Hermite polynomial.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "The decomposition ends up being close to a polynomial kernel of infinite degree, modulated by an exponential [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": ", [2, 1]), which is equivalent to using the eigenvectors of the empirical covariance operator associated with the data (and not the population one associated with the Gaussian distribution with same variance).", "startOffset": 2, "endOffset": 8}, {"referenceID": 0, "context": ", [2, 1]), which is equivalent to using the eigenvectors of the empirical covariance operator associated with the data (and not the population one associated with the Gaussian distribution with same variance).", "startOffset": 2, "endOffset": 8}, {"referenceID": 1, "context": "Note that for q = 1, we obtain ANOVA-like decompositions [2].", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "(1) Our Hilbertian norm is a Hilbert space instantiation of the hierarchical norms recently introduced by [6].", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "While with uni-dimensional groups or kernels, the \u201cl1-norm of l\u221e-norms\u201d allows an efficient path algorithm for the square loss and when the DAG is a tree [6], this is not possible anymore with groups of size larger than one, or when the DAG is a not a tree.", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "Finally, note that in certain settings (finite dimensional Hilbert spaces and distributions with absolutely continuous densities), these norms have the effect of selecting a given kernel only after all of its ancestors [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": "1 Reformulation in terms of multiple kernel learning Following [9, 10], we can simply derive an equivalent formulation of Eq.", "startOffset": 63, "endOffset": 70}, {"referenceID": 9, "context": "1 Reformulation in terms of multiple kernel learning Following [9, 10], we can simply derive an equivalent formulation of Eq.", "startOffset": 63, "endOffset": 70}, {"referenceID": 4, "context": "(2) Following [5], we consider the square of the norm, which does not change the regularization properties, but allow simple links with multiple kernel learning.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "Note that in the case of \u201cflat\u201d regular multiple kernel learning, where the DAG has no edges, we obtain back usual optimality conditions [9, 10].", "startOffset": 137, "endOffset": 144}, {"referenceID": 9, "context": "Note that in the case of \u201cflat\u201d regular multiple kernel learning, where the DAG has no edges, we obtain back usual optimality conditions [9, 10].", "startOffset": 137, "endOffset": 144}, {"referenceID": 10, "context": "Following a common practice for convex sparsity problems [11], we will try to solve a small problem where we assume we know the set of v such that \u2016\u03b2D(v)\u2016 is equal to zero (Section 3.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 8, "context": "Namely, we use the same technique as [9]: we consider for \u03b6 \u2208 Z , the function B(\u03b6) = min\u03b2\u2208Qv\u2208V Fv 1 n \u2211n i=1 l(yi, \u2211 v\u2208V \u3008\u03b2v,\u03a6v(xi)\u3009)+ \u03bb 2 \u2211 w\u2208V \u03b6 \u22121 w \u2016\u03b2w\u2016, which is the optimal value of the single kernel learning problem with kernel matrix \u2211 w\u2208V \u03b6wKw.", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "We can then use the same projected gradient descent strategy as [9] to minimize it.", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "4 Kernel search algorithm We are now ready to present the detailed algorithm which extends the feature search algorithm of [11].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Following [4], we make the following assumptions on the underlying joint distribution of (X,Y ): (a) the joint covariance matrix \u03a3 of (\u03a6(xv))v\u2208V (defined with appropriate blocks of size fv \u00d7 fw) is invertible, (b) E(Y |X) = \u2211 w\u2208W \u3008\u03b2w,\u03a6w(x)\u3009 with W \u2282 V and var(Y |X) = \u03c32 > 0 almost surely.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Note that the last two propositions are not consequences of the similar results for flat MKL [4], because the groups that we consider are overlapping.", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "We are currently investigating extensions to the non parametric case [4], in terms of pattern selection and universal consistency.", "startOffset": 69, "endOffset": 72}, {"referenceID": 12, "context": "We then compare the performance of our hierarchical multiple kernel learning method (HKL) with the polynomial kernel decomposition presented in Section 2 to other methods that use the same kernel and/or decomposition: (a) the greedy strategy of selecting basis kernels one after the other, a procedure similar to [13], and (b) the regular polynomial kernel regularization with the full kernel (i.", "startOffset": 313, "endOffset": 317}, {"referenceID": 13, "context": "UCI datasets For regression datasets, we compare HKL with polynomial (degree 4) and Gaussian-RBF kernels (each dimension decomposed into 9 kernels) to the following approaches with the same kernel: regular Hilbertian regularization (L2), same greedy approach as earlier (greedy), regularization by the l1-norm directly on the vector \u03b1, a strategy which is sometimes used in the context of sparse kernel learning [14] but does not use the Hilbertian structure of the kernel (lasso-\u03b1), multiple kernel learning with the p kernels obtained by summing all kernels associated with a single variable, a strategy suggested by [5] (MKL).", "startOffset": 412, "endOffset": 416}, {"referenceID": 4, "context": "UCI datasets For regression datasets, we compare HKL with polynomial (degree 4) and Gaussian-RBF kernels (each dimension decomposed into 9 kernels) to the following approaches with the same kernel: regular Hilbertian regularization (L2), same greedy approach as earlier (greedy), regularization by the l1-norm directly on the vector \u03b1, a strategy which is sometimes used in the context of sparse kernel learning [14] but does not use the Hilbertian structure of the kernel (lasso-\u03b1), multiple kernel learning with the p kernels obtained by summing all kernels associated with a single variable, a strategy suggested by [5] (MKL).", "startOffset": 619, "endOffset": 622}, {"referenceID": 14, "context": "We are currently investigating applications to other kernels, such as the pyramid match kernel [15], string kernels, and graph kernels [2].", "startOffset": 95, "endOffset": 99}, {"referenceID": 1, "context": "We are currently investigating applications to other kernels, such as the pyramid match kernel [15], string kernels, and graph kernels [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 11, "context": "v leaf \u03b6vd 2 v 6 1, which is clearly convex [12].", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "2 Fenchel conjugates Following [16, 17], in order to derive optimality conditions for all losses, we need to introduce Fenchel conjugates.", "startOffset": 31, "endOffset": 39}, {"referenceID": 16, "context": "2 Fenchel conjugates Following [16, 17], in order to derive optimality conditions for all losses, we need to introduce Fenchel conjugates.", "startOffset": 31, "endOffset": 39}, {"referenceID": 11, "context": "Let \u03c8i : R 7\u2192 R, be the Fenchel conjugate [12] of the convex function \u03c6i : a 7\u2192 l(yi, a), defined as \u03c8i(b) = max a\u2208R ab\u2212 \u03c6i(a).", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211 v\u2208W dv\u2016\u03b2D(v)\u2016: min \u03b3\u2208 Q", "startOffset": 54, "endOffset": 66}, {"referenceID": 18, "context": "With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211 v\u2208W dv\u2016\u03b2D(v)\u2016: min \u03b3\u2208 Q", "startOffset": 54, "endOffset": 66}, {"referenceID": 19, "context": "With these assumptions, we can follow the approach of [18, 19, 20] : that is, if \u03bbn tends to zero faster than n\u22121/2, then the estimate \u03b2\u0302 converges in probability to the generating \u03b2, and we have the expansion \u03b2\u0302 = \u03b2 + \u03bbn\u03b3\u0302 where \u03b3\u0302 is the solution of the following optimization problem, with \u03b4 = \u2211 v\u2208W dv\u2016\u03b2D(v)\u2016: min \u03b3\u2208 Q", "startOffset": 54, "endOffset": 66}, {"referenceID": 18, "context": "The consistency condition is then obtained by studying when the first order expansion indeed has the correct sparsity pattern (for more precise statements and arguments, see [19]).", "startOffset": 174, "endOffset": 178}], "year": 2008, "abstractText": "For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the l-norm or the block l-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.", "creator": "LaTeX with hyperref package"}}}