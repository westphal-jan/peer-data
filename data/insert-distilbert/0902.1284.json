{"id": "0902.1284", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2009", "title": "Multi-Label Prediction via Compressed Sensing", "abstract": "we consider multi - label prediction problems with large dimensional output spaces under the assumption of output sparsity - - that the target vectors have small support. we develop a general theory for a regression variant of the popular ecoc ( error correcting _ output information code ) scheme, based on ideas from compressed sensing for processors exploiting this sparsity. the method can be regarded as a simple deviation reduction experiment from multi - label regression problems to binary regression problems. it is shown that the number of subproblems need only be logarithmic in the total number of label values, making this inference approach radically more efficient than others. we also state scenarios and prove performance guarantees for this method, and test it most empirically.", "histories": [["v1", "Sun, 8 Feb 2009 02:30:06 GMT  (50kb)", "http://arxiv.org/abs/0902.1284v1", null], ["v2", "Tue, 2 Jun 2009 16:23:28 GMT  (38kb)", "http://arxiv.org/abs/0902.1284v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel j hsu", "sham kakade", "john langford", "tong zhang 0001"], "accepted": true, "id": "0902.1284"}, "pdf": {"name": "0902.1284.pdf", "metadata": {"source": "CRF", "title": "Multi-Label Prediction via Compressed Sensing", "authors": ["Daniel Hsu"], "emails": ["djhsu@cs.ucsd.edu", "sham@tti-c.org", "jl@hunch.net", "tongz@rci.rutgers.edu"], "sections": [{"heading": null, "text": "ar X\niv :0\n90 2.\n12 84\nv1 [\ncs .L\nG ]"}, {"heading": "1. Introduction", "text": "Suppose we have a large database of images, and we want to learn to predict which objects are in any given one. A standard approach to this task is to collect a sample of these images x along with corresponding labels y = (y1, . . . , yd) \u2208 {0, 1}d, where yi = 1 if and only if object i is pictured in x, and then feed the labeled sample to a multi-label learning algorithm. Here, d is the total number of objects depicted in the entire database. When d is very large (e.g. 103, 104), the simple one-against-all approach of learning a sin-\nPreliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.\ngle predictor for each object i = 1, . . . , d can become prohibitively expensive at both training and test time.\nOur motivation for the present work comes from the observation that although the output (label) space may be very high dimensional, the actual labels are often sparse. That is, in each image, only a small number of objects may be present and there may only be a small amount of ambiguity in what they are. In this work, we consider how this sparsity in the output space, or output sparsity, eases the burden of multilabel learning.\nExploiting output sparsity. A subtle but critical point that distinguishes output sparsity from more common notions of sparsity (say, in feature or weight vectors) is that we are interested in sparsity in E[y|x] rather than y. In general, E[y|x] may be sparse while the actual outcome y may not (e.g. if there is much unbiased noise); and, vice versa, y may be sparse with probability one but E[y|x] may have full support. Conventional linear algebra suggests that we must predict d parameters in order to find the value of E[y|x] for each x. A crucial observation \u2013 central to the area of compressed sensing (Donoho, 2006) \u2013 is that methods exist to recover E[y|x] from just O(k log d) measurements when E[y|x] is k-sparse. This is the basis of our approach.\nOur contributions. We show how to apply algorithms for compressed sensing to the output coding approach (Dietterich & Bakiri, 1995). At a high level, the output coding approach creates a collection of subproblems of the form \u201cIs the label in this subset or its complement?\u201d, solves these problems, and then uses their solution to predict the final label.\nOur application of compressed sensing is distinct from its more conventional uses for data dimension reduction. Although we do employ a sensing matrix to compress training data, we ultimately are not interested in recovering data explicitly compressed this way. Rather, we learn to predict compressed label vectors, then use sparse reconstruction algorithms to recover uncompressed labels from these predictions. Thus we are interested in reconstruction accuracy of predictions, averaged over the data distribution.\nThe main contributions of this work are:\n1. A formal application of compressed sensing to prediction problems with output sparsity.\n2. An efficient output coding method, in which the number of required predictions is only logarithmic in the number of labels d, making it applicable to very large-scale problems.\n3. Robustness guarantees, in the form of regret transform bounds (in general) and a further detailed analysis for the linear prediction setting.\nPrior work. The idea of output coding was introduced in (Dietterich & Bakiri, 1995) and shown to be useful experimentally. Relative to this work, we expand the scope of the approach to multi-label prediction and provide bounds on regret and error which guide the design of codes.\nThe loss based decoding approach (Allwein et al., 2000) suggests decoding so as to minimize loss. However, it does not provide significant guidance in the choice of encoding technique, or the feedback between encoding and decoding which we analyze here.\nThe output coding approach is inconsistent when classifiers are used and the underlying problems being encoded are noisy. This is proved and analyzed in (Langford & Beygelzimer, 2005), where it is shown that using a Hadamard code creates a robust consistent predictor when reduced to binary regression. The approaches here require exponentially (in d) fewer predictors. The robustness guarantees here are weaker by a constant factor for the single label case, but stronger for the multi-label case. The multi-label case wasn\u2019t specifically analyzed for PECOC, instead the arbitrary cost-sensitive case was analyzed. Carefully tuning the SECOC algorithm there gives a k2 dependence in the regret bound, which is functionally worse than the bounds here.\nOur algorithms rely on several compressed sensing approaches, which we detail where used."}, {"heading": "2. Preliminaries", "text": "Let X be an arbitrary input space and Y \u2282 Rd be a ddimensional output (label) space. We assume the data source is defined by a fixed but unknown distribution over X\u00d7Y. Our goal is to learn a predictor F : X \u2192 Y with low expected \u211322-error\nEx\u2016F (x)\u2212 E[y|x]\u201622 (the sum of mean-squared-errors over all labels) using a random i.i.d. sample of n data {(xi, yi)}ni=1. We focus on the regime in which the output space is very high-dimensional (d very large), but for any given x \u2208 X , the expected value E[y|x] of the corresponding label y \u2208 Y has only a few non-zero entries. A vector is k-sparse if it has at most k non-zero entries."}, {"heading": "3. Learning and Prediction", "text": ""}, {"heading": "3.1. Learning Compressed Labels", "text": "Let A : Rd \u2192 Rm be a linear compression function, where m \u2264 d (but hopefully m \u226a d). We use A to compress (i.e. reduce the dimension of) the labels Y, and learn a predictor H : X \u2192 A(Y) of these compressed labels. Since A is linear, we simply represent A \u2208 Rm\u00d7d as a matrix. Specifically, given a sample {(xi, yi)}ni=1, we form a compressed sample {(xi, Ayi)}ni=1 and then learn a predictor H of E[Ay|x] = AE[y|x] with the objective of minimizing the \u211322-error\nEx\u2016H(x)\u2212 E[Ay|x]\u201622."}, {"heading": "3.2. Prediction", "text": "To obtain a predictor F of E[y|x], we compose the predictor H of E[Ay|x] (learned using the compressed sample) with a reconstruction algorithm R : Rm \u2192 Rd. The algorithm Rmaps predictions of compressed labels Ay \u2208 AY to predictions of labels y \u2208 Y in the original output space.\nRecent developments in the area of compressed sensing have produced a spate of algorithms R with strong performance guarantees when the compression function A satisfies certain properties. We abstract out the relevant aspects of these guarantees in the following definition.\nDefinition. An algorithm R is a valid reconstruction algorithm for a family of compression functions {Ak \u2282\u22c3\nm\u22651 R m\u00d7d : k \u2208 N} and sparsity error sperr : N \u00d7 R d \u2192 R, if there exists a function f : N \u2192 N and constants C1, C2 \u2208 R such that: on input k \u2208 N, A \u2208\nAlgorithm 1 Training algorithm\nparameters sparsity level k, regression learning algorithm L, compression function A \u2208 Rm\u00d7d input training data S \u2282 X \u00d7 Rd for i = 1, . . . ,m do hi \u2190 L({(x, (Ay)i) : (x, y) \u2208 S})\nend for\noutput regressors H = [h1, . . . , hm]\nAk with m rows, and h \u2208 Rm, the algorithm R(k,A, h) returns an f(k)-sparse vector y\u0302 satisfying\n\u2016y\u0302 \u2212 y\u201622 \u2264 C1 \u00b7 \u2016h\u2212Ay\u201622 + C2 \u00b7 sperr(k, y)\nfor all y \u2208 Rd. The function f is the output sparsity of R and the constants C1 and C2 are the regret factors.\nLoosely put, a reconstruction algorithm should be agnostic about the sparsity of the original signal (sperr(k,E[y|x])), as well as the \u201cmeasurement noise\u201d (the prediction error \u2016H(x)\u2212 E[Ay|x]\u20162). We make a few additional remarks on the definition.\n1. The minimum number of rows of matrices A \u2208 Ak may in general depend on k (as well as the ambient dimension d). In the next section, we show how to construct such A with close to the optimal number of rows.\n2. The sparsity error sperr(k, y) should measure how poorly y \u2208 Rd is approximated by a k-sparse vector.\n3. A reasonable output sparsity f(k) for sparsity level k should not be much more than k, e.g. f(k) = O(k).\nConcrete examples of valid reconstruction algorithms (along with the associated Ak, sperr, etc.) are given in the next section."}, {"heading": "4. Algorithms", "text": "Our prescribed recipe is summarized in Algorithms 1 and 2. We give some examples of compression functions and reconstruction algorithms in the following subsections."}, {"heading": "4.1. Compression Functions", "text": "Several valid reconstruction algorithms are known for compression matrices that satisfy a restricted isometry property.\nAlgorithm 2 Prediction algorithm\nparameters sparsity level k, compression function A \u2208 Ak, valid reconstruction algorithm R for Ak, input regressors H = [h1, . . . , hm], test point x \u2208 X output y\u0302 = ~R(k,A, [h1(x), . . . , hm(x)])\nAlgorithm 3 Prediction algorithm with R = OMP\nparameters sparsity level k, compression function A = [a1| . . . |ad] \u2208 Ak, input regressors H = [h1, . . . , hm], test point x \u2208 X b \u2190 [h1(x), . . . , hm(x)]\u22a4 y\u0302 \u2190 ~0, J \u2190 \u2205, r \u2190 b for i = 1, . . . , 2k do j\u2217 \u2190 argmaxj |r\u22a4aj | J \u2190 J \u222a {j\u2217} y\u0302J \u2190 (AJ )\u2020b (least-squares restricted to J) y\u0302Jc \u2190 ~0 r \u2190 b\u2212Ay\u0302\nend for output y\u0302\nDefinition. A matrix A \u2208 Rm\u00d7d satisfies the (k, \u03b4)restricted isometry property ((k, \u03b4)-RIP), \u03b4 \u2208 (0, 1), if\n(1\u2212 \u03b4)\u2016x\u201622 \u2264 \u2016Ax\u201622 \u2264 (1 + \u03b4)\u2016x\u201622 for all k-sparse x \u2208 Rd.\nWhile some explicit constructions of (k, \u03b4)-RIP matrices are known (e.g. (DeVore, 2007)), the best guarantees are obtained when the matrix is chosen randomly from an appropriate distribution, such as one of the following (Mendelson et al., 2008; Rudelson & Vershynin, 2006).\n\u2022 All entries i.i.d. Gaussian N(0, 1/m), with m = O(k log(d/k)).\n\u2022 All entries i.i.d. Bernoulli B(1/2, 1/2) over {\u00b11/\u221am}, with m = O(k log(d/k)).\n\u2022 Random subset of rows from the d\u00d7 d Hadamard matrix over {\u00b11/\u221am}, with m = O(k log5 d).\nThe hidden constants in the big-O notation depend inversely on \u03b4 and the probability of success.\nA striking feature of these constructions is the very mild dependence of m on the ambient dimension d. This translates to a significant savings in the number of learning problems one has to solve after employing our reduction.\nSome reconstruction algorithms require a stronger guarantee of bounded coherence \u00b5(A) \u2264 O(1/k),\nwhere \u00b5(A) defined as\n\u00b5(A) = max 1\u2264i<j\u2264d\n|(A\u22a4A)i,j |/ \u221a |(A\u22a4A)i,i||(A\u22a4A)j,j |\nIt is easy to check that the Gaussian, Bernoulli, and Hadamard-based random matrices given above have coherence bounded by O( \u221a (log d)/m) with high probability. Thus, one can take m = O(k2 log d) to guarantee 1/k coherence. This is a factor k worse than what was needed for (k, \u03b4)-RIP, but the dependence on d is still small."}, {"heading": "4.2. Reconstruction Algorithms", "text": "In Table 1, we give some examples of valid reconstruction algorithms. These algorithms are valid for the Ak listed in the table, and sparsity error\nsperr(k, y) = \u2016y \u2212 y(1:k)\u201622 + 1\nk \u2016y \u2212 y(1:k)\u201621\nwhere y(1:k) is the best k-sparse approximation of y (i.e. the vector with just the k largest (in magnitude) coefficients of y).\nThe following theorem relates reconstruction quality to approximate sparse regression, giving a sufficient condition for an algorithm to be valid for RIP matrices. Theorem 1. Let Ak = {(k + f(k), \u03b4)-RIP matrices} for some function f : N \u2192 N, and let A \u2208 Ak have m rows. If for any h \u2208 Rm, a reconstruction algorithm R returns an f(k)-sparse solution y\u0302 = R(k,A, h) satisfying\n\u2016Ay\u0302 \u2212 h\u201622 \u2264 inf y\u2208Rd C\u2016Ay(1:k) \u2212 h\u201622,\nthen it is a valid reconstruction algorithm for Ak and sperr given above, with output sparsity f and regret factors C1 = 2(1 \u2212 \u03b4)\u22121(1 + \u221a C)2 and C2 = 4(1 \u2212\n\u03b4)\u22122(2 + \u221a C)2.\nWe defer all proofs to the appendices.\nIterative and greedy algorithms. Orthogonal Matching Pursuit (OMP) (Mallat & Zhang, 1993), FoBa (Zhang, 2008), and CoSaMP (Needell & Tropp, 2007) are examples of iterative or greedy reconstruction algorithms. OMP is a greedy forward selection method that repeatedly selects a new column of A to use in fitting h (see Algorithm 3). FoBa is similar, except it also incorporates backward steps to un-select columns that are later discovered to be unnecessary. CoSaMP is also similar to OMP, but instead selects larger sets of columns in each iteration.\nThe validity of FoBa and CoSaMP are apparent from the cited references. For OMP, we give the following guarantee.\nTheorem 2. If \u00b5(A) \u2264 0.1/k, then after f(k) = 2k steps of OMP, the algorithm returns y\u0302 satisfying\n\u2016Ay\u0302 \u2212 h\u201622 \u2264 23\u2016Ay(1:k) \u2212 h\u201622 (\u2200y \u2208 Rd).\n\u21131 algorithms. Basis Pursuit (BP) (Cande\u0300s et al., 2006) and its variants are based on finding the minimum \u21131-norm solution to a linear system. While the basic form of BP is ill-suited for our application (it requires the user to supply the amount of measurement error \u2016Ay\u2212 h\u20162), its more advanced path-following or multi-stage variants may be valid. We use the popular LARS/Lasso algorithm (Efron et al., 2004) in our experiments."}, {"heading": "5. Analysis", "text": ""}, {"heading": "5.1. Regret Analysis", "text": "We now state our main regret transform bound, which follows immediately from the definition of a valid reconstruction algorithm and linearity of expectation.\nTheorem 3 (Regret Transform). Let R be a valid reconstruction algorithm for {Ak : k \u2208 N} and sperr : N \u00d7 Rd \u2192 R. Then there exists some constants C1 and C2 such that the following holds. Pick any k \u2208 N, A \u2208 Ak with m rows, and H : X \u2192 Rm. Let F : X \u2192 Rd be the composition of R(k,A, \u00b7) and H, i.e. F (x) = R(k,A,H(x)). Then\nEx\u2016F (x)\u2212 E[y|x]\u201622 \u2264 C1 \u00b7 Ex\u2016H(x)\u2212 E[Ay|x]\u201622 + C2 \u00b7 sperr(k,E[y|x]).\nIn order compare regret bound in Theorem 3 with the bounds afforded by Sensitive Error Correcting Output Codes (SECOC) (Langford & Beygelzimer, 2005), we need to relate Ex\u2016H(x)\u2212E[Ay|x]\u201622 to the average scaled mean-squared-error over all induced regression problems, where error is scaled by the maximum difference Li = maxy(Ay)i\u2212miny(Ay)i between induced labels:\nr\u0304 = 1\nm\nm\u2211\ni=1\nEx\n( H(x)i \u2212 E[(Ay)i|x]\nLi\n)2 .\nIn these terms, SECOC can be tuned to yield\nEx\u2016F (x)\u2212 E[y|x]\u201622 \u2264 4k2 \u00b7 r\u0304\nfor general k.\nFor now, ignore the sparsity error. Let A \u2208 Rm\u00d7d with N(0, 1/m) entries, where m = O(k log d). One can show that with high probability over the choice of A, supy \u2016Ay\u2016\u221e = O( \u221a k log(m)/m), where the supremum is taken over k-sparse vectors y \u2208 {0, 1}d (see Lemma 8 in Appendix C). In this event, we have Li = O( \u221a k log(m)/m), so we have the bound\nC1 \u00b7 Ex\u2016H(x)\u2212 E[Ay|x]\u201622 \u2264 O(k logm) \u00b7 r\u0304 = O(k log k + k log log d) \u00b7 r\u0304.\nThe improved dependence on k is at the cost of larger constants and log log d.\nNow we consider the sparsity error. When m = d (as is the case for SECOC), E[y|x] is allowed to be fully dense (k = d), in which case sperr(k,E[y|x]) = 0. Thus we pay a penalty for inducing just k < d problems unless E[y|x] is truly k-sparse."}, {"heading": "5.2. Linear Prediction", "text": "A danger of using generic reductions is that one creates a problem instance that is even harder to solve than the original problem. This is an oft cited issue with using output codes for multi-class problems.\nIn the case of linear prediction, however, the danger is mitigated. As a warm up, suppose there is a perfect linear predictor of E[y|x], i.e. E[y|x] = B\u22a4x for some B \u2208 Rp\u00d7d (here X = Rp). Then it is easy to see that H = BA\u22a4 is a perfect linear predictor of E[Ay|x]: H\u22a4x = AB\u22a4x = AE[y|x] = E[Ay|x]. The following theorem generalizes this observation to imperfect linear predictors for certain well-behaved A.\nTheorem 4. Suppose X \u2282 Rp. Let B \u2208 Rp\u00d7d be a linear function with\nEx \u2225\u2225B\u22a4x\u2212 E[y|x] \u2225\u22252 2 = \u01eb.\nLet A \u2208 Rm\u00d7d have entries drawn i.i.d. from N(0, 1/m), and let H = BA\u22a4. Then with high probability (over the choice of A),\nEx\u2016H\u22a4x\u2212AE[y|x]\u201622 \u2264 ( 1 +O(1/ \u221a m) ) \u01eb.\nRemark 5. Similar guarantees can be proven for the Bernoulli-based matrices. Note that d does not appear in the bound. This is in contrast to the expected spectral norm of A, which is 1 +O( \u221a d/m).\nTheorem 4 implies that the errors of any linear predictor are not magnified much by the compression function. So a good linear predictor for the original problem implies an almost-as-good linear predictor for the induced problem."}, {"heading": "6. Experiments", "text": "We conducted an empirical assessment of the proposed reduction on two labeled data sets with large label spaces. These experiments demonstrate the feasibility of the method \u2013 a sanity check that the reduction does in fact preserve learnability \u2013 and compare different compression and reconstruction options. However, the scale of these experiments, in terms of data size and predictor complexity, is not yet able to show computational savings in terms of wall-clock measurements."}, {"heading": "6.1. Data", "text": "Image data.1 The first data set was collected by the ESP Game (von Ahn & Dabbish, 2004), an online game in which players ultimately provide word tags for a diverse set of web images.\nThe set contains nearly 68000 images, with about 22000 unique labels. We retained just the 1000 most frequent labels: the least frequent of these occurs 39 times in the data, and the most frequent occurs about 12000 times. Each image contains about four labels on average. We used half of the data for training and half for testing.\nWe represented each image as a bag-of-features vector in a manner similar to (Marsza lek et al., 2007). Specifically, we identified 1024 representative SURF features points (Bay et al., 2008) from 10 \u00d7 10 grayscale patches chosen randomly from the training images; this partitions the space of image patches (represented with SURF features) into Voronoi cells. We then built a histogram for each image, counting the number of patches that fall in each cell.\nText data.2 The second data set was collected by Tsoumakas et al. (2008) from del.icio.us, a social bookmarking service in which users assign descriptive textual tags to web pages.\nThe set contains about 16000 labeled web page and 983 unique labels. The least frequent label occurs 21 times and the most frequent occurs almost 6500 times. Each web page is assigned 19 labels on average. Again, we used half the data for training and half for testing.\n1http://hunch.net/\u223clearning/ESP-ImageSet.tar.gz 2http://mlkd.csd.auth.gr/multilabel.html\nEach web page is represented as a boolean bag-ofwords vector, with the vocabulary chosen using a combination of frequency thresholding and \u03c72 feature ranking. See (Tsoumakas et al., 2008) for details.\nEach binary label vector (for both data sets) has ones in the coordinates corresponding to the labels assigned to the data point."}, {"heading": "6.2. Output Sparsity", "text": "We first performed a bit of exploratory data analysis to get a sense of how sparse the target in our data is. We computed the least-squares linear regressor B\u0302 \u2208 Rp\u00d7d on the training data (without any output coding) and predicted the label probabilities p\u0302(x) = B\u0302\u22a4x on the test data (clipping values to the range [0, 1]). Using p\u0302(x) as a surrogate for the actual target E[y|x], we examined the relative \u211322 error of p\u0302 and its best k-sparse\napproximation \u01eb(k, p\u0302(x)) = \u2211d\ni=k+1 p\u0302(i)(x) 2/\u2016p\u0302(x)\u201622,\nwhere p\u0302(1)(x) \u2265 . . . \u2265 p\u0302(d)(x). Examining Ex\u01eb(k, p\u0302(x)) as a function of k, we saw that in both the image and text data, the fall-off with k is eventually super-polynomial, but we are interested in the behavior for small k where it appears polynomial k\u2212r for some r. Around k = 10, we estimated an exponent of 0.50 for the image data and 0.55 for the text data. This is somewhat below the standard of what is considered sparse (e.g. vectors with small \u21131-norm show k\u22121 decay). Thus, we expect the reconstruction algorithms will have to contend with the sparsity error of the target."}, {"heading": "6.3. Procedure", "text": "We used least-squares linear regression as our base learning algorithm, with no regularization on the image data and with ridge regularization with the text data (\u03bb = 0.01) for numerical stability reasons. We did not attempt any parameter tuning.\nThe compression functions we used were generated by selecting m random rows of the 1024\u00d71024 Hadamard matrix, for m \u2208 {100, 200, 300, 400}. We also experimented with Gaussian matrices, but these uniformly yielded worse results.\nWe tested the greedy reconstruction algorithms described earlier (OMP, FoBa, and CoSaMP) as well as a path-following version of Lasso based on LARS (Efron et al., 2004). Each algorithm was used to recover a ksparse label vector y\u0302k from the predicted compressed label H(x), for k = 1, . . . , 10. We measured the \u211322 distance \u2016y\u0302k\u2212y\u201622 of the prediction to the true test label y. In addition, we measured the precision of the predicted\nsupport at various values of k using the 10-sparse label prediction. That is, we ordered the coefficients of each 10-sparse label prediction y\u030210 by magnitude, and measured the precision of predicting the first k coordinates | supp(y\u030210(1:k)) \u2229 supp(y)|/k. Actually, for k \u2265 6, we used y\u03022k instead of y\u030210.\nWe used correlation decoding (CD) as a baseline method, as it is a standard decoding method for ECOC approaches. CD predicts using the top k coordinates in A\u22a4H(x), ordered by magnitude. For mean-squarederror comparisons, we used the least-squares approximation of H(x) using these k columns of A. In contrast to the other algorithms, CD does not enjoy the robustness guarantees required by our reduction."}, {"heading": "6.4. Results", "text": "As expected, the performance of the reduction, using any reconstruction algorithm, improves as the number of induced subproblems m is increased (see Figures 1 and 2; at m = 300, 400, the precision-at-k is nearly the same as one-against-all, i.e. m = 1024). When m is small and A 6\u2208 AK , the reconstruction algorithm cannot reliably choose k \u2265 K coordinates, so its performance may degrade after this point by over-fitting. But when the compression function A is in AK for a sufficiently large K, then the squared-error decreases as the output sparsity k increases up to K. Note the fact that precision-at-k decreases as k increases is expected, as fewer data will have at least k correct labels.\nAll of the reconstruction algorithms at least match or out-performed the baseline on the mean-squared-error criterion, except when m = 100. When A has few rows, (1) A \u2208 AK only for very small K, and (2) many of its columns will have significant correlation. In this case, when choosing k > K columns, it is better to choose correlated columns to avoid over-fitting. Both OMP and FoBa explicitly avoid this and thus do not fare well; but CoSaMP, Lasso, and CD do allow selecting correlated columns and thus perform better in this regime.\nThe results for precision-at-k are similar to that of mean-squared-error, except that choosing correlated columns does not necessarily help in the small m regime. This is because the extra correlated columns need not correspond to accurate label coordinates.\nIn summary, the experiments demonstrate the feasibility and robustness of our reduction method for two natural multi-label prediction tasks. They show that predictions of relatively few compressed labels are sufficient to recover an accurate sparse label vector, and as our theory suggests, the robustness of the recon-\nstruction algorithms is a key factor in their success."}, {"heading": "A. Proof of Theorem 1", "text": "Without loss of generality, assume |y1| \u2265 . . . \u2265 |yd|. Let \u2113 = k + f(k) and \u2206 = y \u2212 y(1:k). We decompose \u2206 as \u2206 = \u2211 i\u22650 yJi , where Ji = {k + i\u2113 + 1, . . . , k + (i + 1)\u2113}. Ho\u0308lder\u2019s inequality implies \u2016yJi\u201622 \u2264 \u2016yJi\u2016\u221e\u2016yJi\u20161 \u2264 \u2016yJi\u2016\u221e\u2016yJi\u22121\u20161, and for i \u2265 1, we have \u2016yJi\u22121\u20161 \u2265 \u2113\u2016yJi\u2016\u221e by definition. Therefore \u2016yJi\u201622 \u2264 \u2016yJi\u22121\u201621/\u2113 for i \u2265 1. Now exploiting the RIP guarantee for A and the above inequality, we have\n\u2016A\u2206\u20162 \u2264 \u2211\ni\u22650\n\u2016AyJi\u20162 \u2264 \u221a 1 + \u03b4 \u2211\ni\u22650\n\u2016yJi\u20162\n= \u221a 1 + \u03b4[\u2016yJ0\u20162 + \u2211\ni\u22651\n\u2016yJi\u20162]\n\u2264 \u221a 1 + \u03b4[\u2016yJ0\u20162 + \u2211\ni\u22650\n\u2016yJi\u20161/ \u221a \u2113]\n\u2264 \u221a 1 + \u03b4[\u2016\u2206\u20162 + \u2016\u2206\u20161/ \u221a \u2113].\nTherefore \u2016Ay(1:k) \u2212 h\u20162 \u2264 \u2016Ay \u2212 h\u20162 + \u2016A\u2206\u20162 \u2264 \u2016Ay \u2212 h\u20162 + \u221a 1 + \u03b4[\u2016\u2206\u20162 + \u2016\u2206\u20161/ \u221a \u2113]. Hence\n\u2016y\u0302 \u2212 y\u20162 \u2264 \u2016y\u0302 \u2212 y(1:k)\u20162 + \u2016\u2206\u20162 \u2264(1\u2212 \u03b4)\u22121/2\u2016A(y\u0302 \u2212 y(1:k))\u20162 + \u2016\u2206\u20162 \u2264(1\u2212 \u03b4)\u22121/2[\u2016Ay\u0302 \u2212 h\u20162 + \u2016Ay(1:k) \u2212 h\u20162] + \u2016\u2206\u20162 \u2264(1\u2212 \u03b4)\u22121/2[(1 + \u221a C)\u2016Ay(1:k) \u2212 h\u20162] + \u2016\u2206\u20162 \u2264(1\u2212 \u03b4)\u22121/2(1 + \u221a C)\u2016Ay \u2212 h\u20162\n+ ((1 \u2212 \u03b4)\u22121(2 + \u221a C))[\u2016\u2206\u20161/ \u221a \u2113+ \u2016\u2206\u20162].\nTo conclude, we square both sides and repeatedly simplify with the fact (x+ y)2 \u2264 2x2 + 2y2."}, {"heading": "B. Proof of Theorem 2", "text": "Without loss of generality, we assume that [ATA]i,i = 1 (this can be achieved by normalizing the columns of A) and the support of y(1:k) is {1 . . . k}. Let aj be the j-th column of A.\nWe consider three solution vectors:\n1. y: the k-sparse solution we want to compare to.\n2. y\u2032: the \u2264 (2k \u2212 1)-sparse solution obtained by running OMP starting from y, with the property that the amount of progress (reduction in squared-error) to be made by another iteration is small: at most \u2016h \u2212 Ay\u201622/k. Since y\u2032 is obtained by starting with y, it can only have smaller squared-error.\n3. y\u0302i: the actual solution returned by OMP up to the point at which it chooses a column j 6\u2208 supp(y\u2032). When this happens, we have the guarantee that |a\u22a4j (h\u2212Ay\u0302i)| \u2265 |a\u22a4\u2113 (h\u2212Ay\u0302i)| for all \u2113 \u2208 supp(y\u2032).\nLet \u2206 = y\u0302i \u2212 y\u2032. The goal is to bound \u2016A\u2206\u201622 in terms of \u2016h \u2212 Ay\u2032\u201622 and |a\u22a4j (h \u2212 Ay\u2032)|2. A key step is in bounding |a\u22a4j A\u2206|, which uses the fact that j 6\u2208 supp(\u2206) \u2286 supp(y\u2032) and that the coherence of A is bounded \u00b5(A) \u2264 0.1/k.\nProof of Theorem 2. Consider running k iterations of OMP starting at the solution y(0) = y(1:k), denoting the intermediate solutions by y(1), . . . , y(k). Let r(i) = h\u2212Ay(t) be the residual after step i, ai be the additional column of A included in y(i), and \u03c1i be the reduction in squared-error \u03c1i = \u2016h\u2212Ay(i\u22121)\u201622 \u2212\u2016h\u2212 Ay(i)\u201622 = \u2016r(i\u22121)\u201622 \u2212 \u2016r(i)\u201622 from the previous step. We want to bound \u03c1i from below. To this end, consider the progress made by using y\u0303(i) = y(i\u22121) + \u03b1iai instead of y(i), where \u03b1i = (a \u22a4 i r (i\u22121)):\n\u2016r(i\u22121)\u201622 \u2212 \u2016h\u2212Ay\u0303(i)\u201622 =\u2016r(i\u22121)\u201622 \u2212 \u2016h\u2212 (A(y(i\u22121) + \u03b1iai))\u201622 =\u2016r(i\u22121)\u201622 \u2212 \u2016r(i\u22121) \u2212 \u03b1iai\u201622 =\u2016r(i\u22121)\u201622 \u2212 (\u2016r(i\u22121)\u201622 \u2212 2\u03b1ia\u22a4i r(i\u22121) + \u03b12i ) =(a\u22a4i r (i\u22121))2.\nSince y(i) is the least-squares fit of h using the same columns of A as y\u0303(i), its reduction in error can only be greater. Hence \u03c1i \u2265 (a\u22a4i r(i\u22121))2, and thus\n\u2211k i=1 \u03c1i \u2265\u2211k\ni=1(a \u22a4 i r\n(i\u22121))2. The total reduction in error \u2211t\ni=1 \u03c1i is at most \u2016r(0)\u201622, so there must be some i\u2217 for which (a\u22a4i\u2217r (i\u2217\u22121))2 \u2264 \u2016r(0)\u201622/k, i.e.\n(a\u22a4i\u2217(h\u2212Ay(i \u2217\u22121)))2 \u2264 \u2016h\u2212Ay(1:k)\u201622/k\n(else we have the contradiction \u2016h \u2212 Ay(1:k)\u201622 \u2265\u2211 i \u03c1i \u2265 \u2211 i(a \u22a4 i r\n(i))2 > \u2016h \u2212 Ay(1:k)\u201622). Let y\u2032 = y(i \u2217\u22121) be the solution prior to this step, and let ai\u2217 be the selected column. It is clear that \u2016h\u2212 Ay\u2032\u201622 \u2264 \u2016h \u2212 Ay(1:k)\u201622 (the error could only have improved).\nWithout loss of generality, let the support of y\u2032 be (a subset of) {1 . . .2k}. Now consider the actual execution of OMP (not starting from y(1:k)). We will compare the solution returned after 2k steps to the y\u2032 defined above.\nLet i + 1 be the first time that OMP selects a column j not in {1 . . . 2k} (if this never happens, then \u2016h \u2212 Ay\u0302\u201622 \u2264 \u2016h \u2212 Ay(1:k)\u201622 and we\u2019re done). Then i \u2264 2k, and we only need to bound the error \u2016h\u2212Ay\u0302i\u201622 after time i where y\u0302i is the current OMP solution with support in {1 . . .2k}, since any later yi will have smaller error.\nLet \u2206 = y\u0302i \u2212 y\u2032, ri = h\u2212Ay\u0302i, and r = h\u2212Ay\u2032. Then\n\u2016A\u2206\u201622 = r\u22a4A\u2206+ (A\u2206\u2212 r)\u22a4A\u2206 = r\u22a4A\u2206\u2212 (ri)\u22a4A\u2206 \u2264 \u2016r\u20162\u2016A\u2206\u20162 + |(ri)\u22a4A\u2206|\nby Cauchy-Schwarz. Using the fact x \u2264 b\u221ax + c \u21d2 x \u2264 (4/3)(b2 + c) (which in turn can be checked using the quadratic formula and the fact 2xy \u2264 x2+y2), the above inequality implies\n\u2016A\u2206\u201622 \u2264 4\n3\n( \u2016r\u201622 + |(ri)\u22a4A\u2206| ) . (1)\nThe selection criterion of OMP implies that if it choose a column j > 2k at time i, then\n|a\u22a4j ri| \u2265 |a\u22a4\u2113 ri| (\u2200\u2113 \u2264 2k). (2)\nNote that since \u2206 has support {1 . . .2k}, A\u2206 = A(1:2k)\u2206 where A(1:2k) is the matrix A with zeros in all but the first 2k columns. Therefore,\n|(ri)\u22a4A\u2206| = |(ri)\u22a4A(1:2k)\u2206| \u2264\u2016(ri)\u22a4A(1:2k)\u2016\u221e\u2016\u2206\u20161 (Cauchy-Schwarz) \u2264|a\u22a4j ri|\u2016\u2206\u20161 (ineq. (2)) \u2264 ( |a\u22a4j r|+ |a\u22a4j A\u2206| ) \u2016\u2206\u20161 (triangle ineq.) \u2264 ( |a\u22a4j r|+ \u2016a\u22a4j A(1:2k)\u2016\u221e\u2016\u2206\u20161 ) \u2016\u2206\u20161 (Cauchy-Sch.) \u2264|a\u22a4j r|\u2016\u2206\u20161 + \u00b5(A)\u2016\u2206\u201621. (defn. coherence)\nTherefore, continuing from ineq. (1), we have\n3 4 \u2016A\u2206\u201622 \u2264 \u2016r\u201622 + |a\u22a4j r|\u2016\u2206\u20161 + \u00b5(A)\u2016\u2206\u201621. (3)\nNow we relate \u2016\u2206\u201621 to \u2016A\u2206\u201622. Since \u00b5(A) \u2264 1/10k and \u2206 is 2k-sparse, Lemma 6 (below) implies that \u2016A\u2206\u201622 \u2265 (1\u22120.2)\u2016\u2206\u201622. By Cauchy-Schwarz, we have\n\u2016\u2206\u201621 \u2264 2.5k\u2016A\u2206\u201622. (4)\nCombining ineq. (4) with ineq. (3) gives (3/4)\u2016A\u2206\u201622 \u2264 \u2016r\u201622 + |a\u22a4j r|\u2016\u2206\u20161 + \u00b5(A)\u2016\u2206\u201621 \u2264 \u2016r\u201622 + |a\u22a4j r|\u2016\u2206\u20161 + (1/4)\u2016A\u2206\u201621, which rearranges to become\n1 2 \u2016A\u2206\u201622 \u2264 \u2016r\u201622 + |a\u22a4j r|\u2016\u2206\u20161. (5)\nThe second term on the right side can be bounded using the fact xy \u2264 (x2 + y2)/2:\n|a\u22a4j r|\u2016\u2206\u20161 \u2264 5k\n2 |a\u22a4j r|2 + \u2016\u2206\u201621 10k\n\u2264 5k 2 |a\u22a4j r|2 + 1 4 \u2016A\u2206\u201622, (ineq. (4))\nwhereupon\n1 4 \u2016A\u2206\u201622 \u2264 \u2016r\u201622 + 2.5k|a\u22a4j r|2. (6)\nFinally, we have |a\u22a4j r| \u2264 |a\u22a4i\u2217r|, where i\u2217 \u2264 2k was defined as the next column that OMP would select when the current solution is y\u2032 (i.e. when the residual is r = h \u2212 Ay\u2032). Since |a\u22a4i\u2217r| \u2264 \u2016h\u2212 Ay(1:k)\u201622/k, and also \u2016r\u201622 = \u2016h\u2212Ay\u2032\u201622 \u2264 \u2016h\u2212Ay(1:k)\u201622, we conclude from ineq. (6) that \u2016A\u2206\u201622 \u2264 14\u2016h\u2212Ay(1:k)\u201622, so\n\u2016h\u2212Ay\u0302i\u2016 \u2264 \u2016A\u2206\u20162 + \u2016h\u2212Ay\u2032\u20162 (triangle ineq.) \u2264 \u221a 14\u2016h\u2212Ay(1:k)\u20162 + \u2016h\u2212Ay\u2032\u20162\n\u2264 (1 + \u221a 14)\u2016h\u2212Ay(1:k)\u20162.\nSquaring both sides gives the conclusion.\nLemma 6. (Donoho et al., 2006) If y \u2208 Rd is k-sparse and \u00b5(A) \u2264 \u03b4/(k \u2212 1), then \u2016Ay\u201622 \u2265 (1 \u2212 \u03b4)\u2016y\u201622."}, {"heading": "C. Output Range of Induced Regression Problems", "text": "Lemma 8 implies that the output range of the regression problems induced by our reduction is bounded by O( \u221a k log(m)/m). Its proof is an easy consequence of the following concentration theorem.\nLemma 7 (Eq. (2.35) in (Ledoux, 2001)). Let \u03b3 be the standard D-dimensional Gaussian distribution N(0, ID). Then, for every C-Lipschitz function F : R D \u2192 R and every r \u2265 0,\n\u03b3({F \u2265 EF + r}) \u2264 e\u2212r2/2C2 .\nTo apply Lemma 7 to functions of matrices, the function should be Lipschitz with respect to the Frobenius norm \u2016 \u00b7 \u2016F . Lemma 8. Let \u0398 \u2208 Rm\u00d7d, where \u0398i,j are i.i.d. standard Gaussian random variables, and let Y = {y \u2208 {0, 1}d : \u2016y\u20160 \u2264 k}. Then supy\u2208Y \u2016\u0398y\u2016\u221e \u2264 O( \u221a k lnm+ \u221a k ln(1/\u03b4)) with probability at least 1\u2212\u03b4.\nProof. Let f(\u0398) = supy\u2208Y \u2016\u0398y\u2016\u221e. For J \u2286 {1, . . . , d} = [d], denote by \u0398J the matrix \u0398 with zeros in all columns except those indicated by J . First, note that for any \u0398,\u0398\u2032 \u2208 Rm\u00d7d, we have f(\u0398) \u2212 f(\u0398\u2032) \u2264 supy\u2208Y \u2016(\u0398\u2212\u0398\u2032)y\u2016\u221e \u2264 supJ\u2282[d]:|J|=k \u2016\u0398J \u2212 \u0398\u2032J\u2016\u221e \u2264 \u221a k\u2016\u0398 \u2212 \u0398\u2032\u20162 \u2264 \u221a k\u2016\u0398 \u2212 \u0398\u2032\u2016F , so f is\u221a\nk-Lipschitz (\u2016 \u00b7 \u2016\u221e is the induced matrix p-norm with p = \u221e). Next, to compute Ef(\u0398), we use the fact that supy\u2208Y \u2016\u0398y\u2016\u221e = \u221a kmaxi=1,...,m |Zi|, where Z1, . . . , Zm are i.i.d. N(0, 1). By Jensen\u2019s inequality, Emaxi |Zi| \u2264 \u221a Emaxi Z2i . Now consider exp(tEmaxi Z 2 i ) for some t > 0. Again by Jensen\u2019s, this is at most Emaxi e tZ2 i \u2264 \u2211i EetZ 2 i = m/ \u221a 1\u2212 2t, using the moment generating function of \u03c72 random variables. So, we have Emaxi Z 2 i \u2264 ln(m/ \u221a 1\u2212 2t)/t, which is at most (2.1) lnm+ 4 for t = 0.49. Applying Lemma 7 concludes the proof."}, {"heading": "D. Proof of Theorem 4", "text": "We make use of the following Chernoff bound for sums of \u03c72 random variables (since the square of standard normal random variables are \u03c72-distributed with one degree of freedom), a proof of which can be found in the Appendix A of (Dasgupta, 2000).\nLemma 9. Fix any \u03bb1 \u2265 . . . \u2265 \u03bbD > 0, and let X1, . . . , XD be i.i.d. random variables following the \u03c7 2 distribution with one degree of freedom. Then\nPr\n[ D\u2211\ni=1\n\u03bbiXi > (1 + t)\nD\u2211\ni=1\n\u03bbi\n] \u2264 exp ( \u2212Dt 2\n24 \u00b7 \u03bb \u03bb1\n)\nfor any 0 < t < 1, where \u03bb = (\u03bb1 + . . .+ \u03bbD)/D.\nProof of Theorem 4. Write A = (1/ \u221a m)[\u03b81| \u00b7 \u00b7 \u00b7 |\u03b8m]\u22a4, where each \u03b8i is an independent d-dimensional Gaussian random vector N(0, Id). Define vx = B\n\u22a4x \u2212 E[y|x] so Ex\u2016vx\u201622 = \u01eb, and assume without loss of generality that vx has full d-dimensional support. Our goal is to show that with high probability, Ex\u2016Avx\u201622 \u2264 (1 + O(1/ \u221a m))\u01eb. Using this definition and linearity of expectation, we have Ex\u2016Avx\u201622 = (1/m)Ex \u2211m i=1(\u03b8 \u22a4 i vx) 2 = (1/m) \u2211m i=1 \u03b8 \u22a4 i (Exvxv \u22a4 x )\u03b8i. Since N(0, Id) is rotationally invariant and Exvxv \u22a4 x is symmetric and positive definite, we can assume Exvxv \u22a4 x is diagonal with eigenvalues \u03bb1 \u2265\n. . . \u2265 \u03bbd > 0. Then (1/m) \u2211m i=1 \u03b8 \u22a4 i (Exvxv \u22a4 x )\u03b8i =\n(1/m) \u2211m\ni=1 \u2211d j=1 \u03bbj\u03b8 2 ij . Each \u03b8 2 ij is a \u03c7 2 random\nvariable with one degree of freedom, so E\u03b82ij = 1. Thus, the expected value of the previous sum is\u2211d\nj=1 trace(Exvxv \u22a4 x ) = Ex trace(vxv \u22a4 x ) = Ex\u2016vx\u201622. Now applying Lemma 9, with D = md variables and \u03bb = (\u03bb1 + . . .+ \u03bbd)/d, we have Pr[(1/m) \u2211 i,j \u03bbj\u03b8 2 ij >\n(1 + t)\u01eb] \u2264 exp(\u2212(mdt2/24)(\u03bb/\u03bb1)) \u2264 exp(\u2212mt2/24) (using the fact \u03bb1 \u2264 d\u03bb). This bound is \u03b4 when t = \u221a (24/m) ln(1/\u03b4)."}], "references": [{"title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "author": ["E. Allwein", "R. Schapire", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Allwein et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Allwein et al\\.", "year": 2000}, {"title": "SURF: Speeded up robust features", "author": ["H. Bay", "A. Ess", "T. Tuytelaars", "L.V. Gool"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "Bay et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bay et al\\.", "year": 2008}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "Comm. Pure Appl. Math.,", "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cand\u00e8s et al\\.", "year": 2006}, {"title": "Learning probability distributions. Doctoral dissertation, University of California", "author": ["S. Dasgupta"], "venue": null, "citeRegEx": "Dasgupta,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta", "year": 2000}, {"title": "Deterministic constructions of compressed sensing matrices", "author": ["R. DeVore"], "venue": "J. of Complexity,", "citeRegEx": "DeVore,? \\Q2007\\E", "shortCiteRegEx": "DeVore", "year": 2007}, {"title": "Solving multiclass learning problems via error-correcting output codes", "author": ["T. Dietterich", "G. Bakiri"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Dietterich and Bakiri,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Bakiri", "year": 1995}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Trans. Info. Theory,", "citeRegEx": "Donoho,? \\Q2006\\E", "shortCiteRegEx": "Donoho", "year": 2006}, {"title": "Stable recovery of sparse overcomplete representations in the presence of noise", "author": ["D. Donoho", "M. Elad", "V. Temlyakov"], "venue": "IEEE Trans. Info. Theory,", "citeRegEx": "Donoho et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Donoho et al\\.", "year": 2006}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Sensitive error correcting output codes", "author": ["J. Langford", "A. Beygelzimer"], "venue": "Proc. Conference on Learning Theory", "citeRegEx": "Langford and Beygelzimer,? \\Q2005\\E", "shortCiteRegEx": "Langford and Beygelzimer", "year": 2005}, {"title": "The concentration of measure phenomenon", "author": ["M. Ledoux"], "venue": null, "citeRegEx": "Ledoux,? \\Q2001\\E", "shortCiteRegEx": "Ledoux", "year": 2001}, {"title": "Matching pursuits with time-frequency dictionaries", "author": ["S. Mallat", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Mallat and Zhang,? \\Q1993\\E", "shortCiteRegEx": "Mallat and Zhang", "year": 1993}, {"title": "Learning object representations for visual object class recognition", "author": ["M. lek", "C. Schmid", "H. Harzallah", "J. van de Weijer"], "venue": "Visual Recognition Challange Workshop,", "citeRegEx": "lek et al\\.,? \\Q2007\\E", "shortCiteRegEx": "lek et al\\.", "year": 2007}, {"title": "Uniform uncertainty principle for Bernoulli and subgaussian ensembles", "author": ["S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann"], "venue": "Constructive Approximation,", "citeRegEx": "Mendelson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mendelson et al\\.", "year": 2008}, {"title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples", "author": ["D. Needell", "J. Tropp"], "venue": "Applied and Computational Harmonic Analysis", "citeRegEx": "Needell and Tropp,? \\Q2007\\E", "shortCiteRegEx": "Needell and Tropp", "year": 2007}, {"title": "Sparse reconstruction by convex relaxation: Fourier and Gaussian measurements", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Proc. Conference on Information Sciences and Systems", "citeRegEx": "Rudelson and Vershynin,? \\Q2006\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2006}, {"title": "Effective and efficient multilabel classification in domains with large number of labels", "author": ["G. Tsoumakas", "I. Katakis", "I. Vlahavas"], "venue": "Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data", "citeRegEx": "Tsoumakas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2008}, {"title": "Labeling images with a computer game", "author": ["L. von Ahn", "L. Dabbish"], "venue": "Proc. ACM Conference on Human Factors in Computing Systems", "citeRegEx": "Ahn and Dabbish,? \\Q2004\\E", "shortCiteRegEx": "Ahn and Dabbish", "year": 2004}, {"title": "Adaptive forward-backward greedy algorithm for sparse learning with linear models", "author": ["T. Zhang"], "venue": "Proc. Neural Information Processing Systems", "citeRegEx": "Zhang,? \\Q2008\\E", "shortCiteRegEx": "Zhang", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "A crucial observation \u2013 central to the area of compressed sensing (Donoho, 2006) \u2013 is that methods exist to recover E[y|x] from just O(k log d) measurements when E[y|x] is k-sparse.", "startOffset": 66, "endOffset": 80}, {"referenceID": 0, "context": "The loss based decoding approach (Allwein et al., 2000) suggests decoding so as to minimize loss.", "startOffset": 33, "endOffset": 55}, {"referenceID": 4, "context": "(DeVore, 2007)), the best guarantees are obtained when the matrix is chosen randomly from an appropriate distribution, such as one of the following (Mendelson et al.", "startOffset": 0, "endOffset": 14}, {"referenceID": 13, "context": "(DeVore, 2007)), the best guarantees are obtained when the matrix is chosen randomly from an appropriate distribution, such as one of the following (Mendelson et al., 2008; Rudelson & Vershynin, 2006).", "startOffset": 148, "endOffset": 200}, {"referenceID": 18, "context": "Orthogonal Matching Pursuit (OMP) (Mallat & Zhang, 1993), FoBa (Zhang, 2008), and CoSaMP (Needell & Tropp, 2007) are examples of iterative or greedy reconstruction algorithms.", "startOffset": 63, "endOffset": 76}, {"referenceID": 2, "context": "Basis Pursuit (BP) (Cand\u00e8s et al., 2006) and its variants are based on finding the minimum l1-norm solution to a linear system.", "startOffset": 19, "endOffset": 40}, {"referenceID": 8, "context": "We use the popular LARS/Lasso algorithm (Efron et al., 2004) in our experiments.", "startOffset": 40, "endOffset": 60}, {"referenceID": 1, "context": "Specifically, we identified 1024 representative SURF features points (Bay et al., 2008) from 10 \u00d7 10 grayscale patches chosen randomly from the training images; this partitions the space of image patches (represented with SURF features) into Voronoi cells.", "startOffset": 69, "endOffset": 87}, {"referenceID": 1, "context": "Specifically, we identified 1024 representative SURF features points (Bay et al., 2008) from 10 \u00d7 10 grayscale patches chosen randomly from the training images; this partitions the space of image patches (represented with SURF features) into Voronoi cells. We then built a histogram for each image, counting the number of patches that fall in each cell. Text data. The second data set was collected by Tsoumakas et al. (2008) from del.", "startOffset": 70, "endOffset": 426}, {"referenceID": 16, "context": "See (Tsoumakas et al., 2008) for details.", "startOffset": 4, "endOffset": 28}, {"referenceID": 8, "context": "We tested the greedy reconstruction algorithms described earlier (OMP, FoBa, and CoSaMP) as well as a path-following version of Lasso based on LARS (Efron et al., 2004).", "startOffset": 148, "endOffset": 168}], "year": 2017, "abstractText": "We consider multi-label prediction problems with large output spaces under the assumption of output sparsity \u2013 that the target vectors have small support. We develop a general theory for a variant of the popular ECOC (error correcting output code) scheme, based on ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multilabel regression problems to binary regression problems. It is shown that the number of subproblems need only be logarithmic in the total number of label values, making this approach radically more efficient than others. We also state and prove performance guarantees for this method, and test it empirically.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}