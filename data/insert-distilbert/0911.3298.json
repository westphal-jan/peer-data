{"id": "0911.3298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2009", "title": "Understanding the Principles of Recursive Neural networks: A Generative Approach to Tackle Model Complexity", "abstract": "recursive neural networks are non - linear adaptive models programs that are able to learn deep structured information. however, these models have not yet been broadly accepted. this fact occurs is mainly due to its inherent complexity. in particular, not only that for being extremely complex information processing models, but also because of a computational expensive ensemble learning phase. the most popular training method for these models is back - propagation initiated through creating the structure. this algorithm has always been revealed deliberately not to be the ones most appropriate for structured processing due to systemic problems of convergence, while more sophisticated training methods enhance the speed of convergence at running the expense by of increasing significantly the computational cost. in this original paper, we firstly perform an analysis of the underlying principles behind these models aimed directly at understanding their computational power. secondly, we propose an approximate second order stochastic learning algorithm. the proposed algorithm dynamically adapts slowly the learning rate throughout only the training phase of rebuilding the network without incurring excessively expensive computational effort. the selection algorithm operates in both on - line and batch modes. furthermore, the resulting learning scheme is robust against the vanishing gradients problem. the overall advantages of the proposed design algorithm type are demonstrated with proving a real - world application example.", "histories": [["v1", "Tue, 17 Nov 2009 13:17:05 GMT  (362kb)", "http://arxiv.org/abs/0911.3298v1", "11 pages, 4 figures, Recurrent Networks session ICANN 2009"]], "COMMENTS": "11 pages, 4 figures, Recurrent Networks session ICANN 2009", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["alejandro chinea"], "accepted": false, "id": "0911.3298"}, "pdf": {"name": "0911.3298.pdf", "metadata": {"source": "CRF", "title": "Understanding the Principles of Recursive Neural Networks: A Generative Approach to Tackle Model Complexity", "authors": ["Alejandro Chinea"], "emails": [], "sections": [{"heading": null, "text": "Keywords: recursive neural networks, structural patterns, machine learning, generative principles."}, {"heading": "1 Introduction", "text": "Recursive neural networks were introduced last decade [1],[2] as promising machine learning models for processing data from structured domains (i.e.: protein topologies, HTML web pages, DNA regulatory networks, parse trees in natural language processing, and image analysis amongst others). These computational models are suited for both classification and regression problems being capable of solving supervised and non-supervised learning tasks. One of the goals behind this approach was to fill the existing gap between symbolic and sub-symbolic processing models. Specifically, to develop computational schemes able to combine numerical\nand symbolic information in the same model. Moreover, the principal advantage associated to them was their ability to work with patterns of information of different sizes and topologies (i.e. trees or graphs) as opposed to feature-based approaches where the information pertinent to the problem is encoded using fixed-size vectors. Graphs are more flexible data structures than vectors since they may consist of an arbitrary number of nodes and edges, while vectors are constrained to a predefined length which has to be preserved by all patterns composing the training set. So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.\nHowever, despite the initial interest motivated by these models their development is still in its infancy. This fact can be explained in part because the inherent restrictions associated to the first models where the recursive paradigm was limited to work only with acyclic structures under causality and stationary assumptions, something too restrictive for many real-world problems. Another limiting factor to be considered is that structured domains possess very little mathematical structure. Specifically, basic mathematical operations such us computing the sum or the covariance of two graph objects are not available. Furthermore, it is also important to note that nowadays, early research problems like learning generic mappings between two structured domains (e.g.: IO-isomorphic and non IO-isomorphic structured transductions) still remains as challenging open research problems.\nAlthough some advances have been recently reported regarding not only the recursive processing of cyclic structures [7],[8] but the contextual processing of information (i.e. recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models. It is important to note that learning in structured domains has been traditionally considered a very difficult task. Furthermore, it has been recently pointed out [13] that two important future challenges for these models will rely on the design of efficient learning schemes, and tackling appropriately theoretical problems as learning structural transductions or structure inference as occur in various machine learning areas such as the inference of protein structures or parse trees.\nIn this paper we present an approximate stochastic second order training algorithm aimed to overcome the complexity of the training phase associated to these models. The main advantage of this training method is that is robust against the vanishing gradients problem. Furthermore, the resulting scheme leads to an algorithm which achieves an optimal trade-off between speed of convergence and the required computational effort. In addition, this paper puts also the emphasis on the analysis of the underlying principles of the computational model associated to recursive neural networks. This analysis will permit us to better understand their computational power. The rest of the paper is organized as follows: In the next section we analyze in detail the principles behind the computational model implemented by recursive neural networks. Section 3, is devoted to show the background of the approximate second order stochastic algorithm. In section 4, experimental results are provided comparing\nthe proposed algorithm with existing approaches. Finally, concluding remarks are outlined in section 5."}, {"heading": "2 Implications of the Recursive Neural Model", "text": ""}, {"heading": "2.1 Definitions and Notations", "text": "A graph U is a pair (V,E), where V is the set of nodes and E represents the set of edges. Given a graph U and a vertex v  V, pa[v] is the set of parents of v, while ch[v] represents the set of its children. The in-degree of v is the cardinality of pa[v], while its out-degree is the cardinality of ch[v]. Under the recursive model the patterns of information are labeled graphs. Specifically, the graph nodes contain a set of domain variables characterized by a vector of real and categorical variables. Furthermore, each node encodes a fragment of information that is believed to play an important role in the task at hand. The presence of a branch (v,w) between two nodes explicitly models a logical relationship between the fragments of information represented by nodes v and w.\nThe recursive neural network model is composed of a state transition function f and an output function g (see figure 1). These functions are usually implemented by multi-layer perceptron networks. The standard model is suited to process directed positional acyclic graphs with a super-source node. Furthermore, they implement deterministic IO-isomorphic transductions based on the following recursive state representation:\n(1)\nIn expression (1) Wf and Wg represent the synaptic weights (model parameters) of networks f and g respectively. In order to process a graph U the state transition network is unfolded through the structure of the input graph leading to the encoding network. This unfolding procedure is followed in both learning and recall phases of the neural network. The resulting network has the same structure of the input graph, while nodes are replaced by copies of the state transition network and a copy of the output network is inserted at the super-source. Afterwards, a feed-forward computation is carried out on the encoding network. More specifically, at each node v of the graph, the state a(v) is computed by the transition network as a function of the input label I(v) and the state of its children (first equation of expression (1)) with:\n(2)\nIn expression (2) the index o stands for the maximum out-degree of node v. The base case for the recursion is a(nil) = a0 which correspond to a frontier state (e.g. if node v lacks of its i-th child). At the super-source (node s) the output is computed as y = g(a(s)). It is important to note that the standard model of recursive neural networks implements transductions consisting on the mapping of input structures U into an\noutput structure Y which is always made of a single node (the output is not structured)."}, {"heading": "2.2 Generative Principles of Intelligence", "text": "Generally speaking, the notion of intelligence cannot be conceived without the existence of two fundamental principles [14]: The maximization of transfer and the recoverability principle. Basically, the first principle states that whenever it is possible an intelligent system builds knowledge by maximizing the transfer of previously acquired knowledge. Specifically, more complex structures are the result of assembling previously learnt or memorized structures (e.g. to transfer actions used in previous situations to handle new situations). However, this mechanism of transfer must be understood as a gestalt procedure (the resulting structures being more than a simply combination of previously learnt structures). In addition, the recoverability principle is associated to the concept of memory and inference. It states that a system displaying intelligence must be able to recover itself from its actions. Specifically, an intelligent system must be able to infer causes from its own current state in order to identify what it failed or succeed something not possible without the existence of memory.\nIt is has been recently shown [15] that the human information processing system follows these two generative principles. Specifically, the perception system organizes the world by using cohesive structures. Furthermore, such a structure is the result of a hierarchically organized information processing system [16],[17] that generates structure by correlating the information processed at the different levels of its hierarchy. As a result of this process, world complexity is turned into understanding by finding and assigning structure. This mechanism, amongst many other things, permits us to relate objects of different kinds. In addition, the nature of the perceived structures and their relationships is also linked to the context under consideration. For instance, the relationships between objects can be causal in a temporal context,\ngeometrical in pattern recognition problems (e.g. the shapes appearing in an image) or topological in bio-informatics problems (e.g. chemical compounds, protein structures etc)."}, {"heading": "2.3 Recursive Networks as Generative Models", "text": "Under the recursive neural networks framework the perceived structure of a problem is captured and expressed by using graphical models. In particular, the patterns used for the learning and recall phases not only encode the fragments of information (e.g.: information that can be characterized by specific attributes that are quantifiable and/or measurable) which play an important role in the machine learning problem but also the logical relationships between them. The nature of such relations is determined by the application context and attempts to explicitly model the logical correlations between fragments of information. For instance, in a temporal domain the fragments of information are events and the co-occurrence of two or more events is interpreted as an existing or possible correlation between them. Therefore, this information encoding procedure contains more knowledge rather than if such pieces of information were considered in isolation. It is important to note that the notion of information content is strongly connected to the notion of structure. Indeed, the fact of building more complex structures from more basic ones is reflecting the first of the two generative principles related to the concept of intelligence resulting in a gain not only in information representation but in information content.\nOn the other hand, the computational scheme imposed by the recursive state equation (1) leads to a sequential propagation of information which follows a reversed topological sort of the input graph during the recall phase of the network (hidden states are updated starting from leaves toward the super-source node). In addition, this flow of information is bidirectional during the learning phase. The main consequence of this message passing procedure is that the computational model of recursive networks can be viewed as an inference system that learns the hidden dependencies explicitly encoded within the structural patterns used for the training phase of the network. Furthermore, as stated in [18] recursive neural networks can be viewed as limits, both in distribution and probability, of Bayesian networks with local conditional distributions. Therefore, they implement in a simplified form the notion of recoverability."}, {"heading": "3 Reducing Complexity of the Training Phase", "text": ""}, {"heading": "3.1 Theoretical Background", "text": "The concept of learning in neural networks is associated to the minimization of some error function E(W) by changing network parameters W. In the case of recursive\nnetworks the learning phase consists in finding the appropriate model parameters for implementing the state transition network f and the output network g with regards to the given task and data. Without a loss of generality let us suppose that we express the parameters of function f and g in a vector W=[w1,w2,w3,\u2026.,wm]. A perturbation of the error function around some point of the model parameters which can be written as follows: E(W+\u2206W) = E(w1+ \u2206w1, w2+ \u2206w2,\u2026., wm+ \u2206wm). Considering the Taylor expansion of the error function around the perturbation \u2206W we obtain:\n(3)\nThe training phase consists roughly in updating the model parameters after the presentation of a single training pattern (on-line mode) or batches of the training set (off-line or batch mode). Each update of model parameters can be viewed as perturbations (e.g. noise) around the current point given by the m dimensional vector of model parameters. Let us assume a given sequence of N disturbance vectors \u2206W. Ignoring third and higher order terms in expression (3), the expectation of the error <E(W)> can be expressed as:\n(4)\nRearranging the previous expression we obtain a series expansion of the expectation of the error in terms of the moments of the random perturbations:\n(5)\nIn addition, the weight increment associated to the gradient descent rule is \u2206wi = -\u03b7gi . The third term of expression (4) concerning the covariance can be ignored supposing that the elements of the disturbance vectors are uncorrelated over the index n. This is a plausible hypothesis given that patterns are presented randomly to the network during the learning phase. Moreover, close to a local minimum we can assume that mean(\u2206wi)\u22480 (the analysis of this approximation is omitted for brevity, but they show that its effects can be neglected). Taking into account these considerations the expectation of the error is then given by:\n(6)\nFrom equation (5) it is easy to deduce that the expected value of the error increases as the variance (represented by the symbol \u03c32) of the disturbance vectors (gradients) increases. This observation suggests that the error function should be strongly penalized around such weights. Therefore, to cancel the noise term in the expected value of the error function the gradient descent rule must be changed to \u2206wi = - \u03b7gi/\u03c3(gi). This normalization is known as vario-eta and was proposed [19] for static networks. Specifically, the learning rate is renormalized by the stochasticity of the error signals. In this line, it is important to note that the error signals contain less and less information at the extent they pass through a long sequence of layers, due to the\nshrinking procedure carried out by the non-linear (hyperbolic tangent or sigmoidal functions) network units. This is a well-known problem [20] that makes very difficult to find long-term dependencies that could eventually appear encoded within the structural patterns of the training set. For the case of recursive neural networks this problem is even worst due to the unfolding procedure (error signals can traverse many replicas of the same network). However, this normalization procedure avoids the gradients vanishing thanks to the scaling of network weights."}, {"heading": "3.2 Algorithm Description", "text": "The whole algorithmic description is provided in figure 2. By inspection of the pseudo-code, the algorithm proceeds as follows: after the initialization of algorithm parameters (lines 1 and 2), the algorithm enters in two nested loops: The external or control loop is in charge of monitoring the performance of the algorithm.\nIn addition, it also performs the update of model parameters (line 12) following the derived learning rule (\u2206wi = -\u03b7gi/\u03c3(gi)). The constant \u03c6 is summed to the standard deviation of the error gradients for avoiding eventual numerical problems.\nFigure 3 shows the pseudo-code of the function S_Gradients (S stands for Structure). This function is in charge of computing the first derivatives of the error function with respect to model parameters. This function takes as arguments a structural pattern U together with its category Y and returns the error gradient vector. The details of the notation used can be found in [22].\nThe internal loop (lines 5 up to 11) operates recursively in order to obtain the variance (line 9) of the error gradients (line 7). Similarly, the mean value of the gradients is computed recursively in line 8. It is important to note that the algorithm can operate in both batch mode Dk =max (number of training patterns) or in on-line mode\nDk <<max just by selecting appropriately the value of kmax. Finally, it is important to note that the proposed algorithm can be easily adapted to other extensions of the recursive model like contextual models or graph neural networks."}, {"heading": "3.3 Preliminary Complexity Analysis", "text": "From a computational point of view, the proposed algorithm scales O(W) in terms of memory storage requirements, where W= Wf \u222a Wg is the number of parameters of the model. In addition, the computational cost scales roughly as O(NW) where N is the number of patterns in the data set. In this line, it must be noted that quasi-Newton methods that builds iterative approximations of the inverse Hessian lead in general, to algorithms with overall computational cost of O(NW2) and memory storage requirements of O(W2). Similarly, conjugate-gradients method achieves a memory storage of O(W) at the same computational cost of a quasi-Newton method. In addition, it is important to note that a rigorous complexity analysis would require a careful study of the statistical distribution of the gradient errors throughout the optimization procedure followed by an analysis of the generating functions associated to the recursions (lines 8 and 9 of pseudo-code of figure 2).\nTherefore, taking into account that the proposed learning rule behaves like a stochastic approximation of a quasi-Newton method, the proposed algorithm achieves a good trade-off in terms of memory storage and computational complexity."}, {"heading": "4 Experimental Results", "text": "The performance of the algorithm was tested for the problem exposed in [21]. Specifically, this application comes from the intelligent transportation research field and consists on the development of an advanced intersection safety system. The ultimate goal is to provide appropriate warnings to the driver to avoid fatal collisions. For this task, the structural patterns are trees ranging from one up to sixteen levels depth encoding temporal situations at road intersections. Generally speaking, a road intersection situation is composed of a set of dynamic (eg: vehicles, pedestrians, traffic lights, etc) and static entities (eg: trees, bushes, road signs, etc) interacting during a variable time frame. For this application, the pattern set is composed of 4000 structural patterns where approximately half of them representing highly-risky situations (e.g.: situations leading to collisions). The dynamic aspects of road intersections are encoded within the topology of the trees while static aspects are encoded within the label space of the tree-graphs. This task provides an illustrative example of how an extremely complex problem is modeled using the recursive paradigm.\nFigure 4 provides a comparison of Back-propagation through the structure (Bpts), a quasi-Newton through the structure algorithm [21] (Qnts) and the proposed algorithm (Vets) running in batch mode. Graphics depict the result of averaging 10 simulations for two different network architectures. Specifically, each simulation consist on running a training phase of 20 epochs for each algorithm starting from identical weight initialization conditions. Afterwards, the resulting error values are normalized for the three algorithms in the interval [0,1] (the value that is mapped to 0 is the minimum value reached at epoch 20 by the best performing algorithm).\nFinally, the normalized error values are averaged over the 10 simulations. The left side of the figure shows the averaged results for an architecture of 23x160x1 (23 units implementing the state transition function and 161 units for the output function). Similarly, the right side of the picture shows the results for a 60x80x1 architecture. Due to the prohibitive memory requirements of the quasi-Newton algorithm (Hessian matrix contains more than 107 elements) the comparison was only possible with the Bpts algorithm for this network architecture.\nTherefore, although further experimentation must be carried out the proposed stochastic algorithm provides a good trade-off between the memory storage requirements and algorithm complexity."}, {"heading": "5 Conclusions", "text": "In this paper we have described the principles behind the recursive neural network model. It was shown that associated with any given problem, the information content presents a certain geometry that these models can attempt to exploit. Furthermore, the fact of using structured representations of information is translated into a substantial gain in information content. In addition, in order to tackle the inherent complexity of these models a stochastic learning algorithm was also described. The proposed algorithm is able to achieve a good trade-off between speed of convergence and the computational effort required by setting the local learning rate for each weight inversely proportional to the standard deviation of its stochastic gradient. The scaling properties of the algorithm make it suitable for the computational requirements of the recursive model. Furthermore, the proposed learning scheme can be easily adapted to other recursive models such as contextual models or graph neural networks. The computer simulations demonstrated the efficiency of the algorithm for a practical learning task."}], "references": [{"title": "Learning Task-Dependent Distributed Structure-Representations by Backpropagation Through Structure", "author": ["C. Goller", "A. Kuchler"], "venue": "Proceedings of the IEEE International Conference on Neural Networks (ICNN 1996), pp. 347-352, Washington", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "A General Framework for Adaptive Processing of Data Structures", "author": ["P. Frasconi", "M Gori", "A. Sperduti"], "venue": "IEEE Transactions on Neural Networks 9 (5), pp. 768-786", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning Protein Secondary Structure From Sequential and Relational Data", "author": ["A. Ceroni", "P. Frasconi", "G. Pollastri"], "venue": "Neural Networks. 18, 1029-1039", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "The Pricipled Design of Large-Scale Recursive Neural Networks Architectures-DAG-RNNs and the Protein Structure Prediction Problem", "author": ["P. Baldi", "G. Pollastri"], "venue": "Journal of Machine Learning Research. 4, 575-602", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Similarity Learning for Graph-based Image Representations", "author": ["C.D. Mauro", "M. Diligenti", "M. Gori", "M. Maggini"], "venue": "Pattern Recognition Letters. 24(8), 1115-1122", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards Incremental Parsing of Natural Language Using Recursive Neural Networks", "author": ["F. Costa", "P. Frasconi", "V. Lombardo", "G. Soda"], "venue": "Applied Intelligence. 19, 9-25", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Recursive Processing of Cyclic Graphs", "author": ["M. Bianchini", "M Gori", "L. Sarti", "F. Scarselli"], "venue": "IEEE Transactions on Neural Networks 9 (17), pp. 10-18", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "A New Model for Learning in Graph Domains", "author": ["M. Gori", "G. Monfardini", "L. Scarselli"], "venue": "Proceedings of the 18 IEEE International Joint Conference on Neural Networks, pp. 729734, Montreal", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive Contextual Processing of Structured Data by Recursive Neural Networks: A Survey of Computational Properties", "author": ["B. Hammer", "A. Micheli", "A. Sperduti"], "venue": "Hammer, B., Hitzler, P. (eds.), Perspectives of Neural-Symbolic Integration, Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual Processing of Structured Data by Recursive Cascade Correlation", "author": ["A. Micheli", "D. Sona", "A. Sperduti"], "venue": "IEEE Transactions on Neural Networks 15(6)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Universal Aproximation Capabilities of Cascade Correlation for Structures", "author": ["B. Hammer", "A. Micheli", "A. Sperduti"], "venue": "Neural Computation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Support Vector Machine Learning for Interdependent and Structured Output Spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "Brodley, C. E. (Ed.), ICML\u201904: Twenty-first international conference on Machine Learning. ACM Press, New York", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Editorial of the Special issue on Neural Networks and Kernel Methods for Structured Domains", "author": ["B. Hammer", "C. Saunders", "A. Sperdutti"], "venue": "Neural Networks 18 (8), 1015-1018", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A Generative Theory of Shape", "author": ["M. Leyton"], "venue": "LNCS, vol 2145, pp. 1-76. Springer-Verlag", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Symmetry, Causality, Mind", "author": ["M. Leyton"], "venue": "MIT Press, Massachusetts", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "The Computational Brain, MIT", "author": ["P. Churchland", "T. Sejnowski"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Confabulation Theory: The Mechanism of Thought", "author": ["R. Hecht-Nielsen"], "venue": "In : Proceedings of the 20 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Recursive neural networks were introduced last decade [1],[2] as promising machine learning models for processing data from structured domains (i.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Recursive neural networks were introduced last decade [1],[2] as promising machine learning models for processing data from structured domains (i.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "Although some advances have been recently reported regarding not only the recursive processing of cyclic structures [7],[8] but the contextual processing of information (i.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Although some advances have been recently reported regarding not only the recursive processing of cyclic structures [7],[8] but the contextual processing of information (i.", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Furthermore, it has been recently pointed out [13] that two important future challenges for these models will rely on the design of efficient learning schemes, and tackling appropriately theoretical problems as learning structural transductions or structure inference as occur in various machine learning areas such as the inference of protein structures or parse trees.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Generally speaking, the notion of intelligence cannot be conceived without the existence of two fundamental principles [14]: The maximization of transfer and the recoverability principle.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "It is has been recently shown [15] that the human information processing system follows these two generative principles.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Furthermore, such a structure is the result of a hierarchically organized information processing system [16],[17] that generates structure by correlating the information processed at the different levels of its hierarchy.", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "Furthermore, such a structure is the result of a hierarchically organized information processing system [16],[17] that generates structure by correlating the information processed at the different levels of its hierarchy.", "startOffset": 109, "endOffset": 113}, {"referenceID": 0, "context": "Afterwards, the resulting error values are normalized for the three algorithms in the interval [0,1] (the value that is mapped to 0 is the minimum value reached at epoch 20 by the best performing algorithm).", "startOffset": 95, "endOffset": 100}], "year": 2009, "abstractText": "Recursive Neural Networks are non-linear adaptive models that are able to learn deep structured information. However, these models have not yet been broadly accepted. This fact is mainly due to its inherent complexity. In particular, not only for being extremely complex information processing models, but also because of a computational expensive learning phase. The most popular training method for these models is back-propagation through the structure. This algorithm has been revealed not to be the most appropriate for structured processing due to problems of convergence, while more sophisticated training methods enhance the speed of convergence at the expense of increasing significantly the computational cost. In this paper, we firstly perform an analysis of the underlying principles behind these models aimed at understanding their computational power. Secondly, we propose an approximate second order stochastic learning algorithm. The proposed algorithm dynamically adapts the learning rate throughout the training phase of the network without incurring excessively expensive computational effort. The algorithm operates in both on-line and batch modes. Furthermore, the resulting learning scheme is robust against the vanishing gradients problem. The advantages of the proposed algorithm are demonstrated with a real-world application example.", "creator": "PScript5.dll Version 5.2.2"}}}