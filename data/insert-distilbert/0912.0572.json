{"id": "0912.0572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2009", "title": "Isometric Multi-Manifolds Learning", "abstract": "isometric feature mapping ( isomap ) is a promising manifold access learning method. historically however, isomap fails to work on data which distribute on clusters in a single manifold or manifolds. theoretically many works have been done on extending isomap to multi - topic manifolds learning. in this milestone paper, we first proposed a new multi - manifolds learning algorithm ( m - isomap ) with help learning of a general inference procedure. currently the new algorithm preserves intra - layer manifold geodesics and multiple manifold inter - manifolds edges precisely. although compared with previous methods, this algorithm can isometrically learn discrete data distributed on precisely several manifolds. secondly, consequently the original multi - cluster manifold learning algorithm first proposed in \\ cite { dcisomap } and hence called d - c isomap has been revised so that the revised estimate d - c isomap can learn multi - manifolds data. however finally, furthermore the features and effectiveness of the proposed multi - manifolds learning algorithms itself are demonstrated and compared through experiments.", "histories": [["v1", "Thu, 3 Dec 2009 03:05:59 GMT  (1791kb,S)", "http://arxiv.org/abs/0912.0572v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mingyu fan", "hong qiao", "bo zhang"], "accepted": false, "id": "0912.0572"}, "pdf": {"name": "0912.0572.pdf", "metadata": {"source": "CRF", "title": "Isometric Multi-Manifolds Learning", "authors": [], "emails": [], "sections": [{"heading": null, "text": "\u221210 \u22128 \u22126 \u22124 \u22122 0 2 4 6 8 10 \u221210\n\u22125\n0\n5\n10\n15\n20\n(e)\n\u221210 \u22125\n0 5\n10\n0\n2\n4\n6\n8 \u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n(a)\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 2\nx 10 4\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5 x 10\n4\n\u221215 \u221210 \u22125 0 5 10 15 \u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1 Improved D\u2212C Isomap on data set A\nX\nO3\nO1\nO2\n\u221210 \u22125 0 5 10 15 \u221210\n\u22125\n0\n5\n10\n15\n20\n25 Improved D\u2212C Isomap on data set B\nX\nO3\nO1\nO2\n\u221220 \u221210 0 10 20 30 40 \u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\n10\n15\n20\n25 Improved D\u2212C Isomap on data set C\nX\nO1\nO2\nO3\n\u22121.5 \u22121\n\u22120.5 0\n0.5 1\n1.5 2\nx 10 4\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n\u22121\n\u22120.5\n0\n0.5\n1\nx 10 4\nY\nImproved D\u2212C Isomap on teapot data set\nX\n\u22123.5 \u22123 \u22122.5 \u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 \u22125\n0\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n(a)\nar X\niv :0\n91 2.\n05 72\nv1 [\ncs .L\nG ]\n3 D\nec 2\n00 9\n1\nIsometric Multi-Manifolds Learning\nAbstract\u2014 Isometric feature mapping (Isomap) is a promising manifold learning method. However, Isomap fails to work on data which distribute on clusters in a single manifold or manifolds. Many works have been done on extending Isomap to multi-manifolds learning. In this paper, we proposed a new multi-manifolds learning algorithm (M-Isomap) with the help of a general procedure. The new algorithm preserves intramanifold geodesics and multiple inter-manifolds edges faithfully. Compared with previous approaches, this algorithm can isometrically learn data distribute on several manifolds. Some revisions have been made on the original multi-cluster manifold learning algorithm called D-C Isomap [24] such that the revised D-C Isomap can learn multi-manifolds data. Finally, the features and effectiveness of the proposed multi-manifolds learning algorithms are demonstrated and compared through experiments.\nIndex Terms\u2014 Isomap, nonlinear dimensionality reduction, manifold learning, pattern analysis, multi-manifolds learning.\nI. Introduction\nChallenges, known as \u201dthe curse of dimensionality\u201d, are usually confronted when scientists are doing researches on high dimensional data. Dimensionality reduction is a promising tool to circumvent these problems. Principal component analysis (PCA) [1] and multidimensional scaling (MDS) [2] are two important linear dimensionality reduction methods. Due to their linear model assumption, both of the methods will fail to discover nonlinear intrinsic structure of data.\nRecently, there are more and more interests in nonlinear dimensionality reduction (NLDR). NLDR is used to learn nonlinear intrinsic structure of data, which is considered to be the first step of \u201dmachine learning and pattern recognition: observe and explore the phenomena\u201d [3]. Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000. LLE assumes that data points locally distribute on a linear patch of a manifold. It preserves local linear coefficients, which best reconstruct each data point by its neighbors, into a lower dimensional space. Isomap is based on classical MDS method. Instead of preserving pairwise Euclidean distance, it preserves geodesic distance on the manifold. The geodesic between two data points is approximated by the shortest path on a constructed graph. Both of the methods are computational efficient and able to achieves global optimality. Besides, there are many other important nonlinear dimensionality reduction methods. Laplacian eigenmap [7] utilizes the approximation of the Laplace-Beltrami operator on manifold to provide an optimal embedding. Hessian LLE [8] resembles Laplacian eigenmap by using the approximation of Hessian operator instead of Laplacian operator. Local tangent space alignment(LTSA) [9] method learns local geometry by constructing a local tangent space of each data point and then aligns these local tangent spaces into a single global coordinates system with respect to the underlying manifold. Diffusion maps [10] applies diffusion semigroups to produce multiscale geometries to represent complex structure. Riemannian manifold learning (RML) [11] method uses the constructed Riemannian\nnormal coordinate chart to map the input data into a lower dimensional space. NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.\nAs Isomap emphasizes on the global geometric relationship of data points, it is very illustrative in data visualization and pattern analysis [13]. Although Isomap algorithm implicitly requires the data set is convex, it still demonstrates very meaningful results on non convex data sets. Therefore, in this paper, we will focus attention on extending Isomap to multi-manifolds learning. The first step of Isomap algorithm is to construct a neighborhood graph which connects all the data points. This step is of vital importance because the success of following steps depend on how well the constructed neighborhood graph is. However, it is hard to build a totally connected neighborhood graph and guarantee the topological stability of classical Isomap algorithm when points of the data set distribute on clusters in a manifold or manifolds (multiple manifolds). Many works have been done on extending Isomap to multi-manifolds data. Some methods try to do this by providing new neighborhood graph construction algorithms. Yiming Wu et al [23] introduced a split and augment procedure for neighborhood graph construction which could produce a totally connected neighborhood graph. Li Yang [26]\u2013[29] introduced several neighborhood graph construction algorithms using techniques from discrete mathematics, graph theory. Deyu Meng et al [24] proposed a decomposition and composition Isomap (DC Isomap).\nThe rest of the paper is organized as follows: In Section II, main issues and limitations of classical Isomap algorithm are presented. The problem of multi-manifolds learning is also investigated. In Section III, previous methods on multi-manifolds learning are briefly introduced and discussed. In Section IV, a general procedure for designing multi-manifolds learning algorithms is first proposed. With the proposed procedure, a new multi-manifolds learning algorithm (M-Isomap) is designed and analyzed. With some revisions on the original algorithm, the main limitations of D-C Isomap are resolved. Finally, in Section V, the effectiveness of these multi-manifolds learning algorithms has been demonstrated by experiments. Comparisons of these algorithms have also been made.\nII. Classical Isometric FeatureMapping and Its Limitations\nIsomap is able to recover the intrinsic geometric structure and converge as the number of data points increases [4] if data lie on a manifold, . Like PCA and MDS, Isomap has the advantage of simple implementation and computational efficiency. The algorithm also guarantees a globally optimal solution.\nIt is assumed that data set X = {x1, x2, \u00b7 \u00b7 \u00b7 , xN} is in high dimensional space RD and the feature space is Rd. The classical Isomap algorithm has three steps.\nStep 1: Identify the neighbors for all the data points to construct a neighborhood graph. With the given parameter\n2 k or \u01eb, there are two ways to construct a neighborhood graph for X:\n\u2022 if x j is one of xi\u2019s k nearest neighbors, they are connected by an edge (the k-NN method). \u2022 xi and x j satisfy \u2016xi \u2212 x j\u2016 < \u03b5, they are connected by an edge (the \u03b5-NN method).\nStep 2: Use Dijkstra\u2019s or Floyd-Warshall algorithm to compute the length of shortest path dG(xi, x j) between any couple of data points xi and x j on the graph. It is proved that dG(xi, x j) is a good approximation of geodesic distance dM(xi, x j) on the manifold as the number of data points increases. Step 3: Perform classical MDS on the graph distance matrix DG whose (i, j)-th element is dG(i, j). Minimize a cost function\nE(Y) = \u2016\u03c4(DG) \u2212 \u03c4(DY )\u2016F2\nThe operator \u03c4 is defined as \u03c4(D) = \u2212HS H2 , where H = I \u2212 1n eeT , S = (D2i, j), I is the identity matrix and e = (1, 1, \u00b7 \u00b7 \u00b7 , 1)T . DY = (\u2016yi\u2212y j\u2016). Assuming that, in decreasing order, \u03bbi is the i-th eigenvalue of \u03c4(DG) and \u03bdi is the corresponding eigenvector to \u03bbi, then the low dimensional embedding Y is given by:\nY = [y1, , y2, \u00b7 \u00b7 \u00b7 , yn] =\n         \n\u221a \u03bb1\u03bd T 1 \u00b7 \u00b7 \u00b7\u221a \u03bbd\u03bd T d          \nThe properties of Isomap algorithm are well understood [14] [12]. However, the success of Isomap algorithm depends on two issues. One issue is how to choose the correct intrinsic dimensionality d. Setting a lower dimensionality d will lead to a loss of data structure information. On the other hand, setting a higher dimensionality d will cause that redundant information is kept. This issue has been well investigated. The other issue is the quality of the constructed neighborhood graph. It is known that the problem on neighborhood graph construction is still a tricky one. Both the k-NN and \u03b5-NN methods have their limitations. Under the assumption that data points distribute on a single manifold, if the neighborhood size k or \u03b5 is chosen too small, the constructed neighborhood graph will be very sparse. Thus geodesics can not be satisfyingly approximated. Otherwise, if neighborhood size k or \u01eb is chosen too large to cause short-circuit edges, these edges will have a significant negative influence on the topological stability of Isomap algorithm [22].\nNonetheless, it is a relative simpler problem if data points distribute uniformly on one manifold. Both the \u201dshort circuit\u201d and \u201ddiscontinuity\u201d problem can be circumvented by carefully choosing an appropriate neighborhood size k or \u03b5. It is a different problem if the data distribute on clusters or manifolds. k-NN or \u03b5-NN do not guarantee that the whole data set is totally connected and the quality of approximated geodesics.\nIn real world, \u201ddata missing\u201d and \u201ddata mixture\u201d are common problems in data analysis. Under manifold assumption, these two problems cause that data distribute on different clusters in a manifold or manifolds. Here, the main problems of multimanifolds learning are presented, the data may have these properties: First, data points on different manifolds may have different input dimensionality D (dimensionality of the ambient space). This usually happens in \u201ddata mixture\u201d cases. Second, learning\ndifferent data manifolds may need different value of input parameters, i.e. appropriate neighborhood size (k or \u03b5) and intrinsic dimensionality d for each data manifold. The case when data points distribute on pieces of a single manifold is referred to as multi-cluster manifold learning; meanwhile, the case when data points distribute on multiple manifolds is referred to as multimanifolds learning. In this paper, we will concentrate on designing multi-manifolds learning algorithms to data with these properties.\nIII. PreviousWorks onMulti-Manifolds Learning"}, {"heading": "A. Multi-manifolds learning by new neighborhood graph construction method", "text": "Wu and Chan [23] proposed a split-augment approach to construct a neighborhood graph. Their method can be regarded as a variation of the k-NN method and can be summarized as below:\n1. k-NN method is applied to the data set. Every data point is connected with its neighbors. If the data lies on multiple manifolds, several disconnected graph components (data manifolds) will be formed. 2. Each couple of graph components are connected by their nearest couple of inter-components points.\nThis method is simple to implement and has the same computational complexity as k-NN method. However, as there is only one edge connecting every two graph components, geodesics across components are poorly approximated; meanwhile, their low dimensional embedding can be rotated arbitrarily. This method can not be directly applied to data lying on three or more data manifolds. If more than two graph components exist, intracomponent shortest distances may be changed in the totally connected graph.\nLi [26]\u2013[29] introduced four methods to construct a connected neighborhood graph. The k minimum spanning trees (k-MST) [26] method repeatly extracts k minimum spanning trees (MSTs) from the complete Euclidean graph of all data points. Edges of the k MSTs form a k-connected neighborhood graph. Instead of extracting k MSTs, the minimum-k-spanning trees (min-kST) [27] method finds k edge-disjoint spanning trees from the complete Euclidean graph, and the sum of the total edge length of the k edge-disjoint spanning trees attains a minimum. The kedge-connected (k-EC) [28] method constructs a connected neighborhood graph by adding edges in a non-increasing order from the complete Euclidean graph. An edge is added if its two end vertices do not have k edge-disjoint paths connected with each other. The k-vertices-connected (k-VC) [29] method add edges in a nonincreasing order from the complete Euclidean graph, an edge is added if its two end vertices would be disconnected by removing some k \u2212 1 vertices. Finally, the constructed neighborhood graph would not be disconnected by removing any k \u2212 1 vertices.\nThe methods introduced in [26]\u2013[29] advantage over k-NN method for two reasons: First, the local neighbor relationship is affected by the global distribution of data points. This is beneficial for adaptively preservation of the global geometric metrics. Second, these methods could guarantee that the constructed neighborhood graph is totally connected. Compared with k-NN method, Li\u2019s methods construct a neighborhood graph with more edges corresponding to the same neighborhood size k. This property can assure the quality of the neighborhood graphs.\n3"}, {"heading": "B. Multi-manifolds learning by decomposition-composition Isomap", "text": "In [24], Meng et al. proposed a decomposition-composition method (D-C Isomap) which extends Isomap to multi-cluster manifold learning. The purpose of their method is to preserve intra-cluster and inter-cluster distances separately. Because a revised version of D-C Isomap will be introduced in the next section, we present the details of D-C Isomap algorithm in the following\nStep I: decomposition process 1. Given neighborhood size k or \u03b5, if the data is of multi-\ncluster, several disconnected graph components can be identified when every data point is connected with its neighbors by k-NN or \u03b5-NN. 2. Assuming that there are M components, the m-th component is also denoted as a cluster Xm = {xm1 , \u00b7 \u00b7 \u00b7 , xmlm }. Clusters Xm and Xn are connected by their nearest inter-cluster data points nxmn and nx n m whose pairwise distance is assumed to\nbe d0m,n. 3. Apply k-NN Isomap or \u01eb-NN Isomap on each cluster Xm.\nDenote the geodesic distance matrix for Xm as Dm = (Dmi, j), the corresponding low dimensional embedding as Ym = {ym1 , \u00b7 \u00b7 \u00b7 , ymlm }, and the embedding point corresponding to nx m n as nymn , where ny m n \u2208 Ym . Step II: composition process 1. The set of centers of clusters is denoted as CX =\n{cx1, \u00b7 \u00b7 \u00b7 , cxM}, where every center is computed by\ncxm = arg min xi\u2208Xm\n(\nmax x j\u2208Xm\n(Dmi j)\n)\nm = 1, \u00b7 \u00b7 \u00b7 , M.\n2. The distance matrix for CX can be computed by\nD\u0303 = {D\u0303mn}, D\u0303mn = { dm,n + d0m,n + dn,m m , n 0, m = n\nwhere dm,n is the distance of shortest path between cxm and nxmn on the graph component X\nm. 3. Plug distance matrix D\u0303 and neighborhood size d into clas-\nsical Isomap algorithm. The embedding of CX is denoted by CY = {cy1, \u00b7 \u00b7 \u00b7 , cyM} \u2282 Rd (CY is called the translation reference set). Assuming that the d nearest neighbors of cxm are {cxm1 , \u00b7 \u00b7 \u00b7 , cxmd }, the low dimensional representation corresponding to nxmmi is computed as\nsymmi = cym + dm,mi\ndm,mi + d 0 m,mi + dmi,m\n(cymi \u2212 cym)\ni = 1, \u00b7 \u00b7 \u00b7 , d, m = 1, \u00b7 \u00b7 \u00b7 , M. 4. Construct the rotation matrix Am for Ym,m = 1, \u00b7 \u00b7 \u00b7 , M.\nAssuming that QNm is the principal component matrix of NYm = {nymm1, \u00b7 \u00b7 \u00b7 , nymmd } and QS m is the principal component matrix of S Ym = {symm1, \u00b7 \u00b7 \u00b7 , symmd }, then the rotation matrix for Ym is Am = QS mQNTm. 5. Transform Ym,m = 1, \u00b7 \u00b7 \u00b7 , M into a single coordinate system by Euclidean transformations\nFYm = { f ymi = Amymi + cym, i = 1, \u00b7 \u00b7 \u00b7 , lm}, m = 1, \u00b7 \u00b7 \u00b7 , M.\nThen Y = \u22c3M m=1 FY m is the final output.\nFirstly, the D-C Isomap reduces the dimensionality of clusters separately; meanwhile, it preserves a skeleton of the whole data. Secondly, using Euclidean transformations, embedding of each cluster is placed into the corresponding position by referring to the skeleton. In this way, intra-cluster geodesics are exactly preserved. As D-C Isomap method uses circumcenters to construct the skeleton of whole data, its learning results unstably depend on the mutual position of these circumcenters. It is known that at least d+ 1 reference points are needed to anchor a d-dimensional simplex. However, in D-C Isomap algorithm, the number of the reference data points is limited by the number of clusters."}, {"heading": "C. Constrained Maximum Variance Mapping", "text": "There is also a newly proposed algorithm called constrained maximum variance mapping (CMVM) [25] for multi-manifolds learning. CMVM method is proposed on the notion of maximizing dissimilarities between classes while holding up the intra-class similarity.\nIV. IsometricMulti-Manifolds Learning"}, {"heading": "A. The general procedure for isometric multi-manifolds learning", "text": "Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29]. However, shortest paths across clusters or data manifolds are bad approximations of geodesics. In Isomap, bad local approximation always leads to the deformation of global low dimensional embedding.\nUnder the continuous assumption, it is assumed that \u2126 is an open, convex and compact set in Rd, and f : \u2126 \u2192 RD, d << D is a continues mapping. f (\u2126) = M is defined as a d dimensional parameterized manifold. K(x, y) (x, y \u2208 M) is a specially defined kernel and a reproducing kernel Hilbert space (RKHS) H is constructed with this kernel. \u03c6 j(x) is the eigenfunction corresponding to the j-th largest eigenvalue \u03bb j of K(x, y) in H , which is also the j-th element of Isomap embedding. The geodesic distance on manifold M is written as d2(x, y) = d2( f (\u03c4), f (\u2227\u03c4)) = \u03b1\u2016\u03c4 \u2212 \u2227\u03c4\u2016 + \u03b7(\u03c4, \u2227\u03c4), where \u03c4, \u2227\u03c4 \u2208 \u2126, \u03b1 is a constant and \u03b7(\u03c4, \u2227\u03c4) is the deviation from isometry. The constant vector\nC =\n\u222b\nM x\u03c1(x)dx \u222b\nM \u03c1(x)dx =\n\u222b\n\u2126 \u03c4H(\u03c4)d\u03c4 \u222b\n\u2126 H(\u03c4)d\u03c4\nwhere \u03c1(x) and H(\u03c4) are density functions of M and \u2126. With the assumptions above, the following theorem is proved by Zha et al [12].\nTheorem 4.1: There is a constant vector P j such that \u03c6 j(x) = PTj (\u03c4 \u2212 C) + e j(\u03c4), where e j(\u03c4) = \u01eb (0) j \u2212 \u01eb j(\u03c4) has zero mean, i.e., \u222b\n\u2126 H(\u03c4)e j(\u03c4)d\u03c4 = 0, with \u01eb j(\u03c4) = 12\u03bb j\n\u222b\n\u2126 \u03b7(\u03c4,\n\u2227 \u03c4)H(\u2227\u03c4)\u03c6 j(x)d \u2227 \u03c4, \u01eb(0)j =\n1 \u222b\n\u2126 H(\u03c4)d\u03c4\n\u222b\n\u2126 \u01eb j(\u03c4)H(\u03c4)d\u03c4.\nBy theorem 4.1, even if the deviation \u03b7(\u03c4, \u2227 \u03c4) is not zero with only a limited range of (\u03c4, \u2227 \u03c4). The coordinate of the low dimensional embedding \u03c6 j(x) is still deformed, and the deformation is measured by e j(\u03c4).\nIn order to get a better understanding of multi-manifolds data, it is profitable to preserve intra-manifold relationship (where \u03b7(\u03c4, \u2227 \u03c4) = 0) and inter-manifolds relationship (where \u03b7(\u03c4, \u2227 \u03c4) , 0) separately. This is because that sometimes we care more about the information within the same data manifold. Here we propose\n4\na general procedure for isometric multi-manifolds learning algorithms.\nStep I: The decomposition process 1. Cluster the whole data set. If data distribute on different\nclusters in a manifold or manifolds, the clusters or manifolds should be identified. Many clustering method could be used, such as K-means, Isodata and methods introduced in [15] [33]. Even if the manifolds overlay with each other, they can still be identified and clustered [39]. At this step, data set X is clustered into several components and every component is considered as a data manifold. 2. Estimate parameters of data manifolds. For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37]. Assume that dm is the intrinsic dimensionality of the m-th data manifold. Let d = max dm\nm .\nFor neighborhood size, [32] introduces a method on automatically generating parameters for Isomap on one single data manifold. For convenience, appropriate neighborhood sizes (km or \u03b5m for Xm) could be given manually for data manifolds. 3. Learn the data manifolds individually. One data manifold can be learned by traditional manifold learning algorithms. Here, we propose to rebuild a graph on each data manifold with new neighborhood size to better approximate intramanifold geodesics. Methods of Li\u2019s works [26]\u2013[29] or the k-CC method is preferred, where the k-CC graph construction method will be described later. It is assumed that the low dimensional embedding for Xm is Ym.\nStep II: The composition process 1. Preserve a skeleton I of the whole data set in low dimen-\nsional space Rd. The skeleton I should be carefully designed such that it can represent the global structure of X. Let RYI be the low dimensional embedding of I. 2. Transform Yms into a single coordinate system by referring to I. In order to faithfully preserve intra-manifold relationship, Euclidean transformations could be constructed and used. Using embedding points RYm \u2282 RY I and corresponding points from Ym, we can construct an Euclidean transformation from Ym to the coordinate system of I.\nAlthough the idea about decomposition-composition is not new,\nwhich is first used by Wu et al. [23] in their split-augment process and well developed and used in [24]. The procedure we proposed here aims to solve a more general problem. Step I.1 permits that the designed learning method has a good ability to identify data manifolds. Step I.2 gives a guideline on learning manifolds with different intrinsic dimensionality and neighborhood sizes. Step I.3 learns data manifolds individually so that the intra-manifold relationship can be faithfully preserved. Step II.1 is the most flexible part of the procedure which allows us to design new multimanifolds learning algorithms. A well designed skeleton I could better represent the inter-manifolds relationship. In the following, we will introduce a new multi-manifolds learning algorithm and revise the original D-C Isomap algorithm with the help of this general procedure."}, {"heading": "B. A new algorithm for isometric multi-manifolds learning", "text": "Based on the proposed procedure, we designed a new multimanifolds learning algorithm. As an extension of Isomap method for multi-manifolds data, the method will be referred to as multimanifolds Isomap (M-Isomap). It is assumed that X is also interchangeable to represent the matrix [x1, x2, \u00b7 \u00b7 \u00b7 , xN], where {xi, i = 1, \u00b7 \u00b7 \u00b7 , N} are column vectors in RD.\n1) Using k-CC method to construct a neighborhood graph and identify manifolds: Table II shows the time complexity of kNN, K-Min-ST, k-EC and k-VC methods on neighborhood graph construction. As it is shown in the table, k-NN method has the lowest computational complexity O(kN2).\nFor incremental learning, the computational complexity of kNN, k-MSTs and k-VC are O(kN), O(N log N) and O(N log N + kN) respectively [30] [31]. The computational complexity of Mink-ST, k-EC methods for incremental learning are unavailable. For data on one single data manifold, the improvement of performance\n5 of Li\u2019s methods becomes insignificant when the neighborhood size k for k-NN method increases. More importantly, k-NN implicitly has the property of clustering to multi-manifolds data. Data points of the same manifold tend to be connected by paths and disconnected otherwise when every data point is connected with its neighbors by edges. Although k-NN is not a robust clustering algorithm, it is computational efficient for both clustering and graph construction. Therefore, we introduce a variation of k-NN method which inherits computational advantage of kNN method. The method is also able to identify data manifolds and construct a totally connected neighborhood graph. In the rest of the paper, the proposed neighborhood graph construction method will be referred as k-edge connected components (the k-CC method).\nThe summary of k-CC algorithm: First, given a neighborhood size k or \u03b5, every data point is connected with its neighbors. If the data points distribute on several clusters or manifolds, several disconnected graphs will be constructed. Data points are assigned to the same data manifold if there is a path connects them on the graphs. Then, we connect each pair of graphs by k nearest pairs of data points. Concerning about robustness of the algorithm, every data point is only allowed to have one inter-manifolds edge at most.\nAlgorithm:(k-CC method) Input: Euclidean distance matrix D, whose (i, j)-th entry is \u2016xi \u2212 x j\u2016. Neighborhood size k or \u03b5. Output: Graph G = (V, E), number of clusters M, label of the data.\nInitialization: V = {x1, \u00b7 \u00b7 \u00b7 , xn}, V \u2032 = V, E = \u03c6, Queue = \u03c6 1: for i=1 to N do 2: Identify nearest neighbors {xi1, \u00b7 \u00b7 \u00b7 , xili } for xi by k-nearest-\nneighbors or \u03b5-nearest-neighbors. Let E = E \u22c3{ei1, \u00b7 \u00b7 \u00b7 eili }"}, {"heading": "3: end for", "text": "4: Set M = 1 5: while {V \u2032 is not empty} do 6: x \u2208 V \u2032, in-Queue{x}, label(x)=M, V \u2032 = V \u2032 \u2212 {x} 7: while {Queue is not empty} do 8: x=de-Queue \u2200y: y is connected with x by an edge 9: if {y is not labeled} do 10: in-Queue{y}, label(y)=M, V \u2032 = V \u2032 \u2212 {y} 11: end if 12: end while 15: M = M + 1 16: end while 17: M = M \u2212 1 18: if ( M \u2265 2 ) 19: k =average({li}Ni=1) 20: for i = 1 to M do 21: for j = i + 1 to M do 22: Find k shortest inter-manifolds edges e1, \u00b7 \u00b7 \u00b7 , ek between\ndata manifolds i and j and make sure that their ending vertices are not identical. Let E = E\n\u22c3{e1, \u00b7 \u00b7 \u00b7 , ek} 23: end for 24: end for 25: end if\nThe main difference between k-NN and k-CC is lines (4-25),\nwhich identify components (data manifolds) and connect different components of the graph. This change makes the constructed graph totally k-edge connected. Compared with the method proposed in [23], k-CC method constructs a neighborhood graph with k inter-manifolds edges, which is able to control the rotation of the embedding of data manifolds. In Section V, the method, which uses k-CC to construct a totally connected graph and then perform classical Isomap on the graph, will be referred to as kCC Isomap. It can be easily inferred that k-CC Isomap suffers the limitation which has been shown by Theorem 4.1.\nAt this step, it is assumed that X is clustered into data manifolds {Xm}Mm=1 and {xmn(i)}ki=1 is the subset of Xm whose data points connect with Xn(i), i = 1, \u00b7 \u00b7 \u00b7 , k.\n2) Learn data manifolds individually: As Xm is considered as a single data manifold in RD, it is possible to find its intrinsic parameters. The incising ball method [37] is utilized to estimate the intrinsic dimensionality, which is simple to implement and always outputs an integer result. Assume that d is the highest intrinsic dimensionality of data manifolds. Neighborhood size km or \u03b5m of each data manifold is given manually and the graph on data manifold Xm is rebuilt. It is hoped that the new neighborhood graph on Xm can make better approximations of intra-manifold geodesics. The approximated geodesic distance matrix for Xm is written as Dm. By applying classical MDS on Dm , the low dimensional embedding for Xm can be got as Ym = {ymi } S m i=1.\n3) Preserve a skeleton of data manifold X: First, intermanifolds distances are computed. Assuming that xmp and x n q are any data points with xmp \u2208 Xm and xnq \u2208 Xn, their distance can be computed by\nd(xmp , x n q) = min t=1\u00b7\u00b7\u00b7k {dm(xmp , xmn(t)) + \u2016xmn(t), xnm(t\u2032)\u2016 + dm(xnn(t\u2032), xnq)}, (1)\nwhere d(xmp , x m n(t)) is the shortest path on the neighborhood graph of Xm. Although d(xmp , x m n(t)) may not be the shortest path on the totally connected graph of X, Eq. (1) is an efficient way to approximate distances across manifolds. Dmn is assumed to be the distance matrix across over Xm and Xn. The furthest intermanifolds data points are computed by\n{ f xmn , f xnm} = arg max d(xmi , xnj ), d(xmi , xnj ) \u2208 Dmn. (2)\nWithout lose of generality, we assume Im = {xmi } lm i=1 = \u22c3M n=1{xmn(1), \u00b7 \u00b7 \u00b7 , xmn(k), f xmn } . I = \u22c3M m=1 I\nm is considered as the global skeleton of X. On the data manifold X, it can be seen that the skeleton I formulates a sparse graph. We assume that DI = (dI (i, j)) is the distance matrix of I, where\ndI(i, j) =\n{ d(xmi , x n j ) \u2208 Dmn, xi \u2208 Xm, x j \u2208 Xn\nd(xmi , x m j ) \u2208 Dm, xi, x j \u2208 Xm\n(3)\nBy applying classical MDS algorithm on DI , the low dimensional embedding of I can be got as RYI . It is assumed that RYmI \u2282 RYI is the embedding of Im and RYmI = {rymi } lm i=1.\n4) Euclidean transformations : Assuming that YmI = {ymi } lm i=1 \u2282 Ym and ymi corresponds to x m i , an Euclidean transformation from YmI to RY m I could be constructed.\n6 The general Euclidean transformation can be written as\nry = Ay + \u03b2,\nwhere A is an orthonormal matrix and \u03b2 is a position translation vector. For the m-th data manifold, it is assumed that the Euclidean transformation is\nrymi = Amymi + \u03b2m i = 1, \u00b7 \u00b7 \u00b7 , lm,\nGenerally, it could be written in form of matrix\nRYmI = AmYmI + \u03b2meT = ( Am \u03b2m )\n(\nYmI eT\n)\n(4)\nwhere e is a vector with all ones. Problem (4) can be solved by the least square strategy, and the solution is\n( Am \u03b2m ) = RYmI\n(\nYmI eT\n)T  \n   \n(\nYmI eT\n) (\nYmI eT\n)T\n+ \u03bbI\n     \n\u22121\n(5)\nwhere I is the identity matrix and \u03bb is the regularization parameter in case singular. However, least square solution does not provides an orthonormal matrix Am. Here we propose to use the orthonormal matrix which is computed by QR decomposition. The QR process can be written as\n( Am R ) = QR(Am). (6)\nwith the diagonal elements of R to be forced non negative. Then \u03b2m can be recomputed by minimizing a cost function\nC(\u03b2m) = lm \u2211\ni=1\n\u2016Amymi + \u03b2m \u2212 rymi \u20162.\nBy taking derivative \u2202C(\u03b2m ) \u03b2m = 0, we have\n\u03b2m = 1 lm\nlm \u2211\ni=1\n(rymi \u2212Amymi ) (7)\nLow dimensional embedding Ym, i = 1, \u00b7 \u00b7 \u00b7 , M could be translated into a global coordinate system by the constructed Euclidean transformations.\n5) The complete algorithm of M-Isomap: To give an compact presentation of M-Isomap, the algorithm is summarized in the following table.\nInput: X = {xi}Ni=1 with xi \u2208 RD. Initial neighborhood size k or \u03b5. Step I.1 Perform k-CC on X. Data manifolds {Xm}Mm=1 and the set of inter-manifolds points {xmn(i)}ki=1 of Xm can be obtained. Step I.2 Estimate parameters of data manifolds. It is assumed that intrinsic dimensionality dm and neighborhood size (km or \u03b5m) are parameters for Xm. Let d = max\nm {dm} and rebuild neighborhood\ngraph on Xm. Step I.3 Classical Isomap is performed on the new graphs\nsuperimposed on Xm,m = 1, \u00b7 \u00b7 \u00b7 , M. The corresponding low dimensional embedding of Xm\nis denoted as Ym. Step II.1 Inter-manifolds distance matrix Dmn is computed\nby Eq. (1), thus { f xmn }Mm,n can be found by Eq. (2). Distance matrix DI for the skeleton I is computed by Eq. (3). Applying classical MDS on DI , we denote the low dimensional embedding of I as RYI . RYmI \u2282 RYI and RYmI is assumed to be the embedding of Im.\nStep II.2 Construct Euclidean transformations by Eq. (5-7). Using the Euclidean transformations, it is assumed that Ym,m = 1, \u00b7 \u00b7 \u00b7 , M are transformed to RYm, m = 1, \u00b7 \u00b7 \u00b7 , M. Step II.3 Y = \u22c3M\nm=1 RY m is the final output."}, {"heading": "C. Computational complexity of M-Isomap method", "text": "Computational complexity is a basic issue for application. For M-Isomap method, k-CC method needs O((k + 1)N2) time to construct a totally connected graph and identify the manifolds. Computing the shortest path on every data manifold needs O(\n\u2211M m=1 S 2 m log S m) time, and performing classical MDS on the\ndistance matrixes of data manifolds needs O( \u2211M m=1 S 3 m) time. The time complexity of computing the shortest path across data manifolds is O(\n\u2211M m<n kS mS n) and finding f x i j, f x j i is O( \u2211M m<n S mS n).\nPerforming classical MDS on skeleton I needs O(( \u2211M m=1 lm) 3) computational time. The time complexity of least square solution and QR decomposition process for M data manifolds is O(Md3). Finally, transforming Yms into a single coordinate system needs O(d2N) computational time.\nTherefore, the total time complexity of the M-Isomap method is\nO((k + 1)N2 + M \u2211\nm=1\n(S 3m + S 2 m log S m) +\nM \u2211\nm<n\n(k + 1)S mS n\n+( M \u2211\nm=1\nlm)3 + Md3 + d2N)\n. For a large data set when N >> M and N >> d, the overall time complexity of M-Isomap can be approximated by\nO((k + 1)N2 + M \u2211\nm=1\n(S 3m + S 2 m log S m) +\nM \u2211\nm<n\n(k + 1)S mS n)\n."}, {"heading": "D. The revised D-C Isomap method", "text": "D-C Isomap applies the decomposition-composition procedure. Therefore, it is able to preserve intra-cluster distances faithfully.\n7 (a) (b) r r r r nx11 nx12 nx21\nnx31\nTT\nbO1\nr\nr\nr\nr\nnx13\nnx12\nnx31\nnx21 PPPPP bO1\nFig. 1 Two basic cases of the relationship of the center and inter-manifolds points\nHowever, this method suffers from several limitations. In the following, revisions will be made on the original D-C Isomap algorithm to overcome its limitations.\n1) Selection of centers: D-C Isomap implicitly assumes that the inter-cluster point nxmn is in the line which connects centers Om and nxnm. Thus it is more sensible that Om is chosen by referring to the inter-cluster points. Fig. 1 illustrates two basic cases about the relationship of the center and inter-cluster points. Although the points nx11, nx 2 1, nx 3 1, nx 1 2 and O1 do not have to really lie on the same plane in the ambient space. It is assumed that these points formulate a triangle in the low dimensional space. Fig. 1(a) shows the case when \u2220nx11nx 2 1nx 3 1 + \u2220nx 1 2nx 3 1nx 2 1 < 180\no. In triangle \u2206O1nx31nx 2 1, the edge d(nx 2 1, nx 3 1) can be computed\nas \u2016nx21 \u2212 nx31\u2016. We also have\n\u2220O1nx 2 1nx 3 1 = arc cos < nx11 \u2212 nx21, nx31 \u2212 nx21 > \u2016nx11 \u2212 nx21\u2016\u2016nx31 \u2212 nx21\u2016\n\u2220O1nx 3 1nx 2 1 = arc cos < nx12 \u2212 nx31, nx21 \u2212 nx31 > \u2016nx12 \u2212 nx31\u2016\u2016nx21 \u2212 nx31\u2016\nSubsequently, the length of edges d(O1, nx21) and d(O1, nx 3 1) can be calculated by the Law of Sines in \u2206O1nx31nx 2 1. Suggested distances between center to inter-cluster points can be calculated as\nd\u2032(O1, nx 1 1) = d(O1, nx 2 1) \u2212 \u2016nx21 \u2212 nx11\u2016 d\u2032(O1, nx12) = d(O1, nx 3 1) \u2212 \u2016nx31 \u2212 nx11\u2016.\nFor a cluster with intrinsic dimensionality 2, it is sufficient to estimate position of O1 in the cluster by solving the following optimization problem:\nO1 = arg min o\u2208X1 f (o) (8)\nwhere\nf (o) = 2 \u2211\ni=1\n\u2016d(O1, nx1i ) \u2212 d\u2032(O1, nx1i )\u2016.\nHere d(O1, nx1i ) is the length of shortest path between O1 and nx 1 1 on graph X1. For a cluster with intrinsic dimensionality dm, at least\nr\nr\nr\nr\nnx11\nnx12\nnx21\nnx22\nr\nr\nm1\nm2\nX1 X2\nX3 r\nFig. 2 An illustration of how to add a new cluster for D-C Isomap algorithm.\ndm distances d\u2032(O1, nx1i ), i = 1, \u00b7 \u00b7 \u00b7 , dm are needed to estimate the position of center O1, and in this case f (o) is given by\nf (o) = dm \u2211\ni=1\n\u2016d(O1, nx1i ) \u2212 d\u2032(O1, nx1i )\u2016\nIf we can not find sufficient d\u2032(O1, nx1i )s to locate the center, there must be many inter-cluster points located in space as illustrated in Fig. 1(b). In this case, we have \u2220nx11nx 2 1nx 3 1 + \u2220nx12nx 3 1nx 2 1 \u2265 180o, when the center O1 could never be in the line of nx12nx 2 1 and nx 1 3nx 3 1. In order to get a better preservation of inter-cluster relationship, O1 should be placed as far as possible from these inter-cluster points. For a cluster with intrinsic dimensionality 2, it is suggested that O1 should be chosen by\nO1 = arg max o\u2208X1\n{g(o)} (9)\nwhere\ng(o) = d(o, nx11) + d(o, nx 1 2) \u2212 \u2016d(o, nx11) \u2212 d(o, nx12)\u2016\nIf the intrinsic dimensionality of X1 is dm and {nx1i , i = 1, \u00b7 \u00b7 \u00b7 , dm} is the set of inter-cluster points in X1, the function g(o) should be:\ng(o) = dm \u2211\ni< j\n(\nd(o, nx1i ) + d(o, nx 1 j ) \u2212 \u2016d(o, nx1i ) \u2212 d(o, nx1j )\u2016\n)\n2) Degenerative and unworkable cases: As original D-C Isomap algorithm relies on the position of centers to preserve inter-cluster relationship, the algorithm can not work on data under some circumstances. Considering about a simple case of two data clusters with d = 2, the method does not work because that it implicitly requires an another data cluster to provide sufficient rotation reference data points. Because that the low dimensional embedding of each clusters are relocated by referring to position of the centers. When there are three or more clusters and the centers of them are nearly in a line, the original D-C Isomap can not find the exact rotation matrix.\nTherefore, we propose an algorithm to solve the problems by adding new clusters. This algorithm applies a trial and error procedure to determine the position of the new clusters. In the following, the case about two clusters is used as an example.\n8 As shown in Fig. 2, the nearest couple of inter-cluster points of clusters X1 and X2 are assumed to be nx11 and nx 2 1. m1 is the middle point of nx11 and nx 2 1. The second nearest couple of intercluster points are nx12 and nx 2 2. m2 is the middle point of them. Then the third cluster X3 is suggested to be produced by\nX3 = m1 + \u03b3\u2016nx11 \u2212 nx21\u2016 \u00d7 m2 \u2212 m1 \u2016m2 \u2212 m1\u2016 .\nThe parameter \u03b3 can be decided by a trial and error procedure. Given a positive value \u03b2 > 1, it is assumed that X3 should satisfies\n1 \u03b2 < \u2016X3 \u2212 X1\u2016 \u2016X3 \u2212 X2\u2016 < \u03b2 (10)\nwhere \u2016X3 \u2212X1\u2016 is the shortest distance between data points from clusters X1 and X3. If condition (10) is not satisfied, \u03b3 changes in a pre-given range like {\u00b7 \u00b7 \u00b7 ,\u22123,\u22122,\u22121,\u2212 12 ,\u2212 1 3 , \u00b7 \u00b7 \u00b7 , 1 3 , 1 2 , 1, 2, \u00b7 \u00b7 \u00b7 }.\nWhen there are M clusters in the data set with M < d + 1, we can start from the couple of clusters with maximum nearest inter-cluster distance. Assume that X1 and X2 satisfy\n\u2016X1 \u2212 X2\u2016 = max i min j \u2016Xi \u2212 X j\u2016\nand nx11, nx 1 2, m1, nx 2 1, nx 2 2, m2 are defined as above. The M+1-th cluster XM+1 could be generated as\nXM+1 = m1 + \u03b3\u2016nx11 \u2212 nx21\u2016 m2 \u2212 m1 \u2016m2 \u2212 m1\u2016\nIf Xp and Xq are the two nearest clusters of XM+1, given \u03b2 > 0, it is assumed that XM+1 should satisfies\n1 \u03b2 < \u2016XM+1 \u2212 Xp\u2016 \u2016XM+1 \u2212 Xq\u2016 < \u03b2.\nIf M + 1 < d + 1, replace M by M + 1 and repeat the generating procedure presented above.\nPCA can be used to find out the dimensionality of the subspace on which the centers are lying. A new cluster is also needed if the dimensionality of the subspace is lower than d. New clusters should be added until the centers could anchor a d dimensional simplex.\n3) The complete algorithm of the revised D-C Isomap: To give a compact representation of the revised D-C Isomap algorithm, and compare the difference between the revised and original D-C Isomap algorithm, the integral revised D-C Isomap algorithm is presented as bellow:\nInput: X = {xi}Ni=1, with xi \u2208 RD. Initial neighborhood size k or \u03b5. Step I.1 The same to Step I.1 of the original D-C Isomap algorithm. Step I.2 Estimate parameters, intrinsic dimensionality {dm}Mm=1 and neighborhood sizes ({km}Mm=1 or {\u03b5m}Mm=1), of clusters. Let d = maxm dm and rebuild neighborhood graphs on clusters. If M < d + 1, new clusters should be added until M \u2265 d + 1. Step I.3-4 The same to Step I.2-3 of the original D-C Isomap algorithm. Step II.1 Centers of clusters are computed by (8) or (9). New clusters should be added until centers could anchor a d dimensional simplex. Step II.2-4 The same as Step II.2-4 of the original D-C Isomap. It is assumed that Ym is transformed to TYm. Step II.5 Y = \u22c3M\nm=1 TY m is the final output.\nV. Experiments\nA. 3-D data sets\nIn this subsection, we compare k-CC Isomap, M-Isomap and the revised D-C Isomap on three 3-D data sets. It should be noted that during all experiments, the size of the neighborhood is chosen corresponding to the best performance of each algorithm.\nFig. 3(a) is a two-manifolds data set with N = 1200 data points, and the data set is generated by the following matlab code:\nt=(1*pi/6)*(1+2*rand(1,N));\nxx=t.*cos(t);yy=t.*sin(t);\nzz =[unifrnd(1,10,1,N/2) unifrnd(16,25,1,N/2)];\nX=[xx;zz;yy];\nIt can be seen that each data manifold is intrinsically a rectangular region with 600 data points. Fig. 3(b) shows the result got by k-CC Isomap, whose neighborhood graph is constructed by using 8-CC method. It can be seen that the embedding shrinks along the edges in low dimensional space and edges of the embedding turn into noisy. Fig. 3(c) shows the result got by M-Isomap method with neighborhood size k = 8. As it can be seen, each of data manifold is exactly unrolled, and the inter-manifolds distance is precisely preserved. Fig. 3(d) illustrates the initialization step of the revised D-C Isomap algorithm. First, two data manifolds X1 and X2 are identified. Then the third data cluster X3 is constructed, where the parameter \u03bb = 0.1. Finally, centers O1 and O2 of the data manifolds are computed by referring to the nearest neighbors. The center of X3 is also the data point X3. Fig. 3(e) shows the result of the revised D-C Isomap method. It is can be seen that the embedding exactly preserves both the intra-manifold distances and inter-manifolds distances.\nFig. 4(a) is another two-manifolds data set with N = 1200 data points, and the data set is generated by the following matlab code:\nt=[unifrnd(pi*11/12,pi*14/12,1,N/2)\nunifrnd(pi*16/12,pi*19/12,1,N/2)];\nxx=t.*cos(tt);yy=t.*sin(tt);\nzz=unifrnd(1,25,1,N);\nY = [xx;zz;yy];\nEach data manifold has 600 data points. One data manifold is a\nrectangular region and another data manifold is a round region. Fig. 4(b) shows the result got by k-CC Isomap with neighborhood size k = 10. It can be seen that the rectangular region bent outwards and the round region is prolonged. Fig. 4(c) shows the result got by M-Isomap method with the neighborhood size k = 8. As it can be seen, every data manifolds is exactly unrolled, and the inter-manifolds relationship is precisely preserved. Fig. 4(d) illustrates the initialization step of the revised D-C Isomap algorithm. The parameter \u03bb = 0.5 for production of the new cluster X3. Fig. 4(e) shows the result of the revised D-C Isomap method with neighborhood size k = 5. It is can be seen that the embedding exactly preserves both the intra-manifold distances and inter-manifolds distances. Fig. 5(a) shows a three-manifolds data set with N = 1600 data points on the Swiss roll manifold. The data set is generated by the following matlab code:\n10\nt1 = [unifrnd(pi*5/6,pi*16/12,1,N/4)];\nt2 = [unifrnd(pi*18/12,pi*12/6,1,N/4)];\nt3=(5*pi/6)*(1+7/5*rand(1,N/2));\na1=t1.*cos(t1); b1=t1.*sin(t1);\nc1=[unifrnd(-1,3,1,N/4)];\na2=t2.*cos(t2); b2=t2.*sin(t2);\nc2=[unifrnd(-1,3,1,N/4)];\na3=t3.*cos(t3); b3=t3.*sin(t3);\nc3=[unifrnd(6,10,1,N/2)];\nx1=[a1;c1;b1]; x2=[a2;c2;b2]; x3=[a3;c3;b3]\nZ=[x1 x2 x3];\nThere are three rectangular regions on the Swiss roll manifold. The longest data manifold has 800 data points, the other two shorter data manifolds each contain 400 data points. Fig. 5(b) shows the result got by k-CC Isomap with neighborhood size k = 10. Because of bad approximation of the inter-manifolds geodesics, edges of the data manifolds bend outwards. Fig. 5(c) shows the result got by M-Isomap method, where the neighborhood size k is set to be 8. As it can be seen, all data manifolds are exactly unrolled, and the inter-manifolds relationships of the three data manifolds are faithfully preserved. Fig. 5(d) illustrates the initiation step of the revised D-C Isomap algorithm. Fig. 5(e) shows the result of the revised D-C Isomap method. It is can be seen that the embedding do not exactly preserve the inter-manifolds distances. That is because the shape of the data manifolds are very narrow. The selected reference data points can not efficiently relocate each piece of data manifold."}, {"heading": "B. Real world data sets", "text": "Fig. 6(a) shows some samples of the faces data [38] which contains face images of five persons 1. The data set consists of 153 images and has 34, 35, 26, 24, 34 images corresponding to each face. These images are gray scale with resolution of 112\u00d792. They are transformed into vectors in 10304-dimensional\n1 http://www.cs.toronto.edu/\u223croweis/data.html\nEuclidean space. In order to show the inter-manifolds relationship with more details, we embed the data into 3-dimensional space. Fig. 6(b) is the three dimensional embedding by PCA method. It can be observed that data manifolds of faces are mixed up, and the intra-face information is also not preserved. Fig. 6(c) is the result got by k-CC Isomap method with k = 3. As it can be seen, although the data points are clustered, their inter-face distances are not well preserved. The five lines mix up at one of their endings. Fig. 6(d) shows the result got by M-Isomap method with k = 3. Due to the limitation of k-NN method in clustering, only two data manifolds are identified. Although the data set is not well clustered, the result of M-Ismap shows that the low dimensional embedding can be separated up easily. Fig. 6(e) is the result got by the original D-C Isomap method, where the faces are spit up beforehand. The circumcenters are used as their centers. However, as it can be seen, two faces are mixed up.\nFig. 7(a) shows some samples of the teapot data set with 300 data points, where \u2019 \u2019 stands for the teapot bird-view images, \u2019\u2206\u2019 stands for the teapot back-forth rotation images and \u2019\u00a9\u2019 stands for the teapot side-view images. Each of the images is an 80\u00d760\u00d73 RGB colored picture, i.e. a vector in 14400 dimensional input space. Because the data points do not distribute on a single global manifold, this problem will poses a great challenge to classical manifold learning methods. The experiment shows that the three data manifolds can be identified by k-CC method. In order to show their exact embedding, Fig. 7(b)-(d) present the embedding of each data manifold by classical Isomap with neighborhood size k = 3. Fig. 7(e) shows the result got by PCA method. It can be seen that the data set is clustered, but the shape of each embedding is deformed because of the linear characteristic of the PCA method. Fig. 7(f) is the result got by k-CC Isomap method with neighborhood size k = 3. The bad approximations of intermanifolds geodesics lead to the deformation of the embedding in low dimensional space. Fig. 7(g) shows the result by M-Isomap method with neighborhood size k = 3. The data set is clearly clustered and intra-manifolds relationships are exactly preserved.\n11\nrotation set. (d) The result of Isomap on the teapot side view rotation set. (e) The result of PCA on teapot data set. (f) The result of k-CC Isomap on teapot\ndata set. (g) The result ofM-Isomap on teapot data set. (h) The result of D-C Isomap on teapot data set.\nFig. 7(h) shows the result got by revised D-C Isomap method with neighborhood size k = 3. It can be seen that the revised algorithm produces a satisfying result.\nFig. 8(a) shows samples of the IsoFACE and teapot rotation bird-view data. IsoFACE data consists of 698 images and each image is a 64\u00d764 (4096-dimensional) gray scale picture. As the input dimension of IsoFACE data set is different from the input dimension of teapot data set, we increase the dimension of IsoFACE set by adding zeros to the bottom of these vectors. The scale of the teapot data set should also be changed such that the scales of two embedding can be comparable. Teapot data vectors are divided by 100, i.e. the scale of teapot data points shrink to 1100 of its original ones. Fig. 8(b) is the 3-D embedding of IsoFACE by classical Isomap with neighborhood size k=5. Fig. 8(c) is the scaled teapot 3-D embedding got by classical Isomap with neighborhood size k = 5. Fig. 8(d) is the result got by 5-CC\nIsomap method. We can see that the shape of IsoFACE data is distorted badly. Fig. 8(e) is the result got by M-Isomap method with neighborhood size k=5. The performance is significantly improved compared with k-CC Isomap. Fig. 8(f) is the result got by the revised D-C Isomap method."}, {"heading": "C. Discussion", "text": "In our experiments, there are several important properties which should be considered:\n1) As k-CC Isomap tries to preserve poor and good approximations of geodesics simultaneously, its low dimensional embedding is usually deformed. This method works well if each data manifold has comparable number of data points and the data manifolds can not be very far from each other, and the algorithm does not work well otherwise.\n12\nTo sum up, Table III shows the comparison of the general performance of the five versions of Isomap algorithms: classical Isomap, k-CC Isomap , Original D-C Isomap, revised D-C Isomap and M-Isomap. The labels \u201d\u2206\u201d stands for poor performance, \u201d \u201d stands for not bad and \u201d\u00a9\u201d stands for good. Density means the generalize ability on manifolds with different density, i.e. different neighborhood sizes; dimensionality means the generalization ability on manifolds with different intrinsic dimensionality; isometric means the property of isometry in preserving the inter and intramanifold relationship; finally the generalization means the overall generalization ability to learn data from multiple manifolds.\nVI. Conclusion\nIn this paper, the problem of multi-manifolds learning is presented and defined for the first time. A general procedure for\nisometric multi-manifolds learning is proposed. The procedure can be used to build multi-manifolds learning algorithms which are not only able to faithfully preserve intra-manifold geodesic distances, but also the inter-manifolds geodesic distances. MIsomap is an implementation of the procedure and shows promising results in multi-manifolds learning. Compared with k-CC Isomap, it has the advantage of low computational complexity. With the procedure, the revised D-C Isomap becomes more effective in learning multi-manifolds data sets. Future work will be conducted on the applications of the multi-manifolds learning algorithms.\nReferences\n[1] I. T. Jolliffe, \u201cPrincipal Component Analysis,\u201d Springer-Varlag, New York, 1989. ISBN 0-387-96269-7 [2] T. F. Cox, M. A. Cox, \u201cMultidimensional Scaling,\u201d Chapman & Hall, London, 2001. ISBN 1-58488-094-5. [3] Eric Mjolsness, Dennis DeCoste, \u201cMachine Learning for Science: State of the Art and Future Prospects,\u201d Science, vol. 293, pp. 2051-2055, Sep. 2001. [4] J.B. Tenenbaum, V. de Sliva, and J. C. Landford, \u201cA global geometric framework for nonlinear dimensionality reduction,\u201d Science, vol. 290, pp. 2319-2323, Dec. 2000. [5] S. T. Roweis and L. K. Saul, \u201cNonlinear dimensionality reduction by local linear embedding,\u201d Science, vol. 290, pp. 2323-2326, Dec. 2000. [6] H. S. Seung, D. D. Lee, \u201cThe manifold ways of perception,\u201d Science, vol. 290, pp. 2268-2269, Dec. 2000. [7] M. Belkin and P. Niyogi, \u201cLaplacian Eigenmaps for Dimensionality Reduction and Data Representation,\u201d Neural Computation, vol. 15, no 6, pp. 1373-1396, June 2003. [8] D. Donoho, C. Grimes, \u201cHessian Eigenmaps: New Locally Linear Embedding Techniques for High-Dimensional Data,\u201d Proc. National Academy of Sciences, vol. 100, no. 10, pp. 5591-5596, 2003. [9] Zhenyue Zhang, Hongyuan Zha, \u201cPrincipal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment,\u201d SIAM Journal on Scientific Computing , vol. 26, issue. 1, pp. 313-338, 2005. [10] R. R. Coifman, S. Lafon, A. B. Lee, M. Maggoni, B. Nadler, F. Warner, S. W. Zuck, \u201cGeometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps,\u201d Proc. National Academy of Sciences, vol. 102, no. 21, pp. 7426-7431, May. 2005.\n13\n[11] Tong Lin, Hongyuan Zha, \u201cRiemannian Manifold Learning,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no5, pp. 796- 809, May. 2008. [12] Hongyuan Zha, Zhenyue Zhang, \u201cContinuum Isomap for manifold learning,\u201d Computational Statistics & Data Analysis, vol. 52, issue. 1, pp. 184-200, Sep. 2007. [13] Martin H. C. Law, A. K. Jain, \u201cIncremental Nonlinear Dimensionality Reduction by Manifold Learning,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 377-391, March. 2006. [14] M. Bernstein, V. de Silva, J. C. Langford, J. B. Tenenbaum, \u201cGraph approximations to geodesics on embedded manifolds,\u201d Technical report, Dept. of Psychology, Stanford Univ., Dec. 2000. [15] Maurizio Filippone, Francesco Camastra, Francesco Masulli, Stefano Rovetta, \u201cA survey of kernel and spectral methods for clustering,\u201d Pattern Recognition, vol. 41, pp. 176-190, May. 2007. [16] Ming-Hsuan Yang, \u201cExtended Isomap for Pattern Classification,\u201d ICPR 2002: Proceedings - International Conference on Pattern Recognition, vol. 3, pp. 30615, Aug. 2002 [17] Xiaofei He, Shuicheng Yan, Tuxiao Hu, P. Niyogi, Hong-jiang Zhang, \u201cFace recognition using Laplacianfaces,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, issue:3, pp. 328-340, March. 2005. [18] M. Belkin, V. Sindhwani, and P. Niyogi, \u201cManifold Regularization: a Geometric Framework for Learning from Examples,\u201d Journal of Machine Learning Research, vol. 7, pp. 2399-2434, Dec. 2006. [19] Xin Geng, De-Chuan Zhang, Zhi-hua Zhou, \u201cSupervised nonlinear dimensionality reduction for visualization and classification,\u201d IEEE Trans on Systems, Man, and Cybernetics Part B, vol. 35, no. 6, pp. 1098-1107, Dec 2005. [20] O. C. Jenkins, Maja J. Mataric, \u201cA Spatio-temporal Extension to Isomap Nonlinear Dimension Reduction,\u201d ACM. Proc. 21st Int\u2019l Conf. on Machine learning, vol. 69, pp. 56-66, 2004 [21] A. Rahimi, B. Recht, T. Darrell, \u201cLearning to Transform Time Series with a Few Examples ,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 10, pp. 1759-1775, Oct. 2007. [22] J.B. Tenenbaum, V. de Sliva, and J. C. Landford \u201cRespond to Comments on the Isomap Algorithm and Topological Stability,\u201d Sciences, vol. 295, no. 5552, pp. 7, Jan. 2002. [23] Yiming Wu, Kap Luk Chan \u201cAn Extended Isomap Algorithm for Learning Multi-Class Manifold,\u201d Proceedings of 2004 International Conference on Machine Learning and Cybernetics, vol. 6, pp. 3429-3433, Aug. 2004. [24] Deyu Meng, Yee Leung, Tung Fung, Zongben Xu \u201cNonlinear Dimensionality Reduction of Data Lying on the Multicluster Manifold,\u201d IEEE Trans on Systems, Man, and Cybernetics Part B, vol. 38, issue. 4, pp. 1111-1122. Aug. 2008. [25] Bo Li, De-Shuang Huang, Chao Wang, Kun-Hong Liu \u201cFeature extraction using constrained maximum variance mapping,\u201d Pattern Recognition, vol. 41, pp. 3287-3294. May. 2008. [26] Li Yang, \u201cK-Edge Connected Neighborhood Graph for Geodesic Distance Estimation and Nonlinear Projection,\u201d Proceedings of the Pattern Recognition, 17th International Conference on (ICPR\u201904), vol. 1, pp. 196- 199. 2004. [27] Li Yang, \u201cBuilding k Edge-Disjoint Spanning Trees of Minimum Total Length for Isometric Data Embedding,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1680-1683, Oct. 2005. [28] Li Yang, \u201cBuilding k Edge-connected neighborhood graph for distancebased data projection,\u201d Pattern Recognition Letters, vol. 26, issue. 13, pp. 2015-2021, Oct. 2005. [29] Li Yang, \u201cBuilding k-Connected Neighborhood Graphs for Isometric Data Embedding,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, issue. 5, pp. 827-831, May. 2006. [30] Dongfang Zhao, Li Yang, \u201cIncremental Construction of Neighborhood Graphs for Nonlinear Dimensionality Reduction,\u201d International Conference on Pattern Recognition, vol. 28, issue. 5, pp. 827-831, May. 2006. [31] Dongfang Zhao, Li Yang, \u201cIncremental Isometric Embedding of HighDimensional Data Using Connected Neighborhood Graphs,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 1, pp. 86-98, Jan. 2009. [32] O. Samko, A. D. Marshall, P. L. Rosin \u201c Selection of the optimal parameter value for the Isomap algorithm,\u201d Pattern Recognition Letters vol. 27, issue. 9, pp. 968-979, Feb. 2006. [33] R. Xu, D. Wunsch II, \u201dSurvey of clustering algorithms,\u201d IEEE Trans on Neural Networks, vol. 16, no. 3, pp. 645-678, May. 2005. [34] F. Camastra and A. Vinciarelli, \u201dEstimating the intrinsic dimension of data with a fractal-based approach,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence vol. 24, no. 10, pp. 1404-1407, Oct. 2002. [35] E. Levina and P. J. Bickel \u201cMaximum likelihood estimation of intrinsic dimension,\u201d NIPS 04: Neural Information Processing Systems [36] D.J.C. MacKay and Z. Ghahramani, \u201dComments on \u2019Maximum likelihood estimation of intrinsic dimension\u2019 by E. Levina and P. Bickel (2005),\u201d in: http://www.inference.phy.cam.ac.uk/mackay/dimension/ [37] Mingyu Fan, Hong Qiao, Bo Zhang, \u201dIntrinsic dimension estimation of manifolds by incising balls,\u201d Pattern Recognition vol. 42, issue. 5, pp. 780-787, May. 2009. [38] B Graham and Nigel M Allinson, \u201dCharacterizing Virtual Eigensignatures for General Purpose Face Recognition\u201d, In H. Wechsler, P. J. Phillips, V. Bruce, and F. Fogelman-Soulie and T. S. Huang (eds): \u201dFace Recognition: From Theory to Applications;\u201d NATO ASI Series F, Computer and Systems Sciences, Vol. 163; pp 446-456, 1998. [39] D. Kushnir, M. Galun, A. Brandt \u201cFast multiscale clustering and manifold identification,\u201d Pattern Recognition vol. 28, issue. 10, pp. 1876- 1891, Oct. 2006.\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2 0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n(b)\n\u22122.5 \u22122 \u22121.5 \u22121 \u22120.5 0 \u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n\u22123.5 \u22123 \u22122.5 \u22122 \u22121.5 \u22121 \u22120.5 0 0.5 1 1.50\n10\n20\n30\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\n(a)\n\u221215 \u221210 \u22125 0 5 10 15 \u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5 kcc Isomap on data set A\nX\n\u221215 \u221210 \u22125 0 5 10 15 \u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6 kcc Isomap on data set B\nX\n\u221230 \u221220 \u221210 0 10 20 30 \u221210\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8 kcc Isomap on data set C\nX\n\u221215 \u221210 \u22125 0 5 10 15 \u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n(b)\n\u22121 0\n1 2\n3 4\nx 10\n\u22121.5 \u22121\n\u22120.5 0\n0.5 1\n1.5x 10 4\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nx 10 4\nX\nkcc Isomap on teapot data set\nY\nar X\niv :0\n91 2.\n05 72\nv1 [\ncs .L\nG ]\n3 D\nec 2\n00 9\n1\nIsometric Multi-Manifolds Learning Mingyu Fan, Hong Qiao, Senior Member, IEEE, and Bo Zhang\nAbstract\u2014 Isometric feature mapping (Isomap) is a promising manifold learning method. However, Isomap fails to work on data which distribute on clusters in a single manifold or manifolds. Many works have been done on extending Isomap to multimanifolds learning. In this paper, we first proposed a new multimanifolds learning algorithm (M-Isomap) with help of a general procedure. The new algorithm preserves intra-manifold geodesics and multiple inter-manifolds edges precisely. Compared with previous methods, this algorithm can isometrically learn data distributed on several manifolds. Secondly, the original multicluster manifold learning algorithm first proposed in [24] and called D-C Isomap has been revised so that the revised D-C Isomap can learn multi-manifolds data. Finally, the features and effectiveness of the proposed multi-manifolds learning algorithms are demonstrated and compared through experiments.\nIndex Terms\u2014 Isomap, nonlinear dimensionality reduction, manifold learning, pattern analysis, multi-manifolds learning.\nI. Introduction\nChallenges, known as \u201dthe curse of dimensionality\u201d, are usually confronted when scientists are dealing with high dimensional data. Dimensionality reduction is a promising tool to circumvent these problems. Principal component analysis (PCA) [1] and multidimensional scaling (MDS) [2] are two important linear dimensionality reduction methods. Due to their linear model assumption, both of the methods will fail to discover nonlinear intrinsic structures of data.\nRecently, there are more and more interests in nonlinear dimensionality reduction (NLDR). NLDR is used to learn nonlinear intrinsic structures of data, which is considered to be the first step of machine learning and pattern recognition [3]. Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000. LLE assumes that data points locally distribute on a linear patch of a manifold. It preserves local linear coefficients, which best reconstruct each data point by its neighbors, into a lower dimensional space. Isomap is based on the classical MDS method. Instead of preserving pairwise Euclidean distance, it preserves the geodesic distance on the manifold. The geodesic between two data points is approximated by the shortest path on a constructed graph. Both of the methods are computationally efficient and able to achieve global optimality. There are also many other important nonlinear dimensionality reduction methods. Laplacian eigenmap [7] utilizes an approximation of the Laplace-Beltrami operator on manifolds to provide an optimal embedding. Hessian LLE [8] resembles Laplacian eigenmap by using an approximation of the Hessian operator\nM. Fan and B. Zhang are with LSEC and the Institute of Applied Mathematics, AMSS, Chinese Academy of Sciences, Beijing 100190, China (email: fanmingyu@amss.ac.cn, b.zhang@amt.ac.cn)\nH. Qiao is with the Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China (email: hong.qiao@mail.ia.ac.cn)\nCorresponding author: Bo Zhang\ninstead of Laplacian operator. The local tangent space alignment (LTSA) [9] method learns the local geometry by constructing a local tangent space of each data point and then aligns these local tangent spaces into a single global coordinates system with respect to the underlying manifold. Diffusion maps [10] applies diffusion semigroups to produce multi-scale geometries to represent complex structures. Riemannian manifold learning (RML) [11] method uses the constructed Riemannian normal coordinate chart to map the input data into a lower dimensional space. NLDR is a fast growing research activity and has been proved very useful in many fields and applications, such as classification using Isomap [16] and Laplacian eigenmap [17], geometry based semi-supervised learning method using Laplacian eigenmap [18], data visualization [19] and time series manifold learning [20], [21].\nAs Isomap emphasizes on the global geometric relationship of data points, it is very illustrative in data visualization and pattern analysis [13]. Although Isomap algorithm implicitly requires the data set to be convex [8], it still provides very meaningful results on non-convex data sets. In this paper, we will focus on extending Isomap to multi-manifolds learning. The first step of Isomap algorithm is to construct a neighborhood graph which connects all the data points. This step is of vital importance because the success of the following steps depends on how well the constructed neighborhood graph is. However, it is hard to build a totally connected neighborhood graph in order to guarantee the topological stability of the classical Isomap algorithm when points of the data set distribute on clusters in a manifold or manifolds (multiple manifolds). It should be remarked that several methods have been proposed to extend Isomap to multi-manifolds data, and some of them are based on providing new neighborhood graph construction algorithms. For example, Wu et al [23] introduced a split and augment procedure for neighborhood graph construction which could produce a totally connected neighborhood graph. In a series of papers [26]\u2013[29], Yang introduced several neighborhood graph construction algorithms using techniques from discrete mathematics, graph theory. Meng et al [24] proposed a decomposition and composition Isomap (D-C Isomap).\nThe rest of the paper is organized as follows. In Section II, the main issues and limitations of the classical Isomap algorithm are presented. The problem of multi-manifolds learning is also investigated. In Section III, previous methods on multi-manifolds learning are briefly introduced and discussed. In Section IV, a general procedure for designing multi-manifolds learning algorithms is first proposed. With the proposed procedure, a new multi-manifolds learning algorithm (M-Isomap) is then designed and analyzed. In addition, the original D-C Isomap algorithm is revised to overcome its main limitation. In Section V, the effectiveness of these multi-manifolds learning algorithms has been demonstrated by experiments. Comparisons of these algorithms have also been made. Some concluding remarks are provided in Section VI.\n2 II. Classical Isometric FeatureMapping and Its Limitations\nIsomap is an efficient NLDR algorithm to recover the intrinsic geometric structure of a data set if the data points lie on a single manifold [4]. Assume that the data set X = {x1, x2, \u00b7 \u00b7 \u00b7 , xN } is in a high dimensional space RD and the feature space is Rd. Then the classical Isomap algorithm has the following three steps [4].\nStep 1: Identify the neighbors of all the data points to construct a neighborhood graph. With the given parameter k or \u01eb, there are two ways to construct a neighborhood graph for X:\n\u2022 if x j is one of xi\u2019s k nearest neighbors, they are connected by an edge (the k-NN method). \u2022 if xi and x j satisfy \u2016xi \u2212 x j\u2016 < \u03b5, they are connected by an edge (the \u03b5-NN method).\nStep 2: Use Dijkstra\u2019s or Floyd-Warshall\u2019s algorithm to compute the length of the shortest path dG(xi, x j) between any two data points xi and x j on the graph. It is proved that dG(xi, x j) is a good approximation of the geodesic distance dM(xi, x j) on the manifold as the number of data points increases. Step 3: Perform the classical MDS on the graph distance matrix DG whose (i, j)-th element is dG(i, j). Minimize the cost function\nE(Y) = \u2016\u03c4(DG) \u2212 \u03c4(DY )\u2016F2\nThe operator \u03c4 is defined as \u03c4(D) = \u22121 2 HS H, where H = I \u2212 1 n\neeT with I the identity matrix and e = (1, 1, \u00b7 \u00b7 \u00b7 , 1)T , S = (D2i j) with Di j being the (i, j)-th element of D and DY = (\u2016yi \u2212y j\u2016). Assume that, in descending order, \u03bbi is the i-th eigenvalue of \u03c4(DG) with the corresponding eigenvector \u03bdi. Then the low-dimensional embedding Y is given by:\nY = [y1, , y2, \u00b7 \u00b7 \u00b7 , yn] =\n         \n\u221a \u03bb1\u03bd T 1 \u00b7 \u00b7 \u00b7\u221a \u03bbd\u03bd T d          \nThe property of the Isomap algorithm is well understood [12], [14]. However, the success of the Isomap algorithm depends on two issues. One is how to choose the correct intrinsic dimensionality d. Setting a lower dimensionality d will lead to a loss of data structure information. On the other hand, setting a higher dimensionality d will make some redundant information to be kept. This issue has been well investigated so far. The other issue is the quality of the constructed neighborhood graph. It is known that constructing an appropriate neighborhood graph is still a tricky task. Both the k-NN and \u03b5-NN methods have their limitations. Under the assumption that data points distribute on a single manifold, if the neighborhood size k or \u03b5 is chosen to be too small, the constructed neighborhood graph will be very sparse and therefore the geodesics can not be satisfactorily approximated. On the other hand, if the neighborhood size k or \u01eb is chosen to be too large, short-circuit edges may occur which will have a significant negative influence on the topological stability of the Isomap algorithm [22].\nNonetheless, if data points distribute uniformly on one manifold, then both the \u201dshort circuit\u201d problem and the \u201ddiscontinuity\u201d problem can be circumvented by carefully choosing an appropriate neighborhood size k or \u03b5. However, if data points distribute on\nseveral clusters or manifolds, then neither of the k-NN method and the \u03b5-NN method can guarantee that the whole data set is totally connected and the geodesics is satisfactorily approximated.\nHowever, both data missing and data mixture are common problems in practical data analysis. These two cases often cause data points to distribute on different clusters in a manifold or manifolds. Data points on different manifolds may have different input dimensionality D (the dimensionality of the ambient space). This usually happens in the case of data mixture. On the other hand, learning different data manifolds may need different values of input parameters, that is, appropriate neighborhood size (k or \u03b5) and intrinsic dimensionality d for each data manifold. In this paper, we will focus on designing new multi-manifolds learning algorithms for data distributing on multiple manifolds. The case when data points distribute on pieces of a single manifold is referred to as multi-cluster manifold learning, while the case when data points distribute on multiple manifolds is referred to as multi-manifolds learning.\nIII. PreviousWorks onMulti-Manifolds Learning"}, {"heading": "A. Multi-manifolds learning by new neighborhood graph construction method", "text": "Wu and Chan [23] proposed a split-augment approach to construct a neighborhood graph. Their method can be regarded as a variation of the k-NN method and can be summarized as below:\n1. the k-NN method is applied to the data set. Every data point is connected with its neighbors. If the data lies on multiple manifolds, several disconnected graph components (data manifolds) will be formed. 2. Each couple of graph components are connected by their nearest couple of inter-components points.\nThis method is simple to implement and has the same computational complexity as the k-NN method has. However, as there is only one edge connecting every two graph components, geodesics across components are poorly approximated and, meanwhile, their low dimensional embedding can be rotated arbitrarily. This method can not be directly applied to data lying on three or more data manifolds. If more than two graph components exist, the intra-component shortest distances may be changed in the totally connected graph.\nYang [26]\u2013[29] introduced four methods to construct a connected neighborhood graph. The k minimum spanning trees (kMST) method [26] repeatedly extracts k minimum spanning trees (MSTs) from the complete Euclidean graph of all data points. Edges of the k MSTs form a k-connected neighborhood graph. Instead of extracting k MSTs, the minimum-k-spanning trees (min-k-ST) method [27] finds k edge-disjoint spanning trees from the complete Euclidean graph, and the sum of the total edge length of the k edge-disjoint spanning trees attains a minimum. The k-edge-connected (k-EC) method [28] constructs a connected neighborhood graph by adding edges in a non-increasing order from the complete Euclidean graph. An edge is added if its two end vertices do not have k edge-disjoint paths connected with each other. The k-vertices-connected (k-VC) method [29] adds edges in a non-increasing order from the complete Euclidean graph, where an edge is added if its two end vertices would be disconnected by removing some k \u2212 1 vertices. And the constructed neighborhood graph would not be disconnected by removing any k\u2212 1 vertices.\nThe methods introduced in [26]\u2013[29] have the following advantages over the k-NN method. First, the local neighbor relationship\n3 is affected by the global distribution of data points. This is beneficial for adaptively preservation of the global geometric metrics. Secondly, these methods can guarantee that the constructed neighborhood graph is totally connected. Compared with the kNN method, Yang\u2019s methods construct a neighborhood graph with more edges corresponding to the same neighborhood size k. This property ensures the quality of the neighborhood graphs."}, {"heading": "B. Multi-manifolds learning by decomposition-composition Isomap", "text": "In [24], Meng et al. proposed a decomposition-composition method (D-C Isomap) which extends Isomap to multi-cluster manifold learning. The purpose of the method is to preserve intracluster and inter-cluster distances separately. In the next section, we will introduce a revised version of the D-C Isomap to extend the application range of the original D-C Isomap. To this end, we present the details of the D-C Isomap algorithm as follows. Step I: Decomposition process\n1. Given an appropriate neighborhood size k or \u03b5, if data is comprised of multiple clusters, several disconnected graph components will be formed when each data point is connected with its neighbors by the k-NN or \u03b5-NN method. 2. Assume that there are M graph components and a graph component is also considered as a cluster. Data points of the m-th cluster is denoted as Xm = {xm1 , \u00b7 \u00b7 \u00b7 , xmlm }. Clusters Xm and Xn are connected by their nearest inter-cluster edge, whose ending vertices are assumed as nxmn and nx n m\nand edge length as d0m,n. 3. Apply the k-NN Isomap or \u03b5-NN Isomap on cluster\nXm. Denote by Dm = (Dmi, j) the geodesic distance matrix for Xm, by Ym = {ym1 , \u00b7 \u00b7 \u00b7 , ymlm } the corresponding lowdimensional embedding to Xm and by nymn the embedding point corresponding to nxmn , where ny m n \u2208 Ym.\nStep II: Composition process\n1. The set of centers of clusters is denoted as CX = {cx1, \u00b7 \u00b7 \u00b7 , cxM}, where each center is computed by\ncxm = arg min xi\u2208Xm\n(\nmax x j\u2208Xm\n(Dmi j)\n)\nm = 1, \u00b7 \u00b7 \u00b7 , M.\n2. The distance matrix for CX can be computed by\nD\u0303 = {D\u0303mn}, D\u0303mn = { dm,n + d0m,n + dn,m m , n 0, m = n\nwhere dm,n is the distance of the shortest path between cxm and nxmn on the graph component X\nm. 3. Plug the distance matrix D\u0303 and the neighborhood size d\ninto the classical Isomap algorithm. The embedding of CX is denoted by CY = {cy1, \u00b7 \u00b7 \u00b7 , cyM} \u2282 Rd (CY is called the translation reference set). Assume that the d nearest neighbors of cxm are {cxm1 , \u00b7 \u00b7 \u00b7 , cxmd }. Then the lowdimensional representation corresponding to nxmmi can be computed as\nsymmi = cym + dm,mi\ndm,mi + d 0 m,mi + dmi,m\n(cymi \u2212 cym),\nwhere i = 1, \u00b7 \u00b7 \u00b7 , d and m = 1, \u00b7 \u00b7 \u00b7 , M. 4. Construct the rotation matrix Am for Ym with m = 1, \u00b7 \u00b7 \u00b7 , M.\nAssume that QNm is the principal component matrix of\nNYm = {nymm1, \u00b7 \u00b7 \u00b7 , nymmd } and QS m is the principal component matrix of S Ym = {symm1, \u00b7 \u00b7 \u00b7 , symmd }. Then the rotation matrix for Ym is Am = QS mQNTm . 5. Transform Ym (m = 1, \u00b7 \u00b7 \u00b7 , M) into a single coordinate systemby Euclidean transformations:\nFYm = { f ymi = Amymi + cym, i = 1, \u00b7 \u00b7 \u00b7 , lm}, m = 1, \u00b7 \u00b7 \u00b7 , M.\nThen Y = \u22c3M m=1 FY m is the final output.\nFirst, the D-C Isomap algorithm reduces the dimensionality of clusters separately and meanwhile, preserves a skeleton of the whole data. Secondly, using the Euclidean transformations, the embedding of each cluster is placed into the corresponding position by referring to the skeleton. In this way, the intracluster geodesics are exactly preserved. Since the D-C Isomap method uses circumcenters to construct the skeleton of the whole data, its learning results depend on the mutual position of these circumcenters, which would make the learning results unstable. On the other hand, it is known that at least d+ 1 reference points are needed to anchor a d-dimensional simplex. However, in the D-C Isomap algorithm, the number of the reference data points is limited by the number of clusters."}, {"heading": "C. Constrained Maximum Variance Mapping", "text": "Recently in [25], Li et al. proposed the constrained maximum variance mapping (CMVM) algorithm for multi-manifolds learning. The CMVM method is proposed based on the notion of maximizing the dissimilarity between classes while holding up the intra-class similarity.\nIV. IsometricMulti-Manifolds Learning\nIn this section, we first introduce a general procedure for the design of isometric multi-manifolds learning algorithms and then present our new multi-manifolds learning algorithm called MIsomap. Finally, we make a revision of the original D-C Isomap algorithm to extend its application range."}, {"heading": "A. The general procedure for isometric multi-manifolds learning", "text": "Many previous methods extend Isomap to multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29]. However, the shortest paths across clusters or data manifolds are bad approximations of geodesics. In Isomap, bad local approximation always leads to deformation of the global low-dimensional embedding.\nAssumed that \u2126 is an open, convex and compact set in Rd and f : \u2126 \u2192 RD is a continues mapping, where d << D. Then f (\u2126) = M is defined as a d dimensional parameterized manifold. Let K(x, y) (x, y \u2208 M) be a specially defined kernel. Then a reproducing kernel Hilbert space (RKHS) H is constructed with this kernel. Denote by \u03c6 j(x) the eigenfunction corresponding to the j-th largest eigenvalue \u03bb j of K(x, y) in H , which is also the j-th element of the Isomap embedding. The geodesic distance on the manifold M is written as\nd2(x, y) = d2( f (\u03c4), f ( \u2227 \u03c4)) = \u03b1\u2016\u03c4 \u2212 \u2227\u03c4\u2016 + \u03b7(\u03c4, \u2227\u03c4),\nwhere \u03c4, \u2227 \u03c4 \u2208 \u2126, \u03b1 is a constant and \u03b7(\u03c4, \u2227\u03c4) is the deviation from isometry. The constant vector\nC =\n\u222b\nM x\u03c1(x)dx \u222b\nM \u03c1(x)dx =\n\u222b\n\u2126 \u03c4H(\u03c4)d\u03c4 \u222b\n\u2126 H(\u03c4)d\u03c4\n4\nwhere \u03c1(x) and H(\u03c4) are density functions of M and \u2126. With the above notations, the following theorem is proved by Zha et al [12].\nTheorem 4.1: There is a constant vector P j such that \u03c6 j(x) = PTj (\u03c4\u2212C)+ e j(\u03c4). Here e j(\u03c4) = \u01eb (0) j \u2212 \u01eb j(\u03c4) has zero mean, that is, \u222b\n\u2126\nH(\u03c4)e j(\u03c4)d\u03c4 = 0, where\n\u01eb j(\u03c4) = 1\n2\u03bb j\n\u222b\n\u2126\n\u03b7(\u03c4, \u2227 \u03c4)H(\u2227\u03c4)\u03c6 j(x)d \u2227 \u03c4,\n\u01eb (0) j =\n1 \u222b\n\u2126 H(\u03c4)d\u03c4\n\u222b\n\u2126\n\u01eb j(\u03c4)H(\u03c4)d\u03c4.\nBy Theorem 4.1, even if the deviation \u03b7(\u03c4, \u2227 \u03c4) is not zero with only a limited range of (\u03c4, \u2227 \u03c4), then the coordinate of the low-dimensional embedding \u03c6 j(x) is still deformed with the deformation being measured by e j(\u03c4).\nIn order to get a better understanding of multi-manifolds data, it is profitable to preserve intra-manifold relationship (where \u03b7(\u03c4, \u2227 \u03c4) = 0) and inter-manifolds relationship (where \u03b7(\u03c4, \u2227 \u03c4) , 0) separately. This is because sometimes we care more about the information within the same data manifold. Here we propose a general procedure for the design of isometric multi-manifolds learning algorithms.\nStep I: The decomposition process\n1. Cluster the whole data set. If data distribute on multiple clusters in a manifold or manifolds, the clusters or manifolds should be identified. Many clustering methods can be used for this; for example, K-means, Isodata and other methods introduced in [15], [33]. Even if the manifolds overlay with each other, they can still be identified and clustered [39]. At this step, the data set X is clustered into several components, and each component is considered as a data manifold. 2. Estimate parameters of data manifolds. For intrinsic dimensionality estimation, many methods can be used: for example, the fractal based method [34], the MLE method [35], [36] and the incising ball method [37]. Assume that dm is the intrinsic dimension of the m-th data manifold. Let d = max dm\nm . For the neighborhood size, [32] introduces a\nmethod on automatically generating parameters for Isomap\non one single data manifold. For convenience, appropriate neighborhood sizes (km or \u03b5m for Xm) can be given manually for data manifolds. 3. Learn the data manifolds individually. One data manifold can be learned by traditional manifold learning algorithms. Here, we propose to rebuild a graph on each data manifold with a new neighborhood size to better approximate the intra-manifold geodesics. In doing so, Yang\u2019s methods [26]\u2013[29] and thek-CG graph construction method are preferred, where thek-CG graph construction method will be described later. It is assumed that the low-dimensional embedding for Xm is Ym.\nStep II: The composition process\n1. Preserve a skeleton I of the whole data set in a lowdimensional space Rd. The skeleton I should be carefully designed so that it can represent the global structure of X. Let RYI be the low-dimensional embedding of I. 2. Transform Yms into a single coordinate system by referring to RYI . In order to faithfully preserve the intra-manifold relationship, Euclidean transformations can be constructed and used. Using the embedding points RYm \u2282 RY I and the corresponding points from Ym, we can construct an Euclidean transformation from Ym to the coordinate system of RYI .\nThe idea of using a decomposition-composition procedure is not new, which was first used by Wu et al. [23] in their split-augment process and well developed and used in [24]. The procedure we proposed here aims to solve a more general problem. Step I.1 permits that the designed learning algorithm has a good ability to identify data manifolds. Step I.2 gives a guideline on learning manifolds with different intrinsic dimensionality and neighborhood sizes. Step I.3 learns data manifolds individually so that the intra-manifold relationship can be faithfully preserved. Step II.1 is the most flexible part of the procedure which allows us to design new isometric multi-manifolds learning algorithms. A well designed skeleton I can better represent the inter-manifolds relationship. In the following subsections, we will introduce a new multi-manifolds learning algorithm and revise the original D-C Isomap algorithm with the help of this general procedure.\n5 TABLE II\nComputational complexity comparison of k-NN, k-MSTs, Min-k-ST, k-EC\nand k-VC methods, where TC stands for time complexity and IL stands for\nthe time complexity for incremental learning\nk-NN k-MST Ming-k-ST k-EC k-VC TC O(kN2) O(k2N2) O(k2N2) O(k2N2) O(N3) IL O(kN) O(N ln N) O(N ln N + kN)"}, {"heading": "B. A new algorithm for isometric multi-manifolds learning", "text": "Based on the proposed procedure, we design a new multimanifolds learning algorithm. As an extension of the classical Isomap method to multi-manifolds data, the method will be referred to as multi-manifolds Isomap or M-Isomap. It is assumed that X is also interchangeable to represent the matrix [x1, x2, \u00b7 \u00b7 \u00b7 , xN ], where xi, i = 1, \u00b7 \u00b7 \u00b7 , N are column vectors in R\nD. 1) Using the k-CC method to construct a neighborhood graph and identify manifolds: Table II shows the time complexity of the k-NN, K-Min-ST, k-EC and k-VC methods on the neighborhood graph construction. As shown in the table, the k-NN method has the lowest computational complexity O(kN2).\nFor incremental learning, the computational complexity of the k-NN, k-MSTs and k-VC methods are O(kN), O(N ln N) and O(N ln N + kN), respectively [30], [31]. The computational complexity of the Min-k-ST and k-EC methods for incremental learning are unavailable. For data on one single manifold, the improvement of performance of Yang\u2019s methods becomes insignificant when the neighborhood size k increases for the k-NN method. More importantly, the k-NN method implicitly has the property of clustering to multi-manifolds data. Data points of the same manifold tend to be connected by paths and disconnected otherwise when each data point is connected with its neighbors by edges. Although the k-NN method is not a robust clustering algorithm, it is computationally efficient for both clustering and graph construction. Therefore, we introduce a variation of the kNN method which inherits the computational advantage of the k-NN method. The methodis able to identify data manifolds and construct a totally connected neighborhood graph. In the rest of the paper, the proposed neighborhood graph construction method will be referred as the k-edge connected graph method (the k-CG method).\nThe summary of the k-CG algorithm is follows. First, given a neighborhood size k or \u03b5, each data point is connected with its neighbors. If the data points distribute on several clusters or manifolds, several disconnected graphs will be constructed. Data points are assigned to the same data manifold if there is a path connecting them on the graphs. Then, we connect each pair of graphs by k nearest pairs of data points. For robustness of the algorithm, each data point is only allowed to have one inter-manifolds edge at most.\nAlgorithm 4.1: (The k-CG algorithm:) Input: Euclidean distance matrix D, whose (i, j)-th entry is \u2016xi \u2212 x j\u2016 and neighborhood size k or \u03b5. Output: Graph G = (V, E), the number M of clusters, label of the data. Initialization: V = {x1, \u00b7 \u00b7 \u00b7 , xn}, V \u2032 = V, E = \u03c6, Queue = \u03c6\n1: for i=1 to N do\n2: Identify nearest neighbors {xi1, \u00b7 \u00b7 \u00b7 , xili } for xi by k-nearestneighbors or \u03b5-nearest-neighbors. Let E = E\n\u22c3{ei1, \u00b7 \u00b7 \u00b7 , eili } 3: end for 4: Set M = 1 5: while V \u2032 is not empty do 6: x \u2208 V \u2032, in-Queue{x}, label(x)=M, V \u2032 = V \u2032 \u2212 {x} 7: while Queue is not empty do 8: x=de-Queue 9: \u2200y: y is connected with x by an edge 10: if y is not labeled then 11: in-Queue{y}, label(y)=M, V \u2032 = V \u2032 \u2212 {y} 12: end if 13: end while 14: M=M+1 15: end while 16: M=M-1 17: if M \u2265 2 then 18: k =average({si}Ni=1 { where si is the number of neighbors of xi} 19: for i = 1 to M do 20: for j = i + 1 to M do 21: Find k shortest inter-manifolds edges e1, \u00b7 \u00b7 \u00b7 , ek be-\ntween data manifolds i and j and make sure that their ending vertices are not identical. Let E = E \u22c3{e1, \u00b7 \u00b7 \u00b7 , ek}\n22: end for 23: end for 24: end if\nThe main difference between the k-NN method and the kCG method is lines 4 to 25, which identify components (data manifolds) and connect different components of the graph. This change makes the constructed graph totally k-edge connected. Compared with the method proposed in [23], the k-CG method constructs a neighborhood graph with k inter-manifolds edges, which is able to control the rotation of the embedding of the data manifolds. In Section V, the k-CG Isomap method, which uses the k-CG method to construct a totally connected graph and then perform the classical Isomap on the graph, is compared with the M-Isomap method. It can be easily seen that the k-CG Isomap suffers the limitation which has been shown by Theorem 4.1.We assume that {xmn(i)}ki=1 is the subset of Xm whose data points connect with manifold Xn(i), i = 1, \u00b7 \u00b7 \u00b7 , k.\n2) Learn data manifolds individually: As Xm is considered as a single data manifold in RD, it is possible to find its intrinsic parameters. The incising ball method [37] is utilized to estimate the intrinsic dimensionality, which is simple to implement and always outputs an integer result. Assume that d is the highest intrinsic dimensionality of data manifolds. The neighborhood size km or \u03b5m of each data manifold is given manually and the graph on the data manifold Xm is rebuilt. It is expected that the new neighborhood graph on Xm can give better approximations to the intra-manifold geodesics. The approximated geodesic distance matrix for Xm is written as Dm. By applying the classical MDS on Dm, the low-dimensional embedding for Xm can be obtained as Ym = {ymi } S m i=1.\n3) Preserve a skeleton of the data manifold X: First, intermanifolds distances are computed. Assuming that xmp and x n q are any data points with xmp \u2208 Xm and xnq \u2208 Xn, their distance can be\n6 computed by\ndG(x m p , x n q) = mint=1\u00b7\u00b7\u00b7k {dG(xmp , xmn(t)) + \u2016xmn(t), xnm(t\u2032)\u2016 + dG(xnn(t\u2032), xnq)}, (1)\nwhere dG(xmp , x m n(t)) is the shortest path on the neighborhood graph of Xm. Although dG(xmp , x m n(t)) may not be the shortest path on the totally connected graph of X, Eq. (1) is an efficient way to approximate distances across manifolds. Assume that Dmn is the distance matrix across over Xm and Xn. Then the furthest intermanifolds data points are computed by\n{ f xmn , f xnm} = arg max dG(xmi , xnj ), dG(xmi , xnj ) \u2208 Dmn. (2)\nWithout loss of generality, we may assume\nIm = {xmi } lm i=1 =\nM \u22c3\nn=1\n{xmn(1), \u00b7 \u00b7 \u00b7 , xmn(k), f xmn }.\nThen I = \u22c3M m=1 I m is considered as the global skeleton of X. On the data manifold X, it can be seen that the skeleton I formulates a sparse graph. We assume that DI = (dI(i, j)) is the distance matrix of I, where\ndI(i, j) =\n{ dG(xmi , x n j ) \u2208 Dmn, xi \u2208 Xm, x j \u2208 Xn\ndG(xmi , x m j ) \u2208 Dm, xi, x j \u2208 Xm\n(3)\nBy applying the classical MDS algorithm on DI , the lowdimensional embedding RYI of I can be obtained. It is assumed that RYmI = {rymi } lm i=1 \u2282 RYI is the embedding of Im.\n4) Euclidean transformations: Assume that YmI = {ymi } lm i=1 \u2282 Ym and ymi corresponds to x m i . Then the Euclidean transformation from YmI to RY m I can be constructed as follows.\nThe general Euclidean transformation can be written as\nry = Ay + \u03b2,\nwhere A is an orthonormal matrix and \u03b2 is a translation vector. For the m-th data manifold, it is assumed that the Euclidean transformation is\nrymi = Amymi + \u03b2m, i = 1, \u00b7 \u00b7 \u00b7 , lm.\nThe above Euclidean transformation can be rewritten in the matrix form:\nRYmI = AmYmI + \u03b2meT = ( Am \u03b2m )\n(\nYmI eT\n)\n(4)\nwhere e is a vector with all ones. Equation (4) can be solved using the least square method, and the solution is given by\n( Am \u03b2m ) = RYmI\n(\nYmI eT\n)T  \n   \n(\nYmI eT\n) (\nYmI eT\n)T\n+ \u03bbI\n     \n\u22121\n(5)\nwhere I is the identity matrix and \u03bb is a regularization parameter in the singular case. However, the least square solution does not necessarily provide an orthonormal matrix Am. We now propose to use the QR decomposition to get the orthonormal matrix Am. The QR process can be written as\n( Am R ) = QR(Am) (6)\nwhere the diagonal elements of R are forced to be nonnegative. Then \u03b2m can be recomputed by minimizing the cost function\nC(\u03b2m) = lm \u2211\ni=1\n\u2016Amymi + \u03b2m \u2212 rymi \u20162.\nSolving the equation \u2202C(\u03b2m) \u2202\u03b2m = 0 gives\n\u03b2m = 1 lm\nlm \u2211\ni=1\n(rymi \u2212 Amymi ) (7)\nThe low-dimensional embeddings Ym (i = 1, \u00b7 \u00b7 \u00b7 , M) can be formed into a global coordinate system using the constructed Euclidean transformations.\n5) The M-Isomap algorithm: The detailed M-Isomap algorithm is summarized in the following table.\nInput: X = {xi}Ni=1 with xi \u2208 RD. Initial neighborhood size k or \u03b5. Step I.1 Perform the k-CG algorithm on X. Data manifolds {Xm}Mm=1 and the set of inter-manifolds points {xmn(i)}ki=1 of Xm can be obtained. Step I.2 Estimate parameters of the data manifolds. Assume that the intrinsic dimension dm and neighborhood size (km or \u03b5m) are parameters for Xm. Let d = max\nm {dm} and rebuild the neighbor-\nhood graph on Xm. Step I.3 Classical Isomap algorithm is performed on Xm\nwith new neighborhood graph, (m = 1, \u00b7 \u00b7 \u00b7 , M). The corresponding low-dimensional embedding of Xm is denoted by Ym.\nStep II.1 Inter-manifolds distance matrix Dmn is computed by Eq. (1); thus { f xmn }Mm,n can be found by Eq. (2). Distance matrix DI for the skeleton I is computed by Eq. (3). Classical MDS is performed on DI to obtain the low-dimensional embedding of I, which is written as RYI . Assume that RYmI \u2282 RYI is the embedding of Im. Step II.2 Construct Euclidean transformations by Eqs. (5)-(7). Use the Euclidean transformations to transform Ym into RYm, m = 1, \u00b7 \u00b7 \u00b7 , M. Step II.3 Y = \u22c3M\nm=1 RY m is the final output."}, {"heading": "C. Computational complexity of the M-Isomap algorithm", "text": "Computational complexity is a basic issue for application. In the M-Isomap method, the k-CC algorithm needs O((k + 1)N2) time to construct a totally connected graph and identify the manifolds. It needs O(\n\u2211M m=1 S 2 m ln S m) time to compute the\nshortest path on each data manifold and O( \u2211M m=1 S 3 m) time to perform the classical MDS on the distance matrices of data manifolds. The time complexity of computing the shortest path across data manifolds is O(\n\u2211M m<n kS mS n) and that of finding\nf xij, f x j i is O( \u2211M m<n S mS n). Performing the classical MDS on the skeleton I needs O(( \u2211M\nm=1 lm) 3) computational time. The time\ncomplexity of finding the least square solution and processing the QR decomposition for M data manifolds is O(Md3). Finally, transforming Yms into a single coordinate system needs O(d2N) computational time.\nTherefore, the total time complexity of the M-Isomap method is\nO((k + 1)N2 + M \u2211\nm=1\n(S 3m + S 2 m ln S m) +\nM \u2211\nm<n\n(k + 1)S mS n\n+( M \u2211\nm=1\nlm) 3 + Md3 + d2N).\n7 (a) (b) r r r r nx11 nx12 nx21\nnx31\nTT\nbO1\nr\nr\nr\nr\nnx13\nnx12\nnx31\nnx21 PPPPP bO1\nFig. 1 Two basic cases of the relationship between the center of each cluster or manifold and the inter-manifolds points\nFor a large data set where N >> M and N >> d, the overall time complexity of the M-Isomap algorithm can be approximated by\nO((k + 1)N2 + M \u2211\nm=1\n(S 3m + S 2 m ln S m) +\nM \u2211\nm<n\n(k + 1)S mS n)."}, {"heading": "D. The revised D-C Isomap method", "text": "D-C Isomap applies the decomposition-composition procedure. Therefore, it is able to preserve intra-cluster distances correctly. However, this method suffers from several limitations. In the following, the original D-C Isomap algorithm will be revised to overcome its limitations.\n1) Selection of centers: D-C Isomap implicitly assumes that the inter-cluster point nxmn is on the line which connects centers Om and nxnm. Thus it is more sensible that Om is chosen by referring to the inter-cluster points. Fig. 1 illustrates two basic cases about the relationship of the center and inter-cluster points. Although the points nx11, nx 2 1, nx 3 1, nx 1 2 and O1 do not have to really lie on the same plane in the ambient space. It is assumed that these points formulate a triangle in the low-dimensional space. Fig. 1 (a) shows the case when \u2220nx11nx 2 1nx 3 1+\u2220nx 1 2nx 3 1nx 2 1 < 180o. In the triangle \u2206O1nx31nx 2 1, the edge d(nx 2 1 , nx 3 1) can be computed as \u2016nx21 \u2212 nx31\u2016. We also have\n\u2220O1nx 2 1nx 3 1 = arccos < nx11 \u2212 nx21, nx31 \u2212 nx21 > \u2016nx11 \u2212 nx21\u2016\u2016nx31 \u2212 nx21\u2016\n\u2220O1nx 3 1nx 2 1 = arccos < nx12 \u2212 nx31, nx21 \u2212 nx31 > \u2016nx12 \u2212 nx31\u2016\u2016nx21 \u2212 nx31\u2016\nSubsequently, the length of edges d(O1, nx21) and d(O1, nx 3 1) can be calculated by the Law of Sines in the triangle \u2206O1nx31nx 2 1. Suggested distances between the center O1 to the inter-cluster points can be calculated as\nd\u2032(O1, nx11) = d(O1, nx 2 1) \u2212 \u2016nx21 \u2212 nx11\u2016 d\u2032(O1, nx 1 2) = d(O1, nx 3 1) \u2212 \u2016nx31 \u2212 nx11\u2016.\nFor a cluster with the intrinsic dimension 2, it is sufficient to estimate the position of O1 in the cluster by solving the following optimization problem:\nO1 = arg min o\u2208X1 f (o), (8)\nr\nr\nr\nr\nnx11\nnx12\nnx21\nnx22\nr\nr\nm1\nm2\nX1 X2\nX3 r\nFig. 2 An illustration of how to add a new cluster for D-C Isomap algorithm.\nwhere\nf (o) = 2 \u2211\ni=1\n\u2016d(O1, nx1i ) \u2212 d\u2032(O1, nx1i )\u2016.\nHere, d(O1, nx1i ) is the length of the shortest path between O1 and nx11 on the graph X\n1. For a cluster with intrinsic dimension dm, at least dm distances d\u2032(O1, nx1i ), i = 1, \u00b7 \u00b7 \u00b7 , dm, are needed to estimate the position of the center O1. In this case, f (o) is given by\nf (o) = dm \u2211\ni=1\n\u2016d(O1, nx1i ) \u2212 d\u2032(O1, nx1i )\u2016\nIf we can not find sufficiently many distances d\u2032(O1, nx1i ) to locate the center, there must be many inter-cluster points located in space, as illustrated in Fig. 1 (b). In this case, and when the center O1 is never on the line passing through nx12nx 2 1 and nx 1 3nx 3 1, we have \u2220nx11nx 2 1nx 3 1 + \u2220nx 1 2nx 3 1nx 2 1 \u2265 180o.\nIn order to get a better preservation of the inter-cluster relationship, O1 should be placed as far as possible from these intercluster points. For a cluster with intrinsic dimension 2, it is suggested that O1 should be chosen as\nO1 = arg max o\u2208X1\n{g(o)} (9)\nwhere\ng(o) = d(o, nx11) + d(o, nx 1 2) \u2212 \u2016d(o, nx11) \u2212 d(o, nx12)\u2016\nIf the intrinsic dimension of X1 is dm and {nx1i , i = 1, \u00b7 \u00b7 \u00b7 , dm} is the set of inter-cluster points in X1, then the function g(o) should be given as\ng(o) = dm \u2211\ni< j\n(\nd(o, nx1i ) + d(o, nx 1 j ) \u2212 \u2016d(o, nx1i ) \u2212 d(o, nx1j )\u2016\n)\n2) Degenerate and unworkable cases: Since the original D-C Isomap algorithm relies on the position of the center of each cluster to preserve the inter-cluster relationship, the algorithm does not work under certain circumstances. Consider a simple case of two data clusters with the intrinsic dimension d = 2. The method does not work in this case because it implicitly requires an another data cluster to provide sufficient rotation reference data points. Since the low-dimensional embedding of each cluster is relocated by referring to the position of the center of each cluster.\n8 In the case when there are three or more clusters and their centers are nearly on a line, the original D-C Isomap can not find the exact rotation matrix.\nThis issue is solved in this paper by adding fictitious clusters. The algorithm applies a trial and error procedure to determine the position of the fictitious clusters. As an example, we now consider the case of two clusters. As shown in Fig. 2, the nearest couple of inter-cluster points of the clusters X1 and X2 are assumed to be nx11 and nx 2 1, and m1 is the middle point between nx 1 1 and nx 2 1. The second nearest couple of inter-cluster points are nx12 and nx 2 2, and m2 is the middle point between them. The third fictitious cluster X3 is then suggested to be given by\nX3 = m1 + \u03b3\u2016nx11 \u2212 nx21\u2016 m2 \u2212 m1 \u2016m2 \u2212 m1\u2016 ,\nwhere the parameter \u03b3 can be decided by a trial and error procedure. Given a positive value \u03b2 > 1, X3 is assumed to satisfy that\n1 \u03b2 < \u2016X3 \u2212 X1\u2016 \u2016X3 \u2212 X2\u2016 < \u03b2, (10)\nwhere \u2016X3 \u2212 X1\u2016 is the shortest distance between the data points from clusters X1 and X3. If condition (10) is not satisfied, then \u03b3 can be chosen in a pre-given range such as\n{\u00b7 \u00b7 \u00b7 ,\u22123,\u22122,\u22121,\u22121 2 ,\u22121 3 , \u00b7 \u00b7 \u00b7 , 1 3 , 1 2 , 1, 2, \u00b7 \u00b7 \u00b7 }.\nIn the case when there are M clusters in the data set with M < d + 1, we can start from the couple of clusters with the maximum nearest inter-cluster distance. Assume that X1 and X2 satisfy that\n\u2016X1 \u2212 X2\u2016 = max i min j \u2016Xi \u2212 X j\u2016\nwith nx11, nx 1 2, m1, nx 2 1, nx 2 2, m2 being defined as above. Then\nthe (M + 1)-th cluster XM+1 can be generated as\nXM+1 = m1 + \u03b3\u2016nx11 \u2212 nx21\u2016 m2 \u2212 m1 \u2016m2 \u2212 m1\u2016\nIf Xp and Xq are the two nearest clusters of XM+1, then, given \u03b2 > 0, it is assumed that XM+1 should satisfy\n1 \u03b2 < \u2016XM+1 \u2212 Xp\u2016 \u2016XM+1 \u2212 Xq\u2016 < \u03b2.\nIf M+1 < d+1, then replace M by M+1 and repeat the generating procedure presented above.\nPCA can be used to find the dimensionality of the subspace on which the centers are lying. If the dimensionality of the subspace is smaller than d, then fictitious clusters should be added until the centers of the clusters can anchor a d-dimensional simplex.\n3) The revised D-C Isomap algorithm: The revised D-C Isomap algorithm can be given as follows.\nInput: X = {xi}Ni=1, with xi \u2208 RD. Initial neighborhood size k or \u03b5. Step I.1 Same as Step I.1 of the original D-C Isomap algorithm. Step I.2 Estimate the parameters, intrinsic dimension {dm}Mm=1 and neighborhood sizes ({km}Mm=1 or {\u03b5m}Mm=1), of the clusters. Let d = maxm dm and rebuild the neighborhood graph for each cluster. Step I.3-4 Same as Step I.2-3 of the original D-C Isomap algorithm. Step II.1 Centers of the clusters are computed by (8) or (9). Fictitious clusters should be added until centers of the clusters can anchor a d-dimensional simplex. Step II.2-4 Same as Step II.2-4 of the original D-C Isomap. Assume that Ym is transformed into TYm. Step II.5 Y = \u22c3M\nm=1 TY m is the final output.\nV. Experiments\nA. 3-D data sets\nIn this subsection, we compare the k-CC Isomap, the M-Isomap and the revised D-C Isomap on three 3-D data sets. It should be noted that in all experiments, the neighborhood size is chosen corresponding to the best performance of each algorithm.\nFig. 3 (a) is a two-manifolds data set with N = 1200 data points, and the data set is generated by the following matlab code:\nt=(1*pi/6)*(1+2*rand(1,N));\nxx=t.*cos(t);yy=t.*sin(t);\nzz =[unifrnd(1,10,1,N/2) unifrnd(16,25,1,N/2)];\nX=[xx;zz;yy];\nIt can be seen that each data manifold is intrinsically a rectangular region with 600 data points. Fig. 3(b) shows the result obtained by the k-CC Isomap, whose neighborhood graph is constructed by using the 8-CC method. It can be seen that the embedding shrinks along the edges in the low-dimensional space and the edges of the embedding become noisy. Fig. 3(c) shows the result obtained by the M-Isomap method with the neighborhood size k = 8. As can be seen, each data manifold is exactly unrolled, and the intermanifolds distance is precisely preserved. Fig. 3(d) illustrates the initialization step of the revised D-C Isomap algorithm. First, the two data manifolds X1 and X2 are identified. Then the third data cluster X3 is constructed, where the parameter \u03bb = 0.1. Finally, the centers O1 and O2 of the data manifolds are computed by referring to the nearest neighbors. The center of X3 is also the data point X3. Fig. 3(e) shows the result of the revised D-C Isomap method. It is seen that the embedding exactly preserves both the intra-manifold distances and inter-manifolds distances.\nFig. 4(a) is another two-manifolds data set with N = 1200 data points, and the data set is generated by the following matlab code:\nt=[unifrnd(pi*11/12,pi*14/12,1,N/2)\nunifrnd(pi*16/12,pi*19/12,1,N/2)];\nxx=t.*cos(tt);yy=t.*sin(tt);\nzz=unifrnd(1,25,1,N);\nY = [xx;zz;yy];\nEach data manifold has 600 data points. One data manifold is a rectangular region and the other one is a round region. Fig. 4(b) shows the result obtained by the k-CC Isomap with the\nneighborhood size k = 10. It can be seen that the rectangular region bent outwards and the round region is prolonged. Fig. 4(c) shows the result obtained by the M-Isomap method with the neighborhood size k = 8. As can be seen, each data manifold is exactly unrolled, and the inter-manifolds relationship is precisely preserved. Fig. 4(d) illustrates the initialization step of the revised D-C Isomap algorithm. The parameter \u03bb = 0.5 for the production of the new cluster X3. Fig. 4(e) shows the result of the revised D-C Isomap method with the neighborhood size k = 5. It can be seen that the embedding exactly preserves both the intra-manifold distances and inter-manifolds distances.\nFig. 5(a) shows a three-manifolds data set with N = 1600 data points on the Swiss roll manifold. The data set is generated by\nthe following matlab code: t1 = [unifrnd(pi*5/6,pi*16/12,1,N/4)];\nt2 = [unifrnd(pi*18/12,pi*12/6,1,N/4)];\nt3=(5*pi/6)*(1+7/5*rand(1,N/2));\na1=t1.*cos(t1); b1=t1.*sin(t1);\nc1=[unifrnd(-1,3,1,N/4)];\na2=t2.*cos(t2); b2=t2.*sin(t2);\nc2=[unifrnd(-1,3,1,N/4)];\na3=t3.*cos(t3); b3=t3.*sin(t3);\nc3=[unifrnd(6,10,1,N/2)];\nx1=[a1;c1;b1]; x2=[a2;c2;b2]; x3=[a3;c3;b3]\nZ=[x1 x2 x3];\nThere are three rectangular regions on the Swiss roll manifold.\n10\nThe longest data manifold has 800 data points, and each of the other two shorter data manifolds contains 400 data points. Fig. 5(b) shows the result obtained by the k-CC Isomap algorithm with the neighborhood size k = 10. Due to the bad approximation of the inter-manifolds geodesics, edges of the data manifolds bend outwards. Fig. 5(c) shows the result obtained by the M-Isomap method, where the neighborhood size k is set to be 8. As can be seen, all data manifolds are exactly unrolled, and the inter-manifolds relationships of the three data manifolds are precisely preserved. Fig. 5(d) illustrates the initiation step of the revised D-C Isomap algorithm. The result of the revised D-C Isomap method is presented in Fig. 5(e). As seen in Fig. 5(e), the embedding does not exactly preserve the inter-manifolds distances. This is because the shape of the data manifolds are very narrow. The selected reference data points can not efficiently relocate each piece of the data manifold."}, {"heading": "B. Real world data sets", "text": "Fig. 6(a) shows samples of the faces data [38] which contains face images of five persons1. The data set consists of 153 images and has 34, 35, 26, 24, 34 images corresponding to each face. These images are gray scale with resolution of 112 \u00d7 92. They are transformed into vectors in a 10304-dimensional Euclidean space. In order to show the inter-manifolds relationship with more details, the data is embedded into a three-dimensional space. Fig. 6(b) is the three-dimensional embedding by the PCA method. It can be observed that data manifolds of faces are mixed up, and the intra-face information is not preserved. Fig. 6(c) is the result obtained by the k-CC Isomap method with k = 3. As seen from Fig. 6(c), although the data points are clustered, their interface distances are not well preserved. The five lines are mixed up at one of their endings. Fig. 6(d) shows the result obtained by the M-Isomap method with k = 3. Due to the limitation of the k-NN method in clustering, only two data manifolds are\n1http://www.cs.toronto.edu/ roweis/data.html\nidentified. Although the data set is not well clustered, the result of the M-Ismap shows that the low-dimensional embedding can be separated easily. Fig. 6(e) presents the result obtained by the original D-C Isomap method, where the faces are split up beforehand. The circumcenters are used as their centers. However, as can be seen, two faces are mixed up.\nFig. 7(a) presents samples of the teapot data set with 300 data points, where \u2019 \u2019 stands for the teapot bird-view images, \u2019\u2206\u2019 stands for the teapot back-forth rotation images and \u2019\u00a9\u2019 stands for the teapot side-view images. Each image is an 80 \u00d7 60 \u00d7 3 RGB colored picture, that is, a vector in a 14400-dimensional input space. The data points do not distribute on a single global manifold, which is a great challenge to the classical manifold learning methods. The experiments show that the three data manifolds can be identified by the k-CC method. In order to show their exact embedding, Fig. 7(b)-(d) present the embedding of each data manifold by the classical Isomap with the neighborhood size k = 3. Fig. 7(e) gives the result obtained by the PCA method. It can be seen that the data set is clustered but the shape of each embedding is deformed because of the linear characteristic of the PCA method. Fig. 7(f) is the result obtained by the kCC Isomap method with the neighborhood size k = 3. The bad approximations of the inter-manifolds geodesics lead to the deformation of the embedding in the low-dimensional space. Fig. 7(g) shows the result obtained by the M-Isomap method with the neighborhood size k = 3. From Fig. 7(g), it is seen clearly that the data set is clearly clustered and the intra-manifolds relationships are exactly preserved. Fig. 7(h) gives the result obtained by the revised D-C Isomap method with the neighborhood size k = 3. It is seen that the revised D-C Isomap algorithm also produces a satisfactory result.\nFig. 8(a) shows samples of the IsoFACE and teapot rotation bird-view data set. The IsoFACE data set consists of 698 images and each image is a 64\u00d764 (4096-dimensional) gray scale picture. Since the input dimension of the IsoFACE data set is different from that of the teapot data set, the dimension of the IsoFACE\n11\n(a) The teapot data set. In the experiments, \u2019@\u2019 stands for the vertical view rotation of the teapot, \u2019 a \u2019 stands for the side view back-forth rotation of the teapot and \u2019\u00a9\u2019 stand for the side view rotation of the teapot. (b) The result of Isomap on the teapot vertical view rotation set. (c) The result of Isomap on the teapot side view back-forth rotation set. (d) The result of Isomap on the teapot side view rotation set. (e) The result of PCA on the teapot data set. (f)\nThe result of the k-CC Isomap on the teapot data set. (g) The result of theM-Isomap on the teapot data set. (h) The result of the revised D-C Isomap on the\nteapot data set.\nset is increased by adding zeros to the bottom of the face image vectors. The scale of the teapot data set should also be changed such that the scales of the two embeddings is compatible. The teapot data vectors are divided by 100, that is, the scale of the teapot data points shrinks to 1100 of the original one. Fig. 8(b) is the 3-D embedding of the IsoFACE data set by the classical Isomap with the neighborhood size k = 5. Fig. 8(c) is the 3-D embedding of the scaled teapot data set obtained by the classical Isomap with the neighborhood size k = 5. Fig. 8(d) gives the result obtained by the 5-CC Isomap method. It can be seen that the shape of the IsoFACE data set is distorted badly. Fig. 8(e) shows the result obtained by the M-Isomap method with the neighborhood size k = 5. The performance of the M-Isomap method is significantly improved compared with that of the kCC Isomap. Fig. 8(f) presents the result obtained by the revised D-C Isomap method, which is also satisfactory."}, {"heading": "C. Discussion", "text": "In our experiments, there are several important features which should be considered: 1) Since the k-CC Isomap tries to preserve poor and good approximations of geodesics simultaneously, its lowdimensional embedding is usually deformed. This method works well if each data manifold has comparable number of data points and the data manifolds can not be very far from each other. The algorithm does not work well otherwise.\n2) The revised D-C Isomap overcomes some limitations of the original D-C Isomap. Meanwhile, the robustness of\n12\nTo sum up, Table III shows the comparison of the general performance of the five versions of Isomap algorithms: classical Isomap, k-CC Isomap , Original D-C Isomap, revised D-C Isomap and M-Isomap. The labels \u201d\u2206\u201d stands for poor performance, \u201d \u201d stands for not bad and \u201d\u00a9\u201d stands for good. \u201dDensity\u201d means the generalization ability on manifolds with different density, that is, different neighborhood sizes, \u201dDimensionality\u201d means the generalization ability on manifolds with different intrinsic dimensionality, \u201dIsometric\u201d means the property of isometry in preserving the inter and intra-manifold relationship and \u201dGeneralization\u201d means the overall generalization ability to learn from multiple data manifolds.\nVI. Conclusion\nIn this paper, the problem of multi-manifolds learning is discussed. A general procedure for isometric multi-manifolds learning is proposed. The procedure can be used to build multimanifolds learning algorithms which are able to faithfully preserve not only intra-manifold geodesic distances but also the inter-manifolds geodesic distances. The M-Isomap is an implementation of the procedure and shows promising results in multimanifolds learning. Compared with the k-CC Isomap which was also introduced in this paper based on the general procedure, the M-Isomap has the advantage of low computational complexity. With the procedure, the D-C Isomap proposed in [24] was revised to overcome some of its limitations. Compared with the original D-C Isomap, the revised D-C Isomap is more effective in learning multi-manifolds data sets. Experiments have also been conducted on both synthetic and images data sets to illustrate the efficiency of the above multi-manifolds learning algorithms. Future work will be conducted on the application of the multi-manifolds learning algorithms.\nAcknowledgment\nThe work of the second author (HQ) was partly supported by the Chinese Academy of Sciences through the Hundred Talents Program, the NNSF of China under grant 60675039 and grant 60621001, the 863 Program of China under grant 2006AA04Z217 and the Outstanding Youth Fund of the NNSF of China under grant 60725310. The third author (BZ) was partly supported by the Chinese Academy of Sciences through the Hundred Talents Program, the 863 Program of China under grant 2007AA04Z228, the 973 Program of China under grant 2007CB311002 and the NNSF of China under grant 90820007.\nReferences\n[1] I. T. Jolliffe, \u201cPrincipal Component Analysis,\u201d Springer-Varlag, New York, 1989.\n13\n[2] T. F. Cox and M. A. Cox, \u201cMultidimensional Scaling,\u201d Chapman & Hall, London, 2001. [3] E. Mjolsness and D. De Coste, \u201cMachine learning for science: State of the art and future prospects,\u201d Science, vol. 293, pp. 2051-2055, Sep. 2001. [4] J.B. Tenenbaum, V. de Sliva and J. C. Landford, \u201cA global geometric framework for nonlinear dimensionality reduction,\u201d Science, vol. 290, pp. 2319-2323, Dec. 2000. [5] S.T. Roweis and L. K. Saul, \u201cNonlinear dimensionality reduction by local linear embedding,\u201d Science, vol. 290, pp. 2323-2326, Dec. 2000. [6] H.S. Seung and D.D. Lee, \u201cThe manifold ways of perception,\u201d Science, vol. 290, pp. 2268-2269, Dec. 2000. [7] M. Belkin and P. Niyogi, \u201cLaplacian eigenmaps for dimensionality reduction and data representation,\u201d Neural Computation, vol. 15, no 6, pp. 1373-1396, June 2003. [8] D. Donoho and C. Grimes, \u201cHessian eigenmaps: new locally linear embedding techniques for high-dimensional data,\u201d Proc. Nat. Acad. Sci. USA, vol. 100, no. 10, pp. 5591-5596, 2003. [9] Z. Zhang and H. Zha, \u201cPrincipal manifolds and nonlinear dimensionality reduction via tangent space alignment,\u201d SIAM J. Sci. Comput., vol. 26, pp. 313-338, 2004. [10] R. R. Coifman, S. Lafon, A.B. Lee, M. Maggoni, B. Nadler, F. Warner and S.W. Zuck, \u201cGeometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps,\u201d Proc. Nat. Acad. Sci. USA, vol. 102, no. 21, pp. 7426-7431, May. 2005. [11] T. Lin and H. Zha, \u201cRiemannian Manifold Learning,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no. 5, pp. 796-809, May. 2008. [12] H. Zha and Z. Zhang, \u201cContinuum Isomap for manifold learning,\u201d Computational Statistics & Data Analysis, vol. 52, issue. 1, pp. 184-200, Sep. 2007. [13] M.H.C. Law and A.K. Jain, \u201cIncremental nonlinear dimensionality reduction by manifold learning,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 377-391, March. 2006. [14] M. Bernstein, V. de Silva, J.C. Langford and J.B. Tenenbaum, \u201cGraph approximations to geodesics on embedded manifolds,\u201d Technical report, Dept. of Psychology, Stanford Univ., Dec. 2000. [15] M. Filippone, F. Camastra, F. Masulli and S. Rovetta, \u201cA survey of kernel and spectral methods for clustering,\u201d Pattern Recognition, vol. 41, pp. 176-190, May. 2007. [16] M.-H. Yang, \u201cExtended Isomap for pattern classification,\u201d ICPR 2002: Proc. Int. Conf. Pattern Recognition, vol. 3, Aug. 2002. [17] X. He, S. Yan, T. Hu, P. Niyogi and H. Zhang, \u201cFace recognition using Laplacianfaces,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 3, pp. 328-340, March. 2005. [18] M. Belkin, V. Sindhwani and P. Niyogi, \u201cManifold regularization: a geometric framework for learning from examples,\u201d J. Machine Learning Research, vol. 7, pp. 2399-2434, Dec. 2006. [19] X. Geng, D. Zhang and Z. Zhou, \u201cSupervised nonlinear dimensionality reduction for visualization and classification,\u201d IEEE Trans. Systems, Man and Cybern. B, vol. 35, no. 6, pp. 1098-1107, Dec 2005. [20] O.C. Jenkins and M.J. Mataric, \u201cA Spatio-temporal extension to Isomap nonlinear dimension reduction,\u201d ACM. Proc. 21st Int. Conf. on Machine learning, vol. 69, pp. 56-66, 2004. [21] A. Rahimi, B. Recht and T. Darrell, \u201cLearning to transform time series with a few examples ,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 10, pp. 1759-1775, Oct. 2007. [22] J.B. Tenenbaum, V. de Sliva and J.C. Landford \u201cResponse to Comments on the Isomap algorithm and topological stability,\u201d Sciences, vol. 295, no. 5552, pp. 7, Jan. 2002. [23] Y. Wu and K.L. Chan \u201cAn extended Isomap algorithm for learning multiclass manifold,\u201d Proc. of the 2004 Int. Conf. on Machine Learning and Cybernetics, vol. 6, pp. 3429-3433, Aug. 2004. [24] D. Meng, Y. Leung, T. Fung and Z. Xu \u201cNonlinear dimensionality reduction of data lying on the multicluster manifold,\u201d IEEE Trans. Systems, Man and Cybern. B, vol. 38, issue. 4, pp. 1111-1122. Aug. 2008. [25] B. Li, D. Huang, C. Wang and K. Liu \u201cFeature extraction using constrained maximum variance mapping,\u201d Pattern Recognition, vol. 41, pp. 3287-3294. May. 2008. [26] L. Yang, \u201cK-Edge Connected Neighborhood Graph for Geodesic Distance Estimation and Nonlinear Projection,\u201d Proc. of 17th Int. Conf. on Pattern Recognition (ICPR\u201904), vol. 1, pp. 196-199. 2004. [27] L. Yang, \u201cBuilding k Edge-Disjoint Spanning Trees of Minimum Total Length for Isometric Data Embedding,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1680-1683, Oct. 2005. [28] L. Yang, \u201cBuilding k Edge-connected neighborhood graph for distancebased data projection,\u201d Pattern Recognition Letters, vol. 26, no. 13, pp. 2015-2021, Oct. 2005. [29] L. Yang, \u201cBuilding k-Connected Neighborhood Graphs for Isometric Data Embedding,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, issue. 5, pp. 827-831, May. 2006. [30] D. Zhao and L. Yang, \u201cIncremental Construction of Neighborhood Graphs for Nonlinear Dimensionality Reduction,\u201d Proc. of Int. Conf. on Pattern Recognition, vol. 28, no. 5, pp. 827-831, May. 2006. [31] D. Zhao and L. Yang, \u201cIncremental Isometric Embedding of HighDimensional Data Using Connected Neighborhood Graphs,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 1, pp. 86-98, Jan. 2009. [32] O. Samko, A.D. Marshall and P.L. Rosin \u201c Selection of the optimal parameter value for the Isomap algorithm,\u201d Pattern Recognition Letters, vol. 27, no. 9, pp. 968-979, Feb. 2006. [33] R. Xu and D. Wunsch II, \u201dSurvey of clustering algorithms,\u201d IEEE Trans on Neural Networks, vol. 16, no. 3, pp. 645-678, May. 2005. [34] F. Camastra and A. Vinciarelli, \u201dEstimating the intrinsic dimension of data with a fractal-based approach,\u201d IEEE Trans. Pattern Analysis and Machine Intelligence vol. 24, no. 10, pp. 1404-1407, Oct. 2002. [35] E. Levina and P. J. Bickel \u201cMaximum likelihood estimation of intrinsic dimension,\u201d NIPS 04: Neural Information Processing Systems,2005. [36] D.J.C. MacKay and Z. Ghahramani, \u201dComments on \u2019Maximum likelihood estimation of intrinsic dimension\u2019 by E. Levina and P. Bickel (2005),\u201d in: http://www.inference.phy.cam.ac.uk/mackay/dimension/ [37] M. Fan, H. Qiao and B. Zhang, \u201dIntrinsic dimension estimation of manifolds by incising balls,\u201d Pattern Recognition vol. 42, no. 5, pp. 780- 787, May. 2009. [38] B. Graham and N.M. Allinson, \u201dCharacterizing Virtual Eigensignatures for General Purpose Face Recognition\u201d, In: H. Wechsler, P.J. Phillips, V. Bruce, F. Fogelman-Soulie and T.S. Huang (eds): \u201dFace Recognition: From Theory to Applications;\u201d NATO ASI Series F, Computer and Systems Sciences, Vol. 163, pp. 446-456, 1998. [39] D. Kushnir, M. Galun and A. Brandt \u201cFast multiscale clustering and manifold identification,\u201d Pattern Recognition, vol. 28, no. 10, pp. 1876- 1891, Oct. 2006.\n0 5 10 15 20 25\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\n0\n1\nX 1\nX 2\nX 3\nO 1\nO\n(d)\n\u221210 \u22128 \u22126 \u22124 \u22122 0\n2 4 6 8\n0\n5\n10 \u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n(b)\n\u221215 \u221210 \u22125 0 5 10 15 \u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1 Twice Isomap on data set A\nX\n\u221230 \u221220 \u221210 0 10 20 30 \u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10 Twice Isomap on data set C\nX\n\u221215 \u221210 \u22125 0 5 10 15 \u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n(c)\n\u22124 \u22123 \u22122 \u22121 0 1 2 3\nx 10 4\n2 0\n2\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\nx 10 4\nTwice Isomap on teapot data set\nXY\n\u221210 \u22128 \u22126 \u22124 \u22122 0\n2 4 6 8\n0\n10\n20\n30\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n(a)\n\u221230 \u221225 \u221220 \u221215 \u221210 \u22125 0 5 10 15 20 \u221215\n\u221210\n\u22125\n0\n5\n10\n15\n\u221220 \u221215 \u221210 \u22125 0 5 10 15 20 \u221240\n\u221230\n\u221220\n\u221210\n0\n10\n20\n30 Two\u2212dimensional twice Isomap embedding\nx"}], "references": [{"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer-Varlag, New York, 1989.  13", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Multidimensional Scaling", "author": ["T.F. Cox", "M.A. Cox"], "venue": "Chapman & Hall, London, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Machine learning for science: State of the art and future prospects", "author": ["E. Mjolsness", "D. De Coste"], "venue": "Science, vol. 293, pp. 2051-2055, Sep. 2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Sliva", "J.C. Landford"], "venue": "Science, vol. 290, pp. 2319-2323, Dec. 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by local linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, pp. 2323-2326, Dec. 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "The manifold ways of perception", "author": ["H.S. Seung", "D.D. Lee"], "venue": "Science, vol. 290, pp. 2268-2269, Dec. 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, vol. 15, no 6, pp. 1373-1396, June 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Hessian eigenmaps: new locally linear embedding techniques for high-dimensional data", "author": ["D. Donoho", "C. Grimes"], "venue": "Proc. Nat. Acad. Sci. USA, vol. 100, no. 10, pp. 5591-5596, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Principal manifolds and nonlinear dimensionality reduction via tangent space alignment", "author": ["Z. Zhang", "H. Zha"], "venue": "SIAM J. Sci. Comput., vol. 26, pp. 313-338, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps", "author": ["R.R. Coifman", "S. Lafon", "A.B. Lee", "M. Maggoni", "B. Nadler", "F. Warner", "S.W. Zuck"], "venue": "Proc. Nat. Acad. Sci. USA, vol. 102, no. 21, pp. 7426-7431, May. 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Riemannian Manifold Learning", "author": ["T. Lin", "H. Zha"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no. 5, pp. 796-809, May. 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Continuum Isomap for manifold learning", "author": ["H. Zha", "Z. Zhang"], "venue": "Computational Statistics & Data Analysis, vol. 52, issue. 1, pp. 184-200, Sep. 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Incremental nonlinear dimensionality reduction by manifold learning", "author": ["M.H.C. Law", "A.K. Jain"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 377-391, March. 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Graph approximations to geodesics on embedded manifolds", "author": ["M. Bernstein", "V. de Silva", "J.C. Langford", "J.B. Tenenbaum"], "venue": "Technical report, Dept. of Psychology, Stanford Univ., Dec. 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, pp. 176-190, May. 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Extended Isomap for pattern classification", "author": ["M.-H. Yang"], "venue": "ICPR 2002: Proc. Int. Conf. Pattern Recognition, vol. 3, Aug. 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Face recognition using Laplacianfaces", "author": ["X. He", "S. Yan", "T. Hu", "P. Niyogi", "H. Zhang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 3, pp. 328-340, March. 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Manifold regularization: a geometric framework for learning from examples", "author": ["M. Belkin", "V. Sindhwani", "P. Niyogi"], "venue": "J. Machine Learning Research, vol. 7, pp. 2399-2434, Dec. 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Supervised nonlinear dimensionality reduction for visualization and classification", "author": ["X. Geng", "D. Zhang", "Z. Zhou"], "venue": "IEEE Trans. Systems, Man and Cybern. B, vol. 35, no. 6, pp. 1098-1107, Dec 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "A Spatio-temporal extension to Isomap nonlinear dimension reduction", "author": ["O.C. Jenkins", "M.J. Mataric"], "venue": "ACM. Proc. 21st Int. Conf. on Machine learning, vol. 69, pp. 56-66, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to transform time series with a few examples", "author": ["A. Rahimi", "B. Recht", "T. Darrell"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 10, pp. 1759-1775, Oct. 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Landford \u201cResponse to Comments on the Isomap algorithm and topological stability,", "author": ["J.B. Tenenbaum", "J.C.V. de Sliva"], "venue": "Sciences, vol. 295,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "An extended Isomap algorithm for learning multiclass manifold,", "author": ["Y. Wu", "K.L. Chan"], "venue": "Proc. of the 2004 Int. Conf. on Machine Learning and Cybernetics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Nonlinear dimensionality reduction of data lying on the multicluster manifold,", "author": ["D. Meng", "Y. Leung", "T. Fung", "Z. Xu"], "venue": "IEEE Trans. Systems, Man and Cybern. B,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Feature extraction using constrained maximum variance mapping,", "author": ["B. Li", "D. Huang", "C. Wang", "K. Liu"], "venue": "Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "K-Edge Connected Neighborhood Graph for Geodesic Distance Estimation and Nonlinear Projection", "author": ["L. Yang"], "venue": "Proc. of 17th Int. Conf. on Pattern Recognition (ICPR\u201904), vol. 1, pp. 196-199. 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Building k Edge-Disjoint Spanning Trees of Minimum Total Length for Isometric Data Embedding", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1680-1683, Oct. 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Building k Edge-connected neighborhood graph for distancebased data projection", "author": ["L. Yang"], "venue": "Pattern Recognition Letters, vol. 26, no. 13, pp. 2015-2021, Oct. 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Building k-Connected Neighborhood Graphs for Isometric Data Embedding", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, issue. 5, pp. 827-831, May. 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Incremental Construction of Neighborhood Graphs for Nonlinear Dimensionality Reduction", "author": ["D. Zhao", "L. Yang"], "venue": "Proc. of Int. Conf. on Pattern Recognition, vol. 28, no. 5, pp. 827-831, May. 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Incremental Isometric Embedding of High- Dimensional Data Using Connected Neighborhood Graphs", "author": ["D. Zhao", "L. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 1, pp. 86-98, Jan. 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating the intrinsic dimension of data with a fractal-based approach,", "author": ["F. Camastra", "A. Vinciarelli"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence vol. 24,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Maximum likelihood estimation of intrinsic dimension,", "author": ["E. Levina", "P.J. Bickel"], "venue": "NIPS 04: Neural Information Processing Systems,2005", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Comments on \u2019Maximum likelihood estimation of intrinsic dimension", "author": ["D.J.C. MacKay", "Z. Ghahramani"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Intrinsic dimension estimation of manifolds by incising balls,", "author": ["M. Fan", "H. Qiao", "B. Zhang"], "venue": "Pattern Recognition vol. 42,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Characterizing Virtual Eigensignatures for General Purpose Face Recognition", "author": ["B. Graham", "N.M. Allinson"], "venue": "NATO ASI Series F, Computer and Systems Sciences,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1998}, {"title": "Fast multiscale clustering and manifold identification,", "author": ["D. Kushnir", "M. Galun", "A. Brandt"], "venue": "Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}], "referenceMentions": [{"referenceID": 23, "context": "Some revisions have been made on the original multi-cluster manifold learning algorithm called D-C Isomap [24] such that the revised D-C Isomap can learn multi-manifolds data.", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Principal component analysis (PCA) [1] and multidimensional scaling (MDS) [2] are two important linear dimensionality reduction methods.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Principal component analysis (PCA) [1] and multidimensional scaling (MDS) [2] are two important linear dimensionality reduction methods.", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "NLDR is used to learn nonlinear intrinsic structure of data, which is considered to be the first step of \u201dmachine learning and pattern recognition: observe and explore the phenomena\u201d [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000.", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "Laplacian eigenmap [7] utilizes the approximation of the Laplace-Beltrami operator on manifold to provide an optimal embedding.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "Hessian LLE [8] resembles Laplacian eigenmap by using the approximation of Hessian operator instead of Laplacian operator.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "Local tangent space alignment(LTSA) [9] method learns local geometry by constructing a local tangent space of each data point and then aligns these local tangent spaces into a single global coordinates system with respect to the underlying manifold.", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "Diffusion maps [10] applies diffusion semigroups to produce multiscale geometries to represent complex structure.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "Riemannian manifold learning (RML) [11] method uses the constructed Riemannian normal coordinate chart to map the input data into a lower dimensional space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 256, "endOffset": 260}, {"referenceID": 19, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 292, "endOffset": 296}, {"referenceID": 20, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 298, "endOffset": 302}, {"referenceID": 12, "context": "As Isomap emphasizes on the global geometric relationship of data points, it is very illustrative in data visualization and pattern analysis [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "Yiming Wu et al [23] introduced a split and augment procedure for neighborhood graph construction which could produce a totally connected neighborhood graph.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "Li Yang [26]\u2013[29] introduced several neighborhood graph construction algorithms using techniques from discrete mathematics, graph theory.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Li Yang [26]\u2013[29] introduced several neighborhood graph construction algorithms using techniques from discrete mathematics, graph theory.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "Deyu Meng et al [24] proposed a decomposition and composition Isomap (DC Isomap).", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Classical Isometric FeatureMapping and Its Limitations Isomap is able to recover the intrinsic geometric structure and converge as the number of data points increases [4] if data lie on a manifold, .", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "The properties of Isomap algorithm are well understood [14] [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "The properties of Isomap algorithm are well understood [14] [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "Otherwise, if neighborhood size k or \u01eb is chosen too large to cause short-circuit edges, these edges will have a significant negative influence on the topological stability of Isomap algorithm [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "Wu and Chan [23] proposed a split-augment approach to construct a neighborhood graph.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Li [26]\u2013[29] introduced four methods to construct a connected neighborhood graph.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Li [26]\u2013[29] introduced four methods to construct a connected neighborhood graph.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "The k minimum spanning trees (k-MST) [26] method repeatly extracts k minimum spanning trees (MSTs) from the complete Euclidean graph of all data points.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Instead of extracting k MSTs, the minimum-k-spanning trees (min-kST) [27] method finds k edge-disjoint spanning trees from the complete Euclidean graph, and the sum of the total edge length of the k edge-disjoint spanning trees attains a minimum.", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "The kedge-connected (k-EC) [28] method constructs a connected neighborhood graph by adding edges in a non-increasing order from the complete Euclidean graph.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "The k-vertices-connected (k-VC) [29] method add edges in a nonincreasing order from the complete Euclidean graph, an edge is added if its two end vertices would be disconnected by removing some k \u2212 1 vertices.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "The methods introduced in [26]\u2013[29] advantage over k-NN method for two reasons: First, the local neighbor relationship is affected by the global distribution of data points.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "The methods introduced in [26]\u2013[29] advantage over k-NN method for two reasons: First, the local neighbor relationship is affected by the global distribution of data points.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Multi-manifolds learning by decomposition-composition Isomap In [24], Meng et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Constrained Maximum Variance Mapping There is also a newly proposed algorithm called constrained maximum variance mapping (CMVM) [25] for multi-manifolds learning.", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "The general procedure for isometric multi-manifolds learning Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29].", "startOffset": 203, "endOffset": 207}, {"referenceID": 25, "context": "The general procedure for isometric multi-manifolds learning Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29].", "startOffset": 209, "endOffset": 213}, {"referenceID": 28, "context": "The general procedure for isometric multi-manifolds learning Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29].", "startOffset": 214, "endOffset": 218}, {"referenceID": 11, "context": "With the assumptions above, the following theorem is proved by Zha et al [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Many clustering method could be used, such as K-means, Isodata and methods introduced in [15] [33].", "startOffset": 89, "endOffset": 93}, {"referenceID": 36, "context": "Even if the manifolds overlay with each other, they can still be identified and clustered [39].", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 33, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 34, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Methods of Li\u2019s works [26]\u2013[29] or the k-CC method is preferred, where the k-CC graph construction method will be described later.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "Methods of Li\u2019s works [26]\u2013[29] or the k-CC method is preferred, where the k-CC graph construction method will be described later.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "[23] in their split-augment process and well developed and used in [24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[23] in their split-augment process and well developed and used in [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 29, "context": "For incremental learning, the computational complexity of kNN, k-MSTs and k-VC are O(kN), O(N log N) and O(N log N + kN) respectively [30] [31].", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": "For incremental learning, the computational complexity of kNN, k-MSTs and k-VC are O(kN), O(N log N) and O(N log N + kN) respectively [30] [31].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "Compared with the method proposed in [23], k-CC method constructs a neighborhood graph with k inter-manifolds edges, which is able to control the rotation of the embedding of data manifolds.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "The incising ball method [37] is utilized to estimate the intrinsic dimensionality, which is simple to implement and always outputs an integer result.", "startOffset": 25, "endOffset": 29}, {"referenceID": 35, "context": "6(a) shows some samples of the faces data [38] which contains face images of five persons 1.", "startOffset": 42, "endOffset": 46}], "year": 2009, "abstractText": "Isometric feature mapping (Isomap) is a promising manifold learning method. However, Isomap fails to work on data which distribute on clusters in a single manifold or manifolds. Many works have been done on extending Isomap to multi-manifolds learning. In this paper, we proposed a new multi-manifolds learning algorithm (M-Isomap) with the help of a general procedure. The new algorithm preserves intramanifold geodesics and multiple inter-manifolds edges faithfully. Compared with previous approaches, this algorithm can isometrically learn data distribute on several manifolds. Some revisions have been made on the original multi-cluster manifold learning algorithm called D-C Isomap [24] such that the revised D-C Isomap can learn multi-manifolds data. Finally, the features and effectiveness of the proposed multi-manifolds learning algorithms are demonstrated and compared through experiments.", "creator": "LaTeX with hyperref package"}}}