{"id": "1002.4286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2010", "title": "Redundancy, Deduction Schemes, and Minimum-Size Bases for Association Rules", "abstract": "association rules are among the most widely employed data analysis methods in the field of data mining. an association rule is a form of partial interval implication between two sets of binary variables. in the most common approach, association rules are parameterized by a lower bound on their confidence, which is the empirical equivalent conditional probability consisting of their consequent given the antecedent, and / or by some other parameter bounds so such as \" support \" or deviation from independence. we study here notions of redundancy among association rules from a fundamental perspective. we see each transaction in a dataset as an interpretation ( or model ) in the propositional logic sense, and consider existing notions of redundancy, that is, of logical entailment, among association rules, of constructing the form \" any dataset satisfying in which this first rule holds must obey also that second rule, therefore the second is redundant \". we discuss several existing alternative definitions of minimum redundancy between comparable association rules and provide new characterizations and relationships among them. we show that the mutually main alternatives we discuss correspond actually to just two variants, which differ in the treatment of full - confidence implications. for each of these two notions of redundancy, we provide a sound and complete deduction calculus, and we show how to construct complete bases ( that is, axiomatizations ) of absolutely minimum size in terms of the number of rules. we explore finally an approach to redundancy with respect to several association gate rules, and fully characterize its simplest case of two partial premises.", "histories": [["v1", "Tue, 23 Feb 2010 10:02:24 GMT  (93kb,D)", "https://arxiv.org/abs/1002.4286v1", "LMCS accepted paper"], ["v2", "Sat, 26 Jun 2010 22:44:45 GMT  (96kb,D)", "http://arxiv.org/abs/1002.4286v2", "LMCS accepted paper"]], "COMMENTS": "LMCS accepted paper", "reviews": [], "SUBJECTS": "cs.LO cs.AI", "authors": ["jose l balcazar"], "accepted": false, "id": "1002.4286"}, "pdf": {"name": "1002.4286.pdf", "metadata": {"source": "CRF", "title": "REDUNDANCY, DEDUCTION SCHEMES, AND MINIMUM-SIZE BASES FOR ASSOCIATION RULES", "authors": ["JOS\u00c9 L. BALC\u00c1ZAR", "Jos\u00e9 L. Balc\u00e1zar"], "emails": ["joseluis.balcazar@unican.es"], "sections": [{"heading": "1. Introduction", "text": "The relatively recent discipline of Data Mining involves a wide spectrum of techniques, inherited from different origins such as Statistics, Databases, or Machine Learning. Among them, Association Rule Mining is a prominent conceptual tool and, possibly, a cornerstone notion of the field, if there is one. Currently, the amount of available knowledge regarding association rules has grown to the extent that the tasks of creating complete surveys and websites that maintain pointers to related literature become daunting. A survey, with plenty\n1998 ACM Subject Classification: I.2.3, H.2.8, I.2.4, G.2.3, F.4.1. Key words and phrases: Data mining, association rules, implications, redundancy, deductive calculus, optimum bases. This work is supported in part by project TIN2007-66523 (FORMALISM) of Programa Nacional de Investigacio\u0301n, Ministerio de Ciencia e Innovacio\u0301n (MICINN), Spain, and by the PASCAL2 Network of Excellence of the European Union.\nLOGICAL METHODSl IN COMPUTER SCIENCE DOI:10.2168/LMCS-6 (2:3) 2010 c\u00a9 Jos\u00e9 L. Balc\u00e1zarCC\u00a9 Creative Commons\nof references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.\nGiven an agreed general set of \u201citems\u201d, association rules are defined with respect to a dataset that consists of \u201ctransactions\u201d, each of which is, essentially, a set of items. Association rules are customarily written as X \u2192 Y , for sets of items X and Y , and they hold in the given dataset with a specific \u201cconfidence\u201d quantifying how often Y appears among the transactions in which X appears.\nA close relative of the notion of association rule, namely, that of exact implication in the standard propositional logic framework, or, equivalently, association rule that holds in 100% of the cases, has been studied in several guises. Exact implications are equivalent to conjunctions of definite Horn clauses: the fact, well-known in logic and knowledge representation, that Horn theories are exactly those closed under bitwise intersection of propositional models leads to a strong connection with Closure Spaces, which are characterized by closure under intersection (see the discussions in [15] or [26]). Implications are also very closely related to functional dependencies in databases. Indeed, implications, as well as functional dependencies, enjoy analogous, clear, robust, hardly disputable notions of redundancy that can be defined equivalently both in semantic terms and through the same syntactic calculus. Specifically, for the semantic notion of entailment, an implication X \u2192 Y is entailed from a set of implications R if every dataset in which all the implications of R hold must also satisfy X \u2192 Y ; and, syntactically, it is known that this happens if and only if X \u2192 Y is derivable from R via the Armstrong axiom schemes, namely, Reflexivity (X \u2192 Y for Y \u2286 X), Augmentation (if X \u2192 Y and X \u2032 \u2192 Y \u2032 then XX \u2032 \u2192 Y Y \u2032, where juxtaposition denotes union) and Transitivity (if X \u2192 Y and Y \u2192 Z then X \u2192 Z).\nAlso, such studies have provided a number of ways to find implications (or functional dependencies) that hold in a given dataset, and to construct small subsets of a large set of implications, or of functional dependencies, from which the whole set can be derived; in Closure Spaces and in Data Mining these small sets are usually called \u201cbases\u201d, whereas in Dependency Theory they are called \u201ccovers\u201d, and they are closely related to deep topics such as hypergraph theory. Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.\nHowever, the fact has been long acknowledged (e.g. already in [33]) that, often, it is inappropriate to search only for absolute implications in the analysis of real world datasets. Partial rules are defined in relation to their \u201cconfidence\u201d: for a given rule X \u2192 Y , the ratio of how often X and Y are seen together to how often X is seen. Many other alternative measures of intensity of implication exist [20], [21]; we keep our focus on confidence because, besides being among the most common ones, it has a natural interpretation for educated users through its correspondence with the observed conditional probability.\nThe idea of restricting the exploration for association rules to frequent itemsets, with respect to a support threshold, gave rise to the most widely discussed and applied algorithm, called Apriori [3], and to an intense research activity. Already with full-confidence implications, the output of an association mining process often consists of large sets of rules, and a well-known difficulty in applied association rule mining lies in that, on large datasets, and for sensible settings of the confidence and support thresholds and other parameters, huge amounts of association rules are often obtained. Therefore, besides the interesting progress in the topic of how to organize and query the rules discovered (see [31], [32], [42]),\none research topic that has been worthy of attention is the identification of patterns that indicate redundancy of rules, and ways to avoid that redundancy; and each proposed notion of redundancy opens up a major research problem, namely, to provide a general method for constructing bases of minimum size with respect to that notion of redundancy.\nFor partial rules, the Armstrong schemes are not valid anymore. Reflexivity does hold, but Transitivity takes a different form that affects the confidence of the rules: if the rule A \u2192 B (or A \u2192 AB, which is equivalent) and the rule B \u2192 C both hold with confidence at least \u03b3, we still know nothing about the confidence of A \u2192 C; even the fact that both A \u2192 AB and AB \u2192 C hold with confidence at least \u03b3 only gives us a confidence lower bound of \u03b32 < \u03b3 for A\u2192 C (assuming \u03b3 < 1). Augmentation does not hold at all; indeed, enlarging the antecedent of a rule of confidence at least \u03b3 may give a rule with much smaller confidence, even zero: think of a case where most of the times X appears it comes with Z, but it only comes with Y when Z is not present; then the confidence of X \u2192 Z may be high whereas the confidence of XY \u2192 Z may be null. Similarly, if the confidence of X \u2192 Y Z is high, it means that Y and Z appear together in most of the transactions having X, whence the confidences of X \u2192 Y and X \u2192 Z are also high; but, with respect to the converse, the fact that both Y and Z appear in fractions at least \u03b3 of the transactions having X does not inform us that they show up together at a similar ratio of these transactions: only a ratio of 2\u03b3 \u2212 1 < \u03b3 is guaranteed as a lower bound. In fact, if we look only for association rules with singletons as consequents (as in some of the analyses in [1], or in the \u201cbasic association rules\u201d of [30], or even in the traditional approach to association rules [2] and the useful apriori implementation of Borgelt available on the web [8]) we are almost certain to lose information. As a consequence of these failures of the Armstrong schemes, the canonical and minimum-size cover construction methods available for implications or functional dependencies are not appropriate for partial association rules.\nOn the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]). All of these are weaker than the notion that we would consider natural by comparison with implications (of which we start the study in the last section of this paper). We observe here that one may wish to fulfill two different roles with a basis, and that both appear (somewhat mixed) in the literature: as a computer-supported data structure from which confidences and supports of rules are computed (a role for which we use the closures lattice instead) or, in our choice, as a means of providing the user with a smallish set of association rules for examination and, if convenient, posterior enumeration of the rules that follow from each rule in the basis. That is, we will not assume to have available, nor to wish to compute, exact values for the confidence, but only discern whether it stays above a certain user-defined threshold. We compute actual confidences out of the closure lattice only at the time of writing out rules for the user.\nThis paper focuses mainly on several such notions of redundancy, defined in a rather general way, by resorting to confidence and support inequalities: essentially, a rule is redundant with respect to another if it has at least the same confidence and support of the latter for every dataset. We also discuss variants of this proposal and other existing definitions given in set-theoretic terms. For the most basic notion of redundancy, we provide formal proofs of the so far unstated equivalence among several published proposals, including a\nsyntactic calculus and a formal proof of the fact, also previously unknown, that the existing basis known as the Essential Rules or the Representative Rules ([1], [27], [38]) is of absolutely minimum size.\nIt is natural to wish further progress in reducing the size of the basis. Our theorems indicate that, in order to reduce further the size without losing information, more powerful notions or redundancy must be deployed. We consider for this role the proposal of handling separately, to a given extent, full-confidence implications from lower-than-1-confidence rules, in order to profit from their very different combinatorics. This separation is present in many constructions of bases for association rules [33], [36], [44]. We discuss corresponding notions of redundancy and completeness, and prove new properties of these notions; we give a sound and complete deductive calculus for this redundancy; and we refine the existing basis constructions up to a point where we can prove again that we attain the limit of the redundancy notion.\nNext, we discuss yet another potential for strengthening the notion of redundancy. So far, all the notions have just related one partial rule to another, possibly in the presence of full implications. Is it possible to combine two partial rules, of confidence at least \u03b3, and still obtain a partial rule obeying that confidence level? Whereas the intuition is that these confidences will combine together to yield a confidence lower than \u03b3, we prove that there is a specific case where a rule of confidence at least \u03b3 is nontrivially entailed by two of them. We fully characterize this case and obtain from the caracterization yet another deduction scheme. We hope that further progress along the notion of a set of partial rules entailing a partial rule will be made along the coming years.\nPreliminary versions of the results in sections 3.1, 4.2, 4.3, and 5 have been presented at Discovery Science 2008 [6]; preliminary versions of the remaining results (except those in section 4.5, which are newer and unpublished) have been presented at ECMLPKDD 2008 [5]."}, {"heading": "2. Preliminaries", "text": "Our notation and terminology are quite standard in the Data Mining literature. All our developments take place in the presence of a \u201cuniverse\u201d set U of atomic elements called items; their absence or presence in sets or items plays the same role as binary-valued attributes of a relational table. Subsets of U are called itemsets. A dataset D is assumed to be given; it consists of transactions, each of which is an itemset labeled by a unique transaction identifier. The identifiers allow us to distinguish among transactions even if they share the same itemset. Upper-case, often subscripted letters from the end of the alphabet, like X1 or Y0, denote itemsets. Juxtaposition denotes union of itemsets, as in XY ; and Z \u2282 X denotes proper subsets, whereas Z \u2286 X is used for the usual subset relationship with potential equality.\nFor a transaction t, we denote t |= X the fact that X is a subset of the itemset corresponding to t, that is, the transaction satisfies the minterm corresponding to X in the propositional logic sense.\nFrom the given dataset we obtain a notion of support of an itemset: sD(X) is the cardinality of the set of transactions that include it, {t \u2208 D \u2223\u2223 t |= X}; sometimes, abusing language slightly, we also refer to that set of transactions itself as support. Whenever D is clear, we drop the subindex: s(X). Observe that s(X) \u2265 s(Y ) whenever X \u2286 Y ; this is immediate from the definition. Note that many references resort to a normalized notion of support by dividing by the dataset size. We chose not to, but there is no essential issue\nhere. Often, research work in Data Mining assumes that a threshold on the support has been provided and that only sets whose support is above the threshold (then called \u201cfrequent\u201d) are to be considered. We will require this additional constraint occassionally for the sake of discussing the applicability of our developments.\nWe immediately obtain by standard means (see, for instance, [19] or [44]) a notion of closed itemsets, namely, those that cannot be enlarged while maintaining the same support. The function that maps each itemset to the smallest closed set that contains it is known to be monotonic, extensive, and idempotent, that is, it is a closure operator. This notion will be reviewed in more detail later on. Closed sets whose support is above the support threshold, if given, are usually termed closed frequent sets.\nAssociation rules are pairs of itemsets, denoted as X \u2192 Y for itemsets X and Y . Intuitively, they suggest the fact that Y occurs particularly often among the transactions in which X occurs. More precisely, each such rule has a confidence associated: the confidence cD(X \u2192 Y ) of an association rule X \u2192 Y in a dataset D is s(XY )s(X) . As with support, often we drop the subindex D. The support in D of the association rule X \u2192 Y is sD(X \u2192 Y ) = sD(XY ).\nWe can switch rather freely between right-hand sides that include the left-hand side and right-hand sides that don\u2019t:\nDefinition 2.1. Rules X0 \u2192 Y0 and X1 \u2192 Y1 are equivalent by reflexivity if X0 = X1 and X0Y0 = X1Y1.\nClearly, cD(X \u2192 Y ) = cD(X \u2192 XY ) = cD(X \u2192 X \u2032Y ) and, likewise, sD(X \u2192 Y ) = sD(X \u2192 XY ) = sD(X \u2192 X \u2032Y ) for any X \u2032 \u2286 X; that is, the support and confidence of rules that are equivalent by reflexivity always coincide. A minor notational issue that we must point out is that, in some references, the left-hand side of a rule is required to be a subset of the right-hand side, as in [33] or [38], whereas many others require the leftand right-hand sides of an association rule to be disjoint, such as [29] or the original [2]. Both the rules whose left-hand side is a subset of the right-hand side, and the rules that have disjoint sides, may act as canonical representatives for the rules equivalent to them by reflexivity. We state explicitly one version of this immediate fact for later reference:\nProposition 2.2. If rules X0 \u2192 Y0 and X1 \u2192 Y1 are equivalent by reflexivity, X0\u2229Y0 = \u2205, and X1 \u2229 Y1 = \u2205, then they are the same rule: X0 = X1 and Y0 = Y1.\nIn general, we do allow, along our development, rules where the left-hand side, or a part of it, appears also at the right-hand side, because by doing so we will be able to simplify the mathematical arguments. We will assume here that, at the time of printing out the rules found, that is, for user-oriented output, the items in the left-hand side are removed from the right-hand side; accordingly, we write our rules sometimes as X \u2192 Y \u2212X to recall this convention.\nAlso, many references require the right-hand side of an association rule to be nonempty, or even both sides. However, empty sets can be handled with no difficulty and do give meaningful, albeit uninteresting, rules. A partial rule X \u2192 \u2205 with an empty right-hand side is equivalent by reflexivity to X \u2192 X, or to X \u2192 X \u2032 for any X \u2032 \u2286 X, and all of these rules have always confidence 1. A partial rule with empty left-hand side, as employed, for instance, in [29], actually gives the normalized support of the right-hand side as confidence value:\nFact 2.3. In a dataset D of n transactions, c(\u2205 \u2192 Y ) = s(Y )/n.\nAgain, these sorts of rules could be omitted from user-oriented output, but considering them conceptually valid simplifies the mathematical development. We also resort to the convention that, if s(X) = 0 (which implies that s(XY ) = 0 as well) we redefine the undefined confidence c(X \u2192 Y ) as 1, since the intuitive expression \u201call transactions having X do have also Y \u201d becomes vacuously true. This convention is irrespective of whether Y 6= \u2205.\nThroughout the paper, \u201cimplications\u201d are association rules of confidence 1, whereas \u201cpartial rules\u201d are those having a confidence below 1. When the confidence could be 1 or could be less, we say simply \u201crule\u201d."}, {"heading": "3. Redundancy Notions", "text": "We start our analysis from one of the notions of redundancy defined formally in [1]. The notion is employed also, generally with no formal definition, in several papers on association rules, which subsequently formalize and study just some particular cases of redundancy (e.g. [27], [40]); thus, we have chosen to qualify this redundancy as \u201cstandard\u201d. We propose also a small variation, seemingly less restrictive; we have not found that variant explicitly defined in the literature, but it is quite natural.\nDefinition 3.1. (1) [1] X0 \u2192 Y0 has standard redundancy with respect to X1 \u2192 Y1 if the confidence and support of X0 \u2192 Y0 are larger than or equal to those of X1 \u2192 Y1, in all datasets. (2) X0 \u2192 Y0 has plain redundancy with respect to X1 \u2192 Y1 if the confidence of X0 \u2192 Y0 is larger than or equal to the confidence of X1 \u2192 Y1, in all datasets.\nGenerally, we will be interested in applying these definitions only to rules X0 \u2192 Y0 where Y0 6\u2286 X0 since, otherwise, c(X0 \u2192 Y0) = 1 for all datasets and the rule is trivially redundant. We state and prove separately, for later use, the following new technical claim:\nLemma 3.2. Assume that rule X0 \u2192 Y0 is plainly redundant with respect to rule X1 \u2192 Y1, and that Y0 6\u2286 X0. Then X0Y0 \u2286 X1Y1.\nProof. Assume X0Y0 6\u2286 X1Y1, to argue the contrapositive. Then, we can consider a dataset consisting of one transaction X0 and, say, m transactions X1Y1. No transaction includes X0Y0, therefore c(X0 \u2192 Y0) = 0; however, c(X1 \u2192 Y1) is either 1 or m/(m+ 1), which can be pushed up as much as desired by simply increasing m. Then, plain redundancy does not hold, because it requires c(X0 \u2192 Y0) \u2265 c(X1 \u2192 Y1) to hold for all datasets whereas, for this particular dataset, the inequality fails.\nThe first use of this lemma is to show that plain redundancy is not, actually, weaker than standard redundancy.\nTheorem 3.3. Consider any two rules X0 \u2192 Y0 and X1 \u2192 Y1 where Y0 6\u2286 X0. Then X0 \u2192 Y0 has standard redundancy with respect to X1 \u2192 Y1 if and only if X0 \u2192 Y0 has plain redundancy with respect to X1 \u2192 Y1.\nProof. Standard redundancy clearly implies plain redundancy by definition. Conversely, plain redundancy implies, first, c(X0 \u2192 Y0) \u2265 c(X1 \u2192 Y1) by definition and, further, X0Y0 \u2286 X1Y1 by Lemma 3.2; this implies in turn s(X0 \u2192 Y0) = s(X0Y0) \u2265 s(X1Y1) = s(X1 \u2192 Y1), for all datasets, and standard redundancy holds.\nThe reference [1] also provides two more direct definitions of redundancy:\nDefinition 3.4. (1) if X1 \u2282 X0 and X0Y0 = X1Y1, rule X0 \u2192 Y0 is simply redundant with respect to X1 \u2192 Y1. (2) if X1 \u2286 X0 and X0Y0 \u2282 X1Y1, rule X0 \u2192 Y0 is strictly redundant with respect to X1 \u2192 Y1.\nSimple redundancy in [1] is explained as a potential connection between rules that come from the same frequent set, in our case X0Y0 = X1Y1. The formal definition is not identical to our rendering: in its original statement in [1], rule XZ \u2192 Y is simply redundant with respect to X \u2192 Y Z, provided that Z 6= \u2205. The reason is that, in that reference, rules are always assumed to have disjoint sides, and then both formalizations are clearly equivalent. We do not impose disjointness, so that the natural formalization of their intuitive explanation is as we have just stated in Definition 3.4. The following is very easy to see (and is formally proved in [1]).\nFact 3.5. [1] Both simple and strict redundancies imply standard redundancy.\nNote that, in principle, there could possibly be many other ways of being redundant beyond simple and strict redundancies: we show below, however, that, in essence, this is not the case. We can relate these notions also to the cover operator of [27]:\nDefinition 3.6. [27] Rule X1 \u2192 Y1 covers rule X0 \u2192 Y0 when X1 \u2286 X0 and X0Y0 \u2286 X1Y1.\nHere, again, the original definition, according to which rule X \u2192 Y covers rule XZ \u2192 Y \u2032 if Z \u2286 Y and Y \u2032 \u2286 Y (plus some disjointness and nonemptiness conditions that we omit) is appropriate for the case of disjoint sides. The formalization we give is stated also in [27] as a property that characterizes covering. Both simple and strict redundancies become thus merged into a single definition. We observe as well that the same notion is also employed, without an explicit name, in [38].\nAgain, it should be clear that, in Definition 3.6, the covered rule is indeed plainly redundant: whatever the dataset, changing from X0 \u2192 Y0 to X1 \u2192 Y1 the confidence stays equal or increases since, in the quotient s(XY )s(X) that defines the confidence of a rule X \u2192 Y , the numerator cannot decrease from s(X0Y0) to s(X1Y1), whereas the denominator cannot increase from s(X1) to s(X0). Also, the proposals in Definition 3.4 and 3.6 are clearly equivalent:\nFact 3.7. Rule X1 \u2192 Y1 covers rule X0 \u2192 Y0 if and only if rule X0 \u2192 Y0 is either simply redundant or strictly redundant with respect to X1 \u2192 Y1, or they are equivalent by reflexivity.\nIt turns out that all these notions are, in fact, fully equivalent to plain redundancy; indeed, the following converse statement is a main new contribution of this section:\nTheorem 3.8. Assume rule X0 \u2192 Y0 is plainly redundant with respect to X1 \u2192 Y1, where Y0 6\u2286 X0. Then rule X1 \u2192 Y1 covers rule X0 \u2192 Y0.\nProof. By Lemma 3.2, X0Y0 \u2286 X1Y1. To see the other inclusion, X1 \u2286 X0, assume to the contrary that X1 6\u2286 X0. Then we can consider a dataset in which one transaction consists of X1Y1 and, say, m transactions consist of X0. Since X1 6\u2286 X0, these m transactions do not count towards the supports of X1 or X1Y1, so that the confidence of X1 \u2192 Y1 is 1;\nalso, X0 is not adding to the support of X0Y0 since Y0 6\u2286 X0. As X0Y0 \u2286 X1Y1, exactly one transaction includes X0Y0, so that c(X0 \u2192 Y0) = 1/m, which can be made as low as desired. This would contradict plain redundancy. Hence, plain redundancy implies the two inclusions in the definition of cover.\nCombining the statements so far, we obtain the following characterization:\nCorollary 3.9. Consider any two rules X0 \u2192 Y0 and X1 \u2192 Y1 where Y0 6\u2286 X0. The following are equivalent: (1) X1 \u2286 X0 and X0Y0 \u2286 X1Y1 (that is, rule X1 \u2192 Y1 covers rule X0 \u2192 Y0); (2) rule X0 \u2192 Y0 is either simply redundant or strictly redundant with respect to rule\nX1 \u2192 Y1, or they are equivalent by reflexivity; (3) rule X0 \u2192 Y0 is plainly redundant with respect to rule X1 \u2192 Y1; (4) rule X0 \u2192 Y0 is standard redundant with respect to rule X1 \u2192 Y1.\nMarginally, we note here an additional strength of the proofs given. One could consider attempts at weakening the notion of plain redundancy by allowing for a \u201cmargin\u201d or \u201cslack\u201d, appropriately bounded, but whose value is independent of the dataset, upon comparing confidences. The slack could be additive or multiplicative: conditions such as cD(X0 \u2192 Y0) \u2265 cD(X1 \u2192 Y1)\u2212 \u03b4 or cD(X0 \u2192 Y0) \u2265 \u03b4cD(X1 \u2192 Y1), for all D and for \u03b4 independent of D, could be considered. However, such approaches do not define different redundancy notions: they result in formulations actually equivalent to plain redundancy. This is due to the fact that the proofs in Lemma 3.2 and Theorem 3.8 show that the gap between the confidences of rules that do not exhibit redundancy can be made as large as desired within (0, 1). Likewise, if we fix a confidence threshold \u03b3 \u2208 (0, 1) beforehand and use it to define redundancy as cD(X0 \u2192 Y0) \u2265 \u03b3 \u21d2 cD(X1 \u2192 Y1) \u2265 \u03b3 for all D, again an equivalent notion is obtained, independently of the concrete value of \u03b3; whereas, for \u03b3 = 1, this is, instead, a characterization of Armstrong derivability.\n3.1. Deduction Schemes for Plain Redundancy. From the characterization just given, we extract now a sound and complete deductive calculus. It consists of three inference schemes: right-hand Reduction (rR), where the consequent is diminished; right-hand Augmentation (rA), where the consequent is enlarged; and left-hand Augmentation (`A), where the antecedent is enlarged. As customary in logic calculi, our rendering of each rule means that, if the facts above the line are already derived, we can immediately derive the fact below the line.\n(rR) X\u2192Y, Z\u2286Y X\u2192Z (rA) X\u2192YX\u2192XY (`A) X\u2192Y ZXY\u2192Z\nWe also allow always to state trivial rules:\n(r\u2205) X\u2192\u2205 Clearly, scheme (`A) could be stated equivalently with XY \u2192 Y Z below the line, by (rA):\n(`A\u2032) X\u2192Y ZXY\u2192Y Z In fact, (`A) is exactly the simple redundancy from Definition 3.4 and, in the cases where Y \u2286 X, it provides a way of dealing with one direction of equivalence by reflexivity; the other direction is a simple combination of the other two schemes. The Reduction\nScheme (rR) allows us to \u201close\u201d information from the right-hand side; it corresponds to strict redundancy.\nAs further alternative options, it is easy to see that we could also join (rR) and (rA) into a single scheme:\n(rA\u2032) X\u2192Y, Z\u2286XY\nX\u2192Z but we consider that this option does not really simplify, rather obscures a bit, the proof of our Corollary 3.10 below. Also, we could allow as trivial rules X \u2192 Y whenever Y \u2286 X, which includes the case of Y = \u2205; such rules also follow from the calculus given by combining (r\u2205) with (rA) and (rR).\nThe following can be derived now from Corollary 3.9:\nCorollary 3.10. The calculus given is sound and complete for plain redundancy; that is, rule X0 \u2192 Y0 is plainly redundant with respect to rule X1 \u2192 Y1 if and only if X0 \u2192 Y0 can be derived from X1 \u2192 Y1 using the inference schemes (rR), (rA), and (`A).\nProof. Soundness, that is, all rules derived are plainly redundant, is simple to argue by checking that, in each of the inference schemes, the confidence of the rule below the line is greater than or equal to the confidence of the rule above the line: these facts are actually the known statements that each of equivalence by reflexivity, simple redundancy, and strict redundancy imply plain redundancy. Also, trivial rules with empty right-hand side always hold. To show completeness, assume that rule X0 \u2192 Y0 is plainly redundant with respect to rule X1 \u2192 Y1. If Y0 \u2286 X0, apply (r\u2205) and use (rA) to copy X0 and, if necessary, (rR) to leave just Y0 in the right-hand side. If Y0 6\u2286 X0, by Corollary 3.9, we know that this implies that X1 \u2286 X0 and X0Y0 \u2286 X1Y1. Now, to infer X0 \u2192 Y0 from X1 \u2192 Y1, we chain up applications of our schemes as follows:\nX1 \u2192 Y1 `(rA) X1 \u2192 X1Y1 `(rR) X1 \u2192 X0Y0 `(`A) X0 \u2192 Y0 where the second step makes use of the inclusion X0Y0 \u2286 X1Y1, and the last step makes use of the inclusion X1 \u2286 X0. Here, the standard derivation symbol ` denotes derivability by application of the scheme indicated as a subscript.\nWe note here that [38] proposes a simpler calculus that consists, essentially, of (`A) (called there \u201cweak left augmentation\u201d) and (rR) (called there \u201cdecomposition\u201d). The point is that these two schemes are sufficient to prove completeness of the \u201crepresentative basis\u201d as given in that reference, due to the fact that, in that version, the rules of the representative basis include the left-hand side as part of the right-hand side; but such a calculus is incomplete with respect to plain redundancy because it offers no rule to move items from left to right.\n3.2. Optimum-Size Basis for Plain Redundancy. A basis is a way of providing a shorter list of rules for a given dataset, with no loss of information, in the following sense:\nDefinition 3.11. Given a set of rules R, B \u2286 R is a complete basis if every rule of R is plainly redundant with respect to some rule of B.\nBases are analogous to covers in functional dependencies, and we aim at constructing bases with properties that correspond to minimum size and canonical covers. The solutions for functional dependencies, however, are not valid for partial rules due to the failure of the Armstrong schemes.\nIn all practical applications, R is the set of all the rules \u201cmined from\u201d a given dataset D at a confidence threshold \u03b3 \u2208 (0, 1]. That is, the basis is a set of rules that hold with confidence at least \u03b3 in D, and such that each rule holds with confidence at least \u03b3 in D if and only if it is plainly redundant with respect to some rule of B; equivalently, the rules in R can be inferred from B through the corresponding deductive calculus. All along this paper, such a confidence threshold is denoted \u03b3, and always \u03b3 > 0. We will employ two simple but useful definitions.\nDefinition 3.12. Fix a dataset D. Given itemsets Y and X \u2286 Y , X is a \u03b3-antecedent for Y if c(X \u2192 Y ) \u2265 \u03b3, that is, s(Y ) \u2265 \u03b3s(X).\nNote that we allow X = Y , that is, the set itself as its own \u03b3-antecedent; this is just to simplify the statement of the following rather immediate lemma:\nLemma 3.13. If X is a \u03b3-antecedent for Y and X \u2286 Z \u2286 Y , then X is a \u03b3-antecedent for Z and Z is a \u03b3-antecedent for Y .\nProof. From X \u2286 Z \u2286 Y we have s(X) \u2265 s(Z) \u2265 s(Y ), so that s(Z) \u2265 s(Y ) \u2265 \u03b3s(X) \u2265 \u03b3s(Z). The lemma follows.\nWe make up for proper antecedents as part of the next notion:\nDefinition 3.14. Fix a dataset D. Given itemsets Y and X \u2282 Y (proper subset), X is a valid \u03b3-antecedent for Y if the following holds:\n(1) X is a \u03b3-antecedent of Y , (2) no proper subset of X is a \u03b3-antecedent of Y , and (3) no proper superset of Y has X as a \u03b3-antecedent.\nThe basis we will focus on now is constructed from each Y and each valid antecedent of Y ; we consider that this is the most clear way to define and study it, and we explain below why it is essentially identical to two existing, independent proposals.\nDefinition 3.15. Fix a dataset D and a confidence threshold \u03b3. The representative rules for D at confidence \u03b3 are all the rules X \u2192 Y \u2212 X for all itemsets Y and for all valid \u03b3-antecedents X of Y .\nIn the following, we will say \u201clet X \u2192 Y \u2212X be a representative rule\u201d to mean \u201clet Y be a set having valid \u03b3-antecedents, and let X be one of them\u201d; the parameter \u03b3 > 0 will always be clear from the context. Note that some sets Y may not have valid antecedents, and then they do not generate any representative rules.\nBy the conditions on valid antecedents in representative rules, the following relatively simple but crucial property holds; beyond the use of our Corollary 3.9, the argument follows closely that of related facts in [29]:\nProposition 3.16. Let rule X \u2192 Y \u2212 X be among the representative rules for D at confidence \u03b3. Assume that it is plainly redundant with respect to rule X \u2032 \u2192 Y \u2032, also of confidence at least \u03b3; then, they are equivalent by reflexivity and, in case X \u2032 \u2229 Y \u2032 = \u2205, they are the same rule.\nProof. Let X \u2192 Y \u2212X be a representative rule, so that X \u2282 Y and X is a valid \u03b3-antecedent of Y . By Corollary 3.9, X \u2032 \u2192 Y \u2032 must cover X \u2192 Y \u2212X: X \u2032 \u2286 X \u2286 X(Y \u2212X) = Y \u2286 X \u2032Y \u2032. As c(X \u2032 \u2192 Y \u2032) \u2265 \u03b3, X \u2032 is a \u03b3-antecedent of X \u2032Y \u2032. We first show that Y = X \u2032Y \u2032; assume Y \u2282 X \u2032Y \u2032, and apply Lemma 3.13 to X \u2032 \u2286 X \u2286 Y \u2282 X \u2032Y \u2032: X \u2032 is also a \u03b3-antecedent of Y ,\nand the minimality of valid \u03b3-antecedent X gives us X = X \u2032. X is, thus, a \u03b3-antecedent of X \u2032Y \u2032 which properly includes Y , contradicting the third property of valid antecedents.\nHence, Y = X \u2032Y \u2032, so thatX \u2032 is a \u03b3-antecedent ofX \u2032Y \u2032 = Y ; but againX is a minimal \u03b3antecedent of X \u2032Y \u2032 = Y , so that necessarily X = X \u2032, which, together with X \u2032Y \u2032 = Y = XY , proves equivalence by reflexivity. Under the additional condition X \u2032 \u2229 Y \u2032 = \u2205, both rules coincide as per Proposition 2.2.\nIt easily follows that our definition is equivalent to the definition given in [27], except for a support bound that we will explain later; indeed, we will show in Section 4.5 that all our results carry over when a support bound is additionally enforced.\nCorollary 3.17. Fix a dataset D and a confidence threshold \u03b3. Let X \u2282 Y . The following are equivalent: (1) Rule X \u2192 Y \u2212X is among the representative rules for D at confidence \u03b3; (2) [27] c(X \u2192 Y \u2212X) \u2265 \u03b3 and there does not exist any other rule X \u2032 \u2192 Y \u2032 with X \u2032\u2229Y \u2032 = \u2205,\nof confidence at least \u03b3 in D, that covers X \u2192 Y \u2212X.\nProof. Let rule X \u2192 Y \u2212X be among the representative rules for D at confidence \u03b3, and let rule X \u2032 \u2192 Y \u2032 cover it, while being also of confidence at least \u03b3 and with X \u2032\u2229Y \u2032 = \u2205. Then, by Corollary 3.9 X \u2032 \u2192 Y \u2032 makes X \u2192 Y \u2212X plainly redundant, and by Proposition 3.16 they must coincide. To show the converse, we must see that X \u2192 Y \u2212X is a representative rule under the conditions given. The fact that c(X \u2192 Y \u2212 X) \u2265 \u03b3 gives that X is a \u03b3antecedent of Y , and we must see its validity. Assume that a proper subset X \u2032 \u2282 X is also a \u03b3-antecedent of Y : then the rule X \u2032 \u2192 Y \u2212X \u2032 would be a different rule of confidence at least \u03b3 covering X \u2192 Y \u2212X, which cannot be. Similarly, assume that X is a \u03b3-antecedent of Y \u2032 where Y \u2282 Y \u2032: then the rule X \u2192 Y \u2032 \u2212X would be a different rule of confidence at least \u03b3 covering X \u2192 Y \u2212X, which cannot be either.\nSimilarly, and with the same proviso regarding support, our definition is equivalent to the \u201cessential rules\u201d of [1]. There, the set of minimal \u03b3-antecedents of a given itemset is termed its \u201cboundary\u201d. The following statement is also easy to prove:\nCorollary 3.18. Fix a dataset D and a confidence threshold \u03b3. Let X \u2286 Y . The following are equivalent: (1) Rule X \u2192 Y \u2212X is among the representative rules for D at confidence \u03b3; (2) [1] X is in the boundary of Y but is not in the boundary of any proper superset of Y ;\nthat is, X is a minimal \u03b3-antecedent of Y but is not a minimal \u03b3-antecedent of any itemset strictly containing Y .\nProof. If X \u2192 Y \u2212X is among the representative rules, X must be a minimal \u03b3-antecedent of Y by the conditions of valid antecedents; also, X is not a \u03b3-antecedent at all (and, thus, not a minimal \u03b3-antecedent) of any Y \u2032 properly including Y . Conversely, assume that X is in the boundary of Y but is not in the boundary of any proper superset of Y ; first, X must be a minimal \u03b3-antecedent of Y so that the first two conditions of valid \u03b3-antecedents hold. Assume that X \u2192 Y \u2212X is not among the representative rules; the third property must fail, and X must be a \u03b3-antecedent of some Y \u2032 with Y \u2282 Y \u2032. Our hypotheses tell us that X is not a minimal \u03b3-antecedent of Y \u2032. That is, there is a proper subset X \u2032 \u2282 X that is also a \u03b3-antecedent of Y \u2032. It suffices to apply Lemma 3.13 to X \u2032 \u2282 X \u2286 Y \u2282 Y \u2032 to reach a contradiction, since it implies that X \u2032 is a \u03b3-antecedent of Y and therefore X would not be a minimal \u03b3-antecedent of Y .\nThe representative rules are indeed a basis:\nFact 3.19. ([1], [27]) Fix a dataset D and a confidence threshold \u03b3, and consider the set of representative rules constructed from D; it is a complete basis: (1) all the representative rules hold with confidence at least \u03b3; (2) all the rules of confidence at least \u03b3 in D are plainly redundant with respect to the\nrepresentative rules.\nThe first part follows directly from the use of \u03b3-antecedents as left-hand sides of representative rules. For the second part, also almost immediate, suppose c(X \u2192 Y ) \u2265 \u03b3, and let Z = XY ; since X is now a \u03b3-antecedent of Z, it must contain a minimal \u03b3-antecedent of Z, say X \u2032 \u2286 X. Let Z \u2032 be the largest superset of Z such that X \u2032 is still a \u03b3-antecedent of Z \u2032. Thus, X \u2032 \u2192 Z \u2032 \u2212 X \u2032 is among the representative rules and covers X \u2192 Y . Small examples of the construction of representative rules can be found in the same references; we also provide one below.\nAn analogous fact is proved in [38] through an incomplete deductive calculus consisting of the schemes that we have called (lA) and (rR), and states that every rule of confidence at least \u03b3 can be inferred from the representative rules by application of these two inference schemes. Since representative rules in the formulation of [38] have a right-hand side that includes the left-hand side, this inference process does not need to employ (rA).\nNow we can state and prove the most interesting novel property of this basis, which again follows from our main result in this section, Corollary 3.9. As indicated, representative rules were known to be irredundant with respect to simple and strict redundancy or, equivalently, with respect to covering. But, for standard redundancy, in principle there was actually the possibility that some other basis, constructed in an altogether different form, could have less rules. We can state and prove now that this is not so: there is absolutely no other way of constructing a basis smaller than this one, while preserving completeness with respect to plain redundancy, because it has absolutely minimum size among all complete bases. Therefore, in order to find smaller bases, a notion of redundancy more powerful than plain (or standard) redundancy is unavoidably necessary.\nTheorem 3.20. Fix a dataset D, and let R be the set of rules that hold with confidence \u03b3 in D. Let B\u2032 \u2286 R be an arbitrary basis, complete so that all the rules in R are plainly redundant with respect to B\u2032. Then, B\u2032 must have at least as many rules as the representative rules. Moreover, if the rules in B\u2032 are such that antecedents and consequents are disjoint, then all the representative rules belong to B\u2032.\nProof. By the assumed completeness of B\u2032, each representative rule X \u2192 Y \u2212X must be redundant with respect to some rule X \u2032 \u2192 Y \u2032 \u2208 B\u2032 \u2286 R. By Corollary 3.9, X \u2032 \u2192 Y \u2032 covers X \u2192 Y \u2212X. Then Proposition 3.16 applies: they are equivalent by reflexivity. This means X = X \u2032 and Y = X \u2032Y \u2032, hence X \u2032 \u2192 Y \u2032 uniquely identifies which representative rule it covers, if any; hence, B\u2032 needs, at least, as many rules as the number of representative rules. Moreover, as stated also in Proposition 3.16, if the disjointness condition X \u2032\u2229Y \u2032 = \u2205 holds, then both rules coincide.\nExample 3.21. We consider a small example consisting of 12 transactions, where there are actually only 7 itemsets, but some of them are repeated across several transactions. We can simplify our study as follows: if X is not a closed set for the dataset, that is, if it has some superset X \u2032 \u2283 X with the same support, then clearly it has no valid \u03b3-antecedents (see also\nFact 4.3 below); thus we concentrate on closed sets. Figure 1 shows the example dataset and the corresponding (semi-)lattice of closures, depicted as a Hasse diagram (that is, transitive edges have been removed to clarify the drawing); edges stand for the inclusion relationship.\nFor this example, the implications can be summarized by six rules, namely, AC \u21d2 B, BC \u21d2 A, AD \u21d2 B, BD \u21d2 A, CF \u21d2 D, and DF \u21d2 C, which are also the representative rules at confidence 1. At confidence \u03b3 = 0.75, we find that, first, the left-hand sides of the six implications are still valid \u03b3-antecedents even at this lower confidence, so that the implications still belong to the representative basis. Then, we see that two of the closures, ABC and CD, have additionally one valid \u03b3-antecedent each, whereas AB has two. The following four rules hold: A \u2192 B, B \u2192 A, AB \u2192 C, and D \u2192 C. These four rules, jointly with the six implications indicated, constitute exactly the ten representative rules at confidence 0.75."}, {"heading": "4. Closure-Based Redundancy", "text": "Theorem 3.20 in the previous section tells us that, for plain redundancy, the absolute limit of a basis at any given confidence threshold is reached by the set of representative rules. Several studies, prominently [44], have put forward a different notion of redundancy; namely, they give a separate role to the full-confidence implications, often through their associated closure operator. Along this way, one gets a stronger notion of redundancy and, therefore, a possibility that smaller bases can be constructed.\nIndeed, implications can be summarized better, because they allow for Transitivity and Augmentation to apply in order to find redundancies; moreover, they can be combined in certain forms of transitivity with partial rules: as a simple example, if c(X \u2192 Y ) \u2265 \u03b3 and c(Y \u2192 Z) = 1, that is, if a fraction \u03b3 or more of the support of X has Y and all the transactions containing Y do have Z as well, clearly this implies that c(X \u2192 Z) \u2265 \u03b3. Observe, however, that the directionality is relevant: from c(X \u2192 Y ) = 1 and c(Y \u2192 Z) \u2265 \u03b3 we infer nothing about c(X \u2192 Z), since the high confidence of Y \u2192 Z might be due to a large number of transactions that do not include X.\nWe will need some notation about closures. Given a dataset D, the closure operator associated to D maps each itemset X to the largest itemset X that contains X and has the same support as X in D: s(X) = s(X), and X is as large as possible under this condition. It is known and easy to prove that X exists and is unique. Implications that hold in the dataset correspond to the closure operator ([19], [23], [36], [43], [44]): c(X \u2192 X) = 1, and X is as large as possible under this condition. Equivalently, the closure of itemset X is the intersection of all the transactions that contain X; this is because X \u2286 X implies that all transactions counted for the support of X are counted as well for the support of X, hence, if the support counts coincide they must count exactly the same transactions.\nAlong this section, as in [36], we denote full-confidence implications using the standard logic notation X0 \u21d2 Y0; thus, X0 \u21d2 Y0 if and only if Y0 \u2286 X0.\nA basic fact from the theory of Closure Spaces is that closure operators are characterized by three properties: extensivity (X \u2286 X), idempotency (X = X), and monotonicity (if X \u2286 Y then X \u2286 Y ). As an example of the use of these properties, we note the following simple consequence for later use:\nLemma 4.1. XY \u2286 XY \u2286 X Y \u2286 XY , and XY = XY = X Y = XY = XY .\nWe omit the immediate proof. A set is closed if it coincides with its closure. Usually we speak of the lattice of closed sets (technically it is just a semilattice but it allows for a standard transformation into a lattice [14]). When X = Y we also say that X is a generator of Y ; if the closures of all proper subsets of X are different from Y , we say that X is a minimal generator. Note that some references use the term \u201cgenerator\u201d to mean our \u201cminimal generator\u201d; we prefer to make explicit the minimality condition in the name. In some works, often database-inspired, minimal generators are termed sometimes \u201ckeys\u201d. In other works, often matroid-inspired, they are termed also \u201cfree sets\u201d. Our definition says explicitly that s(X) = s(X). We will make liberal use of this fact, which is easy to check also with other existing alternative definitions of the closure operator, as stated in [36], [44], and others. Several quite good algorithms exist to find the closed sets and their supports (see section 4 of [12]).\nRedundancy based on closures is a natural generalization of equivalence by reflexivity; it works as follows ([44], see also [29] and section 4 in [36]):\nLemma 4.2. Given a dataset and the corresponding closure operator, two partial rules X0 \u2192 Y0 and X1 \u2192 Y1 such that X0 = X1 and X0Y0 = X1Y1 have the same support and the same confidence.\nThe rather immediate reason is that s(X0) = s(X0) = s(X1) = s(X1), and s(X0Y0) = s(X0Y0) = s(X1Y1) = s(X1Y1). Therefore, groups of rules sharing the same closure of the antecedent, and the same closure of the union of antecedent and consequent, give cases of redundancy. On account of these properties, there are some proposals of basis constructions from closed sets in the literature, reviewed below. But the first fact that we must mention to relate the closure operator with our explanations so far is the following:\nFact 4.3. [28] Let X \u2192 Y \u2212X be a representative rule as per Definition 3.15. Then Y is a closed set and X is a minimal generator.\nThe proof is direct from Definitions 3.14 and 3.15, and can be found in [28], [29], [38]. These references employ this property to improve on the earlier algorithms to compute the representative rules, which considered all the frequent sets, by restricting the exploration to\nclosures and minimal generators. Also the authors of [40] do the same, seemingly unaware that the algorithm in [28] already works just with closed itemsets. Fact 4.3 may shed doubts on whether closure-based redundancy actually can lead to smaller bases. We prove that this is sometimes the case, due to the fact that the redundancy notion itself changes, and allows for a form of Transitivity, which we show can take again the form of a deductive calculus. Then, we will be able to refine the notion of valid antecedent of the previous section and provide a basis for which we can prove that it has the smallest possible size among the bases for partial rules, with respect to closure-based completeness. That is, we will reach the limit of closure-based redundancy in the same manner as we did for standard redundancy in the previous section.\n4.1. Characterizing Closure-Based Redundancy. Let B be the set of implications in the dataset D; alternatively, B can be any of the bases already known for implications in a dataset. In our empirical validations below we have used as B the Guigues-Duquenne basis, or GD-basis, that has been proved to be of minimum size [23], [43]. An apparently popular and interesting alternative, that has been rediscovered over and over in different guises, is the so-called iteration-free basis of [43], which coincides with the proposal in [37] and with the exact min-max basis of [36] (also called sometimes generic basis [29]); because of Fact 4.3, it coincides exactly also with the representative rules of confidence 1, that is: implications that are not plainly redundant with any other implication according to Definition 3.1. Also, it coincides with the \u201cclosed-key basis\u201d for frequent sets in [39], which in principle is not intended as a basis for rules, and has a different syntactic sugar, but differs in essence from the iteration-free basis only in the fact that the support of each rule is explicitly recorded together with it.\nClosure-based redundancy takes into account B as follows:\nDefinition 4.4. Let B be a set of implications. Partial rule X0 \u2192 Y0 has closure-based redundancy relative to B with respect to rule X1 \u2192 Y1, denoted B, {X1 \u2192 Y1} |= X0 \u2192 Y0, if any dataset D in which all the rules in B hold with confidence 1 gives cD(X0 \u2192 Y0) \u2265 cD(X1 \u2192 Y1).\nIn some cases, it might happen that the dataset at hand does not satisfy any nontrivial rule with confidence 1; then, this notion will not be able to go beyond plain redundancy. However, it is usual that some full-confidence rules do hold, and, in these cases, as we shall see, closure-based redundancy may give more economical bases. More generally, all our results only depend on the implications reaching indeed full confidence in the dataset; but they are not required to capture all of these: the implications in B (with their consequences according to the Armstrong schemes) could constitute just a part of the full-confidence rules in the dataset. In particular, plain redundancy reappears by choosing B = \u2205, whether the dataset satisfies or not any full-confidence implication.\nWe continue our study by showing a necessary and sufficient condition for closure-based redundancy, along the same lines as the one in the previous section.\nTheorem 4.5. Let B be a set of exact rules, with associated closure operator mapping each itemset Z to its closure Z. Let X0 \u2192 Y0 be a rule not implied by B, that is, where Y0 6\u2286 X0. Then, the following are equivalent: (1) X1 \u2286 X0 and X0Y0 \u2286 X1Y1; (2) B, {X1 \u2192 Y1} |= X0 \u2192 Y0.\nProof. The direct proof is simple: the inclusions given imply that s(X1) \u2265 s(X0) = s(X0) and s(X0Y0) \u2265 s(X1Y1) = s(X1Y1); then c(X0 \u2192 Y0) = s(X0Y0)s(X0) \u2265 s(X1Y1) s(X1)\n= c(X1 \u2192 Y1). Conversely, for Y0 6\u2286 X0, we argue that, if either of X1 \u2286 X0 and X0Y0 \u2286 X1Y1 fails, then there is a dataset where B holds with confidence 1 and X1 \u2192 Y1 holds with high confidence but the confidence of X0 \u2192 Y0 is low.\nWe observe first that, in order to satisfy B, it suffices to make sure that all the transactions in the dataset we are to construct are closed sets according to the closure operator corresponding to B.\nAssume now that X1 6\u2286 X0: then a dataset consisting only of one or more transactions with itemset X0 satisfies (vacuously) X1 \u2192 Y1 with confidence 1 but, given that Y0 6\u2286 X0, leads to confidence zero for X0 \u2192 Y0. It is also possible to argue without resorting to vacuous satisfaction: simply take one transaction consisting of X1Y1 and, in case this transaction satisfies X0 \u2192 Y0, obtain as low a confidence as desired for X0 \u2192 Y0 by adding as many transactions X0 as necessary; these will not change the confidence of X1 \u2192 Y1 since X1 6\u2286 X0.\nThen consider the case where X1 \u2286 X0, whence the other inclusion fails: X0Y0 6\u2286 X1Y1. Consider a dataset of, say, n transactions, where one transaction consists of the itemset X0 and n\u2212 1 transactions consist of the itemset X1Y1. The confidence of X1 \u2192 Y1 is at least n\u22121 n , which can be made as close to 1 as desired by increasing n, whereas the presence of at least one X0 and no transaction at all containing X0Y0 gives confidence zero to X0 \u2192 Y0. Thus, in either case, we see that redundancy does not hold.\n4.2. Deduction Schemes for Closure-Based Redundancy. We provide now a stronger calculus that is sound and complete for this more general case of closure-based redundancy. For clarity, we chose to avoid the closure operator in our deduction schemes, writing instead explicitly each implication.\nOur calculus for closure-based redundancy consists of four inference schemes, each of which reaches a partial rule from premises including a partial rule. Two of the schemes correspond to variants of Augmentation, one for enlarging the antecedent, the other for enlarging the consequent. The other two correspond to composition with an implication, one in the antecedent and one in the consequent: a form of controlled transitivity. Their names (rA), (`A), (rI), and (`I) indicate whether they operate at the right or left-hand side and whether their effect is Augmentation or composition with an Implication.\n(rA) X\u2192Y, X\u21d2Z\nX\u2192Y Z\n(rI) X\u2192Y, Y\u21d2Z X\u2192Z (`A) X\u2192Y ZXY\u2192Z (`I) X\u2192Y, Z\u2286X, Z\u21d2X\nZ\u2192Y Again we allow to state rules with empty right-hand side directly:\n(r\u2205) X\u2192\u2205 Alternatively, we could state trivial rules with a subset of the left-hand side at the righthand side. Note that this opens the door to using (rA) with an empty Y , and this allows us to \u201cdowngrade\u201d an implication into the corresponding partial rule. Again, (`A) could be stated equivalently as (`A\u2032) like in Section 3.1. In fact, the whole connection with the\nsimpler calculus in Section 3.1 should be easy to understand: first, observe that the (`A) rules are identical. Now, if implications are not considered separately, the closure operator trivializes to identity, Z = Z for every Z, and the only cases where we know that X1 \u21d2 Y1 are those where Y1 \u2286 X1; we see that (rI) corresponds, in that case, to (rR), whereas the (rA) schemes only differ on cases of equivalence by reflexivity. Finally, in that case (`I) becomes fully trivial since Z \u21d2 X becomes X \u2286 Z and, together with Z \u2286 X, leads to X = Z: then, the partial rules above and below the line would coincide.\nSimilarly to the plain case, there exists an alternative deduction system, more compact, whose equivalence with our four schemes is rather easy to see. It consists of just two forms of combining a partial rule with an implication:\n(rI \u2032) X\u2192Y, XY\u21d2Z\nX\u2192Z\n(`I \u2032) X\u2192Y, Z\u2286XY, Z\u21d2X Z\u2192Y However, in our opinion, the use of these schemes in our further developments is less intuitive, so we keep working with the four schemes above. In the remainder of this section, we denote as B, {X \u2192 Y } ` X \u2032 \u2192 Y \u2032 the fact that, in the presence of the implications in the set B, rule X \u2032 \u2192 Y \u2032 can be derived from rule X \u2192 Y using zero or more applications of the four deduction schemes; along such a derivation, any rule of B (or derived from B by the Armstrong schemes) can be used whenever an implication of the form X \u21d2 Y is required.\n4.3. Soundness and Completeness. We can characterize the deductive power of this calculus as follows: it is sound and complete with respect to the notion of closure-based redundancy; that is, all the rules it can prove are redundant, and all the redundant rules can be proved:\nTheorem 4.6. Let B consist of implications. Then, B, {X1 \u2192 Y1} ` X0 \u2192 Y0 if and only if rule X0 \u2192 Y0 has closure-based redundancy relative to B with respect to rule X1 \u2192 Y1: B, {X1 \u2192 Y1} |= X0 \u2192 Y0.\nProof. Soundness corresponds to the fact that every rule derived is redundant: it suffices to prove it individually for each scheme; the essentials of some of these arguments are also found in the literature. For (rA), the inclusions XY \u2286 XY Z \u2286 XY prove that the partial rules above and below the line have the same confidence. For (rI), one has XZ \u2286 XY \u2286 XY , thus s(XZ) \u2265 s(XY ) and the confidence of the rule below the line is at least that of the one above, or possibly greater. Scheme (`A) is unchanged from the previous section. Finally, for (`I), we have Z \u2286 X \u2286 Z so that s(Z) = s(X), and ZY \u2286 XY so that s(ZY ) \u2265 s(XY ), and again the confidence of the rule below the line is at least the same as the confidence of the one above.\nTo prove completeness, we must see that all redundant rules can be derived. We assume B, {X1 \u2192 Y1} |= X0 \u2192 Y0 and resort to Theorem 4.5: we know that the inclusions X1 \u2286 X0 and X0Y0 \u2286 X1Y1 must hold. From Lemma 4.1, we have that X0Y0 \u2286 X1Y1.\nNow we can write a derivation in our calculus, taking into account these inclusions, as follows:\nX1 \u2192 Y1 `(rA) X1 \u2192 X1Y1 `(rI) X1 \u2192 X0Y0 `(`A) X0 \u2192 Y0 `(`I) X0 \u2192 Y0 Thus, indeed the redundant rule is derivable, which proves completeness.\n4.4. Optimum-Size Basis for Closure-Based Redundancy. In a similar way as we did for plain redundancy, we study here bases corresponding to closure-based redundancy.\nSince the implications become \u201cfactored out\u201d thanks to the stronger notion of redundancy, we can focus on the partial rules. A formal definition of completeness for a basis is, therefore, as follows:\nDefinition 4.7. Given a set of partial rules R and a set of implications B, closure-based completeness of a set of partial rules B\u2032 \u2286 R holds if every partial rule of R has closure-based redundancy relative to B with respect to some rule of B\u2032.\nAgain R is intended to be the set of all the partial rules \u201cmined from\u201d a given dataset D at a confidence threshold \u03b3 < 1 (recall that always \u03b3 > 0), whereas B is intended to be the subset of rules in R that hold with confidence 1 in D or, rather, a basis for these implications. There exist several proposals for constructing bases while taking into account the implications and their closure operator. We use the same intuitions and modus operandi to add a new proposal which, conceptually, departs only slightly from existing ones. Its main merit is not the conceptual novelty of the basis itself but the mathematical proof that it achieves the minimum possible size for a basis with respect to closure-based redundancy, and is therefore at most as large as any alternative basis and, in many cases, smaller than existing ones.\nOur new basis is constructed as follows. For each closed set Y , we will consider a number of closed sets X properly included in Y as candidates to act as antecedents:\nDefinition 4.8. Fix a dataset D, and consider the closure operator corresponding to the implications that hold in D with confidence 1. For each closed set Y , a closed proper subset X \u2282 Y is a basic \u03b3-antecedent if the following holds: (1) X is a \u03b3-antecedent of Y : s(Y ) \u2265 \u03b3s(X); (2) no proper closed subset of X is a \u03b3-antecedent of Y , and (3) no proper closed superset of Y has X as a \u03b3-antecedent.\nBasic antecedents follow essentially the same pattern as the valid antecedents (Definition 3.14), but restricted to closed sets only, that is, instead of minimal antecedents, we pick just minimal closed antecedents. Then we can use them as before:\nDefinition 4.9. Fix a dataset D and a confidence threshold \u03b3. (1) The basis B?\u03b3 consists of all the rules X \u2192 Y \u2212 X for all closed sets Y and all basic\n\u03b3-antecedents X of Y . (2) A minmax variant of the basis B?\u03b3 is obtained by replacing each left-hand side in B?\u03b3\nby a minimal generator: that is, for a closed set Y , each rule X \u2192 Y \u2212 X becomes X \u2032 \u2192 Y \u2212X for one minimal generator X \u2032 of the (closed) basic \u03b3-antecedent X. (3) A minmin variant of the basis B?\u03b3 is obtained by replacing by a minimal generator both the left-hand and the right-hand sides in B?\u03b3 : for each closed set Y and each basic \u03b3antecedent X of Y , the rule X \u2192 Y \u2212X becomes X \u2032 \u2192 Y \u2032 \u2212X where Y \u2032 is chosen a minimal generator of Y and X \u2032 is chosen a minimal generator of X.\nThe variants are defined only for the purpose of discussing the relationship to previous works along the next few paragraphs; generally, we will use only the first version of B?\u03b3 . Note the following: in a minmax variant, at the time of substituting a generator for the left-hand side closure, in case we consider a rule from B?\u03b3 that has a left-hand side with\nseveral minimal generators, only one of them is to be used. Also, all of X (and not only X \u2032) can be removed from the right-hand side: (rA) can be used to recover it.\nThe basis B?\u03b3 is uniquely determined by the dataset and the confidence threshold, but the variants can be constructed, in general, in several ways, because each closed set in the rule may have several minimal generators, and even several different generators of minimum size. We can see the variants as applications of our deduction schemes. The result of substituting a generator for the left-hand side of a rule is equivalent to the rule itself: in one direction it is exactly scheme (`I), and in the other is a chained application of (rA) to add the closure to the right-hand side and (`A) to put it back in the left-hand side. Substituting a generator for the right-hand side corresponds to scheme (rI) in both directions.\nThe use of generators instead of closed sets in the rules is discussed in several references, such as [36] or [44]. In the style of [36], we would consider a minmax variant, which allows one to show to the user minimal sets of antecedents together with all their nontrivial consequents. In the style of [44], we would consider a minmin variant, thus reducing the total number of symbols if minimum-size generators are used, since we can pick any generator. Each of these known bases incurs a risk of picking more than one minimum generator for the same closure as left-hand sides of rules with the same closure of the right-hand side: this is where they may be (and, in actual cases, have been empirically found to be) larger than B?\u03b3 , because, in a sense, they would keep in the basis all the variants. Facts analogous to Corollaries 3.17 and 3.18 hold as well if the closure condition is added throughout, and provide further alternative definitions of the same basis. We use one of them in our experimental setting, described in Section 4.6. We now see that this set of rules entails exactly the rules that reach the corresponding confidence threshold in the dataset:\nTheorem 4.10. Fix a dataset D and a confidence threshold \u03b3. Let B be any basis for implications that hold with confidence 1 in D. (1) All the rules in B?\u03b3 hold with confidence at least \u03b3. (2) B?\u03b3 is a complete basis for the partial rules under closure-based redundancy.\nProof. All the rules in B?\u03b3 must hold indeed because all the left-hand sides are actually \u03b3antecedents. To prove that all the partial rules that hold are entailed by rules in B?\u03b3 , assume that indeed X \u2192 Y holds with confidence \u03b3, that is, s(XY ) = s(XY ) \u2265 \u03b3s(X); thus X is a \u03b3-antecedent of XY . If Y \u2286 X, then c(X \u2192 Y ) = 1 and the implication will follow from B; we have to discuss only the case where Y 6\u2286 X, which implies that X \u2282 XY . Consider the family of closed sets that include XY and have X as \u03b3-antecedent; it is a nonempty family, since XY fulfills these conditions. Pick Z maximal in that family. Then X \u2282 Z = Z since X \u2286 Z and X \u2282 XY \u2286 Z. Now, X is a \u03b3-antecedent of Z, but not of any strictly larger closed itemset. Also, any subset of X is a proper subset of Z.\nLet X \u2032 \u2286 X be closed, a \u03b3-antecedent of Z, and minimal with respect to these properties; assume that X \u2032 is a \u03b3-antecedent of a closed set Z \u2032 strictly larger than Z. From X \u2032 \u2286 X \u2286 Z \u2282 Z \u2032 and Lemma 3.13, X would be also a \u03b3-antecedent of Z \u2032, which would contradict the maximality of Z. Therefore, X \u2032 cannot be a \u03b3-antecedent of a closed set strictly larger than Z and, together with the facts that define X \u2032, we have that X \u2032 is a basic \u03b3-antecedent of Z whence X \u2032 \u2192 Z \u2212X \u2032 \u2208 B?\u03b3 .\nWe gather the following inequalities: X \u2032 \u2286 X and XY \u2286 Z = Z = X \u2032(Z \u2212X \u2032); this is exactly what we need to infer that B, {X \u2032 \u2192 Z \u2212X \u2032} |= X \u2192 Y from Theorem 4.5.\nNow we can move to the main result of this section: this basis has a minimum number of rules among all bases that are complete for the partial rules, according to closure-based redundancy with respect to B.\nTheorem 4.11. Fix a dataset D, and let R be the set of rules that hold with confidence \u03b3 in D. Let B be a basis for the set of implications in R. Let B\u2032 \u2286 R be an arbitrary basis, having closure-based completeness for R with respect to B. Then, B\u2032 must have at least as many rules as B?\u03b3.\nProof. First, we will prove the following intermediate claim: for each partial rule in B?\u03b3 , say X \u2192 Y \u2212 X, there is in B\u2032 a corresponding partial rule of the form X \u2032 \u2192 Y \u2032 with X \u2032Y \u2032 = Y and X \u2032 = X. We pick any rule X \u2192 Y \u2212 X \u2208 B?\u03b3 , that is, where X is a basic \u03b3-antecedent of Y ; this rule must be redundant, relative to the implications in B, with respect to the new basis B\u2032 under consideration: for some rule X \u2032 \u2192 Y \u2032 \u2208 B\u2032, we have that B, {X \u2032 \u2192 Y \u2032} |= X \u2192 Y \u2212 X which, by Theorem 4.5, is the same as X \u2032 \u2286 X = X and Y \u2286 X \u2032Y \u2032, together with c(X \u2032 \u2192 Y \u2032) \u2265 \u03b3. We consider some support ratios: s(X\n\u2032Y \u2032) s(X) = s(X\u2032Y \u2032) s(X) \u2265 s(X \u2032Y \u2032) s(X\u2032) \u2265 \u03b3, which means that X is a \u03b3-antecedent of X \u2032Y \u2032, a\nclosed set including Y ; by the second condition in the definition of basic \u03b3-antecedent, this cannot be the case unless X \u2032Y \u2032 = Y .\nThen, again, c(X \u2032 \u2192 Y ) = c(X \u2032 \u2192 X \u2032Y \u2032) = c(X \u2032 \u2192 Y \u2032) \u2265 \u03b3, that is, X \u2032 is a \u03b3antecedent of Y , and X \u2032 \u2286 Y = Y is as well; but X \u2032 \u2286 X = X and, by minimality of X as a basic \u03b3-antecedent of Y , it must be that X \u2032 = X.\nNow, to complete the proof of the theorem, we observe that each such rule X \u2032 \u2192 Y \u2032 in B\u2032 determines univocally both closed sets X and Y , so that the same rule in B\u2032 cannot correspond to more than one of the rules in B?\u03b3 . This requires B\u2032, therefore, to have at least as many rules as B?\u03b3 .\nIn applications of B?\u03b3 , one needs, in general, as a basis both B?\u03b3 and a basis for the implications, such as the GD-basis. On the other hand, in many practical cases, implications provide little new knowledge, most often just showing existing (and known) properties of the attributes. If a user is satisfied with the B?\u03b3 basis, and does not ask for a basis for the implications nor the representative rules, then (s)he may get results faster, since in this case the algorithms would not need to compute minimal generators, and just mining closures and their supports (and organizing them via the subset relation) would suffice.\nNote that the joint consideration of the GD-basis and B?\u03b3 incurs the risk of being a larger set of rules than the representative rules, due to the fact that some rules in the GD-basis could be, in fact, plainly redundant (ignoring the closure-related issues) with a representative rule. We have observed empirically that, at high confidence thresholds, the representative rules tend to be a large basis due to the lack of specific minimization of implications, whereas the union of the GD-basis and B?\u03b3 tends to be quite smaller; conversely, at lower confidence levels, the availability of many partial rules increases the chances of covering a large part of the GD-basis, so that the representative rules are a smaller basis than the union of B?\u03b3 plus GD, even if they are more in number than B?\u03b3 . That is: closurebased redundancy may be either stronger or weaker, in terms of the optimum basis sizes, than plain redundancy. Sometimes, B?\u03b3 even fully coincides with the partial representative rules. This is, in fact, illustrated in the following example.\nExample 4.12. We revisit the example in Figure 1. As indicated at the end of Section 3.2, the basis for implications consists of six rules: AC \u21d2 B, AD \u21d2 B, BC \u21d2 A, BD \u21d2 A,\nCF \u21d2 D, and DF \u21d2 C; the iteration-free basis [43] and the Guigues-Duquenne basis [23] coincide here, and these implications are also the representative rules at confidence 1. At confidence \u03b3 = 0.75, these are kept and four representative rules are added: A \u2192 B, B \u2192 A, AB \u2192 C, and D \u2192 C. Since the four left-hand sides are, actually, closed sets, which is not guaranteed in general, the basis B?\u03b3 at this confidence includes exactly these four rules: no other closure is a basic \u03b3-antecedent.\nHowever, if the confidence threshold is lowered to \u03b3 = 0.6, we find seven rules in the B\u22170.6 basis: A \u2192 BC, B \u2192 AC, C \u2192 D, D \u2192 C, CD \u2192 F , and F \u2192 CD, plus the somewhat peculiar \u2205 \u2192 C, since indeed the support of C is above the same threshold; the rules A\u2192 B, B \u2192 A, and AB \u2192 C also hold, but they are redundant with respect to A\u2192 BC or B \u2192 AC: A and B are \u03b3-antecedents of AB but are not basic (by way of being also \u03b3-antecedents of ABC), whereas AB is a \u03b3-antecedent of ABC but is not basic either since it is not minimal.\nAdditionally, the sizes of the rules can be reduced somewhat: A \u2192 C suffices to give A \u2192 BC or indeed A \u2192 ABC since A \u2192 C is equivalent by reflexivity to A \u2192 AC and there is a full-confidence implication AC \u21d2 B in the GD-basis that gives us A \u2192 ABC. This form of reasoning is due to [44], and a similar argument can be made for several of the other rules. Alternatively, there exists the option of omitting those implications that, seen as partial rules, are already covered by a partial rule: in this example, these are AC \u21d2 B and BC \u21d2 A, covered by A \u2192 BC (but not by A \u2192 C, which needs AC \u21d2 B to infer A \u2192 BC); similarly, CF \u21d2 D and CD \u21d2 F are plainly redundant with C \u2192 DF . In fact, it can be readily checked that the seven partial rules in B\u22170.6 plus the two remaining implications in the GD-basis, AD \u21d2 B and BD \u21d2 A, form exactly the representative rules at this confidence threshold.\n4.5. Double-Support Mining. For many real-life datasets, including all the standard benchmarks in the field, the closure space is huge, and reaches easily hundreds of thousands of nodes, or indeed even millions. A standard practice, as explained in the introduction, is to impose a support constraint, that is, to ignore (closed) sets that do not appear often enough. It has been observed also that the rules removed by this constraint are often appropriately so, in that they are less robust and prone to represent statistical artifacts rather than true information [34]. Hence, we discuss briefly what happens to our basis proposal if we work under such a support constraint.\nFor a dataset D and confidence and support thresholds \u03b3 and \u03c4 , respectively, denote by R\u03b3,\u03c4 the set of rules that hold in D with confidence at least \u03b3 and support at least \u03c4 . We may want to construct either of two similar but different sets of rules: we can ask just how to compute the set of rules in B?\u03b3 that reach that support or, more likely, we may wish a minimum-size basis for R\u03b3,\u03c4 . We solve both problems.\nWe first discuss a minimum-size basis for R\u03b3,\u03c4 . Of course, the natural approach is to compute the rule basis exactly as before, but only using closed sets above the support threshold. Indeed this works:\nProposition 4.13. Fix a dataset D. For any fixed confidence threshold \u03b3 and support threshold \u03c4 , the construction of basic \u03b3-antecedents, applied only to closed sets of support at least \u03c4 , provides a minimum-size basis for R\u03b3,\u03c4 . Proof. Consider any rule X \u2192 Y of support at least \u03c4 and confidence at least \u03b3. Then X is a \u03b3-antecedent of XY ; also, s(X) = s(X) \u2265 s(XY ) = s(XY ) \u2265 \u03c4 .\nArguing as in the proof of Theorem 4.10 but restricted to the closures with support at least \u03c4 , we can find a rule X \u2032 \u2192 Y \u2032 \u2212 X \u2032 where both X \u2032 and X \u2032Y \u2032 have support at least \u03c4 , X \u2032 is a basic \u03b3-antecedent of X \u2032Y \u2032, and such that X \u2032 \u2286 X and XY \u2286 X \u2032Y \u2032 so that it covers X \u2192 Y . Minimum size is argued exactly as in the proof of Theorem 4.11: following the same steps, one proves that any complete basis consisting of rules in R\u03b3,\u03c4 must have separate rules to cover each of the rules formed by basic \u03b3-antecedents of closures of support \u03c4 .\nWe are therefore safe if we apply the basis construction for B?\u03b3 to a lattice of frequent closed sets above support \u03c4 , instead of the whole lattice of closed sets. However, this fact does not ensure that the basis obtained coincides with the set of rules in the whole basis B?\u03b3 having support above \u03c4 . There may be rules that are not in B?\u03b3 because a large closure, of low support, prevents some X from being a basic antecedent. If the large closure is pruned by the support constraint, then X may become a basic antecedent. The following result explains with more precision the relationship between the basis B?\u03b3 and the rules of support \u03c4 .\nProposition 4.14. Fix a dataset D, a confidence threshold \u03b3, and a support threshold \u03c4 . Assume that X \u2286 Y and that s(Y ) \u2265 \u03c4 ; then X \u2192 Y \u2212X \u2208 B?\u03b3 if and only if X is a basic \u03b3-antecedent of Y in the set of all closures of support at least \u03b3 \u00d7 \u03c4 .\nThis proposition says that, in order to find B?\u03b3 \u2229R\u03b3,\u03c4 , that is, the set of rules in B?\u03b3 that have support at least \u03c4 , we do not need to compute all the closures and construct the whole of B?\u03b3 ; it suffices to perform the B?\u03b3 construction on the set of closures of support \u03b3 \u00d7 \u03c4 . Of course, in both cases we must then discard the rules of support less than \u03c4 . We call this sort of process double-support mining: given user-defined \u03b3 and \u03c4 , use the product to find all closures of support \u03b3 \u00d7 \u03c4 , compute B?\u03b3 on these closures, and finally prune out the rules with support less than \u03c4 to obtain B?\u03b3 \u2229R\u03b3,\u03c4 , if that is what is desired.\nProof. Consider a pair of closed sets X \u2282 Y with s(X) > s(Y ) \u2265 \u03c4 ; we must discuss whether X is a basic \u03b3-antecedent of Y in two different closure lattices: the one of all the closed sets and the one of frequent closures at support threshold \u03b3 \u00d7 \u03c4 .\nThe properties of being a \u03b3-antecedent and of being minimally so refer to X and Y themselves or to even smaller sets, and are therefore unaffected by the support constraint. We must discuss just the existence of some proper superset of Y having X as a \u03b3-antecedent. In case X is a basic \u03b3-antecedent of Y , no proper superset Z of Y has X as \u03b3-antecedent, whatever the support of Z; therefore, X will be found to be a basic \u03b3-antecedent of Y also in the smaller lattice of frequent closures.\nTo show the converse, it suffices to argue that, for any proper superset Z of Y , if X is a \u03b3-antecedent of Z, then s(Z) \u2265 \u03b3 \u00d7 \u03c4 . Indeed, s(Z) \u2265 \u03b3s(X) \u2265 \u03b3 \u00d7 \u03c4 ; hence, if no such Z is found in the frequent closures lattice at support threshold \u03b3 \u00d7 \u03c4 , no such Z exists at all.\n4.6. Empirical Evaluation. Whereas our interests in this paper are rather foundational, we wish to describe briefly the direct applicability of our results so far. We have chosen an approach that conveniently uses as a black-box a separate closed itemsets miner due to Borgelt [8]. We have implemented a construction of the GD basis using a hypergraph transversal method to construct representative rules of confidence 1 following the guidelines of [37] and subsequently simplifying them to obtain the GD basis as per [4]; and we have\nimplemented a simple algorithm that scans repeatedly the closed sets mined by the separate program and constructs all basic \u03b3-antecedents. A first scan picks up \u03b3-antecedents from the proper closed subsets and filters them for minimality; once all minimal antecedents are there for all closures, a subsequent scan filters out those that are not basic by way of being antecedents of larger sets. Effectively the algorithm does not implement the definition but the immediate extension of the characterization in Corollary 3.18 to the closure-based case.\nA natural alternative consists in preprocessing the lattice as a graph in order to find the predecessors of a node directly; however, in practice, with this alternative, whenever the graph requires too much space, we found that the computation slows down unacceptably, probably due to a worse fit to virtual memory caching. Our implementation gives us answers in just seconds in most cases, on a mid-range Windows XP laptop, taking a few minutes when the closure space reaches a couple dozen thousand itemsets.\nOn the basis of this implementation, we have undertaken some empirical evaluations of the sizes of the basis. We consider that the key point of our contribution is the mathematical proof of absolute size minimality, but, as a mere illustration, we show the figures of some of the cases explored in [44] in Table 1. The datasets and thresholds are set exactly as per that reference; column \u201cS/C\u201d is the confidence and support parameters. Columns \u201cTraditional\u201d (for the number of rules under the standard traditional definition [2]) and \u201cClosure-based\u201d (for the number of rules obtained by the closure-based method proposed in [44]) are taken verbatim from the same reference. We have added the number of rules in the representative basis for implications at 100% confidence \u201cRRImp\u201d, that coincides with the iteration-free basis [43] and other proposals as discussed at the beginning of Subsection 4.1; the size of the GD basis for the same implications (often yielding huge savings); and the number of rules in the B?\u03b3 basis of partial rules, which, in the totality of these cases, did coincide with the representative rules at the corresponding thresholds. As discussed in the end of Section 4.4, representative rules encompass implications but B?\u03b3 must be taken jointly with the GD basis, so we give also the corresponding sum.\nThe confidence chosen in [44] for this comparison, namely, coincident with the support threshold, is, in our opinion, too low to provide a good perspective; at these thresholds, representative rules essentially correspond to support bounds (rules with empty left-hand side).\nTo complement the intuition, we provide the evolution of the sizes of the representative rules and the B?\u03b3 basis for the dataset pumsb-star, downloaded from [17], at the same support thresholds of 40% and 60% used in Table 1, with confidence ranging from 99% to 51%, at 1% granularity. The Guigues-Duquenne bases at these support thresholds consist of 48 and 5 rules respectively. These have been added to the size of B?\u03b3 in Figures 2 and 3. At these confidence tresholds, the traditional notion of association rules gives from 105086 up to 179684 rules at support 40%, and between 268 and 570 rules at support 60%. Note that, in that notion, association rules are restricted, by definition, to singleton consequents; larger numbers would be found if this condition is lifted for a fairer comparison with the bases we study. These figures show the advantage of the closure-based basis over representative rules up to the point where the implications become subsumed by partial representative rules.\nWe want to point out as well one interesting aspect of the figures obtained. The standard settings for association rules lead to a monotonicity property, by which lower confidence thresholds allow for more rules, so that the size of the output grows (sometimes enormously) as the confidence threshold decreases. However, in the case of the B?\u03b3 basis and the representative rules, some datasets exhibit a nonmonotonic evolution: at lesser confidence thresholds, sometimes less rules are obtained. Inspecting the actual rules, we can find the reason: sometimes there are several rules at, say, 90% confidence that become simultaneously redundant due to a single rule of smaller confidence, say 85%, which does not appear at 90% confidence. This may reduce the set of rules upon lowering the confidence threshold."}, {"heading": "5. Towards General Entailment", "text": "We move on towards a further contribution of this paper: we propose a stronger notion of redundancy, as progress towards a complete logical approach, where redundancy would play the role of entailment and a sound and complete deductive calculus is sought. Considering the redundancy notions described so far, the following question naturally arises: beyond all these notions of redundancy that relate one partial rule to another partial rule, possibly in presence of implications, is it indeed possible that a partial rule is entailed jointly by two partial rules, but not by a single one of them? and, if so, when does this happen? We will fully answer this question below.\nThe failures of Transitivity and Augmentation may suggest the intuition of a negative answer: it looks like any combination of two partial rules of confidence at least \u03b3, but with \u03b3 < 1, will require us to multiply confidences, reaching as low as \u03b32 or lower; but this intuition is wrong. We will characterize precisely the case where, at a fixed confidence threshold, a partial rule follows from exactly two partial rules, a case where our previous calculus becomes incomplete; and we will identify one extra deduction scheme that allows us to conclude as consequent a partial rule from two premise partial rules in a sound form. The calculus obtained is complete with respect to entailment from two premise rules. We present the whole setting in terms of closure-based redundancy, but the development carries over for plain redundancy, simply by taking the identity as closure operator.\nA first consideration is that we no longer have a single value of the confidence to compare; therefore, we take a position like the one in most cases of applications of association rule mining in practice, namely: we fix a confidence threshold, and consider only rules whose confidence is above it. An alternative view, further removed from practice, would be\nto require just that the confidence of all our conclusions should be at least the same as the minimum of the confidences of the premises.\nAs an example, consider the following fact (the analogous statement for \u03b3 < 1/2 does not hold, as discussed below):\nProposition 5.1. Let \u03b3 \u2265 1/2. Assume that items A, B, C, D are present in U and that the confidence of the rules A \u2192 BC and A \u2192 BD is above \u03b3 in dataset D. Then, the confidence of the rule ACD \u2192 B in D is also above \u03b3.\nWe do not provide a formal proof of this claim since it is just the simplest particular case of Theorem 5.3 below. We consider the following definition:\nDefinition 5.2. Given a set B of implications, and a set R of partial rules, rule X0 \u2192 Y0 is \u03b3-redundant with respect to them (or also \u03b3-entailed by them), denoted B,R |=\u03b3 X0 \u2192 Y0, if every dataset in which the rules of B have confidence 1 and the confidence of all the rules in R is at least \u03b3 must satisfy as well X0 \u2192 Y0 with confidence at least \u03b3. The entailment is called \u201cproper\u201d if it does not hold for proper subsets of R; otherwise it is \u201cimproper\u201d.\nNote that, in this case, the parameter \u03b3 is necessary to qualify the entailment relation itself. In previous sections we had a mere confidence inequality that did not depend on \u03b3. The main result of this section is now:\nTheorem 5.3. Let B be a set of implications, and let 1/2 \u2264 \u03b3 < 1. Consider three partial rules, X0 \u2192 Y0, X1 \u2192 Y1, and X2 \u2192 Y2. Then, B, {X1 \u2192 Y1, X2 \u2192 Y2} |=\u03b3 X0 \u2192 Y0 if and only if either: (1) Y0 \u2286 X0, or (2) B, {X1 \u2192 Y1} |= X0 \u2192 Y0, or (3) B, {X2 \u2192 Y2} |= X0 \u2192 Y0, or (4) all the following conditions simultaneously hold:\n(i) X1 \u2286 X0 (ii) X2 \u2286 X0 (iii) X1 \u2286 X2Y2 (iv) X2 \u2286 X1Y1 (v) X0 \u2286 X1Y1X2Y2 (vi) Y0 \u2286 X0Y1 (vii) Y0 \u2286 X0Y2\nProof. Let us discuss first the leftwards implication. In case (1), ruleX0 \u2192 Y0 holds trivially. Clearly cases (2) and (3) also give (improper) entailment. For case (4), we must argue that, if all the seven conditions hold, then the entailment relationship also holds. Thus, fix any dataset D where the confidences of the premise rules are at least \u03b3: these assumptions can\nbe written, respectively, s(X1Y1) \u2265 \u03b3s(X1) and s(X2Y2) \u2265 \u03b3s(X2), or equivalently for the corresponding closures.\nWe have to show that the confidence of X0 \u2192 Y0 in D is also at least \u03b3. Consider the following four sets of transactions from D: A = {t \u2208 D \u2223\u2223 t |= X0Y0}\nB = {t \u2208 D \u2223\u2223 t |= X0, t 6|= X0Y0}\nC = {t \u2208 D \u2223\u2223 t |= X1Y1, t 6|= X0}\nD = {t \u2208 D \u2223\u2223 t |= X2Y2, t 6|= X0}\nand let a, b, c, and d be the respective cardinalities. We first argue that all four sets are mutually disjoint. This is easy for most pairs: clearly A and B have incompatible behavior with respect to Y0; and a tuple in either A or B has to satisfy X0, which makes it impossible that that tuple is accounted for in either C or D. The only place where we have to argue a bit more carefully is to see that C and D are disjoint as well: but a tuple t that satisfies both X1Y1 and X2Y2, that is, satisfies their union X1Y1X2Y2, must satisfy every subset of the corresponding closure as well, such as X0, due to condition (v). Hence, C and D are disjoint.\nNow we bound the supports of the involved itemsets as follows: clearly, by definition of A, s(X0Y0) = a. All tuples that satisfy X0 are accounted for either as satisfying Y0 as well, in A, or in B in case they don\u2019t; disjointness then guarantees that s(X0) = a+ b.\nWe see also that s(X1) \u2265 a + b + c + d, because X1 is satisfied by the tuples in C, by definition; by the tuples in A or B, by condition (i); and by the tuples in D, by condition (iii); again disjointness allows us to sum all four cardinalities. Similarly, using instead (ii) and (iv), we obtain s(X2) \u2265 a+ b+ c+ d.\nThe next delicate point is to show an upper bound on s(X1Y1) (and on s(X2Y2) symmetrically). We split all the tuples that satisfy X1Y1 into two sets, those that additionally satisfy X0, and those that don\u2019t. Tuples that satisfy X1Y1 and not X0 are exactly those in C, and there are exactly c many of them. Satisfying X1Y1 and X0 is the same as satisfying X0Y1 by condition (i), and tuples that do it must also satisfy Y0 by condition (vi). Therefore, they satisfy both X0 and Y0, must belong to A, and there can be at most a many of them. That is, s(X1Y1) \u2264 a + c and, symmetrically, resorting to (ii) and (vii), s(X2Y2) \u2264 a+ d.\nThus we can write the following inequations:\na+ c \u2265 s(X1Y1) \u2265 \u03b3s(X1) \u2265 \u03b3(a+ b+ c+ d) a+ d \u2265 s(X2Y2) \u2265 \u03b3s(X2) \u2265 \u03b3(a+ b+ c+ d)\nAdding them up, using \u03b3 \u2265 12 , we get 2a+ c+ d \u2265 2\u03b3(a+ b+ c+ d) = 2\u03b3(a+ b) + 2\u03b3(c+ d) \u2265 2\u03b3(a+ b) + c+ d\nthat is, a \u2265 \u03b3(a+ b), so that\nc(X0 \u2192 Y0) = s(X0Y0)\ns(X0) =\na\na+ b \u2265 \u03b3\nas was to be shown. Now we prove the rightwards direction; the bound \u03b3 \u2265 12 is not necessary for this part. Since all our supports are integers, we can assume that the threshold is a rational number, \u03b3 = mn , so that we can count on n \u2212 m > 0 and 1 \u2264 m \u2264 n \u2212 1. We will argue the\ncontrapositive, assuming that we are in neither of the four cases, and showing that the entailment does not happen, that is, it is possible to construct a counterexample dataset for which all the implications in B hold, and the two premise partial rules have confidence at least \u03b3, whereas the rule in the conclusion has confidence strictly below \u03b3. This requires us to construct a number of counterexamples through a somewhat long case analysis. In all of them, all the tuples will be closed sets with respect to B; this ensures that these implications are satisfied in all the transactions. We therefore assume that case (1) does not happen, that is, Y0 6\u2286 X0; and that cases (2) and (3) do not happen either. Now, Theorem 4.5 tells us that X1 \u2286 X0 implies X0Y0 6\u2286 X1Y1, and that X2 \u2286 X0 implies X0Y0 6\u2286 X2Y2. Along the rest of the proof, we will refer to the properties explained in this paragraph as the \u201cknown facts\u201d.\nThen, assuming that case (4) does not hold either, we have to consider multiple ways for the conditions (i) to (vii) to fail. Failures of (i) and (ii), however, cannot be argued separately, and we discuss them together. Case A. Exactly one of (i) and (ii) fails. By symmetry, renaming X1 \u2192 Y1 into X2 \u2192 Y2 if necessary, we can assume that (i) fails and (ii) holds. Thus, X1 6\u2286 X0 but X2 \u2286 X0. Then, by the known facts, X0Y0 6\u2286 X2Y2. We consider a dataset consisting of one transaction with the itemset X2Y2, mn\u22121 transactions with the set X0X1Y1X2Y2, and n(n\u2212m) transactions with the set X0, for a total of n 2 transactions. Then, the support of X0 is either n 2\u22121 or n2, and the support of X0Y0 is at most mn\u2212 1, for a confidence bounded by mn\u22121n2\u22121 < mn n2 = \u03b3 for the rule X0 \u2192 Y0. However, the premise rules hold: since (i) fails, the support of X1 is at most mn, and the support of X1Y1 is at least mn \u2212 1, for a confidence at least mn\u22121 mn \u2265 m n = \u03b3 for X1 \u2192 Y1; whereas the support of X2 is n\n2, that of X2Y2 is at least nm, and therefore the confidence is at least m/n = \u03b3.\nCase B. This corresponds to both of (i) and (ii) failing. Then, for a dataset consisting only of X0\u2019s, the premise rules hold vacuously whereas X0 \u2192 Y0 fails. We can also avoid arguing through rules holding vacuously by means of a dataset consisting of one transaction X0X1Y1X2Y2 and n transactions X0.\nRemark. For the rest of the cases, we will assume that both of (i) and (ii) hold, since the other situations are already covered. Then, by the known facts, we can freely use the properties X0Y0 6\u2286 X1Y1 and X0Y0 6\u2286 X2Y2. Case C. Assume (iii) fails, X1 6\u2286 X2Y2, and consider a dataset consisting of one transaction X0, n transactions X1Y1, and n\n2 transactions X2Y2. Here, by the known facts, the support of X0Y0 is zero. It suffices to check that the antecedent rules hold. Since (iii) fails, and (i) holds, the support of X1 is exactly n + 1 and the support of X1Y1 is at least n, for a confidence of at least nn+1 > n\u22121 n \u2265 m n = \u03b3; whereas the support of X2 is at most n 2 +n+ 1 (depending on whether (iv) holds) for a confidence of rule X2 \u2192 Y2 of at least n 2\nn2+n+1 which\nis easily seen to be above n\u22121n \u2265 m n = \u03b3.\nThe case where (iv) fails is fully symmetrical and can be argued just interchanging the roles of X1 \u2192 Y1 and X2 \u2192 Y2. Case D. Assume (v) fails. It suffices to consider a dataset with one transaction X0 and n\u22121 transactions X1Y1X2Y2. Using (i) and (ii), for both premises the confidence is n\u22121 n \u2265 \u03b3, the support of X0 is 1, and the support of X0Y0 is zero by the known fact Y0 6\u2286 X0 and the failure of (v).\nCase E. We assume that (vi) fails, but a symmetric argument takes care of the case where (vii) fails. Thus, we have Y0 6\u2286 X0Y1. By treating this case last, we can assume (i), (ii), and (v) hold, and also the known facts that X0Y0 6\u2286 X1Y1 and X0Y0 6\u2286 X2Y2. We consider a dataset with one transaction X0Y1, one transaction X2Y2, m\u2212 1 transactions X1Y1X2Y2, and n\u2212m\u2212 1 transactions X0 (note that this last part may be empty, but n\u2212m\u2212 1 \u2265 0; the total is n transactions). By (v), the support of X0 is at least n\u22121, whereas the support of X0Y0 is at most m \u2212 1, given the available facts. Since m\u22121n\u22121 < \u03b3, rule X0 \u2192 Y0 does not hold. However, the premises hold: all supports are at most n, the total size, and the supports of X1Y1 (using (i)) and X2Y2 are both m.\nThis completes the proof.\nA small point that remains to be clarified is the role of the condition \u03b3 \u2265 1/2. As indicated in the proof of the theorem, that condition is only necessary in one of the two directions. If there is entailment, the conditions enumerated must hold irrespective of the value of \u03b3. In fact, for 0 < \u03b3 < 1/2, proper entailment from a set of two (or more) premises never holds, and \u03b3-entailment in general is characterized as (closure-based) redundancy as per Theorem 4.5 and the corresponding calculus. Indeed:\nTheorem 5.4. Let 0 < \u03b3 < 1/2. Then, B, {X1 \u2192 Y1, X2 \u2192 Y2} |=\u03b3 X0 \u2192 Y0 if and only if either: (1) Y0 \u2286 X0, or (2) B, {X1 \u2192 Y1} |= X0 \u2192 Y0, or (3) B, {X2 \u2192 Y2} |= X0 \u2192 Y0.\nProof. The leftwards proof is already part of Theorem 5.3. For the converse, assume that the three conditions fail: similarly to the previous proof, we have as known facts the following: Y0 6\u2286 X0, X1 \u2286 X0 implies X0Y0 6\u2286 X1Y1 and X2 \u2286 X0 implies X0Y0 6\u2286 X2Y2. We prove that there are datasets giving low confidence to X0 \u2192 Y0 and high confidence to both premise rules. If both X1 6\u2286 X0 and X2 6\u2286 X0 then we consider one transaction X1Y1, one transaction X2Y2, and a large number m of transactions X0 which do not change the confidences of the premises but lead to a confidence of at most 2/m for X0 \u2192 Y0. Also, if X1 6\u2286 X0 but X2 \u2286 X0, where the symmetric case is handled analogously, we are exactly as in Case A in the proof of Theorem 5.3 and argue in exactly the same way.\nThe interesting case is when both X1 \u2286 X0 and X2 \u2286 X0; then both X0Y0 6\u2286 X1Y1 and X0Y0 6\u2286 X2Y2. We fix any integer k \u2265 \u03b31\u22122\u03b3 and use the fact that \u03b3 < 1/2 to ensure that the fraction is positive and that the inequality can be transformed, by solving for \u03b3, into k 2k+1 \u2265 \u03b3 (following these steps for \u03b3 \u2265 1/2 either makes the denominator null or reverses the inequality due to a negative sign). We consider a dataset with one transaction for X0 and k transactions for each of X1Y1 and X2Y2. Even in the worst case that either or both of X1 and X2 show up in all transactions, the confidences of X1 \u2192 Y1 and X2 \u2192 Y2 are at least k2k+1 \u2265 \u03b3, whereas the confidence of X0 \u2192 Y0 is zero.\n5.1. Extending the calculus. We work now towards a rule form, in order to enlarge our calculus with entailment from larger sets of premises. We propose the following additional rule:\n(2A) X1\u2192Y1, X2\u2192Y2, X1Y1\u21d2X2, X2Y2\u21d2X1, X1Y1X2Y2\u21d2Z1, X1Y1Z1\u21d2Z2, X2Y2Z1\u21d2Z2\nX1X2Z1\u2192Z2\nand state the following properties:\nTheorem 5.5. Given a threshold \u03b3 \u2265 1/2 and a set B of implications, (1) this deduction scheme is sound, and (2) together with the deduction schemes in Section 4.2, it gives a calculus complete with\nrespect to all entailments with two partial rules in the antecedent.\nProof. This follows easily from Theorem 5.3, in that it implements the conditions of case (4); soundness is seen by directly checking that the conditions (i) to (vii) in case 4 of Theorem 5.3 hold: let X0 = X1X2Z1 and Y0 = Z2; then, conditions (i) and (ii) hold trivially, and the rest are explicitly required in the form of implications in the premises (notice that X1Y1 \u21d2 X2 implies that X1Y1Z1 \u21d2 Z2 and X1X2Y1Z1 \u21d2 Z2 are equivalent). Completeness is argued by considering any rule X0 \u2192 Y0 entailed by X1 \u2192 Y1 and X2 \u2192 Y2 jointly with respect to confidence threshold \u03b3; if the entailment is improper, apply Theorem 4.6, otherwise just apply this new deduction scheme with Z1 = X0 and Z2 = Y0 to get X0 \u2192 Y0 and apply (`I) to obtain X0 \u2192 Y0. It is easy to see that the scheme is indeed applicable: proper entailment implies that all seven conditions in case (4) hold and, for Z1 = X0, we get from (i) and (ii) that X1X2Z1 = Z1; under this equality, the remaining five conditions provide exactly the premises of the new deduction scheme."}, {"heading": "6. Discussion", "text": "Our main contribution, at a glance, is a study of confidence-bounded association rules in terms of a family of notions of redundancy. We have provided characterizations of several existing redundancy notions; we have described how these previous proposals, once the relationship to the most robust definitions has been clarified, provide a sound and complete deductive calculus for each of them; and we have been able to prove global optimality of an existing basis proposal, for the plain notion of redundancy, and also to improve the constructions of bases for closure-based redundancy, up to global optimality as well.\nMany existing notions of redundancy discuss redundancy of a partial rule only with respect to another single partial rule; in our Section 5, we have moved beyond into the use of two partial rules. For this approach to redundancy, we believe that this last step has been undertaken for the first time here; the only other reference we are aware of, where a consideration is made of several partial rules entailing a partial rule, is the early [33], which used a much more demanding notion of redundancy in which the exact values of the confidence of the rules were both available on the premises and required in the conclusion. In our simpler context, we have shown that the following holds: for 0 < \u03b3 < 1/2, there is no case of proper \u03b3-entailment from two premises; beyond 1/2, there are such cases, and they are fully captured in terms of set inclusion relationships between the itemsets involved. We conjecture that a more general pattern holds.\nMore precisely, we conjecture the following: for values of the confidence parameter \u03b3 6= 0, such that n\u22121n \u2264 \u03b3 < n n+1 (where n \u2265 1), there are partial rules that are properly entailed from n premises, partial rules themselves, but there are no proper entailments from n+ 1 or more premises. That is, intuitively, higher values of the confidence threshold correspond, successively, to the ability of using more and more partial premises. However, the combinatorics to fully characterize the case of two premises are already difficult enough\nfor the current state of the art, and progress towards proving this conjecture requires to build intuition to much further a degree.\nThis may be, in fact, a way towards stronger redundancy notions and always smaller bases of association rules. We wish to be able to establish such more general methods to reach absolutely minimum-size bases with respect to general entailment, possibly depending on the value of the confidence threshold \u03b3 as per our conjecture as just stated.\nWe observe the following: after constructing a basis, be it either the representative rules or the B?\u03b3 family, it is a simple matter to scan it and check for the existence of pairs of rules that generate a third rule in the basis according to Theorem 5.3: then, removing such third rules gives a smaller basis with respect to this more general entailment. However, we must say that some preliminary empirical tests suggest that this sort of entailments from two premises seems to appear in practice very infrequently, so that the check is computationally somewhat expensive compared to the scarce savings it provides for the basis size.\nNow that all our contributions are in place, let us review briefly a point that we made in the Introduction regarding what is expected to be the role of the basis. The statement that association rule mining produces huge outputs, and that this is indeed a problem, not only is acknowledged in many papers but also becomes self-evident to anyone who has looked at the output of any of the association miner implementations freely accessible on the web (say [8] for one). However, we do not agree that it is one problem: to us, it is, in fact, two slightly different problems, and confusing them may lead to controversies that are easier to settle if we understand that different persons may be interested in different problems, even if they are stated similarly. Specifically, let us ask whether a huge output of an association miner is a problem for the user, who needs to receive the output of the mining process in a form that a human can afford to read and understand, or for the software that is to store all these rules, with their supports and confidences. Of course, the answer is \u201cboth\u201d, but the solutions may not coincide.\nIndeed, sophisticated conceptual advances have provided data structures to be computed from the given dataset in such a way that, within reasonable computational resource limits, they are able to give us the support and confidence of any given rule in the given dataset; maybe a good approximation is satisfactory enough, and this may allow us to obtain some efficiency advantages. The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].\nOur approach is, rather, logical in nature, and aimed at the other variant of the problem: what rules are irredundant, in a general sense. From these, redundant rules reaching the thresholds can be found, \u201cjust as rules\u201d. So, we formalize a situation closer to the practitioner\u2019s process, where a confidence threshold \u03b3 is enforced beforehand and the rules with confidence at least \u03b3 are to be discussed; but we do not need to infer from the basis the value of the confidence of each of these other rules, because we can recompute it immediately as a quotient of two supports, found in an additional data structure that we assume kept, such as the closures lattice with the supports of each closed set.\nTherefore, our bases, namely, the already-known representative rules and our new closure-based proposal B?\u03b3 , are rather \u201cuser-oriented\u201d: we know that all rules above the threshold can be obtained from the basis, and we know how to infer them when necessary; thus, we could, conceivably, guide (or be guided by) the user if (s)he wishes to see all the rules that can be derived from one of the rules in the basis; this user-guided exploration of the rules resulting from the mining process is alike to the \u201cdirection-setting rules\u201d of [31],\nwith the difference that their proposal is based on statistical considerations rather than the logic-based approach we have followed.\nThe advantage is that our basis is not required to provide as much information as the bases we have mentioned so far, because the notion of redundancy does not require us to be able to compute the confidence of the redundant rules. This is why we can reach an optimum size, and indeed, compared to [36] or [44], B?\u03b3 differs because these proposals, essentially, pick all minimal generators of each antecedent, which we avoid. The difference is marginal in the conceptual sense; however the figures in practical cases may differ considerably, and the main advantage of our construction is that we can actually prove that there is no better alternative as a basis for the partial rules with respect to closure-based redundancy.\nFurther research may proceed along several questions. We believe that a major breakthrough in intuition is necessary to fully understand entailment among partial rules in its full generality, either as per our conjecture above or against it; variations of our definition may be worth study as well, such as removing the separate confidence parameter and requiring that the conclusion holds with a confidence at least equal to the minimum of the confidences of the premises.\nOther questions are how to extend this approach to the mining of more complex dependencies [41] or of dependencies among structured objects; however, extending the development to sequences, partial orders, and trees, is not fully trivial, because, as demonstrated in [7], there are settings where the combinatorial structures may make redundant certain rules that would not be redundant in a propositional (item-based) framework; additionally, an intriguing question is: what part of all this discussion remains true if implication intensity measures different from confidence ([20], [21]) are used?"}, {"heading": "Acknowledgements", "text": "The author is grateful to his research group at UPC and to the regular seminar attendees; also, for many observations, suggestions, comments, references, and improvements, the author gratefully acknowledges Cristina T\u0302\u0131rna\u0306uca\u0306, Vero\u0301nica Dahl, Tijl de Bie, Jean-Franc\u0327ois Boulicaut, the participants in seminars where the author has presented this work, the reviewers of the conference papers where most of the results of this paper were announced, and the reviewers of the present paper."}], "references": [{"title": "A New Approach to Online Generation of Association Rules", "author": ["C C Aggarwal", "P S Yu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "A Swami: Mining Association Rules between Sets of Items in Very Large Databases", "author": ["R Agrawal", "T Imielinski"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "A I Verkamo: Fast Discovery of Association Rules", "author": ["R Agrawal", "H Mannila", "R Srikant", "H Toivonen"], "venue": "Advances in Knowledge Discovery and Data Mining, U Fayyad et al. (eds.),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Construction and Learnability of Canonical Horn Formulas. Submitted. Preliminary version in ALT\u20192009", "author": ["M Arias", "J L Balc\u00e1zar"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Deduction Schemes for Association Rules", "author": ["J L Balc\u00e1zar"], "venue": "Discovery Science", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Mining Implications from Lattices of Closed Trees", "author": ["J L Balc\u00e1zar", "A Bifet", "A Lozano"], "venue": "Extraction et Gestion des Connaissances", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Efficient Implementations of Apriori and Eclat", "author": ["C Borgelt"], "venue": "Workshop on Frequent Itemset Mining Implementations", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Free-Sets: A Condensed Representation of Boolean Data for the Approximation of Frequency Queries", "author": ["J-F Boulicaut", "A Bykowski", "C Rigotti"], "venue": "Data Min. Knowl. Discov. 7,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Mining All Non-Derivable Frequent Itemsets", "author": ["T Calders", "B Goethals"], "venue": "PKDD", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Boulicaut: A Survey on Condensed Representations for Frequent Sets. Constraint-Based Mining and Inductive Databases", "author": ["T Calders", "C Rigotti", "J-F"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Generating an Informative Cover for Association Rules", "author": ["L Cristofor", "D Simovici"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "A Priestley: Introduction to Lattices and Order", "author": ["H B A Davey"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1990}, {"title": "Pearl: Structure Identification in Relational Data", "author": ["J R Dechter"], "venue": "Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Identifying the Minimal Transversals of a Hypergraph and Related Problems", "author": ["T Eiter", "G Gottlob"], "venue": "SIAM J. Comput", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Understanding the Crucial Differences between Classification and Discovery of Association Rules", "author": ["A Freitas"], "venue": "SIGKDD Explorations,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Statistical Strategies for Pruning All the Uninteresting Association Rules", "author": ["G C Garriga"], "venue": "ECAI", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Interestingness Measures for Data Mining: A Survey", "author": ["L Geng", "H J Hamilton"], "venue": "ACM Computing Surveys", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Mining Non-Derivable Association Rules", "author": ["B Goethals", "J Muhonen", "H Toivonen"], "venue": "SDM", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Famille minimale d\u2019implications informatives r\u00e9sultant d\u2019un tableau de donn\u00e9es binaires", "author": ["J-L Guigues", "V Duquenne"], "venue": "Mathe\u0301matiques et Sciences Humaines", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1986}, {"title": "Discovering All Most Specific Sentences", "author": ["D Gunopulos", "R Khardon", "H Mannila", "S Saluja", "H Toivonen", "RS Sharma"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Reasoning with Models", "author": ["R Khardon", "D Roth"], "venue": "Artificial Intelligence", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Fast Discovery of Representative Association Rules", "author": ["M Kryszkiewicz"], "venue": "RSCTC,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Concise Representations of Association Rules", "author": ["M Kryszkiewicz"], "venue": "Pattern Detection and Discovery", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}, {"title": "Pruning and Summarizing the Discovered Associations", "author": ["B Liu", "W Hsu", "Y Ma"], "venue": "KDD", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1999}, {"title": "Multi-Level Organization and Summarization of the Discovered Rules", "author": ["B Liu", "M Hu", "W Hsu"], "venue": "KDD", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2000}, {"title": "Discovering Predictive Association Rules", "author": ["N Megiddo", "R Srikant"], "venue": "KDD", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1998}, {"title": "Closed Non-Derivable Itemsets", "author": ["J Muhonen", "H Toivonen"], "venue": "PKDD", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2006}, {"title": "Generating a Condensed Representation for Association Rules", "author": ["N Pasquier", "R Taouil", "Y Bastide", "G Stumme", "L Lakhal"], "venue": "Journal of Intelligent Information Systems", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Taylor: Scientific Discovery through Iterative Transformations of Concept Lattices", "author": ["C M J L Pfaltz"], "venue": "Workshop on Discrete Mathematics and Data Mining at SDM", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "The Representative Basis for Association Rules", "author": ["V Phan-Luong"], "venue": "ICDM", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2001}, {"title": "The Closed Keys Base of Frequent Itemsets", "author": ["V Phan-Luong"], "venue": "DaWaK", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2002}, {"title": "Using Closed Itemsets for Discovering Representative Association Rules", "author": ["J Saquer", "J S Deogun"], "venue": "IS- MIS 2000,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1932}, {"title": "Cristofor: Mining Purity Dependencies in Databases", "author": ["D A Simovici", "L D Cristofor"], "venue": "Extraction et Gestion des Connaissances EGC", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2002}, {"title": "Querying Multiple Sets of Discovered Rules", "author": ["A Tuzhilin", "B Liu"], "venue": "KDD", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2002}, {"title": "A Theory of Finite Closure Spaces Based on Implications", "author": ["M Wild"], "venue": "Advances in Mathematics", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1994}, {"title": "Mining Non-Redundant Association Rules", "author": ["M Zaki"], "venue": "Data Mining and Knowledge Discovery", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2004}, {"title": "Theoretical Foundations of Association Rules", "author": ["M Zaki", "M Ogihara"], "venue": "Workshop on research issues in DMKD", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1998}], "referenceMentions": [{"referenceID": 1, "context": "of references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "of references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.", "startOffset": 86, "endOffset": 89}, {"referenceID": 14, "context": "of references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.", "startOffset": 91, "endOffset": 95}, {"referenceID": 27, "context": "of references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.", "startOffset": 97, "endOffset": 101}, {"referenceID": 35, "context": "of references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.", "startOffset": 103, "endOffset": 107}, {"referenceID": 36, "context": "of references, is [12], and additional materials are available in [25]; see also [2], [3], [18], [36], [44], [45], and the references and discussions in their introductory sections.", "startOffset": 109, "endOffset": 113}, {"referenceID": 12, "context": "Exact implications are equivalent to conjunctions of definite Horn clauses: the fact, well-known in logic and knowledge representation, that Horn theories are exactly those closed under bitwise intersection of propositional models leads to a strong connection with Closure Spaces, which are characterized by closure under intersection (see the discussions in [15] or [26]).", "startOffset": 359, "endOffset": 363}, {"referenceID": 20, "context": "Exact implications are equivalent to conjunctions of definite Horn clauses: the fact, well-known in logic and knowledge representation, that Horn theories are exactly those closed under bitwise intersection of propositional models leads to a strong connection with Closure Spaces, which are characterized by closure under intersection (see the discussions in [15] or [26]).", "startOffset": 367, "endOffset": 371}, {"referenceID": 12, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 237, "endOffset": 241}, {"referenceID": 13, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 243, "endOffset": 247}, {"referenceID": 18, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 255, "endOffset": 259}, {"referenceID": 19, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 261, "endOffset": 265}, {"referenceID": 20, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 267, "endOffset": 271}, {"referenceID": 28, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 273, "endOffset": 277}, {"referenceID": 34, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 279, "endOffset": 283}, {"referenceID": 36, "context": "Associated natural notions of minimality (when no implication can be removed), minimum size, and canonicity of a cover or basis do exist; again it is inappropriate to try to give a complete set of references here, but see, for instance, [15], [16], [19], [23], [24], [26], [37], [43], [45], and the references therein.", "startOffset": 285, "endOffset": 289}, {"referenceID": 15, "context": "Many other alternative measures of intensity of implication exist [20], [21]; we keep our focus on confidence because, besides being among the most common ones, it has a natural interpretation for educated users through its correspondence with the observed conditional probability.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "Many other alternative measures of intensity of implication exist [20], [21]; we keep our focus on confidence because, besides being among the most common ones, it has a natural interpretation for educated users through its correspondence with the observed conditional probability.", "startOffset": 72, "endOffset": 76}, {"referenceID": 2, "context": "The idea of restricting the exploration for association rules to frequent itemsets, with respect to a support threshold, gave rise to the most widely discussed and applied algorithm, called Apriori [3], and to an intense research activity.", "startOffset": 198, "endOffset": 201}, {"referenceID": 23, "context": "Therefore, besides the interesting progress in the topic of how to organize and query the rules discovered (see [31], [32], [42]),", "startOffset": 112, "endOffset": 116}, {"referenceID": 24, "context": "Therefore, besides the interesting progress in the topic of how to organize and query the rules discovered (see [31], [32], [42]),", "startOffset": 118, "endOffset": 122}, {"referenceID": 33, "context": "Therefore, besides the interesting progress in the topic of how to organize and query the rules discovered (see [31], [32], [42]),", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "In fact, if we look only for association rules with singletons as consequents (as in some of the analyses in [1], or in the \u201cbasic association rules\u201d of [30], or even in the traditional approach to association rules [2] and the useful apriori implementation of Borgelt available on the web [8]) we are almost certain to lose information.", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "In fact, if we look only for association rules with singletons as consequents (as in some of the analyses in [1], or in the \u201cbasic association rules\u201d of [30], or even in the traditional approach to association rules [2] and the useful apriori implementation of Borgelt available on the web [8]) we are almost certain to lose information.", "startOffset": 216, "endOffset": 219}, {"referenceID": 6, "context": "In fact, if we look only for association rules with singletons as consequents (as in some of the analyses in [1], or in the \u201cbasic association rules\u201d of [30], or even in the traditional approach to association rules [2] and the useful apriori implementation of Borgelt available on the web [8]) we are almost certain to lose information.", "startOffset": 290, "endOffset": 293}, {"referenceID": 0, "context": "On the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]).", "startOffset": 186, "endOffset": 189}, {"referenceID": 10, "context": "On the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]).", "startOffset": 191, "endOffset": 195}, {"referenceID": 27, "context": "On the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 29, "context": "On the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]).", "startOffset": 215, "endOffset": 219}, {"referenceID": 35, "context": "On the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]).", "startOffset": 221, "endOffset": 225}, {"referenceID": 22, "context": "On the semantic side, a number of formalizations of the intuition of redundancy among association rules exist in the literature, often with proposals for defining irredundant bases (see [1], [13], [27], [33], [36], [38], [44], the survey [29], and section 6 of the survey [12]).", "startOffset": 238, "endOffset": 242}, {"referenceID": 0, "context": "syntactic calculus and a formal proof of the fact, also previously unknown, that the existing basis known as the Essential Rules or the Representative Rules ([1], [27], [38]) is of absolutely minimum size.", "startOffset": 158, "endOffset": 161}, {"referenceID": 29, "context": "syntactic calculus and a formal proof of the fact, also previously unknown, that the existing basis known as the Essential Rules or the Representative Rules ([1], [27], [38]) is of absolutely minimum size.", "startOffset": 169, "endOffset": 173}, {"referenceID": 27, "context": "This separation is present in many constructions of bases for association rules [33], [36], [44].", "startOffset": 86, "endOffset": 90}, {"referenceID": 35, "context": "This separation is present in many constructions of bases for association rules [33], [36], [44].", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "3, and 5 have been presented at Discovery Science 2008 [6]; preliminary versions of the remaining results (except those in section 4.", "startOffset": 55, "endOffset": 58}, {"referenceID": 35, "context": "We immediately obtain by standard means (see, for instance, [19] or [44]) a notion of closed itemsets, namely, those that cannot be enlarged while maintaining the same support.", "startOffset": 68, "endOffset": 72}, {"referenceID": 29, "context": "A minor notational issue that we must point out is that, in some references, the left-hand side of a rule is required to be a subset of the right-hand side, as in [33] or [38], whereas many others require the leftand right-hand sides of an association rule to be disjoint, such as [29] or the original [2].", "startOffset": 171, "endOffset": 175}, {"referenceID": 22, "context": "A minor notational issue that we must point out is that, in some references, the left-hand side of a rule is required to be a subset of the right-hand side, as in [33] or [38], whereas many others require the leftand right-hand sides of an association rule to be disjoint, such as [29] or the original [2].", "startOffset": 281, "endOffset": 285}, {"referenceID": 1, "context": "A minor notational issue that we must point out is that, in some references, the left-hand side of a rule is required to be a subset of the right-hand side, as in [33] or [38], whereas many others require the leftand right-hand sides of an association rule to be disjoint, such as [29] or the original [2].", "startOffset": 302, "endOffset": 305}, {"referenceID": 22, "context": "A partial rule with empty left-hand side, as employed, for instance, in [29], actually gives the normalized support of the right-hand side as confidence value:", "startOffset": 72, "endOffset": 76}, {"referenceID": 0, "context": "We start our analysis from one of the notions of redundancy defined formally in [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 31, "context": "[27], [40]); thus, we have chosen to qualify this redundancy as \u201cstandard\u201d.", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "(1) [1] X0 \u2192 Y0 has standard redundancy with respect to X1 \u2192 Y1 if the confidence and support of X0 \u2192 Y0 are larger than or equal to those of X1 \u2192 Y1, in all datasets.", "startOffset": 4, "endOffset": 7}, {"referenceID": 0, "context": "The reference [1] also provides two more direct definitions of redundancy:", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "Simple redundancy in [1] is explained as a potential connection between rules that come from the same frequent set, in our case X0Y0 = X1Y1.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "The formal definition is not identical to our rendering: in its original statement in [1], rule XZ \u2192 Y is simply redundant with respect to X \u2192 Y Z, provided that Z 6= \u2205.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "The following is very easy to see (and is formally proved in [1]).", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "[1] Both simple and strict redundancies imply standard redundancy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 29, "context": "We observe as well that the same notion is also employed, without an explicit name, in [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 29, "context": "We note here that [38] proposes a simpler calculus that consists, essentially, of (`A) (called there \u201cweak left augmentation\u201d) and (rR) (called there \u201cdecomposition\u201d).", "startOffset": 18, "endOffset": 22}, {"referenceID": 22, "context": "9, the argument follows closely that of related facts in [29]:", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "Similarly, and with the same proviso regarding support, our definition is equivalent to the \u201cessential rules\u201d of [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "(1) Rule X \u2192 Y \u2212X is among the representative rules for D at confidence \u03b3; (2) [1] X is in the boundary of Y but is not in the boundary of any proper superset of Y ; that is, X is a minimal \u03b3-antecedent of Y but is not a minimal \u03b3-antecedent of any itemset strictly containing Y .", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "([1], [27]) Fix a dataset D and a confidence threshold \u03b3, and consider the set of representative rules constructed from D; it is a complete basis:", "startOffset": 1, "endOffset": 4}, {"referenceID": 29, "context": "An analogous fact is proved in [38] through an incomplete deductive calculus consisting of the schemes that we have called (lA) and (rR), and states that every rule of confidence at least \u03b3 can be inferred from the representative rules by application of these two inference schemes.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "Since representative rules in the formulation of [38] have a right-hand side that includes the left-hand side, this inference process does not need to employ (rA).", "startOffset": 49, "endOffset": 53}, {"referenceID": 35, "context": "Several studies, prominently [44], have put forward a different notion of redundancy; namely, they give a separate role to the full-confidence implications, often through their associated closure operator.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "Implications that hold in the dataset correspond to the closure operator ([19], [23], [36], [43], [44]): c(X \u2192 X) = 1, and", "startOffset": 80, "endOffset": 84}, {"referenceID": 27, "context": "Implications that hold in the dataset correspond to the closure operator ([19], [23], [36], [43], [44]): c(X \u2192 X) = 1, and", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "Implications that hold in the dataset correspond to the closure operator ([19], [23], [36], [43], [44]): c(X \u2192 X) = 1, and", "startOffset": 92, "endOffset": 96}, {"referenceID": 35, "context": "Implications that hold in the dataset correspond to the closure operator ([19], [23], [36], [43], [44]): c(X \u2192 X) = 1, and", "startOffset": 98, "endOffset": 102}, {"referenceID": 27, "context": "Along this section, as in [36], we denote full-confidence implications using the standard logic notation X0 \u21d2 Y0; thus, X0 \u21d2 Y0 if and only if Y0 \u2286 X0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "a standard transformation into a lattice [14]).", "startOffset": 41, "endOffset": 45}, {"referenceID": 27, "context": "We will make liberal use of this fact, which is easy to check also with other existing alternative definitions of the closure operator, as stated in [36], [44], and others.", "startOffset": 149, "endOffset": 153}, {"referenceID": 35, "context": "We will make liberal use of this fact, which is easy to check also with other existing alternative definitions of the closure operator, as stated in [36], [44], and others.", "startOffset": 155, "endOffset": 159}, {"referenceID": 35, "context": "Redundancy based on closures is a natural generalization of equivalence by reflexivity; it works as follows ([44], see also [29] and section 4 in [36]):", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "Redundancy based on closures is a natural generalization of equivalence by reflexivity; it works as follows ([44], see also [29] and section 4 in [36]):", "startOffset": 124, "endOffset": 128}, {"referenceID": 27, "context": "Redundancy based on closures is a natural generalization of equivalence by reflexivity; it works as follows ([44], see also [29] and section 4 in [36]):", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "[28] Let X \u2192 Y \u2212X be a representative rule as per Definition 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "15, and can be found in [28], [29], [38].", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "15, and can be found in [28], [29], [38].", "startOffset": 30, "endOffset": 34}, {"referenceID": 29, "context": "15, and can be found in [28], [29], [38].", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "Also the authors of [40] do the same, seemingly unaware that the algorithm in [28] already works just with closed itemsets.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "Also the authors of [40] do the same, seemingly unaware that the algorithm in [28] already works just with closed itemsets.", "startOffset": 78, "endOffset": 82}, {"referenceID": 18, "context": "In our empirical validations below we have used as B the Guigues-Duquenne basis, or GD-basis, that has been proved to be of minimum size [23], [43].", "startOffset": 137, "endOffset": 141}, {"referenceID": 34, "context": "In our empirical validations below we have used as B the Guigues-Duquenne basis, or GD-basis, that has been proved to be of minimum size [23], [43].", "startOffset": 143, "endOffset": 147}, {"referenceID": 34, "context": "An apparently popular and interesting alternative, that has been rediscovered over and over in different guises, is the so-called iteration-free basis of [43], which coincides with the proposal in [37] and with the exact min-max basis of [36] (also called sometimes generic basis [29]); because of Fact 4.", "startOffset": 154, "endOffset": 158}, {"referenceID": 28, "context": "An apparently popular and interesting alternative, that has been rediscovered over and over in different guises, is the so-called iteration-free basis of [43], which coincides with the proposal in [37] and with the exact min-max basis of [36] (also called sometimes generic basis [29]); because of Fact 4.", "startOffset": 197, "endOffset": 201}, {"referenceID": 27, "context": "An apparently popular and interesting alternative, that has been rediscovered over and over in different guises, is the so-called iteration-free basis of [43], which coincides with the proposal in [37] and with the exact min-max basis of [36] (also called sometimes generic basis [29]); because of Fact 4.", "startOffset": 238, "endOffset": 242}, {"referenceID": 22, "context": "An apparently popular and interesting alternative, that has been rediscovered over and over in different guises, is the so-called iteration-free basis of [43], which coincides with the proposal in [37] and with the exact min-max basis of [36] (also called sometimes generic basis [29]); because of Fact 4.", "startOffset": 280, "endOffset": 284}, {"referenceID": 30, "context": "Also, it coincides with the \u201cclosed-key basis\u201d for frequent sets in [39], which in principle is not intended as a basis for rules, and has a different syntactic sugar, but differs in essence from the iteration-free basis only in the fact that the support of each rule is explicitly recorded together with it.", "startOffset": 68, "endOffset": 72}, {"referenceID": 27, "context": "The use of generators instead of closed sets in the rules is discussed in several references, such as [36] or [44].", "startOffset": 102, "endOffset": 106}, {"referenceID": 35, "context": "The use of generators instead of closed sets in the rules is discussed in several references, such as [36] or [44].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "In the style of [36], we would consider a minmax variant, which allows one to show to the user minimal sets of antecedents together with all their nontrivial consequents.", "startOffset": 16, "endOffset": 20}, {"referenceID": 35, "context": "In the style of [44], we would consider a minmin variant, thus reducing the total number of symbols if minimum-size generators are used, since we can pick any generator.", "startOffset": 16, "endOffset": 20}, {"referenceID": 34, "context": "CF \u21d2 D, and DF \u21d2 C; the iteration-free basis [43] and the Guigues-Duquenne basis [23] coincide here, and these implications are also the representative rules at confidence 1.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "CF \u21d2 D, and DF \u21d2 C; the iteration-free basis [43] and the Guigues-Duquenne basis [23] coincide here, and these implications are also the representative rules at confidence 1.", "startOffset": 81, "endOffset": 85}, {"referenceID": 35, "context": "This form of reasoning is due to [44], and a similar argument can be made for several of the other rules.", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "It has been observed also that the rules removed by this constraint are often appropriately so, in that they are less robust and prone to represent statistical artifacts rather than true information [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 6, "context": "We have chosen an approach that conveniently uses as a black-box a separate closed itemsets miner due to Borgelt [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 28, "context": "We have implemented a construction of the GD basis using a hypergraph transversal method to construct representative rules of confidence 1 following the guidelines of [37] and subsequently simplifying them to obtain the GD basis as per [4]; and we have", "startOffset": 167, "endOffset": 171}, {"referenceID": 3, "context": "We have implemented a construction of the GD basis using a hypergraph transversal method to construct representative rules of confidence 1 following the guidelines of [37] and subsequently simplifying them to obtain the GD basis as per [4]; and we have", "startOffset": 236, "endOffset": 239}, {"referenceID": 35, "context": "We consider that the key point of our contribution is the mathematical proof of absolute size minimality, but, as a mere illustration, we show the figures of some of the cases explored in [44] in Table 1.", "startOffset": 188, "endOffset": 192}, {"referenceID": 1, "context": "Columns \u201cTraditional\u201d (for the number of rules under the standard traditional definition [2]) and \u201cClosure-based\u201d (for the number of rules obtained by the closure-based method proposed in [44]) are taken verbatim from the same reference.", "startOffset": 89, "endOffset": 92}, {"referenceID": 35, "context": "Columns \u201cTraditional\u201d (for the number of rules under the standard traditional definition [2]) and \u201cClosure-based\u201d (for the number of rules obtained by the closure-based method proposed in [44]) are taken verbatim from the same reference.", "startOffset": 188, "endOffset": 192}, {"referenceID": 34, "context": "We have added the number of rules in the representative basis for implications at 100% confidence \u201cRRImp\u201d, that coincides with the iteration-free basis [43] and other proposals as discussed at the beginning of Subsection 4.", "startOffset": 152, "endOffset": 156}, {"referenceID": 35, "context": "The confidence chosen in [44] for this comparison, namely, coincident with the support threshold, is, in our opinion, too low to provide a good perspective; at these thresholds, representative rules essentially correspond to support bounds (rules with empty left-hand side).", "startOffset": 25, "endOffset": 29}, {"referenceID": 6, "context": "The statement that association rule mining produces huge outputs, and that this is indeed a problem, not only is acknowledged in many papers but also becomes self-evident to anyone who has looked at the output of any of the association miner implementations freely accessible on the web (say [8] for one).", "startOffset": 292, "endOffset": 295}, {"referenceID": 2, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 133, "endOffset": 137}, {"referenceID": 26, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 151, "endOffset": 155}, {"referenceID": 35, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 157, "endOffset": 161}, {"referenceID": 9, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 22, "context": "The set of frequent sets, the set of frequent closures, and many other methods have been proposed for this task; see [3], [9], [10], [13], [33], [35], [36], [44], and the surveys [11] and [29].", "startOffset": 188, "endOffset": 192}, {"referenceID": 23, "context": "Therefore, our bases, namely, the already-known representative rules and our new closure-based proposal B? \u03b3 , are rather \u201cuser-oriented\u201d: we know that all rules above the threshold can be obtained from the basis, and we know how to infer them when necessary; thus, we could, conceivably, guide (or be guided by) the user if (s)he wishes to see all the rules that can be derived from one of the rules in the basis; this user-guided exploration of the rules resulting from the mining process is alike to the \u201cdirection-setting rules\u201d of [31],", "startOffset": 536, "endOffset": 540}, {"referenceID": 27, "context": "This is why we can reach an optimum size, and indeed, compared to [36] or [44], B? \u03b3 differs because these proposals, essentially, pick all minimal generators of each antecedent, which we avoid.", "startOffset": 66, "endOffset": 70}, {"referenceID": 35, "context": "This is why we can reach an optimum size, and indeed, compared to [36] or [44], B? \u03b3 differs because these proposals, essentially, pick all minimal generators of each antecedent, which we avoid.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "Other questions are how to extend this approach to the mining of more complex dependencies [41] or of dependencies among structured objects; however, extending the development to sequences, partial orders, and trees, is not fully trivial, because, as demonstrated in [7], there are settings where the combinatorial structures may make redundant certain rules that would not be redundant in a propositional (item-based) framework; additionally, an intriguing question is: what part of all this discussion remains true if implication intensity measures different from confidence ([20], [21]) are used?", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "Other questions are how to extend this approach to the mining of more complex dependencies [41] or of dependencies among structured objects; however, extending the development to sequences, partial orders, and trees, is not fully trivial, because, as demonstrated in [7], there are settings where the combinatorial structures may make redundant certain rules that would not be redundant in a propositional (item-based) framework; additionally, an intriguing question is: what part of all this discussion remains true if implication intensity measures different from confidence ([20], [21]) are used?", "startOffset": 267, "endOffset": 270}, {"referenceID": 15, "context": "Other questions are how to extend this approach to the mining of more complex dependencies [41] or of dependencies among structured objects; however, extending the development to sequences, partial orders, and trees, is not fully trivial, because, as demonstrated in [7], there are settings where the combinatorial structures may make redundant certain rules that would not be redundant in a propositional (item-based) framework; additionally, an intriguing question is: what part of all this discussion remains true if implication intensity measures different from confidence ([20], [21]) are used?", "startOffset": 578, "endOffset": 582}, {"referenceID": 16, "context": "Other questions are how to extend this approach to the mining of more complex dependencies [41] or of dependencies among structured objects; however, extending the development to sequences, partial orders, and trees, is not fully trivial, because, as demonstrated in [7], there are settings where the combinatorial structures may make redundant certain rules that would not be redundant in a propositional (item-based) framework; additionally, an intriguing question is: what part of all this discussion remains true if implication intensity measures different from confidence ([20], [21]) are used?", "startOffset": 584, "endOffset": 588}], "year": 2010, "abstractText": "Association rules are among the most widely employed data analysis methods in the field of Data Mining. An association rule is a form of partial implication between two sets of binary variables. In the most common approach, association rules are parametrized by a lower bound on their confidence, which is the empirical conditional probability of their consequent given the antecedent, and/or by some other parameter bounds such as \u201csupport\u201d or deviation from independence. We study here notions of redundancy among association rules from a fundamental perspective. We see each transaction in a dataset as an interpretation (or model) in the propositional logic sense, and consider existing notions of redundancy, that is, of logical entailment, among association rules, of the form \u201cany dataset in which this first rule holds must obey also that second rule, therefore the second is redundant\u201d. We discuss several existing alternative definitions of redundancy between association rules and provide new characterizations and relationships among them. We show that the main alternatives we discuss correspond actually to just two variants, which differ in the treatment of full-confidence implications. For each of these two notions of redundancy, we provide a sound and complete deduction calculus, and we show how to construct complete bases (that is, axiomatizations) of absolutely minimum size in terms of the number of rules. We explore finally an approach to redundancy with respect to several association rules, and fully characterize its simplest case of two partial premises.", "creator": "LaTeX with hyperref package"}}}