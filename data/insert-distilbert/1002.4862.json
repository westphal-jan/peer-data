{"id": "1002.4862", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2010", "title": "Less Regret via Online Conditioning", "abstract": "we analyze and evaluate an online gradient downhill descent algorithm with adaptive per - coordinate coordinate adjustment of learning rates. our algorithm can again be efficiently thought of as an online version construct of batch gradient descent with a diagonal preconditioner. this specialized approach furthermore leads to regret bounds that are stronger than those of standard online relational gradient descent for general domain online convex optimization problems. experimentally, we show that our algorithm is competitive with state - of - the - art algorithms for large scale machine learning problems.", "histories": [["v1", "Thu, 25 Feb 2010 20:31:05 GMT  (15kb)", "http://arxiv.org/abs/1002.4862v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["matthew streeter", "h brendan mcmahan"], "accepted": false, "id": "1002.4862"}, "pdf": {"name": "1002.4862.pdf", "metadata": {"source": "CRF", "title": "Less Regret via Online Conditioning", "authors": ["Matthew Streeter"], "emails": ["mstreeter@google.com", "mcmahan@google.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 2.\n48 62\nv1 [\ncs .L\nG ]\n2 5\nFe b\n20 10\nWe analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems."}, {"heading": "1 Introduction", "text": "In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16]. In addition to their simplicity and generality, online algorithms are natural choices for problems where new data is constantly arriving and rapid adaptation is imporant.\nCompared to the study of convex optimization in the batch (offline) setting, the study of online convex optimization is relatively new. In light of this, it is not surprising that performance-improving techniques that are well known and widely used in the batch setting do not yet have online analogues. In particular, convergence rates in the batch setting can often be dramatically improved through the use of preconditioning. Yet, the online convex optimization literature provides no comparable method for improving regret (the online analogue of convergence rates).\nA simple and effective form of preconditioning is to re-parameterize the loss function so that its magnitude is the same in all coordinate directions. Without this modification, a batch algorithm such as gradient descent will tend to take excessively small steps along some axes and to oscillate back and forth along others, slowing convergence. In the online setting, this rescaling cannot be done up front because the loss functions vary over time and are not known in advance. As a result, when existing no-regret algorithms for online convex optimization are applied to machine learning problems, they tend to overfit the data with respect to certain features and underfit with respect to others (we give a concrete example of this behavior in \u00a72).\nWe show that this problem can be overcome in a principled way by using online gradient descent1 with adaptive, per-coordinate learning rates. Our algorithm comes with worst-case regret bounds (see Theorem 3) that are never worse than those of standard online gradient descent, and are much better when the magnitude of the gradients varies greatly across coordinates (this structure is common in largescale problems of practical interest). Extending this approach, we give improved bounds for generalized notions of strong convexity, bounds in terms of the variance of cost functions, and bounds on adaptive regret (regret against a drifting comparator). Experimentally, we show that our algorithm dramatically outperforms standard online gradient descent on real-world problems, and is competitive with state-of-the-art algorithms for online binary classification.\n1When loss functions are drawn IID, as when online gradient descent is applied to a batch learning problem, the term stochastic gradient descent is often used."}, {"heading": "1.1 Background and notation", "text": "In an online convex optimization problem, we are given as input a closed, convex feasible set F . On each round t, we must pick a point xt \u2208 F . We then incur loss ft(xt), where ft is a convex function. At the end of round t, the loss function ft is revealed to us. Our regret at the end of T rounds is the difference between our total loss and that of the best fixed x \u2208 F in hindsight, that is\nRegret \u2261 T \u2211\nt=1\nft(xt)\u2212min x\u2208F\n{\nT \u2211\nt=1\nft(x)\n}\n.\nSequential prediction using a generalized linear model is an important special case of online convex optimization. In this case, each xt \u2208 Rn is a vector of weights, where xt,i is the weight assigned to feature i on round t. On round t, the algorithm makes a prediction pt(xt) = \u2113(xt \u00b7\u03b8t), where \u03b8t \u2208 Rn is a feature vector and \u2113 is a fixed link function (e.g., \u2113(\u03b1) = 11+exp(\u2212\u03b1) for logistic regression, \u2113(\u03b1) = \u03b1 for linear regression). The algorithm then incurs loss that is some function of the prediction pt and the label yt \u2208 R of the example. For example, in logistic regression the loss is ft(x) = yt log pt(x)+(1\u2212yt) log(1\u2212pt(x)), and in least squares linear regression the loss is ft(x) = (yt \u2212 pt(x))2. In both of these examples, it can be shown that ft is a convex function of x.\nWe are particularly interested in online gradient descent and generalizations thereof. Online gradient descent chooses x1 arbitrarily, and thereafter plays\nxt+1 = P (xt \u2212 \u03b7tgt) (1)\nwhere \u03b71, \u03b72, . . . , \u03b7T is a sequence of learning rates, gt \u2208 \u25bdft(xt) is a subgradient of ft(xt), and P (x) = argminy\u2208F {\u2016x\u2212 y\u2016} is the projection operator, where \u2016 \u00b7 \u2016 is the L2 norm. When the learning rates are chosen appropriately, online gradient descent obtains regret O(GD \u221a T ), where D = maxx,y\u2208F {\u2016x\u2212 y\u2016} is the diameter of the feasible set and G = maxt {\u2016gt\u2016} is the maximum norm of the gradients. Thus, as T \u2192 \u221e, the average loss of the points x1, x2, . . . , xT selected by online gradient descent is as good as that of any fixed point x \u2208 F in the feasible set. It is perhaps surprising that this performance guarantee holds for any sequence of loss functions, and in particular that the bounds holds even if the sequence is chosen adversarially."}, {"heading": "2 Motivations", "text": "It is well-known that batch gradient descent performs poorly in the presence of so-called ravines, surfaces that curve more steeply in some directions than in others [15]. In this section we give examples showing that when the slope of the loss function or the size of the feasible set varies widely across coordinates, gradient descent incurs high regret in the online setting. These observations motivate the use of per-coordinate learning rates (which can be thought of as an adaptive diagonal preconditioner)."}, {"heading": "2.1 A motivating application", "text": "Consider the problem of trying to predict the probability that a user will click on an ad when it is shown alongside search results for a particular query, using a generalized linear model. For simplicity, imagine there is only one ad, and we wish to predict its click-through rate on many different queries. On a large search engine, a popular query will occur orders of magnitude more often than a rare query. For queries that occur rarely, it is necessary to use a relatively large learning rate in order for the associated feature weights to move significantly away from zero. But for popular queries, the use of such a large learning rate will cause the feature weights to oscillate wildly, and so the predictions made by the algorithm will be unstable. Thus, gradient descent with a global learning rate cannot simultaneously perform well on common queries and on rare ones. Because rare queries are more numerous than common ones, performing poorly on either category leads to substantial regret."}, {"heading": "2.2 Tradeoffs in one dimension", "text": "We first consider gradient descent in one dimension, with a fixed learning rate \u03b7 (later we generalize to arbitrary non-increasing sequences of learning rates).\nIf \u03b7 is too large, the algorithm may oscillate about the optimal point and thereby incur high regret. As a simple example, suppose the feasible set is [0, D], and the loss function on each round is ft(x) = G|x\u2212 \u01eb|, for some small positive \u01eb. Then \u25bdft(x) = \u2212G if x < \u01eb and \u25bdft(x) = G if x > \u01eb. It is easy to verify that if the algorithm plays x1 = 0 initially, it will play xt = 0 on odd rounds and xt = G\u03b7 on even rounds, assuming \u01eb < G\u03b7 \u2264 D. Thus, after T rounds the algorithm incurs total loss T2G\u01eb + T2G(G\u03b7 \u2212 \u01eb) = T2G2\u03b7. Always playing x = \u01eb would incur zero loss, so the regret is T2G\n2\u03b7. On the other hand, if \u03b7 is too small then xt may stay close to zero long after the data indicates that a larger x would incur smaller loss. For example, suppose ft(x) = \u2212Gx always. Then xt = min {D, (t\u2212 1)G\u03b7}. For the first D2G\u03b7 rounds, xt \u2264 D2 and therefore our per-round regret relative to the comparator x = D is at least GD2 on these rounds. Thus, overall regret is at least GD 2 min { T, D2G\u03b7 } = D 2 4\u03b7 , assuming that D\n2G\u03b7 \u2264 T . Thus, for any choice of \u03b7 there exists a problem where\nmax\n{\nD2 4\u03b7 ,G2\u03b7 T 2\n}\n\u2264 Regret \u2264 D 2\n2\u03b7 +G2\u03b7\nT 2 ,\nwhere the upper bound is adapted from Zinkevich [17]. Thus, by setting \u03b7 = D G \u221a T (which minimizes the upper bound) we minimize worst-case regret up to a constant factor. Note that this choice of \u03b7 satisfies the constraints D2T \u2264 G\u03b7 \u2264 D, as was assumed earlier.\nThe fact that the optimal choice of \u03b7 is proportional to D G\ncaptures a fundamental tradeoff. When the feasible set is large and the gradients are small, we must use a larger learning rate in order to be competitive with points in the far extremes of the feasible set. On the other hand, when the feasible set is small and the gradients are large, we must use a smaller learning rate in order to avoid the possibility of oscillating between the extremes and performing poorly relative to points in the center.\nBecause the relevant values of D and G will in general be different for different coordinates, a gradient descent algorithm that uses the same learning rate for all coordinates is doomed to either underfit on some coordinates or oscillate on others. To handle this, we must use different learning rates for different coordinates. Furthermore, because the magnitude G of the gradients is not known in advance and can change over time, we must incorporate it into our choice of learning rate in an online fashion."}, {"heading": "2.3 A bad example for global learning rates", "text": "We now exhibit a class of online convex optimization problems where the use of a coordinate-independent learning rate forces regret to grow at an asymptotically larger rate than with a per-coordinate learning rate. This result is summarized in the following theorem.\nTheorem 1. There exists a family of online convex optimization problems, parameterized by their lengths (number of rounds T ), where gradient descent with a non-increasing global learning rate incurs regret at least \u2126(T 2 3 ), whereas gradient descent with an appropriate per-coordinate learning rate has regret O( \u221a T ).\nThe \u2126(T 2 3 ) lower bound stated in Theorem 1 does not contradict the previously-stated O(GD \u221a T ) upper\nbound on the regret of online gradient descent, because in this family of problems D = T 1 6 (and G = 1).\nProof of Theorem 1. To prove this theorem, we interleave instances of the two classes of one-dimensional subproblem discussed in \u00a72.2, setting G = 1 and setting the feasible set to [0, 1]. We have one subproblem of the first type, lasting for T0 rounds, followed by C subproblems of the second type, each lasting T1 rounds. Each subproblem is assigned its own coordinate. Formally, the loss function is\nft(xt) =\n{\n|xt,1 \u2212 \u01eb| if t \u2264 T0 \u2212xt,j if t > T0 where j = 1 + \u2308\nT\u2212T0 T1\n\u2309\nOn each round, only one component of the gradient vector is non-zero. Thus, running gradient descent with global learning rate \u03b7 is equivalent to running a separate copy of gradient descent on each subproblem, where each copy uses learning rate \u03b7. Moreover, overall regret is simply the sum of the regret on each subproblem. Thus, by the lower bounds stated \u00a72.2, regret is at least\nT0 2 \u03b7 + C 2 min\n{\nT1, 1\n2\u03b7\n}\n(note that G = D = 1).\nIf we set C = T1 = T 1 3 0 , this expression is \u2126(T 2\n3 ). To see this, first note that if T1 \u2264 12\u03b7 then the second term is already \u2126(T 2 3\n0 ) = \u2126(T 2 3 ) (note that T = T0 + T 2 3\n0 \u2264 2T0). Otherwise, a simple minimization over \u03b7 shows that the sum is \u2126(T 2 3\n0 ). Because regret on the first subproblem is an increasing function of \u03b7, and\nregret on all later subproblems is a decreasing function of \u03b7, the same \u2126(T 2\n3 ) lower bound holds for any non-increasing sequence \u03b71, \u03b72, . . . , \u03b7T of per-round learning rates. Thus, we have proved the first part of the theorem.\nNow consider the alternative of letting the learning rate for each coordinate vary independently. On a one-dimensional subproblem with feasible set [0, 1] and gradients of magnitude at most 1, gradient descent using learning rate 1\u221a\ns on round s of the subproblem obtains regret O(\n\u221a S) on a subproblem of length S\n[17]. Thus, if we ran an independent copy of this algorithm on each coordinate, we would obtain regret O( \u221a T0 + C \u221a T1) = O( \u221a T0) = O( \u221a T ), which completes the proof."}, {"heading": "3 Improved Regret Bounds using Per-Coordinate Learning Rates", "text": "Zinkevich [17] proved bounds on the regret of online gradient descent (which chooses xt according to Equation (1)). Building on his analysis, we improve these bounds by adjusting the learning rates on a percoordinate basis. Specifically, we obtain these bounds by constructing the vector yt by\nyt,i = xt,i \u2212 gt,i\u03b7t,i (2)\nwhere \u03b7t is a vector of learning rates, one for each coordinate. We then play xt = P (yt). We prove bounds for feasible sets defined by axis-aligned constraints, F = \u00d7ni=1[ai, bi]. Many machine learning problems can be solved using feasible sets of this form, as our experiments demonstrate.2"}, {"heading": "3.1 A better global learning rate", "text": "We first give an improved regret bound for gradient descent with a global (coordinate-independent) learning rate. In the next subsection, we make use of this improved bound in order to prove the desired bounds on the regret of gradient descent with a per-coordinate learning rate.\nZinkevich [17] showed that if we run gradient descent with a non-increasing sequence \u03b71, \u03b72, . . . , \u03b7T of learning rates, regret is bounded by\nB(\u03b71, \u03b72, . . . , \u03b7T ) = D 2 1\n2\u03b7T +\n1\n2\nT \u2211\nt=1\n\u2016gt\u20162\u03b7t. (3)\nTo guard against the worst case, it is natural to choose our sequence of learning rates so as to minimize this bound. Doing so is problematic, however, because in the online setting the gradients g1, g2, . . . , gT are not known in advance. Perhaps surprisingly, we can come within a factor of \u221a 2 of the optimal bound even without having this information up front, as the following theorem shows.\n2Our techniques can be extended to arbitrary feasible sets using a somewhat different algorithm, but the proofs are signicantly more technical [14].\nTheorem 2. Setting \u03b7t = D\u221a\n2 \u2211\nt s=1 \u2016gs\u20162 yields regret D\n\u221a\n2 \u2211T t=1 \u2016gt\u20162 = \u221a 2\u00b7Rmin, where Rmin = min\u03b71,\u03b72,...,\u03b7T : \u03b71\u2265\u03b72\u2265...\u2265\u03b7T {\nProof. Plugging the formula for \u03b7t into (3), and then using Lemma 1 (below), we see that regret is bounded by\n1 2 D\n\n\n\u221a \u221a \u221a \u221a2 T \u2211\nt=1\n\u2016gt\u20162 + T \u2211\nt=1\n\u2016gt\u20162 \u221a\n2 \u2211t s=1 \u2016gs\u20162\n\n\n\u2264 D\n\u221a \u221a \u221a \u221a2 T \u2211\nt=1\n\u2016gt\u20162 .\nWe now compute Rmin. First, note that if \u03b7t > \u03b7t+1 for some t then we could reduce the second term in B({\u03b7t}) by making \u03b7t smaller. Because the sequence is constrained to be non-increasing, it follows that the bound is minimized using a constant learning rate \u03b7. A simple minimization then shows that it is optimal\nto set \u03b7 = D\u221a\u2211 T t=1 \u2016gt\u20162 . which gives regret D\n\u221a\n\u2211T t=1 \u2016gt\u20162 = Rmin.\nA related result appears in [1], giving improved bounds in the case of strongly convex functions but worse constants than ours in the case of linear functions.\nLemma 1. For any non-negative real numbers x1, x2, . . . , xn,\nn \u2211\ni=1\nxi \u221a\n\u2211i j=1 xj\n\u2264 2\n\u221a \u221a \u221a \u221a n \u2211\ni=1\nxi .\nProof. The lemma is clearly true for n = 1. Fix some n, and assume the lemma holds for n\u2212 1. Thus,\nn \u2211\ni=1\nxi \u221a\n\u2211i j=1 xj\n\u2264 2\n\u221a \u221a \u221a \u221a n\u22121 \u2211\ni=1\nxi + xn\n\u221a \u2211n\ni=1 xi\n= 2 \u221a Z \u2212 x+ x\u221a\nZ\nwhere we define Z = \u2211n\ni=1 xi and x = xn. The derivative of the right hand side with respect to x is\u22121\u221a Z\u2212x + 1\u221a Z , which is negative for x > 0. Thus, subject to the constraint x \u2265 0, the right hand side is\nmaximized at x = 0, and is therefore at most 2 \u221a Z."}, {"heading": "3.2 A per-coordinate learning rate", "text": "We can improve the above bound by running, for each coordinate, a separate copy of gradient descent that uses the learning rate given in the previous section (see Algorithm 1). Specifically, we use the update of Equation (2) with \u03b7t,i =\nDi\u221a\u2211 t s=1 g2 s,i , where Di = bi\u2212ai is the diameter of the feasible set along coordinate i. The following theorem makes three important points about the performance of Algorithm 1: (i), its regret is bounded by a sum of per-coordinate bounds, each of the same form as (3); (ii) the algorithm\u2019s choice of \u03b7t,i gives a regret bound that is only a factor of \u221a 2 worse than if the bound had been optimized knowing g1, g2, . . . , gT in advance; and, (iii), the regret bound of Algorithm 1 is never worse than the bound for global learning rates stated in Theorem 2. Futhermore, as illustrated in Theorem 1, the per-coordinate bound can be better by an arbitrarily large factor if the magnitude of the gradients varies widely across coordinates.\nAlgorithm 1 Per-coordinate gradient descent\nInput: feasible set F = \u00d7ni=1[ai, bi] Initialize x1 = 0 and Di = bi \u2212 ai. for t = 1 to T do Play the point xt. Receive loss function ft, set gt = \u25bdft(xt). Let yt+1 be a vector whose i\nth component is yt+1,i = xt,i \u2212 \u03b7t,igt,i, where \u03b7t,i = Di\u221a\u2211t s=1 g2 s,i .\nSet xt+1 = P (yt+1). end for\nTheorem 3. Let F = \u00d7ni=1[ai, bi]. Then, Algorithm 1 has regret bounded by \u2211n i=1 Bi({\u03b7t,i}), where\nBi({\u03b7t,i}) \u2261 D2i 1\n2\u03b7T,i +\n1\n2\nT \u2211\nt=1\ng2t,i\u03b7t,i .\nSetting \u03b7t,i = Di\u221a\u2211 t s=1 g2 s,i , the bound becomes\nn \u2211\ni=1\nDi\n\u221a \u221a \u221a \u221a2 T \u2211\nt=1\ng2t,i = \u221a 2 n \u2211\ni=1\nRimin (4)\nwhere Rimin = min{\u03b7t,i}:\u03b71,i\u2265\u03b72,i\u2265...\u2265\u03b7T,i {Bi({\u03b7t,i})}. This is a stronger guarantee than Theorem 2, in that\nn \u2211\ni=1\nDi\n\u221a \u221a \u221a \u221a2 T \u2211\nt=1\ng2t,i \u2264 D\n\u221a \u221a \u221a \u221a2 T \u2211\nt=1\n\u2016gt\u20162 (5)\nwhere D = \u221a \u2211n\ni=1 D 2 i is the diameter of the set F .\nProof. Zinkevich[17] showed that, so long as our algorithm only makes use of \u25bdft(xt), we may assume without loss of generality that ft is linear, and therefore ft(x) = gt \u00b7 x for all x \u2208 F . If F is a hypercube, then the projection operator P (x) simply projects each coordinate xi indepdently onto the interval [ai, bi]. Thus, in this special case, we can think of each coordinate i as solving a separate online convex optimization problem where the loss function on round t is gt,i \u00b7 x. Thus, Equation (3) implies that for each i,\nT \u2211\nt=1\ngt,ixt,i \u2212 min y\u2208[ai,bi]\n{\nT \u2211\nt=1\ngt,iy\n}\n\u2264 Bi({\u03b7t,i}) .\nSumming this bound over all i, we get the regret bound\nT \u2211\nt=1\ngt \u00b7 xt \u2212min x\u2208F\n{\nT \u2211\nt=1\ngt \u00b7 x } \u2264 n \u2211\ni=1\nBi({\u03b7t,i}). (6)\nApplying Theorem 2 to each one-dimensional problem, we get Bi({\u03b7t,i}) = Di \u221a 2 \u2211T t=1 g 2 t,i = \u221a 2 \u00b7Rimin \u2200i.\nTo prove inequality (5), let ~D \u2208 Rn be a vector whose ith component is Di, and let ~g \u2208 Rn be a vector whose ith component is \u221a 2 \u2211T\nt=1 g 2 t,i, so the left-hand side of (5) can be written as ~D \u00b7 ~g. Then, using the Cauchy-Schwarz inequality,\n~D \u00b7 ~g \u2264 \u2016 ~D\u2016 \u00b7 \u2016~g\u2016 =\n\u221a \u221a \u221a \u221a n \u2211\ni=1\nD2i\n\u221a \u221a \u221a \u221a2 n \u2211\ni=1\nT \u2211\nt=1\ng2t,i .\nThe right hand side simplifies to D \u221a 2 \u2211T\nt=1 \u2016gt\u20162."}, {"heading": "4 Additional Improved Regret Bounds", "text": "The approach of bounding overall regret in terms of the sum of regret on a set of one-dimensional problems can be used to obtain additional regret bounds that improve over those of previous work, in the special case where the feasible set is a hypercube. The key observation is captured in the following lemma.\nLemma 2. Consider an online optimization problem with feasible set F = \u00d7ni=1[ai, bi] and loss functions f1, f2, . . . , fT . For each t, let \u2113t(x) = \u2211n i=1 \u2113t,i(xi) be a lower bound on ft (i.e., ft(x) \u2265 \u2113t(x) for all x \u2208 F ). Further suppose that ft(xt) = \u2113t(xt) for all t, where {xt} is the sequence of points played by an online algorithm. Consider the composite online algorithm formed by running a 1-dimensional algorithm independently for each coordinate i on feasible set [ai, bi] \u2286 Rn, with loss function \u2113t,i on round t. Let\nR = T \u2211\nt=1\nft(xt)\u2212min x\u2208F\n{\nT \u2211\nt=1\nft(x)\n}\nbe the total regret of the composite algorithm, and let\nRi = T \u2211\nt=1\n\u2113t,i(xt,i)\u2212 min xi\u2208[ai,bi]\n{\nT \u2211\nt=1\n\u2113t,i(xi)\n}\nbe the regret incurred by the algorithm responsible for choosing the ith coordinate. Then R \u2264 \u2211ni=1 Ri.\nProof. Because ft(x) \u2265 \u2113t(x) \u2200x, and ft(xt) = \u2113t(xt),\nR \u2264 T \u2211\nt=1\n\u2113t(xt)\u2212min x\u2208F\n{\nT \u2211\nt=1\n\u2113t(x)\n}\n=\nT \u2211\nt=1\n\u2113t(xt)\u2212 n \u2211\ni=1\nmin xi\u2208[ai,bi]\n{\nT \u2211\nt=1\n\u2113t,i(xi)\n}\n=\nn \u2211\ni=1\nRi .\nImportantly, for arbitrary convex functions, we can always construct such independent lower bounds by choosing \u2113t(x) = ft(x) +\u25bdf(xt)(x\u2212 xt), as long as we add a \u201cbias\u201d coordinate where ai = bi = 1. A similar observation was originally used by Zinkevich [17] to show that any algorithm for online linear optimization can be used for online convex optimization. We used this fact in the proof of Theorem 3, where we only analyzed the linear case.\nThis simple lemma has powerful ramifications. We now discuss several improved guarantees that can be obtained by applying it to known online algorithms. For simplicity, when stating these bounds we assume that the feasible set is F = [0, 1]n and that the gradients of the loss functions are componentwise upper bounded by 1 (that is, |(\u25bdft(xt))i| \u2264 1 for all t and i)."}, {"heading": "4.1 More general notions of strong convexity", "text": "A function f is H-strongly convex if, for all x, y \u2208 F , it holds that f(y) \u2265 f(x)+\u25bdf(x) \u00b7 (y\u2212x)+ H2 \u2016y\u2212x\u20162. Strongly convex functions arise, for example, when solving learning problems subject to L2 regularization.\nBartlett et al. [1] give an online convex optimization algorithm whose regret is\nO\n( n \u00b7min {\u221a T , 1\nH logT\n})\nwhere H is the largest constant such that each ft is H-strongly convex. We can generalize the concept of strong convexity as follows. We say that f is strongly convex with respect to the vector ~H if, for all\nx, y \u2208 F , f(y) \u2265 f(x) + \u25bdf(x) \u00b7 (y \u2212 x) +\u2211ni=1 ~Hi 2 (yi \u2212 xi)2. Suppose we run the algorithm of Bartlett et al. independently for each coordinate, feeding back \u2113t,i(yi) = 1 n ft(xt) +\u25bdft(xt)i \u00b7 (yi \u2212 xt,i) + ~Hi2 (yi \u2212 xt,i)2 to the algorithm responsible for choosing coordinate i (we can always choose ~Hi \u2265 H). Applying Lemma 2, we obtain a regret bound\nO\n(\nn \u2211\ni=1\nmin {\u221a T , 1\n~Hi logT\n}\n)\n.\nThis bound is never worse than the previous one, and is better if the degree of strong convexity differs substantially across different coordinates (e.g., if using different L2 regularization parameters for different classes of features)."}, {"heading": "4.2 Tighter bounds in terms of variance", "text": "Hazan and Kale [9] give a bound on gradient descent\u2019s regret in terms of the variance of the sequence of gradients. Specifically, their algorithm has regret O( \u221a nV ), where V =\n\u2211T t=1 \u2016gt \u2212 \u00b5\u20162 and \u00b5 = 1T \u2211T t=1 gt,\nwhere gt = \u25bdft(xt). By running a separate copy of their algorithm on each coordinate, we can instead obtain a bound of O( \u2211n\ni=1\n\u221a Vi), where Vi = \u2211T t=1(gt,i \u2212 \u00b5i)2.\nTo compare the bounds, let ~v \u2208 Rn be a vector whose ith component is \u221a Vi, and let ~1 \u2208 Rn be a vector\nwhose components are all 1. Note that \u2016~v\u2016 = \u221a \u2211n i=1 Vi = \u221a V . Using the Cauchy-Schwarz inequality,\nn \u2211\ni=1\n\u221a Vi = ~1 \u00b7 ~v \u2264 \u2016~1\u2016 \u00b7 \u2016~v\u2016 = \u221a nV .\nThus, the bound obtained by running separate copies of the algorithm for each coordinate is never worse than the original bound, and is substantially better when the variance Vi varies greatly across coordinates."}, {"heading": "4.3 Adaptive regret", "text": "One weakness of standard regret bounds like those stated so far is that they bound performance only in terms of the static optimal solution over all T rounds. In a non-stationary environment, it is desirable to obtain stronger guarantees. For example, suppose the feasible set is [0, 1], ft(x) = x for the first T 2 rounds and ft(x) = \u2212x thereafter. Then an algorithm that plays xt = 0 for all t has 0 regret, yet its loss on the final T2 rounds is T2 worse than if it had played the point x = 1 for those rounds. Indeed, standard regret-minimizing algorithms fail to adapt in simple examples such as this.\nHazan and Seshadhri [10] define adaptive regret as the maximum, over all intervals [T0, T1], of the regret \u2211T1\nt=T0 ft(xt) \u2212 minx\u2208F\n{\n\u2211T1 t=T0 ft(x) } incurred over that interval. For H-strongly convex functions, their\nalgorithm achieves adaptive regret O ( 1 H log2 T )\n. By running an independent copy of their algorithm on each coordinate, we can obtain the following\nguarantee. Consider an arbitrary sequence Z = \u3008z1, z2, . . . , zT \u3009 of points in F , and let RZ = \u2211T t=1 ft(xt)\u2212 ft(zt) be the regret relative to that sequence. Holding H constant for simplicity, the adaptive regret bound just stated implies that the algorithm of Hazan and Seshadhri [10] obtains RZ = O((N + 1) log\n2 T ), where N is the number of values of t for which zt 6= zt+1 (this follows by summing adaptive regret over the N + 1 intervals where zt is constant). Using separate copies for each coordinate, we instead obtain\nRZ = O\n(\nn \u2211\ni=1\n(Ni + 1) log 2 T\n)\nwhere Ni is the number of values of t where zt,i 6= zt+1,i. This bound is never worse than the previous one, and is better when some coordinates of the vectors in Z change more frequently than others.\nThis provides an improved performance guarantee when the environment is stationary with respect to some coordinates and non-stationary with respect to others. This could happen, for example, if the effect of certain features (e.g., features for advertisers in certain business sectors) changes over time, but the effect of other features remains constant."}, {"heading": "5 Experimental Evaluation", "text": "In this section, we evaluate gradient descent with per-coordinate learning rates experimentally on several machine learning problems."}, {"heading": "5.1 Online binary classification", "text": "We first compare the performance of online gradient descent with that of two recent algorithms for text classification: the Passive-Aggressive (PA) algorithm [4], and confidence-weighted (CW) linear classification [7]. The latter algorithm has been demonstrated to have state-of-the-art performance on large real-world problems [13].\nWe used four sentiment classification data sets (Books, Dvd, Electronics, and Kitchen), available from [6], each with 1000 positive examples and 1000 negative examples,3 as well as the scaled versions of the rcv1.binary (677,399 examples) and news20.binary (19,996 examples) data sets from LIBSVM [3]. For each data set, we shuffled the examples and then ran each algorithm for one pass over the data, computing the loss on each event before training on it.\nFor the online gradient descent algorithms, we set F = [\u2212R,R]n for R = 100. We found that the learning rate suggested by Theorem 3 was too aggressive in practice when the feasible set is large (note that it moves a feature\u2019s weight to the maximum value the first time it sees a non-zero gradient for that feature). In order to improve performance, we did some parameter tuning. For Algorithm 1 (Per-Coord), we scaled the learning rate formula by a factor of 0.6/R, and for the global learning rate (Global) we scaled it by 0.2/R. We estimate the diameter D in the global learning rate formula online, based on the number of attributes seen so far. For CW, we found that the parameters \u03c6 = 1.0 and a = 1.0 worked well in practice.\nTable 1 presents average hinge loss and the fraction of classification mistakes for each algorithm. The Global and Per-Coord algorithms are designed to minimize hinge loss, and at this objective the Per-Coord algorithm consistently wins. CW and PA are designed to maximize classification accuracy, and on this\n3We used the features provided in processed acl.tar.gz, and scaled each vector of counts to unit length.\nobjective Per-Coord and CW are the best algorithms. The fact that the classification accuracy of Per-Coord is comparable to that of a state-of-the-art binary classification algorithm is impressive given the former algorithm\u2019s generality (i.e., its applicability to arbitrary online convex optimization problems such as online shortest paths)."}, {"heading": "5.2 Large-scale logistic regression", "text": "We collected data from a large search engine4 consisting of random samples of queries that contained a particular phrase, for example \u201cauto insurance\u201d. Each data set has a few million examples. We transformed this data into an online logistic regression problem with a feature vector \u03b8t for each ad impression, using features based on the text of the ad and the query. The target label \u2113t is 1 if the ad was clicked, and -1 otherwise. The loss function ft is the sum of the logistic loss, log (1 + exp(\u2212\u2113txt\u03b8t)), and an L2 regularization term.\nWe compare gradient descent using the global learning rate from \u00a73.1 with gradient descent using the per-coordinate rate given in \u00a73.2. We scaled the formulas given in those sections by 0.1; this improved performance for both algorithms but did not change the relative comparison. The feasible set was [\u22121, 1]n.\nTable 2 shows the regret incurred by the two algorithms on various data sets. Gradient descent with a per-coordinate learning rate consistently obtains an order of magnitude lower regret than with a global learning rate. To calculate regret, we computed the static optimal loss minx\u2208F { \u2211T t=1 ft(x) } by running our per-coordinate algorithm through the data many times until convergence."}, {"heading": "6 Related Work", "text": "The use of different learning rates for different coordinates has been investigated extensively in the neural network community. There the focus has been on empirical performance in the batch setting, and a large number of algorithms have been developed; see for example [12]. These algorithms are not designed to perform well in an adversarial online setting, and for many of them it is straightforward to construct examples where the algorithm incurs high regret.\nMore recently, Hsu et al. [11] gave an algorithm for choosing per-coordinate learning rates for gradient descent, derive asymptotic rates of convergence in the batch setting, and present a number of positive experimental results.\nConfidence-weighted linear classification [7] and AROW [5] are similar to our algorithm in that they make different-sized adjustments for different coordinates, and in that common features are updated less aggressively than rare ones. Unlike our algorithm, these algorithms apply only to classification problems and not to general online convex optimization, and the guarantees are in the form of mistake bounds rather than regret bounds.\n4No user-specific data was used in these experiments.\nIn concurrent work [14], we generalize the results of this paper to handle arbitrary feasible sets and a matrix (rather than a vector) of learning rate parameters. Similar theoretical results were obtained independently by Duchi et al. [8]."}], "references": [{"title": "Adaptive online gradient descent", "author": ["Peter L. Bartlett", "Elad Hazan", "Alexander Rakhlin"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Online passiveaggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Adaptive regularization of weight vectors", "author": ["Koby Crammer", "Alex Kulesza", "Mark Drezde"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Multi-domain sentiment dataset (v2.0)", "author": ["Mark Dredze"], "venue": "http://www.cs.jhu.edu/~mdredze/datasets/sentiment/,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Confidence-weighted linear classification", "author": ["Mark Drezde", "Koby Crammer", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In COLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Efficient learning algorithms for changing environments", "author": ["Elad Hazan", "C. Seshadhri"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning", "author": ["Chun-Nan Hsu", "Han-Shen Huang", "Yu-Ming Chang", "Yuh-Jye Lee"], "venue": "Maching Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["Robert A. Jacobs"], "venue": "Neural Networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Identifying suspicious URLs: an application of large-scale online learning", "author": ["Justin Ma", "Lawrence K. Saul", "Stefan Savage", "Geoffrey M. Voelker"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Two problems with backpropagation and other steepest-descent learning procedures for networks", "author": ["Richard S. Sutton"], "venue": "In Proc. Eighth Annual Conference of the Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16].", "startOffset": 150, "endOffset": 161}, {"referenceID": 11, "context": "1 Introduction In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16].", "startOffset": 150, "endOffset": 161}, {"referenceID": 14, "context": "1 Introduction In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16].", "startOffset": 150, "endOffset": 161}, {"referenceID": 13, "context": "2 Motivations It is well-known that batch gradient descent performs poorly in the presence of so-called ravines, surfaces that curve more steeply in some directions than in others [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "Thus, for any choice of \u03b7 there exists a problem where max { D 4\u03b7 ,G\u03b7 T 2 } \u2264 Regret \u2264 D 2 2\u03b7 +G\u03b7 T 2 , where the upper bound is adapted from Zinkevich [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "2, setting G = 1 and setting the feasible set to [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "On a one-dimensional subproblem with feasible set [0, 1] and gradients of magnitude at most 1, gradient descent using learning rate 1 \u221a s on round s of the subproblem obtains regret O( \u221a S) on a subproblem of length S [17].", "startOffset": 50, "endOffset": 56}, {"referenceID": 15, "context": "On a one-dimensional subproblem with feasible set [0, 1] and gradients of magnitude at most 1, gradient descent using learning rate 1 \u221a s on round s of the subproblem obtains regret O( \u221a S) on a subproblem of length S [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "3 Improved Regret Bounds using Per-Coordinate Learning Rates Zinkevich [17] proved bounds on the regret of online gradient descent (which chooses xt according to Equation (1)).", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "Zinkevich [17] showed that if we run gradient descent with a non-increasing sequence \u03b71, \u03b72, .", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Our techniques can be extended to arbitrary feasible sets using a somewhat different algorithm, but the proofs are signicantly more technical [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "A related result appears in [1], giving improved bounds in the case of strongly convex functions but worse constants than ours in the case of linear functions.", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "Zinkevich[17] showed that, so long as our algorithm only makes use of \u25bdft(xt), we may assume without loss of generality that ft is linear, and therefore ft(x) = gt \u00b7 x for all x \u2208 F .", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "A similar observation was originally used by Zinkevich [17] to show that any algorithm for online linear optimization can be used for online convex optimization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "For simplicity, when stating these bounds we assume that the feasible set is F = [0, 1] and that the gradients of the loss functions are componentwise upper bounded by 1 (that is, |(\u25bdft(xt))i| \u2264 1 for all t and i).", "startOffset": 81, "endOffset": 87}, {"referenceID": 0, "context": "[1] give an online convex optimization algorithm whose regret is O (", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "2 Tighter bounds in terms of variance Hazan and Kale [9] give a bound on gradient descent\u2019s regret in terms of the variance of the sequence of gradients.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "For example, suppose the feasible set is [0, 1], ft(x) = x for the first T 2 rounds and ft(x) = \u2212x thereafter.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "Hazan and Seshadhri [10] define adaptive regret as the maximum, over all intervals [T0, T1], of the regret \u2211T1 t=T0 ft(xt) \u2212 minx\u2208F {", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Holding H constant for simplicity, the adaptive regret bound just stated implies that the algorithm of Hazan and Seshadhri [10] obtains RZ = O((N + 1) log 2 T ), where N is the number of values of t for which zt 6= zt+1 (this follows by summing adaptive regret over the N + 1 intervals where zt is constant).", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "1 Online binary classification We first compare the performance of online gradient descent with that of two recent algorithms for text classification: the Passive-Aggressive (PA) algorithm [4], and confidence-weighted (CW) linear classification [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 5, "context": "1 Online binary classification We first compare the performance of online gradient descent with that of two recent algorithms for text classification: the Passive-Aggressive (PA) algorithm [4], and confidence-weighted (CW) linear classification [7].", "startOffset": 245, "endOffset": 248}, {"referenceID": 11, "context": "The latter algorithm has been demonstrated to have state-of-the-art performance on large real-world problems [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 4, "context": "We used four sentiment classification data sets (Books, Dvd, Electronics, and Kitchen), available from [6], each with 1000 positive examples and 1000 negative examples, as well as the scaled versions of the rcv1.", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "There the focus has been on empirical performance in the batch setting, and a large number of algorithms have been developed; see for example [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "[11] gave an algorithm for choosing per-coordinate learning rates for gradient descent, derive asymptotic rates of convergence in the batch setting, and present a number of positive experimental results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Confidence-weighted linear classification [7] and AROW [5] are similar to our algorithm in that they make different-sized adjustments for different coordinates, and in that common features are updated less aggressively than rare ones.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "Confidence-weighted linear classification [7] and AROW [5] are similar to our algorithm in that they make different-sized adjustments for different coordinates, and in that common features are updated less aggressively than rare ones.", "startOffset": 55, "endOffset": 58}], "year": 2010, "abstractText": "We analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems.", "creator": "LaTeX with hyperref package"}}}