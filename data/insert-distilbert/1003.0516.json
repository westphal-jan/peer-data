{"id": "1003.0516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2010", "title": "Model Selection with the Loss Rank Principle", "abstract": "a key issue in statistics and machine driven learning is to automatically select the \" right \" model by complexity, whether e. g., deciding the number of neighbors to commonly be averaged over in k nearest neighbor ( knn ) weighted regression or the polynomial degree in regression with polynomials. usually we suggest a similarly novel principle - the loss zero rank principle ( just lorp ) - for model selection in regression samples and classification. overall it is based on the maximal loss rank, which counts how many other ( fictitious ) data would be fitted better. lorp selects the model variable that has minimal loss rank. unlike most penalized maximum likelihood variants ( aic, bic, mdl ), lorp depends only on either the regression functions omitted and the loss function. it works without a stochastic noise model, and is directly applicable to any non - parametric regressor, like knn.", "histories": [["v1", "Tue, 2 Mar 2010 08:21:07 GMT  (41kb)", "http://arxiv.org/abs/1003.0516v1", "31 LaTeX pages, 1 figure"]], "COMMENTS": "31 LaTeX pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcus hutter", "minh-ngoc tran"], "accepted": false, "id": "1003.0516"}, "pdf": {"name": "1003.0516.pdf", "metadata": {"source": "CRF", "title": "Model Selection with the Loss Rank Principle", "authors": ["Marcus Hutter"], "emails": ["RSISE@ANU", "SML@NICTA,", "marcus@hutter1.net", "ngoctm@nus.edu.sg"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 3.\n05 16\nv1 [\ncs .L\nG ]\nContents"}, {"heading": "1 Introduction 2", "text": ""}, {"heading": "2 The Loss Rank Principle 4", "text": ""}, {"heading": "3 LoRP for y-Linear Models 8", "text": ""}, {"heading": "4 Optimality Properties of LoRP for Variable Selection 11", "text": ""}, {"heading": "5 Experiments 13", "text": ""}, {"heading": "6 Comparison to Gaussian Bayesian Linear Regression 17", "text": ""}, {"heading": "7 Comparison to other Model Selection Schemes 19", "text": ""}, {"heading": "8 Loss Functions and their Selection 23", "text": ""}, {"heading": "9 Self-Consistent Regression 24", "text": ""}, {"heading": "10 Nearest Neighbors Classification 26", "text": "11 Conclusion and Outlook 28 References 30\nKeywords\nModel selection, loss rank principle, non-parametric regression, classification, general loss function, k nearest neighbors."}, {"heading": "1 Introduction", "text": "Regression. Consider a regression or classification problem in which we want to determine the functional relationship yi\u2248ftrue(xi) from dataD={(x1,y1),...,(xn,yn)}, i.e., we seek a function r(.|D)\u2261 r(D)(.) such that r(x|D)\u2261 r(D)(x) is close to the unknown ftrue(x) for all x. One may define r(.|D) directly, e.g., \u201caverage the y values of the k nearest neighbors (kNN) of x in D\u201d, or select r(.|D) from a class of functions F that has smallest (training) error on D. If the class F is not too large, e.g., the polynomials of fixed reasonable degree d, this often works well.\nModel selection. What remains is to select the right model complexity c, like k or d. This selection cannot be based on the training error, since the more complex the model (large d, small k) the better the fit on D (perfect for d=n and k=1). This problem is called overfitting, for which various remedies have been suggested.\nThe most popular ones in practice are based on a test set D\u2032 used for selecting the c for which the function rc(.|D) has smallest (test) error on D\u2032, or improved versions like cross-validation [All74]. Typically D\u2032 is cut from D, thus reducing the sample size available for regression. Test set methods often work well in practice, but the reduced sample decreases accuracy, which can be a serious problem if n is small. We will not discuss empirical test set methods any further. See [Mac92] for a comparison of cross-validation with Bayesian model selection.\nThere are also various model selection methods that allow to use all data D for regression. The most popular ones can be regarded as penalized versions of Maximum Likelihood (ML). In addition to the function class Fc (subscript c belonging to some set indexing the complexity), one has to specify a sampling model P(D|f), e.g., that the yi have independent Gaussian distribution with mean f(xi). ML chooses rc(D)=argmaxf\u2208FcP(D|f), Penalized ML (PML) then chooses c\u0302=argminc{\u2212logP(D|rc(D))+Penalty(c)}, where the penalty depends on the used approach (MDL [Ris78], BIC [Sch78], AIC [Aka73]). All PML variants rely on a proper sampling model (which may be difficult to establish), ignore (or at least do not tell how to incorporate) a potentially given loss function (see [Yam99, Gru\u030804] for exceptions), are based on distribution-independent penalties (which may result in bad performance for specific distributions), and are typically limited to (semi)parametric models.\nMain idea. The main goal of the paper is to establish a criterion for selecting the \u201cbest\u201d model complexity c based on regressors rc given as a black box without insight into the origin or inner structure of rc, that does not depend on things often not given (like a stochastic noise model), and that exploits what is/should be given (like the loss function, note that the criterion can also be used for loss-function selection, see Section 8). The key observation we exploit is that large classes Fc or more flexible regressors rc can fit more data well than more rigid ones. We define the loss rank of rc as the number of other (fictitious) data D\n\u2032 that are fitted better by rc(D\n\u2032) than D is fitted by rc(D), as measured by some loss function. The loss rank is large for regressors fitting D not well and for too flexible regressors (in both\ncases the regressor fits many other D\u2032 better). The loss rank has a minimum for not too flexible regressors which fit D not too bad. We claim that minimizing the loss rank is a suitable model selection criterion, since it trades off the quality of fit with the flexibility of the model. Unlike PML, our Loss Rank Principle (LoRP) works without a noise (stochastic sampling) model, and is directly applicable to any non-parametric regressor, like kNN.\nRelated ideas. There are various other ideas that somehow count fictitious data. In normalized ML [Gru\u030804], the complexity of a stochastic model class is defined as the log sum over all D\u2032 of maximum likelihood probabilities. In the luckiness framework for classification [Her02, Chp.4], the loss rank is related to the level of a hypothesis, if the empirical loss is used as an unluckiness function. The empirical Rademacher complexity [Kol01, BBL02] averages over all possible relabeled instances. Finally, instead of considering all D\u2032 one could consider only the set of all permutations of {y1,...,yn}, like in permutation tests [ET93]. The test statistic would here be the empirical loss.\nContents. In Section 2, after giving a brief introduction to regression, we formally state LoRP for model selection. Explicit expressions for the loss rank for the important class of linear regressors are derived in Section 3; this class includes kNN, polynomial, linear basis function (LBFR), kernel, projective regression, and some others. In Section 4, we establish optimality properties of LoRP for linear regression, namely model consistency and asymptotic mean efficiency. Experiments are presented in Section 5: We compare LoRP to other selection methods and demonstrate the use of LoRP for some specific problems like choosing tuning parameters in kNN and spline regression. In Section 6 we compare linear LoRP to Bayesian model selection for linear regression with Gaussian noise and prior, and in Section 7 to PML, in particular MDL, BIC, and AIC, and then discuss two trace formulas for the effective dimension. Sections 8-10 can be considered as extension sections. In Section 8 we show how to generalize linear LoRP to non-quadratic loss, in particular to other norms. We also discuss how LoRP can be used to select the loss function itself, in case it is not part of the problem specification. In Section 9 we briefly discuss interpolation. LoRP only depends on the regressor on data D and not on x 6\u2208 {x1,...,xn}. We construct canonical regressors for off-data interpolation from regressors given only on-data, in particular for kNN, Kernel, and LBFR, and show that they are canonical. In Section 10 we derive exact expressions for kNN when {x1,...,xn} forms a discrete d-dimensional hypercube, and discuss the limits n\u2192\u221e, k\u2192\u221e, and d\u2192\u221e. Section 11 contains the conclusions of our work and further considerations that could be elaborated on in the future.\nThe main idea of LoRP has already been presented at the COLT 2007 conference [Hut07]. In this paper we present LoRP more thoroughly, discover its theoretical properties and evaluate the method through some experiments."}, {"heading": "2 The Loss Rank Principle", "text": "After giving a brief introduction to regression, classification, model selection, overfitting, and some reoccurring examples, we state our novel Loss Rank Principle for model selection. We first state it for classification (Principle 3 for discrete values), and then generalize it for regression (Principle 5 for continuous values), and exemplify it on two (over-simplistic) artificial Examples 4 and 6. Thereafter we show how to regularize LoRP for realistic regression problems.\nSetup and notation. We assume data D = (x,y) := {(x1,y1),...,(xn,yn)} \u2208 (X \u00d7 Y)n=:D has been observed. We think of the y as having an approximate functional dependence on x, i.e., yi \u2248 ftrue(xi), where \u2248 means that the yi are distorted by noise from the unknown \u201ctrue\u201d values ftrue(xi). We will write (x,y) for generic data points, use vector notation x=(x1,...,xn) \u22a4 and y=(y1,...,yn) \u22a4, and D\u2032=(x\u2032,y\u2032) for generic (fictitious) data of size n. A full list of abbreviations and notations used throughout the paper is placed in the appendix.\nRegression and classification. In regression problems Y is typically (a subset of) the real set IR or some more general measurable space like IRm. In classification, Y is a finite set or at least discrete. We impose no restrictions on X . Indeed, x will essentially be fixed and plays only a spectator role, so we will often notationally suppress dependencies on x. The goal of regression/classification is to find a function fD \u2208F \u2282X \u2192Y \u201cclose\u201d to ftrue based on the past observations D. Or phrased in another way: we are interested in a mapping r :D\u2192F such that y\u0302 := r(x|D)\u2261 r(D)(x)\u2261fD(x)\u2248ftrue(x) for all x\u2208X . Example 1 (polynomial regression) For X = Y = IR, consider the set Fd := {fw(x) = wdxd\u22121+ ...+w2x+w1 :w \u2208 IRd} of polynomials of degree d\u22121. Fitting the polynomial to data D, e.g., by least squares regression, we estimate w with w\u0302D. The regression function y\u0302= rd(x|D) = fw\u0302D(x) can be written down in closed form (see Example 9). \u2666\nExample 2 (k nearest neighbors) Let Y be some vector space like IR and X be a metric space like IRm with some (e.g., Euclidean) metric d(\u00b7,\u00b7). kNN estimates ftrue(x) by averaging the y values of the k nearest neighbors Nk(x) of x in D, i.e., rk(x|D)= 1k \u2211\ni\u2208Nk(x)yi with |Nk(x)|=k such that d(x,xi)\u2264d(x,xj) for all i\u2208Nk(x) and j 6\u2208Nk(x). \u2666\nParametric versus non-parametric regression. Polynomial regression is an example of parametric regression in the sense that rd(D) is the optimal function from a family of functions Fd indexed by d<\u221e real parameters (w). In contrast, the kNN regressor rk is directly given and is not based on a finite-dimensional family of functions. In general, r may be given either directly or be the result of an optimization process.\nLoss function. The quality of fit to the data is usually measured by a loss function Loss(y,y\u0302), where y\u0302i = f\u0302D(xi) is an estimate of yi. Often the loss is additive:\nLoss(y,y\u0302)= \u2211n i=1Loss(yi,y\u0302i). If the class F is not too large, good regression functions r(D) can be found by minimizing the loss w.r.t. all f \u2208 F . For instance, rd(D)=argminf\u2208Fd \u2211n i=1(yi\u2212f(xi))2 and y\u0302=rd(x|D) in Example 1. Regression class and loss. In the following we assume a class of regressors R (whatever their origin), e.g., the kNN regressors {rk : k \u2208 IN} or the least squares polynomial regressors {rd : d \u2208 IN0 := IN\u222a{0}}. Each regressor r can be thought of as a model. Throughout the paper, we use the terms \u201cregressor\u201d and \u201cmodel\u201d interchangeably. Note that unlike f \u2208F , regressors r \u2208R are not functions of x alone but depend on all observations D, in particular on y. Like for functions f , we can compute the empirical loss of each regressor r\u2208R:\nLossr(D) \u2261 Lossr(y|x) := Loss(y, y\u0302) = n \u2211\ni=1\nLoss(yi, r(xi|x,y))\nwhere y\u0302i= r(xi|D) in the third expression, and the last expression holds in case of additive loss.\nOverfitting. Unfortunately, minimizing Lossr w.r.t. r will typically not select the \u201cbest\u201d overall regressor. This is the well-known overfitting problem. In case of polynomials, the classes Fd\u2282Fd+1 are nested, hence Lossrd is monotone decreasing in d with Lossrn \u2261 0 perfectly fitting the data. In case of kNN, Lossrk is more or less an increasing function in k with perfect regression on D for k=1, since no averaging takes place. In general, R is often indexed by a \u201cflexibility\u201d or smoothness or complexity parameter, which has to be properly determined. The more flexible r is, the closer it can fit the data. Hence such r has smaller empirical loss, but is not necessarily better since it has higher variance. Clearly, too inflexible r also lead to a bad fit (\u201chigh bias\u201d).\nMain goal. The main goal of the paper is to establish a selection criterion in order to specify the smallest model to which ftrue belongs or is close to, and simultaneously determine the \u201cbest\u201d fitting function r(D). The criterion\n\u2022 is based on r given as a black box that does not require insight into the origin or inner structure of r; \u2022 does not depend on things often not given (like a stochastic noise model); and \u2022 exploits what is or should be given (like the loss function).\nDefinition of loss rank. We first consider discrete Y (i.e., classification), fix x, y is the observed data and y\u2032 are fictitious others. The key observation we exploit is that a more flexible r can fit more data D\u2032\u2208D well than a more rigid one. The more flexible r is, the smaller the empirical loss Lossr(y|x) is. Instead of minimizing the unsuitable Lossr(y|x) w.r.t. r, we could ask how many y\u2032\u2208Yn lead to smaller Lossr than y. We define the loss rank of r (w.r.t. y) as the number of y\n\u2032\u2208Yn with smaller or equal empirical loss than y:\nRankr(y|x)\u2261 Rankr(L) := #{y\u2032\u2208Yn : Lossr(y\u2032|x)\u2264L} with L := Lossr(y|x) (1)\nWe claim that the loss rank of r is a suitable model selection measure. For (1) to make sense, we have to assume (and will later assure) that Rankr(L)<\u221e, i.e., there are only finitely many y\u2032\u2208Yn having loss smaller than L.\nSince the logarithm is a strictly monotone increasing function, we can also consider the logarithmic rank LRr(y|x) := logRankr(y|x), which will be more convenient.\nPrinciple 3 (LoRP for classification) For discrete Y, the best classifier/regressor r : D\u00d7X \u2192 Y in some class R for data D = (x,y) is the one with the smallest loss rank:\nrbest = argmin r\u2208R LRr(y|x) \u2261 argmin r\u2208R Rankr(y|x) (2)\nwhere Rankr is defined in (1).\nWe give a simple example for which we can compute all ranks by hand to help the reader better grasp how the principle works.\nExample 4 (simple discrete) Consider X = {1,2}, Y = {0,1,2}, and two points D={(1,1),(2,2)} lying on the diagonal x=y, with polynomial (zero, constant, linear) least squares regressors R= {r0,r1,r2} (see Ex.1). r0 is simply 0, r1 the y-average, and r2 the line through points (1,y1) and (2,y2). This, together with the quadratic Loss for generic y\u2032 and observed y=(1,2) and fixed x=(1,2), is summarized in the following table\nd rd(x|x,y\u2032) Lossd(y\u2032|x) Lossd(D) 0 0 y\u20321 2 + y\u20322 2 5 1 1 2 (y\u20321 + y \u2032 2) 1 2 (y\u20322 \u2212 y\u20321)2 12 2 (y\u20322 \u2212 y\u20321)(x\u2212 1) + y\u20321 0 0\nFrom the Loss we can easily compute the Rank for all nine y\u2032\u2208{0,1,2}2. Equal rank due to equal loss is indicated by a \u201c=\u201d in the table below. Whole equality groups are actually assigned the rank of their right-most member, e.g., for d=1 the ranks of (y\u20321,y \u2032 2)=(0,1),(1,0),(2,1),(1,2) are all 7 (and not 4,5,6,7).\nRankrd(y \u2032 1y \u2032 2|12)\nd 1 2 3 4 5 6 7 8 9 Rankrd(D) 0 y\u20321y \u2032 2 = 00 < 01 = 10 < 11 < 02 = 20 < 21 = 12 < 22 8 1 y\u20321y \u2032 2 = 00 = 11 = 22 < 01 = 10 = 21 = 12 < 02 = 20 7 2 y\u20321y \u2032 2 = 00 = 01 = 02 = 10 = 11 = 20 = 21 = 22 = 12 9\nSo LoRP selects r1 as best regressor, since it has minimal rank on D. r0 fits D too badly and r2 is too flexible (perfectly fits all D\n\u2032). \u2666 LoRP for continuous Y. We now consider the case of continuous or measurable spaces Y , i.e., normal regression problems. We assume Y = IR in the following exposition, but the idea and resulting principle hold for more general measurable\nspaces like IRm. We simply reduce the model selection problem to the discrete case by considering the discretized space Y\u03b5=\u03b5ZZ for small \u03b5>0 and discretize y;y\u03b5\u2208\u03b5ZZn (\u201c;\u201d means \u201cis replaced by\u201d). Then Rank\u03b5r(L) :=#{y\u2032\u03b5\u2208Yn\u03b5 :Lossr(y\u2032\u03b5|x)\u2264L} with L=Lossr(y\u03b5|x) counting the number of \u03b5-grid points in the set\nVr(L) := {y\u2032 \u2208 Yn : Lossr(y\u2032|x) \u2264 L} (3)\nwhich we assume (and later assure) to be finite, analogous to the discrete case. Hence Rank\u03b5r(L) \u00b7\u03b5n is an approximation of the loss volume |Vr(L)| of set Vr(L), and typically Rank\u03b5r(L) \u00b7\u03b5n = |Vr(L)| \u00b7(1+O(\u03b5)) \u2192 |Vr(L)| for \u03b5 \u2192 0. Taking the logarithm we get LR\u03b5r(y|x)=logRank\u03b5r(L)=log|Vr(L)|\u2212nlog\u03b5+O(\u03b5). Since nlog\u03b5 is independent of r, we can drop it in comparisons like (2). So for \u03b5\u21920 we can define the log-loss \u201crank\u201d simply as the log-volume\nLRr(y|x) := log |Vr(L)|, where L := Lossr(y|x) (4)\nPrinciple 5 (LoRP for regression) For measurable Y, the best regressor r :D\u00d7 X \u2192Y in some class R for data D=(x,y) is the one with the smallest loss volume:\nrbest = argmin r\u2208R LRr(y|x) \u2261 argmin r\u2208R |Vr(L)|\nwhere LR, Vr, and L are defined in (3) and (4), and |Vr(L)| is the volume of Vr(L)\u2286 Yn.\nFor discrete Y with counting measure we recover the discrete LoRP (Principle 3).\nExample 6 (simple continuous) Consider Example 4 but with interval Y=[0,2]. The first table remains unchanged, while the second table becomes\nd Vd(L) = {y\u2032 \u2208 [0, 2]2 : ...} |Vd(L)| Lossd(D) |Vd(Lossd(D))| 0 y\u20321 2 + y\u20322 2 \u2264 L \u03c0 4 L if L\u22644; 4 if L\u22658;\n2 \u221a L\u22124+L(\u03c0\n4 \u2212cos\u22121( 2\u221a L )) else\n5 . = 3.6\n1 1 2 (y\u20322 \u2212 y\u20321)2 \u2264 L 4 \u221a 2L\u22122L if L\u22642; 4 if L\u22652\n1 2\n3\n2 0 \u2264 L 4 0 4\nSo LoRP again selects r1 as best regressor, since it has smallest loss volume on D. \u2666\nInfinite rank or volume. Often the loss rank/volume will be infinite, e.g., if we had chosen Y = ZZ in Ex.4 or Y = IR in Ex.6. There are various potential remedies. We could modify (a) the regressor r or (b) the Loss to make LRr finite, (c) the Loss Rank Principle itself, or (d) find problem-specific solutions. Regressors r with infinite rank might be rejected for philosophical or pragmatic reasons. We will briefly consider (a) for linear regression later, but to fiddle around with r in a generic (blackbox way) seems difficult. We have no good idea how to tinker with LoRP (c), and also a patched LoRP may be less attractive. For kNN on a grid we\nlater use remedy (d). While in (decision) theory, the application\u2019s goal determines the loss, in practice the loss is often more determined by convenience or rules of thumb. So the Loss (b) seems the most inviting place to tinker with. A very simple modification is to add a small penalty term to the loss.\nLossr(y|x) ; Loss\u03b1r (y|x) := Lossr(y|x) + \u03b1\u2016y\u20162, \u03b1 > 0 \u201csmall\u201d (5)\nThe Euclidean norm \u2016y\u20162 :=\u2211ni=1y2i is default, but other (non)norm regularizations are possible. The regularized LR\u03b1r (y|x) based on Loss\u03b1r is always finite, since {y : \u2016y\u20162\u2264L} has finite volume. An alternative penalty \u03b1y\u0302\u22a4y\u0302, quadratic in the regression estimates y\u0302i=r(xi|x,y) is possible if r is unbounded in every y\u2192\u221e direction.\nA scheme trying to determine a single (flexibility) parameter (like d and k in the above examples) would be of no use if it depended on one (or more) other unknown parameters (\u03b1), since varying through the unknown parameter leads to any (non)desired result. Since LoRP seeks the r of smallest rank, it is natural to also determine \u03b1=\u03b1min by minimizing LR \u03b1 r w.r.t. \u03b1. The good news is that this leads to meaningful results. Interestingly, as we will see later, a clever choice of \u03b1 may also result in alternative optimalities of the selection procedure."}, {"heading": "3 LoRP for y-Linear Models", "text": "In this section we consider the important class of y-linear regressions with quadratic loss function. By \u201cy-linear regression\u201d, we mean the linearity is only assumed in y and the dependence on x can be arbitrary. This class is richer than it may appear. It includes the normal linear regression model, kNN (Example 7), kernel (Example 8), and many other regression models. For y-linear regression and Y=IR, the loss rank is the volume of an n-dimensional ellipsoid, which can efficiently be computed in time O(n3) (Theorem 10). For the special case of projective regression, e.g., linear basis function regression (Example 9), we can even determine the regularization parameter \u03b1 analytically (Theorem 11).\ny-Linear regression. We assume Y = IR in this section; generalization to IRm is straightforward. A y-linear regressor r can be written in the form\ny\u0302 = r(x|x,y) = n \u2211\nj=1\nmj(x,x)yj \u2200x \u2208 X and some mj : X \u00d7 X n \u2192 IR (6)\nParticularly interesting is r for x=x1,...,xn.\ny\u0302i = r(xi|x,y) = \u2211\nj\nMij(x)yj with M : X n \u2192 IRn\u00d7n (7)\nwhere matrix Mij(x)=mj(xi,x). Since LoRP needs r only on the training data x, we only need M .\nExample 7 (kNN ctd.) For kNN of Ex.2 we have mj(x,x)= 1 k if j\u2208Nk(x) and 0 else, and Mij(x)= 1 k if j\u2208Nk(xi) and 0 else. \u2666\nExample 8 (kernel regression) Kernel regression takes a weighted average over y, where the weight of yj to y is proportional to the similarity of xj to x, measured by a kernel K(x,xj), i.e., mj(x,x)=K(x,xj)/ \u2211n j=1K(x,xj). For example the Gaussian kernel for X =IRm is K(x,xj)=e\u2212\u2016x\u2212xj\u201622/2\u03c32 . The width \u03c3 controls the smoothness of the kernel regressor, and LoRP selects the real-valued \u201ccomplexity\u201d parameter \u03c3. \u2666 Example 9 (linear basis function regression, LBFR) Let \u03c61(x),...,\u03c6d(x) be a set or vector of \u201cbasis\u201d functions often called \u201cfeatures\u201d. We place no restrictions on X or \u03c6 :X \u2192IRd. Consider the class of functions linear in \u03c6:\nFd = {fw(x) = \u2211d a=1wa\u03c6a(x) = w \u22a4\u03c6(x) : w \u2208 IRd}\nFor instance, for X=IR and \u03c6a(x)=xa\u22121 we would recover the polynomial regression Example 1. For quadratic loss function Loss(yi,y\u0302i)=(yi\u2212y\u0302i)2 we have\nLossw(y|\u03c6) := n \u2211\ni=1\n(yi \u2212 fw(xi))2 = y\u22a4y \u2212 2y\u22a4\u03a6w +w\u22a4Bw\nwhere matrix \u03a6 is defined by \u03a6ia = \u03c6a(xi) and B is a symmetric matrix with Bab = \u2211n i=1\u03c6a(xi)\u03c6b(xi) = [\u03a6\n\u22a4\u03a6]ab. The loss is quadratic in w with minimum at w=B\u22121\u03a6\u22a4y. So the least squares regressor is y\u0302= y\u22a4\u03a6B\u22121\u03c6(x), hence mj(x,x) = (\u03a6B\u22121\u03c6(x))j and M(x)=\u03a6B\n\u22121\u03a6\u22a4. \u2666 Consider now a general linear regressor M with quadratic loss and quadratic\npenalty Loss\u03b1M(y|x) = n \u2211\ni=1\n(\nyi \u2212 \u2211n j=1Mijyj\n)2\n+ \u03b1\u2016y\u20162 = y\u22a4S\u03b1y,\nwhere1 S\u03b1 = (I \u2212M)\u22a4(I \u2212M) + \u03b1I (8) (I is the identity matrix). S\u03b1 is a symmetric matrix. For \u03b1>0 it is positive definite and for \u03b1=0 positive semidefinite. If \u03bb1,...,\u03bbn \u2265 0 are the eigenvalues of S0, then \u03bbi+\u03b1 are the eigenvalues of S\u03b1. V (L)={y\u2032\u2208IRn :y\u2032\u22a4S\u03b1y\u2032\u2264L} is an ellipsoid with the eigenvectors of S\u03b1 being the main axes and \u221a\nL/(\u03bbi+\u03b1) being their length. Hence the volume is\n|V (L)| = vn n \u220f\ni=1\n\u221a\nL\n\u03bbi + \u03b1 =\nvnL n/2\n\u221a detS\u03b1\nwhere vn=\u03c0 n/2/n\n2 ! is the volume of the n-dimensional unit sphere, z! :=\u0393(z+1), and\ndet is the determinant. Taking the logarithm we get\nLR\u03b1M(y|x) = log |V (Loss\u03b1M(y|x))| = n2 log(y\u22a4S\u03b1y)\u2212 12 log detS\u03b1 + log vn (9) Since vn is independent of \u03b1 and M it is possible to drop vn. Consider now a class of linear regressors M= {M}, e.g., the kNN regressors {Mk : k \u2208 IN} or the d-dimensional linear basis function regressors {Md :d\u2208IN0}.\n1The mentioned alternative penalty \u03b1\u2016y\u0302\u20162 would lead to S\u03b1 =(I\u2212M)\u22a4(I\u2212M)+\u03b1M\u22a4M . For LBFR, penalty \u03b1\u2016w\u0302\u20162 is popular (ridge regression). Apart from being limited to parametric regression, it has the disadvantage of not being reparametrization invariant. For instance, scaling \u03c6a(x);\u03b3a\u03c6a(x) does not change the class Fd, but changes the ridge regressor.\nTheorem 10 (LoRP for y-linear regression) For Y=IR, the best linear regressor M :X n\u2192IRn\u00d7n in some class M for data D=(x,y) is\nM best = argmin M\u2208M,\u03b1\u22650 {n 2 log(y\u22a4S\u03b1y)\u2212 12 log detS\u03b1} = argmin M\u2208M \u03b1\u22650\n{ y\u22a4S\u03b1y\n(detS\u03b1)1/n\n}\n(10)\nwhere S\u03b1=S\u03b1(M) is defined in (8).\nThe last expression shows that linear LoRP minimizes the Loss times the geometric average of the squared axes lengths of ellipsoid V (1). Note that M best depends on y unlike the M \u2208M. Nullspace of S0. If M has an eigenvalue 1, then S0 = (I\u2212M)\u22a4(I\u2212M) has a zero eigenvalue and \u03b1 > 0 is necessary, since detS0 = 0. Actually this is true for most practical M . Most linear regressors are invariant under a constant shift of y, i.e., r(x|x,y+c) = r(x|x,y)+c, which implies that M has eigenvector (1,...,1)\u22a4 with eigenvalue 1. This can easily be checked for kNN (Ex.2), kernel (Ex.8), and LBFR (Ex.9). Such a generic 1-eigenvector effecting all M \u2208M could easily and maybe should be filtered out by considering only the orthogonal space or dropping these \u03bbi=0 when computing detS0. The 1-eigenvectors that depend on M are the ones where we really need a regularizer \u03b1 > 0. For instance, Md in LBFR has d eigenvalues 1, and MkNN has as many eigenvalues 1 as there are disjoint components in the graph determined by the edges Mij>0. In general we need to find the optimal \u03b1 numerically. If M is a projection we can find \u03b1m analytically.\nNumerical approximation of (detS\u03b1) 1/n and the computational complexity of linear LoRP. For each \u03b1 and candidate model, the determinant of S\u03b1 in the general case can be computed in time O(n3). Often M is a very sparse matrix (like in kNN) or can be well approximated by a sparse matrix (like for kernel regression), which allows us to approximate detS\u03b1 sometimes in linear time [Reu02]. To search the optimal \u03b1 and M , the computational cost depends on the range of \u03b1 we search and the number of candidate models we have.\nProjective regression. Consider a projection matrix M =P =P 2 with d(= trP ) eigenvalues 1, and n\u2212d zero eigenvalues. For instance, M=\u03a6B\u22121\u03a6\u22a4 of LBFR Ex.9 is such a matrix. This implies that S\u03b1 has d eigenvalues \u03b1 and n\u2212d eigenvalues 1+\u03b1, thus detS\u03b1=\u03b1 d(1+\u03b1)n\u2212d. Let \u03c1=\u2016y\u2212y\u0302\u20162/\u2016y\u20162, then y\u22a4S\u03b1y=(\u03c1+\u03b1)y\u22a4y and\nLR\u03b1P = n 2 logy\u22a4y + n 2 log(\u03c1+ \u03b1)\u2212 d 2 log\u03b1\u2212 n\u2212d 2 log(1 + \u03b1). (11)\nSolving \u2202LR\u03b1P/\u2202\u03b1=0 w.r.t. \u03b1 we get a minimum at \u03b1=\u03b1m := \u03c1d\n(1\u2212\u03c1)n\u2212d provided that 1\u2212\u03c1>d/n. After some algebra we get\nLR\u03b1mP = n 2 logy\u22a4y\u2212 n 2 KL( d n \u20161\u2212\u03c1), where KL(p\u2016q) := plog p q +(1\u2212p)log 1\u2212p 1\u2212q (12)\nis the relative entropy or the Kullback-Leibler divergence. Note that (12) is still valid without the condition 1\u2212\u03c1>d/n (the term log((1\u2212\u03c1)n\u2212d) has been canceled\nin the derivation). What we need when using (12) is that d<n and \u03c1< 1, which are very reasonable in practice. Interestingly, if we use the penalty \u03b1\u2016y\u0302\u20162 instead of \u03b1\u2016y\u20162, the loss rank then has the same expression as (12) without any condition2.\nMinimizing LR\u03b1mP w.r.t. P is equivalent to maximizing KL( d n \u20161\u2212\u03c1). The term \u03c1 is a measure of fit. If d increases, then \u03c1 decreases and otherwise. We are seeking a tradeoff between the model complexity d and the measure of fit \u03c1, and LoRP suggests the optimal tradeoff by maximizing KL.\nTheorem 11 (LoRP for projective regression) The best projective regressor P :X n\u2192IRn\u00d7n with P =P 2 in some projective class P for data D=(x,y) is\nP best = argmax P\u2208P KL( trP (x) n \u2016y\u22a4P (x)y y\u22a4y ). (13)"}, {"heading": "4 Optimality Properties of LoRP for Variable Se-", "text": "lection\nIn the previous sections, LoRP was stated for general-purpose model selection. By restricting attention to linear regression models, we will point out in this section some theoretical properties of LoRP for variable (also called feature or attribute) selection.\nVariable selection is probably the most fundamental and important topic in linear regression analysis. At the initial stage of modeling, a large number of potential covariates are often introduced; one then has to select a smaller subset of the covariates to fit/interpret the data. There are two main goals of variable selection, one is model identification, the other is regression estimation. The former aims at identifying the true subset generating the data, while the latter aims at estimating efficiently the regression function, i.e., selecting a subset that has the minimum mean squared error loss. Note that whether or not there is a selection criterion achieving simultaneously these two goals is still an open question [Yan05, Gru\u030804]. We show that with the optimal parameter \u03b1 (defined as \u03b1m that minimizes the loss rank LR \u03b1 M in \u03b1), LoRP satisfies the first goal, while with a suitable choice of \u03b1, LoRP satisfies the second goal.\nGiven d+1 potential covariates X0\u22611,X1,...,Xd and a response variable Y , let X=x be a non-random design matrix of size n\u00d7(d+1) and y be a response vector respectively (if y and X are centered, then the covariate 1 can be omitted from the models). Denote by S={0,j1,...j|S|\u22121} the candidate model that has covariates X0,Xj1,...,Xj|S|\u22121. Under a proposed model S, we can write\ny = XS\u03b2S + \u03c3S\u01eb\n2 Then S\u03b1=(In\u2212P )\u22a4(In\u2212P )+\u03b1P\u22a4P = In+(\u03b1\u22121)P has d eigenvalues \u03b1 and n\u2212d eigenvalues 1, thus det(S\u03b1)=\u03b1 d. The loss rank LR\u03b1P = n 2 logy\n\u22a4y+ n2 log(1+(\u03b1\u22121)(1\u2212\u03c1))\u2212 d2 log\u03b1 is minimized at \u03b1m= \u03c1d (1\u2212\u03c1)(n\u2212d) . After some algebra we get the same expression of LR \u03b1m P as (12).\nwhere \u01eb is noise with expectation E[\u01eb] = 0 and covariance Cov(\u01eb) = In, \u03c3S > 0, \u03b2S = (\u03b20,\u03b2j1 ,...,\u03b2j|S|\u22121)\n\u22a4, and XS is the n\u00d7|S| design matrix obtained from X by removing the (j+1)st column for all j 6\u2208S. Model consistency of LoRP for variable selection. The ordinary least squares (OLS) fitted vector under model S is y\u0302S=MSy with MS=XS(X\u22a4SXS)\u22121X\u22a4S being a projection matrix. From Theorem 11 the best subset chosen by LoRP is\nS\u0302n = argminS LR \u03b1m S = argmaxS {KL( |S| n \u20161\u2212 \u03c1S)}, \u03c1S = \u2016y\u2212y\u0302S\u2016 2 \u2016y\u20162 .\nThe term \u03c1S is a measure of fit. It will be very close to 0 if model S is big, otherwise, it will be close to 1 if S is too small. Therefore, it is reasonable to consider only cases in which \u03c1S is bounded away from 0 and 1. In order to prove the theoretical properties of LoRP, we need the following technical assumption.\n(A) For each candidate model S, \u03c1S is bounded away from 0 and 1, i.e., there are constants c1 and c2 such that 0<c1\u2264\u03c1S \u2264c2<1 with probability 1 (w.p.1).\nLet \u03c3\u03022S=\u2016y\u2212y\u0302S\u20162/n and Snull={0}. It is easy to see that for every S 1\u2212\u03c1S = \u2016y\u0302S\u20162/\u2016y\u20162, n\u03c3\u03022S = \u03c1S\u2016y\u20162, n y\u03042 = \u2016y\u0302Snull\u20162 \u2264 \u2016y\u0302S\u20162 \u2264 \u2016y\u20162 (14) where y\u0304 denotes the arithmetic mean \u2211n\ni=1yi/n. Assumption (A) follows from\n(A\u2019) 0< lim inf n\u2192\u221e (y\u0304)2\u2264 lim sup n\u2192\u221e ( 1 n \u2016y\u20162)<\u221e and \u2200S : \u03c3\u03022S\u2192\u03c32S>0 w.p.1.\nThe first condition of (A\u2019) is obviously very mild and satisfied in almost all cases in practice. The second one is routinely used to derive asymptotic properties of model selection criteria (e.g., Theorem 2 of [Sha97] and Condition 1 of [WLT07]).\nLemma 12 (LoRP for variable selection) The loss rank of model S is LRS \u2261 LR\u03b1mS = n2 log(n\u03c3\u03022S) + n2H( |S| n ) + d 2 log 1\u2212\u03c1S \u03c1S (15) where \u03c1S and \u03c3\u03022S are defined in (14), and H(p) :=\u2212plogp\u2212(1\u2212p)log(1\u2212p) is the entropy of p. Under Assumption (A) or (A\u2019), after neglecting constants independent of S, the loss rank of model S has the form\nLRS = n 2 log \u03c3\u03022S + |S| 2 log n+OP(1), (16)\nwhere OP(1) denotes a bounded random variable w.p.1.\nProof. Inserting y\u22a4y = n\u03c3\u03022S/\u03c1S into (12) and rearranging terms gives (15). By Assumption (A) the last term in (15) is bounded w.p.1. Taylor expansion log(1\u2212p) =\u2212p+O(p2) implies H(p)/p+logp\u2192 1, hence n\n2 H( |S| n ) = |S| 2 logn+O(1).\nFinally, dropping the S-independent term n 2 logn from (15) gives (16).\nThis lemma implies that the loss rank LRS here is a BIC-type criterion, thus we immediately can state without proof the following theorem which is the well-known model consistency of BIC-type criteria (interested readers can find the routine proof in, for example, [Cha06]).\nTheorem 13 (Model consistency) Under Assumption (A) or (A\u2019), LoRP is model consistent for variable selection in the sense that the probability of selecting the true model goes to 1 for data size n\u2192\u221e.\nThe optimal regression estimation of LoRP.The second goal of model selection is often measured by the (asymptotic) mean efficiency [Shi83] which is briefly defined as follows. Let ST denote the true model (which may contain an infinite number of covariates). For a candidate model S, let Ln(S)=\u2016XST\u03b2ST\u2212XS\u03b2\u0302S\u20162 be the squared loss where \u03b2\u0302S is the OLS estimate, and Rn(S)=E[Ln(S)] be the risk. The mean efficiency of a selection criterion \u03b4 is defined by the ratio\neff(\u03b4) = infS Rn(S) E[Ln(S\u03b4)] \u2264 1\nwhere S\u03b4 is the model selected by \u03b4. \u03b4 is said to be asymptotically mean efficient if lim infn\u2192\u221eeff(\u03b4)=1.\nBy minimizing the loss rank in \u03b1 we have shown in the previous paragraph that LoRP satisfies the first goal of model selection. We now show that with a suitable choice of \u03b1, LoRP also satisfies the second goal.\nFrom (11), we have\nLR\u03b1S(y|x) = n2 log(\u03c3\u03022S + \u03b1ny\u22a4y) + n2 log n\u2212 |S| 2 log(\u03b1)\u2212 n\u2212|S| 2 log(1 + \u03b1).\nBy choosing \u03b1=\u03b1\u0303=exp(\u2212 n(n+|S|)|S|(n\u2212|S|\u22122)), under Assumption (A), the loss rank of model S (neglecting the common constant n\n2 logn) is proportional to\nLR\u03b1\u0303S(y|x) = n log \u03c3\u03022S + n(n+|S|)n\u2212|S|\u22122 + oP(1),\nwhich is the corrected AIC of [HT89]. As a result, LoRP(\u03b1\u0303) is optimal in terms of regression estimation, i.e., it is asymptotically mean efficient ([Shi83], 1983; [Sha97], 1997).\nTheorem 14 (Asymptotic mean efficiency) Under Assumption (A) or (A\u2019), with a suitable choice of \u03b1, the loss rank is proportional to the corrected AIC. As a result, LoRP is asymptotically mean efficient."}, {"heading": "5 Experiments", "text": "In this section we present a simulation study for LoRP, compare it to other methods and also demonstrate how LoRP can be used for some specific problems like choosing tuning parameters for kNN and spline regression. All experiments are conducted by using MATLAB software and the source code is freely available at http://www.hutter1.net/ai/lorpcode.zip.\nComparison to AIC and BIC for model identification. Samples are generated from the model\ny = \u03b20 + \u03b21X1 + ...+ \u03b2dXd + \u01eb, \u01eb \u223c N(0, \u03c32) (17)\nwhere \u03b2 is the vector of coefficients with some zero entries. Without loss of generality, we assume that \u03b20=0, otherwise, we can center the response vector y and standardize the design matrix X to exclude \u03b20 from the model. We shall compare the performance of LoRP to that of BIC and AIC with various factors n, d and signal-to-noise ratio (SNR) which is \u2016\u03b2\u20162/\u03c32 (\u2016\u03b2\u20162 is often called the length of the signal).\nFor a given set of factors (n, d, SNR), the way we simulate a dataset from model (17) is as follows. Entries ofX are sampled from a uniform distribution on [\u22121,1]. To generate \u03b2, we first create a vector u=(u1,...,ud)\n\u22a4 whose entries are sampled from a uniform distribution on [\u22121,1]. The number of true covariates d\u2217 is randomly selected from {1,2,...,d}, the last d\u2212d\u2217 entries of u are set to zero, then coefficient vector \u03b2 is computed by \u03b2i = {length of signal}\u2217ui/||u||. In our simulation, the length of signal was fixed to be 10. n observation errors \u01eb1,...,\u01ebn are sampled from a normal distribution with mean 0 and variance \u03c32=||\u03b2||2/SNR. Finally, the response vector is computed by y=X\u03b2+\u01eb. For each set of factors (n, d, SNR), 1000 datasets are simulated in the same manner to assess the average performance of the methods. For simplicity, a candidate model is specified by its order, i.e., we search the best model among only d models {1},{1,2}...,{1,2,...,d}. For the general case, an efficient branch-and-bound algorithm [Mil02, Chp.3] can be used to exhaustively search for the best subsets.\nTable 1 presents percentages of correctly-fitted models with various factors n, d and SNR. As shown, LoRP outperforms the others. The better performance of LoRP over BIC, which is the most popular criterion for model identification, is very encouraging. This is probably because LoRP is a selection criterion with a datadependent penalty. This improvement needs a theoretical justification which we intend to do in the future.\nComparison to AIC and BIC for regression estimation. Consider the following model which is from [Shi83]\ny = y(x) = log 1 1\u2212x + \u01eb, \u01eb \u223c N(0, \u03c32), x \u2208 [0, 1). (18)\nWe approximate the true function by a Fourier series and consider the problem of choosing a good order among models\ny = \u03b20 +\nk\u22121 \u2211\nl=1\ncos(\u03c0lx/\u03b4) l+1 \u03b2l + \u01eb, k = 1, ..., K.\nIn the present context, a model in Section 4 is completely specified by the order K of the Fourier series. Samples are created from (18) at the points xi=\u03b4 i n+1\n, i=1,...,n. As in [Shi83], we take \u03b4= .99, and K=163 with various n and \u03c3. The performance is measured by the estimate of mean efficiency over 1000 replications.\nTable 2 represents the simulation results. In general, LoRP (with \u03b1= \u03b1\u0303 as in Section 4) outperforms the others, except for cases with unrealistically high noise level. For cases with high noise, mean efficiency of BIC is often larger than that of AIC and LoRP. This was also shown in the simulation study of [Shi83], Table 1. This phenomenon can be explained as follows.\nThe risk of model k (the model specified by its order k) is Rn(k) = \u2016(I\u2212 Mk)ytrue\u20162+k\u03c32 where Mk is the regression matrix under model k and ytrue is the vector of true values y(xi). When \u03c3\u2192\u221e, the ideal k\u22c6=arginfkRn(k)\u21921. Because BIC penalizes the model complexity more strongly than AIC and LoRP do, the order chosen by BIC is closer to k\u22c6=1 than the ones chosen by AIC and LoRP. As a result, mean efficiency of BIC is larger than that of the others.\nLoRP for selecting a good number of neighbors in kNN. Let us now see how LoRP can be applied to select a good parameter k in kNN regression.\nWe created a dataset of n=100 observations (xi,yi) from the model:\ny = f(x) + \u03b5, with f(x) = sin(12(x+0.2)) x+0.2 , x \u2208 [0, 1] (19)\nwhere \u03b5\u223cN(0,\u03c32) with \u03c3=0.5. The regression matrix M (k) for kNN regression is determined by M\n(k) ij = 1 k if j\u2208Nk(xi) and 0 else. Then, the loss rank is\nLR(k) = inf \u03b1\u22650\n{n 2 log(y\u22a4S(k)\u03b1 y)\u2212 12 log detS(k)\u03b1 },\nwhere S (k) \u03b1 = (I\u2212M (k))\u22a4(I\u2212M (k))+\u03b1I. The most widely-used method to select a good k is probably Generalized Cross-Validation (GCV) [CW79]: GCV(k)=n\u2016(I\u2212 M (k))y\u20162/[tr(I\u2212M (k))]2. To judge how well GCV and LoRP work, we compare them to the expected prediction error defined as\nEPE(k) =\nn \u2211\ni=1\nE(yi \u2212 y\u0302i)2 = n \u2211\ni=1\n[\n\u03c32 + (f(xi)\u2212 1k \u2211\nj\u2208Nk(xi) f(xj))\n2 + \u03c3 2\nk\n]\n.\nFigure 1(a) shows the curves LR(k), GCV(k), EPE(k) for k = 2,...,20 (the trivial case k=1 is omitted), in which k=7-nearest neighbors is chosen by LoRP and k=8 is chosen by GCV. The \u201cideal\u201d k is 5. Both LoRP and GCV do a reasonable job. LoRP works slightly better than GCV.\nLoRP for selecting a good smoothing parameter. We now further demonstrate the use of LoRP in selecting a good smoothing parameter for spline regression. Consider the following problem: find a function belonging to the class of functions with continuous 2nd derivative that minimizes the following penalized residual sum\nof squares:\nRSS(f) = n \u2211\ni=1\n(yi \u2212 f(xi))2 + \u03bb \u222b (f \u2032\u2032(t))2dt,\nwhere \u03bb is called the smoothing parameter. The second term penalizes the curvature of function f and the smoothing parameter \u03bb controls the amount of penalty. Our goal is to choose a good \u03bb.\nIt is well known (see, e.g., [HTF01], Section 5.4) that the solution is a natural spline f(x)=\n\u2211n j=1Nj(x)\u03b8j where N1(x),...,Nn(x) are the basis functions of the\nnatural cubic spline:\nN1(x) = 1, N2(x) = x, Nk+2(x) = dk(x)\u2212 dn\u22121(x) with dk(x) = (x\u2212xk) 3 +\u2212(x\u2212xn)3+ xn\u2212xk .\nThe problem thus reduces to finding a vector \u03b8\u2208IRn that minimizes\nRSS(\u03b8) = (y \u2212N\u03b8)\u22a4(y \u2212N\u03b8) + \u03bb\u03b8\u22a4\u2126\u03b8\nwhere Nij =Nj(xi) and \u2126ij = \u222b N \u2032\u2032i (x)N \u2032\u2032 j (x)dx. It is easy to see that the solution is \u03b8\u0302\u03bb = (N \u22a4N+\u03bb\u2126)\u22121N\u22a4y, and the fitted vector is y\u0302 = N \u03b8\u0302\u03bb = M\u03bby with M\u03bb = N(N\u22a4N+\u03bb\u2126)\u22121N\u22a4y. The fitted vector is linear in y, thus the loss rank is\nLR(\u03bb) = argmin \u03b1\u22650\n{n 2 log(y\u22a4S\u03b1\u03bby)\u2212 12 log detS\u03b1\u03bb}\nwhere S\u03b1\u03bb =(I\u2212M\u03bb)\u22a4(I\u2212M\u03bb)+\u03b1I. Let us consider again the dataset generated from model (19). Figure 1(b) shows the curves LR(\u03bb), GCV(\u03bb) and EPE(\u03bb). The derivation of expressions for GCV(\u03bb) and EPE(\u03bb) is similar to the previous example. \u03bb\u2248 3\u00d710\u22124 is the optimal value selected by the \u201cideal\u201d criterion EPE. \u03bb\u22485\u00d710\u22124 and \u03bb\u22487\u00d710\u22124 are selected by LoRP and GCV, respectively. One again, like the previous example, LoRP selects a better \u03bb than GCV does."}, {"heading": "6 Comparison to Gaussian Bayesian Linear Re-", "text": "gression\nWe now consider LBFR from a Bayesian perspective with Gaussian noise and prior, and compare it to LoRP. In addition to the noise model as in PML, one also has to specify a prior. Bayesian model selection (BMS) proceeds by selecting the model that has largest evidence. In the special case of LBFR with Gaussian noise and prior and a type II maximum likelihood estimate for the noise variance, the expression for the evidence has a similar structure as the expression of the loss rank.\nGaussian Bayesian LBFR / MAP. Recall from Sec.3 Ex.9 that Fd is the class of functions fw(x)=w \u22a4\u03c6(x) (w\u2208IRd) that are linear in feature vector \u03c6. Let\nGaussN(z|\u00b5, \u03c3) := exp(\u22121 2 (z \u2212 \u00b5)\u22a4\u03c3\u22121(z \u2212 \u00b5)) (2\u03c0)N/2 \u221a det \u03c3\n(20)\ndenote a general N -dimensional Gaussian distribution with mean \u00b5 and covariance matrix \u03c3. We assume that observations y are perturbed from fw(x) by independent additive Gaussian noise with variance \u03b2\u22121 and zero mean, i.e., the likelihood of y under model w is P(y|w) =Gaussn(y|\u03a6w,\u03b2\u22121I), where \u03a6ia =\u03c6a(xi). A Bayesian assumes a prior (before seeing y) distribution on w. We assume a centered Gaussian with covariance matrix (\u03b1C)\u22121, i.e., P(w)=Gaussd(w|0,\u03b1\u22121C\u22121). From the prior and the likelihood one can compute the evidence and the posterior\nEvidence: P(y) =\n\u222b\nP(y|w)P(w)dw = Gaussn(y|0, \u03b2\u22121S\u22121) (21)\nPosterior: P(w|y) = P(y|w)P(w)/P (y) = Gaussd(w|w\u0302, A\u22121)\nB := \u03a6\u22a4\u03a6, A := \u03b1C + \u03b2B, M := \u03b2\u03a6A\u22121\u03a6\u22a4, S := I \u2212M, (22) w\u0302 := \u03b2A\u22121\u03a6\u22a4y, y\u0302 := \u03a6w\u0302 = My\nA standard Bayesian point estimate for w for fixed d is the one that maximizes the posterior (MAP) (which in the Gaussian case coincides with the mean) w\u0302 = argmaxwP(w|y) = \u03b2A\u22121\u03a6\u22a4y. For \u03b1 \u2192 0, MAP reduces to Maximum Likelihood (ML), which in the Gaussian case coincides with the least squares regression of Ex.9. For \u03b1>0, the regression matrix M is not a projection anymore.\nBayesian model selection. Consider now a family of models {F1,F2,...}. Here the Fd are the linear regressors with d basis functions, but in general they could be completely different model classes. All quantities in the previous paragraph implicitly depend on the choice of F , which we now explicate with an index. In particular, the evidence for model class F is PF (y). BMS chooses the model class (here d) F of highest evidence:\nFBMS = argmax F PF(y)\nOnce the model class FBMS is determined, the MAP (or other) regression function fw\nFBMS or MFBMS are chosen. The data variance \u03b2 \u22121 may be known or estimated from the data, C is often chosen I, and \u03b1 has to be chosen somehow. Note that while \u03b1\u21920 leads to a reasonable MAP=ML regressor for fixed d, this limit cannot be used for BMS.\nComparison to LoRP. Inserting (20) into (21) and taking the logarithm we see that BMS minimizes\n\u2212 log PF (y) = \u03b22y\u22a4Sy \u2212 12 log detS \u2212 n2 log \u03b2 2\u03c0\n(23)\nw.r.t. F . Let us estimate \u03b2 by ML: We assume a broad prior \u03b1\u226a\u03b2 so that \u03b2 \u2202S \u2202\u03b2 = O(\u03b1 \u03b2 ) can be neglected. Then \u2212\u2202logPF (y) \u2202\u03b2 = 1 2 y\u22a4Sy\u2212 n 2\u03b2 +O(\u03b1 \u03b2 n) = 0 \u21d4 \u03b2 \u2248 \u03b2\u0302 := n/(y\u22a4Sy). Inserting \u03b2\u0302 into (23) we get\n\u2212 log PF(y) = n2 logy\u22a4Sy \u2212 12 log detS \u2212 n2 log n2\u03c0e (24)\nTaking an improper prior P(\u03b2)\u221d\u03b2\u22121 and integrating out \u03b2 leads for small \u03b1 to a similar result. The last term in (24) is a constant independent of F and can be ignored. The first two terms have the same structure as in linear LoRP (10), but the matrix S is different. In both cases, \u03b1 act as regularizers, so we may minimize over \u03b1 in BMS like in LoRP. For \u03b1=0 (which neither makes sense in BMS nor in LoRP), M in BMS coincides with M of Ex.9, but still the S0 in LoRP is the square of the S in BMS. For \u03b1>0, M of BMS may be regarded as a regularized regressor as suggested in Sec.2 (a), rather than a regularized loss function (b) used in LoRP. Note also that BMS is limited to (semi)parametric regression, i.e., does not cover the non-parametric kNN Ex.2 and kernel Ex.8, unlike LoRP.\nSince B only depends on x (and not on y), and all P are implicitly conditioned on x, one could choose C = B. In this case, M = \u03b3\u03a6B\u22121\u03a6\u22a4, with \u03b3 = \u03b2\n\u03b1+\u03b2 < 1\nfor \u03b1> 0, is a simple multiplicative regularization of projection \u03a6B\u22121\u03a6\u22a4, and (24) coincides with (11) for suitable \u03b1, apart from an irrelevant additive constant, hence minimizing (24) over \u03b1 also leads to (12)."}, {"heading": "7 Comparison to other Model Selection Schemes", "text": "In this section we give a brief introduction to PML for (semi)parametric regression, and its major instantiations, AIC, BIC, and MDL principle, whose penalty terms are all proportional to the number of parameters d. The effective number of parameters is often much smaller than d, e.g., if there are soft constraints like in ridge regression. We compare MacKay\u2019s trace formula [Mac92] for Gaussian Bayesian LBFR and Hastie\u2019s et al. trace formula [HTF01] for general linear regression with LoRP.\nPenalized ML (AIC, BIC, MDL). Consider a d-dimensional stochastic model class like the Gaussian Bayesian linear regression example of Section 6. Let Pd(y|w) be the data likelihood under d-dimensional model w\u2208IRd. The maximum likelihood (ML) estimator for fixed d is\nw\u0302 = argmax w Pd(y|w) = argmin w {\u2212 log Pd(y|w)} (25)\nSince \u2212logPd(y|w) decreases with d, we cannot find the model dimension by simply minimizing over d (overfitting). Penalized ML adds a complexity term to get reasonable results\nd\u0302 = argmin d {\u2212 log Pd(y|w\u0302) + Penalty(d)} (26)\nThe penalty introduces a tradeoff between the first and second term with a minimum at d\u0302<\u221e. Various penalties have been suggested: AIC [Aka73] uses d, BIC [Sch78] and the (crude) MDL [Ris78, Gru\u030804] use d\n2 logn for Penalty(d). There are at least\nthree important conceptual differences to LoRP:\n\u2022 In order to apply PML one needs to specify not only a class of regression functions, but a full probabilistic model Pd(y|w),\n\u2022 PML ignores or at least does not tell how to incorporate a potentially given loss-function, \u2022 PML is mostly limited to selecting between (semi)parametric models.\nWe discuss two approaches to the last item in the remainder of this section (where AIC, BIC, and MDL are not directly applicable): (a) for non-parametric models like kNN or kernel regression, or (b) if d does not reflect the \u201ctrue\u201d complexity of the model. [Mac92] suggests an expression for the effective number of parameters deff as a substitute for d in case of (b), while [HTF01] introduce another expression which is applicable for both (a) and (b).\nThe trace penalty for parametric Gaussian LBFR. We continue with the Gaussian Bayesian linear regression example (see Section 6 for details and notation). Performing the integration in (21), [Mac92, Eq.(21)] derives the following expression for the Bayesian evidence for C=I\n\u2212 log P(y) = (\u03b1E\u0302W + \u03b2E\u0302D) + (12 log detA\u2212 d2 log\u03b1)\u2212 n2 log \u03b2 2\u03c0\n(27)\nE\u0302D = 1 2 \u2016\u03a6w\u0302 \u2212 y\u201622, E\u0302W = 12\u2016w\u0302\u201622\n(the first bracket in (27) equals \u03b2 2 y\u22a4Sy and the second equals \u22121 2 logdetS, cf. (23)). Minimizing (27) w.r.t. \u03b1 leads to the following relation:\n0 = \u2212\u2202 log P(y) \u2202\u03b1 = E\u0302W + 1 2 trA\u22121 \u2212 d 2\u03b1 ( \u2202 \u2202\u03b1 log detA = trA\u22121)\nHe argues that \u03b1\u2016w\u0302\u201622 corresponds to the effective number of parameters, hence\ndMcKeff := \u03b1\u2016w\u0302\u201622 = 2\u03b1E\u0302W = d\u2212 \u03b1trA\u22121 (28)\nThe trace penalty for general linear models. We now return to general linear regression y\u0302=M(x)y (7). LBFR is a special case of a projection matrix M =M2 with rank d = trM being the number of basis functions. M leaves d directions untouched and projects all other n\u2212d directions to zero. For general M , [HTF01, Sec.5.4.1] argue to regard a direction that is only somewhat shrunken, say by a factor of 0< \u03b2 < 1, as a fractional parameter (\u03b2 degrees of freedom). If \u03b21,...,\u03b2n are the shrinkages = eigenvalues of M , the effective number of parameters could be defined as [HTF01, Sec.7.6]\ndHTFeff :=\nn \u2211\ni=1\n\u03b2i = trM,\nwhere HTF stands for Hastie-Tibshirani-Friedman, which generalizes the relation d=trM beyond projections. For MacKay\u2019s M (22), trM=d\u2212\u03b1trA\u22121, i.e., dHTFeff is consistent with and generalizes dMcKeff .\nProblems. Though nicely motivated, the trace formula is not without problems. First, since for projections, M=M2, one could have argued equally well for dHTFeff =\ntrM2. Second, for kNN we have trM= n k (since M is 1 k on the diagonal), which does not look unreasonable. Consider now kNN\u2019, which is defined as follows: we average over the k nearest neighbors excluding the closest neighbor. For sufficiently smooth functions, kNN\u2019 for suitable k is still a reasonable regressor, but trM=0 (since M is zero on the diagonal). So dHTFeff =0 for kNN\u2019, which makes no sense and would lead one to always select the k=1 model.\nRelation to LoRP. In the case of kNN\u2019, trM2 would be a better estimate for the effective dimension. In linear LoRP, \u2212logdetS\u03b1 serves as complexity penalty. Ignoring the nullspace of S0=(I\u2212M)\u22a4(I\u2212M) (8), we can Taylor expand \u221212 logdetS0 in M\n\u22121 2 log detS0 = \u2212tr log(I\u2212M) =\n\u221e \u2211\ns=1\n1 s tr(Ms) = trM + 1 2 trM2 + ...\nFor BMS (24) with S= I\u2212M (22) we get half of this value. So the trace penalty may be regarded as a leading order approximation to LoRP. The higher order terms prevent peculiarities like in kNN\u2019.\nCoding/MDL interpretation of LoRP. The basic idea of MDL is as follows [Gru\u030804]: \u201cThe goal of statistical inferences may be cast as trying to find regularity in the data. \u2018Regularity\u2019 may be identified with \u2018ability to compress\u2019. MDL combines these two insights by viewing learning as data compression: it tells us that, for a given set of hypotheses H and data set D, we should try to find the hypothesis or combination of hypotheses in H that compress D most.\u201d\nThe standard incarnation of (crude) MDL is as follows: If H is a stochastic model of (discrete) data D, we can code D (by Shannon-Fano) in \u2308\u2212log2P(D|H)\u2309 bits. If we have a class of models H, we also have to code H (somehow in, say, L(H) bits) in order to be able to decode D. MDL chooses the hypothesis HMDL= argminH\u2208H{\u2212log2P(D|H)+L(H)} of minimal two-part code. For instance, if H is the class of all polynomials of all degrees with each coefficient coded to 1\n2 log2n bits\n(i.e., O(n\u22121/2) accuracy) and we condition on x, i.e., D;y|x, MDL takes the form (25) and (26), i.e., HMDL=(w\u0302,d\u0302).\nWe now give LoRP (for discrete D) a data compression/MDL interpretation. For simplicity, we will first assume that all loss values are different, i.e., if Lossr(y\n\u2032|x) 6=Lossr(y\u2032\u2032|x) for y\u2032 6= y\u2032\u2032 (adding infinitesimal random noise to Lossr easily ensures this). In this case, Rankr(\u00b7|x) :Yn\u2192 IN is an order preserving bijection, i.e., Rankr(y\n\u2032|x)<Rankr(y\u2032\u2032|x) iff Lossr(y\u2032|x)<Lossr(y\u2032\u2032|x) with no gaps in the range of Rankr(\u00b7|x).\nPhrased differently, Rankr(\u00b7|x) codes each y\u2032 \u2208 Yn as a natural number m in increasing loss-order. The natural number m can itself be coded in \u2308log2m\u2309 bits (using plain not prefix coding). Let us call this code of y\u2032 the Loss Rank Code (LRC). LRC has a nice characterization: LRC is the shortest loss-order preserving code. Ignoring the rounding, the Length of LRCr(y \u2032|x) is LRr(y\u2032|x):\nProposition 15 (Minimality property) If all loss values are different, i.e., if\nLossr(y \u2032|x) 6= Lossr(y\u2032\u2032|x) for all y\u2032 6= y\u2032\u2032\nthen the loss rank (code) of y is the smallest/shortest among all loss-order preserving rankings/codes C in the sense that\nRank(y) = min{C(y) : C \u2208 Yn\u2192IN \u2227 (\u22c6) } \u230aLR(y)/ log 2\u230b = min{Length(C(y)) : C \u2208 Yn\u2192{0, 1}\u2217 \u2227 (\u22c6) }\n(\u22c6) := [Loss(y\u2032) < Loss(y\u2032\u2032) \u21d4 C(y\u2032) < C(y\u2032\u2032), \u2200y\u2032,y\u2032\u2032]\nThe proof follows from the fact that if a discrete injection (code) is order preserving, there exists a \u201csmallest\u201d one without gaps in the range. So LoRP minimizes the Loss Rank Code, where LRC itself is the shortest among all loss-order preserving codes. From this perspective, LoRP is just a different (non-stochastic, non-parametric, loss-based) incarnation of MDL. The MDL philosophy provides a justification of LoRP (2), its regularization (5), and loss function selection (Section 8). This identification should also allow to apply or adapt the various consistency results of MDL, implying that LoRP is consistent under some mild conditions.\nIf some losses are equal, Rankr(\u00b7|x) :Yn\u2192IN still preserves the order \u2264, but the mapping is neither surjective nor injective anymore.\nLarge regression classes R. The classes R of regressors we considered so far were discrete and \u201csmall\u201d, often indexed by an integer complexity index (like k in kNN or d in LBFR). But large classes are also thinkable.\nAs an extreme case, consider the class of all regressors. Clearly, there is an r=rD which \u201cknows\u201d D and perfectly fits D (r(xi|D)= yi, \u2200i), but is the worst possible on all other D\u2032 (r(xi|D\u2032)=\u221e, \u2200i, \u2200D\u2032 6=D). This r has (discrete) Rank 1, so is best according to LoRP. So if R is too large, LoRP can overfit too.\nConsider a more realistic example by not taking all of the first d basis functions in LBFR, but selecting some basis functions \u03c6i1 ,...,\u03c6id, i.e., R is indexed by d integers, and d may be variable too.\nOne solution approach is to group more regressors in R into one function class F , e.g., the class of functions Fk,d={w1\u03c6i1+...wd\u03c6id :w\u2208IRd, 1\u2264i1<...<id\u2264k} that are linear in d of the first k bases. Now R is a small class indexed by d and k only.\nLooking at the coding interpretation of LRr and the MDL philosophy, suggests to assign a code to r\u2208IR in order to get a complete code for D:\nrbest = argmin r {LRr(y|x) + L(r)}\nwhere r is the length of a code for r (given R). For R \u2243 IN a single integer has to be coded, e.g., k in L(r) = L(k)\u2248 log2k bits, which can usually be safely dropped/ignored. For more complex classes like the (ungrouped) LBFR subset selection above, L(r)=L(i1,...,id,d)\u2248dlog2k+log2d can become important."}, {"heading": "8 Loss Functions and their Selection", "text": "General additive loss. Linear LoRP y\u0302=M(x)y of Section 3 can easily be generalized to non-quadratic loss. Let us consider the \u03c1>0 loss\nLossM(y|x) := ( \u2211n i=1(yi \u2212 y\u0302i)\u03c1) 1/\u03c1 = \u2016y \u2212 y\u0302\u2016\u03c1 = \u2016(I\u2212M)y\u2016\u03c1\nV (L) = {y\u2032 \u2208 IRn : \u2016(I\u2212M)y\u2032\u2016\u03c1 \u2264 L} = {(I\u2212M)\u22121z \u2208 IRn : \u2016z\u2016\u03c1 \u2264 L} Let v\u03c1n := |{z \u2208 IRn : \u2016z\u2016\u03c1 \u2264 1}| = 2n \u220fn\u22121 i=1 i \u03c1 !1 \u03c1 !/ i+1 \u03c1 !,\nwhere i \u03c1 ! :=\u0393( i \u03c1 +1), be the volume of the unit d-dimensional \u03c1-norm \u201cball\u201d. Since V (L) is a linear transformation of this ball with transformation matrix (I\u2212M)\u22121 and scaling L, we have |V (L)|=v\u03c1nLn/det(I\u2212M), hence\nLRM(y|x) = log |V (LossM(y|x))| = n log \u2016(I\u2212M)y\u2016\u03c1 \u2212 log det(I\u2212M) + log v\u03c1n (29) For the \u03c1=2 norm, (29) reduces to LR0M (9). Note that LossM :=g(\u2016y\u2212y\u0302\u2016\u03c1) leads to the same result (29) for any monotone increasing g, i.e., only the order of the loss matters, not its absolute value. More generally LossM = g( \u2211\nih(yi\u2212 y\u0302i)) for any h implies\nLRM(y|x) = n log vhn( \u2211 i h(yi \u2212 y\u0302i))\u2212 log det(I\u2212M), where vhn(l) := |{z \u2208 IRn : \u2211 i h(zi) \u2264 l}| 1/n\nis a one-dimensional function of l (independent D and M), once to be determined (e.g., vhn(l) = l \u00b7(v\u03c1n)\n1/n \u221d l for \u03c1-norm loss). Regularization may be performed by M;\u03b3M with optimization over \u03b3<1.\nLoss-function selection. In principle, the loss function should be part of the problem specification, since it characterizes the ultimate goal. For instance, whether a test should more likely classify a healthy person as sick than a sick person as healthy, depends on the severity of a misclassification (loss) in each direction. In reality, though, having to specify the loss function can be a nuisance. Sure, the loss has to respect some general features, e.g., that it increases with the deviation of y\u0302i from yi. Otherwise it is chosen by convenience or rules of thumb, rather than by elicitation of the real goal, for instance preferring the Euclidean norm over \u03c1 6= 2 norms. If we subscribe to the procedure of choosing the loss function, we could ask whether this may be done in a more principled way. Consider a (not too large) class of loss functions Loss\u03b1, indexed by some parameter \u03b1. For instance, Loss\u03b1= \u2016y\u2212y\u0302\u2016\u03b1 from the previous paragraph. The regularized loss (5) also constitutes a class of losses. In this case we minimized over the regularization parameter \u03b1. This suggests to choose in general the loss function that has minimal loss rank LR\u03b1r . The justifications are similar to the ones for minimizing LR\u03b1r w.r.t. r. Note that the term logv\u03c1n cannot be dropped anymore, unlike in (10)."}, {"heading": "9 Self-Consistent Regression", "text": "So far we have considered only \u201con-data\u201d regression. LoRP only depends on the regressor r on data D and not on x 6\u2208 {x1,...,xn}. We now construct canonical regressors for off-data x from regressors given only on-data. First, this may ease the specification of the regression functions, second, it is a canonical way for interpolation (LoRP can\u2019t distinguish between r that are identical on D), and third, we show that many standard regressors (kNN, Kernel, LBFR) are self-consistent in the sense that they are canonical. We limit our exposition to linear regression.\nOff-data regression. A linear regressor is completely determined by the n functions mj (6), but not by the matrix function M (7). Indeed, two sets {mj} and {m\u2032j} that coincide on D=(x,y), i.e. mj(xi|x)=m\u2032j(xi|x) \u2200i,j but possibly differ for x 6\u2208x, lead to the same matrix Mij(x)=mj(xi|x)=m\u2032j(xi|x). LoRP has the advantage of only depending on M , but this also means that it cannot distinguish between an mj that behaves well on x 6\u2208x and one that, e.g., wildly oscillates outside x.\nTypically, the mj are given and, provided the model complexity is chosen appropriately e.g. by LoRP, they properly interpolate x. Nevertheless, a canonical extension from M to mj would be nice. In this way LoRP would not be vulnerable to bad mj , and we could interpolate D (predict y for any x\u2208X ) even without mj given a-priori.\nWe define a self-consistent regression scheme based only on M (for all n). We ask for an estimate y\u0302 of y for x 6\u2208x. We add a virtual data point (x0,y0) to D, where x0=x. If we knew y0= y we could estimate y\u03020= r(x0|{(x0,y0)}\u222aD), but we don\u2019t know y0. But we could require a self-consistency condition, namely that y\u03020=y0 for x0 6\u2208x.\nDefinition 16 (canonical and self-consistent regressors) Let M \u2032ij(x \u2032)0\u2264i,j\u2264n be the regression matrix for the data set D\u2032={(x0,y0)}\u222aD=((x0,x),(y0,y))=(x\u2032,y\u2032) of size n+1.\n(i) A linear regressor y\u03030 = r\u0303(x0|D) is called a canonical regressor for M \u2032 if the consistency condition y\u03030=r(x0|D\u2032)\u2261 \u2211n j=0M \u2032 0jyj holds \u2200x0,D.\n(ii) A regressor r is called self-consistent if r\u0303=r, i.e. if r(x0|{(x0,r(x0|D))}\u222aD)=r(x0|D) \u2200x0,D.\n(iii) A class of regressors R={r} is called self-consistent if R\u0303={r\u0303}\u2286R.\nWe denote the solution of the self-consistency condition y0= \u2211n j=0M \u2032 0jyj by y\u03030.\nSo we have to solve\ny\u03030 =\nn \u2211\nj=1\nM \u20320jyj +M \u2032 00y\u03030 =\u21d2 y\u03030 =\n\u2211n j=1M \u2032 0jyj\n1\u2212M \u203200 =\n\u2211n j=1M \u2032 0jyj\n\u2211n j=1M \u2032 0j\nwhere the last equality only holds if \u2211n j=0M \u2032 0j =1, which is often the case, in particular for kNN and Kernel regression, but not necessarily for LBFR.\nProposition 17 (canonical regressor) The linear regressor\ny0 = r\u0303(x0|D) := n \u2211\nj=1\nm\u0303j(x0|x)yj, where m\u0303j(x0|x) := M \u20320j(x \u2032)\n1\u2212M \u203200(x\u2032)\nis the unique canonical regressor for M \u2032 (if M \u203200<1).\nExample 18 (self-consistent kNN, \u2191Ex.2) M \u20320j(x\u2032) = 1k for xj \u2208 N \u2032k(x0) and 0 else. The k nearest neighbors N \u2032k(x0) of x0 among x\u2032 consist of x0 and the k\u22121 nearest neighbors Nk\u22121(x0)=:J of x0 among x, i.e. N \u2032k(x0)={x0}\u222aNk\u22121(x0). Hence\ny\u03030 =\n\u2211n j=1M \u2032 0jyj\n\u2211n j=1M \u2032 0j\n=\n\u2211\nj\u2208J 1 k yj\n\u2211\nj\u2208J 1 k\n= \u2211\nj\u2208J\n1 k\u22121yj =\nn \u2211\nj=1\nM (k\u22121) 0j yj = rk\u22121(x0|D) = y\u03020\nCanonical kNN is equivalent to standard (k\u20131)NN, so the class of canonical kNN regressors coincides with the standard kNN class. \u2666 Example 19 (self-consistent kernel)\nM \u20320j(x \u2032) = K(x0, xj) \u2211n\nj=0K(x0, xj) =\u21d2 y\u03030 =\n\u2211n j=1K(x0, xj)yj\n\u2211n j=1K(x0, xj)\n= r(x0|D) = y\u03020\nCanonical kernel regression coincides with the standard kernel smoother. \u2666 Example 20 (self-consistent LBFR)\nB\u2032 = n \u2211\ni=0\n\u03c6(xi)\u03c6(xi) \u22a4= B + \u03c6(x0)\u03c6(x0) \u22a4\n\u21d2 M \u20320j = \u03c6(x0)\u22a4B\u2032\u22121\u03c6(xj) = \u03c6(x0)\u22a4 [ B\u22121 \u2212 B \u22121\u03c6(x0)\u03c6(x0)\u22a4B\u22121\n1 + \u03c6(x0)\u22a4B\u22121\u03c6(x0)\n]\n\u03c6(xj)\n= M0j \u2212 M00M0j 1 +M00 = M0j 1 +M00 \u21d2 1\u2212M \u203200 =\n1\n1 +M00\nIn the first line we used the Sherman-Morrison formula for inverting B\u2032. In the second line we defined M0j=\u03c6(x0) \u22a4B\u22121\u03c6(xj), extending M .\n\u21d2 y\u03030 = \u2211n j=1M \u2032 0jyj\n1\u2212M \u203200 =\nn \u2211\nj=1\nM0jyj =\nn \u2211\nj=1\nmj(x0,x)yj = y\u03020\nCanonical LBFR coincides with standard LBFR. \u2666\nProposition 21 (self-consistent regressors) Kernel regression and linear basis function regression are self-consistent. kNN is not self-consistent but the class of kNN regressors R={rkNN :k\u2208IN} is self-consistent.\nTo summarize, we expect LoRP to select good regressors with proper interpolation behavior for canonical and self-consistent regressors."}, {"heading": "10 Nearest Neighbors Classification", "text": "We now consider k-nearest neighbors classification in more detail. In order to get more insight into LoRP we seek a case that allows analytic solution. In general, the determinant detS\u03b1 cannot be computed analytically, but for x lying on a hypercube of the regular grid X =ZZd we can. We derive exact expressions, and consider the limits n\u2192\u221e, k\u2192\u221e, and d\u2192\u221e.\nkNN on one-dimensional grid. We consider the d=1 dimensional case first. We assume x= (1,2,3,...,n), a circular metric d(xi,xj) = d(i,j) =min{|i\u2212j|,n\u2212|i\u2212j|}, and odd k\u2264n. The kNN regression matrix\nMij = bi\u2212j with bi\u2212j = 1 k if d(i, j) \u2264 k\u22121 2 and 0 otherwise\nis a diagonal-constant (Toeplitz) matrix with circularity property bi\u2212j=bi\u2212j+n. For instance, for k=3 and n=5\nM = 1\n3\n\n 1 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 1 1\n\n\nFor every circulant matrix, the eigenvectors v1,...,vn are waves vlj = \u03b8 jl with \u03b8 = e2\u03c0 \u221a \u22121/n. The eigenvalues are the fourier transform b\u0302l = \u2211n j=1bj\u03b8\n\u2212jl of b, since \u2211\njMijv l j=\n\u2211\njbi\u2212j\u03b8 jl=\n\u2211 jbj\u03b8 (i\u2212j)l=vli \u2211 jbj\u03b8 \u2212jl= b\u0302lvli, where we exploited circularity\nof b and \u03b8jl. For MkNN in particular we get\nb\u0302l =\n\u2191 circularity\n1\nk\nk\u22121 2 \u2211\nj=\u2212 k\u22121 2\n\u03b8\u2212jl =\n\u2191 geometric sum\n1\nk \u03b8lk/2 \u2212 \u03b8\u2212lk/2 \u03b8l/2 \u2212 \u03b8\u2212l/2 =\u2191\ninsert \u03b8\nsin(\u03c0lk/n) k sin(\u03c0l/n) < 1 for l 6= n\nand b\u0302n=1. The only 1-vector v n=1 corresponds to a constant shift yi;yi+c under which kNN (like many other regressors) is invariant. Instead of regularizing LoRP with \u03b1> 0 we can restrict V (L)\u2282 IRn to the space orthogonal to vn, which means dropping b\u0302n=1 in the determinant. Intuitively, since this invariant direction is the same for all k, we can drop the same additive infinite constant from LR for every k, which is irrelevant for comparisons (formally we should compute lim\u03b1\u21920{LR\u03b1k1\u2212 LR\u03b1k2}). The exact expression for the restricted log-determinant (denoted by a prime) is\n\u22121 2 log det\u2032S0 = \u2212 log det\u2032(1\u2212M) = \u2212\nn\u22121 \u2211\nl=1\nlog(1\u2212b\u0302l) =: nk c1nk = c1nktrM\nFor large n (and large k) the expression can be simplified. The exact, large n, and\nlarge k\u226an expressions are\nc1nk = \u2212 k\nn\nn\u22121 \u2211\nl=1\nlog ( 1\u2212 sin(\u03c0lk/n) k sin(\u03c0l/n) )\nc1\u221ek = \u2212 k\n\u03c0\n\u222b \u03c0/2\n\u2212\u03c0/2 log\n( 1\u2212 sin(kz) k sin(z) ) dz\n(\nz = \u03c0l/n for l < n 2\nz = \u03c0l/n\u2212 \u03c0 else\n)\nc1\u221e\u221e = \u2212 1\n\u03c0\n\u222b \u221e\n\u2212\u221e log\n( 1\u2212 sin t t ) dt =\u0307 3.202 (t = kz, sin(z) \u223c z)\nFurther, c1\u221e3=3log3=\u03073.295. Since c 1 \u221ek is decreasing in k, c 1 \u221ek equals 3.2 within 3% for all k.\nkNN on d-dimensional grid. We now consider x = X d = {1,...,n1}d on a ddimensional complete hypercube grid with n= nd1 points and Manhattan distance d(xi,xj)=d(i,j)= \u2211d a=1d1(ia,ja) for all xi= i\u2208X d and xj =j\u2208X d, where d1 is the one-dimensional circular distance defined above (so actually X d is a discrete torus). For k= kd1, the neighborhood Nk(x) of x is a cube of side-length k1. In this case, M=M1\u2297...\u2297M1 is a d-fold tensor product of the 1d k1NN matrices M1 of sample size n1. The eigenvectors of M are v l1\u2297...\u2297vld with eigenvalues b\u0302l1 \u00b7...\u00b7 b\u0302ld. We get\n\u2212 log det\u2032(1 \u2212M) = \u2212 n1\u22121 \u2211\nl1=1\n...\nnd\u22121 \u2211\nld=1\nlog(1\u2212 b\u0302l1 \u00b7 ... \u00b7 b\u0302ld) (30)\nn\u226bk\u2192\u221e\u2212\u2192 \u2212 1 \u03c0d\n\u222b\nIRd log\n( 1\u2212 d \u220f\na=1\nsin ta ta ) ddt =: n k cd\u221e\u221e\nFor instance, for d=2, numerical integration gives c2\u221e\u221e=\u03072.2 compared to 3.2 in one dimension. For higher dimensions, evaluation of the d-dimensional integral becomes cumbersome, and we resort to a different approximation.\nTaylor series in M . We can also (not only for kNN) expand logdetS0 in a Taylor series in M :\n\u2212 log det\u2032(1\u2212M) = \u2212tr\u2032 log(1\u2212M) = \u221e \u2211\ns=1\n1 s tr\u2032(Ms)\n= \u221e \u2211\ns=1\n1 s (tr\u2032Ms1 ) d = n k\n\u221e \u2211\ns=1\n1 s (An1k1s) d =: n k cdnk\nwhere we used tr(A\u2297B)=tr(A)\u00b7tr(B) and (A\u2297B)s=As\u2297Bs and defined\nAn1k1s := k1 n1 tr\u2032(Ms1 ) = k1 n1\nn1\u22121 \u2211\nl=1\n(b\u0302l) s n\u226bk\u2192\u221e\u2212\u2192 1\n\u03c0\n\u222b \u221e\n\u2212\u221e\n(sin t\nt\n)s\ndt\nThe one-dimensional integral can be expressed as a finite sum with s terms or evaluated numerically. For any n and k one can show that Ank1=Ank2=1>Anks for\ns>2. So the expansion above is useful for large d. Note also that cdnk is monotone decreasing in d. For d\u2192\u221e we have\nc\u221enk = \u221e \u2211\ns=1\n1 s (Anks) \u221e = 1 + 1 2 + 0 + ... = 3 2\ni.e. cdnk decreases monotone in d from about 3.2 to 3 2 .\nThe practical implication of this observation, though, is limited, since k=kd1\u2192\u221e is actually not fixed for d\u2192\u221e. Indeed, in practical high-dimensional problems, k\u226an\u226a3d, but in our grid example k=kd1\u22653d. Real data do not form full grids but sparse neighborhoods if d is large."}, {"heading": "11 Conclusion and Outlook", "text": "We introduced a new method, the Loss Rank Principle, for model selection. The loss rank of a model is defined as the number of other data that fit the model better than the training data. The model chosen by LoRP is the one of smallest loss rank. The loss rank has an explicit expression in case of linear models. Model consistency and asymptotic efficiency of LoRP were considered. The numerical experiments suggest that LoRP works well in practice. A comparison between LoRP and other methods for model selection was also presented.\nIn this paper, we have only scratched at the surface of LoRP. LoRP seems to be a promising principle with a lot of potential, leading to a rich field. In the following we briefly summarize miscellaneous considerations.\nComparison to Rademacher complexities. For a (binary) classification problem, the rank (1) of classifier r can be re-formulated as the probability that a randomly relabeled sample y\u2032 behaves better than the actual y. The more flexible r is, the larger its rank is. The Rademacher complexity [Kol01, BBL02] of r is the expectation of the difference between the misclassifying loss under the actual y and the misclassifying loss under a randomly relabeled sample y\u2032. The more flexible r is, the larger its Rademacher complexity is. Therefore, there is a close connection between LoRP and Rademacher complexities. Model selection based on Rademacher complexities has a number of attractive properties and has been attracting many researchers, thus it\u2019s worth discovering this connection. Some results have been recently already obtained, however, to keep the present paper not so long, we decide to present the results in another paper.\nMonte Carlo estimates for non-linear LoRP. For non-linear regression we did not present an efficient algorithm for the loss rank/volume LRr(y|x). The high-dimensional volume |Vr(L)| (3) may be computed by Monte Carlo algorithms. Normally Vr(L) constitutes a small part of Yn, and uniform sampling over Yn is not feasible. Instead one should consider two competing regressors r and r\u2032 and compute |V \u2229V \u2032|/|V | and |V \u2229V \u2032|/|V \u2032| by uniformly sampling from V and V \u2032 respectively e.g., with a Metropolis-type algorithm. Taking the ratio we get |V \u2032|/|V | and hence the\nloss rank difference LRr\u2212LRr\u2032, which is sufficient for LoRP. The usual tricks and problems with sampling apply here too.\nLoRP for hybrid model classes. LoRP is not restricted to model classes indexed by a single integral \u201ccomplexity\u201d parameter, but may be applied more generally to selecting among some (typically discrete) class of models/regressors. For instance, the class could contain kNN and polynomial regressors, and LoRP selects the complexity and type of regressor (non-parametric kNN versus parametric polynomials).\nGenerative versus discriminative LoRP. We have concentrated on counting y\u2019s given fixed x, which corresponds to discriminative learning. LoRP might equally well be used for counting (x,y), as alluded to in the introduction. This would correspond to generative learning. Both regimes are used in practice. See [LJ08] for some recent results on their relative benefit, and further references.\nAcknowledgement. We would like to thank two anonymous reviewers for their detailed and helpful comments. The second author would like to thank the SML@NICTA for supporting a visit which led to the present paper."}, {"heading": "Appendix: List of Abbreviations and Notations", "text": "AIC= Akaike Information Criterion. BIC= Bayesian Information Criterion. BMS= Bayesian Model Selection kNN= k Nearest Neighbors. LBFR= Linear Basis Function Regression. LoRP= Loss Rank Principle. LRC = Loss Rank Code. MAP= Maximum a Posterior. MDL= Minimum Description Length. ML= Maximum Likelihood. PML= Penalized Maximum Likelihood. D={(x1,y1),...,(xn,yn)}= observed data. D={D}= set of all possible data D. X\u00d7Y=observation space. x=(x1,...,xn)= vector of x-observations, similarly y. f :X \u2192Y= functional dependence between x and y. F= (\u201csmall\u201d) class of functions f . H= class of stochastic hypotheses/models. r :D\u2192F= regressor/model. y\u0302i=r(xi|D)= r-estimate of yi. R= (\u201csmall\u201d) class of regressors/models. w\u2208IRd= parametrization of Fd. Nk(x)= set of indices of the k nearest neighbors of x in D. L=Lossr(D)=Loss(y,y\u0302)= empirical loss of r on D.\nRankr(L)=#{y\u2032\u2208Yn :Lossr(y\u2032|x)\u2264L}= loss rank of r. V (L)= volume of D under r. LRr(y|x)= log rank/volume of D. LR\u03b1r= regularized LRr. deff= effective dimension. mj(x,x)= coefficients of linear regressor. M(x)= linear regression matrix or \u201chat\u201d matrix. log= natural logarithm. a;b: a is replaced by b."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "In Proc. 2nd International Symposium on Information Theory,", "citeRegEx": "Akaike.,? \\Q1973\\E", "shortCiteRegEx": "Akaike.", "year": 1973}, {"title": "The relationship between variable selection and data augmentation and a method for prediction", "author": ["D. Allen"], "venue": null, "citeRegEx": "Allen.,? \\Q1974\\E", "shortCiteRegEx": "Allen.", "year": 1974}, {"title": "Model selection and error estimation", "author": ["P. Bartlett", "S. Boucheron", "G. Lugosi"], "venue": "Machine Learning,", "citeRegEx": "Bartlett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2002}, {"title": "Testing the order of a model", "author": ["A. Chambaz"], "venue": "Ann. Stat.,", "citeRegEx": "Chambaz.,? \\Q2006\\E", "shortCiteRegEx": "Chambaz.", "year": 2006}, {"title": "Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the methods of generalized cross-validation", "author": ["P. Craven", "G. Wahba"], "venue": "Numerische Mathematik,", "citeRegEx": "Craven and Wahba.,? \\Q1979\\E", "shortCiteRegEx": "Craven and Wahba.", "year": 1979}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani.,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1993}, {"title": "Tutorial on minimum description length. In Minimum Description Length: recent advances in theory and practice, page Chapters 1 and 2", "author": ["P.D. Gr\u00fcnwald"], "venue": "http://www.cwi.nl/\u223cpdg/ftp/mdlintro.pdf", "citeRegEx": "Gr\u00fcnwald.,? \\Q2004\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2004}, {"title": "Learning Kernel Classifiers", "author": ["R. Herbrich"], "venue": null, "citeRegEx": "Herbrich.,? \\Q2002\\E", "shortCiteRegEx": "Herbrich.", "year": 2002}, {"title": "Regression and time series model selection in small samples", "author": ["C.M. Hurvich", "C.L. Tsai"], "venue": null, "citeRegEx": "Hurvich and Tsai.,? \\Q1989\\E", "shortCiteRegEx": "Hurvich and Tsai.", "year": 1989}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "The loss rank principle for model selection", "author": ["M. Hutter"], "venue": "In Proc. 20th Annual Conf. on Learning Theory (COLT\u201907),", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "Rademacher penalties and structural risk minimization", "author": ["V. Koltchinskii"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Koltchinskii.,? \\Q2001\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2001}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M. Jordan"], "venue": "In Proc. 25th International Conf. on Machine Learning (ICML-2008),", "citeRegEx": "Liang and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Liang and Jordan.", "year": 2008}, {"title": "Subset Selection in Regression", "author": ["A. Miller"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Miller.,? \\Q2002\\E", "shortCiteRegEx": "Miller.", "year": 2002}, {"title": "Approximation of the determinant of large sparse symmetric positive definite matrices", "author": ["A. Reusken"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Reusken.,? \\Q2002\\E", "shortCiteRegEx": "Reusken.", "year": 2002}, {"title": "Modeling by shortest data", "author": ["J.J. Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Annals of Statistics,", "citeRegEx": "Schwarz.,? \\Q1978\\E", "shortCiteRegEx": "Schwarz.", "year": 1978}, {"title": "An asymptotic theory for linear model selection", "author": ["J. Shao"], "venue": "Statistica Sinica,", "citeRegEx": "Shao.,? \\Q1997\\E", "shortCiteRegEx": "Shao.", "year": 1997}, {"title": "Asymptotic mean efficiency of a selection of regression variables", "author": ["R. Shibata"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Shibata.,? \\Q1983\\E", "shortCiteRegEx": "Shibata.", "year": 1983}, {"title": "Tuning parameter selectors for the smoothly clipped absolute deviation method", "author": ["H. Wang", "R. Li", "C.L. Tsai"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Extended stochastic complexity and minimax relative loss analysis", "author": ["K. Yamanishi"], "venue": "Proc. 10th International Conference on Algorithmic Learning Theory - ALT\u2019", "citeRegEx": "Yamanishi.,? \\Q1999\\E", "shortCiteRegEx": "Yamanishi.", "year": 1999}, {"title": "Can the strengths of aic and bic be shared? a conflict between model identification and regression estimation", "author": ["Y. Yang"], "venue": null, "citeRegEx": "Yang.,? \\Q2005\\E", "shortCiteRegEx": "Yang.", "year": 2005}], "referenceMentions": [], "year": 2010, "abstractText": "A key issue in statistics and machine learning is to automatically select the \u201cright\u201d model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle the Loss Rank Principle (LoRP) for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.", "creator": "LaTeX with hyperref package"}}}