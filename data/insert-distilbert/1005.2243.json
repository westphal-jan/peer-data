{"id": "1005.2243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2010", "title": "Robustness and Generalization", "abstract": "we derive generalization bounds for interactive learning algorithms based on their robustness : the property that if a testing event sample is \" similar \" to a training sample, then the testing error statement is close to the training error. but this provides a novel approach, physically different from the complexity or stability arguments, to study generalization functions of agile learning algorithms. we both further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental natural property for learning algorithms to work.", "histories": [["v1", "Thu, 13 May 2010 01:59:57 GMT  (90kb)", "http://arxiv.org/abs/1005.2243v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["huan xu", "shie mannor"], "accepted": false, "id": "1005.2243"}, "pdf": {"name": "1005.2243.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Huan Xu", "Shie Mannor"], "emails": ["huan.xu@mail.utexas.edu", "shie@technion.ee.ac.il"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 5.\n22 43\nv1 [\ncs .L\nG ]\n1 3\nWe derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is \u201csimilar\u201d to a training sample, then the testing error is close to the training error. This provides a novel approach, different from the complexity or stability arguments, to study generalization of learning algorithms. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property for learning algorithms to work."}, {"heading": "1. Introduction", "text": "The key issue in the task of learning from a set of observed samples is the estimation of the risk (i.e., generalization error) of learning algorithms. Typically, its empirical measurement (i.e., training error) provides an optimistically biased estimation, especially when the number of training samples is small. Several approaches have been proposed to bound the deviation of the risk from its empirical measurement, among which methods based on uniform convergence and stability are most widely used.\nUniform convergence of empirical quantities to their mean (e.g., Vapnik and Chervonenkis,\n1974, 1991) provides ways to bound the gap between the expected risk and the empirical risk by the complexity of the hypothesis set. Examples to complexity measures are the VapnikChervonenkis (VC) dimension (e.g., Vapnik and Chervonenkis, 1991; Evgeniou et al., 2000), the fat-shattering dimension (e.g., Alon et al., 1997; Bartlett, 1998), and the Rademacher complexity (Bartlett and Mendelson, 2002; Bartlett et al., 2005). Another well-known approach is based on stability. An algorithm is stable if its output remains \u201csimilar\u201d for different sets of training samples that are identical up to removal or change of a single sample. The first results that relate stability to generalizability track back to Devroye and Wagner (1979a) and Devroye and Wagner (1979b). Later, McDiarmid\u2019s (McDiarmid, 1989), concentration inequalities facilitated new bounds on generalization error (e.g., Bousquet and Elisseeff, 2002; Poggio et al., 2004; Mukherjee et al., 2006).\nIn this paper we explore a different approach which we term algorithmic robustness. Briefly speaking, an algorithm is robust if its solution has the following property: it achieves \u201csimilar\u201d performance on a testing sample and a training sample that are \u201cclose\u201d. This notion of robustness is rooted in robust optimization (Ben-tal and Nemirovski, 1998; Ben-Tal and Nemirovski, 1999; Bertsimas and Sim, 2004) where a decision maker aims to find a solution x that minimizes a (parameterized) cost function f(x, \u03be) with the knowledge that the unknown true parameter \u03be may deviate from the observed parameter \u03be\u0302. Hence, instead of solving minx f(x, \u03be\u0302) one solves minx[max\u03be\u0303\u2208\u2206 f(x, \u03be\u0303)], where \u2206 includes all possible realizations of \u03be. Robust optimization was introduced in machine learning tasks to handle exogenous noise (e.g., Bhattacharyya et al., 2004; Shivaswamy et al., 2006; Globerson and Roweis, 2006), i.e., the learning algorithm only has access to inaccurate observation of training samples. Later on, Xu et al. (2009b,a) showed that both Support Vector Machine(SVM) and Lasso have robust optimization interpretation, i.e., they can be reformulated as\nmin h\u2208H max (\u03b41,\u00b7\u00b7\u00b7 ,\u03b4n)\u2208\u2206\nn \u2211\ni=1\nl(h, zi + \u03b4i),\nfor some \u2206. Here zi are the observed training samples and l(\u00b7, \u00b7) is the loss function (hingeloss for SVM, and squared loss for Lasso), which means that SVM and Lasso essentially minimize the empirical error under the worst possible perturbation. Indeed, as the authors of Xu et al. (2009b,a) showed, this reformulation leads to requiring that the loss of a sample \u201cclose\u201d to zi is small, which further implies statistical consistency of these two algorithms. In this paper we adopt this approach and study the (finite sample) generalization ability of learning algorithms by investigating the loss of learned hypotheses on samples that slightly deviate from training samples.\nOf special interest is that robustness is more than just another way to establish generalization bounds. Indeed, we show that a weaker notion of robustness is a necessary and sufficient condition of (asymptotic) generalizability of (general) learning algorithms. While it is known having a finite VC-dimension (Vapnik and Chervonenkis, 1991) or equivalently being CVEEEloo stable (Mukherjee et al., 2006) is necessary and sufficient for the Empirical Risk Minimization (ERM) to generalize, much less is known in the general case. Recently, Shalev-Shwartz et al. (2009) proposed a weaker notion of stability that is necessary and sufficient for a learning algorithm to be consistent and generalizing, provided that the problem itself is learnable. However, learnability requires that the convergence rate is uniform with respect to all distributions, and is hence a fairly strong assumption. In particular, the standard supervised learning setup where the hypothesis set is the set of measurable functions is not learnable since no algorithm can achieve a uniform convergence rate (cf Devroye et al., 1996). Indeed, as the authors of Shalev-Shwartz et al. (2009) stated, for supervised learning problem learnability is equivalent to the generalizability of ERM, and hence reduce to the aforementioned results on ERM algorithms.\nIn particular, our main contributions are the following:\n1. We propose a notion of algorithmic robustness. Algorithmic robustness is a desired\nproperty for a learning algorithm since it implies a lack of sensitivity to (small) disturbances in the training data.\n2. Based on the notion of algorithmic robustness, we derive generalization bound for IID\nsamples as well as samples drawn according to a Markovian chain.\n3. To illustrate the applicability of the notion of algorithmic robustness, we provide some\nexamples of robust algorithms, including SVM, Lasso, feed-forward neural networks and PCA.\n4. We propose a weaker notion of robustness and show that it is both necessary and\nsufficient for a learning algorithm to generalize. This implies that robustness is an essential property needed for a learning algorithm to work.\nNote that while stability and robustness are similar on an intuitive level, there is a difference between the two: stability requires that nearly identical training sets with a single sample removed lead to similar prediction rules, whereas robustness requires that a prediction rule has comparable performance if tested on a sample close to a training sample.\nThis paper is organized as follows. We define the notion of robustness in Section 2, and prove generalization bounds for robust algorithms in Section 3. In Section 4 we propose a relaxed notion of robustness, which is termed as pseudo-robustness, and show corresponding generalization bounds. Examples of learning algorithms that are robust or pseudo-robust are provided in Section 5. Finally, we show that robustness is necessary and sufficient for generalizability in Section 6."}, {"heading": "1.1 Preliminaries", "text": "We consider the following general learning model: a set of training samples are given, and the goal is to pick a hypothesis from a hypothesis set. Unless otherwise mentioned, throughout this paper the size of training set is fixed as n. Therefore, we drop the dependence of parameters on the number of training samples, while it should be understood that parameters may vary with the number of training samples. We use Z and H to denote the set from which each sample is drawn, and the hypothesis set, respectively. Throughout the paper we use s to denote the training sample set consists of n training samples (s1, \u00b7 \u00b7 \u00b7 , sn). A learning algorithm A is thus a mapping from Zn to H. We use As to represent the hypothesis learned (given training set s). For each hypothesis h \u2208 H and a point z \u2208 Z, there is an associated loss l(h, z). We ignore the issue of measurability and further assume that l(h, z) is non-negative and upper-bounded uniformly by a scalar M .\nIn the special case of supervised learning, the sample space can be decomposed as Z = Y \u00d7 X , and the goal is to learn a mapping from X to Y, i.e., to predict the ycomponent given x-component. We hence use As(x) to represent the prediction of x \u2208 X if trained on s. We call X the input space and Y the output space. The output space can\neither be Y = {\u22121,+1} for a classification problem, or Y = R for a regression problem. We use |x and |y to denote the x-component and y-component of a point. For example, si|x is the x-component of si. To simplify notations, for a scaler c, we use [c] + to represent its non-negative part, i.e., [c]+ , max(0, c).\nWe recall the following standard notion of covering number from van der Vaart and Wellner\n(2000).\nDefinition 1 (cf. van der Vaart and Wellner (2000)) For a metric space S, \u03c1 and T \u2282 S we say that T\u0302 \u2282 S is an \u01eb-cover of T , if \u2200t \u2208 T , \u2203t\u0302 \u2208 T\u0302 such that \u03c1(t, t\u0302) \u2264 \u01eb. The \u01ebcovering number of T is\nN (\u01eb, T, \u03c1) = min{|T\u0302 | : T\u0302 is an \u01eb\u2212 cover of T}."}, {"heading": "2. Robustness of Learning Algorithms", "text": "Before providing a precise definition of what we mean by \u201crobustness\u201d of an algorithm, we provide some motivating examples which share a common property: if a testing sample is close to a training sample, then the testing error is also close, a property we will later formalize as \u201crobustness\u201d.\nWe first consider large-margin classifiers: Let the loss function be l(As, z) = 1(As(z|x) 6=\nz|y). Fix \u03b3 > 0. An algorithm As has a margin \u03b3 if for j = 1, \u00b7 \u00b7 \u00b7 , n\nAs(x) = As(sj|x); \u2200x : \u2016x\u2212 sj|x\u20162 < \u03b3.\nThat is, any training sample is at least \u03b3 away from the classification boundary.\nExample 1 Fix \u03b3 > 0 and put K = 2N (\u03b3/2,X , \u2016 \u00b7 \u20162). If As has a margin \u03b3, then Z can be partitioned into K disjoint sets, denoted by {Ci} K i=1, such that if sj and z \u2208 Z belong to a same Ci, then |l(As, sj)\u2212 l(As, z)| = 0.\nProof By definition of covering number, we can partition X into N (\u03b3/2,X , \u2016 \u00b7 \u20162) subsets (denoted X\u0302i) such that each subset has a diameter less or equal to \u03b3. Further, Y can be partitioned to {\u22121} and {+1}. Thus, we can partition Z into 2N (\u03b3/2,X , \u2016 \u00b7 \u20162) subsets such that if z1, z2 belong to a same subset, then y1|y = y2|y and \u2016x1|y \u2212 x2|y\u2016 \u2264 \u03b3. By definition of margin, this guarantees that if sj and z \u2208 Z belong to a same Ci, then |l(As, sj)\u2212 l(As, z)| = 0.\nThe next example is a linear regression algorithm. Let the loss function be l(As, z) = |z|y \u2212As(z|x)|, and let X be a bounded subset of R m and fix c > 0. The norm-constrained linear regression algorithm is\nAs = min w\u2208Rm:\u2016w\u20162\u2264c\nn \u2211\ni=1\n|si|y \u2212 w \u22a4si|x|, (1)\ni.e., minimizing the empirical error among all linear classifiers whose norm is bounded.\nExample 2 Fix \u01eb > 0 and put K = N (\u01eb/2,X , \u2016\u00b7\u20162)\u00d7N (\u01eb/2,Y, |\u00b7|). Consider the algorithm as in (1). The set Z can be partitioned into K disjoint sets, such that if sj and z \u2208 Z belong to a same Ci, then\n|l(As, sj)\u2212 l(As, z)| \u2264 (c+ 1)\u01eb.\nProof Similarly to the previous example, we can partition Z toN (\u01eb/2,X , \u2016\u00b7\u20162)\u00d7N (\u01eb/2,Y, |\u00b7 |) subsets, such that if z1, z2 belong to a same Ci, then \u2016z1|x\u2212z2|x\u20162 \u2264 \u01eb, and |z1|y\u2212z2|y| \u2264 \u01eb. Since \u2016w\u20162 \u2264 c, we have\n|l(w, z1)\u2212 l(w(s), z2)| = \u2223 \u2223 \u2223 |z1|y \u2212 w \u22a4z1|x| \u2212 |z2|y \u2212 w \u22a4z2|x| \u2223 \u2223 \u2223\n\u2264 \u2223 \u2223\n\u2223 (z1|y \u2212 w \u22a4z1|x)\u2212 (z2|y \u2212 w \u22a4z2|x)\n\u2223 \u2223 \u2223\n\u2264|z1|y \u2212 z2|y|+ \u2016w\u20162\u2016z1|x \u2212 z2|x\u20162\n\u2264(1 + c)\u01eb,\nwhenever z1, z2 belong to a same Ci.\nThe two motivating examples both share a property: we can partition the sample set into finite subsets, such that if a new sample falls into the same subset as a testing sample, then the loss of the former is close to the loss of the latter. We call an algorithm having this property \u201crobust.\u201d\nDefinition 2 Algorithm A is (K, \u01eb(s)) robust if Z can be partitioned into K disjoint sets, denoted as {Ci} K i=1, such that \u2200s \u2208 s,\ns, z \u2208 Ci, =\u21d2 |l(As, s)\u2212 l(As, z)| \u2264 \u01eb(s). (2)\nIn the definition, both K and the partition sets {Ci} K i=1 do not depend on the training set s. Note that the definition of robustness requires that (2) holds for every training sample. Indeed, we can relax the definition, so that the condition needs only hold for a subset of training samples. We call an algorithm having this property \u201cpseudo robust\u201d. See Section 4 for details."}, {"heading": "3. Generalization of Robust Algorithms", "text": "In this section we investigate generalization property of robust algorithms. In particular, in the following subsections we derive PAC bounds for robust algorithms under three different conditions: (1) The ubiquitous learning setup where the samples are i.i.d. and the goal of learning is to minimize expected loss. (2) The learning goal is to minimize quantile loss. (3) The samples are generated according to a (Doeblin) Markovian chain. Indeed, the fact that we can provide results in (2) and (3) indicates the fundamental nature of robustness as a property of learning algorithms."}, {"heading": "3.1 IID samples and expected loss", "text": "In this section, we consider the standard learning setup, i.e., the sample set s consists of n i.i.d. samples generated by an unknown distribution \u00b5, and the goal of learning is to minimize expected test loss. Let l\u0302(\u00b7) and lemp(\u00b7) denote the expected error and the training error, i.e.,\nl\u0302(As) , Ez\u223c\u00b5l(As, z); lemp(As) , 1\nn\n\u2211\nsi\u2208s\nl(As, si).\nRecall that the loss function l(\u00b7, \u00b7) is upper bounded by M .\nTheorem 3 If s consists of n i.i.d. samples, and A is (K, \u01eb(s))-robust, then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 \u2264 \u01eb(s) +M\n\u221a\n2K ln 2 + 2 ln(1/\u03b4)\nn .\nProof LetNi be the set of index of points of s that fall into the Ci. Note that (|N1|, \u00b7 \u00b7 \u00b7 , |NK |) is an IID multinomial random variable with parameters n and (\u00b5(C1), \u00b7 \u00b7 \u00b7 , \u00b5(CK)). The following holds by the Breteganolle-Huber-Carol inequality (cf Proposition A6.6 of van der Vaart and Wellner, 2000):\nPr\n{\nK \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223 \u2265 \u03bb\n}\n\u2264 2K exp( \u2212n\u03bb2\n2 ).\nHence, the following holds with probability at least 1\u2212 \u03b4,\nK \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223 \u2264\n\u221a\n2K ln 2 + 2 ln(1/\u03b4)\nn . (3)\nWe have \u2223\n\u2223 \u2223 l\u0302(As)\u2212 lemp(As)\n\u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) \u00b5(Ci)\u2212 1\nn\nn \u2211\ni=1\nl(As, si)\n\u2223 \u2223 \u2223 \u2223 \u2223\n(a) \u2264\n\u2223 \u2223 \u2223 \u2223 \u2223 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) |Ni|\nn \u2212\n1 n\nn \u2211\ni=1\nl(As, si)\n\u2223 \u2223 \u2223 \u2223 \u2223\n+\n\u2223 \u2223 \u2223 \u2223 \u2223 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) \u00b5(Ci)\u2212 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) |Ni|\nn\n\u2223 \u2223 \u2223 \u2223 \u2223\n(b) \u2264\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 n K \u2211\ni=1\n\u2211\nj\u2208Ni\nmax z2\u2208Ci |l(As, sj)\u2212 l(As, z2)|\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 + \u2223 \u2223 \u2223 \u2223 \u2223 max z\u2208Z |l(As,z)| K \u2211\ni=1\n\u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223\n\u2223 \u2223 \u2223 \u2223 \u2223\n(c) \u2264\u01eb(s) +M K \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223 ,\n(4)\nwhere (a), (b), and (c) are due to the triangle inequality, the definition of Ni, and the definition of \u01eb(s) and M , respectively. Note that the right-hand-side of (4) is upper-bounded\nby \u01eb(s)+M\n\u221a\n2K ln 2+2 ln(1/\u03b4) n with probability at least 1\u2212\u03b4 due to (3). The theorem follows.\nTheorem 3 requires that we fix a K a priori. However, it is often worthwhile to consider adaptive K. For example, in the large-margin classification case, typically the margin is known only after s is realized. That is, the value of K depends on s. Because of this dependency, we needs a generalization bound that holds uniformly for all K.\nCorollary 4 If s consists of n i.i.d. samples, and A is (K, \u01ebK(s)) robust for all K \u2265 1, then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 \u2264 inf\nK\u22651\n\n\u01ebK(s) +M\n\u221a\n2K ln 2 + 2 ln K(K+1)\u03b4 n\n\n .\nProof Let\nE(K) ,\n \n\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 > \u01ebK(s) +M\n\u221a\n2K ln 2 + 2 ln K(K+1)\u03b4 n\n \n\n.\nFrom Theorem 3 we have Pr(E(K)) \u2264 \u03b4/(K(K + 1)) = \u03b4/K \u2212 \u03b4/(K + 1). By the union bound we have\nPr\n \n\n\u22c3\nK\u22651\nE(K)\n \n\n\u2264 \u2211\nK\u22651\nPr (E(K)) \u2264 \u2211\nK\u22651\n[\n\u03b4\nK \u2212\n\u03b4\nK + 1\n]\n= \u03b4,\nand the corollary follows.\nIf \u01eb(s) does not depend on s, we can sharpen the bound given in Corollary 4.\nCorollary 5 If s consists of n i.i.d. samples, and A is (K, \u01ebK) robust for all K \u2265 1, then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 \u2264 inf\nK\u22651\n\n\u01ebK +M\n\u221a\n2K ln 2 + 2 ln 1\u03b4 n\n\n .\nProof The right hand side does not depend on s, and hence the optimal K\u2217. Therefore, plugging K\u2217 into Theorem 3 establishes the corollary."}, {"heading": "3.2 Quantile Loss", "text": "So far we considered the standard expected loss setup. In this section we consider some less extensively investigated loss functions, namely quantile value and truncated expectation (see the following for precise definitions). These loss functions are of interest because they are less sensitive to the presence of outliers than the standard average loss (Huber, 1981).\nDefinition 6 For a non-negative random variable X, the \u03b2-quantile value is\nQ\u03b2(X) , inf { c \u2208 R : Pr ( X \u2264 c ) \u2265 \u03b2 } .\nThe \u03b2-truncated mean is\nT\u03b2(X) ,\n \n\nE [ X \u00b7 1(X < Q\u03b2(X)) ]\nif Pr [ X = Q\u03b2(X) ] = 0;\nE [ X \u00b7 1(X < Q\u03b2(X)) ] + \u03b2\u2212Pr\n[ X<Q\u03b2(X) ]\nPr [ X=Q\u03b2(X) ] Q\u03b2(X) otherwise.\nIn words, the \u03b2\u2212quantile loss is the smallest value that is larger or equal to X with probability at least \u03b2. The \u03b2-truncated mean is the contribution to the expectation of the leftmost \u03b2 fraction of the distribution. For example, suppose X is supported on {c1, \u00b7 \u00b7 \u00b7 , c10} (c1 < c2 < \u00b7 \u00b7 \u00b7 < c10) and the probability of taking each value equals 0.1. Then the 0.63-quantile loss of X is c7, and the 0.63-truncated mean of X equals 0.1( \u22116 i=1 ci +0.3c7).\nGiven h \u2208 H, \u03b2 \u2208 (0, 1), and a probability measure \u00b5 on Z, let\nQ(h, \u03b2, \u00b5) , Q\u03b2(l(h, z)); where: z \u223c \u00b5;\nand\nT (h, \u03b2, \u00b5) , T\u03b2(l(h, z)); where: z \u223c \u00b5;\ni.e., the \u03b2-quantile value and \u03b2-truncated mean of the (random) testing error of hypothesis h if the testing sample follows distribution \u00b5. We have the following theorem that is a special case of Theorem 13, hence we omit the proof.\nTheorem 7 (Quantile Value & Truncated Mean) Suppose s are n i.i.d. samples drawn\naccording to \u00b5, and denote the empirical distribution of s by \u00b5emp. Let \u03bb0 =\n\u221a\n2K ln 2+2 ln(1/\u03b4) n .\nIf 0 \u2264 \u03b2 \u2212 \u03bb0 \u2264 \u03b2 + \u03bb0 \u2264 1 and A is (K, \u01eb(s)) robust, then with probability at least 1 \u2212 \u03b4, the followings hold\n(I) Q (As, \u03b2 \u2212 \u03bb0, \u00b5emp)\u2212 \u01eb(s) \u2264 Q (As, \u03b2, \u00b5) \u2264 Q (As, \u03b2 + \u03bb0, \u00b5emp) + \u01eb(s);\n(II) T (As, \u03b2 \u2212 \u03bb0, \u00b5emp)\u2212 \u01eb(s) \u2264 T (As, \u03b2, \u00b5) \u2264 T (As, \u03b2 + \u03bb0, \u00b5emp) + \u01eb(s).\nIn words, Theorem 7 essentially means that with high probability, the \u03b2-quantile value/truncated mean of the testing error (recall that the testing error is a random variable) is (approximately) bounded by the (\u03b2\u00b1\u03bb0)-quantile value/truncated mean of the empirical error, thus providing a way to estimate the quantile value/truncated expectation of the testing error based on empirical observations."}, {"heading": "3.3 Markovian samples", "text": "The robustness approach is not restricted to the IID setup. In many applications of interest, such as reinforcement learning and time series forecasting, the IID assumption is violated. In such applications there is a time driven process that generates samples that depend on the previous samples (e.g., the observations of a trajectory of a robot). Such a situation can be modeled by stochastic process such as a Markov processes. In this section we establish similar result to the IID case for samples that are drawn from a Markov chain. The state space can be general, i.e., it is not necessarily finite or countable. Thus, a certain ergodic structure of the underlying Markov chain is needed. We focus on chains that converge to equilibrium exponentially fast and uniformly in the initial condition. It is known that this is equivalent to the class of of Doeblin chains (Meyn and Tweedie, 1993). Recall the following definition (cf Meyn and Tweedie, 1993; Doob, 1953)).\nDefinition 8 A Markov chain {zi} \u221e i=1 on a state space Z is a Doeblin chain (with \u03b1 and T ) if there exists a probability measure \u03d5 on Z, \u03b1 > 0, an integer T \u2265 1 such that\nPr(zT \u2208 H|z0 = z) \u2265 \u03b1\u03d5(H); \u2200measureable H \u2286 Z; \u2200z \u2208 Z.\nThe class of Doeblin chains is probably the \u201cnicest\u201d class of general state-space Markov chains. We notice that such assumption is not overly restrictive, since by requiring that an ergodic theorem holds for all bounded functions uniformly in the initial distribution itself implies that a chain is Doeblin (Meyn and Tweedie, 1993). In particular, an ergodic chain defined on a finite state-space is a Doeblin chain.\nIndeed, the Doeblin chain condition guarantees that an invariant measure \u03c0 exists. Furthermore, we have the following lemma adapted from Theorem 2 of Glynn and Ormoneit (2002).\nLemma 9 Let {zi} be a Doeblin chain as in Definition 8. Fix a function f : Z \u2192 R such that \u2016f\u2016\u221e \u2264 C. Then for n > 2CT/\u01eb\u03b1 the following holds\nPr\n(\n1 n\nn \u2211\ni=1\nf(zi)\u2212\n\u222b\nZ f(z)\u03c0(dz)s \u2265 \u01eb\n)\n\u2264 exp\n(\n\u2212 \u03b12(n\u01eb\u2212 2CT/\u03b1)2\n2nC2T 2\n)\n.\nThe following is the main theorem of this section that establishes a generalization bound for robust algorithms with samples drawn according to a Doeblin chain.\nTheorem 10 Let s = {s1, \u00b7 \u00b7 \u00b7 , sn} be the first n outputs of a Doeblin chain with \u03b1 and T such that n > 2T/\u03b1, and suppose that A is (K, \u01eb(s))-robust. Then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 \u2264 \u01eb(s) +M\n(\n8T 2(K ln 2 + ln(1/\u03b4))\n\u03b12n\n)1/4\n.\nProof We prove the following slightly stronger statement:\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 \u2264 \u01eb(s) +M\n\u221a\nT\n\u03b1n\n\u221a\n\u221a\n2n(K ln 2 + ln(1/\u03b4)) + 2. (5)\nLet \u03bb0 = \u221a T \u03b1n\n\u221a\n\u221a 2n(K ln 2 + ln(1/\u03b4)) + 2, we have that \u03bb0 > \u221a 2T/\u03b1n. Since n > 2T/\u03b1,\nwe have n > \u221a 2Tn/\u03b1, which leads to\nn > 2T\n\u03b1 \u221a 2T/\u03b1n >\n2T\n\u03b1\u03bb0 .\nLet Ni be the set of index of points of s that fall into the Ci. Consider the set of functions\nH = {1(x \u2208 H)|H = \u22c3\ni\u2208I Ci; \u2200I \u2286 {1, \u00b7 \u00b7 \u00b7 ,K}}, i.e., the set of indicator functions of all\ndifferent unions of Ci. Then |H| = 2 K . Furthermore, fix a h0 \u2208 H,\nPr(\nK \u2211\nj=1\n\u2223 \u2223 \u2223 \u2223 |Nj |\nn \u2212 \u03c0(Cj)\n\u2223 \u2223 \u2223 \u2223 \u2265 \u03bb)\n=Pr {\nsup h\u2208H\n[ 1\nn\nn \u2211\ni=1\nh(si)\u2212 E\u03c0h(s)] \u2265 \u03bb }\n\u22642KPr[ 1\nn\nn \u2211\ni=1\nh0(si)\u2212 E\u03c0h0(s) \u2265 \u03bb].\nSince \u2016h0\u2016\u221e = 1, we can apply Lemma 9 to get for n > 2T/\u03bb\u03b1\nPr[ 1\nn\nn \u2211\ni=1\nh0(si)\u2212 E\u03c0h0(s) \u2265 \u03bb] \u2264 exp\n(\n\u2212 \u03b12(n\u03bb2 \u2212 2T/\u03b1)2\n2nT 2\n)\n.\nSubstitute in \u03bb0,\nPr( K \u2211\nj=1\n\u2223 \u2223 \u2223 \u2223 |Nj|\nn \u2212 \u03c0(Cj)\n\u2223 \u2223 \u2223 \u2223 \u2265 \u03bb0) \u2264 2 K exp ( \u2212 \u03b12(n\u03bb20 \u2212 2T/\u03b1) 2\n2nT 2\n)\n= \u03b4.\nThus, (5) follows by an identical argument as the proof of Theorem 3.\nTo complete the proof of the theorem, note that n > 2T/\u03b1 implies n \u2265 2, hence \u221a\n2n(K ln 2 + ln(1/\u03b4)) \u2265 2. Therefore,\n\u221a\nT\n\u03b1n\n\u221a\n\u221a\n2n(K ln 2 + ln(1/\u03b4)) + 2 \u2264\n\u221a\nT\n\u03b1n\n\u221a\n2 \u221a 2n(K ln 2 + ln(1/\u03b4))\n=\n(\n8T 2(K ln 2 + ln(1/\u03b4))\n\u03b12n\n)1/4\n,\nand the theorem follows."}, {"heading": "4. Pseudo Robustness", "text": "In this section we propose a relaxed definition of robustness that accounts for the case where Equation (2) holds for most of training samples, as opposed to Definition 6 where Equation (2) holds for all training samples. Recall that the size of training set is fixed as n.\nDefinition 11 Algorithm A is (K, \u01eb(s), n\u0302) pseudo robust if Z can be partitioned into K disjoint sets, denoted as {Ci} K i=1, and a subset of training samples s\u0302 with |s\u0302| = n\u0302 such that \u2200s \u2208 s\u0302,\ns, z \u2208 Ci, =\u21d2 |l(As, s)\u2212 l(As, z)| \u2264 \u01eb(s).\nObserve that (K, \u01eb(s))-robust is equivalent to (K, \u01eb(s), n) pseudo robust.\nTheorem 12 If s consists of n i.i.d. samples, and A is (K, \u01eb(s), n\u0302) pseudo robust, then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4,\n\u2223 \u2223 \u2223 l\u0302(As)\u2212 lemp(As) \u2223 \u2223 \u2223 \u2264 n\u0302\nn \u01eb(s) +M\n(\nn\u2212 n\u0302\nn +\n\u221a\n2K ln 2 + 2 ln(1/\u03b4)\nn\n)\n.\nProof Let Ni and N\u0302i be the set of indices of points of s and s\u0302 that fall into the Ci, respectively. Similarly to the proof of Theorem 3, we note that (|N1|, \u00b7 \u00b7 \u00b7 , |NK |) is an IID multinomial random variable with parameters n and (\u00b5(C1), \u00b7 \u00b7 \u00b7 , \u00b5(CK)). And hence due to Breteganolle-Huber-Carol inequality, the following holds with probability at least 1\u2212 \u03b4,\nK \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223 \u2264\n\u221a\n2K ln 2 + 2 ln(1/\u03b4)\nn . (6)\nFurthermore, we have \u2223\n\u2223 \u2223 l\u0302(As)\u2212 lemp(As)\n\u2223 \u2223 \u2223\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) \u00b5(Ci)\u2212 1\nn\nn \u2211\ni=1\nl(As, si)\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264\n\u2223 \u2223 \u2223 \u2223 \u2223 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) |Ni|\nn \u2212\n1 n\nn \u2211\ni=1\nl(As, si)\n\u2223 \u2223 \u2223 \u2223 \u2223\n+\n\u2223 \u2223 \u2223 \u2223 \u2223 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) \u00b5(Ci)\u2212 K \u2211\ni=1\nE ( l(As, z)|z \u2208 Ci ) |Ni|\nn\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 n K \u2211\ni=1\n[ |Ni| \u00d7 E ( l(As, z)|z \u2208 Ci ) \u2212 \u2211\nj\u2208N\u0302i\nl(As, sj)\u2212 \u2211\nj\u2208Ni,j 6\u2208N\u0302i\nl(As, sj) ]\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n+\n\u2223 \u2223 \u2223 \u2223 \u2223 max z\u2208Z |l(As,z)| K \u2211\ni=1\n\u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223\n\u2223 \u2223 \u2223 \u2223 \u2223 .\nNote that due to the triangle inequality as well as the assumption that the loss is nonnegative and upper bounded by M , the right-hand side can be upper bounded by \u2223\n\u2223 \u2223 \u2223 \u2223 \u2223 1 n K \u2211\ni=1\n\u2211\nj\u2208N\u0302i\nmax z2\u2208Ci |l(As, sj)\u2212 l(As, z2)|\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 + \u2223 \u2223 \u2223 \u2223 \u2223 \u2223 1 n K \u2211\ni=1\n\u2211\nj\u2208Ni,j 6\u2208N\u0302i\nmax z2\u2208Ci |l(As, sj)\u2212 l(As, z2)|\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n+M K \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223\n\u2264 n\u0302\nn \u01eb(s) +\nn\u2212 n\u0302\nn M +M\nK \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223 .\nwhere the inequality holds due to definition of Ni and N\u0302i. The theorem follows by applying (6).\nSimilarly, Theorem 7 can be generalized to the pseudo robust case. The proof is lengthy\nand hence postponed to Appendix A.1.\nTheorem 13 (Quantile Value & Truncated Expectation) Suppose s has n samples drawn i.i.d. according to \u00b5, and denote the empirical distribution of s as \u00b5emp. Let \u03bb0 = \u221a\n2K ln 2+2 ln(1/\u03b4) n . Suppose 0 \u2264 \u03b2 \u2212 \u03bb0 \u2212 (n \u2212 n\u0302)/n \u2264 \u03b2 + \u03bb0 + (n \u2212 n\u0302)/n \u2264 1 and A is\n(K, \u01eb(s), n\u0302) pseudo robust. Then with probability at least 1\u2212 \u03b4, the followings hold\n(I) Q\n(\nAs, \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn , \u00b5emp\n)\n\u2212 \u01eb(s)\n\u2264 Q (As, \u03b2, \u00b5) \u2264 Q\n(\nAs, \u03b2 + \u03bb0 + n\u2212 n\u0302\nn , \u00b5emp\n)\n+ \u01eb(s);\n(II) T\n(\nAs, \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn , \u00b5emp\n)\n\u2212 \u01eb(s)\n\u2264 T (As, \u03b2, \u00b5) \u2264 T\n(\nAs, \u03b2 + \u03bb0 + n\u2212 n\u0302\nn , \u00b5emp\n)\n+ \u01eb(s)."}, {"heading": "5. Examples of Robust Algorithms", "text": "In this section we provide some examples of robust algorithms. The proofs of the examples can be found in Appendix. Our first example is Majority Voting (MV) classification (cf Section 6.3 of Devroye et al., 1996) that partitions the input space X and labels each partition set according to a majority vote of the training samples belonging to it.\nExample 3 (Majority Voting) Let Y = {\u22121,+1}. Partition X to C1, \u00b7 \u00b7 \u00b7 , CK , and use C(x) to denote the set to which x belongs. A new sample xa \u2208 X is labeled by\nAs(xa) ,\n{\n1, if \u2211\nsi\u2208C(xa) 1(si|y = 1) \u2265\n\u2211\nsi\u2208C(xa) 1(si|y = \u22121);\n\u22121, otherwise.\nIf the loss function is l(As, z) = f(z|y,As(z|x)) for some function f , then MV is (2K, 0) robust.\nMV algorithm has a natural partition of the sample space that makes it robust. Another class of robust algorithms are those that have approximately the same testing loss for testing samples that are close (in the sense of geometric distance) to each other, since we can partition the sample space with norm balls. The next theorem states that an algorithm is robust if two samples being close implies that they have similar testing error.\nTheorem 14 Fix \u03b3 > 0 and metric \u03c1 of Z. Suppose A satisfies\n|l(As, z1)\u2212 l(As, z2)| \u2264 \u01eb(s), \u2200z1, z2 : z1 \u2208 s, \u03c1(z1, z2) \u2264 \u03b3,\nand N (\u03b3/2,Z, \u03c1) < \u221e. Then A is ( N (\u03b3/2,Z, \u03c1), \u01eb(s) ) -robust.\nProof Let {c1, \u00b7 \u00b7 \u00b7 , cN (\u03b3/2,Z,\u03c1)} be a \u03b3/2-cover of Z. whose existence is guaranteed by the definition of covering number. Let C\u0302i = {z \u2208 Z|\u03c1(z, ci) \u2264 \u03b3/2}, and Ci = C\u0302i \u22c2 ( \u22c3i\u22121 j=1 C\u0302j )c . Thus, C1, \u00b7 \u00b7 \u00b7 , CN (\u03b3/2,Z,\u03c1) is a partition of Z, and satisfies\nz1, z2 \u2208 Ci =\u21d2 \u03c1(z1, z2) \u2264 \u03c1(z1, ci) + \u03c1(z2, ci) \u2264 \u03b3.\nTherefore,\n|l(As, z1)\u2212 l(As, z2)| \u2264 \u01eb(s), \u2200z1, z2 : z1 \u2208 s, \u03c1(z1, z2) \u2264 \u03b3,\nimplies\nz1 \u2208 s z1, z2 \u2208 Ci =\u21d2 |l(As, z1)\u2212 l(As, z2)| \u2264 \u01eb(s),\nand the theorem follows.\nTheorem 14 immediately leads to the next example: if the testing error given the output of an algorithm is Lipschitz continuous, then the algorithm is robust.\nExample 4 (Lipschitz continuous functions) If Z is compact w.r.t. metric \u03c1, l(As, \u00b7) is Lipschitz continuous with Lipschitz constant c(s), i.e.,\n|l(As, z1)\u2212 l(As, z2)| \u2264 c(s)\u03c1(z1, z2), \u2200z1, z2 \u2208 Z,\nthen A is ( N (\u03b3/2,Z, \u03c1), c(s)\u03b3 ) -robust for all \u03b3 > 0.\nTheorem 14 also implies that SVM, Lasso, feed-forward neural network and PCA are robust, as stated in Example 5 to Example 8. The proofs are deferred to Appendix A.3 to A.6.\nExample 5 (Support Vector Machine) Let X be compact. Consider the standard SVM formulation (Cortes and Vapnik, 1995; Scho\u0308lkopf and Smola, 2002)\nMinimize:w,d c\u2016w\u2016 2 H +\n1 n\nn \u2211\ni=1\n\u03bei\ns. t. 1\u2212 si|y[\u3008w, \u03c6(si|x)\u3009+ d] \u2264 \u03bei;\n\u03bei \u2265 0.\nHere \u03c6(\u00b7) is a feature mapping, \u2016 \u00b7 \u2016H is its RKHS kernel, and k(\u00b7, \u00b7) is the kernel function. Let l(\u00b7, \u00b7) be the hinge-loss, i.e., l ( (w, d), z )\n= [1\u2212 z|y(\u3008w,\u03c6(z|x)\u3009+d)] +, and define fH(\u03b3) ,\nmaxa,b\u2208X ,\u2016a\u2212b\u20162\u2264\u03b3 ( k(a,a)+k(b,b)\u22122k(a,b) ) . If k(\u00b7, \u00b7) is continuous, then for any \u03b3 > 0, fH(\u03b3) is finite, and SVM is (2N (\u03b3/2,X , \u2016 \u00b7 \u20162), \u221a fH(\u03b3)/c) robust.\nExample 6 (Lasso) Let Z be compact and the loss function be l(As, z) = |z|y \u2212 As(z|x)|. Lasso (Tibshirani, 1996), which is the following regression formulation:\nmin w\n: 1\nn\nn \u2211\ni=1\n(si|y \u2212 w \u22a4si|x) 2 + c\u2016w\u20161, (7)\nis ( N (\u03b3/2,Z, \u2016 \u00b7 \u2016\u221e), (Y (s)/c + 1)\u03b3 ) -robust for all \u03b3 > 0, where Y (s) , 1n \u2211n i=1 si|y 2 .\nExample 7 (Feed-forward Neural Networks) Let Z be compact and the loss function be l(As, z) = |z|y \u2212 As(z|x)|. Consider the d-layer neural network (trained on s), which is the following predicting rule given an input x \u2208 X\nx0 := z|x\n\u2200v = 1, \u00b7 \u00b7 \u00b7 , d\u2212 1 : xvi := \u03c3(\nNv\u22121 \u2211\nj=1\nwv\u22121ij x v\u22121 j ); i = 1, \u00b7 \u00b7 \u00b7 , Nv;\nAs(x) := \u03c3(\nNd\u22121 \u2211\nj=1\nwd\u22121j x d\u22121 j );\nIf there exists \u03b1, \u03b2 such that the d-layer neural network satisfying that |\u03c3(a)\u2212\u03c3(b)| \u2264 \u03b2|a\u2212b|, and \u2211Nv\nj=1 |w v ij | \u2264 \u03b1 for all v, i, then it is\n(\nN (\u03b3/2,Z, \u2016 \u00b7 \u2016\u221e), \u03b1 d\u03b2d\u03b3\n)\n-robust, for all \u03b3 > 0.\nWe remark that in Example 7, the number of hidden units in each layer has no effect on the robustness of the algorithm and consequently the bound on the testing error. This indeed agrees with Bartlett (1998), where the author showed (using a different approach based on fat-shattering dimension) that for neural networks, the weight plays a more important role than the number of hidden units.\nThe next example considers an unsupervised learning algorithm, namely the principal component analysis. We show that it is robust if the sample space is bounded. Note that, this does not contradict with the well known fact that the principal component analysis is sensitive to outliers which are far away from the origin.\nExample 8 (Principal Component Analysis (PCA)) Let Z \u2282 Rm, such that maxz\u2208Z \u2016z\u20162 \u2264 B. If the loss function is l((w1, \u00b7 \u00b7 \u00b7 , wd), z) = \u2211d k=1(w \u22a4 k z) 2, then finding the first d principal components, which solves the following optimization problem of w1, \u00b7 \u00b7 \u00b7 , wd \u2208 R m,\nMaximize:\nn \u2211\ni=1\nd \u2211\nk=1\n(w\u22a4k si) 2\nSubject to: \u2016wk\u20162 = 1, k = 1, \u00b7 \u00b7 \u00b7 , d;\nw\u22a4i wj = 0, i 6= j.\nis (N (\u03b3/2,Z, \u2016 \u00b7 \u20162), 2d\u03b3B)-robust.\nThe last example is large-margin classification, which is a generalization of Example 1. We need the following standard definition (e.g., Bartlett, 1998) of the distance of a point to a classification rule.\nDefinition 15 Fix a metric \u03c1 of X . Given a classification rule \u2206 and x \u2208 X , the distance of x to \u2206 is\nD(x,\u2206) , inf{c \u2265 0|\u2203x\u2032 \u2208 X : \u03c1(x, x\u2032) \u2264 c,\u2206(x) 6= \u2206(x\u2032)}.\nA large margin classifier is a classification rule such that most of the training samples\nare \u201cfar away\u201d from the classification boundary.\nExample 9 (Large-margin classifier) If there exist \u03b3 and n\u0302 such that\nn \u2211\ni=1\n1 ( D(si|x,As) > \u03b3 ) \u2265 n\u0302,\nthen algorithm A is (2N (\u03b3/2,X , \u03c1), 0, n\u0302) pseudo robust, provided that N (\u03b3/2,X , \u03c1) < \u221e.\nNote that if we take \u03c1 to be the Euclidean norm, and let n\u0302 = n, then we recover Example 1."}, {"heading": "6. Necessity of Robustness", "text": "Thus far we have considered finite sample generalization bounds of robust algorithms. We now turn to asymptotic analysis, i.e., we are given an increasing set of training samples s = (s1, s2, \u00b7 \u00b7 \u00b7 ) and tested on an increasing set of testing samples t = (t1, t2, \u00b7 \u00b7 \u00b7 ). We use s(n) and t(n) to denote the first n elements of training samples and testing samples respectively. For succinctness, we let L(\u00b7, \u00b7) to be the average loss given a set of samples, i.e., for h \u2208 H,\nL(h, t(n)) \u2261 1\nn\nn \u2211\ni=1\nl(h, ti).\nWe show in this section that robustness is an essential property of successful learning. In particular, a (weaker) notion of robustness characterizes generalizability, i.e., a learning algorithm generalizes if and only if it is weakly robust. To make this precise, we define the notion of generalizability and weak robustness first.\nDefinition 16 1. A learning algorithm A generalizes w.r.t. s if\nlim sup n\n{\nEt\n( l(As(n), t) ) \u2212 L(As(n), s(n)) } \u2264 0.\n2. A learning algorithm A generalize w.p. 1 if it generalize w.r.t. almost every s.\nWe remark that the proposed notion of generalizability differs slightly from the standard one in the sense that the latter requires that the empirical risk and the expected risk converges in mean, while the proposed notion requires convergence w.p.1. It is straightforward that the proposed notion implies the standard one.\nDefinition 17 1. A learning algorithm A is weakly robust w.r.t s if there exists a\nsequence of {Dn \u2286 Z n} such that Pr(t(n) \u2208 Dn) \u2192 1, and\nlim sup n\n{\nmax s\u0302(n)\u2208Dn\n[ L(As(n), s\u0302(n))\u2212L(As(n), s(n)) ]\n}\n\u2264 0.\n2. A learning algorithm A is a.s. weakly robust if it is robust w.r.t. almost every s.\nWe briefly comment on the definition of weak robustness. Recall that the definition of robustness requires that the sample space can be partitioned into disjoint subsets such that if a testing sample belongs to the same partitioning set of a training sample, then they have similar loss. Weak robustness generalizes such notion by considering the average loss of testing samples and training samples. That is, if for a large (in the probabilistic sense) subset of Zn, the testing error is close to the training error, then the algorithm is weakly robust. It is easy to see, by Breteganolle-Huber-Carol lemma, that if for any fixed \u01eb > 0 there exists K such that A is (K, \u01eb) robust, then A is weakly robust.\nWe now establish the main result of this section: weak robustness and generalizability\nare equivalent.\nTheorem 18 An algorithm A generalizes w.r.t. s if and only if it is weakly robust w.r.t. s.\nProof We prove the sufficiency of weak robustness first. When A is weakly robust w.r.t. s, by definition there exists {Dn} such that for any \u03b4, \u01eb > 0, there exists N(\u03b4, \u01eb) such that for all n > N(\u03b4, \u01eb), Pr(t(n) \u2208 Dn) > 1\u2212 \u03b4, and\nsup s\u0302(n)\u2208Dn L(As(n), s\u0302(n))\u2212 L(As(n), s(n)) < \u01eb. (8)\nTherefore, the following holds for any n > N(\u03b4, \u01eb),\nEt\n( l(As(n), t) ) \u2212 L(As(n), s(n))\n=Et(n)\n( L(As(n), t(n)) ) \u2212 L(As(n), s(n))\n=Pr(t(n) 6\u2208 Dn)E ( L(As(n), t(n))|t(n) 6\u2208 Dn ) +Pr(t(n) \u2208 Dn)E ( L(As(n), t(n))|t(n) \u2208 Dn )\n\u2212 L(As(n), s(n))\n\u2264\u03b4M + sup s\u0302(n)\u2208Dn\n{ L(As(n), s\u0302(n))\u2212 L(As(n), s(n)) } \u2264 \u03b4M + \u01eb.\nHere, the first equality holds by i.i.d. of t(n), and the second equality holds by conditional expectation. The inequalities hold due to the assumption that the loss function is upper bounded by M , as well as (8).\nWe thus conclude that the algorithm A generalizes for s, because \u01eb, \u03b4 can be arbitrary. Now we turn to the necessity of weak robustness. First, we establish the following\nlemma.\nLemma 19 Given s, if algorithm A is not weakly robust w.r.t. s, then there exists \u01eb\u2217, \u03b4\u2217 > 0 such that the following holds for infinitely many n,\nPr ( L(As(n), t(n)) \u2265 L(As(n), s(n)) + \u01eb \u2217 ) \u2265 \u03b4\u2217. (9)\nProof We prove the lemma by contradiction. Assume that such \u01eb\u2217 and \u03b4\u2217 do not exist. Let \u01ebv = \u03b4v = 1/v for v = 1, 2 \u00b7 \u00b7 \u00b7 , then there exists a non-decreasing sequence {N(v)} \u221e v=1 such that for all v, if n \u2265 N(v) then Pr ( L(As(n), t(n)) \u2265 L(As(n), s(n)) + \u01ebv ) < \u03b4v. For each n, define the following set:\nDvn , {s\u0302(n)|L(As(n), s\u0302(n))\u2212L(As(n), s(n)) < \u01ebv}.\nThus, for n \u2265 N(v) we have\nPr(t(n) \u2208 Dvn) = 1\u2212 Pr ( L(As(n), t(n)) \u2265 L(As(n), s(n)) + \u01ebv ) > 1\u2212 \u03b4v .\nFor n \u2265 N(1), define Dn , D v(n) n , where: v(n) , max ( v|N(t) \u2264 n; v \u2264 n ) . Thus for all n \u2265 N(1) we have that Pr(t(n) \u2208 Dn) > 1 \u2212 \u03b4v(n) and sups\u0302(n)\u2208Dn L(As(n), s\u0302(n)) \u2212 L(As(n), s(n)) < \u01ebv(n). Note that v(n) \u2191 \u221e, it follows that \u03b4v(n) \u2192 0 and \u01ebv(n) \u2192 0. Therefore, Pr(t(n) \u2208 Dn) \u2192 1, and\nlim sup n\u2192\u221e\n{\nsup s\u0302(n)\u2208Dn\nL(As(n), s\u0302(n))\u2212L(As(n), s(n)) } \u2264 0.\nThat is, A is weakly robust w.r.t. s, which is a desired contradiction.\nWe now prove the necessity of weak robustness. Recall that l(\u00b7, \u00b7) is uniformly bounded. Thus by Hoeffding\u2019s inequality we have that for any \u01eb, \u03b4, there exists n\u2217 such that for any n > n\u2217, with probability at least 1 \u2212 \u03b4, we have \u2223 \u2223\n\u2223 1 n \u2211n i=1 l(As(n), ti) \u2212 Et(l(As(n), t))\n\u2223 \u2223 \u2223 \u2264 \u01eb.\nThis implies that\nL(As(n), t(n))\u2212 Etl(As(n), t) Pr \u2212\u2192 0. (10)\nSince algorithm A is not robust, Lemma 19 implies that (9) holds for infinitely many n. This, combined with Equation (10) implies that for infinitely many n,\nEtl(As(n), t) \u2265 L(As(n), s(n)) + \u01eb\u2217\n2 ,\nwhich means that A does not generalize. Thus, the necessity of weak robustness is established.\nTheorem 18 immediately leads to the following corollary.\nCorollary 20 An algorithm A generalizes w.p. 1 if and only if it is a.s. weakly robust."}, {"heading": "7. Discussion", "text": "In this paper we investigated the generalization ability of learning algorithm based on their robustness: the property that if a testing sample is \u201csimilar\u201d to a training sample, then its loss is close to the training error. This provides a novel approach, different from the complexity or stability argument, in studying the performance of learning algorithms. We further showed that a weak notion of robustness characterizes generalizability, which implies that robustness is a fundamental property for learning algorithms to work.\nBefore concluding the paper, we outline several directions for future research.\n\u2022 Adaptive partition: In Definition 2 when the notion of robustness was introduced, we\nrequired that the partitioning of Z into K sets is fixed. That is, regardless of the training sample set, we partition Z into the same K sets. A natural and interesting question is what if such fixed partition does not exist, while instead we can only partition Z intoK sets adaptively, i.e., for different training set we will have a different partitioning of Z. Adaptive partition setup can be used to study algorithms such as k-NN. Our current proof technique does not straightforwardly extend to such a setup, and we would like to understand whether a meaningful generalization bound under this weaker notion of robustness can be obtained.\n\u2022 Mismatched datasets: One advantage of algorithmic robustness framework is the abil-\nity to handle non-standard learning setups. For example, in Section 3.2 and 3.3 we derived generalization bounds for quantile loss and for samples drawn from a Markovian chain, respectively. A problem of the same essence is the mismatched datasets, where the training samples are generated according to a distribution slightly different from that of the testing samples, e.g., the two distributions may have a small K-L divergence. We conjecture that in this case a generalization bound similar to Theorem 3 would be possible, with an extra term depending on the magnitude of the difference of the two distributions.\n\u2022 Outlier removal: One possible reason that the training samples is generated differently\nfrom the testing sample is outlier corruption. It is often the case that the training sample set is corrupted by some outliers. In addition, algorithms designed to be outlier resistent abound in the literature (e.g., Huber, 1981; Rousseeuw and Leroy, 1987). The robust framework may provide a novel approach in studying both the generalization ability and the outlier resistent property of these algorithms. In particular, the results reported in Section 3.2 can serve as a starting point of future research in this direction.\n\u2022 Consistency: We addressed in this paper the relationship between robustness and gen-\neralizability. An equally important feature of learning algorithms is consistency: the property that a learning algorithm guarantees to recover the global optimal solution as the number of training data increases. While it is straightforward that if an algorithm\nminimizes the empirical error asymptotically and also generalizes (or equivalently is weakly robust), then it is consistent, much less is known for a necessary condition for an algorithm to be consistent. It is certainly interesting to investigate the relationship between consistency and robustness, and in particular whether robustness is necessary for consistency, at least for algorithms that asymptotically minimize the empirical error.\n\u2022 Other robust algorithms: The proposed robust approach considers a general learning\nsetup. However, except for PCA, the algorithms investigated in Section 5 all belong to the supervised learning setting. One natural extension is to investigate other robust unsupervised and semi-supervised learning algorithms. One difficulty is that compared to supervised learning case, the analysis of unsupervised/semi-supervised learning algorithms can be challenging, due to the fact that many of them are random iterative algorithms (e.g., k-means)."}, {"heading": "Appendix A. Proofs", "text": ""}, {"heading": "A.1 Proof of Theorem 13", "text": "We observe the following properties of quantile value and truncated mean:\n1. If X is supported on R+ and \u03b21 \u2265 \u03b22, then\nQ\u03b21(X) \u2265 Q\u03b22(X); T\u03b21(X) \u2265 T\u03b22(X).\n2. If Y stochastically dominates X, i.e., Pr(Y \u2265 a) \u2265 Pr(X \u2265 a) for all a \u2208 R, then for\nany \u03b2,\nQ\u03b2(Y ) \u2265 Q\u03b2(X); T\u03b2(Y ) \u2265 T\u03b2(X).\n3. The \u03b2-truncated mean of empirical distribution of nonnegative (x1, \u00b7 \u00b7 \u00b7 , xn) is given\nby\nmin \u03b1:0\u2264\u03b1i\u22641/n, \u2211n i=1 \u03b1i\u2264\u03b2\nn \u2211\ni=1\n\u03b1ixi.\nBy definition of pseudo-robustness, Z can be partitioned into K disjoint sets, denoted\nas {Ci} K i=1, and a subset of training samples s\u0302 with |s\u0302| = n\u0302 such that\nz1 \u2208 s\u0302, z1, z2 \u2208 Ci, =\u21d2 |l(As, z1)\u2212 l(As, z2)| \u2264 \u01eb(s); \u2200s.\nLet Ni be the set of index of points of s that fall into the Ci. Let E be the event that\nthe following holds:\nK \u2211\ni=1\n\u2223 \u2223 \u2223 \u2223 |Ni|\nn \u2212 \u00b5(Ci)\n\u2223 \u2223 \u2223 \u2223 \u2264\n\u221a\n2K ln 2 + 2 ln(1/\u03b4)\nn .\nFrom the proof of Theorem 3, Pr(E) \u2265 1 \u2212 \u03b4. Hereafter we restrict the discussion to the case when E holds.\nDenote\nvj = argmin z\u2208Cj l(As, z).\nBy symmetry, without loss of generality we assume that 0 \u2264 l(As, v1) \u2264 l(As, v2) \u2264 \u00b7 \u00b7 \u00b7 \u2264 l(As, vK) \u2264 M . Define a set of samples s\u0303 as\ns\u0303i =\n{\nsi if si \u2208 s\u0302; vj if si 6\u2208 s\u0302, si \u2208 Cj.\nDefine discrete probability measures \u00b5\u0302 and \u00b5\u0303, supported on {v1, \u00b7 \u00b7 \u00b7 , vK} as\n\u00b5\u0302({vj}) = \u00b5(Cj); \u00b5\u0303({vj}) = |Nj |\nn .\nFurther, let \u00b5\u0303emp denote the empirical distribution of sample set s\u0303.\nProof of (I): Observe that \u00b5 stochastically dominates \u00b5\u0302, hence\nQ(As, \u03b2, \u00b5\u0302) \u2264 Q(As, \u03b2, \u00b5). (11)\nAlso by definition of Q(\u00b7) and \u00b5\u0302,\nQ(As, \u03b2, \u00b5\u0302) = vk\u2217 ; where: k \u2217 = min{k :\nk \u2211\ni=1\n\u00b5\u0302(vi) \u2265 \u03b2}.\nLet s be the set of all samples si such that si \u2208 s\u0302, and si \u2208 Cj for some j \u2264 k \u2217. Observe that\n\u2200si \u2208 s\u0302 : l(As, si) \u2264 vk\u2217 + \u01eb(s) = Q(As, \u03b2, \u00b5\u0302) + \u01eb(s). (12)\nNote that E implies\n1 n\nk\u2217 \u2211\nj=1\n\u2211\nsi\u2208Cj\n1 \u2265 k\u2217 \u2211\nj=1\n\u00b5(Cj)\u2212 \u03bb0 = k \u2211\nj=1\n\u00b5\u0302(vj)\u2212 \u03bb0 \u2265 \u03b2 \u2212 \u03bb0.\nSince As is pseudo robust, we have\n1 n \u2211\nsi 6\u2208s\u0302\n= n\u2212 n\u0302\nn .\nTherefore\n1 n\nk\u2217 \u2211\nj=1\n\u2211\nsi\u2208s,si\u2208Cj\n1 \u2265 1\nn\nk\u2217 \u2211\nj=1\n\u2211\nsi\u2208Cj\n1\u2212 1\nn\n\u2211\nsi 6\u2208s\u0302\n1 \u2265 \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn .\nThus, s is a subset of s of at least n(\u03b2 \u2212 \u03bb0 \u2212 (n\u2212 n\u0302)/n) elements. Thus (11) and (12) lead to\nQ(As, \u03b2 \u2212 \u03bb0 \u2212 (n\u2212 n\u0302)/n, \u00b5emp) \u2264 max{si : si \u2208 s} \u2264 Q(As, \u03b2, \u00b5) + \u01eb(s).\nThus, we establish the left inequality. The proof of the right one is identical and hence omitted.\nProof of (II): The proof constitutes four steps. Step 1: Observe that \u00b5 stochastically dominates \u00b5\u0302, hence\nT (As, \u03b2, \u00b5\u0302) \u2264 T (As, \u03b2, \u00b5).\nStep 2: We prove that\nT (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303) \u2264 T (As, \u03b2, \u00b5\u0302).\nNote that t E implies for all j, we have\n\u00b5\u0303({v1, \u00b7 \u00b7 \u00b7 , vj})\u2212 \u03bb0 \u2264 \u00b5\u0302({v1, \u00b7 \u00b7 \u00b7 , vj}),\nTherefore, there uniquely exists a non-negative integer j\u2217 and a c\u2217 \u2208 [0, 1) such that\n\u00b5\u0302({v1, \u00b7 \u00b7 \u00b7 , vj\u2217}) + c \u2217\u00b5\u0302({vj\u2217+1}) = \u03b2,\nand define\n\u03b2\u0302 =\nj\u2217 \u2211\ni=1\nmin(\u00b5\u0303({vi}), \u00b5\u0302({vi})) + c \u2217 min(\u00b5\u0303({vj\u2217+1}), \u00b5\u0302({vj\u2217+1})), (13)\nthen we have \u03b2\u0302 \u2265 \u03b2 \u2212 \u03bb0, which leads to\nT (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303) \u2264 T (As, \u03b2\u0302, \u00b5\u0303)\n(a) \u2264\nj\u2217 \u2211\ni=1\nl(As, vi)min(\u00b5\u0303({vi}), \u00b5\u0302({vi})) + c \u2217l(As, vj\u2217+1)min(\u00b5\u0303({vj\u2217+1}), \u00b5\u0302({vj\u2217+1}))\n\u2264\nj\u2217 \u2211\ni=1\nl(As, vi)\u00b5\u0302({vi}) + c \u2217l(As, vj\u2217+1)\u00b5\u0302({vj\u2217+1}) = T (As, \u03b2, \u00b5\u0302),\nwhere (a) holds because Equation (13) essentially means that T (As, \u03b2\u0302, \u00b5\u0303) is a weighted sum with total weights equals to \u03b2\u0302, which puts more weights on small terms, and hence is smaller.\nStep 3: We prove that\nT (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303emp)\u2212 \u01eb(s) \u2264 T (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303).\nLet t\u0303 be a set of n samples, such that Nj of them are vj for j = 1, \u00b7 \u00b7 \u00b7 ,K. Observe that \u00b5\u0303 is the empirical distribution of t\u0303. Further note that there is a one-to-one mapping between samples in s\u0303 and that in t\u0303 such that each pair (say s\u0303i, t\u0303i) of samples belongs to the same Cj. By definition of s\u0303 this guarantees that |l(As, s\u0303i)\u2212 l(As, t\u0303i)| \u2264 \u01eb(s), which implies\nT (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303emp)\u2212 \u01eb(s) \u2264 T (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303).\nStep 4: We prove that\nT (As, \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn , \u00b5emp) \u2264 T (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303emp).\nLet I = {i : si = s\u0303i}), the following holds:\nn \u2211\ni=1\n\u03b1il(As, s\u0303i) \u2265 \u2211\ni\u2208I\n\u03b1il(As, s\u0303i) = \u2211\ni\u2208I\n\u03b1il(As, si); \u2200\u03b1 : 0 \u2264 \u03b1i \u2264 1\nn ;\nn \u2211\ni=1\n\u03b1i = \u03b2 \u2212 \u03bb0.\nNote that |{i 6\u2208 I}| = n \u2212 n\u0302, then \u2211 i\u2208I \u03b1i \u2265 \u03b2 \u2212 \u03bb0 \u2212 n\u2212n\u0302 n . Thus we have \u2200\u03b1 : 0 \u2264 \u03b1i \u2264 1 n ; \u2211n i=1 \u03b1i = \u03b2 \u2212 \u03bb0,\n\u2211\ni\u2208I\n\u03b1il(As, si) \u2265 min \u03b1\u2032:0\u2264\u03b1\u2032i\u2264 1 n , \u2211n i=1 \u03b1 \u2032 i\u2264\u03b2\u2212\u03bb0\u2212 n\u2212n\u0302 n\nn \u2211\ni=1\n\u03b1\u2032il(As, si) = T (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303emp).\nTherefore,\nn \u2211\ni=1\n\u03b1il(As, s\u0303i) \u2265 T (As, \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn , \u00b5emp); \u2200\u03b1 : 0 \u2264 \u03b1i \u2264\n1 n ;\nn \u2211\ni=1\n\u03b1i = \u03b2 \u2212 \u03bb0.\nMinimization over \u03b1 on both side. We proved\nT (As, \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn , \u00b5emp) \u2264 T (As, \u03b2 \u2212 \u03bb0, \u00b5\u0303emp).\nCombining all four steps, we proved the left inequality, i.e.,\nT (As, \u03b2 \u2212 \u03bb0 \u2212 n\u2212 n\u0302\nn , \u00b5emp)\u2212 \u01eb(s) \u2264 T (As, \u03b2, \u00b5).\nThe right inequality can be proved identically and hence omitted."}, {"heading": "A.2 Proof of Example 3", "text": "We can partition Z as {\u22121}\u00d7C1, \u00b7 \u00b7 \u00b7 , {\u22121}\u00d7CK , {+1}\u00d7C1, \u00b7 \u00b7 \u00b7 , {+1}\u00d7CK . Consider za, zb that belong to a same set, then za|y = zb|y, and \u2203i such that za|x, zb|x \u2208 Ci, which by the definition of Majority Voting algorithm implies that As(za|x) = As(zb|x). Thus, we have\nl(As, za) = f(za|y,As(za|x)) = f(zb|y,As(zb|x)) = l(As, zb).\nHence MV is (2K, 0)-robust."}, {"heading": "A.3 Proof of Example 5", "text": "The existence of fH(\u03b3) follows from the compactness of X and continuity of k(\u00b7, \u00b7).\nTo prove the robustness of SVM, let (w\u2217, d\u2217) be the solution given training data s. To avoid notation clutter, let yi = si|y and xi = si|x. Thus, we have (due to optimality of w\u2217, d\u2217)\nc\u2016w\u2217\u20162H + 1\nn\nn \u2211\ni=1\n[1\u2212 yi(\u3008w \u2217, \u03c6(xi)\u3009+ d \u2217)]+ \u2264 c\u20160\u20162H + 1\nn\nn \u2211\ni=1\n[1\u2212 yi(\u30080, \u03c6(xi)\u3009+ 0)] + = 1,\nwhich implies \u2016w\u2217\u2016H \u2264 \u221a 1/c. Let c1, \u00b7 \u00b7 \u00b7 , cN (\u03b3/2,X ,\u2016\u00b7\u20162) be a \u03b3/2-cover of X (recall that X is compact), then we can partition Z as 2N (\u03b3/2,X , \u2016 \u00b7 \u20162) sets, such that if (y1, x1) and (y2, x2) belongs to the same set, then y1 = y2 and \u2016x1 \u2212 x2\u20162 \u2264 \u03b3/2.\nFurther observe that if y1 = y2 and \u2016x1 \u2212 x2\u20162 \u2264 \u03b3/2, then\n|l ( (w\u2217, d\u2217), z1 ) \u2212 l ( (w\u2217, d\u2217), z2) ) |\n= \u2223 \u2223[1\u2212 y1(\u3008w \u2217, \u03c6(x1)\u3009+ d \u2217)]+ \u2212 [1\u2212 y2(\u3008w \u2217, \u03c6(x2)\u3009+ d \u2217)]+ \u2223 \u2223 \u2264 |\u3008w\u2217, \u03c6(x1)\u2212 \u03c6(x2)\u3009| \u2264 \u2016w\u2217\u2016H \u221a \u3008\u03c6(x1)\u2212 \u03c6(x2), \u03c6(x1)\u2212 \u03c6(x2)\u3009 \u2264 \u221a\nfH(\u03b3)/c.\nHere the last inequality follows from the definition of fH. Hence, the example holds by Theorem 14."}, {"heading": "A.4 Proof of Example 6", "text": "It suffices to show the following lemma, which establish that loss of Lasso solution is Liptschitz continuous.\nLemma 21 If w\u2217(s) is the solution of Lasso given training set s, then\n|l(w\u2217(s), za)\u2212 l(w \u2217(s), zb)| \u2264\n[ 1\nnc\nn \u2211\ni=1\nsi|y 2 + 1\n]\n\u2016za \u2212 zb\u2016\u221e.\nProof For succinctness we let yi = si|y, xi = si|x for i = 1, \u00b7 \u00b7 \u00b7 , n. Similarly, we let ya(b) = za(b)|y and xa(b) = za(b)|x. Since w \u2217(s) is the solution of Lasso, we have (due to optimality)\n1 n\nn \u2211\ni=1\n(yi \u2212 x \u22a4 i w \u2217(s))2 + c\u2016w\u2217(s)\u20161 \u2264 1\nn\nn \u2211\ni=1\n(yi \u2212 x \u22a4 i 0) 2 + c\u20160\u20161 = 1\nn\nn \u2211\ni=1\nyi 2,\nwhich implies \u2016w\u2217\u20161 \u2264 1 nc \u2211n i=1 yi 2. Therefore,\n|l(w\u2217(s), za)\u2212 l(w \u2217(s), zb)| = ||ya \u2212 w \u2217(s)xa| \u2212 |yb \u2212w \u2217(s)xb||\n\u2264 |(ya \u2212w \u2217(s)xa)\u2212 (yb \u2212 w \u2217(s)xb)| \u2264|ya \u2212 yb|+ \u2016w \u2217(s)\u20161\u2016xa \u2212 xb\u2016\u221e \u2264(\u2016w\u2217(s)\u20161 + 1)\u2016za \u2212 zb\u2016\u221e\n= [ 1\nnc\nn \u2211\ni=1\nyi 2 + 1\n]\n\u2016za \u2212 zb\u2016\u221e.\nHere the first two inequalities holds from triangular inequality, and the last inequality holds due to z = (x, y)."}, {"heading": "A.5 Proof of Example 7", "text": "To see why the example holds, it suffices to show the following lemma, which establishes that the neural network mentioned is Lipschitz continuous. For simplicity, we write the prediction given x \u2208 X as NN(x).\nLemma 22 Fixed \u03b1, \u03b2, if a d-layer neural network satisfying that |\u03c3(a)\u2212 \u03c3(b)| \u2264 \u03b2|a\u2212 b|, and \u2211Nv\nj=1 |w v ij | \u2264 \u03b1 for all v, i, then the following holds:\n|l(As, z)\u2212 l(As, z\u0302)| \u2264 (1 + \u03b1 d\u03b2d)\u2016z \u2212 z\u0302\u2016\u221e.\nProof Let xvi and x\u0302 v i be the output of the i th unit of the vth layer for samples z and z\u0302 respectively. Let xv and x\u0302v be the vector such that the ith elements are xvi and x\u0302 v i respectively. From \u2211Nv\ni=1 |w v i | \u2264 \u03b1 we have\n|xvi \u2212 x\u0302 v i | =\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 \u03c3( Nv \u2211\nj=1\nwvijx v\u22121 i )\u2212 \u03c3(\nNv \u2211\nj=1\nwvijx\u0302 v\u22121 j )\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u03b2\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 Nv \u2211\nj=1\nwvijx v\u22121 i \u2212\nNv \u2211\nj=1\nwvijx\u0302 v\u22121 j\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 \u03b2\u03b1\u2016xv\u22121 \u2212 x\u0302v\u22121\u2016\u221e.\nHere, the first inequality holds from the Lipschitz condition of \u03c3, and the second inequality holds from \u2211Nv\nj=1 |w v ij| \u2264 \u03b1. Iterating over d layers, we have\n|NN(z|x)\u2212NN(z\u0302|x)| = |x d \u2212 x\u0302d| \u2264 \u03b1d\u03b2d\u2016x\u2212 x\u0302\u2016\u221e,\nwhich implies\n|l(As, z)\u2212 l(As, z\u0302)| = \u2223 \u2223|z|y \u2212NN(z|x)| \u2212 |z\u0302|y \u2212NN(z\u0302|x)| \u2223 \u2223\n\u2264\u2016z|y \u2212 z\u0302|y|+ |NN(z|x)\u2212NN(z\u0302|x)| \u2264(1 + \u03b1d\u03b2d)\u2016z \u2212 z\u0302\u2016\u221e.\nThis proves the lemma."}, {"heading": "A.6 Proof of Example 8", "text": "We show that the loss to PCA is Lipschitz continuous, and then apply Theorem 14.\nLet (w\u22171(s), \u00b7 \u00b7 \u00b7 , w \u2217 d(s)) be the solution of PCA trained on s. Thus we have\n|l((w\u22171(s), \u00b7 \u00b7 \u00b7 , w \u2217 d(s)), za)\u2212 l((w \u2217 1(s), \u00b7 \u00b7 \u00b7 , w \u2217 d(s)), zb)|\n=\n\u2223 \u2223 \u2223 \u2223 \u2223 d \u2211\nk=1\n(w\u2217k(s) \u22a4za)\n2 \u2212 d \u2211\nk=1\n(w\u2217k(s) \u22a4zb) 2\n\u2223 \u2223 \u2223 \u2223 \u2223\n\u2264 d \u2211\nk=1\n\u2223 \u2223 \u2223 [w\u2217k(s) \u22a4za \u2212 w \u2217 k(s) \u22a4zb][w \u2217 k(s) \u22a4za + w \u2217 k(s) \u22a4zb] \u2223 \u2223 \u2223\n\u22642dB\u2016za \u2212 zb\u20162,\nwhere the last inequality holds because \u2016w\u2217k(s)\u20162 = 1 and \u2016za\u2016, \u2016zb\u2016 \u2264 B. Hence, the example holds by Theorem 14."}, {"heading": "A.7 Proof of Example 9", "text": "Set s\u0302 as\ns\u0302 , {si \u2208 s|D(si,As) > \u03b3}.\nAnd let c1, \u00b7 \u00b7 \u00b7 , cN (\u03b3/2,X ,\u03c1) be a \u03b3/2 cover of X . Thus, we can partition Z to 2N (\u03b3/2,X , \u03c1) subsets {Ci}, such that if\nz1, z2 \u2208 Ci; =\u21d2 y1 = y2; & \u03c1(x1, x2) \u2264 \u03b3.\nThis implies that:\nz1 \u2208 s\u0302, z1, z2 \u2208 Ci; =\u21d2 y1 = y2; As(x1) = As(x2); =\u21d2 l(As, z1) = l(As, z2).\nBy definition, A is (2N (\u03b3/2,X , \u03c1), 0, n\u0302) pseudo robust."}], "references": [{"title": "Scale-sensitive dimension, uniform convergence, and learnability", "author": ["N. Alon", "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler"], "venue": "Journal of the ACM,", "citeRegEx": "Alon et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1997}, {"title": "The sample complexity of pattern classification with neural networks: The size of the weight is more important than the size of the network", "author": ["P.L. Bartlett"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Bartlett.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett.", "year": 1998}, {"title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Local Rademacher complexities", "author": ["P.L. Bartlett", "O. Bousquet", "S. Mendelson"], "venue": "The Annals of Statistics,", "citeRegEx": "Bartlett et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2005}, {"title": "Robust convex optimization", "author": ["A. Ben-tal", "A. Nemirovski"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Ben.tal and Nemirovski.,? \\Q1998\\E", "shortCiteRegEx": "Ben.tal and Nemirovski.", "year": 1998}, {"title": "The price of robustness", "author": ["D. Bertsimas", "M. Sim"], "venue": "Operations Research,", "citeRegEx": "Bertsimas and Sim.,? \\Q1999\\E", "shortCiteRegEx": "Bertsimas and Sim.", "year": 1999}, {"title": "Support vector networks", "author": ["Cortes", "V.N. Vapnik"], "venue": "Machine Learning,", "citeRegEx": "Cortes and Vapnik.,? \\Q2002\\E", "shortCiteRegEx": "Cortes and Vapnik.", "year": 2002}, {"title": "Markov Chains and Stochastic Stability", "author": ["S.P. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "1989", "shortCiteRegEx": "1989", "year": 1993}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Learnability and stability in the general learning setting", "author": ["S. Shalev-Shwartz", "O. Shamir", "N. Srebro", "K. Sridharan"], "venue": "In Proceedings of 22nd Annual Conference of Learning Theory,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2009}, {"title": "Second order cone programming approaches for handling missing and uncertain data", "author": ["P.K. Shivaswamy", "C. Bhattacharyya", "A.J. Smola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Shivaswamy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Shivaswamy et al\\.", "year": 2006}, {"title": "Regression shrinkage and selection via the Lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Weak Convergence and Empirical Processes", "author": ["A.W. van der Vaart", "J.A. Wellner"], "venue": null, "citeRegEx": "Vaart and Wellner.,? \\Q2000\\E", "shortCiteRegEx": "Vaart and Wellner.", "year": 2000}, {"title": "The necessary and sufficient conditions for consistency in the empirical risk minimization method", "author": ["V.N. Vapnik", "A. Chervonenkis"], "venue": "Pattern Recognition and Image Analysis,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q1991\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 1991}, {"title": "Robust regression and Lasso", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2009}, {"title": "Robustness and regularization of support vector machines", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Xu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": ", 2000), the fat-shattering dimension (e.g., Alon et al., 1997; Bartlett, 1998), and the Rademacher complexity (Bartlett and Mendelson, 2002; Bartlett et al.", "startOffset": 38, "endOffset": 79}, {"referenceID": 2, "context": ", 1997; Bartlett, 1998), and the Rademacher complexity (Bartlett and Mendelson, 2002; Bartlett et al., 2005).", "startOffset": 55, "endOffset": 108}, {"referenceID": 3, "context": ", 1997; Bartlett, 1998), and the Rademacher complexity (Bartlett and Mendelson, 2002; Bartlett et al., 2005).", "startOffset": 55, "endOffset": 108}, {"referenceID": 0, "context": ", Alon et al., 1997; Bartlett, 1998), and the Rademacher complexity (Bartlett and Mendelson, 2002; Bartlett et al., 2005). Another well-known approach is based on stability. An algorithm is stable if its output remains \u201csimilar\u201d for different sets of training samples that are identical up to removal or change of a single sample. The first results that relate stability to generalizability track back to Devroye and Wagner (1979a) and Devroye and Wagner (1979b).", "startOffset": 2, "endOffset": 432}, {"referenceID": 0, "context": ", Alon et al., 1997; Bartlett, 1998), and the Rademacher complexity (Bartlett and Mendelson, 2002; Bartlett et al., 2005). Another well-known approach is based on stability. An algorithm is stable if its output remains \u201csimilar\u201d for different sets of training samples that are identical up to removal or change of a single sample. The first results that relate stability to generalizability track back to Devroye and Wagner (1979a) and Devroye and Wagner (1979b). Later, McDiarmid\u2019s (McDiarmid, 1989), concentration inequalities facilitated new bounds on generalization error (e.", "startOffset": 2, "endOffset": 463}, {"referenceID": 4, "context": "This notion of robustness is rooted in robust optimization (Ben-tal and Nemirovski, 1998; Ben-Tal and Nemirovski, 1999; Bertsimas and Sim, 2004) where a decision maker aims to find a solution x that minimizes a (parameterized) cost function f(x, \u03be) with the knowledge that the unknown true parameter \u03be may deviate from the observed parameter \u03be\u0302.", "startOffset": 59, "endOffset": 144}, {"referenceID": 10, "context": "Robust optimization was introduced in machine learning tasks to handle exogenous noise (e.g., Bhattacharyya et al., 2004; Shivaswamy et al., 2006; Globerson and Roweis, 2006), i.", "startOffset": 87, "endOffset": 174}, {"referenceID": 13, "context": "While it is known having a finite VC-dimension (Vapnik and Chervonenkis, 1991) or equivalently being CVEEEloo stable (Mukherjee et al.", "startOffset": 47, "endOffset": 78}, {"referenceID": 9, "context": "Recently, Shalev-Shwartz et al. (2009) proposed a weaker notion of stability that is necessary and sufficient for a learning algorithm to be consistent and generalizing, provided that the problem itself is learnable.", "startOffset": 10, "endOffset": 39}, {"referenceID": 9, "context": "Recently, Shalev-Shwartz et al. (2009) proposed a weaker notion of stability that is necessary and sufficient for a learning algorithm to be consistent and generalizing, provided that the problem itself is learnable. However, learnability requires that the convergence rate is uniform with respect to all distributions, and is hence a fairly strong assumption. In particular, the standard supervised learning setup where the hypothesis set is the set of measurable functions is not learnable since no algorithm can achieve a uniform convergence rate (cf Devroye et al., 1996). Indeed, as the authors of Shalev-Shwartz et al. (2009) stated, for supervised learning problem learnability is equivalent to the generalizability of ERM, and hence reduce to the aforementioned results on ERM algorithms.", "startOffset": 10, "endOffset": 632}, {"referenceID": 12, "context": "We recall the following standard notion of covering number from van der Vaart and Wellner (2000). Definition 1 (cf.", "startOffset": 72, "endOffset": 97}, {"referenceID": 12, "context": "We recall the following standard notion of covering number from van der Vaart and Wellner (2000). Definition 1 (cf. van der Vaart and Wellner (2000)) For a metric space S, \u03c1 and T \u2282 S we say that T\u0302 \u2282 S is an \u01eb-cover of T , if \u2200t \u2208 T , \u2203t\u0302 \u2208 T\u0302 such that \u03c1(t, t\u0302) \u2264 \u01eb.", "startOffset": 72, "endOffset": 149}, {"referenceID": 8, "context": "Consider the standard SVM formulation (Cortes and Vapnik, 1995; Sch\u00f6lkopf and Smola, 2002) Minimize:w,d c\u2016w\u2016 2 H + 1 n n \u2211", "startOffset": 38, "endOffset": 90}, {"referenceID": 11, "context": "Lasso (Tibshirani, 1996), which is the following regression formulation: min w : 1 n n \u2211", "startOffset": 6, "endOffset": 24}, {"referenceID": 1, "context": "This indeed agrees with Bartlett (1998), where the author showed (using a different approach based on fat-shattering dimension) that for neural networks, the weight plays a more important role than the number of hidden units.", "startOffset": 24, "endOffset": 40}], "year": 2010, "abstractText": "We derive generalization bounds for learning algorithms based on their robustness: the property that if a testing sample is \u201csimilar\u201d to a training sample, then the testing error is close to the training error. This provides a novel approach, different from the complexity or stability arguments, to study generalization of learning algorithms. We further show that a weak notion of robustness is both sufficient and necessary for generalizability, which implies that robustness is a fundamental property for learning algorithms to work.", "creator": "LaTeX with hyperref package"}}}