{"id": "1007.3799", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jul-2010", "title": "Adapting to the Shifting Intent of Search Queries", "abstract": "search engines today present results that are often oblivious to abrupt shifts in intent. for example, the query ` independence day'usually refers to a us holiday, but the intent of this query abruptly changed during the inevitable release of a major film by that name. while no studies exactly quantify the magnitude of intent - shifting traffic, studies suggest discovering that news events, seasonal topics, pop culture, music etc account appropriately for 50 % survival of all unwanted search queries. this paper shows that the new signals a search engine receives can be used to should both determine properly that a shift in intent has happened, as well as find necessarily a result that is now more relevant. we initially present a meta - logical algorithm that marries a classifier with a bandit processing algorithm to achieve acceptable regret rate that depends logarithmically on the number of query impressions, under certain assumptions. hopefully we provide moderately strong evidence that this regret is close to the best achievable. proceeding finally, via a series of experiments, we demonstrate that our algorithm algorithm outperforms prior approaches, particularly harder as the amount of intent - shifting traffic increases.", "histories": [["v1", "Thu, 22 Jul 2010 04:58:24 GMT  (80kb)", "http://arxiv.org/abs/1007.3799v1", "This is the full version of the paper in NIPS'09"]], "COMMENTS": "This is the full version of the paper in NIPS'09", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["umar syed", "aleksandrs slivkins", "nina mishra"], "accepted": true, "id": "1007.3799"}, "pdf": {"name": "1007.3799.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["usyed@cis.upenn.edu", "slivkins@microsoft.com", "ninam@microsoft.com"], "sections": [{"heading": null, "text": "ar X\niv :1\n00 7.\n37 99\nv1 [\ncs .L\nG ]\n2 2\nJu l 2"}, {"heading": "1 Introduction", "text": "Search engines typically use a ranking function to order results. The function scores a document by the extent to which it matches the query, and documents are ordered according to this score. Usually, this function is fixed in the sense that it does not change from one query to another and also does not change over time.\nIntuitively, a query is \u201cintent-shifting\u201d if the most desired search result(s) change over time. More concretely, a query\u2019s intent has shifted if the click distribution over search results at some time differs from the click distribution at a later time. For the query \u2018tomato\u2019 on the heels of a tomato salmonella outbreak, the probability a user clicks on a news story describing the outbreak increases while the probability a user clicks on the Wikipedia entry for tomatoes rapidly decreases. There are studies that suggest that queries likely to be intent-shifting \u2014 such as pop culture, news events, trends, and seasonal topics queries \u2014 constitute roughly half of the search queries that a search engine receives [10].\nThe goal of this paper is to devise an algorithm that quickly adapts search results to shifts in user intent. Ideally, for every query and every point in time, we would like to display the search result that users are most likely to click. Since traditional ranking features like PageRank [4] change slowly over time, and may be misleading if user intent has shifted very recently, we want to use just the observed click behavior of users to decide which search results to display.\nThere are many signals a search engine can use to detect when the intent of a query shifts. Query features such as as volume, abandonment rate, reformulation rate, occurrence in news articles, and\n\u2217This is the full version of a paper in NIPS 2009. \u2020This work was done while the author was an intern at Microsoft Research and a student in the Department\nof Computer Science, Princeton University.\nthe age of matching documents can all be used to build a classifier which, given a query, determines whether the intent has shifted. We refer to these features as the context, and an occassion when a shift in intent occurs as an event.\nOne major challenge in building an event classifier is obtaining training data. For most query and date combinations (e.g. \u2018tomato, 06/09/2008\u2019), it will be difficult even for a human labeler to recall in hindsight whether an event related to the query occurred on that date. In this paper, we propose a novel solution that learns from unlabeled contexts and user click activity.\nContributions. We describe a new algorithm that leverages the information contained in contexts, provided that such information is sufficiently rich. Specifically, we assume that there exists a deterministic oracle (unknown to the algorithm) which inputs the context and outputs a correct binary prediction of whether an event has occurred in the current round. To simulate such an oracle, we use a classification algorithm. However, we do not assume that we have a priori labeled samples to train such a classifier. Instead, we generate the labels ourselves.\nOur algorithm is in fact a meta-algorithm that combines a bandit algorithm designed for the eventfree setting with an online classification algorithm. The classifier uses the contexts to predict when events occur, and the bandit algorithm \u201cstarts over\u201d on positive predictions. The bandit algorithm provides feedback to the classifier by checking, soon after each of the classifier\u2019s positive predictions, whether the optimal search result actually changed. In such a setup, one needs to overcome several technical hurdles, e.g. ensure that the feedback is not \u201ccontaminated\u201d by events in the past and in the near future. We design the whole triad \u2014 the bandit algorithm, the classifier, and the meta-algorithm \u2014 so as to obtain strong provable guarantees. Our bandit subroutine \u2014 a novel version of algorithm UCB1 from [2] which additionally provides high-confidence estimates on the suboptimality of arms \u2014 may be of independent interest.\nFor suitable choices of the bandit and classifier subroutines, the regret incurred by our metaalgorithm is (under certain mild assumptions) at most O(k + dF )( n\u2206 logT ), where k is the number of events, dF is a certain measure of the complexity of the concept class F used by the classifier, n is the number of relevant search results,1 \u2206 is the \u201cminimum suboptimality\u201d of any search result (defined formally in Section 3), and T is the total number of impressions. This regret bound has a very weak dependence on T , which is highly desirable for search engines that receive much traffic.\nThe context turns out to be crucial for achieving logarithmic dependence on T . Indeed, we show that any bandit algorithm that ignores context suffers regret \u2126( \u221a T ), even when there is only one event. Unlike many lower bounds for bandit problems, our lower bound holds even when \u2206 is a constant independent of T . We also show that assuming a logarithmic dependence on T , the dependence on k and dF is essentially optimal.\nFor empirical evaluation, we ideally need access to the traffic of a real search engine so that search results can be adapted based on real-time click activity. Since we did not have access to live traffic, we instead conduct a series of synthetic experiments. The experiments show that if there are no events then the well-studied UCB1 algorithm [2] performs the best. However, when many different queries experience events, the performance of our algorithm significantly outperforms prior methods."}, {"heading": "2 Related Work", "text": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time. Online bandit algorithms (see [7] for background) have been considered in the context of ranking. For instance, Radlinski et al [20] showed how to compose several instantiations of a bandit algorithm to produce a ranked list of search results. Pandey et al [19] showed that bandit algorithms can be effective in serving advertisements to search engine users. These approaches also assume a stationary inference problem.\nAlthough no existing bandit algorithms are specifically designed for our problem setting, there are two well-known algorithms that we compare against in this paper. The UCB1 algorithm [2] assumes fixed click probabilities and has regret at most O( n\u2206 logT ). The EXP3.S algorithm [3] assumes\n1In practice, the arms can be restricted to, say, the top ten results that match the query.\nthat click probabilities can change on every round and has regret at most O(k \u221a\nnT log(nT )) for arbitrary pt\u2019s. Note that the dependence of EXP3.S on T is substantially stronger.\nThe \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours. A key difference is that the context received in each round is assumed to contain information about the identity of an optimal result i\u2217t , a considerably stronger assumption than we make. Our context includes only side information such as volume of the query, but we never actually receive information about the identity of the optimal result.\nA different approach is to build a statistical model of user click behavior. This approach has been applied to the problem of serving news articles on the web. Diaz [9] used a regularized logistic model to determine when to surface news results for a query. Agarwal et al [1] used several models, including a dynamic linear growth curve model.\nThere has also been work on detecting bursts in data streams. For example, Kleinberg [15] describes a state-based model for inferring stages of burstiness. The goal of our work is not to detect bursts, but rather to predict shifts in intent.\nIn a recent concurrent and independent work, Yu et al [23] studied bandit problems with \u201cpiecewisestationary\u201d distributions, a notion that closely resembles our definition of events. However, they make different assumptions than we do about the information a bandit algorithm can observe. Expressed in the language of our problem setting, they assume that from time-to-time a bandit algorithm receives information about how users would have responded to search results that are never actually displayed. For our setting, this assumption is clearly inappropriate."}, {"heading": "3 Problem Formulation and Preliminaries", "text": "We view the problem of deciding which search results to display in response to user click behavior as a bandit problem, a well-known type of sequential decision problem. For a given query q, the task is to determine, at each round t \u2208 {1, . . . , T } that q is issued by a user to our search engine, a single result it \u2208 {1, . . . , n} to display.2 This result is clicked by the user with probability pt(it). A bandit algorithm A chooses it using only observed information from previous rounds, i.e., all previously displayed results and received clicks. The performance of an algorithm A is measured by its regret: RA(T ) , E [\u2211T t=1 pt(i \u2217 t )\u2212 pt(it) ] , where an optimal result i\u2217t \u2208 argmaxi pt(i) is\none with maximum click probability, and the expectation is taken over the randomness in the clicks and the internal randomization of the algorithm. Note our unusually strong definition of regret: we are competing against the best result on every round.\nWe call an event any round t where pt\u22121 6= pt. It is reasonable to assume that the number of events k \u226a T , since we believe that abrupt shifts in user intent are relatively rare. Most existing bandit algorithms make no attempt to predict when events will occur, and consequently suffer regret \u2126( \u221a T ). On the other hand, a typical search engine receives many signals that can be used to predict events, such as bursts in query reformulation, average age of retrieved document, etc.\nWe assume that our bandit algorithm receives a context xt \u2208 X at each round t, and that there exists a function f \u2208 F , in some known concept class F , such that f(xt) = +1 if an event occurs at round t, and f(xt) = \u22121 otherwise.3 In other words, f is an event oracle. The tractability of F will be characterized by a number dF called the diameter of F , detailed in Section 5. At each round t, an eventful bandit algorithm must choose a result it using only observed information from previous rounds, i.e., all previously displayed results and received clicks, plus all contexts up to round t.\nIn order to develop an efficient eventful bandit algorithm, we make an additional key assumption: At least one optimal result before an event is significantly suboptimal after the event. More precisely, we assume there exists a minimum shift \u01ebS > 0 such that, whenever an event occurs at round t, we have pt(i\u2217t\u22121) < pt(i \u2217 t ) \u2212 \u01ebS for at least one previously optimal search result i\u2217t\u22121. For our problem setting, this assumption is relatively mild: the events we are interested in tend to have a\n2For simplicity, we focus on the task of returning a single result, and not a list of results. Techniques from [20] may be adopted to find a good list of results.\n3In some of our analysis, we require contexts be restricted to a strict (concept-specific) subset of X ; the value of f outside this subset will technically be null. See Section 5 for more details.\nrather dramatic effect on the optimal search results. Moreover, our bounds are parameterized by \u2206 = mint mini6=i\u2217t pt(i \u2217 t )\u2212 pt(i), the minimum suboptimality of any suboptimal result.\nWe summarize the notation in Table 1.\nLet S be the set of all contexts which correspond to an event. When the classifier receives a context x and predicts a \u201cpositive\u201d, this prediction is called a true positive if x \u2208 S, and a false positive otherwise. Likewise, when the classifier predicts a \u201cnegative\u201d, the prediction is called a true negative if x 6\u2208 S, and a false negative otherwise. The sample (x, l) is correctly labeled if l = (x \u2208 S)."}, {"heading": "4 Bandit with Classifier", "text": "Our algorithm is called BWC, or \u201cBandit with Classifier\u201d. Ideally, we would like to use a bandit algorithm for the event-free setting, such as UCB1, and restart it every time there is an event. Since we do not have an oracle to tell whether an event has happened, we use a classifier which looks at the current context and makes a binary prediction. As we mentioned in the introduction, we assume that a priori there are no labeled samples to train such a classifier, so we need to generate the labels ourselves. The high-level idea is to restart the bandit algorithm every time the classifier predicts an event, and use subsequent rounds to generate feedback (labeled samples) to train the classifier. Thus, we have a feedback loop between the bandit algorithm and a classifier, in which the latter provides predictions and the former verifies whether they are correct, see Figure 1.\nSo what prevents us from simply combining an off-the-shelf bandit algorithm with an off-the-shelf classifier? The central challenge is how to define the feedback. Let us outline several hurdles that we need to overcome here. A single false negative prediction will cause BWC to miss an event, which may result in a very high regret (since it may take the bandit algorithm a very long time to adjust). Incorrectly labeled samples may contaminate the classifier, perhaps even permanently. To generate a label for a given sample, one needs to compare the state right before the current round with the state right after, in a conclusive way. Both states are not known to the algorithm a priori, and can only be learned probabilistically via exploration. A particular challenge is to ensure that such exploration is not contaminated by events in the past rounds, as well as by events that happen soon after the current round. Moreover, this exploration is generally too expensive to perform upon negative predictions \u2014 indeed, the whole point of BWC is that in the absence of an event the bandit algorithm converges\nto the best arm and (essentially) keeps playing it \u2014 so the classifier receives labels only upon the positive predictions."}, {"heading": "4.1 The meta-algorithm", "text": "We will present our algorithm in a modular way, as a meta-algorithm which uses the following two components: classifier and bandit. In each round, classifier inputs a context xt and outputs a \u201cpositive\u201d or \u201cnegative\u201d prediction of whether an event has happened in this round. Also, it may input labeled samples of the form (x, l), where x is a context and l is a boolean label, which it uses for training. Algorithm bandit is a bandit algorithm that is tuned for the event-free runs.\nAs described above, we further require bandit to provide feedback to the classifier about whether the best result has actually changed. The standard bandit framework does not immediately provide us with estimates from which such feedback can be obtained. Therefore we require bandit to provide the following additional functionality: after each round t of execution, it outputs a pair (G+, G\u2212) of subsets of arms;4 we call this pair the t-th round guess.5 The meaning of G+ and G\u2212 is that they are algorithm\u2019s estimates for, respectively, the sets of all optimal and (at least) \u01ebSsuboptimal arms. We use (G+, G\u2212) to predict whether an event has happened between two runs of bandit. The idea is that any such event causes some arm from G+ of the first run to migrate to G\u2212 of the second run. Accordingly, we generate a negative label if G+i \u2229 G\u2212j = \u2205, where i and j refers to the first and the second run, respectively (see Line 10 of Algorithm 1).\nWe formalize our assumptions on classifier and bandit as follows:\nDefinition 1. classifier is safe for a given concept class if, given only correctly labeled samples, it never outputs a false negative. bandit is called (L, \u01eb)-testable, for some L \u2208 N and \u01eb \u2208 (0, 1), if the following holds. Consider an event-free run of bandit, and let (G+, G\u2212) be its L-th round guess. Then with probability at least 1 \u2212 T\u22122, each optimal arm lies in G+ but not in G\u2212, and any arm that is at least \u01eb-suboptimal lies in G\u2212 but not in G+. 6\nWe will discuss efficient implementations of a safe classifier and a (L, \u01ebS)-testable bandit in Sections 5 and Section 6, respectively. For bandit, we build on a standard algorithm UCB1 [2]; as it turns out, making it (L, \u01ebS)-testable requires a significantly extended analysis.\nFor correctness, we require bandit to be (L, \u01ebS)-testable, where \u01ebS is the minimum shift. The performance of bandit is quantified via its event-free regret, i.e. regret on the event-free runs. Likewise, for correctness we need classifier to be safe, and we quantify its performance via the following property, termed FP-complexity, which refers to the maximum number of false positives.\nDefinition 2. Given a concept class F , the FP-complexity of classifier is the maximum possible number of false positives it can make in an online prediction game where in each round, an adversary selects a sample, classifier makes a prediction, and then (in some rounds) receives a correct label. Specifically, classifier receives a correct label if and only if the prediction is a false positive. The maximum is taken over all event oracles f \u2208 F and all possible sequences of samples.\nNow we are ready to present our meta-algorithm, called BWC. It runs in phases of two alternating types: odd phases are called \u201ctesting\u201d phases, and even phases are called \u201cadapting\u201d phases. The first round of phase j is denoted tj . In each phase we run a fresh instance of bandit. Each testing phase lasts for L rounds, where L is a parameter. Each adapting phase j ends as soon as classifier predicts \u201cpositive\u201d; the round t when this happens is round tj+1. Phase j is called full if it lasts at least L rounds. For a full phase j, let (G+j , G \u2212 j ) be the L-th round guess in this phase. After each testing phase j, we generate a boolean prediction l of whether there was an event in the first round thereof. Specifically, letting i be the most recent full phase before phase j, we set ltj = false if and only if G+i \u2229G\u2212j = \u2205. If ltj is false, the labeled sample (xtj , ltj ) is fed back to the classifier.\n4Following established convention, we call the options available to a bandit algorithm \u201carms\u201d. In our setting, each arm corresponds to a search result.\n5Both classifier and bandit make predictions (about events and arms, respectively). For clarity, we will use the term \u201cguess\u201d exclusively to refer to predictions made by bandit, and reserve the term \u201cprediction\u201d for classifier.\n6Recall that T here is the overall time horizon, as defined in Section 3.\nAlgorithm 1 Meta-algorithm BWC (\u201cBandit with Classifier\u201d) 1: Given: Parameter L, a (L, \u01ebS)-testable bandit, and a safe classifier. 2: for phase j = 1, 2, . . . do 3: Initialize bandit. Let tj be current round. 4: if j is odd then 5: {testing phase} 6: for round t = tj . . . tj + L do 7: Select arm it according to bandit. 8: Observe click w.p. pt(it) and update bandit. 9: Let i be the most recent full phase before phase j. 10: If G+i \u2229G\u2212j = \u2205 {label is false} 11: Let ltj = false and pass training example (xtj , ltj ) to classifier. 12: else 13: {adapting phase} 14: for round t = tj , tj + 1, . . . do 15: Select arm it according to bandit. 16: Observe click w.p. pt(it) and update bandit; pass context xt to classifier. 17: if classifier predicts \u201cpositive\u201d then 18: Terminate inner for loop.\nNote that classifier never receives true-labeled samples. Pseudocode for BWC is given in Algorithm 1.\nDisregarding the interleaved testing phases for the moment, BWC restarts bandit whenever classifier predicts \u201cpositive\u201d, optimistically assuming that the prediction is correct. By our assumption that events cause some optimal arm to become significantly suboptimal (see Section 3), a correct prediction should result in G+i \u2229 G\u2212j 6= \u2205, where i is a phase before the putative event, and j is a phase after it. We use this condition in Line 10 of the pseudocode to generate the label. However, to ensure that the estimates Gi and Gj are reliable, we require that phases i and j are full. And to ensure that the full phases closest to a putative event are not too far from it, we interleave a full testing phase every other phase."}, {"heading": "4.2 Provable guarantees", "text": "We present provable guarantees for BWC in a modular way, in terms of FP-complexity, event-free regret, and the number of events. This is the main technical result in the paper.\nTheorem 1. Consider an instance of the eventful bandit problem with number of rounds T , n arms, k events and minimum shift \u01ebS; assume that any two events are at least 2L rounds apart. Consider algorithm BWC with parameter L and components classifier and bandit that are, respectively, safe and (L, \u01ebS)-testable. Suppose the event-free regret of bandit is bounded from above by a concave function R0(\u00b7). Then the regret of BWC is\nRBWC(T ) \u2264 (2k + dFP)R0 (\nT 2k+dFP\n) + (k + dFP)R0(L) + kL, (1)\nwhere dFP is the FP-complexity of classifier.\nRemark. We define a safe classifier whose FP-complexity is bounded in terms of some properties of the underlying concept class (see Section 5). Our instantiation of bandit is (L, \u01ebS)-testable for L = \u0398( n\n\u01eb2S logT ), with concave event-free regret matching that of UCB1 (see Section 6).\nRemark. The right-hand side of (1) can be parsed as follows. The three summands in (1) correspond to contributions of, respectively, adapting phases, event-free testing phases, and testing phases during which an event has occurred. For the first summand, we show that BWC incurs regret R0(t) for each adapting phase of length t, bound the number of adapting phases by 2k+ dFP, and then bound the total contribution of all such phases using concavity. For the second summand, we bound the number of clean testing phases by k+dFP, and note that each such phase contributes at most R0(L)\nto regret. For the third summand, each \u201ceventful\u201d testing phase contributes at most L to regret, and we show that there can be at most k such phases.7\nRemark. Assuming that any two events are at least 2L rounds apart ensures that of any two consecutive phases, one much be event-free. This, in turn, let us invoke the (L, \u01ebS)-testability of bandit.\nOverview of the proof. The essential difficulty the analysis of BWC is that an event might happen while the algorithm is testing for another (suspected) event. The corresponding technical difficulty is that the correct operation of the components of BWC \u2014 classifier and bandit \u2014 is interdependent, so one needs to be careful to avoid a circular argument. In particular, one challenge is to handle events that occur during the first L rounds of a phase; these events may potentially \u201ccontaminate\u201d the L-th round guesses and cause incorrect feedback to classifier.\nFirst, we argue away the probabilistic nature of the problem. We focus on a given testing phase j. For each of the two preceding phases i \u2208 {j \u2212 1, j \u2212 2}, consider the number N of events between the first round of phase i and the first round of phase j. We would like to establish the following separation property: that we can separate (tell apart) the cases of N = 0 and N = 1 using the testing condition in Line 10 of the pseudocode. Capitalizing on (L, \u01ebS)-testability, we define a technical condition which implies the separation property with very high probability. Regret incurred if the implication \u201ctechnical condition \u21d2 separation property\u201d 8 fails to hold is negligible. Thus we can assume that this implication holds always, and argue deterministically from now on.\nIt is worth noting that we consider two preceding phases i \u2208 {j \u2212 1, j \u2212 2} because either can be used in in Line 10 of the pseudocode (depending on whether phase j \u2212 1 is full). A crucial point here is that one of these two phases must be event-free.\nSecond, we argue that classifier receives only correctly labeled samples. We do it in two steps. Using the well-detectable property, we show that if classifier receives an incorrectly labeled sample after some testing phase j, then an event must have occurred during the (adapting) phase j \u2212 1. Then using the safety property of classifier, we prove that each adapting phase is event-free.\nThird, we bound from above the number of testing and adapting phases, using the maximal number of events and the FP-complexity of the classifier. To this end, we establish that if during a testing phase j there are no events, and furthermore there are no events during the two preceding phases, then in the end of j BWC generates a correct label l = false. Then the regret bound (1) follows easily from the event-free regret of bandit.\nNow let us present the full proof which fills the gaps in the above overview.\nFull Proof. Let tj be the first round of phase j. Recall that phase j is called full if it lasts at least L rounds. For a full phase j, let us say that the phase is event-free if no events happened during interval (tj , tj + L], and let (G + j , G \u2212 j ) be the L-th round guess in this phase. For two full phases i < j, let us write i\u2295 j if and only if G+i \u2229G\u2212j = \u2205. Recall that i\u2295 j (as a boolean property) is our algorithm\u2019s estimate of whether there was no event in round tj .\nA testing phase j is called well-detectable if for each phase i \u2208 {j\u22122, j\u22121} the following property holds: if phases i and j are full and event-free, then: (i) if there are no events in the interval (ti, tj ] then i \u2295 j, (ii) if in the interval (ti, tj ] there is exactly one event, then \u00ac(i \u2295 j). Since bandit is (L, \u01ebS)-testable, each testing phase j is well-detectable with probability at least 1 \u2212 2T\u22122. Thus, with probability at least 1 \u2212 \u2126(T\u22121) each testing phase is well-detectable. Thus, regret incurred in the case that a phase fails to be well-detectable is negligible. So in the rest of the proof, we will assume that each testing phase is well-detectable.\nWe claim that if classifier receives an incorrectly labeled sample after some testing phase j, then an event must have occurred during the (adapting) phase j \u2212 1. Indeed, by the algorithm specification this sample is (xtj ,false), where tj is the first round of phase j. Thus, an event has happened in round tj , and yet we have i \u2295 j, where i is the most recent full phase before phase j.\n7In fact, the k in the +kL term in (1) can be replaced by the (potentially much smaller) number of testing phases that contain both a false positive in round 1 of the phase and an actual event later in the phase.\n8In the full proof, this implication is called well-detectability.\nSince each testing phase is well-detectable, it follows that at least one more event happened between the beginning of phase i and the end of phase j. Since any two events are at least 2L rounds apart, phase i started at some round ti < tj \u2212 2L, and an event has happened in the interval [ti, tj \u2212 2L). To prove the claim, it suffices to show that i = j \u2212 1. Now, if phase j \u2212 1 lasted less than L steps, then i = j\u2212 2 is a testing phase, and so ti \u2265 tj \u2212 2L, contradiction. Thus phase j\u2212 1 lasted at least L steps, and so i = j \u2212 1, claim proved. We claim that all adapting phases are event-free. For the sake of contradiction, suppose an event occurs during an adapting phase, and let t be the first round at which this happens. We know that classifier output a (false) negative in this round, since otherwise a new testing phase would have started at round t. Since classifier is safe, at some round before t it must have received an incorrectly labeled sample. By the algorithm specification, this must have happened after some testing phase j which ended before round t. But then (by the previous claim) an event must have occurred during the (adapting) phase j \u2212 1, which contradicts the choice of t. Claim proved. From the previous two claims, it follows that classifier receives only correctly labeled samples.\nWe claim that if there are no events during some testing phases j \u2212 2 and j, then at the end of phase j we generate a label l = false. Indeed, suppose not. Then \u00ac(i \u2295 j), where i is the most recent full phase before phase j. Either i = j \u2212 2 or i = j \u2212 1; in either case, \u00ac(i \u2295 j) implies that there is an event in the interval [ti, tj). Since there are no events during adapting phases, it follows that i = j \u2212 2, contradiction. Claim proved. We claim that there can be at most 2k + dFP testing phases (and hence at most as many adapting phases), including at most k + dFP event-free testing phases. Indeed, in the first round of each testing phase j classifier generates a \u201cpositive\u201d, and in the end of the phase we generate a label l \u2208 {true,false}. We examine each case separately: (i) if l = false then classifier receives feedback, so there can be at most dFP such phases j, (ii) if l = true then an event has occurred in phase j or j \u2212 2, so there can be at most 2k such phases j, of which at most k phases can be event-free. Claim proved.\nTo obtain the regret bound (1), note that regret in each event-free phase of length t is R0(t), see the second remark after Theorem 1 for details."}, {"heading": "5 Safe Classifier", "text": "In this section, we show how safe classifiers with low FP-complexity can be constructed for specific concept classes. Recall that a classifier is called safe if (assuming it inputs only correctly labeled samples) it never outputs a false negative, and the definition of FP-complexity, motivated by the specification of the BWC algorithm, essentially assumes that all labeled samples correspond to false positives.\nWe first describe a generic classifier, called SafeCl, that is safe for any concept class F , and bound its FP-complexity using a certain property of F . In the event that the concept class is all ddimensional axis-parallel hyper-rectangles with margin 1/\u03b4, we show that this bound is proportional to d/\u03b4. And in the event that the concept class is all d-dimensional hyperplanes with margin \u03b4, we show that this bound is exponential in d. Unfortunately, the exponential dependence cannot be improved, as we will see in Section 7.\nThe classifier SafeCl is defined as follows.\nSafeCl classifies a given unlabeled context x as negative if and only if there exists no concept f \u2208 F such that f(x) = +1 and f(x\u2032) = \u22121 for each false-labeled example x\u2032 received so far.\nIt is easy to see that this classifier is indeed safe. Moreover, we bound its FP-complexity in terms of the following property of the concept class F : Definition 3. The diameter of F , denoted dF , is equal to the length of the longest sequence x1, . . . , xm \u2208 X such that for each t = 1, . . . ,m there exists a concept f \u2208 F with the following property: f(xt) = +1, and f(xs) = \u22121 for all s < t. Claim 1. SafeCl is safe, and its FP-complexity is at most dF .\nProof. Assume all false-labeled examples input by SafeCl are correctly labeled. Suppose SafeCl outputs a false negative, with concept f \u2208 F and unlabeled sample x. Then f(x) = +1 and f(x\u2032) = \u22121 for each false-labeled example x\u2032 received so far. But by definition of SafeCl such concept does not exist, contradiction. Therefore, SafeCl is safe. Regarding the FP-complexity, consider the prediction game in Definition 2. Any sequence x1, . . . , xm of false positives output by SafeCl satisfies the property in Definition 3, so m \u2264 dF .\nBy using SafeCl as our classifier, we introduce dF into the regret bound of bwc, and this quantity can be large. However, in Section 7 we show that the regret of any algorithm must depend on dF , unless it depends strongly on the number of rounds T .\nBelow we give examples of common concept classes with efficiently computable safe functions, and prove bounds on their diameter. Recall that for a given universe X of examples, a concept is a function f : X \u2192 {\u22121,+1,null}, where the null value refers to the examples that are not feasible under a given concept (i.e., if f is the true concept, then we will never observe an example x such that f(x) = null).\nIn what follows, for each N \u2282 X define SF (N) \u2282 X as the set of all x \u2208 X for which there is no concept f \u2208 F such that f(x) = +1 and f(x\u2032) = \u22121 for each x\u2032 \u2208 N . Note that SafeCl outputs a negative prediction on x if and only if x \u2208 SF (N), where N is the set of false-labeled samples received so far. Likewise, in Definition 3 the sequence {xt} satisfies xt /\u2208 SF({x1, . . . , xt\u22121}) for each t.\nFor convenience, define a \u201c\u03b4-ball\u201d around a set S \u2282 Rd in the d-dimensional Lp-norm as\nB d p(S, \u03b4) , {x \u2208 Rd : Lp(x, S) \u2264 \u03b4}, where Lp(x, S) , miny\u2208S \u2016x\u2212 y\u2016p.\nHere Lp(x, S) is the Lp-norm distance between a point x and a set S."}, {"heading": "5.1 Axis-parallel rectangles with margin \u03b4", "text": "One very simple concept is an axis-parallel hyper-rectangle. This type of concept can be used to test whether any one of several features is outside of its \u2018normal\u2019 range. This is a particularly wellsuited concept class for predicting events that may affect a search engine query, since these events are typically preceded by a large change in some statistic related to the query, such as its volume or abandonment rate.\nFix the dimension d, and let X \u2286 Rd be the d-dimensional L\u221e-norm unit ball around the origin. A d-rectangle in Rd is the cross-product of d non-empty intervals in R. Given \u03b4 > 0 and a d-rectangle R, define a function fR,\u03b4 : X \u2192 {\u22121,+1,null} as follows: fR,\u03b4(x) equals +1 if L\u221e(x,R) \u2265 \u03b4; it equals \u22121 if x \u2208 R, and it equals null otherwise (note that the margin \u03b4 only applies only outside of R). The concept class of d-dimensional axis-parallel rectangles with margin \u03b4 is defined as FAPR(d, \u03b4) = {fR,\u03b4 : all d-rectangles R}. We bound the diameter of FAPR(d, \u03b4) as follows. Claim 2. If F = FAPR(d, \u03b4), then dF \u2264 O(d/\u03b4).\nProof. Consider a sequence x1, . . . , xm \u2208 X such that xt /\u2208 SF ({x1, . . . , xt\u22121}) for all 1 \u2264 t \u2264 m. Let Rt be the \u03b4-ball in L\u221e around the smallest d-rectangle containing x1, . . . , xt. By definition of the sequence, at least one of the one-dimensional intervals defining Rt+1 must be \u03b4 larger than the same interval in Rt. Since \u2016xt\u2016\u221e \u2264 1, m \u2264 O(d/\u03b4).\nClearly, for the concept class FAPR(d, \u03b4), the classifier SafeCl simply maintains the smallest ddimensional rectangle R(N) containing the set of all previously false-labeled examples N , and classifies a new example x as negative if and only if x lies within \u03b4 (measured in L\u221e-norm) of R(N). In other words\nSafeCl on FAPR(d, \u03b4): classify x \u2208 X as negative \u21d0\u21d2 x \u2208 Bd\u221e(R(N), \u03b4), where N is the set of all false-labeled examples received so far."}, {"heading": "5.2 Hyperplanes with margin \u03b4", "text": "Hyperplanes are perhaps the most widely-used concept in classification problems. Fix the dimension d, and let X \u2286 Rd be the d-dimensional L2-norm unit ball around the origin. Given u,w \u2208 Rd and \u03b4 > 0, define a function fu,w,\u03b4 : X \u2192 {\u22121,+1,null} as follows: fu,w,\u03b4(x) equals +1 if w \u00b7 (x+ u) \u2265 \u03b4, it equals \u22121 if w \u00b7 (x+ u) < \u2212\u03b4, and it equals null otherwise. Here w is the unit normal of the hyperplane, and u is the shift vector. The concept class of d-dimensional hyperplanes with margin \u03b4 is defined as\nFHYP(d, \u03b4) = {fu,w,\u03b4 : u,w \u2208 Rd, \u2016u\u20162 \u2264 1, \u2016w\u20162 = 1}. We bound the diameter of FHYP(d, \u03b4) as follows: Claim 3. If F = FHYP(d, \u03b4), then dF \u2264 (1 + 1\u03b4 )d.\nProof. Consider a sequence x1, . . . , xm \u2208 X such that xt /\u2208 SF ({x1, . . . , xt\u22121}) for all 1 \u2264 t \u2264 m, as in Definition 3. Then for each s and t such that 1 \u2264 s < t \u2264 m there exist u,w \u2208 Rd such that \u2016u\u20162 \u2264 1, \u2016w\u20162 = 1, w \u00b7 (xt + u) \u2265 \u03b4 and w \u00b7 (xs + u) < \u2212\u03b4. By Ho\u0308lder\u2019s inequality, it follows that\n\u2016xt \u2212 xs\u20162 = \u2016w\u20162 \u2016xt \u2212 xs\u20162 \u2265 w \u00b7 (xt \u2212 xs) > 2\u03b4. (2) Now, place an L2-ball of radius \u03b4 around each point xt. By (2), none of these balls can intersect. A radius-r ball in d dimensions has volumeCd rd, where Cd is a constant that depends only on d. Thus the total volume of the balls is mCd \u03b4d. On the other hand, \u2016xt\u20162 \u2264 1 for each t, so each of these balls lies in the radius-(1 + \u03b4) ball around the origin, so their total volume is at most Cd (1 + \u03b4)d. It follows that m \u2264 (1 + 1\u03b4 )d.\nWe now show that there is a computationally efficient way to implement the classifier SafeCl for hypothesis class FHYP(d, \u03b4). Specifically, we show that the classifier SafeCl simply maintains the convex hull Co(N) of all previously false-labeled examples N , classifies a new example x as negative if and only if x lies within 2\u03b4 (measured in L2-norm) of Co(N). In other words\nSafeCl on FHYP(d, \u03b4): classify x \u2208 X as negative \u21d0\u21d2 x \u2208 Bd2(Co(N), 2\u03b4), where N is the set of all false-labeled examples received so far.\nClaim 4. If F = FHYP(d, \u03b4) and N \u2282 X then SF(N) = X \u2229 Bd2(Co(N), 2\u03b4), where Co(N) is the convex hull of N .\nProof. Fix xt \u2208 X . We divide the proof into two parts. First, we show that if xt is contained in the 2\u03b4-ball around Co(N), then no hyperplane in F can separate xt from N . Next, we show that if xt is outside the 2\u03b4-ball around Co(N), then at least one hyperplane in F separates xt from N . More precisely, we prove that\n(i) If xt \u2208 Bd2(Co(N), 2\u03b4) then there does not exist f \u2208 FHYP(d, \u03b4) such that f(xs) = \u22121 for all xs \u2208 N and f(xt) = +1.\n(ii) If xt /\u2208 Bd2(Co(N), 2\u03b4) then there exists f \u2208 FHYP(d, \u03b4) such that f(xs) = \u22121 for all xs \u2208 N and f(xt) = +1.\nProof of (i): Suppose for contradiction that there exist u,w \u2208 Rd, with \u2016u\u20162 \u2264 1 and \u2016w\u20162 = 1, such that w \u00b7 (xs + u) < \u2212\u03b4 for all xs \u2208 N and w \u00b7 (xt + u) \u2265 \u03b4. Choose x\u2217 \u2208 Co(N) so that \u2016xt \u2212 x\u2217\u20162 = L2(xt,Co(N)), i.e. x\u2217 is a closest point in Co(N) to xt (we know x\u2217 exists because Co(N) is closed). Since xt \u2208 Bd2(Co(N), 2\u03b4), we have that \u2016xt \u2212 x\u2217\u20162 \u2264 2\u03b4. We know that w \u00b7 (x\u2217 + u) < \u2212\u03b4 because x\u2217 is a convex combination of the examples in N . Therefore, by the intermediate value theorem, there exists x\u2032 \u2208 X and \u03b8 \u2208 [0, 1] such that x\u2032 = (1\u2212 \u03b8)xt + \u03b8x\u2217 and w \u00b7 (x\u2032 + u) = 0.\nSome algebra shows that \u2016xt \u2212 x\u2032\u20162 = \u03b8\u2016xt \u2212 x\u2217\u20162 and \u2016x\u2032 \u2212 x\u2217\u20162 = (1\u2212 \u03b8)\u2016xt \u2212 x\u2217\u20162. Adding these equations yields\n\u2016xt \u2212 x\u2032\u20162 + \u2016x\u2032 \u2212 x\u2217\u20162 = \u2016xt \u2212 x\u2217\u20162\nBecause w \u00b7 (x\u2032 + u) = 0, by Ho\u0308lder\u2019s inequality we have \u2016xt \u2212 x\u2032\u20162 = \u2016w\u20162\u2016xt \u2212 x\u2032\u20162 \u2265 w \u00b7 (xt \u2212 x\u2032) = w \u00b7 (xt + u)\u2212 w \u00b7 (x\u2032 + u) \u2265 \u03b4\nand\n\u2016x\u2032 \u2212 x\u2217\u20162 = \u2016w\u20162\u2016x\u2032 \u2212 x\u2217\u20162 \u2265 w \u00b7 (x\u2032 \u2212 x\u2217) = w \u00b7 (x\u2032 + u)\u2212 w \u00b7 (x\u2217 + u) > \u03b4 which implies \u2016xt \u2212 x\u2217\u20162 > 2\u03b4, which is a contradiction. Proof of (ii): We will use the well-known separating hyperplane theorem [21]: If nonempty convex sets X,Y \u2208 Rd do not intersect, then there exist a \u2208 Rd \\ {0} and b \u2208 R such that\na \u00b7 x \u2265 b for all x \u2208 X and a \u00b7 y \u2264 b for all y \u2208 Y (3) Since xt /\u2208 Bd2 (Co(N), 2\u03b4) there must exist \u01eb > 0 such that the sets X = Bd2({xt}, \u03b4) and Y = Bd2 (Co(N), \u03b4+ \u01eb) do not intersect. For these choices for X and Y , let us fix a \u2208 Rd \\{0} and b \u2208 R that satisfy (3).\nNote that xt + z \u2208 X for all z \u2208 Rd such that \u2016z\u20162 \u2264 \u03b4. Also note that xs + z \u2208 Y for all xs \u2208 N and z \u2208 Rd such that \u2016z\u20162 \u2264 \u03b4 + \u01eb. So by (3) we have\na \u00b7 ( xt \u2212 \u03b4 a\n\u2016a\u20162\n) \u2265 b and a \u00b7 ( xs + (\u03b4 + \u01eb) a\n\u2016a\u20162\n) \u2264 b for all xs \u2208 N\nLetting w = a\u2016a\u20162 and rearranging we have\nw \u00b7 xt \u2265 b\n\u2016a\u20162 + \u03b4 and w \u00b7 xs \u2264\nb\n\u2016a\u20162 \u2212 (\u03b4 + \u01eb) for all xs \u2208 N (4)\nSince \u2016w\u20162 = 1 and \u2016x\u20162 \u2264 1 for all x \u2208 X , it follows from (4) that \u2223\u2223\u2223 b\u2016a\u20162 \u2223\u2223\u2223 \u2264 1. Thus there exists u \u2208 Rd such that \u2016u\u20162 \u2264 1 and w \u00b7 u = \u2212 b\u2016a\u20162 . It now follows that\nw \u00b7 (xt + u) \u2265 \u03b4 and w \u00b7 (xs + u) \u2264 \u2212\u03b4 \u2212 \u01eb for all xs \u2208 N So the function fu,w,\u03b4 \u2208 FHYP(d, \u03b4) satisfies the claim."}, {"heading": "6 Testable Bandit Algorithms", "text": "In this section we will consider the stochastic n-armed bandit problem. We are looking for (L, \u01eb)testable algorithms with low regret. TheL will need to be sufficiently large, on the order of \u2126(n\u01eb\u22122).\nA natural candidate would be algorithm UCB1 from [2] which does very well on event-free regret:\nR0(L) \u2264 O(min( n\u2206 logL, \u221a nL logL)). (5)\nUnfortunately, UCB1 does not immediately provide a way to define the t-th round best guess (G+, G\u2212) so as to guarantee (L, \u01eb)-testability. One simple fix is to choose an arm at random in each of the first L rounds, use these samples to form the best guess, in a straightforward way, and then run UCB1. However, in the first L rounds this algorithm incurs regret of \u2126(L), which is very suboptimal compared to R0(L) from (5).\nIn this section, we develop an algorithm which has the same regret bound as UCB1, and is (L, \u01eb)testable. We state this result more generally, in terms of estimating expected payoffs; we believe it may be of independent interest. The (L, \u01eb)-testability is then an easy corollary.\nSince our analysis in this section is for the event-free setting, we can drop the subscript t from much of our notation. Let p(u) denote the (time-invariant) expected payoff of arm u. Let p\u2217 = maxu p(u), and let \u2206(u) = p\u2217 \u2212 p(u) be the \u201csuboptimality\u201d of arm u. For round t, let \u00b5t(u) be the sample average of arm u, and let nt(u) be the number of times arm u has been played.\nAlgorithm 2 The (L, \u01eb)-testable bandit algorithm with low regret. 1: Given: Time horizon T , parameter \u01eb \u2208 (0, 1). 2: for all arms u do 3: n(u) \u2190 0, x(u) \u2190 0, \u00b5(u) \u2190 0 {#samples, total reward, sample average} 4: for rounds t = 1, 2, . . . , T do\n5: Pick arm u with the maximal index I(u) = \u00b5(u) + 12 \u221a\n2 log(t+T ) 1+n(u) .\n6: Observe payoff x, update n(u) \u2190 n(u) + 1, x(u) \u2190 x(u) + x, \u00b5(u) \u2190 x(u)/n(u). 7: { Form the t-th round guess } 8: v\u2217 \u2190 arm played most often in the last t/2 rounds. 9: for all arms v do\n10: \u2206\u0302(v) \u2190 \u00b5(v\u2217)\u2212 \u00b5(v) {the t-th round estimate of \u2206(v)} 11: Output (G+, G\u2212) = ( {v : \u2206\u0302(v) \u2264 \u01eb/4}, {v : \u2206\u0302(v) > \u01eb/2} ) .\nWe will use a slightly modified algorithm UCB1 from [2], with a significantly extended analysis. Recall that in each round t algorithm UCB1 chooses an arm u with the highest index It(u) = \u00b5t(u)+rt(u), where rt(u) = \u221a 8 log(t)/nt(u) is a term that we\u2019ll call the confidence radius whose meaning is that |p(u)\u2212 \u00b5t(u)| \u2264 rt(u) with high probability. For our purposes here it is instructive to re-write the index as It(u) = \u00b5t(u) + \u03b1 rt(u) for some parameter \u03b1. Also, to better bound the early failure probability we will re-define the confidence radius as rt(u) = \u221a 8 log(t0 + t)/nt(u) for some parameter t0. We will denote this parameterized version by UCB1(\u03b1, t0).\nThe original regret analysis of UCB1 in [2] carries over to UCB1(\u03b1, t0) so as to guarantee event-free regret (5); we omit the details.\nOur contribution concerns estimating the \u2206(u)\u2019s. We estimate the maximal expected reward p\u2217 via the sample average of an arm that has been played most often. More precisely, in order to bound the failure probability we consider an arm that has been played most often in the last t/2 rounds. For a given round t let vt be one such arm (ties broken arbitrarily), and let \u2206t(u) = \u00b5t(vt) \u2212 \u00b5t(u) will be our estimate of \u2206(u). This estimate (and the provable guarantee thereon) is the main technical contribution of this section.\nWe obtain an (L, \u01eb)-testable algorithm from UCB1(6, T ), where T is the time horizon, by defining the t-th round guess as\n(G+, G\u2212) = ({v : \u2206t(v) \u2264 \u01eb/4}, {v : \u2206t(v) > \u01eb/2}). (6) The pseudocode is in Algorithm 2.\nLet us pass to the provable guarantees. We express the \u201cquality\u201d of our estimate \u2206t as follows:\nTheorem 2. Consider the stochastic n-armed bandits problem. Suppose algorithm UCB1(6, t0) has been played for t steps, and t+ t0 \u2265 32. Then with probability at least 1\u2212 (t0 + t)\u22122 for any arm u we have |\u2206(u)\u2212\u2206t(u)| < 14\u2206(u) + \u03b4(t) (7) where \u03b4(t) = O( \u221a n t log(t+ t0)).\nRemark. Either we know that \u2206(u) is small, or we can approximate it up to a constant factor. Specifically, if \u03b4(t) < 12 \u2206t(u) then \u2206(u) \u2264 2\u2206t(u) \u2264 5\u2206(u) else \u2206(u) \u2264 4\u03b4(t).\nProof. Fix round t, let v\u2217 = vt and let s be the last round this arm has been played before round t. Recall that s \u2265 t/2 by definition of vt. Since by pigeonhole principle nt(v\u2217) \u2265 t2n , it follows that rt(v) \u2264 O(\u03b4) where \u03b4 = \u221a n t log(t+ t0). It is easy to see that\nrs(v \u2217) \u2264 2 rs+1(v\u2217) \u2264 2 rt(v\u2217) = O(\u03b4).\nThen with probability at least 1\u2212 (t0 + t)\u22122 for any arm u we have p(v\u2217) +O(\u03b4) \u2265 p(v\u2217) + 7rs(v\u2217) \u2265 Is(v\u2217) \u2265 Is(u) \u2265 p(u) + 5rs(u). (8)\nIf u\u2217 is the arm with maximal expected reward, then plugging u = u\u2217 into (8) gives \u2206(v\u2217) \u2264 O(\u03b4).\nWe claim that (8) implies rt(u) \u2264 14 \u2206(u) +O(\u03b4). Indeed, we can re-write (8) as 5rs(u) \u2264 p(v\u2217)\u2212 p(u) +O(\u03b4) \u2264 \u2206(u) +O(\u03b4).\nThe claim follows since rt(u) \u2264 rs(u) log(t0 + t)/ log(t0 + s) \u2264 54 rs(u). Now we are ready for the final calculation. Let p\u2217 be the maximal expected reward. Then\n|\u2206(u)\u2212\u2206t(u)| = |p\u2217 \u2212 p(u)\u2212 \u00b5t(v\u2217) + \u00b5t(u)| = |(p\u2217 \u2212 p(v\u2217)) + (p(v\u2217)\u2212 \u00b5t(v\u2217)) + (\u00b5t(u)\u2212 p(u))| \u2264 \u2206(v\u2217) + |p(v\u2217)\u2212 \u00b5t(v\u2217)|+ |\u00b5t(u)\u2212 p(u)| \u2264 \u2206(v\u2217) + rt(v\u2217) + rt(u\u2217) \u2264 14 \u2206(u) + O(\u03b4).\nFinally, let us prove that Algorithm 2 is (L, \u01eb)-testable as long as L \u2265 \u2126( n\u01eb2 logT ). Theorem 3. Consider algorithm UCB1(6, T ) where T is the time horizon and the t-th round guess is given by (6). Assume that \u03b4(L) \u2264 \u01eb/4, where \u03b4(t) is from (7). Then the algorithm is (L, \u01eb)-testable.\nProof. If u is an optimal arm, then \u2206(u) = 0, so by (7) we have \u2206t(u) \u2264 \u03b4(t) \u2264 \u01eb/4. If \u2206(u) \u2265 \u01eb then by (7) we have \u2206t(u) \u2265 \u2206(u)/2 \u2265 \u01eb/2."}, {"heading": "7 Upper and Lower Bounds", "text": "Plugging the classifier from Section 5 and the bandit algorithm from Section 6 into the metaalgorithm from Section 4, we obtain the following numerical guarantee.\nTheorem 4. Consider an instance S of the eventful bandit problem with number of rounds T , n arms, k events, minimum shift \u01ebS , minimum suboptimality \u2206, and concept class diameter dF . Assume that any two events are at least 2L rounds apart, where L = \u0398( n\n\u01eb2S logT ). Consider\nthe BWC algorithm with parameter L and components classifier and bandit as presented, respectively, in Section 5 and Section 6. Then the regret of BWC is\nRBWC(T ) \u2264 ( (3k + 2dF) n \u2206 + k n \u01eb2S ) (logT ).\nWhile the linear dependence on n in this bound may seem large, note that without additional assumptions, regret must be linear in n, since each arm must be pulled at least once. In an actual search engine application, the arms can be restricted to, say, the top ten results that match the query.\nWe now state two lower bounds about eventful bandit problems. Theorem 5 shows that in order to achieve regret that is logarithmic in the number of rounds, a context-aware algorithm is necessary, assuming there is at least one event. Incidentally, this lowerbound can be easily extended to prove that, in our model, no algorithm can achieve logarithmic regret when an event oracle f is not contained in the concept class F . Theorem 5. Consider the eventful bandit problem with number of rounds T , two arms, minimum shift \u01ebS and minimum suboptimality \u2206, where \u01ebS = \u2206 = \u01eb, for an arbitrary \u01eb \u2208 (0, 12 ). For any context-ignoring bandit algorithm A, there exists a problem instance with a single event such that regret RA(T ) \u2265 \u2126(\u01eb \u221a T ).\nProof. For simplicity, assume that N = \u221a T is an integer. Define problem instances Ii, 0 \u2264 i \u2264 N as follows. In each of these instances, the T rounds are partitioned into N phases, each of length N . There are two arms, call them y and z. Set pt(y) = 12 for all t. For the problem instance I0, pt(z) = 1 2 \u2212 \u01eb for all t. For problem instances Ii, i \u2265 1 set pt(z) = 12 \u2212 \u01eb in all phases j < i, and pt(z) = 1 2 + \u01eb in all phases j \u2265 i. (Thus, in each instance Ii there is a single event that occurs in the first round of phase i.)\nNow, let qi be the probability that on problem instance I0, arm z is chosen by algorithm A at least once during phase i. If qi \u2265 12 for each phase i, then on the problem instance I0 each phase i contributes at least \u01eb/2 to regret, so the total regret is at least \u01ebN/2. Otherwise, qi < 12 for some i. Since instances I0 and Ii coincide on the first i \u2212 1 phases, algorithm A behaves the same way on\nboth instances up to the end of phase i \u2212 1. Moreover, A behaves the same way on both instances throughout phase i assuming that it never plays arm z during that phase. Therefore with probability 1 \u2212 qi its regret on instance Ii due to phase i alone is \u01eb per each round in this phase; so the total regret is at least \u01ebN/2.\nTheorem 6 proves that in Theorem 4, linear dependence on k + dF is essentially unavoidable. If we desire a regret bound that has logarithmic dependence on the number of rounds, then a linear dependence on k + dF is necessary.\nTheorem 6. Consider the eventful bandit problem with number of rounds T and concept class diameter dF . Let A be an eventful bandit algorithm.\n(i) There exists a problem instance with n arms, k events, minimum shift \u01ebS , minimum suboptimality \u2206, where \u01ebS = \u2206 = \u01eb, for arbitrary k \u2265 1, n \u2265 3, and \u01eb \u2208 (0, 14 ), such that RA(T ) \u2265 \u2126(k n\u01eb ) log(T/k).\n(ii) There exists a problem instance with two arms, a single event, minimum shift \u0398(1) and minimum suboptimality \u0398(1) such that regret RA(T ) \u2265 \u2126(T 1/3) or RA(T ) \u2265 \u2126(dF logT ).\nProof. For part (i), construct the family of problem instances as follows. In each instance, there are k phases of length T/k each. For each phase i, one arm, call it yi, has payoff pt(yi) = 12 + \u01eb, and all other arms y have payoff pt(y) = 12 \u2212 \u01eb. We have one problem instance for each sequence {yi} such that yi 6= yi+1 for each i. Note that there is an event in the first round of each phase; without loss of generality let us assume that this is known to the algorithm. Then in each phase i \u2265 1 the algorithm (essentially) needs to solve a fresh instance of the stochastic bandit problem on n\u22121 arms with time horizon T/k and payoffs 12 \u00b1 \u01eb, which implies regret \u2126(n\u01eb ) log(T/k) [16, 2]. We omit the easy formal details.\nFor part (ii), partition the T rounds into N = min(dF , T 1/3) phases, each of length at least T 2/3. We define problem instances Ii, 0 \u2264 i \u2264 N , in a similar way as in Theorem 5. There are two arms, y and z. Set pt(y) = 12 for all t. For problem instance I0, set pt(z) = 11+e1/3 . In problem instance Ii, for i \u2265 1, set pt(z) = 11+e1/3 in all phases j < i, and pt(z) = e1/3 1+e1/3 in all phases j \u2265 i. Note that 1 1+e1/3 < 12 < e1/3 1+e1/3 .\nIn Appendix A, we show how to define the context sequence {xt} in a way consistent with all our assumptions, in such a way that the contexts for problem instances I0 and Ii agree in the first i phases. The idea is that for both problem instances, the first round of each phase j < i triggers a false positive; this is possible since (essentially) we are allowed dF false positives.\nThe rest of the proof involves calculations similar to those in the proof of Theorem 5. First, suppose dF \u2265 T 1/3. Define qi as in the proof of Theorem 5. If qi \u2265 12 for each phase j, then for the problem instance I0 we have RA(T ) \u2265 \u2126(T 1/3). Otherwise, let i be such that qi < 12 . By our construction, with probability 1 \u2212 qi algorithm A behaves identically on instances I0 and Ii through the first i phases. Thus, on instance Ii in phase i alone it incurs regret \u2126(1) per each round of the phase, for a total of RA(T ) \u2265 \u2126(T 2/3). Next, suppose dF < T 1/3. Let qi,j be the probability that for problem instance Ij , arm z is chosen by A at least logT times during phase i. If qi,0 \u2265 12 for each phase i, then RA(T ) \u2265 \u2126(dF logT ) on problem instance I0. Otherwise, let i be such that qi,0 < 12 . In Appendix A, we give a calculation that shows that (1\u2212qi,i) \u2265 T\u22121/3(1\u2212qi,0), which implies that RA(T ) \u2265 \u2126( 1T 1/3 )(T 2/3\u2212logT ) \u2265 \u2126(T 1/3) on problem instance Ii."}, {"heading": "8 Experiments", "text": "To truly demonstrate the benefits of BWC requires real-time manipulation of search results. Since we did not have the means to deploy a system that monitors click/skip activity and correspondingly alters search results with live users, we describe a collection of experiments on synthetically generated data.\nWe begin with a head-to-head comparison of BWC versus a baseline UCB1 algorithm and show that BWC\u2019s performance improves substantially upon UCB1. Next, we compare the performance of these algorithms as we vary the fraction of intent-shifting queries: as the fraction increases, BWC\u2019s performance improves even further upon prior approaches. Finally, we compare the performance as we vary the number of features. While our theoretical results suggest that regret grows with the number of features in the context space, in our experiments, we surprisingly find that BWC is robust to higher dimensional feature spaces.\nSetup: We synthetically generate data as follows. We assume that there are 100 queries where the total number of times these queries are posed is 3M. Each query has five search results for a user to select from. If a query does not experience any events \u2014 i.e., it is not \u201cintent-shifting\u201d \u2014 then the optimal search result is fixed over time; otherwise the optimal search result may change. Only 10% of the queries are intent-shifting, with at most 10 events per such query. Due to the random nature with which data is generated, regret is reported as an average over 10 runs. The event oracle is an axis-parallel rectangle anchored at the origin, where points inside the box are negative and points outside the box are positive. Thus, if there are two features, say query volume and query abandonment rate, an event occurs if and only if both the volume and abandonment rate exceed certain thresholds.\nBandit with Classifier (BWC): Figure 2(a) shows the average cumulative regret over time of three algorithms. Our baseline comparison is UCB1 which assumes that the best search result is fixed throughout. In addition, we compare to an algorithm we call ORA, which uses the event oracle to reset UCB1 whenever an event occurs. We also compared to EXP3.S, but its performance was dramatically worse and thus we have not included it in the figure.\nIn the early stages of the experiment before any intent-shifting event has happened, UCB1 performs the best. BWC\u2019s safe classifier makes many mistakes in the beginning and consequently pays the price of believing that each query is experiencing an event when in fact it is not. As time progresses, BWC\u2019s classifier makes fewer mistakes, and consequently knows when to reset UCB1 more accurately. UCB1 alone ignores the context entirely and thus incurs substantially larger cumulative regret by the end.\nFraction of Intent-Shifting Queries: In the next experiment, we varied the fraction of intentshifting queries. Figure 2(b) shows the result of changing the distribution from 0, 1/8, 1/4, 3/8 and 1/2 intent-shifting queries. If there are no intent-shifting queries, then UCB1\u2019s regret is the best. We expect this outcome since BWC\u2019s classifier, because it is safe, initially assumes that all queries are intent-shifting and thus needs time to learn that in fact no queries are intent-shifting. On the other hand, BWC\u2019s regret dominates the other approaches, especially as the fraction of intentshifting queries grows. EXP3.S\u2019s performance is quite poor in this experiment \u2014 even when all queries are intent-shifting. The reason is that even when a query is intent-shifting, there are at most 10 intent-shifting events, i.e., each query\u2019s intent is not shifting all the time.\nWith more intent-shifting queries, the expectation is that regret monotonically increases. In general, this seems to be true in our experiment. There is however a decrease in regret going from 1/4 to 3/8 intent-shifting queries. We believe that this is due to the fact that each query has at most 10 intentshifting events spread uniformly and it is possible that there were fewer events with potentially smaller shifts in intent in those runs. In other words, the standard deviation of the regret is large. Over the ten 3/8 intent-shifting runs for ORA, BWC, UCB1 and EXP3.S, the standard deviation was roughly 1K, 10K, 12K and 6K respectively.\nNumber of Features: Finally, we comment on the performance of our approach as the number of features grows. Our theoretical results suggest that BWC\u2019s performance should deteriorate as the number of features grows. Surprisingly, BWC\u2019s performance is consistently close to the Oracle\u2019s. In Figure 2(b), we show the cumulative regret after 3M impressions as the dimensionality of the context vector grows from 10 to 40 features. BWC\u2019s regret is consistently close to ORA as the number of features grows. On the other hand, UCB1\u2019s regret though competitive is worse than BWC, while EXP3.S\u2019s performance is across the board poor. Note that both UCB1 and EXP3.S\u2019s regret is completely independent of the number of features. The standard deviation of the regret over the 10 runs is substantially lower than the previous experiment. For example, over 10 features, the standard deviation was 355, 1K, 5K, 4K for ORA, BWC, UCB1 and EXP3.S, respectively."}, {"heading": "9 Future Work", "text": "The most immediate open question is whether we could train the classifier faster. One idea is to use a more efficient classifier, especially if we can relax the \u201csafety\u201d requirement and somehow recover from false negatives. Another idea is to generate labeled samples not only upon positive predictions but upon negative ones as well, trading off the regret from additional exploration against the benefits of generating extra labeled samples. Finally, it would be desirable to supplement the existing worstcase provable guarantees with stronger ones for settings in which the contexts are sampled from a \u201cbenign\u201d distribution.\nTheoretically, the main drawback of our approach is that we assume the existence of a \u201cperfect oracle\u201d \u2014 a deterministic boolean function on contexts which correctly predicts whether a temporal event has occurred in the current round. It is desirable to extend our results to scenarios in which the contexts allow only approximate or probabilistic prediction. Even though such contexts contain useful signal, exploiting this signal for our purposes appears quite challenging. In particular, it seems to require making the \u201cbandit plus classifier\u201d setup resilient against (infrequent) incorrectly labeled samples and perhaps also against (infrequent) false negatives. It should be noted that the aforementioned resiliency can potentially lead to large improvements in the present oracle-based setting as well, as we might be able to deploy much more efficient classifiers.\nEmpirically, the main question left for future work is testing the \u201cbandit plus classifier\u201d approach in a realistic setting. The challenge here is two-fold. First, one needs to select which features to use for contexts, and verify experimentally how informative they are in predicting the temporal events. Second, since gaining access to live search traffic is difficult, one would need to simulate it using the search logs, the difficulty being is that the search logs might not have enough data points for alternatives that have not been chosen frequently by the search engine.\nAcknowledgements. We thank Rakesh Agrawal, Alan Halverson, Krishnaram Kenthapadi, Robert Kleinberg, Robert Schapire and Yogi Sharma for their helpful comments and suggestions."}, {"heading": "A Details for the proof of Theorem 6(ii)", "text": "Claim 5. We can define context sequences {x0t} and {x1t }, . . . , {xNt } with the following properties: (1) each sequence {xit}, when paired with a problem instance Ii, defines an eventful bandit problem consistent with all our assumptions, and (2) the sequences {x0t} and {xii} agree through the first i phases.\nProof. Let y1, . . . , ydF \u2208 X be a sequence of contexts such that yj /\u2208 SF({y1, . . . , yj\u22121}) for all j = 1, . . . , dF . We know this sequence exists by the definition of dF . Also assume there exists an \u201calways negative\u201d context x\u2212 such that f(x\u2212) = \u22121 for all f \u2208 F (this assumption is not necessary, but is convenient). Let tj be the first round of phase j.\nDefine {x0t} as follows: let x0tj = yj for each phase 1 \u2264 j \u2264 N , and let x0t = x\u2212 for all other rounds.\nFor 1 \u2264 i \u2264 N , define {xit} as follows: let xitj = yj for each phase 1 \u2264 j \u2264 i, and let xit = x\u2212 for all other rounds.\nClaim 6. (1 \u2212 qi,i) \u2265 T\u22121/3(1\u2212 qi,0)\nProof. Throughout this proof, we fix phase i. Define a realization to be a particular sequence of outcomes of all random samples from click distributions, as well as all random choices (if any), during an execution of algorithm A through the end of phase i. For example, if s = s1, . . . , sM is a realization, then s1 might correspond to the click observed in the first round, s2, . . . , s5 might correspond to random choices made by the algorithm, s6 might correspond to the click observed in the second round, and so on. By the chain rule, for any j \u2208 {0, i}:\nPrIj [s] = PrIj [s1] PrIj [s2|s1] \u00b7 \u00b7 \u00b7PrIj [sM |s1, . . . , sM\u22121]\nFor any realization s, let PrIj [s\u03b1] be the product of terms in the above product that correspond to outcomes other than observed clicks in phase i. Let S be the set of realizations in which arm z is selected by A less than logT times in phase i. Let na,c(s) be the number of times in realization s that arm a \u2208 {y, z} is selected in phase i and payoff c \u2208 {0, 1} is observed as a result. Then\n(1\u2212 qi,0) = \u2211\ns\u2208S\nPrI0 [s]\n= \u2211\ns\u2208S\nPrI0 [s\u03b1] ( 1 2 )ny,0(s) ( 1 2 )ny,1(s) ( e1/3 1+e1/3 )nz,0(s) ( 1 1+e1/3 )nz,1(s)\n= \u2211\ns\u2208S\nPrI0 [s\u03b1] ( 1 2 )ny,0(s) ( 1 2 )ny,1(s) ( e1/3 \u00b7 1 1+e1/3 )nz,0(s) ( 1 e1/3 \u00b7 e1/3 1+e1/3 )nz,1(s)\n\u2264 \u2211\ns\u2208S\nPrI0 [s\u03b1](e 1/3)nz,0(s) ( 1 2 )ny,0(s) ( 1 2 )ny,1(s) ( 1 1+e1/3 )nz,0(s) ( e1/3 1+e1/3 )nz,1(s)\n\u2264 (e1/3)log T \u2211\ns\u2208S\nPrI0 [s\u03b1] ( 1 2 )ny,0(s) ( 1 2 )ny,1(s) ( 1 1+e1/3 )nz,0(s) ( e1/3 1+e1/3 )nz,1(s)\n= T 1/3 \u2211\ns\u2208S\nPrIi [s]\n= T 1/3(1\u2212 qi,i)"}], "references": [{"title": "Online models for content optimization", "author": ["Deepak Agarwal", "Bee-Chung Chen", "Pradheep Elango", "Nitin Motgi", "Seung-Taek Park", "Raghu Ramakrishnan", "Scott Roy", "Joe Zachariah"], "venue": "In 22nd Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Paul Fischer"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["Peter Auer", "Nicol\u00f2 Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "The anatomy of a large-scale hypertextual Web search engine", "author": ["Sergey Brin", "Lawrence Page"], "venue": "Computer Networks and ISDN Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Learning to rank using gradient descent", "author": ["Christopher J.C. Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Gregory N. Hullender"], "venue": "In 22nd Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In 24th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Learning to order things", "author": ["William W. Cohen", "Robert E. Schapire", "Yoram Singer"], "venue": "J. of Artificial Intelligence Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Integration of news content into web results", "author": ["Fernando Diaz"], "venue": "In 2nd Intl. Conf. on Web Search and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Search engine users", "author": ["D. Fallows"], "venue": "Pew Internet and American Life Project,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Yoav Freund", "Raj Iyer", "Robert E. Schapire", "Yoram Singer"], "venue": "J. of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Online Learning with Prior Knowledge", "author": ["Elad Hazan", "Nimrod Megiddo"], "venue": "In 20th Conference on Learning Theory (COLT),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Efficient bandit algorithms for online multiclass prediction", "author": ["Sham M. Kakade", "Shai Shalev-Shwartz", "Ambuj Tewari"], "venue": "In 25th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Bursty and hierarchical structure in streams", "author": ["Jon M. Kleinberg"], "venue": "In 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "Herbert Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1985}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In 21st Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Bandits for Taxonomies: A Model-based Approach", "author": ["Sandeep Pandey", "Deepak Agarwal", "Deepayan Chakrabarti", "Vanja Josifovski"], "venue": "In SIAM Intl. Conf. on Data Mining (SDM),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Multi-armed Bandit Problems with Dependent Arms", "author": ["Sandeep Pandey", "Deepayan Chakrabarti", "Deepak Agarwal"], "venue": "In 24th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["Filip Radlinski", "Robert Kleinberg", "Thorsten Joachims"], "venue": "In 25th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Convex Analysis", "author": ["R. Tyrrell Rockafellar"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1970}, {"title": "Bandit problems with side observations", "author": ["Chih-Chun Wang", "Sanjeev R. Kulkarni", "H. Vincent Poor"], "venue": "IEEE Trans. on Automatic Control,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "There are studies that suggest that queries likely to be intent-shifting \u2014 such as pop culture, news events, trends, and seasonal topics queries \u2014 constitute roughly half of the search queries that a search engine receives [10].", "startOffset": 223, "endOffset": 227}, {"referenceID": 3, "context": "Since traditional ranking features like PageRank [4] change slowly over time, and may be misleading if user intent has shifted very recently, we want to use just the observed click behavior of users to decide which search results to display.", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "Our bandit subroutine \u2014 a novel version of algorithm UCB1 from [2] which additionally provides high-confidence estimates on the suboptimality of arms \u2014 may be of independent interest.", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "The experiments show that if there are no events then the well-studied UCB1 algorithm [2] performs the best.", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 4, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 12, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 7, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 5, "context": "While there has been a substantial amount of work on ranking algorithms [11, 5, 13, 8, 6], all of these results assume that there is a fixed ranking function to learn, not one that shifts over time.", "startOffset": 72, "endOffset": 89}, {"referenceID": 6, "context": "Online bandit algorithms (see [7] for background) have been considered in the context of ranking.", "startOffset": 30, "endOffset": 33}, {"referenceID": 19, "context": "For instance, Radlinski et al [20] showed how to compose several instantiations of a bandit algorithm to produce a ranked list of search results.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "Pandey et al [19] showed that bandit algorithms can be effective in serving advertisements to search engine users.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "The UCB1 algorithm [2] assumes fixed click probabilities and has regret at most O( n \u2206 logT ).", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "S algorithm [3] assumes", "startOffset": 12, "endOffset": 15}, {"referenceID": 21, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 17, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 11, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 16, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 13, "context": "The \u201ccontextual bandits\u201d problem setting [22, 18, 12, 17, 14] is similar to ours.", "startOffset": 41, "endOffset": 61}, {"referenceID": 8, "context": "Diaz [9] used a regularized logistic model to determine when to surface news results for a query.", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "Agarwal et al [1] used several models, including a dynamic linear growth curve model.", "startOffset": 14, "endOffset": 17}, {"referenceID": 14, "context": "For example, Kleinberg [15] describes a state-based model for inferring stages of burstiness.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "Techniques from [20] may be adopted to find a good list of results.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "For bandit, we build on a standard algorithm UCB1 [2]; as it turns out, making it (L, \u01ebS)-testable requires a significantly extended analysis.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "Therefore, by the intermediate value theorem, there exists x \u2208 X and \u03b8 \u2208 [0, 1] such that x = (1\u2212 \u03b8)xt + \u03b8x and w \u00b7 (x + u) = 0.", "startOffset": 73, "endOffset": 79}, {"referenceID": 20, "context": "Proof of (ii): We will use the well-known separating hyperplane theorem [21]: If nonempty convex sets X,Y \u2208 R do not intersect, then there exist a \u2208 R \\ {0} and b \u2208 R such that a \u00b7 x \u2265 b for all x \u2208 X and a \u00b7 y \u2264 b for all y \u2208 Y (3)", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "A natural candidate would be algorithm UCB1 from [2] which does very well on event-free regret:", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "We will use a slightly modified algorithm UCB1 from [2], with a significantly extended analysis.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "The original regret analysis of UCB1 in [2] carries over to UCB1(\u03b1, t0) so as to guarantee event-free regret (5); we omit the details.", "startOffset": 40, "endOffset": 43}, {"referenceID": 15, "context": "Then in each phase i \u2265 1 the algorithm (essentially) needs to solve a fresh instance of the stochastic bandit problem on n\u22121 arms with time horizon T/k and payoffs 1 2 \u00b1 \u01eb, which implies regret \u03a9(n\u01eb ) log(T/k) [16, 2].", "startOffset": 210, "endOffset": 217}, {"referenceID": 1, "context": "Then in each phase i \u2265 1 the algorithm (essentially) needs to solve a fresh instance of the stochastic bandit problem on n\u22121 arms with time horizon T/k and payoffs 1 2 \u00b1 \u01eb, which implies regret \u03a9(n\u01eb ) log(T/k) [16, 2].", "startOffset": 210, "endOffset": 217}], "year": 2010, "abstractText": "Search engines today present results that are often oblivious to abrupt shifts in intent. For example, the query \u2018independence day\u2019 usually refers to a US holiday, but the intent of this query abruptly changed during the release of a major film by that name. While no studies exactly quantify the magnitude of intent-shifting traffic, studies suggest that news events, seasonal topics, pop culture, etc account for 50% of all search queries. This paper shows that the signals a search engine receives can be used to both determine that a shift in intent has happened, as well as find a result that is now more relevant. We present a meta-algorithm that marries a classifier with a bandit algorithm to achieve regret that depends logarithmically on the number of query impressions, under certain assumptions. We provide strong evidence that this regret is close to the best achievable. Finally, via a series of experiments, we demonstrate that our algorithm outperforms prior approaches, particularly as the amount of intent-shifting traffic increases.", "creator": "LaTeX with hyperref package"}}}