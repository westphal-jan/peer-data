{"id": "1008.3169", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2010", "title": "Don't 'have a clue'? Unsupervised co-learning of downward-entailing operators", "abstract": "researchers in textual entailment have begun to consider systematic inferences involving'downward - entailing operators ', an interesting and important class of lexical items that change back the corresponding way textual inferences are frequently made. recent work proposed a method for learning english downward - entailing operators? that requires access to processing a high - quality collection of'negative polarity items'( npis ). however, english is one of the very few languages for which such a list exists. \" we propose the first approach that can be applied to the many languages for which there is no pre - reported existing high - precision database of npis. presented as a preliminary case study, we independently apply our method to vocabulary romanian and show where that our method yields good results. also, we perform a cross - lateral linguistic analysis that suggests interesting connections owing to some findings in linguistic typology.", "histories": [["v1", "Wed, 18 Aug 2010 20:08:22 GMT  (28kb,D)", "https://arxiv.org/abs/1008.3169v1", "pp 1-6 are identical to the ACL 2010 published version; pp. 7-8 are the \"externally-available appendices\""], ["v2", "Sat, 27 Nov 2010 01:51:03 GMT  (28kb,D)", "http://arxiv.org/abs/1008.3169v2", "pp 1-6 are identical to the ACL 2010 published version; pp. 7-8 are the \"externally-available appendices\". Revision contains an additional appendix correcting the origin of the term \"pseudo-polarity item\""]], "COMMENTS": "pp 1-6 are identical to the ACL 2010 published version; pp. 7-8 are the \"externally-available appendices\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cristian danescu-niculescu-mizil", "lillian lee"], "accepted": true, "id": "1008.3169"}, "pdf": {"name": "1008.3169.pdf", "metadata": {"source": "CRF", "title": "Don\u2019t \u2018have a clue\u2019? Unsupervised co-learning of downward-entailing operators", "authors": ["Cristian Danescu-Niculescu-Mizil", "Lillian Lee"], "emails": ["cristian@cs.cornell.edu,", "llee@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "Cristi: \u201cNicio\u201d ... is that adjective you\u2019ve mentioned. Anca: A negative pronominal adjective. Cristi: You mean there are people who analyze that\nkind of thing? Anca: The Romanian Academy. Cristi: They\u2019re crazy.\n\u2014From the movie Police, adjective\nDownward-entailing operators are an interesting and varied class of lexical items that change the default way of dealing with certain types of inferences. They thus play an important role in understanding natural language [6, 18\u201320, etc.].\nWe explain what downward entailing means by first demonstrating the \u201cdefault\u201d behavior, which is upward entailing. The word \u2018observed\u2019 is an example upward-entailing operator: the statement\n(i) \u2018Witnesses observed opium use.\u2019 implies (ii) \u2018Witnesses observed narcotic use.\u2019\nbut not vice versa (we write i \u21d2 ( 6\u21d0) ii). That is, the truth value is preserved if we replace the\nargument of an upward-entailing operator by a superset (a more general version); in our case, the set \u2018opium use\u2019 was replaced by the superset \u2018narcotic use\u2019.\nDownward-entailing (DE) (also known as downward monotonic or monotone decreasing) operators violate this default inference rule: with DE operators, reasoning instead goes from \u201csets to subsets\u201d. An example is the word \u2018bans\u2019:\n\u2018The law bans opium use\u2019 6\u21d2 (\u21d0) \u2018The law bans narcotic use\u2019.\nAlthough DE behavior represents an exception to the default, DE operators are as a class rather common. They are also quite diverse in sense and even part of speech. Some are simple negations, such as \u2018not\u2019, but some other English DE operators are \u2018without\u2019, \u2018reluctant to\u2019, \u2018to doubt\u2019, and \u2018to allow\u2019.1 This variety makes them hard to extract automatically.\nBecause DE operators violate the default \u201csets to supersets\u201d inference, identifying them can potentially improve performance in many NLP tasks. Perhaps the most obvious such tasks are those involving textual entailment, such as question answering, information extraction, summarization, and the evaluation of machine translation [4]. Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1\u20133, 15, 16].2 Other application areas are naturallanguage generation and human-computer interaction, since downward-entailing inferences induce\n1Some examples showing different constructions for analyzing these operators: \u2018The defendant does not own a blue car\u2019 6\u21d2 (\u21d0) \u2018The defendant does not own a car\u2019; \u2018They are reluctant to tango\u2019 6\u21d2 (\u21d0) \u2018They are reluctant to dance\u2019; \u2018Police doubt Smith threatened Jones\u2019 6\u21d2 (\u21d0) \u2018Police doubt Smith threatened Jones or Brown\u2019; \u2018You are allowed to use Mastercard\u2019 6\u21d2 (\u21d0) \u2018You are allowed to use any credit card\u2019.\n2The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later.\nar X\niv :1\n00 8.\n31 69\nv2 [\ncs .C\nL ]\n2 7\nN ov\n2 01\n0\ngreater cognitive load than inferences in the opposite direction [8].\nMost NLP systems for the applications mentioned above have only been deployed for a small subset of languages. A key factor is the lack of relevant resources for other languages. While one approach would be to separately develop a method to acquire such resources for each language individually, we instead aim to ameliorate the resource-scarcity problem in the case of DE operators wholesale: we propose a single unsupervised method that can extract DE operators in any language for which raw text corpora exist.\nOverview of our work Our approach takes the English-centric work of Danescu-Niculescu-Mizil et al. [5] \u2014 DLD09 for short \u2014 as a starting point, as they present the first and, until now, only algorithm for automatically extracting DE operators from data. However, our work departs significantly from DLD09 in the following key respect.\nDLD09 critically depends on access to a highquality, carefully curated collection of negative polarity items (NPIs) \u2014 lexical items such as \u2018any\u2019, \u2018ever\u2019, or the idiom \u2018have a clue\u2019 that tend to occur only in negative environments (see \u00a72 for more details). DLD09 use NPIs as signals of the occurrence of downward-entailing operators. However, almost every language other than English lacks a high-quality accessible NPI list.\nTo circumvent this problem, we introduce a knowledge-lean co-learning approach. Our algorithm is initialized with a very small seed set of NPIs (which we describe how to generate), and then iterates between (a) discovering a set of DE operators using a collection of pseudo-NPIs \u2014 a concept we introduce \u2014 and (b) using the newlyacquired DE operators to detect new pseudo-NPIs.\nWhy this isn\u2019t obvious Although the algorithmic idea sketched above seems quite simple, it is important to note that prior experiments in that direction have not proved fruitful. Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results [14]. Hoeksema [10] discusses why NPIs might be hard to learn from data.3 We circumvent this problem because we are not interested in learning NPIs per se; rather, for our pur-\n3In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of Ku\u0308rschner [12]\u2019s 344 manually collected German NPIs.\nposes, pseudo-NPIs suffice. Also, our preliminary work determined that one of the most famous colearning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem.4\nContributions To begin with, we apply our algorithm to produce the first large list of DE operators for a language other than English. In our case study on Romanian (\u00a74), we achieve quite high precisions at k (for example, iteration achieves a precision at 30 of 87%).\nAuxiliary experiments explore the effects of using a large but noisy NPI list, should one be available for the language in question. Intriguingly, we find that co-learning new pseudo-NPIs provides better results.\nFinally (\u00a75), we engage in some cross-linguistic analysis based on the results of applying our algorithm to English. We find that there are some suggestive connections with findings in linguistic typology.\nAppendix available A more complete account of our work and its implications can be found in a version of this paper containing appendices, available at www.cs.cornell.edu/\u02dccristian/acl2010/."}, {"heading": "2 DLD09: successes and challenges", "text": "In this section, we briefly summarize those aspects of the DLD09 method that are important to understanding how our new co-learning method works.\nDE operators and NPIs Acquiring DE operators is challenging because of the complete lack of annotated data. DLD09\u2019s insight was to make use of negative polarity items (NPIs), which are words or phrases that tend to occur only in negative contexts. The reason they did so is that Ladusaw\u2019s hypothesis [7, 13] asserts that NPIs only occur within the scope of DE operators. Figure 1 depicts examples involving the English NPIs \u2018any\u20195 and \u2018have a clue\u2019 (in the idiomatic sense) that illustrate this relationship. Some other English NPIs are \u2018ever\u2019, \u2018yet\u2019 and \u2018give a damn\u2019.\nThus, NPIs can be treated as clues that a DE operator might be present (although DE operators may also occur without NPIs).\n4We explored three different edge-weighting schemes based on co-occurrence frequencies and seed-set membership, but the results were extremely poor; HITS invariably retrieved very frequent words.\n5The free-choice sense of \u2018any\u2019, as in \u2018I can skim any paper in five minutes\u2019, is a known exception.\nDLD09 algorithm Potential DE operators are collected by extracting those words that appear in an NPI\u2019s context at least once.6 Then, the potential DE operators x are ranked by\nf(x) := fraction of NPI contexts that contain x relative frequency of x in the corpus ,\nwhich compares x\u2019s probability of occurrence conditioned on the appearance of an NPI with its probability of occurrence overall.7\nThe method just outlined requires access to a list of NPIs. DLD09\u2019s system used a subset of John Lawler\u2019s carefully curated and \u201cmoderately complete\u201d list of English NPIs.8 The resultant rankings of candidate English DE operators were judged to be of high quality.\nThe challenge in porting to other languages: cluelessness Can the unsupervised approach of DLD09 be successfully applied to languages other than English? Unfortunately, for most other languages, it does not seem that large, high-quality NPI lists are available.\nOne might wonder whether one can circumvent the NPI-acquisition problem by simply translating a known English NPI list into the target language. However, NPI-hood need not be preserved under translation [17]. Thus, for most languages, we lack the critical clues that DLD09 depends on."}, {"heading": "3 Getting a clue", "text": "In this section, we develop an iterative colearning algorithm that can extract DE operators in the many languages where a high-quality NPI\n6DLD09 policies: (a) \u201cNPI context\u201d was defined as the part of the sentence to the left of the NPI up to the first comma, semi-colon or beginning of sentence; (b) to encourage the discovery of new DE operators, those sentences containing one of a list of 10 well-known DE operators were discarded. For Romanian, we treated only negations (\u2018nu\u2019 and \u2018n-\u2019) and questions as well-known environments.\n7DLD09 used an additional distilled score, but we found that the distilled score performed worse on Romanian.\n8http://www-personal.umich.edu/\u223cjlawler/aue/npi.html\ndatabase is not available, using Romanian as a case study."}, {"heading": "3.1 Data and evaluation paradigm", "text": "We used Rada Mihalcea\u2019s corpus of\u22481.45 million sentences of raw Romanian newswire articles.\nNote that we cannot evaluate impact on textual inference because, to our knowledge, no publicly available textual-entailment system or evaluation data for Romanian exists. We therefore examine the system outputs directly to determine whether the top-ranked items are actually DE operators or not. Our evaluation metric is precision at k of a given system\u2019s ranked list of candidate DE operators; it is not possible to evaluate recall since no list of Romanian DE operators exists (a problem that is precisely the motivation for this paper).\nTo evaluate the results, two native Romanian speakers labeled the system outputs as being \u201cDE\u201d, \u201cnot DE\u201d or \u201cHard (to decide)\u201d. The labeling protocol, which was somewhat complex to prevent bias, is described in the externallyavailable appendices (\u00a77.1). The complete system output and annotations are publicly available at: http://www.cs.cornell.edu/\u02dccristian/acl2010/."}, {"heading": "3.2 Generating a seed set", "text": "Even though, as discussed above, the translation of an NPI need not be an NPI, a preliminary review of the literature indicates that in many languages, there is some NPI that can be translated as \u2018any\u2019 or related forms like \u2018anybody\u2019. Thus, with a small amount of effort, one can form a minimal NPI seed set for the DLD09 method by using an appropriate target-language translation of \u2018any\u2019. For Romanian, we used \u2018vreo\u2019 and \u2018vreun\u2019, which are the feminine and masculine translations of English \u2018any\u2019."}, {"heading": "3.3 DLD09 using the Romanian seed set", "text": "We first check whether DLD09 with the twoitem seed set described in \u00a73.2 performs well on\nRomanian. In fact, the results are fairly poor: for example, the precision at 30 is below 50%. (See blue/dark bars in figure 3 in the externallyavailable appendices for detailed results.)\nThis relatively unsatisfactory performance may be a consequence of the very small size of the NPI list employed, and may therefore indicate that it would be fruitful to investigate automatically extending our list of clues."}, {"heading": "3.4 Main idea: a co-learning approach", "text": "Our main insight is that not only can NPIs be used as clues for finding DE operators, as shown by DLD09, but conversely, DE operators (if known) can potentially be used to discover new NPI-like clues, which we refer to as pseudo-NPIs (or pNPIs for short). By \u201cNPI-like\u201d we mean, \u201cserve as possible indicators of the presence of DE operators, regardless of whether they are actually restricted to negative contexts, as true NPIs are\u201d. For example, in English newswire, the words \u2018allegation\u2019 or \u2018rumor\u2019 tend to occur mainly in DE contexts, like \u2018 denied \u2019 or \u2018 dismissed \u2019, even though they are clearly not true NPIs (the sentence \u2018I heard a rumor\u2019 is fine). Given this insight, we approach the problem using an iterative co-learning paradigm that integrates the search for new DE operators with a search for new pNPIs.\nFirst, we describe an algorithm that is the \u201creverse\u201d of DLD09 (henceforth rDLD), in that it retrieves and ranks pNPIs assuming a given list of DE operators. Potential pNPIs are collected by extracting those words that appear in a DE context (defined here, to avoid the problems of parsing or\nscope determination, as the part of the sentence to the right of a DE operator, up to the first comma, semi-colon or end of sentence); these candidates x are then ranked by\nfr(x) := fraction of DE contexts that contain x relative frequency of x in the corpus .\nThen, our co-learning algorithm consists of the iteration of the following two steps:\n\u2022 (DE learning) Apply DLD09 using a set N of pseudo-NPIs to retrieve a list of candidate DE operators ranked by f (defined in Section 2). Let D be the top n candidates in this list.\n\u2022 (pNPI learning) Apply rDLD using the set D to retrieve a list of pNPIs ranked by fr; extend N with the top nr pNPIs in this list. Increment n.\nHere, N is initialized with the NPI seed set. At each iteration, we consider the output of the algorithm to be the ranked list of DE operators retrieved in the DE-learning step. In our experiments, we initialized n to 10 and set nr to 1."}, {"heading": "4 Romanian results", "text": "Our results show that there is indeed favorable synergy between DE-operator and pNPI retrieval. Figure 2 plots the number of correctly retrieved DE operators in the top k outputs at each iteration. The point at iteration 0 corresponds to a datapoint already discussed above, namely, DLD09 applied to the two \u2018any\u2019-translation NPIs. Clearly, we see general substantial improvement over DLD09, although the increases level off in later iterations.\n(Determining how to choose the optimal number of iterations is a subject for future research.)\nAdditional experiments, described in the externally-available appendices (\u00a77.2), suggest that pNPIs can even be more effective clues than a noisy list of NPIs. (Thus, a larger seed set does not necessarily mean better performance.) pNPIs also have the advantage of being derivable automatically, and might be worth investigating from a linguistic perspective in their own right."}, {"heading": "5 Cross-linguistic analysis", "text": "Applying our algorithm to English: connections to linguistic typology So far, we have made no assumptions about the language on which our algorithm is applied. A valid question is, does the quality of the results vary with choice of application language? In particular, what happens if we run our algorithm on English?\nNote that in some sense, this is a perverse question: the motivation behind our algorithm is the non-existence of a high-quality list of NPIs for the language in question, and English is essentially the only case that does not fit this description. On the other hand, the fact that DLD09 applied their method for extraction of DE operators to English necessitates some form of comparison, for the sake of experimental completeness.\nWe thus ran our algorithm on the English BLLIP newswire corpus with seed set {\u2018any\u2019}. We observe that, surprisingly, the iterative addition of pNPIs has very little effect: the precisions at k are good at the beginning and stay about the same across iterations (for details see figure 5 in in the externally-available appendices). Thus, on English, co-learning does not hurt performance, which is good news; but unlike in Romanian, it does not lead to improvements.\nWhy is English \u2018any\u2019 seemingly so \u201cpowerful\u201d, in contrast to Romanian, where iterating beyond the initial \u2018any\u2019 translations leads to better results? Interestingly, findings from linguistic typology may shed some light on this issue. Haspelmath [9] compares the functions of indefinite pronouns in 40 languages. He shows that English is one of the minority of languages (11 out of 40)9 in which there exists an indefinite pronoun series that occurs in all (Haspelmath\u2019s) classes of DE contexts, and thus can constitute a sufficient seed on\n9English, Ancash Quechua, Basque, Catalan, French, Hindi/Urdu, Irish, Portuguese, Swahili, Swedish, Turkish.\nits own. In the other languages (including Romanian),10 no indirect pronoun can serve as a sufficient seed. So, we expect our method to be viable for all languages; while the iterative discovery of pNPIs is not necessary (although neither is it harmful) for the subset of languages for which a sufficient seed exists, such as English, it is essential for the languages for which, like Romanian, \u2018any\u2019-equivalents do not suffice.\nUsing translation Another interesting question is whether directly translating DE operators from English is an alternative to our method. First, we emphasize that there exists no complete list of English DE operators (the largest available collection is the one extracted by DLD09). Second, we do not know whether DE operators in one language translate into DE operators in another language. Even if that were the case, and we somehow had access to ideal translations of DLD09\u2019s list, there would still be considerable value in using our method: 14 (39%) of our top 36 highestranked Romanian DE operators for iteration 9 do not, according to the Romanian-speaking author, have English equivalents appearing on DLD09\u2019s 90-item list. Some examples are: \u2018abt\u0327inut\u2019 (abstained), \u2018criticat\u2019 (criticized) and \u2018react\u0327ionat\u2019 (reacted). Therefore, a significant fraction of the DE operators derived by our co-learning algorithm would have been missed by the translation alternative even under ideal conditions."}, {"heading": "6 Conclusions", "text": "We have introduced the first method for discovering downward-entailing operators that is universally applicable. Previous work on automatically detecting DE operators assumed the existence of a high-quality collection of NPIs, which renders it inapplicable in most languages, where such a resource does not exist. We overcome this limitation by employing a novel co-learning approach, and demonstrate its effectiveness on Romanian.\nAlso, we introduce the concept of pseudo-NPIs. Auxiliary experiments described in the externallyavailable appendices show that pNPIs are actually more effective seeds than a noisy \u201ctrue\u201d NPI list.\nFinally, we noted some cross-linguistic differences in performance, and found an interesting connection between these differences and Haspelmath\u2019s [9] characterization of cross-linguistic variation in the occurrence of indefinite pronouns.\n10Examples: Chinese, German, Italian, Polish, Serbian.\nAcknowledgments We thank Tudor Marian for serving as an annotator, Rada Mihalcea for access to the Romanian newswire corpus, and Claire Cardie, Yejin Choi, Effi Georgala, Mark Liberman, Myle Ott, Joa\u0303o Paula Muchado, Stephen Purpura, Mark Yatskar, Ainur Yessenalina, and the anonymous reviewers for their helpful comments. Supported by NSF grant IIS-0910664."}, {"heading": "7 Appendices", "text": ""}, {"heading": "7.1 Labeling protocol", "text": "Following DLD09, system outputs (i.e., candidate DE operators) were combined with a set of distractors \u2014 words related to but not synonymous with the system outputs, according to WordNet. We chose distractors that were similar to real system outputs so that the distractors would not obviously \u201cstand out\u201d as seeming unusual.\nThe combined collection was presented in randomized order to two Romanian native speakers (one an author, one not), whose task was to label each item as either \u201cDE\u201d, \u201cnot DE\u201d, or \u201cHard\u201d, meaning \u201chard to decide\u201d, and to justify their choice by providing an inference example of the kind we discussed in the Introduction. The judges were informed of the presence of distractors; this served to guard against the judges being biased towards simply defaulting to the \u201cDE\u201d label. We should mention that the annotation process was very time consuming because generating definitive example inferences can be quite difficult, limiting the number of output items that we could get labeled.11 Also, because creativity can be needed to construct the requisite supporting inferences, the two judges first worked independently, and then conferred to resolve their disagreements. The complete system output and the annotations are publicly available at: http://www.cs.cornell.edu/\u02dccristian/acl2010/."}, {"heading": "7.2 Re-ranking marginal NPIs", "text": "In this paper we proposed a DE-operator discovery algorithm that can be applied to languages for which there is no pre-existing collection of NPIs; this is useful because collecting NPIs is, as discussed in the Introduction, quite difficult \u2014 determining NPI-hood can be hard even for experts.\nHowever, there are a few languages wherein the effort has been expended to collect a large noisy collection of NPIs, and Romanian is one of them. (This motivated our choice of this language.) CoDII-NPI.ro12 contains 58 Romanian NPIs, but in the opinion of the Romanian-speaking author, many are marginal, by which we mean that their NPI sense is very infrequent.13 The existence\n11Annotators reported a rate of about 10 examples per hour.\n12http://www.sfb441.uni-tuebingen.de/a5/codii/, see Trawn\u0301ski and Soehn, \u201cA Multilingual Database of Polarity Items\u201d, LREC 2008.\n13For example, CoDII-NPI.ro contains the Romanian verb\nof CoDII-NPI.ro allows us to ask: which is more effective, a list including both true and marginal NPIs (presuming one is lucky enough to have one), or pseudo-NPIs?\nFirst, it turns out that the CoDII-NPI.ro list containing marginal NPIs is much less effective input to the DLD09 method than are the two \u2018any\u2019translation NPIs, as shown in Figure 3. Given that \u2018vreo\u2019 and \u2018vreun\u2019 are contained in CoDIINPI.ro, we can conclude that the DLD09 algorithm is highly sensitive to the marginal items in that list.\nThere is another way to employ the CoDIINPI.ro list: use our co-learning algorithm, but try to have the iterations only add high-quality CoDIINPI.ro items, rather than arbitrary pseudo-NPIs; essentially, this amounts to re-ranking CoDIINPI.ro. We implement this by altering the pNPIlearning step in our algorithm as follows: extend N with the top nr NPIs on the CoDII-NPI.ro list, as opposed to the top nr pNPIs overall. However, Figure 4 demonstrates that this substantially underperforms the algorithm as originally proposed, i.e., based on pNPIs.\nAll this suggests that pNPIs might be more effective clues than a mixture of non-marginal and marginal NPIs. In addition, pNPIs are derivable automatically, and might even be worth investigat-\n\u2018a mis\u0327ca\u2019 (\u2018to move\u2019), which is very frequently used in positive contexts. The inclusion of marginal NPIs is presumably due to a design decision to make the list have high recall.\ning from a linguistic perspective in their own right.\n7.3 Co-learning results on English"}, {"heading": "7.4 Corrigendum", "text": "In the proceedings version, we missed the fact that the concept of pseudo-NPIs had been previously discussed by Hoeksema [10]. Hoeksema considers a few examples of what he called \u201cpseudopolarity items\u201d in the context of speculating on how polarity sensitivity might arise."}], "references": [{"title": "Efficient semantic deduction and approximate matching over compact parse forests", "author": ["Roy Bar-Haim", "Jonathan Berant", "Ido Dagan", "Iddo Greental", "Shachar Mirkin", "Eyal Shnarch", "Idan Szpektor"], "venue": "In Proceedings of the Text Analysis Conference (TAC),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "A simple system for detecting non-entailment", "author": ["Eric Breck"], "venue": "In Proceedings of the Text Analysis Conference (TAC),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Creating a natural logic inference system with combinatory categorial grammar", "author": ["Christos Christodoulopoulos"], "venue": "Master\u2019s thesis, University of Edinburgh,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "The PASCAL Recognising Textual Entailment challenge. In Machine Learning Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First PAS", "author": ["Ido Dagan", "Oren Glickman", "Bernardo Magnini"], "venue": "CAL Machine Learning Challenges Workshop,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Without a \u2018doubt\u2019? Unsupervised discovery of downwardentailing operators", "author": ["Cristian Danescu-Niculescu-Mizil", "Lillian Lee", "Richard Ducott"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "The role of negative polarity and concord marking in natural language reasoning", "author": ["David Dowty"], "venue": "Proceedings of SALT IV,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Polarity and the scale principle", "author": ["Gilles Fauconnier"], "venue": "In Proceedings of the Chicago Linguistic Society (CLS), pages 188\u2013199,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "Monotonicity and processing load", "author": ["Bart Geurts", "Frans van der Slik"], "venue": "Journal of Semantics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Indefinite Pronouns", "author": ["Martin Haspelmath"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Corpus study of negative polarity items", "author": ["Jack Hoeksema"], "venue": "IV-V Jornades de corpus linguistics 1996-1997,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["Jon Kleinberg"], "venue": "In Proceedings of the 9th ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 668\u2013677,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Studien zur Negation im Deutschen", "author": ["Wilfried K\u00fcrschner"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1983}, {"title": "Polarity Sensitivity as Inherent Scope Relations", "author": ["William A. Ladusaw"], "venue": "Ph.D. thesis date", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1980}, {"title": "The retrieval and classification of Negative Polarity Items using statistical profiles", "author": ["Timm Lichte", "Jan-Philipp Soehn"], "venue": "In Sam Featherston and Wolfgang Sternefeld, editors, Roots: Linguistics in Search of its Evidential Base,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Modeling semantic containment and exclusion in natural language inference", "author": ["Bill MacCartney", "Christopher D. Manning"], "venue": "In Proceedings of COLING,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Computing relative polarity for textual inference", "author": ["Rowan Nairn", "Cleo Condoravdi", "Lauri Karttunen"], "venue": "In Proceedings of Inference in Computational Semantics (ICoS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Negative polarity items: Corpus linguistics, semantics, and psycholinguistics: Day 2: Corpus linguistics", "author": ["Frank Richter", "Janina Rad\u00f3", "Manfred Sailer"], "venue": "Tutorial slides: http://www.sfs.uni-tuebingen.de/\u223cfr/ esslli/08/byday/day2/day2-part1.pdf,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Studies on natural logic and categorial grammar", "author": ["V\u0131\u0301ctor S\u00e1nchez Valencia"], "venue": "PhD thesis, University of Amsterdam,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Essays in Logical Semantics", "author": ["Johan van Benthem"], "venue": "Reidel, Dordrecht,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}], "referenceMentions": [{"referenceID": 3, "context": "Perhaps the most obvious such tasks are those involving textual entailment, such as question answering, information extraction, summarization, and the evaluation of machine translation [4].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1\u20133, 15, 16].", "startOffset": 246, "endOffset": 259}, {"referenceID": 1, "context": "Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1\u20133, 15, 16].", "startOffset": 246, "endOffset": 259}, {"referenceID": 2, "context": "Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1\u20133, 15, 16].", "startOffset": 246, "endOffset": 259}, {"referenceID": 14, "context": "Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1\u20133, 15, 16].", "startOffset": 246, "endOffset": 259}, {"referenceID": 15, "context": "Researchers are in fact beginning to build textualentailment systems that can handle inferences involving downward-entailing operators other than simple negations, although these systems almost all rely on small handcrafted lists of DE operators [1\u20133, 15, 16].", "startOffset": 246, "endOffset": 259}, {"referenceID": 1, "context": "The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "The exception [2] employs the list automatically derived by Danescu-Niculescu-Mizil, Lee, and Ducott [5], described later.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "greater cognitive load than inferences in the opposite direction [8].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "[5] \u2014 DLD09 for short \u2014 as a starting point, as they present the first and, until now, only algorithm for automatically extracting DE operators from data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Preliminary work on learning (German) NPIs using a small list of simple known DE operators did not yield strong results [14].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Hoeksema [10] discusses why NPIs might be hard to learn from data.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of K\u00fcrschner [12]\u2019s 344 manually collected German NPIs.", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "In fact, humans can have trouble agreeing on NPI-hood; for instance, Lichte and Soehn [14] mention doubts about over half of K\u00fcrschner [12]\u2019s 344 manually collected German NPIs.", "startOffset": 135, "endOffset": 139}, {"referenceID": 10, "context": "Also, our preliminary work determined that one of the most famous colearning algorithms, hubs and authorities or HITS [11], is poorly suited to our problem.", "startOffset": 118, "endOffset": 122}, {"referenceID": 6, "context": "The reason they did so is that Ladusaw\u2019s hypothesis [7, 13] asserts that NPIs only occur within the scope of DE operators.", "startOffset": 52, "endOffset": 59}, {"referenceID": 12, "context": "The reason they did so is that Ladusaw\u2019s hypothesis [7, 13] asserts that NPIs only occur within the scope of DE operators.", "startOffset": 52, "endOffset": 59}, {"referenceID": 16, "context": "However, NPI-hood need not be preserved under translation [17].", "startOffset": 58, "endOffset": 62}, {"referenceID": 8, "context": "Haspelmath [9] compares the functions of indefinite pronouns in 40 languages.", "startOffset": 11, "endOffset": 14}, {"referenceID": 8, "context": "Finally, we noted some cross-linguistic differences in performance, and found an interesting connection between these differences and Haspelmath\u2019s [9] characterization of cross-linguistic variation in the occurrence of indefinite pronouns.", "startOffset": 147, "endOffset": 150}], "year": 2017, "abstractText": "Researchers in textual entailment have begun to consider inferences involving downward-entailing operators, an interesting and important class of lexical items that change the way inferences are made. Recent work proposed a method for learning English downward-entailing operators that requires access to a high-quality collection of negative polarity items (NPIs). However, English is one of the very few languages for which such a list exists. We propose the first approach that can be applied to the many languages for which there is no pre-existing high-precision database of NPIs. As a case study, we apply our method to Romanian and show that our method yields good results. Also, we perform a cross-linguistic analysis that suggests interesting connections to some findings in linguistic typology.", "creator": "LaTeX with hyperref package"}}}