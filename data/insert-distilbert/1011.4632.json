{"id": "1011.4632", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Nov-2010", "title": "Random Projections for $k$-means Clustering", "abstract": "this 2005 paper discusses the topic of spectral dimensionality reduction for $ k $ - means clustering. we prove that any set of $ n $ points in $ d $ dimensions ( rows in a matrix $ a \\ in \\ rr ^ { n \\ times d } $ ) can be slowly projected into $ pairs t = \\ omega ( log k / \\ eps ^ 2 ) $ 3 dimensions, for course any $ \\ eps \\ in ( 0, 1 / 3 ) $, shown in $ + o ( n d \\ lceil \\ partial eps ^ { - 2 } k / \\ log ( d ) \\ rceil ) $ time, constructed such that with constant average probability the optimal $ k $ - partition of the point set population is preserved within a column factor of $ 0 2 + \\ eps $. the projection is done by post - multiplying $ a $ with a $ d \\ times t $ random matrix $ r $ having entries $ + no 1 / \\ sqrt { t } $ or $ - 1 / \\ sqrt { t } $ with equal probability. a 2d numerical implementation of our technique and experiments on a simple large face where images and dataset verify the euclidean speed and the accuracy bounds of our theoretical results.", "histories": [["v1", "Sun, 21 Nov 2010 02:37:10 GMT  (52kb)", "http://arxiv.org/abs/1011.4632v1", "Neural Information Processing Systems (NIPS) 2010"]], "COMMENTS": "Neural Information Processing Systems (NIPS) 2010", "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["christos boutsidis", "anastasios zouzias", "petros drineas"], "accepted": true, "id": "1011.4632"}, "pdf": {"name": "1011.4632.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n01 1.\n46 32\nv1 [\ncs .A\nI] 2\n1 N\nov 2\n01 0\n\u221a t or \u22121/ \u221a t with equal\nprobability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results."}, {"heading": "1 Introduction", "text": "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20]. In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4]. This paper focuses on the application of the random projection method (see Section 2.3) to the k-means clustering problem (see Definition 1). Formally, assuming as input a set of n points in d dimensions, our goal is to randomly project the points into d\u0303 dimensions, with d\u0303 \u226a d, and then apply a k-means clustering algorithm (see Definition 2) on the projected points. Of course, one should be able to compute the projection fast without distorting significantly the \u201cclusters\u201d of the original point set. Our algorithm (see Algorithm 1) satisfies both conditions by computing the embedding in time linear in the size of the input and by distorting the \u201cclusters\u201d of the dataset by a factor of at most 2 + \u03b5, for some \u03b5 \u2208 (0, 1/3) (see Theorem 1). We believe that the high dimensionality of modern data will render our algorithm useful and attractive in many practical applications [9].\nDimensionality reduction encompasses the union of two different approaches: feature selection, which embeds the points into a low-dimensional space by selecting actual dimensions of the data, and feature extraction, which finds an embedding by constructing new artificial features that are, for example, linear combinations of the original features. Let A be an n \u00d7 d matrix containing n d-dimensional points (A(i) denotes the i-th point of the set), and let k be the number of clusters (see also Section 2.2 for more notation). We slightly abuse notation by also denoting by A the n-point set formed by the rows of A. We say that an embedding f : A \u2192 Rd\u0303 with f(A(i)) = A\u0303(i) for all i \u2208 [n] and some d\u0303 < d, preserves the clustering structure of A within a factor \u03c6, for some \u03c6 \u2265 1, if finding an optimal clustering in A\u0303 and plugging it back to A is only a factor of \u03c6 worse than finding the optimal clustering directly in A. Clustering optimality and approximability are formally presented in Definitions 1 and 2, respectively. Prior efforts on designing provably accurate dimensionality reduction methods for k-means clustering include: (i) the Singular Value Decomposition (SVD), where one finds an embedding with image A\u0303 = Uk\u03a3k \u2208 Rn\u00d7k such that the clustering structure is preserved within a factor of two; (ii) random projections, where one projects the input points into t = \u2126(log(n)/\u03b52) dimensions such that with constant probability the clustering structure is preserved within a factor of 1+\u03b5 (see Section 2.3); (iii) SVD-based feature selection, where one can use the SVD to find c = \u2126(k log(k/\u03b5)/\u03b52) actual features, i.e. an embedding with image A\u0303 \u2208 Rn\u00d7c containing (rescaled) columns fromA, such that with constant probability the clustering structure is preserved within a factor of 2 + \u03b5. These results are summarized in Table 1. A head-to-head comparison of our algorithm with existing results allows us to claim the following improvements: (i)\nreduce the running time by a factor of min{n, d}\u2308\u03b52 log(d)/k\u2309, while losing only a factor of \u03b5 in the approximation accuracy and a factor of 1/\u03b52 in the dimension of the embedding; (ii) reduce the dimension of the embedding and the running time by a factor of log(n)/k while losing a factor of one in the approximation accuracy; (iii) reduce the dimension of the embedding by a factor of log(k/\u03b5) and the running time by a factor of min{n, d}\u2308\u03b52 log(d)/k\u2309, respectively. Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]). However, they lack a theoretical worst case analysis of the form we describe in this work."}, {"heading": "2 Preliminaries", "text": "We start by formally defining the k-means clustering problem using matrix notation. Later in this section, we precisely describe the approximability framework adopted in the k-means clustering literature and fix the notation.\nDefinition 1. [THE K-MEANS CLUSTERING PROBLEM] Given a set of n points in d dimensions (rows in an n\u00d7 d matrix A) and a positive integer k denoting the number of clusters, find the n\u00d7 k indicator matrix Xopt such that\nXopt = arg min X\u2208X\n\u2225 \u2225A\u2212XX\u22a4A \u2225 \u2225 2\nF . (1)\nHere X denotes the set of all n\u00d7k indicator matrices X . The functional F (A,X) = \u2225 \u2225A\u2212XX\u22a4A \u2225 \u2225 2\nF is the so-called\nk-means objective function. An n \u00d7 k indicator matrix has exactly one non-zero element per row, which denotes cluster membership. Equivalently, for all i = 1, . . . , n and j = 1, . . . , k, the i-th point belongs to the j-th cluster if and only if Xij = 1/ \u221a zj , where zj denotes the number of points in the corresponding cluster. Note that X\u22a4X = Ik, where Ik is the k \u00d7 k identity matrix.\n2.1 Approximation Algorithms for k-means clustering\nFinding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation algorithms for k-means clustering. The following definition captures the framework of such efforts.\nDefinition 2. [K-MEANS APPROXIMATION ALGORITHM] An algorithm is a \u201c\u03b3-approximation\u201d for the k-means clustering problem (\u03b3 \u2265 1) if it takes inputs A and k, and returns an indicator matrix X\u03b3 that satisfies with probability at least 1\u2212 \u03b4\u03b3 ,\n\u2225 \u2225A\u2212X\u03b3X\u22a4\u03b3 A \u2225 \u2225 2 F \u2264 \u03b3 min\nX\u2208X\n\u2225 \u2225A\u2212XX\u22a4A \u2225 \u2225 2\nF . (2)\nIn the above, \u03b4\u03b3 \u2208 [0, 1) is the failure probability of the \u03b3-approximation k-means algorithm.\nFor our discussion, we fix the \u03b3-approximation algorithm to be the one presented in [14], which guarantees \u03b3 = 1+ \u03b5\u2032 for any \u03b5\u2032 \u2208 (0, 1] with running time O(2(k/\u03b5\u2032)O(1)dn)."}, {"heading": "2.2 Notation", "text": "Given an n \u00d7 d matrix A and an integer k with k < min{n, d}, let Uk \u2208 Rn\u00d7k (resp. Vk \u2208 Rd\u00d7k) be the matrix of the top k left (resp. right) singular vectors of A, and let \u03a3k \u2208 Rk\u00d7k be a diagonal matrix containing the top\nk singular values of A in non-increasing order. If we let \u03c1 be the rank of A, then A\u03c1\u2212k is equal to A \u2212 Ak, with Ak = Uk\u03a3kV \u22a4 k . By A(i) we denote the i-th row of A. For an index i taking values in the set {1, . . . , n} we write i \u2208 [n]. We denote, in non-increasing order, the non-negative singular values of A by \u03c3i(A) with i \u2208 [\u03c1]. \u2016A\u2016F and \u2016A\u20162 denote the Frobenius and the spectral norm of a matrix A, respectively. A\u2020 denotes the pseudo-inverse of A, i.e. the unique d \u00d7 n matrix satisfying A = AA\u2020A, A\u2020AA\u2020 = A\u2020, (AA\u2020)\u22a4 = AA\u2020, and (A\u2020A)\u22a4 = A\u2020A. Note also that \u2225 \u2225A\u2020\n\u2225 \u2225 2 = \u03c31(A\n\u2020) = 1/\u03c3\u03c1(A) and \u2016A\u20162 = \u03c31(A) = 1/\u03c3\u03c1(A\u2020). A useful property of matrix norms is that for any two matrices C and T of appropriate dimensions, \u2016CT \u2016F \u2264 \u2016C\u2016F \u2016T \u20162; this is a stronger version of the standard submultiplicavity property. We call P a projector matrix if it is square and P 2 = P . We use E [Y ] and Var [Y ] to take the expectation and the variance of a random variable Y and P (e) to take the probability of an event e. We abbreviate \u201cindependent identically distributed\u201d to \u201ci.i.d.\u201d and \u201cwith probability\u201d to \u201cw.p.\u201d. Finally, all logarithms are base two."}, {"heading": "2.3 Random Projections", "text": "A classical result of Johnson and Lindenstrauss states that any n-point set in d dimensions - rows in a matrix A \u2208 Rn\u00d7d - can be linearly projected into t = \u2126(log(n)/\u03b52) dimensions while preserving pairwise distances within a factor of 1\u00b1\u03b5 using a random orthonormal matrix [12]. Subsequent research simplified the proof of the above result by showing that such a projection can be generated using a d\u00d7 t random Gaussian matrix R, i.e., a matrix whose entries are i.i.d. Gaussian random variables with zero mean and variance 1/ \u221a t [11]. More precisely, the following inequality holds with high probability over the randomness of R,\n(1\u2212 \u03b5) \u2225 \u2225A(i) \u2212A(j) \u2225 \u2225 2 \u2264 \u2225 \u2225A(i)R\u2212A(j)R \u2225 \u2225 2 \u2264 (1 + \u03b5) \u2225 \u2225A(i) \u2212A(j) \u2225 \u2225 2 . (3)\nNotice that such an embedding A\u0303 = AR preserves the metric structure of the point-set, so it also preserves, within a factor of 1 + \u03b5, the optimal value of the k-means objective function of A. Achlioptas proved that even a (rescaled) random sign matrix suffices in order to get the same guarantees as above [1], an approach that we adopt here (see step two in Algorithm 1). Moreover, in this paper we will heavily exploit the structure of such a random matrix, and obtain, as an added bonus, savings on the computation of the projection.\n3 A random-projection-type k-means algorithm Algorithm 1 takes as inputs the matrix A \u2208 Rn\u00d7d, the number of clusters k, an error parameter \u03b5 \u2208 (0, 1/3), and some \u03b3-approximation k-means algorithm. It returns an indicator matrix X\u03b3\u0303 determining a k-partition of the rows of A.\nInput: n\u00d7 d matrix A (n points, d features), number of clusters k, error parameter \u03b5 \u2208 (0, 1/3), and \u03b3-approximation k-means algorithm. Output: Indicator matrix X\u03b3\u0303 determining a k-partition on the rows of A.\n1. Set t = \u2126(k/\u03b52), i.e. set t = to \u2265 ck/\u03b52 for a sufficiently large constant c. 2. Compute a random d\u00d7 t matrix R as follows. For all i \u2208 [d], j \u2208 [t]\nRij =\n{ +1/ \u221a t,w.p. 1/2,\n\u22121/ \u221a t,w.p. 1/2.\n3. Compute the product A\u0303 = AR. 4. Run the \u03b3-approximation algorithm on A\u0303 to obtain X\u03b3\u0303 ; Return the indicator matrix X\u03b3\u0303\nAlgorithm 1: A random projection algorithm for k-means clustering."}, {"heading": "3.1 Running time analysis", "text": "Algorithm 1 reduces the dimensions of A by post-multiplying it with a random sign matrix R. Interestingly, any \u201crandom projection matrix\u201d R that respects the properties of Lemma 2 with t = \u2126(k/\u03b52) can be used in this step. If R is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and\ncompute the productAR in O(nd\u2308\u03b5\u22122k/ log(d)\u2309) time. Indeed, the mailman algorithm computes (after preprocessing 1) a matrix-vector product of any d-dimensional vector (row of A) with an d \u00d7 log(d) sign matrix in O(d) time. By partitioning the columns of our d \u00d7 t matrix R into \u2308t/ log(d)\u2309 blocks, the claim follows. Notice that when k = O(log(d)), then we get an - almost - linear time complexity O(nd/\u03b52). The latter assumption is reasonable in our setting since the need for dimension reduction in k-means clustering arises usually in high-dimensional data (large d). Other choices of R would give the same approximation results; the time complexity to compute the embedding would be different though. A matrix where each entry is a random Gaussian variable with zero mean and variance 1/ \u221a t would imply an O(knd/\u03b52) time complexity (naive multiplication). In our experiments in Section 5 we experiment with the matrix R described in Algorithm 1 and employ MatLab\u2019s matrix-matrix BLAS implementation to proceed in the third step of the algorithm. We also experimented with a novel MatLab/C implementation of the mailman algorithm but, in the general case, we were not able to outperform MatLab\u2019s built-in routines (see section 5.2).\nFinally, note that any \u03b3-approximation algorithm may be used in the last step of Algorithm 1. Using, for example, the algorithm of [14] with \u03b3 = 1 + \u03b5 would result in an algorithm that preserves the clustering within a factor of 2 + \u03b5, for any \u03b5 \u2208 (0, 1/3), running in time O(nd\u2308\u03b5\u22122k/ log(d)\u2309 + 2(k/\u03b5)O(1)kn/\u03b52). In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well. We thus employ the Lloyd algorithm for our experimental evaluation of our algorithm in Section 5. Note that, after using the proposed dimensionality reduction method, the cost of the Lloyd heuristic is only O(nk2/\u03b52) per iteration. This should be compared to the cost of O(knd) per iteration if applied on the original high dimensional data."}, {"heading": "4 Main Theorem", "text": "Theorem 1 is our main quality-of-approximation result for Algorithm 1. Notice that if \u03b3 = 1, i.e. if the k-means problem with inputs A\u0303 and k is solved exactly, Algorithm 1 guarantees a distortion of at most 2 + \u03b5, as advertised.\nTheorem 1. Let the n \u00d7 d matrix A and the positive integer k < min{n, d} be the inputs of the k-means clustering problem. Let \u03b5 \u2208 (0, 1/3) and assume access to a \u03b3-approximation k-means algorithm. Run Algorithm 1 with inputs"}, {"heading": "A, k, \u03b5, and the \u03b3-approximation algorithm in order to construct an indicator matrix X\u03b3\u0303 . Then with probability at", "text": "least 0.97\u2212 \u03b4\u03b3 ,\n\u2225 \u2225A\u2212X\u03b3\u0303X\u22a4\u03b3\u0303 A \u2225 \u2225 2 F \u2264 (1 + (1 + \u03b5)\u03b3) \u2225 \u2225A\u2212XoptX\u22a4optA \u2225 \u2225 2 F . (4)\nProof of Theorem 1\nThe proof of Theorem 1 employs several results from [19] including Lemma 6, 8 and Corollary 11. We summarize these results in Lemma 2 below. Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19]. Theorem 1.1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].\nLemma 2. Assume that the matrix R is constructed by using Algorithm 1 with inputs A, k and \u03b5.\n1. Singular Values Preservation: For all i \u2208 [k] and w.p. at least 0.99, |1\u2212 \u03c3i(V \u22a4k R)| \u2264 \u03b5.\n2. Matrix Multiplication: For any two matrices S \u2208 Rn\u00d7d and T \u2208 Rd\u00d7k,\nE [\u2225 \u2225ST \u2212 SRR\u22a4T \u2225 \u2225 2\nF\n]\n\u2264 2 t \u2016S\u20162F \u2016T \u2016 2 F .\n3. Moments: For any C \u2208 Rn\u00d7d: E [ \u2016CR\u20162F ] = \u2016C\u20162F and Var [\u2016CR\u2016F] \u2264 2 \u2016C\u2016 4 F /t.\nThe first statement above assumes c being sufficiently large (see step 1 of Algorithm 1). We continue with several novel results of general interest.\n1Reading the input d \u00d7 log d sign matrix requires O(d log d) time. However, in our case we only consider multiplication with a random sign matrix, therefore we can avoid the preprocessing step by directly computing a random correspondence matrix as discussed in [15, Preprocessing Section].\nLemma 3. Under the same assumptions as in Lemma 2 and w.p. at least 0.99, \u2225 \u2225 \u2225(V \u22a4k R)\n\u2020 \u2212 (V \u22a4k R)\u22a4 \u2225 \u2225 \u2225 2 \u2264 3\u03b5. (5)\nProof. Let \u03a6 = V \u22a4k R; note that \u03a6 is a k\u00d7 t matrix and the SV D of \u03a6 is \u03a6 = U\u03a6\u03a3\u03a6V \u22a4\u03a6 , where U\u03a6 and \u03a3\u03a6 are k\u00d7k matrices, and V\u03a6 is a t\u00d7 k matrix. By taking the SVD of (V \u22a4k R) \u2020 and (V \u22a4k R)\n\u22a4 we get \u2225 \u2225 \u2225(V \u22a4k R)\n\u2020 \u2212 (V \u22a4k R)\u22a4 \u2225 \u2225 \u2225 2 = \u2225 \u2225V\u03a6\u03a3 \u22121 \u03a6 U \u22a4 \u03a6 \u2212 V\u03a6\u03a3\u03a6U\u22a4\u03a6 \u2225 \u2225 2 = \u2225 \u2225V\u03a6(\u03a3 \u22121 \u03a6 \u2212 \u03a3\u03a6)U\u22a4\u03a6 \u2225 \u2225 2 = \u2225 \u2225\u03a3\u22121\u03a6 \u2212 \u03a3\u03a6 \u2225 \u2225 2 ,\nsince V\u03a6 and U\u22a4\u03a6 can be dropped without changing any unitarily invariant norm. Let \u03a8 = \u03a3 \u22121 \u03a6 \u2212 \u03a3\u03a6; \u03a8 is a k \u00d7 k diagonal matrix. Assuming that, for all i \u2208 [k], \u03c3i(\u03a6) and \u03c4i(\u03a8) denote the i-th largest singular value of \u03a6 and the i-th diagonal element of \u03a8, respectively, it is\n\u03c4i(\u03a8) = 1\u2212 \u03c32i (\u03a6) \u03c3i(\u03a6) .\nSince \u03a8 is a diagonal matrix,\n\u2016\u03a8\u20162 = max1\u2264i\u2264k \u03c4i(\u03a8) = max1\u2264i\u2264k 1\u2212 \u03c32i (\u03a6) \u03c3i(\u03a6) .\nThe first statement of Lemma 2, our choice of \u03b5 \u2208 (0, 1/3), and elementary calculations suffice to conclude the proof.\nLemma 4. Under the same assumptions as in Lemma 2 and for any n\u00d7 d matrix C w.p. at least 0.99, \u2016CR\u2016F \u2264 \u221a (1 + \u03b5) \u2016C\u2016F . (6)\nProof. Notice that there exists a sufficiently large constant c such that t \u2265 ck/\u03b52. Then, setting Z = \u2016CR\u20162F, using the third statement of Lemma 2, the fact that k \u2265 1, and Chebyshev\u2019s inequality we get\nP\n( |Z \u2212 E [Z] | \u2265 \u03b5 \u2016C\u20162F ) \u2264 Var [Z] \u03b52 \u2016C\u20164F \u2264 2 \u2016C\u2016 4 F t\u03b52 \u2016C\u20164F \u2264 2 ck \u2264 0.01.\nThe last inequality follows assuming c sufficiently large. Finally, taking square root on both sides concludes the proof.\nLemma 5. Under the same assumptions as in Lemma 2 and w.p. at least 0.97,\nAk = (AR)(V \u22a4 k R) \u2020 V \u22a4k + E, (7)\nwhere E is an n\u00d7 d matrix with \u2016E\u2016F \u2264 4\u03b5 \u2016A\u2212Ak\u2016F.\nProof. Since (AR)(V \u22a4k R) \u2020 V \u22a4k is an n \u00d7 d matrix, let us write E = Ak \u2212 (AR)(V \u22a4k R) \u2020 V \u22a4k . Then, setting A = Ak +A\u03c1\u2212k, and using the triangle inequality we get\n\u2016E\u2016F \u2264 \u2225 \u2225 \u2225Ak \u2212AkR(V \u22a4k R) \u2020 V \u22a4k \u2225 \u2225 \u2225\nF +\n\u2225 \u2225 \u2225A\u03c1\u2212kR(V \u22a4 k R) \u2020 V \u22a4k \u2225 \u2225 \u2225\nF .\nThe first statement of Lemma 2 implies that rank(V \u22a4k R) = k thus (V \u22a4 k R)(V \u22a4 k R) \u2020 = Ik, where Ik is the k\u00d7k identity matrix. Replacing Ak = Uk\u03a3kV \u22a4k and setting (V \u22a4 k R)(V \u22a4 k R) \u2020 = Ik we get that\n\u2225 \u2225 \u2225Ak \u2212AkR(V \u22a4k R) \u2020 V \u22a4k \u2225 \u2225 \u2225\nF =\n\u2225 \u2225 \u2225Ak \u2212 Uk\u03a3kV \u22a4k R(V \u22a4k R) \u2020 V \u22a4k \u2225 \u2225 \u2225\nF =\n\u2225 \u2225Ak \u2212 Uk\u03a3kV \u22a4k \u2225 \u2225 F = 0.\nTo bound the second term above, we drop V \u22a4k , add and subtract the matrix A\u03c1\u2212kR(V \u22a4 k R) \u22a4V \u22a4k , and use the triangle inequality and submultiplicativity:\n\u2225 \u2225 \u2225A\u03c1\u2212kR(V \u22a4 k R) \u2020 V \u22a4k \u2225 \u2225 \u2225\nF \u2264\n\u2225 \u2225A\u03c1\u2212kR(V \u22a4 k R) \u22a4 \u2225 \u2225 F + \u2225 \u2225 \u2225A\u03c1\u2212kR((V \u22a4 k R) \u2020 \u2212 (V \u22a4k R)\u22a4) \u2225 \u2225 \u2225\nF\n\u2264 \u2225 \u2225A\u03c1\u2212kRR \u22a4Vk \u2225 \u2225\nF + \u2016A\u03c1\u2212kR\u2016F \u2225 \u2225 \u2225(V \u22a4k R) \u2020 \u2212 (V \u22a4k R)\u22a4 \u2225 \u2225 \u2225 2 .\nNow we will bound each term individually. A crucial observation for bounding the first term is that A\u03c1\u2212kVk = U\u03c1\u2212k\u03a3\u03c1\u2212kV \u22a4 \u03c1\u2212kVk = 0 by orthogonality of the columns of Vk and V\u03c1\u2212k. This term now can be bounded using the second statement of Lemma 2 with S = A\u03c1\u2212k and T = Vk. This statement, assuming c sufficiently large, and an application of Markov\u2019s inequality on the random variable \u2225 \u2225A\u03c1\u2212kRR \u22a4Vk \u2212A\u03c1\u2212kVk \u2225 \u2225\nF give that w.p. at least 0.99,\n\u2225 \u2225A\u03c1\u2212kRR \u22a4Vk \u2225 \u2225\nF \u2264 0.5\u03b5 \u2016A\u03c1\u2212k\u2016F . (8)\nThe second two terms can be bounded using Lemma 3 and Lemma 4 on C = A\u03c1\u2212k. Hence by applying a union bound on Lemma 3, Lemma 4 and Inq. (8), we get that w.p. at least 0.97,\n\u2016E\u2016F \u2264 \u2225 \u2225A\u03c1\u2212kRR \u22a4Vk \u2225 \u2225 F + \u2016A\u03c1\u2212kR\u2016F \u2225 \u2225 \u2225(V \u22a4k R) \u2020 \u2212 (V \u22a4k R)\u22a4 \u2225 \u2225 \u2225 2\n\u2264 0.5\u03b5 \u2016A\u03c1\u2212k\u2016F + \u221a\n(1 + \u03b5) \u2016A\u03c1\u2212k\u2016F \u00b7 3\u03b5 \u2264 0.5\u03b5 \u2016A\u03c1\u2212k\u2016F + 3.5\u03b5 \u2016A\u03c1\u2212k\u2016F = 4\u03b5 \u00b7 \u2016A\u03c1\u2212k\u2016F .\nThe last inequality holds thanks to our choice of \u03b5 \u2208 (0, 1/3). Proposition 6. A well-known property connects the SVD of a matrix and k-means clustering. Recall Definition 1, and notice that XoptX\u22a4optA is a matrix of rank at most k. From the SVD optimality we immediately get that\n\u2016A\u03c1\u2212k\u20162F = \u2016A\u2212Ak\u2016 2 F \u2264 \u2225 \u2225A\u2212XoptX\u22a4optA \u2225 \u2225 2 F . (9)\n4.1 The proof of Eqn. (4) of Theorem 1\nWe start by manipulating the term \u2225 \u2225A\u2212X\u03b3\u0303X\u22a4\u03b3\u0303 A \u2225 \u2225 2 F in Eqn. (4). Replacing A by Ak + A\u03c1\u2212k , and using the Pythagorean theorem (the subspaces spanned by the components Ak \u2212 X\u03b3\u0303X\u22a4\u03b3\u0303 Ak and A\u03c1\u2212k \u2212 X\u03b3\u0303X\u22a4\u03b3\u0303 A\u03c1\u2212k are perpendicular) we get\n\u2225 \u2225A\u2212X\u03b3\u0303X\u22a4\u03b3\u0303 A \u2225 \u2225 2\nF =\n\u2225 \u2225(I \u2212X\u03b3\u0303X\u22a4\u03b3\u0303 )Ak \u2225 \u2225 2 F \ufe38 \ufe37\ufe37 \ufe38\n\u03b821\n+ \u2225 \u2225(I \u2212X\u03b3\u0303X\u22a4\u03b3\u0303 )A\u03c1\u2212k \u2225 \u2225 2\nF \ufe38 \ufe37\ufe37 \ufe38\n\u03b822\n. (10)\nWe first bound the second term of Eqn. (10). Since I \u2212 X\u03b3\u0303X\u22a4\u03b3\u0303 is a projector matrix, it can be dropped without increasing a unitarily invariant norm. Now Proposition 6 implies that\n\u03b822 \u2264 \u2016A\u03c1\u2212k\u2016 2 F \u2264 \u2225 \u2225A\u2212XoptX\u22a4optA \u2225 \u2225 2 F . (11)\nWe now bound the first term of Eqn. (10):\n\u03b81 \u2264 \u2225 \u2225 \u2225(I \u2212X\u03b3\u0303X\u22a4\u03b3\u0303 )AR(VkR)\u2020V \u22a4k \u2225 \u2225 \u2225\nF + \u2016E\u2016F (12)\n\u2264 \u2225 \u2225(I \u2212X\u03b3\u0303X\u22a4\u03b3\u0303 )AR \u2225 \u2225\nF\n\u2225 \u2225 \u2225(VkR) \u2020 \u2225 \u2225 \u2225 2 + \u2016E\u2016F (13)\n\u2264 \u221a\u03b3 \u2225 \u2225(I \u2212XoptX\u22a4opt)AR \u2225 \u2225\nF\n\u2225 \u2225 \u2225(VkR) \u2020 \u2225 \u2225 \u2225 2 + \u2016E\u2016F (14)\n\u2264 \u221a\u03b3 \u221a (1 + \u03b5) \u2225 \u2225(I \u2212XoptX\u22a4opt)A \u2225 \u2225\nF\n1 1\u2212 \u03b5 + 4\u03b5 \u2225 \u2225(I \u2212XoptX\u22a4opt)A \u2225 \u2225 F (15)\n\u2264 \u221a\u03b3(1 + 2.5\u03b5) \u2225 \u2225(I \u2212XoptX\u22a4opt)A \u2225 \u2225 F + \u221a \u03b3 4\u03b5 \u2225 \u2225(I \u2212XoptX\u22a4opt)A \u2225 \u2225 F (16) \u2264 \u221a\u03b3(1 + 6.5\u03b5) \u2225 \u2225(I \u2212XoptX\u22a4opt)A \u2225 \u2225 F (17)\nIn Eqn. (12) we used Lemma 5, the triangle inequality, and the fact that I \u2212 X\u0303\u03b3X\u0303\u22a4\u03b3 is a projector matrix and can be dropped without increasing a unitarily invariant norm. In Eqn. (13) we used submultiplicativity (see Section 2.2) and the fact that V \u22a4k can be dropped without changing the spectral norm. In Eqn. (14) we replaced X\u03b3\u0303 by Xopt and the factor \u221a \u03b3 appeared in the first term. To better understand this step, notice that X\u03b3\u0303 gives a \u03b3-approximation to the optimal k-means clustering of the matrix AR, and any other n \u00d7 k indicator matrix (for example, the matrix Xopt) satisfies\n\u2225 \u2225 ( I \u2212X\u03b3\u0303X\u22a4\u03b3\u0303 ) AR \u2225 \u2225 2 F \u2264 \u03b3 min\nX\u2208X\n\u2225 \u2225(I \u2212XX\u22a4)AR \u2225 \u2225 2\nF \u2264 \u03b3 \u2225 \u2225 ( I \u2212XoptX\u22a4opt ) AR \u2225 \u2225 2 F .\nIn Eqn. (15) we used Lemma 4 with C = (I \u2212 XoptX\u22a4opt)A, Lemma 3 and Proposition 6. In Eqn. (16) we used the fact that \u03b3 \u2265 1 and that for any \u03b5 \u2208 (0, 1/3) it is ( \u221a 1 + \u03b5)/(1\u2212 \u03b5) \u2264 1 + 2.5\u03b5. Taking squares in Eqn. (17) we get\n\u03b821 \u2264 \u03b3(1 + 28\u03b5) \u2225 \u2225(I \u2212XoptX\u22a4opt)A \u2225 \u2225 2\nF .\nFinally, rescaling \u03b5 accordingly and applying the union bound on Lemma 5 and Definition 2 concludes the proof."}, {"heading": "5 Experiments", "text": "This section describes an empirical evaluation of Algorithm 1 on a face images collection. We implemented our algorithm in MatLab and compared it against other prominent dimensionality reduction techniques such as the Local Linear Embedding (LLE) algorithm and the Laplacian scores for feature selection. We ran all the experiments on a Mac machine with a dual core 2.26 Ghz processor and 4 GB of RAM. Our empirical findings are very promising indicating that our algorithm and implementation could be very useful in real applications involving clustering of large-scale data."}, {"heading": "5.1 An application of Algorithm 1 on a face images collection", "text": "We experiment with a face images collection. We downloaded the images corresponding to the ORL database from [21]. This collection contains 400 face images of dimensions 64 \u00d7 64 corresponding to 40 different people. These images form 40 groups each one containing exactly 10 different images of the same person. After vectorizing each 2-D image and putting it as a row vector in an appropriate matrix, one can construct a 400 \u00d7 4096 image-by-pixel matrix A. In this matrix, objects are the face images of the ORL collection while features are the pixel values of the images. To apply the Lloyd\u2019s heuristic on A, we employ MatLab\u2019s function kmeans with the parameter determining the maximum number of repetitions setting to 30. We also chose a deterministic initialization of the Lloyd\u2019s iterative E-M procedure, i.e. whenever we call kmeans with inputs a matrix A\u0303 \u2208 R400\u00d7d\u0303, with d\u0303 \u2265 1, and the integer k = 40, we initialize the cluster centers with the 1-st, 11-th,..., 391-th rows of A\u0303, respectively. Note that this initialization corresponds to picking images from the forty different groups of the available collection, since the images of every group are stored sequentially in A. We evaluate the clustering outcome from two different perspectives. First, we measure and report the objective function F of the k-means clustering problem. In particular, we report a normalized version of F , i.e. F\u0303 = F/||A||2F . Second, we report the mis-classification accuracy of the clustering result. We denote this number by P (0 \u2264 P \u2264 1), where P = 0.9, for example, implies that 90% of the objects were assigned to the correct cluster after the application of the clustering algorithm. In the sequel, we first perform experiments by running Algorithm 1 with everything fixed but t, which denotes the dimensionality of the projected data. Then, for four representative values of t, we compare Algorithm 1 with three other dimensionality reduction methods as well with the approach of running the Lloyd\u2019s heuristic on the original high dimensional data.\nWe run Algorithm 1 with t = 5, 10, ..., 300 and k = 40 on the matrix A described above. Figure 1 depicts the results of our experiments. A few interesting observations are immediate. First, the normalized objective function F\u0303 is a piece-wise non-increasing function of the number of dimensions t. The decrease in F\u0303 is large in the first few choices\nof t; then, increasing the number of dimensions t of the projected data decreases F\u0303 by a smaller value. The increase of t seems to become irrelevant after around t = 90 dimensions. Second, the mis-classification rate P is a piece-wise non-decreasing function of t. The increase of t seems to become irrelevant again after around t = 90 dimensions. Another interesting observation of these two plots is that the mis-classification rate is not directly relevant to the objective function F . Notice, for example, that the two have different behavior from t = 20 to t = 25 dimensions. Finally, we report the running time T of the algorithm which includes only the clustering step. Notice that the increase in the running time is - almost - linear with the increase of t. The non-linearities in the plot are due to the fact that the number of iterations that are necessary to guarantee convergence of the Lloyd\u2019s method are different for different values of t. This observation indicates that small values of t result to significant computational savings, especially when n is large. Compare, for example, the one second running time that is needed to solve the k-means problem when t = 275 against the 10 seconds that are necessary to solve the problem on the high dimensional data. To our benefit, in this case, the multiplication AR takes only 0.1 seconds resulting to a total running time of 1.1 seconds which corresponds to an almost 90% speedup of the overall procedure.\nWe now compare our algorithm against other dimensionality reduction techniques. In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab\u2019s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation. The results of our experiments on A, k = 40 and t = 10, 20, 50, 100 are shown in Table 2. In terms of computational complexity, for example t = 50, the time (in seconds) needed for all five methods (only the dimension reduction step) are TSVD = 5.9, TLLE = 4.4, TLS = 0.32, THD = 0, and TRP = 0.03. Notice that our algorithm is much faster than the other approaches while achieving worse (t = 10, 20), slightly worse (t = 50) or slightly better (t = 100) approximation accuracy results."}, {"heading": "5.2 A note on the mailman algorithm for matrix-matrix and matrix-vector multiplication", "text": "In this section, we compare three different implementations of the third step of Algorithm 1. As we already discussed in Section 3.1, the mailman algorithm is asymptotically faster than naively multiplying the two matrices A and R. In this section we want to understand whether this asymptotic behavior of the mailman algorithm is indeed achieved in a practical implementation. We compare three different approaches for the implementation of the third step of our algorithm: the first is MatLab\u2019s function times(A,R) (MM1); the second exploits the fact that we do not need to explicitly store the whole matrix R, and that the computation can be performed on the fly (column-by-column) (MM2); the last is the mailman algorithm [15] (see Section 3.1 for more details). We implemented the last two algorithms in C using MatLab\u2019s MEX technology. We observed that when A is a vector (n = 1), then the mailman algorithm is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15]. Moreover, it\u2019s worth-noting that (MM2) is also superior compared to (MM1). On the other hand, our best implementation of the mailman algorithm for matrix-matrix operations is inferior to both (MM1) and (MM2) for any 10 \u2264 n \u2264 10, 000. Based on these findings, we chose to use (MM1) for our experimental evaluations.\nAcknowledgments: Christos Boutsidis was supported by NSF CCF 0916415 and a Gerondelis Foundation Fellowship; Petros Drineas was partially supported by an NSF CAREER Award and NSF CCF 0916415.\n2In particular, we run W = constructW (A); Scores = LaplacianScore(A,W );"}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "author": ["N. Ailon", "B. Chazelle"], "venue": "In ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "NP-hardness of Euclidean sum-of-squares clustering", "author": ["D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "In ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Unsupervised feature selection for the k-means clustering problem", "author": ["C. Boutsidis", "M.W. Mahoney", "P. Drineas"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Clustering in large graphs and matrices", "author": ["P. Drineas", "A. Frieze", "R. Kannan", "S. Vempala", "V. Vinay"], "venue": "In ACM- SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "An optimal set of discriminant vectors", "author": ["D. Foley", "J. Sammon"], "venue": "IEEE Transactions on Computers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1975}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Result analysis of the NIPS 2003 feature selection challenge", "author": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In ACM Symposium on Theory of Computing (STOC),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1984}, {"title": "A simple linear time (1+\u03b5)-approximation algorithm for k-means clustering in any dimensions", "author": ["A. Kumar", "Y. Sabharwal", "S. Sen"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "The Mailman algorithm: A note on matrix-vector multiplication", "author": ["E. Liberty", "S. Zucker"], "venue": "Information Processing Letters,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Least squares quantization in PCM", "author": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1982}, {"title": "The effectiveness of Lloyd-type methods for the k-means problem", "author": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290:5500,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarlos"], "venue": "In IEEE Symposium on Foundations of Computer Science (FOCS),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu"], "venue": "Knowledge and Information Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [{"referenceID": 14, "context": "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].", "startOffset": 33, "endOffset": 37}, {"referenceID": 18, "context": "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].", "startOffset": 126, "endOffset": 130}, {"referenceID": 11, "context": "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].", "startOffset": 92, "endOffset": 96}, {"referenceID": 1, "context": "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 8, "context": "We believe that the high dimensionality of modern data will render our algorithm useful and attractive in many practical applications [9].", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "Description Dimensions Time Accuracy 1999 [6] SVD - feature extraction k O(ndmin{n, d}) 2 - Folklore RP - feature extraction \u03a9(log(n)/\u03b5) O(nd\u2308\u03b5\u22122 log(n)/ log(d)\u2309) 1 + \u03b5 2009 [5] SVD - feature selection \u03a9(k log(k/\u03b5)/\u03b5) O(ndmin{n, d}) 2 + \u03b5 2010 This paper RP - feature extraction \u03a9(k/\u03b5) O(nd\u2308\u03b5\u22122k/ log(d)\u2309) 2 + \u03b5", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Description Dimensions Time Accuracy 1999 [6] SVD - feature extraction k O(ndmin{n, d}) 2 - Folklore RP - feature extraction \u03a9(log(n)/\u03b5) O(nd\u2308\u03b5\u22122 log(n)/ log(d)\u2309) 1 + \u03b5 2009 [5] SVD - feature selection \u03a9(k log(k/\u03b5)/\u03b5) O(ndmin{n, d}) 2 + \u03b5 2010 This paper RP - feature extraction \u03a9(k/\u03b5) O(nd\u2308\u03b5\u22122k/ log(d)\u2309) 2 + \u03b5", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).", "startOffset": 180, "endOffset": 187}, {"referenceID": 2, "context": "Finding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation algorithms for k-means clustering.", "startOffset": 50, "endOffset": 53}, {"referenceID": 12, "context": "For our discussion, we fix the \u03b3-approximation algorithm to be the one presented in [14], which guarantees \u03b3 = 1+ \u03b5 for any \u03b5 \u2208 (0, 1] with running time O(2(k/\u03b5\u2032)O(1)dn).", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "A classical result of Johnson and Lindenstrauss states that any n-point set in d dimensions - rows in a matrix A \u2208 R - can be linearly projected into t = \u03a9(log(n)/\u03b5) dimensions while preserving pairwise distances within a factor of 1\u00b1\u03b5 using a random orthonormal matrix [12].", "startOffset": 270, "endOffset": 274}, {"referenceID": 10, "context": "Gaussian random variables with zero mean and variance 1/ \u221a t [11].", "startOffset": 61, "endOffset": 65}, {"referenceID": 0, "context": "Achlioptas proved that even a (rescaled) random sign matrix suffices in order to get the same guarantees as above [1], an approach that we adopt here (see step two in Algorithm 1).", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "If R is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "Using, for example, the algorithm of [14] with \u03b3 = 1 + \u03b5 would result in an algorithm that preserves the clustering within a factor of 2 + \u03b5, for any \u03b5 \u2208 (0, 1/3), running in time O(nd\u2308\u03b5\u22122k/ log(d)\u2309 + 2(k/\u03b5)O(1)kn/\u03b52).", "startOffset": 37, "endOffset": 41}, {"referenceID": 14, "context": "In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well.", "startOffset": 40, "endOffset": 48}, {"referenceID": 15, "context": "In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well.", "startOffset": 40, "endOffset": 48}, {"referenceID": 17, "context": "The proof of Theorem 1 employs several results from [19] including Lemma 6, 8 and Corollary 11.", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].", "startOffset": 5, "endOffset": 8}, {"referenceID": 17, "context": "1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab\u2019s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation.", "startOffset": 302, "endOffset": 306}, {"referenceID": 9, "context": "In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab\u2019s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation.", "startOffset": 478, "endOffset": 482}, {"referenceID": 13, "context": "We compare three different approaches for the implementation of the third step of our algorithm: the first is MatLab\u2019s function times(A,R) (MM1); the second exploits the fact that we do not need to explicitly store the whole matrix R, and that the computation can be performed on the fly (column-by-column) (MM2); the last is the mailman algorithm [15] (see Section 3.", "startOffset": 348, "endOffset": 352}, {"referenceID": 13, "context": "We observed that when A is a vector (n = 1), then the mailman algorithm is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15].", "startOffset": 165, "endOffset": 169}], "year": 2013, "abstractText": "This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A \u2208 R) can be projected into t = \u03a9(k/\u03b5) dimensions, for any \u03b5 \u2208 (0, 1/3), in O(nd\u2308\u03b5\u22122k/ log(d)\u2309) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + \u03b5. The projection is done by post-multiplying A with a d \u00d7 t random matrix R having entries +1/ \u221a t or \u22121/ \u221a t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.", "creator": "LaTeX with hyperref package"}}}