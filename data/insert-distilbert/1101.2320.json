{"id": "1101.2320", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2011", "title": "Review and Evaluation of Feature Selection Algorithms in Synthetic Problems", "abstract": "the main purpose of feature subset selection is to find a reduced subset of attributes from a data set described by a feature set. the task of a feature selection algorithm ( fsa ) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure. in this paper incorporating several thoroughly fundamental algorithms together are studied to assess their performance in a controlled experimental scenario. a measure design to evaluate fsas is devised that computes the degree of matching between the output given previously by a fsa and the known optimal solutions. an extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms or in terms dimensions of solution accuracy and size as a function of the relevance, irrelevance, redundancy utilization and size of the data sequence samples. the controlled experimental conditions facilitate the derivation of several better - supported and meaningful conclusions.", "histories": [["v1", "Wed, 12 Jan 2011 10:49:51 GMT  (238kb)", "http://arxiv.org/abs/1101.2320v1", "13 pages, 3 figures"]], "COMMENTS": "13 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["l a belanche", "f f gonz\\'alez"], "accepted": false, "id": "1101.2320"}, "pdf": {"name": "1101.2320.pdf", "metadata": {"source": "CRF", "title": "Feature Selection Algorithms in Synthetic Problems", "authors": ["L.A. Belanche"], "emails": ["belanche@lsi.upc.edu", "fgonzalez@lsi.upc.edu"], "sections": [{"heading": null, "text": "ar X\niv :1\n10 1.\n23 20\nv1 [\ncs .A\nKeywords: Feature Selection Algorithms; Empirical Evaluations; Attribute relevance and redundancy."}, {"heading": "1 INTRODUCTION", "text": "The feature selection problem is ubiquitous in an inductive machine learning or data mining setting and its importance is beyond doubt. The main benefit of a correct selection is the improvement of the inductive learner, either in terms of learning speed, generalization capacity or simplicity of the induced model. On the other hand, there are the scientific benefits associated with a smaller number of features: a reduced measurement cost and hopefully a better understanding of the domain. A feature selection algorithm (FSA) is a computational solution that should be guided by a certain definition of subset relevance, although in many cases this definition is implicit or followed in a loose sense. This is so because, from the inductive learning perspective, the relevance of a feature may have several definitions depending on the precise objective that is looked for (Caruana and Freitag, 1994). Thus the need arises to count on common sense criteria that enables to adequately decide which algorithm to use (or not to use) in certain situations.\nThis work reviews the merits of several fundamental fea-\nture subset selection algorithms in the literature and assesses their performance in an artificial controlled experimental scenario. A scoring measure computes the degree of matching between the output given by the algorithm and the known optimal solution. This measure ranks the algorithms by taking into account the amount of relevance, irrelevance and redundancy on synthetic data sets of discrete features. Sample size effects are also studied.\nThe results illustrate the strong dependence on the particular conditions of the algorithm used, as well as on the amount of irrelevance and redundancy in the data set description, relative to the total number of features. This should prevent the use of a single algorithm specially when there is poor knowledge available about the structure of the solution. More importantly, it points in the direction of using principled combinations of algorithms for a more reliable assessment of feature subset performance. The paper is organized as follows: we begin in Section 2 reviewing relevant related work. In section 3 we set a precise definition of the feature selection problem and briefly survey the main categorization of feature selection algorithms. We then provide an algorithmic description and\ncomment on several of the most widespread algorithms in section 4. The methodology and tools used for the empirical evaluation are covered in section 5. The experimental study, its results and a general advice to the data mining practitioner are developed in section 6. The paper ends with the conclusions and prospects for future work."}, {"heading": "2 MOTIVATION AND RELATED WORK", "text": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b). Some of these studies use artificially generated data sets, like the widespread Parity, Led or Monks problems (Thrun, 1991). Demonstrating improvement on synthetic data sets can be more convincing that doing so in typical scenarios where the true solution is completely unknown. However, there is a consistent lack of systematical experimental work using a common benchmark suite and equal experimental conditions. This hinders a wider exploitation of the power inherent in fully controlled experimental environments: the knowledge of the (set of) optimal solution(s), the possibility of injecting a desired amount of relevance, irrelevance and redundancy and the unlimited availability of data. Another important issue is the way FSA performance is assessed. This is normally done by handing over the solution encountered by the FSA to a specific inducer (during of after the feature selection process takes place). Leaving aside the dependence on the particular inducer chosen, there is a much more critical aspect, namely, the relation between the performance as reported by the inducer and the true merits of the subset being evaluated. In this sense, it is our hypothesis that FSAs are very affected by finite sample sizes, which distort reliable assessments of subset relevance, even in the presence of a very sophisticated search algorithm (Reunanen, 2003). Therefore, sample size should also be a matter of study in a through experimental comparison. This problem is aggravated when using filter measures, since in this case the relation to true generalization ability (as expressed by the Bayes error) can be very loose (Ben-Bassat, 1982). A further problem with traditional benchmarking data sets is the implicit assumption that the used data sets are actually amenable to feature selection. By this it is meant that performance benefits clearly from a good selection process (and less clearly or even worsens with a bad one). This criterion is not commonly found in similar experimental work. In summary, the rationale for using exclusively synthetic data sets is twofold:\n1. Controlled studies can be developed by systematically varying chosen experimental conditions, thus facilitating the derivation of more meaningful conclusions.\n2. Synthetic data sets allow full control of the experimental conditions, in terms of amount of relevance,\nirrelevance and redundancy, as well as sample size and problem difficulty. An added advantage is the knowledge of the set of optimal solutions, in which case the degree of closeness to any of these solutions can thus be assessed in a confident and automated way.\nThe procedure followed in this work consists in generating sample data sets from synthetic functions of a number of discrete relevant features. These sample data sets are then corrupted with irrelevant and/or redundant features and handed over to different FSAs to obtained a hypothesis. A scoring measure is used in order to compute the degree of matching between this hypothesis and the known optimal solution. The score takes into account the amount of relevance, irrelevance and redundancy in each suboptimal solution as yielded by an algorithm.\nThe main criticism associated with the use of artificial data is the likelihood that such a problem be found in realworld scenarios. In our opinion this issue is more than compensated by the mentioned advantages. A FSA that is not able to work properly in simple experimental conditions (like those developed in this work) is in strong suspect of being inadequate in general."}, {"heading": "3 THE FEATURE SELECTION PROBLEM", "text": "LetX be the original set of features, with cardinality |X | = n. The continuous feature selection problem (also called Feature Weighing) refers to the assignment of weights wi to each feature xi \u2208 X in such a way that the order corresponding to its theoretical relevance is preserved. The binary feature selection problem (also called Feature Subset Selection) refers to the choice of a subset of features that jointly maximize a certain measure related to subset relevance. This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution. Although both types can be seen in an unified way (the latter case corresponds to the assignment of weights in {0, 1}), these are quite different problems that reflect different design objectives. In the continuous case, one is interested in keeping all the features but in using them differentially in the learning process. On the contrary, in the binary case one is interested in keeping just a subset of the features and (most likely) using them equally in the learning process.\nA common instance of the feature selection problem can be formally stated as follows. Let J be a performance evaluation measure to be optimized (say to maximize) defined as J : P(X) \u2192 R+ \u222a {0}. This function accounts for a general evaluation measure, that may or may not be inspired in a precise and previous definition of relevance. Let c(x) \u2265 0 represent the cost of variable x (measurement cost, needed technical skill, etc) and call c(X \u2032) = \u2211\nx\u2208X\u2032 c(x),\nfor X \u2032 \u2208 P(X). Let CX = c(X) be the cost of the whole feature set. It is assumed here that c is additive,\nthat is, c(X \u2032 \u222a X \u2032\u2032) = c(X \u2032) + c(X \u2032\u2032) (together with nonnegativeness, this implies that c is monotone).\nDefinition 1 (Feature Subset Selection) The selection of an optimal feature subset (\u201cFeature Subset Selection\u201d) is either of two scenarios:\n(1) Fix C0 \u2264 CX . Find the X \u2032 \u2208 P(X) of maximum J(X \u2032) among those that fulfill c(X \u2032) \u2264 C0.\n(2) Fix J0 \u2265 0. Find the X \u2032 \u2208 P(X) of minimum c(X \u2032) among those that fulfill J(X \u2032) \u2265 J0.\nIf the costs are unknown, a meaningful choice is obtained by setting c(x) = 1 for all x \u2208 X . Then c(X \u2032) = |X \u2032| and c can be interpreted as the complexity of the solution. In this case, (1) amounts to finding the subset with highest J among those having a maximum pre-specified size. In scenario (2), it amounts to finding the smallest subset among those having a minimum pre-specified performance (as measured by J). Only with these restrictions, an optimal subset of features need not exist; if it does, is not necessarily unique. In scenario (1), a solution always exists by defining c(\u2205) = 0 and J(\u2205) to be the value of J with no features. In case (2), if there is no solution, an adequate policy may be to set J0 = \u01ebJ(X), \u01eb > 0, and progressively lower the value of \u01eb. If there is more than one solution (of equal performance and cost, by definition) one is usually interested in them all. We shall speak of a FSA of Type 1 (resp. Type 2) when it has been designed to solve the first (resp. second) scenario in Def. 1. If the FSA can be used in both scenarios, we shall speak of a general-type algorithm. In addition, if one has no control whatsoever, we shall speak of a free-type algorithm. We shall use the notation S(X \u2032) to indicate the subsample of S described by the features in X \u2032 \u2286 X only."}, {"heading": "4 FEATURE SUBSET SELECTION ALGORITHMS", "text": "The relationship between a FSA and the inductive learning method used to infer a model can take three main forms: filter, wrapper or embedded, which we call the mode:\nEmbedded Mode: The inducer has its own FSA (either explicit or implicit). The methods to induce logical conjunctions (Vere, 1975; Winston, 1975), decision trees or artificial neural networks are examples of this embedding.\nFilter Mode: If feature selection takes place before the induction step, the former can be seen as a filter (of nonuseful features). In a general sense it can be seen as a particular case of the embedded mode in which feature selection is used as a pre-processing. The filter mode is then independent of the inducer that evaluates the model after the feature selection process.\nWrapper Mode: Here the relationship is taken the other way around: the FSA uses the learning algorithm as a subroutine (John et al., 1994). The argument in favor of this\nmode is to equal the bias of the FSA and the inducer that will be used later to assess the goodness of the model. A main disadvantage is the computational burden that comes from calling the inducer to evaluate each and every subset of considered features. In what follows several of the currently most widespread FSAs in machine learning are described and briefly commented on. General-purpose search algorithms, as genetic algorithms, are excluded from the review. None of the algorithms allow the specification of costs in the features. Most of them can work in filter or wrapper mode. The feature weighing algorithmRelief has been included, both in the review and in the experimental comparison, as a complement. This is so because it can also be used to select a subset of features, although a way of getting a subset out of the weights has to be devised. In the following we assume again that the evaluation measure J is to be maximized."}, {"heading": "4.1 Algorithm LVF", "text": "Lvf (Las Vegas Filter) (Liu and Setiono, 1998a) is a type 2 algorithm that repeatedly generates random subsets and computes the consistency of the sample: an inconsistency in X \u2032 and S is defined as two instances in S that are equal when considering only the features inX \u2032 and that belong to different classes. The aim is to find the minimum subset of features leading to zero inconsistencies. The inconsistency count of an instance A \u2208 S is defined as:\nICX\u2032(A) = X \u2032(A) \u2212max k X \u2032k(A) (1)\nwhere X \u2032(A) is the number of instances in S equal to A using only the features in X \u2032 and X \u2032k(A) is the number of instances in S of class k equal to A using only the features in X \u2032 (Liu and Setiono, 1998b). The inconsistency rate of a feature subset in a sample S is then:\nIR(X \u2032) =\n\u2211\nA\u2208S ICX\u2032(A)\n|S| (2)\nThis is a monotonic measure, in the sense\nX1 \u2282 X2 \u21d2 IR(X1) \u2265 IR(X2)\nThe evaluation measure is then J(X \u2032) = 1 IR(X\u2032)+1 \u2208 (0, 1] that can be evaluated in O(|S|) time using a hash table. Lvf is described as Algorithm 1. It has been found to be particularly efficient for data sets having redundant features (Dash et al., 1997). Arguably its main advantage may be that it quickly reduces the number of features in the initial stages with certain confidence (Dash and Liu, 1998; Dash et al., 2000); however, many poor solution subsets are analyzed, wasting computing resources."}, {"heading": "4.2 Algorithm LVI", "text": "Lvi (Las Vegas Incremental) is also a type 2 algorithm and an evolution of Lvi. It is based on the grounds that it is not necessary to use the whole sample S in order to\nInput : max \u2212 the maximum number o f i t e r a t i o n s J \u2212 eva luat i on measure S(X) \u2212 a sample S descr ibed by X , |X| = n Output : L \u2212 a l l equ i va l en t s o l u t i o n s found\nL : = [ ] // L s t o r e s equa l l y good s e t s Best := X // I n i t i a l i z e bes t s o l u t i o n J0 := J(S(X)) // minimum al lowed value o f J r epeat max t imes\nX\u2032 := Random SubSet (Best) i f J(S(X\u2032)) \u2265 J0 then\ni f |X\u2032| < |Best| then Best := X\u2032\nL : = [X\u2032 ] // L i s r e i n i t i a l i z e d e l s e\ni f |X\u2032| = |Best| then L := append (L,X\u2032 )\nend end\nend end\nAlgorithm 1: Lvf (Las Vegas Filter).\nevaluate the measure J , which for this algorithm is again consistency (Liu and Setiono, 1998b). The algorithm is described as Algorithm 2. It departs from a portion S0 of S; if Lvf finds a sufficiently good solution in S0 then Lvi halts. Otherwise the set of instances in S \\ S0 making S1 inconsistent is added to S0, this new portion is handed over to Lvf and the process is iterated. Intuitively, the portion cannot be too small or too big. If it is too small, after the first iteration many inconsistencies will be found and added to the current portion, which will hence be very similar to S. If it is too big, the computational savings will be modest. The authors suggest p = 10% or a value proportional to the number of features. In Liu and Motoda (1998) it is reported experimentally that Lvi adequately chooses relevant features, but may fail for noisy data sets, in which case the algorithm it is shown to consider irrelevant features. Probably Lvi is more sensible to noise than Lvf in cases of small sample sizes."}, {"heading": "4.3 Algorithm RELIEF", "text": "Relief (Kira and Rendell, 1992) is a general-type algorithm that works exclusively in filter mode. The algorithm randomly chooses an instance I \u2208 S and finds its near hit and its near miss. The former is the closest instance to I among all the instances in the same class of I. The latter is the closest instance to I among all the instances in a different class. The underlying idea is that a feature is more relevant to I the more it separates I and its near miss, and the least it separates I and its near hit. The result is a weighed version of the original feature set. The algorithm for two classes is described as Algorithm 3.\nWhen costs are just sizes, the algorithm can be used to simulate a type 1 scenario by iteratively checking the se-\nInput : max \u2212 the maximum number o f i t e r a t i o n s J \u2212 eva luat i on measure S(X) \u2212 a sample S descr ibed by X, |X| = n p \u2212 i n i t i a l per centage Output : X\u2032 \u2212 s o l u t i o n found\nS0 := por t i on (S, p ) / / I n i t i a l por t i on S1 := S \\ S0 J0 := J(S(X)) // Minimum al lowed value o f J r epeat f o r e v e r\nX\u2032 := LVF (max, J, S0(X)) i f J(S1(X\u2032)) \u2265 J0 then stop e l s e\nC := {x \u2208 S1(X\u2032) making S1(X\u2032) i n c on s i s t e n t } S0 := S0 \u222a C S1 := S1 \\ C\nend end\nAlgorithm 2: Lvi (Las Vegas Incremental).\nInput : p \u2212 sampling per centage d \u2212 d i s t ance measure S(X) \u2212 a sample S descr ibed by X, |X| = n Output : w \u2212 array o f f e a tu r e weights\nl e t m := p|S| i n i t i a l i z e array w [ ] to zero do m times\nI := Random Instance (S) Inh := Near\u2212Hit (I, S) Inm := Near\u2212Miss (I, S) f o r each i \u2208 [1..n] do\nw[i] := w[i] + di(I, Inm)/m\u2212 di(I, Inh)/m end\nend\nAlgorithm 3: Relief.\nquence of the first C0 nested subsets in the order given by decreasing weights, calling the J measure, and returning that subset with the highest value of J . To simulate a type 2 scenario, the same sequence is checked looking for the first element in the sequence that yields a value of J not less than the chosen J0. The more important advantage of Relief is the rapid assessment of irrelevant features with a principled approach; however it does not make a good discrimination among redundant features. The algorithm has been found to choose correlated features instead of relevant features (Dash et al., 1997), and therefore the optimal subset can be far from assured (Kira and Rendell, 1992). Some variants have been proposed to account for several classes (Kononenko, 1994), where the k more similar instances are selected and their averages computed."}, {"heading": "4.4 Algorithms SFG/SBG", "text": "These two are classical general-type algorithms that may work in filter or wrapper mode. Sfg (Sequential Forward Generation) iteratively adds features to an initial subset, trying to improve a measure J , always taking into account those features already selected. Consequently, an ordered list can also be obtained. Sbg (Sequential Backward Generation) is the backward counterpart. They are jointly described as Algorithm 4. When the number of features is small, Doak (1992) reported that Sbg tends to show better performance than Sfg, most likely because Sbg evaluates the contribution of all features from the onset. In addition, Aha and Bankert (1995) points out that Sfg is preferable when the number of relevant features is (known to be) small; otherwise Sbg should be used. Interestingly, it was also reported that Sbg did not always have better performance than Sfg, contrary to the conclusions in Doak (1992). Besides, Sfg is faster in practice. The algorithms W-Sfg and W-Sbg (W for wrapper) use the accuracy of an inducer as evaluation measure.\nInput : S(X) \u2212 a sample S descr ibed by X, |X| = n J \u2212 eva luat i on measure Output : X\u2032 \u2212 s o l u t i o n found\nX\u2032 := \u2205 /\u2217 Forward \u2217/ or X\u2032 := X /\u2217 Backward \u2217/ r epeat x\u2032 := argmax{J(S(X\u2032 \u222a {x})) |x \u2208 X \\X\u2032} /\u2217 Forward \u2217/ x\u2032 := argmax{J(S(X\u2032 \\ {x})) |x \u2208 X\u2032} /\u2217 Backward \u2217/ X\u2032 := X\u2032 \u222a {x\u2032} /\u2217 Forward \u2217/ X\u2032 := X\u2032 \\ {x\u2032} /\u2217 Backward \u2217/ un t i l no improvement in J i n l a s t j s t ep s\nor X\u2032 = X /\u2217Forward \u2217/ or X\u2032 = \u2205 /\u2217Backward\u2217/\nAlgorithm 4: Sbg/Sfg (Sequential Backward/Forward Generation)."}, {"heading": "4.5 Algorithms SFFG/SFBG", "text": "These are free-type algorithms that may work in filter or wrapper mode. Sffg (Sequential Floating Forward Generation) (Pudil et al., 1994) is an exponential cost algorithm that operates in a sequential fashion, performing a forward step followed by a variable (and possibly null) number of backward ones. In essence, a feature is first unconditionally added and then features are removed as long as the generated subsets are the best among their respective size. The algorithm (described in Algorithm 5 as a flow-chart) is so-called because it has the characteristic of floating around a potentially good solution of the specified size. The backward counterpart Sfbg performs a backward step followed by zero or more forward steps. These two algorithms have been found to be very effective in some situations (Jain and Zongker, 1997), and are among the most popular nowadays. Their main drawbacks are the computational cost, that may be unaffordable when the number\nInput: S(X) - a sample S described by X, |X| = n J - evaluation measure d - desired size of the solution \u2206 - maximum deviation allowed with respect to d Output: solution of size d\u00b1\u2206\n!\n\" \"\n#\n$%& ! '\n\"\n\"(\n' %& ! !\n\" ) &*\n+#\n, ! %& ! \"\n!\n-\n.\n' ' ) ) */0\n\" !\n\"1\nAlgorithm 5: Sffg (Sequential Floating Forward Generation). The set Xk denotes the current solution (of size k); S(Xk) is the sample described by the features in Xk only.\nof features nears the hundred (Bins and Draper, 2001) and the need to fix the size of the final desired subset."}, {"heading": "4.6 Algorithm QBB", "text": "The Qbb (Quick Branch and Bound) algorithm (Dash and Liu, 1998) (described as Algorithm 7) is a type 1 algorithm. Actually it is a hybrid one, composed of Lvf and Abb. The origin of Abb is in Branch & Bound (Narendra and Fukunaga, 1977), an optimal search algorithm. Given a threshold \u03b2 (specified by the user), the search stops at each node the evaluation of which is lower than \u03b2, so that efferent branches are pruned. Abb (Automatic Branch & Bound) (Liu et al., 1998) is a variant having its bound as the inconsistency rate of the data when the full set of features is used (Algorithm 6). The basic idea of Qbb consists in using Lvf to find good starting points for Abb. It is expected that Abb can explore the remaining search space efficiently. The authors reported that Qbb is, in general, more efficient than Lvf or Abb in terms of average cost of execution and selected relevant features."}, {"heading": "5 EMPIRICAL EVALUATION OF FSAs", "text": "The main question arising in a feature selection experimental design is: what are the aspects that we would like to evaluate of a FSA solution in a given data set? Certainly a good algorithm is one that maintains a well-balanced tradeoff between small-sized and competitive solutions. To assess these two issues at the same time is a difficult un-\nInput : S(X) \u2212 a sample S descr ibed by X, |X| = n J \u2212 eva luat i on measure ( monotonic ) Output : L \u2212 a l l equ i va l en t s o l u t i o n s found\nprocedure ABB (S(X) : sample ; var L\u2032 : l i s t o f s e t ) f o r each x i n X do\nenqueue (Q,X \\ {x} ) / / remove a f ea tu r e at a time\nend whi le not empty (Q ) do\nX\u2032 := dequeue (Q) // X\u2032 i s l e g i t ima t e i f i t i s not a subset o f\na pruned s ta t e i f l e g i t ima t e (X\u2032 ) and J(S(X\u2032)) \u2265 J0 then\nL\u2032 := append (L\u2032,X\u2032 ) ABB(S(X\u2032), L\u2032 )\nend end\nend\nbegin Q := \u2205 // Queue o f pending s t a t e s L\u2032 := [X] // L i s t o f s o l u t i o n s J0 := J(S(X)) // Minimum al lowed value o f J ABB (S(X), L\u2032) // I n i t i a l c a l l to ABB k := sma l l e s t s i z e o f a subset in L\u2032\nL := s e t o f e l ements o f L\u2032 o f s i z e k end\nAlgorithm 6: Abb (Automatic Branch and Bound).\nInput : max \u2212 the maximum number o f i t e r a t i o n s J \u2212 monotonic eva luat i on measure S(X) \u2212 a sample S descr ibed by X, |X| = n Output : L \u2212 a l l equ i va l en t s o l u t i o n s found\nL ABB : = [ ] L LV F := LVF (max, J, S(X)) f o r each X\u2032 \u2208 L LV F do\nL ABB := concat (L ABB,ABB(S(X\u2032), J)) end k := sma l l e s t s i z e o f a subset in L ABB L := s e t o f e l ements o f L ABB o f s i z e k\nAlgorithm 7: Qbb (Quick Branch and Bound).\ndertaking in practice, given that their optimal relationship is user-dependent. In the present controlled experimental scenario, the task is greatly eased since the size and performance of the optimal solution is known in advance. The aim of the experiments is precisely to contrast the ability of the different FSAs to hit a solution with respect to relevance, irrelevance, redundancy and sample size.\nRelevance: Different families of problems are generated by varying the number of relevant features NR. These are features that will have an influence on the output and whose role can not be assumed by any other subset.\nIrrelevance: Irrelevant features are defined as those not\nhaving any influence on the output. Their values are generated at random for each example. For a problem with NR relevant features, different numbers of irrelevant features NI are added to the corresponding data sets (thus providing with several subproblems for each choice of NR).\nRedundancy: In this work, a redundancy exists when a feature can take the role of another. Following a parsimony principle, we are interested in the behaviour of the algorithms in front of this simplest case. If an algorithm fails to identify redundancy in this situation (something that is actually found in the experiments reported below), then this is interesting and something we should be aware of. This effect is obtained by choosing a relevant feature randomly and replicating it in the data set. For a problem with NR relevant features, different numbers of redundant features NR\u2032 are added in a way analogous to the generation of irrelevant features.\nSample Size: number of instances |S| of a data sample S. In these experiments, |S| = \u03b1kNT c, where \u03b1 is a constant, k is a multiplying factor, NT is the total number of features (NR + NI + NR\u2032) and c is the number of classes of the problem. This means that the sample size will depend linearly on the total number of features."}, {"heading": "5.1 Evaluation of performance", "text": "We derive in this section a scoring measure to capture the degree to which a solution obtained by a FSA matches (one of) the correct solution(s). This criterion behaves as a similarity s : P(X)\u00d7 P(X) \u2192 [0, 1], between subsets of X in the data analysis sense (Chandon and Pinson, 1981), where s(X1, X2) > s(X1, X3) indicates that X2 is more similar to X1 than X3, and satisfying s(X1, X2) = 1 \u21d0\u21d2 X1 = X2 and s(X1, X2) = s(X2, X1). Let us denote by X the total set of features, partitioned in X = XR\u222aXI\u222aXR\u2032 , being XR, XI , XR\u2032 the subsets of relevant, irrelevant and redundant features of X , respectively and callX\u2217 \u2286 X any of the correct solutions (all and only relevant variables, no redundancy). Let us denote by A the feature subset selected by a FSA. The idea is to check how much A and X\u2217 have in common. Let us define AR = XR \u2229 A, AI = XI \u2229 A and AR\u2032 = XR\u2032 \u2229 A. In general, we have AT = XT \u2229 A (hereafter T stands for a subindex in {R, I,R\u2032}). Since necessarily A \u2286 X , we have that A = AR \u222a AI \u222a AR\u2032 is a partition of A. The score SX(A) : P(X) \u2192 [0, 1] is defined in terms of the similarity in that for all A \u2286 X,SX(A) = s(A, X\u2217). Thus, SX(A) > SX(A\u2032) indicates that A is more similar to X\u2217 than A\u2032. The idea is to make a flexible measure, so that it can ponder each type of divergence (in relevance, irrelevance and redundancy) to the correct solution. To this end, a set of parameters is collected as \u03b1 = {\u03b1R, \u03b1I , \u03b1R\u2032} with \u03b1T \u2265 0 and \u2211 \u03b1T = 1.\nIntuitive Description. The criterion SX(A) penalizes three situations: (1) There are relevant features lacking in A (the solution is incomplete), (2) There are more than enough relevant features in A (the solution is redundant)\nand (3) There are some irrelevant features in A (the solution is incorrect).\nAn order of importance and a weight will be assigned (via the \u03b1T parameters), to each of these situations. The precedent point (3) is simple to model: if suffices to check whether |AI | > 0, being A the solution of the FSA. Relevance and redundancy are strongly related given that a feature is redundant or not depending on what other relevant features are present in A. Notice then that the correct solution X\u2217 is not unique, and all of them should be equally valid. To this end, the features are broken down in equivalence classes, where elements of the same class are redundant to each other (i.e., any correct solution must comprise only one feature of each equivalence class). Being A a feature set, we define a binary relation between two features xi, xj \u2208 A as: xi \u223c xj \u21d0\u21d2 xi and xj represent the same information. Clearly \u223c is an equivalence relation. Let A/\u223c be the quotient set of A under \u223c; any correct solution must be of the same size than XR and have one element in every subset of (XR \u222aXR\u2032)/\u223c.\nConstruction of the score. The set to be split in equivalence classes is formed by all the relevant features (redundant or not) chosen by a FSA. Define \u03c1A = (AR \u222aAR\u2032)/\u223c (equivalence classes in which the relevant and redundant features chosen by a FSA are split), \u03c1X = (XR \u222a XR\u2032)/\u223c (same with respect to the original set of features) and \u03c1A\u2286X = {x \u2208 \u03c1X | \u2203y \u2208 \u03c1A, y \u2286 x}. For Q quotient set, let:\nF (Q) = \u2211\nx\u2208Q\n(|x| \u2212 1)\nThe idea is to express the quotient between the number of redundant features chosen by the FSA and the number it could have chosen, given the relevant features present in its solution. In the precedent notation, this is written (provided the denominator is not null):\nF (\u03c1A)\nF (\u03c1A\u2286X)\nLet us finally build the score, formed by three terms: relevance, irrelevance and redundancy. Defining:\nIA = 1\u2212 |AI |\n|XI | , RA =\n|\u03c1A| |XR| ,\nR\u2032A =\n{\n0 if F (\u03c1A\u2286X) = 0 (\n1\u2212 F (\u03c1A) F (\u03c1A\u2286X )\n)\notherwise.\nfor any A \u2286 X the score is defined as SX(A) = s(A, X\u2217) = \u03b1RRA + \u03b1R\u2032R \u2032 A + \u03b1IIA. This score fulfills the two conditions (proof is given in the Appendix):\n1. SX(A) = 0 \u21d0\u21d2 A = XI\n2. SX(A) = 1 \u21d0\u21d2 A = X \u2217\nWe can establish now the desired restrictions on the behavior of the score. From the more to the less severe: there are relevant features lacking, there are irrelevant features, and there is redundancy in the solution. This is reflected in the following conditions on the \u03b1T :\n1. Choosing an irrelevant feature is better than missing a relevant one: \u03b1R|XR| > \u03b1I |XI |\n2. Choosing a redundant feature is better than choosing an irrelevant one: \u03b1I|XI | > \u03b1R\u2032 |XR\u2032 |\nWe also define \u03b1T = 0 if |XT | = 0. Observe that the denominators are important for, say, expressing the fact that it is not the same choosing an irrelevant feature when there were only two that when there were three (in the latter case, there is an irrelevant feature that could have been chosen when it was not). In order to translate the previous inequalities into workable conditions, a parameter \u01eb \u2208 (0, 1] is introduced to express the precise relation between the \u03b1T . Let \u03b1T = \u03b1T |XT | . The following equations have to be satisfied, together with \u03b1R + \u03b1I + \u03b1R\u2032 = 1:\n\u03b2R\u03b1R = \u03b1I , \u03b2I\u03b1I = \u03b1R\u2032\nfor suitable chosen values of \u03b2R and \u03b2I . Reasonable settings are obtained by taking \u03b2R = \u01eb/2 and \u03b2I = 2\u01eb/3, though other settings are possible, depending on the evaluator\u2019s needs. With these values, at equal |XR|, |XI |, |XR\u2032 |, \u03b1R is at least twice more important than \u03b1I (because of the \u01eb/2) and \u03b1I is at least one and a half times more important than \u03b1R\u2032 . Specifically, the minimum values are attained for \u01eb = 1 (i.e., \u03b1R counts twice \u03b1I). For \u01eb < 1 the differences widen proportionally to the point that, for \u01eb \u2248 0, only \u03b1RR will practically count on the overall score."}, {"heading": "6 EXPERIMENTAL EVALUATION", "text": "In the following sections we detail the experimental methodology and quantify the various parameters of the experiments. The basic idea consists on generating sample data sets using synthetic functions f with known relevant features. These data sets (of different sizes) are corrupted with irrelevant and/or redundant features and handed over to the different FSAs to obtained a hypothesis H . The divergence between the defined function f and the obtained hypothesis H will be evaluated by the score criterion (with \u01eb = 1). This experimental design is illustrated in Fig. 1."}, {"heading": "6.1 Description of the FSAs used", "text": "Up to ten FSAs were used in the experiments. These are E-Sfg, Qbb, Lvf, Lvi, C-Sbg, Relief, Sfbg, Sffg, WSbg, and W-Sfg. The algorithms E-Sfg, W-Sfg are versions of Sfg using entropy and the accuracy of a C4.5 inducer, respectively. The algorithms C-Sbg, W-Sbg are versions of Sbg using consistency and the accuracy of a C4.5 inducer, respectively. Since Relief and E-Sfg yield\nan ordered list of features xi according to their weight wi, an automatic filtering criterion is necessary to transform every solution into a subset of features. The procedure used here to determine a suitable cut point is simple: first the weights are sorted in decreasing order (with wn the greatest weight, corresponding to the most relevant feature). Then those weights further than two variances from the mean are discarded (that is to say, with very high or very low weights). The idea is to look for the feature xj such that\nwn\u2212wj wn\u2212w1 j n\nis maximum. Intuitively, this corresponds to obtaining the maximum weight with the lowest number of features. The cut point is then set between xj and xj\u22121."}, {"heading": "6.2 Implementations of data families", "text": "A total of twelve families of data sets were generated studying three different problems and four instances of each, by varying the number of relevant features NR. Let x1, . . . , xn be the relevant features of a problem f .\nParity: This is the classic problem where the output is f(x1, \u00b7 \u00b7 \u00b7 , xn) = 1 if the number of xi = 1 is odd and f(x1, \u00b7 \u00b7 \u00b7 , xn) = 0 otherwise.\nDisjunction: Here we have f(x1, \u00b7 \u00b7 \u00b7 , xn) = 1 if (x1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 xn\u2032) \u2228 (xn\u2032+1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 xn), with n\u2032 = n div 2 (n even) and n\u2032 = (n div 2) + 1 (n odd).\nGMonks: This problem is a generalization of the classic monks problems (Thrun, 1991). In its original version, three independent problems were applied on sets of n = 6 features that take values of a discrete, finite and unordered set (nominal features). Here we have grouped the three problems in a single one computed on each chunk of 6 features. Let n be multiple of 6, k = n div 6 and b = 6(k\u2032\u22121)+1, for 1 \u2264 k\u2032 \u2264 k. Let us denote for \u201c1\u201d the first value of a feature, for \u201c2\u201d the second, etc. The problems are the following:\n1. P1 : (xb = xb+1) \u2228 xb+4 = 1\n2. P2 : two or more xi = 1 in xb \u00b7 \u00b7 \u00b7xb+5\n3. P3 : (xb+4 = 3 \u2227 xb+3 = 1) \u2228 (xb+4 6= 3 \u2227 xb+1 6= 2)\nFor each chunk, the boolean condition P2\u2227\u00ac(P1\u2227P3) is checked. If it is satisfied for nc div 2 or more chunks (being nc the number of chunks) the function Gmonks is 1; otherwise, it is 0."}, {"heading": "6.3 Experimental setup", "text": "The experiments are divided in three main groups. The first group explores the relationship between irrelevance vs. relevance. The second one explores the relationship between redundancy vs. relevance. The last group is the study of the effect of different sample sizes. Each group uses three families of problems (Parity, Disjunction and GMonks) with four different instances for each one, varying the number of relevant features NR, as indicated:\nRelevance: The different numbers NR vary for each problem, as follows: {4, 8, 16, 32} (for Parity), {5, 10, 15, 20} (for Disjunction) and {6, 12, 18, 24} (for GMonks). Irrelevance: In these experiments, NI runs from zero to twice the value of NR. Specifically, NI \u2208 {(k \u00b7NR)/p, k = 0, 1, . . . , 10} (that is, eleven different experiments of irrelevance for each NR). The value of p is chosen so that all the involved quantities are integer: p = 4 for Parity, p = 5 for Disjunction and p = 6 for GMonks. Redundancy: Analogously to the generation of irrelevant features, we have NR\u2032 running from zero to twice the value of NR (eleven experiments of irrelevance for each NR). Sample Size: Given the formula |S| = \u03b1kNT c (see \u00a75), different problems were generated considering k \u2208 {0.25, 0.5, 0.75, 1.0, 1.25, 1.75, 2.0}, NT = NR+NI +NR\u2032 , c = 2 and \u03b1 = 20. The values of NI and NR\u2032 were fixed as NI = NR\u2032 = NR div 2."}, {"heading": "6.4 Discussion of the results", "text": "Due to space reasons, only a representative sample of the results is presented, in graphical form, in Figs. 2 and 3. In all the plots, each point represents the average of 10 independent runs with different random data samples. The Figs. 2(a) and (b) are examples of irrelevance vs. relevance for four instances of the problems, (c), (d) are examples of redundancy vs. relevance and (e), (f) of sample size experiments. In all cases, the horizontal axis represents the ratios between these particulars as explained above. The vertical axis represents the average results given by the score criterion.\n\u2022 In Fig. 2(a) the C-Sbg algorithm shows at first a good performance but clearly falls dramatically (below the 0.5 level from NI = NR on) as the irrelevance ratio increases. Note that for NR = 4 performance is perfect (the plot is on top of the graphic). In contrast, in Fig. 2(b) the Relief algorithm presents very similar and fairly good results for the four instances of the problem, being almost insensitive to the total number of features.\n\u2022 In Fig. 2(c) the Lvf algorithm presents a very good and stable performance for the different problem instances of Parity. In contrast, in 2(d) Qbb tends to a poor general performance in the Disjunction problem when the total number of features increases.\n\u2022 The plots in Figs. 2(e) and (f) show additional interesting results because we can appreciate the curse\nof dimensionality (Jain and Zongker, 1997). In these figures, Lvi and W-Sfg perform increasingly poorly (see the figure from top to bottom) with higher numbers of features, provided the number of examples is increased in a linear way. However, in general, as long as more examples are added, performance is better (left to right).\nA summary of the complete results is displayed in Fig. 3 for the ten algorithms, allowing for a comparison across all the sample datasets with respect to each studied particular. Specifically, Figs. 3(a), (c) and (d) show the average score of each algorithm for irrelevance, redundancy and sample size, respectively. Moreover, Figs. 3(b), (d) and (f) show the same average weighed by NR, in such a way that more weight is assigned to more difficult problems (higher NR). In each graphic there are two keys: the key to the left shows the algorithms ordered by total average performance, from top to bottom. The key to the right shows the algorithms ordered by average performance on the last abscissa value, also from top to bottom. In other words, the left list is topped by the algorithm that wins on average, while the right list is topped by the algorithm that ends on the lead. This is also useful to help reading the graphics.\n\u2022 Fig. 3(a) shows thatRelief ends up on the lead of the irrelevance vs. relevance problems, while Sffg shows the best average performance. The algorithm W-Sfg is also well positioned.\n\u2022 Fig. 3(c) shows the algorithms Lvf and Lvi, together with C-Sbg, as the overall best. In fact, there is a bunch of algorithms that also includes the two floating and Qbb showing a close performance. Note how Relief and the wrappers are very poor performers.\n\u2022 Fig. 3(e) shows how the wrapper algorithms extract the most of the data when there is a shortage of it. Surprisingly, the backward wrapper is just fairly positioned on average. The Sffg algorithm is again quite good on average, together with C-Sbg. However, all of the algorithms are quite close and show the same kind of dependency to the amount of available data. Note the general poor performance of E-Sfg, most likely due to the fact that it is the only algorithm that computes its evaluation measure (entropy in this case) independently for each feature.\nThe weighed versions of the plots (Fig. 3 (b),(d) and (f)) do not seem to alter the picture very much. A closer look reveals that the differences between the algorithms have widened. Very interesting is the change for Relief, that takes the lead both on irrelevance and sample size, but not on redundancy."}, {"heading": "6.5 General considerations", "text": "The results point to Sffg as the best algorithm on average in complete ignorance of the particulars of the data\nset, or whenever one is willing to use a single algorithm. However, in view of the reported results, a better strategy would be to run various algorithms in a coupled way (i.e., in different execution orders and piping the respective solutions) and observe the results. Specifically, we suggest to use Relief when one is interested in detecting irrelevance, Lvf for detecting redundancy and W-Sfg in presence of small sample size situations. In light of this, we conjecture that Sffg used in a wrapper fashion could be a better one-fits-all option for small to moderate size problems.\nWe would like to bring to attention the following points:\n1. The wild differences in performance for different algorithms and data particulars: fixing an algorithm A and a problem P , performance of A is dramatically different for the various particulars considered (but in a consistent way in all instances of P ). However, these results are coherent and scale quite well for increasing numbers of relevant features.\n2. The score criterion seems to reliably capture what intuition tells about the quality of a solution at this simple level.\nWe would also like to emphasize the fact that the differences in the outcome yielded by the algorithms are not entirely due to their different approach to the problem. Rather, they are also attributable to the lack of a precise optimization goal, for example in the form described in Definition 1. Another good deal is the finite (and possibly very limited) sample size which, on the one hand, hinders the obtention of an accurate evaluation of relevance. On the other, the dependence on a specific sample reminds us that every evaluation of relevance in a feature subset should be regarded as the outcome of a random variable, different samples yielding different outcomes. In this vein, the use of resampling techniques like Random Forests (Breiman, 2001) is strongly recommended.\nA final interesting point is the relation between the evaluation given by a specific inducer and the score. We were interested in ascertaining whether higher inducer evaluations imply higher scores. We next provide evidence that this need not be the case by means of a counterexample.\nConjecture: given a FSA and the solution it yields in a data set, we know this solution is suboptimal in the sense that better solutions may exist but are not found. However, we would expect the solution to be better (i.e. have a higher score) the better its performance is.\nExperiment: we run W-Sfg in 10 independent runs with different random data samples of size 600 using Na\u0308\u0131ve Bayes as inducer in an instance of the GMonks problem, described byNR = 24, NR\u2032 = 12 andNI = 24 forNT = 60. Table 1 shows the results: for each run, the final inducer performance is given, as well as the score of the solutions. Runs 5 and 8 correspond to very different solutions (number 5 being much better than number 8) that have almost the same inducer evaluation. Run 5 also has a lower evaluation than run 9, but a greater score."}, {"heading": "2 0.869 0.601 24 14", "text": ""}, {"heading": "3 0.884 0.588 19 10", "text": ""}, {"heading": "4 0.858 0.609 22 13", "text": ""}, {"heading": "5 0.876 0.730 30 19", "text": ""}, {"heading": "6 0.875 0.475 5 0", "text": ""}, {"heading": "7 0.872 0.456 8 6", "text": ""}, {"heading": "8 0.880 0.412 5 2", "text": ""}, {"heading": "9 0.881 0.630 14 4", "text": "This same experiment can be used to show the variability in the results as a function of the data sample. It can be seen that the numbers of relevant and redundant as well as irrelevant features depend very much on the sample. A look at the precise features chosen reveals that they are very different solutions (a fact that is also indicated by the score) that nonetheless give a similar evaluation by the inducer. Given the incremental nature of W-Sfg, it can be deduced that classifier improvements where obtained by adding completely irrelevant features."}, {"heading": "7 CONCLUSIONS", "text": "The task of a feature selection algorithm (FSA) is to provide with a computational solution to the feature selection problem motivated by a certain definition of relevance or, at least, by a performance evaluation measure. This algorithm should also be increasingly reliable with sample size and pursue the solution of a clearly stated optimization goal. The many algorithms proposed in the literature are based on quite different principles and loosely follow these recommendations, if at all. In this research, several fundamental algorithms have been studied to assess their performance in a controlled experimental scenario. A measure to evaluate FSAs has been devised that computes the degree of matching between the output given by a FSA and the known optimal solution. This measure takes into account the particulars of relevance, irrelevance, redundancy and size of synthetic data sets. Our results illustrate the pitfall in relying in a single algorithm and sample data set, very specially when there is poor knowledge available about the structure of the solution or the sample data size is limited. The results also illustrate the strong dependence on the particular conditions in the data set description, namely the amount of irrelevance and redundancy relative to the total number of features. Finally, we have shown by a simple example how the evaluation of a feature subset can be misleading even when using a reliable inducer. All this points in the direction of using hybrid algorithms (or principled combinations of algorithms) as well as resampling for a more reliable assessment of feature subset performance.\nThis work can be extended in many ways, to carry up more general evaluations (considering richer forms of redundancy) and using other kinds of data (e.g., continuous data). A specific line of research is the corresponding extension of the scoring criterion."}], "references": [{"title": "A Comparative Evaluation of Sequential Feature Selection Algorithms", "author": ["D.W. Aha", "R.L. Bankert"], "venue": "In Proc. of the 5th International Workshop on Artificial Intelligence and Statistics ,", "citeRegEx": "Aha and Bankert,? \\Q1995\\E", "shortCiteRegEx": "Aha and Bankert", "year": 1995}, {"title": "Learning with Many Irrelevant Features", "author": ["H. Almuallim", "T.G. Dietterich"], "venue": "In Proc. of the 9th National Conference on Artificial Intelligence,", "citeRegEx": "Almuallim and Dietterich,? \\Q1991\\E", "shortCiteRegEx": "Almuallim and Dietterich", "year": 1991}, {"title": "Use of Distance Measures, Information Measures and Error Bounds in Fuature Evaluation", "author": ["M. Ben-Bassat"], "venue": "Handbook of Statistics ,", "citeRegEx": "Ben.Bassat,? \\Q1982\\E", "shortCiteRegEx": "Ben.Bassat", "year": 1982}, {"title": "Feature Selection from Huge Feature Sets", "author": ["J. Bins", "B. Draper"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Bins and Draper,? \\Q2001\\E", "shortCiteRegEx": "Bins and Draper", "year": 2001}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "Breiman,? \\Q2001\\E", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "Greedy Attribute Selection", "author": ["R.A. Caruana", "D. Freitag"], "venue": "In Proc. of the 11th International Conference on Machine Learning,", "citeRegEx": "Caruana and Freitag,? \\Q1994\\E", "shortCiteRegEx": "Caruana and Freitag", "year": 1994}, {"title": "Analyse Typologique", "author": ["S. Chandon", "L. Pinson"], "venue": null, "citeRegEx": "Chandon and Pinson,? \\Q1981\\E", "shortCiteRegEx": "Chandon and Pinson", "year": 1981}, {"title": "Hybrid Search of Feature Subsets", "author": ["M. Dash", "H. Liu"], "venue": "Proc. of the 15th Pacific Rim International Conference on AI ,", "citeRegEx": "Dash and Liu,? \\Q1998\\E", "shortCiteRegEx": "Dash and Liu", "year": 1998}, {"title": "Feature Selection for Classification", "author": ["M. Dash", "H. Liu", "H. Motoda"], "venue": "Intelligence Data Analysis: An International Journal", "citeRegEx": "Dash et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dash et al\\.", "year": 1997}, {"title": "Consistency Based Feature Selection", "author": ["M. Dash", "H. Liu", "H. Motoda"], "venue": "In Pacific\u2013Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Dash et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dash et al\\.", "year": 2000}, {"title": "An Evaluation of Feature Selection Methods and their Application to Computer Security", "author": ["J. Doak"], "venue": "Technical Report CSE\u201392\u201318,", "citeRegEx": "Doak,? \\Q1992\\E", "shortCiteRegEx": "Doak", "year": 1992}, {"title": "Correlation\u2013based Feature Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "PhD thesis,", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Feature Selection: Evaluation, Application, and Small Sample Performance", "author": ["A.K. Jain", "D. Zongker"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Jain and Zongker,? \\Q1997\\E", "shortCiteRegEx": "Jain and Zongker", "year": 1997}, {"title": "Irrelevant Features and the Subset Selection Problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "In Proc. of the 11th International Conference on Machine Learning,", "citeRegEx": "John et al\\.,? \\Q1994\\E", "shortCiteRegEx": "John et al\\.", "year": 1994}, {"title": "A Practical Approach to Feature Selection", "author": ["K. Kira", "L. Rendell"], "venue": "In Proc. of the 9th International Conference on Machine Learning,", "citeRegEx": "Kira and Rendell,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell", "year": 1992}, {"title": "Estimating Attributes: Analysis and Extensions of Relief", "author": ["I. Kononenko"], "venue": "In Proc. of the European Conference on Machine Learning,", "citeRegEx": "Kononenko,? \\Q1994\\E", "shortCiteRegEx": "Kononenko", "year": 1994}, {"title": "A Comparative Evaluation of medium and large\u2013scale Feature Selectors for Pattern Classifiers", "author": ["M. Kudo", "J. Sklansky"], "venue": "In Proc. of the 1st International Workshop on Statistical Techniques in Pattern Recognition,", "citeRegEx": "Kudo and Sklansky,? \\Q1997\\E", "shortCiteRegEx": "Kudo and Sklansky", "year": 1997}, {"title": "Feature Selection for Knowledge Discovery and Data Mining", "author": ["H. Liu", "H. Motoda"], "venue": null, "citeRegEx": "Liu and Motoda,? \\Q1998\\E", "shortCiteRegEx": "Liu and Motoda", "year": 1998}, {"title": "A Monotonic Measure for Optimal Feature Selection", "author": ["H. Liu", "H. Motoda", "M. Dash"], "venue": "In Proc. of the European Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1998}, {"title": "Incremental feature selection", "author": ["H. Liu", "R. Setiono"], "venue": "Applied Intelligence,", "citeRegEx": "Liu and Setiono,? \\Q1998\\E", "shortCiteRegEx": "Liu and Setiono", "year": 1998}, {"title": "Scalable Feature Selection for Large Sized Databases", "author": ["H. Liu", "R. Setiono"], "venue": "In Proc. of the 4th World Congress on Expert Systems ,", "citeRegEx": "Liu and Setiono,? \\Q1998\\E", "shortCiteRegEx": "Liu and Setiono", "year": 1998}, {"title": "A Branch and Bound Algorithm for Feature Subset Selection", "author": ["P. Narendra", "K. Fukunaga"], "venue": "IEEE Transactions on Computer ,", "citeRegEx": "Narendra and Fukunaga,? \\Q1977\\E", "shortCiteRegEx": "Narendra and Fukunaga", "year": 1977}, {"title": "Floating Search Methods in Feature Selection", "author": ["P. Pudil", "J. Novovicov\u00e1", "J. Kittler"], "venue": "Pattern Recognition", "citeRegEx": "Pudil et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pudil et al\\.", "year": 1994}, {"title": "Overfitting in Making Comparisons Between Variable Selection Methods", "author": ["J. Reunanen"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Reunanen,? \\Q2003\\E", "shortCiteRegEx": "Reunanen", "year": 2003}, {"title": "The MONK\u2019s Problems: A Performance Comparison of Different Learning Algorithms", "author": ["Thrun", "S. e"], "venue": "Technical Report CS-91-197,", "citeRegEx": "Thrun and e.,? \\Q1991\\E", "shortCiteRegEx": "Thrun and e.", "year": 1991}, {"title": "Induction of Concepts in the Predicate Calculus", "author": ["S.A. Vere"], "venue": "In Proc. of the 4th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Vere,? \\Q1975\\E", "shortCiteRegEx": "Vere", "year": 1975}, {"title": "Learning Structural Descriptions from Examples", "author": ["P.H. Winston"], "venue": "The Psychology of Computer Vision. McGrawHill", "citeRegEx": "Winston,? \\Q1975\\E", "shortCiteRegEx": "Winston", "year": 1975}], "referenceMentions": [{"referenceID": 5, "context": "This is so because, from the inductive learning perspective, the relevance of a feature may have several definitions depending on the precise objective that is looked for (Caruana and Freitag, 1994).", "startOffset": 171, "endOffset": 198}, {"referenceID": 23, "context": "In this sense, it is our hypothesis that FSAs are very affected by finite sample sizes, which distort reliable assessments of subset relevance, even in the presence of a very sophisticated search algorithm (Reunanen, 2003).", "startOffset": 206, "endOffset": 222}, {"referenceID": 2, "context": "This problem is aggravated when using filter measures, since in this case the relation to true generalization ability (as expressed by the Bayes error) can be very loose (Ben-Bassat, 1982).", "startOffset": 170, "endOffset": 188}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 115}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 128}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 153}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 179}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b). Some of these studies use artificially generated data sets, like the widespread Parity, Led or Monks problems (Thrun, 1991).", "startOffset": 92, "endOffset": 207}, {"referenceID": 1, "context": "This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution.", "startOffset": 50, "endOffset": 121}, {"referenceID": 5, "context": "This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution.", "startOffset": 50, "endOffset": 121}, {"referenceID": 11, "context": "This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution.", "startOffset": 50, "endOffset": 121}, {"referenceID": 25, "context": "The methods to induce logical conjunctions (Vere, 1975; Winston, 1975), decision trees or artificial neural networks are examples of this embedding.", "startOffset": 43, "endOffset": 70}, {"referenceID": 26, "context": "The methods to induce logical conjunctions (Vere, 1975; Winston, 1975), decision trees or artificial neural networks are examples of this embedding.", "startOffset": 43, "endOffset": 70}, {"referenceID": 13, "context": "Wrapper Mode: Here the relationship is taken the other way around: the FSA uses the learning algorithm as a subroutine (John et al., 1994).", "startOffset": 119, "endOffset": 138}, {"referenceID": 8, "context": "It has been found to be particularly efficient for data sets having redundant features (Dash et al., 1997).", "startOffset": 87, "endOffset": 106}, {"referenceID": 7, "context": "Arguably its main advantage may be that it quickly reduces the number of features in the initial stages with certain confidence (Dash and Liu, 1998; Dash et al., 2000); however, many poor solution subsets are analyzed, wasting computing resources.", "startOffset": 128, "endOffset": 167}, {"referenceID": 9, "context": "Arguably its main advantage may be that it quickly reduces the number of features in the initial stages with certain confidence (Dash and Liu, 1998; Dash et al., 2000); however, many poor solution subsets are analyzed, wasting computing resources.", "startOffset": 128, "endOffset": 167}, {"referenceID": 17, "context": "In Liu and Motoda (1998) it is reported experimentally that Lvi adequately chooses relevant features, but may fail for noisy data sets, in which case the algorithm it is shown to consider irrelevant features.", "startOffset": 3, "endOffset": 25}, {"referenceID": 14, "context": "Relief (Kira and Rendell, 1992) is a general-type algorithm that works exclusively in filter mode.", "startOffset": 7, "endOffset": 31}, {"referenceID": 8, "context": "The algorithm has been found to choose correlated features instead of relevant features (Dash et al., 1997), and therefore the optimal subset can be far from assured (Kira and Rendell, 1992).", "startOffset": 88, "endOffset": 107}, {"referenceID": 14, "context": ", 1997), and therefore the optimal subset can be far from assured (Kira and Rendell, 1992).", "startOffset": 66, "endOffset": 90}, {"referenceID": 15, "context": "Some variants have been proposed to account for several classes (Kononenko, 1994), where the k more similar instances are selected and their averages computed.", "startOffset": 64, "endOffset": 81}, {"referenceID": 9, "context": "When the number of features is small, Doak (1992) reported that Sbg tends to show better performance than Sfg, most likely because Sbg evaluates the contribution of all features from the onset.", "startOffset": 38, "endOffset": 50}, {"referenceID": 0, "context": "In addition, Aha and Bankert (1995) points out that Sfg is preferable when the number of relevant features is (known to be) small; otherwise Sbg should be used.", "startOffset": 13, "endOffset": 36}, {"referenceID": 0, "context": "In addition, Aha and Bankert (1995) points out that Sfg is preferable when the number of relevant features is (known to be) small; otherwise Sbg should be used. Interestingly, it was also reported that Sbg did not always have better performance than Sfg, contrary to the conclusions in Doak (1992). Besides, Sfg is faster in practice.", "startOffset": 13, "endOffset": 298}, {"referenceID": 22, "context": "Sffg (Sequential Floating Forward Generation) (Pudil et al., 1994) is an exponential cost algorithm that operates in a sequential fashion, performing a forward step followed by a variable (and possibly null) number of backward ones.", "startOffset": 46, "endOffset": 66}, {"referenceID": 12, "context": "These two algorithms have been found to be very effective in some situations (Jain and Zongker, 1997), and are among the most popular nowadays.", "startOffset": 77, "endOffset": 101}, {"referenceID": 3, "context": "of features nears the hundred (Bins and Draper, 2001) and the need to fix the size of the final desired subset.", "startOffset": 30, "endOffset": 53}, {"referenceID": 7, "context": "The Qbb (Quick Branch and Bound) algorithm (Dash and Liu, 1998) (described as Algorithm 7) is a type 1 algorithm.", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "The origin of Abb is in Branch & Bound (Narendra and Fukunaga, 1977), an optimal search algorithm.", "startOffset": 39, "endOffset": 68}, {"referenceID": 18, "context": "Abb (Automatic Branch & Bound) (Liu et al., 1998) is a variant having its bound as the inconsistency rate of the data when the full set of features is used (Algorithm 6).", "startOffset": 31, "endOffset": 49}, {"referenceID": 6, "context": "This criterion behaves as a similarity s : P(X)\u00d7 P(X) \u2192 [0, 1], between subsets of X in the data analysis sense (Chandon and Pinson, 1981), where s(X1, X2) > s(X1, X3) indicates that X2 is more similar to X1 than X3, and satisfying s(X1, X2) = 1 \u21d0\u21d2 X1 = X2 and s(X1, X2) = s(X2, X1).", "startOffset": 112, "endOffset": 138}, {"referenceID": 12, "context": "of dimensionality (Jain and Zongker, 1997).", "startOffset": 18, "endOffset": 42}, {"referenceID": 4, "context": "this vein, the use of resampling techniques like Random Forests (Breiman, 2001) is strongly recommended.", "startOffset": 64, "endOffset": 79}], "year": 2011, "abstractText": "The main purpose of Feature Subset Selection is to find a reduced subset of attributes from a data set described by a feature set. The task of a feature selection algorithm (FSA) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure. In this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario. A measure to evaluate FSAs is devised that computes the degree of matching between the output given by a FSA and the known optimal solutions. An extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance, irrelevance, redundancy and size of the data samples. The controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions.", "creator": "LaTeX with hyperref package"}}}