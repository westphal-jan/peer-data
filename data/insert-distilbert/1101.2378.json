{"id": "1101.2378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2011", "title": "Extracting Features from Ratings: The Role of Factor Models", "abstract": "yet performing effective preference - ranking based data retrieval requires detailed and preferentially meaningful human structurized information about the current user as well as the items under strategic consideration. a common problem is that representations of items often only consist of mere technical attributes, which don't resemble human perception. this is particularly true for integral items such as movies or songs. it is often claimed that meaningful competitive item features could be extracted from collaborative rating data, which is becoming available through standardized social networking aggregation services. however, there is only anecdotal evidence supporting this claim ; but if it is true, the extracted information could very valuable elements for preference - based data retrieval. in implementing this paper, we propose a methodology here to systematically check this common claim. we performed a preliminary investigation on a large collection of movie ratings and present initial evidence.", "histories": [["v1", "Wed, 12 Jan 2011 14:56:01 GMT  (28kb)", "http://arxiv.org/abs/1101.2378v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joachim selke", "wolf-tilo balke"], "accepted": false, "id": "1101.2378"}, "pdf": {"name": "1101.2378.pdf", "metadata": {"source": "CRF", "title": "Extracting Features from Ratings: The Role of Factor Models", "authors": ["Joachim Selke", "Wolf-Tilo Balke"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 1.\n23 78\nv1 [\ncs .A\nI] 1\n2 Ja\nn 20\n11"}, {"heading": "1 INTRODUCTION", "text": "Recommender systems [1, 17] are one of the most prominent applications of preference handling technology [6] and a highly active area of research. In particular, fueled by the Netflix competition and its one million dollar prize money [2], research on collaborative recommendation techniques [21] has recently made significant advances, most notably through the introduction of factor models [16, 22].\nIn collaborative recommender systems, users repeatedly express their preferences for items, which usually is done by giving explicit ratings on some predefined numerical scale. This data can be modeled using a rating matrix, whose rows correspond to items, columns to users, and entries to ratings. Typically, ratings matrices are very sparse, that is, only a small fraction of all possible ratings have actually been observed. Personalized recommendations are generated by predicting unobserved ratings from the available data and, for each user, selecting those items considered to be most appealing.\nMost state-of-the-art collaborative recommendation methods\u2014 including the winner of the Netflix Prize\u2014are based on factor models, which are known to yield much more accurate predictions than traditional neighborhood-based methods [14, 15, 22, 23, 24]. In factor models, each user and each item is represented by a vector in some shared real coordinate space. The vectors are chosen such that each observed rating is closely approximated by the dot product of the corresponding item and user vectors. The selection of coordinates usually is formalized as an optimization problem. Predictions for unobserved ratings are generated by computing the respective scalar products. Equivalently, this approach can be seen as a factorization of the rating matrix into the product of an item matrix (whose rows\n1 Institut f\u00fcr Informationssysteme, Technische Universit\u00e4t Braunschweig, Germany\nare the item vectors) and a user matrix (whose columns are the user vectors).\nThe success of factor models is usually attributed to the intuition that the coordinate space used to represent items and users actually is a latent feature space. That is, its dimensions capture the items\u2019 perceptual properties as well as the users\u2019 preference judgments regarding these properties. For example, when items are movies, the individual dimensions are generally thought to measure (more of less) \u201cobvious\u201d features such as horror vs. romance, the level of sophistication, or orientation towards adults. For users, each coordinate is thought to describe the relative degree of importance attached to the respective dimension. This understanding of factor models can be found throughout the literature, for example, in [2, 15, 16, 18, 23].\nAlthough it is intuively appealing, to our knowledge, the correspondence to features has never been systematically proven, but is only reported anecdotically. For example, Koren et al. [16] performed a factorization on the Netflix movie data set and manually interpreted the first two coordinates for selected movies as follows:\nSomeone familiar with the movies shown can see clear meaning in the latent factors. The first factor has on one side lowbrow comedies and horror movies, aimed at a male or adolescent audience, while the other side contains drama or comedy with serious undertones and strong female leads. The second factorization axis has independent, critically acclaimed, quirky films on the top, and on the bottom, mainstream formulaic films.\nFurther evidence has been provided by Tak\u00e1cs et al. [23]. After performing a factorization of the Netflix data set, they manually assigned labels to individual dimensions of their coordinate space, such as Legendary, Typical for men, Romantic, and NOT Monty Python.\nIn this paper, we propose a systematic method for studying the coordinate spaces derived from factor models and apply it the MovieLens 10M data set, a large real-world collection of movie ratings. The main contribution of our work consists in laying important groundwork, on which further research in recommender systems and preference handling can be build. In particular, we see two concrete directions for future work:\n\u2022 First, knowing what kind of semantic information is extracted by factor models\u2014and how it is represented in coordinate spaces\u2014 will enable a deeper understanding of these methods. Ultimately, these findings may lead to a more systematic development and refinement of recommender systems. In particular, a systematic assessment of semantic structures provides an additional way of evaluating the effectiveness of factor-based recommenders. This would perfectly complement traditional evaluation methods [11], which focus on predictive accuracy.\n\u2022 Second, we believe that factor models might be a powerful tool for automatically extracting meaningful descriptions of otherwise hard-to-describe items such as movies or songs\u2014particularly, essential features of movies cannot be characterized at all by purely technical features such as runtime, language, or release date.2 But given a coordinate representation of movies that matches human perception, the full machinery developed in preference handling research can be applied [6, 9]. For example, clustering techniques can give user an initial high-level impression of the available items, item rankings can be learnt from ordinal preference statements [10] or utilities [5], and the best items can be retrieved by means of Top-k algorithms [12].\nSince our primary research interest lies in applying preferencebased retrieval techniques to item collections, in this paper we will concentrate on evaluating the semantic structures contained in the item matrix A. Performing a similar analysis of the user matrix B may require entirely different methods.\nThe paper is structured as follows: After introducing notation and reviewing the most important factor models, we develop general guidelines on how to evaluate coordinate spaces for semantic information. Then, we illustrate how to apply these guidelines to the evaluation of factor spaces generated from movie rating data and perform experiments on the MovieLens 10M data set."}, {"heading": "2 PRELIMINARIES", "text": "In the following, we use the variables i and j to identify items, whereas u and v denote users. We are dealing with ratings given to I items by U users. Let R = (ri,u) \u2208 {R \u222a \u2205}I\u00d7U be the corresponding rating matrix, where ri,u = \u2205, if item i has not been rated by user u; otherwise, ri,u expresses the strength of user u\u2019s preference for item i. Ratings are usually limited to a fixed integer scale (for example, one to ten stars). Moreover, R = { (i, u) | ri,u 6= \u2205 }\nis the set of all item\u2013user pairs for which ratings are known. Let n be the total number of ratings observed (the cardinality of R). Typically, n is very small compared to the number of possible ratings I \u00b7 U (for example, in the Netflix data set it is n\nI\u00b7U \u2248 1.4%).\nGiven some target dimensionality d, the basic idea underlying factor models is to find matrices A = (ai,r) \u2208 RI\u00d7d and B = (br,u) \u2208 R\nd\u00d7U such that their product R\u0302 = A \u00b7 B closely resembles R on all known entries. To quantify this notion of \u201cclose resemblance,\u201d the sum of squared errors (SSE) is popularly chosen. The SSE difference between the rating matrix R and its estimation R\u0302 = (r\u0302i,u) is defined as\nSSE ( R, R\u0302 ) = \u2211\n(i,u)\u2208R\n( ri,u \u2212 r\u0302i,u )2 .\nFactor models are typically formulated as optimization problems over A and B, in which the SSE (or some other measure) is to be minimized.\nProbably the most popular factor model is Brandyn Webb\u2019s regularized SVD model [16, 18], in which A and B are defined as the solution of the least squares problem\nmin A,B\nSSE ( R,A \u00b7B ) + \u03bb \u2211\n(i,u)\u2208R\nd \u2211\nr=1\n(\na2i,r + b 2 r,u\n)\n.\nHere, \u03bb \u2265 0 is a regularization constant used to avoid overfitting.\n2 A complementary approach to closing this semantic gap is content-based image and video retrieval [8].\nMore advanced versions of the SVD model exclude systematic rating deviations from the factorization and model them explicitly using new variables. Bell and Koren [3] propose to estimate rating ri,u by\nr\u0302i,u = \u00b5+ \u03b4i + \u03b4u +\nd \u2211\nr=1\nai,rbr,u,\nwhere the constant \u00b5 denotes the mean of all observed ratings; \u03b4i and \u03b4u are I + U new model parameters expressing systematic item and user deviations from \u00b5. Again, the parameters are chosen according to a regularized least squares problem:\nmin A,B,\u03b4\u22c6\nSSE ( R, R\u0302 ) + \u03bb \u2211\n(i,u)\u2208R\n(\nd \u2211\nr=1\n(\na2i,r + b 2 r,u\n)\n+ \u03b4i + \u03b4u\n)\n.\nThe rationale underlying this approach\u2014which we refer to as \u03b4-SVD in the following\u2014is that the removal of item- and user-specific general trends from the factorization allows to focus on more sophisticated rating patterns.\nThe third basic factor model being relevant to our work performs a non-negative factorization of the rating matrix [23]. It is identical to the regularized SVD model up to the additional constraint that all entries of A and B must be non-negative. Extending this model by explicit item and users deviations is not reasonable since this would require negative entries in A and B to approximate R close enough. The non-negative matrix factorization model aims at creating a coordinate space in which effects of different dimensions on the estimated ratings cannot cancel out each other. Henceforth, we refer to this model as NNMF."}, {"heading": "3 EVALUATING COORDINATE SPACES", "text": "Given an item\u2013feature matrix A \u2208 RI\u00d7d generated by some factor model, how can we determine whether the items\u2019 coordinates in this d-dimensional space resemble a \u201csemantically meaningful\u201d pattern? The most straightforward approach consists in extending and systematizing the casual investigations described in the introduction. This could easily be done by presenting the item coordinate space to a number of different people and asking them to label its dimensions. The correspondence between the generated item coordinates and human perception could, for example, be done by measuring the degree of consensus among people or the average time needed to come up with adequate labels.\nAlthough this kind of investigation seems very reasonable, it contains some severe flaws, which cannot be fixed by careful study design:\n1. The dimensionality chosen in most applications of factor models typically ranges between d = 10 and d = 100. A comprehensive analysis of the resulting data sets would require the users to comprehend high-dimensional spaces, which is impossible even when using advanced visualization techniques. 2. Due to hindsight bias, given enough time, users will be able to assign a fitting label to almost any dimension of the coordinate space. Chances are good that this effect accounts for rather questionable labels such as NOT Monty Python. 3. By using free association to name dimensions, the collection of resulting labels tend to show a high variability and reflect individual differences between users. To produce statistically significant results, either the sample size must be extended (which requires more study participants and results in higher costs), or the variability must be reduced, for example, by training participants to\nuse an established domain-specific vocabulary to articulate the semantic properties they recognize in the data (which also increases time and effort). 4. Typically, there are many near-optimal solutions to the above mentioned optimization problems, which can be transformed into one another by rotation of the coordinate axes. This is because, for any invertible matrix M \u2208 Rd, the solution pairs (A,B) and (AM,M\u22121B) produce the same SSE. Although regularization usually enforces the theoretical existence of a unique optimal solution pair, in practice the enormous problem size often allows only finding one of the many near-optimal solutions. Consequently, the direction of the coordinate axes is completely arbitrary, which makes the task of assigning labels a hopeless undertaking."}, {"heading": "3.1 Some Guidelines", "text": "In this section, we devise a set of guidelines on which to base more appropriate approaches to the analysis of coordinate spaces.\n\u2022 In the view of problems (1) and (4), we recommend to avoid any direct human interaction with item coordinates. Instead, human input should concentrate on describing item properties, which in turn are related to coordinates as well as compared by algorithmic means. \u2022 The only effective way to eliminate hindsight bias (2) is collecting feedback on items before generating and presenting any information extracted by the factor models under consideration. \u2022 To resolve problem (3), we primarily recommend to adapt a domain-specific vocabulary to allow a structurized description of items. For example, to characterize music, the rich vocubulary developed by allmusic3 seems appropriate; amongst others, it includes very detailed information about genres, styles, moods, and connections between artists. Since this kind of semantic information can be (or already have been) provided by a small number of experts and usually is little prone to debate, it is easy to assemble and work with. In later stages of analysis, unrestricted user feedback may be included to reveal the position and extent of more fine-grained and rather subjective concepts in the coordinate space.\nWe also propose to apply a standardization procedure to the generated coordinate space. This is for the following reasons: First, recall that, for any invertible matrix M \u2208 Rd, the solution pairs (A,B) and (AM,M\u22121B) are equivalent; to enable comparisons between different factor models and even different runs of the same optimization algorithms, we need to define one solution pair as the standard representation. Second, to enable a better separation of different effects in the data, the axes of the item (and user) coordinate space should be chosen to be orthogonal. Moreover, axes should be ordered according to their relative importance (measured by the variance of data along each axis); that is, the first dimension should be assigned to the most important axis.\nThe perfect tool for matching these requirements is the singular value decomposition, a well-known matrix factorization technique from linear algebra, which inspired the SVD factor model. It is based on the fact that, for any rank-d matrix X \u2208 I\u00d7 U, there is a columnorthonormal matrix U \u2208 RI\u00d7d, a diagonal matrix S \u2208 Rd\u00d7d, and a row-orthonormal matrix V \u2208 \u00d7 U such that X = USV . By reordering rows and columns, S can be chosen such that its diagonal\n3 http://www.allmusic.com\nelements are ordered by increasing magnitude. Moreover, the diagonal matrix S can be eliminated from this factorization by setting X = U \u2032V \u2032, where U \u2032 = US 1 2 and V \u2032 = US 1 2 . The matrices U \u2032 and V \u2032 are unique if all diagonal elements of S have been mutually different.\nIn our setting, we will apply the singular value decomposition to transform the product X = A \u00b7 B into a new product A\u2032 \u00b7 B\u2032 as just described. Since rating data tends to be very \u201cnoisy,\u201d we can safely assume that (A\u2032, B\u2032) is a unique representation of (A,B); we did not encounter any counterexamples during our experiments on large realworld rating data. Moreover, any equivalent pair (AM,M\u22121B) also gets transformed into (A\u2032, B\u2032), which we define as the corresponding standard representation. It can be computed efficiently using the product decomposition algorithm proposed in [7, Sec. 3]."}, {"heading": "3.2 Use Case: Movie Ratings", "text": "Based on these guidelines, we now present a concrete method for performing a basic evaluation of coordinate spaces generated from movie ratings. Our focus rests on immediate applicability, so we relate the item coordinates to reference data that is already available.\nThe reference source for all kinds of movie-related information is IMDb, the Internet Movie Database4, which currently covers about 1.6 million titles. Most of IMDb\u2019s data has been created with the help of its users. Therefore, a large proportion of the available content can freely be downloaded and used for non-commercial purposes5. Based on this comprehensive data, one should be able to cross-reference any collection of movie ratings with IMDb.\nFor the semantic evaluations we are going to perform, the following attributes of titles may prove helpful: genres, certifications (e.g., USA:PG for parental guidance suggested), year of release, and plot keywords. To illustrate the general procedure, we will only exploit genre information in this paper. Extendig our method to other types of semantic information is straightforward. Checking the correspondence between genres and item coordinates also makes up a good first test of whether at least some basic semantic properties of movies are represented in coordinate spaces, which is exactly the purpose of the current work.\nIMDb recognizes 28 different genres, from Action to Western, where each movie may belong to multiple genres. The assignment of genres is done by IMDb\u2019s expert staff in cooperation with IMDb users. To enforce consistency, this process is based upon a collection of publicly available guidelines6 . Therefore, this data source matches the requirements developed in the previous section.\nTo analyze whether the distribution of genres in coordinate space displays any significant pattern, we turn to established classification algorithms, which explicitly have been designed to exploit any relevant patterns in the data if there are any. In particular, we propose to measure the degree of adherence to a pattern by the classification accuracy shown by these algorithms when predicting the genre of movies based on their coordinates. In essence, we transform our analysis into a sequence of binary classification problems (one for each genre), which enables us to build on solid grounds. Following the common methodology, we use cross-validation; that is, accuracy is measured on a data set, which is independent of the one used to train the classifier. By applying proven techniques to counter overfitting, our approach also overcomes any possible problems related to hindsight bias."}, {"heading": "4 http://www.imdb.com", "text": ""}, {"heading": "5 http://www.imdb.com/interfaces#plain", "text": "6 http://www.imdb.com/updates/guide/genres\nFor a start, we selected two popular classification algorithms, which are able to detect different kinds of patterns in the data: support vector machines and kNN-classifiers.\nSupport vector machines will be used in two different flavors: first, using a linear kernel (refered to as SVM-lin), and second, using a Gaussian radial basis function kernel (SVM-RBF). Linear support vector machines will show a high classification accuracy if most movies of the respective genre are grouped at one side of the data set, which can be separated from all remaining movies by a hyperplane. For example, this can be used to disprove the hypothesis that there exists a direction in the coordinate space along which, say, the amount of action, increases monotonically. In contrast, the SVM-RBF classifier detects whether groups of movies with the same genre tend to be located in close vincinity.\nkNN-classifiers perform well if the distance between movies having the same genre typically is smaller than the distance to movies not having this genre. Therefore, they can be used to check whether genres form spatially separated patterns in coordinate space. Since factor models are not based on a notion of proximity, it is not clear what measure of distance suits factor models best. We will try out the following four measures: Euclidean distance, standardized Euclidean distance (where, to ensure equally weighted dimensions, coordinate values are divided by the standard deviation of the data with respect each dimension), negative scalar product (which essentially adapts the method of rating prediction to measure distance), and cosine similarity (which is monotonically related to the angle between two vectors).\nTo evaluate the true benefit of coordinate spaces generated from factor models, we propose the following baseline, which is derived from traditional neighborhood-based recommendation methods [20] and constructed as follows: First, for any items i and j, we compute their Pearson correlation coefficient\n\u033ai,j =\n\u2211\nu\u2208Ri,j (ri,u \u2212 \u00b5i,j)(rj,u \u2212 \u00b5j,i)\n\u221a\n\u2211\nu\u2208Ri,j (ri,u \u2212 \u00b5i,j)2\n\u221a\n\u2211\nu\u2208Ri,j (rj,u \u2212 \u00b5j,i)2\n,\nwhere Ri,j is the set of all users who rated both i and j, and \u00b5i,j is the mean rating given to item i by users who rated both i and j. If Ri,j is empty, then \u033ai,j is undefined. The Pearson correlation coefficient \u033ai,j measures the tendency of users to rate items i and j similarly. To avoid biased estimates in cases where ni,j = |Ri,j | is very small, we derive a new measure of similarity\nsi,j = ni,j\nni,j + \u03bb \u00b7 \u033ai,j\nfrom \u033ai,j by shrinking towards zero [15]. Here, \u03bb \u2265 0 is a regularization parameter. Finally, we carry over these similarity into distances by applying a logarithmic transformation:\ndi,j = \u2212 ln\n(\n1 + si,j 2\n)\n.\nTo derive a d-dimensional coordinate space in which items i and j approximately have distance di,j , we use metric multidimensional scaling [4]. Since neighborhood-based recommendation methods are usually outperformed by factor models, we expect our baseline coordinate space to be far inferior to those constructed using factor models. We refer to our baseline model as MDS."}, {"heading": "4 EXPERIMENTS ON MOVIELENS 10M", "text": "We applied our approach to the MovieLens 10M data set 7, which consists of about 10 million ratings collected by the online movie recommender service MovieLens8. After postprocessing the original data (removing one non-existing movie, merging several duplicate movie entries, and removing movies that received less than 20 ratings), our new data set consists of 9,984,419 ratings of 8938 movies provided by 69878 users. The ratings use a 10-point scale from 0.5 (worst) to 5 (best). Each user contributed at least 14 ratings.\nOur analysis requires the genre information maintained by IMDb, so we had to map each movie in the data set to its corresponding IMDb entry. This task has been simplified a lot by the fact that all items in the MovieLens 10M data set are relatively well-known movies developed for cinema.9 We mapped about 8000 movies automatically by comparing titles and release years; the remaining movies have been assigned manually or semi-automatically.\nTo avoid the problem of learning from very small samples for now, we did not use all 28 genres distinguished by IMDb. Instead, we take only those genres into consideration that have been assigned to at least 5% of all movies in our data set. Table 1 lists all remaining 13 genres and their relative frequencies. On average, 2.3 genres have been assigned to each movie."}, {"heading": "4.1 Generating Coordinate Spaces", "text": "We implemented each of the four coordinate extraction methods in MATLAB and executed them on our rating data.\nFor SVD, \u03b4-SVD, and NNMF, we followed the literature and used an optimization procedure based on gradient descent; to reduce computation time, we applied the Hessian speedup proposed in [19]. Adapting the common methodology, we chose the regularization parameter \u03bb by cross-validation such that the SSE is minimized on randomly chosen test sets. We ended up with a value of \u03bb = 0.04 for each of the three algorithms.\nSince optimization by gradient descent is known to get stuck in local extrema of the function to be minimized, we ran the three procedures at least three times, each with different initial coordinates, which have been chosen randomly. For each result, we computed the standardized solution pair as described in the previous section. We found that the solutions generated by each extractor do not differ significantly after standardization. This indicates that our coordinate spaces match the unique solution of each optimization problem.\nFor our MDS procedure, we used the regularization constant \u03bb = 20, which we determined by adapting the recommendation Koren"}, {"heading": "7 http://www.grouplens.org/node/73", "text": ""}, {"heading": "8 http://www.movielens.org", "text": "9 This is the reason why we did not consider the Netflix data set. It consists of\nall kinds of DVD titles, which often lack a clear correspondence in IMDb.\ngave for the Netflix data set [15]. The coordinates have been generated by MATLAB\u2019s mdscale function using the metric stress criterion. Since in our data set about 14 percent of all movie\u2013movie pairs had no raters in common, we treated the respective entries of the distance matrix as missing data.\nTo measure the effect of dimensionality, we generated three different coordinate spaces with each extractor by varying the parameter d. We chose d = 10, d = 50, and d = 100."}, {"heading": "4.2 Applying the Classifiers", "text": "In total, we used 14 different classifiers to evaluate each of the 12 coordinate spaces with respect to each of the 13 genres.\nWe implemented the two support vector machine classifiers by soft-margin SVMs with parameters C = 4 and (for SVM-RBF) \u03b3 = 0.1, which have been determined by cross-validation to maximize classification accuracy.\nEach of the four different kNN-classifiers will be applied to the data sets with three different choices of k. To measure whether movies of the same genre tend to occur in larger groups, we chose k = 1, k = 3, and k = 9. In the following, we will refer to these 12 classifiers as kNN-Eucl, kNN-sEucl, kNN-scal, and kNN-cos.\nTo enable comparisons among classifiers and data sets, we generated 20 pairs of training and test sets, each by randomly chosing 40% of all movies for training and 10% (of the remaining movies) for testing. For each of the resulting 2184 combinations of coordinate spaces, classifiers and genres, we use the same 20 pairs of item sets for training and testing. In each case, we measured the classification accuracy. All results reported below are averages over the 20 runs."}, {"heading": "4.3 Results", "text": "Probably the most popular way of assessing a classifier\u2019s performance is measuring its accuracy, that is, the fraction of test items which have been classified correctly. However, in our setting, this measure is not very helpful. To see this, recall that the relative frequency of genres is very different in our data set. For example, over half of all movies belong to the genre Drama, but there are only about 5% War movies. While attaining an accuracy of 95% would be significant for the genre Drama, it can easily be achieved for the genre War just by classifying any movie as non-War. To enable comparisons across genres, we propose to use a modified version of Cohen\u2019s kappa measure.\nAny result of a binary classification task can be described by four numbers, which sum up to 1: the fraction of true positives (\u03b1tp), the fraction of false positives (\u03b1fp), the fraction of false negatives (\u03b1fn), and the fraction of true negatives (\u03b1tn). Accuracy is defined as acc = \u03b1tp + \u03b1tn. Moreover, the accuracy of a static majoritybased classifier (which always returns the label of the more frequent class) is accmaj = max{\u03b1tp + \u03b1fn, \u03b1fp + \u03b1tn}. We propose to use this kind of naive classifier for normalizing the accuracy and define \u03ba = (acc \u2212 accmaj)/(1 \u2212 accmaj). This measure expresses a classifier\u2019s relative performance with respect to the majority-based classifier. If acc = 1 then \u03ba = 1, if acc > accmaj, then \u03ba > 0, if acc = accmaj, then \u03ba = 0, and if acc < accmaj, then \u03ba < 0.\nBy measuring accuracy in terms of \u03ba, we can average classification performance over different genres. Tables 2\u20135 report the mean \u03bas over all 260 classification results obtained for each combination of coordinate space and classifier type. All entries larger than 0.10 have been marked in boldface. We can observe the following:\n\u2022 The coordinate space derived by NNMF does not contain much helpful information about genres that can be exploited by our classifiers. The performance in all other spaces is significantly better. \u2022 Except for NN-sEucl, classification performance generally improves with increasing dimensionality. However, the difference in performance between d = 10 and d = 50 is much larger than the one between d = 50 and d = 100. This indicates that our ordering of dimensions during standardization indeed captures some notion of relative importance. This is probably also the reason for NN-sEucl\u2019s decreasing performance with growing d; treating all dimensions equally seems to overweight information from dimensions at the end of the list. \u2022 The SVM-RBF classifier slightly outperforms SVM-lin, but is comparable in performance to 9NN-Eucl, 9NN-scal, and 9NNcos. This indicates that genres indeed tend to cluster in coordinate spaces, even with respect to different measures of distance. \u2022 The NN-classifiers display bad performance for k = 1 and k = 3, which indicates that, although movies of the same genre roughly occur in clusters, each cluster usually also contains movies that do not have assigned the respective genre. \u2022 In contrast to our expectations, the performance in coordinate spaces generated by factor models is comparable to the performance shown on our baseline coordinate space MDS.\nMoreover, the results suggest that the performance of kNNclassifiers might even further increase for larger values of k. To check this, we performed some preliminary tests with k \u2248 20, but have not been able to confirm this conjective.\nWe also investigated the influence of individual genres on classification performance; as an example, the results for SVM-RBF are reported in Table 6. Entries larger than 0.20 have been indicated. We can see that some genres, such as Horror and Drama, can clearly be identified by the classifier, while others cannot. We have expected much better performance on clear-cut genres such as War.\nIn summary, these preliminary experiments suggest that the coordinate spaces derived by SVD, \u03b4-SVD, and MDS indeed contain some significant semantic information about the represented movies. However, the situation is by far not as clear as claimed by the literature."}, {"heading": "5 CONCLUSION AND OUTLOOK", "text": "In the current paper, we presented a general methodology for systematically analyzing whether coordinate spaces generated from factor models contain semantic information, as it is commonly claimed.\nWe applied our approach to the MovieLens 10M data set and found initial evidence for this claim.\nOur results encourage us to follow this line of research in several ways. First, we would like to investigate whether our results also carry over to more advanced and complex factor models, which have been proposed very recently [13, 15]. It would also interesting to see what more traditional methods such as multidimensional scaling can contribute to the problem of feature extraction from rating data, since our results indicate that these methods can sucessfully be modified for use in our new setting."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["Gediminas Adomavicius", "Alexander Tuzhilin"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "The million dollar programming prize", "author": ["Robert M. Bell", "Jim Bennett", "Yehuda Koren", "Chris Volinsky"], "venue": "IEEE Spectrum,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["Robert M. Bell", "Yehuda Koren"], "venue": "Proceedings of ICDM", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Preference elicitation with subjective features", "author": ["Craig Boutilier", "Kevin Regan", "Paolo Viappiani"], "venue": "Proceedings of RecSys", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Preference handling: An introductory tutorial", "author": ["Ronen I. Brafman", "Carmel Domshlak"], "venue": "AI Magazine,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Accurate computation of the product-induced singular value decomposition with applications", "author": ["Zlatko Drma\u010d"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Towards a comprehensive survey of the semantic gap in visual image retrieval", "author": ["Peter Enser", "Christine Sandom"], "venue": "Proceedings of CIVR 2003,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["Ralf Herbrich", "Thore Graepel", "Klaus Obermayer"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["Jonathan L. Herlocker", "Joseph A. Konstan", "Loren G. Terveen", "John T. Riedl"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "A survey of top-k query processing techniques in relational database systems", "author": ["Ihab F. Ilyas", "George Beskales", "Mohamed A. Soliman"], "venue": "ACM Computing Surveys,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Factorization meets the neighborhood: A multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": "Proceedings of KDD", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Yehuda Koren"], "venue": "Communications of the ACM,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Factor in the neighbors: Scalable and accurate collaborative filtering", "author": ["Yehuda Koren"], "venue": "ACM Transactions on Knowledge Discovery from Data,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Yehuda Koren", "Robert Bell", "Chris Volinsky"], "venue": "IEEE Computer,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Just for you", "author": ["Don Monroe"], "venue": "Communications of the ACM,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Interview with Simon Funk", "author": ["Gregory Piatetsky-Shapiro"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Principal component analysis for large scale problems with lots of missing values", "author": ["Tapani Raiko", "Alexander Ilin", "Juha Karhunen"], "venue": "Proceedings of ECML 2007,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Collaborative filtering recommender systems\u2019, in The Adaptive Web: Meth-  ods and Strategies of Web Personalization, volume", "author": ["J. Ben Schafer", "Dan Frankowski", "Jon Herlocker", "Shilad Sen"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "A unified view of matrix factorization models", "author": ["Ajit P. Singh", "Geoffrey J. Gordon"], "venue": "Proceedings of ECML PKDD 2008: Part II, volume 5212 of LNCS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Scalable collaborative filtering approaches for large recommender systems", "author": ["G\u00e1bor Tak\u00e1cs", "Istv\u00e1n Pil\u00e1szy", "Botty\u00e1n N\u00e9meth", "Domonkos Tikk"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Improving maximum margin matrix factorization", "author": ["Markus Weimer", "Alexandros Karatzoglou", "Alex Smola"], "venue": "Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Recommender systems [1, 17] are one of the most prominent applications of preference handling technology [6] and a highly active area of research.", "startOffset": 20, "endOffset": 27}, {"referenceID": 14, "context": "Recommender systems [1, 17] are one of the most prominent applications of preference handling technology [6] and a highly active area of research.", "startOffset": 20, "endOffset": 27}, {"referenceID": 4, "context": "Recommender systems [1, 17] are one of the most prominent applications of preference handling technology [6] and a highly active area of research.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "In particular, fueled by the Netflix competition and its one million dollar prize money [2], research on collaborative recommendation techniques [21] has recently made significant advances, most notably through the introduction of factor models [16, 22].", "startOffset": 88, "endOffset": 91}, {"referenceID": 17, "context": "In particular, fueled by the Netflix competition and its one million dollar prize money [2], research on collaborative recommendation techniques [21] has recently made significant advances, most notably through the introduction of factor models [16, 22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 13, "context": "In particular, fueled by the Netflix competition and its one million dollar prize money [2], research on collaborative recommendation techniques [21] has recently made significant advances, most notably through the introduction of factor models [16, 22].", "startOffset": 245, "endOffset": 253}, {"referenceID": 18, "context": "In particular, fueled by the Netflix competition and its one million dollar prize money [2], research on collaborative recommendation techniques [21] has recently made significant advances, most notably through the introduction of factor models [16, 22].", "startOffset": 245, "endOffset": 253}, {"referenceID": 11, "context": "traditional neighborhood-based methods [14, 15, 22, 23, 24].", "startOffset": 39, "endOffset": 59}, {"referenceID": 12, "context": "traditional neighborhood-based methods [14, 15, 22, 23, 24].", "startOffset": 39, "endOffset": 59}, {"referenceID": 18, "context": "traditional neighborhood-based methods [14, 15, 22, 23, 24].", "startOffset": 39, "endOffset": 59}, {"referenceID": 19, "context": "traditional neighborhood-based methods [14, 15, 22, 23, 24].", "startOffset": 39, "endOffset": 59}, {"referenceID": 20, "context": "traditional neighborhood-based methods [14, 15, 22, 23, 24].", "startOffset": 39, "endOffset": 59}, {"referenceID": 1, "context": "This understanding of factor models can be found throughout the literature, for example, in [2, 15, 16, 18, 23].", "startOffset": 92, "endOffset": 111}, {"referenceID": 12, "context": "This understanding of factor models can be found throughout the literature, for example, in [2, 15, 16, 18, 23].", "startOffset": 92, "endOffset": 111}, {"referenceID": 13, "context": "This understanding of factor models can be found throughout the literature, for example, in [2, 15, 16, 18, 23].", "startOffset": 92, "endOffset": 111}, {"referenceID": 15, "context": "This understanding of factor models can be found throughout the literature, for example, in [2, 15, 16, 18, 23].", "startOffset": 92, "endOffset": 111}, {"referenceID": 19, "context": "This understanding of factor models can be found throughout the literature, for example, in [2, 15, 16, 18, 23].", "startOffset": 92, "endOffset": 111}, {"referenceID": 13, "context": "[16] performed", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[23].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "This would perfectly complement traditional evaluation methods [11], which focus on predictive accuracy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "research can be applied [6, 9].", "startOffset": 24, "endOffset": 30}, {"referenceID": 7, "context": "For example, clustering techniques can give user an initial high-level impression of the available items, item rankings can be learnt from ordinal preference statements [10] or utilities [5], and the best items can be retrieved by means of Top-k algorithms [12].", "startOffset": 169, "endOffset": 173}, {"referenceID": 3, "context": "For example, clustering techniques can give user an initial high-level impression of the available items, item rankings can be learnt from ordinal preference statements [10] or utilities [5], and the best items can be retrieved by means of Top-k algorithms [12].", "startOffset": 187, "endOffset": 190}, {"referenceID": 9, "context": "For example, clustering techniques can give user an initial high-level impression of the available items, item rankings can be learnt from ordinal preference statements [10] or utilities [5], and the best items can be retrieved by means of Top-k algorithms [12].", "startOffset": 257, "endOffset": 261}, {"referenceID": 13, "context": "Probably the most popular factor model is Brandyn Webb\u2019s regularized SVD model [16, 18], in which A and B are defined as the solution of the least squares problem", "startOffset": 79, "endOffset": 87}, {"referenceID": 15, "context": "Probably the most popular factor model is Brandyn Webb\u2019s regularized SVD model [16, 18], in which A and B are defined as the solution of the least squares problem", "startOffset": 79, "endOffset": 87}, {"referenceID": 6, "context": "2 A complementary approach to closing this semantic gap is content-based image and video retrieval [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Bell and Koren [3] propose to estimate rating ri,u by", "startOffset": 15, "endOffset": 18}, {"referenceID": 19, "context": "The third basic factor model being relevant to our work performs a non-negative factorization of the rating matrix [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "from \u033ai,j by shrinking towards zero [15].", "startOffset": 36, "endOffset": 40}, {"referenceID": 16, "context": "For SVD, \u03b4-SVD, and NNMF, we followed the literature and used an optimization procedure based on gradient descent; to reduce computation time, we applied the Hessian speedup proposed in [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "gave for the Netflix data set [15].", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "carry over to more advanced and complex factor models, which have been proposed very recently [13, 15].", "startOffset": 94, "endOffset": 102}, {"referenceID": 12, "context": "carry over to more advanced and complex factor models, which have been proposed very recently [13, 15].", "startOffset": 94, "endOffset": 102}], "year": 2011, "abstractText": "Performing effective preference-based data retrieval requires detailed and preferentially meaningful structurized information about the current user as well as the items under consideration. A common problem is that representations of items often only consist of mere technical attributes, which do not resemble human perception. This is particularly true for integral items such as movies or songs. It is often claimed that meaningful item features could be extracted from collaborative rating data, which is becoming available through social networking services. However, there is only anecdotal evidence supporting this claim; but if it is true, the extracted information could very valuable for preference-based data retrieval. In this paper, we propose a methodology to systematically check this common claim. We performed a preliminary investigation on a large collection of movie ratings and present initial evidence.", "creator": "LaTeX with hyperref package"}}}