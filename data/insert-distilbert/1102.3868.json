{"id": "1102.3868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Feb-2011", "title": "Evolved preambles for MAX-SAT heuristics", "abstract": "max - sat heuristics algorithm normally operate from random initial true truth assignments to the variables. we consider the use functions of what agents we call preambles, which theoretically are sequences clusters of variables with corresponding single - variable assignment conditional actions intended to be used to determine a completely more suitable initial truth testing assignment for a given problem instance and a given heuristic. for a similar number of well established max - sat heuristics and benchmark instances, we demonstrate that preambles simultaneously can be evolved by a genetic algorithm such that the heuristics are outperformed in a significant fraction of the cases.", "histories": [["v1", "Fri, 18 Feb 2011 16:37:59 GMT  (13kb)", "http://arxiv.org/abs/1102.3868v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["luis o rigo jr", "valmir c barbosa"], "accepted": false, "id": "1102.3868"}, "pdf": {"name": "1102.3868.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Valmir C. Barbosa"], "emails": ["(valmir@cos.ufrj.br)."], "sections": [{"heading": null, "text": "ar X\niv :1\n10 2.\n38 68\nv1 [\ncs .A\nMAX-SAT heuristics normally operate from random initial truth assignments to the variables. We consider the use of what we call preambles, which are sequences of variables with corresponding single-variable assignment actions intended to be used to determine a more suitable initial truth assignment for a given problem instance and a given heuristic. For a number of well established MAX-SAT heuristics and benchmark instances, we demonstrate that preambles can be evolved by a genetic algorithm such that the heuristics are outperformed in a significant fraction of the cases.\nKeywords: MAX-SAT, MAX-SAT heuristics, Initial truth assignments."}, {"heading": "1 Introduction", "text": "Given a set V of Boolean variables and a set of disjunctive clauses on literals from V (i.e., variables or their negations), MAX-SAT asks for a truth assignment to the variables that maximizes the number of clauses that are satisfied (i.e., made true by that assignment). MAX-SAT is NP-hard [8] but can be approximated in polynomial time, though not as close to the optimum as one wishes (cf. [2] and references therein, as well as [5] for the restricted case of three-literal clauses). MAX-SAT has enjoyed a paradigmatic status over the years, not only because of its close relation to SAT, the first decision problem to be proven NP-complete, but also because of its importance to other areas (e.g., constraint satisfaction in artificial intelligence [6]).\nSince NP-hardness is a property of worst-case scenarios, the difficulty of actually solving a specific instance of an NP-hard problem varies widely with both the instance\u2019s size and internal structure. In fact, in recent years it has become\n\u2217Corresponding author (valmir@cos.ufrj.br).\nincreasingly clear that small changes in either can lead to significant variation in an algorithm\u2019s performance, possibly even to a divide between the instance\u2019s being solvable or unsolvable by that algorithm given the computational resources at hand and the time one is willing to spend [13]. Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].\nHere we investigate a different, though related, approach to method selection in the case of MAX-SAT instances. Since all MAX-SAT heuristics require an initial truth assignment to the variables, and considering that this is invariably chosen at random, a natural question seems to be whether it is worth spending some additional effort to determine an initial assignment that is better suited to the instance at hand. Once we adopt this two-stage template comprising an initial-assignment selection in tandem with a heuristic, fixing the latter reduces the issue of method selection to that of identifying a procedure to determine an appropriate initial assignment. We refer to this procedure as a preamble to the heuristic. As we demonstrate in the sequel, for several state-of-the-art heuristics and problem instances the effort to come up with an appropriate preamble pays off in terms of better solutions for the same amount of time.\nWe proceed in the following manner. First, in Section 2, we define what preambles are in the case of MAX-SAT. Then we introduce an evolutionary method for preamble determination in Section 3 and give computational results in Section 4. We close with concluding remarks in Section 5."}, {"heading": "2 MAX-SAT preambles", "text": "Let n be the number of variables in V . Given a MAX-SAT instance on V , a preamble p of length \u2113 is a sequence of pairs \u3008v1, a1\u3009, \u3008v2, a2\u3009, . . . , \u3008v\u2113, a\u2113\u3009, each representing a computational step to be taken as the preamble is played out. In this sequence, and for 1 \u2264 k \u2264 \u2113, the kth pair is such that vk \u2208 V and ak is one of 2, 1, or 0, indicating respectively whether to leave the value of vk unchanged, to act greedily when choosing a value for vk, or to act contrarily to such greedy assignment. A preamble need not include all n variables, and likewise a variable may appear more than once in it.\nThe greedy action to which ak sometimes refers assigns to vk the truth value that maximizes the number of satisfied clauses at that point in the preamble. Algorithmically, then, playing out p is equivalent to performing the following steps from some initial truth assignment to the variables in V :\n1. k := 1.\n2. If ak = 2, then proceed to Step 5.\n3. Given the current values of all other variables, compute the number of clauses that get satisfied for each of the two possible assignments to vk.\n4. If ak = 1, then set vk to the truth value yielding the greatest number of satisfied clauses. If ak = 0, then do the opposite. Break ties randomly.\n5. k := k + 1. If k \u2264 \u2113, then proceed to Step 2.\nWe use random initial assignments exclusively. A MAX-SAT preamble, therefore, can be thought of as isolating such initial randomness from the heuristic proper that is to follow the preamble. Instead of starting the heuristic at its usual random initial assignment, we start it at the assignment determined by running the preamble.\nIt is curious to note that, as defined, a preamble generalizes the sequence of steps generated by the simulated annealing method [16] when applied to MAXSAT. In fact, what simulated annealing does in this case, following one of its variations [9, 3], is to choose vk by cycling through the members of V and then let ak be either 1 or 0 with the Boltzmann-Gibbs probability. At the high initial temperatures the two outcomes are nearly equally probable, but the near-zero final temperatures imply ak = 1 (i.e., be greedy) with high probability. The generalization that comes with our definition allows for various possibilities of preamble construction, as the evolutionary procedure we describe next."}, {"heading": "3 Methods", "text": "Given a MAX-SAT instance and heuristic H , our approach is to evolve the best possible preamble to H . We do so through a genetic algorithm of the generational type [24]. The description that follows refers to parameter values that were determined in an initial calibration phase. This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18]. Each of the latter involves variables that number in the order of 104 and clauses numbering in the order of 105. Moreover, not all optima are known (cf. Section 4).\nThe genetic algorithm operates on a population of 50 individuals, each being a preamble to heuristic H . The fitness of individual p is computed as follows. First p is run from 10 random truth assignments to the variables, then H is run from the truth assignment resulting from the run of p that satisfied the most clauses (ties between runs of p are broken randomly). Let R(p) denote the number of clauses satisfied by this best run of p and SH(p) the number of clauses satisfied after H is run. The fitness of individual p is the pair \u3008SH(p), R(p)\u3009. Whenever two individuals\u2019 fitnesses are compared, ties are first broken lexicographically, then randomly. Selection is always performed from linearly normalized fitnesses, the fittest individual of the population being 20 times as fit as the least fit.\nFor each MAX-SAT instance and each heuristic H , we let the genetic algorithm run for a fixed amount of time, during which a new population is\nrepeatedly produced from the current one and replaces it. The initial population comprises individuals of maximum length 1.5n, each created randomly to contain at least 0.4n distinct variables. The process of creating each new population starts by an elitist step that transfers the 20% fittest individuals from the current population to the new one. It then repeats the following until the new population is full.\nFirst a decision is made as to whether crossover (with probability 0.25) or mutation (with probability 0.75) is to be performed. For crossover two individuals are selected from the current population and each is partitioned into three sections for application of the standard two-point crossover operator. The partitioning is done randomly, provided the middle section contains exactly 0.4n distinct variables, which is always possible by construction of the initial population (though at times either of the two extreme sections may turn out to be empty). The resulting two individuals (whose lengths are no longer bounded by 1.5n) are added to the new population. For mutation a single individual is selected from the current population and 50% of its pairs are chosen at random. Each of these, say the kth pair, undergoes either a random change to both vk and ak (if this will leave the individual with at least 0.4n distinct variables) or simply a random change to ak (otherwise). The mutant is then added to the new population.\nThe calibration phase referred to above also yielded three champion heuristics, viz. novelty, walksat-tabu, and adaptnovelty+. The results we give in Section 4 refer exclusively to these, used either in conjunction with the genetic algorithm as described above or by themselves. In the latter case each heuristic is run repeatedly, each time from a new random truth assignment to the variables, until the same fixed amount of time used for the genetic algorithm has elapsed. The result reported by the genetic algorithm refers to the fittest individual in the last population to have been filled during that time. As for the heuristic, in order to compare its performance with that of the genetic algorithm as fairly as possible the result that is reported is the best one obtained after every 50 repetitions.\nAll experiments were performed from within the UBCSAT environment [30]. Optima, whenever possible, were discovered separately via the 2010 release of the MSUnCore code to solve MAX-SAT exactly [20]."}, {"heading": "4 Computational results", "text": "In our experiments we tackled all 100 instances of the 2004 SAT competition [18], henceforth referred to as the 2004 dataset, and all 112 instances of the 2008 MAX-SAT evaluation [1], henceforth referred to as the 2008 dataset. The time allotted for each instance to the genetic algorithm or each of the three heuristics by itself was of 60 minutes, always on identical hardware and software, always with exclusive access to the system. We report exclusively on the hardest instances from either dataset, here defined to be those for which MSUnCore found no answer as a result of being stymied by the available 4 gigabytes of\nRAM and the system\u2019s inability to perform further swapping. There are 51 such instances in the 2004 dataset, 11 in the 2008 dataset, totaling 62 instances.\nOur results are given in Tables 1 through 3, respectively for H = novelty, H = walksat-tabu, and H = adaptnovelty+. Each table contains a row for each of the 62 instances, with a horizontal line separating the instances of the 2004 dataset from those of the 2008 dataset. For each instance the number n of variables is given, as well as the number of clauses (m) and results for the genetic algorithm and for the heuristic in question by itself. These results are the number of satisfied clauses and the time at which this solution was first found during the allotted 60 minutes. Missing results indicate either that no population could be filled during this time (in the case of the genetic algorithm) or that no batch of 50 runs of the heuristic could be finished.\nSome entries in the tables are highlighted by a bold typeface to indicate that the genetic algorithm found a solution strictly better than the one found by the heuristic when used by itself, or a solution satisfying the same number of clauses but first encountered in a shorter time. In the former case only the number of satisfied clauses is highlighted, in the latter case the time is highlighted as well. The number of highlighted instances amounts to the ratios given in Table 4. Clearly, with the notable exception of H = walksat-tabu on the 2008 dataset (on which the use of H alone outperformed the genetic algorithm on all 11 instances), the genetic algorithm succeeds well on a significant fraction of the instances.\nRevising these ratios to contemplate all instances from both datasets (i.e., include the results omitted from Tables 1 through 3) yields the ratios in Table 5. These show that the genetic algorithm fares even better when evaluated on all 100 instances of the 2004 dataset. They also show slightly lower ratios for the genetic algorithm on the 112-instance 2008 dataset for H = novelty and H = adaptnovelty+. As for H = walksat-tabu, we see in Table 5 a dramatic increase from the 0.000 of Table 4, indicating that for this particular H on the complete 2008 dataset the genetic algorithm does better than the heuristic alone only on the comparatively easier instances (and then for a significant fraction of them)."}, {"heading": "5 Concluding remarks", "text": "Given a heuristic for some problem of combinatorial optimization, a preamble such as we defined in Section 2 for MAX-SAT is a selector of initial conditions. As such, it aims at isolating the inevitable randomness of the initial conditions one normally uses with such heuristics from the heuristic itself. By doing so, preambles attempt to poise the heuristic to operate from more favorable initial conditions.\nIn this paper we have demonstrated the success of MAX-SAT preambles when they are discovered, given the MAX-SAT instance and heuristic of interest, via an evolutionary algorithm. As we showed in Section 4, for well established benchmark instances and heuristics the resulting genetic algorithm\ncan outperform the heuristics themselves when used alone. We believe further effort can be profitably spent on attempting similar solutions to other problems that, like MAX-SAT, can be expressed as an unconstrained optimization problem on binary variables. Some of them are the maximum independent set and minimum dominating set problems on graphs, both admitting well-known formulations of this type [4]."}, {"heading": "Acknowledgments", "text": "The authors acknowledge partial support from CNPq, CAPES, and a FAPERJ BBP grant."}], "references": [{"title": "Third Max-SAT evaluation", "author": ["J. Argelich", "C.M. Li", "F. Many\u00e0", "J. Planes"], "venue": "URL http://www.maxsat.udl.cat/08/", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Complexity and Approximation: Combinatorial Optimization Problems and their Approximability Properties", "author": ["G. Ausiello", "P. Crescenzi", "G. Gambosi", "V. Kann", "A. Marchetti-Spaccamela", "M. Protasi"], "venue": "Springer-Verlag, Berlin, Germany", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Massively Parallel Models of Computation", "author": ["V.C. Barbosa"], "venue": "Ellis Horwood, Chichester, UK", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "A distributed implementation of simulated annealing", "author": ["V.C. Barbosa", "E. Gafni"], "venue": "J. Parallel Distrib. Comput., 6:411\u2013434", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "MAX SAT approximation beyond the limits of polynomial-time approximation", "author": ["E. Dantsin", "M. Gavrilovich", "E. Hirsch", "B. Konev"], "venue": "Ann. Pure Appl. Logic, 113:81\u201394", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": "Morgan Kaufmann, San Francisco, CA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "How to solve it automatically: Selection among problem-solving methods", "author": ["E. Fink"], "venue": "Proceedings of the Fourth International Conference on Artificial Intelligence Planning Systems, pages 128\u2013136, Menlo Park, CA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Computers and Intractability: A Guide to the Theory of NP-Completeness", "author": ["M.R. Garey", "D.S. Johnson"], "venue": "W. H. Freeman, New York, NY", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1979}, {"title": "Stochastic relaxation", "author": ["S. Geman", "D. Geman"], "venue": "Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. Pattern Anal. Mach. Intell., PAMI-6:721\u2013741", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1984}, {"title": "Towards an understanding of hill-climbing procedures for SAT", "author": ["I.P. Gent", "T. Walsh"], "venue": "Proceedings of the Eleventh National Conference on Artificial Intelligence, pages 28\u201333, Menlo Park, CA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Unsatisfied variables in local search", "author": ["I.P. Gent", "T. Walsh"], "venue": "J. Hallam, editor, Hybrid Problems, Hybrid Solutions, pages 73\u201385, Amsterdam, The Netherlands", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1995}, {"title": "Algorithm portfolios", "author": ["C.P. Gomes", "B. Selman"], "venue": "Artif. Intell., 126:43\u201362", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Phase Transitions in Combinatorial Optimization Problems: Basics", "author": ["A.K. Hartmann", "M. Weigt"], "venue": "Algorithms and Statistical Mechanics. Wiley- VCH, Weinheim, Germany", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "An adaptive noise mechanism for WalkSAT", "author": ["H.H. Hoos"], "venue": "Proceedings of the Eighteenth National Conference on Artificial Intelligence, pages 655\u2013 660, Menlo Park, CA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Scaling and probabilistic smoothing: Efficient dynamic local search for SAT", "author": ["F. Hutter", "D.A.D. Tompkins", "H.H. Hoos"], "venue": "P. van Hentenryck, editor, Principles and Practice of Constraint Programming, volume 2470 of Lecture Notes in Computer Science, pages 233\u2013248, Berlin, Germany", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science, 220:671\u2013680", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1983}, {"title": "Selecting the right algorithm", "author": ["M.G. Lagoudakis", "M.L. Littman", "R.E. Parr"], "venue": "Proceedings of the 2001 AAAI Fall Symposium Series: Using Uncertainty within Computation, pages 74\u201375, Menlo Park, CA", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "The SAT 2004 competition", "author": ["D. Le Berre", "L. Simon"], "venue": "H. H. Hoos and D. G. Mitchell, editors, Theory and Applications of Satisfiability Testing, volume 3542 of Lecture Notes in Computer Science, pages 321\u2013344, Berlin, Germany", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Boosting as a metaphor for algorithm design", "author": ["K. Leyton-Brown", "E. Nudelman", "G. Andrew", "J. McFadden", "Y. Shoham"], "venue": "F. Rossi, editor, Principles and Practice of Constraint Programming, volume 2833 of Lecture Notes in Computer Science, pages 899\u2013903, Berlin, Germany", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Algorithms for weighted Boolean optimization", "author": ["V. Manquinho", "J. Marques-Silva", "J. Planes"], "venue": "O. Kullmann, editor, Theory and Applications of Satisfiability Testing, volume 5584 of Lecture Notes in Computer Science, pages 495\u2013508, Berlin, Germany", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Tabu search for SAT", "author": ["B. Mazure", "L. Sais", "E. Gregoire"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 281\u2013 285, Menlo Park, CA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Evidence for invariants in local search", "author": ["D. McAllester", "B. Selman", "H. Kautz"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 321\u2013326, Menlo Park, CA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Automatically configuring constraint satisfaction programs: A case study", "author": ["S. Minton"], "venue": "Constraints, 1:7\u201343", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1996}, {"title": "An Introduction to Genetic Algorithms", "author": ["M. Mitchell"], "venue": "The MIT Press, Cambridge, MA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}, {"title": "The algorithm selection problem", "author": ["J.R. Rice"], "venue": "M. Rubinoff and M. C. Yovits, editors, Advances in Computers, volume 15, pages 65\u2013118. Academic Press, New York, NY", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1976}, {"title": "Provably bounded-optimal agents", "author": ["S. Russell", "D. Subramanian"], "venue": "J. Artif. Intell. Res., 2:575\u2013609", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Domain-independant extensions to GSAT: Solving large structured variables", "author": ["B. Selman", "H. Kautz"], "venue": "Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, pages 290\u2013295, San Mateo, CA", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1993}, {"title": "A new method for solving hard satisfiability problems", "author": ["B. Selman", "H. Levesque", "D. Mitchell"], "venue": "Proceedings of the Tenth National Conference on Artificial Intelligence, pages 459\u2013465, Menlo Park, CA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1992}, {"title": "Warped landscapes and random acts of SAT solving", "author": ["D.A.D. Tompkins", "H.H. Hoos"], "venue": "Proceedings of the Eighth International Symposium on Artificial Intelligence and Mathematics, Piscataway, NJ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "UBCSAT: An implementation and experimentation environment for SLS algorithms for SAT and MAX-SAT", "author": ["D.A.D. Tompkins", "H.H. Hoos"], "venue": "H. H. Hoos and D. G. Mitchell, editors, Theory and Applications of Satisfiability Testing, volume 3542 of Lecture Notes in Computer Science, pages 306\u2013320, Berlin, Germany", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Confronting hardness using a hybrid approach", "author": ["V. Vassilevska", "R. Williams", "S.L.M. Woo"], "venue": "Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1\u201310, New York, NY", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 7, "context": "MAX-SAT is NP-hard [8] but can be approximated in polynomial time, though not as close to the optimum as one wishes (cf.", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "[2] and references therein, as well as [5] for the restricted case of three-literal clauses).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[2] and references therein, as well as [5] for the restricted case of three-literal clauses).", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": ", constraint satisfaction in artificial intelligence [6]).", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "increasingly clear that small changes in either can lead to significant variation in an algorithm\u2019s performance, possibly even to a divide between the instance\u2019s being solvable or unsolvable by that algorithm given the computational resources at hand and the time one is willing to spend [13].", "startOffset": 288, "endOffset": 292}, {"referenceID": 24, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 22, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 6, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 11, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 16, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 18, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 30, "context": "Following some early groundwork [25], several attempts have been made at providing theoretical foundations or practical guidelines for automatically selecting which method to use given the instance [26, 23, 7, 12, 17, 19, 31].", "startOffset": 198, "endOffset": 225}, {"referenceID": 15, "context": "It is curious to note that, as defined, a preamble generalizes the sequence of steps generated by the simulated annealing method [16] when applied to MAXSAT.", "startOffset": 129, "endOffset": 133}, {"referenceID": 8, "context": "In fact, what simulated annealing does in this case, following one of its variations [9, 3], is to choose vk by cycling through the members of V and then let ak be either 1 or 0 with the Boltzmann-Gibbs probability.", "startOffset": 85, "endOffset": 91}, {"referenceID": 2, "context": "In fact, what simulated annealing does in this case, following one of its variations [9, 3], is to choose vk by cycling through the members of V and then let ak be either 1 or 0 with the Boltzmann-Gibbs probability.", "startOffset": 85, "endOffset": 91}, {"referenceID": 23, "context": "We do so through a genetic algorithm of the generational type [24].", "startOffset": 62, "endOffset": 66}, {"referenceID": 27, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 20, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 140, "endOffset": 144}, {"referenceID": 14, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 28, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "This phase used the heuristics gsat [28], gwsat [27], hsat [10], hwsat [11], gsat-tabu [21], novelty [22], walksat-tabu [22], adaptnovelty+ [14], saps [15], and sapsnr [29] as heuristic H , and also the instances C880mul, am 8 8, c3540mul, term1mul, and vdamul from [18].", "startOffset": 266, "endOffset": 270}, {"referenceID": 29, "context": "All experiments were performed from within the UBCSAT environment [30].", "startOffset": 66, "endOffset": 70}, {"referenceID": 19, "context": "Optima, whenever possible, were discovered separately via the 2010 release of the MSUnCore code to solve MAX-SAT exactly [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 17, "context": "In our experiments we tackled all 100 instances of the 2004 SAT competition [18], henceforth referred to as the 2004 dataset, and all 112 instances of the 2008 MAX-SAT evaluation [1], henceforth referred to as the 2008 dataset.", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "In our experiments we tackled all 100 instances of the 2004 SAT competition [18], henceforth referred to as the 2004 dataset, and all 112 instances of the 2008 MAX-SAT evaluation [1], henceforth referred to as the 2008 dataset.", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "Some of them are the maximum independent set and minimum dominating set problems on graphs, both admitting well-known formulations of this type [4].", "startOffset": 144, "endOffset": 147}], "year": 2011, "abstractText": "MAX-SAT heuristics normally operate from random initial truth assignments to the variables. We consider the use of what we call preambles, which are sequences of variables with corresponding single-variable assignment actions intended to be used to determine a more suitable initial truth assignment for a given problem instance and a given heuristic. For a number of well established MAX-SAT heuristics and benchmark instances, we demonstrate that preambles can be evolved by a genetic algorithm such that the heuristics are outperformed in a significant fraction of the cases.", "creator": "LaTeX with hyperref package"}}}