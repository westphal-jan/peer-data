{"id": "1104.2018", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Apr-2011", "title": "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression", "abstract": "generalized linear models ( glms ) and single weighted index models ( sims ) provide powerful generalizations of linear nonlinear regression, where the target variable is assumed to be a ( possibly seemingly unknown ) 1 - dimensional function of a perfectly linear predictor. in economic general, analyzing these estimation problems entail non - convex estimation procedures, and, in practice, numerous iterative local search heuristics are often used. kalai goldberg and sastry ( 2009 ) recently provided the first provably efficient solution method for learning sims matrices and glms, under the assumptions that the data are in fact generated under a glm engine and under certain monotonicity optimization and lipschitz length constraints. however, wishing to obtain provable performance, the solution method requires a fresh sample every iteration. in this paper, we provide algorithms for learning glms and sims, which are both economically computationally and statistically efficient. we also provide an empirical stability study, demonstrating their feasibility in practice.", "histories": [["v1", "Mon, 11 Apr 2011 18:24:01 GMT  (37kb,D)", "http://arxiv.org/abs/1104.2018v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG stat.ML", "authors": ["sham m kakade", "adam kalai", "varun kanade", "ohad shamir"], "accepted": true, "id": "1104.2018"}, "pdf": {"name": "1104.2018.pdf", "metadata": {"source": "CRF", "title": "Efficient Learning of Generalized Linear and Single Index Models with Isotonic Regression", "authors": ["Sham Kakade", "Adam Tauman Kalai"], "emails": ["skakade@wharton.upenn.edu", "adum@microsoft.com", "vkanade@fas.harvard.edu", "ohadsh@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The oft used linear regression paradigm models a target variable Y as a linear function of a vector-valued input X. Namely, for some vector w, we assume that E[Y |X] = w \u00b7X. Generalized linear models (GLMs) provide a flexible extension of linear regression, by assuming the existence of a \u201clink\u201d function g such that E[Y |X] = g\u22121(w \u00b7 X). g \u201clinks\u201d the conditional expectation of Y to X in a linear manner, i.e. g(E[Y |X]) = w \u00b7X (see [MN89] for a review). This simple assumption immediately leads to many practical models, including logistic regression, the workhorse for binary probabilistic modeling.\nTypically, the link function is assumed to be known (often chosen based on problem-specific constraints), and the parameter w is estimated using some iterative procedure. Even in the setting where g is known, we are not aware of a classical estimation procedure which is computationally efficient, yet achieves a good statistical rate with provable guarantees. The standard procedure is iteratively reweighted least squares, based on Newton-Ralphson (see [MN89]).\nIn Single Index Models (SIMs), both g and w are unknown. Here, we face the more challenging (and practically relevant) question of jointly estimating g and w, where g may come from a large non-parametric family such as all monotonic functions. There are two issues here: 1) What statistical rate is achievable for simultaneous estimation of g and w? 2) Is there a computationally efficient algorithm for this joint estimation? With regards to the former, under mild Lipschitz-continuity restrictions on g\u22121, it is possible to characterize the effectiveness of an (appropriately constrained) joint empirical risk minimization procedure.\n\u2217This work was partially supported by grants NSF-CCF-04-27129 and NSF-CCF-09-64401.\nar X\niv :1\n10 4.\n20 18\nv1 [\ncs .A\nI] 1\nThis suggests that, from a purely statistical viewpoint, it may be worthwhile to attempt to jointly optimize g and w on the empirical data.\nHowever, the issue of computationally efficiently estimating both g and w (and still achieving a good statistical rate) is more delicate, and is the focus of this work. We note that this is not a trivial problem: in general, the joint estimation problem is highly non-convex, and despite a significant body of literature on the problem, existing methods are usually based on heuristics, which are not guaranteed to converge to a global optimum (see for instance [WHI93, HH94, MHS98, NT04, RWY08]). We note that recently, [SSSS10] presented a kernel-based method which does allow (improper) learning of certain types of GLM\u2019s and SIM\u2019s, even in an agnostic setting where no assumptions are made on the underlying distribution. On the flip side, the formal computational complexity guarantee degrades super-polynomially with the norm of w, which [SSSS10] show is provably unavoidable in their setting.\nThe recently proposed Isotron algorithm [KS09] provides the first provably efficient method for learning GLMs and SIMs, under the common assumption that g\u22121 is monotonic and Lipschitz, and assuming the data corresponds to the model. The algorithm attained both polynomial sample and computational complexity, with a sample size dependence that does not depend explicitly on the dimension. The algorithm is a variant of the \u201cgradient-like\u201d perceptron algorithm, with the added twist that on each update, an isotonic regression procedure is performed on the linear predictions. Recall that isotonic regression is a procedure which finds the best monotonic one dimensional regression function. Here, the well-known Pool Adjacent Violator (PAV) algorithm provides a computationally efficient method for this task.\nUnfortunately, a cursory inspection of the Isotron algorithm suggests that, while it is computationally efficient, it is very wasteful statistically, as each iteration of the algorithm throws away all previous training data and requests new examples. Our intuition is that the underlying technical reasons for this are due to the fact that the PAV algorithm need not return a function with a bounded Lipschitz constant. Furthermore, empirically, it not clear how deleterious this issue may be.\nThis work seeks to address these issues both theoretically and practically. We present two algorithms, the GLM-tron algorithm for learning GLMs with a known monotonic and Lipschitz g\u22121, and the L-Isotron algorithm for the more general problem of learning SIMs, with an unknown monotonic and Lipschitz g\u22121. Both algorithms are practical, parameter-free and are provably efficient, both statistically and computationally. Moreover, they are both easily kernelizable. In addition, we investigate both algorithms empirically, and show they are both feasible approaches. Furthermore, our results show that the original Isotron algorithm (ran on the same data each time) is perhaps also effective in several cases, even though the PAV algorithm does not have a Lipschitz constraint.\nMore generally, it is interesting to note how the statistical assumption that the data are in fact generated by some GLM leads to an efficient estimation procedure, despite it being a non-convex problem. Without making any assumptions, i.e. in the agnostic setting, this problem is at least hard as learning parities with noise."}, {"heading": "2 Setting", "text": "We assume the data (x, y) are sampled i.i.d. from a distribution supported on Bd \u00d7 [0, 1], where Bd = {x \u2208 Rd : \u2016x\u2016 \u2264 1} is the unit ball in d-dimensional Euclidean space. Our algorithms and analysis also apply to the case where Bd is the unit ball in some high (or infinite)-dimensional kernel feature space. We assume there is a fixed vector w, such that \u2016w\u2016 \u2264W , and a non-decreasing 1-Lipschitz function u : R\u2192 [0, 1], such that E[y|x] = u(w \u00b7x) for all x. Note that u plays the same role here as g\u22121 in generalized linear models, and we use this notation for convenience. Also, the restriction that u is 1-Lipschitz is without loss of generality, since the norm of w is arbitrary (an equivalent restriction is that \u2016w\u2016 = 1 and that u is W -Lipschitz for an arbitrary W ).\nOur focus is on approximating the regression function well, as measured by the squared loss. For a real valued function h : Bd \u2192 [0, 1], define\nerr(h) = E(x,y) [ (h(x)\u2212 y)2 ] \u03b5(h) = err(h)\u2212 err(E[y|x])\n= E(x,y) [ (h(x)\u2212 u(w \u00b7 x))2 ]\nAlgorithm 1 GLM-tron\nInput: data \u3008(xi, yi)\u3009mi=1 \u2208 Rd \u00d7 [0, 1], u : R\u2192 [0, 1]. w1 := 0; for t = 1, 2, . . . do ht(x) := u(wt \u00b7 x);\nwt+1 := wt + 1\nm m\u2211 i=1 (yi \u2212 u(wt \u00b7 xi))xi;\nend for\nerr(h) measures the error of h, and \u03b5(h) measures the excess error of h compared to the Bayes-optimal predictor x 7\u2192 u(w \u00b7 x). Our goal is to find h such that \u03b5(h) (equivalently, err(h)) is as small as possible.\nIn addition, we define the empirical counterpart e\u0302rr(h), \u03b5\u0302(h), based on a sample (x1, y1), . . . , (xm, ym), to be\ne\u0302rr(h) = 1\nm m\u2211 i=1 (h(xi)\u2212 yi)2\n\u03b5\u0302(h) = 1\nm m\u2211 i=1 (h(xi)\u2212 u(w \u00b7 xi))2.\nNote that \u03b5\u0302 is the standard fixed design error (as this error conditions on the observed x\u2019s). Our algorithms work by iteratively constructing hypotheses ht of the form ht(x) = ut(wt \u00b7 x), where ut is a non-decreasing, 1-Lipschitz function, and wt is a linear predictor. The algorithmic analysis provides conditions under which \u03b5\u0302(ht) is small, and using statistical arguments, one can guarantee that \u03b5(ht) would be small as well.\nTo simplify the presentation of our results, we use the standard O(\u00b7) notation, which always hides only universal constants.\n3 The GLM-tron Algorithm\nWe begin with the simpler case, where the transfer function u is assumed to be known (e.g. a sigmoid), and the problem is estimating w properly. We present a simple, parameter-free, perceptron-like algorithm, GLM-tron, which efficiently finds a close-to-optimal predictor. We note that the algorithm works for arbitrary non-decreasing, Lipschitz functions u, and thus covers most generalized linear models. The pseudo-code appears as Algorithm 1.\nTo analyze the performance of the algorithm, we show that if we run the algorithm for sufficiently many iterations, one of the predictors ht obtained must be nearly-optimal, compared to the Bayes-optimal predictor.\nTheorem 1. Suppose (x1, y1), . . . , (xm, ym) are drawn independently from a distribution supported on Bd \u00d7 [0, 1], such that E[y|x] = u(w \u00b7 x), where \u2016w\u2016 \u2264W , and u : R\u2192 [0, 1] is a known non-decreasing 1-Lipschitz function. Then for any \u03b4 \u2208 (0, 1), the following holds with probability at least 1 \u2212 \u03b4: there exists some iteration t < O(W \u221a m/ log(1/\u03b4)) of GLM-tron such that the hypothesis ht(x) = u(wt \u00b7 x) satisfies\nmax{\u03b5\u0302(ht), \u03b5(ht)} \u2264 O\n(\u221a W 2 log(m/\u03b4)\nm\n) .\nIn particular, the theorem implies that some ht has \u03b5(ht) = O(1/ \u221a m). Since \u03b5(ht) equals err(ht) up to a constant, we can easily find an appropriate ht by using a hold-out set to estimate err(ht), and picking the one with the lowest value.\nThe proof is along similar lines (but somewhat simpler) than the proof of our subsequent Thm. 2. The rough idea of the proof is showing that at each iteration, if \u03b5\u0302(ht) is not small, then the squared\ndistance \u2225\u2225wt+1 \u2212 wt\u2225\u22252 is substantially smaller than \u2016wt \u2212 w\u20162. Since this is bounded below by 0, and\nAlgorithm 2 L-Isotron\nInput: data \u3008(xi, yi)\u3009mi=1 \u2208 Rd \u00d7 [0, 1]. w1 := 0; for t = 1, 2, . . . do ut := LPAV ((wt \u00b7 x1, y1), . . . , (wt \u00b7 xm, ym))\nwt+1 := wt + 1\nm m\u2211 i=1 (yi \u2212 ut(wt \u00b7 xi))xi\nend for\n\u2225\u2225w0 \u2212 w\u2225\u22252 \u2264 W 2, there is an iteration (arrived at within reasonable time) such that the hypothesis ht at that iteration is highly accurate. The proof is provided in Appendix A.1.\n4 The L-Isotron Algorithm\nWe now present L-Isotron, in Algorithm 2, which is applicable to the harder setting where the transfer function u is unknown, except for it being non-decreasing and 1-Lipschitz. This corresponds to the semi-parametric setting of single index models.\nThe algorithm that we present is again simple and parameter-free. The main difference compared to GLM-tron algorithm is that now the transfer function must also be learned, and the algorithm keeps track of a transfer function ut which changes from iteration to iteration. The algorithm is also rather similar to the Isotron algorithm [KS09], with the main difference being that instead of applying the PAV procedure to fit an arbitrary monotonic function at each iteration, we use a different procedure, LPAV, which fits a Lipschitz monotonic function. This difference is the key which allows us to make the algorithm practical while maintaining non-trivial guarantees (getting similar guarantees for the Isotron required a fresh training sample at each iteration).\nThe LPAV procedure takes as input a set of points (z1, y1), . . . , (zm, ym) in R2, and fits a non-decreasing, 1-Lipschitz function u, which minimizes \u2211m i=1(u(zi)\u2212 yi)2. This problem has been studied in the literature, and we followed the method of [YW09] in our empirical studies. The running time of the method proposed in [YW09] is O(m2). While this can be slow for large-scale datasets, we remind the reader that this is a one-dimensional fitting problem, and thus a highly accurate fit can be achieved by randomly subsampling the data (the details of this argument, while straightforward, are beyond the scope of the paper).\nWe now turn to the formal analysis of the algorithm. The formal guarantees parallel those of the previous subsection. However, the rates achieved are somewhat worse, due to the additional difficulty of simultaneously estimating both u and w. It is plausible that these rates are sharp for information-theoretic reasons, based on the 1-dimensional lower bounds in [Zha02a] (although the assumptions are slightly different, and thus they do not directly apply to our setting).\nTheorem 2. Suppose (x1, y1), . . . , (xm, ym) are drawn independently from a distribution supported on Bd \u00d7 [0, 1], such that E[y|x] = u(w \u00b7 x), where \u2016w\u2016 \u2264 W , and u : R \u2192 [0, 1] is an unknown non-decreasing 1-Lipschitz function. Then the following two bounds hold:\n1. (Dimension-dependent) With probability at least 1\u2212\u03b4, there exists some iteration t < O ((\nWm d log(Wm/\u03b4) )1/3) of L-Isotron such that\nmax{\u03b5\u0302(ht), \u03b5(ht)} \u2264 O\n(( dW 2 log(Wm/\u03b4)\nm\n)1/3) .\n2. (Dimension-independent) With probability at least 1\u2212\u03b4, there exists some iteration t < O ((\nWm log(m/\u03b4) )1/4) of L-Isotron such that\nmax{\u03b5\u0302(ht), \u03b5(ht)} \u2264 O\n(( W 2 log(m/\u03b4)\nm\n)1/4)\nAs in the case of Thm. 1, one can easily find ht which satisfies the theorem\u2019s conditions, by running the L-Isotron algorithm for sufficiently many iterations, and choosing the hypothesis ht which minimizes err(ht) based on a hold-out set."}, {"heading": "5 Proofs", "text": ""}, {"heading": "5.1 Proof of Thm. 2", "text": "First we need a property of the LPAV algorithm that is used to find the best one-dimensional non-decreasing 1-Lipschitz function. Formally, this problem can be defined as follows: Given as input \u3008{zi, yi}\u3009mi=1 \u2208 [\u2212W,W ]\u00d7 [0, 1] the goal is to find y\u03021, . . . , y\u0302m such that\n1\nm m\u2211 i=1 (y\u0302i \u2212 yi)2, (1)\nis minimal, under the constraint that y\u0302i = u(zi) for some non-decreasing 1-Lipschitz function u : [\u2212W,W ] 7\u2192 [0, 1]. After finding such values, LPAV obtains an entire function u by interpolating linearly between the points. Assuming that zi are in sorted order, this can be formulated as a quadratic problem with the following constraints:\ny\u0302i \u2212 y\u0302i+1 \u2264 0 1 \u2264 i < m (2) y\u0302i+1 \u2212 y\u0302i \u2212 (zi+1 \u2212 zi) \u2264 0 1 \u2264 i < m (3)\nLemma 1. Let (z1, y1), . . . , (zm, ym) be input to LPAV where zi are increasing and yi \u2208 [0, 1]. Let y\u03021, . . . , y\u0302m be the output of LPAV. Let f be any function such that f(\u03b2)\u2212 f(\u03b1) \u2265 \u03b2 \u2212 \u03b1, for \u03b2 \u2265 \u03b1, then\nm\u2211 i=1 (yi \u2212 y\u0302i)(f(y\u0302i)\u2212 zi) \u2265 0\nProof. We first note that \u2211m j=1(yj\u2212 y\u0302j) = 0, since otherwise we could have found other values for y\u03021, . . . , y\u0302m which make (1) even smaller. So for notational convenience, let y\u03020 = 0, and we may assume w.l.o.g. that f(y\u03020) = 0. Define \u03c3i = \u2211m j=i(yj \u2212 y\u0302j). Then we have\nm\u2211 i=1 (yi \u2212 y\u0302i)(f(y\u0302i)\u2212 zi) =\nm\u2211 i=1 \u03c3i((f(y\u0302i)\u2212 zi)\u2212 (f(y\u0302i\u22121)\u2212 zi\u22121)). (4)\nSuppose that \u03c3i < 0. Intuitively, this means that if we could have decreased all values y\u0302i+1, . . . y\u0302m by an infinitesimal constant, then the objective function (1) would have been reduced, contradicting the optimality of the values. This means that the constraint y\u0302i \u2212 y\u0302i+1 \u2264 0 must be tight, so we have (f(y\u0302i+1) \u2212 zi+1) \u2212 (f(y\u0302i)\u2212zi) = \u2212zi+1+zi \u2264 0 (this argument is informal, but can be easily formalized using KKT conditions). Similarly, when \u03c3i > 0, then the constraint y\u0302i+1\u2212 y\u0302i\u2212 (zi+1\u2212 zi) \u2264 0 must be tight, hence f(y\u0302i+1)\u2212f(y\u0302i) \u2265 y\u0302i+1 \u2212 y\u0302i = (zi+1 \u2212 zi) \u2265 0. So in either case, each summand in (4) must be non-negative, leading to the required result.\nWe also use another result, for which we require a bit of additional notation. At each iteration of the L-Isotron algorithm, we run the LPAV procedure based on the training sample (x1, y1), . . . , (xm, ym) and the current direction wt, and get a non-decreasing Lipschitz function ut. Define\n\u2200i y\u0302ti = ut(wt \u00b7 xi).\nRecall that w, u are such that E[y|x] = u(w\u00b7x), and the input to the L-Isotron algorithm is (x1, y1), . . . , (xm, ym). Define\n\u2200i y\u0304i = u(w \u00b7 xi)\nto be the expected value of each yi. Clearly, we do not have access to y\u0304i. However, consider a hypothetical call to LPAV with inputs \u3008(wt \u00b7 xi, y\u0304i)\u3009mi=1, and suppose LPAV returns the function u\u0303t. In that case, define\n\u2200i y\u0303ti = u\u0303t(wt \u00b7 xi).\nfor all i. Our proof uses the following proposition, which relates the values y\u0302ti (the values we can actually compute) and y\u0303ti (the values we could compute if we had the conditional means of each yi). The proof of Proposition 1 is somewhat lengthy and requires additional technical machinery, and is therefore relegated to Appendix B.\nProposition 1. With probability at least 1 \u2212 \u03b4 over the sample {(xi, yi)}mi=1, it holds for any t that 1 m \u2211m i=1 |y\u0302ti \u2212 y\u0303ti | is at most the minimum of\nO\n(( dW 2 log(Wm/\u03b4)\nm\n)1/3)\nand\nO\n(( W 2 log(m/\u03b4)\nm\n)1/4) .\nThe third auxiliary result we\u2019ll need is the following, which is well-known (see for example [STC04], Section 4.1).\nLemma 2. Suppose z1, . . . , zm are i.i.d. 0-mean random variables in a Hilbert space, such that Pr(\u2016xi\u2016 \u2264 1) = 1. Then with probability at least 1\u2212 \u03b4,\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 zi \u2225\u2225\u2225\u2225\u2225 \u2264 2 ( 1 + \u221a log(1/\u03b4)/2\u221a m )\nWith these auxiliary results in hand, we can now turn to prove Thm. 2 itself. The heart of the proof is the following lemma, which shows that the squared distance \u2016wt \u2212 w\u20162 between wt and the true direction w decreases at each iteration at a rate which depends on the error of the hypothesis \u03b5\u0302(ht):\nLemma 3. Suppose that \u2016wt \u2212 w\u2016 \u2264W and \u2016(1/m) \u2211m i=1(yi \u2212 y\u0304i)xi\u2016 \u2264 \u03b71 and (1/m) \u2211m i=1 |y\u0302ti \u2212 y\u0303ti | \u2264 \u03b72.\nThen \u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2265 \u03b5\u0302(ht)\u2212 5W (\u03b71 + \u03b72) Proof. We have \u2225\u2225wt+1 \u2212 w\u2225\u22252 2 = \u2225\u2225wt+1 \u2212 wt + wt \u2212 w\u2225\u22252 2\n= \u2225\u2225wt+1 \u2212 wt\u2225\u22252 2 + \u2225\u2225wt \u2212 w\u2225\u22252 2 + 2(wt+1 \u2212 wt) \u00b7 (wt \u2212 w)\nSince wt+1 \u2212 wt = (1/m) \u2211m i=1(yi \u2212 y\u0302ti)xi, substituting this above and rearranging the terms we get,\u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252\n= 2\nm m\u2211 i=1 (yi \u2212 y\u0302ti)(w \u00b7 xi \u2212 wt \u00b7 xi)\u2212 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 y\u0302ti)xi \u2225\u2225\u2225\u2225\u2225 2 . (5)\nConsider the first term above,\n2\nm m\u2211 i=1 (yi \u2212 y\u0302ti)(w \u00b7 xi \u2212 wt \u00b7 xi)\n=\n( 2\nm m\u2211 i=1 (yi \u2212 y\u0304i)xi\n) \u00b7 (w \u2212 wt) (6)\n+ 2\nm m\u2211 i=1 (y\u0304i \u2212 y\u0303ti)(w \u00b7 xi \u2212 wt \u00b7 xi) (7)\n+ 2\nm m\u2211 i=1 (y\u0303ti \u2212 y\u0302ti)(w \u00b7 xi \u2212 wt \u00b7 xi) (8)\nThe term (6) is at least \u22122W\u03b71, the term (8) is at least \u22122W\u03b72 (since |(w\u2212wt) \u00b7xi| \u2264W ). We thus consider the remaining term (7). Letting u be the true transfer function, suppose for a minute it is strictly increasing, so its inverse u\u22121 is well defined. Then we have\n2\nm m\u2211 i=1 (y\u0304i \u2212 y\u0303ti)(w \u00b7 xi \u2212 wt \u00b7 xi)\n= 2\nm m\u2211 i=1 (y\u0304i \u2212 y\u0303ti)(w \u00b7 xi \u2212 u\u22121(y\u0303ti))\n+ 2\nm m\u2211 i=1 (y\u0304i \u2212 y\u0303ti)(u\u22121(y\u0303ti)\u2212 wt \u00b7 xi)\nThe second term in the expression above is positive by Lemma 1. As to the first term, it is equal to 2 m \u2211m i=1(y\u0304i\u2212y\u0303ti)(u\u22121(y\u0304i)\u2212u\u22121(y\u0303ti)), which by the Lipschitz property of u is at least 2 m \u2211m i=1(y\u0304i\u2212y\u0303ti)2 = 2\u03b5\u0302(h\u0303t). Plugging this in the above, we get\n2\nm m\u2211 i=1 (yi \u2212 y\u0302ti)(w \u00b7 xi \u2212 wt \u00b7 xi) \u2265 2\u03b5\u0302(h\u0303t)\u2212 2W (\u03b71 + \u03b72) (9)\nThis inequality was obtained under the assumption that u is strictly increasing, but it is not hard to verify that the same would hold even if u is only non-decreasing.\nThe second term in (5) can be bounded, using some tedious technical manipulations (see (14) and (15) in the supplementary material), by \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 y\u0302ti)xi \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u03b5\u0302(ht) + 3W\u03b71 (10)\nCombining (9) and (10)) in (5), we get\u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252\u2265 2\u03b5\u0302(h\u0303t)\u2212 \u03b5\u0302(ht)\u2212W (5\u03b71 + 2\u03b72) (11) Now, we claim that\n\u03b5\u0302(h\u0303t)\u2212 \u03b5\u0302(ht) \u2265 \u2212 2 m m\u2211 i=1 |y\u0302ti \u2212 y\u0303ti | \u2265 \u22122\u03b72,\nsince\n\u03b5\u0302(h\u0303t) = 1\nm m\u2211 i=1 (y\u0303ti \u2212 y\u0304i)2\n= 1\nm m\u2211 i=1 (y\u0303ti \u2212 y\u0302ti + y\u0302ti \u2212 y\u0304i)2\n= 1\nm m\u2211 i=1 (y\u0302ti \u2212 y\u0304i)2\n+\n( 1\nm m\u2211 i=1 (y\u0303ti \u2212 y\u0302ti) ) (y\u0303ti + y\u0302 t i \u2212 2y\u0304i)\nand we have that |y\u0303ti + y\u0302ti \u2212 2y\u0304i| \u2264 2. Plugging this into (11) leads to the desired result.\nThe bound on \u03b5\u0302(ht) in Thm. 2 now follows from Lemma 3. Using the notation from Lemma 3, \u03b71 can be set to the bound in Lemma 2, since {(yi \u2212 y\u0304i)xi}mi=1 are i.i.d. 0-mean random variables with norm bounded by 1. Also, \u03b72 can be set to any of the bounds in Proposition 1. \u03b72 is clearly the dominant term. Thus, we\nget that Lemma 3 holds, so either \u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2264 \u2016wt \u2212 w\u20162 \u2212W (\u03b71 + \u03b72), or \u03b5\u0302(ht) \u2264 3W (\u03b71 + \u03b72). If the\nlatter is the case, we are done. If not, since \u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2265 0, and \u2225\u2225w0 \u2212 w\u2225\u22252 = \u2016w\u20162 \u2264 W 2, there can be at most W 2/(W (\u03b71 + \u03b72)) = W/(\u03b71 + \u03b72) iterations before \u03b5\u0302(h t) \u2264 6W\u03b7. Plugging in the values for \u03b71, \u03b72 results in the bound on \u03b5\u0302(ht). Finally, to get a bound on \u03b5(ht), we utilize the following uniform convergence lemma:\nLemma 4. Suppose that E[y|x] = u(\u3008w, x\u3009) for some non-decreasing 1-Lipschitz u and w such that \u2016w\u2016 \u2264W . Then with probability at least 1 \u2212 \u03b4 over a sample (x1, y1), . . . , (xm, ym), the following holds simultaneously for any function h(x) = u\u0302(w\u0302 \u00b7 x) such that \u2016w\u0302\u2016 \u2264W and a non-decreasing and 1-Lipschitz function u\u0302:\n|\u03b5(h)\u2212 \u03b5\u0302(h)| \u2264 O\n(\u221a W 2 log(m/\u03b4)\nm\n) .\nThe proof of the lemma uses a covering number argument, and is shown as part of the more general Lemma 7 in the supplementary material. This lemma applies in particular to ht. Combining this with the bound on \u03b5\u0302(ht), and using a union bound, we get the result on \u03b5(ht) as well."}, {"heading": "6 Experiments", "text": "In this section, we present an empirical study of the GLM-tron and the L-Isotron algorithms. The first experiment we performed is a synthetic one, and is meant to highlight the difference between L-Isotron and the Isotron algorithm of [KS09]. In particular, we show that attempting to fit the transfer function without any Lipschitz constraints may cause Isotron to overfit, complementing our theoretical findings. The second set of experiments is a comparison between GLM-tron, L-Isotron and several competing approaches. The goal of these experiments is to show that our algorithms perform well on real-world data, even when the distributional assumption required for their theoretical guarantees does not precisely hold.\n6.1 L-Isotron vs Isotron\nAs discussed earlier, our L-Isotron algorithm (Algorithm 2) is similar to the Isotron algorithm of [KS09], with two main differences: First, we apply LPAV at each iteration to find the best Lipschitz monotonic function to the data, while they apply the PAV (Pool Adjacent Violator) procedure to fit a monotonic (generally non-Lipschitz) function. The second difference is the theoretical guarantees, which in the case of Isotron required working with a fresh training sample at each iteration.\nWhile the first difference is inherent, the second difference is just an outcome of the analysis. In particular, one might still try and apply the Isotron algorithm, using the same training sample at each iteration. While\nwe do not have theoretical guarantees for this algorithm, it is computationally efficient, and one might wonder how well it performs in practice. As we see later on, it actually performs quite well on the datasets we examined. However, in this subsection we provide a simple example, which shows that sometimes, the repeated fitting of a non-Lipschitz function, as done in the Isotron algorithm, can cause overfit and thus hurt performance, compared to fitting a Lipschitz function as done in the L-Isotron algorithm.\nWe constructed a synthetic dataset as follows: In a high dimensional space (d = 400), we let w = (1, 0, . . . , 0) be the true direction. The transfer function is u(t) = (1+t)/2. Each data point x is constructed as follows: the first coordinate is chosen uniformly from the set {\u22121, 0, 1}, and out of the remaining coordinates, one is chosen uniformly at random and is set to 1. All other coordinates are set to 0. The y values are chosen at random from {0, 1}, so that E[y|x] = u(w \u00b7 x). We used a sample of size 600 to evaluate the performance of the algorithms.\nIn the synthetic example we construct, the first attribute is the only relevant attribute. However, because of the random noise in the y values, Isotron tends to overfit using the irrelevant attributes. At data points where the true mean value u(w \u00b7x) equals 0.5, Isotron (which uses PAV) tries to fit the value 0 or 1, whichever is observed. On the other hand, L-Isotron (which uses LPAV) predicts this correctly as close to 0.5, because of the Lipschitz constraint. Figure 1 shows the link functions predicted by L-Isotron and Isotron on this dataset. Repeating the experiment 10 times, the error of L-Isotron, normalized by the variance of the y values, was 0.338\u00b10.058, while the normalized error for the Isotron algorithm was 0.526\u00b10.175. In addition, we observed that L-Isotron performed better rather consistently across the folds - the difference between the normalized error of Isotron and L-Isotron was 0.189\u00b1 0.139."}, {"heading": "6.2 Real World Datasets", "text": "We now turn to describe the results of experiments performed on several UCI datasets. We chose the following 5 datasets: communities, concrete, housing, parkinsons, and wine-quality.\nOn each dataset, we compared the performance of L-Isotron (L-Iso) and GLM-tron (GLM-t) with Isotron\nand several other algorithms. These include standard logistic regression (Log-R), linear regression (Lin-R) and a simple heuristic algorithm (SIM) for single index models, along the lines of standard iterative maximumlikelihood procedures for these types of problems (e.g., [Cos83]). The algorithm works by iteratively fixing the direction w and finding the best transfer function u, and then fixing u and optimizing w via gradient descent. For each of the algorithms we performed 10-fold cross validation, using 1 fold each time as the test set, and we report averaged results across the folds.\nTable 1 shows the mean squared error of all the algorithms across ten folds normalized by the variance in the y values. Table 2 shows the difference between squared errors between the algorithms across the folds. The results indicate that the performance of L-Isotron and GLM-tron (and even Isotron) is comparable to other regression techniques and in many cases also slightly better. This suggests that these algorithms should work well in practice, while enjoying non-trivial theoretical guarantees.\nIt is also illustrative to see how the transfer functions found by the two algorithms, L-Isotron and Isotron, compare to each other. In Figure 2, we plot the transfer function for concrete and communities. The plots illustrate the fact that Isotron repeatedly fits a non-Lipschitz function resulting in a piecewise constant function, which is less intuitive than the smoother, Lipschitz transfer function found by the L-Isotron algorithm."}, {"heading": "A Appendix", "text": "A.1 Proof of Thm. 1\nThe reader is referred to GLM-tron (Alg. 1) for notation used in this section. The main lemma shows that as long as the error of the current hypothesis is large the distance of our predicted direction vector wt from the ideal direction w decreases.\nLemma 5. At iteration t in GLM-tron, suppose \u2016wt \u2212 w\u2016 \u2264W , then if \u2016(1/m) \u2211m i=1(yi \u2212 u(w \u00b7 xi))xi\u2016w \u2264\n\u03b7, then \u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2265 \u03b5\u0302(ht)\u2212 5W\u03b7 Proof. We have\n\u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252 = 2 m m\u2211 i=1 (yi \u2212 u(wt \u00b7 xi))(w \u00b7 xi \u2212 wt \u00b7 xi)\u2212 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 u(wt \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 2 . (12)\nConsider the first term above,\n2\nm m\u2211 i=1 (yi\u2212u(wt\u00b7xi))(w\u00b7xi\u2212wt\u00b7xi) = 2 m m\u2211 i=1 (u(w\u00b7xi)\u2212u(wt\u00b7xi))(w\u00b7xi\u2212wt\u00b7xi)+ 2 m ( m\u2211 i=1 (yi \u2212 u(w \u00b7 xi))xi ) \u00b7(w\u2212wt).\nUsing the fact that u is non-decreasing and 1-Lipschitz (for the first term) and \u2016w \u2212 wt\u2016 \u2264 W and \u2016(1/m) \u2211m i=1(yi \u2212 u(w \u00b7 xi))xi\u2016 \u2264 \u03b7, we can lower bound this by\n2\nm m\u2211 i=1 (u(w \u00b7 xi)\u2212 u(wt \u00b7 xi))2 \u2212 2W\u03b7 \u2265 2\u03b5\u0302(ht)\u2212 2W\u03b7. (13)\nFor the second term in (12), we have\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 u(wt \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 u(w \u00b7 xi) + u(w \u00b7 xi)\u2212 u(wt \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 2\n\u2264 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 u(w \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 2 + 2 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 u(w \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 \u00d7 \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (u(w \u00b7 xi)\u2212 u(wt \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 +\n\u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (u(w \u00b7 xi)\u2212 u(wt \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 2\n(14)\nUsing the fact that \u2016(1/m) \u2211m i=1(yi \u2212 u(w \u00b7 xi))xi\u2016 \u2264 \u03b7, and using Jensen\u2019s inequality to show that\n\u2016(1/m) \u2211m i=1(u(w \u00b7 xi)\u2212 u(wt \u00b7 xi))xi\u2016 2 \u2264 (1/m) \u2211m i=1(u(w \u00b7xi)\u2212u(wt \u00b7xi))2 = \u03b5\u0302(ht), and assuming W \u2265 1,\nwe get \u2225\u2225\u2225\u2225\u2225 1m m\u2211 i=1 (yi \u2212 u(w \u00b7 xi))xi \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u03b5\u0302(ht) + 3W\u03b7 (15)\nCombining (13) and (15) in (12), we get\u2225\u2225wt \u2212 w\u2225\u22252 \u2212 \u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2265 \u03b5\u0302(ht)\u2212 5W\u03b7 The bound on \u03b5\u0302(ht) for some t now follows from Lemma 5. Let \u03b7 = 2(1 + \u221a log(1/\u03b4)/1)/ \u221a m. Notice that (yi\u2212 u(w \u00b7 xi))xi for all i are i.i.d. 0-mean random variables with norm bounded by 1, so using Lemma 2, \u2016(1/m)\n\u2211m i=1(yi \u2212 u(w \u00b7 xi))xi\u2016 \u2264 \u03b7. Now using Lemma 5, at each iteration of algorithm GLM-tron, either\u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2264 \u2016wt \u2212 w\u20162 \u2212 W\u03b7, or \u03b5\u0302(ht) \u2264 6W\u03b7. If the latter is the case, we are done. If not, since\n\u2225\u2225wt+1 \u2212 w\u2225\u22252 \u2265 0, and \u2225\u2225w0 \u2212 w\u2225\u22252 = \u2016w\u20162 \u2264W 2, there can be at most W 2/(W\u03b7) = W/(\u03b7) iterations before \u03b5\u0302(ht) \u2264 6W\u03b7. Overall, there is some ht such that\n\u03b5\u0302(ht) \u2264 O\n(\u221a W 2 log(1/\u03b4)\nm\n) .\nIn addition, we can reduce this to a high-probability bound on \u03b5(ht) using Lemma 4, which is applicable since \u2016wt\u2016 \u2264W . Using a union bound, we get a bound which holds simultaneously for \u03b5\u0302(ht) and \u03b5(ht)."}, {"heading": "B Proof of Proposition 1", "text": "To prove the proposition, we actually prove a more general result. Define the function class\nU = {u : [\u2212W,W ]\u2192 [0, 1] : u 1-Lipschitz}.\nand W = {x 7\u2192 \u3008x,w\u3009 : w \u2208 Rd, \u2016w\u2016 \u2264W},\nwhere d is possibly infinite (for instance, if we are using kernels). It is easy to see that the proposition follows from the following uniform convergence guarantee:\nTheorem 3. With probability at least 1\u2212 \u03b4, for any fixed w \u2208 W, if we let\nu\u0302 = arg min u\u2208U\n1\nm m\u2211 i=1 (u(w \u00b7 xi)\u2212 yi)2,\nand define\nu\u0303 = arg min u\u2208U\n1\nm m\u2211 i=1 (u(w \u00b7 xi)\u2212 E[y|xi])2,\nthen\n1\nm m\u2211 i=1 |u\u0302(w\u00b7xi)\u2212u\u0303(w\u00b7xi)| \u2264 O\n( min {( dW 3/2 log(Wm/\u03b4)\nm\n)1/3 + \u221a W 2 log(m/\u03b4)\nm ,\n( W 2 log(m/\u03b4)\nm\n)1/4}) .\nTo prove the theorem, we use the concept of (\u221e-norm) covering numbers. Given a function class F on some domain and some > 0, we define N\u221e( ,F) to be the smallest size of a covering set F \u2032 \u2286 F , such that for any f \u2208 F , there exists some f \u2032 \u2208 F for which supx |f(x) \u2212 f \u2032(x)| \u2264 . In addition, we use a more refined notion of an \u221e-norm covering number, which deals with an empirical sample of size m. Formally, define N\u221e( ,F ,m) to be the smallest integer n, such that for any x1, . . . , xm, one can construct a covering set F \u2032 \u2286 F of size at most n, such that for any f \u2208 F , there exists some f \u2032 \u2208 F such that maxi=1,...,m |f(xi)\u2212 f \u2032(xi)| \u2264 .\nLemma 6. Assuming m, 1/ ,W \u2265 1, we have the following covering number bounds:\n1. N\u221e( ,U) \u2264 1 2 2W/ .\n2. N\u221e( ,W) \u2264 ( 1 + 2W )d .\n3. N\u221e( ,U \u25e6W) \u2264 2 2 4W/ ( 1 + 4W )d .\n4. N\u221e( ,U \u25e6W,m) \u2264 2 (2m+ 1) 1+8W 2/ 2\nProof. We start with the first bound. Discretize [\u2212W,W ]\u00d7 [0, 1] to a two-dimensional grid {\u2212W + a, b}a=0,...,2W/ ,b=0,...,1/ . It is easily verified that for any function u \u2208 U , we can define a piecewise linear function u\u2032, which passes through points in the grid, and in between the points, is either constant or linear with slope 1, and supx |u(x)\u2212u\u2032(x)| \u2264 . Moreover, all such functions are parameterized by their value at \u2212W , and whether they are sloping up or constant at any grid interval afterwards. Thus, their number can be coarsely upper bounded as 22W/ / .\nThe second bound in the lemma is a well known fact - see for instance pg. 63 in [Pis99]). The third bound in the lemma follows from combining the first two bounds, and using the Lipschitz property of u (we simply combine the two covers at an /2 scale, which leads to a cover at scale for U \u25e6W). To get the fourth bound, we note that by corollary 3 in [Zha02b]. N\u221e( ,W,m) \u2264 (2m + 1)1+W\n2/ 2 . Note that unlike the second bound in the lemma, this bound is dimension-free, but has worse dependence on W and . Also, we have N\u221e( ,U ,m) \u2264 N\u221e( ,U) \u2264 1 2\n2W/ by definition of covering numbers and the first bound in the lemma. Combining these two bounds, and using the Lipschitz property of u, we get\n2\n(2m+ 1)1+4W 2/ 224W/ .\nUpper bounding 24W/ by (2m+ 1)4W 2/ 2 , the the fourth bound in the lemma follows.\nLemma 7. With probability at least 1 \u2212 \u03b4 over a sample (x1, y1), . . . , (xm, ym) the following bounds hold simultaneously for any w \u2208 W, u, u\u2032 \u2208 U ,\u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 (u(w \u00b7 xi)\u2212 yi)2 \u2212 E [ (u(w \u00b7 x)\u2212 y)2 ]\u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a W 2 log(m/\u03b4) m ) ,\n\u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 (u(w \u00b7 xi)\u2212 E[y|xi])2 \u2212 E [ (u(w \u00b7 x)\u2212 E[y|x])2 ]\u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a W 2 log(m/\u03b4) m ) ,\n\u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 |u(w \u00b7 xi)\u2212 u\u2032(w \u00b7 xi)| \u2212 E [|u(w \u00b7 x)\u2212 u\u2032(w \u00b7 x)|] \u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a W 2 log(m/\u03b4) m )\nProof. Lemma 6 tells us that N\u221e( ,U \u25e6 W,m) \u2264 2 (2m + 1) 1+8W 2/ 2 . It is easy to verify that the same covering number bound holds for the function classes {(x, y) 7\u2192 (u(w \u00b7 x) \u2212 y)2 : u \u2208 U , w \u2208 W} and {x 7\u2192 (u(w \u00b7 x) \u2212 E[y|x])2 : u \u2208 U , w \u2208 W}, by definition of the covering number and since the loss function is 1-Lipschitz. In a similar manner, one can show that the covering number of the function class {x 7\u2192 |u(w \u00b7 x)\u2212 u\u2032(w \u00b7 x)| : u, u\u2032 \u2208 U , w \u2208 W} is at most 4 (2m+ 1)\n1+32W 2/ 2 . Now, one just need to use results from the literature which provides uniform convergence bounds given a covering number on the function class. In particular, combining a uniform convergence bound in terms of the Rademacher complexity of the function class (e.g. Theorem 8 in [BM02]), and a bound on the Rademacher complexity in terms of the covering number, using an entropy integral (e.g., Lemma A.3 in [SST10]), gives the desired result.\nLemma 8. With probability at least 1\u2212 \u03b4 over a sample (x1, y1), . . . , (xm, ym), the following holds simultaneously for any w \u2208 W: if we let\nu\u0302w(\u3008w, \u00b7\u3009) = arg min u\u2208U\n1\nm m\u2211 i=1 (u(w \u00b7 xi)\u2212 yi)2\ndenote the empirical risk minimizer with that fixed w, then\nE(u\u0302w(w \u00b7 x)\u2212 y)2 \u2212 inf u\u2208U E(u(w \u00b7 x)\u2212 y)2 \u2264 O\n( W ( d log(Wm/\u03b4)\nm\n)2/3) ,\nProof. For generic losses and function classes, standard bounds on the the excess error typically scale as O(1/ \u221a m). However, we can utilize the fact that we are dealing with the squared loss to get better rates. In particular, using Theorem 4.2 in [Men02], as well as the bound on N\u221e( ,U) from Lemma 6, we get that for any fixed w, with probability at least 1\u2212 \u03b4,\nE(u\u0302w(w \u00b7 x)\u2212 y)2 \u2212 inf u\u2208U E(u(w \u00b7 x)\u2212 y)2 \u2264 O\n( W ( log(1/\u03b4)\nm\n)2/3) .\nTo get a statement which holds simultaneously for any w, we apply a union bound over a covering set of W. In particular, by Lemma 6, we know that we can coverW by a setW \u2032 of size at most (1 + 2W/ )d, such that any element in W is at most -far (in an \u221e-norm sense) from some w\u2032 \u2208 W \u2032. So applying a union bound over W \u2032, we get that with probability at least 1\u2212 \u03b4, it holds simultaneously for any w\u2032 \u2208 W that\nE(u\u0302w\u2032(\u3008w\u2032, x\u3009)\u2212 y)2 \u2212 inf u E(u(\u3008w\u2032, x\u3009)\u2212 y)2 \u2264 O\n( W ( log(1/\u03b4) + d log(1 + 2W/ )\nm\n)2/3) . (16)\nNow, for any w \u2208 W, if we let w\u2032 denote the closest element in W \u2032, then u(w \u00b7 x) and u(\u3008w\u2032, x\u3009) are -close uniformly for any u \u2208 U and any x. From this, it is easy to see that we can extend (16) to hold for any W, with an additional O( ) element in the right hand side. In other words, with probability at least 1 \u2212 \u03b4, it holds simultaneously for any w \u2208 W that\nE(u\u0302w(w \u00b7 x)\u2212 y)2 \u2212 inf u E(u(w \u00b7 x)\u2212 y)2 \u2264 O\n( W ( log(2/\u03b4) + d log(1 + 2W/ )\nm\n)2/3) + .\nPicking (say) = 1/m provides the required result.\nLemma 9. Let F be a convex class of functions, and let f\u2217 = arg minf\u2208F E[(f(x) \u2212 y)2]. Suppose that E[y|x] \u2208 U \u25e6W. Then for any f \u2208 F , it holds that\nE[(f(x)\u2212 y)2]\u2212 E[(f\u2217(x)\u2212 y)2] \u2265 E [ (f(x)\u2212 f\u2217(x))2 ] \u2265 (E [|f(x)\u2212 f\u2217(x)|])2 .\nProof. It is easily verified that\nE[(f(x)\u2212 y)2]\u2212 E[(f\u2217(x)\u2212 y)2] = Ex[(f(x)\u2212 E[y|x])2 \u2212 (f\u2217(x)\u2212 E[y|x])2]. (17)\nThis implies that f\u2217 = arg minf\u2208F E[(f(x)\u2212 E[y|x])2]. Consider the L2 Hilbert space of square-integrable functions, with respect to the measure induced by the distribution on x (i.e., the inner product is defined as \u3008f, f \u2032\u3009 = Ex[f(x)f \u2032(x)]). Note that E[y|x] \u2208 U \u25e6W is a member of that space. Viewing E[y|x] as a function y(x), what we need to show is that\n\u2016f \u2212 y\u20162 \u2212 \u2016f\u2217 \u2212 y\u20162 \u2265 \u2016f \u2212 f\u2217\u20162 .\nBy expanding, it can be verified that this is equivalent to showing\n\u3008f\u2217 \u2212 y, f \u2212 f\u2217\u3009 \u2265 0.\nTo prove this, we start by noticing that according to (17), f\u2217 minimizes \u2016f \u2212 y\u20162 over F . Therefore, for any f \u2208 F and any \u2208 (0, 1),\n\u2016(1\u2212 )f\u2217 + f \u2212 y\u20162 \u2212 \u2016f\u2217 \u2212 y\u20162 \u2265 0, (18)\nas (1\u2212 )f\u2217 + f \u2208 F by convexity of F . However, the right hand side of (18) equals\n2 \u2016f \u2212 f\u2217\u20162 + 2 \u3008f\u2217 \u2212 y, f \u2212 f\u2217\u3009,\nso to ensure (18) is positive for any , we must have \u3008f\u2217 \u2212 y, f \u2212 f\u2217\u3009 \u2265 0. This gives us the required result, and establishes the first inequality in the lemma statement. The second inequality is just by convexity of the squared function.\nProof of Thm. 3. We bound 1m \u2211m i=1 |u\u0302(w \u00b7 x) \u2212 u\u0303(w \u00b7 x)| in two different ways, one which is dimensiondependent and one which is dimension independent. We begin with the dimension-dependent bound. For any fixed w, let u\u2217 be arg minu\u2208U E(u(w \u00b7 x)\u2212 y)2. We have from Lemma 8 that with probability at least 1\u2212 \u03b4, simultaneously for all w \u2208 W,\nE(u\u0302(w \u00b7 x)\u2212 y)2 \u2212 E(u\u2217(w \u00b7 x)\u2212 y)2 \u2264 O ( W ( d log(Wm/\u03b4)\nm\n)2/3) ,\nand by Lemma 9, this implies\nE[|u\u0302(w \u00b7 x)\u2212 u\u2217(w \u00b7 x)|] \u2264 O\n(( dW 3/2 log(Wm/\u03b4)\nm\n)1/3) . (19)\nNow, we note that since u\u2217 = arg minu\u2208U E(u(w \u00b7 x) \u2212 y)2, then u\u2217 = arg minu\u2208U E(u(w \u00b7 x) \u2212 E[y|x])2 as well. Again applying Lemma 8 and Lemma 9 in a similar manner, but now with respect to u\u0303, we get that with probability at least 1\u2212 \u03b4, simultaneously for all w \u2208 W,\nE[|u\u0303(w \u00b7 x)\u2212 u\u2217(w \u00b7 x)|] \u2264 O\n(( dW 3/2 log(Wm/\u03b4)\nm\n)1/3) . (20)\nCombining (19) and (20), with a union bound, we have\nE[|u\u0302(w \u00b7 x)\u2212 u\u0303(w \u00b7 x)|] \u2264 O\n(( dW 3/2 log(Wm/\u03b4)\nm\n)1/3) .\nFinally, we invoke the last inequality in Lemma 7, using a union bound, to get\n1\nm m\u2211 i=1 |u\u0302(w \u00b7 x)\u2212 u\u0303(w \u00b7 x)| \u2264 O\n(( dW 3/2 log(Wm/\u03b4)\nm\n)1/3 + \u221a W 2 log(m/\u03b4)\nm\n) .\nWe now turn to the dimension-independent bound. In this case, the covering number bounds are different, and we do not know how to prove an analogue to Lemma 8 (with rate faster than O(1/ \u221a m)). This leads to a somewhat worse bound in terms of the dependence on m. As before, for any fixed w, we let u\u2217 be arg minu\u2208U E[(u(w \u00b7x)\u2212y)2]. Lemma 7 tells us that the empirical\nrisk 1m \u2211m i=1(u(w \u00b7 xi)\u2212 yi)2 is concentrated around its expectation uniformly for any u,w. In particular,\u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 (u\u0302(w \u00b7 xi)\u2212 yi)2 \u2212 E [ u\u0302(w \u00b7 x)\u2212 y)2 ]\u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a W 2 log(m/\u03b4) m\n) as well as \u2223\u2223\u2223\u2223\u2223 1m m\u2211 i=1 (u\u2217(w \u00b7 xi)\u2212 yi)2 \u2212 E [ (u\u2217(w \u00b7 x)\u2212 y)2 ]\u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a W 2 log(m/\u03b4) m ) ,\nbut since u\u0302 was chosen to be the empirical risk minimizer, it follows that\nE [ (u\u0302(w \u00b7 xi)\u2212 yi)2 ] \u2212 E [ (u\u2217(w \u00b7 x)\u2212 y)2 ] \u2264 O\n(\u221a W 2 log(m/\u03b4)\nm\n) ,\nso by Lemma 9,\nE [|u\u2217(w \u00b7 x)\u2212 u\u0302(w \u00b7 x)|] \u2264 O\n(( W 2 log(m/\u03b4)\nm\n)1/4) (21)\nNow, it is not hard to see that if u\u2217 = arg minu\u2208U E[(u(w\u00b7x)\u2212y)2], then u\u2217 = arg minu\u2208U E[(u(w\u00b7x)\u2212E[y|x])2] as well. Again invoking Lemma 7, and making similar arguments, it follows that\nE [|u\u2217(w \u00b7 x)\u2212 u\u0303(w \u00b7 x)|] \u2264 O\n(( W 2 log(m/\u03b4)\nm\n)1/4) . (22)\nCombining (21) and (22), we get\nE [|u\u0302(w \u00b7 x)\u2212 u\u0303(w \u00b7 x)|] \u2264 O\n(( W 2 log(m/\u03b4)\nm\n)1/4) .\nWe now invoke Lemma 7 to get\n1\nm m\u2211 i=1 |u\u0302(w \u00b7 xi)\u2212 u\u0303(w \u00b7 xi)| \u2264 O\n(( W 2 log(m/\u03b4)\nm\n)1/4) . (23)"}], "references": [{"title": "Rademacher and gaussian complexities: Risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2002}, {"title": "Distribution-free maximum-likelihood estimator of the binary choice model", "author": ["S. Cosslett"], "venue": null, "citeRegEx": "Cosslett.,? \\Q1983\\E", "shortCiteRegEx": "Cosslett.", "year": 1983}, {"title": "Direct semiparametric estimation of single-index models with discrete covariates", "author": ["J. Horowitz", "W. H\u00e4rdle"], "venue": null, "citeRegEx": "Horowitz and H\u00e4rdle.,? \\Q1994\\E", "shortCiteRegEx": "Horowitz and H\u00e4rdle.", "year": 1994}, {"title": "The isotron algorithm: High-dimensional isotonic regression", "author": ["A.T. Kalai", "R. Sastry"], "venue": "In COLT \u201909,", "citeRegEx": "Kalai and Sastry.,? \\Q2009\\E", "shortCiteRegEx": "Kalai and Sastry.", "year": 2009}, {"title": "Improving the sample complexity using global data", "author": ["S. Mendelson"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Mendelson.,? \\Q2002\\E", "shortCiteRegEx": "Mendelson.", "year": 2002}, {"title": "Direct estimation of the index coefficients in a singleindex model", "author": ["A. Juditsky M. Hristache", "V. Spokoiny"], "venue": "Technical Report 3433,", "citeRegEx": "Hristache and Spokoiny.,? \\Q1998\\E", "shortCiteRegEx": "Hristache and Spokoiny.", "year": 1998}, {"title": "Generalized Linear Models (2nd ed.)", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder.,? \\Q1989\\E", "shortCiteRegEx": "McCullagh and Nelder.", "year": 1989}, {"title": "Isotonic single-index model for high-dimensional database marketing", "author": ["P. Naik", "C. Tsai"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Naik and Tsai.,? \\Q2004\\E", "shortCiteRegEx": "Naik and Tsai.", "year": 2004}, {"title": "The Volume of Convex Bodies and Banach Space Geometry", "author": ["G. Pisier"], "venue": null, "citeRegEx": "Pisier.,? \\Q1999\\E", "shortCiteRegEx": "Pisier.", "year": 1999}, {"title": "Single index convex experts: Efficient estimation via adapted bregman losses", "author": ["P. Ravikumar", "M. Wainwright", "B. Yu"], "venue": "Snowbird Workshop,", "citeRegEx": "Ravikumar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2008}, {"title": "Learning kernel-based halfspaces with the zeroone loss", "author": ["S. Shalev-Shwartz", "O. Shamir", "K. Sridharan"], "venue": "In COLT,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2010}, {"title": "Smoothness, low-noise and fast rates", "author": ["N. Srebro", "K. Sridharan", "A. Tewari"], "venue": "In NIPS,", "citeRegEx": "Srebro et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2010}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Christianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Christianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Christianini.", "year": 2004}, {"title": "Optimal smoothing in single-index models", "author": ["P. Hall W. H\u00e4rdle", "H. Ichimura"], "venue": "Annals of Statistics,", "citeRegEx": "H\u00e4rdle and Ichimura.,? \\Q1993\\E", "shortCiteRegEx": "H\u00e4rdle and Ichimura.", "year": 1993}, {"title": "Isotonic regression under lipschitz constraint", "author": ["L. Yeganova", "W.J. Wilbur"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "Yeganova and Wilbur.,? \\Q2009\\E", "shortCiteRegEx": "Yeganova and Wilbur.", "year": 2009}, {"title": "Risk bounds for isotonic regression", "author": ["C.H. Zhang"], "venue": "Annals of Statistics,", "citeRegEx": "Zhang.,? \\Q2002\\E", "shortCiteRegEx": "Zhang.", "year": 2002}, {"title": "Covering number bounds for certain regularized function classes", "author": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Zhang.,? \\Q2002\\E", "shortCiteRegEx": "Zhang.", "year": 2002}], "referenceMentions": [{"referenceID": 3, "context": "Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints.", "startOffset": 0, "endOffset": 24}], "year": 2011, "abstractText": "Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide powerful generalizations of linear regression, where the target variable is assumed to be a (possibly unknown) 1-dimensional function of a linear predictor. In general, these problems entail non-convex estimation procedures, and, in practice, iterative local search heuristics are often used. Kalai and Sastry (2009) recently provided the first provably efficient method for learning SIMs and GLMs, under the assumptions that the data are in fact generated under a GLM and under certain monotonicity and Lipschitz constraints. However, to obtain provable performance, the method requires a fresh sample every iteration. In this paper, we provide algorithms for learning GLMs and SIMs, which are both computationally and statistically efficient. We also provide an empirical study, demonstrating their feasibility in practice.", "creator": "LaTeX with hyperref package"}}}