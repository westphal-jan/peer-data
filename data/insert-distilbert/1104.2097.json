{"id": "1104.2097", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2011", "title": "PAC learnability versus VC dimension: a footnote to a basic result of statistical learning", "abstract": "a fundamental result book of linear statistical operations learnig theory states that a concept class is pac learnable if and only if it is a uniform glivenko - cantelli class if and only if the vc dimension of the unit class is finite. however, the theorem is only valid under special assumptions of measurability of deciding the class, in which case the pac learnability even becomes consistent. otherwise, there is a classical universal example, constructed under such the explicit continuum hypothesis by dudley and durst and further adapted by blumer, ehrenfeucht, haussler, and warmuth, of a concept class of vc dimension one which is neither uniform glivenko - cantelli nor consistently pac learnable. we show that, rather surprisingly, under an appropriate additional set - theoretic hypothesis formulation which is much milder than the continuum hypothesis ( martin's axiom ), pac learnability is equivalent to finite vc dimension for every concept class.", "histories": [["v1", "Tue, 12 Apr 2011 01:15:03 GMT  (70kb)", "http://arxiv.org/abs/1104.2097v1", "Revised submission to IJCNN'2011"]], "COMMENTS": "Revised submission to IJCNN'2011", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir pestov"], "accepted": false, "id": "1104.2097"}, "pdf": {"name": "1104.2097.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Vladimir Pestov"], "emails": ["vpest283@uottawa.ca)."], "sections": [{"heading": null, "text": "ar X\niv :1\n10 4.\n20 97\nv1 [\ncs .L\nG ]\n1 2\nA pr\n2 01\n1\nI. INTRODUCTION\nThe following is a fundamental result of statistical learning theory.\nTheorem 1: For a concept class C the following three conditions are equivalent:\n1) C is distribution-free PAC learnable, 2) C is a uniform Glivenko\u2013Cantelli class, and 3) the Vapnik\u2013Chervonenkis dimension of C is finite.\nIt is in this form that the theorem is usually stated in textbooks on the subject, see [1], [2]. The condition 1) means the existence of a learning rule for C which is probably approximately correct.\nHowever, strictly speaking, the result is only true under a suitable measurability assumption on the concept class C . One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x \u2208 Ct, t \u2208 [0, 1] form an analytic subset of \u2126 \u00d7 [0, 1] [3]. Another measurability assumption, more difficult to state, is that of a well-behaved class C [2]. Under either of those conditions, the statement (1) in Theorem 1 can be replaced with\n1\u2032) C is distribution-free consistently PAC learnable,\nmeaning that every consistent learning rule L for C is distribution-free probably approximately correct. In the proof, a measurability hypothesis on C has to be invoked twice, in order to deduce implications (3) \u21d2 (2) and (1\u2032 \u21d2 (1).\nVladimir Pestov is with Departamento de Matema\u0301tica, Universidade Federal de Santa Catarina, Campus Universita\u0301rio Trindade, CEP 88.040-900 Floriano\u0301polis-SC, Brasil (CNPq Visiting Researcher) and the Department of Mathematics and Statistics, University of Ottawa, 585 King Edward Avenue, Ottawa, Ontario, K1N 6N5 Canada (permenent address, phone: 613-562- 5800 ext. 3523, fax: 613-562-5776, email: vpest283@uottawa.ca).\nIn particular, Theorem 1 holds for every countable class C or, more generally, for every universally separable class [4]. It is arguable that every concept class emerging in either theory or applications of statistical learning will be measurable in a sufficiently strong sense. For this reason, a measurability condition on C is typically not even mentioned.\nThe fact remains that Theorem 1 cannot be derived in full generality. An example of a concept class C of Vapnik\u2013 Chervonenkis (VC) dimension one which is not uniform Glivenko\u2013Cantelli was constructed by Durst and Dudley [5], and a further modification of this example, also of VC dimension one, fails consistent PAC learnability [2].\nThis example has been constructed under Continuum Hypothesis (CH), which is arguably not a natural assumption in a probabilistic context [6]. However, the example remains valid under much more relaxed and natural set-theoretic hypothesis: Martin\u2019s Axiom (MA). It is one of the most often used and best studied additional set-theoretic assumptions beyond the standard Zermelo-Frenkel set theory with the Axiom of Choice (ZFC). In particular, Martin\u2019s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+\u00acCH that is really interesting [7], [8], [9].\nIn this note we make the following, somewhat astonishing, observation: under the same assumption (Martin\u2019s Axiom), the conditions (1) and (3) in Theorem 1 are equivalent. Here is our main result.\nTheorem 2: Assume the validity of Martin\u2019s Axiom (MA). Then the following are equivalent for every concept class C consisting of universally measurable subsets of a Borel domain \u2126:\n1) C is distribution-free PAC learnable, and 2) the Vapnik\u2013Chervonenkis dimension of C is finite.\nOf course it is only the implication (2)\u21d2(1) that needs proving, because (1)\u21d2(2) is a well-known classical result from [2] which does not require any assumptions on C .\nWe review a precise formal setting for learnability, after which we proceed to analysis of a counter-example from [5], [2]. We observe that the concept class C in the example is in fact PAC learnable, and this observation provides a clue to a general result.\nThe construction of the learning rule L can be described as a \u201cfirst in, first served\u201d approach. The concept class C is given a minimal well-ordering, \u227a, and L is constructed recursively, by assigning to a learning sample the \u227a-smallest consistent concept C with regard to the the ordering. As a\nconsequence, for every concept C \u2208 C , the image of all learning samples of the form (\u03c3,C \u2229 \u03c3) under L forms a uniform Glivenko\u2013Cantelli class. It is for establishing this property of L that we need Martin\u2019s Axiom. Now the probable approximate correctness of L is straightforward.\nThe present approach goes back to present author\u2019s earlier work [10], but the results are new and have never been stated explicitely before."}, {"heading": "II. THE SETTING", "text": "For obvious reasons, we need to be quite precise when fixing a general setting for learnability. The domain (instance space) \u2126 = (\u2126,A ) is a standard Borel space, that is, a complete separable metric space equipped with the sigmaalgebra of Borel subsets (the smallest family of sets containing all open balls and closed under complements and countable intersections).\nMeasures on \u2126 mean Borel probability measures, that is, countably additive functions on A with values in the unit interval [0, 1], having the property \u00b5(\u2126) = 1. We will not distinguish between a measure \u00b5 and its Lebesgue completion, that is, an extension of \u00b5 over a larger sigma-algebra of Lebesgue \u00b5-measurable subsets of \u2126. Furthermore, recall that a subset A \u2286 \u2126 is universally measurable if it is Lebesgue \u00b5-measurable for every probability measure \u00b5 on \u2126.\nWith this caveat, a concept class, C , is a family of universally measurable subsets of \u2126.\nIn the learning model, a set P of probability measures on \u2126 is fixed. Usually either P = P (\u2126) is the set of all probability measures (distribution-free learning), or P = {\u00b5} is a single measure (learning under fixed distribution). In our article, the case of interest is the former, although some of our results are valid in the case of a general family P \u2286 P (\u2126).\nA learning sample is a pair (\u03c3, \u03c4) of finite subsets of \u2126, where \u03c4 \u2286 \u03c3 is thought of as the set of points belonging to an unknown concept, C. The set of all samples of size n is usually identified with (\u2126\u00d7 {0, 1})n.\nA learning rule (for C ) is a mapping\nL : \u221e \u22c3\nn=1\n\u2126n \u00d7 {0, 1}n \u2192 C\nwhich satisfies the following measurability condition: for every C \u2208 C , n \u2208 N and \u00b5 \u2208 P , the function\n\u2126n \u220b \u03c3 7\u2192 \u00b5 (L(\u03c3,C \u2229 \u03c3)\u25b3 C) \u2208 R (1)\nis measurable. A learning rule L is consistent (with a concept class C ) if for all C \u2208 C , n \u2208 N and \u03c3 \u2208 \u2126n one has\nL(\u03c3,C \u2229 \u03c3) \u2229 \u03c3 = C \u2229 \u03c3.\nA learning rule L is probably approximately correct (PAC) under P if for every \u01eb > 0\n\u00b5\u2297n {\u03c3 \u2208 \u2126n : \u00b5 (L(\u03c3,C \u2229 \u03c3)\u25b3 C) > \u01eb} \u2192 0 (2)\nas n \u2192 \u221e, uniformly over all C \u2208 C and \u00b5 \u2208 P . Here \u00b5\u2297n denotes the product measure on \u2126n.\nIn terms of sample complexity function s(\u01eb, \u03b4), a learning rule L is PAC if for each C \u2208 C and every \u00b5 \u2208 P an independent identically distributed (i.i.d.) sample \u03c3 = (x1, x2, . . . , xn) with n \u2265 s(\u01eb, \u03b4) points has the property \u00b5(C \u25b3L(\u03c3,C \u2229 \u03c3)) < \u01eb with confidence \u2265 1\u2212 \u03b4.\nA concept class C is PAC learnable under P , if there exists a PAC learning rule for C under P . A class C is consistently learnable (under P) if every learning rule consistent with C is PAC under P . If P = P (\u2126) is the set of all probability measures, then C is said to be distribution-free PAC learnable. If P = {\u00b5} is a single probability measure, one is talking of learning under a single distribution. Learnability under intermediate families P is also receiving considerable attention, cf. Chapter 7 in [11].\nNotice that in this paper, we only talk of potential PAC learnability, adopting a purely information-theoretic viewpoint. As a consequence, our statements about learning rules are existential rather than constructive, and building learning rules by transfinite recursion is perfectly acceptable.\nA concept class C is uniform Glivenko\u2013Cantelli with regard to a family of measures P , if for each \u01eb > 0\nsup \u00b5\u2208P\n\u00b5\u2297n {\nsup C\u2208C |\u00b5(C) \u2212 \u00b5n(C)| \u2265 \u01eb\n}\n\u2192 0 as n \u2192 \u221e.\n(3) Here \u00b5n stands for the empirical (uniform) measure on n points, sampled in an i.i.d. fashion from \u2126 according to the distribution \u00b5. In this case, one also says that C has the property of uniform convergence of empirical measures (UCEM property) (with regard to P) [11].\nEvery uniform Glivenko\u2013Cantelli concept class (with regard to P) is consistently PAC learnable (under P), as is easy to verify. In the distribution-free situation (P = P (\u2126)) the converse holds under additional measurability conditions on the class mentioned in the Introduction, but, as we will see, not always.\nMore precisely, every distribution-free PAC learnable class has finite VC dimension (it was proved in [2], Theorem 2.1(i); see also e.g. [11], Lemma 7.2 on p. 279). Now the measurabilty conditions on C assure that a class C of finite VC dimension d is uniform Glivenko\u2013Cantelli, with a sample complexity bound that does not depend on C , but only on \u01eb, \u03b4, and d. The following is a typical (and far from being optimal) such estimate, which can be deduced, for instance, along the lines of [12]:\ns(\u01eb, \u03b4, d) \u2264 128\n\u01eb2\n(\nd log\n(\n2e2\n\u01eb log\n2e\n\u01eb\n)\n+ log 8\n\u03b4\n)\n. (4)\nFor our purposes, we will fix any such bound and refer to it as a \u201cstandard\u201d sample complexity estimate for s(\u01eb, \u03b4, d).\nNow the consistent learnability for C , with the same sample complexity, follows. Of course in order to conclude that C is PAC learnable, it is necessary to prove the existence of a consistent learning rule satisfying Eq. (1). This is usually being done using subtle measurable selection theorems using the same measurability assumptions on C yet again.\nFinally, recall that a subset N \u2286 \u2126 is universal null if for every non-atomic probability measure \u00b5 on (\u2126,A ) one has\n\u00b5(N \u2032) = 0 for some Borel set N \u2032 containing N . Universal null Borel sets are just countable sets."}, {"heading": "III. REVISITING AN EXAMPLE OF DURST AND DUDLEY", "text": "The proof of the implication (3)\u21d2(2) in Theorem 1 depends in an essential way on the Fubini theorem, which is why some measurability restrictions on the class C are unavoidable. Without them, the conclusion is not true in general. Here is a classical example of a concept class having finite VC dimension which is not uniform Glivenko\u2013Cantelli.\nExample 3 (Durst and Dudley [5], Proposition 2.2): Assume the validity of the Continuum Hypothesis (CH). Let \u2126 be an uncountable standard Borel space, that is, up to an isomorphism, a Borel space associated to the unit interval [0, 1]. The statement of CH is equivalent to the existence of a total order \u227a on \u2126 with the property that every half-open initial segment Iy = {x \u2208 \u2126: x \u227a y}, y \u2208 \u2126 is countable, and \u227a is a well-ordering: every non-empty subset of \u2126 has the smallest element. Fix such an order.\nLet C consist of all half-open initial segments Iy , y \u2208 \u2126 as above. Clearly, the VC dimension of the class C is one.\nNow let \u00b5 be a non-atomic Borel probability measure on \u2126 (e.g., the Lebesgue measure on [0, 1]). Under CH, every element of C is a countable set, therefore Borel measurable of measure zero. At the same time, for every n and each i.i.d. random n-sample \u03c3, there is a countable initial segment C = Iy \u2208 C containing all elements of \u03c3. The empirical measure of C with regard to \u03c3 is one. Thus, no finite sample guesses the measure of all elements of C to within an accuracy \u01eb < 1 with a non-vanishing confidence.\nSee also [13], p. 314; [3], pp. 170\u2013171. A further modification of this construction gives an example of a concept class of finite VC dimension which is not consistently PAC learnable.\nExample 4 (Blumer et al. [2], p. 953): Again, assume CH. Add to the concept class C from Example 3 the set \u2126 as an element, forming a new concept class C \u2032 = C \u222a {\u2126}. One still has VC(C \u2032) = 1. For a finite labelled sample (\u03c3, \u03c4) define\nL(\u03c3, \u03c4) = Iz , z = min{y \u2208 (\u2126,\u227a) : \u03c4 \u2286 Iy}. (5)\nThe learning rule L is consistent with the class C \u2032. At the same time, L is not probably approximately correct. Indeed, for the concept C = \u2126 the value of the learning rule L(\u03c3,\u2126 \u2229 \u03c3) = L(\u03c3, \u03c3) will always return a countable concept Iy for some y \u2208 \u2126, and if \u00b5 is a non-atomic Borel probability measure on \u2126, then \u00b5(C\u25b3 Iy) = 1. The concept C = \u2126 cannot be learned to accuracy \u01eb < 1 with a non-zero confidence.\nRemark 5: It is important to note that \u2014 again, under CH \u2014 the class C \u2032 is distribution-free PAC learnable.\nIndeed, redefine a well-ordering on C = {Ix : x \u2208 \u2126} \u222a {\u2126} by making \u2126 the smallest element (instead of the largest one) and keeping the order relation between other elements the same. Denote the new order relation by \u227a1, and define a learning rule L1 similarly to Eq. (5), but this time\nunderstanding the minimum with regard to the well-ordering \u227a1:\nL1(\u03c3, \u03c4) = min (\u227a1)\n\n\n\nC \u2208 C : C \u2229 \u03c3 = \u22c2\n\u03c4\u2286D\nD\n\n\n\n. (6)\nIn essence, L1 examines all the concepts following a transfinite order on them, and returns the first encountered concept consistent with the sample, provided it exists.\nTo see what difference it makes with Example 4, let \u00b5 be again a non-atomic probability measure on \u2126. If C = \u2126, then for every sample \u03c3 consistently labelled with C the rule L1 will return C, because this is the smallest consistent concept encountered by the algorithm. If C 6= \u2126, then for \u00b5almost all samples \u03c3 the labelling on \u03c3 produced by C will be empty, and the concept L1(\u03c3, \u2205) returned by L1, while possibly different from C, will be again a countable concept, meaning that \u00b5(C \u25b3L(\u03c3, \u2205)) = 0.\nTo give a formal proof that L1 is PAC, notice that for every C \u2208 C \u2032 and each n \u2208 N the collection of pairwise distinct concepts L1(\u03c3 \u2229C), \u03c3 \u2208 \u2126n is only countable (under CH), because they are all contained in the \u227a1-initial segment of a minimally ordered set C of cardinality continuum, bounded by C itself. As a consequence, the concept class\nLC1 = {L1(\u03c3 \u2229C) : \u03c3 \u2208 \u2126 n, n \u2208 N} \u2286 C \u2032 (7)\nis also countable (assuming CH). The VC dimension of the family LC1 \u222a{C} is \u2264 1, and being countable, it is a uniform Glivenko\u2013Cantelli class with a standard sample complexity as in Eq. (4). Consequently, given \u01eb, \u03b4 > 0, and assuming that n is sufficiently large, one has for each probability measure \u00b5 on \u2126 and every \u03c3 \u2208 \u2126n\n\u00b5(C \u25b3L(\u03c3,C \u2229 \u03c3)) < \u01eb\nprovided n \u2265 s(\u01eb, \u03b4, 1), as required. Remark 6: Notice that the role of the Continuum Hypothesis in the above examples was merely to assure that every initial segment Iy , y \u2208 \u2126 is a universally measurable set. As we will see, it can be achieved under a much milder assumption of Martin\u2019s Axiom.\nRemark 7: Thus, under the Continuum Hypothesis, the example of Dudley and Durst as modified by Blumer, Ehrenfeucht, Haussler, and Warmuth gives an example of a PAC learnable concept class which is not uniform Glivenko\u2013 Cantelli (even if having finite VC dimension). As it will become clear in the next Section, the assumption of CH can be weakened to Martin\u2019s Axiom. Still, it would be interesting to know whether an example with the same combination of properties can be constructed without additional set-theoretic assumptions.\nA basic observation of this Section is that in order for a learning rule L to be PAC, the assumption on C being uniform Glivenko\u2013Cantelli can be weakened as follows.\nLemma 8: Let C be a concept class and P a family of probability measures on the domain \u2126. Suppose there exists a function s(\u01eb, \u03b4) and a learning rule L for C with the property\nthat for every C \u2208 C , the set LC \u222a{C} is Glivenko\u2013Cantelli with regard to P with the sample complexity s(\u01eb, \u03b4), where\nLC = {L(C \u2229 \u03c3) : \u03c3 \u2208 \u2126n, n \u2208 N} .\nThen L is probably approximately correct under P with sample complexity s(\u01eb, \u03b4).\nThis simple fact becomes useful in combination with the technique of well-orderings. Of course the Continuum Hypothesis is a particularly unnatural assumption in a probabilistic context (cf. [6]). But it is unnecessary. Martin\u2019s Axiom (MA) is a much weaker and natural additional settheoretic axiom, which works just as well."}, {"heading": "IV. LEARNABILITY UNDER MARTIN\u2019S AXIOM", "text": "Martin\u2019s Axiom (MA) says that no compact Hausdorff topological space with the countable chain condition is a union of strictly less than continuum nowhere dense subsets. Thus, it is a stronger statement than the Baire Category Theorem. In particular, the Continuum Hypothesis implies MA. However, MA is compatible with the negation of CH, and this is where the most interesting applications of MA are to be found. We need the following consequence of MA.\nTheorem 9 (Martin-Solovay): Let (\u2126, \u00b5) be a standard Lebesgue non-atomic probability space. Under MA, the Lebesgue measure is 2\u21350 -additive, that is, if \u03ba < 2\u21350 and A\u03b1, \u03b1 < \u03ba is family of pairwise disjoint measurable sets, then \u222a\u03b1<\u03baA\u03b1 is Lebesgue measurable and\n\u00b5\n(\n\u22c3\n\u03b1<\u03ba\nA\u03b1\n)\n= \u2211\n\u03b1<\u03ba\n\u00b5(A\u03b1).\nIn particular, the union of strictly less than continuum null subsets of \u2126 is a null subset.\nFor the proof and more on MA, see [9], Theorem 2.21, or [7], or [8], pp. 563\u2013565.\nLemma 10: Let C be a concept class and P a family of probability measures on a standard Borel domain \u2126. Consider the following properties.\n1) Every countable subclass of C is uniform Glivenko\u2013 Cantelli with regard to P . 2) There is a function s(\u01eb, \u03b4) so that every countable subclass of C is uniform Glivenko\u2013Cantelli with regard to P with sample complexity s(\u01eb, \u03b4). 3) Every subclass C \u2032 of C having cardinality < 2\u21350 is uniform Glivenko\u2013Cantelli with regard to P . 4) There is a function s(\u01eb, \u03b4) so that every subclass C \u2032\nof C having cardinality < 2\u21350 is uniform Glivenko\u2013 Cantelli with regard to P with sample complexity s(\u01eb, \u03b4).\nThen (1)\n\u0580\u0582 \u057f (2) (3)\n\u057f \u0580 (4)\nUnder Martin\u2019s Axiom, all four conditions are equivalent.\nProof: The implications (2) \u21d2 (1), (3) \u21d2 (1), (4) \u21d2 (2) and (4) \u21d2 (3) are trivially true. To show (1) \u21d2 (2), let \u03b4, \u01eb > 0 be artitrary but fixed. For each countable subclass C \u2032, choose the smallest value of sample complexity s = s(C \u2032, \u01eb, \u03b4). The function C \u2032 7\u2192 s(C \u2032, \u01eb, \u03b4) is monotone under inclusions: if C \u2032 \u2286 C \u2032\u2032, then s(C \u2032, \u01eb, \u03b4) \u2264 s(C \u2032\u2032, \u01eb, \u03b4). If C \u2032n is a sequence of countable classes, then the union \u222a\u221en=1C \u2032 n is a countable class, whose sample complexity value bounds from above s(C \u2032, \u01eb, \u03b4), n = 1, 2, . . .. Thus, the function C\n\u2032 7\u2192 s(C \u2032, \u01eb, \u03b4) for \u03b4, \u01eb > 0 fixed is bounded on countable sets of inputs, and therefore bounded.\nNow assume (MA). It is enough to prove (2) \u21d2 (4). This is done by a transfinite induction on the cardinality \u03ba = |C \u2032|, which never exceeds 2\u21350 because C \u2032 consists of Borel subsets of a standard Borel domain. For \u03ba = \u21350 there is nothing to prove. Else, represent C as a union of an increasing transfinite chain of concept classes C\u03b1, \u03b1 < \u03ba, for each of which the statement of (4) holds. For every \u01eb > 0 and n \u2208 N, the set\n{\u03c3 \u2208 \u2126n : supC\u2208C |\u00b5n(\u03c3)(C) \u2212 \u00b5(C)| < \u01eb}\n= \u22c2\n\u03b1<\u03ba\n{ \u03c3 \u2208 \u2126n : supC\u2208C\u03b1 |\u00b5n(\u03c3)(C) \u2212 \u00b5(C)| < \u01eb }\nis measurable by Martin-Solovay\u2019s Theorem 9. Given \u03b4 > 0 and n \u2265 s(\u01eb, \u03b4, d), another application of the same result leads to conclude that for every \u00b5 \u2208 P (\u2126):\n\u00b5\u2297n {\n\u03c3 \u2208 \u2126n : sup C\u2208C |\u00b5n(\u03c3)(C) \u2212 \u00b5(C)| < \u01eb\n}\n= \u00b5\u2297n\n(\n\u22c2\n\u03b1<\u03ba\n{\n\u03c3 \u2208 \u2126n : sup C\u2208C\u03b1 |\u00b5n(\u03c3)(C) \u2212 \u00b5(C)| < \u01eb\n}\n)\n= inf \u03b1<\u03ba\n\u00b5\u2297n {\n\u03c3 \u2208 \u2126n : sup C\u2208C\u03b1 |\u00b5n(\u03c3)(C) \u2212 \u00b5(C)| < \u01eb\n}\n\u2265 1\u2212 \u03b4,\nas required. Lemma 11: Let C be a concept class whose countable subclasses are uniform Glivenko\u2013Cantelli with regard to a family of probability measures P . Let L be a learning rule for C with the property that for every C \u2208 C , the set\nLC,n = {L(C \u2229 \u03c3) : \u03c3 \u2208 \u2126n} (8)\nhas cardinality strictly less than continuum. Under Martin\u2019s Axiom, the rule L is probably approximately correct under P . The common sample complexity bound of countable subclasses of C becomes the sample complexity bound for the learning rule L.\nProof: Recall that 2\u21350 is a regular cardinal, and thus admits no countable cofinal subset. Therefore, under the assumptions of Lemma, the cardinality of LC = \u222a\u221en=1L C,n is still strictly less than continuum. Applying now Lemma 10 and then Lemma 8, we conclude.\nThe following result establishes existence of learning rules with the required property.\nLemma 12: Let C be an infinite concept class on a measurable space \u2126. Denote \u03ba = |C | the cardinality of C . There exists a consistent learning rule L for C with the\nproperty that for every C \u2208 C and each n, the set LC,n (cf. Eq. (8)) has cardinality < \u03ba. Under MA the rule L satisfies the condition in Eq. (1).\nProof: Choose a minimal well-ordering of elements of C :\nC = {C\u03b1 : \u03b1 < \u03ba},\nand set for every \u03c3 \u2208 \u2126n and \u03c4 \u2208 {0, 1}n the value L(\u03c3, \u03c4) equal to C\u03b2 , where\n\u03b2 = min{\u03b1 < \u03ba : C\u03b1 \u2229 \u03c3 = \u03c4},\nprovided such a \u03b2 exists. Clearly, for each \u03b1 < \u03ba one has\nL(\u03c3,C\u03b1 \u2229 \u03c3) \u2208 {C\u03b2 : \u03b2 \u2264 \u03b1},\nwhich assures (8). Besides, the learning rule L is consistent. Fix C = C\u03b1 \u2208 C , \u03b1 < \u03ba. For every \u03b2 \u2264 \u03b1 define D\u03b2 = {\u03c3 \u2208 \u2126n : C \u2229 \u03c3 = C\u03b2 \u2229 \u03c3}. The sets D\u03b2 are measurable, and the function\n\u2126n \u220b \u03c3 7\u2192 \u00b5(L(C \u2229 \u03c3)\u25b3 C) \u2208 R\ntakes a constant value \u00b5(C\u25b3C\u03b1) on each set D\u03b2 \\\u222a\u03b3<\u03b2D\u03b3 , \u03b2 \u2264 \u03b1. Such sets, as well as all their possible unions, are measurable under MA by force of Martin\u2013Solovay\u2019s Theorem 9, and their union is \u2126n. This implies the validity of Eq. (1) for L.\nLemma 11 and Lemma 12 lead to the following result. Theorem 13 (Assuming MA): Let C be a concept class consisting of Borel measurable subsets of a standard Borel domain \u2126, and let P be a family of probability measures on \u2126. Suppose that every countable subclass of C is uniform Glivenko\u2013Cantelli with regard to P . Then the concept class C is PAC learnable under P . In addition, there exists a common sample complexity bound for countable subclasses of C , and any such bound gives a sample complexity bound for PAC learnability of C .\nFinally, we can deduce our main result.\nProof of (2)\u21d2(1) in Theorem 2\nThe implication follows from Theorem 13 with P = P (\u2126) and the common complexity bound (4)."}, {"heading": "V. CONCLUSION", "text": "As a footnote to the fundamental theorem of statistical learing, we have proved that in the presence of a mild settheoretic axiom (Martin\u2019s Axiom), PAC learnability of a concept class C is equivalent to finiteness of VC dimension of C , without any extra assumptions on the measurability of the class C . The price to pay is giving up consistent PAC learnability, as well as constructive choice of a learning rule.\nIt would be interesting to know to what extent the results remain true in the usual ZFC model of set theory. In particular, can an example of a concept class C on a standard Borel domain which has finite VC dimension and still is not cosistently PAC learnable, be constructed without any additional set-theoretic axioms?"}], "references": [{"title": "Chervonenkis, On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V.N. Vapnik", "A.Ya"], "venue": "Theory Probab. Appl. 16,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1971}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["A. Blumer", "A. Ehrenfeucht", "D. Haussler", "M.K. Warmuth"], "venue": "Journal of the ACM, 36(4) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Uniform Central Limit Theorems", "author": ["R.M. Dudley"], "venue": "Cambridge Studies in Advanced Mathematics, 63, Cambridge University Press, Cambridge", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Convergence of Stochastic Processes", "author": ["D. Pollard"], "venue": "Springer-Verlag, New York", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1984}, {"title": "Empirical processes", "author": ["M. Durst", "R.M. Dudley"], "venue": "Vapnik\u2013Chervonenkis classes, and Poisson processes, Prob. and Math. Statistics 1 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1980}, {"title": "Axioms of symmetry: throwing darts at the real number line", "author": ["C. Freiling"], "venue": "J. Symbolic Logic 51 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1986}, {"title": "Consequences of Martin\u2019s Axiom", "author": ["D.H. Fremlin"], "venue": "Cambridge Tracts in Mathematics, 84. Cambridge University Press, Cambridge", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1984}, {"title": "Set Theory", "author": ["T. Jech"], "venue": "Academic Press, New York\u2013London", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1978}, {"title": "Set Theory", "author": ["K. Kunen"], "venue": "North-Holland, Amsterdam", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1980}, {"title": "PAC learnability of a concept class under non-atomic measures: a problem by Vidyasagar", "author": ["V. Pestov"], "venue": "in: Proc. 21st Intern. Conference on Algorithmic Learning Theory ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning and Generalization", "author": ["M. Vidyasagar"], "venue": "with Applications to Neural Networks, 2nd Ed., Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "A few notes on statistical learning theory", "author": ["S. Mendelson"], "venue": "in: S. Mendelson, A.J. Smola, Eds., Advanced Lectures in Machine Learning, LNCS 2600, Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Some special Vapnik\u2013Chervonenkis classes", "author": ["R.S. Wenokur", "P.M. Dudley"], "venue": "Discrete Math. 33 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1981}], "referenceMentions": [{"referenceID": 0, "context": "It is in this form that the theorem is usually stated in textbooks on the subject, see [1], [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "It is in this form that the theorem is usually stated in textbooks on the subject, see [1], [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x \u2208 Ct, t \u2208 [0, 1] form an analytic subset of \u03a9 \u00d7 [0, 1] [3].", "startOffset": 166, "endOffset": 172}, {"referenceID": 0, "context": "One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x \u2208 Ct, t \u2208 [0, 1] form an analytic subset of \u03a9 \u00d7 [0, 1] [3].", "startOffset": 204, "endOffset": 210}, {"referenceID": 2, "context": "One such assumption is that of C being image admissible Souslin: the class C can be parametrized with elements of the unit interval so that pairs (x, t), x \u2208 Ct, t \u2208 [0, 1] form an analytic subset of \u03a9 \u00d7 [0, 1] [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 1, "context": "Another measurability assumption, more difficult to state, is that of a well-behaved class C [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "In particular, Theorem 1 holds for every countable class C or, more generally, for every universally separable class [4].", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "An example of a concept class C of Vapnik\u2013 Chervonenkis (VC) dimension one which is not uniform Glivenko\u2013Cantelli was constructed by Durst and Dudley [5], and a further modification of this example, also of VC dimension one, fails consistent PAC learnability [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "An example of a concept class C of Vapnik\u2013 Chervonenkis (VC) dimension one which is not uniform Glivenko\u2013Cantelli was constructed by Durst and Dudley [5], and a further modification of this example, also of VC dimension one, fails consistent PAC learnability [2].", "startOffset": 259, "endOffset": 262}, {"referenceID": 5, "context": "This example has been constructed under Continuum Hypothesis (CH), which is arguably not a natural assumption in a probabilistic context [6].", "startOffset": 137, "endOffset": 140}, {"referenceID": 6, "context": "In particular, Martin\u2019s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+\u00acCH that is really interesting [7], [8], [9].", "startOffset": 200, "endOffset": 203}, {"referenceID": 7, "context": "In particular, Martin\u2019s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+\u00acCH that is really interesting [7], [8], [9].", "startOffset": 205, "endOffset": 208}, {"referenceID": 8, "context": "In particular, Martin\u2019s Axiom follows from the Continuum Hypothesis (CH), but it is also compatible with the negation of CH, and in fact it is namely the combination MA+\u00acCH that is really interesting [7], [8], [9].", "startOffset": 210, "endOffset": 213}, {"referenceID": 1, "context": "Of course it is only the implication (2)\u21d2(1) that needs proving, because (1)\u21d2(2) is a well-known classical result from [2] which does not require any assumptions on C .", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "We review a precise formal setting for learnability, after which we proceed to analysis of a counter-example from [5], [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "We review a precise formal setting for learnability, after which we proceed to analysis of a counter-example from [5], [2].", "startOffset": 119, "endOffset": 122}, {"referenceID": 9, "context": "The present approach goes back to present author\u2019s earlier work [10], but the results are new and have never been stated explicitely before.", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Measures on \u03a9 mean Borel probability measures, that is, countably additive functions on A with values in the unit interval [0, 1], having the property \u03bc(\u03a9) = 1.", "startOffset": 123, "endOffset": 129}, {"referenceID": 10, "context": "Chapter 7 in [11].", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "In this case, one also says that C has the property of uniform convergence of empirical measures (UCEM property) (with regard to P) [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 1, "context": "More precisely, every distribution-free PAC learnable class has finite VC dimension (it was proved in [2], Theorem 2.", "startOffset": 102, "endOffset": 105}, {"referenceID": 10, "context": "[11], Lemma 7.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The following is a typical (and far from being optimal) such estimate, which can be deduced, for instance, along the lines of [12]:", "startOffset": 126, "endOffset": 130}, {"referenceID": 4, "context": "Example 3 (Durst and Dudley [5], Proposition 2.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "Let \u03a9 be an uncountable standard Borel space, that is, up to an isomorphism, a Borel space associated to the unit interval [0, 1].", "startOffset": 123, "endOffset": 129}, {"referenceID": 0, "context": ", the Lebesgue measure on [0, 1]).", "startOffset": 26, "endOffset": 32}, {"referenceID": 12, "context": "See also [13], p.", "startOffset": 9, "endOffset": 13}, {"referenceID": 2, "context": "314; [3], pp.", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "[2], p.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "For the proof and more on MA, see [9], Theorem 2.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "21, or [7], or [8], pp.", "startOffset": 7, "endOffset": 10}, {"referenceID": 7, "context": "21, or [7], or [8], pp.", "startOffset": 15, "endOffset": 18}], "year": 2011, "abstractText": "A fundamental result of statistical learnig theory states that a concept class is PAC learnable if and only if it is a uniform Glivenko\u2013Cantelli class if and only if the VC dimension of the class is finite. However, the theorem is only valid under special assumptions of measurability of the class, in which case the PAC learnability even becomes consistent. Otherwise, there is a classical example, constructed under the Continuum Hypothesis by Dudley and Durst and further adapted by Blumer, Ehrenfeucht, Haussler, and Warmuth, of a concept class of VC dimension one which is neither uniform Glivenko\u2013 Cantelli nor consistently PAC learnable. We show that, rather surprisingly, under an additional set-theoretic hypothesis which is much milder than the Continuum Hypothesis (Martin\u2019s Axiom), PAC learnability is equivalent to finite VC dimension for every concept class.", "creator": "LaTeX with hyperref package"}}}