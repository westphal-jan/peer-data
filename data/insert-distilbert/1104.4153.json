{"id": "1104.4153", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2011", "title": "Learning invariant features through local space contraction", "abstract": "next we present basically in this paper a novel approach for training deterministic universal auto - encoders. we show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results where that equal or surpass those attained by other regularized partial auto - encoders as well as denoising auto - encoders on a range of datasets. this penalty term corresponds to the frobenius norm of the intrinsic jacobian matrix of handling the encoder activations with respect to the input. we show that discovering this penalty term results in a localized color space contraction effect which in turn hence yields robust features on the activation layer. furthermore, we show how this penalty term is related to both regularized auto - encoders ourselves and denoising encoders ` and how it can be seen as a link breaker between deterministic and classical non - deterministic auto - encoders. we find empirically that this penalty helps to carve a representation that better captures the local algebraic directions of variation dictated by the data, without corresponding strongly to a lower - dimensional convex non - linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. finally, we show that somewhat by using the new learned driver features to difficulty initialize finding a mlp, typically we already achieve state of the art classification error prediction on a range of datasets, surpassing other methods of pre - training.", "histories": [["v1", "Thu, 21 Apr 2011 01:39:25 GMT  (96kb,D)", "http://arxiv.org/abs/1104.4153v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salah rifai", "xavier muller", "xavier glorot", "gregoire mesnil", "yoshua bengio", "pascal vincent"], "accepted": false, "id": "1104.4153"}, "pdf": {"name": "1104.4153.pdf", "metadata": {"source": "CRF", "title": "Learning invariant features through local space contraction", "authors": ["Salah Rifai", "Xavier Muller", "Xavier Glorot", "Gr\u00e9goire Mesnil", "Yoshua Bengio", "Pascal Vincent"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "A recent topic of interest1 in the machine learning community is the development of algorithms for unsupervised learning of a useful representation. This automatic discovery and extraction of features is often used in building a deep hierarchy of features, within the contexts of supervised, semi-supervised, or unsupervised modeling. See Bengio (2009) for a recent review of Deep Learning algorithms. Most of these methods exploit as basic building block algorithms for learning one level of feature extraction: the representation learned at one level is used as input for learning the next level, etc. The objective is that these representations become better as depth is increased, but what defines a good representation? It is fairly well understood what PCA or ICA do, but much\n1see NIPS\u20192010 Workshop on Deep Learning and Unsupervised Feature Learning\nar X\niv :1\n10 4.\n41 53\nv1 [\ncs .A\nI] 2\n1 A\nremains to be done to understand the characteristics and theoretical advantages of the representations learned by a Restricted Boltzmann Machine Hinton et al. (2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images). A simple approach, used here, to empirically verify that the learned representations are useful, is to use them to initialize a classifier (such as a multi-layer neural network), and measure classification error. Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).\nContribution. What principles should guide the learning of such intermediate representations? They should capture as much as possible of the information in each given example, when that example is likely under the underlying generating distribution. That is what auto-encoders Vincent et al. (2008) and sparse coding aim to achieve when minimizing reconstruction error.\nWe would also like these representations to be useful in characterizing the input distribution, and that is what is achieved by directly optimizing a generative model\u2019s likelihood (such as RBMs), or a proxy, such as Score Matching Hyva\u0308rinen (2005). In this paper, we introduce a penalty term that could be added to either of the above contexts, which encourages the intermediate representation to be robust to small changes of the input around the training examples. We show through comparative experiments on many benchmark datasets that this characteristic is useful to learn representations that help training better classifiers. Previous work has shown that deep learners can discover representations whose features are invariant to some of the factors of variation in the input (Goodfellow et al., 2009). It would be nice to move further in this direction, towards representation learning algorithms which help to disentangle the factors of variation that underlie the data generating distribution. We hypothesize that whereas the proposed penalty term encourages the learned features to be locally invariant without any preference for particular directions, when it is combined with a reconstruction error or likelihood criterion we obtain invariance in the directions that make sense in the context of the given training data, i.e., the variations that are present in the data should also be captured in the learned representation, but the other directions may be contracted in the learned representation."}, {"heading": "2 How to extract robust features", "text": "Most successful modern approaches for building deep networks begin by initializing each layer in turn, using a local unsupervised learning technique, to extract potentially useful features for the next layer. When used as feature extractors in this fashion, both RBMs and various flavors of auto-encoders lead\nto a non-linear feature extractor of the exact same form: a linear mapping followed by a sigmoid non-linearity2. From this perspective, these algorithms are but different unsupervised techniques to learn the parameters of a mapping of that form. It is not yet fully understood what properties of such a mappings contribute to superior classification performance (for classifiers initialized with the produced features). It has been argued that mappings that produce a sparse representation are to be encouraged, which inspired several variants of sparse auto-encoders.\nThe research we present here is motivated by a different property: our working hypothesis is that a good representation of a likely input (under the unknown data distribution) should be expected to remain rather stable (i.e. be robust, invariant, insensitive) under tiny perturbations of that input. This prompts us to propose an alternative regularization term for auto-encoders.\nTo encourage robustness of the representation f(x) obtained for a training input x we propose to penalize its sensitivity to that input, measured as the Frobenius norm of the Jacobian Jf (x) of the non-linear mapping. Formally, if input x \u2208 IRdx is mapped by encoding function f to hidden representation h \u2208 IRdh , this sensitivity penalization term is the sum of squares of all partial derivatives of the extracted features with respect to input dimensions:\n\u2016Jf (x)\u20162F = \u2211 ij ( \u2202hj(x) \u2202xi )2 . (1)\nPenalizing \u2016Jf\u20162F encourages the mapping to the feature space to be contractive in the neighborhood of the training data. This geometric perspective, which gives its name to our algorithm, will be further elaborated on, in section 5.3, based on experimental evidence. The flatness induced by having low valued first derivatives will imply an invariance or robustness of the representation for small variations of the input. Thus in this study, terms like invariance, (in-)sensitivity, robustness, flatness and contraction all point to the same notion.\nWhile such a Jacobian term alone would encourage mapping to a useless constant representation, it is counterbalanced in auto-encoder training3 by the need for the learnt representation to allow a good reconstruction of the input4."}, {"heading": "3 Auto-encoders variants", "text": "In its simplest form, an auto-encoder (AE) is composed of two parts, an encoder and a decoder. It was introduced in the late 80\u2019s Rumelhart et al. (1986); Baldi and Hornik (1989) as a technique for dimensionality reduction, where the output of the encoder represents the reduced representation and where the decoder is tuned to reconstruct the initial input from the encoder\u2019s representation through the minimization of a cost function. More specifically when the encoding activation functions are linear and the number of hidden units is inferior to the input\n2This corresponds to the encoder part of the traditional auto-encoder neural-network and its regularized variants. In RBMs, the conditional expectation of the hidden layer given the visible layer has the exact same form.\n3Using also the now common additional constraint of encoder and decoder sharing the same (transposed) weights, which precludes a mere global contracting scaling in the encoder and expansion in the decoder.\n4A likelihood-related criterion would also similarly prevent a collapse of the representation.\ndimension (hence forming a bottleneck), it has been shown that the learnt parameters of the encoder are a subspace of the principal components of the input space Baldi and Hornik (1989). With the use of non-linear activation functions an AE can however be expected to learn more useful feature-detectors than what can be obtained with a simple PCA (Japkowicz et al., 2000). Moreover, contrary to their classical use as dimensionality-reduction techniques, in their modern instantiation auto-encoders are often employed in a so-called over-complete setting to extract a number of features larger than the input dimension, yielding a rich higher-dimensional representation. In this setup, using some form of regularization becomes essential to avoid uninteresting solutions where the auto-encoder could perfectly reconstruct the input without needing to extract any useful feature. This section formally defines the auto-encoder variants considered in this study.\nBasic auto-encoder (AE). The encoder is a function f that maps an input x \u2208 IRdx to hidden representation h(x) \u2208 IRdh . It has the form\nh = f(x) = sf (Wx+ bh), (2)\nwhere sf is a nonlinear activation function, typically a logistic sigmoid(z) = 1 1+e\u2212z . The encoder is parametrized by a dh \u00d7 dx weight matrix W , and a bias vector bh \u2208 IRdh .\nThe decoder function g maps hidden representation h back to a reconstruction y:\ny = g(h) = sg(W \u2032h+ by), (3)\nwhere sg is the decoder\u2019s activation function, typically either the identity (yielding linear reconstruction) or a sigmoid. The decoder\u2019s parameters are a bias vector by \u2208 IRdx , and matrix W \u2032. In this paper we only explore the tied weights case, in which W \u2032 = WT .\nAuto-encoder training consists in finding parameters \u03b8 = {W, bh, by} that minimize the reconstruction error on a training set of examples Dn, which corresponds to minimizing the following objective function:\nJAE(\u03b8) = \u2211 x\u2208Dn L(x, g(f(x))), (4)\nwhere L is the reconstruction error. Typical choices include the squared error L(x, y) = \u2016x\u2212y\u20162 used in cases of linear reconstruction and the cross-entropy loss when sg is the sigmoid (and inputs are in [0, 1]): L(x, y) = \u2212 \u2211dx i=1 xi log(yi)+ (1\u2212 xi) log(1\u2212 yi). Regularized auto-encoders (AE+wd). The simplest form of regularization is weight-decay which favors small weights by optimizing instead the following regularized objective:\nJAE+wd(\u03b8) = ( \u2211 x\u2208Dn L(x, g(f(x))) ) + \u03bb \u2211 ij W 2ij , (5)\nwhere the \u03bb hyper-parameter controls the strength of the regularization. Note that rather than having a prior on what the weights should be, it is possible to have a prior on what the hidden unit activations should be. From\nthis viewpoint, several techniques have been developed to encourage sparsity of the representation (Kavukcuoglu et al., 2008; Lee et al., 2008).\nDenoising Auto-encoders (DAE). A successful alternative form of regularization is obtained through the technique of denoising auto-encoders (DAE) put forward by Vincent et al. (2008, 2010), where one simply corrupts input x before sending it through the auto-encoder, that is trained to reconstruct the clean version (i.e. to denoise). This yields the following objective function:\nJDAE(\u03b8) = \u2211 x\u2208Dn Ex\u0303\u223cq(x\u0303|x)[L(x, g(f(x\u0303)))], (6)\nwhere the expectation is over corrupted versions x\u0303 of examples x obtained from a corruption process q(x\u0303|x). This objective is optimized by stochastic gradient descent (sampling corrupted examples).\nTypically, we consider corruptions such as additive isotropic Gaussian noise: x\u0303 = x + , \u223c N (0, \u03c32I) and a binary masking noise, where a fraction \u03bd of input components (randomly chosen) have their value set to 0. The degree of the corruption (\u03c3 or \u03bd) controls the degree of regularization."}, {"heading": "4 Contracting auto-encoders (CAE)", "text": "From the motivation of robustness to small perturbations around the training points, as discussed in section 2, we propose an alternative regularization that favors mappings that are more strongly contracting at the training samples (see section 5.3 for a longer discussion). The Contracting auto-encoder (CAE) is obtained with the regularization term of eq. 1 yielding objective function\nJCAE(\u03b8) = \u2211 x\u2208Dn ( L(x, g(f(x))) + \u03bb\u2016Jf (x)\u20162F ) (7)\nRelationship with weight decay. It is easy to see that the squared Frobenius norm of the Jacobian corresponds to a L2 weight decay in the case of a linear encoder (i.e. when sf is the identity function). In this special case JCAE and JAE+wd are identical. Note that in the linear case, keeping weights small is the only way to have a contraction. But with a sigmoid non-linearity, contraction and robustness can also be achieved by driving the hidden units to their saturated regime.\nRelationship with sparse auto-encoders. Auto-encoder variants that encourage sparse representations aim at having, for each example, a majority of the components of the representation close to zero. For these features to be close to zero, they must have been computed in the left saturated part of the sigmoid nonlinearity, which is almost flat, with a tiny first derivative. This yields a corresponding small entry in the Jacobian Jf (x). Thus, sparse autoencoders that output many close-to-zero features, are likely to correspond to a highly contractive mapping, even though contraction or robustness are not explicitly encouraged through their learning criterion.\nRelationship with denoising auto-encoders. Robustness to input perturbations was also one of the motivation of the denoising auto-encoder, as stated in Vincent et al. (2010). The CAE and the DAE differ however in the following ways:\n\u2022 CAEs explicitly encourage robustness of representation f(x), whereas DAEs encourages robustness of reconstruction (g \u25e6 f)(x) (which may only partially and indirectly encourage robustness of the representation, as the invariance requirement is shared between the two parts of the autoencoder). We believe that this property make CAEs a better choice than DAEs to learn useful feature extractors. Since we will use only the encoder part for classification, robustness of the extracted features appears more important than robustness of the reconstruction. \u2022 DAEs\u2019 robustness is obtained stochastically (eq. 6) by having several explicitly corrupted versions of a training point aim for an identical reconstruction. By contrast, CAEs\u2019 robustness to tiny perturbations is obtained analytically by penalizing the magnitude of first derivatives \u2016Jf (x)\u20162F at training points only (eq. 7)."}, {"heading": "4.1 Analytical link between denoising auto-encoders and contracting auto-encoders", "text": "If the noise used in the DAE is Gaussian, its effect can be approximated analytically with an added penality term on a standard autoencoder cost functionBishop (1995) and Kingma and LeCun (2010). Let us define L(\u03b8, x) as the loss function of the auto-encoder. We can write the expected cost of our loss as:\nC(\u03b8) = \u222b L(x, \u03b8)p(x)dx (8)\nThe empirical cost can be written by expressing our probabilty distribution as a series of dirac functions centered on our samples:\nCclean(\u03b8) = \u222b L(x, \u03b8)\u03b4(xi \u2212 x)dx = 1\nn n\u2211 i L(xi, \u03b8) (9)\nIf we were to express out density p(x) as a series of Gaussian kernels centered on our samples and with diagonal covariance matrix, our empirical cost function would become:\nCnoisy(\u03b8) = 1 n n\u2211 i \u222b L(x, \u03b8)Nxi,\u03c32(x)dx (10)\nIn the context of a de-noising auto-encoder, we approximate this cost by using samples corrupted with a Gaussian noise. We can express the difference between these two costs as a function. Our goal is to find an approximation for this function.\nCnoisy(\u03b8) = Cclean(\u03b8) + \u03c6(\u03b8) (11)\nand thus:\n\u03c6(\u03b8) = 1\nn n\u2211 i [\u222b L(x, \u03b8)Nxi,\u03c32(x)dx\u2212 L(xi, \u03b8) ] (12)\nWe can write the term inside the integral by defining the noise term \u03b5 = x\u2212 xi:\nD = \u222b L(xi + \u03b5)N0,\u03c32(\u03b5)d\u03b5\u2212 L(xi) (13)\nWe can approximate the first term by a Taylor series:\nL(x+ \u03b5) = L(x) + \u3008JL(x), \u03b5\u3009+ 1\n2 \u03b5T .HL(x).\u03b5+ o(\u03b5) (14)\nBy using this approximation and simplifying our integral, we obtain the following expression for our function:\n\u03c6(\u03b8) \u2248 \u03c3 2\n2n n\u2211 i Tr(HL(xi)) (15)\nCnoise(\u03b8) \u2248 Cclean(\u03b8) + \u03c3 2\n2n n\u2211 i Tr(HC\u0304(\u03b8)) (16)\nThe de-noising autoencoder cost function can thus be approximated by using a classical auto-encoder with a penality on the trace of the Hessian of the cost fucntion versus the inputs. When the cost function is the MSE of the reconstruction, we can write the Hessian as:\nHL(x) = \u2202\n\u2202x\n( \u2202\n\u2202x\n( \u2016 g(f(x))\u2212 x \u20162 )) = 2 \u2202\n\u2202x\n( \u2202g (f(x))\n\u2202x (g(f(x))\u2212 x) ) = 2Hg\u25e6f (x) (g(f(x))\u2212 x) + 2 \u2329 Jg\u25e6f (x) T , Jg\u25e6f (x) \u232a\n(17)\nBy taking the trace of the above results, we get:\nTr (HL(x)) = 2 (g(f(x))\u2212 x) Tr (Hg\u25e6f (x)) + 2 ||Jg\u25e6f (x)||2F (18)\nThe first term of the equation scales with the reconstruction cost and will diminish accordingly. The second term of the equation is the Froebenius norm of the Jacobian. Note however that the Jacobian in this case is on the reconstruction and not on the representation as with our proposed penality.\nWhy Jf (x) is a better choice than Jg\u25e6f (x) . Adding Gaussian noise during the training of an auto-encoder is thus asymptotically equivalent to adding a regularization term to the objective function. In the DAE setting with MSE cost, the penalty term is the norm of the Jacobian of the reconstruction units with respect to the input units. This encourages the output to be invariant to small changes in the inputs. Note that the invariance requirement is shared between the two parts of the auto-encoder and not explicitly on the representation. If the goal of the auto-encoder is to initialise the weights of a Multi-Layer Perceptron (MLP), we do not care about the invariance of the reconstruction since we only use the representation. We would like the invariances captured by the autoecnoder to be predominantly on the representation. By using as a penality the norm of the Jacobian of the representation, we are doing this explicitly.\nThe other drawback of having a regularization over Jg\u25e6f (x) comes from an optimization point of view. The gradient of the total cost function with respect\nto the parameters of the encoder W is a function of Jg(f). Since Jg\u25e6f (x) = Jg(f).Jf (x), minimizing Jg\u25e6f (x) by reducing Jg(f) could lead to difficulties in minimizing the reconstruction cost."}, {"heading": "5 Experiments and results", "text": "Considered models. In our experiments, we compare the proposed Contracting Auto Encoder (CAE) against the following models for unsupervised feature extraction: \u2022 RBM-binary : Restricted Boltzmann Machine trained by Contrastive Di-\nvergence,\n\u2022 AE: Basic auto-encoder, \u2022 AE+wd: Auto-encoder with weight-decay regularization, \u2022 DAE-g: Denoising auto-encoder with Gaussian noise, \u2022 DAE-b: Denoising auto-encoder with binary masking noise, All auto-encoder variants used tied weights, a sigmoid activation function for both encoder and decoder, and a cross-entropy reconstruction error (see Section 3). They were trained by optimizing their (regularized) objective function on the training set by stochastic gradient descent. As for RBMs, they were trained by Contrastive Divergence.\nThese algorithms were applied on the training set without using the labels (unsupervised) to extract a first layer of features. Optionally the procedure was repeated to stack additional feature-extraction layers on top of the first one. Once thus trained, the learnt parameter values of the resulting featureextractors (weight and bias of the encoder) were used as initialisation of a multilayer perceptron (MLP) with an extra random-initialised output layer. The whole network was then fine-tuned by a gradient descent on a supervised objective appropriate for classification 5, using the labels in the training set.\nDatasets used. We have tested our approach on a benchmark of image classification problems, namely: \u2022 CIFAR-10: the image-classification task (32 \u00d7 32 \u00d7 3 channels RGB)\n(Krizhevsky and Hinton, 2009).\n\u2022 CIFAR-bw: a gray-scale version of the original CIFAR-10. The gray-scale versions were obtained with a color weighting of 0.3 for red, 0.59 for green and 0.11 for blue. \u2022 MNIST: the well-known digit classification problem (28 \u00d7 28 gray-scale pixel values scaled to [0,1]). It has 50000 examples for training, 10000 for validation, and 10000 for test.\nSix harder digit recognition problems used in the benchmark of Larochelle et al. (2007)6. They were derived by adding extra factors of variation to MNIST digits. Each has 10000 examples for training, 2000 for validation, 50000 for test. \u2022 basic: Smaller subset of MNIST. \u2022 rot: digits with added random rotation. \u2022 bg-rand: digits with random noise background. \u2022 bg-img: digits with random image background.\n5We used sigmoid+cross-entropy for binary classification, and log of softmax for multi-class problems\n6Datasets available at http://www.iro.umontreal.ca/~lisa/icml2007.\n\u2022 bg-img-rot: digits with rotation and image background. Two artificial shape classification problems from the benchmark of Larochelle et al. (2007): \u2022 rect: Discriminate between tall and wide rectangles (white on black). \u2022 rect-img: Discriminate between tall and wide rectangular image on a dif-\nferent background image."}, {"heading": "5.1 Classification performance", "text": ""}, {"heading": "5.1.1 MNIST and CIFAR-bw", "text": "Our first series of experiments focuses on the MNIST and CIFAR-bw datasets. We compare the classification performance obtained by a neural network with one hidden layer of 1000 units, initialized with each of the unsupervised algorithms under consideration. For each case, we selected the value of hyperparameters (such as the strength of regularization) that yielded, after supervised fine-tuning, the best classification performance on the validation set. Final classification error rate was then computed on the test set. With the parameters obtained after unsupervised pre-training (before fine-tuning), we also computed in each case the average value of the encoder\u2019s contraction \u2016Jf (x)\u2016F on the validation set, as well as a measure of the average fraction of saturated units per example7. These results are reported in Table 1. We see that the local contraction measure (the average \u2016Jf\u2016F ) on the pre-trained model strongly correlates with the final classification error. The CAE, which explicitly tries to minimize this measure while maintaining a good reconstruction, is the bestperforming model. datasets.\nResults given in Table 2 compare the performance of stacked CAEs on the benchmark problems of Larochelle et al. (2007) to the three-layer models reported in Vincent et al. (2010). Stacking a second layer CAE on top of a first layer appears to significantly improves performance, thus demonstrating their usefulness for building deep networks. Moreover on the majority of datasets, 2-layer CAE beat the state-of-the-art 3-layer model."}, {"heading": "5.1.2 CIFAR-10", "text": "The pipeline of preprocessing steps we used here is similar to ?. We randomly extracted 160000 patches 8 \u00d7 8 from the 10000 first images of CIFAR-10. For each patch, we substract the mean and divide by the standard deviation ( local contrast normalization). Then, a PCA is fitted on this set of patches. The 2 first components (corresponding to black patches) are dropped but we kept the next 80 first components (over 192). For building the final training set of patches, we project these patches on the PCA components, perform whitening i.e divide by the eigen values, and pass it through a logistic funtion in order to map it to [0, 1].\nA Contracting-Auto-Encoder with a number of hidden units nhid \u2208 {50, 100, 200, 400} is trained on this set by minimizing the cross-entropy reconstruction error and the regularizer with stochastic gradient descent. We present some filters learned during this process in Figure XX.\n7We consider a unit saturated if its activation is below 0.05 or above 0.95. Note that in general the set of saturated units is expected to vary with each example.\nFinally, we evaluate the classification performance of our algorithm with a linear classifier. The preprocessing steps are applied convolutionnaly with a stride equal to one to get nhid feature maps of size 25\u00d7 25. Features are sumpooled together over the quadrants of the feature maps. By applying this coarse dimensionality reduction technique, we obtain features of dimension 4nhid. We fed a linear L2-regularized SVM with these features and reported the test classification accuracy in TAB XX. L2 regularizer was chosen using a 5-fold cross validation."}, {"heading": "5.2 Closer examination of the contraction", "text": "To better understand the feature extractor produced by each algorithm, in terms of their contractive properties, we used the following analytical tools:\nWhat happens locally: looking at the singular values of the Jacobian. A high dimensional Jacobian contains directional information: the amount of contraction is generally not the same in all directions. This can be examined by performing a singular value decomposition of Jf . We computed the average singular value spectrum of the Jacobian over the validation set for the above models. Results are shown in Figure 2 and will be discussed in section 5.3.\nWhat happens further away: contraction curves. The Frobenius norm of the Jacobian at some point x measures the contraction of the mapping locally at that point. Intuitively the contraction induced by the proposed penalty term can be measured beyond the immediate training examples, by the ratio of the distances between two points in their original (input) space and their distance once mapped in the feature space. We call this measure contraction ratio. In the limit where the variation in the input space is infinitesimal, this\ncorresponds to the derivative (i.e. Jacobian) of the representation map. For any encoding function f , we can measure the average contraction ratio for pairs of points, one of which, x0 is picked from the validation set, and the other x1 randomly generated on a sphere of radius r centered on x0 in input space. How this average ratio evolves as a function of r yields a contraction curve. We have computed these curves for the models for which we reported classification performance (the contraction curves are however computed with their initial parameters prior to fine tuning). Results are shown in Figure 1 for single-layer mappings and in Figure 3 for 2 and 3 layer mappings. They will be discussed in detail in the next section."}, {"heading": "5.3 Discussion: Local Space Contraction", "text": "From a geometrical point of view, the robustness of the features can be seen as a contraction of the input space when projected in the feature space, in particular in the neighborhood of the examples from the data-generating distribution: otherwise (if the contraction was the same at all distances) it would not be useful, because it would just be a global scaling. This is happening with the proposed penalty, but rarely so without it, as illustrated on the contraction curves of Figure 1. For all algorithms tested except the proposed CAE and the Gaussian corruption DAE (DAE-g), the contraction ratio decreases (i.e., towards more contraction) as we move away from the training examples (this is due to more saturation, and was expected), whereas for the CAE the contraction ratio initially increases, up to the point where the effect of saturation takes over (the bump occurs at about the maximum distance between two training examples).\nThink about the case where the training examples congregate near a lowdimensional manifold. The variations present in the data (e.g. translation and rotations of objects in images) correspond to local dimensions along the manifold, while the variations that are small or rare in the data correspond to the directions orthogonal to the manifold (at a particular point near the manifold, corresponding to a particular example). The proposed criterion is trying to make the features invariant in all directions around the training examples, but the reconstruction error (or likelihood) is making sure that that the represen-\ntation is faithful, i.e., can be used to reconstruct the input example. Hence the directions that resist to this contracting pressure (strong invariance to input changes) are the directions present in the training set. Indeed, if the variations along these directions present in the training set were not preserved, neighboring training examples could not be distinguished and properly reconstructed. Hence the directions where the contraction is strong (small ratio, small singular values of the Jacobian matrix) are also the directions where the model believes that the input density drops quickly, whereas the directions where the contraction is weak (closer to 1, larger contraction ratio, larger singular values of the Jacobian matrix) correspond to the directions where the model believes that the input density is flat (and large, since we are near a training example).\nWe believe that this contraction penalty thus helps the learner carve a kind of mountain supported by the training examples, and generalizing to a ridge between them. What we would like is for these ridges to correspond to some directions of variation present in the data, associated to underlying factors of variation. How far do these ridges extend around each training example and how flat are they? This can be visualized comparatively with the analysis of Figure 1, with the contraction ratio for different distances from the training examples.\nNote that different features (elements of the representation vector) would be expected to have ridges (i.e. directions of invariance) in different directions, and that the \u201cdimensionality\u201d of these ridges (we are in a fairly high-dimensional space) gives a hint as to the local dimensionality of the manifold near which the data examples congregate. The singular value spectrum of the Jacobian informs us about that geometry. The number of large singular values should reveal the dimensionality of these ridges, i.e., of that manifold near which examples concentrate. This is illustrated in Figure 2, showing the singular values spectrum of the encoder\u2019s Jacobian. The CAE by far does the best job at representing the data variations near a lower-dimensional manifold, and the DAE is second best, while ordinary auto-encoders (regularized or not) do not succeed at all in this respect.\nWhat happens when we stack a CAE on top of another one, to build a deeper encoder? This is illustrated in Figure 3, which shows the average contraction ratio for different distances around each training point, for depth 1 vs depth 2 encoders.Composing two CAEs yields even more contraction and even more non-linearity, i.e. a sharper profile, with a flatter level of contraction at short and medium distances, and a delayed effect of saturation (the bump only comes up at farther distances). We would thus expect higher-level features to be more invariant in their feature-specific directions of invariance, which is exactly the kind of property that motivates the use of deeper architectures."}, {"heading": "6 Conclusion", "text": "In this paper, we attempt to answer the following question: what makes a good representation?. Besides being useful for a particular task, which we can measure, or towards which we can train a representation, this paper highlights the advantages for representations to be locally invariant in many directions of change of the raw input. This idea is implemented by a penalty on the Frobenius norm of the Jacobian matrix of the encoder mapping, which computes the\nrepresentation. The paper also introduces empirical measures of robustness and invariance, based on the contraction ratio of the learned mapping, at different distances and in different directions around the training examples. We hypothesize that this reveals the manifold structure learned by the model, and we find (by looking at the singular value spectrum of the mapping) that the Contracting Auto-Encoder discovers lower-dimensional manifolds. In addition, experiments on many datasets suggest that this penalty always helps an auto-encoder to perform better, and competes or improves upon the representations learned by Denoising Auto-Encoders or RBMs, in terms of classification error."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks, 2, 53\u201358.", "citeRegEx": "Baldi and Hornik,? 1989", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning , 2(1), 1\u2013127. Also published as a book. Now Publishers, 2009.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS 19 , pages 153\u2013160. MIT Press.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Training with noise is equivalent to Tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural Computation, 7(1), 108\u2013116.", "citeRegEx": "Bishop,? 1995", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "A. Ng"], "venue": "NIPS\u201909 , pages 646\u2013654.", "citeRegEx": "Goodfellow et al\\.,? 2009", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Estimation of non-normalized statistical models using score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, 6, 695\u2013709.", "citeRegEx": "Hyv\u00e4rinen,? 2005", "shortCiteRegEx": "Hyv\u00e4rinen", "year": 2005}, {"title": "Nonlinear autoassociation is not equivalent to PCA", "author": ["N. Japkowicz", "S.J. Hanson", "M.A. Gluck"], "venue": "Neural Computation, 12(3), 531\u2013545.", "citeRegEx": "Japkowicz et al\\.,? 2000", "shortCiteRegEx": "Japkowicz et al\\.", "year": 2000}, {"title": "What is the best multi-stage architecture for object recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "International Conference on Computer Vision (ICCV\u201909). IEEE.", "citeRegEx": "Jarrett et al\\.,? 2009", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "Technical report, Computational and Biological Learning Lab, Courant Institute, NYU. Tech Report CBLL-TR-2008-12-01.", "citeRegEx": "Kavukcuoglu et al\\.,? 2008", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2008}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun"], "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR\u201909). IEEE.", "citeRegEx": "Kavukcuoglu et al\\.,? 2009", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Regularized estimation of image statistics by score matching", "author": ["D. Kingma", "Y. LeCun"], "venue": "J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23 , pages 1126\u20131134.", "citeRegEx": "Kingma and LeCun,? 2010", "shortCiteRegEx": "Kingma and LeCun", "year": 2010}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Z. Ghahramani, editor, ICML 2007 , pages 473\u2013480. ACM.", "citeRegEx": "Larochelle et al\\.,? 2007", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "NIPS\u201907 , pages 873\u2013880. MIT Press, Cambridge, MA.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "L. Bottou and M. Littman, editors, ICML 2009 . ACM, Montreal (Qc), Canada.", "citeRegEx": "Lee et al\\.,? 2009", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "NIPS\u201906 .", "citeRegEx": "Ranzato et al\\.,? 2007", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323, 533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A."], "venue": "ICML 2008 .", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "JMLR, 11(3371\u20133408).", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Deep learning via semisupervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008 , pages 1168\u20131175, New York, NY, USA. ACM.", "citeRegEx": "Weston et al\\.,? 2008", "shortCiteRegEx": "Weston et al\\.", "year": 2008}, {"title": "Deconvolutional networks", "author": ["M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus"], "venue": "Proc. of the 23rd IEEE Computer Scociety Conference on Computer Vision and Pattern Recognition (CVPR). 14", "citeRegEx": "Zeiler et al\\.,? 2010", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "See Bengio (2009) for a recent review of Deep Learning algorithms.", "startOffset": 4, "endOffset": 18}, {"referenceID": 2, "context": "Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).", "startOffset": 83, "endOffset": 148}, {"referenceID": 8, "context": "Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).", "startOffset": 83, "endOffset": 148}, {"referenceID": 19, "context": "Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).", "startOffset": 83, "endOffset": 148}, {"referenceID": 4, "context": "Previous work has shown that deep learners can discover representations whose features are invariant to some of the factors of variation in the input (Goodfellow et al., 2009).", "startOffset": 150, "endOffset": 175}, {"referenceID": 2, "context": "remains to be done to understand the characteristics and theoretical advantages of the representations learned by a Restricted Boltzmann Machine Hinton et al. (2006), an auto-encoder Bengio et al.", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al.", "startOffset": 24, "endOffset": 87}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al.", "startOffset": 24, "endOffset": 110}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al.", "startOffset": 24, "endOffset": 137}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al.", "startOffset": 24, "endOffset": 159}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation.", "startOffset": 24, "endOffset": 210}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al.", "startOffset": 24, "endOffset": 418}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images).", "startOffset": 24, "endOffset": 694}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images). A simple approach, used here, to empirically verify that the learned representations are useful, is to use them to initialize a classifier (such as a multi-layer neural network), and measure classification error. Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008). Contribution. What principles should guide the learning of such intermediate representations? They should capture as much as possible of the information in each given example, when that example is likely under the underlying generating distribution. That is what auto-encoders Vincent et al. (2008) and sparse coding aim to achieve when minimizing reconstruction error.", "startOffset": 24, "endOffset": 1472}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images). A simple approach, used here, to empirically verify that the learned representations are useful, is to use them to initialize a classifier (such as a multi-layer neural network), and measure classification error. Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008). Contribution. What principles should guide the learning of such intermediate representations? They should capture as much as possible of the information in each given example, when that example is likely under the underlying generating distribution. That is what auto-encoders Vincent et al. (2008) and sparse coding aim to achieve when minimizing reconstruction error. We would also like these representations to be useful in characterizing the input distribution, and that is what is achieved by directly optimizing a generative model\u2019s likelihood (such as RBMs), or a proxy, such as Score Matching Hyv\u00e4rinen (2005). In this paper, we introduce a penalty term that could be added to either of the above contexts, which encourages the intermediate representation to be robust to small changes of the input around the training examples.", "startOffset": 24, "endOffset": 1791}, {"referenceID": 17, "context": "It was introduced in the late 80\u2019s Rumelhart et al. (1986); Baldi and Hornik (1989) as a technique for dimensionality reduction, where the output of the encoder represents the reduced representation and where the decoder is tuned to reconstruct the initial input from the encoder\u2019s representation through the minimization of a cost function.", "startOffset": 35, "endOffset": 59}, {"referenceID": 0, "context": "(1986); Baldi and Hornik (1989) as a technique for dimensionality reduction, where the output of the encoder represents the reduced representation and where the decoder is tuned to reconstruct the initial input from the encoder\u2019s representation through the minimization of a cost function.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "With the use of non-linear activation functions an AE can however be expected to learn more useful feature-detectors than what can be obtained with a simple PCA (Japkowicz et al., 2000).", "startOffset": 161, "endOffset": 185}, {"referenceID": 0, "context": "dimension (hence forming a bottleneck), it has been shown that the learnt parameters of the encoder are a subspace of the principal components of the input space Baldi and Hornik (1989). With the use of non-linear activation functions an AE can however be expected to learn more useful feature-detectors than what can be obtained with a simple PCA (Japkowicz et al.", "startOffset": 162, "endOffset": 186}, {"referenceID": 9, "context": "this viewpoint, several techniques have been developed to encourage sparsity of the representation (Kavukcuoglu et al., 2008; Lee et al., 2008).", "startOffset": 99, "endOffset": 143}, {"referenceID": 14, "context": "this viewpoint, several techniques have been developed to encourage sparsity of the representation (Kavukcuoglu et al., 2008; Lee et al., 2008).", "startOffset": 99, "endOffset": 143}, {"referenceID": 19, "context": "Robustness to input perturbations was also one of the motivation of the denoising auto-encoder, as stated in Vincent et al. (2010). The CAE and the DAE differ however in the following ways:", "startOffset": 109, "endOffset": 131}, {"referenceID": 3, "context": "If the noise used in the DAE is Gaussian, its effect can be approximated analytically with an added penality term on a standard autoencoder cost functionBishop (1995) and Kingma and LeCun (2010).", "startOffset": 153, "endOffset": 167}, {"referenceID": 3, "context": "If the noise used in the DAE is Gaussian, its effect can be approximated analytically with an added penality term on a standard autoencoder cost functionBishop (1995) and Kingma and LeCun (2010). Let us define L(\u03b8, x) as the loss function of the auto-encoder.", "startOffset": 153, "endOffset": 195}, {"referenceID": 12, "context": "We have tested our approach on a benchmark of image classification problems, namely: \u2022 CIFAR-10: the image-classification task (32 \u00d7 32 \u00d7 3 channels RGB) (Krizhevsky and Hinton, 2009).", "startOffset": 154, "endOffset": 183}, {"referenceID": 12, "context": "We have tested our approach on a benchmark of image classification problems, namely: \u2022 CIFAR-10: the image-classification task (32 \u00d7 32 \u00d7 3 channels RGB) (Krizhevsky and Hinton, 2009). \u2022 CIFAR-bw: a gray-scale version of the original CIFAR-10. The gray-scale versions were obtained with a color weighting of 0.3 for red, 0.59 for green and 0.11 for blue. \u2022 MNIST: the well-known digit classification problem (28 \u00d7 28 gray-scale pixel values scaled to [0,1]). It has 50000 examples for training, 10000 for validation, and 10000 for test. Six harder digit recognition problems used in the benchmark of Larochelle et al. (2007). They were derived by adding extra factors of variation to MNIST digits.", "startOffset": 155, "endOffset": 625}, {"referenceID": 13, "context": "Two artificial shape classification problems from the benchmark of Larochelle et al. (2007): \u2022 rect: Discriminate between tall and wide rectangles (white on black).", "startOffset": 67, "endOffset": 92}, {"referenceID": 13, "context": "Results given in Table 2 compare the performance of stacked CAEs on the benchmark problems of Larochelle et al. (2007) to the three-layer models reported in Vincent et al.", "startOffset": 94, "endOffset": 119}, {"referenceID": 13, "context": "Results given in Table 2 compare the performance of stacked CAEs on the benchmark problems of Larochelle et al. (2007) to the three-layer models reported in Vincent et al. (2010). Stacking a second layer CAE on top of a first layer appears to significantly improves performance, thus demonstrating their usefulness for building deep networks.", "startOffset": 94, "endOffset": 179}], "year": 2011, "abstractText": "We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pre-training.", "creator": "LaTeX with hyperref package"}}}