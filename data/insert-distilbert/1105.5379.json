{"id": "1105.5379", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2011", "title": "Parallel Coordinate Descent for L1-Regularized Loss Minimization", "abstract": "we propose shotgun, a parallel coordinate descent algorithm for tightly minimizing l1 - regularized losses. though coordinate descent seems inherently sequential, we also prove convergence bounds for shotgun orbits which predict linear speedups, up to a problem - dependent limit. we present a comprehensive empirical study of shotgun for lasso and sparse logistic regression. sometimes our theoretical predictions on the potential for parallelism closely match behavior on real data. shotgun always outperforms other often published solvers on a range of large problems, often proving us to be one of the relatively most algorithms scalable algorithms for l1.", "histories": [["v1", "Thu, 26 May 2011 19:19:30 GMT  (221kb,D)", "http://arxiv.org/abs/1105.5379v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IT math.IT", "authors": ["joseph k bradley", "aapo kyrola", "danny bickson", "carlos guestrin"], "accepted": true, "id": "1105.5379"}, "pdf": {"name": "1105.5379.pdf", "metadata": {"source": "CRF", "title": "Parallel Coordinate Descent for L1-Regularized Loss Minimization", "authors": ["Joseph K. Bradley", "Aapo Kyrola", "Danny Bickson"], "emails": ["jkbradle@cs.cmu.edu", "akyrola@cs.cmu.edu", "bickson@cs.cmu.edu", "guestrin@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "Many applications use L1-regularized models such as the Lasso (Tibshirani, 1996) and sparse logistic regression (Ng, 2004). L1 regularization biases learning towards sparse solutions, and it is especially useful for high-dimensional problems with large numbers of features. For example, in logistic regression, it allows sample complexity to scale logarithmically w.r.t. the number of irrelevant features (Ng, 2004).\nMuch effort has been put into developing optimization algorithms for L1 models. These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al., 2007).\nCoordinate descent, which we call Shooting after Fu (1998), is a simple but very effective algorithm which updates one coordinate per iteration. It often requires no tuning of parameters, unlike, e.g., stochastic gradi-\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nent. As we discuss in Sec. 2, theory (Shalev-Shwartz & Tewari, 2009) and extensive empirical results (Yuan et al., 2010) have shown that variants of Shooting are particularly competitive for high-dimensional data.\nThe need for scalable optimization is growing as more applications use high-dimensional data, but processor core speeds have stopped increasing in recent years. Instead, computers come with more cores, and the new challenge is utilizing them efficiently. Yet despite the many sequential optimization algorithms for L1regularized losses, few parallel algorithms exist.\nSome algorithms, such as interior point methods, can benefit from parallel matrix-vector operations. However, we found empirically that such algorithms were often outperformed by Shooting.\nRecent work analyzes parallel stochastic gradient descent for multicore (Langford et al., 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010). These methods parallelize over samples. In applications using L1 regularization, though, there are often many more features than samples, so parallelizing over samples may be of limited utility.\nWe therefore take an orthogonal approach and parallelize over features, with a remarkable result: we can parallelize coordinate descent\u2014an algorithm which seems inherently sequential\u2014for L1-regularized losses. In Sec. 3, we propose Shotgun, a simple multicore algorithm which makes P coordinate updates in parallel. We prove strong convergence bounds for Shotgun which predict speedups over Shooting which are linear in P, up to a problem-dependent maximum P\u2217. Moreover, our theory provides an estimate for this ideal P\u2217 which may be easily computed from the data.\nParallel coordinate descent was also considered by Tsitsiklis et al. (1986), but for differentiable objectives in the asynchronous setting. They give a very\n\u2020 These authors contributed equally to this work.\nar X\niv :1\n10 5.\n53 79\nv1 [\ncs .L\nG ]\n2 6\nM ay\n2 01\ngeneral analysis, proving asymptotic convergence but not convergence rates. We are able to prove rates and theoretical speedups for our class of objectives.\nIn Sec. 4, we compare multicore Shotgun with five state-of-the-art algorithms on 35 real and synthetic datasets. The results show that in large problems Shotgun outperforms the other algorithms. Our experiments also validate the theoretical predictions by showing that Shotgun requires only about 1/P as many iterations as Shooting. We measure the parallel speedup in running time and analyze the limitations imposed by the multicore hardware.\n2. L1-Regularized Loss Minimization\nWe consider optimization problems of the form\nmin x\u2208Rd F (x) = n\u2211 i=1 L(aTi x, yi) + \u03bb\u2016x\u20161 , (1)\nwhere L(\u00b7) is a non-negative convex loss. Each of n samples has a feature vector ai \u2208 Rd and observation yi (where y \u2208 Yn). x \u2208 Rd is an unknown vector of weights for features. \u03bb \u2265 0 is a regularization parameter. Let A \u2208 Rn\u00d7d be the design matrix, whose ith row is ai. Assume w.l.o.g. that columns of A are normalized s.t. diag(ATA) = 1.1\nAn instance of (1) is the Lasso (Tibshirani, 1996) (in penalty form), for which Y \u2261 R and\nF (x) = 1 2 \u2016Ax\u2212 y\u201622 + \u03bb\u2016x\u20161 , (2)\nas well as sparse logistic regression (Ng, 2004), for which Y \u2261 {\u22121,+1} and\nF (x) = n\u2211 i=1 log ( 1 + exp ( \u2212yiaTi x )) + \u03bb\u2016x\u20161 . (3)\nFor analysis, we follow Shalev-Shwartz and Tewari (2009) and transform (1) into an equivalent problem with a twice-differentiable regularizer. We let x\u0302 \u2208 R2d+ , use duplicated features a\u0302i = [ai;\u2212ai] \u2208 R2d, and solve\nmin x\u0302\u2208R2d+ n\u2211 i=1 L(a\u0302Ti x\u0302, yi) + \u03bb 2d\u2211 j=1 x\u0302j . (4)\nIf x\u0302 \u2208 R2d+ minimizes (4), then x : xi = x\u0302d+i\u2212 x\u0302i minimizes (1). Though our analysis uses duplicate features, they are not needed for an implementation."}, {"heading": "2.1. Sequential Coordinate Descent", "text": "Shalev-Shwartz and Tewari (2009) analyze Stochastic Coordinate Descent (SCD), a stochastic version\n1Normalizing A does not change the objective if a separate, normalized \u03bbj is used for each xj .\nAlgorithm 1 Shooting: Sequential SCD\nSet x = 0 \u2208 R2d+ . while not converged do\nChoose j \u2208 {1, . . . , 2d} uniformly at random. Set \u03b4xj \u2190\u2212 max{\u2212xj ,\u2212(\u2207F (x))j/\u03b2}. Update xj \u2190\u2212 xj + \u03b4xj .\nend while\nof Shooting for solving (1). SCD (Alg. 1) randomly chooses one weight xj to update per iteration. It computes the update xj \u2190 xj + \u03b4xj via\n\u03b4xj = max{\u2212xj ,\u2212(\u2207F (x))j/\u03b2} , (5)\nwhere \u03b2 > 0 is a loss-dependent constant.\nTo our knowledge, Shalev-Shwartz and Tewari (2009) provide the best known convergence bounds for SCD. Their analysis requires a uniform upper bound on the change in the loss F (x) from updating a single weight:\nAssumption 2.1. Let F (x) : R2d+ \u2212\u2192 R be a convex function. Assume there exists \u03b2 > 0 s.t., for all x and single-weight updates \u03b4xj, we have:\nF (x + (\u03b4xj)e j) \u2264 F (x) + \u03b4xj(\u2207F (x))j + \u03b2(\u03b4xj)\n2\n2 ,\nwhere ej is a unit vector with 1 in its jth entry. For the losses in (2) and (3), Taylor expansions give\n\u03b2 = 1 (squared loss) and \u03b2 = 14 (logistic loss). (6)\nUsing this bound, they prove the following theorem.\nTheorem 2.1. (Shalev-Shwartz & Tewari, 2009) Let x\u2217 minimize (4) and x(T ) be the output of Alg. 1 after T iterations. If F (x) satisfies Assumption 2.1, then\nE [ F (x(T )) ] \u2212 F (x\u2217) \u2264 d(\u03b2\u2016x\n\u2217\u201622 + 2F (x(0))) T + 1 , (7)\nwhere E[\u00b7] is w.r.t. the random choices of weights j.\nAs Shalev-Shwartz and Tewari (2009) argue, Theorem 2.1 indicates that SCD scales well in the dimensionality d of the data. For example, it achieves better runtime bounds w.r.t. d than stochastic gradient methods such as SMIDAS (Shalev-Shwartz & Tewari, 2009) and truncated gradient (Langford et al., 2009a)."}, {"heading": "3. Parallel Coordinate Descent", "text": "As the dimensionality d or sample size n increase, even fast sequential algorithms become expensive. To scale to larger problems, we turn to parallel computation. In this section, we present our main theoretical contribution: we show coordinate descent can be parallelized by proving strong convergence bounds.\nAlgorithm 2 Shotgun: Parallel SCD\nChoose number of parallel updates P \u2265 1. Set x = 0 \u2208 R2d+ while not converged do\nIn parallel on P processors Choose j \u2208 {1, . . . , 2d} uniformly at random. Set \u03b4xj \u2190\u2212 max{\u2212xj ,\u2212(\u2207F (x))j/\u03b2}. Update xj \u2190\u2212 xj + \u03b4xj .\nend while\nWe parallelize stochastic Shooting and call our algorithm Shotgun (Alg. 2). Shotgun initially chooses P, the number of weights to update in parallel. On each iteration, it chooses P weights independently and uniformly at random from {1, . . . , 2d}; these form a multiset Pt. It updates each xij : ij \u2208 Pt, in parallel using the same update as Shooting (5). Let \u2206x be the collective update to x, i.e., (\u2206x)k = \u2211 ij\u2208Pt: k=ij \u03b4xij .\nIntuitively, parallel updates might increase the risk of divergence. In Fig. 1, in the left subplot, parallel updates speed up convergence since features are uncorrelated; in the right subplot, parallel updates of correlated features risk increasing the objective. We can avoid divergence by imposing a step size, but our experiments showed that approach to be impractical.2\nWe formalize this intuition for the Lasso in Theorem 3.1. We can separate a sequential progress term (summing the improvement from separate updates) from a term measuring interference between parallel updates. If ATA were normalized and centered to be a covariance matrix, the elements in the interference term\u2019s sum would be non-zero only for correlated variables, matching our intuition from Fig. 1. Harmful interference could occur when, e.g., \u03b4xi, \u03b4xj > 0 and features i, j were positively correlated.\nTheorem 3.1. Fix x. If \u2206x is the collective update to x in one iteration of Alg. 2 for the Lasso, then\nF (x + \u2206x)\u2212 F (x) \u2264 \u2212 12 \u2211 ij\u2208Pt (\u03b4xij ) 2\n\ufe38 \ufe37\ufe37 \ufe38 sequential progress\n+ 12 \u2211 ij ,ik\u2208Pt, j 6=k (ATA)ij ,ik\u03b4xij\u03b4xik\n\ufe38 \ufe37\ufe37 \ufe38 interference\n.\nProof Sketch3: Write the Taylor expansion of F around x. Bound the first-order term using (5). In the next section, we show that this intuition holds for the more general optimization problem in (1).\n2A step size of 1 P\nensures convergence since F is convex in x, but it results in very small steps and long runtimes.\n3We include detailed proofs of all theorems and lemmas in the supplementary material."}, {"heading": "3.1. Shotgun Convergence Analysis", "text": "In this section, we present our convergence result for Shotgun. The result provides a problem-specific measure of the potential for parallelization: the spectral radius \u03c1 of ATA (i.e., the maximum of the magnitudes of eigenvalues of ATA). Moreover, this measure is prescriptive: \u03c1 may be estimated via, e.g., power iteration4 (Strang, 1988), and it provides a plug-in estimate of the ideal number of parallel updates.\nWe begin by generalizing Assumption 2.1 to our parallel setting. The scalars \u03b2 for Lasso and logistic regression remain the same as in (6).\nAssumption 3.1. Let F (x) : R2d+ \u2212\u2192 R be a convex function. Assume that there exists \u03b2 > 0 such that, for all x and parallel updates \u2206x, we have\nF (x + \u2206x) \u2264 F (x) + \u2206xT\u2207F (x) + \u03b2 2 \u2206xTATA\u2206x .\nWe now state our main result, generalizing the convergence bound in Theorem 2.1 to the Shotgun algorithm. Theorem 3.2. Let x\u2217 minimize (4) and x(T ) be the output of Alg. 2 after T iterations with P parallel updates/iteration. Let \u03c1 be the spectral radius of ATA. If F (x) satisfies Assumption 3.1 and P < 2d\u03c1 + 1, then\nE [ F (x(T )) ] \u2212 F (x\u2217) \u2264 d ( \u03b2\u2016x\u2217\u201622 + 2F (x(0)) ) (T + 1)P ,\nwhere the expectation is w.r.t. the random choices of weights to update. Choosing a maximal P\u2217 \u2248 2d\u03c1 gives\nE [ F (x(T )) ] \u2212 F (x\u2217) .\n\u03c1 ( \u03b2 2 \u2016x \u2217\u201622 + F (x(0)) )\nT + 1 .\nWithout duplicated features, Theorem 3.2 predicts that we can do up to P < d\u03c1 + 1 parallel updates and achieve speedups linear in P. We denote the predicted\n4For our datasets, power iteration gave reasonable estimates within a small fraction of the total runtime.\nmaximum P as P\u2217 \u2261 ceiling(d\u03c1 ). For an ideal problem with uncorrelated features, \u03c1 = 1, so we could do up to P\u2217 = d parallel updates. For a pathological problem with exactly correlated features, \u03c1 = d, so our theorem tells us that we could not do parallel updates. With P = 1, we recover the result for Shooting in Theorem 2.1.\nTo prove Theorem 3.2, we first bound the negative impact of interference between parallel updates.\nLemma 3.3. Fix x. Under the assumptions and definitions from Theorem 3.2, if \u2206x is the collective update to x in one iteration of Alg. 2, then\nEPt [F (x + \u2206x)\u2212 F (x)] \u2264 PEj [ \u03b4xj(\u2207F (x))j + \u03b22 ( 1\u2212 (P\u22121)\u03c12d ) (\u03b4xj) 2 ] ,\nwhere EPt is w.r.t. a random choice of Pt and Ej is w.r.t. choosing j \u2208 {1, . . . , 2d} uniformly at random.\nProof Sketch: Take the expectation w.r.t. Pt of the inequality in Assumption 3.1.\nEPt [F (x + \u2206x)\u2212 F (x)] \u2264 EPt [ \u2206xT\u2207F (x) + \u03b2 2 \u2206xTATA\u2206x ] (8) Separate the diagonal elements from the second order term, and rewrite the expectation using our independent choices of ij \u2208 Pt. (Here, \u03b4xj is the update given by (5), regardless of whether j \u2208 Pt.)\n= PEj [ \u03b4xj(\u2207F (x))j + \u03b22 (\u03b4xj) 2 ]\n+\u03b2 2 P(P\u2212 1)Ei\n[ Ej [ \u03b4xi(A TA)i,j\u03b4xj ]] (9)\nUpper bound the double expectation in terms of Ej [ (\u03b4xj) 2 ] by expressing the spectral radius \u03c1 of ATA as \u03c1 = maxz: zT z=1 z T (ATA)z.\nEi [ Ej [ \u03b4xi(A TA)i,j\u03b4xj ]] \u2264 \u03c1 2d Ej [ (\u03b4xj) 2] (10) Combine (10) back into (9), and rearrange terms to get the lemma\u2019s result.\nProof Sketch (Theorem 3.2): Our proof resembles Shalev-Shwartz and Tewari (2009)\u2019s proof of Theorem 2.1. The result from Lemma 3.3 replaces Assumption 2.1. One bound requires (P\u22121)\u03c12d < 1.\nOur analysis implicitly assumes that parallel updates of the same weight xj will not make xj negative. Proper write-conflict resolution can ensure this assumption holds and is viable in our multicore setting."}, {"heading": "3.2. Theory vs. Empirical Performance", "text": "We end this section by comparing the predictions of Theorem 3.2 about the number of parallel updates P with empirical performance for Lasso. We exactly\nsimulated Shotgun as in Alg. 2 to eliminate effects from the practical implementation choices made in Sec. 4. We tested two single-pixel camera datasets from Duarte et al. (2008) with very different \u03c1, estimating EPt [ F (x(T )) ] by averaging 10 runs of Shotgun. We used \u03bb = 0.5 for Ball64 singlepixcam to get x\u2217 with about 27% non-zeros; we used \u03bb = 0.05 for Mug32 singlepixcam to get about 20% non-zeros.\nFig. 2 plots P versus the iterations T required for EPt [ F (x(T )) ] to come within 0.5% of the optimum F (x\u2217). Theorem 3.2 predicts that T should decrease as 1\nP as long as P < P\u2217 \u2248 d\u03c1 +1. The empirical behavior follows this theory: using the predicted P\u2217 gives almost optimal speedups, and speedups are almost linear in P. As P exceeds P\u2217, Shotgun soon diverges.\nFig. 2 confirms Theorem 3.2\u2019s result: Shooting, a seemingly sequential algorithm, can be parallelized and achieve near-linear speedups, and the spectral radius of ATA succinctly captures the potential for parallelism in a problem. To our knowledge, our convergence results are the first for parallel coordinate descent for L1-regularized losses, and they apply to any convex loss satisfying Assumption 3.1. Though Fig. 2 ignores certain implementation issues, we show in the next section that Shotgun performs well in practice."}, {"heading": "3.3. Beyond L1", "text": "Theorems 2.1 and 3.2 generalize beyond L1, for their main requirements (Assumptions 2.1, 3.1) apply to a more general class of problems: minF (x) s.t. x \u2265 0, where F (x) is smooth. We discuss Shooting and Shotgun for sparse regression since both the method (coordinate descent) and problem (sparse regression) are arguably most useful for high-dimensional settings."}, {"heading": "4. Experimental Results", "text": "We present an extensive study of Shotgun for the Lasso and sparse logistic regression. On a wide variety of datasets, we compare Shotgun with published state-ofthe-art solvers. We also analyze self-speedup in detail in terms of Theorem 3.2 and hardware issues."}, {"heading": "4.1. Lasso", "text": "We tested Shooting and Shotgun for the Lasso against five published Lasso solvers on 35 datasets. We summarize the results here; details are in the supplement.\n4.1.1. Implementation: Shotgun\nOur implementation made several practical improvements to the basic Shooting and Shotgun algorithms.\nFollowing Friedman et al. (2010), we maintained a vector Ax to avoid repeated computation. We also used their pathwise optimization scheme: rather than directly solving with the given \u03bb, we solved with an exponentially decreasing sequence \u03bb1, \u03bb2, . . . , \u03bb. The solution x for \u03bbk is used to warm-start optimization for \u03bbk+1. This scheme can give significant speedups.\nThough our analysis is for the synchronous setting, our implementation was asynchronous because of the high cost of synchronization. We used atomic compare-andswap operations for updating the Ax vector.\nWe used C++ and the CILK++ library (Leiserson, 2009) for parallelism. All tests ran on an AMD processor using up to eight Opteron 8384 cores (2.69 GHz)."}, {"heading": "4.1.2. Other Algorithms", "text": "L1 LS (Kim et al., 2007) is a log-barrier interior point method. It uses Preconditioned Conjugate Gradient (PCG) to solve Newton steps iteratively and avoid explicitly inverting the Hessian. The implementation is in Matlab R\u00a9, but the expensive step (PCG) uses very efficient native Matlab calls. In our tests, matrixvector operations were parallelized on up to 8 cores.\nFPC AS (Wen et al., 2010) uses iterative shrinkage to estimate which elements of x should be non-zero, as well as their signs. This reduces the objective to a smooth, quadratic function which is then minimized.\nGPSR BB (Figueiredo et al., 2008) is a gradient projection method which uses line search and termination techniques tailored for the Lasso.\nHard l0 (Blumensath & Davies, 2009) uses iterative hard thresholding for compressed sensing. It sets all but the s largest weights to zero on each iteration. We set s as the sparsity obtained by Shooting.\nSpaRSA (Wright et al., 2009) is an accelerated iterative shrinkage/thresholding algorithm which solves a sequence of quadratic approximations of the objective.\nAs with Shotgun, all of Shooting, FPC AS, GPSR BB, and SpaRSA use pathwise optimization schemes.\nWe also tested published implementations of the classic algorithms GLMNET (Friedman et al., 2010) and LARS (Efron et al., 2004). Since we were unable to get them to run on our larger datasets, we exclude their results."}, {"heading": "4.1.3. Results", "text": "We divide our comparisons into four categories of datasets; the supplementary material has descriptions.\nSparco: Real-valued datasets of varying sparsity from the Sparco testbed (van den Berg et al., 2009). n \u2208 [12829166], d \u2208 [128, 29166]. Single-Pixel Camera: Dense compressed sensing problems from Duarte et al. (2008). n \u2208 [410, 4770], d \u2208 [1024, 16384]. Sparse Compressed Imaging : Similar to Single-Pixel Camera datasets, but with very sparse random \u22121/+ 1 measurement matrices. Created by us. n \u2208 [477, 32768], d \u2208 [954, 65536]. Large, Sparse Datasets: Very large and sparse problems, including predicting stock volatility from text in financial reports (Kogan et al., 2009). n \u2208 [30465, 209432], d \u2208 [209432, 5845762].\nWe ran each algorithm on each dataset with regularization \u03bb = 0.5 and 10. Fig. 3 shows runtime results, divided by dataset category. We omit runs which failed to converge within a reasonable time period.\nShotgun (with P = 8) consistently performs well, converging faster than other algorithms on most dataset categories. Shotgun does particularly well on the Large, Sparse Datasets category, for which most algorithms failed to converge anywhere near the ranges plotted in Fig. 3. The largest dataset, whose features are occurrences of bigrams in financial reports (Kogan et al., 2009), has 5 million features and 30K samples. On this dataset, Shooting converges but requires \u223c 4900 seconds, while Shotgun takes < 2000 seconds.\nOn the Single-Pixel Camera datasets, Shotgun (P = 8) is slower than Shooting. In fact, it is surprising that Shotgun converges at all with P = 8, for the plotted datasets all have P\u2217 = 3. Fig. 2 shows Shotgun with P > 4 diverging for the Ball64 singlepixcam dataset; however, after the practical adjustments to Shotgun used to produce Fig. 3, Shotgun converges with P = 8.\nAmong the other solvers, L1 LS is the most robust and even solves some of the Large, Sparse Datasets.\n1 10 100\n1\n10\n100\nO th\ne r\na lg\n. ru\nn ti m\ne (\ns e c )\nShotgun runtime (sec)\nShotgun faster\nShotgun slower\n1 10 40\n1\n10\n40\nO th\ne r\na lg\n. ru\nn ti m\ne (\ns e c )\nShotgun runtime (sec)\nShotgun faster\nShotgun slower\n1 10 44\n1\n10\n44\nO th\ne r\na lg\n. ru\nn ti m\ne (\ns e\nc )\nShotgun runtime (sec)\nShotgun faster\nShotgun slower\n10 100 1000 8000 10\n100\n1000\n8000\nO th\ne r\na lg\n. ru\nn ti m\ne (\ns e c )\nShotgun runtime (sec)\nShotgun faster\nShotgun slower\n(a) Sparco (b) Single-Pixel Camera (c) Sparse Compressed Img. (d) Large, Sparse Datasets\nP\u2217 \u2208 [3, 17366], avg 2987 P\u2217 = 3 P\u2217 \u2208 [2865, 11779], avg 7688 P\u2217 \u2208 [214, 2072], avg 1143\n1 1.2 1.4 1.6 1.8 2 1\n1.2\n1.4\n1.6\n1.8\n2\nShooting L1_LS FPC_AS GPSR_BB\nSpaRSA\nHard_l0\n1 1.2 1.4 1.6 1.8 2 1\n1.2\n1.4\n1.6\n1.8\n2\nShooting L1_LS FPC_AS GPSR_BB SpaRSA Hard_l0\n1 1.2 1.4 1.6 1.8 2 1\n1.2\n1.4\n1.6\n1.8\n2\nShooting L1_LS FPC_AS GPSR_BB SpaRSA Hard_l0\n1 1.2 1.4 1.6 1.8 2 1\n1.2\n1.4\n1.6\n1.8\n2\nShooting L1_LS FPC_AS\nGPSR_BB\nSpaRSA\nHard_l0\n1 1.2 1.4 1.6 1.8 2 1\n1.2\n1.4\n1.6\n1.8\n2\nShooting L1_LS FPC_AS\nGPSR_BB\nSpaRSA\nHard_l0\n1 1.2 1.4 1.6 1.8 2 1\n1.2\n1.4\n1.6\n1.8\n2\nShooting L1_LS FPC_AS GPSR_BB SpaRSA Hard_l0\nFigure 3. Runtime comparison of algorithms for the Lasso on 4 dataset categories. Each marker compares an algorithm with Shotgun (with P = 8) on one dataset (and one \u03bb \u2208 {0.5, 10}). Y-axis is that algorithm\u2019s running time; X-axis is Shotgun\u2019s (P=8) running time on the same problem. Markers above the diagonal line indicate that Shotgun was faster; markers below the line indicate Shotgun was slower.\nIt is difficult to compare optimization algorithms and their implementations. Algorithms\u2019 termination criteria differ; e.g., primal-dual methods such as L1 LS monitor the duality gap, while Shotgun monitors the change in x. Shooting and Shotgun were written in C++, which is generally fast; the other algorithms were in Matlab, which handles loops slowly but linear algebra quickly. Therefore, we emphasize major trends: Shotgun robustly handles a range of problems; Theorem 3.2 helps explain its speedups; and Shotgun generally outperforms published solvers for the Lasso."}, {"heading": "4.2. Sparse Logistic Regression", "text": "For logistic regression, we focus on comparing Shotgun with Stochastic Gradient Descent (SGD) variants. SGD methods are of particular interest to us since they are often considered to be very efficient, especially for learning with many samples; they often have convergence bounds independent of the number of samples.\nFor a large-scale comparison of various algorithms for sparse logistic regression, we refer the reader to the recent survey by Yuan et al. (2010). On L1 logreg (Koh et al., 2007) and CDN (Yuan et al., 2010), our results qualitatively matched their survey. Yuan et al. (2010) do not explore SGD empirically.\n4.2.1. Implementation: Shotgun CDN\nAs Yuan et al. (2010) show empirically, their Coordinate Descent Newton (CDN) method is often orders of magnitude faster than the basic Shooting algorithm (Alg. 1) for sparse logistic regression. Like Shooting, CDN does coordinate descent, but instead of using a fixed step size, it uses a backtracking line search starting at a quadratic approximation of the objective.\nAlthough our analysis uses the fixed step size in (5), we modified Shooting and Shotgun to use line searches as in CDN. We refer to CDN as Shooting CDN, and we refer to parallel CDN as Shotgun CDN.\nShooting CDN and Shotgun CDN maintain an active set of weights which are allowed to become non-zero; this scheme speeds up optimization, though it can limit parallelism by shrinking d."}, {"heading": "4.2.2. Other Algorithms", "text": "SGD iteratively updates x in a gradient direction estimated with one sample and scaled by a learning rate. We implemented SGD in C++ following, e.g., Zinkevich et al. (2010). We used lazy shrinkage updates (Langford et al., 2009a) to make use of sparsity in A. Choosing learning rates for SGD can be challenging. In our tests, constant rates led to faster convergence than decaying rates (decaying as 1/ \u221a T ). For each test, we tried 14 exponentially increasing rates in [10\u22124, 1] (in parallel) and chose the rate giving the best training objective. We did not use a sparsifying step for SGD.\nSMIDAS (Shalev-Shwartz & Tewari, 2009) uses stochastic mirror descent but truncates gradients to sparsify x. We tested their published C++ implementation.\nParallel SGD refers to Zinkevich et al. (2010)\u2019s work, which runs SGD in parallel on different subsamples of the data and averages the solutions x. We tested this method since it is one of the few existing methods for parallel regression, but we note that Zinkevich et al. (2010) did not address L1 regularization in their analysis. We averaged over 8 instances of SGD."}, {"heading": "4.2.3. Results", "text": "Fig. 4 plots training objectives and test accuracy (on a held-out 10% of the data) for two large datasets.\nThe zeta dataset 5 illustrates the regime with n d. It contains 500K samples with 2000 features and is fully dense (in A). SGD performs well and is fairly competitive with Shotgun CDN (with P = 8).\nThe rcv1 dataset 6 (Lewis et al., 2004) illustrates the high-dimensional regime (d > n). It has about twice as many features (44504) as samples (18217), with 17% non-zeros in A. Shotgun CDN (P = 8) was much faster than SGD, especially in terms of the objective. Parallel SGD performed almost identically to SGD.\nThough convergence bounds for SMIDAS are comparable to those for SGD, SMIDAS iterations take much longer due to the mirror descent updates. To execute 10M updates on the zeta dataset, SGD took 728 seconds, while SMIDAS took over 8500 seconds.\nThese results highlight how SGD is orthogonal to Shotgun: SGD can cope with large n, and Shotgun can cope with large d. A hybrid algorithm might be scalable in both n and d and, perhaps, be parallelized over both samples and features."}, {"heading": "4.3. Self-Speedup of Shotgun", "text": "To study the self-speedup of Shotgun Lasso and Shotgun CDN, we ran both solvers on our datasets with varying \u03bb, using varying P (number of parallel updates = number of cores). We recorded the running time as the first time when an algorithm came within 0.5% of the optimal objective, as computed by Shooting.\nFig. 5 shows results for both speedup (in time) and speedup in iterations until convergence. The speedups in iterations match Theorem 3.2 quite closely. However, relative speedups in iterations (about 8\u00d7) are not matched by speedups in runtime (about 2\u00d7 to 4\u00d7).\nWe thus discovered that speedups in time were limited by low-level technical issues. To understand the limiting factors, we analyzed various Shotgun-like algorithms to find bottlenecks.7 We found we were hitting the memory wall (Wulf & McKee, 1995); memory bus bandwidth and latency proved to be the most limiting factors. Each weight update requires an atomic update to the shared Ax vector, and the ratio of memory accesses to floating point operations is only O(1). Data\n5The zeta dataset is from the Pascal Large Scale Learning Challenge: http://www.mlbench.org/instructions/\n6Our version of the rcv1 dataset is from the LIBSVM repository (Chang & Lin, 2001).\n7See the supplement for the scalability analysis details.\naccesses have no temporal locality since each weight update uses a different column of A. We further validated these conclusions by monitoring CPU counters."}, {"heading": "5. Discussion", "text": "We introduced the Shotgun, a simple parallel algorithm for L1-regularized optimization. Our convergence results for Shotgun are the first such results for parallel coordinate descent with L1 regularization. Our bounds predict linear speedups, up to an interpretable, problem-dependent limit. In experiments, these predictions matched empirical behavior.\nExtensive comparisons showed that Shotgun outperforms state-of-the-art L1 solvers on many datasets. We believe that, currently, Shotgun is one of the most efficient and scalable solvers for L1-regularized problems.\nThe most exciting extension to this work might be the hybrid of SGD and Shotgun discussed in Sec. 4.3.\nCode, Data, and Benchmark Results: Available at http://www.select.cs.cmu.edu/projects"}, {"heading": "Acknowledgments", "text": "Thanks to John Langford, Guy Blelloch, Joseph Gonzalez, Yucheng Low and our reviewers for feedback. Funded by NSF IIS-0803333, NSF CNS-0721591, ARO MURI W911NF0710287, ARO MURI W911NF0810242."}], "references": [{"title": "Iterative hard thresholding for compressed sensing", "author": ["T. Blumensath", "M.E. Davies"], "venue": "Applied and Computational Harmonic Analysis,", "citeRegEx": "Blumensath and Davies,? \\Q2009\\E", "shortCiteRegEx": "Blumensath and Davies", "year": 2009}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "Single-pixel", "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics,", "citeRegEx": "Efron et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Efron et al\\.", "year": 2004}, {"title": "Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems", "author": ["Figueiredo", "M.A.T", "R.D. Nowak", "S.J. Wright"], "venue": "IEEE J. of Sel. Top. in Signal Processing,", "citeRegEx": "Figueiredo et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Figueiredo et al\\.", "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Journal of Statistical Software,", "citeRegEx": "Friedman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2010}, {"title": "Penalized regressions: The bridge versus the lasso", "author": ["W.J. Fu"], "venue": "J. of Comp. and Graphical Statistics,", "citeRegEx": "Fu,? \\Q1998\\E", "shortCiteRegEx": "Fu", "year": 1998}, {"title": "An interior-point method for large-scale `1-regularized least squares", "author": ["S.J. Kim", "K. Koh", "M. Lustig", "S. Boyd", "D. Gorinevsky"], "venue": "IEEE Journal of Sel. Top. in Signal Processing,", "citeRegEx": "Kim et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2007}, {"title": "Predicting risk from financial reports with regression", "author": ["S. Kogan", "D. Levin", "B.R. Routledge", "J.S. Sagi", "N.A. Smith"], "venue": "In Human Language Tech.-NAACL,", "citeRegEx": "Kogan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kogan et al\\.", "year": 2009}, {"title": "An interior-point method for large-scale l1-regularized logistic regression", "author": ["K. Koh", "Kim", "S.-J", "S. Boyd"], "venue": "JMLR, 8:1519\u20131555,", "citeRegEx": "Koh et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koh et al\\.", "year": 2007}, {"title": "Sparse online learning via truncated gradient", "author": ["J. Langford", "L. Li", "T. Zhang"], "venue": "In NIPS,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Slow learners are fast", "author": ["J. Langford", "A.J. Smola", "M. Zinkevich"], "venue": "In NIPS,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "The Cilk++ concurrency platform", "author": ["C.E. Leiserson"], "venue": "In 46th Annual Design Automation Conference. ACM,", "citeRegEx": "Leiserson,? \\Q2009\\E", "shortCiteRegEx": "Leiserson", "year": 2009}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "JMLR, 5:361\u2013397,", "citeRegEx": "Lewis et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2004}, {"title": "Efficient large-scale distributed training of conditional maximum entropy models", "author": ["G. Mann", "R. McDonald", "M. Mohri", "N. Silberman", "D. Walker"], "venue": "In NIPS,", "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Feature selection, l1 vs. l2 regularization and rotational invariance", "author": ["A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "Ng,? \\Q2004\\E", "shortCiteRegEx": "Ng", "year": 2004}, {"title": "Stochastic methods for `1 regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz and Tewari,? \\Q2009\\E", "shortCiteRegEx": "Shalev.Shwartz and Tewari", "year": 2009}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "J. Royal Statistical Society,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms", "author": ["J.N. Tsitsiklis", "D.P. Bertsekas", "M. Athans"], "venue": null, "citeRegEx": "Tsitsiklis et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Tsitsiklis et al\\.", "year": 1986}, {"title": "Sparco: A testing framework for sparse reconstruction", "author": ["E. van den Berg", "M.P. Friedlander", "G. Hennenfent", "F. Herrmann", "R. Saab", "O. Y\u0131lmaz"], "venue": "ACM Transactions on Mathematical Software,", "citeRegEx": "Berg et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Berg et al\\.", "year": 2009}, {"title": "A fast algorithm for sparse reconstruction based on shrinkage, subspace optimization and continuation", "author": ["Z. Wen", "Yin", "D.W. Goldfarb", "Y. Zhang"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Wen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2010}, {"title": "Sparse reconstruction by separable approximation", "author": ["S.J. Wright", "D.R. Nowak", "M.A.T. Figueiredo"], "venue": "IEEE Trans. on Signal Processing,", "citeRegEx": "Wright et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wright et al\\.", "year": 2009}, {"title": "Hitting the memory wall: Implications of the obvious", "author": ["W.A. Wulf", "S.A. McKee"], "venue": "ACM SIGARCH Computer Architecture News,", "citeRegEx": "Wulf and McKee,? \\Q1995\\E", "shortCiteRegEx": "Wulf and McKee", "year": 1995}, {"title": "A comparison of optimization methods and software for large-scale l1-reg. linear classification", "author": ["G.X. Yuan", "K.W. Chang", "C.J. Hsieh", "C.J. Lin"], "venue": null, "citeRegEx": "Yuan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2010}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "A.J. Smola", "L. Li"], "venue": "In NIPS,", "citeRegEx": "Zinkevich et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "Introduction Many applications use L1-regularized models such as the Lasso (Tibshirani, 1996) and sparse logistic regression (Ng, 2004).", "startOffset": 75, "endOffset": 93}, {"referenceID": 14, "context": "Introduction Many applications use L1-regularized models such as the Lasso (Tibshirani, 1996) and sparse logistic regression (Ng, 2004).", "startOffset": 125, "endOffset": 135}, {"referenceID": 14, "context": "the number of irrelevant features (Ng, 2004).", "startOffset": 34, "endOffset": 44}, {"referenceID": 5, "context": "These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al.", "startOffset": 52, "endOffset": 62}, {"referenceID": 6, "context": "These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al., 2007).", "startOffset": 158, "endOffset": 176}, {"referenceID": 22, "context": "2, theory (Shalev-Shwartz & Tewari, 2009) and extensive empirical results (Yuan et al., 2010) have shown that variants of Shooting are particularly competitive for high-dimensional data.", "startOffset": 74, "endOffset": 93}, {"referenceID": 5, "context": "These algorithms range from coordinate minimization (Fu, 1998) and stochastic gradient (Shalev-Shwartz & Tewari, 2009) to more complex interior point methods (Kim et al., 2007). Coordinate descent, which we call Shooting after Fu (1998), is a simple but very effective algorithm which updates one coordinate per iteration.", "startOffset": 53, "endOffset": 237}, {"referenceID": 13, "context": ", 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010).", "startOffset": 34, "endOffset": 77}, {"referenceID": 23, "context": ", 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010).", "startOffset": 34, "endOffset": 77}, {"referenceID": 9, "context": "Recent work analyzes parallel stochastic gradient descent for multicore (Langford et al., 2009b) and distributed settings (Mann et al., 2009; Zinkevich et al., 2010). These methods parallelize over samples. In applications using L1 regularization, though, there are often many more features than samples, so parallelizing over samples may be of limited utility. We therefore take an orthogonal approach and parallelize over features, with a remarkable result: we can parallelize coordinate descent\u2014an algorithm which seems inherently sequential\u2014for L1-regularized losses. In Sec. 3, we propose Shotgun, a simple multicore algorithm which makes P coordinate updates in parallel. We prove strong convergence bounds for Shotgun which predict speedups over Shooting which are linear in P, up to a problem-dependent maximum P\u2217. Moreover, our theory provides an estimate for this ideal P\u2217 which may be easily computed from the data. Parallel coordinate descent was also considered by Tsitsiklis et al. (1986), but for differentiable objectives in the asynchronous setting.", "startOffset": 73, "endOffset": 1003}, {"referenceID": 16, "context": "An instance of (1) is the Lasso (Tibshirani, 1996) (in penalty form), for which Y \u2261 R and F (x) = 1 2 \u2016Ax\u2212 y\u20162 + \u03bb\u2016x\u20161 , (2)", "startOffset": 32, "endOffset": 50}, {"referenceID": 14, "context": "as well as sparse logistic regression (Ng, 2004), for which Y \u2261 {\u22121,+1} and", "startOffset": 38, "endOffset": 48}, {"referenceID": 15, "context": "For analysis, we follow Shalev-Shwartz and Tewari (2009) and transform (1) into an equivalent problem with a twice-differentiable regularizer.", "startOffset": 24, "endOffset": 57}, {"referenceID": 15, "context": "Sequential Coordinate Descent Shalev-Shwartz and Tewari (2009) analyze Stochastic Coordinate Descent (SCD), a stochastic version Normalizing A does not change the objective if a separate, normalized \u03bbj is used for each xj .", "startOffset": 30, "endOffset": 63}, {"referenceID": 15, "context": "To our knowledge, Shalev-Shwartz and Tewari (2009) provide the best known convergence bounds for SCD.", "startOffset": 18, "endOffset": 51}, {"referenceID": 13, "context": "As Shalev-Shwartz and Tewari (2009) argue, Theorem 2.", "startOffset": 3, "endOffset": 36}, {"referenceID": 15, "context": "2): Our proof resembles Shalev-Shwartz and Tewari (2009)\u2019s proof of Theorem 2.", "startOffset": 24, "endOffset": 57}, {"referenceID": 4, "context": "Following Friedman et al. (2010), we maintained a vector Ax to avoid repeated computation.", "startOffset": 10, "endOffset": 33}, {"referenceID": 11, "context": "We used C++ and the CILK++ library (Leiserson, 2009) for parallelism.", "startOffset": 35, "endOffset": 52}, {"referenceID": 6, "context": "L1 LS (Kim et al., 2007) is a log-barrier interior point method.", "startOffset": 6, "endOffset": 24}, {"referenceID": 19, "context": "FPC AS (Wen et al., 2010) uses iterative shrinkage to estimate which elements of x should be non-zero, as well as their signs.", "startOffset": 7, "endOffset": 25}, {"referenceID": 3, "context": "GPSR BB (Figueiredo et al., 2008) is a gradient projection method which uses line search and termination techniques tailored for the Lasso.", "startOffset": 8, "endOffset": 33}, {"referenceID": 20, "context": "SpaRSA (Wright et al., 2009) is an accelerated iterative shrinkage/thresholding algorithm which solves a sequence of quadratic approximations of the objective.", "startOffset": 7, "endOffset": 28}, {"referenceID": 4, "context": "We also tested published implementations of the classic algorithms GLMNET (Friedman et al., 2010) and LARS (Efron et al.", "startOffset": 74, "endOffset": 97}, {"referenceID": 2, "context": ", 2010) and LARS (Efron et al., 2004).", "startOffset": 17, "endOffset": 37}, {"referenceID": 7, "context": "Large, Sparse Datasets: Very large and sparse problems, including predicting stock volatility from text in financial reports (Kogan et al., 2009).", "startOffset": 125, "endOffset": 145}, {"referenceID": 17, "context": "Sparco: Real-valued datasets of varying sparsity from the Sparco testbed (van den Berg et al., 2009). n \u2208 [12829166], d \u2208 [128, 29166]. Single-Pixel Camera: Dense compressed sensing problems from Duarte et al. (2008). n \u2208 [410, 4770], d \u2208 [1024, 16384].", "startOffset": 82, "endOffset": 217}, {"referenceID": 7, "context": "The largest dataset, whose features are occurrences of bigrams in financial reports (Kogan et al., 2009), has 5 million features and 30K samples.", "startOffset": 84, "endOffset": 104}, {"referenceID": 8, "context": "On L1 logreg (Koh et al., 2007) and CDN (Yuan et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 22, "context": ", 2007) and CDN (Yuan et al., 2010), our results qualitatively matched their survey.", "startOffset": 16, "endOffset": 35}, {"referenceID": 21, "context": "For a large-scale comparison of various algorithms for sparse logistic regression, we refer the reader to the recent survey by Yuan et al. (2010). On L1 logreg (Koh et al.", "startOffset": 127, "endOffset": 146}, {"referenceID": 8, "context": "On L1 logreg (Koh et al., 2007) and CDN (Yuan et al., 2010), our results qualitatively matched their survey. Yuan et al. (2010) do not explore SGD empirically.", "startOffset": 14, "endOffset": 128}, {"referenceID": 22, "context": "As Yuan et al. (2010) show empirically, their Coordinate Descent Newton (CDN) method is often orders of magnitude faster than the basic Shooting algorithm (Alg.", "startOffset": 3, "endOffset": 22}, {"referenceID": 21, "context": ", Zinkevich et al. (2010). We used lazy shrinkage updates (Langford et al.", "startOffset": 2, "endOffset": 26}, {"referenceID": 9, "context": "We used lazy shrinkage updates (Langford et al., 2009a) to make use of sparsity in A. Choosing learning rates for SGD can be challenging. In our tests, constant rates led to faster convergence than decaying rates (decaying as 1/ \u221a T ). For each test, we tried 14 exponentially increasing rates in [10\u22124, 1] (in parallel) and chose the rate giving the best training objective. We did not use a sparsifying step for SGD. SMIDAS (Shalev-Shwartz & Tewari, 2009) uses stochastic mirror descent but truncates gradients to sparsify x. We tested their published C++ implementation. Parallel SGD refers to Zinkevich et al. (2010)\u2019s work, which runs SGD in parallel on different subsamples of the data and averages the solutions x.", "startOffset": 32, "endOffset": 621}, {"referenceID": 9, "context": "We used lazy shrinkage updates (Langford et al., 2009a) to make use of sparsity in A. Choosing learning rates for SGD can be challenging. In our tests, constant rates led to faster convergence than decaying rates (decaying as 1/ \u221a T ). For each test, we tried 14 exponentially increasing rates in [10\u22124, 1] (in parallel) and chose the rate giving the best training objective. We did not use a sparsifying step for SGD. SMIDAS (Shalev-Shwartz & Tewari, 2009) uses stochastic mirror descent but truncates gradients to sparsify x. We tested their published C++ implementation. Parallel SGD refers to Zinkevich et al. (2010)\u2019s work, which runs SGD in parallel on different subsamples of the data and averages the solutions x. We tested this method since it is one of the few existing methods for parallel regression, but we note that Zinkevich et al. (2010) did not address L1 regularization in their analysis.", "startOffset": 32, "endOffset": 854}, {"referenceID": 12, "context": "The rcv1 dataset 6 (Lewis et al., 2004) illustrates the high-dimensional regime (d > n).", "startOffset": 19, "endOffset": 39}], "year": 2011, "abstractText": "We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1regularized losses. Though coordinate descent seems inherently sequential, we prove convergence bounds for Shotgun which predict linear speedups, up to a problemdependent limit. We present a comprehensive empirical study of Shotgun for Lasso and sparse logistic regression. Our theoretical predictions on the potential for parallelism closely match behavior on real data. Shotgun outperforms other published solvers on a range of large problems, proving to be one of the most scalable algorithms for L1.", "creator": "TeX"}}}