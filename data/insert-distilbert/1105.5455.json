{"id": "1105.5455", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Variational Cumulant Expansions for Intractable Distributions", "abstract": "intractable distributions present a common difficulty model in generating inference programming within the probabilistic knowledge representation framework and variational methods have recently been heavily popular in providing an approximate solution. in this article, eventually we describe a perturbational relaxation approach in the form of a predicted cumulant expansion which, to lowest order, spontaneously recovers the standard kullback - leibler variational decay bound. higher - order terms partially describe corrections on the variational approach without incurring much further computational cost. the relationship to other perturbational approaches such as tap is also elucidated. we demonstrate the method on a particular class of undirected graphical models, boltzmann machines, for which our simulation results confirm improved accuracy and enhanced stability during learning.", "histories": [["v1", "Fri, 27 May 2011 01:51:46 GMT  (128kb)", "http://arxiv.org/abs/1105.5455v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["d barber", "p de van laar"], "accepted": false, "id": "1105.5455"}, "pdf": {"name": "1105.5455.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["David Barber"], "emails": ["davidb@mbfys.kun.nl", "pierre@mbfys.kun.nl"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 10 (1999) 435-455 Submitted 8/98; published 6/99 Variational Cumulant Expansionsfor Intractable DistributionsDavid Barber davidb@mbfys.kun.nlPi erre van de Laar pierre@mbfys.kun.nlRWCP (Real World Computing Partnership)Theoretical Foundation SNN (Foundation for Neural Networks)University of Nijmegen, CPK1 231, Geert Grooteplein 21Nijmegen, The Netherlands AbstractIntractable distributions present a common di culty in inference within the proba-bilistic knowledge representation framework and variational methods have recently beenpopular in providing an approximate solution. In this article, we describe a perturbationalapproach in the form of a cumulant expansion which, to lowest order, recovers the stan-dard Kullback-Leibler variational bound. Higher-order terms describe corrections on thevariational approach without incurring much further computational cost. The relation-ship to other perturbational approaches such as TAP is also elucidated. We demonstratethe method on a particular class of undirected graphical models, Boltzmann machines,for which our simulation results con rm improved accuracy and enhanced stability duringlearning.1. IntroductionIn recent years, interest has steadily grown over many associated elds in representing andprocessing information in a probabilistic framework. Arguably, the reason for this is thatprior knowledge of a problem domain is rarely absolute and the probabilistic frameworktherefore arises naturally as a candidate for dealing with this uncertainty. Better knownexamples of models are Belief Networks (Pearl, 1988) and probabilistic neural networks(Bishop, 1995; MacKay, 1995). However, incorporating many prior beliefs can make modelsof domains so ambitious that dealing with them in a probabilistic framework is intractable,and some form of approximation is inevitable. One well-known approximate technique isMonte Carlo sampling (see e.g., Neal, 1993; Gilks, Richardson, & Spiegelhalter, 1996),which has the bene t of universal applicability. However, not only can sampling be pro-hibitively slow, but also the lack of a suitable convergence criterion can lead to low con dencein the results. Recently, variational methods have provided a popular alternative, since theynot only approximate but can also bound quantities of interest giving, potentially, greatercon dence in the results (Jordan, Gharamani, Jaakola, & Saul, 1998; Barber & Wiegerinck,1999; Barber & Bishop, 1998). The accuracy of variational methods, however, is limited bythe exibility of the variational distribution employed. Whilst, in principle, the exibility,and therefore, the accuracy of the model, can be increased at will (see e.g., Jaakkola & Jor-dan, 1998; Lawrence, Bishop, & Jordan, 1998), this generally is at the expense of incurringeven greater computational di culties and optimization problems.c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nBarber & van de LaarIn this article, we describe an alternative approach which is a perturbation around stan-dard variational methods. This has the property of potentially improving the performanceat only a small extra computational cost. Since most quantities of interest are derivablefrom knowledge of the normalizing constant of a distribution, we present an approximationof this quantity which, to lowest order, recovers the standard Kullback-Leibler variationalbound. Higher-order corrections in this expansion are expected to improve on the accuracyof the variational solution, despite the loss of a strict bound.In section (2) we will brie y discuss why the normalizing constant of probability distri-butions is of such importance. We brie y review the Kullback-Leibler variational bound insection (3) before introducing the perturbational approach in section (4). This approachis illustrated on a well-known class of undirected graphical models, Boltzmann machines,in section (5), in which we also explain the relation of this approach to other perturba-tional methods such as TAP (Thouless, Anderson, & Palmer, 1977; Kappen & Rodr guez,1998a). We conclude in section (6) with a discussion of the advantages and drawbacks ofthis approach compared to better known techniques.2. Normalizing Constants and Generating FunctionsWe consider a family of probability distributions Q over a vector of random variabless = fsiji 2 [1; : : : ; N ]g, parameterized by w,1Q(s;w) = exp(H(s;w))Z(w) . (1)With a slight abuse of notation, we write the normalizing constant asZ(w) = Z ds exp(H(s;w)) . (2)The `potential' H(s;w) thus uniquely determines the distribution Q. We have assumed herethat the random variable s is continuous|if not, the corresponding integral (see equation2) should be replaced by a summation over all possible discrete states.One approach in statistics to obtain marginals and other quantities of interest is givenby considering generating functions (Grimmett & Stirzaker, 1992), which take the formG( ;w) = Z ds exp (logQ(s;w) + s) (3)Averages of variables with respect to Q are given by derivatives of G( ;w) with respect to for xed w, so that, for example, hs1s2i = @2G( ;w)=@ 1@ 2, evaluated at = 0. Wecan equally well consider the functionZ( ;w) = Z ds exp (H(s;w) + s) (4)so thaths1s2i = 1Z( 1; 2;w) @2Z( 1; 2;w)@ 1@ 2 1; 2=0 . (5)1. We assume that the distribution is strictly positive over the domain.436\nVariational Cumulant Expansions for Intractable DistributionsFormally, all quantities of interest can be calculated from \\normalizing constants\" of dis-tributions, possibly modi ed in a suitable manner, such as in (4) above. It is clear thatall moments of Q can be generated by di erentiating (4) in a similar manner, and can beas equally easily obtained from the generating function approach, as from the normalisingconstant approach. We prefer here the normalising constant approach since this enables usmore easily to make contact with results from statistical physics. The normalising constantapproach is also more natural for the examples of undirected networks that we shall considerin later sections.In the following sections, we layout how to approximate the normalising constant, as-suming that any additional terms in the normalising constant, of the form s in equation(4), are absorbed into the de nition of the potential H.As we will see in section (5.2.3), the normalising constant also plays a central rolein learning. Unfortunately, for many probability distributions that arise in applications,calculation of the normalizing constant is intractable due to the high dimensionality of therandom variable s. When s is a N{dimensional binary vector, naively at least, a summationover 2N states has to be performed to calculate the normalizing constant.However, sometimes it is possible to exploit the structure of the distribution in or-der to reduce the computational expense. A trivial example is that of factorised models,Q(s) / Qi (si), in which Z = R ds Qi (si) = Qi R dsi (si), so that the computationscales only linearly with N . Similarly, in the case of a Gaussian distribution, computa-tion scales (roughly) with N3. These tractable distributions have recently been exploitedin the variational method to approximate intractable distributions, see for example (Saul,Jaakkola, & Jordan, 1996; Jaakkola, 1997; Barber & Wiegerinck, 1999; Jordan et al., 1998).In the following section we will brie y review one of the most common variational methodswhich exploits the Kullback-Leibler bound.3. The Kullback-Leibler Variational BoundOur aim in this section is to brie y describe the current state-of-the-art in approximating thenormalizing constant of an intractable probability distribution. We denote the intractabledistribution of interest as Q1 to distinguish it from an auxiliary distribution Q0, we willintroduce. Without loss of generality, we write the distribution over a vector of randomvariables s, parameterized by a vector w asQ1(s;w) = eH1(s;w)Z1(w) , (6)where the potential H1(s;w) is a known function of s and w, e.g., in the case of a factorisedmodel this would have the form of H1 =Pk wksk. The corresponding (assumed intractable)normalizing constant is given byZ1(w) = Z ds eH1(s;w) . (7)We will use a family of tractable distributions, Q0, parameterized by which we write asQ0(s; ) = eH0(s; )Z0( ) with Z0( ) = Z ds eH0(s; ) (8)437\nBarber & van de Laarwhere, by assumption, the normalizing constant Z0 is tractably computable.Standard variational methods attempt to nd the best approximating distributionQ0(s; )that matches Q1(s;w) by minimizing the Kullback-Leibler divergence,KL(Q0; Q1) =Z dsQ0(s; ) log Q0(s; )Q1(s;w) . (9)Since, using Jensen's inequality, KL(Q0; Q1) 0, we immediately obtain the lower boundlogZ1 Z dsQ0(s; ) logQ0(s; ) + Z dsQ0(s; )H1(s;w) . (10)Using the de nition (8) we obtain the bound in the more intuitive form,logZ1 logZ0 + Z dsQ0(s; ) (H1(s;w) H0(s; )) (11)The lower bound is made as tight as possible by maximizing the right-hand side with respectto the parameters of Q0, which corresponds to minimising the Kullback-Leibler divergence(9). This approach has recently received attention in the arti cial intelligence communityand has been demonstrated to be a useful technique (Saul et al., 1996; Jaakkola, 1997;Barber & Wiegerinck, 1999). Note that whilst this lower bound is useful in providing anobjective comparison of two approximations to the normalising constant (the approximationwith the higher bound value is preferred), it does not translate directly into a bound onconditional probabilities. An upper bound on the normalising constant is also required inthis case. Whilst, in some cases, this may be feasible, in general this tends to be a rathermore di cult task, and we restrict our attention to the lower bound (Jaakkola, 1997).In the next section, we describe another approach that enables us to exploit further suchtractable distributions.4. The Variational Cumulant ExpansionIn order to extend the Kullback-Leibler variational lower bound with a view to improving itsaccuracy without much greater computational expense, we introduce the following familyof probability distributions Q , parameterized by , w and :Q (s;w; ) = eH (s;w; )Z (w; ) (12)where the potential is given byH (s;w; ) = H1(s;w) + (1 )H0(s; ) . (13)The probability distributions in this family interpolate between a tractable distribution, Q0which is obtained for = 0, and an intractable distribution, Q1 which is obtained for = 1.For intermediate values of 2 (0; 1), the distributions remain intractable. The normalizingconstant of a distribution from this family is given bylogZ (w; ) = log Z ds eH0(s; )+ (H1(s;w) H0(s; ))= logZ0 + log De (H1(s;w) H0(s; ))E0 , (14)438\nVariational Cumulant Expansions for Intractable Distributionswhere h:i0 denotes expectation with respect to the tractable distribution Q0. The nal termin (14) is intractable for any 6= 0, and we shall therefore develop a Taylor series expansionfor it around = 0. That is,log De (H1(s;w) H0(s; ))E0 hH1(s;w) H0(s)i0+ 22 (H1(s;w) H0(s) hH1(s;w)i+ hH0(s)i0)2 + O( 3) (15)In terms of the random variable H H1(s;w) H0(s; ), we see that (15) contains the rst terms in a power series in H, the rst being the mean, and the second the varianceof H. More formally, we recognize the nal term in (14) as the cumulant generatingfunction K H( ) log De HE0, with respect to the random variable H (Grimmett &Stirzaker, 1992). If the logarithm of the moment generating function De HE0 is nitein the neighborhood of the origin then the cumulant generating function K H( ) has aconvergent Taylor series,K H( ) = 1Xn=1 1n!k0n( H) n , (16)where k0n( H) is the nth cumulant of H with respect to the distribution Q0. (Thede nition of the nth cumulant is k0n( H) = @n@ n log De HE0 =0). For convenience we haveno longer denoted the explicit dependence on the parameters and w.Since the intractable normalizing constant Z1 corresponds to setting = 1 in equation(14), we can write the Taylor series aslogZ1 = logZ0 + lXn=1 1n!k0n( H) + 1(l + 1)!k l+1( H) , (17)where the remainder follows from the the Mean Value Theorem (see, for example Spivak(1967)), and k l+1( H) is the (l+ 1)th cumulant of H with respect to Q , 0 1. Anapproximation is obtained by simply neglecting this (intractable) remainder and using thetruncated expansion:logZ1 logZ0 + lXn=1 1n!k0n( H) , (18)Note that the right-hand side of equation (18) depends on the free parameters , and we shalldiscuss later how these parameters can be set to improve the accuracy of the approximation.In the following two subsections, we will look in more detail at the rst and second-orderapproximation of this Taylor series, since these illuminate the variational method and thedi culties in retaining a lower bound.4.1 First Order and the Variational BoundThe Taylor series (equation (17)) up to rst order yieldslogZ1 = logZ0 + h Hi0 + 12k 2( H) , (19)439\nBarber & van de Laarwhere the remainder k 2( H) is equal to the variance (the second cumulant) of H withrespect to Q with 0 1, i.e., k 2 = var ( H) = D H2E h Hi 2, and h:i denotesexpectation with respect to the distribution Q . Since the variance is nonnegative for anyvalue of , we havelogZ1 logZ0 + h Hi0 , (20)so that logZ1 is not only approximated but also bounded from below by the right hand sideof equation (20). This bound is the standard mean eld bound (see also equation (10)),and is employed in many variational methods (see, for example Saul et al., 1996; Jaakkola,1997; Barber & Wiegerinck, 1999). This bound can then be made as tight as possible byoptimizing with respect to the free parameters of the tractable distribution Q0.4.2 Second and Higher OrderTo second order equation (17) yieldslogZ1 = logZ0 + h Hi0 + 12var0( H) + 16k 3( H) , (21)with the remainder k 3 = D H3E 3D H2E h Hi + 2h Hi3 and 0 1. Since thisremainder cannot be bounded in a tractable manner for general distributions Q0 and Q1,we have no obvious criterion to prefer one set of parameters of the tractable distributionover another. Similarly, for higher-order expansions it is unclear, within this expansion,how to obtain a tractable bound and, consequently, how to select a set of parameters .We stress that any criterion is essentially a heuristic since the error made by the resultingapproximation is, by assumption, not tractably computable. Whilst, in principle, a wholerange of criteria could be considered in this framework, we mention here only two. The rst, the bound criterion, is arguably the simplest and most natural choice. The secondcriterion we brie y mention is the independence method, which we discuss mainly for itsrelation to the TAP method of statistical physics (Thouless et al., 1977).4.2.1 Bound CriterionThe rst criterion assigns the free parameters to maximize the ( rst-order) bound, equa-tion (20). The resulting distribution Q0(s; ) is used in the truncated approximation,equation (18). To second order this is explicitly given bylogZ1 logZ 0 + h Hi 0 + 12var 0( H) , (22)where a ` ' denotes that the distribution Q0(s; ) is to be used. Note that the computa-tional cost involved over that of the variational method of section (4.1) is small. No furtheroptimization is required, only the calculation of the second cumulant var 0( H) which byassumption is tractable. 440\nVariational Cumulant Expansions for Intractable Distributions4.2.2 Independence CriterionThe second criterion we consider is based on the fact that the exact value of Z1 is indepen-dent of the free parameters , i.e.,@ logZ1@ i 0 . (23)Since logZ1 is intractable, we substitute the truncated cumulant expansion into equation(23), @@ i logZ0 + lXn=1 1n!k0n( H)! 0 . (24)In general, there are many solutions resulting from the independence criterion and alone itis too weak to provide reliable solutions. Under certain restrictions, however, this approachis closely related to the TAP approach of statistical mechanics (Thouless et al., 1977; Plefka,1982; Kappen & Rodr guez, 1998a, 1998b), as will also be described in section (5.1.2).5. Boltzmann MachinesTo illustrate the foregoing theory, we will consider a class of discrete undirected graphicalmodels, Boltzmann machines (Ackley, Hinton, & Sejnowski, 1985). These have applicationin arti cial intelligence as stochastic connectionist models (Jordan et al., 1998), in imagerestoration (Geman & Geman, 1984), and in statistical physics (Itzykson & Drou e, 1989).The potential of a Boltzmann machine, with binary random variables si 2 f0; 1g, is givenby H(s;w) =Xi wisi + 12Xi;j wijsisj (25)with wij wji and wii 0. Unfortunately, Boltzmann machines are in general intractablesince calculation of the normalizing constant involves a summation over an exponentialnumber of states. In an early e ort to overcome this intractability, Peterson and Anderson(1987) proposed the variational approximation using a factorised model as a fast alterna-tive to stochastic sampling. However, Galland (1993) pointed out that this approach oftenfails since it inadequately captures second order statistics of the intractable distribution.Using the potentially more accurate TAP approximation did not overcome these di cultiesand often lead to unstable solutions (Galland, 1993). Furthermore, it is not clear how toextend the TAP approach to deal with using more complex, non-factorised approximatingdistributions. We examine the relationship of our approach to the TAP method in sec-tion (5.1.2). Another approach which aims to improve the accuracy of the correlations,though not the normalising constant, is linear response (Parisi, 1988) which was applied toBoltzmann machines by Kappen and Rodr guez (1998a) for the case of using a factorisedmodel, see section (5.1). The approach we take here is a little di erent in that we wishto nd a better approximation primarily to the normalising constant. Approaches such aslinear response can then be applied to this approximation to derive approximations for thecorrelations, if desired. 441\nBarber & van de LaarMore exible approximating distributions have also been considered in the variationalapproach, for example, mixtures of factorised distributions (Jaakkola & Jordan, 1998;Lawrence et al., 1998), and decimatable structures (Saul & Jordan, 1994; Barber &Wiegerinck,1999). We show how to combine the use of such more exible approximating distributionswith higher-order corrections to the variational procedure. For clarity and simplicity, wewill initially approximate the normalizing constant of a general intractable Boltzmann ma-chine Q1, see equation (25), using only a factorised Boltzmann machine as our tractablemodel Q0. Subsequently, in our simulations, we will use a more exible tractable model, adecimatable Boltzmann machine.5.1 Using Factorised Boltzmann MachinesThe potential of a factorised Boltzmann machine is given byH0 =Xi isi . (26)For this simple model, calculating higher-order cumulants is straightforward once the valueof the free parameters are known since,hsi1si2 : : : sili0 = hsi1i0hsi2i0 : : : hsili0 with i1 6= i2 6= : : : 6= il . (27)For notational convenience, we de ne mi to be the expectation value of si,mi = hsii0 = (1 + exp ( i)) 1: (28)5.1.1 Bound CriterionUsing a factorised distribution to approximate the intractable normalizing constant corre-sponding to equation (25) gives, from equation (20), the variational boundlogZ1 S(m) +Xi wimi + 12Xi;j wijmimj , (29)where, for convenience, we have de ned the entropyS(m) = Xi fmi logmi + (1 mi) log (1 mi)g . (30)One can either maximize the bound, i.e., the right-hand side of equation (29), directly, oruse the fact that at the maximum,@@ k 24S(m) +Xi wimi + 12Xi;j wijmimj35 = 0 , (31)which leads to the xed point equation (with the understanding that the means m arerelated to the parameters by equation (28)) i = wi +Xj wijmj : (32)442\nVariational Cumulant Expansions for Intractable DistributionsThe well-known \\mean eld\" equations (Parisi, 1988) are a re-expression of (32) in termsof the means only, given by (28). The optimal parameters of the factorised model mighttherefore also be obtained by iterating, either synchronously or asynchronously (Peterson& Anderson, 1987), these xed point equations. Note that equation (31) also holds forminima and saddle-points, so in general additional checks are needed to assert that theparameters correspond to a maximum of (29). Insertion of the xed point solution into thesecond-order expansion for the bound criterion, equation (22), yields (up to second order)logZ1 S(m)Xi wimi 12Xi;j wijmimj 14Xi;j wij2mi(1 mi)mj(1 mj), (33)where we emphasize that the mi are given by the variational bound solution, i.e., equation(32).5.1.2 Independence CriterionUsing a factorised distribution to approximate the intractable normalizing constant, seeequation (25), gives up to second orderlogZ1 S(m)+Xi wimi+12Xi;j wijmimj+14Xi;j wij2mi (1 mi)mj (1 mj)+ 12Xi 0@ i wi Xj wijmj1A2mi (1 mi) . (34)The independence criterion leads to the equations0B@Xj wij2mj (1 mj) +0@ i wi Xj wijmj1A21CA 12 mi = Xj j wj Xk wjkmk!wijmj (1 mj) . (35)In general, there are many solutions to these equations and the search can be limited byinserting in equation (34) the constraint that the second-order solution is close to the rst-order solution, i.e., i = wi +Pj wijmj + O(w2), and by neglecting terms of O(w4) andhigher. This simpli es equation (34) to FTAP, the negative TAP free energy2 (Thoulesset al., 1977; Plefka, 1982; Kappen & Rodr guez, 1998a). The independence criterion, i.e.,@FTAP=@ = 0, leads now to the xed point condition i = wi +Xj wijmj + 12Xj wij2(1 2mi)mj(1 mj) , (36)These TAP equations can have poor convergence properties and often produce a poorsolution (Bray & Moore, 1979; Nemoto & Takayama, 1985; Galland, 1993). The additional2. In Thouless et al. (1977), Plefka (1982) and Kappen and Rodr guez (1998a) the random variables si 2f 1; 1g. 443\nBarber & van de Laar m mms1s2 s3 1 12 13 2 3 23@ @@@@ @(a) With s1 m ms2 s3 02 03 023@ @(b) Without s1Figure 1: The Boltzmann machine with the random variable s1 can be decimated to aBoltzmann machine without s1.constraint that the TAP solution should correspond to a minimum of FTAP improves thesolution. However, since TAP is therefore essentially an expansion around the rst-ordersolution, we expect the numerical di erence between the bound criterion and convergentTAP results to be small. For these reasons, we prefer the straightforward bound criterionand leave other criteria for separate study.5.2 Numerical ResultsIn this section we present results of computer simulations to validate our approach. Insection (5.2.1) we will compare the di erent methods of section (4.1) and section (4.2) onapproximating the normalizing constant Z. A similar comparison is made for approximatingthe correlations in section (5.2.2). In section (5.2.3) we show the results of a learning problemin which a Boltzmann machine is used to learn a set of sample patterns, a typical machinelearning problem. In all cases, the numbers of random variables in the Boltzmann machinesare chosen to be small to facilitate comparisons with the exact results.In some simulations we will not only use factorised Boltzmann machines but also more exible models that can make the variational bound (see equation (20)) tighter, namelydecimatable Boltzmann machines (Saul & Jordan, 1994; R uger, 1997; Barber & Wiegerinck,1999). Decimation is a technique that eliminates random variables so that the normalizingconstant for the distribution on the remaining variables remains unchanged up to a constantknown factor. For completeness, we brie y describe this technique in the current context.Suppose we have a Boltzmann machine with many random variables, for which a three-variable subgraph is depicted in Figure 1(a), so that random variable s1 is connected onlyto random variables s2 and s3. Mathematically, the condition for invariance of Z afterremoving random variable s1, Figure 1(b), becomesXsns1 e 0+ 02s2+ 03s3+ 023s2s3 =Xs e + 1s1+ 2s2+ 3s3+ 12s1s2+ 13s1s3+ 23s2s3 , (37)444\nVariational Cumulant Expansions for Intractable Distributions ghgh ghghgh gh gh\ngh (a) factorised model gh gh ghghgh gh gh gh PPBBB PP BBB (b) decimatable gh gh ghghgh gh gh gh @@@@@@ @@@@ @@ @@ PPBBB PP BBB!!!!!!! aaaaaaa LLLLLLL !!!!!!! aaaaaaaLLLLLLL(c) fully connectedFigure 2: Boltzmann machines of eight random variableswhere we include the constant for convenience. Invariance is ful lled if 02 = 2 + log 1 + e 1+ 121 + e 1 , 03 = 3 + log 1 + e 1+ 131 + e 1 , 023 = 23 + log 1 + e 1 1 + e 1+ 12+ 13 (1 + e 1+ 12) (1 + e 1+ 13) and 0 = + log 1 + e 1 . (38)For decimatable Boltzmann machines, one can repeat the decimation process until, -nally, one ends up with a Boltzmann machine which consists of a single random variablewhose normalizing constant is trivial to compute. This means that for decimatable Boltz-mann machines the normalizing constant can be computed in time linear in the numberof variables. Decimation in undirected models is similar in spirit to variable eliminationschemes in graphical models. For example, in directed belief networks, \\bucket elimina-tion\" enables variables to be eliminated by passing compensating messages to other buckets(collections of nodes) in such that the desired marginal on the remaining nodes remains thesame (Dechter, 1999).5.2.1 Approximating the Normalizing ConstantAs our `intractable' distribution Q1, we took a fully connected, eight-node Boltzmann ma-chine, which is not decimatable (see Figure 2(c)). As our tractable Boltzmann machine, Q0,we took both the factorised model of Figure 2(a) and the decimatable model of Figure 2(b).We then tried to approximate the normalizing constant for the fully connected machinein which the parameters were randomly drawn from a zero-mean, Gaussian distributionwith unit variance. See the appendix for additional theoretical and experimental detailsregarding the optimization scheme used. The relative error of the approximation,E = logZexact logZapproxlogZexact , (39)was then determined for both the rst and second-order approach using both the factorisedand decimatable model. This was repeated 550 times for di erent random drawings ofthe parameters of the fully connected Boltzmann machine. The resulting histograms are445\nBarber & van de Laardepicted in Figure 3. In these histograms one can clearly see that the bound, while presentin the rst-order approach, is lost in the second-order approach since some normalisingconstant estimates were above the true value, violating the lower bound. In ve out of1100 cases (two times 550) the synchronous mean eld iterations did not converge andthese outliers were neglected in the plots. It is, however, possible to make the number ofnon-convergent cases even lower by using asynchronous instead of synchronous updates ofthe mean eld parameters (Peterson & Anderson, 1987; Ansari, Hou, & Yu, 1995; Saulet al., 1996). Note that, in both cases, the second-order method improves on average onthe standard ( rst order) variational results. Indeed, using only a factorised model withthe second-order correction improves on the standard variational decimatable result.We also determined whether the second-order approach improves the accuracy of theprediction in each individual run. In order to determine this we used the paired di erence, = logZexact logZ rstlogZexact logZexact logZsecondlogZexact . (40)If is positive the second order improves the rst order approach. The correspondingresults of our computer simulations are depicted in Figure 4. In all runs was positive,corresponding to a consistent improvement in accuracy.5.2.2 Approximating the Means and CorrelationsThere are many techniques that can be called upon to approximate moments and correla-tions. Primarily, the cumulant expansion, as we have described it, is intended to improvethe accuracy in estimating the normalising constant, and not directly the moments of thedistribution. Arguably the simplest way to approximate correlations is to use the stan-dard variational approach to nd the best approximating distribution to Q1 and then toapproximate the moments of Q1 by the moments of Q0. In this approach, however, certaincorrelations that are not present in the structure of the approximating distribution Q0,will be trivially approximated. For example, approximating hsisjiQ1 using the factoriseddistribution gives hsisjiQ1 hsiiQ0 hsjiQ0 . We will examine in this section some ways ofimproving the estimation of the correlations.One procedure that can be used to approximate correlations such as hsisjiQ1 is givenby the so-called linear response theory (Parisi, 1988). This approach uses the relationship(5) in which the intractable normalising constant is replaced with its approximation (29).The approach that we adopt here, due to it's straightforward relationship to normalisingconstants, is given by considering the following relationship:hsisjiQ1 = Q1(si = 1; sj = 1) = ehi+hj+2Jij Z [i;j]Z (41)where Z [i;j] is the normalising constant of the original network in which nodes i and j havebeen removed and the biases are transformed to h0k = hk+Jik+Jjk. This means that we needto evaluate O(n2) normalising constants to approximate the correlations. To approximatethe normalising constants, we use the higher-order expansion (33). Similarly, we attemptto obtain a more accurate approximation to the means hsii, based on the identity,hsiiQ1 = Q1(si = 1) = ehiZ [i]Z (42)446\nVariational Cumulant Expansions for Intractable Distributions 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0 20 40 60 80 100 120\nabsolute error\nF re\nqu en\ncy\nFactorized First Order\n(a) Factorised rst order. Themean (absolute) error is 0:036(0:034 without outliers). 0 0.02 0.04 0.06 0.08 0.1 0.12 0.140 50\n100\n150\n200\n250\n300\nabsolute error\nF re\nqu en\ncy\nFactorized Second Order\n(b) Factorised second order. Themean absolute error is 0:0079(0:0074 without outliers). 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0 50 100 150\nabsolute error\nF re\nqu en\ncy\nDecimatable First Order\n(c) Decimatable rst order. Themean (absolute) error is 0:0186(0:0181 without outlier). 0 0.02 0.04 0.06 0.08 0.1 0.12 0.140 100\n200\n300\n400\n500\nabsolute error\nF re\nqu en\ncy\nDecimatable Second Order\n(d) Decimatable second order.The mean absolute error is0:0031 (0:0027 without outlier).Figure 3: Approximation of the normalizing constant of a fully connected, eight-node Boltz-mann machine (see Figure 2(c)), whose parameters are randomly drawn from azero-mean, Gaussian distribution with unit variance. A factorised model (seeFigure 2(a)) is used as the approximating distribution in (a) and (b) and a deci-matable model (see Figure 2(b)) is used in (c) and (d). The histograms are basedon 550 di erent random drawings of the parameters. For the readability of thehistogram, we have excluded ve outliers (four in (a) and (b), one in (c) and (d)),for which the synchronous mean eld iteration did not converge. The mean erroris the mean of the absolute value of equation (39).where Z [i] is the normalising constant of the original network in which node i has beenremoved and the biases are transformed to h0k = hk + Jik. The \\intractable\" Boltzmannmachine in our simulations has 8 fully connected nodes with biases and weights drawnfrom a standard zero-mean, unit-variance Gaussian distribution. In gures 5 and 6 we plotresults for approximating the means hsii and correlations hsisji respectively. As can be seen,the approach we take to approximate the correlations is an improvement on the standardvariational factorised model. We emphasize that, since we are primarily concerned withapproximating the normalising constant, there may be more suitable methods dedicated447\nBarber & van de Laar 0 0.02 0.04 0.06 0.08 0.1 0 20 40 60 80\nabsolute error\nF re\nqu en\ncy\nFactorised Paired First and Second Order (a) Factorised model. 0 0.02 0.04 0.06 0.08 0.1020 40 60 80 absolute error F re qu en cy Decimatable Paired First and Second Order\n(b) Decimatable model.Figure 4: Di erence between rst and second-order approximation of the normalizing con-stant of a fully connected, eight-node Boltzmann machine using (a) a factorisedand (b) a decimatable model. The histograms are based on the same 550 ran-dom drawings as used in Figure 3. All 1100 di erences, including the ve non-convergent runs, were larger than zero. Again for readability of the histogram, thenon-convergent runs are not included. The mean di erence was 0.0281 (0.0269without the non-convergent runs) and 0.0155 (0.0154 without the non-convergentrun) for the factorised model and the decimatable model, respectively.to the task of approximating the correlations themselves. Indeed, one of the drawbacks ofthese perturbative approaches is that physical constraints, such as that moments should bebounded between 0 and 1, can be violated.5.2.3 LearningThe results of the previous section show that, for randomly chosen connection matricesJ , the higher-order terms can signi cantly improve the approximation to the normalisingconstant of the distribution. However, in a learning scenario, such results may be of littleinterest since the connection matrix will become intimately related to the patterns to belearned|that is, J will take on a form that is appropriate for storing the patterns in thetraining set as learning progresses.We are therefore interested here in training networks on a set of visible patterns. Thatis, we split the nodes of the network into a set of visible SV and a set of hidden units SH withthe aim of learning a set of patterns S1V : : : SpV with a Boltzmann machine Q1(SV ; SH). Byusing the KL divergence between an approximating distribution ~Q0(SH jSV ) on the hiddenunits and the conditional distribution Q1(SH jSV ) we obtain the following boundlnQ1(SV ) Z ~Q0(SH jSV ) ln ~Q0(SH jSV ) + Z ~Q0(SH jSV ) lnQ1(SH ; SV ) (43)For the case of Boltzmann machines, lnQ1(SH ; SV ) = H(SH ; SV ) lnZ in which lnZ is(assumed) intractable. In order to obtain an approximation to this quantity, we thereforeintroduce a further variational distribution, Q0(SV ; SH) (not the same as ~Q0(SH jSV )) whichcan be used as in (10) to obtain a lower bound on lnZ. Unfortunately, used in (43), this448\nVariational Cumulant Expansions for Intractable Distributions\n0 0.02 0.04 0.06 0.08 0.1 0\n50\n100\n150\nabsolute error\nF re\nqu en\ncy\nFactorized First Order estimates of first moments\n(a) The mean absolute error inapproximating the means hsii is0.0350. 0 0.02 0.04 0.06 0.08 0.10 50\n100\n150\n200\n250\n300\n350\nabsolute error\nF re\nqu en\ncy\nFactorized Second Order estimates of first moments\n(b) The mean absolute error inapproximating the means hsii is0.0129.Figure 5: Approximating the means hsii for 1000 randomly chosen 8 node fully connectedBoltzmann machines. The weights were drawn from the standard normal distri-bution. (a) The standard variational approach in which the means are estimatedby the means of the best approximating factorised distribution. (b) Using the ra-tio of normalising constants (41), in which second-order corrections are includedin approximating the normalising constants. Using the ratio of normalising con-stants without the higher order corrections gave a mean error of 0.046, slightlyworse than the standard result variational result. 0 0.02 0.04 0.06 0.08 0.1 0.12 0 50 100 150\nabsolute error\nF re\nqu en\ncy\nFactorized First Order estimates of correlations\n(a) Mean absolute error = 0.0329 0 0.02 0.04 0.06 0.08 0.1 0.12050100 150\n200\n250\n300\n350\nabsolute error\nF re\nqu en\ncy\nFactorized Second Order estimates of correlations\n(b) Mean absolute error = 0.0103Figure 6: Approximating the correlations hsisji for 1000 randomly chosen 8 node fullyconnected Boltzmann machines. The weights were drawn from the standardnormal distribution. (a) The standard variational approach in which the meansare estimated by the means of the best approximating factorised distribution, and(b) using the ratio of normalising constants, including second-order corrections.Without second-order corrections, this gives a mean error of 0.0428.449\nBarber & van de Laarlower bound on lnZ does not give a lower bound on the likelihood of the visible unitsQ1(SV ). Nevertheless, we may hope that the approximation to the likelihood gradient issu ciently accurate such that ascent of the likelihood can be made. Taking the derivativeof (43) with respect to the parameters of the Boltzmann machine Q1(SH ; SV ), we arrive atthe following learning rule for gradient ascent given a pattern SV : J = hsisji ~Q0(SH jSV ) hsisjiQ1(SH ;SV ) (44)where is a chosen learning rate. The correlations in the rst term of (44) are straightfor-ward to compute in the case of using a factorised distribution ~Q0. The \\free\" expectationsin the second term are more troublesome and need to be approximated in some manner.We examine using the standard factorised model approximations for the free correlationsand more accurate \\higher-order\" approximations as described in section (5.2.2). Here,we are primarily concerned with monitoring the likelihood bound (43) under such gradientdynamics, so we will look at the exact bound value, it's approximation using a factorisedmodel for lnZ (29), and it's second-order correction (33).We consider a small learning problem with 3 hidden units and 4 visible units. Ten visibletraining patterns were formed in which the elements were chosen to be on (value=1) withprobability 0.4. In gure 7(a) we demonstrate variational learning in which the free statis-tics, required for the likelihood gradient ascent rule (44), are approximated with a simplefactorised assumption, hsisjiQ1(SH ;SV ) mimj(i 6= j). We display the time evolution ofthe exact training pattern likelihood bound3 (43) (solid line), and two approximations to it:the rst approximates the intractable lnZ term by using the standard variational approachwith a factorised model (dashed line). The second approximation uses the second-ordercorrection to the factorised model value for lnZ (dot-dash line). The learning rate was xed at 0.05. In monitoring the progress of learning, the higher-order correction to the like-lihood bound can be seen to be a more accurate approximation of the likelihood comparedto that given by the use of the standard variational approximation alone. Interestingly,using the second-order correction to the likelihood, a maximum in the likelihood is detectedat a point close to saturation of the exact bound. This is not the case using the rst-orderapproximation alone and, with the standard approximation to the likelihood, the dynamicsof the learning process, in terms of the training set likelihood, are poorly monitored.In gure 7(b) the gradient dynamics are provided again by equation (44) but nowwith the free correlations hsisjiQ1(SH ;SV ) approximated by the more accurate ratio-of-normalising-constants method, as described in section (5.2.2). Again, we see an improve-ment in the accuracy of monitoring the progress of the learning dynamics by the simpleinclusion of the higher-order term and, again, the higher-order approximation displays amaximum at roughly the correct position. The likelihood is higher than in gure 7(a) sincethe gradient of the likelihood bound is more accurately approximated through the moreaccurate correlation estimates. Note that the reason that the exact likelihood lower boundcan be above 1 is due to pattern repetitions in the training set. The instabilities in learningafter approximately 120 updates are due to the emergence of multimodality in the learneddistribution Q1(SH ; SV jJ), indicating that a more powerful approximation method, able3. In practice, the likelihood on a test set is more appropriate. However, the small number of possiblepatterns here (16) makes it rather di cult to form a representative test set.450\nVariational Cumulant Expansions for Intractable Distributions\n0 10 20 30 40 0\n0.5\n1\n1.5\n2\ntime\nto ta\nl p at\nte rn\nli ke\nlih oo\nd\nLearning using standard MF estimates\n(a) Learning using dynamics inwhich the free correlations areapproximated using a factorisedmodel. 0 50 100 150 200\n0\n0.5\n1\n1.5\n2\ntime\nto ta\nl p at\nte rn\nli ke\nlih oo\nd\nLearning using second order estimates\n(b) Learning using dynamics inwhich the free correlations aregiven by the ratio of normalisingconstants approach.Figure 7: Learning a set of 10 random, 4-visible-unit patterns with a Boltzmann machinewith 3 hidden units. The solid line is the exact value for the total likelihood bound(43) of all the patterns in the training set. The dashed line is the likelihood boundapproximation based on using a factorised model to approximate lnZ, equation(29). The dash-dot line uses the second order correction to lnZ, equation (33).to capture such multimodal e ects, should be used to obtain further improvements in thelikelihood.6. DiscussionIn this article we have described perturbational approximations of intractable probabilitydistributions. The approximations are based on a Taylor series using a family of prob-ability distributions that interpolate between the intractable distribution and a tractableapproximation thereto. These approximations can be seen as an extension of the variationalmethod, although they no longer bound the quantities of interest. We have illustrated ourapproach both theoretically and by computer simulations for Boltzmann machines. Thesesimulations showed that the approximation can be improved beyond the variational boundby including higher-order terms of the corresponding cumulant expansion.Simulations showed that the accuracy in monitoring the training set likelihood duringthe learning process can be improved by including higher order corrections. However,these perturbational approximations cannot be expected to consistently improve on zerothorder (variational) solutions. For instance, if the distribution is strongly multimodal, thenusing a unimodal variational distribution cannot be expected to be improved much by theinclusion of higher-order perturbational terms. On the other hand, higher-order correctionsto multimodal approximations may well improve the solution considerably.451\nBarber & van de LaarWe elucidated the relationship of our approach to TAP, arguing that our approach isexpected to o er a more stable solution. Furthermore, the application of our approach tomodels other than the Boltzmann machine is transparent. Indeed, these techniques arereadily applicable to other variational methods in a variety of contexts both for discreteand continuous systems (Barber & Bishop, 1998; Wiegerinck & Barber, 1998).One drawback of the perturbational approach that we have described is that known\\physical\" constraints, for example that moments must be bounded in a certain range,are not necessarily adhered to. It would be useful to develop a perturbation method thatensures that solutions are at least physical.We hope to apply these methods to variational techniques other than the standardKullback-Leibler bound, and also to study the evaluation of other suitable criteria for thesetting of the variational parameters.AcknowledgmentsWe would like to thank Tom Heskes, Bert Kappen, Martijn Leisink, Peter Sollich and WimWiegerinck for stimulating and helpful discussions and the referees for excellent commentson an earlier version of this paper.Appendix A.In this appendix we describe how to optimize the variational bound when the normalizationconstant of an intractable Boltzmann machine is approximated using another tractableBoltzmann machine. For convenience, we denote the potential of an intractable BoltzmannMachine asH1 XI wIsI , (45)where the \\extended\" parameters are given by I 2 f i; ijji 2 [1; : : : ; N ]; j = [i+1; : : : ; N ]gand sI 2 fsi; sisj ji 2 [1; : : : ; N ]; j = [i + 1; : : : ; N ]g. The potential of the other tractableBoltzmann Machine is denoted asH0 XI2 IsI , (46)where denotes the set of parameters of the tractable Boltzmann Machine.We want to optimize the bound of equation (20) with respect to the free parameters J(J 2 ), i.e.,@@ J [logZ0 + h Hi0] = 0 , (47)which leads toh HsJi0 h Hi0hsJi0 = 0 (48)452\nVariational Cumulant Expansions for Intractable Distributionsand thus to the xed point equation4 I = XJ2 XK [F 1]IJFJKwK , (49)where the Fisher matrix is given by FIJ = hsIsJi0 hsIi0hsJi0 which depends on the freeparameters , and F is the Fisher matrix of the tractable Boltzmann machine. For afactorised approximating model, F is a diagonal matrix and equation (49) simpli es toequation (32). For a decimatable Boltzmann machine, the elements of the Fisher matrix aretractable. The iteration can be performed either synchronously or asynchronously (Peterson& Anderson, 1987). We prefer here the synchronous case since for all N parameter-updateswe only need to calculate and invert a single Fisher matrix instead of N matrices in theasynchronous case. In most applications, however, the asynchronous method seems to beadvantageous with respect to convergence (Peterson & Anderson, 1987; Ansari et al., 1995;Saul et al., 1996).ReferencesAckley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmannmachines. Cognitive Science, 9, 147{169.Ansari, N., Hou, E. S. H., & Yu, Y. (1995). A new method to optimize the satellitebroadcasting schedules using the mean eld annealing of a Hop eld neural network.IEEE Transactions on Neural Networks, 6 (2), 470{483.Barber, D., & Bishop, C. M. (1998). Ensemble learning in Bayesian neural networks. InBishop, C. M. (Ed.), Proceedings of the Newton Institute Program on Neural Networksand Machine Learning. Kluwer.Barber, D., & Wiegerinck, W. (1999). Tractable variational structures for approximatinggraphical models. In Kearns, M., Solla, S., & Cohn, D. (Eds.), Advances in NeuralInformation Processing Systems NIPS 11. MIT Press.Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Clarendon Press, Oxford.Bray, A. J., & Moore, M. A. (1979). Evidence for massless modes in the `solvable model'of a spin glass. Journal of Physics C: Solid State Physics, 12, L441{L448.Dechter, R. (1999). Bucket elimination: A unifying framework for probabilistic inference.In Jordan, M. I. (Ed.), Learning in Graphical Models, pp. 75{104. Kluwer.Galland, C. C. (1993). The limitations of deterministic Boltzmann machine learning. Net-work: Computation in Neural Systems, 4, 355{379.Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and theBayesian restoration of images. IEEE Transactions on Pattern Analysis and MachineIntelligence, 6 (6), 721{741.4. This xed point equation is the generalized mean eld equation. This form of iteration is not restrictedto Boltzmann machines but is a general characteristic of exponential models.453\nBarber & van de LaarGilks, W., Richardson, S., & Spiegelhalter, D. (Eds.). (1996). Markov chain Monte Carloin pratice. Chapman and Hall.Grimmett, G. R., & Stirzaker, D. R. (1992). Probability and Random Processes (SecondEdition). Oxford Science Publications. Clarendon Press, Oxford.Itzykson, C., & Drou e, J.-M. (1989). Statistical Field Theory. Cambridge Monographs onMathematical Physics. Cambridge University Press.Jaakkola, T. S. (1997). Variational Methods for Inference and Estimation in GraphicalModels. Ph.D. thesis, Massachusetts Institute of Technology.Jaakkola, T. S., & Jordan, M. I. (1998). Improving the mean eld approximation via theuse of mixture distributions. In Jordan, M. I. (Ed.), Learning in Graphical Models,Vol. 89, pp. 163{173. Kluwer.Jordan, M. I., Gharamani, Z., Jaakola, T. S., & Saul, L. K. (1998). An Introductionto Variational Methods for Graphical Models. In Jordan, M. I. (Ed.), Learning inGraphical Models, pp. 105{161. Kluwer.Kappen, H. J., & Rodr guez, F. B. (1998a). Boltzmann machine learning using mean eldtheory and linear response correction. In Jordan, M. I., Kearns, M. J., & Solla,S. A. (Eds.), Advances in Neural Information Processing Systems 10, pp. 280{286.Cambridge. The MIT Press.Kappen, H. J., & Rodr guez, F. B. (1998b). E cient learning in Boltzmann machines usinglinear response theory. Neural Computation, 10 (5), 1137{1156.Lawrence, N. D., Bishop, C. M., & Jordan, M. I. (1998). Mixture representations for infer-ence and learning in Boltzmann machines. In Fourteenth Conference on Uncertaintyin Arti cial Intelligence.MacKay, D. J. C. (1995). Probable networks and plausible predictions - a review of praticalBayesian methods for supervised neural networks. Network: Computation in NeuralSystems, 6 (3), 469{505.Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods.Tech. rep., Department of Computer Science, University of Toronto. CRG-TR-93-1,http://www.cs.toronto.edu/ radford/papers-online.html.Nemoto, K., & Takayama, H. (1985). TAP free energy structure of SK spin glasses. Journalof Physics C: Solid State Physics, 18, L529{L535.Parisi, G. (1988). Statistical Field Theory. Frontiers in Physics Lecture Note Series. AddisonWesley.Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, SanFrancisco.Peterson, C., & Anderson, J. R. (1987). A mean eld theory learning algorithm for neuralnetworks. Complex Systems, 1, 995{1019.454\nVariational Cumulant Expansions for Intractable DistributionsPlefka, T. (1982). Convergence condition of the TAP equation for the in nite-ranged Isingspin glass model. Journal of Physics A: Mathematical and general, 15, 1971{1978.R uger, S. M. (1997). E cient inference and learning ig decimatable boltzmann machines.Tech. rep. TR 97-5, Fachbereich Informatik der Technischen Universit at Berlin.Saul, L., & Jordan, M. I. (1994). Learning in Boltzmann trees. Neural Computation, 6 (6),1174{1184.Saul, L. K., Jaakkola, T., & Jordan, M. I. (1996). Mean eld theory for sigmoid beliefnetworks. Journal of Arti cal Intelligence Research, 4, 61{76.Spivak, M. (1967). Calculus. W.A. Benjamin, Inc., London.Thouless, D. J., Anderson, P. W., & Palmer, R. G. (1977). Solution of `solvable model of aspin glass'. Philosophical Magazine, 35 (3), 593{601.Wiegerinck, W., & Barber, D. (1998). Mean eld theory based on belief networks forapproximate inference. In ICANN98: Proceedings of the 8th International Conferenceon Arti cial Neural Networks.\n455"}], "references": [{"title": "A learning algorithm for Boltzmann", "author": ["D.H. Ackley", "G.E. Hinton", "T.J. Sejnowski"], "venue": null, "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "A new method to optimize the satellite", "author": ["N. Ansari", "E.S.H. Hou", "Y. Yu"], "venue": null, "citeRegEx": "Ansari et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ansari et al\\.", "year": 1995}, {"title": "Ensemble learning in Bayesian neural networks", "author": ["D. Barber", "C.M. Bishop"], "venue": null, "citeRegEx": "Barber and Bishop,? \\Q1998\\E", "shortCiteRegEx": "Barber and Bishop", "year": 1998}, {"title": "Tractable variational structures for approximating", "author": ["D. Barber", "W. Wiegerinck"], "venue": null, "citeRegEx": "Barber and Wiegerinck,? \\Q1999\\E", "shortCiteRegEx": "Barber and Wiegerinck", "year": 1999}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Clarendon Press, Oxford.", "citeRegEx": "Bishop,? 1995", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Evidence for massless modes in the `solvable model", "author": ["A.J. Bray", "M.A. Moore"], "venue": null, "citeRegEx": "Bray and Moore,? \\Q1979\\E", "shortCiteRegEx": "Bray and Moore", "year": 1979}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1999\\E", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "The limitations of deterministic Boltzmann machine learning", "author": ["C.C. Galland"], "venue": "Net-", "citeRegEx": "Galland,? 1993", "shortCiteRegEx": "Galland", "year": 1993}, {"title": "Probability and Random Processes", "author": ["G.R. Grimmett", "D.R. Stirzaker"], "venue": null, "citeRegEx": "Grimmett and Stirzaker,? \\Q1992\\E", "shortCiteRegEx": "Grimmett and Stirzaker", "year": 1992}, {"title": "Variational Methods for Inference and Estimation", "author": ["T.S. Jaakkola"], "venue": null, "citeRegEx": "Jaakkola,? \\Q1997\\E", "shortCiteRegEx": "Jaakkola", "year": 1997}, {"title": "Improving the mean eld approximation via", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": null, "citeRegEx": "Jaakkola and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1998}, {"title": "Boltzmann machine learning using mean eld", "author": ["H.J. Kappen", "F.B. Rodr guez"], "venue": null, "citeRegEx": "Kappen and guez,? \\Q1998\\E", "shortCiteRegEx": "Kappen and guez", "year": 1998}, {"title": "E cient learning in Boltzmann machines", "author": ["H.J. Kappen", "F.B. Rodr guez"], "venue": null, "citeRegEx": "Kappen and guez,? \\Q1998\\E", "shortCiteRegEx": "Kappen and guez", "year": 1998}, {"title": "Mixture representations for infer", "author": ["N.D. Lawrence", "C.M. Bishop", "M.I. Jordan"], "venue": null, "citeRegEx": "Lawrence et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 1998}, {"title": "Probable networks and plausible predictions - a review", "author": ["D.J.C. MacKay"], "venue": null, "citeRegEx": "MacKay,? \\Q1995\\E", "shortCiteRegEx": "MacKay", "year": 1995}, {"title": "Probabilistic inference using Markov chain Monte Carlo methods", "author": ["R.M. Neal"], "venue": null, "citeRegEx": "Neal,? \\Q1993\\E", "shortCiteRegEx": "Neal", "year": 1993}, {"title": "TAP free energy structure of SK spin glasses", "author": ["K. Nemoto", "H. Takayama"], "venue": null, "citeRegEx": "Nemoto and Takayama,? \\Q1985\\E", "shortCiteRegEx": "Nemoto and Takayama", "year": 1985}, {"title": "Statistical Field Theory", "author": ["G. Parisi"], "venue": "Frontiers in Physics Lecture Note Series. Addison", "citeRegEx": "Parisi,? 1988", "shortCiteRegEx": "Parisi", "year": 1988}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": "Morgan Kaufmann, San", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "A mean eld theory learning algorithm for neural", "author": ["C. Peterson", "J.R. Anderson"], "venue": null, "citeRegEx": "Peterson and Anderson,? \\Q1987\\E", "shortCiteRegEx": "Peterson and Anderson", "year": 1987}, {"title": "Convergence condition of the TAP equation for the in nite-ranged", "author": ["T. Plefka"], "venue": null, "citeRegEx": "Plefka,? \\Q1982\\E", "shortCiteRegEx": "Plefka", "year": 1982}, {"title": "E cient inference and learning ig decimatable boltzmann machines", "author": ["S.M. uger"], "venue": null, "citeRegEx": "uger,? \\Q1997\\E", "shortCiteRegEx": "uger", "year": 1997}, {"title": "Learning in Boltzmann trees", "author": ["L. Saul", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Saul and Jordan,? \\Q1994\\E", "shortCiteRegEx": "Saul and Jordan", "year": 1994}, {"title": "Mean eld theory for sigmoid belief", "author": ["L.K. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": null, "citeRegEx": "Saul et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Saul et al\\.", "year": 1996}, {"title": "Calculus", "author": ["M. Spivak"], "venue": "W.A. Benjamin, Inc., London.", "citeRegEx": "Spivak,? 1967", "shortCiteRegEx": "Spivak", "year": 1967}, {"title": "Solution of `solvable model", "author": ["D.J. Thouless", "P.W. Anderson", "R.G. Palmer"], "venue": null, "citeRegEx": "Thouless et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Thouless et al\\.", "year": 1977}, {"title": "Mean eld theory based on belief networks", "author": ["W. Wiegerinck", "D. Barber"], "venue": null, "citeRegEx": "Wiegerinck and Barber,? \\Q1998\\E", "shortCiteRegEx": "Wiegerinck and Barber", "year": 1998}], "referenceMentions": [{"referenceID": 18, "context": "Better known examples of models are Belief Networks (Pearl, 1988) and probabilistic neural networks (Bishop, 1995; MacKay, 1995).", "startOffset": 52, "endOffset": 65}, {"referenceID": 4, "context": "Better known examples of models are Belief Networks (Pearl, 1988) and probabilistic neural networks (Bishop, 1995; MacKay, 1995).", "startOffset": 100, "endOffset": 128}, {"referenceID": 14, "context": "Better known examples of models are Belief Networks (Pearl, 1988) and probabilistic neural networks (Bishop, 1995; MacKay, 1995).", "startOffset": 100, "endOffset": 128}, {"referenceID": 9, "context": "These tractable distributions have recently been exploited in the variational method to approximate intractable distributions, see for example (Saul, Jaakkola, & Jordan, 1996; Jaakkola, 1997; Barber & Wiegerinck, 1999; Jordan et al., 1998).", "startOffset": 143, "endOffset": 239}, {"referenceID": 23, "context": "This approach has recently received attention in the arti cial intelligence community and has been demonstrated to be a useful technique (Saul et al., 1996; Jaakkola, 1997; Barber & Wiegerinck, 1999).", "startOffset": 137, "endOffset": 199}, {"referenceID": 9, "context": "This approach has recently received attention in the arti cial intelligence community and has been demonstrated to be a useful technique (Saul et al., 1996; Jaakkola, 1997; Barber & Wiegerinck, 1999).", "startOffset": 137, "endOffset": 199}, {"referenceID": 9, "context": "Whilst, in some cases, this may be feasible, in general this tends to be a rather more di cult task, and we restrict our attention to the lower bound (Jaakkola, 1997).", "startOffset": 150, "endOffset": 166}, {"referenceID": 24, "context": "Since the intractable normalizing constant Z1 corresponds to setting = 1 in equation (14), we can write the Taylor series as logZ1 = logZ0 + l X n=1 1 n!k0 n( H) + 1 (l + 1)!k l+1( H) , (17) where the remainder follows from the the Mean Value Theorem (see, for example Spivak (1967)), and k l+1( H) is the (l+ 1)th cumulant of H with respect to Q , 0 1.", "startOffset": 269, "endOffset": 283}, {"referenceID": 9, "context": "This bound is the standard mean eld bound (see also equation (10)), and is employed in many variational methods (see, for example Saul et al., 1996; Jaakkola, 1997; Barber & Wiegerinck, 1999).", "startOffset": 112, "endOffset": 191}, {"referenceID": 25, "context": "The second criterion we brie y mention is the independence method, which we discuss mainly for its relation to the TAP method of statistical physics (Thouless et al., 1977).", "startOffset": 149, "endOffset": 172}, {"referenceID": 25, "context": "Under certain restrictions, however, this approach is closely related to the TAP approach of statistical mechanics (Thouless et al., 1977; Plefka, 1982; Kappen & Rodr guez, 1998a, 1998b), as will also be described in section (5.", "startOffset": 115, "endOffset": 186}, {"referenceID": 20, "context": "Under certain restrictions, however, this approach is closely related to the TAP approach of statistical mechanics (Thouless et al., 1977; Plefka, 1982; Kappen & Rodr guez, 1998a, 1998b), as will also be described in section (5.", "startOffset": 115, "endOffset": 186}, {"referenceID": 7, "context": "Using the potentially more accurate TAP approximation did not overcome these di culties and often lead to unstable solutions (Galland, 1993).", "startOffset": 125, "endOffset": 140}, {"referenceID": 17, "context": "Another approach which aims to improve the accuracy of the correlations, though not the normalising constant, is linear response (Parisi, 1988) which was applied to Boltzmann machines by Kappen and Rodr guez (1998a) for the case of using a factorised model, see section (5.", "startOffset": 129, "endOffset": 143}, {"referenceID": 17, "context": "In an early e ort to overcome this intractability, Peterson and Anderson (1987) proposed the variational approximation using a factorised model as a fast alternative to stochastic sampling.", "startOffset": 51, "endOffset": 80}, {"referenceID": 7, "context": "However, Galland (1993) pointed out that this approach often fails since it inadequately captures second order statistics of the intractable distribution.", "startOffset": 9, "endOffset": 24}, {"referenceID": 7, "context": "However, Galland (1993) pointed out that this approach often fails since it inadequately captures second order statistics of the intractable distribution. Using the potentially more accurate TAP approximation did not overcome these di culties and often lead to unstable solutions (Galland, 1993). Furthermore, it is not clear how to extend the TAP approach to deal with using more complex, non-factorised approximating distributions. We examine the relationship of our approach to the TAP method in section (5.1.2). Another approach which aims to improve the accuracy of the correlations, though not the normalising constant, is linear response (Parisi, 1988) which was applied to Boltzmann machines by Kappen and Rodr guez (1998a) for the case of using a factorised model, see section (5.", "startOffset": 9, "endOffset": 732}, {"referenceID": 13, "context": "Barber & van de Laar More exible approximating distributions have also been considered in the variational approach, for example, mixtures of factorised distributions (Jaakkola & Jordan, 1998; Lawrence et al., 1998), and decimatable structures (Saul & Jordan, 1994; Barber &Wiegerinck, 1999).", "startOffset": 166, "endOffset": 214}, {"referenceID": 17, "context": "Variational Cumulant Expansions for Intractable Distributions The well-known \\mean eld\" equations (Parisi, 1988) are a re-expression of (32) in terms of the means only, given by (28).", "startOffset": 98, "endOffset": 112}, {"referenceID": 25, "context": "This simpli es equation (34) to FTAP, the negative TAP free energy2 (Thouless et al., 1977; Plefka, 1982; Kappen & Rodr guez, 1998a).", "startOffset": 68, "endOffset": 132}, {"referenceID": 20, "context": "This simpli es equation (34) to FTAP, the negative TAP free energy2 (Thouless et al., 1977; Plefka, 1982; Kappen & Rodr guez, 1998a).", "startOffset": 68, "endOffset": 132}, {"referenceID": 7, "context": ", @FTAP=@ = 0, leads now to the xed point condition i = wi +Xj wijmj + 12Xj wij2(1 2mi)mj(1 mj) , (36) These TAP equations can have poor convergence properties and often produce a poor solution (Bray & Moore, 1979; Nemoto & Takayama, 1985; Galland, 1993).", "startOffset": 194, "endOffset": 254}, {"referenceID": 7, "context": ", @FTAP=@ = 0, leads now to the xed point condition i = wi +Xj wijmj + 12Xj wij2(1 2mi)mj(1 mj) , (36) These TAP equations can have poor convergence properties and often produce a poor solution (Bray & Moore, 1979; Nemoto & Takayama, 1985; Galland, 1993). The additional 2. In Thouless et al. (1977), Plefka (1982) and Kappen and Rodr guez (1998a) the random variables si 2 f 1; 1g.", "startOffset": 240, "endOffset": 300}, {"referenceID": 7, "context": ", @FTAP=@ = 0, leads now to the xed point condition i = wi +Xj wijmj + 12Xj wij2(1 2mi)mj(1 mj) , (36) These TAP equations can have poor convergence properties and often produce a poor solution (Bray & Moore, 1979; Nemoto & Takayama, 1985; Galland, 1993). The additional 2. In Thouless et al. (1977), Plefka (1982) and Kappen and Rodr guez (1998a) the random variables si 2 f 1; 1g.", "startOffset": 240, "endOffset": 315}, {"referenceID": 7, "context": ", @FTAP=@ = 0, leads now to the xed point condition i = wi +Xj wijmj + 12Xj wij2(1 2mi)mj(1 mj) , (36) These TAP equations can have poor convergence properties and often produce a poor solution (Bray & Moore, 1979; Nemoto & Takayama, 1985; Galland, 1993). The additional 2. In Thouless et al. (1977), Plefka (1982) and Kappen and Rodr guez (1998a) the random variables si 2 f 1; 1g.", "startOffset": 240, "endOffset": 348}, {"referenceID": 6, "context": "For example, in directed belief networks, \\bucket elimination\" enables variables to be eliminated by passing compensating messages to other buckets (collections of nodes) in such that the desired marginal on the remaining nodes remains the same (Dechter, 1999).", "startOffset": 245, "endOffset": 260}, {"referenceID": 23, "context": "It is, however, possible to make the number of non-convergent cases even lower by using asynchronous instead of synchronous updates of the mean eld parameters (Peterson & Anderson, 1987; Ansari, Hou, & Yu, 1995; Saul et al., 1996).", "startOffset": 159, "endOffset": 230}, {"referenceID": 17, "context": "One procedure that can be used to approximate correlations such as hsisjiQ1 is given by the so-called linear response theory (Parisi, 1988).", "startOffset": 125, "endOffset": 139}], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}