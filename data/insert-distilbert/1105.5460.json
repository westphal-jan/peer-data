{"id": "1105.5460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Decision-Theoretic Planning: Structural Assumptions and Computational Leverage", "abstract": "planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including ai planning, decision analysis, operations research, control theory and economics. while the assumptions and perspectives adopted in these disciplinary areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be closely modeled as markov decision processes ( mdps ) and analyzed principally using the techniques of decision theory. this paper presents an overview and synthesis of mdp - related methods, showing how they provide a unifying framework for modeling many classes model of planning problems studied in ai. it also describes structural properties of mdps that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. planning problems commonly possess structure symbols in the reward and value functions used to describe performance criteria, in the functions used to describe observed state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations. specialized representations, and algorithms employing these representations, can achieve computational leverage by exploiting these various forms of structure. certain ai techniques - - \u2013 in particular those based completely on the use of structured, intensional representations - - can be viewed in this way. this paper surveys several types of representations for both classical and decision - theoretic planning problems, and planning algorithms that exploit these representations in a number of physically different ways to ease the computational burden of constructing policies or plans. it focuses primarily works on abstraction, aggregation and decomposition techniques based on ai - style representations.", "histories": [["v1", "Fri, 27 May 2011 01:53:02 GMT  (371kb)", "http://arxiv.org/abs/1105.5460v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c boutilier", "t dean", "s hanks"], "accepted": false, "id": "1105.5460"}, "pdf": {"name": "1105.5460.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Craig Boutilier"], "emails": ["cebly@cs.ubc.ca", "tld@cs.brown.edu", "hanks@cs.washington.edu"], "sections": [{"heading": null, "text": "Journal of Arti cial Intelligence Research 11 (1999) 1{94 Submitted 09/98; published 07/99 Decision-Theoretic Planning: Structural Assumptions andComputational LeverageCraig Boutilier cebly@cs.ubc.caDepartment of Computer Science, University of British ColumbiaVancouver, BC, V6T 1Z4, CanadaThomas Dean tld@cs.brown.eduDepartment of Computer Science, Brown UniversityBox 1910, Providence, RI, 02912, USASteve Hanks hanks@cs.washington.eduDepartment of Computer Science and Engineering, University of WashingtonSeattle, WA, 98195, USA AbstractPlanning under uncertainty is a central problem in the study of automated sequentialdecision making, and has been addressed by researchers in many di erent elds, includingAI planning, decision analysis, operations research, control theory and economics. Whilethe assumptions and perspectives adopted in these areas often di er in substantial ways,many planning problems of interest to researchers in these elds can be modeled asMarkovdecision processes (MDPs) and analyzed using the techniques of decision theory.This paper presents an overview and synthesis of MDP-related methods, showing howthey provide a unifying framework for modeling many classes of planning problems studiedin AI. It also describes structural properties of MDPs that, when exhibited by particu-lar classes of problems, can be exploited in the construction of optimal or approximatelyoptimal policies or plans. Planning problems commonly possess structure in the rewardand value functions used to describe performance criteria, in the functions used to describestate transitions and observations, and in the relationships among features used to describestates, actions, rewards, and observations.Specialized representations, and algorithms employing these representations, can achievecomputational leverage by exploiting these various forms of structure. Certain AI techniques|in particular those based on the use of structured, intensional representations|can beviewed in this way. This paper surveys several types of representations for both classicaland decision-theoretic planning problems, and planning algorithms that exploit these rep-resentations in a number of di erent ways to ease the computational burden of constructingpolicies or plans. It focuses primarily on abstraction, aggregation and decomposition tech-niques based on AI-style representations.1. IntroductionPlanning using decision-theoretic notions to represent domain uncertainty and plan qualityhas recently drawn considerable attention in arti cial intelligence (AI).1 Decision-theoreticplanning (DTP) is an attractive extension of the classical AI planning paradigm because itallows one to model problems in which actions have uncertain e ects, the decision maker has1. See, for example, the recent texts (Dean, Allen, & Aloimonos, 1995; Dean & Wellman, 1991; Russell &Norvig, 1995) and the research reported in (Hanks, Russell, & Wellman, 1994).c 1999 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.\nBoutilier, Dean, & Hanksincomplete information about the world, where factors such as resource consumption lead tosolutions of varying quality, and where there may not be an absolute or well-de ned \\goal\"state. Roughly, the aim of DTP is to form courses of action (plans or policies) that havehigh expected utility rather than plans that are guaranteed to achieve certain goals. WhenAI planning is viewed as a particular approach to solving sequential decision problems ofthis type, the connections between DTP and models used in other elds of research|suchas decision analysis, economics and operations research (OR)|become more apparent. Ata conceptual level, most sequential decision problems can be viewed as instances of Markovdecision processes (MDPs), and we will use the MDP framework to make the connectionsexplicit.Much recent research on DTP has explicitly adopted the MDP framework as an under-lying model (Barto, Bradtke, & Singh, 1995; Boutilier & Dearden, 1994; Boutilier, Dearden,& Goldszmidt, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1993; Koenig, 1991; Simmons& Koenig, 1995; Tash & Russell, 1994), allowing the adaptation of existing results and algo-rithms for solving MDPs (e.g., from the eld of OR) to be applied to planning problems. Indoing so, however, this work has departed from the traditional de nition of the \\planningproblem\" in the AI planning community|one goal of this paper is to make explicit theconnection between these two lines of work.Adopting the MDP framework as a model for posing and solving planning problemshas illuminated a number of interesting connections among techniques for solving decisionproblems, drawing on work from AI planning, reasoning under uncertainty, decision analysisand OR. One of the most interesting insights to emerge from this body of work is that manyDTP problems exhibit considerable structure, and thus can be solved using special-purposemethods that recognize and exploit that structure. In particular, the use of feature-basedrepresentations to describe problems, as is the typical practice in AI, often highlights theproblem's special structure and allows it to be exploited computationally with little e ort.There are two general impediments to the more widespread acceptance of MDPs withinAI as a general model of planning. The rst is the absence of explanations of the MDP modelthat make the connections to current planning research explicit, at either the conceptualor computational level. This may be due in large part to the fact that MDPs have beendeveloped and studied primarily in OR, where the dominant concerns are, naturally, ratherdi erent. One aim of this paper is to make the connections clear: we provide a briefdescription of MDPs as a conceptual model for planning that emphasizes the connectionto AI planning, and explore the relationship between MDP solution algorithms and AIplanning algorithms. In particular, we emphasize that most AI planning models can beviewed as special cases of MDPs, and that classical planning algorithms have been designedto exploit the problem characteristics associated with these cases.The second impediment is skepticism among AI researchers regarding the computationaladequacy of MDPs as a planning model: can the techniques scale to solve planning problemsof reasonable size? One di culty with solution techniques for MDPs is the tendency to relyon explicit, state-based problem formulations. This can be problematic in AI planningsince state spaces grow exponentially with the number of problem features. State space sizeand dimensionality are of somewhat lesser concern in OR and decision analysis. In these elds, an operations researcher or decision analyst will often hand-craft a model that ignorescertain problem features deemed irrelevant, or will de ne other features that summarize a2\nDecision-Theoretic Planning: Structural Assumptionswide class of problem states. In AI, the emphasis is on the automatic solution of problemsposed by users who lack the expertise of a decision analyst. Thus, assuming a well-crafted,compact state space is often not appropriate.In this paper we show how specialized representations and algorithms from AI planningand problem solving can be used to design e cient MDP solution techniques. In particular,AI planning methods assume a certain structure in the state space, in the actions (oroperators), and in the speci cation of a goal or other success criteria. Representations andalgorithms have been designed that make the problem structure explicit and exploit thatstructure to solve problems e ectively. We demonstrate how this same process of identifyingstructure, making it explicit, and exploiting it algorithmically can be brought to bear inthe solution of MDPs.This paper has several objectives. First, it provides an overview of DTP and MDPssuitable for readers familiar with traditional AI planning methods and makes connectionswith this work. Second, it describes the types of structure that can be exploited and howAI representations and methods facilitate computationally e ective planning with MDPs.As such, it is a suitable introduction to AI methods for those familiar with the classicalpresentation of MDPs. Finally, it surveys recent work on the use of MDPs in AI andsuggests directions for further research in this regard, and should therefore be of interest toresearchers in DTP.1.1 General Problem De nitionRoughly speaking, the class of problems we consider are those involving systems whosedynamics can be modeled as stochastic processes, where the actions of decision maker,referred to here as the agent , can in uence the system's behavior. The system's currentstate and the choice of action jointly determine a probability distribution over the system'spossible next states. The agent prefers to be in certain system states (e.g., goal states) overothers, and therefore must determine a course of action|also called a \\plan\" or \\policy\" inthis paper|that is likely to lead to these target states, possibly avoiding undesirable statesalong the way. The agent may not know the system's state exactly in making its decisionon how to act, however|it may have to rely on incomplete and noisy sensors and be forcedto base its choice of action on a probabilistic estimate of the state.To help illustrate the types of problems in which we are interested, consider the followingexample. Imagine that we have a robot agent designed to help someone (the \\user\") in ano ce environment (see Figure 1). There are three activities it might undertake: picking upthe user's mail, getting co ee, or tidying up the user's research lab. The robot can movefrom location to location and perform various actions that tend to achieve certain targetstates (e.g., bringing co ee to the user on demand, or maintaining a minimal level of tidinessin the lab).We might associate a certain level of uncertainty with the e ects of the robot's actions(e.g., when it tries to move to an adjacent location it might succeed 90% of the time and failto move at all the other 10% of the time). The robot might have incomplete access to thetrue state of the system in that its sensors might supply it with incomplete information (itcannot tell whether mail is available for pickup if it is not in the mail room) and incorrect3"}, {"heading": "Boutilier, Dean, & Hanks", "text": "HallwayMy Office\nCoffee\nMailRoom Lab\nFigure 1: A decision-theoretic planning probleminformation (even when in the mail room its sensors occasionally fail to detect the presenceof mail).Finally, the performance of the robot might be measured in various ways: do its actionsguarantee that a goal will be achieved? Do they maximize some objective function de nedover possible e ects of its actions? Do they achieve a goal state with su cient probabil-ity while avoiding \\disastrous\" states with near certainty? The stipulation of optimal oracceptable behavior is an important part of the problem speci cation.The types of problems that can be captured using this general framework include clas-sical (goal-oriented, deterministic, complete knowledge) planning problems and extensionssuch as conditional and probabilistic planning problems, as well as other more generalproblem formulations.The discussion to this point has assumed an extensional representation of the system'sstates|one in which each state is explicitly named. In AI research, intensional represen-tations are more common. An intensional representation is one in which states or sets ofstates are described using sets of multi-valued features. The choice of an appropriate setof features is an important part of the problem design. These features might include thecurrent location of the robot, the presence or absence of mail, and so on. The performancemetric is also typically expressed intensionally. Figure 2 serves as a reference for our exam-ple problem, which we use throughout the paper. It lists the basic features used to describethe states of the system, the actions available to the robot and the exogenous events thatmight occur, together with an intuitive description of the features, actions and events.The remainder of the paper is organized as follows. In Section 2, we present the MDPframework in the abstract, introducing basic concepts and terminology and noting the rela-tionship between this abstract model and the classical AI planning problem. Section 3 sur-veys common solution techniques|algorithms based on dynamic programming for generalMDP problems and search algorithms for planning problems|and points out the relation-ship between problem assumptions and solution techniques. Section 4 turns from algorithmsto representations, showing various ways in which the structured representations commonlyused by AI algorithms can be used to represent MDPs compactly as well. Section 5 surveys4\nDecision-Theoretic Planning: Structural AssumptionsFeatures Denoted DescriptionLocation Loc(M), etc. Location of robot. Five possible locations: mailroom (M), co ee room(C), user's o ce (O), hallway (H), laboratory (L)Tidiness T (0), etc. Degree of lab tidiness. Five possible values: from 0 (messiest) to 4(tidiest)Mail present M;M Is there mail is user's mail box? True (M) or False (M)Robot has mail RHM;RHM Does the robot have mail in its possession?Co ee request CR;CR Is there an outstanding (unful lled) request for co ee by the user?Robot has co ee RHC;RHC Does the robot have co ee in its possession?Actions Denoted DescriptionMove clockwise Clk Move to adjacent location (clockwise direction)Counterclockwise CClk Move to adjacent location (counterclockwise direction)Tidy lab Tidy If the robot is in the lab, the degree of tidiness is increased by 1Pickup mail PUM If the robot is in the mailroom and there is mail present, the robottakes the mail (RHM becomes true and M becomes false)Get co ee GetC If the robot is in the co ee room, it gets co ee (RHC becomes true)Deliver mail DelM If the robot is in the o ce and has mail, it hands the mail to the user(RHM becomes false)Deliver co ee DelC If the robot is in the o ce and has co ee, it hands the co ee to theuser (RHC and CR both become false)Events Denoted DescriptionMail arrival ArrM Mail arrives causing M to become trueRequest co ee ReqC User issues co ee request causing CR to become trueUntidy the lab Mess The lab becomes messier (one degree less tidy)Figure 2: Elements of the robot domain.some recent work on abstraction, aggregation and problem decomposition methods, andshows the connection to more traditional AI methods such as goal regression. This lastsection demonstrates that representational and computational methods from AI planningcan be used in the solution of general MDPs. Section 5 also points out additional ways inwhich this type of computational leverage might be developed in the future.2. Markov Decision Processes: Basic Problem FormulationIn this section we introduce the MDP framework and make explicit the relationship betweenthis model and classical AI planning models. We are interested in controlling a stochasticdynamical system: a system that at any point in time can be in one of a number of distinctstates, and in which the system's state changes over time in response to events. An actionis a particular kind of event instigated by an agent in order to change the system's state.We assume that the agent has control over what actions are taken and when, though thee ects of taking an action might not be perfectly predictable. In contrast, exogenous eventsare not under the agent's control, and their occurrence may be only partially predictable.This abstract view of an agent is consistent both with the \\AI\" view where the agent is anautonomous decision maker and the \\control\" view where a policy is determined ahead oftime, programmed into a device, and executed without further deliberation.5\nBoutilier, Dean, & Hanks2.1 States and State TransitionsWe de ne a state to be a description of the system at a particular point in time. How onede nes states can vary with particular applications, some notions being more natural thanothers. However, it is common to assume that the state captures all information relevantto the agent's decision-making process. We assume a nite state space S = fs1; : : : ; sNgof possible system states.2 In most cases the agent will not have complete informationabout the current state; this uncertainty or incomplete information can be captured usinga probability distribution over the states in S.A discrete-time stochastic dynamical system consists of a state space and probabilitydistributions governing possible state transitions|how the next state of the system dependson past states. These distributions constitute a model of how the system evolves over timein response to actions and exogenous events, re ecting the fact that the e ects of actionsand events may not be perfectly predictable even if the prevailing state is known.Although we are generally concerned with how the agent chooses an appropriate courseof action, for the remainder of this section we assume that the agent's course of action is xed, concentrating on the problem of predicting the system's state after the occurrence ofa predetermined sequence of actions. We discuss the action selection problem in the nextsection.We assume the system evolves in stages, where the occurrence of an event marks thetransition from one stage t to the next stage t + 1. Since events de ne changes in stage,and since events often (but not necessarily) cause state transitions, we often equate stagetransitions with state transitions. Of course, it is possible for an event to occur but leavethe system in the same state.The system's progression through stages is roughly analogous to the passage of time.The two are identical if we assume that some action (possibly a no-op) is taken at eachstage, and that every action takes unit time to complete. We can thus speak loosely as ifstages correspond to units of time, and we refer to T interchangeably as the set of all stagesand the set of all time points.3We can model uncertainty by regarding the system's state at some stage t as a randomvariable St that takes values from S. An assumption of \\forward causality\" requires that thevariable St does not depend directly on the value of future variable Sk (k > t). Roughly,it requires that we model our system such that the past history \\directly\" determinesthe distribution over current states, whereas knowledge of future states can in uence theestimate of the current state only indirectly by providing evidence on what the current statemay have been so as to lead to these future states. Figure 3(a) shows a graphical perspectiveon a discrete-time, stochastic dynamical system. The nodes are random variables denotingthe state at a particular time, and the arcs indicate the direct probabilistic dependenceof states on previous states. To describe this system completely we must also supply theconditional distributions Pr(StjS0; S1; St 1) for all times t.States should be thought of as descriptions of the system being modeled, so the ques-tion arises of how much detail about the system is captured in a state description. More2. Most of the discussion in this paper also applies to cases where the state space is countably in nite. See(Puterman, 1994) for a discussion of in nite and continuous-state problems.3. While we do not deal with such topics here, there is a considerable literature in the OR community oncontinuous-time Markov decision processes (Puterman, 1994).6"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "S(a)\n(b) (c)\nS 0\nS 1 2 t-1 S t S\nS 0\nS 1\nS 2\nS t-1\nS t\nS t-1 S tFigure 3: A general stochastic process (a), a Markov chain (b), and a stationary Markovchain (c).detail implies more information about the system, which in turn often translates into betterpredictions of future behavior. Of course, more detail also implies a larger set S, which canincrease the computational cost of decision making.It is commonly assumed that a state contains enough information to predict the nextstate. In other words, any information about the history of the system relevant to predictingits future is captured explicitly in the state itself. Formally, this assumption, the Markovassumption, says that knowledge of the present state renders information about the pastirrelevant to making predictions about the future:Pr(St+1jSt; St 1; : : : ; S0) = Pr(St+1jSt)Markovian models can be represented graphically using a structure like that in Figure 3(b),re ecting the fact that the present state is su cient to predict future state evolution.4Finally, it is common to assume that the e ects of an event depend only on the prevailingstate, and not the stage or time at which the event occurs.5 If the distribution predictingthe next state is the same regardless of stage, the model is said to be stationary and canbe represented schematically using just two stages, as in Figure 3(c). In this case only asingle conditional distribution is required. In this paper we generally restrict our attentionto discrete-time, nite-state, stochastic dynamical systems with the Markov property, com-monly called Markov chains. Furthermore, most of our discussion is restricted to stationarychains.To complete the model we must provide a probability distribution over initial states,re ecting the probability of being in any state at stage 0. This distribution can be repre-4. It is worth mentioning that the Markov property applies to the particular model and not to the systemitself. Indeed, any non-Markovian model of a system (of nite order, i.e., whose dynamics depend on atmost the k previous states for some k) can be converted to an equivalent though larger Markov model.In control theory, this is called conversion to state form (Luenberger, 1979).5. Of course, this is also a statement about model detail, saying that the state carries enough informationto make the stage irrelevant to predicting transitions.7\nBoutilier, Dean, & Hanks 1 6\n7\n3\n2\n4\n5\n.7\n.3\n.5\n.5\n.8\n.2 1.0\n1.0\n.1\n.1 .9\n.5\n.4\nFigure 4: A state-transition diagram.sented as a real-valued (row) vector of size N = jSj (one entry for each state). We denotethis vector P 0 and use p0i to denote its ith entry, that is, the probability of starting in statesi. We can represent a T -stage nonstationary Markov chain with T transition matrices,each of size N N , where matrix P t captures the transition probabilities governing thesystem as it moves from stage t to stage t + 1. Each matrix consists of probabilities ptij ,where ptij = Pr(St+1 = sjjSt = si). If the process is stationary, the transition matrix isthe same at all stages and one matrix (whose entries are denoted pij) will su ce. Given aninitial distribution over states P 0, the probability distribution over states after n stages isQ0i=n P i.A stationary Markov process can also be represented using a state-transition diagramas in Figure 4. Here nodes correspond to particular states and the stage is not representedexplicitly. Arcs denote possible transitions (those with non-zero probability) and are labeledwith the transition probabilities pij = Pr(St+1 = sjjSt = si). The arc from node i to nodej is labeled with pij if pij > 0.6 The size of such a diagram is at least O(N) and at mostO(N2), depending on the number of arcs. This is a useful representation when the transitiongraph is relatively sparse, for example, when most states have immediate transitions to onlyfew neighbors.Example 2.1 To illustrate these notions, imagine that the robot in Figure 1 is executingthe policy of moving counterclockwise repeatedly. We restrict our attention to twovariables, location Loc and presence of mail M , giving a state space of size 10. Wesuppose that the robot always moves to the adjacent location with probability 1:0.In addition, mail can arrive at the mailroom with probability 0:2 at any time (inde-pendent of the robot's location), causing the variable M to become true. Once Mbecomes true, the robot cannot move to a state where M is false, since the action ofmoving does not in uence the presence of mail. The state-transition diagram for thisexample is illustrated in Figure 5. The transition matrix is also shown. 2The structure of a Markov chain is occasionally of interest to us in planning. A subsetC S is closed if pij = 0 for all i 2 C and j 62 C. It is a proper closed set if no propersubset of C enjoys this property. We sometimes refer to proper closed sets as recurrentclasses of states. If a closed set consists of a single state, then that state is called anabsorbing state. Once an agent enters a closed set or absorbing state, it remains there6. It is important to note that the nodes here do not represent random variables as in the earlier gures.8"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "LM LM OM OM HM HM\nCM CM\nMM MM\n0.2\n0.8\n0.8 0.8\n0.8\n0.8 1.0\n1.0\n1.0\n1.0 0.2 0.2\n1.0 0.20.2\ns3\ns7\ns8\ns9\ns2\ns1 s6\ns10\ns4\ns5 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10s1 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0 0:0s2 0:0 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0s3 0:0 0:0 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0s4 0:0 0:0 0:0 0:0 0:8 0:0 0:0 0:0 0:0 0:2s5 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0 0:0 0:0s6 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0s7 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0s8 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0s9 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0s10 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0Figure 5: The state-transition diagram and transition matrix for a moving robot.forever with probability 1. In the example above (Figure 5), the set of states where Mholds forms a recurrent class. There are no absorbing states in the example, but should weprogram the robot to stay put whenever it is in the state hM;Loc(O)i, then this would bean absorbing state in the altered chain. Finally, we say a state is transient if it does notbelong to a recurrent class. In Figure 5, each state where M holds is transient|eventually(with probability 1), the agent leaves the state and never returns, since there is no way toremove mail once it arrives.2.2 ActionsMarkov chains can be used to describe the evolution of a stochastic system, but they donot capture the fact that an agent can choose to perform actions that alter the state of thesystem. A key element of MDPs is the set of actions available to the decision maker. Whenan action is performed in a particular state, the state changes stochastically in response tothe action. We assume that the agent takes some action at each stage of the process, andthen the system changes state accordingly.At each stage t of the process and each state s, the agent has available a set of actionsAts. This is called the feasible set for s at stage t. To describe the e ects of a 2 Ats, we mustsupply the state-transition distribution Pr(St+1jSt = s;At = a) for all actions a, states s,and stages t. Unlike the case of a Markov chain, the terms Pr(St+1jSt = s;At = a) are nottrue conditional distributions, but rather a family of distributions parameterized by St andAt, since the probability of At is not part of the model. We retain this notation, however,for its suggestive nature.We often assume that the feasible set of actions is the same for all stages and states, inwhich case the set of actions is A = fa1; : : : ; aKg and each can be executed at any time.This contrasts with the AI planning practice of assigning preconditions to actions de ningthe states in which they can meaningfully be executed. Our model takes the view that anyaction can be executed (or \\attempted\") in any state. If the action has no e ect whenexecuted in some state, or its execution leads to disastrous e ects, this can be noted inthe action's transition matrix. Action preconditions are often a computational conveniencerather than a representational necessity: they can make the planning process more e cientby identifying states in which the planner should not even consider selecting that action.Preconditions can be represented in MDPs by relaxing the assumption that the set of9\nBoutilier, Dean, & Hankss1 s2 s3 s4 s5 s6 s7 s8 s9 s10s1 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0 0:2s2 0:0 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0s3 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0 0:0s4 0:0 0:0 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0s5 0:8 0:0 0:0 0:0 0:0 0:2 0:0 0:0 0:0 0:0s6 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0s7 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0s8 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0s9 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0s10 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0 LM LM OM OM HM HM CM CM MMMM 0.8 0.8 0.8 1.0 1.0 1.0 1.0 1.0 0.8 0.8 0.2 0.2 0.2 0.2 0.2 s1 s2 s3 s4 s5 s6 s7 s8 s9 s10\nFigure 6: The transition matrix for Clk and the induced transition diagram for a two-actionpolicy.feasible actions is the same for all states. To illustrate planning concepts below, however,we sometimes assume actions do have preconditions.We again restrict our attention to stationary processes, which in this case means thatthe e ects of each action depends only on the state and not on the stage. Our transitionmatrices thus take the form pkij = Pr(St+1 = sjjSt = si; At = ak), capturing the probabilitythat the system moves to state sj when ak is executed in state si. In stationary models anaction is fully described by a single N N transition matrix P k. It is important to notethat the transition matrix for an action includes not only the direct e ects of executing theaction but also the e ects of any exogenous events that might occur at the same stage.7Example 2.2 The example in Figure 5 can be extended so the agent has two availableactions: moving clockwise and moving counterclockwise. The transition matrix forCClk (with the assumption that mail arrives with probability 0:2) is shown in Figure 5.The matrix for Clk appears on the left in Figure 6. Suppose the agent xes its behaviorso that it moves clockwise in locationsM and C and counterclockwise in locations H,O and L (we address below how the agent might come to know its location so that itcan actually implement this behavior). This de nes the Markov chain illustrated inthe transition diagram on the right in Figure 6. 22.3 Exogenous EventsExogenous events are those events that stochastically cause state transitions, much likeactions, but beyond the control of the decision maker. These might correspond to theevolution of a natural process or the action of another agent. Notice that the e ect ofthe action CClk in Figure 5 \\combines\" the e ects of the robot's action with that of theexogenous event of mail arrival: state-transition probabilities incorporate both the motionof the robot (causing a change in location) and the possible change in mail status dueto mail arrival. For the purposes of decision making, it is precisely this combined e ect7. It is possible to assess the e ects of actions and exogenous events separately, then combine them intoa single transition matrix in certain cases (Boutilier & Puterman, 1995). We discuss this later in thissection. 10\nDecision-Theoretic Planning: Structural Assumptionsthat is important when predicting the distribution over possible states resulting when anaction is taken. We call such models of actions implicit-event models, since the e ects ofthe exogenous event are folded into the transition probabilities associated with the action.However, it is often natural to view these transitions as comprised of these two separateevents, each having its own e ect on the state. More generally, we often think of transitionsas determined by the e ects of the agent's chosen action and those of certain exogenousevents beyond the agent's control, each of which may occur with a certain probability.When the e ects of actions are decomposed in this fashion, we call the action model anexplicit-event model.Specifying a transition function for an action and zero or more exogenous events is notgenerally easy, for actions and events can interact in complex ways. For instance, considerspecifying the e ect of action PUM (pickup mail) at a state where no mail is present butthere is the possibility of \\simultaneous\" mail arrival (i.e., during the \\same unit\" of discretetime). If the event ArrM occurs, does the robot obtain the newly arrived mail, or does themail remain in the mailbox? Intuitively, this depends on whether the mail arrived before orafter the pickup was completed (albeit within the same time quantum). The state transitionin this case can be viewed as the composition of two transitions where the precise descriptionof the composition depends on the ordering of the agent's action and the exogenous event.If mail arrives rst, the transition might be s ! s0 ! s00, where s0 is a state where mailis waiting and s00 is a state where no mail is waiting and the robot is holding mail; but ifthe pickup action is completed rst, the transition would be s! s! s0 (i.e., PUM has noe ect, then mail arrives and remains in the box).The picture is more complicated if the actions and events can truly occur simultaneouslyover some interval|in this case the resulting transition need not be a composition of theindividual transitions. As an example, if the robot lifts the side of a table on which a glassof water is situated, the water will spill; similarly if an exogenous event causes the other sideto be raised. But if the action and event occur simultaneously, the result is qualitativelydi erent (the water is not spilled). Thus, the \\interleaving\" semantics described above isnot always appropriate.Because of such complications, modeling exogenous events and their combination withactions or other events can be approached in many ways, depending on the modeling as-sumptions one is willing to make. Generally, we specify three types of information. First,we provide transition probabilities for all actions and events under the assumption thatthese occur in isolation|these are standard transition matrices. The transition matrix inFigure 5 can be decomposed into the two matrices shown in Figure 7, one for Clk and onefor ArrM.8 Second, for each exogenous event, we must specify its probability of occurrence.Since this can vary with the state, we generally require a vector of length N indicating theprobability of occurrence at each state. The occurrence vector for ArrM would be[0:2 0:2 0:2 0:2 0:2 0:0 0:0 0:0 0:0 0:0]8. The fact that these individual matrices are deterministic is an artifact of the example. In general, theactions and events will each be represented using genuinely stochastic matrices.11\nBoutilier, Dean, & Hankss1 s2 s3 s4 s5 s6 s7 s8 s9 s10s1 0:0 1:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0s2 0:0 0:0 1:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0s3 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0 0:0 0:0s4 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0 0:0s5 1:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0s6 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0s7 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0s8 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0s9 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0s10 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0Action Clk s1 s2 s3 s4 s5 s6 s7 s8 s9 s10s1 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0s2 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0s3 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0s4 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0s5 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0s6 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0 0:0s7 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0 0:0s8 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0 0:0s9 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0 0:0s10 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 0:0 1:0Event ArrMFigure 7: The transition matrices for an action and exogenous event in an explicit-eventmodel.where we assume, for illustration, that mail arrives only when none is present.9 The nalrequirement is a combination function that describes how to \\compose\" the transitions of anaction with any subset of event transitions. As indicated above, this can be very complex,sometimes almost unrelated to the individual action and event transitions. However, undercertain assumptions combination functions can be speci ed reasonably concisely.One way of modeling the composition of transitions is to assume an interleaving seman-tics of the type alluded to above. In this case, one needs to specify the probability thatthe action and events that take place occur in a speci c order. For instance, one mightassume that each event occurs at a time|within the discrete time unit|according to somecontinuous distribution (e.g., an exponential distribution with a given rate). With this in-formation, the probability of any particular ordering of transitions, given that certain eventsoccur, can be computed, as can the resulting distribution over possible next states. In theexample above, the probability of (composed) transitions s1 ! s2 ! s3 and s1 ! s1 ! s2would be given by the probabilities with which mail arrived rst or last, respectively.In certain cases, the probability of this ordering is not needed. To illustrate anothercombination function, assume that the action always occurs before the exogenous events.Furthermore, assume that events are commutative: (a) for any initial state s and any pairof events e1 and e2, the distribution that results from applying event sequence e1 e2 to s isidentical to that obtained from the sequence e2 e1; and (b) the occurrence probabilities atintermediate states are identical. Intuitively, the set of events in our domain, ArrM, ReqCand Mess, has this property. Under these conditions the combined transition distributionfor any action a is computed by considering the probability of any subset of events andapplying that subset in any order to the distribution associated with a.Generally, we can construct an implicit-event model from the various components of theexplicit-event model; thus, the \\natural\" speci cation can be converted to the form usuallyused by MDP solution algorithms. Under the two assumptions above, for instance, we canform an implicit event transition matrix Pr(si; a; sj) for any action a, given the matrixcPra(si; sj) for a (which assumes no event occurrences), the matrices Pre(si; sj) for eventse, and the occurrence vector Pre(si) for each event e. The e ective transition matrix for9. The probability of di erent events may be correlated (possibly at particular states). If this is the case,then it is necessary to specify occurrence probabilities for subsets of events. We will treat event occurrenceprobabilities as independent for ease of exposition.12\nDecision-Theoretic Planning: Structural Assumptionsevent e is de ned as follows:cPre(si; sj) = Pre(si)Pre(si; sj) +( 1 Pre(si) : i = j0 : i 6= jThis equation captures the event transition probabilities with the probability of event oc-currence factored in. If we let E;E0 denote the diagonal matrices with entries Ekk = Pre(sk)and E0kk = 1 Pre(sk), then cPre(si; sj) = E Pre+E0. Under the assumptions above, theimplicit-event matrix Pr(si; a; sj) for action a is then given by Pr = cPre1 cPren Pra forany ordering of the n possible events.Naturally, di erent procedures for constructing implicit-event matrices will be requiredgiven di erent assumptions about action and event interaction. Whether such implicit mod-els are constructed or speci ed directly without explicit mention of the exogenous events, wewill always assume unless stated otherwise that action transition matrices take into accountthe e ects of exogenous events as well, and thus represent the agent's best informationabout what will happen if it takes a particular action.2.4 ObservationsAlthough the e ects of an action can depend on any aspect of the prevailing state, thechoice of action can depend only on what the agent can observe about the current stateand remember about its prior observations. We model the agent's observational or sensingcapabilities by introducing a nite set of observations O = fo1; : : : ; oHg. The agent receivesan observation from this set at each stage prior to choosing its action at that stage. Wecan model this observation as a random variable Ot whose value is taken from O. Theprobability that a particular Ot is generated can depend on: the state of the system at t 1 the action taken at t 1 the state of the system at t after taking the action at t 1 and after the e ects of anyexogenous events at t 1 are realized, but before the action at t is taken.We let Pr(Ot = ohjSt 1 = si; At 1 = ak; St = sj) be the probability that the agent observesoh at stage t given that it performs ak in state si and ends up in state sj. As with actions,we assume that observational distributions are stationary (independent of the stage), usingphi;j;k = Pr(ohjsi; ak; sj) to denote this quantity. We can view the probabilistic dependenciesamong state, action and observation variables as a graph in which the time-indexed variablesare shown as nodes and one variable is directly probabilistically dependent on another ifthere is an edge from the latter to the former; see Figure 8.This model allows a wide variety of assumptions about the agent's sensing capabilities.At one extreme are fully observable MDPs (FOMDPs), in which the agent knows exactlywhat state it is in at each stage t. We model this case by letting O = S and settingPr(ohjsi; ak; sj) = ( 1 i oh = sj0 otherwise13"}, {"heading": "Boutilier, Dean, & Hanks", "text": "S\nt-1 S t\nt-1\nt\nA\nOFigure 8: Graph showing the dependency relationships among states, actions and observa-tions at di erent times.In the example above, this means the robot always knows its exact location and whether ornot mail is waiting in the mailbox, even if it is not in the mailroom when the mail arrives.The agent thus receives perfect feedback about the results of its actions and the e ectsof exogenous events|it has noisy e ectors but complete, noise-free, and \\instantaneous\"sensors. Most recent AI research that adopts the MDP framework explicitly assumes fullobservability.At the other extreme we might consider non-observable systems (NOMDPs) in whichthe agent receives no information about the system's state during execution. We can modelthis case by letting O = fog. Here the same observation is reported at each stage, revealingno information about the state, so that Pr(sjjsi; ak; o) = Pr(sjjsi; ak). In these open-loopsystems, the agent receives no useful feedback about the results of its actions: the agent hasnoisy e ectors and no sensors. In this case an agent chooses its actions according to a planconsisting of a sequence of actions executed unconditionally. In e ect, the agent is relyingon its predictive model to determine good action choices before execution time.Traditionally, AI planning work has implicitly made the assumption of non-observability,often coupled with an omniscience assumption|that the agent knows the initial state withcertainty, can predict the e ects of its actions perfectly, and can precisely predict the oc-currence of any exogenous events and their e ects. Under these circumstances, the agentcan predict the exact outcome of any plan, thus obviating the need for observation. Suchan agent can build a straight-line plan|a sequence of actions to be performed withoutfeedback|that is as good as any plan whose execution might depend on information gath-ered at execution time.These two extremes are special cases of the general observation model described above,which allows the agent to receive incomplete or noisy information about the system state(i.e., partially observable MDPs, or POMDPs). For example, the robot might be able todetermine its location exactly, but might not be able to determine whether mail arrivesunless it is in the mailroom. Furthermore, its \\mail\" sensors might occasionally reportinaccurately, leading to an incorrect belief as to whether there is mail waiting.Example 2.3 Suppose the robot has a \\checkmail\" action that does not change the systemstate but generates an observation that is in uenced by the presence of mail, provided14\nDecision-Theoretic Planning: Structural AssumptionsPr(Obs = mail) Pr(Obs = nomail)Loc(M);M 0:92 0:08Loc(M);M 0:05 0:95Loc(M);M 0:00 1:00Loc(M);M 0:00 1:00Figure 9: Observation probabilities for checking mailbox.the robot is in the mailroom at the time the action is performed. If the robot is notin the mailroom, the sensor always reports \\no mail.\" A noisy \\checkmail\" sensorcan be described by a probability distribution like the one shown in Figure 9. We canview these error probabilities as the probability of \\false positives\" (0:05) and \\falsenegatives\" (0:08). 22.5 System Trajectories and Observable HistoriesWe use the terms trajectory and history interchangeably to describe the system's behaviorduring the course of a problem-solving episode, or perhaps some initial segment thereof.The complete system history is the sequence of states, actions, and observations generatedfrom stage 0 to some time point of interest, and can be of nite or in nite length. Completehistories can be represented by a (possibly in nite) sequence of tuples of the formhhS0; O0; A0i; hS1; O1; A1i; : : : hST ; OT ; AT iiWe can de ne two alternative notions of history that contain less complete information.For some arbitrary stage t we de ne the observable history as the sequencehhO0; A0i; : : : ; hOt 1; At 1iiwhere O0 is the observation of the initial state. The observable history at stage t comprisesall information available to the agent about its history when it chooses its action at stage t.A third type of trajectory is the system trajectory, which is the sequencehhS0; A0i; : : : ; hSt 1; At 1i; Stidescribing the system's behavior in \\objective\" terms, independent of the agent's particularview of the system.In evaluating an agent's performance, we will generally be interested in the systemtrajectory. An agent's policy must be de ned in terms of the observable history, since theagent does not have access to the system trajectory, except in the fully observable case,when the two are equivalent.2.6 Reward and ValueThe problem facing the decision maker is to select an action to be performed at each stageof the decision problem, making this decision on the basis of the observable history. Theagent still needs some way to judge the quality of a course of action. This is done by de ning15"}, {"heading": "Boutilier, Dean, & Hanks", "text": "S\nt-1 S t\nt C\nt-1 A\nt RFigure 10: Decision process with rewards and action costs.a value function V( ) as a function mapping the set of system histories HS into the reals;that is, V : HS ! R.10 The agent prefers system history h to h0 just in case V(h) > V(h0).Thus, the agent judges its behavior to be good or bad depending on its e ect on theunderlying system trajectory. Generally, the agent cannot predict with certainty whichsystem trajectory will occur, and can at best generate a probability distribution over thepossible trajectories caused by its actions. In that case, it computes the expected value ofeach candidate course of action and chooses a policy that maximizes that quantity.Just as with system dynamics, specifying a value function over arbitrary trajectoriescan be cumbersome and unintuitive. It is therefore important to identify structure in thevalue function that can lead to a more parsimonious representation.Two assumptions about value functions commonly made in the MDP literature aretime-separability and additivity. A time-separable value function is de ned in terms ofmore primitive functions that can be applied to component states and actions. The rewardfunction R : S ! R associates a reward with being in a state s. Costs can be assignedto taking actions by de ning a cost function C : S A ! R that associates a cost withperforming an action a in state s. Rewards are added to the value function, while costs aresubtracted.11A value function is time-separable if it is a \\simple combination\" of the rewards and costsaccrued at each stage. \\Simple combination\" means that value is taken to be a function ofcosts and rewards at each stage, where the costs and rewards can depend on the stage t, butthe function that combines these must be independent of the stage, most commonly a linearcombination or a product.12 A value function is additive if the combination function is asum of the reward and cost function values accrued over the history's stages. The additionof rewards and action costs in a system with time-separable value can be viewed graphicallyas shown in Figure 10.The assumption of time-separability is restrictive. In our example, there might becertain goals involving temporal deadlines (have the workplace tidy as soon as possibleafter 9:00 tomorrow morning) and maintenance (do not allow mail to sit in the mailroom10. Technically, the set of histories of interest also depends on the horizon chosen, as described below.11. The term \\reward\" is somewhat of a misnomer in that the reward could be negative, in which case\\penalty\" might be a better word. Likewise, \\costs\" can be either positive (punitive) or negative (ben-e cial). Thus, they admit great exibility in de ning value functions.12. See (Luenberger, 1973) for a more precise de nition of time-separability.16\nDecision-Theoretic Planning: Structural Assumptionsundelivered for more than 10 minutes) that require value functions that are non-separablegiven our current representation of the state. Note, however, that separability|like theMarkov property|is a property of a particular representation. We could add additionalinformation to the state in our example: the clock time, the interval of time between 9:00and the time at which tidiness is achieved, the length of time mail sits in the mail roombefore the robot picks it up, and so on. With this additional information we could re-establish a time-separable value function, but at the expense of an increase in the numberof states and a more ad hoc and cumbersome action representation.132.7 Horizons and Success CriteriaIn order to evaluate a particular course of action, we need to specify how long (in howmany stages) it will be executed. This is known as the problem's horizon. In nite-horizonproblems, the agent's performance is evaluated over a xed, nite number of stages T .Commonly, our aim is to maximize the total expected reward associated with a course ofaction; we therefore de ne the ( nite-horizon) value of any length T history h as (Bellman,1957): V (h) = T 1Xt=0fR(st) C(st; at)g+R(sT )An in nite-horizon problem, on the other hand, requires that the agent's performancebe evaluated over an in nite trajectory. In this case the total reward may be unbounded,meaning that any policy could be arbitrarily good or bad if it were executed for long enough.In this case it may be necessary to adopt a di erent means of evaluating a trajectory. Themost common is to introduce a discount factor, ensuring that rewards or costs accrued atlater stages are counted less than those accrued at earlier stages. The value function foran expected total discounted reward problem is de ned as follows (Bellman, 1957; Howard,1960): V (h) = 1Xt=0 t(R(st) C(st; at))where is a xed discount rate (0 < 1). This formulation is a particularly simpleand elegant way to ensure a bounded measure of value over an in nite horizon, though it isimportant to verify that discounting is in fact appropriate. Economic justi cations are oftenprovided for discounted models|a reward earned sooner is worth more than one earnedlater provided the reward can somehow be invested. Discounting can also be suitable formodeling a process that terminates with probability 1 at at any point in time (e.g., arobot that can break down), in which case discounted models correspond to expected totalreward over a nite but uncertain horizon. For these reasons, discounting is sometimes usedfor nite-horizon problems as well.Another technique for dealing with in nite-horizon problems is to evaluate a trajectorybased on the average reward accrued per stage, or gain. The gain of a history is de ned asg(h) = limn!1 1n nXt=0fR(st) C(st; at)g13. See (Bacchus, Boutilier, & Grove, 1996, 1997), however, for a systematic approach to handling certaintypes of history-dependent reward functions. 17\nBoutilier, Dean, & HanksRe nements of this criterion have also been proposed (Puterman, 1994).Sometimes the problem itself ensures that total reward over any in nite trajectory isbounded, and thus the expected total reward criterion is well-de ned. Consider the casecommon in AI planners in which the agent's task is to bring the system to a goal state. Apositive reward is received only when the goal is reached, all actions incur a non-negativecost, and when a goal is reached the system enters an absorbing state in which no furtherrewards or costs are accrued. As long as the goal can be reached with certainty, thissituation can be formulated as an in nite-horizon problem where total reward is boundedfor any desired trajectory (Bertsekas, 1987; Puterman, 1994). In general, such problemscannot be formulated as ( xed) nite-horizon problems unless an a priori bound on thenumber of steps needed to reach the goal can be established. These problems are sometimescalled inde nite-horizon problems: from a practical point of view, the agent will continue toexecute actions for some nite number of stages, but the exact number cannot be determinedahead of time.2.8 Solution CriteriaTo complete our de nition of the planning problem we need to specify what constitutesa solution to the problem. Here again we see a split between explicit MDP formulationsand work in the AI planning community. Classical MDP problems are generally stated asoptimization problems: given a value function, a horizon, and an evaluation metric (e.g.,expected total reward, expected total discounted reward, expected average reward per stage)the agent seeks a behavioral policy that maximizes the objective function.Work in AI often seeks satis cing solutions to such problems. In the planning literature,it is generally taken that any plan that satis es the goal is equally preferred to any otherplan that satis es the goal, and that any plan that satis es the goal is preferable to anyplan that does not.14 In a probabilistic framework, we might seek the plan that satis esthe goal with maximum probability (an optimization), but this can lead to situations inwhich the optimal plan has in nite length if the system state is not fully observable. Thesatis cing alternative (Kushmerick, Hanks, & Weld, 1995) is to seek any plan that satis esthe goal with a probability exceeding a given threshold.Example 2.4 We extend our running example to demonstrate an in nite-horizon, fullyobservable, discounted reward situation. We begin by adding one new dimension tothe state description, the boolean variable RHM (does the robot have mail), giving usa system with 20 states. We also provide the agent with two additional actions: PUM(pickup mail) and DelM (deliver mail) as described in Figure 2. We can now rewardthe agent in such a way that mail delivery is encouraged: we associate a reward of10 with each state in which RHM and M are both false and 0 with all other states.If actions have no cost, the agent gets a total reward of 20 for this six-stage systemtrajectory:hLoc(M);M;RHMi;Stay; hLoc(M);M;RHMi;PUM; hLoc(M);M;RHMi;Clk; hLoc(H);M;RHMi;Clk; hLoc(O);M;RHMi;DelM; hLoc(O);M;RHMi14. Though see (Haddawy & Hanks, 1998; Williamson & Hanks, 1994) for a restatement of planning as anoptimization problem. 18\nDecision-Theoretic Planning: Structural AssumptionsIf we assign an action cost of 1 for each action except Stay (which has 0 cost),the total reward becomes 16. If we use a discount rate of 0:9 to discount futurerewards and costs, this initial segment of an in nite-horizon history would contribute10 + :9( 1) + :81( 1) + :729( 1) + :6561( 1) + :59054( 1 + 10) = 12:2 to the totalvalue of the trajectory (as subsequently extended). Furthermore, we can establish abound on the total expected value of this trajectory. In the best case, all subsequentstages will yield a reward of 10, so the expected total discounted reward is boundedby 12:2 + :96(10) + :97(10) + : : : = 12:2 + 10 :96 1Xi=0 0:9i < 66A similar e ect on behavior can be achieved by penalizing states (i.e., having negativerewards) in which either M or RHM is true. 22.9 PoliciesWe have mentioned policies (or courses of action, or plans) informally to this point, andnow provide a precise de nition. The decision problem facing an agent can be viewed mostgenerally as deciding which action to perform given the current observable history. Wede ne a policy to be a mapping from the set of observable histories HO to actions, thatis, : HO ! A. Intuitively, the agent executes actionat = (hho0; a0i; : : : ; hot 1; at 1i; oti)at stage t if it has performed the actions a0; at 1 and made observations o0; ot 1 atearlier stages, and has just made observation ot at the current stage.A policy induces a distribution Pr(hj ) over the set of system histories HS ; this proba-bility distribution depends on the initial distribution P 0. We de ne the expected value of apolicy to be: EV( ) = Xh2HS V(h) Pr(hj )We would like the agent to adopt a policy that either maximizes this expected value or, ina satis cing context, has an acceptably high expected value.The general form of a policy, depending as it does on an arbitrary observation history,can lead to very complicated policies and policy-construction algorithms. In special cases,however, assumptions about observability and the structure of the value function can resultin optimal policies that have a much simpler form.In the case of a fully observable MDP with a time-separable value function, the optimalaction at any stage can be computed using only information about the current state andthe stage: that is, we can restrict policies to have the simpler form : S T ! A withoutdanger of acting suboptimally. This is due to the fact that full observability allows the stateto be observed completely, and the Markov assumption renders prior history irrelevant.In the non-observable case, the observational history contains only vacuous observationsand the agent must choose its actions using only knowledge of its previous actions and thestage; however, since incorporates previous actions, it takes the form : T ! A. This19\nBoutilier, Dean, & Hanksform of policy corresponds to a linear, unconditional sequence of actions ha1; a2; : : : ; aT i, ora straight-line plan in AI nomenclature.152.10 Model Summary: Assumptions, Problems, and ComputationalComplexityThis concludes our exposition of the MDP model for planning under uncertainty. Its gen-erality allows us to capture a wide variety of the problem classes that are currently beingstudied in the literature. In this section we review the basic components of the model,describe problems commonly studied in the DTP literature with respect to this model, andsummarize known complexity results for each. In Section 3, we describe some of the spe-cialized computational techniques used to solve problems in each of these problem classes.2.10.1 Model Summary and AssumptionsThe MDP model consists of the following components: The state space S, a nite or countable set of states. We generally make the Markovassumption, which requires that each state convey all information necessary to predictthe e ects of all actions and events independent of any further information aboutsystem history. The set of actions A. Each action ak is represented by a transition matrix of sizejSj jSj representing the probability pkij that performing action ak in state si will movethe system into state sj. We assume throughout that the action model is stationary,meaning that transition probabilities do not vary with time. The transition matrixfor an action is generally assumed to account for any exogenous events that mightoccur at the stage at which the action is executed. The set of observation variablesO. This is the set of \\messages\" sent to the agent afteran action is performed, that provide execution-time information about the currentsystem state. With each action ak and pair of states si, sj, such that pkij > 0,we associate a distribution over possible observations: pkmij denotes the probabilityof obtaining observation om given that action ak was taken in si and resulted in atransition to state sj. The value function V . The value function maps a state history into a real numbersuch that V (h1) V (h2) just in case the agent considers history h1 at least as goodas h2. A state history records the progression of states the system assumes alongwith the actions performed. Assumptions such as time-separability and additivity arecommon for V . In particular, we generally use a reward function R and cost functionC when de ning value. The horizon T . This is the number of stages over which the state histories should beevaluated using V .15. Many algorithms in the AI literature produce a partially ordered sequence of actions. These plans donot, however, involve conditional or nondeterministic execution. Rather, they represent the fact thatany linear sequence consistent with the partial order will solve the problem. Thus, a partially orderedplan is a concise representation for a particular set of straight-line plans.20\nDecision-Theoretic Planning: Structural Assumptions An optimality criterion. This provides a criterion for evaluating potential solutionsto planning problems.2.10.2 Common Planning ProblemsWe can use this general framework to classify various problems commonly studied in theplanning and decision-making literature. In each case below, we note the modeling assump-tions that de ne the problem class.Planning Problems in the OR/Decision Sciences Tradition Fully Observable Markov Decision Processes (FOMDPs) | There is an ex-tremely large body of research studying FOMDPs, and we present the basic algorith-mic techniques in some detail in the next section. The most commonly used formula-tion of FOMDPs assumes full observability and stationarity, and uses as its optimalitycriterion the maximization of expected total reward over a nite horizon, maximiza-tion of expected total discounted reward over an in nite horizon, or minimization ofthe expected cost to a goal state.FOMDPs were introduced by Bellman (1957) and have been studied in depth in the elds of decision analysis and OR, including the seminal work of Howard (1960). Re-cent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994). Average re-ward optimality has also received attention in this literature (Blackwell, 1962; Howard,1960; Puterman, 1994). In the AI literature, discounted or total reward models havebeen most popular as well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kael-bling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterionhas been proposed as more suitable for modeling AI planning problems (Boutilier &Puterman, 1995; Mahadevan, 1994; Schwartz, 1993). Partially Observable Markov Decision Processes (POMDPs) | POMDPsare closer than FOMDPs to the general model of decision processes we have described.POMDPs have generally been studied with the assumption of stationarity and opti-mality criteria identical to those of FOMDPs, though the average-reward criterionhas not been widely considered. As we discuss below, a POMDP can be viewed asa FOMDP with a state space consisting of the set of probability distributions overS. These probability distributions represent states of belief: the agent can \\observe\"its state of belief about the system although it does not have exact knowledge of thesystem state itself.POMDPs have been widely studied in OR and control theory (Astr om, 1965; Love-joy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasingattention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998;Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Bur-gard, 1998; Zhang & Liu, 1997). In uence diagrams (Howard & Matheson, 1984;Shachter, 1986) are a popular model for decision making in AI and are, in fact, astructured representational method for POMDPs (see Section 4.3).Planning Problems in the AI Tradition21\nBoutilier, Dean, & Hanks Classical Deterministic Planning | The classical AI planning model assumesdeterministic actions: any action ak taken at any state si has at most one successor sj.The other important assumptions are non-observability and that value is determinedby reaching a goal state: any plan that leads to a goal state is preferred to anythat does not. Often there is a preference for shorter plans; this can be representedby using a discount factor to \\encourage\" faster goal achievement or by assigning acost to actions. Reward is associated only with transitions to goal states, which areabsorbing. Action costs are typically ignored, except as noted above.In classical models it is usually assumed that the initial state is known with certainty.This contrasts with the general speci cation of MDPs above, which does not assumeknowledge of or even distributional information about the initial state. Policies arede ned to be applicable no matter what state (or distribution over states) one ndsoneself in|action choices are de ned for every possible state or history. Knowledge ofthe initial state and determinism allow optimal straight-line plans to be constructed,with no loss in value associated with non-observability, but unpredictable exogenousevents and uncertain action e ects cannot be modeled consistently if these assump-tions are adopted.For an overview of early classical planning research and the variety of approachesadopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classicalassumptions of complete information and determinism, but tries to recast the planningproblem as an optimization that relaxes the implicit assumption of \\achieve the goalat all costs.\" At the same time, these methods use the same sort of representationsand algorithms applied to satis cing planning.Haddawy and Hanks (1998) present a multi-attribute utility model for planners thatkeeps the explicit information about the initial state and goals, but allows prefer-ences to be stated about the partial satisfaction of the goals as well as the cost of theresources consumed in satisfying them. The model also allows the expression of pref-erences over phenomena like temporal deadlines and maintenance intervals that aredi cult to capture using a time-separable additive value function. Williamson (1996)(see also Williamson & Hanks, 1994). implements this model by extending a clas-sical planning algorithm to solve the resulting optimization problem. Haddawy andSuwandi (1994) also implement this model in a complete decision-theoretic framework.Their model of planning, re nement planning, di ers somewhat from the generativemodel discussed in this paper. In their model the set of all possible plans is pre-storedin an abstraction hierarchy, and the problem solver's job is to nd in the hierarchythe optimal choice of concrete actions for a particular problem.Perez and Carbonell's (1994) work also incorporates cost information into the classicalplanning framework, but maintains the split between a classical satis cing plannerand additional cost information provided in the utility model. The cost information isused to learn search-control rules that allow the classical planner to generate low-costgoal-satisfying plans. 22\nDecision-Theoretic Planning: Structural Assumptions Conditional Deterministic Planning | The classical planning assumption ofomniscience can be relaxed somewhat by allowing the state of some aspects of theworld to be unknown. The agent is thus in a situation where it is certain that thesystem is one of a particular set of states, but does not know which one. Unknowntruth values can be included in the initial state speci cation, and taking actions cancause a proposition to become unknown as well.Actions can provide the agent with information while the plan is being executed: con-ditional planners introduce the idea of actions providing runtime information aboutthe prevailing state, distinguishing between an action that makes proposition P trueand an action that will tell the agent whether P is true when the action is executed.An action can have both causal and informational e ects, simultaneously changingthe world and reporting on the value of one or more propositions. This second sortof information is not useful at planning time except that it allows steps in the planto be executed conditionally, depending on the runtime information provided by priorinformation-producing steps. The value of such actions lies in the fact that di erentcourses of action may be appropriate under di erent conditions|these informationale ects allow runtime selection of actions based on the observations produced, muchlike the general POMDP model.Examples of conditional planners in the classical framework include early work byWarren (1976) and the more recent CNLP (Peot & Smith, 1992), Cassandra (Pryor& Collins, 1993), Plynth (Goldman & Boddy, 1994), and UWL (Etzioni, Hanks,Weld, Draper, Lesh, & Williamson, 1992) systems. Probabilistic Planning Without Feedback | A direct probabilistic extensionof the classical planning problem can be stated as follows (Kushmerick et al., 1995):take as input (a) a probability distribution over initial states, (b) stochastic actions(explicit or implicit transition matrices), (c) a set of goal states, and (d) a probabilitysuccess threshold . The objective is to produce a plan that reaches any goal statewith probability at least , given the initial state distribution. No provision is madefor execution-time observation, thus straight-line plans are the only form of policypossible. This is a restricted case of the in nite-horizon NOMDP problem, one inwhich actions incur no cost and goal states o er positive reward and are absorbing.It is also a special case in that the objective is to nd a satis cing policy rather thanan optimal one. Probabilistic Planning With Feedback | Draper et al. (1994a) have proposedan extension of the probabilistic planning problem in which actions provide feedback,using exactly the observation model described in Section 2.4. Again, the problem isposed as that of building a plan that leaves the system in a goal state with su cientprobability. But a plan is no longer a simple sequence of actions|it can contain con-ditionals and loops whose execution depends on the observations generated by sensingactions. This problem is a restricted case of the general POMDP problem: absorb-ing goal states and cost-free actions are used, and the objective is to nd any policy(conditional plan) that leaves the system in a goal state with su cient probability.23\nBoutilier, Dean, & HanksComparing the Frameworks: Task-oriented Versus Process-oriented ProblemsIt is useful at this point to pause and contrast the types of problems considered in the clas-sical planning literature with those typically studied within the MDP framework. Althoughproblems in the AI planning literature have emphasized a goal-pursuit or \\one-shot\" view ofproblem solving, in some cases viewing the problem as an in nite-horizon decision problemresults in a more satisfying formulation. Consider our running example involving the o cerobot. It is simply not possible to model the problem of responding to co ee requests, mailarrival and keeping the lab tidy as a strict goal-satisfaction problem while capturing thepossible nuances of intuitively optimal behavior.The primary di culty is that no explicit and persistent goal states exist. If we weresimply to require that the robot attain a state where the lab is tidy, no mail awaits, and noun lled co ee requests exist, no \\successful\" plan could anticipate possible system behaviorafter a goal state was reached. The possible occurrence of exogenous events after goalachievement requires that the robot bias its methods for achieving its goals in a way thatbest suits the expected course of subsequent events. For instance, if co ee requests arevery likely at any point in time and unmet requests are highly penalized, the robot shouldsituate itself in the co ee room in order to satisfy an anticipated future request quickly.Most realistic decision scenarios involve both task-oriented and process-oriented behavior,and problem formulations that take both into account will provide more satisfying modelsfor a wider range of situations.2.10.3 The Complexity of Policy ConstructionWe have now de ned the planning problem in several di erent ways, each having a di erentset of assumptions about the state space, system dynamics and actions (deterministic orstochastic), observability (full, partial, or none), value function (time-separable, goal only,goal rewards and action costs, partially satis able goals with temporal deadlines), planninghorizon ( nite, in nite, or inde nite), and optimality criterion (optimal or satis cing solu-tions). Each set of assumptions puts the corresponding problem in a particular complexityclass, which de nes worst-case time and space bounds on any representation and algorithmfor solving that problem. Here we summarize known complexity results for each of theproblem classes de ned above.Fully Observable Markov Decision Processes Fully observable MDPs (FOMDPs)with time-separable, additive value functions can be solved in time polynomial in the sizeof the state space, the number of actions, and the size of the inputs.16 The most com-mon algorithms for solving FOMDPs are value iteration and policy iteration, which aredescribed in the next section. Both nite-horizon and discounted in nite-horizon problemsrequire a polynomial amount of computation per iteration|O(jSj2jAj) and O(jSj2jAj+jSj3),respectively|and converge in a polynomial number of iterations (with factor 11 in thediscounted case). On the other hand, these problems have been shown to be P-complete(Papadimitriou & Tsitsiklis, 1987), which means that an e cient parallel solution algorithmis unlikely.17 The space required to store the policy for an n-stage nite-horizon problem16. More precisely, the maximum number of bits required to represent any of the transition probabilities orcosts.17. See (Littman, Dean, & Kaelbling, 1995) for a summary of these complexity results.24\nDecision-Theoretic Planning: Structural Assumptionsis O(jSjn). For most interesting classes of in nite-horizon problems, speci cally those in-volving discounted models with time-separable additive reward, the optimal policy can beshown to be stationary, and the policy can be stored in O(jSj) space.Bear in mind that these are worst-case bounds. In many cases, better time bounds andmore compact representations can be found. Sections 4 and 5 explore ways to representand solve these problems more e ciently.Partially Observable Markov Decision Processes POMDPs are notorious for theircomputational di culty. As mentioned above, a POMDP can be viewed as a FOMDPwith an in nite state space consisting of probability distributions over S, each distributionrepresenting the agent's state of belief at a point in time (Astr om, 1965; Smallwood &Sondik, 1973). The problem of nding an optimal policy for a POMDP with the objectiveof maximizing expected total reward or expected total discounted reward over a nitehorizon T has been shown to be exponentially hard both in jSj and in T (Papadimitriou& Tsitsiklis, 1987). The problem of nding a policy that maximizes or approximatelymaximizes the expected discounted total reward over an in nite horizon is shown to beundecidable (Madani, Condon, & Hanks, 1999).Even restricted cases of the POMDP problem are computationally di cult in the worstcase. Littman (1996) considers the special case of boolean rewards: determining whetherthere is an in nite-horizon policy with nonzero total reward given that the rewards associ-ated with all states are non-negative. He shows that the problem is EXPTIME-complete ifthe transitions are stochastic, and PSPACE-hard if the transitions are deterministic.Deterministic Planning Recall that the classical planning problem is de ned quitedi erently from the MDP problems above: the agent has no ability to observe the state buthas perfect predictive powers, knowing the initial state and the e ects of all actions withcertainty. In addition, rewards come only from reaching a goal state, and any plan thatachieves the goal su ces.Planning problems are typically de ned in terms of a set P of boolean features orpropositions: a complete assignment of truth values to features describes exactly one state,and a partial assignment of truth values describes a set of states. A set of propositions Pinduces a state space of size 2jPj. Thus, the space required to represent a planning problemusing a feature-based representation can be exponentially smaller than that required by a at representation for the same problem (see Section 4).The ability to represent planning problems compactly has a dramatic impact on worst-case complexity. Bylander (1994) shows that the deterministic planning problem withoutobservation is PSPACE-complete. Roughly speaking, this means that at worst planningtime will increase exponentially with P and A, and further, that the size of a solution plancan grow exponentially with the problem size. These results hold even when the actionspace A is severely restricted. For example, the planning problem is NP-complete evenin cases where each action is restricted to one precondition feature and one postconditionfeature. Conditional and optimal planning are PSPACE-complete as well. These resultsare for inputs that are generally more compact (generally exponentially so) than those interms of which the complexity of the FOMDP and POMDP problems are phrased.Probabilistic Planning In probabilistic goal-oriented planning, as for POMDPs, wetypically search for a solution in a space of probability distributions over states (or over25\nBoutilier, Dean, & Hanksformulas that describe states). Even the simplest problem in probabilistic planning|onethat admits no observability|is undecidable at worst (Madani et al., 1999). The intuitionis that even though the set of states is nite, the set of distributions over those states is not,and at worst the agent may have to search an in nite number of plans before being ableto determine whether or not a solution exists. An algorithm can be guaranteed to nd asolution plan eventually if one exists, but cannot be guaranteed to terminate in nite timeif there is no solution plan. Conditional probabilistic planning is a generalization of thenon-observable probabilistic planning problem, and thus is undecidable as well.It is interesting to note a connection between conditional probabilistic planning andPOMDPs. The actions and observations of the two problems have equivalent expressivepower, but the reward structure of the conditional probabilistic planning problem is quiterestrictive: goal states have positive rewards, all other states have no reward, and goal statesare absorbing. Since we cannot put an a priori bound on the length of a solution plan,conditional probabilistic planning must be viewed as an in nite-horizon problem where theobjective is to maximize total expected undiscounted reward. Note, however, that since goalstates are absorbing, we can guarantee that total expected reward will be non-negative andbounded, even over an in nite horizon. Technically this means that the conditional proba-bilistic planning problem is a restricted case of an in nite-horizon positive-bounded problem(Puterman, 1994, Section 7.2). We can therefore conclude that the problem of solving anarbitrary in nite-horizon undiscounted positive-bounded POMDP is also undecidable. Themore commonly studied problem is the in nite-horizon POMDP with a criterion of maxi-mizing expected discounted total reward; nding optimal or near-optimal solutions to thatproblem is also undecidable, as noted above.2.10.4 ConclusionWe end this section by noting again that these results are algorithm-independent and de-scribe worst-case behavior. In e ect, they indicate how badly any algorithm can be made toperform on an \\arbitrarily unfortunate\" problem instance. The more interesting questionis whether we can build representations, techniques, and algorithms that typically performwell on problem instances that typically arise in practice. This concern leads us to examinethe problem characteristics with an eye toward exploiting the restrictions placed on thestates and actions, on observability, and on the value function and optimality criterion. Webegin with algorithmic techniques that focus on the value function|particularly those thattake advantage of time-separability and goal orientation. Then in the following section weexplore complementary techniques for building compact problem representations.3. Solution Algorithms: Dynamic Programming and SearchIn this section we review standard algorithms for solving the problems described above interms of the \\unstructured\" or \\ at\" problem representations. As noted in the analysisabove, fully observable Markov decision processes (FOMDPs) are by far the most widelystudied models in this general class of stochastic sequential decision problems. We begin bydescribing techniques for solving FOMDPs, focusing on techniques that exploit structure inthe value function like time-separability and additivity.26\nDecision-Theoretic Planning: Structural Assumptions3.1 Dynamic Programming ApproachesSuppose we are given a fully-observable MDP with a time-separable, additive value function.In other words, we are given the state space S, action space A, a transition matrix Pr(s0js; a)for each action a, a reward function R, and a cost function C. We start with the problemof nding the policy that maximizes expected total reward for some xed, nite-horizon T .Suppose we are given a policy such that (s; t) is the action to be performed by the agentin state s with t stages remaining to act (for 0 t T ).18 Bellman (1957) shows that theexpected value of such a policy at any state can be computed using the set of t-stage-to-govalue functions V t . We de ne V 0 (s) to be R(s), then de ne:V t (s) = R(s) + C( (s; t)) + Xs02SfPr(s0j (s; t); s)V t 1(s0)g (1)This de nition of the value function for makes its dependence on the initial state clear.We say a policy is optimal if V T (s) V 0T (s) for all policies 0 and all s 2 S.The optimal T -stage-to-go value function, denoted V T , is simply the value function of anyoptimal T -horizon policy. Bellman's principle of optimality (Bellman, 1957) forms the basisof the stochastic dynamic programming algorithms used to solve MDPs, establishing thefollowing relationship between the optimal value function at tth stage and the optimal valuefunction at the previous stage:V t (s) = R(s) + maxa2A fC(a) +Xs02S Pr(s0ja; s)V t 1(s0)g (2)3.1.1 Value IterationEquation 2 forms the basis of the value iteration algorithm for nite-horizon problems.Value iteration begins with the value function V 0 = R, and uses Equation 2 to compute insequence the value functions for longer time intervals, up to the horizon T . Any action thatmaximizes the right-hand side of Equation 2 can be chosen as the policy element (s; t).The resulting policy is optimal for the T -stage, fully observable MDP, and indeed for anyshorter horizon t < T .It is important to note that a policy describes what should be done at every stage andfor every state of the system, even if the agent cannot reach certain states given the system'sinitial con guration and its available actions. We return to this point below.Example 3.1 Consider a simpli ed version of the robot example, in which we have fourstate variables M , CR, RHC, and RHM (movement to various locations is ignored),and four actions GetC, PUM, DelC, and DelM. Actions GetC and PUM make RHCand RHM, respectively, true with certainty. Action DelM, when RHM holds, makesbothM and RHM false with probability 1.0; DelC makes both CR and RHC false withprobability 0.3, leaving the state unchanged with probability 0.7. A reward of 3 isassociated with CR and a reward of 1 is associated withM . The reward for any stateis the sum of the rewards for each objective satis ed in that state. Figure 11 showsthe optimal 0-stage, 1-stage and 2-stage value functions for various states, along with18. Recall that for FOMDPs other aspects of history are not relevant.27\nBoutilier, Dean, & HanksState V 0 V 1 (1) V 2 (2)s0 = hM;RHM;CR;RHCi 0 0 any 1 PUMs1 = hM;RHM;CR;RHCi 0 1 DelM 2 DelMs2 = hM;RHM;CR;RHCi 0 0.9 DelC 2.43 DelCs3 = hM;RHM;CR;RHCi 0 1 DelM 2.9 DelMs4 = hM;CR;RHCi 1 2.9 DelC 5.43 DelCs5 = hM;CR;RHCi 1 2 any 3.9 GetCs6 = hM;RHM;CRi 3 7 DelM 11 DelMs7 = hM;RHM;CRi 3 6 any 10 PUMs8 = hM;CRi 4 8 any 12 anyFigure 11: Finite-horizon optimal value and policy.the optimal choice of action at each state-stage pairing (the values for any \\state\"with missing variables hold for all instantiations of those variables). Note that V 0 (s)is simply R(s) for each state s.To illustrate the application of Equation 2, rst consider the calculation of V 1 (s3).The robot has the choice of delivering co ee or delivering mail, and the expected valueof each option, with one stage remaining, is given by:EV1(s3;DelC) = 0:3V 0 (s6) + 0:7V 0 (s3) = 0:9EV1(s3;DelM) = 1:0V 0 (s4) = 1:0Thus (s3; 1) = DelM and V 1 (s3) is the value of this maximizing choice. Notice thatthe robot with one action to perform will aim for the \\lesser\" objective M due to therisk of failure inherent in delivering co ee. With two stages remaining at the samestate, the robot will again deliver mail, but with certainty will move to s4 with onestage to go, where it will attempt to deliver co ee ( (s4; 1) = DelC).To illustrate the e ects a xed nite horizon can have on policy choice, note that (s0; 2) = PUM. With two stages remaining and the choice of getting mail or co ee,the robot will get mail because subsequent delivery (in the last stage) is guaranteedto succeed, whereas subsequent co ee delivery may fail. However, if we compute (s0; 3), we see: EV3(s0;GetC) = 1:0V 2 (s2) = 2:43EV3(s0;PUM) = 1:0V 2 (s1) = 2:0With three stages to go, the robot will instead retrieve co ee at s0. Once it hasco ee, it has two chances at successful delivery. The expected value of this courseof action is greater than that of (guaranteed) mail delivery. Note that three stagesdoes not allow su cient time to try to achieve both objectives at s0. In fact, thelarger reward associated with co ee delivery ensures that with any greater number ofstages remaining, the robot should focus rst on co ee retrieval and delivery, and thenattempt mail retrieval and delivery once co ee delivery is successfully completed. 2Often we are faced with tasks that do not have a xed nite horizon. For example, wemay want our robot to perform the tasks of keeping the lab tidy, picking up mail whenever it28\nDecision-Theoretic Planning: Structural Assumptionsarrives, responding to co ee requests, and so on. There is no xed time horizon associatedwith these tasks|they are to be performed as the need arises. Such problems are bestmodeled as in nite-horizon problems.We consider here the problem of building a policy that maximizes the discounted sumof expected rewards over an in nite horizon.19 Howard (1960) showed that there alwaysexists an optimal stationary policy for such problems. Intuitively, this is the case becauseno matter what stage the process is in, there are still an in nite number of stages remaining;so the optimal action at any state is independent of the stage. We can therefore restrictour attention to policies that choose the same action for a state regardless of the stage ofthe process. Under this restriction, the policy will have the same size jSj regardless of thenumber of stages over which the policy is executed|the policy has the form : S ! A.In contrast, optimal policies for nite-horizon problems are generally nonstationary, asillustrated in Example 3.1.Howard also shows that the value of policy satis es the following recurrence:V (s) = R(s) + fC( (s)) + Xs02S Pr(s0j (s); s)V (s0)g (3)and that the optimal value function V satis es:V (s) = R(s) + maxa2A fC(a) + Xs02S Pr(s0ja; s)V (s0)g (4)The value of a xed policy can be evaluated using the method of successive approxi-mation, which is almost identical to the procedure described in Equation 1 above. We beginwith an arbitrary assignment of values to V 0 (s), then de ne:V t (s) = R(s) + C( (s; t)) + Xs02SfPr(s0j (s; t); s)V t 1(s0)g (5)The sequence of functions V t converges linearly to the true value function V .One can also alter the value-iteration algorithm slightly so it builds optimal policies forin nite-horizon discounted problems. The algorithm starts with a value function V0 thatassigns an arbitrary value to each s 2 S. Given value estimate Vt(s) for each state s, Vt+1(s)is calculated as: Vt+1(s) = R(s) + maxa2A fC(a) + Xs02S Pr(s0ja; s) Vt(s0)g (6)The sequence of functions Vt converges linearly to the optimal value function V (s). Aftersome nite number of iterations n, the choice of maximizing action for each s forms anoptimal policy and Vn approximates its value.2019. This is by far the most commonly studied problem in the literature, though it is argued in (Boutilier &Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) that such problems are often best modeled usingaverage reward per stage as the optimality criterion. For a discussion of average reward optimality andits many variants and re nements, see (Puterman, 1994).20. The number of iterations n is based on a stopping criterion that generally involves measuring the dif-ference between Vt and Vt+1. For a discussion of stopping criteria and convergence of the algorithm, see(Puterman, 1994). 29\nBoutilier, Dean, & Hanks3.1.2 Policy IterationHoward's (1960) policy-iteration algorithm is an alternative to value iteration for in nite-horizon problems. Rather than iteratively improving the estimated value function, it insteadmodi es the policies directly. It begins with an arbitrary policy 0, then iterates, computing i+1 from i.Each iteration of the algorithm comprises two steps, policy evaluation and policy im-provement:1. (Policy evaluation) For each s 2 S, compute the value function V i(s) based on thecurrent policy i.2. (Policy improvement) For each s 2 S, nd the action a that maximizesQi+1(a; s) = R(s) + C(a) + Xs02S Pr(s0ja; s) V i(s0) (7)If Qi+1(a ; s) > V i(s), let i+1 = a ; otherwise i+1(s) = i(s).21The algorithm iterates until i+1(s) = i(s) for all states s. Step 1 evaluates the currentpolicy by solving the N N linear system represented by Equation 3 (one equation foreach s 2 S), and can be computationally expensive. However, the algorithm converges toan optimal policy at least linearly and under certain conditions converges superlinearly orquadratically (Puterman, 1994). In practice, policy iteration tends to converge in manyfewer iterations than does value iteration. Policy iteration thus spends more computationaltime at each individual stage, with the result that fewer stages need be computed.22Modi ed policy iteration (Puterman & Shin, 1978) provides a middle ground betweenpolicy iteration and value iteration. The structure of the algorithm is exactly the same asthat of policy iteration, alternating evaluation and improvement phases. The key insight isthat one need not evaluate a policy exactly in order to improve it. Therefore, the evaluationphase involves some (usually small) number of iterations of successive approximation (i.e.,setting V = V t for some small t, using Equation 6). With some tuning of the valueof t used at each iteration, modi ed policy iteration can work extremely well in practice(Puterman, 1994). Both value iteration and policy iteration are special cases of modi edpolicy iteration, corresponding to setting t = 0 and t =1, respectively.A number of other variants of both value and policy iteration have been proposed. Forinstance, asynchronous versions of these algorithms do not require that the value functionbe constructed, or policy improved, at each state in lockstep. In the case of value iterationfor in nite-horizon problems, as long as each state is updated su ciently often, convergencecan be assured. Similar guarantees can be provided for asynchronous forms of policy it-eration. Such variants are important tools for understanding various online approaches tosolving MDPs (Bertsekas & Tsitsiklis, 1996). For a nice discussion of asynchronous dynamicprogramming, see (Bertsekas, 1987; Bertsekas & Tsitsiklis, 1996).21. The Q-function de ned by Equation 7, and so called because of its use in Q-learning (Watkins & Dayan,1992), gives the value of performing action a at state s assuming the value function V accurately re ectsfuture value.22. See (Littman et al., 1995) for a discussion of the complexity of the algorithm.30\nDecision-Theoretic Planning: Structural Assumptions3.1.3 Undiscounted Infinite-Horizon ProblemsThe di culty with nding optimal solutions to in nite-horizon problems is that total rewardcan grow without limit over time. Thus, the problem de nition must provide some way toensure that the value metric is bounded over arbitrarily long horizons. The use of expectedtotal discounted reward as the optimality criterion o ers a particularly elegant way toguarantee a bound, since the in nite sum of discounted rewards is nite. However, althoughdiscounting is appropriate for certain classes of problems (e.g., economic problems, or thosewhere the system may terminate at any point with a certain probability), for many realisticAI domains it is di cult to justify counting future rewards less than present rewards, andthe discounted-reward criterion is not appropriate.There are a variety of ways to bound total reward in undiscounted problems. In somecases the problem itself is structured so that reward is bounded. In planning problems, forexample, the goal reward can be collected at most once, and all actions incur a cost. Inthat case total reward is bounded from above and the problem can legitimately be posedin terms of maximizing total expected undiscounted reward in many cases (e.g., if the goalcan be reached with certainty).In cases where discounting is inappropriate and total reward is unbounded, di erentsuccess criteria can be employed. For example, the problem can instead be posed as onein which we wish to maximize expected average reward per stage, or gain. Computationaltechniques for constructing gain-optimal policies are similar to the dynamic-programmingalgorithms described above, but are generally more complicated, and the convergence ratetends to be quite sensitive to the communicating structure and periodicity of the MDP.Re nements to gain optimality have also been studied. For example, bias optimality canbe used to distinguish two gain-optimal polices by giving preference to the policy whose totalreward over some initial segment of policy execution is larger. Again, while the algorithmsare more complicated than those for discounted problems, they are variants of standardpolicy or value iteration. See (Puterman, 1994) for details.3.1.4 Dynamic Programming and POMDPsDynamic programming techniques can be applied in partially observable settings as well(Smallwood & Sondik, 1973). The main di culty in building policies for situations in whichthe state is not fully observable is that, since past observations can provide informationabout the system's current state, decisions must be based on information gleaned in thepast. As a result, the optimal policy can depend on all observations the agent has made sincethe beginning of execution. These history-dependent policies can grow in size exponentialin the length of the horizon. While history-dependence precludes dynamic programming,the observable history can be summarized adequately with a probability distribution over S(Astr om, 1965), and policies can be computed as a function of these distributions, or beliefstates.A key observation of Sondik (Smallwood & Sondik, 1973; Sondik, 1978) is that whenone views a POMDP with a time-separable value function by taking the state space to bethe set of probability distributions over S, one obtains a fully observable MDP that canbe solved by dynamic programming. The (computational) problem with this approach is31\nBoutilier, Dean, & Hanksthat the state space for this FOMDP is an N -dimensional continuous space,23 and specialtechniques must be used to solve it (Smallwood & Sondik, 1973; Sondik, 1978).We do not explore these techniques here, but note that they are currently practicalonly for very small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997;Littman, 1996; Lovejoy, 1991b). A number of approximation methods, developed both inOR (Lovejoy, 1991a; White III & Scherer, 1989) and AI (Brafman, 1997; Hauskrecht, 1997;Parr & Russell, 1995; Zhang & Liu, 1997), can be used to increase the range of solvableproblems, but even these techniques are presently of limited practical value.POMDPs play a key role in reinforcement learning as well, where the \\natural statespace\" consisting of agent observations provides incomplete information about the under-lying system state (see, e.g., McCallum, 1995).3.2 AI Planning and State-Based SearchWe noted in Section 2.7 that the classical AI planning problem can be formulated as anin nite-horizon MDP and can therefore be solved using an algorithm like value iteration.Recall that two assumptions in classical planning specialize the general MDP model, namelydeterminism of actions and the use of goal states instead of a more general reward function.A third assumption|that we want to construct an optimal course of action starting from aknown initial state|does not have a counterpart in the FOMDP model as presented above,since the policy dictates the optimal action from any state at any stage of the plan. As wewill see below, the interest in online algorithms within AI has led to revised formulationsof FOMDPs that do take initial and current states into account.Though we de ned the classical planning problem earlier as a non-observable process(NOMDP), it can be solved as if it were fully observable. We let G be the set of goal statesand sinit be the initial state. Applying value iteration to this type of problem is equivalentto determining the reachability of goal states from all system states. For instance, if wemake goal states absorbing, assign a reward of 1 to all transitions from any s 2 S Gto some g 2 G and 0 to all others, then the set of all states where V k (s) > 0 is exactlythe set of states that can lead to a goal state.24 In particular, if V k (sinit) > 0, then asuccessful plan can be constructed by extracting actions from the k-stage ( nite-horizon)policy produced by value iteration. The determinism assumption means that the agent canpredict the state perfectly at every stage of execution; the fact that it cannot observe thestate is unimportant.The assumptions commonly made in classical planning can be exploited computation-ally in value iteration. First, we can terminate the process at the rst iteration k whereV k (sinit) > 0, because we are interested only in plans that begin at sinit, not in actingoptimally from every possible start state. Second, we can terminate value iteration after jSjiterations: if V jSj(sinit) = 0 at that point, the algorithm will have searched every possiblestate and can guarantee that no solution plan exists. Therefore, we can view classical plan-ning as a nite-horizon decision problem with a horizon of jSj. This use of value iteration23. More accurately, it is an N -dimensional simplex, or (N 1)-dimensional space.24. Speci cally, V k (s) indicates the probability with which one reaches the goal region under the optimalpolicy from s 2 S G in stochastic settings. In the deterministic case being discussed, this value mustbe 1 or 0. 32\nDecision-Theoretic Planning: Structural Assumptionsis equivalent to using the Floyd-Warshall algorithm to nd a minimum-cost path througha weighted graph (Floyd, 1962).3.2.1 Planning and SearchWhile value iteration can, in theory, be used for classical planning, it does not take advantageof the fact that the goal and initial states are known. In particular, it computes the valueand policy assignment for all states at all stages. This can be very wasteful since optimalactions will be computed for states that cannot be reached from sinit or that cannot possiblylead to any state g 2 G. It is also problematic when jSj is large, since each iteration ofvalue iteration requires O(jSjjAj) computations. For this reason dynamic programmingapproaches have not been used extensively in AI planning.The restricted form of the value function, especially the fact that initial and goal statesare given, makes it more advantageous to view planning as a graph-search problem. Unlikegeneral FOMDPs, where it is generally not known a priori which states are most desirablewith respect to (long-term) value, the well-de ned set of target states in a classical planningproblem makes search-based algorithms appropriate. This is the approach taken by mostAI planning algorithms.One way to formulate the problem as a graph search is to make each node of the graphcorrespond to a state in S. The initial state and goal states can then be identi ed, andthe search can proceed either forward or backward through the graph, or in both directionssimultaneously.In forward search, the initial state is the root of the search tree. A node is then chosenfrom the tree's fringe (the set of all leaf nodes), and all feasible actions are applied. Eachaction application extends the plan by one step (or one stage) and generates a unique newsuccessor state, which is a new leaf node in the tree. This node can be pruned if the state itde nes is already in the tree. The search ends when a state is identi ed as a member of thegoal set (in which case a solution plan can be extracted from the tree), or when all brancheshave been pruned (in which case no solution plan exists). Forward search attempts to builda plan from beginning to end, adding actions to the end of the current sequence of actions.Forward search never considers states that cannot be reached from the sinit.Backward search can be viewed in several di erent ways. We could arbitrarily selectsome g 2 G as the root of the search tree, and expand the search tree at the fringe byselecting a state on the fringe and adding to the tree all states such that some action wouldcause the system to enter the chosen state. In general, an action can give rise to more thanone predecessor vertex, even if actions are deterministic. A state can again be pruned if itappears in the search tree already. The search terminates when the initial state is added tothe tree, and a solution plan can again be extracted from the tree. This search is similarto dynamic-programming-based algorithms for nding the shortest path through a graph.The di erence is that backward search considers only those states at a depth k in the searchtree that can reach the chosen goal state within k steps. Dynamic programming algorithms,in contrast, visit every state at every stage of the search.One di culty with the backward approach as described above is the commitment toa particular goal state. Of course, this assumption can be relaxed, as an algorithm could\\simultaneously\" search for paths to all goal states by adding at the rst level of the search33\nBoutilier, Dean, & Hankstree any vertex that can reach some g 2 G. We will see in Section 5 that goal regressioncan be viewed as doing this, at least implicitly.It is generally thought that regression (or backward) techniques are more e ective inpractice than progression (or forward) methods. The reasoning is that the branching factorin the forward graph, which is the number of actions that can feasibly be applied in a givenstate, is substantially larger than the branching factor in the reverse graph, which is thenumber of operators that could bring the system into a given state.25 This is especially truewhen goal sets are represented by a small set of propositional literals (Section 5). The twoapproaches are not mutually exclusive, however: one can mix forward and backward expan-sions of the underlying problem graph and terminate when a forward path and backwardpath meet.The important thing to observe about these algorithms is that they restrict their atten-tion to the relevant and reachable states. In forward search, only those states that can bereached from sinit are ever considered: this can provide bene t over dynamic programmingmethods if few states are reachable, since unreachable states cannot play a role in construct-ing a successful plan. In backward approaches, similarly, only states lying on some pathto the goal region G are considered, and this can have signi cant advantages over dynamicprogramming if only a fraction of the state space is connected to the goal region.In contrast, dynamic programming methods (with the exception of asynchronous meth-ods) must examine the entire state space at every iteration. Of course, the ability to ignoreparts of the state space comes from planning's stringent de nition of what is relevant: statesin G have positive reward, no other states matter except to the extent they move the agentcloser to the goal, and the choice of action at states unreachable from sinit is not of interest.While state-based search techniques use knowledge of a speci c initial state and a speci cgoal set to constrain the search process, forward search does not exploit knowledge of thegoal set, nor does backward search exploit knowledge of the initial state. The GraphPlanalgorithm (Blum & Furst, 1995) can be viewed as a planning method that integrates thepropagation of forward reachability constraints with backward goal-informed search. Wedescribe this approach in Section 5. Furthermore, work on partial order planning (POP)can be viewed as a slightly di erent approach to this form of search. It too is describedin Section 5, after we discuss feature-based or intensional representations for MDPs andplanning problems.3.2.2 Decision Trees and Real-time Dynamic ProgrammingState-based search techniques are not limited to deterministic, goal-oriented domains. Knowl-edge of the initial state can be exploited in more general MDPs as well, forming the basis ofdecision tree search algorithms. Assume we have been given a nite-horizon FOMDP withhorizon T and initial state sinit. A decision tree rooted at sinit is constructed in much thesame way as a search tree for a deterministic planning problem (French, 1986). Each actionapplicable at sinit forms level 1 of the tree. The states s0 that result with positive proba-bility when any of those actions occur are applied at sinit are placed at level 2, with an arc25. See Bacchus et al. (1995, 1998) for some recent work that makes the case for progression with goodsearch control, and Bonet et al. (1997) who argue that progression in deterministic planning is usefulwhen integrating planning and execution. 34"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "a 1 a 2 a 1 a 2 a 1 a 2 a 1 a 2\ns 1 s 2 s 3 s 4\na 2a 1\ns init\np1\nV = max(V , V )21 V V = p V + p V2 1\n3 3 4 4\nV V3 4\np2 p3 p4\nFigure 12: The initial stages of a decision tree for evaluating action choices at sinit. Thevalue of an action is the expected value of its successor states, while the valueof a state is the maximum of the values of its successor actions (as indicated bydashed arrows at selected nodes).labeled with probability Pr(s0ja; sinit) relating s0 with a. Level 3 has the actions applicableat the states at level 2, and so on, until the tree is grown to depth 2T , at which point eachbranch of the tree is a path consisting of a positive-probability length-T trajectory rootedat sinit (see Figure 12).The relevant part of the optimal T -stage value function and the optimal policy can easilybe computed using this tree. We say that value of any node in the tree labeled with anaction is the expected value of its successor states in the tree (using the probabilities labelingthe arcs), while the value of any node in the tree labeled with state s is the sum of R(s) andthe maximum value of its successor actions.26 The rollback procedure, whereby value at theleaves of the tree are rst computed and then values at successively higher levels of the treeare determined using the preceding values, is, in fact, a form of value iteration. The valueof any state s at level 2t is precisely V T t(s) and the maximizing actions form the optimal nite-horizon policy. This form of value iteration is directed: (T t)-stage-to-go valuesare computed only for states that are reachable from sinit within t steps. In nite-horizonproblems can be solved in an analogous fashion if one can determine a priori the depthrequired (i.e., the number of iterations of value iteration needed) to ensure convergence toan optimal policy.Unfortunately, the branching factor for stochastic problems is generally much greaterthan that for deterministic problems. More troublesome still is the fact that one mustconstruct the entire decision tree to be sure that the proper values are computed, and hencethe optimal policy constructed. This stands in contrast with classical planning search,where attention can be focused on a single branch: if a goal state is reached, the pathconstructed determines a satisfactory plan. While worst-case behavior for planning mayrequire searching the whole tree, decision-tree evaluation is especially problematic because26. States at level 2T are given value R(s). 35\nBoutilier, Dean, & Hanksthe entire tree must be generated in general to ensure optimal behavior. Furthermore,in nite-horizon problems pose the di culty of determining a su ciently deep tree.One way around this di culty is the use of real time search (Korf, 1990). In particular,real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as away of approximately solving large MDPs in an online fashion. One can interleave searchwith execution of an approximately optimal policy using a form of RTDP similar to decision-tree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It canthen build a partial search tree to some depth, perhaps uniformly or perhaps with somebranches expanded more deeply than others. Partial tree construction may be halted due totime pressure or due to an assessment by the agent that further expansion of the tree maynot be fruitful. When a decision to act must be made, the rollback procedure is applied tothis partial, possibly unevenly expanded decision tree.Reward values can be used to evaluate the leaves of the tree, but this may o er aninaccurate picture of the value of nodes higher in the tree. Heuristic information can beused to estimate the long-term value of states labeling leaves. As with value iteration, thedeeper the tree, the more accurate the estimated value at the root (generally speaking)for a xed heuristic. We will see in Section 5 that structured representations of MDPs canprovide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally,with admissible heuristics or upper and lower bounds on the true values of leaf nodes in thetree, methods such as A* or branch-and-bound search can be used.A key advantage of integrating search with execution is that the actual outcome of theaction taken can be used to prune from the tree the branches rooted at the unrealizedoutcomes. The subtree rooted at the realized state can then be expanded further to makethe next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed asa variant of these methods in which stationary policies (i.e., state-action mappings) can beextracted during the search process.RTDP is formulated by Barto et al. (1995) more generally as a form of online, asyn-chronous value iteration. Speci cally, the values \\rolled backed\" can be cached and usedas improved heuristic estimates of the value function at the states in question. This tech-nique is also investigated in (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig& Simmons, 1995), and is closely tied to Korf's (1990) LRTA* algorithm. These valueupdates also need not proceed strictly using a decision tree to determine the states; the keyrequirement of RTDP is simply that the actual state sinit be one of the states whose valueis updated at each decision-action iteration.A second way to avoid some of the computational di culties that arise in large searchspaces is to use sampling methods. These methods sample from the space of possible trajec-tories and use this sampled information to provide estimates of the values of speci c coursesof action. This approach is quite common in reinforcement learning (Sutton & Barto, 1998),where simulation models are often used to generate experience from which a value functioncan be learned. In the present context, Kearns, Mansour and Ng (Kearns, Mansour, &Ng, 1999) have investigated search methods for in nite-horizon MDPs where the successorstates of any speci c action are randomly sampled according to the transition distribution.Thus, rather than expand all successor states, only sampled states are searched. Thoughthis method is exponential in the \\e ective\" horizon (or mixing rate) of the MDP and isrequired to expand all actions, the number of states expanded can be less than that required36\nDecision-Theoretic Planning: Structural Assumptionsby full search, even if the underlying transition graph is not sparse. They are able to pro-vide polynomial bounds (ignoring action branching and horizon e ects) on the number oftrajectories that need to be sampled in order to generate approximately optimal behaviorwith high probability.3.3 SummaryWe have seen that dynamic programming methods and state-based search methods canboth be used for fully observable and non-observable MDPs, with forward search meth-ods interpretable as \\directed\" forms of value iteration. Dynamic programming algorithmsgenerally require explicit enumeration of the state space at each iteration, while searchtechniques enumerate only reachable states; but the branching factor may require that,at su cient depth in the search tree, search methods enumerate individual states multipletimes, whereas they are considered only once per stage in dynamic programming. Overcom-ing this di culty in search requires the use of cycle-checking and multiple-path-checkingmethods.We note that search techniques can be applied to partially observable problems as well.One way to do this is to search through the space of belief states (just as dynamic pro-gramming can be applied to the belief space MDP|see Section 2.10.2). Speci cally, beliefstates play the role of system states and the stochastic e ects of actions on belief states areinduced by speci c observation probabilities, since each observation has a distinct, but xede ect on any belief state. This type of approach has been pursued in (Bonet & Ge ner,1998; Koenig & Simmons, 1995).4. Factored RepresentationsTo this point our discussion of MDPs has used an explicit or extensional representation forthe set of states (and actions) in which states are enumerated directly. In many cases itis advantageous, from both the representational and computational point of view, to talkabout properties of states or sets of states: the set of possible initial states, the set ofstates in which action a can be executed, and so on. It is generally more convenient andcompact to describe sets of states based on certain properties or features than to enumeratethem explicitly. Representations in which descriptions of objects substitute for the objectsthemselves are called intensional and are the technique of choice in AI systems.An intensional representation for planning systems is often built by de ning a set offeatures that are su cient to describe the state of the dynamic system of interest. In theexample in Figure 2, the state was described by a set of six features: the robot's location, thelab's tidiness, whether or not mail is present, whether or not the robot has mail, whether ornot there is a pending co ee request, and whether or not the robot has co ee. The rst andsecond features can each take one of ve values, and the last four can each take one of twovalues (true or false). An assignment of values to the six features completely de nes a state;the state space thus comprises all possible combinations of feature values, with jSj = 400.Each feature, or factor, is typically assigned a unique symbolic name, as indicated in thesecond column of Figure 2. The fundamental tradeo between extensional and intensionalrepresentations becomes clear in this example. An extensional representation of the co eeexample views the space of possible states as a single variable that takes on 400 possible37\nBoutilier, Dean, & Hanksvalues, whereas the intensional or factored representation views a state as the cross productof six variables, each of which takes on substantially fewer values. Generally, the state spacegrows exponentially in the number of features required to describe a system.The fact that the state of a system can be described using a set of features allows oneto adopt factored representations of actions, rewards and other components of an MDP. Ina factored action representation, for instance, one generally describes the e ect of an actionon speci c state features rather than on entire states. This often provides considerable rep-resentational economy. For instance, in the Strips action representation (Fikes & Nilsson,1971), the state transitions induced by actions are represented implicitly by describing thee ects of actions on only those features that change value when the action is executed.Factored representations can be very compact when individual actions a ect relatively fewfeatures, or when their e ects exhibit certain regularities. Similar remarks apply to therepresentation of reward functions, observation models, and so on. The regularities thatmake factored representations suitable for many planning problems can often be exploitedby planning and decision-making algorithms.While factored representations have long been used in classical AI planning, similarrepresentations have also been adopted in the recent use of MDP models within AI. Inthis section (Section 4), we focus on the economy of representation a orded by exploitingthe structure inherent in many planning domains. In the following section (Section 5), wedescribe how this structure|when made explicit by the factored representations|can beexploited computationally in plan and policy construction.4.1 Factored State Spaces and Markov ChainsWe begin by examining structured states, or systems whose state can be described using a nite set of state variables whose values change over time.27 To simplify our illustration ofthe potential space savings, we assume that these state variables are boolean. If there areM such variables, then the size of the state space is jSj = N = 2M . For large M , specifyingor representing the dynamics explicitly using state-transition diagrams or N N matricesis impractical. Furthermore, representing a reward function as an N -vector, and specifyingthe observational probabilities, is similarly infeasible. In Section 4.2, we de ne a class ofproblems in which the dynamics can be represented in O(M) space in many cases. We beginby considering how to represent Markov chains compactly and then consider incorporatingactions, observations and rewards.We let a state variable X take on a nite number of values and let X stand for theset of possible values. We assume that X is nite, though much of what follows can beapplied to countable state and action spaces as well. We say the state space is at if it isspeci ed using one state variable (this variable is denoted S as in the general model, takingvalues from S). The state space is factored if there is more than one state variable. A stateis any possible assignment of values to these variables. Letting Xi represent the ith statevariable, the state space is the cross product of the value spaces for the individual statevariables; that is, S = Mi=1 Xi . Just as St denotes the state of the process at stage t, welet Xti be the random variable representing the value of the ith state variable at stage t.27. These variables are often called uents in the AI literature (McCarthy & Hayes, 1969). In classicalplanning, these are the atomic propositions used to describe the domain.38\nDecision-Theoretic Planning: Structural AssumptionsA Bayesian network (Pearl, 1988) is a representational framework for compactly repre-senting a probability distribution in factored form. Although these networks have most typi-cally been used to represent atemporal problem domains, we can apply the same techniquesto represent Markov chains, encoding the chain's transition probabilities in the networkstructure (Dean & Kanazawa, 1989).Formally, a Bayes net is a directed acyclic graph in which vertices correspond to randomvariables and an edge between two variables indicates a direct probabilistic dependencybetween them. A network so constructed also re ects implicit independencies among thevariables. The network must be quanti ed by specifying a probability for each variable(vertex) conditioned on all possible values of its immediate parents in the graph. In addition,the network must include a marginal distribution: an unconditional probability for eachvertex that has no parents. This quanti cation is captured by associating a conditionalprobability table (CPT) with each variable in the network. Together with the independenceassumptions de ned by the graph, this quanti cation de nes a unique joint distributionover the variables in the network. The probability of any event over this space can then becomputed using algorithms that exploit the independencies represented within the graphicalstructure. We refer to Pearl (1988) for details.Figures 3(a)-(c) (page 7) are special cases of Bayes nets called \\temporal\" Bayesiannetworks. In these networks, vertices in the graph represent the system's state at di erenttime points and arcs represent dependencies across time points. In these temporal networks,each vertex's parent is its temporal predecessor, the conditional distributions are transitionprobability distributions, and the marginal distributions are distributions over initial states.The networks in Figure 3 re ect an extensional representation scheme in which states areexplicitly enumerated, but techniques for building and performing inference in probabilis-tic temporal networks are designed especially for application to factored representations.Figure 13 illustrates a two-stage temporal Bayes net (2TBN) describing the state-transitionprobabilities associated with the Markov chain induced by the xed policy of executingthe action CClk (repeatedly moving counterclockwise). In a 2TBN, the set of variables ispartitioned into those corresponding to state variables at a given time (or stage) t and thosecorresponding to state variables at time t + 1. Directed arcs indicate probabilistic depen-dencies between those variables in the Markov chain. Diachronic arcs are those directedfrom time t variables to time t + 1 variables, while synchronic arcs are directed betweenvariables at time t + 1. Figure 13 contains only diachronic arcs; synchronic arcs will bediscussed later in this section.Given any state at time t, the network induces a unique distribution over states at t+1.The quanti cation of the network describes how the state of any particular variable changesas a function of certain state variables. The lack of a direct arc (or more generally a directedpath if there are synchronic arcs among the t+ 1 variables) from a variable Xt to anothervariable Yt+1 means that knowledge of Xt is irrelevant to the prediction of the (immediate,or one-stage) evolution of variable Y in the Markov process.Figure 13 shows how compact this representation can be in the best of circumstances, asmany of the potential links between one stage and the next can be omitted. The graphicalrepresentation makes explicit the fact that the policy (i.e., the action CClk) can a ect onlythe state variable Loc, and the exogenous events ArrM, ReqC, and Mess can a ect only39"}, {"heading": "Boutilier, Dean, & Hanks", "text": "P(RHC )t+1\nM\nRHM RHM\nRHC RHC\nCR CR\nT T\nLoc Loc\nM\nO L C M H\nO 0.1 0 0 0 0.9 L 0.9 0.1 0 0 0 C 0 0.9 0.1 0 0 M 0 0 0.9 0.1 0 H 0 0 0 0.9 0.1\nt f t 1.0 0.2 f 0 0.8\nt f t 1.0 0 f 0 1.0\ntRHC\nP(Loc )t+1\nP(CR )t+1 CRt\nLoc t\nTime t Time t+1Figure 13: A factored 2TBN for the Markov chain induced by moving counterclockwise(with selected CPTs shown).the variables M , CR, and Tidy, respectively.28 Furthermore, the dynamics of Loc (andthe other variables) can be described using only knowledge of the state of their parentvariables; for instance, the distribution over Loc at t+1 depends only on the value of Loc atthe previous stage (e.g., if Loct = O, then Loct+1 =M with probability 0:9 and Loct+1 = Owith probability 0:1). Similarly, CR can become true with probability 0:2 (due to a ReqCevent), but once true, cannot become false (under this simple policy); and RHC remainstrue (or false) with certainty if it was true (or false) at the previous stage. Finally, thee ects on the relevant variables are independent. For any instantiation of the variables attime t, the distribution over next states can be computed by multiplying the conditionalprobabilities of relevant t+ 1 variables.The ability to omit arcs from the graph based on the locality and independence of actione ects has a strong e ect on the number of parameters that must be supplied to completethe model. Although the full transition matrix for CClk would be of size 4002 = 160000,the transition model in Figure 13 requires only 66 parameters.29The example above shows how 2TBNs exploit independence to represent Markov chainscompactly, but the example is extreme in that there is e ectively no relationship between thevariables|the chain can be viewed as the product of six independently evolving processes.28. We show only some of the CPTs for brevity.29. In fact, we can exploit the fact that probabilities sum to one and leave one entry unspeci ed per row ofany CPT or explicit transition matrix. In this case, the 2TBN requires only 48 explicit parameters, whilethe transition matrix requires 400 300 = 159; 600 entries. We generally ignore this fact when comparingthe sizes of representations. 40"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "41\nBoutilier, Dean, & HanksIn general, these \\subprocesses\" will interact, but still exhibit certain independencies andregularities that can be exploited by a 2TBN representation. We consider two distinctMarkov chains that exhibit di erent types of dependencies.Figure 14 illustrates a 2TBN representing the Markov chain induced by the followingpolicy: the robot consistently moves counterclockwise unless it is in the o ce and hasco ee, in which case it delivers co ee to the user. Notice that di erent variables are nowdependent: for instance, predicting the value of RHC at t+ 1 requires knowing the valuesof Loc and RHC at t. The CPT for RHC shows that the robot retains co ee at stage t+ 1with certainty, if it has it at stage t, in all locations except O (where it executes DelC,thus losing the co ee). The variable Loc also depends on the value of RHC. The locationwill change as in Figure 13 with one exception: if the robot is in the o ce with co ee, thelocation remains the same (since the robot does not move, but executes DelC). The e ecton the variable CR is explained as follows: if the robot is in the o ce and delivers co ee inits possession, it will ful ll any outstanding co ee request. However, the 0:05 chance of CRremaining true under these conditions indicates a 5% chance of spilling the co ee.Even though there are more dependencies (i.e., additional diachronic arcs) in this 2TBN,it still requires only 118 parameters. Again, the distribution over resulting states is deter-mined by multiplying the conditional distributions for the individual variables. Even thoughthe variables are \\related,\" when the state St is known, the variables at time t+1 (Loct+1,RHCt+1, etc.) are independent. In other words,Pr(Loct+1; T t+1;CRt+1;RHCt+1;RHMt+1;M t+1jSt) =Pr(Loct+1jSt) Pr(T t+1jSt) Pr(CRt+1jSt) Pr(RHCt+1jSt) Pr(RHMt+1jSt) Pr(M t+1jSt)(8)Figure 15 illustrates a 2TBN representing the Markov chain induced by the same policyas above, but where we assume that the act of moving counterclockwise has a slightlydi erent e ect. We now suppose that, when the robot moves from the hallway into someadjacent location, it has a 0:3 chance of spilling any co ee it has in its possession: thefragment of the CPT for RHC in Figure 15 illustrates this possibility. Furthermore, shouldthe robot be carrying mail whenever it loses co ee (whether \\accidentally\" or \\intentionally\"via the DelC action), there is a 0:5 chance it will lose the mail. Notice that the e ects of thispolicy on the variables RHC and RHM are correlated: one cannot accurately predict theprobability of RHMt+1 without determining the probability of RHCt+1. This correlation ismodeled by the synchronic arc between RHC and RHM at the t+ 1 slice of the network.The independence of the t+1 variables given St does not hold in 2TBNs with synchronicarcs. Determining the probability of a resulting state requires some simple probabilisticreasoning, for example, application of the chain rule. In this example, we can writePr(RHCt+1;RHMt+1jSt) = Pr(RHMt+1jRHCt+1; St) Pr(RHCt+1jSt)The joint distribution over t + 1 variables given St can then be computed as in Equa-tion 8 above, with this term replacing the Pr(RHCt+1jSt) Pr(RHMt+1jSt)|while these twovariables are correlated, the remaining variables are independent.We refer to 2TBNs with no synchronic arcs, like the one in Figure 14, as simple 2TBNs.General 2TBNs allow synchronic as well as diachronic arcs, as in Figure 15.42"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "M\nRHM RHM\nRHC RHC\nCR CR\nT T\nLoc Loc\nM\nt t t t f f f f\nt t f f t t f f\nt f t f t f t f t 1.0 0.0 0.5 0.0 1.0 0.0 1.0 0.0 f 0.0 1.0 0.5 1.0 0.0 1.0 0.0 1.0\nTime t Time t+1\nRHC RHC RHMt t+1 t\n1.0 0.0 0.7 0.0 1.0 0.0\netc.\nO O H H C C t f t f t f\n1.0 1.0 0.3 0.0 1.0\nt f\netc.\n1.0\nRHCLoc t t\nPr(RHC )\nPr(RHM )t+1\nt+1\nFigure 15: A 2TBN for the Markov chain induced by moving counterclockwise and deliv-ering co ee with correlated e ects.4.2 Factored Action RepresentationsJust as we extended Markov chains to account for di erent actions, we must extend the2TBN representation to account for the fact that the state transitions are in uenced bythe agent's choice of action. We discuss a variety of techniques for specifying the transitionmatrices that exploit the factored state representation to produce representations that aremore natural and compact than explicit transition matrices.4.2.1 Implicit-Event ModelsWe begin with the implicit-event model from Section 2.3 in which the e ects of actionsand exogenous events are combined in a single transition matrix. We will consider explicit-event models in Section 4.2.4. As we saw in the previous section, algorithms such as valueand policy iteration require the use of transition models that re ect the ultimate transitionprobabilities, including the e ects of any exogenous events.One way to model the dynamics of a fully observable MDP is to represent each actionby a separate 2TBN. The 2TBN shown above in Figure 13 can be seen as a representationof the action CClk (since the policy inducing the Markov chain in that example consistsof the repeated application of that action alone). The network fragment in Figure 16(a)illustrates the interesting aspects of the 2TBN for the DelC action including the e ects ofexogenous events. As above, the robot satis es an outstanding co ee request if it deliversco ee while it is in the o ce and has co ee (with a 0:05 chance of spillage), as shown in theconditional probability table for CR. The e ect on RHC can be explained as follows: the43"}, {"heading": "Boutilier, Dean, & Hanks", "text": "Loc Loc\nRHC RHC\nCR CR\n1.0\nCR\n0.2\nCR\n0.05 0.2\n1.0\nCR\n0.2\nRHC\nLoc Off\nt\nt t\nt\nelse\nf\nf f\nf\nRHC\nLoc Off\nt\nt\nf else\nCR\n0.05\nCR\n1.00.2\ntf f\n(a) (c)\n(b)\nTime t Time t+1\nO O O O L L L L t t f f t t f f t f t f t f t f\nt f\n.05 0.2 1.0 0.2 1.0 0.2 1.0 0.2 .95 0.8 0.0 0.8 0.0 0.8 0.0 0.8\netc. etc.\nLoc RHC CR\n0.0 0.0 0.3 0.0 0.3 0.0\netc.\nO O L L C C t f t f t f\n1.0 1.0 0.7 1.0 0.7 1.0\nt f\netc.\nRHCLoc t t\nttt\nPr(RHC )\nPr(CR )\nt+1\nt+1\nFigure 16: A factored 2TBN for action DelC (a) and structured CPT representations (b,c).robot loses the co ee (to the user or to spillage) if it delivers it in the o ce; if it attemptsdelivery elsewhere, there is a 0:7 chance that a random passerby will take the co ee fromthe robot.As in the case of Markov chains, the e ects of actions on di erent variables can becorrelated, in which case we must introduce synchronic arcs. Such correlations can bethought of as rami cations (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).4.2.2 Structured CPTsThe conditional probability table (CPT) for the node CR in Figure 16(a) has 20 rows, onefor each assignment to its parents. However, the CPT contains a number of regularities.Intuitively, this re ects the fact that the co ee request will be met successfully (i.e., thevariable becomes false) 95% of the time when DelC is executed, if the robot has co ee andis in the right location (the user's o ce). Otherwise, CR remains true if it was true andbecomes true with probability 0:2 if it was not. In other words, there are three distinct casesto be considered, corresponding to three \\rules\" governing the (stochastic) e ect of DelCon CR. This can be represented more compactly by using a decision tree representation(with \\else\" branches to summarize groups of cases involving multivalued variables suchas Loc) like that shown in Figure 16(b), or more compactly still using a decision graph(Figure 16(c)). In tree- and graph-based representations of CPTs, interior nodes are labeledby parent variables, edges by values of the variables, and leaves or terminals by distributionsover the child variable's values.30Decision-tree and decision-graph representations are used to represent actions in fullyobservable MDPs in (Boutilier et al., 1995; Hoey, St-Aubin, Hu, & Boutilier, 1999) and30. When the child is boolean, we label the leaves with only the probability of that variable being true (theprobability of the variable being false is one minus this value).44\nDecision-Theoretic Planning: Structural Assumptionsare described in detail in (Poole, 1995; Boutilier & Goldszmidt, 1996).31 Intuitively, treesand graphs embody the rule-like structure present in the family of conditional distributionsrepresented by the CPT, and in the settings we consider often yield considerable represen-tational compactness. Rule-based representations have been used directly by Poole (1995,1997a) in the context of decision processes and can often be more compact than trees (Poole,1997b). We generically refer to representations of this type as 2TBNs with structured CPTs.4.2.3 Probabilistic STRIPS OperatorsThe 2TBN representation can be viewed as oriented toward describing the e ects of actionson distinct variables. The CPT for each variable expresses how it (stochastically) changes(or persists), perhaps as a function of the state of certain other variables. However, ithas long been noted in AI research on planning and reasoning about action that mostactions change the state in limited ways; that is, they a ect a relatively small number ofvariables. One di culty with variable-oriented representations such as 2TBNs is that onemust explicitly assert that variables una ected by a speci c action persist in value (e.g.,see the CPT for RHC in Figure 13)|this is an instance of the infamous frame problem(McCarthy & Hayes, 1969).Another form of representation for actions might be called an outcome-oriented repre-sentation: one explicitly describes the possible outcomes of an action or the possible jointe ects over all variables. This was the idea underlying the Strips representation fromclassical planning (Fikes & Nilsson, 1971).A classical Strips operator is described by a precondition and a set of e ects. Theformer identi es the set of states in which the action can be executed, and the latterdescribes how the input state changes as a result of taking the action. A probabilisticStrips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995)extends the Strips representation in two ways. First, it allows actions to have di erente ects depending on context, and second, it recognizes that the e ects of actions are notalways known with certainty.32Formally, a PSO consists of a set of mutually exclusive and exhaustive logical formulae,called contexts, and a stochastic e ect associated with each context. Intuitively, a con-text discriminates situations under which an action can have di ering stochastic e ects.A stochastic e ect is itself a set of change sets|a simple list of variable values|with aprobability attached to each change set, with the requirement that these probabilities sumto one. The semantics of a stochastic e ect can be described as follows: when the stochastice ect of an action is applied at state s, the possible resulting states are determined by thechange sets, each occurring with the corresponding probability; the resulting state associ-ated with a change set is constructed by changing variable values at state s to match thosein the change set, while all unmentioned variables persist in value. Note that since only one31. The fact that certain direct dependencies among variables in a Bayes net are rendered irrelevant underspeci c variable assignments has been studied more generally in the guise of context-speci c independence(Boutilier, Friedman, Goldszmidt, & Koller, 1996); see (Geiger & Heckerman, 1991; Shimony, 1993) forrelated notions.32. The conditional nature of e ects is also a feature of a deterministic extension of Strips known as ADL(Pednault, 1989). 45"}, {"heading": "Boutilier, Dean, & Hanks", "text": "+Loc(L) nil 0.9 0.1 +Loc(C) nil 0.9 0.1 +Loc(M) nil 0.9 0.1 +Loc(H) nil 0.9 0.1\n+Loc(O) -RHC -RHM +Loc(O) -RHC +Loc(O) -RHC -RHM -RHC nil 0.135 0.135 0.63 0.015 0.015 0.07\nLoc\nO L C M\nH\nFigure 18: A PSO representation of a simpli ed CClk action.context can hold in any state s, the transition distribution for the action at any state s iseasily determined.Figure 17 gives a graphical depiction of the PSO for the DelC action (shown as a 2TBNin Figure 16). The three contexts :RHC, RHC^Loc(O) and RHC^:Loc(O) are representedusing a decision tree. At the leaf of each branch in the decision tree is the stochastic e ect(set of change sets and associated probabilities) determined by the corresponding context.For example, when RHC^Loc(O) holds, the action has four possible e ects: the robot losesthe co ee; it may or may not satisfy the co ee request (due to the 0:05 chance of spillage);and mail may or may not arrive. Notice that each outcome is spelled out completely. Thenumber of outcomes in the other two contexts is rather large due to possible exogenousevents (we discuss this further in Section 4.2.4).33A key di erence between PSOs and 2TBNs lies in their treatment of persistence. Allvariables that are una ected by an action must be given CPTs in the 2TBN model, whilesuch variables are not mentioned at all in the PSO model (e.g., compare the variable Loc inboth representations of DelC). In this way, PSOs can be said to \\solve\" the frame problem,since una ected variables need not be mentioned in an action's description.3433. To keep Figure 17 manageable, we ignore the e ect of the exogenous event Mess on variable T .34. For a discussion of the frame problem in 2TBNs, see (Boutilier & Goldszmidt, 1996).46\nDecision-Theoretic Planning: Structural Assumptions\n\u03b51 \u03b52\nRHM\nRHC\nLoc\nCR\nM T\nRHM\nRHC\nLoc\nCR\nM T\nRHM\nRHC\nCR\nM T"}, {"heading": "ArrM", "text": "Loc\nRHM\nRHC\nCR\nM T\nMess\nLoc\nt+ t+t t+1Figure 19: An simpli ed explicit-event model for DelC.PSOs can provide an e ective means for representing actions with correlated e ects.Recall the description of the CClk action captured in Figure 15, where the robot maydrop its co ee as it moves from the hallway, and may drop its mail only if it drops theco ee. In the 2TBN representation of CClk, one must have both RHCt and RHCt+1 asparents of RHMt+1: we must model the dependence of RHM on a change in value in thevariable RHC. Figure 18 shows the CClk action in PSO format (for simplicity, we ignorethe occurrence of exogenous events). The PSO representation can o er an economicalrepresentation of correlated e ects such as this since the possible outcomes of moving in thehallway are spelled out explicitly. Speci cally, the (possible) simultaneous change in valuesof the variables in question is made clear.4.2.4 Explicit-Event ModelsExplicit-event models can also be represented using 2TBNs in a somewhat di erent form.As in our discussion in Section 2.3, the form taken by explicit-event models depends cru-cially on one's assumptions about the interplay between the e ects of the action itself andexogenous events. However, under certain assumptions even explicit-event models can berather concise.To illustrate, Figure 19 shows the deliver-co ee action represented as a 2TBN withexogenous events explicitly represented. The rst \\slice\" of the network shows the e ects ofthe action DelC without the presence of exogenous events. The subsequent slices describethe e ects of the events ArrM andMess (we use only two events for illustration). Notice thepresence of the extra random variables representing the occurrence of the events in question.The CPTs for these nodes re ect the occurrence probabilities for the events under various47\nBoutilier, Dean, & Hanksconditions, while the directed arcs from the event variables to state variables indicate thee ects of these events. These probabilities do not depend on all state variables in general;thus, this 2TBN represents the occurrence vectors (see Section 2.3) in a compact form. Alsonotice that, in contrast to the event occurrence variables, we do not explicitly represent theaction occurrence as a variable in the network, since we are modeling the e ect on thesystem given that the action was taken.35This example re ects the assumptions described in Section 2.3, namely, that eventsoccur after the action takes place and that event e ects are commutative, and for thisreason the ordering of the events ArrM and Mess in the network is irrelevant. Under thismodel, the system actually passes through two intermediate though not necessarily distinctstates as it goes from stage t to stage t + 1; we use subscripts \"1 and \"2 to suggest thisprocess. Of course, as described earlier, not all actions and events can be combined in such adecomposable way; more complex combination functions can also be modeled using 2TBNs(for one example, see Boutilier & Puterman, 1995).4.2.5 Equivalence of RepresentationsAn obvious question one might ask concerns the extent to which certain representations areinherently more concise than others. Here we focus on the standard implicit-event models,describing some of the domain features that make the di erent representations more or lesssuitable.Both 2TBN and PSO representations are oriented toward representing the changes inthe values of the state variables induced by an action; a key distinction lies in the fact that2TBNs model the in uence on each variable separately, while the PSO model explicitlyrepresents complete outcomes. A simple 2TBN|a network with no synchronic arcs|canbe used to represent an action in cases where there are no correlations among the action'se ect on di erent state variables. In the worst case, when the e ect on each variabledi ers at each state, each time t + 1 variable must have all time t variables as parents.If there are no regularities that can be exploited in structured CPT representations, thensuch an action requires the speci cation of O(n2n) parameters (assuming boolean variables),compared with the 22n entries required by an explicit transition matrix. When the number ofparents of any variable is bounded by k, then we need specify no more than n2k conditionalprobabilities. This can be further reduced if the CPTs exhibit structure (e.g., can berepresented concisely in a decision tree). For instance, if the CPT can be captured by therepresentation of choice with no more than f(k) entries, where f is a polynomial function ofthe number of parents of a variable, then the representation size, O(n f(k)), is polynomialin the number of state variables. This is often the case, for instance, in actions where oneof its (stochastic) e ects on a variable requires that some number of (pre-) conditions hold;if any of them do not, a di erent e ect comes into play.A PSO representation may not be as concise as a 2TBN when an action has multipleindependent stochastic e ects. A PSO requires that each possible change list be enumer-ated with its corresponding probability of occurrence. The number of such changes growsexponentially with the number of variables a ected by the action. This fact is evident in35. Sections 4.2.7 and 4.3 discuss representations that model the choice of action explicitly as a variable inthe network. 48"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "-RHC -CR -RHC 0.95 0.05 -RHC nil 0.7 0.3 nil 1.0 t f Loc elseO RHC +M nil 0.2 0.8\nFigure 20: A \\factored\" PSO representation for the DelC action.Figure 17, where the impact of exogenous events a ects a number of variables stochasti-cally and independently. The problem can arise with respect to \\direct\" action e ects, aswell. Consider an action in which a set of 10 unpainted parts is spray painted; each part issuccessfully painted with probability 0:9, and these successes are uncorrelated. Ignoring thecomplexity of representing di erent conditions under which the action could take place, asimple 2TBN can represent such an action with 10 parameters (one success probability perpart). In contrast, a PSO representation might require one to list all 210 distinct changelists and their associated probabilities. Thus, a PSO representation can be exponentiallylarger (in the number of a ected variables) than a simple 2TBN representation.Fortunately, if certain variables are a ected deterministically, these do not cause thePSO representation to blow up. Furthermore, PSO representations can also be modi edto exploit the independence of an action's e ects on di erent state variables (Boutilier &Dearden, 1994; Dearden & Boutilier, 1997), thus escaping this combinatorial di culty. Forinstance, we might represent the DelC action shown in Figure 17 in the more \\factoredform\" illustrated in Figure 20 (for simplicity, we show only the e ect of the action andthe exogenous event ArrM). Much like a 2TBN, we can determine an overall e ect bycombining the change sets (in the appropriate contexts) and multiplying the correspondingprobabilities.Simple 2TBNs de ned over the original set of state variables are not su cient to rep-resent all actions.36 Correlated action e ects require the presence of synchronic arcs. Inthe worst case, this means that time t + 1 variables can have up to 2n 1 parents. Infact, the acyclicity condition assures that in the worst case, the total number of parentsis Pnk=1 2k 1; thus, we end up specifying O(22n) entries, the same as required by anexplicit transition matrix. However, if the number of parents (whether occurring withinthe time slice t or t + 1) can be bounded, or if regularities in the CPTs allow a compactrepresentation, then 2TBNs can still be pro tably used.PSO representations compare more favorably to 2TBNs in cases in which most of anaction's e ects on di erent variables are correlated. In this case, PSOs can provide asomewhat more economical representation of action e ects, primarily because one needn'tworry about frame conditions. The main advantage of PSOs is that one need not enlist theaid of probabilistic reasoning procedures to determine the transitions induced by actionswith correlated e ects. Contrast the explicit speci cation of outcomes in PSOs with thetype of reasoning required to determine the joint e ects of an action represented in 2TBN36. However, Section 4.2.6 discusses certain problem transformations that do render simple 2TBNs su cientfor any MDP. 49\nBoutilier, Dean, & Hanksform with synchronic arcs, as described in Section 4.1. Essentially, correlated e ects are\\compiled\" into explicit outcomes in PSOs.Recent results by Littman (1997) have shown that simple 2TBNs and PSOs can bothbe used to represent any action represented as a 2TBN without an exponential blowupin representation size. This is e ected by a clever problem transformation in which newsets of actions and propositional variables are introduced (using either a simple 2TBN orPSO representation). The structure of the original 2TBN is re ected in the new planningproblem, incurring no more than a polynomial increase in the size of the input actiondescriptions and the description of any policy. Though the resulting policy consists of actionsthat do not exist in the underlying domain, extracting the true policy is not di cult. Itshould be noted, however, that while such a representation can automatically be constructedfrom a general 2TBN speci cation, it is unlikely that it could be provided directly, sincethe actions and variables in the transformed problem have no \\physical\" meaning in theoriginal MDP.4.2.6 Transformations to Eliminate Synchronic ConstraintsThe discussion above has assumed that the variables or propositions used in the 2TBN orPSO action descriptions are the original state variables. However, certain problem trans-formations can be used to ensure that one can represent any action using simple 2TBNs, aslong as one does not require the original state variables to be used. One such transformationsimply clusters all variables on which some action has a correlated e ect. A new compoundvariable|which takes as values assignments to the clustered variables|can then be usedin the 2TBN, removing the need for synchronic arcs. Of course, this variable will have adomain size exponential in the number of clustered variables.Some of the intuitions underlying PSOs can be used to convert general 2TBN action de-scriptions to simple 2TBN descriptions with explicit \\events\" dictating the precise outcomeof the action. Intuitively, this event can occur in k di erent forms, each corresponding to adi erent change list induced by the action (or a change list with respect to the variables inquestion). As an example, we can convert the \\action\" description for CClk in Figure 15into the explicit-event model shown in Figure 21.37 Notice that the \\event\" takes on valuescorresponding to the possible e ects on the correlated variables RHC and RHM. Speci -cally, a denotes the event of the robot escaping the hallway successfully without losing itscargo, b denotes the event of the robot losing only its co ee, and c denotes the event of losingboth the co ee and the mail. In e ect, the event space represents all possible \\combined\"e ects, obviating the need for synchronic arcs in the network.4.2.7 Actions as Explicit Nodes in the NetworkOne di culty with the 2TBN and PSO approach to action description is that each actionis represented separately, o ering no opportunity to exploit patterns across actions. Forinstance, the fact that location persists in all actions except moving clockwise or counter-clockwise means that the \\frame axiom\" is duplicated in the 2TBN for all other actions(this is not the case for PSOs, of course). In addition, rami cations (or correlated action37. While Figure 15 describes a Markov chain induced by a policy, the representation of CClk can easily beextracted from it. 50"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "Loc Loc\nRHC RHC RHM RHM Eventa b c\n0.0 0.0 1.0\nRHC t f\nEvent 0.0 a\nb c\n1.0 0.0 0.0\nEvent\nRHM t f\n0.0Loc Off else\n1.0\nLoc Hall else\nRHC t\nt\nf\nf RHM\na: 1.0 b: 0.0 c: 0.0\na:0.7 c:0.15 a:0.7 b:0.3 c:0.0\na: 1.0 b: 0.0 c: 0.0\nb:0.15\nTime t Time t+1Figure 21: An explicit-event model that removes correlations.e ects) are duplicated across actions as well. For instance, if a co ee request occurs (withprobability 0:2) only when the robot ends up in the o ce, then this correlation is duplicatedacross all actions. A more compelling example might be one in which the robot can movea briefcase to a new location in one of a number of ways. We'd like to capture the fact (orrami cation) that the contents of the briefcase move to the same location as the briefcaseregardless of the action that moves the briefcase.To circumvent this di culty, we can introduce the choice of action as a \\random vari-able\" in the network, conditioning the distribution over state variable transitions on thevalue of this variable. Unlike state variables (or event variables in explicit event models),we do not generally require a distribution over this action variable|the intent is simplyto model schematically the conditional state-transition distributions given any particularchoice of action. This is because the choice of action will be dictated by the decision makeronce a policy is determined. For this reason, anticipating terminology used for in uencediagrams (see Section 4.3), we call these nodes decision nodes and depict them in our net-work diagrams with boxes. Such a variable can take as its value any action available to theagent.A 2TBN with an explicit decision node is shown in Figure 22. In this restricted example,we might imagine the decision node can take one of two values, Clk or CClk. The fact thatthe issuance of a co ee request at t+1 depends on whether the robot successfully moved from(or remained in) the o ce is now represented \\once\" by the arc between Loct+1 and CRt+1,rather than repeated across multiple action networks. Furthermore, the noisy persistenceof M under both actions is also represented only once (adding the action PUM, however,undercuts this advantage as we will see when we try to combine actions).One di culty with this straightforward use of decision nodes (which is the standardrepresentation in the in uence diagram literature) is that adding candidate actions cancause an explosion in the network's dependency structure. For example, consider the two51"}, {"heading": "Boutilier, Dean, & Hanks", "text": "CR CR\nLoc Loc\nAct\nMM\nTime t Time t+1Figure 22: An in uence diagram for a restricted process.\nAct\nY Z\nX\nY\n1.0\nY\n1.0\nY\n1.0 X\n0.9 0 0.9 0\nZ 0\nAct\na1 a2\nt\nt\nt\nt\ntf\nf\nf f\nf\nelse\nY Z\nX\nY Z\nX\nY Z\nX\nY Z\nX\nY Z\nX\n(a) action a1 (b) action a2 (d) CPT for Y(c) influence diagramFigure 23: Unwanted dependencies in in uence diagrams. 52\nDecision-Theoretic Planning: Structural Assumptionsaction networks shown in Figure 23(a) and (b). Action a1 makes Y true with probability0:9 if X is true (having no e ect otherwise), while a2 makes Y true if Z is true.Combining these actions in a single network in the obvious way produces the in uencediagram shown in Figure 23(c). Notice that Y now has four parent nodes, inheriting theunion of all its parents in the individual networks (plus the action node) and requiring aCPT with 16 entries for actions a1 and a2 together with eight additional entries for eachaction that does not a ect Y . The individual networks re ect the fact that Y dependson X only when a1 is performed and on Z only when a2 is performed. This fact is lostin the naively constructed in uence diagram. However, structured CPTs can be used torecapture this independence and compactness of representation: the tree of Figure 23(d)captures the distribution much more concisely, requiring only eight entries. This structuredrepresentation also allows us concisely to express that Y persists under all other actions. Inlarge domains, we expect variables to generally be una ected by a substantial number of(perhaps most) actions, thus requiring representations such as this for in uence diagrams.See (Boutilier & Goldszmidt, 1996) for a deeper discussion of this issue and its relationshipto the frame problem.While we provide no distributional information over the action choice, it is not hard tosee that a 2TBN with an explicit decision node can be used to represent the Markov chaininduced by a particular policy in a very natural way. Speci cally, by adding arcs from statevariables at time t to the decision node, the value of the decision node (i.e., the choice ofaction at that point) can be dictated by the prevailing state.384.3 In uence DiagramsIn uence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networksto include special decision nodes to represent action choices, and value nodes to representthe e ect of action choice on a value function. The presence of decision nodes means thataction choice is treated as a variable under the decision maker's control. Value nodes treatreward as a variable in uenced (usually deterministically) by certain state variables.In uence diagrams have not typically been associated with the schematic representationof stationary systems, instead being used as a tool for decision analysts where the sequentialdecision problem is carefully handcrafted. This more generic use of in uence diagrams hasbeen discussed by Tatman and Shachter (1990). In any event, there is no theory of planconstruction associated with in uence diagrams: the choice of all possible actions at eachstage must be explicitly encoded in the model. In uence diagrams are, therefore, usuallyused to model nite-horizon decision problems by explicitly describing the evolution of theprocess at each stage in terms of state variables.As in Section 4.2.7, decision nodes take as values speci c actions, though the set ofpossible actions can be tailored to the particular stage. In addition, an analyst will generallyinclude at each stage only state variables that are thought relevant to the decision at thator subsequent stages. Value nodes are also a key feature of in uence diagrams and arediscussed Section 4.5. Usually, a single value node is speci ed, with arcs indicating the38. More generally, a randomized policy can be represented by specifying a distribution over possible actionsconditioned on the state. 53"}, {"heading": "Boutilier, Dean, & Hanks", "text": "Rew\nT\n0 1 42 3\n-4 -3.5 -3 -2.5 -2-7 -6.5 -6 -5.5 -5\nT\n0 1 42 3\netc.\netc.\nRHM\nM\nCR\nT M RHM\nCRFigure 24: The representation of a reward function in an in uence diagram.in uence of particular state and decision variables (often over multiple stages) on the overallvalue function.In uence diagrams are typically used to model partially observable problems. An arcfrom a state variable to a decision node re ects the fact that the value of that state variableis available to the decision maker at the time the action is to be chosen. In other words,this variable's value forms part of the observation made at time t prior to the action beingselected at time t+1, and the policy constructed can refer to this variable. Once again, thisallows a compact speci cation of the observation probabilities associated with a system. Thefact that the probability of a given observation depends directly only on certain variablesand not on others can mean that far fewer model parameters are required.4.4 Factored Reward RepresentationWe have already noted that it is very common in formulating MDP problems to adopt asimpli ed value function: assigning rewards to states and costs to actions, and evaluat-ing histories by combining these factors according to some simple function like addition.This simpli cation alone allows a representation for the value function signi cantly moreparsimonious than one based on a more complex comparison of complete histories. Eventhis representation requires an explicit enumeration of the state and action space, however,motivating the need for more compact representations for these parameters. Factored rep-resentations for rewards and action costs can often obviate the need to enumerate state andaction parameters explicitly.Like an action's e ect on a particular variable, the reward associated with a state oftendepends only on the values of certain features of the state. For example, in our robotdomain, we can associate rewards or penalties with undelivered mail, with unful lled co eerequests and with untidiness in the lab. This reward or penalty is independent of othervariables, and individual rewards can be associated with the groups of states that di er onthe values of the relevant variables. The relationship between rewards and state variables isrepresented in value nodes in in uence diagrams, represented by the diamond in Figure 24.The conditional reward table (CRT) for such a node is a table that associates a reward withevery combination of values for its parents in the graph. This table, not shown in Figure 24,is locally exponential in the number of relevant variables. Although Figure 24 shows thecase of a stationary Markovian reward function, in uence diagrams can be used to represent54\nDecision-Theoretic Planning: Structural Assumptionsnonstationary or history-dependent rewards and are often used to represent value functionsfor nite-horizon problems.Although in the worst case the CRT will take exponential space to store, in manycases the reward function exhibits structure, allowing it to be represented compactly usingdecision trees or graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden,1994), or logical rules (Poole, 1995, 1997a). Figure 24 shows a fragment of one possibledecision-tree representation for the reward function used in the running example.The independence assumptions studied in multiattribute utility theory (Keeney & Rai a,1976) provide yet another way in which reward functions can be represented compactly. Ifwe assume that the component attributes of the reward function make independent contri-butions to a state's total reward, the individual contributions can be combined functionally.For instance, we might imagine penalizing states where CR holds with a (partial) rewardof 3, penalizing situations where there is undelivered mail (M _ RHM) with 2, andpenalizing untidiness T (i) with i 4 (i.e., in proportion to how untidy things are). Thereward for any state can then be determined simply by adding the individual penalties as-sociated with each feature. The individual component rewards along with the combinationfunction constitute a compact representation of the reward function. The tree fragment inFigure 24, which re ects the additive independent structure just described, is considerablymore complex than a representation that de nes the (independent) rewards for individualpropositions separately. The use of additive reward functions for MDPs is considered in(Boutilier, Brafman, & Geib, 1997; Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean,& Boutilier, 1998; Singh & Cohn, 1998).Another example of structured rewards is the goal structure studied in classical planning.Goals are generally speci ed by a single proposition (or a set of literals) to be achieved.As such, they can generally be represented very compactly. Haddawy and Hanks (1998)explore generalizations of goal-oriented models that permit extensions such as partial goalsatisfaction, yet still admit compact representations.4.5 Factored Policy and Value Function RepresentationThe techniques studied so far have been concerned with the input speci cation of the MDP:the states, actions, and reward function. The components of a problem's solution|the pol-icy and optimal value function|are also candidates for compact structured representation.In the simplest case, that of a stationary policy for a fully observable problem, a policymust associate an action with every state, nominally requiring a representation of sizeO(jSj). The problem is exacerbated for nonstationary policies and POMDPs. For example,the policy for a nite-horizon FOMDP with T stages generates a policy of size O(T jSj).For a nite-horizon POMDP, each possible observable history of length t < T might requirea di erent action choice; as many as PTk=1 bk such histories can be generated by a xedpolicy, where b is the maximum number of possible observations one can make following anaction.39The fact that policies require too much space motivates the need to nd compact func-tional representations, and standard techniques like the tree structures discussed above for39. Other methods of dealing with POMDPs, by conversion to FOMDPs over belief space (see Section 2.10.2),are more complex still. 55\nBoutilier, Dean, & Hanks etc.\nDelC Clk Clk\nO L C\nLoc\nM H\nClk\nM\nLoc\nH O L C\nRHC\nCR\nGetCCclkM\nPUM\nM\nPUM\nCclk\nCclk Cclk\nHRM\nDelM CclkFigure 25: A tree representation of a policy.actions and reward functions can be used to represent policies and value functions as well.Here we focus on stationary policies and value functions for FOMDPs, for which any logicalfunction representation may be used. For example, Schoppers (1987) uses a Strips-stylerepresentation for universal plans, which are deterministic, plan-like policies. Decision treeshave also been used for policies and value functions (Boutilier et al., 1995; Chapman &Kaelbling, 1991). An example policy for the robot domain speci ed with a decision tree isgiven in Figure 25. This policy dictates that, for instance, if CR and RHC are true: (a) therobot deliver the co ee to the user if it is in the o ce, and (b) it move toward the o ceif it is not in the o ce, unless (c) there is mail and it is in the mailroom, in which case itshould pickup the mail on its way.4.6 SummaryIn this section we discussed a number of compact factored representations for components ofan MDP. We began by discussing intensional state representations, then temporal Bayesiannetworks as a device for representing the system dynamics. Tree-structured conditionalprobability tables (CPTs) and probabilistic Strips operators (PSOs) were introduced as analternative to transition matrices. Similar tree structures and other logical representationswere introduced for representing reward functions, value functions, and policies.While these representations can often be used to describe a problem compactly, bythemselves they o er no guarantee that the problem can be solved e ectively. In the nextsection we explore algorithms that use these factored representations to avoid iteratingexplicitly over the entire set of states and actions.5. Abstraction, Aggregation, and Decomposition MethodsThe greatest challenge in using MDPs as the basis for DTP lies in discovering computation-ally feasible methods for the construction of optimal, approximately optimal or satis cingpolicies. Of course, arbitrary decision problems are intractable|even producing satis cingor approximately optimal policies is generally infeasible. However, the previous sectionssuggest that many realistic application domains may exhibit considerable structure, andfurthermore that the structure can be modeled explicitly and exploited so that typicalproblems can be solved e ectively. For instance, structure of this type can lead to compact56\nDecision-Theoretic Planning: Structural Assumptionsfactored representations of both input data and output policies, often polynomial-sized withrespect to the number of variables and actions describing the problem. This suggests thatfor these compact problem representations, policy construction techniques can be devel-oped that exploit this structure and are tractable for many commonly occurring probleminstances.Both the dynamic programming and state-based search techniques described in Sec-tion 3 exploit structure of a di erent kind. Value functions that can be decomposed intostate-dependent reward functions, or state-based goal functions, can be tackled by dynamicprogramming and regression search, respectively. These algorithms exploit the structurein decomposable value functions to prevent having to search explicitly through all possiblepolicies. However, while these algorithms are polynomial in the size of the state space,the curse of dimensionality makes even these algorithms infeasible for practical problems.Though compact problem representations aid in the speci cation of large problems, it isclear that a large system can be speci ed compactly only if the representation exploits\\regularities\" found in the domain. Recent AI research on DTP has stressed using theregularities implicit in compact representations to speed up the planning process. Thesetechniques focus on both optimal and approximately optimal policy construction.In the following subsection we focus on abstraction and aggregation techniques, espe-cially those that manipulate factored representations. Roughly, these techniques allow theexplicit or implicit grouping of states that are indistinguishable with respect to certain char-acteristics (e.g., value or optimal action choice). We refer to a set of states grouped in thismanner as an aggregate or abstract state, or sometimes as a cluster, and assume that theset of abstract states constitutes a partition of the state space; that is to say, every state isin exactly one abstract state and the union of all abstract states comprises the entire statespace.40 By grouping similar states, each abstract state can be treated as a single state, thusalleviating the need to perform computations for each state individually. These techniquescan be used for approximation if the elements of an abstract state are only approximatelyindistinguishable (e.g., if the values of those states lie within some small interval).We then look at the use of problem decomposition techniques in which an MDP isbroken into various pieces, each of which is solved independently; the solutions are thenpieced together or used to guide the search for a global solution. If subprocesses whosesolutions interact minimally are treated as independent, we might expect an approximatelyoptimal global solution. Furthermore, if the structure of the problem requires a solutionto a particular subproblem only, then the solutions to other subproblems can be ignoredaltogether.Related is the use of reachability analysis to restrict attention to \\relevant\" regions ofstate space. Indeed, reachability analysis and the communicating structure of an MDPcan be used to form certain types of decompositions. Speci cally, we distinguish serialdecompositions from parallel decompositions.The result of a serial decomposition can be viewed as a partitioning of the state spaceinto blocks, each representing a (more or less) independent subprocess to be solved. Inserial decomposition, the relationship between blocks is generally more complicated thanin the case of abstraction or aggregation. In a partition resulting from decomposition, the40. We might also group states into non-disjoint sets that cover the entire state space. We do not considersuch soft-state aggregation here, but see (Singh, Jaakkola, & Jordan, 1994).57\nBoutilier, Dean, & Hanksstates within a particular block may behave quite di erently with respect to (say) value ordynamics. The important consideration in choosing a decomposition is that it is possible torepresent each block compactly and to compute e ciently the consequences of moving fromone block to another and, further, that the subproblems corresponding to the subprocessescan themselves be solved e ciently.A parallel decomposition is somewhat more closely related to an abstract MDP. AnMDP is divided into \\parallel sub-MDPs\" such that each decision or action causes thestate to change within each sub-MDP. Thus, the MDP is the cross product or join of thesub-MDPs (in contrast to the union, as in serial decomposition). We brie y discuss severalmethods that are based on parallel MDP decomposition.5.1 Abstraction and AggregationOne way problem structure can be exploited in policy construction relies on the notionof aggregation|grouping states that are indistinguishable with respect to certain problemcharacteristics. For example, we might group together all states that have the same optimalaction, or that have the same value with respect to the k-stage-to-go value function. Theseaggregates can be constructed during the solution of the problem.In AI, emphasis has generally been placed on a particular form of aggregation, namelyabstraction methods, in which states are aggregated by ignoring certain problem features.The policy in Figure 25 illustrates this type of abstraction: those states in which CR,RHC and Loc(O) are true are grouped, and the same action is selected for each suchstate. Intuitively, when these three propositions hold, other problem features are ignoredand abstracted away (i.e., they are deemed irrelevant). A decision-tree representation of apolicy or a value function partitions the state space into a distinct cluster for each leaf ofthe tree. Other representations (e.g., Strips-like rules) abstract the state space similarly.It is precisely this type of abstraction that is used in the compact, factored represen-tations of actions and goals discussed in Section 4. In the 2TBN shown in Figure 16, thee ect of the action DelC on the variable CR is given by the CPT for CRt+1; however,this (stochastic) e ect is the same at any state for which the parent variables have thesame value. This representation abstracts away other variables, combining states that havedistinct values for the irrelevant (non-parent) variables. Intensional representations oftenmake it easy to decide which features to ignore at a certain stage of problem solving, andthus (implicitly) how to aggregate the state space.There are at least three dimensions along which abstractions of this type can be com-pared. The rst is uniformity: a uniform abstraction is one in which variables are deemedrelevant or irrelevant uniformly across the state space, while a nonuniform abstraction al-lows certain variables to be ignored under certain conditions and not under others. Thedistinction is illustrated schematically in Figure 26. The tabular representation of a CPTcan be viewed as a form of uniform abstraction|the e ect of an action on a variable isdistinguished for all clusters of states that di er on the value of a parent variable, and isnot distinguished for states that agree on parent variables but disagree on others|while adecision tree representation of a CPT embodies a nonuniform abstraction.A second dimension of comparison is accuracy. States are grouped together on thebasis of certain characteristics, and the abstraction is called exact if all states within a58"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "A B C A B C A B CA B C A B C A B C A B C A B C A B C A B C A B C Uniform Nonuniform\n5.3 2.9 5.3 9.0"}, {"heading": "Exact Approximate", "text": "Adaptive Fixed\nA BA\n5.3\n5.3\n5.3 5.3\n2.9 2.9 9.3 9.3\n=\n5.2 5.5\n2.7 9.3\nFigure 26: Di erent forms of state space abstraction.cluster agree on this characteristic. A non-exact abstraction is called approximate. Thisis illustrated schematically in Figure 26: the exact abstraction groups together states thatagree on the value assigned to them by a value function, while the approximate abstractionallows states to be grouped together that di er in value. The extent to which these statesdi er is often used as a measure of the quality of an approximate abstraction.A third dimension is adaptivity. Technically, this is a property not of an abstractionitself, but of how abstractions are used by a particular algorithm. An adaptive abstractiontechnique is one in which the abstraction can change during the course of computation, whilea xed abstraction scheme groups together states once and for all (again, see Figure 26).For example, one can imagine using an abstraction in the representation of a value functionV k, then revising this abstraction to represent V k+1 more accurately.Abstraction and aggregation techniques have been studied in the OR literature onMDPs. Bertsekas and Castanon (1989) develop an adaptive aggregation (as opposed toabstraction) technique. The proposed method operates on at state spaces, however, andtherefore does not exploit implicit structure in the state space itself. An adaptive, uniformabstraction method is proposed by Schweitzer et al. (1985) for solving stochastic queu-ing models. These methods, often referred to as aggregation-disaggregation procedures, aretypically used to accelerate the calculation of the value function for a xed policy. Value-function calculation requires computational e ort at least quadratic in the size of the statespace, which is impractical for very large state spaces. In aggregation-disaggregation pro-cedures, the states are rst aggregated into clusters. A system of equations is then solved,or a series of summations performed, requiring e ort no more than cubic in the number ofclusters. Next, a disaggregation step is performed for each cluster, requiring e ort at leastlinear in the size of the cluster. The net result is that the total work, while at least linearin the total number of states, is at worst cubic in the size of the largest cluster.In DTP it is generally assumed that computations even linear in the size of the fullstate space are infeasible. Therefore it is important to develop methods that perform59\nBoutilier, Dean, & Hankswork polynomial in the log of the size of the state space. Not all problems are amenableto such reductions without some (perhaps unacceptable) sacri ce in solution quality. Inthe following section, we review some recent techniques for DTP aimed at achieving suchreductions.5.1.1 Goal Regression and Classical PlanningIn Section 3.2 we introduced the general technique of regression (or backward) searchthrough state space to solve classical planning problems, those involving deterministic ac-tions and performance criteria speci ed in terms of reaching a goal-satisfying state. Onedi culty is that such a search requires that any branch of the search tree lead to a particulargoal state. This commitment to a goal state may have to be retracted (by backtrackingthe search process) if no sequence of actions can lead to that particular goal state from theinitial state. However, a goal is usually speci ed as a set of literals G representing a set ofstates, where reaching any state in G is equally suitable|it may, therefore, be wasteful torestrict the search to nding a plan that reaches a particular element of G.Goal regression is an abstraction technique that avoids the problem of choosing a partic-ular goal state to pursue. A regression planner works by searching for a sequence of actionsas follows: the current set of subgoals SG0 is initialized as G. At each iteration an action is selected that achieves one or more of the current subgoals of SGi without deletingthe others, and whose preconditions do not con ict with the \\unachieved subgoals.\" Thesubgoals so achieved are removed from the current subgoal set and replaced by a formularepresenting the context under which will achieve the current subgoals, forming SGi+1.This process is known as regressing SGi through . The process is repeated until one oftwo conditions holds: (a) the current subgoal set is satis ed by the initial state, in whichcase the current sequence of actions so selected is a successful plan; or (b) no action can beapplied, in which case the current sequence cannot be extended into a successful plan andsome earlier action choice must be reconsidered.Example 5.1 As an example, consider the simpli ed version of the robot planning exam-ple used in Section 3.1 to illustrate value iteration: the robot has only four actionsPUM, GetC, DelC and DelM, which we make deterministic in the obvious way. Theinitial state sinit is hCR;M;RHC;RHMi and the goal set G is fCR;Mg. Regress-ing G through DelM results in SG1 = fCR;M;RHMg. Regressing SG1 throughDelC results in SG2 = fRHC;M;RHMg. Regressing SG2 through PUM results inSG3 = fRHC;Mg. Regressing SG3 through GetC results in SG4 = fMg. Note thatsinit 2 SG4, so the sequence of actions GetC, PUM, DelC, DelM will successfully reacha goal state. 2To see how this algorithm implements a form of abstraction, rst note that the goalitself provides an initial partition of the state space, dividing it into one set of states inwhich the goal is satis ed (G) and a second set in which it is not (G). Viewed as a partitionof a zero-stage-to-go value function, G represents those states whose value is positive whileG represents those states whose value is zero.Every regression step can be thought of as revising this partition. When the planningalgorithm attempts to satisfy the current subgoal set SGi by applying action , it uses60"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "CR\nM\nCR\nM\nRHM"}, {"heading": "RHC", "text": "M\nRHM\nRHC\nM M"}, {"heading": "GetC PUM DelC DelM", "text": "GoalSSSS 4 3 2 1Figure 27: An example of goal regression.regression to compute the (largest) set of states such that, after executing , all subgoalsare satis ed. In particular, the state space is repartitioned into two abstract states: SGi+1and SGi+1. In this way, the abstraction mechanism implemented by goal regression shouldbe considered adaptive. This can be viewed as an (i + 1)-stage value function: any statesatisfying SGi+1 can reach a goal state in i+1 steps using the action sequence that producedSGi+1.41 The regression process can be stopped when the initial state is a member of theabstract state SGi+1. Figure 27 illustrates the repartitioning of the state space into thedi erent regions SGi+1 for each of the steps in the example above.While regression produces a compact representation of something like a value function(as in our discussion of deterministic, goal-based dynamic programming in Section 3.2), theanalogy is not exact in that the regions produced by regression record only the property ofgoal reachability contingent on a particular choice of action or action sequence.Standard dynamic programming methods can be implemented in a structured way bysimply noticing that a number of di erent regions can be produced at the ith iterationby considering all actions that can be regressed at that stage. The union of all of theseregressions form the states that have positive values in Vi, thus making the representation ofthe i-stage-to-go value function exact. Notice that each iteration is now more costly, sinceregression through all actions must be attempted, but this approach obviates the need forbacktracking and can ensure that a shortest plan is found. Standard regression does notprovide such guarantees without commitment to a particular search strategy (e.g., breadth- rst). This use of dynamic programming using Strips action descriptions forms the basicidea of Schoppers's universal planning method (Schoppers, 1987).Another general technique for solving classical planning problems is partial order plan-ning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algo-rithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).42The main motivation for the least-commitment approach comes from the realization thatregression techniques are incrementally building a plan from the end to the beginning (inthe temporal dimension). Thus, each iteration must commit to inserting a step last in theplan.In many cases it can be determined that a particular step must appear somewhere in theplan, but not necessarily as the last step in the plan; and, indeed, in many cases the step41. It is not the case, however, that states in SGi+1 cannot reach the goal region in i + 1 steps. It is onlythe case that they cannot do so using the speci c sequence of actions chosen so far.42. This type of planning is also sometimes called nonlinear or least-commitment planning. See Weld's(1994) survey for a nice overview. 61\nBoutilier, Dean, & Hanksunder consideration cannot appear last, but this fact cannot be recognized until later choicesreveal an inconsistency. In these cases, a regression algorithm will prematurely commit tothe incorrect ordering and will eventually have to backtrack over that choice. For example,suppose in the problem scenario above that the robot can hold only one item at a time,co ee or mail. Picking up mail causes the robot to spill any co ee in its possession, andsimilarly grasping the co ee makes it drop the mail. The plan generated by regression wouldno longer be valid: once the rst two actions (DelC and DelM) have been inserted into theplan, no action can be added to achieve RHC or RHM without making the other one false;the search for a plan would have to backtrack. Ultimately it would be discovered that nosuccessful plan can end with these two actions performed in sequence.Partial-order planning algorithms proceed much like regression algorithms, choosingactions to achieve unachieved subgoals and using regression to determine new subgoals,but leaving actions unordered to whatever extent possible. Strictly speaking, subgoal setsaren't regressed; rather, each unachieved goal or action precondition is addressed separately,and actions are ordered relative to one another only if one action threatens to negate thedesired e ect of another. In the example above, the algorithm might rst place actionsDelC and DelM into the plan, but leave them unordered. PUM can be added to the planto achieve the requirement RHM of DelM; it is ordered before DelM but is still unorderedwith respect to DelC. When GetC is nally added to the plan so as to achieve RHC foraction DelC, two threats arise. First, GetC threatens the desired e ect RHM of PUM. Thiscan be resolved by ordering GetC before PUM or after DelM. Assume the former orderingis chosen. Second, PUM threatens the desired e ect RHC of GetC. This threat can alsobe resolved by placing PUM before GetC or after DelC; since the rst threat was resolvedby ordering GetC before PUM, the latter ordering is the only consistent one. The resultis the plan GetC, DelC, PUM, DelM. No backtracking was required to generate the plan,because the actions were initially unordered, and orderings were introduced only when thediscovery of threats required them.In terms of abstraction, any incomplete, partially ordered plan that is threat-free,but perhaps has certain \\open conditions\" (unachieved preconditions or subgoals), can beviewed in much the same way as a partially completed regression plan: any state satisfyingthe open conditions can reach a goal state by executing any total ordering of the plan'sactions consistent with current set of ordering constraints. See (Kambhampati, 1997) for aframework that uni es various approaches to solving classical plan-generation problems.While techniques relying on regression have been studied extensively in the deterministicsetting, they have only recently been applied to probabilistic unobservable (Kushmericket al., 1995) and partially observable (Draper, Hanks, & Weld, 1994b) domains. For themost part, these techniques assume a goal-based performance criterion and attempt toconstruct plans whose probability of reaching a goal state exceeds some threshold. Theseaugment standard POP methods with techniques for evaluating a plan's probability ofachieving the goal, and techniques for improving this probability by adding further structureto the plan. In the next section, we consider how to use regression-related techniques tosolve MDPs with performance criteria more general than goals.62\nDecision-Theoretic Planning: Structural Assumptions5.1.2 Stochastic Dynamic Programming with Structured RepresentationsA key idea underlying propositional goal regression|that one need only regress the rele-vant propositions through an action|can be extended to stochastic dynamic programmingmethods, like value iteration and policy iteration, and used to solve general MDPs. Thereare, however, two key di culties to overcome: the lack of a speci c goal region and theuncertainty associated with action e ects.Instead of viewing the state space as partitioned into goal and non-goal clusters, weconsider grouping states according to their expected values. Ideally, we might want togroup states according to their value with respect to the optimal policy. Here we considera somewhat less di cult task, that of grouping states according to their value with respectto a xed policy. This is essentially the task performed by the policy evaluation step inpolicy iteration, and the same insights can be used to construct optimal policies.For a xed policy, we want to group states that have the same value under that policy.Generalizing the goal versus non-goal distinction, we begin with a partition that groupsstates according their immediate rewards. Then, using an analogue of regression developedfor the stochastic case, we reason backward to construct a new partition in which statesare grouped according to their value with respect to the one-stage-to-go value function. Weiterate in this manner so that on the kth iteration we produce a new partition that groupsstates according the k-stage-to-go value function.On each iteration, we perform work polynomial in the number of abstract states (andthe size of the MDP representation) and, if we are lucky, the total number of abstract stateswill be bounded by some logarithmic factor of the size of the state space. To implement thisscheme e ectively, we have to perform operations like regression without ever enumeratingthe set of all states, and this is where the structured representations for state-transition,value, and policy functions play a role.For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dear-den, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich &Flann, 1995; Hoey et al., 1999). We illustrate the basic intuitions behind this approachby describing how value iteration for discounted in nite-horizon FOMDPs might work. Weassume that the MDP is speci ed using a compact representation of the reward function(such as a decision tree) and actions (such as 2TBNs).In value iteration, we produce a sequence of value functions V0; V1; ; Vn, each Vkrepresenting the utility of the optimal k-stage policy. Our aim is to produce a compactrepresentation of each value function and, using Vn for some suitable n, produce a compactrepresentation of the optimal stationary policy. Given a compact representation of thereward function R, it is clear that this constitutes a compact representation of V0. Asusual, we think of each leaf of the tree as a cluster of states having identical utility. Toproduce V1 in compact form, we can proceed in two phases.Each branch of the tree for V0 provides an intensional description|namely, the con-junction of variable values labeling the branch|of an abstract state, or region, comprisingstates with identical value with respect to the initial value function V0. For any determin-istic action , we can perform a regression step using this description to determine theconditions under which, should we perform , we would end up in this cluster. This would,furthermore, determine a region of the state space containing states of identical future value63"}, {"heading": "Boutilier, Dean, & Hanks", "text": "X X Y Y Z Z X Y0.9 1.0 0.0\n0.9\n1.0 0.0\nX\nY\nZ\n0.01.0\nTime t+1Time tFigure 28: An example action.with respect to the execution of with one stage to go.43 Unfortunately, nondeterministicactions cannot be handled in quite this way: at any given state, the action might lead toseveral di erent regions of V0 with non-zero probability. However, for each leaf in the treerepresenting V0 (i.e., for each region of V0), we can regress the conjunction X describingthat region through action to produce the conditions under which X becomes true orfalse with a speci ed probability. In other words, instead of regressing in the standard fash-ion to determine the conditions under which X becomes true, we produce a set of distinctconditions under which X becomes true with di erent probabilities. By piecing togetherthe regions produced for the di erent labels in the description of V0, we can construct aset of regions such that each state in a given region: (a) transitions (under action ) to aparticular part of V0 with identical probability; and hence (b) has identical expected futurevalue (Boutilier et al., 1995). We can view this as a generalization of propositional goalregression suitable for decision-theoretic problems.Example 5.2 To illustrate, consider the example action a shown in Figure 28 and the valuefunction V 0 shown to the left of Figure 29. In order to generate the set of regionsconsisting of states whose future value (w.r.t. V 0) under a is identical, we proceed intwo steps (see Figure 29). We rst determine the conditions under which a has a xedprobability of making Y true (hence we have a xed probability of moving to the leftor right subtree of V 0). These conditions are given by the tree representing the CPTfor node Y , which makes up the rst portion of the tree representing V 1|see Step 1of Figure 29. Notice that this tree has leaves labeled with the probability of makingY true or (implicitly) false.If a makes Y true, then we know that its future value (i.e., value with zero stagesto go) is 8.1; but if Y becomes false, we need to know whether a makes Z true (to43. We ignore immediate reward and cost distinctions within the region so produced in our description;recall that the value of performing at any state s is given by R(s), C( ; s) and expected future value.We simply focus on abstract states whose elements have identical future expected value. Di erences inimmediate reward and cost can be added after the fact.64"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "Step 1\nX\nY 0.9 Y\nY 1.0 Y 0.0\nStep 2\nX\nY 0.9 Y 1.0\nY 0.9\nY\nY 0.0 Y 0.0Y 0.9\nY\nZ 0.9 Z\nZ 1.0 Z 0.0\nZ\nZ 1.0 Z 0.0\n8.1\n9.0\nY\nZ\n0.0\nV0Figure 29: An iteration of decision-theoretic regression. Step 1 produces the portion of thetree with dashed lines, while Step 2 produces the portion with dotted lines.determine whether the future value is 0 or 9:0). The probability with which Z becomestrue is given by the tree representing the CPT for node Z. In Step 2 in Figure 29,the conditions in that CPT are conjoined to the conditions required for predictingY 's probability (by \\grafting\" the tree for Z to the tree for Y given by the rst step).This grafting is slightly di erent at each of the three leaves of the tree for Y : (a) thefull tree for Z is attached to the leaf X = t; (b) the tree for Z is simpli ed where it isattached to to the leaf X = f ^ Y = f by removal of the redundant test on variableY ; (c) notice that there is no need to attach the tree for Z to the leaf X = f ^ Y = t,since a makes Y true with probability 1 under those conditions (and Z is relevant tothe determination of V 0 only when Y is false).At each of the leaves of the newly formed tree we have both Pr(Y ) and Pr(Z). Each ofthese joint distributions over Y and Z (the e ect of a and these variables is indepen-dent by the semantics of the network) tells us the probability of having Y and Z truewith zero stages to go given that the conditions labeling the appropriate branch of thetree hold with one stage to go. In other words, the new tree uniquely determines, forany state with one stage remaining, the probability of making any of the conditionslabeling the branches of V 0 true. The computation of expected future value obtainedby performing a with one stage to go can then be placed at the leaves of this tree bytaking expectation over the values at the leaves of V 0. 2The new set of regions produced this way describes the function Q 1 , where Q 1 (s) is thevalue associated with performing at state s with one stage to go and acting optimallythereafter. These functions (for each action ) can be pieced together (i.e., \\maxed\"|seeSection 3.1) to determine V1. Of course, the process can be repeated some number of timesto produce Vn for some suitable n, as well as the optimal policy with respect to Vn.This basic technique can be used in a number of di erent ways. Dietterich and Flann(1995) propose ideas similar to these, but restrict attention to MDPs with goal regions65\nBoutilier, Dean, & Hanksand deterministic actions (represented using Strips operators), thus rendering true goal-regression techniques directly applicable.44 Boutilier et al. (1995) develop a version ofmodi ed policy iteration to produce tree-structured policies and value functions, whileBoutilier and Dearden (1996) develop the version of value iteration described above. Thesealgorithms are extended to deal with correlations in action e ects (i.e., synchronic arcs in the2TBNs) in (Boutilier, 1997). These abstraction schemes can be categorized as nonuniform,exact and adaptive.The utility of such exact abstraction techniques has not been tested on real-world prob-lems to date. In (Boutilier et al., 1999), results on a series of abstract process-planningexamples are reported, and the scheme is shown to be very useful, especially for largerproblems. For example, in one speci c problem with 1.7 million states, the tree repre-sentation of the value function has only 40,000 leaves, indicating a tremendous amount ofregularity in the value function. Schemes like this exploit such regularity to solve problemsmore quickly (in this example, in much less than half the time required by modi ed pol-icy iteration) and with much lower memory demands. However, these schemes do involvesubstantial overhead in tree construction, and for smaller problems with little regularity,the overhead is not repaid in time savings (simple vector-matrix representations methodsare faster), though they still generally provide substantial memory savings. What might beviewed as best- and worst-case behavior is also described in (Boutilier et al., 1999). In aseries of \\linear\" examples (i.e., problems with value functions that can be represented withtrees whose size is linear in the number of problem variables), the tree-based scheme solvesproblems many orders of magnitude faster than classical state-based techniques. In con-trast, problems with exponentially-many distinct values are also tested (i.e., with a distinctvalue at each state): here tree-construction methods are required to construct a completedecision tree in addition to performing the same number of expected value and maximizationcomputations as classical methods. In this worst case, tree-construction overhead makesthe algorithm run about 100 times slower than standard modi ed policy iteration.In (Hoey et al., 1999), a similar algorithm is described that uses algebraic decisiondiagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) ratherthan trees. ADDs are a simple generalization of boolean decision diagrams (BDDs) (Bryant,1986) that allow terminal nodes to be labeled with real values instead of just boolean values.Essentially, ADD-based algorithms are similar to the tree-based algorithms except thatisomorphic subtrees can be shared. This lets ADDs provide more compact representationsof certain types of value functions. Highly optimized ADD manipulation and evaluationsoftware developed in the veri cation community can also be applied to solving MDPs.Initial results provided in (Hoey et al., 1999) are encouraging, showing considerable savingsover tree-based algorithms on the same problems. For example, the ADD algorithm appliedto the 1.7-million-state example described above revealed the value function to have only178 distinct values (cf. the 40,000 tree leaves required) and produced an ADD descriptionof the value function with less than 2200 internal nodes. It also solved the same problemin seven minutes, about 40 times faster than earlier reported timing results using decisiontrees (though some of this improvement was due to the use of optimized ADD softwarepackages). Similar results obtain with other problems (problems of up to 268 million states44. Dietterich and Flann (1995) also describe their work in the context of reinforcement learning rather thanas a method for solving MDPs directly. 66\nDecision-Theoretic Planning: Structural Assumptionswere solved in about four hours). Most encouraging is the fact that on the worst-case(exponential) examples, the overhead associated with using ADDs|compared to classical,vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\"modi ed policy iteration with 12 state variables), and lessens as problems become larger.Like tree-based algorithms, these methods have yet to be applied to real-world problems.With these exact abstraction schemes it is clear that, while in some examples the result-ing policies and value functions may be compact, in others the set of regions may get verylarge (even reaching the level of individual states Boutilier et al., 1995), thus precludingany computational savings. Boutilier and Dearden (1996) develop an approximation schemethat exploits the tree-structured nature of the value functions produced. At each stage k,the value function Vk can be pruned to produce a smaller, less accurate tree that approxi-mates Vk. Speci cally, approximate value functions are represented using trees whose leavesare labeled with an upper and lower bound on the value function in that region; decision-theoretic regression is performed on these bounds. Certain subtrees of the value tree canbe pruned when leaves of the subtree are very close in value or when the tree is too largegiven computational constraints. This scheme is nonuniform, approximate and adaptive.This approximation scheme can be tailored to provide (roughly) the most accurate valuefunction of a given maximum tree size, or the smallest value function (with respect to treesize) of some given minimum accuracy. Results reported in (Boutilier & Dearden, 1996)show that approximation on a small set of examples (including the worst-case examples fortree-based algorithms) allows substantial reduction in computational cost. For instance, ina 10-variable worst-case example, a small amount of pruning introduced an average error ofonly 0.5% but reduced computation time by a factor of 50. More aggressive pruning tendsto increase error and decrease computation time very rapidly; making appropriate tradeo sin these two dimensions is still to be addressed. This method too remains to be tested andevaluated on realistic problems.Structured representations and solution algorithms can be applied to problems otherthan FOMDPs. Methods for solving in uence diagrams (Shachter, 1986) exploit structurein a natural way; Tatman and Shachter (1990) explore the connection between in uence dia-grams and FOMDPs and the relationship between in uence diagram solution techniques anddynamic programming. Boutilier and Poole (1996) show how classic history-independentmethods for solving POMDPs, based on conversion to a FOMDP with belief states, can ex-ploit the types of structured representations described here. However, exploiting structuredrepresentations of POMDPs remains to be explored in depth.5.1.3 Abstract PlansOne of the di culties with the adaptive abstraction schemes suggested above is the factthat di erent abstractions must be constructed repeatedly, incurring substantial compu-tational overhead. If this overhead is compensated by the savings obtained during policyconstruction|e.g., by reducing the number of backups|then it is not problematic. But inmany cases the savings can be dominated by the time and space required to generate theabstractions, and thus motivates the development of cheaper but less accurate approximateclustering schemes. 67\nBoutilier, Dean, & HanksAnother way to reduce this overhead is to adopt a xed abstraction scheme so thatonly one abstraction is ever produced. This approach has been adopted in classical plan-ning in hierarchical or abstraction-based planners, pioneered by Sacerdoti's AbStrips sys-tem (Sacerdoti, 1974). A similar form of abstraction is studied by Knoblock (1993) (see alsoKnoblock, Tenenberg, & Yang, 1991). In this work, variables (in this case propositional) areranked according to criticality (roughly, how important such variables are to the solutionof the planning problem) and an abstraction is constructed by deleting from the problemdescription a set of propositions of low criticality. A solution to this abstract problem is aplan that achieves the elements of the original goal that have not been deleted. However,preconditions and e ects of actions that have been deleted are not accounted for in this so-lution, so it might not be a solution to the original problem. Even so, the abstract solutioncan be used to restrict search for a solution in the underlying concrete space. Very oftenhierarchies of more and more re ned abstractions are used and propositions are introducedback into the domain in stages.This form of abstraction is uniform (propositions are deleted uniformly) and xed. Sincethe abstract solution need not be a solution to the problem, we might be tempted to viewit as an approximate abstraction method. However, it is best not to think of the abstractplan as a solution at all, rather as a form of heuristic information that can help solve thetrue problem more quickly.The intuitions underlying Knoblock's scheme are applied to DTP by Boutilier and Dear-den (1994, 1997): variables are ranked according to their degree of in uence on the rewardfunction and a subset of the most important variables is deemed relevant. Once this subsetis determined, those variables that in uence the relevant variables through the e ects ofactions (which can be determined easily using Strips or 2TBN action descriptions) arealso deemed relevant, and so on. All remaining variables are deemed irrelevant and aredeleted from the description of the problem (both action and reward descriptions). Thisleaves an abstract MDP with a smaller state space (i.e., fewer variables) that can be solvedby standard methods. Recall that the state space reduction is exponential in the number ofvariables removed. We can view this method as an uniform xed approximate abstractionscheme. Unlike the output of classical abstraction methods, the abstract policy producedcan be implemented and has a value. The degree to which the optimal abstract policy andthe true optimal policy di er in value can be bounded a priori once the abstraction is xed.Example 5.3 As a simple illustration, suppose that the reward for satisfying co ee requests(or penalty for not satisfying them) is substantially greater than that for keeping thelab tidy or for delivering mail. Suppose that time pressure requires our agent to focuson a speci c subset of objectives in order to produce a small abstract state space. Inthis case, of the four reward-laden variables in our problem (see Figure 24), only CRwill be judged to be important. When the action descriptions are used to determinethe variables that can (directly or indirectly) a ect the probability of achieving CR,only CR, RHC and Loc will be deemed relevant, allowing T , M , and RHM to beignored. The state space is thus reduced from size 400 to size 20. In addition, severalof the action descriptions (e.g., Tidy) become trivial and can be deleted. 268\nDecision-Theoretic Planning: Structural AssumptionsThe advantage of these abstractions is that they are easily computed and incur littleoverhead. The disadvantages are that the uniform nature of such abstractions is restrictive,and the relevant \\reward variables\" are determined before the policy is constructed andwithout knowledge of the agent's ability to control these variables. As a result, importantvariables|those that have a large impact on reward|but over which the agent has nocontrol, may be taken into account, while less important variables that the agent can actuallyin uence are ignored. However, a series of such abstractions can be used that take intoaccount objectives of decreasing importance, and the a posteriori most valuable objectivescan be dealt with once risk and controllability are taken into account (Boutilier et al.,1997). The policies generated at more abstract levels can also be used to \\seed\" value orpolicy iteration at less abstract levels, in certain cases reducing the time to convergence(Dearden & Boutilier, 1997). It has also been suggested (Dearden & Boutilier, 1994, 1997)that the abstract value function be used as a heuristic in an online search for policies thatimprove the abstract policy so constructed, as discussed in Section 3.2.2. Thus, the errorin the approximate value function is overcome to some extent by search, and the heuristicfunction can be improved by asynchronous updates.A di erent use of abstraction is adopted in the DRIPS planner (Haddawy & Suwandi,1994; Haddawy & Doan, 1994). Actions can be abstracted by collapsing \\branches,\" or pos-sible outcomes, and maintaining probabilistic intervals over the abstract, disjunctive e ects.Actions are also combined in an decomposition hierarchy, much like those in hierarchicaltask networks. Planning is done by evaluating abstract plans in the decomposition net-work, producing ranges of utility for the possible instantiations of those plans, and re ningonly those plans that are possibly optimal. The use of task networks means that search isrestricted to nite-horizon, open-loop plans with action choice restricted to possible re ne-ments of the network. Such task networks o er a useful way to encode a priori heuristicknowledge about the structure of good plans.5.1.4 Model Minimization and Reduction MethodsThe abstraction techniques de ned above can be recast in terms of minimizing a stochasticautomaton, providing a unifying view of the di erent methods and o ering new insightsinto the abstraction process (Dean & Givan, 1997). From automata theory we know thatfor any given nite-state machineM recognizing a language L there exists a unique minimal nite-state machine M 0 that also recognizes L. It could be that M =M 0, but it might alsobe that M 0 is exponentially smaller than M . This minimal machine, called the minimalmodel for the language L, captures every relevant aspect of M and so the machines aresaid to be equivalent. We can de ne similar notions of equivalence for MDPs. Since we areprimarily concerned with planning, it is important that equivalent MDPs agree on the valuefunctions for all policies. From a practical standpoint, it may not be necessary to nd theminimal model if we can nd a reduced model that is su ciently small but still equivalent.We apply the idea of model minimization (or model reduction) to planning as follows:we begin by using an algorithm that takes as input an implicit MDP model in factored formand produces (if we are lucky) an explicit, reduced model whose size is within a polynomialfactor of the size of the factored representation. We then use our favorite state-baseddynamic programming algorithms to solve the explicit model.69\nBoutilier, Dean, & HanksWe can think of the dynamic programming techniques that rely on structured representa-tions discussed earlier as operating on a reduced model without ever explicitly constructingthat model. In some cases, building the reduced model once and for all may be appropriate;in other cases, one might save considerable e ort by explicitly constructing only those partsof the reduced model that are absolutely necessary.There are some potential computational problems with the model-minimization tech-niques sketched above. A small minimal model may exist, but it may be hard to nd.Instead, we might look for a reduced model that is easier to nd but not necessarily mini-mal. This too could fail, in which case we might look for a model small enough to be usefulbut only approximately equivalent to the original factored model. We have to be carefulwhat we mean by \\approximate,\" but intuitively two MDPs are approximately equivalentif the corresponding optimal value functions are within some small factor of one another.In order to be practical, MDP model reduction schemes operate directly on the implicitor factored representation of the original MDP. Lee and Yannakakis (1992) call this onlinemodel minimization. Online model minimization starts with an initial partition of the states.Minimization then iteratively re nes the partition by splitting clusters into smaller clusters.A cluster is split if and only if the states in the cluster behave di erently with respect totransitions to states in the same or other clusters. If this local property is satis ed by allclusters in a given partition, then the model consisting of aggregate states that correspondto the clusters of this partition is equivalent to the original model. In addition, if theinitial partition and the method of splitting clusters satisfy certain properties,45 then weare guaranteed to nd the minimal model. In the case of MDP reduction, the initial partitiongroups together states that have the same reward, or nearly the same reward in the case ofapproximation methods.The clusters of the partitions manipulated by online model reduction methods are rep-resented intensionally as formulas involving the state variables. For instance, the formulaRHC ^Loc(M) represents the set of all states such that the robot has co ee and is locatedin the mail room. The operations performed on these clusters require conjoining, comple-menting, simplifying, and checking for satis ability. In the worst case, these operations areintractable, and so the successful application of these methods depends critically on theproblem and the way in which it is represented. We illustrate the basic idea on a simpleexample.Example 5.4 Figure 30 depicts a simple version of our running example with a singleaction. There are three boolean state variables corresponding to RHC|the robot hasco ee (or not, RHC), CR|there is an outstanding request for co ee (or not, CR),and, considering only two location possibilities, Loc(C)|the robot is in the co eeroom (or not, Loc(C)). Whether there is an outstanding co ee request depends onwhether there was a request in the previous stage and whether the robot was in theco ee room. Location depends only on the location at the previous stage, and thereward depends only on whether or not there is an outstanding co ee request.45. The property required of the initial partition is that, if two states are in the same cluster of the partitionde ning the minimal model (recall that the minimal model is unique), then they must be in same clusterin the initial partition. 70"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "CRCR\nCR Loc C( )\u2227\nCR\n(a) (b)\nCR Loc C( )\u2227\nFigure 31: Models involving aggregate states: (a) the model corresponding to the initialpartition and (b) the minimal model.The initial partition shown in Figure 31(a) is de ned in terms of immediate rewards.We say that all the states in a particular starting cluster behave the same with respectto a particular destination cluster if the probability of ending up in the destinationcluster is the same for all states in the starting cluster. This property is not satis edfor starting cluster CR and destination cluster CR in Figure 31(a), and so we split thecluster labeled CR to obtain the model in Figure 31(b). Now the property is satis edfor all pairs of clusters and the model in Figure 31(b) is the minimal model. 2The Lee and Yannakakis algorithm for non-deterministic nite-state machines has beenextended by Givan and Dean to handle classical Strips planning problems (Givan & Dean,1997) and MDPs (Dean & Givan, 1997). The basic step of splitting a cluster is closelyrelated to goal regression, a relationship explored in (Givan & Dean, 1997). Variants ofthe model reduction approach apply when the action space is large and represented in afactored form (Dean, Givan, & Kim, 1998); for example, when each action is speci edby a set of parameters such as those corresponding to the allocations of several di erentresources in an optimization problem. There also exist algorithms for computing approxi-71"}, {"heading": "Boutilier, Dean, & Hanks", "text": "A\nR\nA\nG\nB\nG\nP\nB\nA B C\nD E\n(a) (b) (c)Figure 32: Reachability and serial problem decomposition.mate models (Dean, Givan, & Leach, 1997) and e cient planning algorithms that use theseapproximate models (Givan, Leach, & Dean, 1997).5.2 Reachability Analysis and Serial Problem Decomposition5.2.1 Reachability AnalysisThe existence of goal states can be exploited in di erent settings. For instance, in determin-istic classical planning problems, regression can be viewed as a form of directed dynamicprogramming. Without uncertainty, a certain policy either reaches a goal state or does not,and the dynamic programming backups need be performed only from goal states, not fromall possible states. Regression, therefore, implicitly exploits certain reachability character-istics of the domain along with the special structure of the value function.Reachability analysis applied much more broadly forms the basis for various types ofproblem decomposition. In decomposition problem solving, the MDP is broken into severalsubprocesses that can be solved independently, or roughly independently, and the solutionscan be pieced together. If subprocesses whose solutions interact marginally are treated asindependent, we might expect a good but nonoptimal global solution to result. Furthermore,if the structure of the problem requires that only a solution to a particular subproblem isneeded, then the solutions to other subproblems can be ignored or need not be computedat all. For instance, in regression analysis, the optimal action for states that cannot reacha goal region is irrelevant to the solution of a classical AI planning problem. This is shownschematically in Figure 32(a), where regions A and B are never explored in the backwardsearch through state space: only states that can reach the goal within the search horizonare ever deemed relevant. While regions A and B may be reachable from the start state, thefact that they do not reach the goal state means they are known to be irrelevant. Shouldthe system dynamics be stochastic, such a scheme can form the basis of an approximatelyoptimal solution method: regions A and B can be ignored if they are unlikely to transitionto the regression of the goal region (region R). Similar remarks using progression or forwardsearch from the start state apply, as illustrated in Figure 32(b).72\nDecision-Theoretic Planning: Structural AssumptionsSeveral schemes have been proposed in the AI literature for exploiting such reachabilityconstraints, apart from the usual forward- or backward-search approaches. Peot and Smith(1993) introduce the operator graph, a structure computed prior to problem solving thatcaches reachability relationships among propositions. The graph can be consulted duringthe planning process in deciding which actions to insert into the plan and how to resolvethreats.The GraphPlan algorithm of Blum and Furst (1995) attempts to blend considerationsof both forward and backward reachability in a deterministic planning context. One of thedi culties with regression is that we may regress the goal region through a sequence ofoperators only to nd ourselves in a region that cannot be reached from the initial state.In Figure 32(a), for example, not all states in region R may be reachable from the initialstate. GraphPlan constructs a variant of the operator graph called the planning graph, inwhich certain forward reachability constraints are posted. Regression is then implementedas usual, but if the current subgoal set violates the forward reachability constraints at anypoint, this subgoal set is abandoned and the regression search backtracks.Conceptually, one might think of GraphPlan as constructing a forward search treethrough state space with the initial state as the root, then doing a backward search fromthe goal region backward through this tree. Of course, the process is not state-based:instead, constraints on the possible variable values that can hold simultaneously at di erentplanning stages are recorded, and regression is used to search backward through the planninggraph. In a sense, GraphPlan can be viewed as constructing an abstraction in whichforward-reachable states are distinguished from unreachable states at each planning stage,and using this distinction among abstract states quickly to identify infeasible regressionpaths. Note, however, the GraphPlan approximates this distinction by overestimatingthe set of reachable states. Overestimation (as opposed to underestimation) ensures thatthe regression search space contains all legitimate plans.Reachability has also been exploited in the solution of more general MDPs. Deanet al. (1995) propose an envelope method for solving \\goal-based\" MDPs approximately.Assuming some path can be generated quickly from a given start state to the goal region,an MDP consisting of the states on this path and perhaps neighboring states is solved. Todeal with transitions that lead out of this envelope, a heuristic method estimates a value forthese states.46 As time permits, the set of neighboring states can be expanded, increasingsolution quality by more accurately evaluating the quality of alternative actions.Some of the ideas underlying GraphPlan have been applied to more general MDPs in(Boutilier, Brafman, & Geib, 1998), where the construction of a planning graph is general-ized to deal with the stochastic, conditional action representation o ered by 2TBNs. Givenan initial state (or set of initial states), this algorithm discovers reachability constraintsthat have a form like those in GraphPlan| for instance, that two variable values X = x1and Y = y3 cannot both obtain simultaneously; that is, no action sequence starting at thegiven initial state can lead to a state in which these values both hold.47 The reachabilityconstraints discovered by this process are then used to simplify the action and reward repre-sentation of an MDP so that it refers only to reachable states. In this case, any action that46. The approximate abstraction techniques described in Section 5.1.3 might be used to generate suchheuristic information.47. General k-ary constraints of this type are considered in (Boutilier et al., 1998).73\nBoutilier, Dean, & Hanksrequires an unreachable set of values to hold is e ectively deleted. In some cases, certainvariables are discovered to be immutable given the initial conditions and can themselves bedeleted, leading to much smaller MDPs. This simpli ed representation retains the originalpropositional structure so standard abstraction methods can be applied to the reachableMDP. It is also suggested that a strong synergy exists between abstraction and reachabil-ity analysis such that together these techniques reduce the size of the \\e ective\" MDP tobe solved much more dramatically than either does in isolation. Just as reachability con-straints can be used to prune regression paths in deterministic domains, they can be usedto prune value function and policy estimates generated by decision-theoretic regression andabstraction algorithms (Boutilier et al., 1998).The results reported in (Boutilier et al., 1998) are limited to a single process-planningdomain, but show that reachability analysis together with abstraction can provide substan-tial reductions in the size of the e ective MDP that must be solved, at least in some domains.In a domain with 31 binary variables, reachability considerations generally eliminated onthe order of 10 to 15 variables (depending on the initial state and the arity|binary orternary|of the constraints considered), reducing the state space from size 231 to anywherefrom 222 to 215. Incorporating abstraction on the reachable MDP provided considerablymore reduction, reducing the MDP to sizes ranging from 28 to e ectively zero states. Thelatter case would occur if it is discovered that no values of variables that impact rewardcan be altered|in which case every course of action has the same expected utility and theMDP needn't be solved (or can be solved by applying null actions with zero cost).5.2.2 Serial Problem Decomposition and Communicating StructureThe communicating or reachability structure of an MDP provides a way to formalize dif-ferent types of problem decomposition. We can classify an MDP according to the Markovchains induced by the stationary policies it admits. For a xed Markov chain, we can groupstates into maximal recurrent classes and transient states, as described in Section 2.1. AnMDP is recurrent if each policy induces a Markov chain with a single recurrent class. AnMDP is unichain if each policy induces a single recurrent class with (possibly) some tran-sient states. An MDP is communicating if for any pair of states s; t, there is some policyunder which s can reach t. An MDP is weakly communicating if there exists a closed setof states that is communicating plus (possibly) a set of states transient under every policy.We call other MDPs noncommunicating.These notions are crucial in the construction of optimal average-reward policies, but canalso be exploited in problem decomposition. Suppose an MDP is discovered to consist of aset of recurrent classes C1; Cn (i.e., no matter what policy is adopted, the agent cannotleave any such class once it enters that class) and a set of transient states.48 It is clear thatoptimal policy restricted to any class Ci can be constructed without reference to the policydecisions made at any states outside of Ci or even their values. Essentially, each Ci can beviewed as an independent subprocess.48. A simple way to view these classes is to think of the agent adopting a randomized policy where each actionis adopted at any state with positive probability. The classes of the induced Markov chain correspondto the classes of the MDP. 74\nDecision-Theoretic Planning: Structural AssumptionsThis observation leads to the following suggestion for optimal policy construction:49 wesolve the subprocesses consisting of the recurrent classes for the MDPs; we then removethese states from the MDP, forming a reduced MDP consisting only of the transient states.We then break the reduced MDP into its recurrent classes and solve these independently.The key to doing this e ectively is to use the value function for the original recurrentstates (computed in solving the independent subproblems in Step 1) to take into accounttransitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDPbroken into the classes that might be constructed this way. In the original MDP, classes Cand E are recurrent and can be solved independently. Once removed from the MDP, classD is recurrent in the reduced MDP. It can, of course, be solved without reference to classesA and B, but does rely on the value of the states that it transitions to in class E. However,the value function for E is available for this purpose, and can be used to solve for D asif D consisted only of jDj states. With this in hand, B can then be solved, and nally Acan be solved. Lin and Dean (1995) provide a version of this type of decomposition thatalso employs a factored representation. The factored representation allows dimensionalityreduction in di erent state subspaces by aggregating states that di er only in the values ofthe irrelevant variables in their subspaces.A key to such a decomposition is the discovery of the recurrent classes of an MDP.Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968)for discovering the structure of Markov chains that is O(N2) (recall N = jSj).50 To alleviatethe di culties of algorithms that work with an explicit state-based representation, Boutilierand Puterman (1995) propose a variant of the algorithm that works with a factored 2TBNrepresentation.One di culty with this form of decomposition is its reliance on strongly independentsubproblems (i.e., recurrent classes) within the MDP. Others have explored exact and ap-proximate techniques that work under less restrictive assumptions. One simple method ofapproximation is to construct \\approximately recurrent classes.\" In Figure 32(c) we mightimagine that C and E are nearly independent in the sense that all transitions between themare very low-probability or high-cost. Treating them as independent might lead to approx-imately optimal policies whose error can be bounded. If the solutions to C and E interactstrongly enough that the solutions should not be constructed completely independently, adi erent approach to solving the decomposed problem can be taken.If we have the optimal value function for E then, as pointed out, we can calculate theoptimal value function for D. The rst thing to note is that we don't need to know thevalue function for all of the states in E, just the value of every state in E that is reachablefrom some state in D in a single step. The set of all states outside D reachable in a singlestep from a state inside D is referred to as the states in the periphery of D. The values ofthe states in the intersection of E and the periphery of D summarize the value of exiting Dand ending up in E. We refer to the set of all states that are in the periphery of some blockas the kernel of the MDP. All of the di erent blocks interact with one another throughstates in the kernel.49. Ross and Varadarajan (1991) make a related suggestion for solving average-reward problems.50. A slight correction is made to the suggested algorithm in (Boutilier & Puterman, 1995).75"}, {"heading": "Boutilier, Dean, & Hanks", "text": "76\nDecision-Theoretic Planning: Structural AssumptionsExample 5.5 Spatial features often provide a natural dimension along which to decom-pose a domain. In our running example, the location of the robot might be used todecompose the state space into blocks of states, one block for each of the possible lo-cations. Figure 33 shows such a decomposition superimposed over the state-transitiondiagram for the MDP. States in the kernel are shaded and might correspond to theentrances and exits of locations. The star-shaped topology, induced by the kerneldecomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustratedin Figure 34. In Figure 33, the hallway location is not explicitly represented. Thissimpli cation may be reasonable if the hallway is only a conduit for moving fromone room to another; in this case the function of the hallway is accounted for in thedynamics governing states in the kernel. Figures 33 and 34 are idealized in that, giventhe full set of features in our running example, the kernel would contain many morestates. 2One technique for computing the optimal policy for the entire MDP involves repeatedlysolving the MDPs corresponding to the individual blocks. The techniques works as follows:initially, we guess the value of every state in the kernel.51 Given a current estimate for thevalues of the kernel states, we solve the component MDPs; this solution produces a newestimate for the states in the kernel. We adjust the values of the states in the kernel byconsidering the di erence between the current and the new estimates and iterate until thisdi erence is negligible.This iterative method for solving a decomposed MDP is a special case of the Lagrangianmethod for nding the extrema of a function. The OR literature is replete with suchmethods for both linear and nonlinear systems of equations (Winston, 1992). It is possibleto formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig andWolfe (1960) developed a method of decomposing a system of equations involving a verylarge number of variables into a set of smaller systems of equations interacting through a setof coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfedecomposition method, the original, very large system of equations is solved by iterativelysolving the smaller systems and adjusting the coupling variables on each iteration until nofurther adjustment is required. In the linear programming formulation of an MDP, thevalues of the states are encoded as variables.Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programsby using the Dantzig-Wolfe decomposition method to solve MDPs involving a large numberof states. Dean and Lin (1995) describe a general framework for solving decomposed MDPspointing to the work of Kushner and Chen as a special case, but neither work addressesthe issue of where the decompositions come from. Dean et al. (1995) investigate methodsfor decomposing the state space into two blocks: those reachable in k steps or fewer andthose not reachable in k steps (see the discussion of reachability above). The set of statesreachable in k or fewer steps is used to construct an MDP that is the basis for a policy thatapproximates the optimal policy. As k increases, the size of the block of states reachable ink steps increases, ensuring a better solution; but the amount of time required to compute a51. Ideally we would aggregate kernel states with the same value so as to provide a compact representation.In the remainder of this section, however, we won't consider this or any other opportunities for combiningaggregation and decomposition methods. 77\nBoutilier, Dean, & Hankssolution also increases. Dean et al. (1995) discuss methods for solving MDPs in time-criticalproblems by trading o quality against time.We have ignored the issue of how to obtain decompositions that expedite our calcu-lations. Ideally, each component of the decomposition would yield to simpli cation viaaggregation and abstraction, reducing the dimensionality in each component and therebyavoiding explicit enumeration of all the states. Lin (1997) presents methods for exploitingstructure for certain special cases in which the communicating structure is revealed by adomain expert. In general, however, nding a decomposition so as to minimize the e ortspent in solving the component MDPs is quite hard (at least as hard as nding the small-est circuit consistent with a given input-output behavior) and so the best we can hope forare good heuristic methods. Unfortunately, we are not aware of any particularly usefulheuristics for nding serial decompositions for Markov decision processes. Developing suchheuristics is clearly an area for investigation.Related to this form of decomposition is the development of macro operators for MDPs(Sutton, 1995). Macros have a long history in classical planning and problem solving (Fikes,Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs(Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998;Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz,1995). In most of this work, a macro is taken to be a local policy over a region of statespace (or block in the above terminology). Given an MDP comprising these blocks and aset of macros de ned for each block, the MDP can be solved by selecting a macro actionfor each block such that the global policy induced by the set of macros so picked is closeto optimal, or at the very least is the best combination of macros from the set available.In (Sutton, 1995; Precup et al., 1998), macros are treated as temporally-abstract actionsand models are de ned by which a macro can be treated as if it were a single action andused in policy or value iteration (along with concrete actions). In (Hauskrecht et al., 1998;Parr, 1998; Parr & Russell, 1998), these models are exploited in a hierarchical fashion, witha high-level MDP consisting only of states lying on the boundaries of blocks, and macrosthe only \\actions\" that can be chosen at these states. The issue of macro generation|constructing a set of macros guaranteed to provide the exibility to select close to optimalglobal behavior|is addressed in (Hauskrecht et al., 1998; Parr, 1998). The relationshipto serial decomposition techniques is quite close; thus, the problems of discovering gooddecompositions, constructing good sets of macros, and exploiting intensional representationsare areas in which clearer, compelling solutions are required. To date, work in this area hasnot provided much computational utility in the solution of MDPs|except in cases wheregood, hand-crafted, region-based decompositions and macros can be provided|and littleof this work has taken into account the factored nature of many MDPs. For this reason, wedo not discuss it in detail. However, the general notion of serial decomposition continues todevelop and shows great promise.5.3 Multiattribute Reward and Parallel DecompositionAnother form of decomposition is parallel decomposition, in which an MDP is broken intoa set of sub-MDPs that are \\run in parallel.\" Speci cally, at each stage of the (global)decision process, the state of each subprocess in a ected. For instance, in Figure 35, action78"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "MDP1 MDP2 MDP3\na a a\nFigure 35: Parallel problem decomposition.a a ects the state of each subprocess. Intuitively, an action is suitable for execution in theoriginal MDP at some state if it is reasonably good in each of the sub-MDPs.Generally, the sub-MDPs form either a product or join decomposition of the originalstate space (contrast this with the union decompositions of state space determined by serialdecompositions): the state space is formed by taking the cross product of the sub-MDP statespaces, or the join if certain states in the subprocesses cannot be linked. The subprocessesmay have identical action spaces (as in Figure 35), or each may have its own action space,with the global action choice being factored into a choice for each subprocess. In the lattercase, the sub-MDPs may be completely independent, in which case the (global) MDP can besolved exponentially faster. A more challenging problem arises when there are constraintson the legal action combinations. For example, if the actions in the subprocesses eachrequire certain shared resources, interactions in the global choice may arise.In a parallel MDP decomposition, we wish to solve the sub-MDPs and use the policiesor value functions generated to help construct an optimal or approximately optimal solutionto the original MDP, highlighting the need to nd appropriate decompositions for MDPsand to develop suitable merging techniques. Recent parallel decomposition methods haveall involved decomposing an MDP into subprocesses suitable for distinct objectives. Sincereward functions often deal with multiple objectives, each associated with an independentreward, and whose rewards can be summed to determine a global reward, this is often avery natural way to decompose MDPs. Thus, ideas from multiattribute utility theory canbe seen to play a role in the solution of MDPs.Boutilier et al. (1997) decompose an MDP speci ed using 2TBNs and an additive rewardfunction using the abstraction technique described in Section 5.1.3. For each componentof the reward function, abstraction is used to generate an MDP referring only to variablesrelevant to that component.52 Since certain state variables may be present in multiplesub-MDPs (i.e., relevant to more than one objective), the original state space in the join ofthe subspaces. Thus, decomposition is tackled automatically. Merging is tackled in severalways. One involves using the sum of the value functions obtained by solving the sub-MDPsas a heuristic estimate of the true value function. This heuristic is then used to guide online,state-based search (see Section 3.2.1). If the sub-MDPs do not interact, then this heuristicis perfect and leads to backtrack-free optimal action selection; if they interact, search is52. Note that the existence of a factored MDP representation is crucial for this abstraction method.79\nBoutilier, Dean, & Hanksrequired to detect con icts. Note that each sub-MDP has identical sets of actions. If theaction space is large, the branching factor of the search process may be prohibitive.Singh and Cohn (1998) also deal with parallel decomposition, though they assume theglobal MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositionsof a global MDP is not at issue. The global MDP is given by the cross product of the stateand action spaces of these sub-MDPs and the reward functions are summed. However,constraints on the feasible action combinations couple the solutions of these sub-MDPs. Tosolve the global MDP, the sum of the sub-MDP value functions is used as an upper boundon the optimal global value function, while the maximum of these (at any global state) isused as a lower bound. These bounds then form the basis of an action-elimination procedurein a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iterationis run over the explicit state space of the global MDP. Since the action space is also a crossproduct, this is a potential computational bottleneck for value iteration, as well.Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochas-tic resource allocation problems with very large state and action spaces. Much like Singhand Cohn (1998), an MDP is speci ed in terms of a number of independent MDPs, eachinvolving a distinct objective, whose action choices are linked through shared resource con-straints. The value functions for the individual MDPs are constructed o ine and then usedin set of online action-selection procedures. Unlike many of the approximation procedureswe have discussed, this approach makes no attempt to construct a policy explicitly (andis similar to real-time search or RTDP in this respect) nor to construct the value functionexplicitly. This method has been applied to very large MDPs, with state spaces of size21000 and actions spaces that are even larger, and can solve such problems in roughly halfan hour. The solutions produced are approximate, but the size of the problem precludesexact solution; so good estimates of solution quality are hard to derive. However, when thesame method is applied to smaller problems of the same nature whose exact solution canbe computed, the approximations have very high quality (Meuleau et al., 1998). While ableto solve very large MDPs (with large, but factored, state and action spaces), the modelrelies on somewhat restrictive assumptions about the nature of the local value functionsthat ensure good solution quality. However, the basic approach appears to be generalizable,and o ers great promise for solving very large factored MDPs.The algorithms in both (Singh & Cohn, 1998) and (Meuleau et al., 1998) can be seento rely at least implicitly on structured MDP representations involving almost independentsubprocesses. It seems likely that such approaches could take further advantage of automaticMDP decomposition algorithms such as that of (Boutilier et al., 1997), where factoredrepresentations explicitly play a part.5.4 SummaryWe have seen a number of ways in which intensional representations can be exploited tosolve MDPs e ectively without enumeration of the state space. These include techniquesfor abstraction of MDPs, including those based on relevance analysis, goal regression anddecision-theoretic regression; techniques relying on reachability analysis and serial decom-position; and methods for parallel MDP decomposition exploiting the multiattribute nature53. Singh and Cohn (1998) also incorporate methods for removing unreachable states during value iteration.80\nDecision-Theoretic Planning: Structural Assumptionsof reward functions. Many of these methods can, in fortunate circumstances, o er exponen-tial reduction is solution time and space required to represent a policy and value function;but none come with guarantees of such reductions except in certain special cases. Whilemost of the methods described provide approximate solutions (often with error bounds pro-vided), some of them o er optimality guarantees in general, and most can provide optimalsolutions under suitable assumptions.One avenue that has not been explored in detail is the relationship between the struc-tured solution methods developed for MDPs described above and techniques used for solvingBayesian networks. Since many of the algorithms discussed in this section rely on the struc-ture inherent in the 2TBN representation of the MDP, it is natural to ask whether theyembody some of the intuitions that underlie solution algorithms for Bayes nets, and thuswhether the solution techniques for Bayes nets can be (directly or indirectly) applied toMDPs in ways that give rise to algorithms similar to those discussed here. This remains anopen question at this point, but undoubtedly some strong ties exist. Tatman and Shachter(1990) have explored the connections between in uence diagrams and MDPs. Kjaerul (1992) has investigated computational considerations involved in applying join tree methodsfor reasoning tasks such as monitoring and prediction in temporal Bayes nets. The abstrac-tion methods discussed in Section 5.1.2 can be interpreted as a form of variable elimination(Dechter, 1996; Zhang & Poole, 1996). Elimination of variables occurs in temporal order,but good orderings within a time slice must also exploit the tree or graph structure of theCPTs. Approximation schemes based on variable elimination (Dechter, 1997; Poole, 1998)may also be related to certain of the approximation methods developed for MDPs. Theindependence-based decompositions of MDPs discussed in Section 5.3 can clearly be viewedas exploiting the independence relations made explicit by \\unrolling\" a 2TBN. The develop-ment of these and other connections to Bayes net inference algorithms will no doubt provevery useful in enhancing our understanding of existing methods, increasing their range ofapplicability and pointing to new algorithms.6. Concluding RemarksThe search for e ective algorithms for controlling automated agents has a long and impor-tant history, and the problem will only continue to grow in importance as more decision-making functionality is automated. Work in several disciplines, among them AI, decisionanalysis, and OR, has addressed the problem, but each has carried with it di erent prob-lem de nitions, di erent sets of simplifying assumptions, di erent viewpoints, and hencedi erent representations and algorithms for problem solving. More often than not, the as-sumptions seem to have been made for historical reasons or reasons of convenience, and itis often di cult to separate the essential assumptions from the accidental. It is importantto clarify the relationships among problem de nitions, crucial assumptions, and solutiontechniques, because only then can a meaningful synthesis take place.In this paper we analyzed various approaches to a particular class of sequential deci-sion problems that have been studied in the OR, decision analysis, and AI literature. Westarted with a general, reasonably neutral statement of the problem, couched, for conve-nience, in the language of Markov decision processes. From there we demonstrated howvarious disciplines de ne the problem (i.e., what assumptions they make), and the e ect81\nBoutilier, Dean, & Hanksof these assumptions on the worst-case time complexity of solving the problem so de ned.Assumptions regarding two main factors seem to distinguish the most commonly studiedclasses of decision problems: observation or sensing: does sensing tend to be fast, cheap, and accurate or laborious,costly and noisy? the incentive structure for the agent: is its behavior evaluated on its ability to performa particular task, or on its ability to control a system over an interval of time?Moving beyond the worst-case analysis, it is generally assumed that, although patho-logical cases are inevitably di cult, the agent should be able to solve \\typical\" or \\easy\"cases e ectively. To do so, the agent needs to be able to identify structure in the problemand to exploit that structure algorithmically.We identi ed three ways in which structural regularities can be recognized, represented,and exploited computationally. The rst is structure induced by domain-level simplifyingassumptions like full observability, goal satisfaction or time-separable value functions, andso on. The second is structure exploited by compact domain-speci c encodings of states,actions, and rewards. The designer can use these techniques to make structure explicit, anddecision-making algorithms can then exploit the structural regularities as they apply to theparticular problem at hand. The third involves aggregation, abstraction and decomposi-tion techniques, whereby structural regularities can be discovered and exploited during theproblem-solving process itself. In developing this framework|one that allows comparisonof domains, assumptions, problems, and techniques drawn from di erent disciplines|wediscover the essential problem structure required for speci c representations and algorithmsto prove e ective; and we do so in such a way that the insights and techniques developedfor certain problems, or within certain disciplines, can be evaluated and potentially appliedto new problems, or within other disciplines.A main focus of this work has been the elucidation of various forms of structure indecision problems and of how each can be exploited representationally or computationally.For the most part, we have focused on propositional structure, which is most commonly as-sociated with planning in AI circles. A more complete treatment would also have includedother compact representations of dynamics, rewards, policies, and value functions oftenconsidered in continuous, real-valued domains. For instance, we have not discussed lineardynamics and quadratic cost functions, often used in control theory (Caines, 1988), or theuse of neural-network representations of value functions, as frequently adopted within thereinforcement learning community (Bertsekas & Tsitsiklis, 1996; Tesauro, 1994),54 nor havewe discussed the partitioning of continuous state spaces often addressed in reinforcementlearning (Moore & Atkeson, 1995). Neither have we addressed the relational or quanti ca-tional structure used in rst-order planning representations. However, even these techniquescan be cast within the framework described here; for example, the use of piecewise-linearvalue functions can be seen as a form of abstraction in which di erent linear componentsare applied to di erent regions or clusters of state space.54. Bertsekas and Tsitsiklis (1996) provide an in-depth treatment of neural network and linear functionapproximators for MDPs and reinforcement learning.82\nDecision-Theoretic Planning: Structural AssumptionsAlthough in certain cases we have indicated how to devise methods that exploit severaltypes of structure at once, research along these lines has been limited. To some extent,many of the representations and algorithms described in this paper are complementary andshould pose few obstacles to combination. It remains to be seen how they interact withtechniques developed for other forms of structure, such as those used for continuous stateand action spaces.So our analysis raises opportunities and challenges: by understanding the assumptions,the techniques, and their relationships, a designer of decision-making agents has many moretools with which to build e ective problem solvers; and the challenges lie in the developmentof additional tools and the integration of existing ones.AcknowledgmentsMany thanks to the careful comments of the referees. Thanks to Ron Parr and RobertSt-Aubin for their comments on an earlier draft of this paper. The students taking CS3710(Spring 1999) taught by Martha Pollack at the University of Pittsburgh and CPSC522(Winter 1999) at the University of British Columbia also deserve thanks for their detailedcomments.Boutilier was supported by NSERC Research Grant OGP0121843, and the NCE IRIS-II program Project IC-7. Dean was supported in part by a National Science FoundationPresidential Young Investigator Award IRI-8957601 and by the Air Force and the AdvancedResearch Projects Agency of the Department of Defense under Contract No. F30602-91-C-0041. Hanks was supported in part by ARPA / Rome Labs Grant F30602{95{1{0024 andin part by NSF grant IRI{9523649.ReferencesAllen, J., Hendler, J., & Tate, A. (Eds.). (1990). Readings in Planning. Morgan-Kaufmann,San Mateo.Astr om, K. J. (1965). Optimal control of Markov decision processes with incomplete stateestimation. J. Math. Anal. Appl., 10, 174{205.Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. In Proceedings ofthe Thirteenth National Conference on Arti cial Intelligence, pp. 1160{1167 Portland,OR.Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods for non-Markovian decision processes. In Proceedings of the Fourteenth National Conferenceon Arti cial Intelligence, pp. 112{117 Providence, RI.Bacchus, F., & Kabanza, F. (1995). Using temporal logic to control searchin a forward chaining planner. In Proceedings of the Third EuropeanWorkshop on Planning (EWSP'95) Assisi, Italy. Available via the URLftp://logos.uwaterloo.ca:/pub/tlplan/tlplan.ps.Z.Bacchus, F., & Teh, Y. W. (1998). Making forward chaining relevant. In Proceedings of theFourth International Conference on AI Planning Systems, pp. 54{61 Pittsburgh, PA.83\nBoutilier, Dean, & HanksBahar, R. I., Frohm, E. A., Gaona, C. M., Hachtel, G. D., Macii, E., Pardo, A., & Somenzi,F. (1993). Algebraic decision diagrams and their applications. In International Con-ference on Computer-Aided Design, pp. 188{191. IEEE.Baker, A. B. (1991). Nonmonotonic reasoning in the framework of the situation calculus.Arti cial Intelligence, 49, 5{23.Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning to act using real-time dynamicprogramming. Arti cial Intelligence, 72 (1{2), 81{138.Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.Bertsekas, D. P., & Castanon, D. A. (1989). Adaptive aggregation for in nite horizondynamic programming. IEEE Transactions on Automatic Control, 34 (6), 589{598.Bertsekas, D. P. (1987). Dynamic Programming. Prentice-Hall, Englewood Cli s, NJ.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont,MA.Blackwell, D. (1962). Discrete dynamic programming. Annals of Mathematical Statistics,33, 719{726.Blum, A. L., & Furst, M. L. (1995). Fast planning through graph analysis. In Proceedingsof the Fourteenth International Joint Conference on Arti cial Intelligence, pp. 1636{1642 Montreal, Canada.Bonet, B., & Ge ner, H. (1998). Learning sorting and decision trees with POMDPs. InProceedings of the Fifteenth International Conference on Machine Learning, pp. 73{81Madison, WI.Bonet, B., Loerincs, G., & Ge ner, H. (1997). A robust and fast action selection mechanism.In Proceedings of the Fourteenth National Conference on Arti cial Intelligence, pp.714{719 Providence, RI.Boutilier, C. (1997). Correlated action e ects in decision theoretic regression. In Proceed-ings of the Thirteenth Conference on Uncertainty in Arti cial Intelligence, pp. 30{37Providence, RI.Boutilier, C., Brafman, R. I., & Geib, C. (1997). Prioritized goal decomposition of Markovdecision processes: Toward a synthesis of classical and decision theoretic planning. InProceedings of the Fifteenth International Joint Conference on Arti cial Intelligence,pp. 1156{1162 Nagoya, Japan.Boutilier, C., Brafman, R. I., & Geib, C. (1998). Structured reachability analysis for Markovdecision processes. In Proceedings of the Fourteenth Conference on Uncertainty inArti cial Intelligence, pp. 24{32 Madison, WI.Boutilier, C., & Dearden, R. (1994). Using abstractions for decision-theoretic planningwith time constraints. In Proceedings of the Twelfth National Conference on Arti cialIntelligence, pp. 1016{1022 Seattle, WA.84\nDecision-Theoretic Planning: Structural AssumptionsBoutilier, C., & Dearden, R. (1996). Approximating value trees in structured dynamicprogramming. In Proceedings of the Thirteenth International Conference on MachineLearning, pp. 54{62 Bari, Italy.Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy con-struction. In Proceedings of the Fourteenth International Joint Conference on Arti cialIntelligence, pp. 1104{1111 Montreal, Canada.Boutilier, C., Dearden, R., & Goldszmidt, M. (1999). Stochastic dynamic programmingwith factored representations. (manuscript).Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-speci c indepen-dence in Bayesian networks. In Proceedings of the Twelfth Conference on Uncertaintyin Arti cial Intelligence, pp. 115{123 Portland, OR.Boutilier, C., & Goldszmidt, M. (1996). The frame problem and Bayesian network actionrepresentations. In Proceedings of the Eleventh Biennial Canadian Conference onArti cial Intelligence, pp. 69{83 Toronto.Boutilier, C., & Poole, D. (1996). Computing optimal policies for partially observabledecision processes using compact representations. In Proceedings of the ThirteenthNational Conference on Arti cial Intelligence, pp. 1168{1175 Portland, OR.Boutilier, C., & Puterman, M. L. (1995). Process-oriented planning and average-reward op-timality. In Proceedings of the Fourteenth International Joint Conference on Arti cialIntelligence, pp. 1096{1103 Montreal, Canada.Brafman, R. I. (1997). A heuristic variable-grid solution method for POMDPs. In Pro-ceedings of the Fourteenth National Conference on Arti cial Intelligence, pp. 727{733Providence, RI.Bryant, R. E. (1986). Graph-based algorithms for boolean function manipulation. IEEETransactions on Computers, C-35 (8), 677{691.Bylander, T. (1994). The computational complexity of propositional STRIPS planning.Arti cial Intelligence, 69, 161{204.Caines, P. E. (1988). Linear stochastic systems. Wiley, New York.Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally in partiallyobservable stochastic domains. In Proceedings of the Twelfth National Conference onArti cial Intelligence, pp. 1023{1028 Seattle, WA.Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: A sim-ple, fast, exact method for pomdps. In Proceedings of the Thirteenth Conference onUncertainty in Arti cial Intelligence, pp. 54{61 Providence, RI.Chapman, D. (1987). Planning for conjunctive goals. Arti cial Intelligence, 32 (3), 333{377.85\nBoutilier, Dean, & HanksChapman, D., & Kaelbling, L. P. (1991). Input generalization in delayed reinforcementlearning: An algorithm and performance comparisons. In Proceedings of the TwelfthInternational Joint Conference on Arti cial Intelligence, pp. 726{731 Sydney, Aus-tralia.Dantzig, G., & Wolfe, P. (1960). Decomposition principle for dynamic programs. OperationsResearch, 8 (1), 101{111.Dean, T., Allen, J., & Aloimonos, Y. (1995). Arti cial Intelligence: Theory and Practice.Benjamin Cummings.Dean, T., & Givan, R. (1997). Model minimization in Markov decision processes. InProceedings of the Fourteenth National Conference on Arti cial Intelligence, pp. 106{111 Providence, RI. AAAI.Dean, T., Givan, R., & Kim, K.-E. (1998). Solving planning problems with large state andaction spaces. In Proceedings of the Fourth International Conference on AI PlanningSystems, pp. 102{110 Pittsburgh, PA.Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques for computing ap-proximately optimal solutions for Markov decision processes. In Proceedings of theThirteenth Conference on Uncertainty in Arti cial Intelligence, pp. 124{131 Provi-dence, RI.Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1993). Planning with deadlines instochastic domains. In Proceedings of the Eleventh National Conference on Arti cialIntelligence, pp. 574{579.Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning under time con-straints in stochastic domains. Arti cial Intelligence, 76 (1-2), 3{74.Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.Computational Intelligence, 5 (3), 142{150.Dean, T., & Lin, S.-H. (1995). Decomposition techniques for planning in stochastic do-mains. In Proceedings of the Fourteenth International Joint Conference on Arti cialIntelligence, pp. 1121{1127.Dean, T., & Wellman, M. (1991). Planning and Control. Morgan Kaufmann, San Mateo,California.Dearden, R., & Boutilier, C. (1994). Integrating planning and execution in stochasticdomains. In Proceedings of the Tenth Conference on Uncertainty in Arti cial Intelli-gence, pp. 162{169 Washington, DC.Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision theoretic plan-ning. Arti cial Intelligence, 89, 219{283.Dechter, R. (1996). Bucket elimination: A unifying framework for probabilistic inference.In Proceedings of the Twelfth Conference on Uncertainty in Arti cial Intelligence, pp.211{219 Portland, OR. 86\nDecision-Theoretic Planning: Structural AssumptionsDechter, R. (1997). Mini-buckets: A general scheme for generating approximations inautomated reasoning in probabilistic inference. In Proceedings of the Fifteenth Inter-national Joint Conference on Arti cial Intelligence, pp. 1297{1302 Nagoya, Japan.D'Epenoux, F. (1963). Sur un probl eme de production et de stockage dans l'al eatoire.Management Science, 10, 98{108.Dietterich, T. G., & Flann, N. S. (1995). Explanation-based learning and reinforcementlearning: A uni ed approach. In Proceedings of the Twelfth International Conferenceon Machine Learning, pp. 176{184 Lake Tahoe, NV.Draper, D., Hanks, S., & Weld, D. (1994a). A probabilistic model of action for least-commitment planning with information gathering. In Proceedings of the Tenth Con-ference on Uncertainty in Arti cial Intelligence, pp. 178{186 Washington, DC.Draper, D., Hanks, S., & Weld, D. (1994b). Probabilistic planning with information gather-ing and contingent execution. In Proceedings of the Second International Conferenceon AI Planning Systems, pp. 31{36.Etzioni, O., Hanks, S., Weld, D., Draper, D., Lesh, N., & Williamson, M. (1992). Anapproach to planning with incomplete information. In Proceedings of the Third Inter-national Conference on Principles of Knowledge Representation and Reasoning, pp.115{125 Boston, MA.Fikes, R., Hart, P., & Nilsson, N. (1972). Learning and executing generalized robot plans.Arti cial Intelligence, 3, 251{288.Fikes, R., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theoremproving to problem solving. Arti cial Intelligence, 2, 189{208.Finger, J. (1986). Exploiting Constraints in Design Synthesis. Ph.D. thesis, Stanford Uni-versity, Stanford.Floyd, R. W. (1962). Algorithm 97 (shortest path). Communications of the ACM, 5 (6),345.Fox, B. L., & Landi, D. M. (1968). An algorithm for identifying the ergodic subchains andtransient states of a stochastic matrix. Communications of the ACM, 2, 619{621.French, S. (1986). Decision Theory. Halsted Press, New York.Geiger, D., & Heckerman, D. (1991). Advances in probabilistic reasoning. In Proceedingsof the Seventh Conference on Uncertainty in Arti cial Intelligence, pp. 118{126 LosAngeles, CA.Givan, R., & Dean, T. (1997). Model minimization, regression, and propositional STRIPSplanning. In Proceedings of the Fifteenth International Joint Conference on Arti cialIntelligence, pp. 1163{1168 Nagoya, Japan.87\nBoutilier, Dean, & HanksGivan, R., Leach, S., & Dean, T. (1997). Bounded-parameter Markov decision processes. InProceedings of the Fourth European Conference on Planning (ECP'97), pp. 234|246Toulouse, France.Goldman, R. P., & Boddy, M. S. (1994). Representing uncertainty in simple planners.In Proceedings of the Fourth International Conference on Principles of KnowledgeRepresentation and Reasoning, pp. 238{245 Bonn, Germany.Haddawy, P., & Doan, A. (1994). Abstracting probabilistic actions. In Proceedings of theTenth Conference on Uncertainty in Arti cial Intelligence, pp. 270{277 Washington,DC.Haddawy, P., & Hanks, S. (1998). Utility Models for Goal-Directed Decision-TheoreticPlanners. Computational Intelligence, 14 (3).Haddawy, P., & Suwandi, M. (1994). Decision-theoretic re nement planning using inheri-tence abstraction. In Proceedings of the Second International Conference on AI Plan-ning Systems, pp. 266{271 Chicago, IL.Hanks, S. (1990). Projecting plans for uncertain worlds. Ph.D. thesis 756, Yale University,Department of Computer Science, New Haven, CT.Hanks, S., & McDermott, D. V. (1994). Modeling a dynamic and uncertain world I: Symbolicand probabilistic reasoning about change. Arti cial Intelligence, 66 (1), 1{55.Hanks, S., Russell, S., & Wellman, M. (Eds.). (1994). Decision Theoretic Planning: Pro-ceedings of the AAAI Spring Symposium. AAAI Press, Menlo Park.Hansen, E. A., & Zilberstein, S. (1998). Heuristic search in cyclic AND/OR graphs. InProceedings of the Fifteenth National Conference on Arti cial Intelligence, pp. 412{418 Madison, WI.Hauskrecht, M. (1997). A heuristic variable-grid solution method for POMDPs. In Pro-ceedings of the Fourteenth National Conference on Arti cial Intelligence, pp. 734{739Providence, RI.Hauskrecht, M. (1998). Planning and Control in Stochastic Domains with Imperfect Infor-mation. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge.Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchicalsolution of Markov decision processes using macro-actions. In Proceedings of theFourteenth Conference on Uncertainty in Arti cial Intelligence, pp. 220{229 Madison,WI.Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planningusing decision diagrams. In Proceedings of the Fifteenth Conference on Uncertaintyin Arti cial Intelligence Stockholm. To appear.Howard, R. A. (1960). Dynamic Programming and Markov Processes. MIT Press, Cam-bridge, Massachusetts. 88\nDecision-Theoretic Planning: Structural AssumptionsHoward, R. A., & Matheson, J. E. (1984). In uence diagrams. In Howard, R. A., & Math-eson, J. E. (Eds.), The Principles and Applications of Decision Analysis. StrategicDecisions Group, Menlo Park, CA.Kambhampati, S. (1997). Re nement planning as a unifying framework for plan synthesis.AI Magazine, Summer 1997, 67{97.Kearns, M., Mansour, Y., & Ng, A. Y. (1999). A sparse sampling algorithm for near-optimal planning in large markov decision processes. In Proceedings of the SixteenthInternational Joint Conference on Arti cial Intelligence Stockholm. To appear.Keeney, R. L., & Rai a, H. (1976). Decisions with Multiple Objectives: Preferences andValue Tradeo s. John Wiley and Sons, New York.Kjaerul , U. (1992). A computational scheme for reasoning in dynamic probabilistic net-works. In Proceedings of the Eighth Conference on Uncertainty in AI, pp. 121{129Stanford.Knoblock, C. A. (1993). Generating Abstraction Hierarchies: An Automated Approach toReducing Search in Planning. Kluwer, Boston.Knoblock, C. A., Tenenberg, J. D., & Yang, Q. (1991). Characterizing abstraction hier-archies for planning. In Proceedings of the Ninth National Conference on Arti cialIntelligence, pp. 692{697 Anaheim, CA.Koenig, S. (1991). Optimal probabilistic and decision-theoretic planning using Markoviandecision theory. M.sc. thesis UCB/CSD-92-685, University of California at Berkeley,Computer Science Department.Koenig, S., & Simmons, R. (1995). Real-time search in nondeterministic domains. InProceedings of the Fourteenth International Joint Conference on Arti cial Intelligence,pp. 1660{1667 Montreal, Canada.Korf, R. (1985). Macro-operators: A weak method for learning. Arti cial Intelligence, 26,35{77.Korf, R. E. (1990). Real-time heuristic search. Arti cial Intelligence, 42, 189{211.Kushmerick, N., Hanks, S., & Weld, D. (1995). An Algorithm for Probabilistic Planning.Arti cial Intelligence, 76, 239{286.Kushner, H. J., & Chen, C.-H. (1974). Decomposition of systems governed by Markovchains. IEEE Transactions on Automatic Control, 19 (5), 501{507.Lee, D., & Yannakakis, M. (1992). Online minimization of transition systems. In Proceedingsof 24th Annual ACM Symposium on the Theory of Computing, pp. 264{274 Victoria,BC.Lin, F., & Reiter, R. (1994). State constraints revisited. Journal of Logic and Computation,4 (5), 655{678. 89\nBoutilier, Dean, & HanksLin, S.-H. (1997). Exploiting Structure for Planning and Control. Ph.D. thesis, Departmentof Computer Science, Brown University.Lin, S.-H., & Dean, T. (1995). Generating optimal policies for high-level plans with con-ditional branches and loops. In Proceedings of the Third European Workshop onPlanning (EWSP'95), pp. 187{200.Littman, M. L. (1997). Probabilistic propositional planning: Representations and complex-ity. In Proceedings of the Fourteenth National Conference on Arti cial Intelligence,pp. 748{754 Providence, RI.Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). On the complexity of solvingMarkov decision problems. In Proceedings of the Eleventh Conference on Uncertaintyin Arti cial Intelligence, pp. 394{402 Montreal, Canada.Littman, M. L. (1996). Algorithms for sequential decision making. Ph.D. thesis CS{96{09,Brown University, Department of Computer Science, Providence, RI.Lovejoy, W. S. (1991a). Computationally feasible bounds for partially observed Markovdecision processes. Operations Research, 39 (1), 162{175.Lovejoy, W. S. (1991b). A survey of algorithmic methods for partially observed Markovdecision processes. Annals of Operations Research, 28, 47{66.Luenberger, D. G. (1973). Introduction to Linear and Nonlinear Programming. Addison-Wesley, Reading, Massachusetts.Luenberger, D. G. (1979). Introduction to Dynamic Systems: Theory, Models and Applica-tions. Wiley, New York.Madani, O., Condon, A., & Hanks, S. (1999). On the undecidability of probabilistic planningand in nite-horizon partially observable Markov decision problems. In Proceedings ofthe Sixteenth National Conference on Arti cial Intelligence Orlando, FL. To appear.Mahadevan, S. (1994). To discount or not to discount in reinforcement learning: A casestudy in comparing R-learning and Q-learning. In Proceedings of the Eleventh Inter-national Conference on Machine Learning, pp. 164{172 New Brunswick, NJ.McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings ofthe Ninth National Conference on Arti cial Intelligence, pp. 634{639 Anaheim, CA.McCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning withhidden state. In Proceedings of the Twelfth International Conference on MachineLearning, pp. 387{395 Lake Tahoe, Nevada.McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint ofarti cial intelligence. Machine Intelligence, 4, 463{502.90\nDecision-Theoretic Planning: Structural AssumptionsMeuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.(1998). Solving very large weakly coupled Markov decision processes. In Proceedingsof the Fifteenth National Conference on Arti cial Intelligence, pp. 165{172 Madison,WI.Moore, A. W., & Atkeson, C. G. (1995). The parti-game algorithm for variable resolutionreinforcement learning in multidimensional state spaces. Machine Learning, 21, 199{234.Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov chain decisionprocesses. Mathematics of Operations Research, 12 (3), 441{450.Parr, R. (1998). Flexible decomposition algorithms for weakly coupled Markov decisionprocesses. In Proceedings of the Fourteenth Conference on Uncertainty in Arti cialIntelligence, pp. 422{430 Madison, WI.Parr, R., & Russell, S. (1995). Approximating optimal policies for partially observablestochastic domains. In Proceedings of the Fourteenth International Joint Conferenceon Arti cial Intelligence, pp. 1088{1094 Montreal.Parr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. InJordan, M., Kearns, M., & Solla, S. (Eds.), Advances in Neural Information ProcessingSystems 10, pp. 1043{1049. MIT Press, Cambridge.Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of PlausibleInference. Morgan Kaufmann, San Mateo.Pednault, E. (1989). ADL: Exploring the middle ground between STRIPS and the situa-tion calculus. In Proceedings of the First International Conference on Principles ofKnowledge Representation and Reasoning, pp. 324{332 Toronto, Canada.Penberthy, J. S., &Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner forADL. In Proceedings of the Third International Conference on Principles of KnowledgeRepresentation and Reasoning, pp. 103{114 Boston, MA.Peot, M., & Smith, D. (1992). Conditional Nonlinear Planning. In Proceedings of the FirstInternational Conference on AI Planning Systems, pp. 189{197 College Park, MD.Perez, M. A., & Carbonell, J. G. (1994). Control knowledge to improve plan quality. InProceedings of the Second International Conference on AI Planning Systems, pp. 323{328 Chicago, IL.Poole, D. (1995). Exploiting the rule structure for decision making within the independentchoice logic. In Proceedings of the Eleventh Conference on Uncertainty in Arti cialIntelligence, pp. 454{463 Montreal, Canada.Poole, D. (1997a). The independent choice logic for modelling multiple agents under un-certainty. Arti cial Intelligence, 94 (1{2), 7{56.91\nBoutilier, Dean, & HanksPoole, D. (1997b). Probabilistic partial evaluation: Exploiting rule structure in probabilisticinference. In Proceedings of the Fifteenth International Joint Conference on Arti cialIntelligence, pp. 1284{1291 Nagoya, Japan.Poole, D. (1998). Context-speci c approximation in probabilistic inference. In Proceedingsof the Fourteenth Conference on Uncertainty in Arti cial Intelligence, pp. 447{454Madison, WI.Precup, D., Sutton, R. S., & Singh, S. (1998). Theoretical results on reinforcement learningwith temporally abstract behaviors. In Proceedings of the Tenth European Conferenceon Machine Learning, pp. 382{393 Chemnitz, Germany.Pryor, L., & Collins, G. (1993). CASSANDRA: Planning for contingencies. Technicalreport 41, Northwestern University, The Institute for the Learning Sciences.Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York.Puterman, M. L., & Shin, M. (1978). Modi ed policy iteration algorithms for discountedMarkov decision problems. Management Science, 24, 1127{1137.Ross, K. W., & Varadarajan, R. (1991). Multichain Markov decision processes with asample-path constraint: A decomposition approach. Mathematics of Operations Re-search, 16 (1), 195{207.Russell, S., & Norvig, P. (1995). Arti cial Intelligence: A Modern Approach. Prentice Hall,Englewood Cli s, NJ.Sacerdoti, E. D. (1974). Planning in a hierarchy of abstraction spaces. Arti cial Intelligence,5, 115{135.Sacerdoti, E. D. (1975). The nonlinear nature of plans. In Proceedings of the FourthInternational Joint Conference on Arti cial Intelligence, pp. 206{214.Schoppers, M. J. (1987). Universal plans for reactive robots in unpredictable environments.In Proceedings of the Tenth International Joint Conference on Arti cial Intelligence,pp. 1039{1046 Milan, Italy.Schwartz, A. (1993). A reinforcement learning method for maximizing undiscounted re-wards. In Proceedings of the Tenth International Conference on Machine Learning,pp. 298{305 Amherst, MA.Schweitzer, P. L., Puterman, M. L., & Kindle, K. W. (1985). Iterative aggregation-disaggregation procedures for discounted semi-Markov reward processes. OperationsResearch, 33, 589{605.Shachter, R. D. (1986). Evaluating in uence diagrams. Operations Research, 33 (6), 871{882.Shimony, S. E. (1993). The role of relevance in explanation I: Irrelevance as statisticalindependence. International Journal of Approximate Reasoning, 8 (4), 281{324.92\nDecision-Theoretic Planning: Structural AssumptionsSimmons, R., & Koenig, S. (1995). Probabilistic robot navigation in partially observableenvironments. In Proceedings of the Fourteenth International Joint Conference onArti cial Intelligence, pp. 1080{1087 Montreal, Canada.Singh, S. P., & Cohn, D. (1998). How to dynamically merge Markov decision processes. InAdvances in Neural Information Processing Systems 10, pp. 1057{1063. MIT Press,Cambridge.Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Reinforcement learning with soft stateaggregation. In Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances in NeuralInformation Processing Systems 7. Morgan-Kaufmann, San Mateo.Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observableMarkov processes over a nite horizon. Operations Research, 21, 1071{1088.Smith, D., & Peot, M. (1993). Postponing threats in partial-order planning. In Proceedingsof the Eleventh National Conference on Arti cial Intelligence, pp. 500{506 Washing-ton, DC.Sondik, E. J. (1978). The optimal control of partially observable Markov processes over thein nite horizon: Discounted costs. Operations Research, 26, 282{304.Stone, P., & Veloso, M. (1999). Team-partitioned, opaque-transition reinforcement learning.In Asada, M. (Ed.), RoboCup-98: Robot Soccer World Cup II. Springer Verlag, Berlin.Sutton, R. S. (1995). TD models: Modeling the world at a mixture of time scales. InProceedings of the Twelfth International Conference on Machine Learning, pp. 531{539 Lake Tahoe, NV.Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,Cambridge, MA.Tash, J., & Russell, S. (1994). Control strategies for a stochastic planner. In Proceedingsof the Twelfth National Conference on Arti cial Intelligence, pp. 1079{1085 Seattle,WA.Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming and in uence diagrams.IEEE Transactions on Systems, Man, and Cybernetics, 20 (2), 365{379.Tesauro, G. J. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6, 215{219.Thrun, S., Fox, D., & Burgard, W. (1998). A probabilistic approach to concurrent mappingand localization for mobile robots. Machine Learning, 31, 29{53.Thrun, S., & Schwartz, A. (1995). Finding structure in reinforcement learning. In Tesauro,G., Touretzky, D., & Leen, T. (Eds.), Advances in Neural Information ProcessingSystems 7 Cambridge, MA. MIT Press.Warren, D. (1976). Generating conditional plans and programs. In Proceedings of AISBSummer Conference, pp. 344{354 University of Edinburgh.93\nBoutilier, Dean, & HanksWatkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279{292.Weld, D. S. (1994). An introduction to least commitment planning. AI Magazine, Winter1994, 27{61.White III, C. C., & Scherer, W. T. (1989). Solutions procedures for partially observedMarkov decision processes. Operations Research, 37 (5), 791{797.Williamson, M. (1996). A value-directed approach to planning. Ph.D. thesis 96{06{03,University of Washington, Department of Computer Science and Engineering.Williamson, M., & Hanks, S. (1994). Optimal planning with a goal-directed utility model.In Proceedings of the Second International Conference on AI Planning Systems, pp.176{180 Chicago, IL.Winston, P. H. (1992). Arti cial Intelligence, Third Edition. Addison-Wesley, Reading,Massachusetts.Yang, Q. (1998). Intelligent Planning : A Decomposition and Abstraction Based Approach.Springer Verlag.Zhang, N. L., & Liu, W. (1997). A model approximation scheme for planning in partiallyobservable stochastic domains. Journal of Arti cial Intelligence Research, 7, 199{230.Zhang, N. L., & Poole, D. (1996). Exploiting causal independence in Bayesian networkinference. Journal of Arti cial Intelligence Research, 5, 301{328.\n94"}], "references": [{"title": "Optimal control of Markov decision processes with incomplete state", "author": ["K.J. Astr\u007fom"], "venue": null, "citeRegEx": "Astr\u007fom,? \\Q1965\\E", "shortCiteRegEx": "Astr\u007fom", "year": 1965}, {"title": "Structured solution methods for non", "author": ["F. Bacchus", "C. Boutilier", "A. Grove"], "venue": null, "citeRegEx": "Bacchus et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 1997}, {"title": "Using temporal logic to control search", "author": ["F. Bacchus", "F. Kabanza"], "venue": null, "citeRegEx": "Bacchus and Kabanza,? \\Q1995\\E", "shortCiteRegEx": "Bacchus and Kabanza", "year": 1995}, {"title": "Making forward chaining relevant", "author": ["F. Bacchus", "Y.W. Teh"], "venue": null, "citeRegEx": "Bacchus and Teh,? \\Q1998\\E", "shortCiteRegEx": "Bacchus and Teh", "year": 1998}, {"title": "Algebraic decision diagrams and their applications", "author": ["F."], "venue": "International Con-", "citeRegEx": "F.,? 1993", "shortCiteRegEx": "F.", "year": 1993}, {"title": "Nonmonotonic reasoning in the framework of the situation calculus", "author": ["A.B. Baker"], "venue": null, "citeRegEx": "Baker,? \\Q1991\\E", "shortCiteRegEx": "Baker", "year": 1991}, {"title": "Learning to act using real-time dynamic", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singh"], "venue": null, "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Adaptive aggregation for in nite horizon", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": null, "citeRegEx": "Bertsekas and Castanon,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1989}, {"title": "Dynamic Programming", "author": ["D.P. Bertsekas"], "venue": "Prentice-Hall, Englewood Cli s, NJ.", "citeRegEx": "Bertsekas,? 1987", "shortCiteRegEx": "Bertsekas", "year": 1987}, {"title": "Discrete dynamic programming", "author": ["D. Blackwell"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Blackwell,? 1962", "shortCiteRegEx": "Blackwell", "year": 1962}, {"title": "Fast planning through graph analysis", "author": ["A.L. Blum", "M.L. Furst"], "venue": null, "citeRegEx": "Blum and Furst,? \\Q1995\\E", "shortCiteRegEx": "Blum and Furst", "year": 1995}, {"title": "Learning sorting and decision trees with POMDPs", "author": ["B. Bonet", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet and ner,? \\Q1998\\E", "shortCiteRegEx": "Bonet and ner", "year": 1998}, {"title": "A robust and fast action selection mechanism", "author": ["B. Bonet", "G. Loerincs", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bonet et al\\.", "year": 1997}, {"title": "Correlated action e ects in decision theoretic regression", "author": ["C. Boutilier"], "venue": "Proceed-", "citeRegEx": "Boutilier,? 1997", "shortCiteRegEx": "Boutilier", "year": 1997}, {"title": "Prioritized goal decomposition", "author": ["C. Boutilier", "R.I. Brafman", "C. Geib"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1997}, {"title": "Structured reachability analysis", "author": ["C. Boutilier", "R.I. Brafman", "C. Geib"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1998}, {"title": "Using abstractions for decision-theoretic planning", "author": ["C. Boutilier", "R. Dearden"], "venue": null, "citeRegEx": "Boutilier and Dearden,? \\Q1994\\E", "shortCiteRegEx": "Boutilier and Dearden", "year": 1994}, {"title": "Approximating value trees in structured dynamic", "author": ["C. Boutilier", "R. Dearden"], "venue": null, "citeRegEx": "Boutilier and Dearden,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Dearden", "year": 1996}, {"title": "Exploiting structure in policy", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "Stochastic dynamic programming", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "The frame problem and Bayesian network action", "author": ["C. Boutilier", "M. Goldszmidt"], "venue": null, "citeRegEx": "Boutilier and Goldszmidt,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Goldszmidt", "year": 1996}, {"title": "Computing optimal policies for partially observable", "author": ["C. Boutilier", "D. Poole"], "venue": null, "citeRegEx": "Boutilier and Poole,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Poole", "year": 1996}, {"title": "Process-oriented planning and average-reward", "author": ["C. Boutilier", "M.L. Puterman"], "venue": null, "citeRegEx": "Boutilier and Puterman,? \\Q1995\\E", "shortCiteRegEx": "Boutilier and Puterman", "year": 1995}, {"title": "A heuristic variable-grid solution method for POMDPs", "author": ["R.I. Brafman"], "venue": "Pro-", "citeRegEx": "Brafman,? 1997", "shortCiteRegEx": "Brafman", "year": 1997}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "IEEE", "citeRegEx": "Bryant,? 1986", "shortCiteRegEx": "Bryant", "year": 1986}, {"title": "The computational complexity of propositional STRIPS planning", "author": ["T. Bylander"], "venue": null, "citeRegEx": "Bylander,? \\Q1994\\E", "shortCiteRegEx": "Bylander", "year": 1994}, {"title": "Linear stochastic systems", "author": ["P.E. Caines"], "venue": "Wiley, New York.", "citeRegEx": "Caines,? 1988", "shortCiteRegEx": "Caines", "year": 1988}, {"title": "Acting optimally in partially", "author": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": null, "citeRegEx": "Cassandra et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1994}, {"title": "Incremental pruning: A", "author": ["A.R. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": null, "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Planning for conjunctive goals", "author": ["D. Chapman"], "venue": "Arti cial Intelligence, 32 (3), 333{377.", "citeRegEx": "Chapman,? 1987", "shortCiteRegEx": "Chapman", "year": 1987}, {"title": "Input generalization in delayed reinforcement", "author": ["D. Chapman", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Chapman and Kaelbling,? \\Q1991\\E", "shortCiteRegEx": "Chapman and Kaelbling", "year": 1991}, {"title": "Decomposition principle for dynamic programs", "author": ["G. Dantzig", "P. Wolfe"], "venue": null, "citeRegEx": "Dantzig and Wolfe,? \\Q1960\\E", "shortCiteRegEx": "Dantzig and Wolfe", "year": 1960}, {"title": "Model minimization in Markov decision processes", "author": ["T. Dean", "R. Givan"], "venue": null, "citeRegEx": "Dean and Givan,? \\Q1997\\E", "shortCiteRegEx": "Dean and Givan", "year": 1997}, {"title": "Solving planning problems with large state and", "author": ["T. Dean", "R. Givan", "Kim", "K.-E"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1998}, {"title": "Model reduction techniques for computing", "author": ["T. Dean", "R. Givan", "S. Leach"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1997}, {"title": "Planning with deadlines", "author": ["T. Dean", "L. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1993}, {"title": "Planning under time", "author": ["T. Dean", "L. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "A model for reasoning about persistence and causation", "author": ["T. Dean", "K. Kanazawa"], "venue": null, "citeRegEx": "Dean and Kanazawa,? \\Q1989\\E", "shortCiteRegEx": "Dean and Kanazawa", "year": 1989}, {"title": "Decomposition techniques for planning in stochastic do", "author": ["T. Dean", "Lin", "S.-H"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Planning and Control", "author": ["T. Dean", "M. Wellman"], "venue": null, "citeRegEx": "Dean and Wellman,? \\Q1991\\E", "shortCiteRegEx": "Dean and Wellman", "year": 1991}, {"title": "Integrating planning and execution in stochastic", "author": ["R. Dearden", "C. Boutilier"], "venue": null, "citeRegEx": "Dearden and Boutilier,? \\Q1994\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1994}, {"title": "Abstraction and approximate decision theoretic plan", "author": ["R. Dearden", "C. Boutilier"], "venue": null, "citeRegEx": "Dearden and Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1997}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1996\\E", "shortCiteRegEx": "Dechter", "year": 1996}, {"title": "Mini-buckets: A general scheme for generating approximations", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1997\\E", "shortCiteRegEx": "Dechter", "year": 1997}, {"title": "Sur un probl", "author": ["F. D'Epenoux"], "venue": null, "citeRegEx": "D.Epenoux,? \\Q1963\\E", "shortCiteRegEx": "D.Epenoux", "year": 1963}, {"title": "Explanation-based learning and reinforcement", "author": ["T.G. Dietterich", "N.S. Flann"], "venue": null, "citeRegEx": "Dietterich and Flann,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Flann", "year": 1995}, {"title": "A probabilistic model of action", "author": ["D. Draper", "S. Hanks", "D. Weld"], "venue": null, "citeRegEx": "Draper et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Draper et al\\.", "year": 1994}, {"title": "Probabilistic planning with information", "author": ["D. Draper", "S. Hanks", "D. Weld"], "venue": null, "citeRegEx": "Draper et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Draper et al\\.", "year": 1994}, {"title": "Learning and executing generalized robot plans", "author": ["R. Fikes", "P. Hart", "N. Nilsson"], "venue": null, "citeRegEx": "Fikes et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Fikes et al\\.", "year": 1972}, {"title": "STRIPS: A new approach to the application of theorem", "author": ["R. Fikes", "N.J. Nilsson"], "venue": null, "citeRegEx": "Fikes and Nilsson,? \\Q1971\\E", "shortCiteRegEx": "Fikes and Nilsson", "year": 1971}, {"title": "Exploiting Constraints in Design Synthesis", "author": ["J. Finger"], "venue": "Ph.D. thesis, Stanford Uni-", "citeRegEx": "Finger,? 1986", "shortCiteRegEx": "Finger", "year": 1986}, {"title": "Algorithm 97 (shortest path)", "author": ["R.W. Floyd"], "venue": "Communications of the ACM, 5 (6),", "citeRegEx": "Floyd,? 1962", "shortCiteRegEx": "Floyd", "year": 1962}, {"title": "An algorithm for identifying the ergodic subchains and", "author": ["B.L. Fox", "D.M. Landi"], "venue": null, "citeRegEx": "Fox and Landi,? \\Q1968\\E", "shortCiteRegEx": "Fox and Landi", "year": 1968}, {"title": "Decision Theory", "author": ["S. French"], "venue": "Halsted Press, New York.", "citeRegEx": "French,? 1986", "shortCiteRegEx": "French", "year": 1986}, {"title": "Advances in probabilistic reasoning", "author": ["D. Geiger", "D. Heckerman"], "venue": null, "citeRegEx": "Geiger and Heckerman,? \\Q1991\\E", "shortCiteRegEx": "Geiger and Heckerman", "year": 1991}, {"title": "Model minimization, regression, and propositional STRIPS", "author": ["R. Givan", "T. Dean"], "venue": null, "citeRegEx": "Givan and Dean,? \\Q1997\\E", "shortCiteRegEx": "Givan and Dean", "year": 1997}, {"title": "Bounded-parameter Markov decision processes", "author": ["R. Givan", "S. Leach", "T. Dean"], "venue": null, "citeRegEx": "Givan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Givan et al\\.", "year": 1997}, {"title": "Representing uncertainty in simple planners", "author": ["R.P. Goldman", "M.S. Boddy"], "venue": null, "citeRegEx": "Goldman and Boddy,? \\Q1994\\E", "shortCiteRegEx": "Goldman and Boddy", "year": 1994}, {"title": "Abstracting probabilistic actions", "author": ["P. Haddawy", "A. Doan"], "venue": null, "citeRegEx": "Haddawy and Doan,? \\Q1994\\E", "shortCiteRegEx": "Haddawy and Doan", "year": 1994}, {"title": "Utility Models for Goal-Directed Decision-Theoretic", "author": ["P. Haddawy", "S. Hanks"], "venue": null, "citeRegEx": "Haddawy and Hanks,? \\Q1998\\E", "shortCiteRegEx": "Haddawy and Hanks", "year": 1998}, {"title": "Decision-theoretic re nement planning using inheri", "author": ["P. Haddawy", "M. Suwandi"], "venue": null, "citeRegEx": "Haddawy and Suwandi,? \\Q1994\\E", "shortCiteRegEx": "Haddawy and Suwandi", "year": 1994}, {"title": "Projecting plans for uncertain worlds", "author": ["S. Hanks"], "venue": "Ph.D. thesis 756, Yale University,", "citeRegEx": "Hanks,? 1990", "shortCiteRegEx": "Hanks", "year": 1990}, {"title": "Modeling a dynamic and uncertain world I: Symbolic", "author": ["S. Hanks", "D.V. McDermott"], "venue": null, "citeRegEx": "Hanks and McDermott,? \\Q1994\\E", "shortCiteRegEx": "Hanks and McDermott", "year": 1994}, {"title": "Decision Theoretic Planning", "author": ["S. Hanks", "S. Russell", "M. Wellman"], "venue": null, "citeRegEx": "Hanks et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hanks et al\\.", "year": 1994}, {"title": "Heuristic search in cyclic AND/OR graphs", "author": ["E.A. Hansen", "S. Zilberstein"], "venue": null, "citeRegEx": "Hansen and Zilberstein,? \\Q1998\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 1998}, {"title": "A heuristic variable-grid solution method for POMDPs", "author": ["M. Hauskrecht"], "venue": "Pro-", "citeRegEx": "Hauskrecht,? 1997", "shortCiteRegEx": "Hauskrecht", "year": 1997}, {"title": "Planning and Control in Stochastic Domains with Imperfect", "author": ["M. Hauskrecht"], "venue": null, "citeRegEx": "Hauskrecht,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht", "year": 1998}, {"title": "SPUDD: Stochastic planning", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": null, "citeRegEx": "Hoey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 1999}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": "MIT Press, Cam-", "citeRegEx": "Howard,? 1960", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Re nement planning as a unifying framework for plan synthesis", "author": ["S. Kambhampati"], "venue": null, "citeRegEx": "Kambhampati,? \\Q1997\\E", "shortCiteRegEx": "Kambhampati", "year": 1997}, {"title": "A sparse sampling algorithm for near", "author": ["M. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": null, "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Decisions with Multiple Objectives: Preferences", "author": ["R.L. Keeney", "H. Rai a"], "venue": null, "citeRegEx": "Keeney and a,? \\Q1976\\E", "shortCiteRegEx": "Keeney and a", "year": 1976}, {"title": "A computational scheme for reasoning in dynamic probabilistic net", "author": ["U. Kjaerul"], "venue": null, "citeRegEx": "Kjaerul,? \\Q1992\\E", "shortCiteRegEx": "Kjaerul", "year": 1992}, {"title": "Generating Abstraction Hierarchies: An Automated Approach", "author": ["C.A. Knoblock"], "venue": null, "citeRegEx": "Knoblock,? \\Q1993\\E", "shortCiteRegEx": "Knoblock", "year": 1993}, {"title": "Characterizing abstraction hier", "author": ["C.A. Knoblock", "J.D. Tenenberg", "Q. Yang"], "venue": null, "citeRegEx": "Knoblock et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Knoblock et al\\.", "year": 1991}, {"title": "Optimal probabilistic and decision-theoretic planning using Markovian", "author": ["S. Koenig"], "venue": null, "citeRegEx": "Koenig,? \\Q1991\\E", "shortCiteRegEx": "Koenig", "year": 1991}, {"title": "Real-time search in nondeterministic domains", "author": ["S. Koenig", "R. Simmons"], "venue": null, "citeRegEx": "Koenig and Simmons,? \\Q1995\\E", "shortCiteRegEx": "Koenig and Simmons", "year": 1995}, {"title": "Macro-operators: A weak method for learning", "author": ["R. Korf"], "venue": "Arti cial Intelligence, 26,", "citeRegEx": "Korf,? 1985", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Real-time heuristic search", "author": ["R.E. Korf"], "venue": "Arti cial Intelligence, 42, 189{211.", "citeRegEx": "Korf,? 1990", "shortCiteRegEx": "Korf", "year": 1990}, {"title": "An Algorithm for Probabilistic Planning", "author": ["N. Kushmerick", "S. Hanks", "D. Weld"], "venue": null, "citeRegEx": "Kushmerick et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kushmerick et al\\.", "year": 1995}, {"title": "Decomposition of systems governed by Markov", "author": ["H.J. Kushner", "Chen", "C.-H"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 1974}, {"title": "Online minimization of transition systems", "author": ["D. Lee", "M. Yannakakis"], "venue": null, "citeRegEx": "Lee and Yannakakis,? \\Q1992\\E", "shortCiteRegEx": "Lee and Yannakakis", "year": 1992}, {"title": "State constraints revisited", "author": ["F. Lin", "R. Reiter"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Lin and Reiter,? \\Q1994\\E", "shortCiteRegEx": "Lin and Reiter", "year": 1994}, {"title": "Exploiting Structure for Planning and Control", "author": ["Lin", "S.-H."], "venue": "Ph.D. thesis, Department", "citeRegEx": "Lin and S..H.,? 1997", "shortCiteRegEx": "Lin and S..H.", "year": 1997}, {"title": "Generating optimal policies for high-level plans", "author": ["Lin", "S.-H", "T. Dean"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lin et al\\.", "year": 1995}, {"title": "Probabilistic propositional planning: Representations and complex", "author": ["M.L. Littman"], "venue": null, "citeRegEx": "Littman,? \\Q1997\\E", "shortCiteRegEx": "Littman", "year": 1997}, {"title": "On the complexity of solving", "author": ["M.L. Littman", "T.L. Dean", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Algorithms for sequential decision making", "author": ["M.L. Littman"], "venue": "Ph.D. thesis CS{96{09,", "citeRegEx": "Littman,? 1996", "shortCiteRegEx": "Littman", "year": 1996}, {"title": "Computationally feasible bounds for partially observed", "author": ["W.S. Lovejoy"], "venue": null, "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "A survey of algorithmic methods for partially observed", "author": ["W.S. Lovejoy"], "venue": null, "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Introduction to Linear and Nonlinear Programming", "author": ["D.G. Luenberger"], "venue": "Addison-", "citeRegEx": "Luenberger,? 1973", "shortCiteRegEx": "Luenberger", "year": 1973}, {"title": "Introduction to Dynamic Systems: Theory, Models and Applica", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1979\\E", "shortCiteRegEx": "Luenberger", "year": 1979}, {"title": "On the undecidability of probabilistic planning", "author": ["O. Madani", "A. Condon", "S. Hanks"], "venue": null, "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "To discount or not to discount in reinforcement learning: A case", "author": ["S. Mahadevan"], "venue": null, "citeRegEx": "Mahadevan,? \\Q1994\\E", "shortCiteRegEx": "Mahadevan", "year": 1994}, {"title": "Systematic nonlinear planning", "author": ["D. McAllester", "D. Rosenblitt"], "venue": null, "citeRegEx": "McAllester and Rosenblitt,? \\Q1991\\E", "shortCiteRegEx": "McAllester and Rosenblitt", "year": 1991}, {"title": "Instance-based utile distinctions for reinforcement learning", "author": ["R.A. McCallum"], "venue": null, "citeRegEx": "McCallum,? \\Q1995\\E", "shortCiteRegEx": "McCallum", "year": 1995}, {"title": "Some philosophical problems from the standpoint", "author": ["J. McCarthy", "P.J. Hayes"], "venue": null, "citeRegEx": "McCarthy and Hayes,? \\Q1969\\E", "shortCiteRegEx": "McCarthy and Hayes", "year": 1969}, {"title": "The parti-game algorithm for variable resolution", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": null, "citeRegEx": "Moore and Atkeson,? \\Q1995\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1995}, {"title": "The complexity of Markov chain decision", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision", "author": ["R. Parr"], "venue": null, "citeRegEx": "Parr,? \\Q1998\\E", "shortCiteRegEx": "Parr", "year": 1998}, {"title": "Approximating optimal policies for partially observable", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "Parr and Russell,? \\Q1995\\E", "shortCiteRegEx": "Parr and Russell", "year": 1995}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "Parr and Russell,? \\Q1998\\E", "shortCiteRegEx": "Parr and Russell", "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "ADL: Exploring the middle ground between STRIPS and the situa", "author": ["E. Pednault"], "venue": null, "citeRegEx": "Pednault,? \\Q1989\\E", "shortCiteRegEx": "Pednault", "year": 1989}, {"title": "UCPOP: A sound, complete, partial order planner", "author": ["J.S. Penberthy", "D.S. Weld"], "venue": null, "citeRegEx": "Penberthy and Weld,? \\Q1992\\E", "shortCiteRegEx": "Penberthy and Weld", "year": 1992}, {"title": "Conditional Nonlinear Planning", "author": ["M. Peot", "D. Smith"], "venue": "In Proceedings of the First", "citeRegEx": "Peot and Smith,? \\Q1992\\E", "shortCiteRegEx": "Peot and Smith", "year": 1992}, {"title": "Control knowledge to improve plan quality", "author": ["M.A. Perez", "J.G. Carbonell"], "venue": null, "citeRegEx": "Perez and Carbonell,? \\Q1994\\E", "shortCiteRegEx": "Perez and Carbonell", "year": 1994}, {"title": "Exploiting the rule structure for decision making within the independent", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1995\\E", "shortCiteRegEx": "Poole", "year": 1995}, {"title": "The independent choice logic for modelling multiple agents under", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1997\\E", "shortCiteRegEx": "Poole", "year": 1997}, {"title": "Probabilistic partial evaluation: Exploiting rule structure in probabilistic", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1997\\E", "shortCiteRegEx": "Poole", "year": 1997}, {"title": "Context-speci c approximation in probabilistic inference", "author": ["D. Poole"], "venue": "Proceedings", "citeRegEx": "Poole,? 1998", "shortCiteRegEx": "Poole", "year": 1998}, {"title": "Theoretical results on reinforcement learning", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": null, "citeRegEx": "Precup et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1998}, {"title": "CASSANDRA: Planning for contingencies", "author": ["L. Pryor", "G. Collins"], "venue": null, "citeRegEx": "Pryor and Collins,? \\Q1993\\E", "shortCiteRegEx": "Pryor and Collins", "year": 1993}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Modi ed policy iteration algorithms for discounted", "author": ["M.L. Puterman", "M. Shin"], "venue": null, "citeRegEx": "Puterman and Shin,? \\Q1978\\E", "shortCiteRegEx": "Puterman and Shin", "year": 1978}, {"title": "Multichain Markov decision processes with a", "author": ["K.W. Ross", "R. Varadarajan"], "venue": null, "citeRegEx": "Ross and Varadarajan,? \\Q1991\\E", "shortCiteRegEx": "Ross and Varadarajan", "year": 1991}, {"title": "Planning in a hierarchy of abstraction spaces", "author": ["E.D. Sacerdoti"], "venue": "Arti cial Intelligence,", "citeRegEx": "Sacerdoti,? 1974", "shortCiteRegEx": "Sacerdoti", "year": 1974}, {"title": "The nonlinear nature of plans", "author": ["E.D. Sacerdoti"], "venue": "Proceedings of the Fourth", "citeRegEx": "Sacerdoti,? 1975", "shortCiteRegEx": "Sacerdoti", "year": 1975}, {"title": "Universal plans for reactive robots in unpredictable environments", "author": ["M.J. Schoppers"], "venue": null, "citeRegEx": "Schoppers,? \\Q1987\\E", "shortCiteRegEx": "Schoppers", "year": 1987}, {"title": "A reinforcement learning method for maximizing undiscounted", "author": ["A. Schwartz"], "venue": null, "citeRegEx": "Schwartz,? \\Q1993\\E", "shortCiteRegEx": "Schwartz", "year": 1993}, {"title": "Evaluating in uence diagrams", "author": ["R.D. Shachter"], "venue": "Operations Research, 33 (6), 871{", "citeRegEx": "Shachter,? 1986", "shortCiteRegEx": "Shachter", "year": 1986}, {"title": "The role of relevance in explanation I: Irrelevance", "author": ["S.E. Shimony"], "venue": null, "citeRegEx": "Shimony,? \\Q1993\\E", "shortCiteRegEx": "Shimony", "year": 1993}, {"title": "Probabilistic robot navigation in partially observable", "author": ["R. Simmons", "S. Koenig"], "venue": null, "citeRegEx": "Simmons and Koenig,? \\Q1995\\E", "shortCiteRegEx": "Simmons and Koenig", "year": 1995}, {"title": "How to dynamically merge Markov decision processes", "author": ["S.P. Singh", "D. Cohn"], "venue": null, "citeRegEx": "Singh and Cohn,? \\Q1998\\E", "shortCiteRegEx": "Singh and Cohn", "year": 1998}, {"title": "Reinforcement learning with soft state", "author": ["S.P. Singh", "T. Jaakkola", "M.I. Jordan"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1994}, {"title": "The optimal control of partially observable", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": null, "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Postponing threats in partial-order planning", "author": ["D. Smith", "M. Peot"], "venue": null, "citeRegEx": "Smith and Peot,? \\Q1993\\E", "shortCiteRegEx": "Smith and Peot", "year": 1993}, {"title": "The optimal control of partially observable Markov processes over", "author": ["E.J. Sondik"], "venue": null, "citeRegEx": "Sondik,? \\Q1978\\E", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "Team-partitioned, opaque-transition reinforcement learning", "author": ["P. Stone", "M. Veloso"], "venue": null, "citeRegEx": "Stone and Veloso,? \\Q1999\\E", "shortCiteRegEx": "Stone and Veloso", "year": 1999}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In", "citeRegEx": "Sutton,? 1995", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Control strategies for a stochastic planner", "author": ["J. Tash", "S. Russell"], "venue": null, "citeRegEx": "Tash and Russell,? \\Q1994\\E", "shortCiteRegEx": "Tash and Russell", "year": 1994}, {"title": "Dynamic programming and in uence diagrams", "author": ["J.A. Tatman", "R.D. Shachter"], "venue": null, "citeRegEx": "Tatman and Shachter,? \\Q1990\\E", "shortCiteRegEx": "Tatman and Shachter", "year": 1990}, {"title": "TD-Gammon, a self-teaching backgammon program, achieves master", "author": ["G.J. Tesauro"], "venue": null, "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "A probabilistic approach to concurrent mapping", "author": ["S. Thrun", "D. Fox", "W. Burgard"], "venue": null, "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Finding structure in reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": null, "citeRegEx": "Thrun and Schwartz,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Schwartz", "year": 1995}, {"title": "Generating conditional plans and programs", "author": ["D. Warren"], "venue": "Proceedings of AISB", "citeRegEx": "Warren,? 1976", "shortCiteRegEx": "Warren", "year": 1976}, {"title": "An introduction to least commitment planning", "author": ["D.S. Weld"], "venue": "AI Magazine, Winter", "citeRegEx": "Weld,? 1994", "shortCiteRegEx": "Weld", "year": 1994}, {"title": "Solutions procedures for partially observed", "author": ["C.C. White III", "W.T. Scherer"], "venue": null, "citeRegEx": "III and Scherer,? \\Q1989\\E", "shortCiteRegEx": "III and Scherer", "year": 1989}, {"title": "A value-directed approach to planning", "author": ["M. Williamson"], "venue": "Ph.D. thesis 96{06{03,", "citeRegEx": "Williamson,? 1996", "shortCiteRegEx": "Williamson", "year": 1996}, {"title": "Optimal planning with a goal-directed utility model", "author": ["M. Williamson", "S. Hanks"], "venue": null, "citeRegEx": "Williamson and Hanks,? \\Q1994\\E", "shortCiteRegEx": "Williamson and Hanks", "year": 1994}, {"title": "Arti cial Intelligence, Third Edition", "author": ["P.H. Winston"], "venue": "Addison-Wesley, Reading,", "citeRegEx": "Winston,? 1992", "shortCiteRegEx": "Winston", "year": 1992}, {"title": "Intelligent Planning : A Decomposition and Abstraction Based Approach", "author": ["Q. Yang"], "venue": null, "citeRegEx": "Yang,? \\Q1998\\E", "shortCiteRegEx": "Yang", "year": 1998}, {"title": "A model approximation scheme for planning", "author": ["N.L. Zhang", "W. Liu"], "venue": null, "citeRegEx": "Zhang and Liu,? \\Q1997\\E", "shortCiteRegEx": "Zhang and Liu", "year": 1997}, {"title": "Exploiting causal independence in Bayesian network", "author": ["N.L. Zhang", "D. Poole"], "venue": null, "citeRegEx": "Zhang and Poole,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1996}], "referenceMentions": [{"referenceID": 4, "context": "Journal of Arti cial Intelligence Research 11 (1999) 1{94 Submitted 09/98; published 07/99 Decision-Theoretic Planning: Structural Assumptions and Computational Leverage Craig Boutilier cebly@cs.", "startOffset": 9, "endOffset": 53}, {"referenceID": 76, "context": "Much recent research on DTP has explicitly adopted the MDP framework as an underlying model (Barto, Bradtke, & Singh, 1995; Boutilier & Dearden, 1994; Boutilier, Dearden, & Goldszmidt, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1993; Koenig, 1991; Simmons & Koenig, 1995; Tash & Russell, 1994), allowing the adaptation of existing results and algorithms for solving MDPs (e.", "startOffset": 92, "endOffset": 294}, {"referenceID": 114, "context": "See (Puterman, 1994) for a discussion of in nite and continuous-state problems.", "startOffset": 4, "endOffset": 20}, {"referenceID": 114, "context": "While we do not deal with such topics here, there is a considerable literature in the OR community on continuous-time Markov decision processes (Puterman, 1994).", "startOffset": 144, "endOffset": 160}, {"referenceID": 92, "context": "In control theory, this is called conversion to state form (Luenberger, 1979).", "startOffset": 59, "endOffset": 77}, {"referenceID": 91, "context": "See (Luenberger, 1973) for a more precise de nition of time-separability.", "startOffset": 4, "endOffset": 22}, {"referenceID": 7, "context": "Commonly, our aim is to maximize the total expected reward associated with a course of action; we therefore de ne the ( nite-horizon) value of any length T history h as (Bellman, 1957): V (h) = T 1 Xt=0fR(st) C(st; at)g+R(sT ) An in nite-horizon problem, on the other hand, requires that the agent's performance be evaluated over an in nite trajectory.", "startOffset": 169, "endOffset": 184}, {"referenceID": 7, "context": "The value function for an expected total discounted reward problem is de ned as follows (Bellman, 1957; Howard, 1960): V (h) = 1 Xt=0 t(R(st) C(st; at)) where is a xed discount rate (0 < 1).", "startOffset": 88, "endOffset": 117}, {"referenceID": 69, "context": "The value function for an expected total discounted reward problem is de ned as follows (Bellman, 1957; Howard, 1960): V (h) = 1 Xt=0 t(R(st) C(st; at)) where is a xed discount rate (0 < 1).", "startOffset": 88, "endOffset": 117}, {"referenceID": 114, "context": "Boutilier, Dean, & Hanks Re nements of this criterion have also been proposed (Puterman, 1994).", "startOffset": 78, "endOffset": 94}, {"referenceID": 9, "context": "As long as the goal can be reached with certainty, this situation can be formulated as an in nite-horizon problem where total reward is bounded for any desired trajectory (Bertsekas, 1987; Puterman, 1994).", "startOffset": 171, "endOffset": 204}, {"referenceID": 114, "context": "As long as the goal can be reached with certainty, this situation can be formulated as an in nite-horizon problem where total reward is bounded for any desired trajectory (Bertsekas, 1987; Puterman, 1994).", "startOffset": 171, "endOffset": 204}, {"referenceID": 9, "context": "Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994).", "startOffset": 31, "endOffset": 48}, {"referenceID": 114, "context": "Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994).", "startOffset": 53, "endOffset": 69}, {"referenceID": 10, "context": "Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard, 1960; Puterman, 1994).", "startOffset": 73, "endOffset": 120}, {"referenceID": 69, "context": "Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard, 1960; Puterman, 1994).", "startOffset": 73, "endOffset": 120}, {"referenceID": 114, "context": "Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard, 1960; Puterman, 1994).", "startOffset": 73, "endOffset": 120}, {"referenceID": 6, "context": "In the AI literature, discounted or total reward models have been most popular as well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 87, "endOffset": 192}, {"referenceID": 76, "context": "In the AI literature, discounted or total reward models have been most popular as well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 87, "endOffset": 192}, {"referenceID": 94, "context": ", 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 199, "endOffset": 260}, {"referenceID": 120, "context": ", 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 199, "endOffset": 260}, {"referenceID": 0, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 57, "endOffset": 128}, {"referenceID": 128, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 57, "endOffset": 128}, {"referenceID": 67, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 180, "endOffset": 346}, {"referenceID": 88, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 180, "endOffset": 346}, {"referenceID": 121, "context": "In uence diagrams (Howard & Matheson, 1984; Shachter, 1986) are a popular model for decision making in AI and are, in fact, a structured representational method for POMDPs (see Section 4.", "startOffset": 18, "endOffset": 59}, {"referenceID": 3, "context": "This provides a criterion for evaluating potential solutions to planning problems. 2.10.2 Common Planning Problems We can use this general framework to classify various problems commonly studied in the planning and decision-making literature. In each case below, we note the modeling assumptions that de ne the problem class. Planning Problems in the OR/Decision Sciences Tradition Fully Observable Markov Decision Processes (FOMDPs) | There is an extremely large body of research studying FOMDPs, and we present the basic algorithmic techniques in some detail in the next section. The most commonly used formulation of FOMDPs assumes full observability and stationarity, and uses as its optimality criterion the maximization of expected total reward over a nite horizon, maximization of expected total discounted reward over an in nite horizon, or minimization of the expected cost to a goal state. FOMDPs were introduced by Bellman (1957) and have been studied in depth in the elds of decision analysis and OR, including the seminal work of Howard (1960).", "startOffset": 26, "endOffset": 941}, {"referenceID": 3, "context": "This provides a criterion for evaluating potential solutions to planning problems. 2.10.2 Common Planning Problems We can use this general framework to classify various problems commonly studied in the planning and decision-making literature. In each case below, we note the modeling assumptions that de ne the problem class. Planning Problems in the OR/Decision Sciences Tradition Fully Observable Markov Decision Processes (FOMDPs) | There is an extremely large body of research studying FOMDPs, and we present the basic algorithmic techniques in some detail in the next section. The most commonly used formulation of FOMDPs assumes full observability and stationarity, and uses as its optimality criterion the maximization of expected total reward over a nite horizon, maximization of expected total discounted reward over an in nite horizon, or minimization of the expected cost to a goal state. FOMDPs were introduced by Bellman (1957) and have been studied in depth in the elds of decision analysis and OR, including the seminal work of Howard (1960). Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994).", "startOffset": 26, "endOffset": 1057}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text.", "startOffset": 151, "endOffset": 1343}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them.", "startOffset": 151, "endOffset": 1762}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are di cult to capture using a time-separable additive value function. Williamson (1996) (see also Williamson & Hanks, 1994).", "startOffset": 151, "endOffset": 2243}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are di cult to capture using a time-separable additive value function. Williamson (1996) (see also Williamson & Hanks, 1994). implements this model by extending a classical planning algorithm to solve the resulting optimization problem. Haddawy and Suwandi (1994) also implement this model in a complete decision-theoretic framework.", "startOffset": 151, "endOffset": 2418}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are di cult to capture using a time-separable additive value function. Williamson (1996) (see also Williamson & Hanks, 1994). implements this model by extending a classical planning algorithm to solve the resulting optimization problem. Haddawy and Suwandi (1994) also implement this model in a complete decision-theoretic framework. Their model of planning, re nement planning, di ers somewhat from the generative model discussed in this paper. In their model the set of all possible plans is pre-stored in an abstraction hierarchy, and the problem solver's job is to nd in the hierarchy the optimal choice of concrete actions for a particular problem. Perez and Carbonell's (1994) work also incorporates cost information into the classical planning framework, but maintains the split between a classical satis cing planner and additional cost information provided in the utility model.", "startOffset": 151, "endOffset": 2837}, {"referenceID": 80, "context": "Probabilistic Planning Without Feedback | A direct probabilistic extension of the classical planning problem can be stated as follows (Kushmerick et al., 1995): take as input (a) a probability distribution over initial states, (b) stochastic actions (explicit or implicit transition matrices), (c) a set of goal states, and (d) a probability success threshold .", "startOffset": 134, "endOffset": 159}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Conditional Deterministic Planning | The classical planning assumption of omniscience can be relaxed somewhat by allowing the state of some aspects of the world to be unknown. The agent is thus in a situation where it is certain that the system is one of a particular set of states, but does not know which one. Unknown truth values can be included in the initial state speci cation, and taking actions can cause a proposition to become unknown as well. Actions can provide the agent with information while the plan is being executed: conditional planners introduce the idea of actions providing runtime information about the prevailing state, distinguishing between an action that makes proposition P true and an action that will tell the agent whether P is true when the action is executed. An action can have both causal and informational e ects, simultaneously changing the world and reporting on the value of one or more propositions. This second sort of information is not useful at planning time except that it allows steps in the plan to be executed conditionally, depending on the runtime information provided by prior information-producing steps. The value of such actions lies in the fact that di erent courses of action may be appropriate under di erent conditions|these informational e ects allow runtime selection of actions based on the observations produced, much like the general POMDP model. Examples of conditional planners in the classical framework include early work by Warren (1976) and the more recent CNLP (Peot & Smith, 1992), Cassandra (Pryor & Collins, 1993), Plynth (Goldman & Boddy, 1994), and UWL (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992) systems.", "startOffset": 124, "endOffset": 1558}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Conditional Deterministic Planning | The classical planning assumption of omniscience can be relaxed somewhat by allowing the state of some aspects of the world to be unknown. The agent is thus in a situation where it is certain that the system is one of a particular set of states, but does not know which one. Unknown truth values can be included in the initial state speci cation, and taking actions can cause a proposition to become unknown as well. Actions can provide the agent with information while the plan is being executed: conditional planners introduce the idea of actions providing runtime information about the prevailing state, distinguishing between an action that makes proposition P true and an action that will tell the agent whether P is true when the action is executed. An action can have both causal and informational e ects, simultaneously changing the world and reporting on the value of one or more propositions. This second sort of information is not useful at planning time except that it allows steps in the plan to be executed conditionally, depending on the runtime information provided by prior information-producing steps. The value of such actions lies in the fact that di erent courses of action may be appropriate under di erent conditions|these informational e ects allow runtime selection of actions based on the observations produced, much like the general POMDP model. Examples of conditional planners in the classical framework include early work by Warren (1976) and the more recent CNLP (Peot & Smith, 1992), Cassandra (Pryor & Collins, 1993), Plynth (Goldman & Boddy, 1994), and UWL (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992) systems. Probabilistic Planning Without Feedback | A direct probabilistic extension of the classical planning problem can be stated as follows (Kushmerick et al., 1995): take as input (a) a probability distribution over initial states, (b) stochastic actions (explicit or implicit transition matrices), (c) a set of goal states, and (d) a probability success threshold . The objective is to produce a plan that reaches any goal state with probability at least , given the initial state distribution. No provision is made for execution-time observation, thus straight-line plans are the only form of policy possible. This is a restricted case of the in nite-horizon NOMDP problem, one in which actions incur no cost and goal states o er positive reward and are absorbing. It is also a special case in that the objective is to nd a satis cing policy rather than an optimal one. Probabilistic Planning With Feedback | Draper et al. (1994a) have proposed an extension of the probabilistic planning problem in which actions provide feedback, using exactly the observation model described in Section 2.", "startOffset": 124, "endOffset": 2674}, {"referenceID": 0, "context": "As mentioned above, a POMDP can be viewed as a FOMDP with an in nite state space consisting of probability distributions over S, each distribution representing the agent's state of belief at a point in time (Astr\u007fom, 1965; Smallwood & Sondik, 1973).", "startOffset": 207, "endOffset": 248}, {"referenceID": 0, "context": "As mentioned above, a POMDP can be viewed as a FOMDP with an in nite state space consisting of probability distributions over S, each distribution representing the agent's state of belief at a point in time (Astr\u007fom, 1965; Smallwood & Sondik, 1973). The problem of nding an optimal policy for a POMDP with the objective of maximizing expected total reward or expected total discounted reward over a nite horizon T has been shown to be exponentially hard both in jSj and in T (Papadimitriou & Tsitsiklis, 1987). The problem of nding a policy that maximizes or approximately maximizes the expected discounted total reward over an in nite horizon is shown to be undecidable (Madani, Condon, & Hanks, 1999). Even restricted cases of the POMDP problem are computationally di cult in the worst case. Littman (1996) considers the special case of boolean rewards: determining whether there is an in nite-horizon policy with nonzero total reward given that the rewards associated with all states are non-negative.", "startOffset": 208, "endOffset": 809}, {"referenceID": 0, "context": "As mentioned above, a POMDP can be viewed as a FOMDP with an in nite state space consisting of probability distributions over S, each distribution representing the agent's state of belief at a point in time (Astr\u007fom, 1965; Smallwood & Sondik, 1973). The problem of nding an optimal policy for a POMDP with the objective of maximizing expected total reward or expected total discounted reward over a nite horizon T has been shown to be exponentially hard both in jSj and in T (Papadimitriou & Tsitsiklis, 1987). The problem of nding a policy that maximizes or approximately maximizes the expected discounted total reward over an in nite horizon is shown to be undecidable (Madani, Condon, & Hanks, 1999). Even restricted cases of the POMDP problem are computationally di cult in the worst case. Littman (1996) considers the special case of boolean rewards: determining whether there is an in nite-horizon policy with nonzero total reward given that the rewards associated with all states are non-negative. He shows that the problem is EXPTIME-complete if the transitions are stochastic, and PSPACE-hard if the transitions are deterministic. Deterministic Planning Recall that the classical planning problem is de ned quite di erently from the MDP problems above: the agent has no ability to observe the state but has perfect predictive powers, knowing the initial state and the e ects of all actions with certainty. In addition, rewards come only from reaching a goal state, and any plan that achieves the goal su ces. Planning problems are typically de ned in terms of a set P of boolean features or propositions: a complete assignment of truth values to features describes exactly one state, and a partial assignment of truth values describes a set of states. A set of propositions P induces a state space of size 2jPj. Thus, the space required to represent a planning problem using a feature-based representation can be exponentially smaller than that required by a at representation for the same problem (see Section 4). The ability to represent planning problems compactly has a dramatic impact on worstcase complexity. Bylander (1994) shows that the deterministic planning problem without observation is PSPACE-complete.", "startOffset": 208, "endOffset": 2140}, {"referenceID": 93, "context": "Even the simplest problem in probabilistic planning|one that admits no observability|is undecidable at worst (Madani et al., 1999).", "startOffset": 109, "endOffset": 130}, {"referenceID": 7, "context": "Bellman's principle of optimality (Bellman, 1957) forms the basis of the stochastic dynamic programming algorithms used to solve MDPs, establishing the following relationship between the optimal value function at tth stage and the optimal value function at the previous stage: V t (s) = R(s) + max a2A fC(a) +X s02S Pr(s0ja; s)V t 1(s0)g (2) 3.", "startOffset": 34, "endOffset": 49}, {"referenceID": 4, "context": "1 Dynamic Programming Approaches Suppose we are given a fully-observable MDP with a time-separable, additive value function. In other words, we are given the state space S, action space A, a transition matrix Pr(s0js; a) for each action a, a reward function R, and a cost function C. We start with the problem of nding the policy that maximizes expected total reward for some xed, nite-horizon T . Suppose we are given a policy such that (s; t) is the action to be performed by the agent in state s with t stages remaining to act (for 0 t T ).18 Bellman (1957) shows that the expected value of such a policy at any state can be computed using the set of t-stage-to-go value functions V t .", "startOffset": 56, "endOffset": 561}, {"referenceID": 94, "context": "This is by far the most commonly studied problem in the literature, though it is argued in (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) that such problems are often best modeled using average reward per stage as the optimality criterion.", "startOffset": 91, "endOffset": 152}, {"referenceID": 120, "context": "This is by far the most commonly studied problem in the literature, though it is argued in (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) that such problems are often best modeled using average reward per stage as the optimality criterion.", "startOffset": 91, "endOffset": 152}, {"referenceID": 114, "context": "For a discussion of average reward optimality and its many variants and re nements, see (Puterman, 1994).", "startOffset": 88, "endOffset": 104}, {"referenceID": 114, "context": "For a discussion of stopping criteria and convergence of the algorithm, see (Puterman, 1994).", "startOffset": 76, "endOffset": 92}, {"referenceID": 4, "context": "There is no xed time horizon associated with these tasks|they are to be performed as the need arises. Such problems are best modeled as in nite-horizon problems. We consider here the problem of building a policy that maximizes the discounted sum of expected rewards over an in nite horizon.19 Howard (1960) showed that there always exists an optimal stationary policy for such problems.", "startOffset": 75, "endOffset": 307}, {"referenceID": 114, "context": "However, the algorithm converges to an optimal policy at least linearly and under certain conditions converges superlinearly or quadratically (Puterman, 1994).", "startOffset": 142, "endOffset": 158}, {"referenceID": 114, "context": "With some tuning of the value of t used at each iteration, modi ed policy iteration can work extremely well in practice (Puterman, 1994).", "startOffset": 120, "endOffset": 136}, {"referenceID": 9, "context": "For a nice discussion of asynchronous dynamic programming, see (Bertsekas, 1987; Bertsekas & Tsitsiklis, 1996).", "startOffset": 63, "endOffset": 110}, {"referenceID": 87, "context": "See (Littman et al., 1995) for a discussion of the complexity of the algorithm.", "startOffset": 4, "endOffset": 26}, {"referenceID": 114, "context": "See (Puterman, 1994) for details.", "startOffset": 4, "endOffset": 20}, {"referenceID": 0, "context": "While history-dependence precludes dynamic programming, the observable history can be summarized adequately with a probability distribution over S (Astr\u007fom, 1965), and policies can be computed as a function of these distributions, or belief states.", "startOffset": 147, "endOffset": 162}, {"referenceID": 128, "context": "A key observation of Sondik (Smallwood & Sondik, 1973; Sondik, 1978) is that when one views a POMDP with a time-separable value function by taking the state space to be the set of probability distributions over S, one obtains a fully observable MDP that can be solved by dynamic programming.", "startOffset": 28, "endOffset": 68}, {"referenceID": 128, "context": "Boutilier, Dean, & Hanks that the state space for this FOMDP is an N -dimensional continuous space,23 and special techniques must be used to solve it (Smallwood & Sondik, 1973; Sondik, 1978).", "startOffset": 150, "endOffset": 190}, {"referenceID": 28, "context": "We do not explore these techniques here, but note that they are currently practical only for very small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997; Littman, 1996; Lovejoy, 1991b).", "startOffset": 113, "endOffset": 203}, {"referenceID": 88, "context": "We do not explore these techniques here, but note that they are currently practical only for very small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997; Littman, 1996; Lovejoy, 1991b).", "startOffset": 113, "endOffset": 203}, {"referenceID": 24, "context": "A number of approximation methods, developed both in OR (Lovejoy, 1991a; White III & Scherer, 1989) and AI (Brafman, 1997; Hauskrecht, 1997; Parr & Russell, 1995; Zhang & Liu, 1997), can be used to increase the range of solvable problems, but even these techniques are presently of limited practical value.", "startOffset": 107, "endOffset": 181}, {"referenceID": 66, "context": "A number of approximation methods, developed both in OR (Lovejoy, 1991a; White III & Scherer, 1989) and AI (Brafman, 1997; Hauskrecht, 1997; Parr & Russell, 1995; Zhang & Liu, 1997), can be used to increase the range of solvable problems, but even these techniques are presently of limited practical value.", "startOffset": 107, "endOffset": 181}, {"referenceID": 52, "context": "Decision-Theoretic Planning: Structural Assumptions is equivalent to using the Floyd-Warshall algorithm to nd a minimum-cost path through a weighted graph (Floyd, 1962).", "startOffset": 155, "endOffset": 168}, {"referenceID": 54, "context": "A decision tree rooted at sinit is constructed in much the same way as a search tree for a deterministic planning problem (French, 1986).", "startOffset": 122, "endOffset": 136}, {"referenceID": 1, "context": "See Bacchus et al. (1995, 1998) for some recent work that makes the case for progression with good search control, and Bonet et al. (1997) who argue that progression in deterministic planning is useful when integrating planning and execution.", "startOffset": 4, "endOffset": 139}, {"referenceID": 79, "context": "One way around this di culty is the use of real time search (Korf, 1990).", "startOffset": 60, "endOffset": 72}, {"referenceID": 6, "context": "In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion.", "startOffset": 76, "endOffset": 96}, {"referenceID": 13, "context": "This technique is also investigated in (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig & Simmons, 1995), and is closely tied to Korf's (1990) LRTA* algorithm.", "startOffset": 39, "endOffset": 116}, {"referenceID": 4, "context": "Furthermore, in nite-horizon problems pose the di culty of determining a su ciently deep tree. One way around this di culty is the use of real time search (Korf, 1990). In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion. One can interleave search with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It can then build a partial search tree to some depth, perhaps uniformly or perhaps with some branches expanded more deeply than others. Partial tree construction may be halted due to time pressure or due to an assessment by the agent that further expansion of the tree may not be fruitful. When a decision to act must be made, the rollback procedure is applied to this partial, possibly unevenly expanded decision tree. Reward values can be used to evaluate the leaves of the tree, but this may o er an inaccurate picture of the value of nodes higher in the tree. Heuristic information can be used to estimate the long-term value of states labeling leaves. As with value iteration, the deeper the tree, the more accurate the estimated value at the root (generally speaking) for a xed heuristic. We will see in Section 5 that structured representations of MDPs can provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally, with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the tree, methods such as A* or branch-and-bound search can be used. A key advantage of integrating search with execution is that the actual outcome of the action taken can be used to prune from the tree the branches rooted at the unrealized outcomes. The subtree rooted at the realized state can then be expanded further to make the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as a variant of these methods in which stationary policies (i.", "startOffset": 0, "endOffset": 1983}, {"referenceID": 4, "context": "Furthermore, in nite-horizon problems pose the di culty of determining a su ciently deep tree. One way around this di culty is the use of real time search (Korf, 1990). In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion. One can interleave search with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It can then build a partial search tree to some depth, perhaps uniformly or perhaps with some branches expanded more deeply than others. Partial tree construction may be halted due to time pressure or due to an assessment by the agent that further expansion of the tree may not be fruitful. When a decision to act must be made, the rollback procedure is applied to this partial, possibly unevenly expanded decision tree. Reward values can be used to evaluate the leaves of the tree, but this may o er an inaccurate picture of the value of nodes higher in the tree. Heuristic information can be used to estimate the long-term value of states labeling leaves. As with value iteration, the deeper the tree, the more accurate the estimated value at the root (generally speaking) for a xed heuristic. We will see in Section 5 that structured representations of MDPs can provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally, with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the tree, methods such as A* or branch-and-bound search can be used. A key advantage of integrating search with execution is that the actual outcome of the action taken can be used to prune from the tree the branches rooted at the unrealized outcomes. The subtree rooted at the realized state can then be expanded further to make the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as a variant of these methods in which stationary policies (i.e., state-action mappings) can be extracted during the search process. RTDP is formulated by Barto et al. (1995) more generally as a form of online, asynchronous value iteration.", "startOffset": 0, "endOffset": 2172}, {"referenceID": 4, "context": "Furthermore, in nite-horizon problems pose the di culty of determining a su ciently deep tree. One way around this di culty is the use of real time search (Korf, 1990). In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion. One can interleave search with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It can then build a partial search tree to some depth, perhaps uniformly or perhaps with some branches expanded more deeply than others. Partial tree construction may be halted due to time pressure or due to an assessment by the agent that further expansion of the tree may not be fruitful. When a decision to act must be made, the rollback procedure is applied to this partial, possibly unevenly expanded decision tree. Reward values can be used to evaluate the leaves of the tree, but this may o er an inaccurate picture of the value of nodes higher in the tree. Heuristic information can be used to estimate the long-term value of states labeling leaves. As with value iteration, the deeper the tree, the more accurate the estimated value at the root (generally speaking) for a xed heuristic. We will see in Section 5 that structured representations of MDPs can provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally, with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the tree, methods such as A* or branch-and-bound search can be used. A key advantage of integrating search with execution is that the actual outcome of the action taken can be used to prune from the tree the branches rooted at the unrealized outcomes. The subtree rooted at the realized state can then be expanded further to make the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as a variant of these methods in which stationary policies (i.e., state-action mappings) can be extracted during the search process. RTDP is formulated by Barto et al. (1995) more generally as a form of online, asynchronous value iteration. Speci cally, the values \\rolled backed\" can be cached and used as improved heuristic estimates of the value function at the states in question. This technique is also investigated in (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig & Simmons, 1995), and is closely tied to Korf's (1990) LRTA* algorithm.", "startOffset": 0, "endOffset": 2537}, {"referenceID": 103, "context": "Decision-Theoretic Planning: Structural Assumptions A Bayesian network (Pearl, 1988) is a representational framework for compactly representing a probability distribution in factored form.", "startOffset": 71, "endOffset": 84}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions A Bayesian network (Pearl, 1988) is a representational framework for compactly representing a probability distribution in factored form. Although these networks have most typically been used to represent atemporal problem domains, we can apply the same techniques to represent Markov chains, encoding the chain's transition probabilities in the network structure (Dean & Kanazawa, 1989). Formally, a Bayes net is a directed acyclic graph in which vertices correspond to random variables and an edge between two variables indicates a direct probabilistic dependency between them. A network so constructed also re ects implicit independencies among the variables. The network must be quanti ed by specifying a probability for each variable (vertex) conditioned on all possible values of its immediate parents in the graph. In addition, the network must include a marginal distribution: an unconditional probability for each vertex that has no parents. This quanti cation is captured by associating a conditional probability table (CPT) with each variable in the network. Together with the independence assumptions de ned by the graph, this quanti cation de nes a unique joint distribution over the variables in the network. The probability of any event over this space can then be computed using algorithms that exploit the independencies represented within the graphical structure. We refer to Pearl (1988) for details.", "startOffset": 107, "endOffset": 1458}, {"referenceID": 5, "context": "Such correlations can be thought of as rami cations (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).", "startOffset": 52, "endOffset": 99}, {"referenceID": 51, "context": "Such correlations can be thought of as rami cations (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).", "startOffset": 52, "endOffset": 99}, {"referenceID": 19, "context": "30 Decision-tree and decision-graph representations are used to represent actions in fully observable MDPs in (Boutilier et al., 1995; Hoey, St-Aubin, Hu, & Boutilier, 1999) and 30.", "startOffset": 110, "endOffset": 173}, {"referenceID": 108, "context": "Decision-Theoretic Planning: Structural Assumptions are described in detail in (Poole, 1995; Boutilier & Goldszmidt, 1996).", "startOffset": 79, "endOffset": 122}, {"referenceID": 62, "context": "A probabilistic Strips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995) extends the Strips representation in two ways.", "startOffset": 38, "endOffset": 101}, {"referenceID": 80, "context": "A probabilistic Strips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995) extends the Strips representation in two ways.", "startOffset": 38, "endOffset": 101}, {"referenceID": 122, "context": "The fact that certain direct dependencies among variables in a Bayes net are rendered irrelevant under speci c variable assignments has been studied more generally in the guise of context-speci c independence (Boutilier, Friedman, Goldszmidt, & Koller, 1996); see (Geiger & Heckerman, 1991; Shimony, 1993) for related notions.", "startOffset": 264, "endOffset": 305}, {"referenceID": 104, "context": "The conditional nature of e ects is also a feature of a deterministic extension of Strips known as ADL (Pednault, 1989).", "startOffset": 103, "endOffset": 119}, {"referenceID": 4, "context": "Boutilier, Dean, & Hanks form with synchronic arcs, as described in Section 4.1. Essentially, correlated e ects are \\compiled\" into explicit outcomes in PSOs. Recent results by Littman (1997) have shown that simple 2TBNs and PSOs can both be used to represent any action represented as a 2TBN without an exponential blowup in representation size.", "startOffset": 25, "endOffset": 192}, {"referenceID": 121, "context": "3 In uence Diagrams In uence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networks to include special decision nodes to represent action choices, and value nodes to represent the e ect of action choice on a value function.", "startOffset": 38, "endOffset": 79}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions action networks shown in Figure 23(a) and (b). Action a1 makes Y true with probability 0:9 if X is true (having no e ect otherwise), while a2 makes Y true if Z is true. Combining these actions in a single network in the obvious way produces the in uence diagram shown in Figure 23(c). Notice that Y now has four parent nodes, inheriting the union of all its parents in the individual networks (plus the action node) and requiring a CPT with 16 entries for actions a1 and a2 together with eight additional entries for each action that does not a ect Y . The individual networks re ect the fact that Y depends on X only when a1 is performed and on Z only when a2 is performed. This fact is lost in the naively constructed in uence diagram. However, structured CPTs can be used to recapture this independence and compactness of representation: the tree of Figure 23(d) captures the distribution much more concisely, requiring only eight entries. This structured representation also allows us concisely to express that Y persists under all other actions. In large domains, we expect variables to generally be una ected by a substantial number of (perhaps most) actions, thus requiring representations such as this for in uence diagrams. See (Boutilier & Goldszmidt, 1996) for a deeper discussion of this issue and its relationship to the frame problem. While we provide no distributional information over the action choice, it is not hard to see that a 2TBN with an explicit decision node can be used to represent the Markov chain induced by a particular policy in a very natural way. Speci cally, by adding arcs from state variables at time t to the decision node, the value of the decision node (i.e., the choice of action at that point) can be dictated by the prevailing state.38 4.3 In uence Diagrams In uence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networks to include special decision nodes to represent action choices, and value nodes to represent the e ect of action choice on a value function. The presence of decision nodes means that action choice is treated as a variable under the decision maker's control. Value nodes treat reward as a variable in uenced (usually deterministically) by certain state variables. In uence diagrams have not typically been associated with the schematic representation of stationary systems, instead being used as a tool for decision analysts where the sequential decision problem is carefully handcrafted. This more generic use of in uence diagrams has been discussed by Tatman and Shachter (1990). In any event, there is no theory of plan construction associated with in uence diagrams: the choice of all possible actions at each stage must be explicitly encoded in the model.", "startOffset": 77, "endOffset": 2617}, {"referenceID": 19, "context": "Although in the worst case the CRT will take exponential space to store, in many cases the reward function exhibits structure, allowing it to be represented compactly using decision trees or graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden, 1994), or logical rules (Poole, 1995, 1997a).", "startOffset": 198, "endOffset": 222}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions nonstationary or history-dependent rewards and are often used to represent value functions for nite-horizon problems. Although in the worst case the CRT will take exponential space to store, in many cases the reward function exhibits structure, allowing it to be represented compactly using decision trees or graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden, 1994), or logical rules (Poole, 1995, 1997a). Figure 24 shows a fragment of one possible decision-tree representation for the reward function used in the running example. The independence assumptions studied in multiattribute utility theory (Keeney & Rai a, 1976) provide yet another way in which reward functions can be represented compactly. If we assume that the component attributes of the reward function make independent contributions to a state's total reward, the individual contributions can be combined functionally. For instance, we might imagine penalizing states where CR holds with a (partial) reward of 3, penalizing situations where there is undelivered mail (M _ RHM) with 2, and penalizing untidiness T (i) with i 4 (i.e., in proportion to how untidy things are). The reward for any state can then be determined simply by adding the individual penalties associated with each feature. The individual component rewards along with the combination function constitute a compact representation of the reward function. The tree fragment in Figure 24, which re ects the additive independent structure just described, is considerably more complex than a representation that de nes the (independent) rewards for individual propositions separately. The use of additive reward functions for MDPs is considered in (Boutilier, Brafman, & Geib, 1997; Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean, & Boutilier, 1998; Singh & Cohn, 1998). Another example of structured rewards is the goal structure studied in classical planning. Goals are generally speci ed by a single proposition (or a set of literals) to be achieved. As such, they can generally be represented very compactly. Haddawy and Hanks (1998) explore generalizations of goal-oriented models that permit extensions such as partial goal satisfaction, yet still admit compact representations.", "startOffset": 104, "endOffset": 2149}, {"referenceID": 19, "context": "Decision trees have also been used for policies and value functions (Boutilier et al., 1995; Chapman & Kaelbling, 1991).", "startOffset": 68, "endOffset": 119}, {"referenceID": 4, "context": "GetC Cclk M PUM M PUM Cclk Cclk Cclk HRM DelM Cclk Figure 25: A tree representation of a policy. actions and reward functions can be used to represent policies and value functions as well. Here we focus on stationary policies and value functions for FOMDPs, for which any logical function representation may be used. For example, Schoppers (1987) uses a Strips-style representation for universal plans, which are deterministic, plan-like policies.", "startOffset": 51, "endOffset": 347}, {"referenceID": 4, "context": "3 Figure 26: Di erent forms of state space abstraction. cluster agree on this characteristic. A non-exact abstraction is called approximate. This is illustrated schematically in Figure 26: the exact abstraction groups together states that agree on the value assigned to them by a value function, while the approximate abstraction allows states to be grouped together that di er in value. The extent to which these states di er is often used as a measure of the quality of an approximate abstraction. A third dimension is adaptivity. Technically, this is a property not of an abstraction itself, but of how abstractions are used by a particular algorithm. An adaptive abstraction technique is one in which the abstraction can change during the course of computation, while a xed abstraction scheme groups together states once and for all (again, see Figure 26). For example, one can imagine using an abstraction in the representation of a value function V k, then revising this abstraction to represent V k+1 more accurately. Abstraction and aggregation techniques have been studied in the OR literature on MDPs. Bertsekas and Castanon (1989) develop an adaptive aggregation (as opposed to abstraction) technique.", "startOffset": 2, "endOffset": 1142}, {"referenceID": 4, "context": "3 Figure 26: Di erent forms of state space abstraction. cluster agree on this characteristic. A non-exact abstraction is called approximate. This is illustrated schematically in Figure 26: the exact abstraction groups together states that agree on the value assigned to them by a value function, while the approximate abstraction allows states to be grouped together that di er in value. The extent to which these states di er is often used as a measure of the quality of an approximate abstraction. A third dimension is adaptivity. Technically, this is a property not of an abstraction itself, but of how abstractions are used by a particular algorithm. An adaptive abstraction technique is one in which the abstraction can change during the course of computation, while a xed abstraction scheme groups together states once and for all (again, see Figure 26). For example, one can imagine using an abstraction in the representation of a value function V k, then revising this abstraction to represent V k+1 more accurately. Abstraction and aggregation techniques have been studied in the OR literature on MDPs. Bertsekas and Castanon (1989) develop an adaptive aggregation (as opposed to abstraction) technique. The proposed method operates on at state spaces, however, and therefore does not exploit implicit structure in the state space itself. An adaptive, uniform abstraction method is proposed by Schweitzer et al. (1985) for solving stochastic queuing models.", "startOffset": 2, "endOffset": 1428}, {"referenceID": 119, "context": "This use of dynamic programming using Strips action descriptions forms the basic idea of Schoppers's universal planning method (Schoppers, 1987).", "startOffset": 127, "endOffset": 144}, {"referenceID": 30, "context": "Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).", "startOffset": 98, "endOffset": 130}, {"referenceID": 118, "context": "Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).", "startOffset": 98, "endOffset": 130}, {"referenceID": 4, "context": "Goal S S S S 4 3 2 1 Figure 27: An example of goal regression. regression to compute the (largest) set of states such that, after executing , all subgoals are satis ed. In particular, the state space is repartitioned into two abstract states: SGi+1 and SGi+1. In this way, the abstraction mechanism implemented by goal regression should be considered adaptive. This can be viewed as an (i + 1)-stage value function: any state satisfying SGi+1 can reach a goal state in i+1 steps using the action sequence that produced SGi+1.41 The regression process can be stopped when the initial state is a member of the abstract state SGi+1. Figure 27 illustrates the repartitioning of the state space into the di erent regions SGi+1 for each of the steps in the example above. While regression produces a compact representation of something like a value function (as in our discussion of deterministic, goal-based dynamic programming in Section 3.2), the analogy is not exact in that the regions produced by regression record only the property of goal reachability contingent on a particular choice of action or action sequence. Standard dynamic programming methods can be implemented in a structured way by simply noticing that a number of di erent regions can be produced at the ith iteration by considering all actions that can be regressed at that stage. The union of all of these regressions form the states that have positive values in Vi, thus making the representation of the i-stage-to-go value function exact. Notice that each iteration is now more costly, since regression through all actions must be attempted, but this approach obviates the need for backtracking and can ensure that a shortest plan is found. Standard regression does not provide such guarantees without commitment to a particular search strategy (e.g., breadthrst). This use of dynamic programming using Strips action descriptions forms the basic idea of Schoppers's universal planning method (Schoppers, 1987). Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).42 The main motivation for the least-commitment approach comes from the realization that regression techniques are incrementally building a plan from the end to the beginning (in the temporal dimension). Thus, each iteration must commit to inserting a step last in the plan. In many cases it can be determined that a particular step must appear somewhere in the plan, but not necessarily as the last step in the plan; and, indeed, in many cases the step 41. It is not the case, however, that states in SGi+1 cannot reach the goal region in i + 1 steps. It is only the case that they cannot do so using the speci c sequence of actions chosen so far. 42. This type of planning is also sometimes called nonlinear or least-commitment planning. See Weld's (1994) survey for a nice overview.", "startOffset": 21, "endOffset": 2991}, {"referenceID": 70, "context": "See (Kambhampati, 1997) for a framework that uni es various approaches to solving classical plan-generation problems.", "startOffset": 4, "endOffset": 23}, {"referenceID": 80, "context": "While techniques relying on regression have been studied extensively in the deterministic setting, they have only recently been applied to probabilistic unobservable (Kushmerick et al., 1995) and partially observable (Draper, Hanks, & Weld, 1994b) domains.", "startOffset": 166, "endOffset": 191}, {"referenceID": 14, "context": "For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich & Flann, 1995; Hoey et al., 1999).", "startOffset": 49, "endOffset": 202}, {"referenceID": 19, "context": "For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich & Flann, 1995; Hoey et al., 1999).", "startOffset": 49, "endOffset": 202}, {"referenceID": 68, "context": "For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich & Flann, 1995; Hoey et al., 1999).", "startOffset": 49, "endOffset": 202}, {"referenceID": 19, "context": "By piecing together the regions produced for the di erent labels in the description of V0, we can construct a set of regions such that each state in a given region: (a) transitions (under action ) to a particular part of V0 with identical probability; and hence (b) has identical expected future value (Boutilier et al., 1995).", "startOffset": 302, "endOffset": 326}, {"referenceID": 4, "context": "V Figure 29: An iteration of decision-theoretic regression. Step 1 produces the portion of the tree with dashed lines, while Step 2 produces the portion with dotted lines. determine whether the future value is 0 or 9:0). The probability with which Z becomes true is given by the tree representing the CPT for node Z. In Step 2 in Figure 29, the conditions in that CPT are conjoined to the conditions required for predicting Y 's probability (by \\grafting\" the tree for Z to the tree for Y given by the rst step). This grafting is slightly di erent at each of the three leaves of the tree for Y : (a) the full tree for Z is attached to the leaf X = t; (b) the tree for Z is simpli ed where it is attached to to the leaf X = f ^ Y = f by removal of the redundant test on variable Y ; (c) notice that there is no need to attach the tree for Z to the leaf X = f ^ Y = t, since a makes Y true with probability 1 under those conditions (and Z is relevant to the determination of V 0 only when Y is false). At each of the leaves of the newly formed tree we have both Pr(Y ) and Pr(Z). Each of these joint distributions over Y and Z (the e ect of a and these variables is independent by the semantics of the network) tells us the probability of having Y and Z true with zero stages to go given that the conditions labeling the appropriate branch of the tree hold with one stage to go. In other words, the new tree uniquely determines, for any state with one stage remaining, the probability of making any of the conditions labeling the branches of V 0 true. The computation of expected future value obtained by performing a with one stage to go can then be placed at the leaves of this tree by taking expectation over the values at the leaves of V 0. 2 The new set of regions produced this way describes the function Q 1 , where Q 1 (s) is the value associated with performing at state s with one stage to go and acting optimally thereafter. These functions (for each action ) can be pieced together (i.e., \\maxed\"|see Section 3.1) to determine V1. Of course, the process can be repeated some number of times to produce Vn for some suitable n, as well as the optimal policy with respect to Vn. This basic technique can be used in a number of di erent ways. Dietterich and Flann (1995) propose ideas similar to these, but restrict attention to MDPs with goal regions 65", "startOffset": 2, "endOffset": 2277}, {"referenceID": 14, "context": ", synchronic arcs in the 2TBNs) in (Boutilier, 1997).", "startOffset": 35, "endOffset": 52}, {"referenceID": 20, "context": "In (Boutilier et al., 1999), results on a series of abstract process-planning examples are reported, and the scheme is shown to be very useful, especially for larger problems.", "startOffset": 3, "endOffset": 27}, {"referenceID": 20, "context": "What might be viewed as best- and worst-case behavior is also described in (Boutilier et al., 1999).", "startOffset": 75, "endOffset": 99}, {"referenceID": 68, "context": "In (Hoey et al., 1999), a similar algorithm is described that uses algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) rather than trees.", "startOffset": 3, "endOffset": 22}, {"referenceID": 25, "context": "ADDs are a simple generalization of boolean decision diagrams (BDDs) (Bryant, 1986) that allow terminal nodes to be labeled with real values instead of just boolean values.", "startOffset": 69, "endOffset": 83}, {"referenceID": 68, "context": "Initial results provided in (Hoey et al., 1999) are encouraging, showing considerable savings over tree-based algorithms on the same problems.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "(1995) develop a version of modi ed policy iteration to produce tree-structured policies and value functions, while Boutilier and Dearden (1996) develop the version of value iteration described above.", "startOffset": 26, "endOffset": 145}, {"referenceID": 4, "context": "(1995) develop a version of modi ed policy iteration to produce tree-structured policies and value functions, while Boutilier and Dearden (1996) develop the version of value iteration described above. These algorithms are extended to deal with correlations in action e ects (i.e., synchronic arcs in the 2TBNs) in (Boutilier, 1997). These abstraction schemes can be categorized as nonuniform, exact and adaptive. The utility of such exact abstraction techniques has not been tested on real-world problems to date. In (Boutilier et al., 1999), results on a series of abstract process-planning examples are reported, and the scheme is shown to be very useful, especially for larger problems. For example, in one speci c problem with 1.7 million states, the tree representation of the value function has only 40,000 leaves, indicating a tremendous amount of regularity in the value function. Schemes like this exploit such regularity to solve problems more quickly (in this example, in much less than half the time required by modi ed policy iteration) and with much lower memory demands. However, these schemes do involve substantial overhead in tree construction, and for smaller problems with little regularity, the overhead is not repaid in time savings (simple vector-matrix representations methods are faster), though they still generally provide substantial memory savings. What might be viewed as best- and worst-case behavior is also described in (Boutilier et al., 1999). In a series of \\linear\" examples (i.e., problems with value functions that can be represented with trees whose size is linear in the number of problem variables), the tree-based scheme solves problems many orders of magnitude faster than classical state-based techniques. In contrast, problems with exponentially-many distinct values are also tested (i.e., with a distinct value at each state): here tree-construction methods are required to construct a complete decision tree in addition to performing the same number of expected value and maximization computations as classical methods. In this worst case, tree-construction overhead makes the algorithm run about 100 times slower than standard modi ed policy iteration. In (Hoey et al., 1999), a similar algorithm is described that uses algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) rather than trees. ADDs are a simple generalization of boolean decision diagrams (BDDs) (Bryant, 1986) that allow terminal nodes to be labeled with real values instead of just boolean values. Essentially, ADD-based algorithms are similar to the tree-based algorithms except that isomorphic subtrees can be shared. This lets ADDs provide more compact representations of certain types of value functions. Highly optimized ADD manipulation and evaluation software developed in the veri cation community can also be applied to solving MDPs. Initial results provided in (Hoey et al., 1999) are encouraging, showing considerable savings over tree-based algorithms on the same problems. For example, the ADD algorithm applied to the 1.7-million-state example described above revealed the value function to have only 178 distinct values (cf. the 40,000 tree leaves required) and produced an ADD description of the value function with less than 2200 internal nodes. It also solved the same problem in seven minutes, about 40 times faster than earlier reported timing results using decision trees (though some of this improvement was due to the use of optimized ADD software packages). Similar results obtain with other problems (problems of up to 268 million states 44. Dietterich and Flann (1995) also describe their work in the context of reinforcement learning rather than as a method for solving MDPs directly.", "startOffset": 26, "endOffset": 3655}, {"referenceID": 121, "context": "Methods for solving in uence diagrams (Shachter, 1986) exploit structure in a natural way; Tatman and Shachter (1990) explore the connection between in uence diagrams and FOMDPs and the relationship between in uence diagram solution techniques and dynamic programming.", "startOffset": 38, "endOffset": 54}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions were solved in about four hours). Most encouraging is the fact that on the worst-case (exponential) examples, the overhead associated with using ADDs|compared to classical, vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\" modi ed policy iteration with 12 state variables), and lessens as problems become larger. Like tree-based algorithms, these methods have yet to be applied to real-world problems. With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very large (even reaching the level of individual states Boutilier et al., 1995), thus precluding any computational savings. Boutilier and Dearden (1996) develop an approximation scheme that exploits the tree-structured nature of the value functions produced.", "startOffset": 73, "endOffset": 822}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions were solved in about four hours). Most encouraging is the fact that on the worst-case (exponential) examples, the overhead associated with using ADDs|compared to classical, vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\" modi ed policy iteration with 12 state variables), and lessens as problems become larger. Like tree-based algorithms, these methods have yet to be applied to real-world problems. With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very large (even reaching the level of individual states Boutilier et al., 1995), thus precluding any computational savings. Boutilier and Dearden (1996) develop an approximation scheme that exploits the tree-structured nature of the value functions produced. At each stage k, the value function Vk can be pruned to produce a smaller, less accurate tree that approximates Vk. Speci cally, approximate value functions are represented using trees whose leaves are labeled with an upper and lower bound on the value function in that region; decisiontheoretic regression is performed on these bounds. Certain subtrees of the value tree can be pruned when leaves of the subtree are very close in value or when the tree is too large given computational constraints. This scheme is nonuniform, approximate and adaptive. This approximation scheme can be tailored to provide (roughly) the most accurate value function of a given maximum tree size, or the smallest value function (with respect to tree size) of some given minimum accuracy. Results reported in (Boutilier & Dearden, 1996) show that approximation on a small set of examples (including the worst-case examples for tree-based algorithms) allows substantial reduction in computational cost. For instance, in a 10-variable worst-case example, a small amount of pruning introduced an average error of only 0.5% but reduced computation time by a factor of 50. More aggressive pruning tends to increase error and decrease computation time very rapidly; making appropriate tradeo s in these two dimensions is still to be addressed. This method too remains to be tested and evaluated on realistic problems. Structured representations and solution algorithms can be applied to problems other than FOMDPs. Methods for solving in uence diagrams (Shachter, 1986) exploit structure in a natural way; Tatman and Shachter (1990) explore the connection between in uence diagrams and FOMDPs and the relationship between in uence diagram solution techniques and dynamic programming.", "startOffset": 73, "endOffset": 2536}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions were solved in about four hours). Most encouraging is the fact that on the worst-case (exponential) examples, the overhead associated with using ADDs|compared to classical, vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\" modi ed policy iteration with 12 state variables), and lessens as problems become larger. Like tree-based algorithms, these methods have yet to be applied to real-world problems. With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very large (even reaching the level of individual states Boutilier et al., 1995), thus precluding any computational savings. Boutilier and Dearden (1996) develop an approximation scheme that exploits the tree-structured nature of the value functions produced. At each stage k, the value function Vk can be pruned to produce a smaller, less accurate tree that approximates Vk. Speci cally, approximate value functions are represented using trees whose leaves are labeled with an upper and lower bound on the value function in that region; decisiontheoretic regression is performed on these bounds. Certain subtrees of the value tree can be pruned when leaves of the subtree are very close in value or when the tree is too large given computational constraints. This scheme is nonuniform, approximate and adaptive. This approximation scheme can be tailored to provide (roughly) the most accurate value function of a given maximum tree size, or the smallest value function (with respect to tree size) of some given minimum accuracy. Results reported in (Boutilier & Dearden, 1996) show that approximation on a small set of examples (including the worst-case examples for tree-based algorithms) allows substantial reduction in computational cost. For instance, in a 10-variable worst-case example, a small amount of pruning introduced an average error of only 0.5% but reduced computation time by a factor of 50. More aggressive pruning tends to increase error and decrease computation time very rapidly; making appropriate tradeo s in these two dimensions is still to be addressed. This method too remains to be tested and evaluated on realistic problems. Structured representations and solution algorithms can be applied to problems other than FOMDPs. Methods for solving in uence diagrams (Shachter, 1986) exploit structure in a natural way; Tatman and Shachter (1990) explore the connection between in uence diagrams and FOMDPs and the relationship between in uence diagram solution techniques and dynamic programming. Boutilier and Poole (1996) show how classic history-independent methods for solving POMDPs, based on conversion to a FOMDP with belief states, can exploit the types of structured representations described here.", "startOffset": 73, "endOffset": 2714}, {"referenceID": 117, "context": "This approach has been adopted in classical planning in hierarchical or abstraction-based planners, pioneered by Sacerdoti's AbStrips system (Sacerdoti, 1974).", "startOffset": 141, "endOffset": 158}, {"referenceID": 4, "context": "A similar form of abstraction is studied by Knoblock (1993) (see also Knoblock, Tenenberg, & Yang, 1991).", "startOffset": 10, "endOffset": 60}, {"referenceID": 15, "context": "However, a series of such abstractions can be used that take into account objectives of decreasing importance, and the a posteriori most valuable objectives can be dealt with once risk and controllability are taken into account (Boutilier et al., 1997).", "startOffset": 228, "endOffset": 252}, {"referenceID": 4, "context": "Boutilier, Dean, & Hanks We can think of the dynamic programming techniques that rely on structured representations discussed earlier as operating on a reduced model without ever explicitly constructing that model. In some cases, building the reduced model once and for all may be appropriate; in other cases, one might save considerable e ort by explicitly constructing only those parts of the reduced model that are absolutely necessary. There are some potential computational problems with the model-minimization techniques sketched above. A small minimal model may exist, but it may be hard to nd. Instead, we might look for a reduced model that is easier to nd but not necessarily minimal. This too could fail, in which case we might look for a model small enough to be useful but only approximately equivalent to the original factored model. We have to be careful what we mean by \\approximate,\" but intuitively two MDPs are approximately equivalent if the corresponding optimal value functions are within some small factor of one another. In order to be practical, MDP model reduction schemes operate directly on the implicit or factored representation of the original MDP. Lee and Yannakakis (1992) call this online model minimization.", "startOffset": 39, "endOffset": 1206}, {"referenceID": 16, "context": "General k-ary constraints of this type are considered in (Boutilier et al., 1998).", "startOffset": 57, "endOffset": 81}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Several schemes have been proposed in the AI literature for exploiting such reachability constraints, apart from the usual forward- or backward-search approaches. Peot and Smith (1993) introduce the operator graph, a structure computed prior to problem solving that caches reachability relationships among propositions.", "startOffset": 108, "endOffset": 237}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Several schemes have been proposed in the AI literature for exploiting such reachability constraints, apart from the usual forward- or backward-search approaches. Peot and Smith (1993) introduce the operator graph, a structure computed prior to problem solving that caches reachability relationships among propositions. The graph can be consulted during the planning process in deciding which actions to insert into the plan and how to resolve threats. The GraphPlan algorithm of Blum and Furst (1995) attempts to blend considerations of both forward and backward reachability in a deterministic planning context.", "startOffset": 108, "endOffset": 554}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Several schemes have been proposed in the AI literature for exploiting such reachability constraints, apart from the usual forward- or backward-search approaches. Peot and Smith (1993) introduce the operator graph, a structure computed prior to problem solving that caches reachability relationships among propositions. The graph can be consulted during the planning process in deciding which actions to insert into the plan and how to resolve threats. The GraphPlan algorithm of Blum and Furst (1995) attempts to blend considerations of both forward and backward reachability in a deterministic planning context. One of the di culties with regression is that we may regress the goal region through a sequence of operators only to nd ourselves in a region that cannot be reached from the initial state. In Figure 32(a), for example, not all states in region R may be reachable from the initial state. GraphPlan constructs a variant of the operator graph called the planning graph, in which certain forward reachability constraints are posted. Regression is then implemented as usual, but if the current subgoal set violates the forward reachability constraints at any point, this subgoal set is abandoned and the regression search backtracks. Conceptually, one might think of GraphPlan as constructing a forward search tree through state space with the initial state as the root, then doing a backward search from the goal region backward through this tree. Of course, the process is not state-based: instead, constraints on the possible variable values that can hold simultaneously at di erent planning stages are recorded, and regression is used to search backward through the planning graph. In a sense, GraphPlan can be viewed as constructing an abstraction in which forward-reachable states are distinguished from unreachable states at each planning stage, and using this distinction among abstract states quickly to identify infeasible regression paths. Note, however, the GraphPlan approximates this distinction by overestimating the set of reachable states. Overestimation (as opposed to underestimation) ensures that the regression search space contains all legitimate plans. Reachability has also been exploited in the solution of more general MDPs. Dean et al. (1995) propose an envelope method for solving \\goal-based\" MDPs approximately.", "startOffset": 108, "endOffset": 2331}, {"referenceID": 16, "context": "Just as reachability constraints can be used to prune regression paths in deterministic domains, they can be used to prune value function and policy estimates generated by decision-theoretic regression and abstraction algorithms (Boutilier et al., 1998).", "startOffset": 229, "endOffset": 253}, {"referenceID": 16, "context": "The results reported in (Boutilier et al., 1998) are limited to a single process-planning domain, but show that reachability analysis together with abstraction can provide substantial reductions in the size of the e ective MDP that must be solved, at least in some domains.", "startOffset": 24, "endOffset": 48}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation.", "startOffset": 82, "endOffset": 1248}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation. The factored representation allows dimensionality reduction in di erent state subspaces by aggregating states that di er only in the values of the irrelevant variables in their subspaces. A key to such a decomposition is the discovery of the recurrent classes of an MDP. Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968) for discovering the structure of Markov chains that is O(N2) (recall N = jSj).", "startOffset": 82, "endOffset": 1628}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation. The factored representation allows dimensionality reduction in di erent state subspaces by aggregating states that di er only in the values of the irrelevant variables in their subspaces. A key to such a decomposition is the discovery of the recurrent classes of an MDP. Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968) for discovering the structure of Markov chains that is O(N2) (recall N = jSj).50 To alleviate the di culties of algorithms that work with an explicit state-based representation, Boutilier and Puterman (1995) propose a variant of the algorithm that works with a factored 2TBN representation.", "startOffset": 82, "endOffset": 1906}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation. The factored representation allows dimensionality reduction in di erent state subspaces by aggregating states that di er only in the values of the irrelevant variables in their subspaces. A key to such a decomposition is the discovery of the recurrent classes of an MDP. Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968) for discovering the structure of Markov chains that is O(N2) (recall N = jSj).50 To alleviate the di culties of algorithms that work with an explicit state-based representation, Boutilier and Puterman (1995) propose a variant of the algorithm that works with a factored 2TBN representation. One di culty with this form of decomposition is its reliance on strongly independent subproblems (i.e., recurrent classes) within the MDP. Others have explored exact and approximate techniques that work under less restrictive assumptions. One simple method of approximation is to construct \\approximately recurrent classes.\" In Figure 32(c) we might imagine that C and E are nearly independent in the sense that all transitions between them are very low-probability or high-cost. Treating them as independent might lead to approximately optimal policies whose error can be bounded. If the solutions to C and E interact strongly enough that the solutions should not be constructed completely independently, a di erent approach to solving the decomposed problem can be taken. If we have the optimal value function for E then, as pointed out, we can calculate the optimal value function for D. The rst thing to note is that we don't need to know the value function for all of the states in E, just the value of every state in E that is reachable from some state in D in a single step. The set of all states outside D reachable in a single step from a state inside D is referred to as the states in the periphery of D. The values of the states in the intersection of E and the periphery of D summarize the value of exiting D and ending up in E. We refer to the set of all states that are in the periphery of some block as the kernel of the MDP. All of the di erent blocks interact with one another through states in the kernel. 49. Ross and Varadarajan (1991) make a related suggestion for solving average-reward problems.", "startOffset": 82, "endOffset": 3545}, {"referenceID": 142, "context": "The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992).", "startOffset": 98, "endOffset": 113}, {"referenceID": 45, "context": "It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994).", "startOffset": 55, "endOffset": 88}, {"referenceID": 114, "context": "It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994).", "startOffset": 55, "endOffset": 88}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks).", "startOffset": 10, "endOffset": 1933}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe decomposition method, the original, very large system of equations is solved by iteratively solving the smaller systems and adjusting the coupling variables on each iteration until no further adjustment is required. In the linear programming formulation of an MDP, the values of the states are encoded as variables. Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number of states.", "startOffset": 10, "endOffset": 2531}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe decomposition method, the original, very large system of equations is solved by iteratively solving the smaller systems and adjusting the coupling variables on each iteration until no further adjustment is required. In the linear programming formulation of an MDP, the values of the states are encoded as variables. Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number of states. Dean and Lin (1995) describe a general framework for solving decomposed MDPs pointing to the work of Kushner and Chen as a special case, but neither work addresses the issue of where the decompositions come from.", "startOffset": 10, "endOffset": 2710}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe decomposition method, the original, very large system of equations is solved by iteratively solving the smaller systems and adjusting the coupling variables on each iteration until no further adjustment is required. In the linear programming formulation of an MDP, the values of the states are encoded as variables. Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number of states. Dean and Lin (1995) describe a general framework for solving decomposed MDPs pointing to the work of Kushner and Chen as a special case, but neither work addresses the issue of where the decompositions come from. Dean et al. (1995) investigate methods for decomposing the state space into two blocks: those reachable in k steps or fewer and those not reachable in k steps (see the discussion of reachability above).", "startOffset": 10, "endOffset": 2922}, {"referenceID": 130, "context": "Related to this form of decomposition is the development of macro operators for MDPs (Sutton, 1995).", "startOffset": 85, "endOffset": 99}, {"referenceID": 78, "context": "Macros have a long history in classical planning and problem solving (Fikes, Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998; Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz, 1995).", "startOffset": 69, "endOffset": 111}, {"referenceID": 100, "context": "Macros have a long history in classical planning and problem solving (Fikes, Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998; Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz, 1995).", "startOffset": 166, "endOffset": 348}, {"referenceID": 130, "context": "Macros have a long history in classical planning and problem solving (Fikes, Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998; Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz, 1995).", "startOffset": 166, "endOffset": 348}, {"referenceID": 130, "context": "In (Sutton, 1995; Precup et al., 1998), macros are treated as temporally-abstract actions and models are de ned by which a macro can be treated as if it were a single action and used in policy or value iteration (along with concrete actions).", "startOffset": 3, "endOffset": 38}, {"referenceID": 112, "context": "In (Sutton, 1995; Precup et al., 1998), macros are treated as temporally-abstract actions and models are de ned by which a macro can be treated as if it were a single action and used in policy or value iteration (along with concrete actions).", "startOffset": 3, "endOffset": 38}, {"referenceID": 100, "context": "In (Hauskrecht et al., 1998; Parr, 1998; Parr & Russell, 1998), these models are exploited in a hierarchical fashion, with a high-level MDP consisting only of states lying on the boundaries of blocks, and macros the only \\actions\" that can be chosen at these states.", "startOffset": 3, "endOffset": 62}, {"referenceID": 100, "context": "The issue of macro generation| constructing a set of macros guaranteed to provide the exibility to select close to optimal global behavior|is addressed in (Hauskrecht et al., 1998; Parr, 1998).", "startOffset": 155, "endOffset": 192}, {"referenceID": 4, "context": "(1995) discuss methods for solving MDPs in time-critical problems by trading o quality against time. We have ignored the issue of how to obtain decompositions that expedite our calculations. Ideally, each component of the decomposition would yield to simpli cation via aggregation and abstraction, reducing the dimensionality in each component and thereby avoiding explicit enumeration of all the states. Lin (1997) presents methods for exploiting structure for certain special cases in which the communicating structure is revealed by a domain expert.", "startOffset": 23, "endOffset": 416}, {"referenceID": 4, "context": "MDP1 MDP2 MDP3 a a a Figure 35: Parallel problem decomposition. a a ects the state of each subprocess. Intuitively, an action is suitable for execution in the original MDP at some state if it is reasonably good in each of the sub-MDPs. Generally, the sub-MDPs form either a product or join decomposition of the original state space (contrast this with the union decompositions of state space determined by serial decompositions): the state space is formed by taking the cross product of the sub-MDP state spaces, or the join if certain states in the subprocesses cannot be linked. The subprocesses may have identical action spaces (as in Figure 35), or each may have its own action space, with the global action choice being factored into a choice for each subprocess. In the latter case, the sub-MDPs may be completely independent, in which case the (global) MDP can be solved exponentially faster. A more challenging problem arises when there are constraints on the legal action combinations. For example, if the actions in the subprocesses each require certain shared resources, interactions in the global choice may arise. In a parallel MDP decomposition, we wish to solve the sub-MDPs and use the policies or value functions generated to help construct an optimal or approximately optimal solution to the original MDP, highlighting the need to nd appropriate decompositions for MDPs and to develop suitable merging techniques. Recent parallel decomposition methods have all involved decomposing an MDP into subprocesses suitable for distinct objectives. Since reward functions often deal with multiple objectives, each associated with an independent reward, and whose rewards can be summed to determine a global reward, this is often a very natural way to decompose MDPs. Thus, ideas from multiattribute utility theory can be seen to play a role in the solution of MDPs. Boutilier et al. (1997) decompose an MDP speci ed using 2TBNs and an additive reward function using the abstraction technique described in Section 5.", "startOffset": 21, "endOffset": 1900}, {"referenceID": 15, "context": "It seems likely that such approaches could take further advantage of automatic MDP decomposition algorithms such as that of (Boutilier et al., 1997), where factored representations explicitly play a part.", "startOffset": 124, "endOffset": 148}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue.", "startOffset": 43, "endOffset": 169}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue. The global MDP is given by the cross product of the state and action spaces of these sub-MDPs and the reward functions are summed. However, constraints on the feasible action combinations couple the solutions of these sub-MDPs. To solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound on the optimal global value function, while the maximum of these (at any global state) is used as a lower bound. These bounds then form the basis of an action-elimination procedure in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration is run over the explicit state space of the global MDP. Since the action space is also a cross product, this is a potential computational bottleneck for value iteration, as well. Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces.", "startOffset": 43, "endOffset": 1148}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue. The global MDP is given by the cross product of the state and action spaces of these sub-MDPs and the reward functions are summed. However, constraints on the feasible action combinations couple the solutions of these sub-MDPs. To solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound on the optimal global value function, while the maximum of these (at any global state) is used as a lower bound. These bounds then form the basis of an action-elimination procedure in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration is run over the explicit state space of the global MDP. Since the action space is also a cross product, this is a potential computational bottleneck for value iteration, as well. Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces. Much like Singh and Cohn (1998), an MDP is speci ed in terms of a number of independent MDPs, each involving a distinct objective, whose action choices are linked through shared resource constraints.", "startOffset": 43, "endOffset": 1319}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue. The global MDP is given by the cross product of the state and action spaces of these sub-MDPs and the reward functions are summed. However, constraints on the feasible action combinations couple the solutions of these sub-MDPs. To solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound on the optimal global value function, while the maximum of these (at any global state) is used as a lower bound. These bounds then form the basis of an action-elimination procedure in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration is run over the explicit state space of the global MDP. Since the action space is also a cross product, this is a potential computational bottleneck for value iteration, as well. Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces. Much like Singh and Cohn (1998), an MDP is speci ed in terms of a number of independent MDPs, each involving a distinct objective, whose action choices are linked through shared resource constraints. The value functions for the individual MDPs are constructed o ine and then used in set of online action-selection procedures. Unlike many of the approximation procedures we have discussed, this approach makes no attempt to construct a policy explicitly (and is similar to real-time search or RTDP in this respect) nor to construct the value function explicitly. This method has been applied to very large MDPs, with state spaces of size 21000 and actions spaces that are even larger, and can solve such problems in roughly half an hour. The solutions produced are approximate, but the size of the problem precludes exact solution; so good estimates of solution quality are hard to derive. However, when the same method is applied to smaller problems of the same nature whose exact solution can be computed, the approximations have very high quality (Meuleau et al., 1998). While able to solve very large MDPs (with large, but factored, state and action spaces), the model relies on somewhat restrictive assumptions about the nature of the local value functions that ensure good solution quality. However, the basic approach appears to be generalizable, and o ers great promise for solving very large factored MDPs. The algorithms in both (Singh & Cohn, 1998) and (Meuleau et al., 1998) can be seen to rely at least implicitly on structured MDP representations involving almost independent subprocesses. It seems likely that such approaches could take further advantage of automatic MDP decomposition algorithms such as that of (Boutilier et al., 1997), where factored representations explicitly play a part. 5.4 Summary We have seen a number of ways in which intensional representations can be exploited to solve MDPs e ectively without enumeration of the state space. These include techniques for abstraction of MDPs, including those based on relevance analysis, goal regression and decision-theoretic regression; techniques relying on reachability analysis and serial decomposition; and methods for parallel MDP decomposition exploiting the multiattribute nature 53. Singh and Cohn (1998) also incorporate methods for removing unreachable states during value iteration.", "startOffset": 43, "endOffset": 3578}, {"referenceID": 43, "context": "2 can be interpreted as a form of variable elimination (Dechter, 1996; Zhang & Poole, 1996).", "startOffset": 55, "endOffset": 91}, {"referenceID": 44, "context": "Approximation schemes based on variable elimination (Dechter, 1997; Poole, 1998) may also be related to certain of the approximation methods developed for MDPs.", "startOffset": 52, "endOffset": 80}, {"referenceID": 111, "context": "Approximation schemes based on variable elimination (Dechter, 1997; Poole, 1998) may also be related to certain of the approximation methods developed for MDPs.", "startOffset": 52, "endOffset": 80}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions of reward functions. Many of these methods can, in fortunate circumstances, o er exponential reduction is solution time and space required to represent a policy and value function; but none come with guarantees of such reductions except in certain special cases. While most of the methods described provide approximate solutions (often with error bounds provided), some of them o er optimality guarantees in general, and most can provide optimal solutions under suitable assumptions. One avenue that has not been explored in detail is the relationship between the structured solution methods developed for MDPs described above and techniques used for solving Bayesian networks. Since many of the algorithms discussed in this section rely on the structure inherent in the 2TBN representation of the MDP, it is natural to ask whether they embody some of the intuitions that underlie solution algorithms for Bayes nets, and thus whether the solution techniques for Bayes nets can be (directly or indirectly) applied to MDPs in ways that give rise to algorithms similar to those discussed here. This remains an open question at this point, but undoubtedly some strong ties exist. Tatman and Shachter (1990) have explored the connections between in uence diagrams and MDPs.", "startOffset": 53, "endOffset": 1255}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions of reward functions. Many of these methods can, in fortunate circumstances, o er exponential reduction is solution time and space required to represent a policy and value function; but none come with guarantees of such reductions except in certain special cases. While most of the methods described provide approximate solutions (often with error bounds provided), some of them o er optimality guarantees in general, and most can provide optimal solutions under suitable assumptions. One avenue that has not been explored in detail is the relationship between the structured solution methods developed for MDPs described above and techniques used for solving Bayesian networks. Since many of the algorithms discussed in this section rely on the structure inherent in the 2TBN representation of the MDP, it is natural to ask whether they embody some of the intuitions that underlie solution algorithms for Bayes nets, and thus whether the solution techniques for Bayes nets can be (directly or indirectly) applied to MDPs in ways that give rise to algorithms similar to those discussed here. This remains an open question at this point, but undoubtedly some strong ties exist. Tatman and Shachter (1990) have explored the connections between in uence diagrams and MDPs. Kjaerul (1992) has investigated computational considerations involved in applying join tree methods for reasoning tasks such as monitoring and prediction in temporal Bayes nets.", "startOffset": 53, "endOffset": 1336}], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}