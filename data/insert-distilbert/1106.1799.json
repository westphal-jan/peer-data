{"id": "1106.1799", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Finding a Path is Harder than Finding a Tree", "abstract": "i consider the problem of learning an optimal path graphical model from data and later show the problem to be np - hard for the maximum partial likelihood and minimum accuracy description length approaches and a bayesian approach. this plausible hardness result holds despite documenting the fact that the problem is a restriction of the polynomially solvable inference problem of finding the optimal tree graphical model.", "histories": [["v1", "Thu, 9 Jun 2011 13:13:51 GMT  (57kb)", "http://arxiv.org/abs/1106.1799v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c meek"], "accepted": false, "id": "1106.1799"}, "pdf": {"name": "1106.1799.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "Finding a Path is Harder than Finding a Tree", "text": "Christopher Meek meek mi rosoft. om Mi rosoft Resear h, Redmond, WA 98052-6399 USA"}, {"heading": "Abstra t", "text": ""}, {"heading": "I onsider the problem of learning an optimal path graphi al model from data and show", "text": "the problem to be NP-hard for the maximum likelihood and minimum des ription length approa hes and a Bayesian approa h. This hardness result holds despite the fa t that the problem is a restri tion of the polynomially solvable problem of nding the optimal tree graphi al model."}, {"heading": "1. Introdu tion", "text": "The problem of learning graphi al models has re eived mu h attention within the Arti -\nial Intelligen e ommunity. Graphi al models are used to represent and approximate joint distributions over sets of variables where the graphi al stru ture of a graphi al model represents the dependen ies among the set of variables. The goal of learning a graphi al model is to learn both the graphi al stru ture and the parameters of the approximate joint distribution from data. In this note, I present a negative hardness result on learning optimal path graphi al models.\nPath graphi al models are an interesting lass of graphi al models with respe t to learning. This is due the fa t that, in many situations, restri ting attention to the lass of path models is justi ed on the basis of physi al onstraints or temporal relationships among the variables. One example of this is the problem of identifying the relative positions of lo i on a segment of DNA (e.g., Boehnke, Lange & Cox, 1991). In addition, one might be interested in obtaining a total order over a set of variables for other purposes su h as visualization (e.g., Ma & Hellerstein, 1999).\nThe main positive results on the hardness of learning graphi al models are for learning tree graphi al models. These have been presented for maximum likelihood (ML) riterion (Edmonds, 1967; Chow & Liu, 1968) and adapted to a Bayesian riterion by He kerman, Geiger, & Chi kering (1995). Two NP-hardness results for learning graphi al models have appeared in the literature. Those are the NP-hardness of nding the optimal Bayesian network stru ture with in-degree greater than or equal to two using a Bayesian optimality\nriterion (Chi kering, 1996) and the problem of nding the ML optimal polytree (Dasgupta,\n1999).\nIn this note, I present a proof of the hardness of nding an optimal path graphi al models for the maximum likelihood (ML) riterion, the minimum des ription length (MDL)\nriterion, and a Bayesian s oring riterion. Unlike the ML hardness result of Dasgupta, I provide an expli it onstru tion of a polynomial sized data set for the redu tion and, unlike the Bayesian hardness result of Chi kering (1996), I use a ommon \\uninformative\" prior.\n2001 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nOne of the primary goals when learning a graphi al model is to obtain an approximate joint distribution over a set of variables from data. In this note, I fo us on dire ted graphi al models for a set of dis rete variables fX\n1\n; : : : ;X\nn\ng. One omponent of a dire ted graphi al\nmodel is its dire ted graphi al stru ture that des ribes dependen ies between the variables. A dire ted graphi al model represents a family of distributions that fa tor a ording to the graphi al stru ture G of the dire ted graphi al model, more spe i ally,\nP\nG\n(X\n1\n; : : : ;X\nn\n) =\nn\nY\ni=1\nP (X\ni\njpa\nG\n(X\ni\n))\nwhere pa\nG\n(X\ni\n) denotes the possibly empty set of parents of vertex X\ni\nin graph G. The\nsubs ript G is omitted when it is lear from ontext. The most ommon methods guiding the hoi e of a distribution from a family of distributions are maximum likelihood estimation and Bayesian estimation. Given a graphi al stru ture and a set of ases for the variables (also a prior distribution over the distributions in the ase of the Bayesian approa h), these methods provide an approximate joint distribution. For more details on graphi al models and estimation see He kerman (1998).\nThis leaves open the question of how one should hoose the appropriate graphi al stru - ture. In the remainder of this se tion, I present the maximum likelihood (ML) riterion, the minimum dis rimination length (MDL) riterion, and a Bayesian riterion for evaluating dire ted graphi al models given a set of ases D. A value of the variable X\ni\nis denoted by\nx\ni\nand a value of the set of variables pa(X\ni\n) is denoted by pa(x\ni\n). The number of ases in\nD in whi h X\ni\n= x\ni\nand pa(X\ni\n) = pa(x\ni\n) is denoted by N(x\ni\n; pa(x\ni\n)) and the total number\nof ases in D is denoted by N .\nOne important property ommon to these s oring riteria is that the s ores fa tor a - ording to the graphi al stru ture of the model. That is, the s ore for a graph G and data\nset D an be written as a sum of lo al s ores for ea h of the variables\nS ore(G;D) =\nX\ni\nLo alS ore(X\ni\n; pa(X\ni\n)):\nThe lo al s ore for a variable X\ni\nis only a fun tion of the ounts for X\ni\nand pa(X\ni\n) in the\ndata set D and the number of possible assignments to the variables X\ni\nand pa(X\ni\n). Thus\nthe stru ture of the graphi al model determines whi h parti ular variables and ounts are needed in the omputation of the lo al s ore for a variable.\nThe log maximum likelihood s oring riterion for a graphi al model is\nS ore\nML\n(G;D) =\nX\ni\nLo alS ore\nML\n(X\ni\n; pa(X\ni\n))\nLo alS ore\nML\n(X\ni\n; pa(X\ni\n)) = N H\nD\n(X\ni\njpa(X\ni\n)) (1)\nwhere H\nD\n(X\ni\njpa(X\ni\n)) is the empiri al onditional entropy of X\ni\ngiven its parents, and is\nequal to\nX\nX\ni\n;pa(X\ni\n)\nN(x\ni\n; pa(x\ni\n))\nN\nlog\nN(x\ni\n; pa(x\ni\n))\nN(pa(x\ni\n))\n:\nstru ture G and G\n0\nwhere G ontains a proper subset of the edges of G\n0\nthe ML s ore will\nnever favor G. Thus, when using an ML s ore to hoose among models without restri ting the lass of graphi al stru tures, a fully onne ted stru ture is guaranteed to have a maximal s ore. This is problemati due to the potential for poor generalization error when using the resulting approximation. This problem is often alled over tting. When using this prin iple it is best to restri t the lass of alternative stru tures under onsideration in some suitable manner.\nThe minimum des ription length s ore an be viewed as a penalized version of the ML\ns ore\nS ore\nMDL\n(G;D) = S ore\nML\n(G;D)\nd logN\n2\n=\nX\ni\nLo alS ore\nMDL\n(G;D)\nLo alS ore\nMDL\n(X\ni\n; pa(X\ni\n)) =\nLo alS ore\nML\n#(pa(X\ni\n)) (#(X\ni\n) 1) logN\n2\n(2)\nwhere d =\nP\ni\n(#(pa(X\ni\n)) (#(X\ni\n) 1)) and #(Y ) is used to denote the number of possible\ndistin t assignments for a set of variables Y and the number of assignments for the empty set of variables is #(;) = 1. The penalty term leads to more parsimonious models, thus, alleviating the over tting problem des ribed above.\nFinally, a Bayesian s ore requires a prior over the alternative models and, for ea h model, a prior over the distributions. A ommonly used family of priors for dire ted graphi al models is des ribed by Cooper & Herskovits (1992). In their approa h, one assumes a uniform prior on alternative graphs, P (G) / 1, and an \\uninformative\" prior over distributions. These assumptions lead to the following s oring fun tion;\nS ore\nBayes\n(G;D) = logP (DjG) + logP (G)\n/\nX\ni\nLo alS ore\nBayes\n(X\ni\n; pa(X\ni\n))\nLo alS ore\nBayes\n(X\ni\n; pa(X\ni\n)) =\nlog\nY\npa(x\ni\n)\n(#(X\ni\n) 1)!\n(#(X\ni\n) 1) +N(pa(x\ni\n)))!\nY\nx\ni\nN(x\ni\n; pa(x\ni\n))! (3)\nAlthough not as apparent as in the MDL s ore, the Bayesian s ore also has a built-in tenden y for parsimony that alleviates the problems of over tting. The hardness results presented below an be extended to a variety of alternative types of priors in luding the BDe prior with an empty prior model (see He kerman et al. 1995).\nThe problem of nding the optimal dire ted graphi al model for a given lass of stru -\ntures G and dataD is the problem of nding the stru tureG 2 G that maximizes S ore(G;D).\nIn this se tion, I onsider the problem of nding the optimal dire ted graphi al model when the lass of stru tures is restri ted to be paths. A dire ted graphi al stru ture is a path if there is one vertex with in-degree zero and all other verti es have in-degree one. I show that the problem of nding the optimal path dire ted graphi al model is NP-hard for the ommonly used s oring fun tions des ribed Se tion 2. To demonstrate the hardness of nding optimal paths the problem needs to be formulated as a de ision problem. The de ision problem version of nding the optimal path dire ted graphi al model is as follows\nThe optimal path (OP) de ision problem: Is there a path graphi al model with s ore greater than or equal to k for data set D?\nIn this se tion I prove the following theorem.\nTheorem 1 The optimal path problem is NP-Hard for the maximum likelihood s ore, the minimum des ription length s ore and a Bayesian s ore.\nTo prove this, I redu e the Hamiltonian Path (HP) de ision problem to the OP de ision problem.\nThe Hamiltonian path (HP) de ision problem: Is there a Hamiltonian path in an undire ted graph G?\nA Hamiltonian path for an undire ted graph G is a non-repeating sequen e of verti es su h that ea h vertex in G o urs on the path and for ea h pair of adja ent verti es in the sequen e there is an edge in G. Let the undire ted graph G = hV;Ei have vertex set V = fX\n1\n; : : : ;X\nn\ng and edge set E.\nThe HP de ision problem is NP- omplete. Loosely speaking, this means that the HP de ision problem is as omputationally di\u00c6 ult as a variety of problems for whi h no known algorithm exists that runs in time that is a polynomial fun tion of the size of the input. Theorem 1 indi ates that the OP de ision problem is at least as di\u00c6 ult as any NP- omplete problem. For more information about the HP de ision problem and NP- ompleteness see Garey & Johnson (1979).\nI redu e the HP de ision problem for G to the OP de ision problem by onstru ting a\nset of ases D with the following properties;\n#(X\ni\n) = #(X\nj\n) (i)\nLo alS ore(X\ni\n; ;) = Lo alS ore(X\nj\n; ;) = (ii)\nLo alS ore(X\ni\n; fX\nj\ng) 2 f ; g < (iii)\nLo alS ore(X\nj\n; fX\ni\ng) = Lo alS ore(X\ni\n; fX\nj\ng) (iv)\nLo alS ore(X\ni\n; fX\nj\ng) = i fX\ni\n;X\nj\ng 2 E (v)\nto the existen e of a path graphi al model with s ore equal to k = + (jV j 1) where jV j = n is the number of verti es in the undire ted graph G. Thus, to redu e the HP problem to the OP problem one needs to e\u00c6 iently onstru t a polynomial sized data set with these properties. In other words, by su h a onstru tion, a general HP de ision problem an be transformed into an OP de ision problem. Be ause the size of the input to the OP problem is a polynomial fun tion of the size of the input for the HP problem, if one an nd an algorithm solve the OP problem in polynomial time then all NP- omplete problems an be solved in polynomial time.\nI onstru t a data set for graph G assuming that ea h variable is ternary to satisfy\nondition (i). For ea h pair of verti es X\ni\nand X\nj\n(i < j) for whi h there is an edge in G,\nadd the following 8 ases in whi h every variable X\nk\n(k 6= i; j) is zero.\nX\n1\n: : : X\ni 1\nX\ni\nX\ni+1\n: : : X\nj 1\nX\nj\nX\nj+1\n: : : X\nn\n0 : : : 0 1 0 : : : 0 1 0 : : : 0 0 : : : 0 1 0 : : : 0 1 0 : : : 0 0 : : : 0 1 0 : : : 0 1 0 : : : 0 0 : : : 0 1 0 : : : 0 2 0 : : : 0 0 : : : 0 2 0 : : : 0 1 0 : : : 0 0 : : : 0 2 0 : : : 0 2 0 : : : 0 0 : : : 0 2 0 : : : 0 2 0 : : : 0 0 : : : 0 2 0 : : : 0 2 0 : : : 0\nFor ea h pair of verti es X\ni\nand X\nj\n(i < j) for whi h there is not an edge in G, add the\nfollowing 8 ases.\nX\n1\n: : : X\ni 1\nX\ni\nX\ni+1\n: : : X\nj 1\nX\nj\nX\nj+1\n: : : X\nn\n0 : : : 0 1 0 : : : 0 1 0 : : : 0 0 : : : 0 1 0 : : : 0 1 0 : : : 0 0 : : : 0 1 0 : : : 0 2 0 : : : 0 0 : : : 0 1 0 : : : 0 2 0 : : : 0 0 : : : 0 2 0 : : : 0 1 0 : : : 0 0 : : : 0 2 0 : : : 0 1 0 : : : 0 0 : : : 0 2 0 : : : 0 2 0 : : : 0 0 : : : 0 2 0 : : : 0 2 0 : : : 0\nFor a set of ases onstru ted as des ribed above, the pairwise ounts for a pair of variables X\ni\nand X\nj\nonne ted by an edge in G are\nX\ni\nX\nj\n0 1 2\n0 4(n\n2\n5n+ 6) 4(n 2) 4(n 2)\n1 4(n 2) 3 1 2 4(n 2) 1 3\nThe pairwise ounts for a pair of variables X\ni\nand X\nj\nnot onne ted by an edge in G are\nX\ni\nX\nj\n0 1 2\n0 4(n\n2\n5n+ 6) 4(n 2) 4(n 2)\n1 4(n 2) 2 2 2 4(n 2) 2 2\nCondition (ii) is satis ed be ause the marginal ounts for ea h variable are identi al. There are two types of pairwise ount tables, thus, there are at most two values for a given type of pairwise Lo alS ore. By using the two pairwise ount tables and Equations 1, 2, and 3, one an easily verify that the lo al s ores for the two tables satisfy ondition (iii). It follows from the symmetry in the two types of pairwise tables and ondition (ii) that ondition (iv) is satis ed. It follows from the onstru tion that ondition (v) is satis ed. Furthermore, the set of ases is e\u00c6 iently onstru ted and has a size whi h is polynomially bounded by the size of the graph G proving the result."}, {"heading": "4. Con lusion", "text": "In this note, I show that the problem of nding the optimal path graphi al models is NPhard for a variety of ommon learning approa hes. The negative result for learning optimal path graphi al models stands in ontrast to the positive result on learning tree graphi al models. This hardness result highlights one potential sour e of the hardness. That is, one an make an easy problem di\u00c6 ult by hoosing an inappropriate sub lass of models. Perhaps, by arefully hoosing a broader lass of models than tree graphi al models one an identify interesting lasses of graphi al models for whi h the problem of nding an optimal model is tra table.\nAnother interesting lass of graphi al models not des ribed in this note is the lass of undire ted graphi al models (e.g., Lauritzen, 1996). The methods for learning undire ted graphi al models are losely related to the methods des ribed in Se tion 2. In fa t, for the\nase of undire ted path models, the s oring formulas des ribed in Se tion 2 are identi al for ea h of the ommon approa hes. Therefore, the NP-hardness result for dire ted path models presented in this note also applies to problem of learning undire ted path models.\nFinally, it is important to note that good heuristi s exist for the problem of nding weighted Hamiltonian paths (Karp & Held, 1971). These heuristi s an be used to identify good quality path models and rely on the fa t that the optimal tree model an be easily found and will have a s ore at least as large as any path model."}, {"heading": "Referen es", "text": "Boehnke, M., Lange, K., & Cox, D. (1991). Statisti al methods for multipoint radiation\nhybrid mapping. Ameri an Journal of Human Geneti s, 49, 1174{1188.\nChi kering, D. (1996). Learning Bayesian networks is NP- omplete. In Fisher, D., & Lenz,\nH. (Eds.), Learning from Data, pp. 121{130. Springer-Verlag.\nChow, C., & Liu, C. (1968). Approximating dis rete probability distributions with depen-\nden e trees. IEEE Transa tions on Information Theory, 14, 462{467.\nnetworks from data. Ma hine Learning, 9, 309{347.\nDasgupta, S. (1999). Learning polytrees. In Pro eedings of the Fifteenth Conferen e on\nUn ertainty in Arti ial Intelligen e, Sto kholm, Sweden, pp. 134{141. Morgan Kaufmann.\nEdmonds, J. (1967). Optimum bran hing. J. Res. NBS, 71B, 233{240.\nGarey, M., & Johnson, D. (1979). Computers and intra tability: A guide to the theory of\nNP- ompleteness. W.H. Freeman, New York.\nHe kerman, D. (1998). A tutorial on learning with Bayesian networks. In Jordan, M. (Ed.),\nLearning in Graphi al Models, pp. 301{354. Kluwer A ademi Publishers.\nHe kerman, D., Geiger, D., & Chi kering, D. (1995). Learning Bayesian networks: The\nombination of knowledge and statisti al data. Ma hine Learning, 20, 197{243.\nKarp, R., & Held, M. (1971). The traveling-salesman problem and minimum spanning trees:\nPart ii. Mathemati al Programming, 1, 6{25.\nLauritzen, S. (1996). Graphi al Models. Oxford University Press.\nMa, S., & Hellerstein, J. (1999). Ordering ategori al data to improve visualization. In\nPro eedings of the IEEE Symposium on Information Visualization, pp. 15{17."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "The problem of learning graphi al models has re eived mu h attention within the Arti ial Intelligen e ommunity. Graphi al models are used to represent and approximate joint distributions over sets of variables where the graphi al stru ture of a graphi al model represents the dependen ies among the set of variables. The goal of learning a graphi al model is to learn both the graphi al stru ture and the parameters of the approximate joint distribution from data. In this note, I present a negative hardness result on learning optimal path graphi al models.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}