{"id": "1106.1818", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Inducing Interpretable Voting Classifiers without Trading Accuracy for Simplicity: Theoretical Results, Approximation Algorithms", "abstract": "recent human advances in the study of voting classification generating algorithms naturally have simply brought empirical and theoretical results clearly after showing the discrimination power of ensemble classifiers. it has been previously argued that hence the search of hiding this classification power in the design of the algorithms has marginalized the need to obtain interpretable classifiers. therefore, the additional question of whether one might have to dispense with interpretability effectively in order longer to keep classification statistical strength is being raised in a growing number of machine learning or data chain mining papers. the purpose of this paper is to study as both theoretically and empirically the problem.. first, we provide numerous results indirectly giving insight into the hardness of the simplicity - accuracy tradeoff for voting classifiers. then we provide an efficient \" top - down cost and prune \" induction heuristic, dubbed widc, mainly derived derived from recent results on the weak learning and boosting frameworks. third it is to our knowledge the first attempt to entirely build a voting classifier as a base formula using the weak learning framework ( the one which was previously highly successful for decision tree learning induction ), and not the strong learning framework ( as usual for such classifiers with distributed boosting - like approaches ). while it uses a seemingly well - known induction enhancement scheme previously successful in other different classes of concept representations, thus making it easy to implement and compare, widc sometimes also relies on recent authors or somewhat new results we give about particular cases of boosting matrices known as partition boosting and ranking loss constraint boosting. experimental results on thirty - one domains, most of which readily available, tend to display the ability of widc to produce small, accurate, and interpretable partial decision committees.", "histories": [["v1", "Thu, 9 Jun 2011 13:56:01 GMT  (160kb)", "http://arxiv.org/abs/1106.1818v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["r nock"], "accepted": false, "id": "1106.1818"}, "pdf": {"name": "1106.1818.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": "Indu ing Interpretable Voting Classi ers without Trading", "text": ""}, {"heading": "A ura y for Simpli ity: Theoreti al Results, Approximation", "text": ""}, {"heading": "Algorithms, and Experiments", "text": "Ri hard No k rno k martinique.univ-ag.fr Universit e Antilles-Guyane Grimaag-D epartement S ienti que Interfa ultaire Campus Universitaire de S hoel her B.P. 7209 97275 S hoel her, Martinique, Fran e"}, {"heading": "Abstra t", "text": "Re ent advan es in the study of voting lassi ation algorithms have brought empiri al and theoreti al results learly showing the dis rimination power of ensemble lassi ers. It has been previously argued that the sear h of this lassi ation power in the design of the algorithms has marginalized the need to obtain interpretable lassi ers. Therefore, the question of whether one might have to dispense with interpretability in order to keep\nlassi ation strength is being raised in a growing number of ma hine learning or data mining papers. The purpose of this paper is to study both theoreti ally and empiri ally the problem. First, we provide numerous results giving insight into the hardness of the simpli ity-a ura y tradeo for voting lassi ers. Then we provide an e\u00c6 ient \\top-down and prune\" indu tion heuristi , WIDC, mainly derived from re ent results on the weak learning and boosting frameworks. It is to our knowledge the rst attempt to build a voting\nlassi er as a base formula using the weak learning framework (the one whi h was previously highly su essful for de ision tree indu tion), and not the strong learning framework (as usual for su h lassi ers with boosting-like approa hes). While it uses a well-known indu tion s heme previously su essful in other lasses of on ept representations, thus making it easy to implement and ompare, WIDC also relies on re ent or new results we give about parti ular ases of boosting known as partition boosting and ranking loss boosting. Experimental results on thirty-one domains, most of whi h readily available, tend to display the ability of WIDC to produ e small, a urate, and interpretable de ision\nommittees."}, {"heading": "1. Introdu tion", "text": "Re ent advan es in the study of voting lassi ation algorithms have brought empiri al and theoreti al results learly showing the dis rimination power of ensemble lassi ers (Bauer & Kohavi, 1999; Breiman, 1996; Dietteri h, 2000; Opitz & Ma lin, 1999; S hapire & Singer, 1998). These methods basi ally rely on voting the de ision of individual lassi ers inside an ensemble. It is widely a epted, and formally proven in ertain ases (S hapire, Freund, Bartlett, & Lee, 1998; S hapire & Singer, 1998), that their power a tually relies on the ability to build potentially very large lassi ers. It has even been observed experimentally that su h an ensemble an sometimes be as large as (or larger than) the data used to build the ensemble (Margineantu & Dietteri h, 1997) ! Then, a simple question arises, namely what is the interest a ustomer an have in using su h a lassi er, instead of simple\n2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nlookups in the data, and using algorithms su h as nearest neighbor lassi ers (Margineantu & Dietteri h, 1997) ?\nAfter some of the most remarkable re ent studies in voting lassi ation algorithms, some authors have pointed out the interest to bring this lassi ation power to data mining, and more pre isely to make interpretability a lear issue in voting lassi ation algorithms (Bauer & Kohavi, 1999; Ridgeway, Madigan, Ri hardson, & O'Kane, 1998). Some authors go even further, and argue that the importan e of interpretability has been marginalized in the design of these algorithms, and put behind the need to devise lassi ers with strong\nlassi ation power (Ridgeway et al., 1998). But interpretability also governs the quality of a model by providing answers to how it is working, and, most importantly, why. A ording to Bauer & Kohavi (1999), striving for omprehensibility in voting models is one of the prin ipal problems requiring future investigations. They also remark that \\voting te hniques usually result in in omprehensible lassi ers that annot easily be shown to users\".\nComprehensibility is, on the other hand, a hard mining issue (Buja & Lee, 2001) : it depends on parameters su h as the type of lassi ers used, the algorithm indu ing the\nlassi ers, the user mining the outputs, et . . Though the quanti ation of interpretability is still opened in the general ase (Buja & Lee, 2001), there are some lues oming from theory and pra ti e of ma hine learning/data mining indi ating some potentially interesting requirements and ompromises to devise an e\u00c6 ient learning/mining algorithm.\nA rst requirement for the algorithm is obviously its generalization abilities: without lassi ation strength, it is pointless to sear h for interesting models of the data. A se ond requirement, more related to mining, is the size of the lassi ers (No k & Gas uel, 1995; No k & Jappy, 1998). If a urate, a lassi er with restri ted size an lead to faster and deeper understanding. This is obviously not an absolute rule, rather an approximate proxy for interpretability : pathologi ases exist in whi h, for example, a large and unbalan ed tree an be very simple to understand (Buja & Lee, 2001). Note that in this example, the authors explain that the tree is simple be ause all its nodes an be des ribed using few\nlauses. Therefore, simpli ity is also asso iated to a short des ription, but using a parti ular lass of on ept representation.\nA third parameter in uen ing omprehensibility is the nature of the algorithm's output. Inside the broad s ope of symboli lassi ers, some lasses of on ept representations appear to o er a greater omfort for interpretation. De ision trees belong to this set (Breiman, Freidman, Olshen, & Stone, 1984), though they also raise some interpretability problems : Kohavi & Sommer eld (1998) quote that\n\\the lients [business users\u2104 found some interesting patterns in the de ision trees, but they did not feel the stru ture was natural for them. They were looking for those two or three attributes and values (e.g. a ombination of geographi and industries) where something \\interesting\" was happening. In addition, they felt it was too limiting that the nodes in a de ision tree represent rules that all start with the same attributes.\"\nAlthough not limiting from a lassi ation viewpoint, the ordering of nodes prior to lassi ation an therefore make it un omfortable to mine a de ision tree. Noti e that this problem might hold for any lass of on ept representation integrating an ordering prior to lassi ation: de ision lists (Rivest, 1987), alternating de ision trees (Freund & Mason,\n1999), bran hing programs (Mansour & M Allester, 2000), et . . There exists, however, a type of lassi ers on whi h related papers appear to be generally unanimous on their mining abilities : disjun tive normal form formulas (DNFs, and their numerous extensions), that is, disjun tions of onjun tions. Interestingly enough, this is the lass whi h motivated early works (and a great amount of works afterwards) on the well-known PAC theory of learning (Valiant, 1984, 1985), partly be ause of the tenden y humans seem to have to represent knowledge using similarly shaped rules (Valiant, 1985). This lass is also the dual of the one impli itly used by Buja & Lee (2001) to ast their size measure for de ision trees (to state whether the on ept represented is simple or not).\nIt is our aim in this paper to propose theoreti al results and approximation algorithms related to the indu tion of very parti ular voting lassi ers, drawing their roots on simple rule sets (like DNF), with the obje tive to keep a tradeo between simpli ity and a ura y. Our aim is also to prove that, in the numerous indu tion algorithms already proposed throughout the ma hine learning and data mining ommunities, some of them, previously used in de ision trees and de ision lists indu tion, an be easily adapted to ope with this obje tive, thereby leading to easy-to-implement (and ompare) algorithms. The next se tion presents a synthesis of our ontribution, whi h is detailed in the rest of the paper."}, {"heading": "2. Our Contribution", "text": "This paper is prin ipally on erned with the theoreti al and experimental study of a set of voting lassi ers whi h we think is likely to provide an a urate answer to the simpli itya ura y tradeo : de ision ommittees (DC) (No k & Gas uel, 1995). DC is informally the Boolean multi lass extension of polynomial dis riminant fun tions. A de ision ommittee\nontains rules, ea h of these being a pair (monomial, ve tor). Ea h monomial is a ondition that, when red, returns its ve tor. After ea h monomial has been tested, the sum of the returned ve tors is used to take the de ision. This additive fashion for ombining rules is absent from lassi al Boolean lassi ers su h as De ision Trees (DT) or De ision Lists (DL). Furthermore, unlike these two latter lasses, the lassi er ontains absolutely no ordering, neither on variables (unlike DT), nor on monomials (unlike DL). When su\u00c6 iently small DCs are built and adequate restri tions are taken, a new dimension in interpreting the lassi er is obtained, whi h does not exist for DT or DL. Namely, any example an satisfy more than one rule, and a DC an therefore be interpreted by means of various rule subsets (in a naive onversion of a DT or a DL into rule sets, any example satis es exa tly one rule). De ision ommittees resemble or generalize other rule sets (Cohen & Singer, 1999). In this paper, the authors onsider DNF-shaped formulas, in whi h the output of a monomial is not a lass ( alled \\positive\"), but a (non-negative) on den e in the\nlassi ation as positive. A default lass predi ts the other lass, alled \\negative\" (this is a setting with two lasses). Computing the lass of an observation boils down to summing the\non den es of the rules it satis es, and then de iding the positive lass if the sum is greater than zero, and the negative lass otherwise. De ision ommittees are a generalization of these formulas, in whi h we remove the setting's onstraint (two lasses) and authorize the membership predi tion to arbitrary lasses, thereby leading to a true voting lassi er. This voting fashion is a feature that de ision ommittees share with de ision tables (Kohavi & Sommer eld, 1998). However, de ision tables lassi ers are based on majority voting of the\nexamples (and not of rules), over a restri ted \\window\" of the des ription variables. They ne essitate the storing of many examples, and the interpretations of the data an only be made through this window, a ording to this potentially large set of examples. De ision\nommittees rather represent an e\u00c6 ient way to en ode a voting method into a small number of rules, and the way a lass is given an be brought ba k to early works in ma hine learning (Clark & Boswell, 1991). More formal details are provided in the next se tion.\nAmong our theoreti al results, that are presented in the following se tion, we provide formal proofs that the simpli ity-a ura y tradeo is also hard to a hieve for DC, as well as for the onstru tion of omplex votes involving DT. This last result shows that, while mixing C4.5 with boosting provides one of the most powerful lassi ation algorithms (Friedman, Hastie, & Tibshirani, 2000), pruning boosting is essentially heuristi (Margineantu & Dietteri h, 1997).\nThe algorithm we propose for the indu tion of DC, WIDC (for Weak Indu tion of De ision Committees), has the following key features. It uses re ent results on partition boosting, ranking loss boosting (S hapire & Singer, 1998) and some about pruning Boolean formulas (Kearns & Mansour, 1998). WIDC follows a s heme lose to C4.5's for de ision trees (Quinlan, 1994), or ICDL's for de ision lists (No k & Jappy, 1998) ; as su h, it di ers from previous studies in voting lassi ers (boosting, bagging (Breiman, 1996)) by features su h as the fa t that no modi ation is made on the example's distribution during indu tion. It is also one if its di eren es with the SLIPPER rule indu tion approa h (Cohen & Singer, 1999).\nOn multi lass and multilabel problems,WIDC proposes a very fast and simple solution to ranking loss boosting, optimal in fairly general ases, and asymptoti ally optimal in most of the remaining ones. The general problem of ranking loss boosting was previously onje - tured NP -Hard (S hapire & Singer, 1998). Though our ranking loss boosting algorithm is not always optimal, we also show that the general ranking loss boosting problem related to S hapire & Singer (1998) is a tually not NP -Hard, and an be solved in polynomial time, though it seems to require the use of omplex and time-expensive algorithms, related to the minimization of (symmetri ) submodular fun tions. This also partially justi es the use of our simple and fast approximation algorithm.\nThe last se tion of this paper presents experimental results obtained with WIDC on thirty-one domains, most of whi h are readily available and an be found on the UCI repository of ma hine learning database (Blake, Keogh, & Merz, 1998).\nIn order to keep the paper self- ontained and as on ise and readable as possible, we have hosen to put an appendix at the end of the paper ontaining all proofs of our results."}, {"heading": "3. De ision Committees", "text": "Let be the number of lasses. Unless otherwise spe i ed, an example e is a ouple e = (o;\no\n) where o is an observation des ribed over n variables, and\no\nits orresponding lass\namong f0; 1; :::; 1g ; to ea h example (o;\no\n) is asso iated a weight w((o;\no\n)), representing\nits appearan e probability with respe t to a learning sample LS whi h we dispose of. LS is itself a subset of a whole domain whi h we denote X . Obviously, we do not have entire a ess to X (LS X ) : in general, we even have jLSj jX j (j:j denotes the ardinality; we suppose in all that follows that X is dis rete with nite ardinality). In the parti ular ase\nwhere = 2, the two lasses are noted \\-\" (\no\n= 0) and \\+\" (\no\n= 1), and alled respe tively\nthe negative and positive lass. The learning sample is the union of two samples, noted LS and LS + , ontaining respe tively the negative and positive examples. It is worthwhile to think the positive examples as belonging to a subset of X ontaining all possible positive examples, usually alled the target on ept.\nAs part of our goal in ma hine learning, is the need to build a reliable approximation to the true lassi ation of the examples in X , that is, a good approximation of the target\non ept, by using only the examples in LS. Good approximations shall have a high a ura y over X , although we do not have a ess to this quantity, but rather to its estimator: a more or less reliable a ura y omputable over LS. We refer the reader to standard ma hine learning books (Mit hell, 1997) for further onsiderations about this issue. A DC ontains two parts:\nA set of unordered pairs (or rules) f(t\ni\n; ~v\ni\n)g\ni=1;2;:::\nwhere ea h t\ni\nis a monomial (a on-\njun tion of literals) over fx\n1\n; x\n1\n; x\n2\n; x\n2\n; :::; x\nn\n; x\nn\ng\nn\n(n being the number of des ription\nvariables, ea h x\nj\nis a positive literal and ea h x\nj\nis a negative literal), and ea h ~v\ni\nis a ve tor in IR . For the sake of readability, this ve torial notation shall be kept throughout all the paper, even for problems with only two lasses. One might hoose to add a single real rather than a 2- omponent ve tor in that ase.\nA Default Ve tor\n~ D in [0; 1\u2104 . Again, in the two- lass ase, it is su\u00c6 ient to repla e\n~ D by a default lass in f+; g.\nFor any observation o and any monomial t\ni\n, the proposition \\o satis es t\ni\n\" is denoted\nby o ) t\ni\n. The opposite proposition \\o does not satisfy t\ni\n\" is denoted by \\o 6) t\ni\n\". The\nlassi ation of any observation o is made in the following way: de ne\n~ V\no\nas follows\n~ V\no\n=\nX\n(t\ni\n; ~v\ni\n)\no) t\ni\n~v\ni\n:\nThe lass assigned to o is then:\nargmax\nj\n~ V\no\nif j argmax\nj\n~ V\no\nj = 1, and\nargmax\nj2argmax\nj\n0\n~ V\no\n~ D otherwise.\nIn other words, if the maximal omponent of\n~ V\no\nis unique, then the index gives the lass\nassigned to o. Otherwise, we take the index of the maximal omponent of\n~ D orresponding\nto the maximal omponent of\n~ V\no\n(ties are solved by a random hoi e among the maximal\nomponents).\nDC ontains a sub lass whi h is among the largest lasses of Boolean formulas to be PAC-learnable (No k & Gas uel, 1995), however this lass is less interesting from a pra ti al viewpoint sin e rules an be numerous and hard to interpret. Nevertheless, a sub lass of DC (No k & Gas uel, 1995) presents an interesting ompromise between representational power and interpretability power. In this lass, whi h is used by WIDC, ea h of the ve tor omponents are restri ted to f 1; 0;+1g and ea h monomial is present at most on e.\nThe values 1, 0, +1 allow natural interpretations of the rules, being either in favor of the orresponding lass (+1), neutral with respe t to the lass (0), or in disfavor of the\norresponding lass ( 1). This sub lass, to whi h we relate as DC\nf 1;0;+1g\n, is, as we now\nprove, su ering the same algorithmi drawba ks as DT (Hya l & Rivest, 1976) and DL (No k & Jappy, 1998): even without restri ting the omponents of the ve tors, or with any restri tion to a set ontaining at least one real value, the onstru tion of small formulas with su\u00c6 iently high a ura y is hard. This is a lear motivation for using heuristi s in de ision ommittee's indu tion."}, {"heading": "4. Building Small A urate De ision Committees (and Alike) is Hard", "text": "We now show that building de ision ommittees is a hard algorithmi task when one strives to obtain both small and a urate formulas. There are two usual notions of size whi h an naturally be used for de ision ommittees. The rst one is the whole number of literals of the formula (if a literal is present i times, it is ounted i times) (No k & Gas uel, 1995; No k & Jappy, 1998), the se ond one is the number of rules of the formula (Kearns, Li, Pitt, & Valiant, 1987). Our results imply that regardless of the restri tion over the values of the ve tors (as long as they are elements of a set with ardinality 2), and already for two- lasses problems, minimizing the size of a de ision ommittee for both size de nitions is as hard as solving well-known NP -Hard problems. Therefore, the task is also hard for DC\nf 1;0;+1g\nwith the parti ular values 1, 0, +1 for the ve tors."}, {"heading": "4.1 The Size of a DC is Measured as its Whole Number of Literals", "text": "Theorem 1 When the size of a DC is measured as its whole number of literals, it is NP - Hard to nd the smallest de ision ommittee onsistent with a set of examples LS.\nProof: See the Appendix.\nWe an easily adapt Theorem 1 to the ase where the rules are repla ed by weighted DT as advo ated in boosted C4.5 (S hapire & Singer, 1998). Here, ea h tree returns a\nlass 2 f+1; 1g, and ea h tree is given a real weight to leverage its vote. The sign of the linear ombination gives the lass of an example. The following theorem holds again with any limitations on the leveraging oe\u00c6 ients (as long as at least one non-zero value is authorized), or without limitation on the oe\u00c6 ients. By this, we mean that for ea h of the appli able limitations (or without), the problem is NP -Hard. The size notion is the sum, over all trees, of their number of nodes.\nTheorem 2 It is NP -Hard to nd the smallest weighted linear ombinations of DT onsistent with a set of examples LS, without limitation on the leveraging oe\u00c6 ients, or for any possible limitation, as long as at least one non-zero value is authorized.\nWhile it is well known that boosting results in a rapid de reasing of the error over LS whi h an easily and rapidly drop down to zero (as long as it is possible), Theorem 2 shows that attempts to e\u00c6 iently redu e the size of the vote when boosting DT is NP -Hard. If the problem is simpli ed to the to pruning of a large onsistent vote of DT (Margineantu & Dietteri h, 1997), to obtain a smaller onsistent (or with limited error) vote with restri ted size, it is again possible (using the same redu tion) to show that this brings NP -Hardness."}, {"heading": "4.2 The Size of a DC is Measured as its Number of Rules", "text": "We now state and prove the equivalent of Theorem 1 with this new size notion.\nTheorem 3 When the size of a DC is measured as its number of rules, it is NP -Hard to nd the smallest de ision ommittee onsistent with a set of examples LS. The result holds even when the on ept labeling the examples is a monotone-DNF formula, that is, a disjun tion of onjun tion (DNF), ea h without negative literals.\nProof: See the Appendix.\nA previous work (Kearns et al., 1987) proves a similar theorem on erning the minimization of the size of a DNF. Theorem 3 an be shown to be more general, as the lass of DC\nf 1;0;+1g\nwith two rules stri tly ontains that of DNF with two monomials.\nThe statement of Theorems 1, 2, 3 as optimization problems was hosen for pure onvenien e ; repla ing them by their asso iated de ision problems (de ide whether there exist a onsistent formula whose size is no more than some xed threshold) would trivially make the problems not only NP -Hard, but also NP -Complete.\n5. Overview of WIDC\nAn algorithm, IDC, was previously proposed (No k & Gas uel, 1995) for building de ision\nommittees. It pro eeds in two stages. The rst stage builds a potentially large subset\nof di erent rules, ea h of whi h is a tually a DC\nf 1;0;+1g\nwith only one rule. In a se ond\nstage, it gradually lusters the de ision ommittees, using the property that the union of two DC\nf 1;0;+1g\ns with di erent rules is still a DC\nf 1;0;+1g\n. At the end of this pro edure, the\nuser obtains a set of DCs, and the most a urate one is hosen and returned. Experimental results display the ability of IDC to build small DCs. In that paper, we provide an algorithm for learning de ision ommittees whi h has a di erent stru ture sin e it builds only one DC. More pre isely, WIDC is a three stage algorithm. It rst builds a set of rules derived from results on boosting de ision trees (S hapire & Singer, 1998). It then al ulates the ve tors using a s heme derived from Ranking loss boosting (S hapire & Singer, 1998). It nally prunes the nal DC\nf 1;0;+1g\nusing two possible s hemes: a natural pruning whi h we all\n\\pessimisti pruning\", and pruning using lo al onvergen e results (Kearns & Mansour, 1998), whi h we all \\optimisti pruning\". The default ve tor is always hosen to be the observed distribution of ambiguously lassi ed examples."}, {"heading": "5.1 Building a Large De ision Committee using Partition Boosting", "text": "Suppose that the hypothesis (not ne essarily a de ision ommittee, it might be e.g. a de ision tree) we build realizes a partition of the domain X into disjoint subsets X\n1\n;X\n2\n; :::;X\nN\n.\nFix as [[ \u2104\u2104 the fun tion returning the truth value of a predi ate . De ne\nW\nj;l + =\nX\n(o;\no\n)2LS\nw((o;\no\n))[[(o;\no\n) 2 X\nj\n^\no\n= l\u2104\u2104 ;\nW\nj;l\n=\nX\n(o;\no\n)2LS\nw((o;\no\n))[[(o;\no\n) 2 X\nj\n^\no\n6= l\u2104\u2104 :\nIn other words, W\nj;l + represents the fra tion of examples of lass l present in subset X j ,\nand W\nj;l\nrepresents the fra tion of examples of lasses 6= l present in subset X\nj\n. A ording\nto S hapire & Singer (1998), a weak learner should minimize the riterion:\nZ = 2\nX\nj\nX\nl\nq\nW\nj;l + W j;l : (1)\nIn the ase of a de ision tree, the partition is that whi h is built at the leaves of the tree (Quinlan, 1994) ; in the ase of a de ision list, the partition is that whi h is built at ea h rule, to whi h we add the subset asso iated to the default lass (No k & Jappy, 1998). Suppose that we en ode the de ision tree in the form of a subset of monomials, by taking for ea h leaf the logi al-^ of all attributes from the root to the leaf. Measuring Z over the tree's leaves is equivalent to measure Z over the partition realized by the set of monomials. However, the monomials are disjoint from ea h other (ea h example satis es exa tly one monomial). Due to this property, only t subsets an be realized with t monomials, or equivalently with a tree having t leaves.\nSuppose that we generalize this observation by removing the disjointness ondition over\nthe monomials. Then a number of subsets of order O(2\nt\n) is now possible with only t\nmonomials, and it appears that the number of realized partitions an be exponentially larger using de ision ommittees than de ision trees. However, the expe ted running time is not bigger when using de ision ommittees, sin e the number of partitions is in fa t bounded by the number of examples, jLSj. Thus, we may expe t some redu tion in the size of the formula we build when using de ision ommittee, whi h is of interest to interpret the\nlassi er obtained.\nAppli ation of this prin iple in WIDC is straightforward: a large de ision ommittee is built by growing iteratively, in a top-down fashion, a urrent monomial. In this monomial, the literal added at the urrent step is the one whi h minimizes the urrent Z riterion, over all possible addition of literals, and given that the new monomial does not exist already in the urrent de ision ommittee (in order to prevent multiple additions of a single monomial). The Z riterion is omputed using the partition indu ed over LS by the urrent set of monomials built (if two examples satisfy the same monomials, they belong to the same subset of the partition). When no further addition of a literal de reases the Z value, a new monomial is reated and initialized at ;, and then is grown using the same prin iple. When no further reation of a monomial de reases the Z value, the algorithm stops and returns the urrent, large de ision ommittee with still empty ve tors. In the following step, WIDC al ulates these ve tors. In a previous approa h to building rule sets for problems with two lasses (Cohen & Singer, 1999), an iterative growing-pruning algorithm is designed (SLIPPER). The rule-growing approa h of SLIPPER is ertainly lose to whatWIDC does for growing a DC sin e it optimizes a Z riterion, yet a notable di eren e is that it does not ompute Z over a partition indu ed by a set of rules. Rather, the hoi e of SLIPPER is to grow at ea h step a single monomial, prune it, and then grow a se ond monomial, prune it, and so on until a nal DNF-shaped formula is omplete and returned. Noti e that SLIPPER also modi es the weight of the examples, in a ordan e with Boosting's standards (S hapire & Singer, 1998)."}, {"heading": "5.2 Cal ulating Rule Ve tors using Ranking Loss Boosting", "text": "S hapire & Singer (1998) have investigated lassi ation problems where the aim of the pro edure is not to provide an a urate lass for some observation. Rather, the algorithm outputs a set of values (one for ea h lass) and we expe t the lass of the observation to re eive the largest value of all, thus being ranked higher than all others. This approa h is parti ularly useful when a given example an belong to more than one lass (multilabel problems), a ase where we expe t ea h of these lasses to re eive the greatest values\nompared to the lasses the examples does not belong to.\nThe ranking loss represents informally the number of times the hypothesis fails to rank the lass of an example higher than a lass to whi h it does not belong. Before going further, we rst generalize our lassi ation setting, and repla e the ommon notation (o;\no\n) for an\nexample by the more general one (o;~\no\n). Here, ~\no\n2 f0; 1g is a ve tor giving, for ea h lass,\nthe membership to the lass (\\0\" is no and \\1\" is yes) of the orresponding observation o. It is important to note that this setting is more general than the usual Bayesian setting, in whi h there an exist examples (o;\no\n) and (o\n0\n;\no\n0\n) (using the non-ve tor notation) for\nwhi h o = o\n0\nbut\no\n6=\no\n0\n. Ranking loss generalizes Bayes to the multilabel problems, and\npostulates that there an be some examples for whi h we annot provide a single lass at a time, even if e.g. any of the lasses to whi h the example belongs are sus eptible to appear independently later with the same observation.\nRanking loss Boosting repla es ea h example (o;~\no\n) by a set of 1\n~\no\n( 1\n~\no\n) examples,\nwhere 1\n~\no\ndenotes the Hamming weight of ~\no\n(i.e. the number of lasses to whi h the\nexample belongs). Ea h of these new examples is denoted (o; k; j), where j and k span all values in f0; 1; :::; 1g 2 . The distribution of the new examples is renormalized, so that\nw((o; k; j)) =\nw((o;~\no\n))\n1\n~\no\n( 1\n~\no\n)\nwhenever ~\no\n[j\u2104 = 1 and ~\no\n[k\u2104 = 0, and 0 otherwise.\nTake some monomial t obtained from the large DC, and all examples satisfying it. We now work with this restri ted subset of examples, while al ulating the orresponding ve tor ~v of t. S hapire & Singer (1998) propose a ost fun tion whi h we should minimize in order to minimize the ranking loss. This fun tion is\nZ =\nX\no;j;k\nw((o; k; j)) e\n1 2 (~v[j\u2104 ~v[k\u2104)\n: (2)\nHere, is a tunable parameter whi h, intuitively, represents the on den e in the hoi e of ~v, and leverages its quality. The better ~v is at lassifying examples, the larger is j j. In our ase however, authorizing 6= 1 is equivalent to authorizing omponents for ~v in sets f x; 0; xg for arbitrary x. To really onstrain the omponents of ~v in f 1; 0;+1g, we have\nhosen to optimize the riterion\nZ =\nX\no;j;k\nw((o; k; j)) e\n1 2 (~v[j\u2104 ~v[k\u2104)\n(3)\n(therefore for ing = 1). S hapire & Singer (1998) onje ture that nding the optimal ve tor minimizing Z in eq. (2) (whi h is similar to an oblivious hypothesis a ording to their de nitions), or Z given a parti ular value of , is NP -Hard when is not xed, and when the omponents of ~v are in the set f 1;+1g. The following se tion addresses dire tly the setting of S hapire & Singer (1998), and presents omplexity-theoreti results showing\nthat the minimization of Z is a tually polynomial, but highly ompli ated to a hieve, all the more for what it is supposed to bring to the minimization of Z in our setting. A striking result we also give, not related to the purpose of the paper, is that it is a tually the maximization of Z whi h is NP -Hard.\nThen, we present the approximation algorithm we have built and implemented to optimize the omputation of ~v in our setting ( omponents of ~v in the set f 1; 0;+1g), along with its properties. While we feel that the ideas used to minimize Z in the setting of S hapire & Singer (1998) an be adapted to our setting to provide an algorithm that is always optimal, our algorithm has the advantage to be simple, fast, and also optimal for numerous ases. In many other ases, we show that it is still asymptoti ally optimal as in reases."}, {"heading": "5.2.1 Optimizing Z in the Setting of S hapire & Singer (1998)", "text": "In the ase where ea h omponent of ~v is restri ted to the set f 1;+1g, S hapire & Singer (1998) give a way to hoose to minimize Z for any possible hoi e of ~v (using our notation):\n=\n1\n2\nlog\nW\n+\nW\n!\n; (4)\nwith:\nW\n+\n=\nX\no;k;j\nw((o; k; j))[[~v[j\u2104 ~v[k\u2104 = 2\u2104\u2104 ; (5)\nW\n=\nX\no;k;j\nw((o; k; j))[[~v[j\u2104 ~v[k\u2104 = 2\u2104\u2104 : (6)\nRepla ing this value of in eq. (2), gives the following new expression for Z:\nZ = W\n0\n+ 2\np\nW\n+\nW ; (7)\nwith W\n0\n=\nP\no;k;j\nw((o; k; j))[[~v [j\u2104 ~v[k\u2104 = 0\u2104\u2104. S hapire & Singer (1998) raise the prob-\nlem of minimizing Z as de ned in equations (2) and (7). We now show that it is polynomial.\nTheorem 4 Minimizing Z as de ned either in equations (2), (3) or (7) is polynomial when the omponents of ~v\ni\nare restri ted to the set f 1;+1g.\nProof: See the Appendix.\nA rather striking result given the onje ture of S hapire & Singer (1998) is that it is the maximization of Z, and not its minimization, whi h is NP -Hard. While this is not the purpose of the present paper (we are interested in minimizing Z), we have hosen to give here a brief proof sket h of the result, whi h uses lassi al redu tions from well-known NP -Hard problems.\nTheorem 5 Maximizing Z as de ned either in equations (2), (3) or (7) is NP -Hard when the omponents of ~v\ni\nare restri ted to the set f 1;+1g.\nProof sket h: See the Appendix."}, {"heading": "5.2.2 Optimizing Z in our Setting", "text": "As previously argued in Theorem 4, minimizing Z in the setting of S hapire & Singer (1998)\nan be done optimally, but at the expense of omplex optimization pro edures, with large omplexities. One an wonder whether su h pro edures, to optimize only the omputation of ~v (a small part of WIDC), are really well worth the adaptation to our setting, in whi h more values are authorized. We are now going to show that a mu h simpler ombinatorial pro edure, with omparatively very low omplexity, an bring optimal results in fairly general situations. The most simple way to des ribe most of these situations is to make the following assumption on the examples:\n(A) Ea h example used to ompute ~v has only one \\1\" in its lass ve tor.\nA areful reading of assumption (A) reveals that it implies that ea h example belongs to exa tly one lass, but it does not prevent an observation to be element of more than one\nlass, as long as di erent examples sharing the same observation have di erent lasses (the \\1\" of the lass ve tors is in di erent positions among these examples). Therefore, even if it does not integrate the most general features of the ranking loss setting, our assumption still authorizes to onsider problems with non zero Bayes optimum. This is really interesting, as many ommonly used datasets fall into the ategory of our assumption, as for example many datasets of the UCI repository of Ma hine Learning database (Blake et al., 1998). Finally, even if the assumption does not hold, we show that in many of the remaining (interesting)\nases, our approximation algorithm is asymptoti ally optimal, that is, nds solutions loser\nto the minimal value of Z as in reases.\nSuppose for now that (A) holds. Our obje tive is to al ulate the ve tor ~v of some\nmonomial t. We use the shorthands W\n+ 0 ;W + 1 ; :::;W + 1 to denote the sum of weights of\nthe examples satisfying t and belonging respe tively to lasses 0; 1; :::; 1. We want to minimize Z as proposed in eq. (3). Suppose without loss of generality that\nW\n+ 0 W + 1\n::: W\n+\n1\n;\notherwise, reorder the lasses so that they verify this assertion. Given only three possible values for ea h omponent of ~v, the testing of all 3 possibilities for ~v is exponential and time- onsuming. But we an propose a very fast approa h. We have indeed\nLemma 1 81 j < k ; ~v[j\u2104 ~v[k\u2104 .\nProof: See the Appendix.\nThus, the optimal ~v does not belong to a set of ardinality 3 , but to a set of ardinality\nO(\n2\n). Our algorithm is then straightforward: simply explore this set of O(\n2\n) elements,\nand keep the ve tor having the lowest value of Z. Note that this ombinatorial algorithm has the advantage to be adaptable to more general settings in whi h l parti ular values are authorized for the omponents of ~v, for any xed l not ne essarily equal to 3. In that ase, the omplexity is larger, but limited to O( l 1 ).\nThere are slightly more general settings in whi h our algorithm remains optimal, in\nparti ular when we an ertify 8j; k, ((W\n+ j > W +\nk\n) , (8i 6= j; k : W\nj;i\n> W\nk;i\n)) _ ((W\n+ j <\nW\n+ k ) , (8i 6= j; k : W j;i > W k;i )). Here, W + x denotes the sum of weights of the examples\nbelonging at least to lass x, andW\nx;y\ndenotes the sum of weights of the examples belonging\nat least to lass x, and not belonging at least to lass y. This shows that even for some parti ular multilabel ases, our approximation algorithm an remain optimal. One an wonder if the optimality is preserved in the unrestri ted multilabel framework. We now show that, if optimality is not preserved, we an still prove the quality of our algorithm for general multilabel ases, showing asymptoti optimality as in reases.\nOur approximation algorithm is run in the multilabel ase by transforming the examples\nas follows: ea h example (o;~\no\n) for whi h 1\n~\no\n> 1 is transformed into 1\n~\no\nexamples, having\nthe same des ription o, and only one \\1\" in their ve tor, in su h a way that we span the 1\n~\no\n> 1 \\1\" of the original example. Their weight is the one of the original example, divided\nby 1\n~\no\n. We then run our algorithm on this new set of examples satisfying assumption (A).\nNow, suppose that for any example (o;~\no\n), we have 1\n~\no\nk for some k. There are two\ninteresting ve tors we use. The rst one is ~v , the optimal ve tor (or an optimal ve tor) minimizing Z over the original set of examples, the se ond one is ~v, the ve tor we nd minimizing Z over the transformed set of examples. What we want is to estimate the quality of ~v with respe t to the optimal value of Z over the original set of examples, Z(~v ) using our notation. The following theorem gives an answer to this problem, by quantifying its onvergen e towards Z(~v ).\nTheorem 6 Z(~v) < Z(~v ) 1 +\ne\nk\n.\nProof: See the Appendix.\nTherefore, in the set of all problems for whi h for some < 1, k , we obtain\nZ(~v) = (1+o(1))Z(~v ), and our bound onverges to the optimum as in reases in this lass of problems. By means of words, our simple approximation algorithm is quite e\u00c6 ient for problems with large number of lasses. Note that using a slightly more involved proof, we\nould have redu ed the onstant \\e\" fa tor in Theorem 6 to the slightly smaller \\e (1=e)\". Now, to x the ideas, the following subse tion displays the expli it (and simple) solution when there are only two lasses."}, {"heading": "5.2.3 Expli it Solution in the Two-Classes Case", "text": "For the sake of simpli ity, rename W\n+ 0 = W and W + 1 = W + representing the fra tion of\nexamples from the negative and positive lass respe tively, satisfying t. The rule to hoose ~v is the following:\nLemma 2 The following table gives the rule to hoose ~v :\nIf then we hoose\nW\n+\nW\ne\n3 2\n~v = ( 1;+1)\np\ne\nW\n+\nW\n< e\n3 2\n~v = ( 1; 0) or ~v = (0;+1)\n1 p\ne\nW\n+\nW\n<\np\ne ~v = ( 1; 1) or ~v = (0; 0) or ~v = (+1;+1)\n1\ne\n3 2\nW\n+\nW\n<\n1 p\ne\n~v = (0; 1) or ~v = (+1; 0)\nW\n+\nW\n<\n1\ne\n3 2\n~v = (+1; 1)\nProof: See the Appendix."}, {"heading": "5.3 Pruning a DC", "text": "The algorithm is a single-pass algorithm: ea h rule is tested only on e, from the rst rule to the last one. For ea h possible rule, a riterion Criterion(.) returns \\TRUE\" or \\FALSE\" depending on whether the rule should be removed or not. There are two versions of this riterion. The rst one, whi h we all \\pessimisti \", is based on onventional error minimization. The se ond one, alled \\optimisti \", is derived from a previous work on pruning de ision-trees (Kearns & Mansour, 1998)."}, {"heading": "5.3.1 Pessimisti Pruning", "text": "Pessimisti pruning builds a sequen e of DC from the initial one. At ea h step, we remove one rule, su h that its removal brings the lowest error among all possible removals of rule in the urrent DC. Ea h time the error of the urrent DC is not greater than the lowest error found already, Criterion(.) returns true for all rules already tested for removal. This pruning returns the smallest DC having the lowest error of the sequen e. This pruning is rather natural (and simple), and motivated by the fa t that the indu tion of the large DC before pruning does not lead to a onventional error minimization. Su h a property is rather seldom in \\top-down and prune\" indu tion algorithms. For example, ommon de ision tree indu tion algorithms in this s heme in orporate very sophisti ated pruning riteria (CART (Breiman et al., 1984), C4.5 (Quinlan, 1994))."}, {"heading": "5.3.2 Optimisti Pruning", "text": "Kearns & Mansour (1998) present a novel algorithm to prune de ision trees, based on a test over lo ally observed errors. Its prin iple is simple: ea h internal node of a DT is tested only on e in a bottom-up fashion, and we estimate the lo al error over the learning examples rea hing this node, before and after the removal of the node. If the lo al error after removal is not greater than the lo al error before, plus a penalty term, then we remove the node and its subtree. The penalty term makes the pruning essentially optimisti , that is, we tend to overprune the de ision tree. However, thanks to lo al uniform onvergen e results, and due to the fa t that ertain sub- lasses of de ision trees are reasonably large, Kearns & Mansour (1998) are able to prove that with high probability, the overpruning will not be too severe with respe t to the optimal subtree of the initial DT. We refer the reader to their paper for further theoreti al results, not needed here. The point is that by using the results of Kearns & Mansour (1998), we an obtain a similar test for DC. We emphasize that our bound might not enjoy the same theoreti al properties as for de ision trees, be ause of the ardinality reasons brie y outlined before. However, su h a test is interesting sin e it may lead espe ially to very small and interpretable de ision ommittees, with the obvious hope that their a ura y will not de rease too mu h. Furthermore, the paper of Kearns & Mansour (1998) does not ontain experimental results. We think our\nriterion as a way to test heuristi ally the experimental feasibility of some of the results of Kearns & Mansour (1998). The prin iple of our riterion is exa tly the same as the original test of Kearns & Mansour (1998) : \\ an we ompare, when testing some rule (t; ~v) and using the examples that satisfy the rule, the errors before and after removing the rule\"? Let\n(t;~v)\nrepresent the error before removing the rule, on the lo al sample LS\n(t;~v)\nsatisfying\nmonomial t. Denote\n;\nas the error before removing (t; ~v), still measured on the lo al sample\nLS\n(t;~v)\n. Then we de ne the heuristi \\penalty\" (proof omitted: it is a rough upperbound\nof Kearns & Mansour (1998), Lemma 1) :\n0 (t;~v) =\ns\n(Set((t; ~v)) + 2) log(n) + log 1=\u00c6\njLS\n(t;~v)\nj\n: (8)\nSet((t; ~v)) denotes the maximum number of literals of all rules ex ept (t; ~v) in the ur-\nrent DC, that an arbitrary example ould satisfy. The fast al ulability of\n0 (t;~v) is obtained\nat the expense of a greater risk of overpruning, whose e e ts on some small datasets were experimentally dramati for the a ura y. In our experiments, whi h ontain very small datasets, we have hosen to tune a parameter limiting the e e ts of this ombinatorial upperbound. More pre isely, We have hosen to uniformly resample LS into a larger subset of 5000 examples, when the initial LS ontained less than 5000 examples. By this, we arti ially in rease jLS\n(t;~v)\nj and mimi for the small domains new domains with an identi-\nal larger size, with the additional bene ts that reasonable omparisons may be made on\npruning.\nThe value of Criterion((t; ~v)) is therefore \\TRUE\" i\n(t;~v)\n+\n0 (t;~v)\n;\n."}, {"heading": "6. Experiments", "text": "Following are three experimental se tions, aimed at testing WIDC on three issues. The\nrst presents extensive results on the tradeo simpli ity-a ura y obtained by WIDC, and ompares the results with those obtained for state-of-the-art algorithms. The se ond goes on in depth analyzes for the mining/interpretability issue, and the third presents results on noise toleran e."}, {"heading": "6.1 Tradeo Simpli ity-A ura y", "text": "Experiments were arried out using three variants of WIDC: with optimisti pruning (o), with pessimisti pruning (p), and without pruning (;). Table 1 presents some results on various datasets, most of whi h were taken from the UCI repository of ma hine learning database (Blake et al., 1998). For ea h dataset, the eventual dis retization of attributes was performed following previous re ommendations and experimental setups (de Carvalho Gomes & Gas uel, 1994). The results were omputed using a ten-fold strati-\ned ross validation pro edure (Quinlan, 1996). The least errors for WIDC are underlined for ea h domain. For the sake of omparisons, olumn \\Others\" points out various results for other algorithms, intended to help getting a general pi ture of what an be the performan es of e\u00c6 ient approa hes with di erent outputs (de ision lists, trees, ommittees, et .), in terms of errors (and, when appli able, sizes). Some of the most relevant results for WIDC are summarized in the s atterplots of Table 2.\nThe interpretation of Table 1 using only errors gives the advantage to WIDC with pessimisti pruning, all the more as WIDC(p) has the advantage of providing simpler formulas than WIDC(;), and has a mu h simpler pruning stage than WIDC(o). Results also ompare favorably to the \\Other\" results, building either DLs, DTs, or DCs. They are all the more interesting if we ompare the errors in the light of the sizes obtained. For the \\E ho\" domain, WIDC with pessimisti pruning beats improved CN2 by two points,\nConventions: l\nDC\nis the whole number of literals of a DC, r\nDC\nis its number of rules. For\n\\Others\", numbers are given on the form error\n(size)\n, where a is improved CN2 (CN2-POE)\nbuilding DLs, size is the number of literals (Domingos, 1998). b is ICDL building DLs, notations follow a (No k & Jappy, 1998). is C4.5 (Fran k & Witten, 1998). d is IDC building DCs, notations follow a (No k & Gas uel, 1995). e is 1-Nearest Neighbor rule and f is C4.5 (pruned, default parameters) building DTs; the size of a tree is its whole number of nodes.\nbut the DC obtained ontains roughly eight times fewer literals than CN2-POE's de ision list. If we ex ept \\Vote0\", on all other problems on whi h we dispose of CN2-POE's\nresults, we outperform CN2-POE on both a ura y and size. Finally, on \\Vote0\", note that WIDC with optimisti pruning is slightly outperformed by CN2-POE by 2:51%, but the DC obtained is fteen times smaller than the de ision list of CN2-POE. If we dwell on the results of C4.5, similar on lusions an be brought: on 12 out of 13 datasets on whi h we ran C4.5, WIDC(p) nds smaller formulas, and still beats C4.5's a ura y on 9 of them. A quantitative omparison of l\nDC\nagainst the number of nodes of the DTs shows that on\n4 datasets out of the 13 (Pole, Shuttle, Ti Ta Toe, Australian), the DCs are more than 6 times smaller, while they only in ur a loss in a ura y for 2 of them, and limited to 1.8%. For this latter problem (Ti Ta Toe), a glimpse at Table 1 shows that the DCs, with less than 7 rules on average, keeps omparatively most of the information ontained in DTs having more than a hundred leaves. On many problems where mining issues are ru ial, su h a size redu tion would be well worth the ( omparatively slight) loss in a ura y, be ause we keep a signi ant part of the information on very small lassi ers, thus likely to be interpretable."}, {"heading": "6.2 Interpretability Issues", "text": "In the XD6 domain, ea h example has 10 binary variables. The tenth is irrelevant in the strongest sense (John, Kohavi, & P eger, 1994). The target on ept f is a 3-DNF (a\n+\ninternal nodes. To lassify an observation, the left edge of a node is followed when an observation ontains (\\Yes\") the positive literal, and the right edge is followed otherwise (i.e. the literal is negative in the observation). The bold square is used to display the presen e of the irrelevant variable in the tree. A naive onversion of this tree in rules for both lasses generates 30 rules, for a total of 179 literals.\nDNF with ea h monomial ontaining at most three literals) over the rst nine variables: (x\n0\n^ x\n1\n^ x\n2\n) _ (x\n3\n^ x\n4\n^ x\n5\n) _ (x\n6\n^ x\n7\n^ x\n8\n). Su h a formula is typi ally hard to en ode\nusing a small de ision tree. In our experiments with WIDC(o) and WIDC(p), we have remarked that the target formula itself is almost always an element of the lassi er built, and the irrelevant attribute is always absent. Figure 1 shows an example of DC whi h was obtained on a run of WIDC. Note that the on ept returned is a 3-DC. Figure 2 depi ts a part of a tree obtained on this domain with C4.5. While the tree appears to be quite large for the domain, note the presen e of the irrelevant variable in the tree, whi h it ontributes to enlarge while making it harder to mine. On many other domains, we observed persistent rules or sub on epts through the 10 ross-validation runs. Similarly to XD6, whenever we\nould mine the results with a su\u00c6 iently a urate knowledge of the domain, these patterns were most interesting. For example, the DCs obtained on the LEDeven domain ontained most of the time a ombination of two rules with one literal ea h, whi h represented a very a urate way to lassify 9 out of the 10 possible lasses. On the Vote0 and Vote1 domains, we also observed onstant patterns, some of whi h are well known (Blake et al., 1998) to provide a very a urate lassi ation for a tiny size. Even for Vote1 where lassi al studies often report errors over 12%, and almost never around 10% (Holte, 1993), we observed on most of the runs a DC ontaining an a urate rule with two literals only, with whi h WIDC(p) provided on average an error under 10%.\nWIDC was also ompared to C4.5 on a real world domain on whi h mining issues are as ru ial as lassi ation strength: agri ulture. An experiment is being arried out in Martinique by the DDAF (Departmental Dire tion of Agri ulture and Forest), to a hieve better understanding of the behavior of farmers, in parti ular regarding their willingness to ontra t a CTE (Farming Territorial Contra t). Usual farming ontra ts with either the state (Fran e) or Europe did not ontain ommitments for the farmer to satisfy. In a CTE, ea h farmer ommits to adapt and/or hange his agri ultural te hniques or produ tions, to ensure sustainable development for lo al agri ulture. In ex hange for this, he re eives the guarantee to obtain nan ial help for this ontra t, and to be trained to new agri ultural te hniques. Su h a domain is a good test bed to evaluate a method on the basis of predi tability and interpretability, be ause of the pla e of un ertainty in agri ulture, and the fa t that obtaining data an be a hard and long task : the DDAF has to be as a -\nurate as possible in its predi tions and interpretations, to manage as best as possible its relationships with farmers, and in the ase of CTEs, to make the best promotion ampaign for these new ontra ts. Agri ulture is also very sensitive to a \\show ase e e t\": provided even few representative farmers will have subs ribed to the ontra ts, omparatively many others are likely to follow.\nIn this study, from the des ription of 52 variables for about 60 representative farmers satisfying the riteria to adhere to a CTE, the aim is to develop models for those who are a tually willing to adhere, those not willing to adhere, and those urrently un ertain. Variables are data on ea h agri ultural exploitation (size, terrain nature, nan ial data, type of produ tion, et .), as well as more personal data on the farmers (edu ation, family status, obje tives, personal answer to a questionnaire, et .). This represents a small dataset to mine, but, interestingly, the results obtained were di erent when pro essing it with C4.5 or WIDC(p).\nadhere ? :adhere\nWe ran both algorithms in a 10-fold strati ed ross-validation experiment. WIDC(p) obtained a 2:8% average error. In 6 out of 10 runs, the same DC was indu ed. It is presented in Figure 3. Basi ally, this DC proves that predi ting the \\:adhere\" lass is the easiest task, followed by the predi tion of the \\adhere\" lass. The \\?\" (un ertain farmers) is predi ted only by the default ve tor. This seems rather natural: whereas the extreme behaviors tend to be lear to determine, the un ertainty is the hardest to predi t.\nC4.5 (default parameters) indu ed a DT whi h was almost the exa t trans ription of rule 1, a rule whi h says that farmers with no edu ation (without any agri ultural diploma or traineeships) and no ongoing proje t are not willing to adhere. This rule is mostly interesting be ause it proves that edu ation is a strong fa tor determining the \\:adhere\" answer. The DTs indu ed also ontained one or two more literals separating the \\adhere\" and \\?\" lasses (average error: 6:7%), but only few other things ould be mined from the trees of C4.5, in the light of the problem addressed.\nRule 2 in Figure 3 did not have the equivalent in the DTs indu ed. What it says is interesting for the DDAF, be ause it brings the following on lusion: farmers without ongoing proje ts, and not selling their produ ts only to a wholesaler, are on the knife edge for their membership (either in \\adhere\", or in \\:adhere\"). Without going further into lo al agri ultural onsiderations, this rule, for the DDAF Engineers, represents an a urate view of the farmers a tually ontrolling their exploitation osts, being either for or against CTEs, and that edu ation pushes towards the membership ( ombination of rules 1 and 2), probably be ause it allows them to see the future potential bene ts of the ontra t, better than its urrent onstraints."}, {"heading": "6.3 Noise Handling", "text": "Noise handling is a ru ial issue for boosting (Bauer & Kohavi, 1999; Opitz & Ma lin, 1999), even onsidered (Bauer & Kohavi, 1999) as its potential main problem. Experimental studies show that substantial noise levels an alter the vote to the point that its a ura y is lower than that of a single of its lassi er (Opitz & Ma lin, 1999). Opitz & Ma lin (1999) point out the reweighting s heme of the examples in boosting as being a potential reason for this behavior. Though we do not use any reweighting s heme, we have hosen for the sake of ompleteness to address the behavior of WIDC(p) against noise, and ompare its results with perhaps the major indu tion algorithm with whi h we share the \\topdown and prune\" indu tion s heme: C4.5 (Quinlan, 1994). This study relies on the XD6 domain, in whi h we repla e the original 10% lass noise (Buntine & Niblett, 1992) by various in reasing amounts of lass noise ranging from 0% to 40% by steps of 2%, or various\nin reasing amounts of attribute noise in the same range. The XD6 domain has the advantage that the target on ept is known, and it has been addressed in a substantial amount of previous experimental works. We have simulated orresponding datasets of 512 examples ea h, for ea h noise level. Ea h su h dataset was pro essed by WIDC(p) and C4.5, using a 10-fold- ross-validation pro edure. Figure 4 depi ts the results obtained for the errors and for the sizes of the lassi ers. The size of a DC is its whole number of literals, and that of a DT is its number of internal nodes.\nWhile the resistan e against noise seems to be relatively well distributed amongWIDC(p)\nand C4.5 (WIDC(p) seems to perform better for lass noise, while C4.5 seems to perform better for attribute noise), a phenomenon more interesting omes from the sizes of the formulas indu ed. First, the DCs have very small size u tuations ompared to the DTs : for\nlass noises greater than 20%, the DTs have size in reasing by a fa tor of 1.5-2. Se ond, note that the ratio between the number of nodes of the target DT, and the number of literals\nof the target DC is 3. For a majority of lass or attribute noise levels, the ratio between the DTs build and the DCs built is > 3, with a pathologi ase for 10% attribute noise, for whi h the ratio is > 6. These remarks, along with the fa t that the DCs built have a very reasonable size when ompared to that of the target DC for any type and level of noise, tend to show a good noise handling for WIDC(p). Apart from these onsiderations, glimpses at the DCs output by WIDC(p) show that even for large noise levels, it manages to nd\non epts synta ti ally lose to the target DC. For example, one of the DCs output at 30% lass noise is exa tly the target DC; also, it is only for lass noise 12% (and attribute\nnoise 16%) that some DCs found do not synta ti ally in lude the target DC anymore."}, {"heading": "7. Con lusion", "text": "Re ent advan es in the study of voting lassi ation algorithms have brought empiri al and theoreti al results learly showing the dis rimination power of ensemble lassi ers. This paper addresses from a theoreti al and empiri al point of view the question of whether one might have to dispense with interpretability in order to keep lassi ation strength. In order to ope with this problem, we have hosen to study a lass of on ept representations resembling multilinear dis riminant polynomials, adequate for mining issues when dealing with voting pro edures, whi h we de ne as De ision Committees. Our theoreti al results show that striving for simpli ity is, like for many other lasses of on ept representations, a hard omputational problem when dealing with DC or other omplex voting pro edures, and proves the heuristi nature of other results trying to prune adaptive boosting. This paper proposes to adapt a previous s heme to build weak learners, su essful for the indu tion of de ision trees and de ision lists, to the ase of DC. This is an original approa h if we refer to the state-of-the-art algorithms building omplex votes pro edures, usually working in the strong learning framework. Our algorithm, WIDC, relies on re ents or new results about partition boosting, ranking loss boosting, and pruning. It omes with two avors, one with optimisti pruning and one with pessimisti pruning. Both obtained experimentally good results on the simpli ity-a ura y tradeo , but whereas optimisti pruning learly outperforms other algorithms in the light of the size of the formulas obtained, pessimisti pruning tends to a hieve a more reasonable tradeo , with high a ura ies obtained on small formulas. This is all the more interesting as pessimisti pruning is based on a natural and simple pruning pro edure."}, {"heading": "8. A knowledgments", "text": "Thanks are due to DDAF Martinique, ENESAD (Etablissement National d'Enseignement Sup erieur Agronomique de Dijon) and Lise Jean-Louis for having provided the agri ultural data, for stimulating dis ussions around our results, and for having authorized the publi-\nation of some of the results obtained. Thanks to Ahmed Ainou he for having pointed out the interest in minimizing submodular fun tions. Finally, the author wishes to thank Pedro Domingos and the reviewers for their valuable suggestions."}, {"heading": "Appendix A", "text": ""}, {"heading": "Proof of Theorem 1", "text": "Sin e the hardness results of Theorems 1 and 3 are stated for the two- lasses ase, we shall use the notation\n(i)\n= ~v\n(i)\n[1\u2104 ~v\n(i)\n[0\u2104 for some arbitrary rule (t\n(i)\n; ~v\n(i)\n), where ~v\n(i)\n[0\u2104 is the\nvalue for lass \\-\" and ~v\n(i)\n[1\u2104 is the value for lass \\+\". A positive value for\n(i)\nmeans\nthat t\n(i)\nis in favor of lass \\+\" whereas a negative value gives a t\n(i)\nin favor of lass \\-\".\nValue 0 for\n(i)\ngives a t\n(i)\nneutral with respe t to the lasses. We use a redu tion from the\nNP -Hard problem \\Minimum Cover\" (Garey & Johnson, 1979):\nName : \\Minimum Cover\".\nInstan e : A olle tion C of subsets of a nite set S. A positive integer K, K jCj.\nQuestion : Does C ontain a over of size at most K, that is, a subset C\n0\nC with\njC\n0\nj K, su h that any element of S belongs to at least one member of C\n0\n?\nThe redu tion is onstru ted as follows : from a \\Minimum Cover\" instan e we build a\nlearning sample LS su h that if there exists a over of size jC\n0\nj K of S, then there exists\na de ision ommittee with jC\n0\nj literals onsistent with LS, and, re ipro ally, if there exists\na de ision ommittee with k literals onsistent with LS, then there exists a over of size k of S. Hen e, nding the smallest de ision ommittee onsistent with LS is equivalent to\nnding the smallest K for whi h there exists a solution to \\Minimum Cover\", and this is\nintra table if P 6= NP .\nLet\nj\ndenote the j\nth\nelement of C, and s\nj\nthe j\nth\nelement of S. We de ne a set\nof jCj Boolean variables in one to one orresponden e with the elements of C, whi h we use to des ribe the examples of LS. The orresponding set of literals is denoted fx\n1\n; x\n1\n; x\n2\n; x\n2\n; :::; x\njCj\n; x\njCj\ng. The sample LS ontains two disjoint subsets : the set of posi-\ntive examples LS\n+\n, and the set of negative ones LS . LS\n+\nontains jSj examples, denoted\nby e\n+ 1 ; e + 2 ; :::; e +\njSj\n. We onstru t ea h positive example so that it en odes the membership\nof the orresponding element of S in the elements of C. More pre isely,\n81 i jSj; e\n+ i =\n0\n^\nj:s\ni\n2\nj\nx\nj\n1\nA\n^\n0\n^\nj:s\ni\n62\nj\nx\nj\n1\nA\n: (9)\nLS\nontains a single negative example, de ned by:\ne\n=\nj=jCj\n^\nj=1\nx\nj\n: (10)\nSuppose there exists a over C\n0\nof S satisfying jC\n0\nj K. We reate a de ision ommittee\nonsisting of K monomials, ea h with one literal only and asso iated to a positive . Ea h\nmonomial odes one of the sets in C\n0\n. The default lass is \\-\". This de ision ommittee\nis onsistent with the examples of LS\n+\n[ LS , otherwise some element of S would not be\novered. If there are only two values authorized for the ve tors and they are 0, we simply reate a DC onsisting of one monomial with negative literals asso iated to a negative\nmonomial t\n(the value for the negative lass is greater than the one of the positive lass); ea h of the negative literals odes one of the sets in C 0 . The default lass is \\+\".\nSuppose now that there exists a de ision ommittee f with at most k literals onsistent\nwith LS. Denote t\n1\n; t\n2\n; :::; t\njf j\nea h monomial of f , in no spe i order, and\n1\n;\n2\n; :::;\njf j\ntheir asso iated values for . The monomials of f an belong to three types of subsets of monomials:\nmonotonous monomials (without negative literals),\nmonomials ontaining only negative literals,\nmonomials ontaining positive and negative literals.\nLet us all respe tively M , N , MN these three lasses. Given that ea h monomial of f an be asso iated to a positive or a negative , there exists on the whole six lasses of rules, presented in Figure 5.\nAny monomial of f ontaining at least one positive literal an only be satis ed by positive examples. Therefore, if there exists rules belonging to lass II or VI, we an remove them without losing onsisten y. Furthermore, sin e e ontains only negative literals, if we remove their negative literals from all rules belonging to lass V (making them go to lass I), we do not lose onsisten y. As a onsequen e, we an suppose without loss of generality that all rules of f are in lass I, III, or IV.\nWe now treat independently two ases, depending on whether the default lass of f is\n\\+\" or \\-\".\n1. The default lass is \\-\". Any positive example satis es therefore a monomial in f .\nThere an exist two types of positive examples: those satisfying at least one rule of\nlass I, and those not satisfying any lass I rule (therefore satisfying at least one rule\nof lass III). e satis es all lass III and IV rules. Therefore,\nX\n(t\ni\n;~v\ni\n)2f\\( lass III [ lass IV )\ni\n0 : (11)\nThis shows that, if a positive example not satisfying any lass I rules would satisfy all\nlass IV rules, then it would be mis lassi ed, whi h is impossible by the onsisten y hypothesis. This gives an important property, namely that any positive example not\nsatisfying any lass I rule annot satisfy all lass IV rules. Let us all P this property in what follows. We now show how to build a valid solution to \\Minimum Cover\" with at most k elements. For any positive example e +\ni\n,\nif e\n+ i satis es at least one lass I rule, hoose in C a subset of S orresponding\nto a positive literal of some satis ed lass I rule. This subset ontains e\n+ i .\nif e\n+ i does not satisfy any lass I rule, there exists from P some lass IV rule whi h is not satis ed. Among all negative literals of a lass IV rule whi h is not satis ed by e +\ni\n, hoose one whi h is positive in e\n+ i ( ausing it not to satisfy the\nrule), and then hoose the orresponding element of C. This subset of S ontains e +\ni\n.\nIterating the above pro edure for all positive examples, we obtain a over of S onsisting of at most k subsets of S.\n2. The default lass is \\+\". e satis es all lass III and IV rules. Therefore,\nX\n(t\ni\n;~v\ni\n)2f\\( lass III [ lass IV )\ni\n< 0 : (12)\nEven if the inequality is now stri t, it gives the same pro edure for e\u00c6 iently building the solution to \\Minimum Cover\" with at most k elements, by using the same argument as in the pre eeding ase.\nThis ends the proof of Theorem 1."}, {"heading": "Proof of Theorem 3", "text": "We use a redu tion from the NP -Hard problem \\2-NM-Colorability\" (Kearns et al., 1987):\nName : \\2-NM-Colorability\".\nInstan e : A nite set S = fs\n1\n; s\n2\n; :::; s\njSj\ng and a olle tion of onstraints over S,\nC = f\n1\n;\n2\n; :::;\njCj\ng, su h that 8i 2 f1; 2; :::; jCjg;\ni\nS.\nQuestion : Does there exist a 2-NM-Coloration of the elements of S, i.e. a fun tion\n: S ! f1; 2g su h that\n(8i 2 f1; 2; :::; jCjg); (9s\nk\n; s\nl\n2\ni\n) : (s\nk\n) 6= (s\nl\n) ?\nThe redu tion is onstru ted as follows : from a \\2-NM-Colorability\" instan e, we build a learning sample LS su h that if there exists a 2-NM-Coloration of the elements of S, then there exists a de ision ommittee with two rules onsistent with LS, and, re ipro ally, if there exists a de ision ommittee with two rules onsistent with LS, then there exists a 2- NM-Coloration of the elements of S. Furthermore, there never exists a de ision ommittee with only one rule onsistent with LS. Hen e, nding the de ision ommittee with the smallest number of rules onsistent with LS is at least as hard as solving \\2-NM-Colorability\", and this is intra table if P 6= NP .\nLet\nj\ndenote the j\nth\nelement of C, and s\nj\nthe j\nth\nelement of S. We de ne a set\nof jSj Boolean variables in one to one orresponden e with the elements of S, whi h we use to des ribe the examples of LS. The orresponding set of literals is denoted fx\n1\n; x\n1\n; x\n2\n; x\n2\n; :::; x\njSj\n; x\njSj\ng. Our redu tion is made in the two- lasses framework. The\nsample LS ontains two disjoint subsets : the set of positive examples LS\n+\n, and the set of\nnegative ones LS . LS\n+\nontains jSj examples, denoted by e\n+ 1 ; e + 2 ; :::; e +\njSj\n. We onstru t\nea h positive example so that it represents an element of S. More pre isely,\n81 i jSj; e\n+ i = x i ^\nj=jSj\n^\nj=1;j 6=i\nx\nj\n: (13)\nLS\nontains jCj examples, denoted by e\n+ 1 ; e + 2 ; :::; e +\njCj\n. We onstru t ea h negative\nexample so that it en odes ea h of the onstraints of C. More pre isely:\n81 i jCj; e\ni\n=\n0\n^\nj:s\nj\n2\ni\nx\nj\n1\nA\n^\n0\n^\nj:s\nj\n62\ni\nx\nj\n1\nA\n: (14)\nWithout loss of generality, we make four assumptions on the instan e of \\2-NM-Colorability\"\ndue to the fa t that it is not trivial:\n1. There does not exist some element of S present in all onstraints. In this ase indeed,\nthe trivial oloration onsists in giving to one of su h elements one olor, and the other olor to all other elements of S.\n2. 8(i; j; k; l) 2 f1; 2; :::; jSjg\n4\nwith i 6= j and k 6= l,\n9o 2 f1; 2; :::; jCjg; fs\ni\n; s\nj\ng 6\no\n^ fs\nk\n; s\nl\ng 6\no\n: (15)\nOtherwise indeed, there would exist (i; j; k; l) 2 f1; 2; :::; jSjg\n4\nwith i 6= j and k 6= l\nsu h that\n8o 2 f1; 2; :::; jCjg; fs\ni\n; s\nj\ng\no\n_ fs\nk\n; s\nl\ng\no\n; (16)\nand in that ase, a trivial solution to \\2-NM-Colorability\" would onsist in giving to s\ni\none olor and to s\nj\nthe other one, and to s\nk\none olor and to s\nl\nthe other one.\n3. Ea h element of S belongs to at least one onstraint in C. Otherwise, it an be\nremoved.\n4. Ea h onstraint ontains at least two elements from S. Otherwise it an be removed.\nSuppose there exists a solution to \\2-NM-Colorability\". We build the DNF with two monomials of (Kearns et al., 1987) onsistent with the examples. Then, we build two rules by asso iating the two monomials to some (arbitrary) positive value. The default lass is \\-\". This leads to a de ision ommittee with two rules onsistent with LS.\nSuppose that there exists a de ision ommittee f with at most two rules onsistent with LS. We now show that there exists a valid 2-NM-Coloration of the elements of S. We\nrst show three lemmas whi h shall be used later on. Then, we show that the de ision ommittee is a tually equivalent to a DNF with two monomials onsistent with LS. We on lude by using previous results (Kearns et al., 1987) on how to transform this DNF into\na valid 2-NM-Coloration of the elements of S.\nLemma 3 If a monomial is not satis ed by any positive example,\neither it ontains at least two negative literals, or\nit is the monomial ontaining all positive literals:\nj=jSj\n^\nj=1\nx\nj\n:\n(Proof straightforward).\nLemma 4 If a monomial is satis ed by all positive examples, it is empty.\n(Indeed, for any variable, there exist two positive examples having the orresponding\npositive literal, and the orresponding negative literal).\nLemma 5 f ontains exa tly two rules.\nProof: Suppose that f ontains one rule, whose monomial is alled t\n1\n. If the default\nlass is \\-\", all positive examples satisfy t\n1\n, whi h is impossible by Lemma 4: the monomial\nwould be empty, and f ould not be onsistent. If the default lass is \\+\", the negative examples are lassi ed by t\n1\nand therefore\n1\n< 0. Thus, no positive example satis es t\n1\n.\nFrom Lemma 3, either t\n1\n=\nV\nj=jSj j=1 x j and no negative example an satisfy it (impossible),\nor t\n1\nontains at least two negative literals, and the onstraints all have in ommon two\nelements of S. Thus, the instan e of \\2-NM-Colorability\" is trivial, whi h is impossible. This ends the proof of Lemma 5.\nWe now show that the default lass of f is \\-\". For the sake of simpli ity, we write the\ntwo monomials of f by t\n1\nand t\n2\n. The default lass is denoted 2 f\\-\", \\+\"g. Making\nthe assumption that =\\+\" implies that all negative examples must satisfy at least one monomial in f .\nSuppose that\n1\n< 0 and\n2\n< 0. Then, no positive example an satisfy either t\n1\nor t\n2\n. From the two possibilities of Lemma 3, only the rst one is valid (\nV\nj=jSj j=1 x j\nannot be satis ed by any negative example). Thus, t\n1\nand t\n2\nontain ea h at least\ntwo negative literals:\nfx\ni\n; x\nj\ng t\n1\n; (17)\nfx\nk\n; x\nl\ng t\n2\n: (18)\nWe are in the se ond ase of triviality of the instan e of \\2-NM-Colorability\", sin e making the assumption that f is onsistent implies:\n9o 2 f1; 2; :::; jCjg; fs\ni\n; s\nj\ng 6\no\n^ fs\nk\n; s\nl\ng 6\no\n: (19)\nSuppose that\n1\n< 0 and\n2\n> 0. All negative examples must satisfy t\n1\n. t\n1\nis for ed\nto be monotonous sin e otherwise (given that =\\+\") all negative examples would share a ommon negative literal, thus all onstraints would share a ommon element of S, and the instan e of \\2-NM-Colorability\" would be trivial. t\n2\nbeing satis ed\nby at least one positive example (otherwise, f would be equivalent to a single-rule de ision ommittee, and we fall in the ontradi tion of Lemma 5), it ontains at most one negative literal. If it ontains exa tly one negative literal, it is satis ed by exa tly one positive example, and we an repla e it by the monotonous monomial with jSj 1 positive literals (we leave empty the position of the initial negative literal). Consequently, similarly for t\n1\n, we an suppose that t\n2\nis monotonous. We distinguish\ntwo ases.\n{ If j\n1\nj > j\n2\nj, no positive example an satisfy t\n1\n. By fa t 3, t\n1\n=\nV\nj=jSj j=1 x j , and\nno negative example an satisfy it, a ontradi tion (f annot be onsistent).\n{ If j\n1\nj j\n2\nj. t\n2\nannot be empty; therefore it ontains a ertain number\nof positive literals. Ea h positive example satisfying t\n2\nmust also satisfy t\n1\n,\nsin e otherwise f is not onsistent; Sin e t\n1\nand t\n2\nare monotonous, t\n2\nis a\ngeneralization of t\n1\n, and any example satisfying t\n1\n(in parti ular, the negative\nexamples) must satisfy t\n2\n, a ontradi tion.\nTherefore =\\-\". This for es all positive examples to satisfy at least one monomial of\nf . Re all that f ontains two monomials. Suppose that\n1\n> 0 and\n2\n< 0. It omes\nt\n1\n= ; (Lemma 4). All negative examples must satisfy t\n2\n, and we also have j\n1\nj j\n2\nj.\nNo positive example an satisfy t\n2\n, and Lemma 3 gives either t\n1\n=\nV\nj=jSj j=1 x j (satis ed by\nno example, impossible) or t\n2\nontains at least two negative literals, whose orresponding\nelements of S are shared by all onstraints, and we obtain again that the instan e of \\2- NM-Colorability\" is trivial.\nTherefore,\n1\n> 0 and\n2\n> 0, and ea h monomial is satis ed by at least one positive\nexample. f is thus equivalent to a DNF with the same two monomials, and we an use a previous solution (Kearns et al., 1987) to build a valid 2-NM-Coloration. First, we an suppose that f is again monotonous (Kearns et al., 1987). Then, sin e ea h positive example satis es at least one monomial ( =\\-\"), then for all variable, there exists a monomial whi h does not ontain the orresponding positive literal. The 2-Coloration is then\n8i 2 f1; 2; :::; jSjg; (s\ni\n) = min\nj2f1;2g\nfj : x\ni\n62 t\nj\ng : (20)\nCould this be invalid ? That would mean that there exists a onstraint\ni\nsu h that\n8s\nj\n2\ni\n; (s\nj\n) = K = st. This would mean that the orresponding negative example\nsatis es t\nK\n, a ontradi tion (Kearns et al., 1987). This ends the proof of Theorem 3."}, {"heading": "Proof of Theorem 4", "text": "De ne the fun tion f : 2\nf0;1;:::; 1g\n! IR su h that\n8A f0; 1; :::; 1g; f [A\u2104 =\nX\no;k;j\nw((o; k; j))q\nA\n(j; k) ; (21)\nwith\nq\nA\n(j; k) = e [[j 2 A ^ k 62 A\u2104\u2104 + e [[j 62 A ^ k 2 A\u2104\u2104 + [[(j 2 A ^ k 2 A) _ (j 62 A ^ k 62 A)\u2104\u2104 :\nNote that f generalizes the three expressions of Z in equations (2), (3), and (7) with\nadequate values for . Now, we he k that f satis es the submodular inequality:\nf [A [B\u2104 + f [A \\B\u2104 f [A\u2104 + f [B\u2104 ; (22)\nfor all subsets A;B f0; 1; :::; 1g. The key is to examine the oe\u00c6 ient of ea h w((o; k; j)), for ea h set f0; 1; :::; 1gn(A [ B), AnB, BnA, A \\ B to whi h j or k an belong. Table 3 presents these oe\u00c6 ients. We get from Table 3 :\nf [A [B\u2104 + f [A \\B\u2104 (f [A\u2104 + f [B\u2104) =\n2 e\ne\nX\no;k;j\nw((o; k; j))[[(j 2 AnB ^ k 2 BnA) _ (j 2 BnA ^ k 2 AnB)\u2104\u2104 :\nThis last quantity is 0 for any possible hoi e of . Therefore, minimizing Z in any of its three forms of eq. (2), (3), and (7) boils down to minimizing f on the submodular system (f0; 1; :::; 1g; f) (with the adequate values of ). This problem admits polynomial-time solving algorithms (Gr ots hel, Lov asz, & S hrijver, 1981; Queyranne, 1998). What is mu h interesting is that the algorithms known are highly ompli ated and time onsuming for the general minimization of f (Queyranne, 1998). However, when using the value of as in eq. (4) and Z as in eq. (7), the orresponding fun tion f be omes submodular symmetri (f [A\u2104 = f [f0; 1; :::; 1gnA\u2104). As su h, more e\u00c6 ient (and simpler) algorithms exist to minimize f . For example, there exists a powerful ombinatorial algorithm working in O( 5 ) (Queyranne, 1998). Note that this is still a very large omplexity."}, {"heading": "Proofsket h of Theorem 5", "text": "The redu tion is made from the NP -Hard problem 3SAT5 (Feige, 1996). This is the lassi al 3SAT problem (Garey & Johnson, 1979), but ea h variable appears in exa tly 5 lauses. Using a well-known redu tion (Garey & Johnson, 1979), page 55, with an additional simple gadget, we an make a redu tion from 3SAT5 to vertex over (thus, independent set), obtaining a graph G in whi h all verti es have degree either 5, or 0, and for whi h the largest independent set (for satis able instan es of 3SAT5) has size jV j=2, where jV j is the number of verti es of G. From this parti ular graph, we build a simple redu tion to our problem of maximizing Z. Note that sin e we are sear hing for an oblivious hypothesis, the observations are not important (we an suppose that all examples have the same observation). That is why the redu tion only builds lass ve tors (over jV j lasses), en oding the lass membership of any of these identi al observations. The idea is that the lasses are in one-to-one mapping with the verti es, and there are two sets of lass ve tors built from G:\na set with jV j ve tors, en oding the verti es of G. Ea h one is a lass ve tor with only one \\1\" orresponding to the vertex, and the remaining omponents are zeroes. Ea h of the orresponding examples have weight W\nv\n.\na set with jEj ve tors, where jEj is the number of edges of G. Ea h one en odes an edge, and therefore ontains two \\1\" (and the remaining are zeroes) orresponding to the two verti es of the edge. Ea h of the orresponding examples have weight W\ne\n.\nConsider formulas (2), (3) for example. They are the sum of the ontribution to Z of\nthe examples having weight W\nv\n, and the examples having weight W\ne\n. In these ases, we\nan rewrite Z using the generi expression:\nZ = Z\nv\n+ Z\ne\n; (23)\nZ\nv\n= W\nv\ne k(jV j k) + k(k 1) + (jV j k)(jV j k 1) + e k(jV j k) ; (24)\nZ\ne\n= W\ne\ne (jV j k)(2C + U) + e k(2M + U) : (25)\nHere, C is the number of edges having their two verti es in the set orresponding to the\n+1 values in ~v\ni\n, M is the number of edges having their two verti es in the set orresponding\nto the 1 values in ~v\ni\n, and U is the number of edges having one of their verti es in the +1\nset, and the other one in the 1 set. k is the number of +1 values in ~v\ni\n.\nSuppose that W\nv\nW\ne\n(e.g. W\nv\n> jV j\n3\nW\ne\n). Then the maximization of Z is the maxi-\nmization of Z\nv\n, followed by the maximization of Z\ne\n. Z\nv\nadmits a maximum for k = jV j=2,\nand with this value for k, it an be shown that maximizing Z\ne\nboils down to maximize\n2M + U , that is, the (weighted) number of edges not falling entirely into the set orresponding to the +1 values; whenever the 3SAT5 instan e is satis able (and using the parti ular degrees of the verti es), this set orresponds to the largest independent set of G."}, {"heading": "Proof of Lemma 1", "text": "The proof of this lemma is quite straightforward, but we give it for ompleteness. Z an be rewritten as\nZ =\nX\nj 6=k\nZ\nj;k\n; (26)\nwith\n8j 6= k; Z\nj;k\n=\nW\n+ j\n1\ne\n1 2 (j;k)\n+\nW\n+ k\n1\ne\n1 2 (j;k)\n; (27)\nwhere (j; k) = ~v[j\u2104 ~v[k\u2104. Suppose for ontradi tion that for some j < k, = (j; k) > 0. We simply permute the two values ~v[j\u2104 and ~v[k\u2104, and we show that the new\nvalue of Z after, Z\na\n, is not greater than Z before permuting, Z\nb\n. The di eren e between\nZ\na\nand Z\nb\nan be easily de omposed using the notation Z\n(i;j)b\n(i; j 2 f0; 1; :::; 1g; i 6= j)\nas the value of Z\ni;j\n(eq. (27)) in Z\nb\n, and Z\n(i;j)a\n(i; j 2 f0; 1; :::; 1g; i 6= j) as the value\nof Z\ni;j\n(eq. (27)) in Z\na\n. We also de ne:\n8i; j; k 2 f0; 1; :::; 1g; i 6= j 6= k; Z\n(i;j;k)a\n= Z\n(i;k);a\n+ Z\n(k;i);a\n+ Z\n(j;k);a\n+ Z\n(k;j);a\n: (28)\nWe de ne in the same way Z\n(i;j;k)b\n. We obtain\nZ\na\nZ\nb\n=\nZ\n(j;k)a\nZ\n(j;k)b\n+\nX\ni=1;i 62fj;kg\nZ\n(j;k;i)a\nZ\n(j;k;i)b\n: (29)\nProving that Z\na\nZ\nb\n0 an be obtained as follows. First,\nZ\n(j;k)a\nZ\n(j;k)b\n=\nW\n+ k W + j e\n1 2\n1\n1 e\n0 :\nWe also have 8i 2 f1; 2; :::; gnfj; kg:\nZ\n(j;k;i)a\nZ\n(j;k;i)b\n=\n2W\n+ j\n1\ne\n1 2 i;k\n+\n2W\n+ k\n1\ne\n1 2 i;j\n2W\n+ j\n1\ne\n1 2 i;j\n2W\n+ k\n1\ne\n1 2 i;k\n=\n2 W\n+ k W + j\n1\ne\n1 2 i;j\ne\n1 2 i;k\n=\n2 W\n+ k W + j e\n1 2 i;j\n1\n1 e\n0 :\nHere we have use the fa t that\ni;k\n=\ni;j\n+ . This shows that Z\na\nZ\nb\n0, and ends\nthe proof of Lemma 1."}, {"heading": "Proof of Theorem 6", "text": "To avoid onfusion, we all Z\n0\nthe value of Z omputed over the transformed set of examples,\nand U(~u) for U 2 fZ;Z\n0\ng and ~u 2 f~v ; ~vg as the value of riterion U using ve tor ~u. It is\nsimple to obtain a \\su\u00c6 ient\" bound to he k the theorem. We have\nZ(~v) = Z\n0\n(~v)\nX\nS V jSj > 1\nW\nS jSj\n0\nX\ni2S\ne\n1 2 ~v[i\u2104\nX\nj2Snfig\ne\n1 2 ~v[j\u2104\n1\nA\n; (30)\nZ\n0\n(~v ) = Z(~v ) +\nX\nS V jSj > 1\nW\nS jSj\n0\nX\ni2S\ne\n1 2 ~v [i\u2104\nX\nj2Snfig\ne\n1 2 ~v [j\u2104\n1\nA\n: (31)\nHere, W\nS\nis the sum of weights of the examples in the original set, whose ve tors have\n\\1\" mat hing the elements of S. Note that Z\n0\n(~v) Z\n0\n(~v ), sin e our algorithm is optimal,\nand we obtain\nZ(~v)\nZ(~v ) +\nX\nS V jSj > 1\nW\nS jSj\n0\nX\ni2S\n2\n4\ne\n1 2 ~v [i\u2104\nX\nj2Snfig\ne\n1 2 ~v [j\u2104\ne\n1 2 ~v[i\u2104\nX\nj2Snfig\ne\n1 2 ~v[j\u2104\n3\n5\n1\nA\n:\nBy taking only the positive part of the right-hand side, and remarking that\n8S V; jSj > 1;8i 2 S,\nP\nj2Snfig\ne\n1 2 ~v [j\u2104\ne\njSj 1\njSj\nP\nj2V nS\ne\n1 2 ~v [j\u2104\n(the right sum\nis ( jSj)e\n1 2\nand the left one is (jSj 1)\np\ne),\nthe oe\u00c6 ient of W\nS\nin Z is\nS\n=\nP\ni2S\ne\n1 2 ~v [i\u2104\nP\nj2V nS\ne\n1 2 ~v [j\u2104\n,\nwe get\nZ(~v) Z(~v ) +\nX\nS V jSj > 1\ne\n(jSj 1)\njSj( jSj)\nW\nS S\n< Z(~v ) + e\n(k 1)\nk( k)\nX\nS V jSj > 1\nW\nS S\n< Z(~v ) 1 +\ne\nk\n;\nas laimed."}, {"heading": "Proof of Lemma 2", "text": "Z be omes in that ase\nZ = W\n+\ne\n1 2\n+W e\n1 2\n; (32)\nwhere = ~v[1\u2104 ~v[0\u2104. There are ve di erent values for , giving rise to nine di erent\n~v:\n= +2 ) ~v = ( 1;+1) ;\n= +1 ) ~v = ( 1; 0) _ ~v = (0;+1) ;\n= 0 ) ~v = ( 1; 1) _ ~v = (0; 0) _ ~v = (+1;+1) ;\n= 1 ) ~v = (0; 1) _ ~v = (+1; 0) ;\n= 2 ) ~v = (+1; 1) :\nFix = k where k 2 f 2; 1; 0; 1; 2g. 8k 2 f 1; 0; 1; 2g, the value = k should be\npreferred to the value = k 1 i the orresponding Z is smaller, that is :\nW\n+\ne\nk 2\n+W\ne\nk 2\n< W\n+\ne\nk 1\n2\n+W\ne\nk 1\n2\n: (33)\nRearranging terms gives W < W\n+\n1\ne\nk\n1 2\n. This leads to the rule of the lemma."}, {"heading": "Referen es", "text": "Bauer, E., & Kohavi, R. (1999). An empiri al omparison of voting lassi ation algorithms:\nBagging, boosting, and variants. Ma hine Learning, 36, 105{139.\nBlake, C. L., Keogh, E., & Merz, C. (1998). UCI repository of ma hine learning databases..\nhttp://www.i s.u i.edu/ mlearn/MLRepository.html.\nBreiman, L. (1996). Bagging predi tors. Ma hine Learning, 24, 123{140.\nBreiman, L., Freidman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classi ation and\nRegression Trees. Wadsworth.\nBuja, A., & Lee, Y.-S. (2001). Data mining riteria for tree-based regression and lassi a-\ntion. In Pro eedings of the 7\nth\nInternational Conferen e on Knowledge Dis overy in\nDatabases, pp. 27{36.\nBuntine, W., & Niblett, T. (1992). A further omparison of splitting rules for De ision-Tree\nindu tion. Ma hine Learning, 8, 75{85.\nClark, P., & Boswell, R. (1991). Rule indu tion with CN2: some re ent improvements. In\nPro eedings of the 6\nth\nEuropean Working Session in Learning, pp. 155{161.\nCohen, W. W., & Singer, Y. (1999). A Simple, Fast and E e tive Rule Learner. In Pro-\needings of the 16\nth\nNational Conferen e on Arti ial Intelligen e, pp. 335{342.\nde Carvalho Gomes, F. A., & Gas uel, O. (1994). SDL, a sto hasti algorithm for learning\nde ision lists with limited omplexity. Annals of Mathemati s and Arti ial Intelligen e, 10, 281{302.\nDietteri h, T. G. (2000). An experimental omparison of three methods for onstru ting\nensembles of de ision trees: Bagging, boosting, and randomization. Ma hine Learning, 40, 139{157.\nDomingos, P. (1998). A Pro ess-oriented Heuristi for Model sele tion. In Pro eedings of\nthe 15\nth\nInternational Conferen e on Ma hine Learning, pp. 127{135.\nFeige, U. (1996). A threshold of ln n for approximating set over. In Pro eedings of the\n28\nth\nACM Symposium on the Theory of Computing, pp. 314{318.\nFran k, E., &Witten, I. (1998). Using a Permutation Test for Attribute sele tion in De ision\nTrees. In Pro eedings of the 15\nth\nInternational Conferen e on Ma hine Learning, pp.\n152{160.\nFreund, Y., & Mason, L. (1999). The alternating de ision tree learning algorithm. In\nPro eedings of the 16\nth\nInternational Conferen e on Ma hine Learning, pp. 124{133.\nFriedman, J., Hastie, T., & Tibshirani, R. (2000). Additive Logisti Regression : a Statisti al\nView of Boosting. Annals of Statisti s, 28, 337{374.\nGarey, M., & Johnson, D. (1979). Computers and Intra tability, a guide to the theory of\nNP-Completeness. Bell Telephone Laboratories.\nGr ots hel, M., Lov asz, L., & S hrijver, A. (1981). The ellipsoid method and its onsequen es\nin ombinatorial optimization. Combinatori a, 1, 169{197.\nHolte, R. (1993). Very simple lassi ation rules perform well on most ommonly used\ndatasets. Ma hine Learning, 11, 63{91.\nHya l, L., & Rivest, R. (1976). Constru ting optimal de ision trees is NP- omplete. Infor-\nmation Pro essing Letters, 5, 15{17.\nJohn, G. H., Kohavi, R., & P eger, K. (1994). Irrelevant features and the subset sele tion\nproblem. In Pro eedings of the 11\nth\nInternational Conferen e on Ma hine Learning,\npp. 121{129.\nKearns, M. J., & Mansour, Y. (1998). A Fast, Bottom-up De ision Tree Pruning algo-\nrithm with Near-Optimal generalization. In Pro eedings of the 15\nth\nInternational\nConferen e on Ma hine Learning, pp. 269{277.\nKearns, M., Li, M., Pitt, L., & Valiant, L. (1987). On the learnability of boolean formulae. In\nPro eedings of the 19\nth\nACM Symposium on the Theory of Computing, pp. 285{295.\nKohavi, D., & Sommer eld, D. (1998). Targetting Business users with De ision Table\nClassi ers. In Pro eedings of the 4\nth\nInternational Conferen e on Knowledge Dis overy\nin Databases, pp. 249{253.\nMansour, Y., & M Allester, D. (2000). Boosting using bran hing programs. In Pro eedings\nof the 13\nth\nInternational Conferen e on Computational Learning Theory, pp. 220{\n224.\nMargineantu, D., & Dietteri h, T. G. (1997). Pruning adaptive boosting. In Pro eedings of\nthe 14\nth\nInternational Conferen e on Ma hine Learning, pp. 211{218.\nMit hell, T. (1997). Ma hine Learning. M Graw-Hill.\nNo k, R., & Gas uel, O. (1995). On learning de ision ommittees. In Pro eedings of the 12\nth\nInternational Conferen e on Ma hine Learning, pp. 413{420. Morgan Kaufmann.\nNo k, R., & Jappy, P. (1998). On the power of de ision lists. In Pro eedings of the 15\nth\nInternational Conferen e on Ma hine Learning, pp. 413{420. Morgan Kaufmann.\nOpitz, D., & Ma lin, R. (1999). Popular ensemble methods: a survey. Journal of Arti ial\nIntelligen e Resear h, 11, 169{198.\nQueyranne, M. (1998). Minimizing symmetri submodular fun tions. Mathemati al Pro-\ngramming, 82, 3{12.\nQuinlan, J. R. (1994). C4.5 : programs for ma hine learning. Morgan Kaufmann.\nQuinlan, J. R. (1996). Bagging, Boosting and C4.5. In Pro eedings of the 13\nth\nNational\nConferen e on Arti ial Intelligen e, pp. 725{730.\nRidgeway, G., Madigan, D., Ri hardson, T., & O'Kane, J. (1998). Interpretable boosted\nnaive bayes lassi ation. In Pro eedings of the 4\nth\nInternational Conferen e on\nKnowledge Dis overy in Databases, pp. 101{104.\nRivest, R. (1987). Learning de ision lists. Ma hine Learning, 2, 229{246.\nS hapire, R. E., Freund, Y., Bartlett, P., & Lee, W. S. (1998). Boosting the Margin : a\nnew explanation for the e e tiveness of Voting methods. Annals of statisti s, 26, 1651{1686.\nS hapire, R. E., & Singer, Y. (1998). Improved boosting algorithms using on den e-rated\npredi tions. In Pro eedings of the 11\nth\nInternational Conferen e on Computational\nLearning Theory, pp. 80{91.\nValiant, L. G. (1984). A theory of the learnable. Communi ations of the ACM, 27, 1134{\n1142.\nValiant, L. G. (1985). Learning disjun tions of onjun tions. In Pro eedings of the 9\nth\nInternational Joint Conferen e on Arti ial Intelligen e, pp. 560{566."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Re ent advan es in the study of voting lassi ation algorithms have brought empiri al and theoreti al results learly showing the dis rimination power of ensemble lassi ers. It has been previously argued that the sear h of this lassi ation power in the design of the algorithms has marginalized the need to obtain interpretable lassi ers. Therefore, the question of whether one might have to dispense with interpretability in order to keep lassi ation strength is being raised in a growing number of ma hine learning or data mining papers. The purpose of this paper is to study both theoreti ally and empiri ally the problem. First, we provide numerous results giving insight into the hardness of the simpli ity-a ura y tradeo for voting lassi ers. Then we provide an e\u00c6 ient \\top-down and prune\" indu tion heuristi , WIDC, mainly derived from re ent results on the weak learning and boosting frameworks. It is to our knowledge the rst attempt to build a voting lassi er as a base formula using the weak learning framework (the one whi h was previously highly su essful for de ision tree indu tion), and not the strong learning framework (as usual for su h lassi ers with boosting-like approa hes). While it uses a well-known indu tion s heme previously su essful in other lasses of on ept representations, thus making it easy to implement and ompare, WIDC also relies on re ent or new results we give about parti ular ases of boosting known as partition boosting and ranking loss boosting. Experimental results on thirty-one domains, most of whi h readily available, tend to display the ability of WIDC to produ e small, a urate, and interpretable de ision ommittees.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}