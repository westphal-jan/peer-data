{"id": "1106.3397", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2011", "title": "Handling uncertainties in SVM classification", "abstract": "this previous paper addresses the implicit pattern classification problem arising when available target data include some significant uncertainty information. target data considered here is either qualitative ( a class label ) methods or quantitative ( an estimation of resolving the posterior probability ). our main contribution is a svm inspired formulation of covering this problem allowing to take into take account class label through a hinge loss principle as well as probability estimates using epsilon - symmetric insensitive cost function together with a minimum norm ( maximum margin ) objective. this formulation shows on a dual form leading to a quadratic problem and allows the use simultaneously of a representer graph theorem and associated kernel. the solution provided can be used for both decision criteria and posterior probability estimation. based on empirical evidence our method method outperforms regular svm in terms results of probability predictions and classification performances.", "histories": [["v1", "Fri, 17 Jun 2011 06:55:24 GMT  (2066kb)", "http://arxiv.org/abs/1106.3397v1", "IEEE Workshop on Statistical Signal Processing, Nice: France (2011)"]], "COMMENTS": "IEEE Workshop on Statistical Signal Processing, Nice: France (2011)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emilie niaf", "r\\'emi flamary", "carole lartizien", "st\\'ephane canu"], "accepted": false, "id": "1106.3397"}, "pdf": {"name": "1106.3397.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["St\u00e9phane CANU"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n10 6.\n33 97\nv1 [\ncs .L\nG ]\n1 7\nJu n\n20 11\nIndex Terms\u2014 support vector machines, maximal margin algorithm, uncertain labels.\n1. INTRODUCTION\nIn the mainstream supervised classification scheme, an expert is required for labelling a set of data used then as inputs for training the classifier. However, even for an expert, this labeling task is likely to be difficult in many applications. In the end the training data set may contain inaccurate classes for some examples, which leads to non robust classifiers[1]. For instance, this is often the case in medical imaging where radiologists have to outline what they think are malignant tissues over medical images without access to the reference histopatologic information. We propose to deal with these uncertainties by introducing probabilistic labels in the learning stage so as to: 1. stick to the real life annotation problem, 2. avoid discarding uncertain data, 3. balance the influence of uncertain data in the classification process. Our study focuses on the widely used Support Vector Machines (SVM) two-class classification problem [2]. This method aims a finding the separating hyperplane maximizing the margin between the examples of both classes. Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4]. In our\napproach, we propose to use both labels and probabilities as input thus learning simultaneously a classifier and a probabilistic output. Note that the output of our classifier may be transformed to probability estimations without using any mapping algorithm. In section 2 we define our new SVM problem formulation (referred to as P-SVM) to deal with certain and probabilistic labels simultaneously. Section 3 describes the whole framework of P-SVM and presents the associated quadratic problem. Finally, in section 5 we compare its performances to the classical SVM formulation (C-SVM) over different data sets to demonstrate its potential.\n2. PROBLEM FORMULATION\nWe present below a new formulation for the two-class classification problem dealing with uncertain labels. Let X be a feature space. We define (xi, li)i=1...m the learning dataset of input vectors (xi)i=1...m \u2208 X along with their corresponding labels (li)i=1...m, the latter of which being\n\u2022 class labels: li = yi \u2208 {\u22121,+1} for i = 1 . . . n (in classification),\n\u2022 real values: li = pi \u2208 [0, 1] for i = n + 1 . . .m (in regression).\npi, associated to point xi allows to consider uncertainties about point xi\u2019s class. We define it as the posterior probability for class 1.\npi = p(xi) = P(Yi = 1 | Xi = xi).\nWe define the associated pattern recognition problem as min w 1 2\u2016w\u2016 2 (1)\nsubject to\n{\nyi(w \u22a4xi + b) \u2265 1, i = 1...n z\u2212i \u2264 w \u22a4xi + b \u2264 z + i , i = n+ 1...m\nWhere boundaries z\u2212i , z + i directly depend on pi. This formulation consists in minimizing the complexity of the model while forcing good classification and good probability estimation (close to pi). Obviously, if n = m, we are brought back to the classical SVM problem formulation.\nFollowing the idea of soft margin introduced in regular SVM to deal with the case of inseparable data, we introduce\nslack variables \u03bei. This measure the degree of misclassification of the datum xi thus relaxing hard constraints of the initial optimization problem which becomes\nmin w,\u03be,\u03be\u2212,\u03be+\n1 2 \u2016w\u20162 + C\nn\u2211\ni=1\n\u03bei + C\u0303\nm\u2211\ni=n+1\n(\u03be\u2212i + \u03be + i ) (2)\nsubject to \n \n\nyi(w \u22a4xi + b) \u2265 1\u2212 \u03bei, i = 1...n z\u2212i \u2212 \u03be \u2212 i \u2264 w \u22a4xi + b \u2264 z + i + \u03be + i , i = n+ 1...m 0 \u2264 \u03bei, i = 1...n 0 \u2264 \u03be\u2212i and 0 \u2264 \u03be + i , i = n+ 1...m\nParametersC and C\u0303 are predefined positive real numbers controlling the relative weighting of classification and regression performances. Let \u03b5 be the labelling precision and \u03b4 the confidence we have in the labelling. Let\u2019s define \u03b7 = \u03b5 + \u03b4. Then, the regression problem consists in finding optimal parameters w and b such that\n| 1\n1 + e\u2212a(w \u22a4xi+b)\n\u2212 pi |< \u03b7 ,\nThus constraining the probability prediction for point xi to remain around to 1\n1+e\u2212a(w \u22a4xi+b)\nwithin distance \u03b7 [5, 6, 7].\nThe boundaries (where w\u22a4xi + b = \u00b11), define parameter a as:\na = ln( 1\u03b7 \u2212 1)\nFinally: max(0, pi \u2212 \u03b7) \u2264\n1\n1 + e\u2212a(w \u22a4xi+b)\n< min(pi + \u03b7, 1),\n\u21d0\u21d2 z\u2212i \u2264 w \u22a4xi + b < z + i ,\nwhere z\u2212i = \u2212 1 a ln( 1 pi\u2212\u03b7 \u2212 1) and z+i = \u2212 1 a ln( 1 pi+\u03b7 \u2212 1).\n3. DUAL FORMULATION\nWe can rewrite the problem in its dual form, introducing Lagrange multipliers. We are looking for a stationary point for the Lagrange function L defined as\nL(w, b, \u03be, \u03be\u2212, \u03be+, \u03b1, \u03b2, \u00b5+, \u00b5\u2212, \u03b3+, \u03b3\u2212) =\n1 2\u2016w\u2016 2 + C\nn\u2211\ni=1\n\u03bei + C\u0303\nm\u2211\ni=n+1\n(\u03be\u2212i + \u03be + i )\n\u2212 n\u2211\ni=1\n\u03b1i(yi(w \u22a4xi + b)\u2212 (1 \u2212 \u03bei))\u2212\nn\u2211\ni=1\n\u03b2i\u03bei\n\u2212\nm\u2211\ni=n+1\n\u00b5\u2212i ((w \u22a4xi + b)\u2212 (z \u2212 i \u2212 \u03be \u2212 i ))\u2212\nm\u2211\ni=n+1\n\u03b3\u2212i \u03be \u2212 i\n\u2212\nm\u2211\ni=n+1\n\u00b5+i ((z + i + \u03be + i )\u2212 (w \u22a4xi + b))\u2212\nm\u2211\ni=n+1\n\u03b3+i \u03be + i\nwith \u03b1 \u2265 0, \u03b2 \u2265 0, \u00b5+ \u2265 0, \u00b5\u2212 \u2265 0,\u03b3+ \u2265 0 and \u03b3\u2212 \u2265 0 Computing the derivatives of L with respect to w, b, \u03be, \u03be\u2212 and\n\u03be+ leads to the following optimality conditions:\n  \n \n0 \u2264 \u03b1i \u2264 C, i = 1...n 0 \u2264 \u00b5+i \u2264 C\u0303, i = n+ 1...m 0 \u2264 \u00b5\u2212i \u2264 C\u0303, i = n+ 1...m\nw = n\u2211\ni=1\n\u03b1iyixi \u2212 m\u2211\ni=n+1\n(\u00b5+i \u2212 \u00b5 \u2212 i )xi\ny\u22a4\u03b1 = m\u2211\ni=n+1\n(\u00b5+i \u2212 \u00b5 \u2212 i )\nwhere e1 = [1 . . . 1\ufe38 \ufe37\ufe37 \ufe38 n times 0 . . . 0 \ufe38 \ufe37\ufe37 \ufe38 (m-n) times ]\u22a4 and e2 = [0 . . . 0\ufe38 \ufe37\ufe37 \ufe38 n times 1 . . . 1 \ufe38 \ufe37\ufe37 \ufe38 (m-n) times ]\u22a4. Calculations simplifications then lead to L(w, b, \u03be, \u03be\u2212, \u03be+, \u03b1, \u03b2, \u00b5, \u03b3+, \u03b3\u2212) =\n\u2212 12w \u22a4w +\nn\u2211\ni=1\n\u03b1i + m\u2211\ni=n+1\n\u00b5\u2212i z \u2212 i \u2212\nm\u2211\ni=n+1\n\u00b5+i z + i\nFinally, let \u0393 = [\u03b11 . . . \u03b1n \u00b5 + n+1 . . . \u00b5 + m \u00b5 \u2212 n+1 . . . \u00b5 \u2212 m] \u22a4 be a vector of dimension 2m\u2212 n. Then\nw\u22a4w = \u0393\u22a4 G \u0393 where\nG =\n\n K1 \u2212 K2 K2 \u2212 K\u22a42 K3 \u2212 K3\nK\u22a42 \u2212 K3 K3\n\n\nwith K1 = (yiyjx \u22a4 i xj)i,j=1...n,\nK2 = (x \u22a4 i xjyi)i=1...n,j=n+1...m, K3 = (x \u22a4 i xj)i,j=n+1...m,\nThe dual formulation becomes\n  \n \nmin \u0393\n1 2\u0393 \u22a4G\u0393\u2212 e\u0303\u22a4\u0393,\nf\u22a4\u0393 = 0 with e\u0303 = [ 1 . . . 1\n\ufe38 \ufe37\ufe37 \ufe38\nn times\n\u2212z+n+1 \u00b7 \u00b7 \u00b7 \u2212 z + m \ufe38 \ufe37\ufe37 \ufe38\nn-m times\nz\u2212n+1 . . . z \u2212 m \ufe38 \ufe37\ufe37 \ufe38\nn-m times\n]\nwith f\u22a4 = [y\u22a4,\u22121 \u00b7 \u00b7 \u00b7 \u2212 1 \ufe38 \ufe37\ufe37 \ufe38\nn-m times\n, 1 . . . 1 \ufe38 \ufe37\ufe37 \ufe38\nn-m times\n]\nand 0 \u2264 \u0393 \u2264 [C . . .C \ufe38 \ufe37\ufe37 \ufe38\nn times\nC\u0303 . . . C\u0303 \ufe38 \ufe37\ufe37 \ufe38\nn-m times\nC\u0303 . . . C\u0303 \ufe38 \ufe37\ufe37 \ufe38\nn-m times\n]\u22a4\n(3)\n4. KERNELIZATION\nFormulations (2) and (3) can be easily generalized by introducing kernel functions. Let k be a positive kernel satisfying Mercer\u2019s condition and H the associated Reproducing Kernel Hilbert Space (RKHS). Within this framework equation (2) becomes\nmin f,b,\u03be,\u03be\u2212,\u03be+\n1 2 \u2016f\u20162H + C\nn\u2211\ni=1\n\u03bei + C\u0303\nm\u2211\ni=n+1\n(\u03be\u2212i + \u03be + i ) (4)\nsubject to\n  \n\nyi(f(xi) + b) \u2265 1\u2212 \u03bei, i = 1...n z\u2212i \u2212 \u03be \u2212 i \u2264 f(xi) + b \u2264 z + i + \u03be + i , i = n+ 1...m 0 \u2264 \u03bei, i = 1...n 0 \u2264 \u03be\u2212i and 0 \u2264 \u03be + i i = n+ 1...m\nFormulation (3) remains identical, with K1 = (yiyjk(xi, xj))i,j=1...n, K2 = (k(xi, xj)yi)i=1...n,j=n+1...m, K3 = (k(xi, xj))i,j=n+1...m,\n5. EXAMPLES\nIn order to experimentally evaluate the proposed method for handling uncertain labels in SVM classification, we have simulated different data sets described below. In these numerical examples, a RBF kernel k(u, v) = e\u2212\u2016u\u2212v\u2016\n2/2\u03c32 is used and C = C\u0303 = 100. We implemented our method using the SVMKM Toolbox [8]. We compare the classification performances and probabilistic predictions of the C-SVM and P-SVM approaches. In the first case, probabilities are estimated by using Platt\u2019s scaling algorithm [3] while in the second case, probabilities are directly estimated via the formula defined in (2): P (y = 1|x) = 1\n1+e\u2212a(w\u22a4x+b) . Performances are evaluated by\ncomputing \u2022 Accuracy (Acc)\nProportion of well predicted examples in the test set (for evaluating classification).\n\u2022 Kullback Leibler distance (KL)\nDKL(P ||Q) =\nn\u2211\ni=1\nP (yi = 1|xi) log( P (yi = 1|xi)\nQ(yi = 1|xi) )\nfor probability distributions P and Q (for evaluating probability estimation).\n5.1. Probability estimation\nWe generate two unidimensional datasets, labelled \u2019+1\u2019 and \u2019-1\u2019, from normal distributions of variances \u03c32\u22121= \u03c3 2 1=0.3 and means \u00b5\u22121=-0.5 and \u00b51=+0.5. Let\u2019s (xli)i=1...nl denote the learning data set (nl=200) and (xti)i=1...nt the test set (nt=1000). We compute, for each point xi, its true probability P (yi = +1|xi) to belong to class \u2019+1\u2019. From here on, learning data are labelled in two ways, as follows\na) For i = 1 . . . nl, we get the regular SVM dataset by simply using a probability of 0.5 as the threshold for assigning class labels yi associated to point xi. This is what would be done in practical cases when the data contains class membership probabilities and a SVM classifier is used.\nif P (yli = 1|x l i) > 0.5, then y l i = 1, if P (yli = 1|x l i) \u2264 0.5, then y l i = \u22121 (5)\nThis dataset (xli, y l i)i=1...nl is used to train the C-SVM\nclassifier. b) We define another data set (xli, y\u0302 l i)i=1...nl such that, for\ni = 1 . . . n,\nif P (yli = 1|x l i) > 1\u2212 \u03b7, then y\u0302 l i = 1, if P (yli = 1|x l i) < \u03b7, then y\u0302 l i = \u22121,\ny\u0302li = P (y l i = 1|x l i) otherwise.\n(6) If the probability values are sufficiently close to 0 or 1 (closeness being defined by the precision and confidence), we admit that they belong respectively to class -1 or 1. This probabilistic dataset (xli, y\u0302 l i)i=1...nl is used to\ntrain the P-SVM algorithm. We compare our two approaches using the test set (xti)i=1...nt . As we know the true probabilities (P (yti = 1|x t i))i=1...nt , we can estimate the probability prediction error (KL). Figure 1 shows the probability predictions performances improvement shown by the P-SVM: the true probabilities (black) and P-SVM estimations (red) are quasi-superimposed (KL=0.2) whereas Platt\u2019s estimations are less accurate (KL=11.3).\n5.2. Noise robustness\nWe generate two 2D datasets, labelled \u2019+1\u2019 and \u2019-1\u2019, from normal distributions of variances \u03c32\u22121=\u03c3 2 1=0.7 and means \u00b5\u22121 = (-0.3, -0.5) and \u00b51=(+0.3, +0.5). As in the previous experiment, we compute class \u20191\u2019 membership probability for each point xl of the learning data set. We simulate classification error by artificially adding a centered uniform noise (\u03b4 of amplitude 0.1), to the probabilities, such that for i = 1 . . . n,\nP\u0302 (yi = 1|xi) = P (yi = 1|xi) + \u03b4i.\nWe then label learning data following the same scheme as described in (5) and (6). Figure 2 shows the margin location and probabilities estimations using the two methods over a grid of values. Far from learning data points, both probability estimations are less accurate, this being directly linked to\nthe choice of a gaussian kernel. However, P-SVM classification and probability estimations obtained for 1000 test points, are clearly more alike the ground truth (AccP-SVM = 99% , KLP-SVM = 3.6) than C-SVM (AccC-SVM = 95%, KLC-SVM = 95). Contrary to P-SVM which, by combining both classification and regression, predicts good probabilities, C-SVM is sensitive to classification noise and is no more converging to the Bayes rule as seen in [1].\nFigure 3 shows the impact of noise amplitude on classifiers performances (values are averaged over 30 random simulations). Even if noise increases, classifications and probability predictions performances of the P-SVM remain significantly higher than those of C-SVM.\n6. CONCLUSION\nThis paper has presented a new way to take into account both qualitative and quantitative target data by shrewdly combin-\ning both SVM classification and regression loss. Experimental results show that our formulation can perform very well on simulated data for discrimination as well as posterior probability estimation. This approach will soon be applied on clinical data thus allowing to assess its usefulness in computer assisted diagnosis for prostate cancer. Note that this framework initially designed for probabilistic labels can also be generalized to other dataset involving quantitative data as it can be used for instance to estimate a conditional cumulative distribution function.\n7. REFERENCES\n[1] G. Stempfel and L. Ralaivola, \u201cLearning SVMs from Sloppily Labeled Data,\u201d Artificial Neural Networks\u2013ICANN 2009, pp. 884\u2013893, 2009.\n[2] Bernhard Scho\u0308lkopf and Alexander J. Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning), The MIT Press, 1st edition, December 2001.\n[3] John C. Platt, \u201cProbabilistic outputs for support vector machines and comparisons to regularized likelihood methods,\u201d in Advances in large margin classifiers. 1999, pp. 61\u201374, MIT Press.\n[4] Peter Sollich, \u201cProbabilistic methods for support vector machines,\u201d in Advances in Neural Information Processing Systems 12. 2000, pp. 349\u2013355, MIT Press.\n[5] S. Ru\u0308ping, \u201cA Simple Method For Estimating Conditional Probabilities For SVMs,\u201d in LWA 2004, Bickel S. Brefeld U. Drost I. Henze N. Herden O. Minor M. Scheffer T. Stojanovic L. Abecker, A. and S. Weibelza hl, Eds., 2004.\n[6] Y. Grandvalet, J. Marie\u0301thoz, and S. Bengio, \u201cA probabilistic interpretation of SVMs with an application to unbalanced classification,\u201d Advances in Neural Information Processing Systems, vol. 18, pp. 467, 2006.\n[7] S. Rueping, \u201cSVM Classifier Estimation from Group Probabilities,\u201d in ICML 2010.\n[8] S. Canu, Y. Grandvalet, V. Guigue, and A. Rakotomamonjy, \u201cSvm and kernel methods matlab toolbox,\u201d Perception Syste\u0300mes et Information, INSA de Rouen, Rouen, France, 2005.\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 0\n0.5\n1\nx\npr ob\nab ili\nty\nP(x|y=1) P(x|y=\u22121) P(y=1|x)\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 0\n0.5\n1\nx\npr ob\nab ili\nty\nP(y=1|x) P(y=1|x) C\u2212SVM + Platt P(y=1|x) P\u2212SVM\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 0\n0.2\n0.4\n0.6\n0.8\n1\nx\npr ob\nab ili\nty\nP(x|y=1) P(x|y=\u22121) P(y=1|x)\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5 0\n0.2\n0.4\n0.6\n0.8\n1\nx\npr ob\nab ili\nty\nP(y=1|x) P(y=1|x) C\u2212SVM + Platt P(y=1|x) P\u2212SVM\n0.5\n0.5\n0.5\n0.5\nC\u2212SVM + Platt probability estimates\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nP\u2212SVM probability estimates\n0.5\n0.5\n0.5\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nC\u2212SVM + Platt probability estimates\n\u22121 0 1\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nP\u2212SVM probability estimates\n\u22121 0 1\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n\u22121 0 1\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0.5\n0.5\n0.5\n0.5\nPlatt probability estimates\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\noutputs of the extended SVM\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0. 5\n0.5\n0.50.5\nPlatt probability estimates\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\noutputs of the extended SVM\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n0.5\n0.5\n0.5\n0.5\nTrue probabilities\n\u22121.5 \u22121 \u22120.5 0 0.5 1 1.5\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5"}], "references": [{"title": "Learning SVMs from Sloppily Labeled Data,", "author": ["G. Stempfel", "L. Ralaivola"], "venue": "Artificial Neural Networks\u2013ICANN", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning)", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,\u201d in Advances in large margin classifiers", "author": ["John C. Platt"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Probabilistic methods for support vector machines,", "author": ["Peter Sollich"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "A Simple Method For Estimating Conditional Probabilities For SVMs,", "author": ["S. R\u00fcping"], "venue": "LWA", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "and S", "author": ["Y. Grandvalet", "J. Mari\u00e9thoz"], "venue": "Bengio, \u201cA probabilistic interpretation of SVMs with an application to unbalanced classification,\u201d Advances in Neural Information Processing Systems, vol. 18, pp. 467", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "SVM Classifier Estimation from Group Probabilities,", "author": ["S. Rueping"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "and A", "author": ["S. Canu", "Y. Grandvalet", "V. Guigue"], "venue": "Rakotomamonjy, \u201cSvm and kernel methods matlab toolbox,\u201d Perception Syst\u00e8mes et Information, INSA de Rouen, Rouen, France", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "In the end the training data set may contain inaccurate classes for some examples, which leads to non robust classifiers[1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Our study focuses on the widely used Support Vector Machines (SVM) two-class classification problem [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4].", "startOffset": 104, "endOffset": 110}, {"referenceID": 3, "context": "Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4].", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "n (in classification), \u2022 real values: li = pi \u2208 [0, 1] for i = n + 1 .", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e \u22a4xi+b) \u2212 pi |< \u03b7 , Thus constraining the probability prediction for point xi to remain around to 1 1+e\u2212a(w xi+b) within distance \u03b7 [5, 6, 7].", "startOffset": 227, "endOffset": 236}, {"referenceID": 5, "context": "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e \u22a4xi+b) \u2212 pi |< \u03b7 , Thus constraining the probability prediction for point xi to remain around to 1 1+e\u2212a(w xi+b) within distance \u03b7 [5, 6, 7].", "startOffset": 227, "endOffset": 236}, {"referenceID": 6, "context": "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e \u22a4xi+b) \u2212 pi |< \u03b7 , Thus constraining the probability prediction for point xi to remain around to 1 1+e\u2212a(w xi+b) within distance \u03b7 [5, 6, 7].", "startOffset": 227, "endOffset": 236}, {"referenceID": 7, "context": "We implemented our method using the SVMKM Toolbox [8].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "In the first case, probabilities are estimated by using Platt\u2019s scaling algorithm [3] while in the second case, probabilities are directly estimated via the formula defined in (2): P (y = 1|x) = 1 1+e\u2212a(w\u22a4x+b) .", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Contrary to P-SVM which, by combining both classification and regression, predicts good probabilities, C-SVM is sensitive to classification noise and is no more converging to the Bayes rule as seen in [1].", "startOffset": 201, "endOffset": 204}], "year": 2011, "abstractText": "This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using \u03b5-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.", "creator": "LaTeX with hyperref package"}}}