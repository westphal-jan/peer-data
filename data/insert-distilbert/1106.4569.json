{"id": "1106.4569", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "The Communicative Multiagent Team Decision Problem: Analyzing Teamwork Theories and Models", "abstract": "despite the theoretically significant progress in multiagent business teamwork, seemingly existing research does not address the optimality of its prescriptions nor the algorithm complexity of the teamwork problem. without a true characterization of the discrete optimality - complexity tradeoffs, it is somewhat impossible to determine then whether the assumptions and approximations made by a particular theory gain enough efficiency to justify the losses in overall performance. to provide a tool designed for use by multiagent researchers in both evaluating this approximation tradeoff, we present a unified framework, the communicative multiagent team decision problem ( com - presented mtdp ). the com - mtdp model combines and extends existing multiagent theories, entities such as decentralized cognitive partially observable markov decision processes framework and economic team theory. in addition to their generality of representation, com - processed mtdps also support the analysis of both the partial optimality of team performance and the total computational complexity parameter of the agents'decision problem. in analyzing complexity, suppose we repeatedly present a breakdown of the computational complexity of constructing new optimal teams under various classes of problem domains, along the dimensions of observability and communication cost. in analyzing optimality, we exploit the com - time mtdp'n s ability to encode existing teamwork theories and behavioral models to encode two instantiations of complex joint intentions theory taken from the literature. alternatively furthermore, the com - mtdp model provides a basis for the detailed development of novel team coordination algorithms. we derive hence a domain - independent criterion for optimal communication and provide a comparative perspective analysis of the two joint intentions instantiations with respect to this optimal position policy. we again have likewise implemented a reusable, domain - independent algorithm software package based on com - mtdps to analyze teamwork problem coordination strategies, and we demonstrate its use by encoding and evaluating the two joint intentions strategies within an example domain.", "histories": [["v1", "Wed, 22 Jun 2011 20:55:38 GMT  (303kb)", "http://arxiv.org/abs/1106.4569v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["d v pynadath", "m tambe"], "accepted": false, "id": "1106.4569"}, "pdf": {"name": "1106.4569.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["David V. Pynadath", "pynadath isi.edu", "Milind Tambe"], "emails": [], "sections": [{"heading": "Journal of Arti ial Intelligen e Resear h 16 (2002) 389-423 Submitted 2/02; published 6/02", "text": "The Communi ative Multiagent Team De ision Problem:Analyzing Teamwork Theories and ModelsDavid V. Pynadath pynadath isi.eduMilind Tambe tambe us .eduInformation S ien es Institute and Computer S ien e DepartmentUniversity of Southern California4676 Admiralty Way, Marina del Rey, CA 90292 USAAbstra tDespite the signi ant progress in multiagent teamwork, existing resear h does not ad-dress the optimality of its pres riptions nor the omplexity of the teamwork problem. With-out a hara terization of the optimality- omplexity tradeo s, it is impossible to determinewhether the assumptions and approximations made by a parti ular theory gain enoughe\u00c6 ien y to justify the losses in overall performan e. To provide a tool for use by mul-tiagent resear hers in evaluating this tradeo , we present a uni ed framework, the COM-muni ative Multiagent Team De ision Problem (COM-MTDP). The COM-MTDP model ombines and extends existing multiagent theories, su h as de entralized partially observ-able Markov de ision pro esses and e onomi team theory. In addition to their generalityof representation, COM-MTDPs also support the analysis of both the optimality of teamperforman e and the omputational omplexity of the agents' de ision problem. In analyz-ing omplexity, we present a breakdown of the omputational omplexity of onstru tingoptimal teams under various lasses of problem domains, along the dimensions of observ-ability and ommuni ation ost. In analyzing optimality, we exploit the COM-MTDP'sability to en ode existing teamwork theories and models to en ode two instantiations ofjoint intentions theory taken from the literature. Furthermore, the COM-MTDP modelprovides a basis for the development of novel team oordination algorithms. We derive adomain-independent riterion for optimal ommuni ation and provide a omparative anal-ysis of the two joint intentions instantiations with respe t to this optimal poli y. We haveimplemented a reusable, domain-independent software pa kage based on COM-MTDPs toanalyze teamwork oordination strategies, and we demonstrate its use by en oding andevaluating the two joint intentions strategies within an example domain.1. Introdu tionA entral hallenge in the ontrol and oordination of distributed agents is enabling themto work together, as a team, toward a ommon goal. Su h teamwork is riti al in a vastrange of domains|for future teams of orbiting spa e raft, sensors for tra king targets, un-manned vehi les for urban battle elds, software agents for assisting organizations in rapid risis response, et . Resear h in teamwork theory has built the foundations for su essfulpra ti al agent team implementations in su h domains. On the forefront are theories basedon belief-desire-intentions (BDI) frameworks, su h as joint intentions (Cohen & Levesque,1991b, 1991a; Levesque, Cohen, & Nunes, 1990), SharedPlans (Grosz, 1996; Grosz & Kraus,1996; Grosz & Sidner, 1990), and others (Sonenberg, Tidhar, Werner, Kinny, Ljungberg,& Rao, 1994; Dunin-Kepli z & Verbrugge, 1996), that have provided pres riptions for o- 2002 AI A ess Foundation and Morgan Kaufmann Publishers. All rights reserved.\nPynadath & Tambeordination in pra ti al systems. These theories have inspired the onstru tion of pra ti- al, domain-independent teamwork models and ar hite tures (Jennings, 1995; Pynadath,Tambe, Chauvat, & Cavedon, 1999; Ri h & Sidner, 1997; Tambe, 1997; Yen, Yin, Ioerger,Miller, Xu, & Volz, 2001), su essfully applied in a range of omplex domains.Yet, two key short omings limit the s alability of these BDI-based theories and imple-mentations. First, there are no te hniques for the quantitative evaluation of the degree ofoptimality of their oordination behavior. While optimal teamwork may be impra ti al inreal-world domains, su h analysis would aid us in omparison of di erent theories/modelsand in identifying feasible improvements. One key reason for the di\u00c6 ulty in quantitativeevaluation of most existing teamwork theories is that they ignore the various un ertain-ties and osts in real-world environments. For instan e, joint intentions theory (Cohen &Levesque, 1991b) pres ribes that team members attain mutual beliefs in key ir umstan es,but it ignores the ost of attaining mutual belief (e.g., via ommuni ation). Implementa-tions that blindly follow su h pres riptions ould engage in highly suboptimal oordination.On the other hand, pra ti al systems have addressed osts and un ertainties of real-worldenvironments. For instan e, STEAM (Tambe, 1997; Tambe & Zhang, 1998) extends jointintentions with de ision-theoreti ommuni ation sele tivity. Unfortunately, the very prag-matism of su h approa hes often ne essarily leads to a la k of theoreti al rigor, so it remainsunanswered whether STEAM's sele tivity is the best an agent an do, or whether it is evenne essary at all. The se ond key short oming of existing teamwork resear h is the la kof a hara terization of the omputational omplexity of various aspe ts of teamwork de i-sions. Understanding the omputational advantages of a pra ti al oordination pres ription ould potentially justify the use of that pres ription as an approximation to optimality inparti ular domains.To address these short omings, we propose a new omplementary framework, the COM-muni ative Multiagent Team De ision Problem (COM-MTDP), inspired by work in e o-nomi team theory (Mars hak & Radner, 1971; Yoshikawa, 1978; Ho, 1980). While ourCOM-MTDP model borrows from a theory developed in another eld, we make several ontributions in applying and extending the original theory, most notably adding expli itmodels of ommuni ation and system dynami s. With these extensions, the COM-MTDPgeneralizes other re ently developed multiagent de ision frameworks, su h as de entralizedPOMDPs (Bernstein, Zilberstein, & Immerman, 2000).Our de nition of a team (like that in e onomi team theory) assumes only that teammembers have a ommon goal and that they work sel essly towards that goal (i.e., theyhave no other private goals of their own). In terms of our de ision-theoreti framework, weassume that all of the team members share the same joint utility fun tion|that is, ea hteam member's individual preferen es are exa tly the preferen es of the other members and,thus, of the team as a whole. Our de nition may appear to be a \\bare-bones\" de nition ofa team, sin e it does not in lude ommon on epts and assumptions from the literature onwhat onstitutes a team (e.g., the teammates form a joint ommitment (Cohen & Levesque,1991b), attain mutual belief upon termination of a joint goal, intend that teammates su - eed in their tasks (Grosz & Kraus, 1996), et .). From our COM-MTDP perspe tive, weview these on epts as more intermediate on epts, as the means by whi h agents improvetheir team's overall performan e, rather than ends in themselves. Our hypothesis in thisinvestigation is that our COM-MTDP-based analysis an provide on rete justi ations for390\nThe Communi ative Multiagent Team De ision Problemthese on epts. For example, while mutual belief has no inherent value, our COM-MTDPmodel an quantify the improved performan e that we would expe t from a team thatattains mutual belief about important aspe ts of its exe ution.More generally, this paper demonstrates three new types of teamwork analyses madepossible by the COM-MTDP model. First, we analyze the omputational omplexity ofteamwork within sub lasses of problem domains. For instan e, some resear hers have ad-vo ated teamwork without ommuni ation (Goldberg & Matari , 1997). We use the COM-MTDP model to show that, in general, the problem of onstru ting optimal teams without ommuni ation is NEXP- omplete, but allowing free ommuni ation redu es the problemto be PSPACE- omplete. This paper presents a breakdown of the omplexity of optimalteamwork over problem domains lassi ed along the dimensions of observability and om-muni ation ost.Se ond, the COM-MTDP model provides a powerful tool for omparing the optimalityof di erent oordination pres riptions a ross lasses of domains. Indeed, we illustrate thatwe an en ode existing team oordination strategies within a COM-MTDP for evaluation.For our analysis, we sele ted two joint intentions-based approa hes from the literature: oneusing the approa h realized within GRATE* and the joint responsibility model (Jennings,1995), and another based on STEAM (Tambe, 1997). Through this en oding, we derive the onditions under whi h these team oordination strategies generate optimal team behavior,and the omplexity of the de ision problems addressed by them. Furthermore, we alsoderive a novel team oordination algorithm that outperforms these existing strategies inoptimality, though not in e\u00c6 ien y. The end result is a well-grounded hara terization ofthe omplexity-optimality tradeo among various means of team oordination.Third, we an use the COM-MTDP model to empiri ally analyze a spe i domain ofinterest. We have implemented reusable, domain-independent algorithms that allow one toevaluate the optimality of the behavior generated by di erent pres riptive poli ies within aproblem domain represented as a COM-MTDP. We apply these algorithms in an exampledomain to empiri ally evaluate the aforementioned team oordination strategies, hara -terizing the optimality of ea h strategy as a fun tion of the properties of the underlyingdomain. For instan e, Jennings reports experimental results (Jennings, 1995) indi atingthat his joint responsibility teamwork model leads to lower waste of ommunity e ort than ompeting methods of a omplishing teamwork. With our COM-MTDP model, we wereable to demonstrate the bene ts of Jennings' approa h under many on gurations of our ex-ample domain. However, in pre isely hara terizing the types of domains that showed su hbene ts, we also identi ed domains where these ompeting methods may a tually performbetter. In addition, we an use our COM-MTDP model to re- reate and explain previouswork that noted an instan e of suboptimality in a STEAM-based, real-world implementa-tion (Tambe, 1997). While this previous work treated that suboptimality as anomalous, ourCOM-MTDP re-evaluation of the domain demonstrated that the observed suboptimalitywas a symptom of STEAM's general propensity towards extraneous ommuni ation in asigni ant range of domain types. Both the algorithms and the example domain model areavailable for publi use in an Online Appendix 1.Se tion 2 presents the COM-MTDP model's representation and pla es it in the ontextof related multiagent models from the literature. Se tion 3 uses the COM-MTDP model tode ne and hara terize the omplexity of designing optimal agent teams. Se tion 4 analyzes391\nPynadath & Tambethe optimality of existing team oordination algorithms and derives a novel oordinationalgorithm. Se tion 5 presents empiri al results from applying our COM-MTDP algorithmsto an example domain. Se tion 6 summarizes our results, and Se tion 7 identi es somepromising future dire tions.2. The COM-MTDP ModelThis se tion de nes and des ribes the COM-MTDP model itself and its ability to representthe important aspe ts of multiagent teamwork. We begin in Se tion 2.1 by de ning theunderlying multiagent team de ision problem with no expli it ommuni ation. Se tion 2.2de nes the omplete COM-MTDP model with its extension to expli itly represent ommu-ni ation. Se tion 2.3 provides an illustration of how the COM-MTDP model represents theexe ution of a team of agents. Finally, Se tion 2.4 des ribes related models of multiagent oordination and shows how the COM-MTDP model generalizes them.2.1 Multiagent Team De ision ProblemsGiven a team of sel ess agents, , who intend to perform some joint task, we wish to evaluatepossible poli ies of behavior. We represent a multiagent team de ision problem (MTDP)model as a tuple, hS;A ; P; ;O ;B ; Ri. We have taken the underlying omponents ofthis model from the initial team de ision model (Ho, 1980), but we have extended them tohandle dynami de isions over time and to more easily represent multiagent domains (inparti ular, agent beliefs). We assume that the model is ommon knowledge to all of theteam members. In other words, all of the agents believe the same model, and they believethat they all believe the same model, et .2.1.1 World States: S S = 1 m: a set of world states, expressed as a fa tored representation (a ross produ t of separate features).The state of the world here is the state of the team's environment (e.g., terrain, lo ation ofenemy). Thus, ea h i represents the domain of an individual feature of that environment,while S represents the domain of all possible ombinations of values over the individualfeatures.2.1.2 Domain-Level A tions: A fAigi2 is a set of a tions for ea h agent to perform to hange its environment, impli itlyde ning a set of ombined a tions, A Qi2 Ai ( orresponding to team theory's de isionvariables).Extension to Dynami Problem: P The original team de ision problem fo used ona one-shot, stati problem. We extend the original on ept so that ea h omponent is atime series of random variables. The e e ts of domain-level a tions (e.g., a ying a tion hanges a heli opter's position) obey a probabilisti distribution, given by a fun tion P :S A S ! [0; 1\u2104. In other words, for ea h initial state s at time t, ombined a tion a392\nThe Communi ative Multiagent Team De ision Problemtaken at time t, and nal state s0 at time t+ 1, Pr(St+1 = s0jSt = s;At = a) = P (s;a; s0).The given de nition of P assumes that the world dynami s obey the Markov assumption.2.1.3 Agent Observations: f igi2 is a set of observations that ea h agent, i, an experien e of its world, impli itlyde ning a ombined observation, Qi2 i. i may in lude elements orrespondingto indire t eviden e of the state (e.g., sensor readings) and a tions of other agents (e.g.,movement of other heli opters). In the original team-theoreti framework, the informationstru ture that represented the observation pro ess of the agents was a set of deterministi fun tions, Oi : S ! i.Extension of Allowable Information Stru tures: O We extend the informationstru ture representation to allow for un ertain observations. We use a general sto hasti model, borrowed from the partially observable Markov de ision pro ess model (Smallwood &Sondik, 1973), with a joint observation fun tion: O (s;a;!) = Pr( t = !jSt = s;At 1 =a). This fun tion models the sensors, representing any errors, noise, et . In some ases, we an separate this joint distribution into individual observation fun tions: O Qi2 Oi,where Oi(s;a; !) = Pr( ti = !jSt = s;At 1 = a). Thus, the probability distributionspe i ed by O forms the ri her information stru ture used in our model. We an makeuseful distin tions between di erent lasses of information stru tures:Colle tive Partial Observability This is the general ase, where we make no assump-tions on the observations.Colle tive Observability There is a unique world state for the ombined observations ofthe team: 8! 2 , 9s 2 S su h that 8s0 6= s, Pr( t = !jSt = s0) = 0. The setof domains that are olle tively observable is a stri t subset of the domains that are olle tively partially observable.Individual Observability There is a unique world state for ea h individual agent's ob-servations: 8! 2 i, 9s 2 S su h that 8s0 6= s, Pr( ti = !jSt = s0) = 0. The setof domains that are individually observable is a stri t subset of the domains that are olle tively observable.Non-Observability The agents re eive no feedba k from the world: 9! 2 i, su h that8s 2 S and 8a 2 A , Pr( ti = !jSt = s;At 1 = a) = 1. This assumption holdsin open-loop systems, whi h ome under frequent onsideration in lassi al plan-ning (Boutilier, Dean, & Hanks, 1999).2.1.4 Poli y (Strategy) Spa e iA is a domain-level poli y (or strategy, in the original team theory spe i ation) to mapan agent's belief state to an a tion. In the original formalism, the agent's beliefs orresponddire tly to its observations (i.e., iA : i ! Ai).Extension to Ri her Belief State Spa e: B We generalize the set of possible strate-gies to apture the more omplex mental states of the agents. Ea h agent, i 2 , forms abelief state, bti 2 Bi, based on its observations seen through time t, where Bi ir ums ribes393\nPynadath & Tambethe set of possible belief states for the agent. Thus, we de ne the set of possible domain-level poli ies as mappings from belief states to a tions, iA : Bi ! Ai. We de ne the setof possible ombined belief states over all agents to be B Qi2 Bi. The orrespondingrandom variable, bt , represents the agents' ombined belief state at time t. We elaborateon di erent types of belief states and the mapping of observations to belief states (i.e., thestate estimator fun tion) in Se tion 2.2.1.2.1.5 Reward Fun tion: RA ommon reward fun tion is entral to the notion of teamwork in a MTDP: R : S A !R. This fun tion represents the team's joint preferen es over states and the ost of domain-level a tions (e.g., destroying enemy is good, returning to home base with only 10% oforiginal for e is bad). We assume that, as sel ess team members, ea h agent shares thesepreferen es at the individual level as well. Therefore, ea h team member wants exa tlywhat is best for the team as a whole.2.2 Extension for Expli it Communi ation: We make an expli it separation between domain-level a tions (A ) and ommuni ativea tions. As de ned in this se tion, ommuni ative a tions a e t the re eiving agents' indi-vidual belief states, but, unlike domain-level a tions, they do not dire tly hange the worldstate. Although this distin tion is sometimes blurry in real-world domains, we make thisexpli it separation so as to isolate, as mu h as possible, the e e ts of the two types ofa tions. The leverage gained from this separation provides the basis for the informative,analyti al results presented in the rest of this paper. To apture this separation, we extendour initial MTDP model to be a ommuni ative multiagent team de ision problem (COM-MTDP), that we de ne as a tuple, hS;A ; ; P; ;O ;B ; Ri, with a new omponent, , and an extended reward fun tion, R.2.2.1 Communi ation: f igi2 is a set of possible messages for ea h agent, impli itly de ning a set of ombined ommuni ations, Qi2 i. An agent, i, may ommuni ate message x 2 i to itsteammates, who interpret the ommuni ation by updating their belief states in response. Asa rst step in this work, we assume that all of the agents re eive the messages instantaneouslyand orre tly (i.e., there is no lag or noise in the ommuni ation hannels). This model is ommon knowledge among all of the team members, so on e an agent has sent a message,it knows that its team members have re eived the message, and its team members knowthat it knows that they have all re eived the message, and so on.With ommuni ation, we divide ea h de ision epo h into two phases: the pre- ommuni- ation and post- ommuni ation phases, denoted by the subs ripts and , respe tively.In parti ular, the agents update their belief states at two distin t points within ea h de- ision epo h: on e upon re eiving observation ti (produ ing the pre- ommuni ation be-lief state bti ), and again upon re eiving the other agents' messages (produ ing the post- ommuni ation belief state bti ). The distin tion allows us to di erentiate between the beliefstate used by the agents in sele ting their ommuni ation a tions and the more \\up-to-date\"belief state used in sele ting their domain-level a tions. We also distinguish between the394\nThe Communi ative Multiagent Team De ision Problemseparate state-estimator fun tions used in ea h update phase:b0i =SE0i () (1)bti =SEi (bt 1i ; ti) (2)bti =SEi (bti ; t ) (3)where SEi : Bi i ! Bi is the pre- ommuni ation state estimator for agent i, andSEi : Bi ! Bi is the post- ommuni ation state estimator for agent i. The initialstate estimator, SE0i : ; ! Bi, spe i es the agent's prior beliefs, before any observationsare made. For ea h of these, we also make the obvious de nitions for the orrespondingestimators for the ombined belief states: SE , SE , and SE0 .In this paper, as a rst step, we assume that the agents have perfe t re all. In otherwords, the agents re all all of their observations, as well as all ommuni ation of the otheragents. Thus, their belief states an represent their entire histories as sequen es of obser-vations and re eived messages: Bi = i , where X denotes the set of all possiblesequen es (of any length) of elements of X. The agents realize perfe t re all through thefollowing state estimator fun tions:SE0i () = hi (4)SEi ( 0i ; 0 ; : : : ; t 1i ; t 1 ; ti)= 0i ; 0 ; : : : ; t 1i ; t 1 ; ti; (5)SEi ( 0i ; 0 ; : : : ; t 1i ; t 1 ; ti; ; t )= 0i ; 0 ; : : : ; ti; t (6)In other words, SE0i initializes agent i's belief state to be an empty history, SEi appends anew observation to agent i's belief state, and SEi appends new messages to agent i's beliefstate. Under this paper's assumptions of perfe t re all, all three state-estimator fun tionstake only onstant time. However, we an potentially allow more omplex fun tions (thoughthe omplexity results presented hold only if the state-estimator fun tions take polynomialtime). For instan e, although we assume perfe t, syn hronous, instantaneous ommuni a-tion here, we ould potentially use the post- ommuni ation state estimator to model anynoise, temporal delays, asyn hrony, ognitive burden, et . present in the ommuni ation hannel.We extend our de nition of a poli y of behavior to in lude a ommuni ation poli y, i : Bi ! i, analogous to Se tion 2.1.4's domain-level poli y. We de ne the joint poli ies, and A, as the ombined poli ies a ross all agents in .2.2.2 Extended Reward Fun tion: RWe extend the team's reward fun tion to also represent the ost of ommuni ative a ts (e.g., ommuni ation hannels may have asso iated ost): R : S A ! R. We assume thatthe ost of ommuni ation and of domain-level a tions are independent of ea h other, so we an de ompose the reward fun tion into two omponents: a ommuni ation-level reward,R : S ! R, and a domain-level reward, RA : S A ! R. The total reward isthe sum of the two omponent values: R(s;a; ) = RA(s;a) + R (s; ). We assume that395\nPynadath & Tambe ommuni ation has no inherent bene t and may instead have some ost, so that for allstates, s 2 S, and messages, 2 , the reward is never positive: R (s; ) 0. However,although we assign ommuni ation no expli it value, it an have signi ant impli it valuethrough its e e t on the agents' belief states and, subsequently, on their future a tions.As with the observability fun tion, we parameterize the ommuni ation osts asso iatedwith message transmissions:General Communi ation: We make no assumptions about ommuni ation.Free Communi ation: R (s; ) = 0 for any 2 , and s 2 S. In other words, ommuni ation a tions have no e e t on the agents' reward.No ommuni ation: = ;, i.e., no expli it ommuni ation. Alternatively, ommuni a-tion may be prohibitively expensive, so that 8 2 , and s 2 S, R (s; ) = 1.The free- ommuni ation ase appears in the literature, when resear hers wish to fo uson issues other than ommuni ation ost. Although, real-world domains rarely exhibitsu h ideal onditions, we may be able to model some domains as having approximately free ommuni ation to a su\u00c6 ient degree. In addition, analyzing this extreme ase gives us someunderstanding of the bene t of ommuni ation, even if the results do not apply a ross alldomains. We also identify the no- ommuni ation ase be ause su h de ision problems havebeen of interest to resear hers as well (Goldberg &Matari , 1997). Of ourse, even if = ;,it is possible that there are domain-level a tions in A that have impli it ommuni ativevalue by a ting as signals that onvey information to the other agents. However, we stilllabel su h agent teams as having no ommuni ation for the purposes of the work here, sin emany of our results exploit an expli it separation between domain- and ommuni ation-levela tions.2.3 Model IllustrationWe an view the evolving state as a Markov hain with separate stages for domain-leveland ommuni ation-level a tions. In other words, ea h agent team member, i 2 beginsin some initial state, S0, with initial belief states, b0i = SE0i (). Ea h agent re eives anobservation 0i drawn a ording to the probability distribution O (S0;null; 0 ) (there areno a tions yet). Then, ea h agent updates its belief state, b0i = SEi (b0i ; 0i ).Next, ea h agent i 2 sele ts a message a ording to its ommuni ation poli y, 0i = i (b0i ), de ning a ombined ommuni ation, 0 . Ea h agent interprets the ommu-ni ations of all of the others by updating its belief state, b0i = SEi (b0i ; 0 ). Ea hthen sele ts an a tion a ording to its domain-level poli y, A0i = iA(b0i ), de ning a ombined a tion A0 . By our entral assumption of teamwork, ea h agent re eives thesame joint reward, R0 = R(S0;A0 ; 0 ). The world then moves into a new state, S1,a ording to the distribution, P (S0;A0 ). Again, ea h agent i re eives an observation 1idrawn from i a ording to the distributionO (S1;A0 ; 1 ), and it updates its belief state,b1i = SEi (b0i ; 1i ).The pro ess ontinues, with agents hoosing ommuni ation- and domain-level a tions,observing the e e ts, and updating their beliefs. Thus, in addition to the time series of worldstates, S0; S1; : : : ; St, the agents themselves determine a time series of ommuni ation-level396\nThe Communi ative Multiagent Team De ision Problemand domain-level a tions, 0 ; 1 ; : : : ; t and A1 ;A1 ; : : : ;At , respe tively. We also havea time series of observations for ea h agent i, 0i ; 1i ; : : : ; ti. Likewise, we an treat the ombined observations, 0 ; 1 ; : : : ; t , as a similar time series of random variables.Finally, the agents re eive a series of rewards, R0; R1; : : : ; Rt. We an de ne the value,V , of the poli ies, A and , as the expe ted reward re eived when exe uting thosepoli ies. Over a nite horizon, T , this value is equivalent to the following:V T ( A; ) = E \" TXt=0 Rt A; # (7)2.4 Related WorkThe COM-MTDP model subsumes many existing multiagent models, as presented in Ta-ble 1 (i.e., we an map any instan e of these models into a orresponding COM-MTDP).This generality enables us to perform novel analyses of real-world teamwork domains, asdemonstrated by Se tion 4's use of the COM-MTDP model for analyzing the optimality of ommuni ation de isions.2.4.1 De entralized POMDPsWith its model of observability and world dynami s, our COM-MTDP model losely par-allels the stru ture of the de entralized partially observable Markov de ision pro ess (DEC-POMDP) (Bernstein et al., 2000). Following our notational onventions, a DEC-POMDPis a tuple, hS;A ; P; ; O ; Ri. There is no set of possible messages, , so the DEC-POMDP falls into the lass of domains with no ommuni ation. The DEC-POMDP obser-vational model, O, is general enough to apture olle tively partially observable domains.2.4.2 Partially Observable Identi al Payoff Sto hasti GamesSto hasti games provide a ri h framework for multiagent de ision making when the agentsmay have their own individual goals and preferen es. The identi al payo sto hasti game(IPSG) restri ts the agents to share a single payo fun tion, appropriate for modelingthe single, global reward fun tion of the team ontext. The partially observable IPSG(POIPSG) (Peshkin, Kim, Meuleau, & Kaelbling, 2000) is a tuple, hS;A ; P; ;O ; Ri,very similar to the DEC-POMDP model. In other words, the observation fun tion, O , isgeneral enough to support olle tively partially observable domains, and there is no ommu-ni ation.2.4.3 Multiagent MDPsAnother relevant model is the multiagent Markov de ision pro ess (MMDP) (Boutilier,1996), whi h is a tuple, hS;A ; P;Ri, in our notation. Like the DEC-POMDP, the MMDPhas no ommuni ation. In addition, the MMDP is a multiagent extension to the ompletelyobservable MDP model, so it assumes an environment that is individually observable. 397\nPynadath & TambeModel O DEC-POMDP no ommuni ation olle tive partial observabilityPOIPSG no ommuni ation olle tive partial observabilityMMDP no ommuni ation individual observabilityXuan-Lesser general ommuni ation olle tive observabilityTable 1: Existing models as COM-MTDP subsets.2.4.4 Xuan-Lesser FrameworkThe COM-MTDP's separation of ommuni ation from other a tions is similar to previouswork on multiagent de ision models (Xuan, Lesser, & Zilberstein, 2001), whi h supportedgeneral ommuni ation. However, while the Xuan-Lesser model generalizes beyond indi-vidually observable environments, it supports only a subset of olle tively observable envi-ronments. In parti ular, the Xuan-Lesser framework annot represent agents who re eivelo al observations of a ommon world state, where the observations of di erent agents ouldpotentially be interdependent.3. COM-MTDP Complexity AnalysisWe an use the COM-MTDP model to prove some results about the omplexity of on-stru ting optimal agent teams (i.e., teams that oordinate to produ e optimal behavior ina problem domain). The problem fa ing these agents (or the designer of these agents) ishow to onstru t the joint poli ies, and A, so as to maximize their joint reward,as represented by the expe ted value, V T ( A; ). In all of the results presented, weassume that all of the values in a model instan e (e.g., transition probabilities, rewards) arerational numbers, so that we an express the parti ular instan e as a nite-sized input.Theorem 1 The de ision problem of whether there exist poli ies, and A, for a givenCOM-MTDP, under general ommuni ation and olle tive partial observability, that yielda total reward at least K over some nite horizon T is NEXP- omplete if j j 2 (i.e.,more than one agent).Proof: To prove that the COM-MTDP de ision problem is NEXP-hard, we redu e a DEC-POMDP (Bernstein et al., 2000) to a COM-MTDP with no ommuni ation by opyingall of the other model features from the given DEC-POMDP. In other words, if we aregiven a DEC-POMDP, S; fAigmi=1; P; f igmi=1; O;R , we an onstru t a COM-MTDP,hS0; fA0igmi=1; 0 ; P 0; f 0igmi=1;O0 ;B0 ; R0i, as follows:S0 = SA0i = Ai 0 = ;P 0(s; ha1; : : : ; ami ; s0) = P (s0js; a1; : : : ; am)398\nThe Communi ative Multiagent Team De ision Problem 0i = iO0 (s; ha1; : : : ; ami ; h!1; : : : ; !mi) = O(!1; : : : ; !mja1; : : : ; am; s)B0i = [Tj=1( i)j (i.e., observation sequen es of length no more than the nite horizon)R0(s; ha1; : : : ; ami ; ) = R(s; a1; : : : ; am)The DEC-POMDP assumes perfe t re all, so we use the state estimator fun tions fromEquations 5 and 6. Sin e there is no ommuni ation for this COM-MTDP, we have a xedsilent poli y, . We an translate any domain-level poli y, A, into a DEC-POMDPjoint poli y, \u00c6, as follows: \u00c6i(oi1; : : : ; oit) iA( oi1; : : : ; oit ) (8)The expe ted utility of following this joint poli y, \u00c6, within the DEC-POMDP is identi alto that of following and A within the onstru ted COM-MTDP. Thus, there existsa poli y with expe ted utility greater than K for the COM-MTDP if and only if thereexists one for the DEC-POMDP. The de ision problem for a DEC-POMDP is known to beNEXP- omplete, so the COM-MTDP problem must be NEXP-hard.To show that the COM-MTDP is in NEXP, our proof pro eeds similarly to that ofthe DEC-POMDP. In other words, we guess the joint poli y, , and write it down inexponential time (we assume that T jSj). We an take the COM-MTDP plus the poli yand generate (in exponential time) a orresponding MDP where the state spa e is the spa eof all possible ombined belief states of the agents. We an then use dynami programmingto determine (in exponential time) whether generates an expe ted reward of at least K.2 In the remainder of this se tion, we examine the e e t of ommuni ation on the om-plexity of onstru ting team poli ies that generate optimal behavior. We start by examiningthe ase under the ondition of free ommuni ation, where we would expe t the bene t of ommuni ation to be the greatest. To begin with, suppose that ea h agent is apable of ommuni ating its entire observation (i.e., i i). Before we analyze the omplexity ofthe team de ision problem, we rst prove that the agents should exploit this apability and ommuni ate their true observation, as long as they in ur no ost in doing so:Theorem 2 Under free ommuni ation, onsider a team of agents using a ommuni ationpoli y: i (bti ) ti. If the domain-level poli y A maximizes V T ( A; ), then this ombined poli y is dominant over any other poli ies. In other words, for all poli ies, 0 Aand 0 , V T ( A; ) V T ( 0 A; 0 ).Proof: Suppose we have some other ommuni ation poli y, 0 , that spe i es somethingother than omplete ommuni ation (e.g., keeping quiet, lying). Suppose that there is somedomain-level poli y, 0 A, that allows the team to attain some expe ted reward, K, whenused in ombination with 0 . Then, we an onstru t a domain-level poli y, A, su hthat the team attains the same expe ted reward, K, when used in onjun tion with the omplete- ommuni ation poli y, , as de ned in the statement of Theorem 2.The ommuni ation poli y, 0 , produ es a di erent set of belief states (denoted b0ti and b0ti ) than those for (denoted bti and bti ). In parti ular, we use state estimator399\nPynadath & Tambefun tions, SE0i and SE0i as de ned in Equations 5 and 6 to generate b0ti and b0ti .Ea h belief state is a omplete history of observation and ommuni ation pairs for ea hagent. On the other hand, under the omplete ommuni ation of , the state estimatorfun tions of Equations 5 and 6 redu e to:SEi ( 0 ; : : : ; t 1 ; ti) = 0 ; : : : ; t 1 ; ti (9)SEi ( 0 ; : : : ; t 1 ; ti ; t ) = 0 ; : : : ; t 1 ; t = 0 ; : : : ; t 1 ; t (10)Thus, A is de ned over a di erent set of belief states than 0 A. In order to determinean equivalent A, we must rst de ne a re ursive mapping, m, that translates the beliefstates de ned by into those de ned by 0 :mi(bti ) =mi bt 1i ; t = mi bt 1i ; ti; t =Dmi(bt 1i );D ti; 0t EE = *mi(bt 1i );* ti;Yj2 0tj++=*mi(bt 1i );* ti;Yj2 0j (SE0j (mj(bt 1j ); tj))++ (11)Given this mapping, we then spe ify: iA(bti ) = 0iA(mi(bti )). Exe uting this domain-level poli y, in onjun tion with the ommuni ation poli y, , results in the identi albehavior as exe ution of the alternate poli ies, 0 A and 0 . Therefore, the team followingthe poli ies, A and will a hieve the same expe ted value of K, as under 0 A and 0 . 2Given this dominan e of the omplete- ommuni ation poli y, we an prove that theproblem of onstru ting teams that oordinate optimally is simpler when ommuni ation isfree.Theorem 3 The de ision problem of determining whether there exist poli ies, and A, for a given COM-MTDP with free ommuni ation under olle tive partial observabil-ity, that yield a total reward at least K over some nite horizon T is PSPACE- omplete.Proof: To prove that the problem is PSPACE-hard, we redu e the single-agent POMDP toa COM-MTDP. In parti ular, if we are given a POMDP, hS;A; P; ; O;Ri, we an onstru ta COM-MTDP, hS0; A01; 01; P 0; 01; O01; B01; R0i, for a single-agent team (i.e., = f1g):S0 = SA01 = A 01 = ;P 0(s; ha1i ; s0) = P (s; a1; s0) 01 = 400\nThe Communi ative Multiagent Team De ision ProblemO01(s; ha1i ; h!1i) = O(s; a1; !1)B01 = [Tj=1( )j (i.e., observation sequen es of length no more than the nite horizon)R0A(s; ha1i) = R(s; a1)R0 (s; ) = 0This COM-MTDP satis es our assumption of free ommuni ation. The POMDP assumesperfe t re all, so we use the state estimator fun tions from Equations 5 and 6. Just as inthe proof of Theorem 1, we an show that there exists a poli y with expe ted utility greaterthan K for this COM-MTDP if and only if there exists one for the POMDP. The de isionproblem for the POMDP is known to be PSPACE-hard (Papadimitriou & Tsitsiklis, 1987),so the COM-MTDP problem under free ommuni ation must be PSPACE-hard.To show that the problem is in PSPACE, we take a COM-MTDP under free ommuni- ation and redu e it to a single-agent POMDP. In parti ular, if we are given a COM-MTDP,hS;A ; ; P; ;O ;B ; Ri, we an onstru t a single-agent POMDP, hS0; A0; P 0; 0; O0;R0i, as follows:S0 = SA0 = A P 0(s;a; s0) = P (s;a; s0) 0 = O0(s;a;!) = O (s;a;!)R0(s;a) = RA(s;a)From Theorem 2, we need to onsider only the omplete- ommuni ation poli y for theCOM-MTDP and this poli y has a zero reward. Therefore, the de ision problem for theCOM-MTDP is simply to nd a domain-level poli y that produ es an expe ted rewardex eeding K. Given full ommuni ation, the state estimator fun tions for the COM-MTDP(as shown in the proof of Theorem 2) redu e to Equation 10. A poli y for our POMDPspe i es an a tion for ea h and every history of observations: 0 : [Tj=1( 0)j ! A0. Thehistory of observations for the single-agent POMDP orresponds to the belief states of ourCOM-MTDP under full ommuni ation. Therefore, we an translate a POMDP-poli y, 0,into an equivalent domain-level poli y for the COM-MTDP: A(h!0;!1; : : : ;!ti) 0(h!0;!1; : : : ;!ti) (12)A team following A will perform the exa t same domain-level a tions as a single agentfollowing 0. Thus, there exists a poli y with expe ted utility greater than K for the COM-MTDP if and only if there exists one for the POMDP. The de ision problem for a POMDPis known to be in PSPACE (Papadimitriou & Tsitsiklis, 1987), so the COM-MTDP problem(under free ommuni ation) must be in PSPACE as well. 2401\nPynadath & TambeTheorem 4 The de ision problem of determining whether there exist poli ies, and A, for a given COM-MTDP with free ommuni ation and olle tive observability, thatyield a total reward at least K over some nite horizon T is P- omplete.Proof: The proof follows that of Theorem 3, but with a redu tion to and from the MDPde ision problem, rather than the POMDP. The MDP de ision problem is P- omplete (Pa-padimitriou & Tsitsiklis, 1987). 2Theorem 5 The de ision problem of determining whether there exist poli ies, and A, for a given COM-MTDP with individual observability, that yield a total reward atleast K over some nite horizon T (given integers K and T ) is P- omplete.Proof: The proof follows that of Theorem 4, ex ept that we an redu e the problem toand from an MDP regardless of what ommuni ation poli y the team uses. 2Theorem 6 The de ision problem of determining whether there exist poli ies, and A, for a given COM-MTDP with non-observability, that yield a total reward at least Kover some nite horizon T (given integers K and T ) is NP- omplete.Proof: The proof follows that of Theorem 4, ex ept that we an redu e the problem to andfrom an single-agent non-observable MDP (NOMDP) regardless of what ommuni ationpoli y the team uses. In parti ular, be ause the agents are all equally ignorant of the state, ommuni ation has no e e t. The NOMDP de ision problem is NP- omplete (Papadim-itriou & Tsitsiklis, 1987). 2Thus, we have used the COM-MTDP framework to hara terize the di\u00c6 ulty of problemdomains in agent teamwork along the dimensions of ommuni ation ost and observability.Table 2 summarizes our results, whi h we an use in de iding where to on entrate ourenergies in atta king teamwork problems. We an use these results to draw some on lusionsabout the hallenges to designers of multiagent teams: The greatest hallenges lie in those domains with either olle tive observability or olle tive partial observability and with nonzero ommuni ation ost. Under olle tive observability and olle tive partial observability, teamwork without ommuni ation is highly intra table, but, with free ommuni ation, the omplexitybe omes on par with that of single-agent planning problems. Agent team designers have mu h to gain by in reasing the observational apabilities oftheir team (e.g., by adding new sensor agents) be ause of the redu tion in omplexitygained by making the domain olle tively observable. Furthermore, the results from Theorems 3 and 4 hold in any domain where the resultfrom Theorem 2 holds (i.e., when omplete ommuni ation is the dominant poli y).Therefore, while perfe tly free ommuni ation may be rare, these results show thatinvestment in ommuni ation in teamwork an pay o with a signi ant simpli ationof optimal teamwork. 402\nThe Communi ative Multiagent Team De ision ProblemIndividually Colle tively Colle tively Non-Observable Observable Partially Observable ObservableNo Comm. P- omplete NEXP- omplete NEXP- omplete NP-CompleteGeneral Comm. P- omplete NEXP- omplete NEXP- omplete NP-CompleteFree Comm. P- omplete P- omplete PSPACE- omplete NP-CompleteTable 2: Time omplexity of COM-MTDPs. On the other hand, when the world is individually observable or non-observable, om-muni ation makes no di eren e in performan e. It should be noted that even under those onditions where the problem is P- omplete,the omplexity of optimal teamwork is polynomial in the number of states of theworld, whi h may still be impra ti ally high. The above omplexity results pertain to nding poli ies that are optimal subje t tothe domain properties. We will nd di erent expe ted rewards of the optimal poli iesunder di erent observability and ommuni ation properties. For instan e, utting o all of the agents' sensors makes the domain non-observable and redu es the omplexityof generating an optimal poli y from NEXP to NP, but we would expe t an asso iateddrop in the expe ted reward a hieved by the team.4. Evaluating Team CoordinationTable 2 shows that providing optimal domain-level and ommuni ation poli ies for teams isa di\u00c6 ult hallenge. Many systems alleviate this di\u00c6 ulty by having domain experts pro-vide the domain-level plans (Tambe, 1997; Tidhar, 1993). Then, the problem for the agentsredu es to generating the appropriate team oordination, , to ensure that they prop-erly exe ute the domain-level plans, A. In this se tion, we demonstrate the COM-MTDPframework's ability to analyze existing teamwork approa hes in the literature. Our method-ology for su h analysis begins by en oding su h a teamwork method as a ommuni ation-level poli y. In other words, we translate the method into an algorithm that maps agentbeliefs (e.g., observation sequen es) into ommuni ation de isions. To evaluate the per-forman e of this poli y, we then instantiate a COM-MTDP that represents the states,transition probabilities, and reward fun tion of a domain of interest. Our methodologyprovides an evaluation of the poli y in terms of the expe ted reward earned by the teamwhen following the poli y in the spe i ed domain.We demonstrate this methodology by using our COM-MTDP framework to analyze jointintentions theory (Cohen & Levesque, 1991b, 1991a; Levesque et al., 1990), whi h providesa ommon basis for many existing approa hes to team oordination. Se tion 4.1 models twokey instantiations of joint intentions taken from the literature (Jennings, 1995; Tambe, 1997)as COM-MTDP ommuni ation poli ies. Se tion 4.2 analyzes the onditions under whi hthese poli ies generate optimal behavior and provides a third andidate poli y that makes ommuni ation de isions that are lo ally optimal within the ontext of joint intentions. In403\nPynadath & Tambeaddition to providing the results for the parti ular team oordination strategies investigated,this se tion also illustrates a general methodology by whi h one an use our COM-MTDPframework to en ode and evaluate oordination strategies proposed by existing multiagentresear h.4.1 Joint Intentions in a COM-MTDPJoint intention theory provides a pres riptive framework for multiagent oordination in ateam setting. It does not make any laims of optimality in its teamwork, but it providestheoreti al justi ations for its pres riptions, grounded in the attainment of mutual beliefamong the team members. We an use the COM-MTDP framework to identify the domainproperties under whi h attaining mutual belief generates optimal behavior and to quantifypre isely how suboptimal the performan e will be otherwise.Joint intentions theory requires that team members jointly ommit to a joint persistentgoal, G. It also requires that when any team member privately believes that G is a hieved(or una hievable or irrelevant), it must then attain mutual belief throughout the teamabout this a hievement (or una hievability or irrelevan e). To en ode this pres ription ofjoint intentions theory within our COM-MTDP model, we rst spe ify the joint goal, G, asa subset of states, G S, where the desired goal is a hieved (or una hievable or irrelevant).Presumably, su h a pres ription indi ates that joint intentions are not spe i ally in-tended for individually observable environments. Upon a hieving the goal in an individuallyobservable environment, ea h agent would simultaneously observe that St 2 G. Be auseof our assumption that the COM-MTDP model omponents (in luding O ) are ommonknowledge to the team, ea h agent would also simultaneously ome to believe that its team-mates have observed that St 2 G, and that its teammates believe that it believes that allof the team members have observed that St 2 G, and so on. Thus, the team immediatelyattains mutual belief in the a hievement of the goal under individual observability withoutany additional ommuni ation ne essary by the team.Instead, the joint intention framework aims at domains with some degree of unobserv-ability. In su h domains, the agents must signal the other agents, either through ommuni- ation or some informative domain-level a tion, to attain mutual belief. However, we analso assume that joint intention theory does not fo us on domains with free ommuni ation,where Theorem 2 shows that we an simply have the agents ommuni ate everything, allthe time, without the need for more omplex pres riptions.The joint intention framework does not spe ify a pre ise ommuni ation poli y for theattainment of mutual belief. In this paper, we fo us on ommuni ation only in the ase ofgoal a hievement, but our methodology extends to handle una hievability and irrelevan e aswell. One well-known approa h (Jennings, 1995) applied joint intentions theory by havingthe agents ommuni ate the a hievement of the joint goal, G, as soon as they believe G to betrue. To instantiate the behavior of Jennings' agents within a COM-MTDP, we onstru t a ommuni ation poli y, J , that spe i es that an agent sends the spe ial message, G, whenit rst believes that G holds. Following joint intentions' assumption of sin erity (Smith &Cohen, 1996), we require that the agents never sele t the spe ial G message in a beliefstate unless they believe G to be true with ertainty. With this requirement and with ourassumption of the team's ommon knowledge of the ommuni ation model, we an assume404\nThe Communi ative Multiagent Team De ision Problemthat all of the other agents immediately a ept the spe ial message, G, as true, and thatthe agents know that all their team members a ept the message as true, and so on. Thus,the team attains mutual belief that G is true immediately upon re eiving the message, G.We an onstru t the ommuni ation poli y, J , in onstant time.The STEAM algorithm is another instantiation of joint intentions that has had su ess inseveral real-world domains (Tambe, 1997; Pynadath et al., 1999; Tambe, Pynadath, Chau-vat, Das, & Kaminka, 2000; Pynadath & Tambe, 2002). Unlike Jennings' instantiation, theSTEAM teamwork model in ludes de ision-theoreti ommuni ation sele tivity. A domainspe i ation in ludes two parameters for ea h joint ommitment, G: , the probability ofmis oordinated termination of G; and Cmt, the ost of mis oordinated termination of G. Inthis ontext, \\mis oordinated termination\" means that some agents immediately observethat the team has a hieved G while the rest do not. STEAM's domain spe i ation alsoin ludes a third parameter, C , to represent the ost of ommuni ation of a fa t (e.g., thea hievement of G). Using these parameters, the STEAM algorithm evaluates whether theexpe ted ost of mis oordination outweighs the ost of ommuni ation. STEAM expressesthis riterion as the following inequality: Cmt > C . We an de ne a ommuni ationpoli y, S based on this riterion: if the inequality holds, then an agent that has observedthe a hievement of G will send the message, G; otherwise, it will not. We an onstru t S in onstant time.4.2 Lo ally Optimal Poli yAlthough the STEAM poli y is more sele tive than Jennings', it remains unansweredwhether it is optimally sele tive, and resear hers ontinue to struggle with the questionof when agents should ommuni ate (Yen et al., 2001). The few reports of suboptimal(in parti ular, ex essive) ommuni ation in STEAM hara terized the phenomenon as anex eptional ir umstan e, but it is also possible that STEAM's optimal performan e is theex eption. We use the COM-MTDP model to derive an analyti al hara terization of opti-mal ommuni ation here, while Se tion 5 provides an empiri al one by reating an algorithmusing that hara terization.Both poli ies, J , and S onsider sending G only when an agent rst believes thatG has been a hieved. On e an agent has the relevant belief, they make di erent hoi es, andwe onsider here what the optimal de ision is at this point. The domain is not individuallyobservable, so ertain agents may be unaware of the a hievement of G. When not sendingthe G message, these unaware agents may unne essarily ontinue performing a tions inthe pursuit of a hieving G. The performan e of these extraneous a tions ould potentiallyin ur osts and lead to a lower utility than one would expe t when sending the G message.The de ision to send G or not matters only if the team a hieves G and one agent omes to know this fa t. We de ne the random variable, TG, to be the earliest time atwhi h an agent knows this fa t. We denote agent KG as the agent who knows of thea hievement at time TG. If KG = i, for some agent, i, and TG = t0, then agent i has somepre- ommuni ation belief state, bt0i = , that indi ates that G has been a hieved. To morepre isely quantify the di eren e between agent i sending the G message at time TG vs. 405\nPynadath & Tambenever sending it, we de ne the following value: T (t0; i; ) E \"T t0Xt=0 Rt0+t t0i = G; TG = t0;KG = i; bt0i = # E \"T t0Xt=0 Rt0+t t0i = null; TG = t0;KG = i; bt0i = # (13)We assume that, for all times other than TG, the agents follow some ommuni ation poli y, , that never spe i es G. Thus, T measures the di eren e in expe ted reward thathinges on agent i's spe i de ision to send or not send G at time t0. Given this de nition,it is lo ally optimal for agent i to send the spe ial message, G, at time t0, if and onlyif T 0. We de ne the ommuni ation poli y, + , as the ommuni ation poli yfollowing for all agents at all times, ex ept for agent i under belief state , whenagent i sends message . With this de nition, + G , is the poli y under whi h agent i ommuni ates the a hievement of G, and +null is the poli y under whi h it does not.Therefore, we an alternatively des ribe agent i's de ision riterion as hoosing + Gover +null if and only if T 0.Unfortunately, while Equation 13 identi es an exa t riterion for lo ally optimal ommu-ni ation, this riterion is not yet operational. In other words, we an not dire tly implementit as a ommuni ation poli y for the agents. Furthermore, Equation 13 hides the underly-ing omplexity of the omputation involved, whi h is one of the key goals of our analysis.Therefore, we use the COM-MTDP model to derive an operational expression of T 0.For simpli ity, we de ne notational shorthand for various sequen es and ombinations ofvalues. We de ne a partial sequen e of random variables, X<t, to be the sequen e of ran-dom variables for all times before t: X0, X1, : : : , Xt 1. We make similar de nitions for theother relational operators (i.e., X>t, X t, et .). The expression, (S)T , denotes the rossprodu t over states of the world, QTt=0 S, as distinguished from the time-indexed randomvariable, ST , whi h denotes the value of the state at time T . The notation, s t0 [t\u2104, spe i esthe element in slot t within the ve tor s t0 . We de ne the fun tion, , as shorthand withinour probability expressions. It allows us to ompa tly represent a parti ular subsequen eof world and agent belief states o urring, onditioned on the urrent situation, as follows:Pr t; t0 ; s; Pr(S t; t0 = s; b t; t0 = TG = t0;KG = i; bt0i = )(14)Informally, (ht; t0i ; s; ) represents the event that the world and belief states from timet through t0 orrespond to the spe i ed sequen es, s and , respe tively, onditioned onagent i being the rst to know of G's a hievement at time t0 with a belief state, . We de nethe fun tion, , to map a pre- ommuni ation belief state into the post- ommuni ationbelief state that arises from a ommuni ation poli y: ( ; ) SE ( ; ( )) (15)This de nition of is a well-de ned fun tion be ause of the deterministi nature of thepoli y, , and state-estimator fun tion, SE .406\nThe Communi ative Multiagent Team De ision ProblemTheorem 7 If we assume that, upon a hievement of G, no ommuni ation other than Gis possible, then the ondition T (t0,i, ) 0 holds if and only if:Xs t02(S)t0 X t0 2Bt0 Pr( (h0; t0i ; s t0 ; t0 )) 0B Xs t02(S)T t0+1 X t0 2BT t0+1 Pr (ht0; T i ; s t0 ; t0 ) t0i = G; (h0; t0i ; s t0 ; t0 ) TXt=t0 RA s t0 [t\u2104; A t0 [t\u2104; + G Xs t02(S)T t0+1 X t0 2BT t0+1 Pr (ht0; T i ; s t0 ; t0 ) t0i = null; (h0; t0i ; s t0 ; t0 ) TXt=t0 RA s t0 [t\u2104; A t0 [t\u2104; +null ! Xs2G X 2B Pr ( (ht0; t0i ; s; ))R (s; G) (16)Proof: The omplete proof of the following theorem appears in Online Appendix 1.The de nition of T in Equation 13 is the di eren e between two expe tations, where ea hexpe tation is a sum over the possible traje tories of the agent team. Ea h traje tory mustin ludes a sequen e of possible world states, sin e the agents' reward at ea h point in timedepends on the parti ular state of the world at that time. The agents' reward also dependson their a tions (both domain- and ommuni ation-level). These a tions are deterministi ,given the agents' poli ies, A and , and their belief states. Thus, in addition to summingover the possible states of the world, we must also sum over the possible states of the agents'\n407\nPynadath & Tambebeliefs (both pre- and post- ommuni ation): T (t0; i; )= Xs T2(S)T X T2(B)T X T2(B)T Pr S T = s T ;b T = T ;b T = Tj t0i = G; TG = t0;KG = i; bt0i = TXt=0 R(s T [t\u2104; A( T [t\u2104); ( T [t\u2104)) Xs T2(S)T X T2(B)T X T2(B)T Pr S T = s T ;b T = T ;b T = Tj t0i = null; TG = t0;KG = i; bt0i = TXt=0 R(s T [t\u2104; A( T [t\u2104); ( T [t\u2104)) (17)We an rewrite these summations more simply using our various shorthand notations:= Xs T2(S)T X T2(B)T Pr( (h0; T i ; s; T )j t0i = G) TXt=0 R(s T [t\u2104; A( ( T [t\u2104; G)); G( T [t\u2104)) Xs T2(S)T X T2(B)T Pr( (h0; T i ; s; T )j t0i = null) TXt=0 R(s T [t\u2104; A( ( T [t\u2104; null)); null( T [t\u2104)) (18)The remaining derivation exploits our Markovian assumptions to rearrange the summationsand an el like terms to produ e the theorem's result. 2Theorem 7 states, informally, that we prefer sending G whenever the the ost of exe- ution after a hieving G outweighs the ost of ommuni ation of the fa t that G has beena hieved. More pre isely, the outer summations on the left-hand side of the inequalityiterate over all possible past histories of world and belief states, produ ing a probabilitydistribution over the possible states the team an be in at time t0. For ea h su h state, theexpression inside the parentheses omputes the di eren e in domain-level reward, over allpossible future sequen es of world and belief states, between sending and not sending G.By our theorem's assumption that no ommuni ation other than G is possible after G hasbeen a hieved, we an ignore any ommuni ation osts in the future. However, if we relaxthis assumption, we an extend the left-hand side in a straightforward manner into a longer408\nThe Communi ative Multiagent Team De ision ProblemIndividually Colle tively Colle tively Non-Observable Observable Partially Observable ObservableNo Comm. (1) (1) (1) (1)General Comm. (1) O((jSj j j)T ) O((jSj j j)T ) (1)Free Comm. (1) (1) (1) (1)Table 3: Time omplexity of lo ally optimal de ision.expression that a ounts for the di eren e in future ommuni ation osts as well. Thus, theleft-hand side aptures our intuition that, when not ommuni ating, the team will in ur a ost if the agents other than i are unaware of G's a hievement. The right-hand side of theinequality is a summation of the ost of sending the G message over possible urrent statesand belief states.We an use Theorem 7 to derive the lo ally optimal ommuni ation de ision a rossvarious lasses of problem domains. Under no ommuni ation, we annot send G. Underfree ommuni ation, the right-hand side is 0, so the inequality is always true, and we knowto prefer sending G. Under no assumptions about ommuni ation, the determination ismore ompli ated. When the domain is individually observable, the left-hand side be omes0, be ause all of the agents know that G has been a hieved (and thus there is no di eren ein exe ution when sending G). Therefore, the inequality is always false (unless under free ommuni ation), and we prefer not sending G. When the environment is not individuallyobservable and ommuni ation is available but not free, then, to be lo ally optimal at timet0, agent i must evaluate Inequality 16 in its full omplexity. Sin e the inequality sumsrewards over all possible sequen es of states and observations, the time omplexity of the orresponding algorithm is O((jSj j j)T ). While this omplexity is una eptable for mostreal-world problems, it still provides an exponential savings over sear hing the entire poli yspa e for the globally optimal poli y, where any agent ould potentially send G at timesother than TG. Table 3 provides a table of the omplexity required to determine the lo allyoptimal poli y under the various domain properties.We an now show that although Theorem 7's algorithm for lo ally optimal ommuni a-tion provides a signi ant omputational savings over nding the global optimum, it stilloutperforms existing teamwork models, as exempli ed by our J and S poli ies. First,we an use the riterion of Theorem 7 to evaluate the optimality of the poli y, J . If T (t0; i; ) 0 for all possible times t0, agents i, and belief states that are onsistentwith the a hievement of the goal G, then the lo ally optimal poli y will always spe ifysending G. In other words, J will be identi al to the lo ally optimal poli y. However,if the inequality of Theorem 7 is ever false, then J is not even lo ally, let alone globally,optimal.Se ond, we an also use Theorem 7 to evaluate STEAM by viewing STEAM's inequality, Cmt > C , as a rude approximation of Inequality 16. In fa t, there is a lear orre-sponden e between the terms in the two inequalities. The left-hand side of Inequality 16 omputes an exa t expe ted ost of mis oordination. However, unlike STEAM's monolithi parameter, the optimal riterion evaluates a omplete probability distribution over allpossible states of mis oordination by onsidering all possible past sequen es onsistent with409\nPynadath & Tambethe agent's urrent beliefs. Likewise, unlike STEAM's monolithi Cmt parameter, the opti-mal riterion looks ahead over all possible future sequen es of states to determine the trueexpe ted ost of mis oordination. Furthermore, we an view STEAM's parameter, C , as anapproximation of the ommuni ation ost omputed by the right-hand side of Inequality 16.Again, STEAM uses a single parameter, while the optimal riterion omputes an expe ted ost over all possible states of the world.STEAM does have some exibility in its representation, be ause Cmt, , and C arenot ne essarily xed a ross the entire domain. For instan e, Cmt may vary based on thespe i joint plan that the agents may have jointly ommitted to (i.e., there may be adi erent Cmt for ea h goal G). Thus, while Theorem 7 suggests signi ant additional exi-bility in omputing Cmt through expli it lookahead, the optimal riterion derived with theCOM-MTDP model also provides a justi ation for the overall stru ture behind STEAM'sapproximate riterion. Furthermore, STEAM's emphasis on on-line omputation makes the omputational omplexity of Inequality 16 (as presented in Table 3) una eptable, so theapproximation error may be a eptable given the gains in e\u00c6 ien y. For a spe i domain,we an use empiri al evaluation (as demonstrated in the next se tion) to quantify the errorand e\u00c6 ien y to pre isely judge this tradeo .5. Empiri al Poli y EvaluationIn addition to providing these analyti al results over general lasses of problem domains, theCOM-MTDP framework also supports the analysis of spe i domains. Given a parti ularproblem domain, we an onstru t an optimal ommuni ation poli y or, if the omplexity of omputing an optimal poli y is prohibitive, we an instead evaluate and ompare andidateapproximate poli ies. To provide a reusable tool for su h evaluations, we have implementedthe COM-MTDP model as a Python lass with domain-independent methods for the eval-uation of arbitrary poli ies and for the generation of both lo ally optimal poli ies usingTheorem 7 and globally optimal poli ies through brute-for e sear h of the poli y spa e.This software is available in Online Appendix 1.This se tion presents results of a COM-MTDP analysis of an example domain involvingagent-piloted heli opters, where we fo us on the key ommuni ation de ision fa ed by manymultiagent frameworks (as des ribed in Se tion 4), but vary the ost of ommuni ation anddegree of observability to generate a spa e of distin t domains with di erent impli ationsfor the agents' performan e. By evaluating ommuni ation poli ies over various on gura-tions of this parti ular testbed domain, we demonstrate a methodology by whi h one anuse the COM-MTDP framework to model any problem domain and to evaluate andidate ommuni ation poli ies for it.5.1 Experimental SetupConsider two heli opters that must y a ross enemy territory to their destination, as il-lustrated in Figure 1. The rst, piloted by agent Transport, is a transport vehi le withlimited repower. The se ond, piloted by agent Es ort, is an es ort vehi le with signi ant repower. Somewhere along their path is an enemy radar unit, but its lo ation is unknown(a priori) to the agents. Es ort is apable of destroying the radar unit upon en ounteringit. However, Transport is not, but it an es ape dete tion by the radar unit by traveling410\nThe Communi ative Multiagent Team De ision Problem\nFigure 1: Illustration of heli opter team s enario.at a very low altitude (nap-of-the-earth ight), though at a lower speed than at its typi al,higher altitude. In this s enario, Es ort will not worry about dete tion, given its superior repower; therefore, it will y at a fast speed at its typi al altitude.The two agents form a top-level joint ommitment, GD, to rea h their destination.There is no in entive for the agents to ommuni ate the a hievement of this goal, sin e theywill both eventually rea h their destination with ertainty. However, in the servi e of theirtop-level goal, GD, the two agents also adopt a joint ommitment, GR, of destroying theradar unit. We onsider here the problem fa ing Es ort with respe t to ommuni ating thea hievement of goal, GR. If Es ort ommuni ates the a hievement of GR, then Transportknows that it is safe to y at its normal altitude (thus rea hing the destination sooner).If Es ort does not ommuni ate the a hievement of GR, there is still some han e thatTransport will observe the event anyway. If Transport does not observe the a hievementof GR, then it must y nap-of-the-earth the whole distan e, and the team re eives a lowerreward be ause of the later arrival. Therefore, Es ort must weigh the in rease in expe tedreward against the ost of ommuni ation.In the COM-MTDP model of this s enario (presented in Figures 2, 3 and 4), the worldstate is the position (along a straight line between origin and destination) of Transport,Es ort, and the enemy radar. The enemy is at a randomly sele ted position somewherein between the agents' initial position and their destination. Transport has no possible ommuni ation a tions, but it an hoose between two domain-level a tions: ying nap-of-the-earth and ying at its normal speed and altitude. Es ort has two domain-level a tions: ying at its normal speed and destroying the radar. Es ort also has the option of ommuni- ating the spe ial message, GR , indi ating that the radar has been destroyed. In the tablesof Figures 2, 3 and 4, the \\ \" symbol represents a wild- ard (or \\don't are\") entry.If Es ort arrives at the radar, then it observes its presen e with ertainty and andestroy it to a hieve GR. The likelihood of Transport's observing the radar's destru tion isa fun tion of its distan e from the radar. We an vary this fun tion's observability parameter411\nPynadath & Tambe = fEs ort (E);Transport (T )gS = E T RPosition of Es ort: E = f0; 1; : : : ; 8; 9;DestinationgPosition of Transport: T = f0; 0:5; : : : ; 9; 9:5;Destination;DestroyedgPosition of Radar: R = f1; 2; : : : ; 8;DestroyedgA = AE AT = f y;destroy;waitg f y-NOE; y-normal;waitg = E T = f lear ( GR);nullg fnullgRA(h E; T ; Ri ;a) = E T a RA0; : : : ; 9 0; : : : ; 9:5;Destroyed 00; : : : ; 9 Destination rTDestination 0; : : : ; 9:5;Destroyed rEDestination Destination rE + rTR (s; hnull;nulli) = 0R (s; h GR ;nulli) = r 2 [0; 1\u2104Figure 2: COM-MTDP model of states, a tions, and rewards for heli opter s enario.( in Figure 4) within the range [0; 1\u2104 to generate distin t domain on gurations (0 meansthat Transport will never observe the radar's destru tion; 1 means Transport will alwaysobserve it). If the observability is 1, then they a hieve mutual belief of the a hievement ofGR as soon as it o urs (following the argument presented in Se tion 4.1). However, for anyobservability less than 1, there is a han e that the agents will not a hieve mutual beliefsimply by ommon observation. The heli opters re eive a xed reward for ea h time stepspent at their destination. Thus, for a xed time horizon, the earlier the heli opters rea hthere, the greater the team's reward. Sin e ying nap-of-the-earth is slower than normalspeed, Transport will swit h to its normal ying as soon as it either observes that GR hasbeen a hieved or Es ort sends the message, GR . Sending the message is not free, so weimpose a variable ommuni ation ost (r in Figure 2), also within the range [0; 1\u2104.We onstru ted COM-MTDP models of this s enario for ea h ombination of observabil-ity and ommuni ation ost within the range [0; 1\u2104 at 0.1 in rements. For ea h ombination,we applied the Jennings and STEAM poli ies, as well as a ompletely silent poli y. For thisdomain, the poli y, J , di tates that Es ort always ommuni ate GR upon destroyingthe radar. For STEAM, we vary the and C parameters with the observability and om-muni ation ost parameters, respe tively. We used two di erent settings (low and medium)for the ost of mis oordination, Cmt. Following the published STEAM algorithm (Tambe,1997), Es ort sends message GR if and only if STEAM's inequality Cmt > C , holds.Thus, the two di erent settings, low and medium, for Cmt generate two distin t ommuni a-tion poli ies; the high setting is stri tly dominated by the other two settings in this domain.We also onstru ted and evaluated lo ally and globally optimal poli ies. In applying ea hof these poli ies, we used our COM-MTDP model to ompute the expe ted reward re eivedby the team when following the sele ted poli y. We an uniquely determine this expe tedreward given the andidate ommuni ation poli y and the parti ular observability and om-muni ation ost parameters, as well as the COM-MTDP model spe i ed in Figures 2, 3,and 4. 412\nThe Communi ative Multiagent Team De ision Problem P (h E0; T0; R0i ; haE; aT i ; h E1; T1; R1i) =PE( E0; aE ; E1) PT (h T0; R0i ; aT ; T1) PR(h E0; R0i ; aE ; R1)Es ort: Initial distribution, Pr( 0E = 0) = 1 E0 aE E1 PEDestination Destination 10; : : : ; 8 y E0 + 1 10; : : : ; 8 destroy E0 + 1 19 y Destination 19 destroy Destination 1 wait E0 1Transport: Initial distribution, Pr( 0T = 0) = 1 T0 R0 aT T1 PTDestination Destination 1Destroyed Destroyed 10; : : : ; 9 y-NOE T0 + 0:5 19:5 y-NOE Destination 10; : : : ; 8:5 Destroyed y-normal T0 + 1 19; 9:5 Destroyed y-normal Destination 1 6= Destroyed y-normal Destroyed 1 wait T0 1Radar: Initial distribution, 8 2 f1; 2; : : : ; 8g, Pr( 0R = ) = 0:125 E0 R0 aE R1 PR E0 destroy Destroyed 1 6= destroy R0 1 6= E0 R0 1Figure 3: COM-MTDP model of transition probabilities for heli opter s enario (ex ludeszero probability rows).\n413\nPynadath & Tambe = E T{ E = E T RE , where agent Es ort's possible observations of the radar onsist of RE = fpresent;destroyed;nullg{ T = E T RT , where agent Transport's possible observations of the radar onsist of RT = fdestroyed;nullg O (s; haE ; aT i ; h!E; !T i) = OE(s; haE; aT i ; !E) OT (s; haE ; aT i ; !T ){ OE(h E; T ; Ri ; haE; aT i ; h E; T ; !REi) = E R aE !RE OE destroyed destroy destroyed 1 destroyed 6= destroy null 1 R 1; : : : ; 9 present 16= R 1; : : : ; 9 null 1{ OT (h E ; T ; Ri ; haE ; aT i ; h E ; T ; !RT i) = T R aE !RT OT0; : : : ; 9:5 destroy destroyed e ( R T )(1 )0; : : : ; 9:5 destroy null 1 e ( R T )(1 )0; : : : ; 9:5 6= destroy null 1destroyed null 1 2 [0; 1\u2104Figure 4: COM-MTDP model of observability for heli opter s enario. These tables ex ludeboth zero probability rows and input feature olumns from whi h O is indepen-dent. For example, both agents' observation fun tions are independent of thetransport's sele ted a tion, so neither table in ludes a aT olumn.\n414\nThe Communi ative Multiagent Team De ision Problem\nFigure 6: Suboptimality of STEAM poli y under both low and medium osts of mis oordi-nation.5.2 Experimental ResultsFigures 5 and 6 plot how mu h utility the team an expe t to lose by following the Jennings,silent, and the two STEAM poli ies instead of the lo ally optimal ommuni ation poli y(thus, higher values mean worse performan e). We an immediately see that the Jenningsand silent poli ies are signi antly suboptimal for many possible domain on gurations. Forexample, not surprisingly, the surfa e for the poli y, J , peaks (i.e., it does most poorly)when the ommuni ation ost is high and when the observability is high, while the silentpoli y does poorly under exa tly the opposite onditions.Previously published results (Jennings, 1995) demonstrated that the Jennings poli yled to better team performan e by redu ing waste of e ort produ ed by alternate poli ieslike our silent one. These earlier results fo used on a single domain, and Figure 5 partially on rms their on lusion and shows that the superiority of the Jennings poli y over thesilent poli y extends over a broad range of possible domain on gurations. On the otherhand, our COM-MTDP results also show that there is a signi ant sub lass of domains (e.g.,when ommuni ation ost and observability are high) where the Jennings poli y is a tuallyinferior to the silent poli y. Thus, with our COM-MTDP model, we an hara terize thetypes of domains where the Jennings poli y outperforms the silent poli y and vi e versa.415\nPynadath & TambeFigure 6 shows the expe ted value lost by following the two STEAM poli ies. We anview STEAM as trying to intelligently interpolate between the Jennings and silent poli iesbased on the parti ular domain properties. In fa t, under a low setting for Cmt, we seetwo thresholds, one along ea h dimension, at whi h STEAM swit hes between following theJennings and silent poli ies, and its suboptimality is highest at these thresholds. Undera medium setting for Cmt, STEAM does not exhibit a threshold along the dimension of ommuni ation ost, due to the in reased ost of mis oordination. Under both settings,STEAM's performan e generally follows the better of those two xed poli ies, so its maxi-mum suboptimality (0.587 under both settings) is signi antly lower than that of the silent(0.700) and Jennings' (1.000) poli ies. Furthermore, STEAM outperforms the two poli ieson average, a ross the spa e of domain on gurations, as eviden ed by its mean subopti-mality of 0.063 under low Cmt and 0.083 under medium Cmt. Both values are signi antlylower than the silent poli y's mean of 0.160 and the Jennings' poli y's mean of 0.161. Thus,we have been able to quantify the savings provided by STEAM over less sele tive poli ieswithin this example domain.However, within a given domain on guration, STEAM must either always or never ommuni ate, and this in exibility leads to signi ant suboptimality a ross a wide rangeof domain on gurations. On the other hand, Figure 6 also shows that there are domain on gurations where STEAM is lo ally optimal. In this relatively small-s ale experimentaltestbed, there is no need to in ur STEAM's suboptimality, be ause the agents an omputethe superior lo ally optimal poli y in under 5 se onds. In larger-s ale domains, on the otherhand, the in reased omplexity of the lo ally optimal poli ies may render its exe utioninfeasible. In su h domains, STEAM's onstant-time exe ution would potentially make it apreferable alternative. This analysis suggests a possible spe trum of algorithms that makedi erent optimality-e\u00c6 ien y tradeo s.To understand the ause of STEAM's suboptimality, we an examine its performan emore deeply in Figures 7 and 8, whi h plot the expe ted number of messages sent usingSTEAM (with both low and medium Cmt) vs. the lo ally optimal poli y, at observabilityvalues of 0.3 and 0.7. STEAM's expe ted number of messages is either 0 or 1, so STEAM an make at most two (instantaneous) transitions between them: one threshold value ea halong the observability and ommuni ation ost dimensions.From Figures 7 and 8, we see that the optimal poli y an be more exible than STEAMby spe ifying ommuni ation ontingent on Es ort's beliefs beyond simply the a hievementof GR. For example, onsider the messages sent under low Cmt in Figure 7, where STEAMmat hes the lo ally optimal poli y at the extremes of the ommuni ation ost dimension.Even if the ommuni ation ost is high, it is still worth sending message GR in states whereTransport is still very far from the destination. Thus, the surfa e for the optimal poli y,makes a more gradual transition from always ommuni ating to never ommuni ating. We an thus view STEAM's surfa e as a rude approximation to the optimal surfa e, subje tto STEAM's fewer degrees of freedom.We an also use Figures 7 and 8 to identify the domain onditions under whi h jointintentions theory's pres ription of attaining mutual belief is or is not optimal. In parti ular,for any domain where the observability is less than 1, the agents will not attain mutual beliefwithout ommuni ation. In both Figures 7 and 8, there are many domain on gurationswhere the lo ally optimal poli y is expe ted to send fewer than 1 GR message. Ea h of416\nThe Communi ative Multiagent Team De ision Problem\n417\nPynadath & Tambe\nFigure 9: Suboptimality of lo ally optimal poli y.these on gurations represents a domain where the lo ally optimal poli y will not attainmutual belief in at least one ase. Therefore, attaining mutual belief is suboptimal in those on gurations!These experiments illustrate that STEAM, despite its de ision-theoreti ommuni ationsele tivity, may ommuni ate suboptimally under a signi ant lass of domain on gura-tions. Previous work on STEAM-based, real-world, agent-team implementations informallynoted suboptimality in an isolated on guration within a more realisti heli opter trans-port domain (Tambe, 1997). Unfortunately, this previous work treated that suboptimality(where the agents ommuni ated more than ne essary) as an isolated aberration, so therewas no investigation of the degree of su h suboptimality, nor of the onditions under whi hsu h suboptimality may o ur in pra ti e. We re- reated these onditions within the experi-mental testbed of this se tion by using a medium Cmt. The resulting experiments (as shownin Figure 7) illustrated that the observed suboptimality was not an isolated phenomenon,but, in fa t, that STEAM has a general propensity towards extraneous ommuni ation insituations involving low observability (i.e., low likelihood of mutual belief) and high om-muni ation osts. This result mat hes the situation where the \\aberration\" o urred in themore realisti domain.The lo ally optimal poli y is itself suboptimal with respe t to the globally optimalpoli y, as we an see from Figure 9. Under domain on gurations with high observability,the globally optimal poli y has the es ort wait an additional time step after destroyingthe radar and then ommuni ate only if the transport ontinues ying nap-of-the-earth.The es ort annot dire tly observe whi h method of ight the transport has hosen, butit an measure the hange in the transport's position (sin e it maintains a history of itspast observations) and thus infer the method of ight with omplete a ura y. In a sense,the es ort following the globally optimal poli y is performing plan re ognition to analyzethe transport's possible beliefs. It is parti ularly noteworthy that our domain spe i ationdoes not expli itly en ode this re ognition apability. In fa t, our algorithm for nding theglobally optimal poli y does not even make any of the assumptions made by our lo allyobservable poli y (i.e., single agent is de iding whether to ommuni ate or not, regardinga single message, at a single point in time); rather, our general-purpose sear h algorithmtraverses the poli y spa e and \\dis overs\" this possible means of inferen e on its own. We418\nThe Communi ative Multiagent Team De ision Problemexpe t that su h COM-MTDP analysis an provide an automati method for dis overingnovel ommuni ation poli ies of this type in other domains, even those modeling real-worldproblems.Indeed, by exploiting this dis overy apability within our example domain, the globallyoptimal poli y gains a slight advantage in expe ted utility over the lo ally optimal poli y,with a mean di eren e of 0.011, standard deviation of 0.027, and maximum of 0.120. On theother hand, our domain-independent ode never requires more than 5 se onds to omputethe lo ally optimal poli y in this testbed, while our domain-independent sear h algorithmalways required more than 150 minutes to nd the globally optimal poli y. Thus, throughTheorem 7, we have used the COM-MTDP model to onstru t a ommuni ation poli ythat, for this testbed domain, performs almost optimally and outperforms existing team-work theories, with a substantial omputational savings over nding the globally optimalpoli y. Although these results hold for an isolated ommuni ation de ision, we expe t therelative performan e of the poli ies to stay the same even with multiple de isions, where thein exibility of the suboptimal poli ies will only exa erbate their losses (i.e., the shapes ofthe graphs would stay roughly the same, but the suboptimality magnitudes would in rease).6. SummaryThe COM-MTDP model is a novel framework that omplements existing teamwork resear hby providing the previously la king apability to analyze the optimality and omplexity ofteam de isions. While grounded within e onomi team theory, the COM-MTDP's exten-sions to in lude ommuni ation and dynamism allow it to subsume many existing multiagentmodels. We were able to exploit the COM-MTDP's ability to represent broad lasses ofmultiagent team domains to derive omplexity results for optimal agent teamwork underarbitrary problem domains. We also used the model to identify domain properties that ansimplify that omplexity.The COM-MTDP framework provides a general methodology for analysis a ross bothgeneral domain sub lasses and spe i domain instantiations. As demonstrated in Se tion 4,we an express important existing teamwork theories within a COM-MTDP framework andderive broadly appli able theoreti al results about their optimality. Se tion 5 demonstratesour methodology for the analysis of a spe i domain. By en oding a teamwork problem asa COM-MTDP, we an use the leverage of our general-purpose software tools (available inOnline Appendix 1) to evaluate the optimality of teamwork based on potentially any otherexisting theory, as demonstrated in this paper using two leading instantiations of jointintentions theory. In ombining both theory and pra ti e, we an use the theoreti al resultsderived using the COM-MTDP framework as the basis for new algorithms to extend oursoftware tools, just as we did in translating Theorem 7 from Se tion 4 into an implementedalgorithm for lo ally optimal ommuni ation in Se tion 5. We expe t that the COM-MTDPframework, the theorems and omplexity results, and the reusable software will form a basisfor further analysis of teamwork, both by ourselves and others in the eld. 419\nPynadath & Tambe7. Future Work for COM-MTDP Team AnalysisWhile our initial COM-MTDP results are promising, there remain at least three key areaswhere future progress in COM-MTDPs is riti al. First, analysis using COM-MTDPs (su has the one presented in Se tion 5) requires knowledge of the rewards, transition probabil-ities, and observation probabilities, as well as of the ompeting poli ies governing agentbehavior. It may not always be possible to have su h a model of the domain and agents'poli ies readily available. Indeed, other proposed team-analysis te hniques (Nair, Tambe,Marsella, & Raines, 2002b; Raines, Tambe, & Marsella, 2000), do not require a priori hand- oding of su h models, but rather a quire them automati ally through ma hine learningover large numbers of runs. Also, in the interests of ombating omputational omplexityand improved understandability, some resear hers emphasize the need for multiple modelsat multiple levels of abstra tion, rather than fo using on a single model (Nair et al., 2002b).For instan e, one level of the model may fo us on the analysis of the individual agents' a -tions in support of a team, while another level may fo us on intera tions among subteamsof a team. We an potentially extend the COM-MTDP model in both of these dire tions(i.e., ma hine learning of model parameters, and hierar hi al representations of the team toprovide multiple levels of abstra tion).Se ond, it is important to extend COM-MTDP analysis to other aspe ts of teamworkbeyond ommuni ation. For instan e, team formation (where agents may be assigned spe- i roles within the team) and reformation (where failure of individual agents leads to rolereassignment within in the team) are key problems in teamwork that appear suitable forCOM-MTDP analysis. Su h analysis may require extensions to the COM-MTDP frame-work (e.g., expli it modeling of roles). Ongoing resear h (Nair, Tambe, & Marsella, 2002a)has begun investigating the impa t of su h extensions and their appli ations in domainssu h as RoboCup Res ue (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjoh, & Shi-mada, 1999). Analysis of more omplex team behaviors may require further extensionsto the COM-MTDP model to expli itly a ount for additional aspe ts of teamwork (e.g.,notions of authority stru ture within teams).Third, extending COM-MTDP analysis beyond teamwork to model other types of o-ordination may require relaxation of COM-MTDP's assumption of sel ess agents re eivingthe same joint reward. More omplex organizations may require modeling other non-jointrewards. Indeed, enri hing the COM-MTDP model in this manner may enable analy-sis of some of the seminal work in multiagent oordination in the tradition of PGP andGPGP (De ker & Lesser, 1995; Durfee & Lesser, 1991). Su h enri hed models may rstrequire new advan es in the mathemati al foundations of our COM-MTDP framework, andultimately ontribute towards the emerging s ien es of agents and multiagent systems.A knowledgmentsThis arti le is a signi antly extended version of a paper, \\Multiagent Teamwork: Analyzingthe Optimality and Complexity of Key Theories and Models\", by the same authors, in thePro eedings of the International Joint Conferen e on Autonomous Agents and Multi-AgentSystems, 2002. This arti le extends the initial ontent by providing proofs missing in theoriginal paper, as well as new theoreti al results, a detailed des ription of our experimental420\nThe Communi ative Multiagent Team De ision Problemsetup, new experimental results, and additional dis ussion and explanations of key points.This resear h was supported by DARPA award No. F30602-98-2-0108, under the Controlof Agent Based Systems program, and managed by AFRL/Rome Resear h Site. We wouldlike to thank Daniel Bernstein, Ashish Goel, Daniel Mar u, Sta y Marsella, Ranjit Nair,and Paul Rosenbloom for valuable dis ussion and feedba k. We also thank the anonymousreviewers for their helpful omments and suggestions.Referen esBernstein, D. S., Zilberstein, S., & Immerman, N. (2000). The omplexity of de entralized ontrol of Markov de ision pro esses. In Pro eedings of the Conferen e on Un ertaintyin Arti ial Intelligen e, pp. 32{37.Boutilier, C. (1996). Planning, learning and oordination in multiagent de ision pro esses.In Pro eedings of the Conferen e on Theoreti al Aspe ts of Rationality and Knowledge,pp. 195{210.Boutilier, C., Dean, T., & Hanks, S. (1999). De ision-theoreti planning: Stru tural as-sumptions and omputational leverage. Journal of Arti ial Intelligen e Resear h,11, 1{93.Cohen, P. R., & Levesque, H. J. (1991a). Con rmation and joint a tion. In Pro eedings ofthe International Joint Conferen e on Arti ial Intelligen e.Cohen, P. R., & Levesque, H. J. (1991b). Teamwork. Nous, 25 (4), 487{512.De ker, K., & Lesser, V. (1995). Designing a family of oordination algorithms. In Pro eed-ings of the International Conferen e on Multi-Agent Systems.Dunin-Kepli z, B., & Verbrugge, R. (1996). Colle tive ommitments. In InternationalConferen e on Multi-Agent Systems, pp. 56{63.Durfee, E., & Lesser, V. (1991). Partial global planning: a oordination framework fordistributed planning. IEEE transa tions on Systems, Man and Cyberneti s, 21 (5).Goldberg, D., & Matari , M. J. (1997). Interferen e as a tool for designing and evaluat-ing multi-robot ontrollers. In Pro eedings of the National Conferen e on Arti ialIntelligen e, pp. 637{642.Grosz, B. (1996). Collaborating systems. Arti ial Intelligen e Magazine, 17 (2), 67{85.Grosz, B., & Kraus, S. (1996). Collaborative plans for omplex group a tions. Arti ialIntelligen e, 86, 269{358.Grosz, B. J., & Sidner, C. L. (1990). Plans for dis ourse. In Cohen, P. R., Morgan,J., & Polla k, M. E. (Eds.), Intentions in Communi ation, pp. 417{444. MIT Press,Cambridge, MA.Ho, Y.-C. (1980). Team de ision theory and information stru tures. Pro eedings of theIEEE, 68 (6), 644{654.Jennings, N. (1995). Controlling ooperative problem solving in industrial multi-agentsystems using joint intentions. Arti ial Intelligen e, 75, 195{240.421\nPynadath & TambeKitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,S. (1999). Robo upres ue: Sear h and res ue for large-s ale disasters as a domain formultiagent resear h. In Pro eedings of the IEEE International Conferen e on Systems,Man and Cyberneti s.Levesque, H. J., Cohen, P. R., & Nunes, J. (1990). On a ting together. In Pro eedings ofthe National Conferen e on Arti ial Intelligen e.Mars hak, J., & Radner, R. (1971). The E onomi Theory of Teams. Yale University Press,New Haven, CT.Nair, R., Tambe, M., & Marsella, S. (2002a). Team formation for reformation for multia-gent domains like robo up res ue. In Pro eedings of the International Symposium onRoboCup.Nair, R., Tambe, M., Marsella, S., & Raines, T. (2002b). Automated assistants for analyzingteam behaviors. Journal of Autonomous Agents and Multiagent Systems, to appear.Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The omplexity of Markov de ision pro- esses. Mathemati s of Operation Resear h, 12 (3), 441{450.Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning to ooperate viapoli y sear h. In Pro eedings of the Conferen e on Un ertainty in Arti ial Intelli-gen e, pp. 489{496.Pynadath, D. V., & Tambe, M. (2002). An automated teamwork infrastru ture for het-erogeneous software agents and humans. Journal of Autonomous Agents and Multi-Agent Systems: Spe ial Issue on Infrastru ture and Requirements for Building Re-sear h Grade Multi-Agent Systems, to appear.Pynadath, D. V., Tambe, M., Chauvat, N., & Cavedon, L. (1999). Toward team-orientedprogramming. In Jennings, N. R., & Lesp eran e, Y. (Eds.), Intelligent Agents VI:Agent Theories, Ar hite tures and Languages, pp. 233{247. Springer-Verlag.Raines, T., Tambe, M., & Marsella, S. (2000). Automated agents that help humans under-stand team behaviors. In Pro eedings of the International Conferen e on AutonomousAgents.Ri h, C., & Sidner, C. (1997). COLLAGEN: When agents ollaborate with people. InPro eedings of the International Conferen e on Autonomous Agents.Smallwood, R. D., & Sondik, E. J. (1973). The optimal ontrol of partially observableMarkov pro esses over a nite horizon. Operations Resear h, 21, 1071{1088.Smith, I. A., & Cohen, P. R. (1996). Toward a semanti s for an agent ommuni ationslanguage based on spee h-a ts. In Pro eedings of the National Conferen e on Arti ialIntelligen e, pp. 24{31.Sonenberg, E., Tidhar, G., Werner, E., Kinny, D., Ljungberg, M., & Rao, A. (1994). Plannedteam a tivity. Te h. rep. 26, Australian AI Institute.Tambe, M. (1997). Towards exible teamwork. Journal of Arti ial Intelligen e Resear h,7, 83{124. 422\nThe Communi ative Multiagent Team De ision ProblemTambe, M., Pynadath, D. V., Chauvat, N., Das, A., & Kaminka, G. A. (2000). Adaptiveagent integration ar hite tures for heterogeneous team members. In Pro eedings ofthe International Conferen e on Multi-Agent Systems, pp. 301{308.Tambe, M., & Zhang, W. (1998). Towards exible teamwork in persistent teams. In Pro- eedings of the International Conferen e on Multi-Agent Systems, pp. 277{284.Tidhar, G. (1993). Team-oriented programming: Preliminary report. Te h. rep. 41, Aus-tralian Arti ial Intelligen e Institute.Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communi ation de isions in multi-agent ooperation: Model and experiments. In Pro eedings of the International Conferen eon Autonomous Agents, pp. 616{623.Yen, J., Yin, J., Ioerger, T. R., Miller, M. S., Xu, D., & Volz, R. A. (2001). CAST:Collaborative agents for simulating teamwork. In Pro eedings of the InternationalJoint Conferen e on Arti ial Intelligen e, pp. 1135{1142.Yoshikawa, T. (1978). De omposition of dynami team de ision problems. IEEE Transa -tions on Automati Control, AC-23 (4), 627{632.\n423"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": null, "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}