{"id": "1106.4632", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2011", "title": "Inferring 3D Articulated Models for Box Packaging Robot", "abstract": "given a point cloud, we consider inferring kinematic models of 3d articulated objects such as meat boxes for the purpose of manipulating toward them. while previous lab work literature has shown nearly how to extract a planar kinematic model ( often represented as a linear segment chain ), such planar models do clearly not uniformly apply to 3d objects that are explicitly composed of segments often linked to the other segments in cyclic configurations. we present an approach for building a model that captures the relation between the input point cloud matching features and the object segment as well as restoring the relation between the smaller neighboring object segments. we instead use a conditional random field operator that allows us to model the dependencies between those different segments of the object. we test our approach on inferring the kinematic structure from partial and seemingly noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and long cardboard cartons of spanned several sizes. the inferred structure enables our robot to successfully close these boxes firmly by manipulating the flaps.", "histories": [["v1", "Thu, 23 Jun 2011 05:32:39 GMT  (7004kb,D)", "http://arxiv.org/abs/1106.4632v1", "For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate"]], "COMMENTS": "For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["heran yang", "tiffany low", "matthew cong", "ashutosh saxena"], "accepted": false, "id": "1106.4632"}, "pdf": {"name": "1106.4632.pdf", "metadata": {"source": "CRF", "title": "Inferring 3D Articulated Models for Box Packaging Robot", "authors": ["Heran Yang", "Tiffany Low", "Matthew Cong"], "emails": ["hy279@cornell.edu,", "twl46@cornell.edu,", "mdc238@cornell.edu,", "asaxena@cs.cornell.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTION\nGiven a point cloud of a scene, we present a method for extracting an articulated 3D model that represents the kinematic structure of an object such as a box. We apply this to enable a robot to autonomously close boxes of several shapes and sizes. Such an ability is of interest to a personal assistant robot as well as to commercial robots in applications such as packaging and shipping.\nPrevious work on articulated structures was able to represent planar kinematic models as linear structures such as a chain of rigid bodies connected by joints. Linear structures greatly simplify the inference problem because they decompose the joint inference problem into independent sub-problems. Katz et al. [1, 2] considered linear articulated structures in 2D by relying on active vision techniques to learn kinematic properties of objects.\nHowever, complex objects cannot be easily represented by linear chains. For example, a box is an example of an oriented arrangement of segments which are highly interconnected. In a standard box (see Fig. 1), we can observe that the sides and base of the box are connected to four other faces while flaps extend outwards from only one face of the box. A linear model is unable to express these relations and constraints on the object\u2019s structure. Our proposed model for 3D kinematic objects allows for increased expressivity while maintaining computational tractability.\nWe present an approach for building such a 3D articulated model that captures the relation between the input\npoint cloud features the object segment as well as the relation between the neighboring object segments. We use an conditional random field (CRF) as an undirected graphical model that allows us to model the different segments of the object independently.\nWe evaluated our algorithm on boxes of varying sizes and structures over multiple experiments. We perceived the scene (that often contains multiple boxes and other clutter) using a Microsoft Kinect camera. The boxes were oriented at various angles causing a varying number of box flaps to be obscured. We obtain an overall accuracy of 76.55% in building the box model and of 86.74% in identifying enough planes to close the box. We have verified the robot closing several boxes in simulation and have also applied our method on a robotic arm closing a box."}, {"heading": "II. RELATED WORK", "text": "Katz et al. [1, 2] developed a relational representation of kinematic structure. In their model, a chain structure is used to model the kinematic properties of objects. Previous work by Sturm et al. [3] models motion that cannot be described by a simple prismatic or revolute joint. They successfully predicted the motion of drawers and cabinet doors. However, these joints are modeled within a linear structure.\nThere has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6]. These methods search through object primitives from the input image data in order to identify complex objects and have been successfully\nar X\niv :1\n10 6.\n46 32\nv1 [\ncs .R\nO ]\n2 3\nJu n\n20 11\napplied in performing a variety of manipulation tasks. For example, Rusu et al. [7] identified planes in a household kitchen environment which were fitted to models of common kitchen objects such as cupboards and tables.\nThe field of computer vision also contains some related work on part-based models involving the decomposition of objects into sub-parts. For example, Crandall et al. [8] used a kinematic tree model to model human motion. This reduces the complexity of the search space [9] but maintains the representational power of the kinematic structure obtained. Hahnel et al. [10] produced accurate models of indoor and outdoor environments that compare favorably to other methods which decompose and approximate environments using flat surfaces. Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks."}, {"heading": "III. OUR APPROACH", "text": "The robot requires a good estimate of the box configuration before it can plan and execute a set of motions to close the box. The robot has to recognize boxes from the input point cloud data (see Fig. 3). The task is challenging because we often have multiple boxes (along with other clutter) in the scene and because we consider a wide variety of boxes. In addition, the boxes are in random orientations and positions within the user environment.\nAn object is composed of several segments. For example, a box can be described as a set of structured planes. We first segment the point cloud into clusters, each of which is then decomposed into segments. Our goal is to learn the kinematic model G by correctly identifying the segments. In our application, the model is a box consisting of four sides, a base, and four flaps. Our goal is to find the optimal model G\u2217 = arg max\nG J(G) with respect to a scoring function\nJ(G)."}, {"heading": "A. Conditional Random Field (CRF)", "text": "The process of identifying the different segments of 3D articulated structures is highly sequential and conditional. For instance, the position and orientation of a segment of a 3D object are likely to be well defined after a connected or nearby segment has been located. The relations we introduce in Section III-D provide further examples. We therefore use a conditional random field (CRF) for the undirected probabilistic graphical model to determine the 3D articulated structure.\nFormally, we define G = (V, E) to be an undirected graph such that there is a node v \u2208 V corresponding to each of the random variables that represents a segment in the 3D structure. Each edge (vi, vj) \u2208 E represents that nodes vi and vj are not independent while a binary potential function, designated as \u0393(vi, vj), describes the relation or dependency between vi and vj . Moreover, in some cases, segments of 3D articulated structures have internal features\nthat are not dependent on any part of the object itself such as the natural orientations or natural sizes. Therefore, each node v \u2208 V is associated with some unary potential functions and that we will refer to as \u03c6(vi)."}, {"heading": "B. Score Function for 3D Structure Modeling", "text": "The scoring function J(G) is a measure of the fitness of G over a set of features \u03c6 and relations \u0393. The corresponding weights are w\u03c6 and w\u0393 for \u03c6 and \u0393 respectively. The detailed explanations of \u03c6 and \u0393 are in Section III-D and the learning algorithm for the weights is covered in Section III-E.\nJ(G) = \u2211 i\u2208V wT\u03c6\u03c6(vi) + \u2211 (i,j)\u2208E wT\u0393 \u0393(vi, vj) (1)"}, {"heading": "C. Complexity Analysis", "text": "An exhaustive search over all matchings of n segments to a model consisting of m segments is combinatorial to the order of O( n!(n\u2212m)! ) = O(n\nm). Allowing for empty planes to be matched to the model segments does not increase the order of complexity O(nm). We show the state space can be significantly reduced in size if the segments are conditionally independent of each other.\nEach node vi has a set \u2206i of adjacent nodes. vi is independent of all nodes outside \u2206i. When we are searching over the different matchings of the model and assigning possible segments to one node v, all the other independent nodes can be ignored due to conditional independence. In other words, if there are ti segments that have not yet been assigned and there are ki nodes \u2208 \u2206i that have not yet been determined, we can restrict the search space to be O( ti!ki! ) for node vi.\nFor the box model in Fig. 2, the problem of matching segments to the flaps and the bottom face becomes a series of problems linear to the size of n if the four sides have been determined. For example, in Fig. 2, if the side plane nodes 0, 1, 2 and 3 are pre-chosen, the search space is O(5(n \u2212 4)) = O(n), giving an overall state space of O(n4)O(n) = O(n5) which is tractable for application problems such as ours."}, {"heading": "D. Feature and Relation Sets", "text": "For any collection of objects in a manipulation task, a set of features and relations can be chosen to distinguish them from other objects in the environment. A feature describes the segment relative to either a ground reference or the segment\u2019s properties. In comparison, a relation encodes the relative values of a set of properties between at least two segments within the model. Both features and relations require the tolerance parameter \u03c4 to define the bound of correctness. \u03c4 is defined as an angle/distance when it is used to bound orientation/location. In our model, we have the following features and relations: \u03c61: Absolute Orientation: A majority of object models have a natural orientation, e.g. windows are vertical. \u03c61 measures the orientation of segment i and compares it with unit vector u. \u03c61 = \u03c4 \u2212 \u2016 arccos( v T i u \u2016vi\u2016 )\u2016.\n\u03c62: Absolute Location: \u03c62 measures the closeness of the object model\u2019s segment i to a reference point L. \u03c62 = \u03c4 \u2212 \u2016vi \u2212 L\u2016. \u03c63: Existence: For real world data, it is hard for robots to distinguish between corrupted data and partially missing data from observation. \u03c63 assigns a reward for finding segment i in the model.\n\u03931: Relative Orientation: Nearly all objects have rigid segments with fixed relative orientations, e.g. legs of tables are always perpendicular to their surfaces. \u03931 compares the difference of the orientations of segment i and j to a specified angle \u03b1. \u03931 = \u03c4\u2212\u2016\u03b1\u2212\u2016 arccos( v T i vj\n\u2016vi\u00b7vj\u2016 )\u2016\u2016. In the box model, connected sides and bottoms are perpendicular to each other and therefore \u03b1 = \u03c0/2.\n\u03932: Relative Location: Nearly all objects have rigid segments with fixed relative position. \u03932 compares the difference of the locations of segment i and j to a specified unit vector u. \u03932 = \u03c4 \u2212 \u2016 arccos( (vi\u2212vj) Tu \u2016vi\u2212vj\u2016 )\u2016. In the box model, flaps are always higher than sides and sides are always higher than base.\n\u03933: Segment Connectivity: For objects with rotatable segments, \u03933 measures the connectivity of the object model\u2019s segments i and j by calculating the distance in between. \u03932 = \u03c4 \u2212\u2016vi\u2212 vj\u2016. In the box model, the sides, base, and flaps are all under such relation.\nFor complicated or obscure box models, more edges have to be added in addition to the base features and relations in Fig. 2. For example, in the box model, a new relation \u03b34 that measures the model\u2019s rectangular structure is later demonstrated via learning to be significantly crucial. For each two side planes i, j that are across to each other, we find the four pairs of associated points (pi, pj) and test if the vector pi \u2212 pj is parallel to the side planes\u2019 orientations. In addition, \u03b35 measures if two side planes that are across from each other are parallel. \u03b34 and \u03b35 combined give a satisfactory evaluation of the model\u2019s rectangular structure. However, \u03b34 and \u03b35 break the original conditional independence property which significantly increases the size of the search space."}, {"heading": "E. Learning", "text": "We collected ground-truth labeled data for training the parameters w\u03c6 and w\u03b3 . Features \u03c6 and \u03b3 are computed from the box model G that our algorithm built from the point cloud. For each pair of \u03c6 and \u03b3, we obtain J(G) and check the correctness of G by comparing it with the labeled data, i.e. the ratio of correctly marked planes to the total number of planes in the labeled model. Note that the equation 1 is linear in w\u03c6 and w\u03b3 . Therefore, we can use the normal equations with regularization to estimate the parameters [15].\nF. Inference\nFollowing the conditional independence assumption encoded by the graphical model in Fig. 2 (see Section III-C), we only have to consider a tractable number of box assignments for inferring the optimal configuration. We evaluate scores for all cases and select the optimal one. In practice, we evaluate the ones with higher \u03c6\u2019s by forming a priority queue which speeds up the inference further.\nHowever, for more complex models, i.e. too many relations in the graph, using the full relation set may cause no independent segments. In this case, we remove certain relations that are determined by learning to be the least important and produce a sparse graph with fewer edges. Then we apply the above procedure to the simple graph and receive the updated list L of all candidate matchings. We then selected the top c\u2016L\u2016 elements of the candidate matchings and put them in a priority queue that ranks by applying the full feature set to G. The top element of the resulting queue is chosen as the optimal predicted solution G\u0302\u2217. Variable c depends on the computational power and we found from experiments that as c becomes larger than 1/4, the optimal solution converges to the predicted G\u0302\u2217."}, {"heading": "G. Segment Identification", "text": "Given the point cloud data, segments need to be extracted from the main clusters and be correctly identified to build representative object models,. We apply the RANSAC algorithm [4] to extract segments from the point cloud. In our case, RANSAC is applied iteratively and the best fitting planes to the clusters are also obtained sequentially. However the planes returned from RANSAC are not bounded by rectangles and may contain outliers. We therefore fetch the extracted planes by filtering out outliers using Euclidean clustering. To obtain the corners from the point cloud associated with each plane, we construct the convex hulls of the points to eliminate redundant points and find the minimum bounding rectangles of the planes.\nBy going through the above procedures, planes can be obtained by almost 100% accuracy. However, any noise in the environment that is attached to the main cluster can also potentially be identified as planes which may confuse the searching algorithm. Therefore we ignore the noise if the\nerror rate evaluated from RANSAC is over a pre-defined limit. We also rely on the feature/relation performance for boxes with noise attached."}, {"heading": "IV. PLANNING AND CONTROL", "text": "To close a box in the environment, the robot needs to identify and locate the box to manipulate it into the desired state. Each flap can be closed from a set of paths. Given the box model with the location and orientation of each flap, we use OpenRAVE\u2019s [16] under-constrained inverse kinematics solver. The program applies brute-force search approach by defining intermediate rotating planes. The planner will pick sampled points from each intermediate planes and search for paths through them.\nTo decide the order of closing the flaps, our program sorts the flaps in order of ascending area, which are given by the box models. Then the manipulator will greedily close the flaps in that order. For the three flaps that are closer to the robot arm, the planner is able to find valid paths in 90% of the experiments. However for the farthest flap, the planner only has a success rate of 50% due to the robot\u2019s limited workspace."}, {"heading": "V. HARDWARE", "text": "Our experiments were conducted using an Adept Viper s850 6-DOF arm with a parallel-plate gripper giving a reach of approximately 100 centimeters. In order to obtain point cloud data, a Microsoft Kinect was mounted on the robot in a position (pictured in Fig. 4) that allowed for changes\nin the orientation of the camera in order to obtain a variety of viewpoints."}, {"heading": "VI. EXPERIMENTS", "text": "To demonstrate the robustness of our algorithm, we collected point cloud data for several classes of boxes including various sizes of standard packaging boxes and unusually shaped boxes (see Fig. 3). We then ran a series of experiments on an extensive dataset to determine the accuracy of the inference algorithm and then verified that the robot was able to close the box through simulation."}, {"heading": "A. Data", "text": "Our testing environment contains a total of 50 point clouds and has a total of 15 different types of boxes in the set. We relied on 12 of these point clouds for training purposes and the rest for testing.\nWe considered a set of standard cardboard boxes which were sorted into three size categories and tested the algorithm on unusual boxes including cake boxes, pizza boxes and a cardboard carton. For each box type, we collected images of the box from different orientations. The point clouds in our dataset often included extraneous common household objects placed among the scene such as toys, dishes, and cups."}, {"heading": "B. Accuracy Evaluation", "text": "We measure flap inference accuracy and full model inference accuracy on our box model (see Table I). We define a model\u2019s flap inference accuracy as (the number of correctly identified flaps - the number of wrongly identified flaps) / (the number of flaps in the point cloud data). Similarly, the full model inference accuracy = (the number of correctly identified planes - the number of wrongly identified planes) / (the number of planes in the point cloud data). Notice that all four rotations of the box model are acceptable. For box closing, flap inference accuracy is the most important metric and we define it to be the percentage of correctly identified flap segments."}, {"heading": "C. Results on the Dataset", "text": "Fig. 3 shows the original image, the point cloud data, the segments observed, the matching returned by the algorithm, and finally the conditional random field graphical model. We see that the robot can identify a diverse set of boxes. The performance remains stable for boxes of various sizes and the algorithm is able to recognize non-standard boxes. This is seen in examples 3 and 7, where the structure of a flap on a flap is correctly identified despite its deviation from the labeled model.\nOur algorithm is able to filter out noise in the data. For the second example, the segments belonging to items in the box are discarded in the matched model. Similarly, the algorithm is able to remove the sources of noise in the data for examples 4 and 7. When more than one box is in the scene, the algorithm is able to recognize the segments as individual boxes. This is seen in example 6.\nThe algorithm is able to identify box models with a 76.55% accuracy. For the closing task, we only require the correct identification of flaps which we obtain with an accuracy of 86.74% Therefore the robot shows good performance in identifying flaps to be closed. This also demonstrates that the algorithm is able to tolerate noise and different box configurations and types.\nFinally, we demonstrate that our algorithm and its associated articulated model can be applied to real-world tasks through a set of robotic experiments where the robot successfully closes the identified flaps. Please see the video at: http://pr.cs.cornell.edu/articulated3d"}, {"heading": "ACKNOWLEDGMENTS", "text": "We acknowledge Yun Jiang, Akram Helou, Marcus Lim and Stephen Moseson for useful discussions."}], "references": [{"title": "Learning to Manipulate Articulated Objects in Unstructured Environments using a Grounded Relational Representation", "author": ["D. Katz", "Y. Pyuro", "O. Brock"], "venue": "RSS, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting Planar Kinematic Models using Interactive Perception", "author": ["D. Katz", "O. Brock"], "venue": "Unifying Perspectives in Computational and Robot Vision. Springer, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "3D Pose Estimation, Tracking and Model Learning of Articulated Objects from Dense Depth Video using Projected Texture Stereo", "author": ["J. Sturm", "K. Konolige", "C. Stachniss", "W. Burgard"], "venue": "RSS, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography", "author": ["M. Fischler", "R. Bolles"], "venue": "Communications of the ACM, 1981.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1981}, {"title": "Efficient RANSAC for Point-Cloud Shape Detection", "author": ["R. Schnabel", "R. Wahl", "R. Klein"], "venue": "Computer Graphics Forum, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalizing the Hough Transform to Detect Arbitrary Shapes", "author": ["D. Ballard"], "venue": "Pattern Recognition, 1981.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1981}, {"title": "Towards 3D Point Cloud Based Object Maps for Household Environments", "author": ["R. Rusu", "Z. Marton", "N. Blodow", "M. Dolha", "M. Beetz"], "venue": "RSS, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Beyond Trees: Common Factor Models for 2D Human Pose Recovery", "author": ["X. Lan", "D.P. Huttenlocher"], "venue": "ICCV, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Spatial Priors for Part-based Recognition using Statistical Models", "author": ["D. Crandall", "P. Felzenszwalb", "D. Huttenlocher"], "venue": "CVPR, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning Compact 3D Models of Indoor and Outdoor Environments with a Mobile Robot", "author": ["D. H\u00e4hnel", "W. Burgard", "S. Thrun"], "venue": "RSS, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient Grasping from RGBD Images: Learning using a New Rectangle Representation", "author": ["Y. Jiang", "S. Moseson", "A. Saxena"], "venue": "ICRA, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Grasp Strategies with Partial Shape Information", "author": ["A. Saxena", "L. Wong", "A.Y. Ng"], "venue": "AAAI, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to Place New Objects", "author": ["Y. Jiang", "C. Zheng", "M. Lim", "A. Saxena"], "venue": "RSS Workshop on Mobile Manipulation, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Human activity detection from rgbd images", "author": ["J.Y. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "AAAI workshop on Pattern, Activity and Intent Recognition (PAIR), 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Openrave: A Planning Architecture for Autonomous Robotics", "author": ["R. Diankov", "J. Kuffner"], "venue": "Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-08-34, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2] considered linear articulated structures in 2D by relying on active vision techniques to learn kinematic properties of objects.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] considered linear articulated structures in 2D by relying on active vision techniques to learn kinematic properties of objects.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1, 2] developed a relational representation of kinematic structure.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] developed a relational representation of kinematic structure.", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[3] models motion that cannot be described by a simple prismatic or revolute joint.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "There has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6].", "startOffset": 104, "endOffset": 110}, {"referenceID": 4, "context": "There has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6].", "startOffset": 104, "endOffset": 110}, {"referenceID": 5, "context": "There has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "[7] identified planes in a household", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used a kinematic tree model to model human motion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This reduces the complexity of the search space [9] but maintains the representational power of the kinematic structure obtained.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "[10] produced accurate models of indoor and outdoor environments that compare favorably to other methods which decompose and approximate environments using flat surfaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 102, "endOffset": 110}, {"referenceID": 12, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Therefore, we can use the normal equations with regularization to estimate the parameters [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "We apply the RANSAC algorithm [4] to extract segments from the point cloud.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "Given the box model with the location and orientation of each flap, we use OpenRAVE\u2019s [16] under-constrained inverse kinematics solver.", "startOffset": 86, "endOffset": 90}], "year": 2011, "abstractText": "Given a point cloud, we consider inferring kinematic models of 3D articulated objects such as boxes for the purpose of manipulating them. While previous work has shown how to extract a planar kinematic model (often represented as a linear chain), such planar models do not apply to 3D objects that are composed of segments often linked to the other segments in cyclic configurations. We present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments. We use a conditional random field that allows us to model the dependencies between different segments of the object. We test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and cardboard cartons of several sizes. The inferred structure enables our robot to successfully close these boxes by manipulating the flaps.", "creator": "LaTeX with hyperref package"}}}