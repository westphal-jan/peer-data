{"id": "1107.1744", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2011", "title": "Stochastic convex optimization with bandit feedback", "abstract": "importantly this paper addresses performing the problem of minimizing a convex, lipschitz function $ f $ over a convex, compact set $ \\ xset $ under a stochastic bandit feedback model. in this model, the algorithm problem is allowed to produce observed noisy realizations of decreasing the function value $ f ( d x ) $ at not any query procedure point $ x \\ in \\ xset $. computing the quantity of interest is regret of the algorithm, term which is the sum of the function values at algorithm's query points r minus the optimal function value. we later demonstrate a generalization of understanding the ellipsoid algorithm that incurs $ \\ otil ( \\ poly ( d ) \\ sqrt { t } ) $ regret. since any algorithm has received regret at least $ \\ omega ( \\ sqrt { t } ) $ on this problem, subsequently our algorithm is immediately optimal in terms of the continuous scaling with $ t $.", "histories": [["v1", "Fri, 8 Jul 2011 22:18:05 GMT  (525kb,D)", "https://arxiv.org/abs/1107.1744v1", null], ["v2", "Sat, 8 Oct 2011 06:06:43 GMT  (526kb,D)", "http://arxiv.org/abs/1107.1744v2", null]], "reviews": [], "SUBJECTS": "math.OC cs.LG cs.SY", "authors": ["alekh agarwal", "dean p foster", "daniel j hsu", "sham m kakade", "alexander rakhlin"], "accepted": true, "id": "1107.1744"}, "pdf": {"name": "1107.1744.pdf", "metadata": {"source": "CRF", "title": "Stochastic convex optimization with bandit feedback", "authors": ["Alekh Agarwal", "Dean P. Foster", "Daniel Hsu", "Sham M. Kakade", "Alexander Rakhlin"], "emails": [], "sections": [{"heading": null, "text": "\u221a T )\nregret. Since any algorithm has regret at least \u2126( \u221a T ) on this problem, our algorithm is optimal in terms of the scaling with T ."}, {"heading": "1 Introduction", "text": "The classical multi-armed bandit problem, formulated by Robbins in 1952, is arguably the most basic setting of sequential decision-making under uncertainty. Upon choosing one of k available actions (\u201carms\u201d), the decision-maker observes an i.i.d. realization of the arm\u2019s cost drawn according to a distribution associated with the arm. The performance of an allocation rule (algorithm) in sequentially choosing the arms is measured by regret, that is the difference between the expected costs of the chosen actions as compared to the expected cost of the best action. Various extensions of the classical formulation have received much attention in recent years. In particular, research has focused on the development of optimal and efficient algorithms for multi-armed bandits with large or even infinite action spaces, relying on various assumptions on the structure of costs (rewards) over the action space. When such a structure is present, the information about the cost of one arm propagates to other arms as well, making the problem tractable. For instance, the mean cost function is assumed to be linear in the paper [9], facilitating global \u201csharing of information\u201d over a compact convex set of actions in a d-dimensional space. A Lipschitz condition on the mean cost function allows a local propagation of information about the arms, as costs cannot change rapidly in a neighborhood of an action. This has been exploited in a number of works, notably [2, 13, 14]. Instead of the Lipschitz condition, Srinivas et al. [18] exploit the structure of Gaussian processes, focusing on the notion of the effective dimension. These various \u201cnon-parametric\u201d bandit problems typically suffer from the curse of dimensionality, that is, the best possible convergence rates (after T queries) are typically of the form T\u03b1, with the exponent \u03b1 approaching 1 for large dimension d.\nThe question addressed in the present paper is: How can we leverage convexity of the mean cost function as a structural assumption? The main contribution of the paper is an algorithm which achieves, with high probability, an O\u0303(poly(d) \u221a T ) regret after T requests. This result holds for all convex Lipschitz mean cost functions. We remark that the rate does not deteriorate with d (except in the multiplicative term) implying that convexity is a strong structural assumption which turns \u201cnon-parametric\u201d Lipschitz problems into \u201cparametric\u201d. Nevertheless, convexity is a very natural and basic assumption, and applications of our method are, therefore, abundant. Let us also remark that \u2126( \u221a dT ) lower bounds have been shown for linear mean cost functions [9], making our algorithm optimal up to factors polynomial in the dimension d and logarithmic in the number of iterations T .\nWe note that our work focuses on the so-called stochastic bandits setting, where the observed costs of an action are i.i.d. draws from a fixed distribution. A parallel line of literature focuses on the more difficult\nar X\niv :1\n10 7.\n17 44\nv2 [\nm at\nh. O\nC ]\n8 O\nct 2\n01 1\nadversarial setting where the costs of actions change arbitrarily from round to round. Leveraging structure in non-stochastic bandit settings is more complex, and is not a goal of this paper.\nWe start by defining some notation and the problem setup below. The next section will survey related prior works and describe their connections with our work in Section 3. Section 4 gives the algorithm and analysis for the special case of univariate optimization. The algorithm for higher dimensions and its analysis are given in Section 5.\nNotation and setup: Let X be a compact and convex subset of Rd, and let f : X \u2192 R be a 1-Lipschitz convex function on X , so f(x)\u2212 f(x\u2032) \u2264 \u2016x\u2212 x\u2032\u2016 for all x, x\u2032 \u2208 X . We assume that X is specified in a way so that an algorithm can efficiently construct the smallest Euclidean ball containing the set. Furthermore, we assume the algorithm has noisy black-box access to f . Specifically, the algorithm is allowed to query the value of f at any x \u2208 X , and the response to the query x is\ny = f(x) + \u03b5\nwhere \u03b5 is an independent \u03c3-subgaussian random variable with mean zero: E[exp(\u03bb\u03b5)] \u2264 exp(\u03bb2\u03c32/2) for all \u03bb \u2208 R. The algorithm incurs a cost f(x) for each query x. The goal of the algorithm is to minimize its regret : after making T queries x1, . . . , xT \u2208 X , the regret of the algorithm is\nRT = T\u2211 t=1 ( f(xt)\u2212 f(x\u2217) ) where x\u2217 is the minimizer of f over X (we do not require uniqueness of x\u2217).\nSince we observe noisy function values, our algorithms will make multiple queries of f at the same point. We will construct an average and confidence interval (henceforth CI) around the average for the function values at points queried by the algorithm. We will use the notation LB\u03b3i(x) and UB\u03b3i(x) to denote the lower and upper bounds of a CI of width \u03b3i for the function estimate of a point x. We will say that CI\u2019s at two points are \u03b3-separated if LB\u03b3i(x) \u2265 UB\u03b3i(y) + \u03b3 or LB\u03b3i(y) \u2265 UB\u03b3i(x) + \u03b3."}, {"heading": "2 Related work", "text": "Asymptotic rates of O( \u221a T ) have been previously achieved by Cope [8] for unimodal functions under stringent conditions (smoothness and strong convexity of the mean cost function, in addition to the unconstrained optimum being achieved inside the constraint set). The method employed by the author is a variant of the classical Kiefer-Wolfowitz procedure [12] for estimation of an optimum point. Further, the rate O\u0303( \u221a T ) has been achieved in Auer et al. [3] for a one-dimensional non-convex problem with finite number of optima. The result assumes continuous second derivatives of the mean function, not vanishing at the optimum, while the first derivative is assumed to be zero at the optima. The method is based on discretizing the interval and does not exploit convexity. Yu and Mannor [19] recently studied unimodal bandits, but they only consider one-dimensional and graph-structured settings. Bubeck et al. [6] consider the general setup of X -armed bandits with Lipschitz mean cost functions and their algorithm does give O(c(d) \u221a T ) regret for a dimension dependent constant c(d) in some cases when the problem has a near-optimality dimension of 0. However, not all convex, Lipschitz functions satisfy this condition, and c(d) can grow exponentially in d even in these special cases.\nThe case of convex, Lipschitz cost functions has been looked at in the harder adversarial model [10, 13] by constructing one-point gradient estimators. However, the best-known regret bounds for these algorithms are O(T 3/4). Agarwal et al. [1] show a regret bound of O( \u221a T ) in the adversarial setup, when two evaluations of the same function are allowed, instead of just one. However, this does not include the stochastic bandit optimization setting since each function evaluation in the stochastic case is corrupted with independent noise, violating the critical requirement of a bounded gradient estimator that their algorithm exploits. Indeed, applying their result in our setup yields a regret bound of O(T 3/4).\nA related line of work attempts to solve convex optimization problems by instead posing the problem of finding a feasible point from a convex set. Different oracle models of specifying the convex set correspond to different optimization settings. The bandit setting is identical to finding a feasible point, given only a membership oracle for the convex set. Since we get only noisy function evaluations, we in fact only have access to a noisy membership oracle. While there are elegant solutions based on random walks in the easier separation oracle model [5], the membership oracle setting has been mostly studied in the noiseless setting only and uses much more complex techniques building on the seminal work of Nemirovski and Yudin [15]. The techniques have the additional drawback that they do not guarantee a low regret since the methods often explore aggressively.\nWe observe that the problem addressed in this paper is closely related to noisy zero-th order (also called derivative-free) convex optimization, whereby the algorithm queries a point of the domain and receives a noisy value of the function. Given > 0, such algorithms are guaranteed to produce an -minimizer at the end of T iterations. While the literature on stochastic optimization is vast, we emphasize that an optimization guarantee does not necessarily imply a bound on regret. We explain this point in more detail below.\nSince f is convex by assumption, the average x\u0304T = 1 T \u2211T t=1 xt must satisfy f(x\u0304T ) \u2212 f(x\u2217) \u2264 RT /T (by Jensen\u2019s inequality). That is, a method guaranteeing small regret is also an optimization algorithm. The converse, however, is not necessarily true. Suppose an optimization algorithm queries T points of the domain and then outputs a candidate minimizer x\u2217T . Without any assumption on the behavior of the optimization method nothing can be said about the regret it suffers over T iterations. In fact, depending on the particular setup, an optimization method might prefer to spend time querying far from the minimum of the function (that is, explore) and then output the solution at the last step. Guaranteeing a small regret typically involves a more careful balancing of exploration and exploitation. This distinction between arbitrary optimization schemes and anytime methods is discussed further in the paper [17].\nWe note that most of the existing approaches to derivative-free optimization outlined in the recent book [7] typically search for a descent or sufficient descent direction and then take a step in this direction. However, most convergence results are asymptotic and do not provide concrete rates even in an optimization error setting. The main emphasis is often on global optimization of non-convex functions, while we are mainly interested in convex functions in this work. Nesterov [16] recently analyzes schemes similar to that of Agarwal et al. [1] with access to noiseless function evaluations, showing O( \u221a dT ) convergence for nonsmooth functions and accelerated schemes for smooth mean cost functions. However, when analyzed in a noisy evaluation setting, his rates suffer from the degradation as those of Agarwal et al. [1]."}, {"heading": "3 Outline of our approach", "text": "The close relationship between convex optimization and the regret-minimization problem suggests a plan of attack: Check whether existing stochastic zeroth order optimization methods (that is, methods that only query the oracle for function values), in fact, minimize regret. Two types of methods for stochastic zeroth order convex optimization are outlined in Nemirovski and Yudin [15, Chapter 9]. The first approach uses the noisy function values to estimate a gradient direction at every step, and then passes this information to a stochastic first-order method. The second approach is to use the zeroth order information to estimate function values and pass this information to a noiseless zeroth order method. Nemirovski and Yudin argue that the latter approach has greater stability when compared to the former. Indeed, for a gradient estimate to be meaningful, function values should be sampled close to the point of interest, which, in turn, results in a poor quality of the estimate. This tension is also the source of difficulty in minimizing regret with a convex mean cost function.\nOwing to the insights of Nemirovski and Yudin [15], we opt for the second approach, giving up the idea of estimating the first-order information. The main novel tool of the paper is a \u201ccenter-point device\u201d that allows to quickly detect that the optimization method might be paying high regret and to act on this information. Unlike discretization-based methods, the proposed algorithm uses convexity in a crucial way. We first demonstrate the device on one-dimensional problems, where the solution is clean and intuitive. We then develop a version of the algorithm for higher dimensions, basing our construction on the beautiful zero-\nth order optimization method of Nemirovski and Yudin [15]. Their method does not guarantee vanishing regret by itself, and a careful fusion of this algorithm with our center-point device is required. The overall approach would be to use center-point device in conjunction with a modification of the classical ellipsoid algorithm.\nTo motivate the center-point device, consider the following situation. Suppose f is the unknown function on X = [0, 1], and assume for now that it is linear with a slope T\u22121/3. Let us sample function values at x = 1/4 and x = 3/4. To even distinguish the slope from a slope \u2212T\u22121/3 (which results in a minimizer on the opposite side of X ), we need O(T 2/3) points. If the function f is linear indeed, we only incur O(T 1/3) regret on these rounds. However, if instead f is a quadratic dipping between the sampled points, we incur regret of O(T 2/3). To quickly detect that the function is not flat between the two sampled points, we additionally sample at x = 1/2. The center point acts as a sentinel : if it is recognized that the function value at the center point is noticeably below the other two values, the region [0, 1/4] \u222a [3/4, 1] can be discarded. If it is recognized that the value of f either at x = 1/4 or at x = 3/4 is greater than others, then either [0, 1/4] or [3/4, 1] can be discarded. Finally, if f at all three points appears to be similar at a given scale, we have a certificate that the algorithm is not paying regret larger than this scale per query. The remaining argument proceeds similarly to the binary search or the method of centers of gravity: since a constant portion of the set is discarded every time, it only requires a logarithmic number of \u201ccuts\u201d. We remark that this novelty is indeed in ensuring that regret is kept small in the process; a simpler algorithm which does not query the center is sufficient to guarantee a small optimization error but incurs a large regret on examples of the form sketched above.\nIn the next section we present the algorithm that results from the above ideas for one-dimensional convex optimization. The general case in higher dimensions is presented in Section 5."}, {"heading": "4 One-dimensional case", "text": "We start with a specialization of the setting to 1-dimension to illustrate some of the key ideas including the center-point device. We assume without loss of generality that the domain X = [0, 1], and f(x) \u2208 [0, 1] (the latter can be achieved by pinning f(x\u2217) = 0 since f is 1-Lipschitz)."}, {"heading": "4.1 Algorithm description", "text": "Algorithm 1 proceeds in a series of epochs demarcated by a working feasible region (the interval [l\u03c4 , r\u03c4 ] in epoch \u03c4). In each epoch, the algorithm aims to discard a portion of the working feasible region determined to only contain suboptimal points. To do this, the algorithm repeatedly makes noisy queries to f at three different points in the working feasible region. Each epoch is further subdivided into rounds, where we query the function (2\u03c3 log T )/\u03b32i times in round i at each of the points. By Hoeffding\u2019s inequality, this implies that we know the function value to within \u03b3i with high probability. The value \u03b3i is halved at every round so that the algorithm can stop the epoch with the minimal number of queries that suffice to resolve the difference between function values at any two of xl, xc, xr, ensuring a low regret regret in each epoch. At the end of an epoch \u03c4 , the working feasible region is reduced to a subset [l\u03c4+1, r\u03c4+1] \u2282 [l\u03c4 , r\u03c4 ] of the current region for the next epoch \u03c4 + 1, and this reduction is such that the new region is smaller in size by a constant fraction. This geometric rate of reduction guarantees that only a small number of epochs can occur before the working feasible region only contains near-optimal points.\nIn order for the algorithm to identify a sizable portion of the working feasible region containing only suboptimal points to discard, the queries in each epoch should be suitably chosen, and the convexity of f must be judiciously exploited. To this end, the algorithm makes its queries at three equally-spaced points xl < xc < xr in the working feasible region.\nCase 1: If the confidence intervals around f(xl) and f(xr) are sufficiently separated, then the algorithm can identify a subset of the feasible region (either to the left of xl or to the right of xr) that contains no near-optimal points\u2014i.e., that every point x in the subset has f(x) f(x\u2217). This subset, which is\nAlgorithm 1 One-dimensional stochastic convex bandit algorithm input noisy black-box access to f : [0, 1]\u2192 R, total number of queries allowed T . 1: Let l1 := 0 and r1 := 1. 2: for epoch \u03c4 = 1, 2, . . . do 3: Let w\u03c4 := r\u03c4 \u2212 l\u03c4 . 4: Let xl := l\u03c4 + w\u03c4/4, xc := l\u03c4 + w\u03c4/2, and xr := l\u03c4 + 3w\u03c4/4. 5: for round i = 1, 2, . . . do 6: Let \u03b3i := 2\n\u2212i. 7: For each x \u2208 {xl, xc, xr}, query f(x) 2\u03c3\u03b32i log T times. 8: if max{LB\u03b3i(xl),LB\u03b3i(xr)} \u2265 min{UB\u03b3i(xl),UB\u03b3i(xr)}+ \u03b3i then 9: {Case 1: CI\u2019s at xl and xr are \u03b3i separated}\n10: if LB\u03b3i(xl) \u2265 LB\u03b3i(xr) then let l\u03c4+1 := xl and r\u03c4+1 := r\u03c4 . 11: if LB\u03b3i(xl) < LB\u03b3i(xr) then let l\u03c4+1 := l\u03c4 and r\u03c4+1 := xr. 12: Continue to epoch \u03c4 + 1. 13: else if max{LB\u03b3i(xl),LB\u03b3i(xr)} \u2265 UB\u03b3i(xc) + \u03b3i then 14: {Case 2: CI\u2019s at xc and xl or xr are \u03b3i separated} 15: if LB\u03b3i(xl) \u2265 LB\u03b3i(xr) then let l\u03c4+1 := xl and r\u03c4+1 := r\u03c4 . 16: if LB\u03b3i(xl) < LB\u03b3i(xr) then let l\u03c4+1 := l\u03c4 and r\u03c4+1 := xr. 17: Continue to epoch \u03c4 + 1. 18: end if 19: end for 20: end for\na fourth of the working feasible region by construction is then discarded and the algorithm continues to the next epoch. This case is depicted in Figure 1.\nCase 2: If the above deduction cannot be made, the algorithm looks at the confidence interval around f(xc). If this interval is sufficiently below at least one of the other intervals (for f(xl) or f(xr)), then again the algorithm can identify a quartile that contains no near-optimal points, and this quartile can then be discarded before continuing to the next epoch. One possible arrangement of CI\u2019s for this case is shown in Figure 2.\nCase 3: Finally, if none of the earlier cases is true, then the algorithm is assured that the function is sufficiently flat on working feasible region and hence it has not incurred much regret so far. The algorithm continues the epoch, with an increased number of queries to obtain smaller confidence intervals at each of the three points. An example arrangement of CI\u2019s for this case is shown in Figure 3."}, {"heading": "4.2 Analysis", "text": "The analysis of Algorithm 1 relies on the function values being contained in the confidence intervals we construct at each round of each epoch. To avoid having probabilities throughout our analysis, we define an event E where at each epoch \u03c4 , and each round i, f(x) \u2208 [LB\u03b3i(x),UB\u03b3i(x)] for x \u2208 {xl, xc, xr}. We will carry out the remainder of the analysis conditioned on E and bound the probability of Ec at the end.\nThe following theorem bounds the regret incurred by Algorithm 1. We note that the regret would be maintained in terms of the points xt queried by the algorithm at time t. Within any given round, the order of queries is immaterial to the regret.\nTheorem 1 (Regret bound for Algorithm 1). Suppose Algorithm 1 is run on a convex, 1-Lipschitz function f bounded in [0,1]. Suppose the noise in observations is i.i.d. and \u03c3-subgaussian. Then with probability at least 1\u2212 1/T we have\nT\u2211 t=1 f(xt)\u2212 f(x\u2217) \u2264 108 \u221a \u03c3T log T log4/3 ( T 8\u03c3 log T ) .\nRemarks: As stated Algorithm 1 and Theorem 1 assume knowledge of T , but we can make the algorithm adaptive to T by a standard doubling argument. We remark that O( \u221a T ) is the smallest possible regret for any algorithm even with noisy gradient information. Hence, this result shows that for purposes of regret, noisy zeroth order information is no worse than noisy first-order information apart from logarithmic factors. We also observe that at the end of the procedure, the mid-point xc of the working feasible region [l\u03c4 , r\u03c4 ] where \u03c4 was the last epoch, has an optimization error of at most O\u0303(1/ \u221a T ). This is unlike noisy first-order methods where all the iterates have to be averaged in order to get a point with low optimization error. The theorem is proved via a series of lemmas in the next few sections. The key idea is to show that the regret on any epoch is small and the total number of epochs is bounded. To bound the per-epoch regret, we will show that the total number of queries made on any epoch depends on how close to flat the function is on the working feasible region. Thus we either take a long time, but the function is very flat, or we stop early when the function has sufficient slope, never accruing too much regret."}, {"heading": "4.2.1 Bounding the regret in one epoch", "text": "We start by showing that each reduction in the working feasible region after each epoch never discards near-optimal points.\nLemma 1. If epoch \u03c4 ends in round i, then the interval [l\u03c4+1, r\u03c4+1] contains every x \u2208 [l\u03c4 , r\u03c4 ] such that f(x) \u2264 f(x\u2217) + \u03b3i. In particular, x\u2217 \u2208 [l\u03c4 , r\u03c4 ] for all epochs \u03c4 .\nProof. Suppose epoch \u03c4 terminates in round i via case 1. This means that either LB\u03b3i(xl) \u2265 UB\u03b3i(xr) + \u03b3i or LB\u03b3i(xr) \u2265 UB\u03b3i(xl) + \u03b3i. Consider the former case (the argument for the latter is analogous). This implies\nf(xl) \u2265 f(xr) + \u03b3i. (1) We need to show that every x \u2208 [l\u03c4 , l\u03c4+1] = [l\u03c4 , xl] has f(x) \u2265 f(x\u2217) + \u03b3i. So pick x \u2208 [l\u03c4 , xl] so that xl \u2208 [x, xr]. Then xl = tx+ (1\u2212 t)xr for some 0 \u2264 t \u2264 1, so by convexity,\nf(xl) \u2264 tf(x) + (1\u2212 t)f(xr),\nwhich in turn implies\nf(x) \u2265 f(xr) + f(xl)\u2212 f(xr)\nt\n\u2265 f(xr) + \u03b3i t\nusing Equation 1\n\u2265 f(x\u2217) + \u03b3i since t \u2264 1\nas required. Now suppose epoch \u03c4 terminates in round i via case 2. This means\nmax{LB\u03b3i(xl),LB\u03b3i(xr)} \u2265 UB\u03b3i(xc) + \u03b3i.\nSuppose LB\u03b3i(xl) \u2265 LB\u03b3i(xr) (the argument for the case LB\u03b3i(xl) < LB\u03b3i(xr) is analogous). The above inequality implies f(xl) \u2265 f(xc) + \u03b3i. We need to show that every x \u2208 [l\u03c4 , l\u03c4+1] = [l\u03c4 , xl] has f(x) \u2265 f(x\u2217) + \u03b3i. But the same argument as given in case 1, with xr replaced with xc, gives the required claim.\nThe fact that x\u2217 \u2208 [l\u03c4 , r\u03c4 ] for all epochs \u03c4 follows by induction.\nThe next two lemmas bound the regret incurred in any single epoch. To show this, we first establish that an algorithm incurs low regret in a round as long as it does not end an epoch. Then, as a consequence of the doubling trick, we show that the regret incurred in an epoch is on the same order as that incurred in the last round of the epoch.\nLemma 2 (Certificate of low regret). If epoch \u03c4 continues from round i to round i + 1, then the regret incurred in round i is at most\n72\u03c3 log T\n\u03b3i .\nRemark 1. A more detailed argument shows that the regret incurred is, in fact, at most 54\u03c3 log T/\u03b3i.\nProof. The regret incurred in round i of epoch \u03c4 is\n2\u03c3 log T \u03b32i \u00b7 ( (f(xl)\u2212 f(x\u2217)) + (f(xc)\u2212 f(x\u2217)) + (f(xr)\u2212 f(x\u2217)) )\nso it suffices to show that f(x) \u2264 f(x\u2217) + 12\u03b3i\nfor each x \u2208 {xl, xc, xr}. The algorithm continues from round i to round i+ 1 iff\nmax{LB\u03b3i(xl),LB\u03b3i(xr)} < min{UB\u03b3i(xl),UB\u03b3i(xr)}+ \u03b3i\nand max{LB\u03b3i(xl),LB\u03b3i(xr)} < UB\u03b3i(xc) + \u03b3i.\nThis implies that f(xl), f(xc), and f(xr) are contained in an interval of width at most 3\u03b3i (recall Figure 3). By Lemma 1, we have x\u2217 \u2208 [l\u03c4 , r\u03c4 ]. Assume x\u2217 \u2264 xc (the case x\u2217 > xc is analogous). There exists t \u2265 0 such that x\u2217 = xc + t(xc \u2212 xr), so xc = 1\n1 + t x\u2217 +\nt\n1 + t xr.\nNote that t \u2264 2 because |xc \u2212 l\u03c4 | = w\u03c4/2 and |xr \u2212 xc| = w\u03c4/4, so\nt = |x\u2217 \u2212 xc| |xr \u2212 xc| \u2264 |l\u03c4 \u2212 xc||xr \u2212 xc| = w\u03c4/2 w\u03c4/4 = 2.\nBy convexity,\nf(xc) \u2264 1 1 + t f(x\u2217) + t 1 + t f(xr)\nso\nf(x\u2217) \u2265 (1 + t) ( f(xc)\u2212 t\n1 + t f(xr) ) = f(xr) + (1 + t) (f(xc)\u2212 f(xr)) \u2265 f(xr)\u2212 (1 + t)|f(xc)\u2212 f(xr)| \u2265 f(xr)\u2212 (1 + t) \u00b7 3\u03b3i \u2265 f(xr)\u2212 9\u03b3i.\nWe conclude that for each x \u2208 {xl, xc, xr},\nf(x) \u2264 f(xr) + 3\u03b3i \u2264 f(x\u2217) + 12\u03b3i.\nLemma 3 (Regret in an epoch). If epoch \u03c4 ends in round i, then the regret incurred in the entire epoch is\n216\u03c3 log T\n\u03b3i .\nProof. If i = 1, then f(x) \u2212 f(x\u2217) \u2264 |x \u2212 x\u2217| \u2264 1 for each x \u2208 {xl, xc, xr} because f is 1-Lipschitz and |x\u2212 x\u2032| \u2264 1 for any x, x\u2032 \u2208 [0, 1]. Therefore, the regret incurred in epoch \u03c4 is\n2\u03c3 log T \u03b321 \u00b7 ( (f(xl)\u2212 f(x\u2217)) + (f(xc)\u2212 f(x\u2217)) + (f(xr)\u2212 f(x\u2217)) ) \u2264 12\u03c3 log T \u03b31 .\nNow assume i \u2265 2. Lemma 2 implies that the regret incurred in round j, for 1 \u2264 j \u2264 i\u2212 1, is at most 72\u03c3 log T\n\u03b3j .\nFurthermore, for round i, we still know that the regret on each query in round i is bounded by 36\u03b3i\u22121 (12\u03b3i\u22121 for each of xl, xc, xr). Recalling that \u03b3i\u22121 = 2\u03b3i and that we make (\u03c3 log T )/\u03b32i queries at round i, the regret incurred in round i (the final round of epoch \u03c4) is at most\n36\u03b3i\u22121 2\u03c3 log T\n\u03b32i =\n144\u03c3 log T\n\u03b3i .\nTherefore, the overall regret incurred in epoch \u03c4 is\ni\u22121\u2211 j=1 72\u03c3 log T \u03b3j + 144\u03c3 log T \u03b3i = i\u22121\u2211 j=1 72\u03c3 log T \u00b7 2j + 144\u03c3 log T \u03b3i < 72\u03c3 log T \u00b7 2i + 144\u03c3 log T \u03b3i = 216\u03c3 log T \u03b3i ."}, {"heading": "4.2.2 Bounding the number of epochs", "text": "To establish the final bound on the overall regret, we bound the number of epochs that can occur before the working feasible region only contains near-optimal points. The final regret bound is simply the product of the number of epochs and the regret incurred in any single epoch.\nLemma 4 (Bound on the number of epochs). The total number of epochs \u03c4 performed by Algorithm 1 is at bounded as\n\u03c4 \u2264 1 2 log4/3\n( T\n8\u03c3 log T\n) .\nProof. The proof is based on observing that \u03b3i \u2265 (T/2\u03c3 log T )\u22121/2 at all epochs and rounds. Indeed if \u03b3i \u2264 (T/2\u03c3 log T )\u22121/2, step 7 of the algorithm would require more than T queries to get the desired confidence intervals in that round. Hence we set \u03b3min = (T/2\u03c3 log T )\n\u22121/2 and define the interval I := [x\u2217\u2212\u03b3min, x\u2217+\u03b3min] which has width 2\u03b3min. For any x \u2208 I,\nf(x)\u2212 f(x\u2217) \u2264 |x\u2212 x\u2217| \u2264 \u03b3min because f is 1-Lipschitz. Moreover, for any epoch \u03c4 \u2032 which ends in round i\u2032, \u03b3min \u2264 \u03b3i\u2032 by definition and therefore by Lemma 1, I \u2286 {x \u2208 [0, 1] : f(x) \u2264 f(x\u2217) + \u03b3i\u2032} \u2286 [l\u03c4 \u2032+1, r\u03c4 \u2032+1]. This implies that 2\u03b3min \u2264 r\u03c4+1 \u2212 l\u03c4+1 = w\u03c4+1. Furthermore, by the definitions of l\u03c4 \u2032+1, r\u03c4 \u2032+1, and w\u03c4 \u2032+1 in the algorithm, it follows that\nw\u03c4 \u2032+1 \u2264 3\n4 \u00b7 w\u03c4 \u2032\nfor any \u03c4 \u2032 \u2208 {1, . . . , \u03c4}. Therefore, we conclude that\n2\u03b3min \u2264 w\u03c4+1 \u2264 ( 3\n4\n)\u03c4 \u00b7 w1 = ( 3\n4 )\u03c4 which gives the claim after rearranging the inequality."}, {"heading": "4.2.3 Proof of Theorem 1", "text": "The statement of the theorem follows by combining the per-epoch regret bound of Lemma 3 with the above bound on the number of epochs, and showing that all these bounds hold with sufficiently high probability.\nLemma 3 implies that the regret incurred in any epoch \u03c4 \u2032 \u2264 \u03c4 that ends in round i\u2032 is at most\n216\u03c3 log T \u03b3i\u2032 \u2264 216\u03c3 log T \u03b3min \u2264 216\n\u221a T\u03c3 log T .\nSo the overall regret incurred in all \u03c4 epochs is at most\n216 \u221a T\u03c3 log T \u00b7 1\n2 log4/3\n( T\n8\u03c3 log T\n) .\nFinally we recall that the entire analysis thus far has been conditioned on the event E where all the confidence intervals we construct do contain the function values. We would now like to control the probability P(Ec). Consider a fixed round and a fixed point x. Then after making 2\u03c3 log T/\u03b32i queries, Hoeffding\u2019s inequality gives that\nP ( |f(x)\u2212 f\u0302(x)| \u2265 \u03b3i ) \u2264 1 T 2 ,\nwhere f\u0302(x) is the average of the observed function values. Once we have a bound for a fixed round of a fixed epoch, we would like to bound this probability uniformly over all rounds played across all epochs. We note that we make at most T queries, which is also an upper bound on the total number of rounds. Hence union bound gives\nP(Ec) \u2264 1 T ,\nwhich completes the proof of the theorem."}, {"heading": "5 Algorithm for optimization in higher dimensions", "text": "We now move to present the general algorithm that works in d-dimensions. The natural approach would be to try and generalize Algorithm 1 to work in multiple dimensions. However, the obvious extension requires constructing a covering of the unit sphere and querying the function along every direction in the covering so that we know the behavior of the function along every direction. While such an approach yields regret that scales as \u221a T , the dependence on dimension d is exponential both in regret and the running time. The same problem was encountered in the scenario of zeroth order optimization by Nemirovski and Yudin [15], and they use a clever construction to capture all the directions in polynomially many queries. We define a pyramid to be a d-dimensional polyhedron defined by d + 1 points; d points form a d-dimensional regular polygon that is the base of the pyramid, and the apex lies above the hyperplane containing the base (see Figure 4 for a graphical illustration in 3 dimensions). The idea of Nemirovski and Yudin was to build a sequence of pyramids, each capturing the variation of function in certain directions, in such a way that in O(d log d) pyramids we can explore all the directions. However, as mentioned earlier, their approach fails to give a low regret. We combine their geometric construction with ideas from the one-dimensional case to obtain a low-regret algorithm as described in Algorithm 2 below. Concretely, we combine the geometrical construction of Nemirovski and Yudin [15] with the center-point device to show low regret.\nJust like the 1-dimensional case, Algorithm 2 proceeds in epochs. We start with the optimization domain X , and at the beginning we set X0 = X . At the beginning of epoch \u03c4 , we have a current feasible set X\u03c4 which contains an approximate optimum of the convex function. The epoch ends with discarding some portion of the set X\u03c4 in such a way that we still retain at least one approximate optimum in the remaining set X\u03c4+1.\nAt the start of the epoch \u03c4 , we apply an affine transformation to X\u03c4 so that the smallest volume ellipsoid containing it is a Euclidean ball of radius R\u03c4 (denoted as B(R\u03c4 )). We define r\u03c4 = R\u03c4/c1d for a constant c1 \u2265 1, so that B(r\u03c4 ) \u2286 X\u03c4 (such a construction is always possible, see, e.g., Lecture 1, p. 2 [4]). We will use the notation B\u03c4 to refer to the enclosing ball. Within each epoch, the algorithm proceeds in several rounds, each round maintaining a value \u03b3i which is successively halved.\nLet x0 be the center of the ball B(R\u03c4 ) containing X\u03c4 . At the start of a round i, we construct a regular simplex centered at x0 and contained in B(r\u03c4 ). The algorithm queries the function f at all the vertices of the simplex, denoted by x1. . . . , xd+1, until the CI\u2019s at each vertex shrink to \u03b3i. The algorithm then picks the\nAlgorithm 2 Stochastic convex bandit algorithm input feasible region X \u2282 Rd; noisy black-box access to f : X \u2192 R, constants c1 and c2, functions \u2206\u03c4 (\u03b3), \u2206\u0304\u03c4 (\u03b3) and number of queries T allowed.\n1: Let X1 := X . 2: for epoch \u03c4 = 1, 2, . . . do 3: Round X\u03c4 so B(r\u03c4 ) \u2286 X\u03c4 \u2286 B(R\u03c4 ), R\u03c4 is minimized, and r\u03c4 := R\u03c4/(c1d). Let B\u03c4 = B(R\u03c4 ). 4: Construct regular simplex with vertices x1, . . . , xd+1 on the surface of B(r\u03c4 ). 5: for round i = 1, 2, . . . do 6: Let \u03b3i := 2\n\u2212i. 7: Query f at xj for each j = 1, . . . , d+ 1\n2\u03c3 log T \u03b32i times.\n8: Let y1 := arg maxxj LB\u03b3i(xj). 9: for pyramid k = 1, 2, . . . do\n10: Construct pyramid \u03a0k with apex yk; let z1, . . . , zd be the vertices of the base of \u03a0k and z0 be the center of \u03a0k. 11: Let \u03b3\u0302 := 2\u22121. 12: loop 13: Query f at each of {yk, z0, z1, . . . , zd} 2\u03c3 log T\u03b3\u03022 times. 14: Let center := z0, apex := yk, top be the vertex v of \u03a0k maximizing LB\u03b3\u0302(v), bottom be the vertex v of \u03a0k minimizing LB\u03b3\u0302(v). 15: if LB\u03b3\u0302(top) \u2265 UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302) and LB\u03b3\u0302(top) \u2265 UB\u03b3\u0302(apex) + \u03b3\u0302 then 16: {Case 1(a)} 17: Let yk+1 := top, and immediately continue to pyramid k + 1. 18: else if LB\u03b3\u0302(top) \u2265 UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302) and LB\u03b3\u0302(top) < UB\u03b3\u0302(apex) + \u03b3\u0302 then 19: {Case 1(b)} 20: Set (X\u03c4+1,B \u2032 \u03c4+1) = Cone-cutting(\u03a0k,X\u03c4 ,B\u03c4 ), and proceed to epoch \u03c4 + 1. 21: else if LB\u03b3\u0302(top) < UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302) and UB\u03b3\u0302(center) \u2265 LB\u03b3\u0302(bottom) \u2212 \u2206\u0304\u03c4 (\u03b3\u0302) then 22: {Case 2(a)} 23: Let \u03b3\u0302 := \u03b3\u0302/2. 24: if \u03b3\u0302 < \u03b3i then start next round i+ 1. 25: else if LB\u03b3\u0302(top) < UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302) and UB\u03b3\u0302(center) < LB\u03b3\u0302(bottom) \u2212 \u2206\u0304\u03c4 (\u03b3\u0302) then 26: {Case 2(b)} 27: Set (X\u03c4+1,B \u2032 \u03c4+1)= Hat-raising(\u03a0k,X\u03c4 ,B\u03c4 ), and proceed to epoch \u03c4 + 1. 28: end if 29: end loop 30: end for 31: end for 32: end for\nAlgorithm 3 Cone-cutting input pyramid \u03a0 with apex y, (rounded) feasible region X\u03c4 for epoch \u03c4 , enclosing ball B\u03c4 1: Let z1, . . . , zd be the vertices of the base of \u03a0, and \u03d5\u0304 the angle at its apex. 2: Define the cone\nK\u03c4 = {x | \u2203\u03bb > 0, \u03b11, . . . , \u03b1d > 0, d\u2211 i=1 \u03b1i = 1 : x = y \u2212 \u03bb d\u2211 i=1 \u03b1i(zi \u2212 y)}\n3: Set B\u2032\u03c4+1 to be the min. volume ellipsoid containing B\u03c4 \\ K\u03c4 . 4: Set X\u03c4+1 = X\u03c4 \u2229 B \u2032 \u03c4+1.\noutput new feasible region X\u03c4+1 and enclosing ellipsoid B \u2032 \u03c4+1.\nAlgorithm 4 Hat-raising input pyramid \u03a0 with apex y, (rounded) feasible region X\u03c4 for epoch \u03c4 , enclosing ball B\u03c4 . 1: Let center be the center of \u03a0. 2: Set y\u2032 = y + (y \u2212 center). 3: Set \u03a0 \u2032 to be the pyramid with apex y\u2032 and same base as \u03a0.\n4: Set (X\u03c4+1,B \u2032 \u03c4+1) = Cone-cutting(\u03a0 \u2032 ,X\u03c4 ,B\u03c4 ).\noutput new feasible region X\u03c4+1 and enclosing ellipsoid B \u2032 \u03c4+1.\npoint y1 for which the average of observed function values is the largest. By construction, we are guaranteed that f(y1) \u2265 f(xj)\u2212 \u03b3i for all j = 1, . . . , d+ 1. This step is depicted in Figure 5.\nThe algorithm now successively constructs a sequence of pyramids, with the goal of identifying a region of the feasible set X\u03c4 such that at least one approximate optimum of f lies outside the selected region. This region will be discarded at the end of the epoch. The construction of the pyramids follows the construction from Section 9.2.2 of the book [15]. The pyramids we construct will have an angle 2\u03d5 at the apex, where cos\u03d5 = c2/d. The base of the pyramid consists of vertices z1, . . . , zd such that zi \u2212 x0 and y1 \u2212 zi are orthogonal. We note that the construction of such a pyramid is always possible\u2014we take a sphere with y1 \u2212 x0 as the diameter, and arrange z1, . . . , zd on the boundary of the sphere such that the angle between y1 \u2212 x0 and y1 \u2212 zi is \u03d5. The construction of the pyramid is depicted in Figure 6. Given this pyramid, we set \u03b3\u0302 = 1, and sample the function at y1 and z1, . . . , zd as well as the center of the pyramid until the CI\u2019s all shrink to \u03b3\u0302. Let top and bottom denote the vertices of the pyramid (including y1) with the largest and\nthe smallest function value estimates resp. For consistency, we will also use apex to denote the apex y1. We then check for one of the following conditions:\n1. If LB\u03b3\u0302(top) \u2265 UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302), we proceed based on the separation between top and apex CI\u2019s as illustrated in Figures 7(a) and 7(b).\n(a) If LB\u03b3\u0302(top) \u2265 UB\u03b3\u0302(apex) + \u03b3\u0302, then we know that with high probability\nf(top) \u2265 f(apex) + \u03b3\u0302 \u2265 f(apex) + \u03b3i. (2) In this case, we set top to be the apex of the next pyramid, reset \u03b3\u0302 = 1 and continue the sampling procedure on the next pyramid.\n(b) If LB\u03b3\u0302(top) \u2264 UB\u03b3\u0302(apex) + \u03b3\u0302, then we know that LB\u03b3\u0302(apex) \u2265 UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302. In this case, we declare the epoch over and pass the current apex to the cone-cutting step.\n2. If LB\u03b3\u0302(top) \u2264 UB\u03b3\u0302(bottom) + \u2206\u03c4 (\u03b3\u0302), then one of the two events depicted in Figures 2(a) or 2(b) has to happen:\n(a) If UB\u03b3\u0302(center) \u2265 LB\u03b3\u0302(bottom)\u2212 \u2206\u0304\u03c4 (\u03b3\u0302), then all of the vertices and the center of the pyramid have their function values within a 2\u2206\u03c4 (\u03b3\u0302) + 3\u03b3\u0302 interval. In this case, we set \u03b3\u0302 = \u03b3\u0302/2. If this sets \u03b3\u0302 < \u03b3i, we start the next round with \u03b3i+1 = \u03b3i/2. Otherwise, we continue sampling the current pyramid with the new value of \u03b3\u0302.\n(b) If UB\u03b3\u0302(center) \u2264 LB\u03b3\u0302(bottom)\u2212 \u2206\u0304\u03c4 (\u03b3\u0302), then we terminate the epoch and pass the center and the current apex to the hat-raising step.\nHat-Raising: This step happens when we construct a pyramid where LB\u03b3\u0302(top) \u2264 UB\u03b3\u0302(bottom)+\u2206\u03c4 (\u03b3\u0302) but UB\u03b3\u0302(center) \u2264 LB\u03b3\u0302(bottom) \u2212 \u2206\u0304\u03c4 (\u03b3\u0302) (see Fig. 2(b) for an illustration). In this case, we will show that if we move the apex of the pyramid a little from yi to y \u2032 i, then y \u2032 i\u2019s CI is above the top CI while the angle of the new pyramid at y \u2032 i is not much smaller than 2\u03d5. In particular, letting centeri denote the center of the pyramid, we set y \u2032 i = yi + (yi \u2212 centeri). Figure 9 shows transformation of the pyramid involved in this step. The correctness of this step and the sufficiency of the perturbation from y to y \u2032\nwill be proved in the next section.\nCone-cutting: This step is the concluding step for an epoch. The algorithm gets to this step either through case 1(b) or through the hat-raising step. In either case, we have a pyramid with an apex y, base z1, . . . , zd and an angle 2\u03d5\u0304 at the apex, where cos(\u03d5\u0304) \u2264 1/2d. We now define a cone\nK\u03c4 = {x | \u2203\u03bb > 0, \u03b11, . . . , \u03b1d > 0, d\u2211 i=1 \u03b1i = 1 : x = y \u2212 \u03bb d\u2211 i=1 \u03b1i(zi \u2212 y)} (3)\nwhich is centered at y and a reflection of the pyramid around the apex. By construction, the cone K\u03c4 has an angle 2\u03d5\u0304 at its apex. We set B\u2032\u03c4+1 to be the ellipsoid of minimum volume containing B\u03c4 \\ K\u03c4 and define X\u03c4+1 = X\u03c4 \u2229 B \u2032 \u03c4+1. This is illustrated in Figure 10. Finally, we put things back into an isotropic position and B\u03c4+1 is the ball containing X\u03c4+1 is in the isotropic coordinates, which is just obtained by applying an affine transformation to B\u2032\u03c4+1.\nLet us end the description with a brief discussion regarding the computational aspects of this algorithm. It is clear that the most computationally intensive steps of this algorithm are the cone-cutting and isotropic\ntransformation at the end. However, these steps are exactly analogous to an implementation of the classical ellipsoid method. In particular, the equation for B\u2032\u03c4+1 is known in closed form [11]. Furthermore, the affine transformations needed to the reshape the set can be computed via rank-one matrix updates and hence computation of inverses can be done efficiently as well (see e.g. [11] for the relevant implementation details of the ellipsoid method)."}, {"heading": "6 Analysis", "text": "We start by showing the correctness of the algorithm and then proceed to regret analysis. To avoid having probabilities throughout our analysis, we define an event E where at each epoch \u03c4 , and each round i, f(x) \u2208 [LB\u03b3i(x),UB\u03b3i(x)] for any point x sampled in the round. We will carry out the remainder of the analysis conditioned on E and bound the probability of Ec at the end. We also assume that the algorithm is run with the settings\n\u2206\u03c4 (\u03b3) =\n( 6c1d 4\nc22 + 3\n) \u03b3 and \u2206\u0304\u03c4 (\u03b3) = ( 6c1d 4\nc22 + 5\n) \u03b3, (4)\nand constants c1 \u2265 64, c2 \u2264 32."}, {"heading": "6.1 Correctness of the algorithm", "text": "In order to complete the proof of our algorithm\u2019s correctness, we only need to further show that when the algorithm proceeds to cone-cutting via case 1(b), then it does not discard all the approximate optima of f by mistake, and show that the hat-raising step is indeed correct as claimed. These two claims are established in the next couple of lemmas.\nFor these two lemmas, we assume that the distance of the apex of any \u03a0 constructed in epoch \u03c4 from the center of B(r\u03c4 ) is at least r\u03c4/d. This assumption will be established later.\nLemma 5. Let K\u03c4 be the cone discarded at epoch \u03c4 which is ended through Case (1b) in round i. Let bottom be the lowest CI of the last pyramid \u03a0 constructed in the epoch, and assume the distance from the apex of \u03a0 to the center of B(r\u03c4 ) is at least r\u03c4/d. Then f(x) \u2265 f(bottom) + \u03b3i for all x \u2208 K\u03c4 .\nProof. Consider any x \u2208 K\u03c4 . By construction, there is a point z in the base of the pyramid \u03a0 such that the apex y of \u03a0 satisfies y = \u03b1z + (1\u2212 \u03b1)x for some \u03b1 \u2208 [0, 1) (see Fig. 11 for a graphical illustration).\nSince f is convex and z is in the base of the pyramid, we have that\nf(z) \u2264 f(top) \u2264 f(y) + 3\u03b3\u0302\n. Also, the condition of Case (1b) ensures\nf(y) > f(bottom) + \u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302\nwhere \u03b3\u0302 is the CI level used for the pyramid. Then by convexity of f\nf(y) \u2264 \u03b1f(z) + (1\u2212 \u03b1)f(x) \u2264 \u03b1(f(y) + 3\u03b3\u0302) + (1\u2212 \u03b1)f(x).\nSimplifying yields\nf(x) \u2265 f(y)\u2212 3 \u03b1 1\u2212 \u03b1\u03b3\u0302 > f(bottom) + \u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302 \u2212 3 \u03b1 1\u2212 \u03b1\u03b3\u0302.\nAlso, we know that \u03b1/(1\u2212 \u03b1) = \u2016y \u2212 x\u2016/\u2016y \u2212 z\u2016. Because x \u2208 B(R\u03c4 ), \u2016y \u2212 x\u2016 \u2264 2R\u03c4 \u2264 2c1dr\u03c4 . Moreover, \u2016y \u2212 z\u2016 is at least the height of \u03a0, which is at least r\u03c4 c22/d3 by Lemma 15. Therefore\n\u03b1 1\u2212 \u03b1 = \u2016y \u2212 x\u2016 \u2016y \u2212 z\u2016 \u2264 2c1dr\u03c4 r\u03c4 c22/d 3 \u2264 2c1d 4 c22 .\nThus, we have\nf(x) > f(bottom) + \u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302 \u2212 6c1d\n4\nc22 \u03b3\u0302 \u2265 f(bottom) + \u03b3i, (5)\nwhere the last line uses the setting of \u2206\u03c4 (\u03b3\u0302) (4), completing the proof of the lemma.\nThis lemma guarantees that we cannot discard all the approximate minima of f by mistake in case 1(b), and that any point discarded by the algorithm through this step in round i has regret at least \u03b3i. The final check that needs to be done is the correctness of the hat-raising step which we do in the next lemma.\nLemma 6. Let \u03a0\u2032 be the new pyramid formed in hat-raising with apex y\u2032 and same base as \u03a0 in round i of epoch \u03c4 , and let K\u2032\u03c4 be the cone discarded. Assume the distance from the apex of \u03a0 to the center of B(r\u03c4 ) is at least r\u03c4/d. Then the \u03a0\n\u2032 has an angle \u03d5\u0304 at the apex with cos \u03d5\u0304 \u2264 2c2/d, height at most 2r\u03c4 c21/d2, and with every point x in the cone K\u2032\u03c4 having f(x) \u2265 f(x\u2217) + \u03b3i.\nProof. Let y\u2032 := y + (y \u2212 center) be the apex of \u03a0\u2032. Let h be the height of \u03a0 (the distance from y to the base), h\u2032 be the height of \u03a0\u2032, and b be the distance from any vertex of the base to the center of the base. Then h\u2032 < 2h \u2264 2r\u03c4 c21/d2 by Lemma 15. Moreover, since cos(\u03d5) = h/ \u221a h2 + b2 = 1/d, we have\ncos(\u03d5\u0304) = h\u2032/ \u221a h\u20322 + b2 \u2264 2h/ \u221a h2 + b2 = 2 cos(\u03d5) = 2c2/d.\nIt remains to show that every x \u2208 K\u2032\u03c4 has f(x) \u2265 f(x\u2217) + \u03b3\u0302. By convexity of f , f(y) \u2264 (f(y\u2032) + f(center))/2, so f(y\u2032) \u2265 2f(y)\u2212 f(center). Since we enter hat-raising via case 2(b) of the algorithm, we know that f(center) \u2264 f(y)\u2212 \u2206\u0304\u03c4 (\u03b3\u0302), so\nf(y\u2032) \u2265 f(y) + \u2206\u0304\u03c4 (\u03b3\u0302). The condition for entering case 2(b) also implies that f(y) > f(top)\u2212\u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302 > f(x)\u2212\u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302 for all x \u2208 \u03a0, and therefore for any z on the base of \u03a0,\nf(y\u2032) > f(z) + \u2206\u0304\u03c4 (\u03b3\u0302)\u2212\u2206\u03c4 (\u03b3\u0302)\u2212 2\u03b3\u0302 \u2265 f(z), where the last line uses the settings of \u2206\u03c4 (\u03b3\u0302) and \u2206\u0304\u03c4 (\u03b3\u0302) (4). Now take any x \u2208 K\u2032\u03c4 . There exists \u03b1 \u2208 [0, 1) and z on the base of \u03a0\u2032 such that y\u2032 = \u03b1z + (1\u2212 \u03b1)x, so by convexity of f , f(y\u2032) \u2264 \u03b1f(z) + (1\u2212 \u03b1)f(x) \u2264 \u03b1f(y\u2032) + (1\u2212 \u03b1)f(x), which implies f(x) \u2265 f(y\u2032) \u2265 f(y) + \u2206\u0304\u03c4 (\u03b3\u0302) \u2265 f(x\u2217) + \u03b3i."}, {"heading": "6.2 Regret analysis", "text": "The following theorem states our regret guarantee on the performance of the algorithm 2.\nTheorem 2. Suppose Algorithm 2 is run with c1 \u2265 64, c2 \u2264 1/32 and parameters\n\u2206\u03c4 (\u03b3) =\n( 6c1d 4\nc22 + 3\n) \u03b3 and \u2206\u0304\u03c4 (\u03b3) = ( 6c1d 4\nc22 + 5\n) \u03b3.\nThen with probability at least 1\u2212 1/T , the net regret incurred by the algorithm is bounded by\n768d3\u03c3 \u221a T log2 T\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 1) c2 )( 4c1d 4 c22 + 11 ) .\nRemarks: The prior knowledge of T in Algorithm 2 and Theorem 2 can again be addressed using a doubling argument. As earlier, Theorem 2 is optimal in the dependence on T . The large dependence on d is also seen in Nemirovski and Yudin [15] who obtain a d7 scaling in noiseless case and leave it an unspecified polynomial in the noisy case. Using random walk ideas [5] to improve the dependence on d is an interesting question for future research.\nThe analysis will start by controlling the regret incurred on different rounds, and then we will piece it together across rounds and epochs to get the net regret for the entire procedure."}, {"heading": "6.2.1 Bounding the regret incurred in one round", "text": "We will start by a simple lemma regarding the regret incurred while playing a pyramid if the condition 2(a) is encountered in the algorithm. This lemma highlights the importance of evaluating the function at the center of the pyramid, a step that was not needed in the framework of Nemirovski and Yudin [15]. We will use the symbol \u03a0 to refer to a generic pyramid constructed by the algorithm during the course of its operation, with apex y, base z1, . . . , zd, center center and with an angle \u03d5 at the apex. We also recall that the pyramids constructed by the algorithm are such that the distance from the center to the base is at least r\u03c4 c 2 2/d 3.\nLemma 7. Suppose the algorithm reaches case 2(a) in round i of epoch \u03c4 , and assume x\u2217 \u2208 B(R\u03c4 ) where x\u2217 is the minimizer of f . Let \u03a0 be the current pyramid and \u03b3\u0302 be the current CI width. Assume the distance from the apex of \u03a0 to the center of B(r\u03c4 ) is at least r\u03c4/d. Then the net regret incurred while evaluating the function on \u03a0 in round i is at most\n6d\u03c3 log T\n\u03b3\u0302\n( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) .\nProof. The proof is a consequence of convexity. We start by bounding the variation of the function inside the pyramid. Since the pyramid is a convex hull of its vertices, we know that the function value at any point in the pyramid is also upper bounded by the largest function value achieved at any vertex. Furthermore, the condition for reaching Case (2a) implies that the function value at any vertex is at most f(center) + \u2206\u03c4 (\u03b3\u0302) + \u2206\u0304\u03c4 (\u03b3\u0302) + 3\u03b3\u0302, and therefore\nf(x) \u2264 f(center) + \u2206\u03c4 (\u03b3\u0302) + \u2206\u0304\u03c4 (\u03b3\u0302) + 3\u03b3\u0302 for all x \u2208 \u03a0. (6)\nFor brevity, we use the shorthand \u03b4 := \u2206\u03c4 (\u03b3\u0302)+\u2206\u0304\u03c4 (\u03b3\u0302)+3\u03b3\u0302. Consider any point x \u2208 \u03a0, and let b be the point where the ray center\u2212x intersects a face of \u03a0 on the other side. Then we know that there is a positive constant \u03b1 \u2208 [0, 1] such that center = \u03b1x+(1\u2212\u03b1)b; in particular, (1\u2212\u03b1)/\u03b1 = \u2016center\u2212x\u2016/\u2016center\u2212b\u2016. Note that \u2016center\u2212x\u2016 is at most the distance from center to a vertex of \u03a0, and \u2016center\u2212b\u2016 is at least the radius of the largest ball centered at center inscribed in \u03a0. Therefore by Lemma 16(b),\n1\u2212 \u03b1 \u03b1 = \u2016center\u2212x\u2016 \u2016center\u2212b\u2016 \u2264 d(d+ 1) c2 .\nThen the convexity of f and the upper bound on function values over \u03a0 from (6) guarantee that\nf(center) \u2264 \u03b1f(x) + (1\u2212 \u03b1)f(b) \u2264 \u03b1f(x) + (1\u2212 \u03b1)(f(center) + \u03b4).\nRearranging, we get\nf(x) \u2265 f(center)\u2212 d(d+ 1)\u03b4 c2 . (7)\nCombining equations (6) and (7) we have shown that for any x, x\u2032 \u2208 \u03a0\n|f(x)\u2212 f(x\u2032)| \u2264 d(d+ 2)\u03b4 c2 . (8)\nNow we will bootstrap to show that the above bound implies low regret while sampling the vertices and center of \u03a0. We first note that if x\u2217 \u2208 \u03a0, then the regret on any vertex or the center is bounded by d(d+ 2)\u03b4/c2. In that case, the regret incurred by sampling the vertices and center of this pyramid (so d+ 2 points) is bounded by (d + 2) \u00b7 d(d + 2)\u03b4/c2. Furthermore, we only need to sample each point pyramid 2\u03c3 log T/\u03b3\u03022 times to get the CI\u2019s of width \u03b3\u0302, which completes the proof in this case, so the total regret incurred is\n(d+ 2) d(d+ 2)\u03b4 c2 \u00b7 2\u03c3 log T \u03b3\u03022 .\nNow we consider the case where x\u2217 /\u2208 \u03a0. Recall that Lemma 5 guarantees that x\u2217 \u2208 B\u03c4 . There is a point b on a face of \u03a0 such that b = \u03b1x\u2217+(1\u2212\u03b1)center for some \u03b1 \u2208 [0, 1]. Then \u03b1 = \u2016center\u2212b\u2016/\u2016center\u2212x\u2217\u2016. By the triangle inequality, \u2016center\u2212x\u2217\u2016 \u2264 2R\u03c4 = 2c1dr\u03c4 . Moreover, \u2016center\u2212b\u2016 is at least the radius of the largest ball centered at center inscribed in \u03a0, which is at least r\u03c4 c 2 2/(2d\n4) by Lemma 16. Therefore \u03b1 \u2265 c22/(4c1d5). By convexity and Equation (7),\nf(center)\u2212 d(d+ 2)\u03b4 c2 \u2264 f(b) \u2264 \u03b1f(x\u2217) + (1\u2212 \u03b1)f(center),\nso\nf(x\u2217) \u2265 f(center)\u2212 d(d+ 2)\u03b4 c2\u03b1 \u2265 f(center)\u2212 4d 7c1\u03b4 c32 \u2265 f(x)\u2212 4d 7c1\u03b4 c32 \u2212 d(d+ 2)\u03b4 c2\nfor any x \u2208 \u03a0. Therefore, using the same argument as before, the net regret incurred in the round is\n(d+ 2) ( 4d7c1 c32 + d(d+ 2) c2 ) \u03b4 \u00b7 2\u03c3 log T \u03b3\u03022 .\nSubstituting in the values of \u2206\u03c4 (\u03b3\u0302) and \u2206\u0304\u03c4 (\u03b3\u0302) completes the proof.\nLemma 7 is critical because it allows us to claim that at any round, when we sample the function over a pyramid with a value \u03b3\u0302, then the regret on that pyramid during this sampling is at most poly(d)/\u03b3\u0302 since we must have been in case 2(a) with 2\u03b3\u0302 if we\u2019re using \u03b3\u0302. The only exception is at first round, where this statement holds trivially as the function is 1-Lipschitz by assumption.\nWe next show that the algorithm can visit the case 1(a) only a bounded number of times every round. The round is ended when the algorithm enters cases 1(b) or 2(b), and the regret incurred on case 2(a) would be bounded using the above Lemma 7.\nThe key idea for this bound is present in Section 9.2.2 of Nemirovski and Yudin [15]. We need a slight modification of their argument due to the fact that the function evaluations have noise and our sampling strategy is a little different from theirs.\nLemma 8. At any round, the number of visits to case 1(a) is 2d2 log d/c22, and each pyramid \u03a0 constructed by the algorithm satisfies \u2016y \u2212 x0\u2016 \u2265 r\u03c4/d, where y is the apex of \u03a0.\nProof. The proof follows by a simple geometric argument that exploits the fact that we have an angle 2\u03d5 at the apex of our pyramid which is almost equal to \u03c0, and that y \u2212 x0 and zi \u2212 x0 are orthogonal for any pyramid \u03a0 we construct (see Figure 6). By definition of case 1(a), top 6= y, so we assume top = z1 wlog. By construction,\n\u2016z1 \u2212 x0\u2016 = sin\u03d5\u2016y \u2212 x0\u2016. (9) Since this step applies every time we enter case 1(a), the total number k of visits to case 1(a) satisfies\n\u2016z1 \u2212 x0\u2016 = (sin\u03d5)kr\u03c4 ,\nwhere we recall that r\u03c4 is the radius of the regular simplex we construct in the first step on every round. We further note that for a regular simplex of radius r\u03c4 , a Euclidean ball of radius r\u03c4/d is contained in the simplex. We also note that by construction, cos\u03d5 = c2/d and hence sin\u03d5 = \u221a 1\u2212 c22/d2 \u2264 1 \u2212 c22/(2d2). Hence, setting k = 2d2 log d/c22 suffices to ensure that \u2016z1\u2212x0\u2016 \u2264 r\u03c4/d guaranteeing that z1 lies in the initial simplex of radius r\u03c4 centered at x0, as depicted in Figure 12.\nLet y1, . . . , yk be the apexes of the pyramids we have constructed in this round. Then by construction, we have a sequence of points such that\nf(z1) = f(top) \u2265 f(yk) + \u03b3 \u2265 f(yk\u22121) + 2\u03b3 \u00b7 \u00b7 \u00b7 \u2265 f(y1) + k\u03b3. On the other hand, we know that y1 satisfies f(y1) \u2265 f(xi) \u2212 \u03b3 for all the vertices xi of the simplex by definition of y1. Since z1 lies in the simplex, convexity of f guarantees that\nf(y1) \u2265 f(z1)\u2212 \u03b3 \u2265 f(y1) + (k \u2212 1)\u03b3, which is a contradiction unless k \u2264 1. Thus it must be the case that z1 is not in the simplex if k > 1, in which case k can be at most 2d2 log d/c22.\nThis lemma guarantees that in at most 2d2 log d/c22 pyramid constructions, the algorithm will enter one of cases 1(b) or 2(b) and terminate the epoch, unless the CI level \u03b3 at this round is insufficient to resolve things and we end in case 2(a). It also shows that all the pyramids constructed by our algorithm are sufficiently far from the center which is assumed by Lemmas 5- 7. Until now, we have focused on controlling the regret on the pyramids we construct, which is convenient since we sample the center points of the pyramids. To bound the regret incurred over one round, we also need to control the regret over the initial simplex we query at every round. We start with a lemma that shows how to control the net regret accrued over an entire round, when the round ends in case 2(a).\nLemma 9. For any round with a CI width of \u03b3 that terminates in case 2(a), the net regret incurred on the round is at most\n24d\u03c3 log T\n\u03b3\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 )\nProof. Suppose we constructed a total of k pyramids on the round, with k \u2264 2d2 log d/c2 by Lemma 8. Then we know that the instantaneous regret on any point of the kth pyramid \u03a0k is bounded by\n\u03b4 := \u03b3 ( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) ,\nby Lemma 7. We also note that by construction, yk is the top vertex of the (k\u2212 1)st pyramid \u03a0k\u22121. Hence by definition of case 1(a) (which caused us to go from \u03a0k\u22121 to \u03a0k), we know that f(x) \u2264 f(yk) + \u03b3 for all x \u2208 \u03a0k\u22121. Reasoning in the same way, we get that the function value at each vertex of the pyramid we constructed in this round is bounded by the function value at yk. Furthermore, just like the proof of Lemma 8, the function value at any vertex of the initial simplex is also bounded by the function value at yk. As a result, the instantaneous regret incurred at any point we sampled in this round is bounded by the net regret at yk which is at most by \u03b4 using Lemma 7. Since every pyramid as well as the simplex samples at most d + 2 vertices, and the total number of pyramids we construct is bounded by Lemma 8, we query at most (d + 2)(2d2/c22 log d + 1) points at any round. In order to bound the number of queries made at any point, we observe that for a CI level \u03b3\u0302, we make 2\u03c3 log T/\u03b3\u03022 queries. Suppose \u03b3 = 2\u22121. Since \u03b3\u0302 is geometrically decreased to \u03b3, the total number of queries made at any point is bounded by\ni\u2211 j=1 2\u03c3 log T 2\u22122j \u2264 8\u03c3 log T22i = 8\u03c3 log T \u03b32 .\nPutting all the pieces together, the net regret accrued over this round is at most\n24d\u03c3 log T\n\u03b3\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) ,\nwhich completes the proof.\nWe are now in a position to state a regret bound on the net regret incurred in any round. The key idea would be to use the bound from Lemma 9 to bound the regret even when the algorithm terminates in cases 1(b) or 2(b).\nLemma 10. For any round that terminates in a CI level \u03b3, the net regret over the round is bounded by\n48d\u03c3 log T\n\u03b3\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) .\nProof. We just need to control the regret incurred in rounds that end in cases 1(b) or 2(b). We recall from the description of the algorithm that a CI level of \u03b3 is used at a round only when the algorithm terminates the round with a CI level of 2\u03b3 in case 2(a). The only exception is the first round with \u03b3 = 1, where the instantaneous regret is bounded by 1 at any point using the Lipschitz assumption. Now suppose we did end a round with CI level 2\u03b3 in case 2(a). In particular, the proof of Lemma 9 guarantees that the instantaneous regret at any vertex of the simplex we construct is at most\n2\u03b3 ( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) Now consider any pyramid constructed on this round. We know that the instantaneous regret incurred if the pyramid ends in case 2(a) is bounded by Lemma 7. Furthermore, if the algorithm was in cases 1(a), 1(b) or 2(b) with a CI level \u03b3\u0302 (which could be larger than \u03b3 in general), then it must have been in case 2(a) with a CI level 2\u03b3\u0302. Hence the instantaneous regret on the vertices of the pyramid is at most\n2\u03b3\u0302 ( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) ,\nand we make at most 8\u03c3 log T\u03b3\u03022 queries on any point of the pyramid by a similar argument like the previous lemma. Thus the net regret incurred at any pyramid constructed by the algorithm is at most\n48d\u03c3 log T\n\u03b3\u0302\n( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) ,\nRecalling our bound on the number of pyramids constructed at any round completes the proof.\nPutting all the pieces together, we have shown that the regret incurred on any round with a CI level \u03b3 is bounded by C/\u03b3, where C comes from the above lemmas. We further observe that since \u03b3 is reduced geometrically, the net regret incurred on an epoch where the largest CI level we encounter is \u03b3 is at most\ni\u2211 j=1 C 2\u2212j \u2264 2C2i = 2C/\u03b3.\nThis allows us to get a bound on the regret of one epoch stated in the next lemma.\nLemma 11. The regret in any epoch which ends in CI level \u03b3 is at most\n96d\u03c3 log T\n\u03b3\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) . (10)"}, {"heading": "6.2.2 Bound on the number of epochs", "text": "In order to bound the number of epochs, we first need to show that the cone-cutting step discards a sizeable chunk of the set X\u03c4 in epoch \u03c4 . Recall that we need to understand the ratio of the volumes of B\u03c4+1 to B\u03c4 in order to understand the amount of volume discarded in any epoch.\nLemma 12. Let B\u03c4 be the smallest ball containing X\u03c4 , and let B \u2032 \u03c4+1 be the minimum volume ellipsoid containing B\u03c4 \\ K\u03c4 . Then for small enough constants c1, c2, vol(B \u2032 \u03c4+1) \u2264 \u03c1 \u00b7 vol(B\u03c4 ) for \u03c1 = exp(\u2212 14(d+1) ).\nProof. This lemma is analogous to the volume reduction results proved in the analysis of ellipsoid method for convex programming with a gradient oracle. We start by arguing that it suffices to consider the intersection of B\u03c4 with a half-space in order to understand the set B\u03c4 \\ K\u03c4 . It is clear from the figure that we only increase the volume of the enclosing ellipsoid B\u2032\u03c4+1 if we consider discarding only the spherical cap instead of discarding the entire cone. But the spherical cap is exactly obtained by taking the intersection of B\u03c4 with a half-space.\nThe choices of the constants c1, c2 earlier guarantee that the distance of the hyperplane from the origin is at most R\u03c4/(4(d+ 1)). This is because the apex of the cone K\u03c4 is always contained in B(r\u03c4 ) by construction and the height of the cone is at most R\u03c4 cos \u03d5\u0304 \u2264 R\u03c4/(8(d+ 1)) where the last inequality will be ensured by construction. Ensuring r\u03c4 \u2264 R\u03c4/(32(d + 1)) suffices to ensure that the distance of the hyperplane to the origin is at most R\u03c4/(4(d+ 1)).\nThus B\u2032\u03c4+1 is the minimum volume ellipsoid enclosing the intersection of a sphere with a hyperplane at a distance at most R\u03c4/(4(d + 1)) from its center. The volume of B \u2032 \u03c4+1 is then bounded as stated by using Theorem 2.1 of Goldfarb and Todd [11] in their work on deep cuts for the ellipsoid algorithm. In particular, we apply their result with \u03b1 = \u22121/(4(d+ 1)) giving the statement of our lemma.\nWe note that the connection from volume reduction to a bound on the number of epochs is somewhat delicate for our algorithm. The key idea is to show that at any epoch that ends with a CI level \u03b3, the cone K\u03c4 contains points with regret at least \u03b3. This will be shown in the next lemma. Lemma 13. At any epoch ending with CI level \u03b3, the instantaneous regret of any point in K\u03c4 is at least \u03b3 Proof. Since every epoch terminates either through case 1(b) or through the case 2(b) followed by hatraising, we just need to check the condition of the lemma for both the cases. If the epoch proceeds to cone-cutting through case 1(b), this is already shown in Equation (5). Thus we only need to verify the claim when we terminate via the hat-raising step. Recall that after hat-raising, the apex y\u2032 of the final pyramid \u03a0\u2032 constructed in the hat-raising step satisfies that f(y\u2032) \u2265 f(zi) + \u03b3 for all the vertices z1, . . . , zd of the pyramid. Consider any point x \u2208 K\u03c4 . This point lies on a ray from the base of \u03a0\u2032 passing through y\u2032. We know the function f is increasing along this ray at y\u2032 and hence continues to increase from y\u2032 to x by convexity of f , as argued in the proof of Lemma 6. Hence in this case also the instantaneous regret of any point in K\u03c4 is at least \u03b3 completing the proof.\nThe above lemma allows us to bound the number of epochs played by the algorithm. Lemma 14. The total number of epochs in the algorithm is bounded by d log Tlog(1/\u03c1) with \u03c1 = exp ( \u2212 14(d+1) ) . Proof. Let x\u2217 be the optimum of f . Since f is 1-Lipschitz, any point in a ball of radius 1/ \u221a T centered\naround x\u2217 has instantaneous regret at most 1/ \u221a T . The volume of this ball is T\u2212d/2Vd, where Vd is the volume of a unit ball in d-dimensions. Suppose the algorithm goes on for k epochs. We know that the volume of X after k epochs is at most \u03c1kVd by Lemma 12. We also note that the instantaneous regret of any point discarded by the algorithm in any epoch is at least 1/ \u221a T using Lemma 13, since we always maintain\n\u03b3 \u2265 1/ \u221a T . Thus any point in the ball of radius 1/ \u221a T around x\u2217 is never discarded by the algorithm. As a result, the algorithm must stop once we have\n\u03c1kVd \u2264 T\u2212d/2Vd,\nwhich means k \u2264 d log T/ log 1/\u03c1 as claimed.\nWe are now in a position to put together all the pieces.\nProof of Theorem 2. We are guaranteed that there are at most d log T/ log(1/\u03c1) epochs where the regret on each epoch is bounded by Equation 10. Observing that \u03b3 \u2265 1/ \u221a T guarantees that every epoch has regret at most\n96d\u03c3 \u221a T log T\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) .\nCombining with the above bound on the number of epochs guarantees that the cumulative regret of our algorithm is bounded by\n96d2\u03c3 \u221a T log2 T\nlog(1/\u03c1)\n( 2d2 log d\nc22 + 1 )( 4d7c1 c32 + d(d+ 2) c2 )( 12c1d 4 c22 + 11 ) .\nFinally, we recall that the entire analysis this far has been conditioned on the even E which assumes that the function value lies in the confidence intervals we construct at every round. By design, just like the proof of Theorem 1, P(Ec) \u2264 1/T . Using this and substituting the value of \u03c1 from Lemma 14 completes the proof of the theorem."}, {"heading": "7 Discussion", "text": "This paper presents a new algorithm for convex optimization when only noisy function evaluations are possible. The algorithm builds on the techniques of Nemirovski and Yudin [15] from zeroth order optimization. The key contribution of our work is to extend their algorithm to a noisy setting in such a way that a low regret on the sequence of points queried can be guaranteed. The new algorithm crucially relies on a center-point device that demonstrates the key differences between a regret minimization and an optimization guarantee. Our algorithm has the optimal O( \u221a T ) scaling of regret up to logarithmic factors. However, our regret guarantee has a rather large dimension dependence. As remarked after Theorem 2, this is unsurprising since the algorithm of Nemirovski and Yudin [15] has a large dimension dependence even in a noiseless case. Random walk approaches [5] have been successful to improve the dimension scaling in the noiseless case, and investigating them for the noisy scenario is an interesting question for future research."}, {"heading": "Acknowledgments", "text": "Part of this work was done while AA and DH were at the University of Pennsylvania. AA was partially supported by MSR and Google PhD fellowships while this work was done. DH was partially supported under grants AFOSR FA9550-09-1-0425, NSF IIS-1016061, and NSF IIS-713540. AR gratefully acknowledges the support of NSF under grant CAREER DMS-0954737."}, {"heading": "A Properties of pyramid constructions", "text": "We outline some properties of the pyramid construction in this appendix. Recall that \u03d5 = arccos(c2/d). For simplicity, we assume d \u2265 2. In this case, cos(\u03d5) = c2/d and sin(\u03d5) = \u221a 1\u2212 c22/d2 \u2265 cos(\u03d5). Also recall that in epoch \u03c4 , the initial simplex is contained in B(r\u03c4 ) where r\u03c4 = R\u03c4/(c1d).\nLemma 15. Let \u03a0k be the k-th pyramid constructed in any round of epoch \u03c4 .\n1. The distance from the center of B(r\u03c4 ) to the apex of \u03a0k is r\u03c4 sink\u22121(\u03d5).\n2. The distance from the apex of \u03a0k to any vertex of the base of \u03a0k is r\u03c4 sin k\u22121(\u03d5) cos(\u03d5).\n3. The height of \u03a0k (distance of the apex from the base) is r\u03c4 sin k\u22121(\u03d5) cos2(\u03d5).\nProof. The proof is by induction on k. Let x0 be the center of B(r\u03c4 ), y1 be the apex of \u03a01, and z1 be any vertex on the base of \u03a01. By construction, y1 \u2212 z1 is perpendicular to z1 \u2212 x0, so we have \u2016y1 \u2212 x0\u2016 = r\u03c4 , \u2016y1 \u2212 z1\u2016 = r\u03c4 cos(\u03d5), and \u2016z1 \u2212 x0\u2016 = r\u03c4 cos(\u03d5). Let p1 be the projection of y1 onto the base of \u03a01. The triangle with vertices y1, z1, x0 is similar to the triangle with vertices y1, p1, z1. Therefore \u2016y1 \u2212 p1\u2016, the height of \u03a01, is r\u03c4 cos\n2(\u03d5). This gives the base case of the induction (see Figure 13). The inductive step follows by noting that the apex of \u03a0k is a vertex on the base of \u03a0k\u22121, and therefore\nthe distances scale as claimed.\nLemma 16. Let \u03a0 be any pyramid constructed in epoch \u03c4 with apex at distance r\u03a0 \u2265 r\u03c4/d from the center of B(r\u03c4 ). Let B\u03a0 be the largest ball in \u03a0 centered at the center of mass c of \u03a0.\n1. B\u03a0 has radius at least r\u03a0 cos2(\u03d5)/(d+ 1) \u2265 r\u03c4 c22/(2d4).\n2. Let x \u2208 \u03a0, and let b \u2208 \u03a0 be the point on the face of \u03a0 such that c = \u03b1x+ (1\u2212\u03b1)b for some 0 < \u03b1 \u2264 1. Then (1\u2212 \u03b1)/\u03b1 \u2264 (d+ 1)d/c2.\nProof. Let h be the height of \u03a0. By Lemma 15, h = r\u03a0 cos 2(\u03d5). The distance from c to the base of \u03a0 is\nh d+ 1 = r\u03a0 cos\n2(\u03d5)\nd+ 1 ,\nand the distance from c to any other face of \u03a0 is\nsin(\u03d5) ( 1\u2212 1\nd+ 1\n) h = \u221a 1\u2212 cos2(\u03d5) ( 1\u2212 1\nd+ 1\n) r\u03a0 cos 2(\u03d5) \u2265 r\u03a0 cos 2(\u03d5)\n2\n(here we have used d \u2265 2 and cos(\u03d5) \u2264 1/d). Therefore B\u03a0 has radius at least\nr\u03a0 cos 2(\u03d5)\nd+ 1 \u2265 r\u03c4 d \u00b7 c\n2 2/d 2\nd+ 1 =\nr\u03c4 c 2 2\nd3(d+ 1) \u2265 r\u03c4 c\n2 2\n2d4 .\nwhich proves the first claim. For the second claim, note that \u03b1 = \u2016b\u2212 c\u2016/(\u2016b\u2212 c\u2016+ \u2016x\u2212 c\u2016); moreover, \u2016b\u2212 c\u2016 is at least the radius of B\u03a0, and \u2016x\u2212 c\u2016 is at most the distance from c to any vertex of \u03a0. By Lemma 15, the distance from c to a vertex on the base of \u03a0 is\u221a(\nr\u03a0 d+ 1 cos2(\u03d5)\n)2 + (r\u03a0 cos(\u03d5) sin(\u03d5)) 2 = r\u03a0 cos 2(\u03d5)\nd+ 1\n\u221a 1 + (d+ 1)2 sin2(\u03d5)\ncos2(\u03d5)\nand the distance from c to the apex of \u03a0 is( 1\u2212 1\nd+ 1\n) h = ( 1\u2212 1\nd+ 1\n) r\u03a0 cos 2(\u03d5) = d\nd+ 1 r\u03a0 cos\n2(\u03d5).\nTherefore, by the first claim and Lemma 15,\n1\u2212 \u03b1 \u03b1 = \u2016x\u2212 c\u2016 \u2016b\u2212 c\u2016 \u2264 max  dr\u03a0 cos 2(\u03d5) d+1\nr\u03a0 cos2(\u03d5) d+1\n,\nr\u03a0 cos 2(\u03d5)\nd+1\n\u221a 1 + (d+1)\n2 sin2(\u03d5) cos2(\u03d5)\nr\u03a0 cos2(\u03d5) d+1  = max { d, \u221a 1 + (d+ 1)2 ( 1 cos2(\u03d5) \u2212 1 )}\n\u2264 max { d, \u221a (d+ 1)2\ncos2(\u03d5)\n}\n= max { d, d+ 1\ncos(\u03d5) } = max { d, (d+ 1)d\nc2 } = (d+ 1)d\nc2 ."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "<lb>This paper addresses the problem of minimizing a convex, Lipschitz function f over a convex, compact<lb>set X under a stochastic bandit feedback model. In this model, the algorithm is allowed to observe noisy<lb>realizations of the function value f(x) at any query point x \u2208 X . The quantity of interest is the regret<lb>of the algorithm, which is the sum of the function values at algorithm\u2019s query points minus the optimal<lb>function value. We demonstrate a generalization of the ellipsoid algorithm that incurs \u00d5(poly(d)<lb>\u221a<lb>T )<lb>regret. Since any algorithm has regret at least \u03a9(<lb>\u221a<lb>T ) on this problem, our algorithm is optimal in terms<lb>of the scaling with T .", "creator": "LaTeX with hyperref package"}}}