{"id": "1112.4394", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2011", "title": "Additive Gaussian Processes", "abstract": "we introduce a gaussian process model of functions which are additive. an additive function is one which decomposes into a sum of low - point dimensional functions, often each depending on only a subset of the input variables. mixed additive gps generalize both generalized additive models, and match the standard gp models which use squared - volume exponential kernels. simultaneous hyperparameter learning operations in this model can be seen historically as bayesian hierarchical kernel learning ( hkl ). we introduce an expressive linear but tractable differential parameterization of the convex kernel function, which allows efficient subjective evaluation of all input interaction terms, whose number multiplication is exponential in the input dimension. the additional structure discoverable knowledge by this model results in increased interpretability, as well as better state - of - the - art predictive power in regression tasks.", "histories": [["v1", "Mon, 19 Dec 2011 16:22:09 GMT  (3861kb,D)", "http://arxiv.org/abs/1112.4394v1", "Appearing in Neural Information Processing Systems 2011"]], "COMMENTS": "Appearing in Neural Information Processing Systems 2011", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["david k duvenaud", "hannes nickisch", "carl edward rasmussen"], "accepted": true, "id": "1112.4394"}, "pdf": {"name": "1112.4394.pdf", "metadata": {"source": "CRF", "title": "Additive Gaussian Processes", "authors": ["David Duvenaud", "Hannes Nickisch"], "emails": ["dkd23@cam.ac.uk", "hn@tue.mpg.de", "cer54@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Most statistical regression models in use today are of the form: g(y) = f(x1)+f(x2)+\u00b7 \u00b7 \u00b7+f(xD). Popular examples include logistic regression, linear regression, and Generalized Linear Models [1]. This family of functions, known as Generalized Additive Models (GAM) [2], are typically easy to fit and interpret. Some extensions of this family, such as smoothing-splines ANOVA [3], add terms depending on more than one variable. However, such models generally become intractable and difficult to fit as the number of terms increases.\nAt the other end of the spectrum are kernel-based models, which typically allow the response to depend on all input variables simultaneously. These have the form: y = f(x1, x2, . . . , xD). A popular example would be a Gaussian process model using a squared-exponential (or Gaussian) kernel. We denote this model as SE-GP. This model is much more flexible than the GAM, but its flexibility makes it difficult to generalize to new combinations of input variables.\nIn this paper, we introduce a Gaussian process model that generalizes both GAMs and the SE-GP. This is achieved through a kernel which allow additive interactions of all orders, ranging from first order interactions (as in a GAM) all the way to Dth-order interactions (as in a SE-GP). Although this kernel amounts to a sum over an exponential number of terms, we show how to compute this kernel efficiently, and introduce a parameterization which limits the number of hyperparameters to O(D). A Gaussian process with this kernel function (an additive GP) constitutes a powerful model that allows one to automatically determine which orders of interaction are important. We show that this model can significantly improve modeling efficacy, and has major advantages for model interpretability. This model is also extremely simple to implement, and we provide example code.\nWe note that a similar breakthrough has recently been made, called Hierarchical Kernel Learning (HKL) [4]. HKL explores a similar class of models, and sidesteps the possibly exponential number of interaction terms by cleverly selecting only a tractable subset. However, this method suffers considerably from the fact that cross-validation must be used to set hyperparameters. In addition, the machinery necessary to train these models is immense. Finally, on real datasets, HKL is outperformed by the standard SE-GP [4].\nar X\niv :1\n11 2.\n43 94\nv1 [\nst at\n.M L\n] 1\n9 D\nec 2"}, {"heading": "2 Gaussian Process Models", "text": "Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks [5]. The kind of structure which can be captured by a GP model is mainly determined by its kernel: the covariance function. One of the main difficulties in specifying a Gaussian process model is in choosing a kernel which can represent the structure present in the data. For small to medium-sized datasets, the kernel has a large impact on modeling efficacy.\nFigure 1 compares, for two-dimensional functions, a first-order additive kernel with a second-order kernel. We can see that a GP with a first-order additive kernel is an example of a GAM: Each function drawn from this model is a sum of orthogonal one-dimensional functions. Compared to functions drawn from the higher-order GP, draws from the first-order GP have more long-range structure.\nWe can expect many natural functions to depend only on sums of low-order interactions. For example, the price of a house or car will presumably be well approximated by a sum of prices of individual features, such as a sun-roof. Other parts of the price may depend jointly on a small set of features, such as the size and building materials of a house. Capturing these regularities will mean that a model can confidently extrapolate to unseen combinations of features."}, {"heading": "3 Additive Kernels", "text": "We now give a precise definition of additive kernels. We first assign each dimension i \u2208 {1 . . . D} a one-dimensional base kernel ki(xi, x\u2032i). We then define the first order, second order and nth order additive kernel as:\nkadd1(x,x \u2032) = \u03c321 D\u2211 i=1 ki(xi, x \u2032 i) (1)\nkadd2(x,x \u2032) = \u03c322 D\u2211 i=1 D\u2211 j=i+1 ki(xi, x \u2032 i)kj(xj , x \u2032 j) (2)\nkaddn(x,x \u2032) = \u03c32n \u2211 1\u2264i1<i2<...<in\u2264D\n[ n\u220f\nd=1\nkid(xid , x \u2032 id )\n] (3)\nwhere D is the dimension of our input space, and \u03c32n is the variance assigned to all nth order interactions. The nth covariance function is a sum of ( D n ) terms. In particular, theDth order additive\ncovariance function has ( D D ) = 1 term, a product of each dimension\u2019s covariance function:\nkaddD (x,x \u2032) = \u03c32D D\u220f d=1 kd(xd, x \u2032 d) (4)\nIn the case where each base kernel is a one-dimensional squared-exponential kernel, the Dth-order term corresponds to the multivariate squared-exponential kernel:\nkaddD (x,x \u2032) = \u03c32D D\u220f d=1 kd(xd, x \u2032 d) = \u03c3 2 D D\u220f d=1 exp ( \u2212 (xd \u2212 x \u2032 d) 2 2l2d ) = \u03c32D exp ( \u2212 D\u2211 d=1 (xd \u2212 x\u2032d)2 2l2d ) (5) also commonly known as the Gaussian kernel. The full additive kernel is a sum of the additive kernels of all orders."}, {"heading": "3.1 Parameterization", "text": "The only design choice necessary in specifying an additive kernel is the selection of a onedimensional base kernel for each input dimension. Any parameters (such as length-scales) of the base kernels can be learned as usual by maximizing the marginal likelihood of the training data.\nIn addition to the hyperparameters of each dimension-wise kernel, additive kernels are equipped with a set of D hyperparameters \u03c321 . . . \u03c3 2 D controlling how much variance we assign to each order of interaction. These \u201corder variance\u201d hyperparameters have a useful interpretation: The dth order variance hyperparameter controls how much of the target function\u2019s variance comes from interactions of the dth order. Table 1 shows examples of normalized order variance hyperparameters learned on real datasets.\nOn different datasets, the dominant order of interaction estimated by the additive model varies widely. An additive GP with all of its variance coming from the 1st order is equivalent to a GAM; an additive GP with all its variance coming from the Dth order is equivalent to a SE-GP.\nBecause the hyperparameters can specify which degrees of interaction are important, the additive GP is an extremely general model. If the function we are modeling is decomposable into a sum of low-dimensional functions, our model can discover this fact and exploit it (see Figure 5) . If this is not the case, the hyperparameters can specify a suitably flexible model."}, {"heading": "3.2 Interpretability", "text": "As noted by Plate [6], one of the chief advantages of additive models such as GAM is their interpretability. Plate also notes that by allowing high-order interactions as well as low-order interactions, one can trade off interpretability with predictive accuracy. In the case where the hyperparameters indicate that most of the variance in a function can be explained by low-order interactions, it is useful and easy to plot the corresponding low-order functions, as in Figure 2."}, {"heading": "3.3 Efficient Evaluation of Additive Kernels", "text": "An additive kernel overD inputs with interactions up to order n hasO(2n) terms. Na\u0131\u0308vely summing over these terms quickly becomes intractable. In this section, we show how one can evaluate the sum over all terms in O(D2).\nThe nth order additive kernel corresponds to the nth elementary symmetric polynomial [7] [8], which we denote en. For example: if x has 4 input dimensions (D = 4), and if we let zi = ki(xi, x\u2032i), then\nkadd1(x,x \u2032) = e1(z1, z2, z3, z4) = z1 + z2 + z3 + z4 kadd2(x,x \u2032) = e2(z1, z2, z3, z4) = z1z2 + z1z3 + z1z4 + z2z3 + z2z4 + z3z4 kadd3(x,x \u2032) = e3(z1, z2, z3, z4) = z1z2z3 + z1z2z4 + z1z3z4 + z2z3z4 kadd4(x,x \u2032) = e4(z1, z2, z3, z4) = z1z2z3z4\nThe Newton-Girard formulae give an efficient recursive form for computing these polynomials. If we define sk to be the kth power sum: sk(z1, z2, . . . , zD) = \u2211D i=1 z k i , then\nkaddn(x,x \u2032) = en(z1, . . . , zD) =\n1\nn n\u2211 k=1 (\u22121)(k\u22121)en\u2212k(z1, . . . , zD)sk(z1, . . . , zD) (6)\nWhere e0 , 1. The Newton-Girard formulae have time complexity O(D2), while computing a sum over an exponential number of terms.\nConveniently, we can use the same trick to efficiently compute all of the necessary derivatives of the additive kernel with respect to the base kernels. We merely need to remove the kernel of interest from each term of the polynomials:\n\u2202kaddn \u2202zj = en\u22121(z1, . . . , zj\u22121, zj+1, . . . zD) (7)\nThis trick allows us to optimize the base kernel hyperparameters with respect to the marginal likelihood."}, {"heading": "3.4 Computation", "text": "The computational cost of evaluating the Gram matrix of a product kernel (such as the SE kernel) is O(N2D), while the cost of evaluating the Gram matrix of the additive kernel is O(N2DR), where R is the maximum degree of interaction allowed (up to D). In higher dimensions, this can be a significant cost, even relative to the fixed O(N3) cost of inverting the Gram matrix. However, as our experiments show, typically only the first few orders of interaction are important for modeling a given function; hence if one is computationally limited, one can simply limit the maximum degree of interaction without losing much accuracy.\nAdditive Gaussian processes are particularly appealing in practice because their use requires only the specification of the base kernel. All other aspects of GP inference remain the same. All of the experiments in this paper were performed using the standard GPML toolbox1; code to perform all experiments is available at the author\u2019s website.2"}, {"heading": "4 Related Work", "text": "Plate [6] constructs a form of additive GP, but using only the first-order and Dth order terms. This model is motivated by the desire to trade off the interpretability of first-order models, with the flexibility of full-order models. Our experiments show that often, the intermediate degrees of interaction contribute most of the variance.\nA related functional ANOVA GP model [9] decomposes the mean function into a weighted sum of GPs. However, the effect of a particular degree of interaction cannot be quantified by that approach. Also, computationally, the Gibbs sampling approach used in [9] is disadvantageous.\nChristoudias et al. [10] previously showed how mixtures of kernels can be learnt by gradient descent in the Gaussian process framework. They call this Bayesian localized multiple kernel learning. However, their approach learns a mixture over a small, fixed set of kernels, while our method learns a mixture over all possible products of those kernels."}, {"heading": "4.1 Hierarchical Kernel Learning", "text": "Bach [4] uses a regularized optimization framework to learn a weighted sum over an exponential number of kernels which can be computed in polynomial time. The subsets of kernels considered by this method are restricted to be a hull of kernels.3 Given each dimension\u2019s kernel, and a pre-defined weighting over all terms, HKL performs model selection by searching over hulls of interaction terms. In [4], Bach also fixes the relative weighting between orders of interaction with a single term \u03b1, computing the sum over all orders by:\nka(x,x \u2032) = v2D D\u220f d=1 (1 + \u03b1kd(xd, x \u2032 d)) (8)\nwhich has computational complexity O(D). However, this formulation forces the weight of all nth order terms to be weighted by \u03b1n.\nFigure 3 contrasts the HKL hull-selection method with the Additive GP hyperparameter-learning method. Neither method dominates the other in flexibility. The main difficulty with the approach\n1Available at http://www.gaussianprocess.org/gpml/code/ 2Example code available at: http://mlg.eng.cam.ac.uk/duvenaud/ 3In the setting we are considering in this paper, a hull can be defined as a subset of all terms such that if term\u220f\nj\u2208J kj(x,x \u2032) is included in the subset, then so are all terms \u220f j\u2208J/i kj(x,x\n\u2032), for all i \u2208 J . For details, see [4].\nof [4] is that hyperparameters are hard to set other than by cross-validation. In contrast, our method optimizes the hyperparameters of each dimension\u2019s base kernel, as well as the relative weighting of each order of interaction."}, {"heading": "4.2 ANOVA Procedures", "text": "Vapnik [11] introduces the support vector ANOVA decomposition, which has the same form as our additive kernel. However, they recommend approximating the sum over all D orders with only one term \u201cof appropriate order\u201d, presumably because of the difficulty of setting the hyperparameters of an SVM. Stitson et al. [12] performed experiments which favourably compared the support vector ANOVA decomposition to polynomial and spline kernels. They too allowed only one order to be active, and set hyperparameters by cross-validation.\nA closely related procedure from the statistics literature is smoothing-splines ANOVA (SS-ANOVA) [3]. An SS-ANOVA model is estimated as a weighted sum of splines along each dimension, plus a sum of splines over all pairs of dimensions, all triplets, etc, with each individual interaction term having a separate weighting parameter. Because the number of terms to consider grows exponentially in the order, in practice, only terms of first and second order are usually considered. Learning in SS-ANOVA is usually done via penalized-maximum likelihood with a fixed sparsity hyperparameter.\nIn contrast to these procedures, our method can easily include all D orders of interaction, each weighted by a separate hyperparameter. As well, we can learn kernel hyperparameters individually per input dimension, allowing automatic relevance determination to operate."}, {"heading": "4.3 Non-local Interactions", "text": "By far the most popular kernels for regression and classification tasks on continuous data are the squared exponential (Gaussian) kernel, and the Mate\u0301rn kernels. These kernels depend only on the scaled Euclidean distance between two points, both having the form: k(x,x\u2032) = f( \u2211D\nd=1 (xd \u2212 x\u2032d) 2 /l2d). Bengio et al. [13] argue that models based on squared-exponential kernels\nare particularily susceptible to the curse of dimensionality. They emphasize that the locality of the kernels means that these models cannot capture non-local structure. They argue that many functions that we care about have such structure. Methods based solely on local kernels will require training examples at all combinations of relevant inputs.\nAdditive kernels have a much more complex structure, and allow extrapolation based on distant parts of the input space, without spreading the mass of the kernel over the whole space. For example, additive kernels of the second order allow strong non-local interactions between any points which are similar in any two input dimensions. Figure 4 provides a geometric comparison between squaredexponential kernels and additive kernels in 3 dimensions."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Synthetic Data", "text": "Because additive kernels can discover non-local structure in data, they are exceptionally well-suited to problems where local interpolation fails. Figure 5 shows a dataset which demonstrates this feature\nof additive GPs, consisting of data drawn from a sum of two axis-aligned sine functions. The training set is restricted to a small, L-shaped area; the test set contains a peak far from the training set locations. The additive GP recovered both of the original sine functions (shown in green), and inferred correctly that most of the variance in the function comes from first-order interactions. The ability of additive GPs to discover long-range structure suggests that this model may be well-suited to deal with covariate-shift problems."}, {"heading": "5.2 Experimental Setup", "text": "On a diverse collection of datasets, we compared five different models. In the results tables below, GP Additive refers to a GP using the additive kernel with squared-exp base kernels. For speed, we limited the maximum order of interaction in the additive kernels to 10. GP-GAM denotes an additive GP model with only first-order interactions. GP Squared-Exp is a GP model with a squaredexponential ARD kernel. HKL4 was run using the all-subsets kernel, which corresponds to the same set of kernels as considered by the additive GP with a squared-exp base kernel.\nFor all GP models, we fit hyperparameters by the standard method of maximizing training-set marginal likelihood, using L-BFGS [14] for 500 iterations, allowing five random restarts. In addition to learning kernel hyperparameters, we fit a constant mean function to the data. In the classification experiments, GP inference was done using Expectation Propagation [15]."}, {"heading": "5.3 Results", "text": "Tables 2, 3, 4 and 5 show mean performance across 10 train-test splits. Because HKL does not specify a noise model, it could not be included in the likelihood comparisons.\nThe model with best performance on each dataset is in bold, along with all other models that were not significantly different under a paired t-test. The additive model never performs significantly worse than any other model, and sometimes performs significantly better than all other models. The\n4Code for HKL available at http://www.di.ens.fr/\u02dcfbach/hkl/\ndifference between all methods is larger in the case of regression experiments. The performance of HKL is consistent with the results in [4], performing competitively but slightly worse than SE-GP.\nThe additive GP performed best on datasets well-explained by low orders of interaction, and approximately as well as the SE-GP model on datasets which were well explained by high orders of interaction (see table 1). Because the additive GP is a superset of both the GP-GAM model and the SE-GP model, instances where the additive GP performs slightly worse are presumably due to overfitting, or due to the hyperparameter optimization becoming stuck in a local maximum. Additive GP performance can be expected to benefit from integrating out the kernel hyperparameters."}, {"heading": "6 Conclusion", "text": "We present additive Gaussian processes: a simple family of models which generalizes two widelyused classes of models. Additive GPs also introduce a tractable new type of structure into the GP framework. Our experiments indicate that such additive structure is present in real datasets, allowing our model to perform better than standard GP models. In the case where no such structure exists, our model can recover arbitrarily flexible models, as well.\nIn addition to improving modeling efficacy, the additive GP also improves model interpretability: the order variance hyperparameters indicate which sorts of structure are present in our model.\nCompared to HKL, which is the only other tractable procedure able to capture the same types of structure, our method benefits from being able to learn individual kernel hyperparameters, as well as the weightings of different orders of interaction. Our experiments show that additive GPs are a state-of-the-art regression model."}, {"heading": "Acknowledgments", "text": "The authors would like to thank John J. Chew and Guillaume Obozonksi for their helpful comments."}], "references": [{"title": "Generalized linear models", "author": ["J.A. Nelder", "R.W.M. Wedderburn"], "venue": "Journal of the Royal Statistical Society. Series A (General),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1972}, {"title": "Generalized additive models", "author": ["T.J. Hastie", "R.J. Tibshirani"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Spline models for observational data", "author": ["G. Wahba"], "venue": "Society for Industrial Mathematics,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1990}, {"title": "High-dimensional non-linear variable selection through hierarchical kernel learning", "author": ["Francis Bach"], "venue": "CoRR, abs/0909.0844,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "CKI Williams"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Accuracy versus interpretability in flexible modeling: Implementing a tradeoff using Gaussian process models", "author": ["T.A. Plate"], "venue": "Behaviormetrika,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Symmetric functions and Hall polynomials", "author": ["I.G. Macdonald"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Enumerative combinatorics", "author": ["R.P. Stanley"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Bayesian functional anova modeling using Gaussian process prior distributions", "author": ["C.G. Kaufman", "S.R. Sain"], "venue": "Bayesian Analysis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Bayesian localized multiple kernel learning", "author": ["M. Christoudias", "R. Urtasun", "T. Darrell"], "venue": "Technical report,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Statistical learning theory, volume 2", "author": ["V.N. Vapnik"], "venue": "Wiley New York,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Support vector regression with ANOVA decomposition kernels. Advances in kernel methods: Support vector learning, pages", "author": ["M. Stitson", "A. Gammerman", "V. Vapnik", "V. Vovk", "C. Watkins", "J. Weston"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "The curse of highly variable functions for local kernel machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Updating quasi-newton matrices with limited storage", "author": ["J. Nocedal"], "venue": "Mathematics of computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1980}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Popular examples include logistic regression, linear regression, and Generalized Linear Models [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "This family of functions, known as Generalized Additive Models (GAM) [2], are typically easy to fit and interpret.", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Some extensions of this family, such as smoothing-splines ANOVA [3], add terms depending on more than one variable.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "We note that a similar breakthrough has recently been made, called Hierarchical Kernel Learning (HKL) [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "Finally, on real datasets, HKL is outperformed by the standard SE-GP [4].", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "2 Interpretability As noted by Plate [6], one of the chief advantages of additive models such as GAM is their interpretability.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "The nth order additive kernel corresponds to the nth elementary symmetric polynomial [7] [8], which we denote en.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "The nth order additive kernel corresponds to the nth elementary symmetric polynomial [7] [8], which we denote en.", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "2 4 Related Work Plate [6] constructs a form of additive GP, but using only the first-order and Dth order terms.", "startOffset": 23, "endOffset": 26}, {"referenceID": 8, "context": "A related functional ANOVA GP model [9] decomposes the mean function into a weighted sum of GPs.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Also, computationally, the Gibbs sampling approach used in [9] is disadvantageous.", "startOffset": 59, "endOffset": 62}, {"referenceID": 9, "context": "[10] previously showed how mixtures of kernels can be learnt by gradient descent in the Gaussian process framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "1 Hierarchical Kernel Learning Bach [4] uses a regularized optimization framework to learn a weighted sum over an exponential number of kernels which can be computed in polynomial time.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "In [4], Bach also fixes the relative weighting between orders of interaction with a single term \u03b1, computing the sum over all orders by: ka(x,x \u2032) = v D D \u220f", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "For details, see [4].", "startOffset": 17, "endOffset": 20}, {"referenceID": 3, "context": "of [4] is that hyperparameters are hard to set other than by cross-validation.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "2 ANOVA Procedures Vapnik [11] introduces the support vector ANOVA decomposition, which has the same form as our additive kernel.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "[12] performed experiments which favourably compared the support vector ANOVA decomposition to polynomial and spline kernels.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "A closely related procedure from the statistics literature is smoothing-splines ANOVA (SS-ANOVA) [3].", "startOffset": 97, "endOffset": 100}, {"referenceID": 12, "context": "[13] argue that models based on squared-exponential kernels are particularily susceptible to the curse of dimensionality.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "For all GP models, we fit hyperparameters by the standard method of maximizing training-set marginal likelihood, using L-BFGS [14] for 500 iterations, allowing five random restarts.", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "In the classification experiments, GP inference was done using Expectation Propagation [15].", "startOffset": 87, "endOffset": 91}, {"referenceID": 3, "context": "The performance of HKL is consistent with the results in [4], performing competitively but slightly worse than SE-GP.", "startOffset": 57, "endOffset": 60}], "year": 2011, "abstractText": "We introduce a Gaussian process model of functions which are additive. An additive function is one which decomposes into a sum of low-dimensional functions, each depending on only a subset of the input variables. Additive GPs generalize both Generalized Additive Models, and the standard GP models which use squared-exponential kernels. Hyperparameter learning in this model can be seen as Bayesian Hierarchical Kernel Learning (HKL). We introduce an expressive but tractable parameterization of the kernel function, which allows efficient evaluation of all input interaction terms, whose number is exponential in the input dimension. The additional structure discoverable by this model results in increased interpretability, as well as state-of-the-art predictive power in regression tasks.", "creator": "LaTeX with hyperref package"}}}