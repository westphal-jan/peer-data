{"id": "1201.6462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Active Learning of Custering with Side Information Using $\\eps$-Smooth Relative Regret Approximations", "abstract": "clustering is considered a non - supervised learning setting, in which the goal is to partition a random collection function of data points into disjoint clusters. often a bound $ 34 k $ on the number of clusters is given or assumed by visiting the practitioner. many versions of answering this problem have been defined, most notably $ 30 k $ - means and $ k $ - tailed median.", "histories": [["v1", "Tue, 31 Jan 2012 07:46:08 GMT  (16kb)", "http://arxiv.org/abs/1201.6462v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nir ailon", "ron begleiter"], "accepted": false, "id": "1201.6462"}, "pdf": {"name": "1201.6462.pdf", "metadata": {"source": "CRF", "title": "Active Learning of Custering with Side Information Using \u03b5-Smooth Relative Regret Approximations", "authors": ["Nir Ailon"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 1.\n64 62\nv1 [\ncs .L\nG ]\n3 1\nJa n\n20 12\nAn underlying problem with the unsupervised nature of clustering it that of determining a similarity function. One approach for alleviating this difficulty is known as clustering with side information, alternatively, semi-supervised clustering. Here, the practitioner incorporates side information in the form of \u201cmust be clustered\u201d or \u201cmust be separated\u201d labels for data point pairs. Each such piece of information comes at a \u201cquery cost\u201d (often involving human response solicitation). The collection of labels is then incorporated in the usual clustering algorithm as either strict or as soft constraints, possibly adding a pairwise constraint penalty function to the chosen clustering objective.\nOur work is mostly related to clustering with side information. We ask how to choose the pairs of data points. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. Roughly speaking, we show that the distribution must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, hence we don\u2019t know the bias. Using the recently introduced method of \u03b5-smooth relative regret approximations of Ailon, Begleiter and Ezra, we can show an iterative process that improves both the clustering and the bias in tandem. The process provably converges to the optimal solution faster (in terms of query cost) than an algorithm selecting pairs uniformly."}, {"heading": "1 Introduction", "text": "Clustering of data is probably the most important problem in the theory of unsupervised learning. In the most standard setting, the goal is to paritition a collection of data points into related groups. Virtually any large scale application using machine learning either uses clustering as a data preprocessing step or as an ends within itself.\nIn the most tranditional sense, clustering is an unsupervised learning problem because the solution is computed from the data itself, with no human labeling involved. There are many versions, most notably k-means and k-median. The number k typically serves as an assumed upper bound on the number of output clusters.\nAn underlying difficulty with the unsupervised nature of clustering is the fact that a similarity (or distance) function between data points must be chosen by the practitioner as a preliminary\nstep. This may often not be an easy task. Indeed, even if our dataset is readily embedded in some natural vector (feature) space, we still have the burden of the freedom of choosing a normed metric, and of applying some transformation (linear or otherwise) on the data for good measure. Many approaches have been proposed to tacle this. In one approach, a metric learning algorithm is executed as a preprocessing step in order to choose a suitable metric (from some family). This approach is supervised, and uses distances between pairs of elements as (possibly noisy) labels. The second approach is known as clustering with side information, alternatively, semi-supervised clustering. This approach should be thought of as adding crutches to a lame distance function the practitioner is too lazy to replace. Instead, she incorporates so-called side information in the form of \u201cmust be clustered\u201d or \u201cmust be separated\u201d labels for data point pairs. Each such label comes at a \u201cquery cost\u201d (often involving human response solicitation). The collection of labels is then incorporated in the chosen clustering algorithm as either strict constraints or as soft ones, possibly adding a pairwise constraint penalty function."}, {"heading": "1.1 Previous Related Work", "text": "Clustering with side information is a fairly new variant of clustering first described, independently, by Demiriz et al. [1999], and Ben-Dor et al. [1999]. In the machine learning community it is also widely known as semi-supervised clustering. There are a few alternatives for the form of feedback providing the side-information. The most natural ones are the single item labels [e.g., Demiriz et al., 1999], and the pairwise constraints [e.g., Ben-Dor et al., 1999].\nIn our study, the side information is pairwise, comes at a cost and is treated frugaly. In a related yet different setting, similarity information for all (quadratically many) pairs is available but is noisy. The combinatorial optimization theoretical problem of cleaning the noise is known as correlation clustering [Bansal et al., 2002] or cluster editing [Shamir et al., 2004]. Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006].\nRoughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints. Basu [2005] in his thesis, which also serves as a comprehensive survey, has championed this approach in conjunction with k-means, and hidden Markov random field clustering algorithms."}, {"heading": "1.2 Our Contribution", "text": "Our main motivation is reducing the number of pairwise similarity labels (query cost) required for k-clustering data using an active learning approach. More precisely, we ask how to choose which pairs of data points to query. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. More precisely, we show that the distribution from which we should draw pairs from must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, let alone the bias. Using the recently introduced method of \u03b5-smooth relative regret approximations (\u03b5-SRRA) of Ailon et al. [2011] we can show an iterative process that improves both the clustering\nand the bias in tandem. The process provably converges to the optimal solution faster (in terms of query cost) than an algorithm uniformly selecting pairs. Optimality here is with respect to the (complete) pairwise constraint penalty function.\nIn Section 2 we define our problem mathematically. We then present the \u03b5-SRRA method of Ailon et al. [2011] for the purpose of self containment in Section 3. Finally, we present our main result in Section 4."}, {"heading": "2 Notation and Definitions", "text": "Let V be a set of points of size n. Our goal is to partition V into k sets (clusters). There are two sources of information guiding us in the process. One is unsupervised, possibly emerging from features attached to each element v \u2208 V together with a chosen distance function. This information is captured in a utility function such as k-means or k-medians. The other type is supervised, and is encoded as an undirected graph G = (V,E). An edge (u, v) \u2208 E corresponds to the constraint u,v should be clustered together and a nonedge (u, v) 6\u2208 E corresponds to the converse. Each edge or nonedge comes at a query cost. This means that G exists only implicitly. We uncover the truth value of the predicate \u201c(u, v) \u2208 E\u201d for any chosen pairs u, v for a price. We also assume that G is riddled with human errors, hence it does not necessarily encode a perfect k clustering of the data. In what follows, we assume G fixed.\nA k-clustering C = {C1, . . . , Ck} is a collection of k disjoint (possibly empty) sets satisfying \u22c3\nCi = V . We use the notation u \u2261C v if u and v are in the same cluster, and u 6\u2261C v otherwise. The cost of C with respect to G is defined as\ncost(C) = \u2211\n(u,v)\u2208E\n1u 6\u2261Cv + \u2211\n(u,v)6\u2208E\n1u\u2261Cv .\nMinimizing cost(C) over clusterings when G is known as correlation clustering (in complete graphs). This problem was defined by Bansal et al. [2004] and has received much attention since (e.g. Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]). 1 Mitra and Samal [2009] achieved a PTAS for this problem, namely, a polynomial time algorithm returning a k-clustering with cost at most (1 + \u03b5) that of the optimal.2 Their PTAS is not query efficient: It requires knowledge of G in its entirety. In this work we study the query complexity required for achieving a (1 + \u03b5) approximation for cost. From a learning theoretical perspective, we want to find the best k-clustering explaining G using as few queries as possibly into G."}, {"heading": "3 The \u03b5-Smooth Relative Regret Approximation (\u03b5-SRRA)Method", "text": "Our search problem can be cast as a special case of the following more general learning problem. Given some possibly noisy structure (e.g. a graph in our case) h, the goal is to find the best explanation using a limited space X of hypothesis (in our case k-clusterings). The goal is to minimize a notion of a nonnegative cost which is defined as the distance d(f, h) between f \u2208 X and h. Assume also that the distance function d between X and h is an extension of a metric on\n1The original problem definition did not limit the number of output clusters. 2The polynomial degree depends on \u03b5.\nX . Ailon et al. [2011] have recently shown the following general scheme for finding the best f \u2208 X . To explain this scheme, we need to define a notion of \u03b5-smooth relative regret approximation.\nGiven a solution f \u2208 X (call it the pivotal solution) and another solution g \u2208 X , we define \u2206f (g) to be d(g, h) \u2212 d(f, h), namely, the difference between the cost of the solution g and the cost of the solution f . We call this the relative regret function with respect to f . Assume we have oracle access to a function \u2206\u0302f : X \u2192 R such that for all g \u2208 X ,\n|\u2206\u0302f (g) \u2212\u2206f (g)| \u2264 \u03b5d(f, g) .\nIf such an estimator function \u2206\u0302f exists, we say that it is an \u03b5-smooth regret approximation (\u03b5SRRA) for with respect to f . Ailon et al. [2011] show that if we have an \u03b5-smooth regret approximation function, then it is possible to obtain a (1 + \u03b5)-approximation to the optimal solution by repeating the iterative process presented in Figure 3. It is shown that this search algorithm converges exponentially fast to an (1+ \u03b5)-approximately optimal one. More precisely, the following is shown:\nTheorem 3.1. [Ailon et al., 2011] Assume input parameters \u03b5 \u2208 (0, 1/5) and initializer f\u03020 \u2208 X of Algorithm 3. Denote OPT := minf\u2208X d(f, h). f\u03020 \u2208 X be an arbitrary function. Then the following holds for f\u0302t obtained in Algorithm 3 for all t \u2265 1:\nd(f\u0302t, h) \u2264 (1 + 8\u03b5) ( 1 + (5\u03b5)t ) OPT+(5\u03b5)td(f\u03020, h). (3.1)\nThere are two questions now: (1) How can we build \u2206\u0302f efficiently? (2) How do we find argming\u2208X \u2206\u0302f (g)? In the case of k-clusterings, the target structure h is the graph G and X is the space of kclusterings over V . The metric d over X is taken to be\nd(C, C\u2032) = 1\n2\n\u2211\nu,v\ndu,v(C, C \u2032)\nwhere du,v(C, C \u2032) = 1u\u2261C\u2032v1u 6\u2261Cv +1u\u2261Cv1u 6\u2261C\u2032v. By defining d(C, G) := cost(C) we clearly extend d to a metric over X \u222a {G}.\n4 \u03b5-Smooth Regret Approximation for k-Correlation Clustering\nDenote costu,v(C) = 1(u,v)\u2208E1u 6\u2261Cv + 1(u,v)6\u2208E1u\u2261Cv , so that cost(C) = 1 2\n\u2211\nu,v costu,v(C). Now consider another clustering C\u2032. We are interested in the change in cost incurred by replacing C by\nC\u2032, in other words in the function f defined as\nf(C\u2032) = cost(C\u2032)\u2212 cost(C) .\nWe would like to be able to compute an approximation f\u0302 of f by viewing only a sample of edges in G. That is, we imagine that each edge query from G costs us one unit, and we would like to reduce that cost while sacrificing our accuracy as little as possible. We will refer to the cost incurred by queries as the query complexity. Consider the following metric on the space of clusterings:\nd(C, C\u2032) = 1\n2\n\u2211\nu,v\ndu,v(C, C \u2032)\nwhere du,v(C, C \u2032) = 1u\u2261\nC\u2032 v1u 6\u2261Cv + 1u\u2261Cv1u 6\u2261C\u2032v. (The distance function simply counts the number\nof unordered pairs on which C and C\u2032 disagree on.) Before we define our sampling scheme, we slightly reorganize the function f . Assume that |C1| \u2265 |C2| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |Ck|. Denote |Ci| by ni. The function f will now be written as:\nf(C\u2032) =\nk \u2211\ni=1\n\u2211\nu\u2208Ci\n\n\n1\n2\n\u2211\nv\u2208Ci\nfu,v(C \u2032) +\nk \u2211\nj=i+1\n\u2211\nv\u2208Cj\nfu,v(C \u2032)\n\n (4.1)\nwhere fu,v(C \u2032) = costu,v(C \u2032)\u2212 costu,v(C) .\nNote that fu,v(C \u2032) \u2261 0 whenever C and C\u2032 agree on the pair u, v. For each i \u2208 [k], let fi(C \u2032) denote the sum running over u \u2208 Ci in (4.1), so that f(C \u2032) = \u2211 fi(C \u2032). Similarly, we now rewrite d(C, C\u2032) as follows:\nd(C, C\u2032) =\nk \u2211\ni=1\n\u2211\nu\u2208Ci\n\n\nk \u2211\nj=i+1\n\u2211\nv\u2208Cj\n1u\u2261C\u2032v + 1\n2\n\u2211\nv\u2208Ci\n1u 6\u2261C\u2032v\n\n (4.2)\nand denote by di(C \u2032) the sum over u \u2208 Ci for i fixed in the last expression, so that d(C, C \u2032) = \u2211k\ni=1 di(C \u2032).\nOur sampling scheme will be done as follows. Let \u03b5 be an error tolerance function, which we set below. For each cluster Ci \u2208 C\n\u2032 and for each element u \u2208 Ci we will draw k\u2212 i+1 independent samples Sui, Su(i+1), . . . , Suk as follows. Each sample Suj is a subset of Cj of size q (to be defined below), chosen uniformly with repetitions from Cj . We will take\nq = c2k 2 log n/\u03b54 .\nwhere \u03b4 is a failure probability (to be used below), and c2 is a universal constant. Finally, we define our estimator f\u0302 of f to be:\nf\u0302(C\u2032) = 1\n2\nk \u2211\ni=1\n|Ci|\nq\n\u2211\nu\u2208Ci\n\u2211\nv\u2208Sui\nfu,v(C \u2032) +\nk \u2211\ni=1\n\u2211\nu\u2208Ci\nk \u2211\nj=i+1\n|Cj|\nq\n\u2211\nv\u2208Suj\nfu,v(C \u2032) .\nClearly for each C\u2032 it holds that f\u0302(C\u2032) is an unbiased estimator of f(C\u2032). We now analyze its error. For each i, j \u2208 [k] let Cij denote Ci \u2229 C \u2032 j. This captures exactly the set of elements in the i\u2019th cluster in C and the j\u2019th cluster in C\u2032. The distance d(C, C\u2032) can be written as follows:\nd(C, C\u2032) = 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\n|Cij \u00d7 (Ci \\ Cij)|+ k \u2211\nj=1\n\u2211\n1\u2264i1<i2\u2264k\n|Ci1j \u00d7 Ci2j | . (4.3)\nWe call each cartesian set product in (4.3) a distance contributing rectangle. Note that unless a pair (u, v) appears in one of the distance contributing rectangles, we have fu,v(C \u2032) = f\u0302u,v(C \u2032) = 0. Hence we can decompose f\u0302 and f in correspondence with the distance contributing rectangles, as follows:\nf(C\u2032) = 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\nFi,j(C \u2032) +\nk \u2211\nj=1\n\u2211\n1\u2264i1<i2\u2264k\nFi1,i2,j (4.4)\nf\u0302(C\u2032) = 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\nF\u0302i,j(C \u2032) +\nk \u2211\nj=1\n\u2211\n1\u2264i1<i2\u2264k\nF\u0302i1,i2,j (4.5)\nwhere\nFi,j(C \u2032) =\n\u2211\nu\u2208Cij\n\u2211\nv\u2208Ci\\Cij\nfu,v(C \u2032) (4.6)\nF\u0302i,j(C \u2032) =\n|Ci|\nq\n\u2211\nu\u2208Cij\n\u2211\nv\u2208(Ci\\Cij)\u2229Sui\nfu,v(C \u2032) (4.7)\nFi1,i2,j(C \u2032) =\n\u2211\nu\u2208Ci1j\n\u2211\nv\u2208Ci2j\nfu,v(C \u2032) (4.8)\nF\u0302i1,i2,j(C \u2032) =\n|Ci2 |\nq\n\u2211\nu\u2208Ci1j\n\u2211\nv\u2208Ci2j\u2229Sui2\nfu,v(C \u2032) (4.9)\n(Note that the Sui\u2019s are multisets, and the inner sums in (4.7) and (4.9) may count elements multiple times.)\nLemma 4.1. With probability at least 1 \u2212 n\u22123, the following holds simultaneously for all kclusterings C\u2032 and all i, j \u2208 [k]:\n|Fi,j(C \u2032)\u2212 F\u0302i,j(C \u2032)| \u2264 \u03b5 \u00b7 |Cij \u00d7 (Ci \\ Cij)| . (4.10)\nProof. Given a k-clustering C\u2032 = {C \u20321, . . . , C \u2032 k}, the predicate (4.10) (for a given i, j) depends only on the set Cij = Ci \u2229 C \u2032 j. Given a subset B \u2286 Ci, we say that C\n\u2032 (i, j)-realizes B if Cij = B. Now fix i, j and B \u2286 Ci. Assume a k-clustering (i, j)-realizes B. Let b = |B| and c = |Ci|. Consider the random variable F\u0302ij(C \u2032) (see (4.7)). Think of the sample Sui as a sequence Sui(1), . . . , Sui(q), where each Sui(s) is chosen uniformly at random from Ci for s = 1, . . . , q We can now rewrite F\u0302ij(C \u2032) as follows:\nF\u0302i,j(C \u2032) =\nc\nq\n\u2211\nu\u2208B\nq \u2211\ns=1\nX(Sui(s))\nwhere\nX(v) =\n{\nfu,v(C \u2032) v \u2208 Ci \\ Cij 0 otherwise .\nFor all s = 1, . . . q the random variable X(Sui(s)) is bounded by 2 almost surely, and its moments satisfy:\nE[X(Sui(s))] = 1\nc\n\u2211\nv\u2208(Ci\\Cij)\nfu,v(C \u2032)\nE[X(Sui(s)) 2] \u2264\n4(c\u2212 b)\nc . (4.11)\nFrom this we conclude using Bernstein inequality that for any t \u2264 b(c\u2212 b),\nPr[|F\u0302i,j(C \u2032)\u2212 Fi,j(C \u2032)| \u2265 t] \u2264 exp\n{\n\u2212 qt2\n16cb(c \u2212 b)\n}\nPlugging in t = \u03b5b(c\u2212 b), we conclude\nPr[|F\u0302i,j(C \u2032)\u2212 Fi,j(C \u2032)| \u2265 \u03b5b(c\u2212 b)] \u2264 exp\n{\n\u2212 q\u03b52b(c\u2212 b)\n16c\n}\nNow note that the number of possible sets B \u2286 Ci of size b is at most n min{b,c\u2212b}. Using union bound and recalling our choice of q, the lemma follows.\nThe Lemma can be easily proven using the Bernstein probability inequality. A bit more involved is the following:\nLemma 4.2. With probability at least 1 \u2212 n\u22123, the following holds simultaneously for all kclusterings C\u2032 and for all i1, i2, j \u2208 [k] with i1 < i2:\n|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| \u2264 \u03b5max\n{\n|Ci1j \u00d7 Ci2j |, |Ci1j \u00d7 (Ci1 \\ Ci1j)| k , |Ci2j \u00d7 (Ci2 \\ Ci2j)| k\n}\n(4.12)\nProof. Given a k-clustering C\u2032 = {C \u20321, . . . , C \u2032 k}, the predicate (4.12) (for a given i1, i2, j) depends only on the sets Ci1j = Ci1 \u2229 C \u2032 j and Ci2j = Ci2 \u2229 C \u2032 j . Given subsets B1 \u2286 Ci2 and B2 \u2286 Ci2 , we say that C\u2032 (i1, i2, j)-realizes (B1, B2) if Ci1j = B1 and Ci2j = B2. We now fix i1 < i2, j and B1 \u2286 Ci1 , B2 \u2286 Ci2 . Assume a k-clustering C\n\u2032 (i1, i2, j)-realizes (B1, B2). For brevity, denote b\u03b9 = |B\u03b9| and c\u03b9 = |Ci\u03b9 | for \u03b9 = 1, 2. Using Bernstein inequality as before, we conclude that\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > t] \u2264 exp\n{\n\u2212 c3t\n2q\nb1b2c2\n}\n. (4.13)\nfor any t in the range [0, b1b2], for some global c4 > 0. For t in the range (b1b2,\u221e),\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > t] \u2264 exp\n{\n\u2212 c5tq\nc2\n}\n. (4.14)\nWe consider the following three cases.\n1. b1b2 \u2265 max{b1(c1 \u2212 b1/k, b2(c2 \u2212 b2)/k}. Hence, b1 \u2265 (c2 \u2212 b2)/k, b2 \u2265 (c1 \u2212 b1)/k. In this case, plugging in (4.13) we get\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > \u03b5b1b2] \u2264 exp\n{\n\u2212 c3\u03b5\n2b1b2q\nc2\n}\n. (4.15)\nConsider two subcases. (i) If b2 \u2265 c2/2 then the RHS of (4.15) is at most exp { \u2212 c3\u03b5 2b1q 2 } . The number of sets B1, B2 of sizes b1, b2 respectively is clearly at most n b1+(c2\u2212b2) \u2264 nb1+kb1 . Therefore, if q = O(\u03b5\u22122k log n), then with probability at least 1 \u2212 n\u22126 simultaneously for all B1, B2 of sizes b1, b2 respectively and for all C\n\u2032 (i1, i2, j)-realizing (B1, B2) we have that |Fi1,i2,j(C \u2032) \u2212 F\u0302i1,i2,j(C \u2032)| \u2264 \u03b5b1b2 . (ii) If b2 < c2/2 then by our assumption, b1 \u2265 c2/2k. Hence the RHS of (4.15) is at most exp {\n\u2212 c3\u03b5 2b2q 2k\n}\n. The number of sets B1, B2 of sizes b1, b2\nrespectively is clearly at most n(c1\u2212b1)+c2 \u2264 nb2(1+k). Therefore, if q = O(\u03b5\u22122k2 log n), then with probability at least 1\u2212 n\u22126 simultaneously for all B1, B2 of sizes b1, b2 respectively and for all C\u2032 (i1, i2, j)-realizing (B1, B2) we have that |Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| \u2264 \u03b5b1b2 .\n2. b2(c2 \u2212 b2)/k \u2265 max{b1b2, b1(c1 \u2212 b1)/k}. We consider two subcases.\n(a) \u03b5b2(c2 \u2212 b2)/k \u2264 b1b2. Using (4.13), we get\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > \u03b5b2(c2 \u2212 b2)/k] \u2264 exp\n{\n\u2212 c3\u03b5\n2b2(c2 \u2212 b2) 2q\nk2b1c2\n}\n(4.16)\nAgain consider two subcases. (i) b2 \u2264 c2/2. In this case we conclude from (4.16)\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > \u03b5b2(c2 \u2212 b2)/k] \u2264 exp\n{\n\u2212 c3\u03b5\n2b2c2q\n4k2b1\n}\n(4.17)\nNow note that by assumption\nb1 \u2264 (c2 \u2212 b2)/k \u2264 c2/k \u2264 c1/k . (4.18)\nAlso by assumption, b1 \u2264 b2(c2 \u2212 b2)/(c1 \u2212 b1) \u2264 b2c2/(c1 \u2212 b1). Plugging in (4.18), we conclude that b1 \u2264 b2c2/(c1(1\u22121/k)) \u2264 2b2c2/c1 \u2264 2b2. From here we conclude that the RHS of (4.17) is at most exp {\n\u2212 c3\u03b5 22c2q 4k2\n}\n. The number of sets B1, B2 of sizes b1, b2 respec-\ntively is clearly at most nb1+b2 \u2264 n2b2+b2 \u2264 n3c2 . Hence, if q = O(\u03b5\u22122k2 log n) then with probability at least 1\u2212n\u22126 simultaneously for all B1, B2 of sizes b1, b2 respectively and for all C\u2032 (i1, i2, j)-realizing (B1, B2) we have that |Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| \u2264 \u03b5b2(c2\u2212b2)/k . In the second subcase (ii) b2 > c2/2. The RHS of (4.16) is at most exp {\n\u2212 c3\u03b5 2(c2\u2212b2)2q 2k2b1\n}\n.\nBy our assumption, (c2 \u2212 b2)/(kb1) \u2265 1, hence this is at most exp { \u2212 c3\u03b5 2(c2\u2212b2)q\n2k\n}\n.\nThe number of sets B1, B2 of sizes b1, b2 respectively is clearly at most n b1+(c2\u2212b2) \u2264 n(c2\u2212b2)/k+(c2\u2212b2) \u2264 n2(c2\u2212b2). Therefore, if q = O(\u03b5\u22122k log n), then with probability at least 1 \u2212 n\u22126 simultaneously for all B1, B2 of sizes b1, b2 respectively and for all C \u2032 (i1, i2, j)-realizing (B1, B2) we have that |Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| \u2264 \u03b5b2(c2 \u2212 b2)/k .\n(b) \u03b5b2(c2 \u2212 b2)/k > b1b2. We now use (4.14) to conclude\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > \u03b5b2(c2 \u2212 b2)/k] \u2264 exp\n{\n\u2212 c5\u03b5b2(c2 \u2212 b2)q\nkc2\n}\n(4.19)\nWe again consider the cases (i) b2 \u2264 c2/2 and (ii) b2 \u2265 c2/2 as above. In (i), we get that the RHS of (4.19) is at most exp {\n\u2212 c5\u03b5b2q2k\n}\n, that b1 \u2264 2b2 and hence the number\nof possibilities for B1, B2 is at most n b1+b2 \u2264 n3b2 . In (ii), we get that the RHS of (4.19) is at most exp {\n\u2212 c5\u03b5(c2\u2212b2)q2k\n}\n, and the number of possibilities for B1, B2 is at\nmost n2(c2\u2212b2). For both (i) and (ii) taking q = O(\u03b5\u22121k log n) ensures with probability at least 1 \u2212 n\u22126 simultaneously for all B1, B2 of sizes b1, b2 respectively and for all C \u2032 (i1, i2, j)-realizing (B1, B2) we have that |Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| \u2264 \u03b5b2(c2 \u2212 b2)/k .\n3. b1(c1 \u2212 b1)/k \u2265 max{b1b2, b2(c2 \u2212 b2)/k}. We consider two subcases.\n\u2022 \u03b5b1(c1 \u2212 b1)/k \u2264 b1b2. Using (4.13), we get\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > \u03b5b1(c1 \u2212 b1)/k] \u2264 exp\n{\n\u2212 c3\u03b5\n2b1(c1 \u2212 b1) 2q\nk2b2c2\n}\n. (4.20)\nAs before, consider case (i) in which b2 \u2264 c2/2 and (ii) in which b2 \u2265 c2/2. For case (i), we notice that the RHS of (4.19) is at most exp { \u2212 c3\u03b5 2b2(c2\u2212b2)(c1\u2212b1)q\nk2b2c2\n}\n(we used the fact\nthat b1(c1\u2212 b1) \u2265 b2(c2\u2212 b2) by assumption). This is hence at most exp { \u2212 c3\u03b5 2(c1\u2212b1)q 2k2 } . The number of possibilities of B1, B2 of sizes b1, b2 is clearly at most n (c1\u2212b1)+b2 \u2264 n(c1\u2212b1)+(c1\u2212b1)/k \u2264 n2(c1\u2212b1). From this we conclude that q = O(\u03b5\u22122k2 log n) suffices for this case. For case (ii), we bound the RHS of (4.20) by exp { \u2212 c3\u03b5 2b1(c1\u2212b1)2q\n2k2b2 2\n}\n.\nUsing the assumption that (c1 \u2212 b1)/b2 \u2265 k, the latter expression is upper bounded by exp {\n\u2212 c3\u03b5 2b1q 2\n}\n. Again by our assumptions,\nb1 \u2265 b2(c2 \u2212 b2)/(c1 \u2212 b1) \u2265 (\u03b5(c1 \u2212 b1)/k)(c2 \u2212 b2)/(c1 \u2212 b1) = \u03b5(c2 \u2212 b2)/k . (4.21)\nThe number of possibilities of B1, B2 of sizes b1, b2 is clearly at most n b1+(c2\u2212b2) which by (4.21) is bounded by nb1+kb1/\u03b5 \u2264 n2kb1/\u03b5. From this we conclude that q = O(\u03b5\u22123k log n) suffices for this case.\n\u2022 \u03b5b1(c1 \u2212 b1)/k > b1b2.\nPr[|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)| > \u03b5b1(c2 \u2212 b1)/k] \u2264 exp\n{\n\u2212 c5\u03b5b1(c1 \u2212 b1)q\nkc2\n}\n(4.22)\nWe consider two sub-cases, (i) b1 \u2264 c1/2 and (ii) b1 > c1/2. In case (i), we have that\nb1(c1 \u2212 b)\nc2 =\n1\n2\nb1(c1 \u2212 b)\nc2 +\n1\n2\nb1(c1 \u2212 b)\nc2\n\u2265 1\n2 b1c1 2c2 + 1 2 b2(c2 \u2212 b2)\nc2 \u2265 b1/4 +min{b2, c2 \u2212 b2}/2 .\nHence, the RHS of (4.22) is bounded above by exp {\n\u2212 c5\u03b5q(b1/4+min{b2,c2\u2212b2}/2)k\n}\n. The\nnumber of possibilities of B1, B2 of sizes b1, b2 is clearly at most n b1+min{b2,c2\u2212b2}, hence it suffices to take q = O(\u03b5\u22121k log n) for this case. In case (ii), we can upper bound the RHS of (4.22) by exp {\n\u2212 c5\u03b5c1(c1\u2212b1)q2kc2\n} \u2265 exp {\n\u2212 c5\u03b5(c1\u2212b1)q2k\n}\n. The number of possi-\nbilities of B1, B2 of sizes b1, b2 is clearly at most n (c1\u2212b1)+b2 which, using our assumptions, is bounded above by n(c1\u2212b1)+(c1\u2212b1)/k \u2264 n2(c1\u2212b2). Hence, it suffices to take q = O(\u03b5\u22121k log n) for this case.\nThis concludes the proof of the lemma.\nAs a consequence, we get the following:\nLemma 4.3. with probability at least 1\u2212n\u22123, the following holds simultaneously for all k-clusterings C\u2032:\n|f(C\u2032)\u2212 f(C)| \u2264 3\u03b5d(C\u2032, C) .\nProof.\n|f(C\u2032)\u2212 f\u0302(C\u2032)| = 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\n|Fi,j(C \u2032)\u2212 F\u0302i,j(C \u2032)|+\nk \u2211\nj=1\n\u2211\n1\u2264i1<i2\u2264k\n|Fi1,i2,j(C \u2032)\u2212 F\u0302i1,i2,j(C \u2032)|\n\u2264 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\n\u03b5\u22122k\u22121|Cij \u00d7 (Ci \\ Cij)|\n+ \u03b5\nk \u2211\nj=1\n\u2211\ni1<i2\n(\n|Ci1j \u00d7 Ci2j|+ k \u22121|Ci1j \u00d7 (Ci1 \\ Ci1j)|+ k \u22121|Ci2j \u00d7 (Ci2 \\ Ci2j)| )\n\u2264 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\n\u03b5\u22122k\u22121|Cij \u00d7 (Ci \\ Cij)|+ \u03b5\nk \u2211\nj=1\n\u2211\ni1<i2\n|Ci1j \u00d7 Ci2j |\n+ \u03b5\nk \u2211\nj=1\nk \u2211\ni1=1\nk \u2211\ni2=1\nk\u22121|Ci1j \u00d7 (Ci1 \\ Ci1j)|+ \u03b5\nk \u2211\nj=1\nk \u2211\ni2=1\nk \u2211\ni1=1\nk\u22121|Ci2j \u00d7 (Ci2 \\ Ci2j)|\n\u2264 1\n2\nk \u2211\ni=1\nk \u2211\nj=1\n\u03b5\u22122k\u22121|Cij \u00d7 (Ci \\ Cij)|+ \u03b5 k \u2211\nj=1\n\u2211\ni1<i2\n|Ci1j \u00d7 Ci2j |\n+ \u03b5 k \u2211\nj=1\nk \u2211\ni1=1\nkk\u22121|Ci1j \u00d7 (Ci1 \\ Ci1j)|+ \u03b5 k \u2211\nj=1\nk \u2211\ni2=1\nkk\u22121|Ci2j \u00d7 (Ci2 \\ Ci2j)|\n\u2264 \u03b5 3\n2\nk \u2211\ni=1\nk \u2211\nj=1\n|Cij \u00d7 (Ci \\ Cij)|+ \u03b5 k \u2211\nj=1\n\u2211\ni1<i2\n|Ci1j \u00d7 Ci2j|\n\u2264 3\u03b5d(C, C\u2032)\nThe first equality was (4.4)-(4.5). The second was Lemmas 4.1- 4.2 (assuming success of a high probability event), the third, fourth and fifth inequalities were rearrangement of the sum, and the final inequality came from (4.3)."}, {"heading": "5 Conclusions and Future Work", "text": "Our study considered the information theoretical problem of choosing which questions to ask in a game in which adversarially noisy combinatorial pairwise information is input to a clustering algorithm. We designed and analyzed a distribution from which drawing pairs is provably superior than the uniform distribution. Our analysis did not take into account geometric information (e.g. a feature vector attached to each data point) and treated the similarity labels as side information, as suggested in a recent line of literature. It would be interesting to study our solution in conjunction with geometric information. It would also be interesting to study our approach in the context of metric learning, where the goal is to cleverly choose which pairs to obtain (noisy) distance labels for."}], "references": [{"title": "A new active learning scheme with applications", "author": ["Nir Ailon", "Ron Begleiter", "Esther Ezra"], "venue": "J. ACM,", "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "Semi-supervised clustering using genetic algorithms", "author": ["Ayhan Demiriz", "Kristin Bennett", "Mark J. Embrechts"], "venue": "Artificial Neural Networks in Engineering", "citeRegEx": "Demiriz et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Demiriz et al\\.", "year": 1999}, {"title": "Correlation clustering with a fixed number of clusters", "author": ["Ioannis Giotis", "Venkatesan Guruswami"], "venue": "Theory of Computing,", "citeRegEx": "Giotis and Guruswami.,? \\Q2006\\E", "shortCiteRegEx": "Giotis and Guruswami.", "year": 2006}, {"title": "From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering", "author": ["Dan Klein", "Sepandar D. Kamvar", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "Approximation algorithm for correlation clustering", "author": ["P. Mitra", "M. Samal"], "venue": "In Networked Digital Technologies,", "citeRegEx": "Mitra and Samal.,? \\Q2009\\E", "shortCiteRegEx": "Mitra and Samal.", "year": 2009}, {"title": "Cluster graph modification problems", "author": ["Ron Shamir", "Roded Sharan", "Dekel Tsur"], "venue": "Discrete Applied Math,", "citeRegEx": "Shamir et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2004}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": ", 2002] or cluster editing [Shamir et al., 2004].", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006].", "startOffset": 84, "endOffset": 112}, {"referenceID": 0, "context": "1 Previous Related Work Clustering with side information is a fairly new variant of clustering first described, independently, by Demiriz et al. [1999], and Ben-Dor et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 0, "context": "1 Previous Related Work Clustering with side information is a fairly new variant of clustering first described, independently, by Demiriz et al. [1999], and Ben-Dor et al. [1999]. In the machine learning community it is also widely known as semi-supervised clustering.", "startOffset": 130, "endOffset": 179}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al.", "startOffset": 106, "endOffset": 518}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al.", "startOffset": 106, "endOffset": 539}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints.", "startOffset": 106, "endOffset": 563}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints. Basu [2005] in his thesis, which also serves as a comprehensive survey, has championed this approach in conjunction with k-means, and hidden Markov random field clustering algorithms.", "startOffset": 106, "endOffset": 732}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints. Basu [2005] in his thesis, which also serves as a comprehensive survey, has championed this approach in conjunction with k-means, and hidden Markov random field clustering algorithms. 1.2 Our Contribution Our main motivation is reducing the number of pairwise similarity labels (query cost) required for k-clustering data using an active learning approach. More precisely, we ask how to choose which pairs of data points to query. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. More precisely, we show that the distribution from which we should draw pairs from must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, let alone the bias. Using the recently introduced method of \u03b5-smooth relative regret approximations (\u03b5-SRRA) of Ailon et al. [2011] we can show an iterative process that improves both the clustering 1", "startOffset": 106, "endOffset": 1631}, {"referenceID": 0, "context": "We then present the \u03b5-SRRA method of Ailon et al. [2011] for the purpose of self containment in Section 3.", "startOffset": 37, "endOffset": 57}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]).", "startOffset": 0, "endOffset": 44}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]).", "startOffset": 0, "endOffset": 68}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]). 1 Mitra and Samal [2009] achieved a PTAS for this problem, namely, a polynomial time algorithm returning a k-clustering with cost at most (1 + \u03b5) that of the optimal.", "startOffset": 0, "endOffset": 95}, {"referenceID": 0, "context": "Ailon et al. [2011] have recently shown the following general scheme for finding the best f \u2208 X .", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Ailon et al. [2011] have recently shown the following general scheme for finding the best f \u2208 X . To explain this scheme, we need to define a notion of \u03b5-smooth relative regret approximation. Given a solution f \u2208 X (call it the pivotal solution) and another solution g \u2208 X , we define \u2206f (g) to be d(g, h) \u2212 d(f, h), namely, the difference between the cost of the solution g and the cost of the solution f . We call this the relative regret function with respect to f . Assume we have oracle access to a function \u2206\u0302f : X \u2192 R such that for all g \u2208 X , |\u2206\u0302f (g) \u2212\u2206f (g)| \u2264 \u03b5d(f, g) . If such an estimator function \u2206\u0302f exists, we say that it is an \u03b5-smooth regret approximation (\u03b5SRRA) for with respect to f . Ailon et al. [2011] show that if we have an \u03b5-smooth regret approximation function, then it is possible to obtain a (1 + \u03b5)-approximation to the optimal solution by repeating the iterative process presented in Figure 3.", "startOffset": 0, "endOffset": 727}], "year": 2012, "abstractText": "Clustering is considered a non-supervised learning setting, in which the goal is to partition a collection of data points into disjoint clusters. Often a bound k on the number of clusters is given or assumed by the practitioner. Many versions of this problem have been defined, most notably k-means and k-median. An underlying problem with the unsupervised nature of clustering it that of determining a similarity function. One approach for alleviating this difficulty is known as clustering with side information, alternatively, semi-supervised clustering. Here, the practitioner incorporates side information in the form of \u201cmust be clustered\u201d or \u201cmust be separated\u201d labels for data point pairs. Each such piece of information comes at a \u201cquery cost\u201d (often involving human response solicitation). The collection of labels is then incorporated in the usual clustering algorithm as either strict or as soft constraints, possibly adding a pairwise constraint penalty function to the chosen clustering objective. Our work is mostly related to clustering with side information. We ask how to choose the pairs of data points. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. Roughly speaking, we show that the distribution must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, hence we don\u2019t know the bias. Using the recently introduced method of \u03b5-smooth relative regret approximations of Ailon, Begleiter and Ezra, we can show an iterative process that improves both the clustering and the bias in tandem. The process provably converges to the optimal solution faster (in terms of query cost) than an algorithm selecting pairs uniformly.", "creator": "LaTeX with hyperref package"}}}