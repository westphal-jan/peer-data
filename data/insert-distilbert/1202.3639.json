{"id": "1202.3639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2012", "title": "Finding a most biased coin with fewest flips", "abstract": "we immediately study the problem of learning the most biased finite coin loops among a set sample of lottery coins by tossing the coin coins intentionally adaptively. the goal is to minimize the number of tosses to rapidly identify a coin i * such that prob { coin i * is most biased } is possible at least 1 - \\ delta \\ f for any given \\ delta & gt ; 0. under a remarkably particular general probabilistic model, we give an optimal algorithm, i. { e., an algorithm that minimizes the expected number of tosses, to learn a most biased coin. the problem is equivalent to finding the best arm in the multi - armed bandit problem using adaptive strategies. dar et al. ( 2002 ) and mannor cohen and tsitsiklis ( 2004 ) show upper and lower bounds matching up to constant factors on the number of additional coin tosses for several appropriate underlying settings of the bias probabilities. for a class of such settings we bridge the constant factor gap by giving an optimal adaptive gambling strategy - - a performance strategy that performs the best possible action under any given iteration history of outcomes. for any given history, tossing the specific coin accordingly chosen by our strategy minimizes the expected number of tosses needed to learn learning a most biased coin. sum to our knowledge, this is the first algorithm that employs an optimal adaptive strategy feasible under a bayesian trial setting for this discrimination problem.", "histories": [["v1", "Thu, 16 Feb 2012 16:40:56 GMT  (67kb,D)", "https://arxiv.org/abs/1202.3639v1", null], ["v2", "Mon, 13 Aug 2012 14:53:35 GMT  (69kb,D)", "http://arxiv.org/abs/1202.3639v2", null], ["v3", "Sat, 7 Sep 2013 17:09:32 GMT  (71kb,D)", "http://arxiv.org/abs/1202.3639v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["karthekeyan chandrasekaran", "richard karp"], "accepted": false, "id": "1202.3639"}, "pdf": {"name": "1202.3639.pdf", "metadata": {"source": "CRF", "title": "Finding a most biased coin with fewest flips", "authors": ["Karthekeyan Chandrasekaran", "Richard Karp"], "emails": ["karthe@seas.harvard.edu,", "karp@icsi.berkeley.edu,"], "sections": [{"heading": null, "text": "whose posterior probability of being most biased is at least 1 \u2212 \u03b4 for a given \u03b4. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy \u2013 a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs tools from the field of Markov games."}, {"heading": "1 Introduction", "text": "The multi-armed bandit problem is a classical decision-theoretic problem with applications in bioinformatics, medical trials, stochastic algorithms, etc. [18]. The input to the problem is a set of arms, each associated with an unknown stochastic reward. At each step, an agent chooses an arm and receives a reward. The objective is to find a strategy for choosing the arms in order to achieve the best expected reward asymptotically. This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].\nThe motivation to identify the best bandit arm arises from problems where one would like to minimize regret within a fixed budget. In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret. Here regret is defined to be the difference between the expected reward of the chosen arm and the expected reward of the optimal arm. The work of [8] suggested that the exploration-exploitation trade offs for this setting are much different from the setting where the number of steps is asymptotic. Following this, Audibert et al. [1] proposed exploration strategies to perform essentially as well as the best strategy that knows all distributions up to permutations of the arms. Gabillon et al. [17] addressed the problem of identifying the best arm for each bandit among a collection of bandits within a fixed budget. They proposed strategies that focus on arms whose expected rewards are closer to that of the optimal arm and show an upper bound on the probability of error for these strategies that decreases exponentially with the number of steps allowed.\n\u2217karthe@seas.harvard.edu, Harvard University. This work was done while the author was a visiting researcher at ICSI, Berkeley. \u2020karp@icsi.berkeley.edu, University of California, Berkeley.\nar X\niv :1\n20 2.\n36 39\nv3 [\ncs .D\nS] 7\nS ep\n2 01\n3\nIn contrast, one could also attempt to optimize the budget subject to the quality of the arm to be identified. This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed. This PAC-style learning formulation was introduced by Even-Dar et al. [14]. Given a collection of n arms, Even-Dar et al. [14] showed that a total of O((n/ 2) log(1/\u03b4)) steps is sufficient to identify an arm whose expected reward is at most away from the optimal arm with correctness at least 1 \u2212 \u03b4. Mannor and Tsitsiklis [22] showed lower bounds matching up to constant factors under various settings of the rewards. We attempt to bridge the constant factor gap by addressing the problem from a decision-theoretic perspective. Given the history of outcomes, does there exist a strategy to choose an arm so that the expected number of steps needed to learn the best arm is minimized? Our notion of learning the best arm is to identify an arm whose posterior probability of being the most-rewarding is at least 1\u2212 \u03b4.\nAlthough the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d. It was introduced for normally distributed rewards by Bechhofer [4]. Adaptive strategies for this problem, known as \u201csequential selection\u201d, can be traced back to Paulson [24]. Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25]. A simple and interesting case of the problem is when the most rewarding arm and the second-most rewarding arm differ in their mean rewards by at least > 0. This special case is known as the \u201cindifference-zone\u201d assumption [4]. Strategies and their measure of optimality are known for various relaxations of independence, normality, equal and known variances and indifference-zone assumptions [20]. In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9]. In this work, we address a particular Bayesian setting for Bernoulli rewards satisfying the indifference-zone assumption.\nIf the rewards from the bandit arms are Bernoulli, then learning the arm with the maximum expected reward is equivalent to learning the most biased coin by tossing them adaptively. So, we will focus on this problem for the rest of the paper. Under the indifference zone assumption, Chernoff bound leads to a trivial upper bound on the number of tosses in the non-adaptive setting \u2013 toss each coin (4/ 2) log (n/\u03b4) times and output the coin with the maximum number of heads outcomes. Let p\u0302i denote the empirical probability of heads for the ith coin. By Chernoff bound, |p\u0302i\u2212 pi| \u2264 /2 with probability at least 1\u2212 \u03b4/n. Therefore, by the union bound, it follows that this trivial toss-each-coin-k-times strategy outputs the most biased coin with probability at least 1\u2212 \u03b4.\nIn this work, we give a simple yet optimal strategy for choosing coins to toss in a particular Bayesian setting. Our strategy is optimal in the sense that given a current history of outcomes of all coins and a threshold, it minimizes the expected number of tosses to find a coin whose posterior probability of being a most-biased coin is at or above the threshold. Our main contribution is a proof of optimality by employing tools from the field of Markov games. We also bound the expected number of coin tosses performed by our strategy. To the best of our knowledge, this is the first provably optimal strategy under a Bayesian setting of the problem with indifference zone assumption.\nSetting. A coin is said to be heavy if the probability of heads for the coin is p+ and not-heavy if the heads probability is p\u2212 for some given \u2208 (0, 1/2) and p \u2208 [ , 1\u2212 ]. We are given an infinite\ncollection of coins where each coin in the collection is heavy with probability \u03b1 and not-heavy with probability 1 \u2212 \u03b1. Given \u03b4 > 0, the algorithm is allowed to toss coins adaptively and has to necessarily perform a coin toss until it identifies a coin whose posterior probability of being heavy is at least 1 \u2212 \u03b4 (i.e., a coin i for which Pr (Coin i is heavy | Outcomes all coin tosses) \u2265 1 \u2212 \u03b4). The goal is to minimize the expected number of tosses required.\nAn adaptive strategy is allowed to choose which coin to toss after observing the history of outcomes of all previous coin tosses. Given the history of outcomes of coin tosses, the cost of an adaptive strategy is equal to the expected number of future coin tosses needed by following this strategy so that it identifies a coin whose posterior probability of being heavy is at least 1\u2212 \u03b4. An adaptive strategy is said to be optimal if it has the minimum cost."}, {"heading": "1.1 Results", "text": "Our main result is an optimal adaptive algorithm for the above setting.\nTheorem 1. Given \u03b4 > 0, there exists an algorithm A that employs an optimal adaptive strategy in tossing coins to identify a coin whose posterior probability of being heavy is at least 1 \u2212 \u03b4. At any step, the time taken by A to identify the coin to toss is O(1).\nWe also quantify the number of tosses performed by our optimal adaptive algorithm. We assume an infinite supply of coins under the same probabilistic setting. Let q := 1 \u2212 p, \u2206H := log ((p+ )/(p\u2212 )), \u2206T := log ((q + )/(q \u2212 )), B(\u03b4) := log ((1\u2212 \u03b1)(1\u2212 \u03b4)/\u03b1\u03b4). Let \u03b40 be determined as follows: Consider the unique real value \u03c1 \u2208 (0, 1) such that \u03c1\u2206H (p+ ) + \u03c1\u2212\u2206T (q\u2212 ) = 1 (the existence and uniqueness of \u03c1 is elaborated in Section 5). Fix \u03b40 to be the largest real value such that (1\u2212 \u03c1B(\u03b4)+\u2206H )/(1\u2212 \u03c1B(\u03b4)+\u2206T ) < 2 and B(\u03b4) \u2265 \u2206H .\nTheorem 2. For every \u03b4 \u2208 (0, \u03b40], the expected number of tosses performed by A to identify a coin whose posterior probability of being heavy is at least 1\u2212 \u03b4 in the above setting, is at most\n16\n2 ( 1\u2212 \u03b1 \u03b1 + log ( (1\u2212 \u03b1)(1\u2212 \u03b4) \u03b1\u03b4 )) .\nThe implications of our upper bound when the number of coins is bounded but much larger than 1/\u03b1 needs to be contrasted with the lower bounds by [22]. In this case, setting n = c/\u03b1 in the above expression suggests that our algorithm beats the lower bound shown in Theorem 9 of [22]. We observe that Theorem 9 of [22] shows a lower bound in the most general Bayesian setting \u2013 there exists a prior distribution of the probabilities of the n coins so that any algorithm requires at least O((n/ 2) log (1/\u03b4)) tosses in expectation. However, our algorithm works in a particular Bayesian setting by exploiting prior knowledge about this setting."}, {"heading": "1.2 Algorithm", "text": "At any stage of the algorithm, let the history of outcomes of a coin i be given by Di := (hi, ti) where hi and ti refer to the number of outcomes that were heads and tails respectively. Given the history Di, we define the likelihood ratio of the coin to be\nLi := Pr (Coin i is heavy|Di)\nPr (Coin i is not-heavy|Di) =\n( p+\np\u2212 )hi (q \u2212 q + )ti .\nAlgorithm Likelihood-Toss\n1. Initialize Li = 1 for the i\u2019th coin.\n2. While (Li < (1\u2212 \u03b1)(1\u2212 \u03b4)/\u03b1\u03b4 \u2200 i \u2208 [n])\n(a) Toss coin i\u2217 such that i\u2217 = arg max{Li : i \u2208 [n]}. (Break ties arbitrarily). Let\nbi\u2217 =\n{ 1 if outcome is heads,\n0 if outcome is tails.\n(b) Update Li\u2217 \u2190 Li\u2217 ( p+ p\u2212 )bi\u2217 (1\u2212p\u2212 1\u2212p+ )1\u2212bi\u2217 .\n3. Output the coin i with maximum Li."}, {"heading": "2 Preliminaries", "text": "Our proof of optimality is based on an optimal strategy for multitoken Markov games. We now formally define the multitoken Markov game and state the optimal strategy that has been studied for this game. We use the notation and results from [12].\nA Markov system S = (V, P,C, s, t) consists of a state space V , a transition probability function P : V \u00d7V \u2192 [0, 1], a positive real cost Cv associated with each state v, a start state s and a target state t. Let v(0), v(1), . . . , v(k) denote a set of states taken by following the Markov system for k steps. The cost of such a trip on S is the sum \u2211k\u22121 i=0 Cv(i) of the costs of the exited states.\nLet S1, . . . , Sn be n Markov systems, each of which has a token on its starting state. A simple multitoken Markov game G = S1 \u25e6 S2 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 Sn consists of a succession of steps in which we choose one of the n tokens, which takes a random step in its system (i.e., according to its Pi). After choosing a token i on state u say, we pay the cost Ci(u) associated with the state u of the system Si. We terminate as soon as one of the tokens reaches its target state for the first time. A strategy denotes the policy employed to pick a token given the state of the n Markov systems. The cost of such a game E[G] is the minimum expected cost taken over all possible strategies. The strategy that achieves the minimum expected cost is said to be optimal. A strategy is said to be pure if the choice of the token at any step is deterministic (entirely determined by the state of all Markov systems).\nTheorem 3. [12] Every Markov game has a pure optimal strategy.\nFor any strategy \u03c0 for a Markov game G, we denote the expected cost incurred by playing \u03c0 on G by E\u03c0[G].\nThe pure optimal strategy in the multitoken Markov game is completely determined by the grade \u03b3 of the states of the systems. The grade \u03b3 of a state is defined as follows: Given a Markov system S = (V, P,C, s, t) and state u, let S(u) = (V, P,C, u, t) denote the Markov system whose starting state is u. Consider the Markov game Sg(u) \u2013 where at any step of the game one is allowed to either play in S(u) or quit. Quitting incurs a cost of g. Playing in S(u) is equivalent to taking a step following the Markov system S incurring the cost associated with the state of\nthe system. The game stops once the target state is reached or once we quit. The grade \u03b3(u) of state u is defined to be the smallest real value g such that there exists an optimal strategy \u03c3 that plays in S(u) in the first step. We note that, by definition, the cost of the game S\u03b3(u)(u) is E[S\u03b3(u)(u)] = \u03b3(u) = E\u03c3[S\u03b3(u)(u)].\nTheorem 4. [12] Given the states u1, . . . , un of the Markov systems in the multitoken Markov game, the unique optimal strategy is to pick the token i such that \u03b3(ui) is minimal.\nWe observe that the above results can be extended in a straightforward manner to the case where (1) the number of Markov systems is countably infinite, i.e., n = \u221e and (2) the Markov systems have infinite state space but all states are locally finite (i.e., the number of possible transitions from any fixed state is finite), by working through the proofs in [12]. The Markov systems that will be considered for our purpose will satisfy these two properties.\nWe use the following results from [13] to bound the number of tosses.\nTheorem 5. [13] Let X \u2208 [\u2212\u03bd, \u00b5] be the random variable that determines the step-sizes of a one dimensional random walk with absorbing barriers at \u2212L and W such that Pr (X > 0) > 0, Pr (X < 0) > 0, E (X) 6= 0. Let L\u2217 = L+ \u03bd, W \u2217 = W + \u00b5 and \u03c6(\u03c1) := E ( \u03c1X ) .\n1. The function \u03c6(\u03c1) is convex. If E (X) 6= 0, there exists a unique \u03c10 \u2208 (0, 1)\u222a (1,\u221e) such that \u03c6(\u03c10) = 1. If E (X) < 0, then \u03c10 > 1 and if E (X) > 0, then \u03c10 < 1.\n2.\nPr (Absorption at W ) \u2265 1\u2212 \u03c1 L 0\n1\u2212 \u03c1L+W \u22170 .\n3. If E (X) < 0, then\nE (Number of steps to absorption) \u2264 L \u2217\n|E (X)| .\n4. If E (X) > 0, then\nE (Number of steps to absorption) \u2264 (L+W \u2217)\nE (X)\n( 1\u2212 \u03c1L\u22170\n1\u2212 \u03c1L\u2217+W0\n) ."}, {"heading": "3 Correctness", "text": "We first argue the correctness of the algorithm.\nLemma 6. Given the history Di for a coin i, Pr (Coin i is heavy|Di) \u2265 1\u2212 \u03b4 if and only if Li \u2265 (\n1\u2212 \u03b4 \u03b4 )( 1\u2212 \u03b1 \u03b1 ) .\nProof. The lemma is a straightforward application of Bayes\u2019 theorem.\nPr (Coin i is heavy|Di) = Pr (Di|Coin i is heavy)Pr (Coin i is heavy)\nPr (Di)\n= \u03b1(p+ )hi(q \u2212 )ti\n\u03b1(p+ )hi(q \u2212 )ti + (1\u2212 \u03b1)(p\u2212 )hi(q + )ti\n= \u03b1Li\n\u03b1Li + (1\u2212 \u03b1) .\nThus, it follows that Pr (Coin i is heavy|Di) \u2265 1\u2212 \u03b4 if and only if Li \u2265 (\n1\u2212 \u03b4 \u03b4 )( 1\u2212 \u03b1 \u03b1 ) .\nThe algorithm computes the likelihood ratio Li for each coin i based on the history of outcomes of the coin. The algorithm repeatedly tosses coins until there exists i\u2217 such that Li\u2217 \u2265 (1\u2212\u03b1)(1\u2212\u03b4)/\u03b1\u03b4. Thus, if i\u2217 is output by Algorithm Likelihood-Toss, then\nPr (Coin i\u2217 is heavy|Di\u2217) \u2265 1\u2212 \u03b4."}, {"heading": "4 Optimality of the Algorithm", "text": "Consider the log-likelihood of a coin i defined as Xi := logLi. Given the history of a coin, the log-likelihood of the coin is determined uniquely. In the beginning, the history is empty and hence all log-likelihoods are identically zero. The influence of a toss on the log-likelihood is a random step for Xi \u2013 if the outcome of the toss is a head, then Xi \u2190 Xi + \u2206H and if the outcome is a tail, then Xi \u2190 Xi \u2212 \u2206T . Thus, the toss outcomes of the coin leads to a 1-dimensional random-walk of the log-likelihood function associated with the coin. Further, since we stop tossing as soon as the log-likelihood of a coin is greater than B = log (1\u2212 \u03b1)(1\u2212 \u03b4)/\u03b1\u03b4, the random-walk has an absorbing barrier at B. We observe that the random walks performed by the coins are independent of each other since each coin being heavy is independent of the rest of the coins.\nThus, we have infinitely many identical Markov systems S1, S2, . . . , with each one starting in state Xi = 0. Each Markov system also has a target state, namely the boundary B. A strategy to pick a coin to toss is equivalent to picking a Markov system i. Each toss outcome is equivalent to the corresponding system taking a step following the transition probability and step size of the system. The goal to minimize the expected number of future tosses is equivalent to minimizing the expected number of steps for one of the Markov systems to reach the target state.\nTherefore, we are essentially seeking an optimal strategy to play a multitoken Markov game. We show that the strategy employed by Algorithm Likelihood-Toss is an optimal strategy to play the multitoken Markov game that arises in our setting.\nLet the Markov system associated with the one-dimensional random walk of the log-likelihood function of the history of the coin be S = (V, P,C, s, t). Here, the state space V consists of every possible real value that is at most B. The target state is a special state determined by t = B. The starting state is s = 0. Given the current state X, the transition cost incurred is one while transition probabilities are defined as follows:\nX \u2192 { min{X + \u2206H , B} with probability Pr (Heads|X), X \u2212\u2206T with probability 1\u2212 Pr (Heads|X)\nwhere\nPr (Heads|X) = Pr (Heads|Heavy coin)Pr (Heavy coin|X) + Pr (Heads|Non-heavy coin)Pr (Non-heavy coin|X)\n= (p+ )\u03b1eX\n\u03b1eX + (1\u2212 \u03b1) + (p\u2212 )(1\u2212 \u03b1) \u03b1eX + (1\u2212 \u03b1) .\nWe observe that the transition probabilities in this random-walk vary with the state of the system (as opposed to the well-known random-walk under uniform transition probability). It is clear that this Markov system is locally finite \u2013 the number of possible states reachable using one transition from any fixed state is only two. In this modeling of the Markov System for the log-likelihood of each coin, we do not condition on the coin being heavy or not-heavy. We are postponing this decision by conditioning based on the history."}, {"heading": "4.1 Proof of Optimality", "text": "We now show that the grade is a monotonically non-increasing function of the log-likelihood.\nLemma 7. Consider the Markov System S = (V, P,C, s, t) associated with the log-likelihood function. Let X,Y \u2208 V such that X \u2265 Y . Then \u03b3(X) \u2264 \u03b3(Y ).\nProof. Let \u03b3(Y ) = g. Then, by definition of grade, it follows that there exists a pure optimal strategy \u03c3 that chooses to toss the coin in the first step in Sg(Y ) and E\u03c3[Sg(Y )] = g. We will specify a mixed strategy \u03c0 for Sg(X) such that E\u03c0[Sg(X)] \u2264 g and \u03c0 chooses to play in the system S(X) in the first step. It follows by definition that \u03b3(X) \u2264 g.\nThe pure strategy \u03c3 can be expressed by a (possibly infinite) binary decision tree D\u03c3 as follows: Each node u has an associated label l(u) \u2208 R. Each edge has a label from {H,T}. The root node v is labeled l(v) = Y . On reaching l(u) < B, if \u03c3 chooses to play in the system, then u has two children - the left and right children uL, uR are labeled l(uL) = l(u) + \u2206H and l(uR) = l(u)\u2212\u2206T respectively. The edges (u, uL), (u, uR) are labeled H and T respectively. On reaching l(u) < B, if \u03c3 decides to quit, then u is a leaf node. Finally, if l(u) \u2265 B, then u is a leaf node. We observe that since \u03c3 plays in the system Sg(Y ) in the first step, the root of D\u03c3 is not a leaf. (See Figure 1 for an example.)\nWe obtain a mixed strategy \u03c0 for Sg(X) by considering the following ternary tree D\u03c0 derived from D\u03c3: Each node u in D\u03c0 has an associated label (lX(u), lY (u)) \u2208 R2. Each edge in D\u03c0 has a label from {HH,HT, TT}. There is an onto mapping m(u) from each node u \u2208 D\u03c0 to a node in D\u03c3. The root node u is labeled (lX(u) = X, lY (u) = Y ) and m(u) =Root(D\u03c3). For any node u, if m(u) is a leaf, then u is a leaf. Let u be a node such that v = m(u) is not a leaf. Let vH and vT denote the left and right children of v. Create children uHH , uHT , uTT as nodes adjacent to edges\nlabeled HH,HT, TT respectively. Define the mapping m(uHH) = vH , m(uHT ) = vT , m(uTT ) = vT and set\nlX(uHH) = lX(u) + \u2206H , lX(uHT ) = lX(u) + \u2206H , lX(uTT ) = lX(u)\u2212\u2206T , lY (uHH) = lY (u) + \u2206H , lY (uHT ) = lY (u)\u2212\u2206T , lY (uTT ) = lY (u)\u2212\u2206T .\nBy construction of D\u03c0, it follows that if X \u2265 Y , then at any node u in D\u03c0, lX(u) \u2265 lY (u) and hence, Pr (Heads|lX(u)) \u2265 Pr (Heads|lY (u)).\nOur mixed strategy \u03c0 for Sg(X) is based on D\u03c0. The strategy at any step maintains a pointer to some node u in D\u03c0. Initialize the pointer to the root node u. If the pointer is at a non-leaf node u, then \u03c0 chooses to play in the system. If the step in the system is a backward step (outcome of coin toss is a tail), then \u03c0 moves the pointer to uTT . If the step in the system is a forward step (outcome of coin toss is a head), then \u03c0 generates a random number r \u2208 [0, 1] and moves the pointer to the node uHH if r < Pr (Heads|lY (u)) /Pr (Heads|lX(u)) and to the node uHT if r \u2265 Pr (Heads|lY (u)) /Pr (Heads|lX(u)). If the pointer is at a leaf node u such that lY (u) < B, then \u03c0 quits the system. Otherwise, lY (u) \u2265 B and hence lX(u) \u2265 B. Thus, the strategy \u03c0 is a valid mixed strategy for Sg(X) and \u03c0 plays in the system Sg(X) in the first step since \u03c3 plays in the system Sg(Y ) in the first step.\nIt only remains to show that E\u03c0[Sg(X)] \u2264 g. This is shown in Claim 8.\nClaim 8. E\u03c0[Sg(X)] \u2264 g.\nProof. The cost of using \u03c3 for Sg(Y ) can be simulated by running a random process in D\u03c3 and considering an associated cost. For each non-leaf node in D\u03c3 associate a cost of 1 and for each leaf node u in D\u03c3 such that l(u) < B, associate a cost of g. Consider the following random process RP1(u) for a node u \u2208 D\u03c3: Begin at node u of D\u03c3. On reaching a non-leaf node v, repeatedly traverse the tree D\u03c3 by taking the left child with probability Pr (Heads|l(v)) and the right child with the remaining probability until a leaf node is reached. The cost of the random process is the sum of the cost incurred along the nodes in the path traversed by the random process. Let E[D\u03c3(u)] denote the expected cost. Then, by construction of D\u03c3, it follows that E[D\u03c3(r)] = E\u03c3[Sg(l(r))] = g for the root node r in D\u03c3.\nNext, we give a random process RP2 on D\u03c0 that relates the expected cost of following strategy \u03c0 on Sg(X) and the expected cost of following strategy \u03c3 on Sg(Y ). We first associate a cost with each node u in D\u03c0: For each non-leaf node u, if lX(u) < B, then cost cX(u) = 1, and if lY (u) < B, then cost cY (u) = 1. For each leaf node u, if lX(u) < B, then cost cX(u) = g and if lY (u) < B, then cost cY (u) = g. The remaining costs are zero. Here, we observe that cX(u) \u2264 cY (u) for every node u \u2208 D\u03c0.\nWe define the random process RP2(v) for a node v \u2208 D\u03c0 as follows: Begin at node v and repeatedly traverse the tree D\u03c0 by taking one of the three children at each non-leaf node until a leaf node is reached. On reaching a non-leaf node u, traverse to uHH with probability Pr (Heads|lY (u)), to vHT with probability Pr (Heads|lX(u))\u2212 Pr (Heads|lY (u)) and to uTT with the remaining probability. Let P (v) be the set of nodes in the path traversed by the random process RP2(v). Let the cost incurred be cX(v) = \u2211 u\u2208P (v) cX(u) and cY (v) = \u2211 u\u2208P (v) cY (u). Now, the cost incurred by following strategy \u03c0 for Sg(X) is the same as the cost cX(r) incurred by the random process RP2(r), where r is the root node in D\u03c0.\nBy construction of D\u03c0 from D\u03c3, it follows that for each node v \u2208 D\u03c0, the expected cost cY (v) of the random process RP2(v) is equal to the expected cost of the random process RP1(m(v)). Hence, E[cY (r)] = E[D\u03c3(m(r))] = g for the root node r in D\u03c0. Next, since cX(u) \u2264 cY (u) for every node u, it follows that E[cX(r)] \u2264 E[cY (r)] = g. Finally, the expected cost incurred by following mixed strategy \u03c0 for Sg(X) is exactly equal to E[cX(r)].\nProof of Theorem 1. We use Algorithm Likelihood-Toss. By Lemma 6, the optimal adaptive strategy also minimizes the expected number of tosses to identify a coin i such that the log-likelihood Xi \u2265 B.\nThe strategy adopted by Algorithm Likelihood-Toss at any stage is to toss the coin with maximum log-likelihood. Let the Markov system associated with the one-dimensional random walk of the log-likelihood function of the history of the coin be S = (V, P,C, s, t). We have infinitely many independent and identical Markov systems S1 = S2 = . . . = S associated with the log-likelihood function of the respective coin. By Theorem 4, the optimal strategy to minimize the expected number of tosses to identify a coin i such that the log-likelihood Xi \u2265 B is to toss the coin i such that \u03b3(Xi) is minimal. Lemma 7 shows that the grade function \u03b3(X) is monotonically non-increasing. Thus, tossing the coin with maximum log-likelihood is an optimal strategy.\nBy the description of the algorithm, it is clear that the algorithm starts tossing a fresh/new coin only if the log-likelihood of the current coin decreases below zero. The time to update the likelihood ratio of the current coin after a coin toss is only a constant and hence the time to identify the coin to toss is O(1)."}, {"heading": "5 Number of Coin Tosses", "text": "In this section, we give an upper bound on the number of coin tosses performed by Algorithm Likelihood-Toss. The algorithm repeatedly tosses a coin while the log-likelihood of the coin is at least zero and starts with a fresh coin if the log-likelihood of the coin is less than zero. The algorithm terminates if the log-likelihood of a coin is at least B.\nConsider the random walk of the log-likelihood function. The random walk has absorbing barriers at B and at every state less than 0.\nLemma 9. Let C and D denote the expected number of tosses to get absorbed for a non-heavy and heavy coin respectively. Let \u03c0 denote the probability that a heavy coin gets absorbed at B. Then, under the assumptions of Theorem 2,\n1.\n\u03c0 \u2265 \u2206H(p+ )\u2212\u2206T (q \u2212 ) 2(\u2206H + \u2206T ) .\n2. D \u03c0 \u2264 (\n8B\n\u2206H(p+ )\u2212\u2206T (q \u2212 ) )( \u2206H + \u2206T \u2206H(p+ ) ) .\n3.\nC \u2264 2(\u2206H + \u2206T ) \u2206T (q + )\u2212\u2206H(p\u2212 ) .\nProof. Consider a modified random walk where the starting state is \u2206H as opposed to zero. Let C \u2032 and D\u2032 denote the expected number of tosses for the modified walk to get absorbed using a non-heavy and heavy coin respectively. Let \u03c0\u2032 denote the probability that the modified walk gets absorbed at B using a heavy coin. Then, D \u2264 D\u2032 + 1 \u2264 2D\u2032, C \u2264 C \u2032 + 1 \u2264 2C \u2032, \u03c0 = (p+ )\u03c0\u2032.\nWe use Theorem 5. For the modified random walk, we have that L = \u2206H , W = B \u2212 \u2206H , \u03bd = \u2206T , \u00b5 = \u2206H . For the modified random walk using a heavy coin, the step sizes are\nX =\n{ \u2206H with probability p+\n\u2212\u2206T with probability q \u2212 ,\nand for the modified random walk using a non-heavy coin, the step sizes are\nY = { \u2206H with probability p\u2212 \u2212\u2206T with probability q + ,\nFor > 0, we have that E (Y ) < 0. Therefore,\nC \u2032 \u2264 \u2206H + \u2206T \u2206T (q + )\u2212\u2206H(p\u2212 )\nand hence we have the bound on C. Now consider the modified random walk using a heavy coin. For > 0, we have that E (X) > 0. Let \u03c10 < 1 be the unique real value such that E ( \u03c1X0 ) = 1. Thus,\n\u03c0\u2032 \u2265 1\u2212 \u03c1 \u2206H 0\n1\u2212 \u03c1B+\u2206H0\nD\u2032 \u2264 (\u2206H +B) E (X) ( 1\u2212 \u03c1\u2206H+\u2206T0 1\u2212 \u03c1B+\u2206T0 ) .\nSince \u03c6(\u03c1) is convex, it can be shown that the minimum value of \u03c6(\u03c1) occurs at\n\u03c1min = ( \u2206T (q \u2212 ) \u2206H(p+ ) ) 1 \u2206H+\u2206T\nand hence, \u03c10 < \u03c1min < 1. Thus,\nD\u2032\n\u03c0\u2032 \u2264 (\u2206H +B) E (X) ( 1\u2212 \u03c1B+\u2206H0 1\u2212 \u03c1B+\u2206T0 )( 1\u2212 \u03c1\u2206H+\u2206T0 1\u2212 \u03c1\u2206H0 )\n\u2264 2B E (X)\n( 1\u2212 \u03c1\u2206H+\u2206T0\n1\u2212 \u03c1\u2206H0\n) (by the assumption \u03b4 < \u03b40)\n< 2B\nE (X)\n( 1\u2212 \u03c1\u2206H+\u2206Tmin\n1\u2212 \u03c1\u2206Hmin\n) (since \u03c10 < \u03c1min)\n= 2B\n\u2206H(p+ )  1 1\u2212 ( \u2206T (q\u2212 ) \u2206H(p+ ) ) \u2206H \u2206H+\u2206T  \u2264 4B(\u2206H + \u2206T )\nE (X) \u2206H .\nand we obtain the bound on the ratio D/\u03c0. Finally, to lower bound \u03c0\u2032, we observe that\n\u03c0\u2032 \u2265 1\u2212 \u03c1 \u2206H 0\n1\u2212 \u03c1B+\u2206H0\n\u2265 1\u2212 \u03c1\u2206Hmin\n1\u2212 \u03c1B+\u2206Hmin \u2265 1\u2212 \u03c1\u2206Hmin \u2265 E (X) 2(\u2206H + \u2206T )(p+ ) .\nProof of Theorem 2. . We use Algorithm Likelihood-Toss. Consider the one-dimensional random walk of the log-likelihood function. The random walk has absorbing barriers at B and at every state less than 0. Let C and D denote the expected number of tosses to get absorbed for a non-heavy and heavy coin respectively. Let \u03c0 denote the probability that a heavy coin gets absorbed at B. Let D0 and D1 denote the expected number of tosses of a heavy coin to get absorbed at 0 and B respectively. Then, D = (1\u2212 \u03c0)D0 + \u03c0D1.\nLet E denote the expected number of tosses performed by algorithm Likelihood-Toss. Then,\nE \u2264 (1\u2212 \u03b1)(C + E) + \u03b1((1\u2212 \u03c0)(D0 + E) + \u03c0D1)\n\u21d2 E \u2264 (1\u2212 \u03b1) \u03b1 C \u03c0 + D \u03c0 .\nBy Lemma 9, we have that E \u2264 ( 4(\u2206H + \u2206T )\n\u2206H(p+ )\u2212\u2206T (q \u2212 ) )(( 1\u2212 \u03b1 \u03b1 )( \u2206H + \u2206T \u2206T (q + )\u2212\u2206H(p\u2212 ) ) + ( 2B \u2206H(p+ ) )) .\nThe final upper bound follows by substituting for \u2206H ,\u2206T and B and using the following inequalities (derived by straightforward calculus),\n2\n\u2265 max\n{ \u2206H + \u2206T\n\u2206H(p+ )\u2212\u2206T (q \u2212 ) , \u2206H + \u2206T \u2206T (q + )\u2212\u2206H(p\u2212 )\n} ,\n\u2206H \u2265\np\u2212 ."}, {"heading": "6 Discussion", "text": "We gave an adaptive strategy that tosses coins in order to achieve a certain stopping condition, namely, the existence of a coin whose posterior probability of being heavy is at least a given threshold. Our strategy has minimum cost where cost is measured by the expected number of future tosses by following the strategy to attain the stopping condition. We achieved this by performing\nthe best possible action after observing the outcome of each coin toss. We note that our algorithm can also be modified to start from any fixed history of outcomes by appropriately modifying the initialization step. The optimality of the action is exhibited using tools from the field of Markov games. A major limitation of our algorithm is that it is optimal only in the setting where the coins are independently heavy and non-heavy. It would be very interesting to devise an adaptive strategy where the coins are not necessarily independent \u2013 say we have n coins with exactly one heavy coin and the goal is to attain the stopping condition. In this setting, we note that the posterior probability of a fixed coin being heavy depends on the outcomes of the tosses of all the coins and not just any fixed coin.\nAcknowledgment. We thank Santosh Vempala for valuable comments."}], "references": [{"title": "Best Arm Identification in Multi-Armed Bandits", "author": ["J.-Y. Audibert", "S. Bubeck", "R. Munos"], "venue": "In Proceedings of the Twenty-third Conference on Learning Theory, COLT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The Nonstochastic Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal of Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances", "author": ["R.E. Bechhofer"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1954}, {"title": "Design and Analysis of Experiments for Statistical Selection, Screening, and Multiple Comparisons", "author": ["R.E. Bechhofer", "T.J. Santner", "D.M. Goldsman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Bandit Problems: Sequential Allocation of Experiments (Monographs on Statistics and Applied Probability)", "author": ["D.A. Berry", "B. Fristedt"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1985}, {"title": "Using Ranking and Selection to \u201cClean Up\u201d after Simulation Optimization", "author": ["J. Boesel", "B.L. Nelson", "S.-H. Kim"], "venue": "Operations Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the 20th international conference on Algorithmic learning theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Economic Analysis of Simulation Selection Problems", "author": ["S.E. Chick", "N. Gans"], "venue": "Management Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "New Two-Stage and Sequential Procedures for Selecting the Best Simulated System", "author": ["S.E. Chick", "K. Inoue"], "venue": "Operations Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection", "author": ["V. Cicirello", "S. Smith"], "venue": "In 20th National Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "On Playing Golf with Two Balls", "author": ["I. Dumitriu", "P. Tetali", "P. Winkler"], "venue": "SIAM Journal of Discrete Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Bounds on Gambler\u2019s Ruin Probabilities in Terms of Moments", "author": ["S.N. Ethier", "D. Khoshnevisan"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "PAC Bounds for Multi-armed Bandit and Markov Decision Processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the 15th Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "A Knowledge-Gradient Policy for Sequential Information Collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Multi-Bandit Best Arm Identification", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Multi-armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "Wiley, 2nd edition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Bayesian look ahead one-stage sampling allocations for selection of the best population", "author": ["S.S. Gupta", "K.J. Miescke"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Selecting the best system", "author": ["S.H. Kim", "B.L. Nelson"], "venue": "Handbooks in Operations Research and Management Science: Simulation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Asymptotically Efficient Adaptive Allocation Rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation", "author": ["O. Maron", "A. Moore"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "A Sequential Procedure for Selecting the Population with the Largest Mean from k Normal Populations", "author": ["E. Paulson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1964}, {"title": "Selection-of-the-best procedures for optimization via simulation", "author": ["J. Pichitlamken", "B.L. Nelson"], "venue": "In Proceeding of the 2001 Winter Simulation Conference,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}], "referenceMentions": [{"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 20, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 1, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 2, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 7, "context": "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.", "startOffset": 28, "endOffset": 38}, {"referenceID": 0, "context": "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.", "startOffset": 28, "endOffset": 38}, {"referenceID": 16, "context": "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.", "startOffset": 28, "endOffset": 38}, {"referenceID": 7, "context": "The work of [8] suggested that the exploration-exploitation trade offs for this setting are much different from the setting where the number of steps is asymptotic.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "[1] proposed exploration strategies to perform essentially as well as the best strategy that knows all distributions up to permutations of the arms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] addressed the problem of identifying the best arm for each bandit among a collection of bandits within a fixed budget.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 14, "context": "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 13, "context": "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] showed that a total of O((n/ 2) log(1/\u03b4)) steps is sufficient to identify an arm whose expected reward is at most away from the optimal arm with correctness at least 1 \u2212 \u03b4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Mannor and Tsitsiklis [22] showed lower bounds matching up to constant factors under various settings of the rewards.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 21, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 10, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 7, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 0, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 16, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 3, "context": "It was introduced for normally distributed rewards by Bechhofer [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 23, "context": "Adaptive strategies for this problem, known as \u201csequential selection\u201d, can be traced back to Paulson [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 4, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 19, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 6, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 24, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 3, "context": "This special case is known as the \u201cindifference-zone\u201d assumption [4].", "startOffset": 65, "endOffset": 68}, {"referenceID": 19, "context": "Strategies and their measure of optimality are known for various relaxations of independence, normality, equal and known variances and indifference-zone assumptions [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 9, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 15, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 8, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 21, "context": "The implications of our upper bound when the number of coins is bounded but much larger than 1/\u03b1 needs to be contrasted with the lower bounds by [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "In this case, setting n = c/\u03b1 in the above expression suggests that our algorithm beats the lower bound shown in Theorem 9 of [22].", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "We observe that Theorem 9 of [22] shows a lower bound in the most general Bayesian setting \u2013 there exists a prior distribution of the probabilities of the n coins so that any algorithm requires at least O((n/ 2) log (1/\u03b4)) tosses in expectation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "We use the notation and results from [12].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "A Markov system S = (V, P,C, s, t) consists of a state space V , a transition probability function P : V \u00d7V \u2192 [0, 1], a positive real cost Cv associated with each state v, a start state s and a target state t.", "startOffset": 110, "endOffset": 116}, {"referenceID": 11, "context": "[12] Every Markov game has a pure optimal strategy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Given the states u1, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": ", the number of possible transitions from any fixed state is finite), by working through the proofs in [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "We use the following results from [13] to bound the number of tosses.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "[13] Let X \u2208 [\u2212\u03bd, \u03bc] be the random variable that determines the step-sizes of a one dimensional random walk with absorbing barriers at \u2212L and W such that Pr (X > 0) > 0, Pr (X < 0) > 0, E (X) 6= 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "If the step in the system is a forward step (outcome of coin toss is a head), then \u03c0 generates a random number r \u2208 [0, 1] and moves the pointer to the node uHH if r < Pr (Heads|lY (u)) /Pr (Heads|lX(u)) and to the node uHT if r \u2265 Pr (Heads|lY (u)) /Pr (Heads|lX(u)).", "startOffset": 115, "endOffset": 121}], "year": 2013, "abstractText": "We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin i\u2217 whose posterior probability of being most biased is at least 1 \u2212 \u03b4 for a given \u03b4. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy \u2013 a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs tools from the field of Markov games.", "creator": "LaTeX with hyperref package"}}}