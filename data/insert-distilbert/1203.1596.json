{"id": "1203.1596", "review": {"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2012", "title": "Multiple Operator-valued Kernel Learning", "abstract": "this paper addresses the problem of learning a finite linear alternative combination of operator - valued kernels. we study this problem in the case of kernel equation ridge regression for functional responses with a consistent lr - norm constraint on the combination coefficients. we propose a unique multiple robust operator - valued kernel learning algorithm based on solving a system network of linear operator equations by diffusion using a block coordinate descent procedure. we experimentally quickly validate our approach on a functional regression task paradigm in the context of frog finger movement prediction in brain - computer interface ( bci ).", "histories": [["v1", "Wed, 7 Mar 2012 20:31:17 GMT  (408kb)", "https://arxiv.org/abs/1203.1596v1", "No. RR-7900 (2012)"], ["v2", "Thu, 14 Jun 2012 18:44:49 GMT  (411kb)", "http://arxiv.org/abs/1203.1596v2", "No. RR-7900 (2012)"]], "COMMENTS": "No. RR-7900 (2012)", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["hachem kadri", "alain rakotomamonjy", "francis r bach", "philippe preux"], "accepted": true, "id": "1203.1596"}, "pdf": {"name": "1203.1596.pdf", "metadata": {"source": "CRF", "title": "Multiple Operator-valued Kernel Learning", "authors": ["Philippe Preux", "Hachem Kadri", "Alain Rakotomamonjy", "Francis Bach"], "emails": ["hachem.kadri@inria.fr", "alain.rakotomamonjy@insa-rouen.fr", "francis.bach@inria.fr", "philippe.preux@inria.fr"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n15 96\nv2 [\nst at\n.M L\n] 1\n4 Ju\nn 20\n12\nappor t de r ech er ch e\nIS S\nN 02\n49 -6\n39 9\nIS R\nN IN\nR IA\n/R R\n-- 79\n00 --\nF R\n+ E\nN G\nTh\u00e8me COG"}, {"heading": "INSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE", "text": ""}, {"heading": "Multiple Operator-valued Kernel Learning", "text": ""}, {"heading": "Hachem Kadri \u2014 Alain Rakotomamonjy \u2014 Francis Bach \u2014 Philippe Preux", "text": "N\u00b0 7900\nFebruary 2012"}, {"heading": "Centre de recherche INRIA Lille \u2013 Nord Europe Parc Scientifique de la Haute Borne", "text": ""}, {"heading": "40, avenue Halley, 59650 Villeneuve d\u2019Ascq", "text": "T\u00e9l\u00e9phone : +33 3 59 57 78 00 \u2014 T\u00e9l\u00e9copie : +33 3 59 57 78 50"}, {"heading": "Multiple Operator-valued Kernel Learning", "text": ""}, {"heading": "Hachem Kadri\u2217, Alain Rakotomamonjy\u2020, Francis Bach\u2021, Philippe", "text": "Preux\u00a7\nThe\u0300me COG \u2014 Syste\u0300mes cognitifs E\u0301quipes-Projets SequeL\nRapport de recherche n\u00b0 7900 \u2014 February 2012 \u2014 20 pages\nAbstract: Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an \u2113r-norm constraint on the combination coefficients (r \u2265 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinatedescent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces.\nKey-words: Operator-valued kernels, multiple kernel learning, nonparametric functional data analysis, function-valued reproducing kernel Hilbert spaces.\n\u2217 Sequel Team, INRIA Lille. E-mail: hachem.kadri@inria.fr \u2020 LITIS, Universite\u0301 de Rouen. E-mail: alain.rakotomamonjy@insa-rouen.fr \u2021 Sierra Team/INRIA, Ecole Normale Supe\u0301rieure. E-mail: francis.bach@inria.fr \u00a7 Sequel/INRIA-Lille, LIFL/CNRS. E-mail: philippe.preux@inria.fr"}, {"heading": "Apprentissage de Noyaux a\u0300 Valeurs Ope\u0301rateurs", "text": ""}, {"heading": "Multiples", "text": "Re\u0301sume\u0301 : Dans cet article, nous proposons une me\u0301thode d\u2019apprentissage de noyaux multiples a\u0300 valeurs ope\u0301rateurs dans le cas d\u2019une re\u0301gression ridge a\u0300 re\u0301ponse fonctionnelle. Notre me\u0301thode est base\u0301e sur la re\u0301solution d\u2019un syste\u0300me d\u2019e\u0301quations line\u0301aires d\u2019ope\u0301rateurs en utilisant une proce\u0301dure de type Iterative Coordinate Descent. Nous validons expe\u0301rimentalement notre approche sur un proble\u0300me de pre\u0301diction de mouvement de doigt dans un contexte d\u2019Interface Cerveau-Machine.\nMots-cle\u0301s : noyaux a\u0300 valeurs ope\u0301rateurs, apprentissage de noyaux multiples, analyse des donne\u0301es fonctionnelles, espace de Hilbert a\u0300 noyau reproduisant"}, {"heading": "Multiple Operator-valued Kernel Learning 3", "text": ""}, {"heading": "1 Introduction", "text": "During the past decades, a large number of algorithms have been proposed to deal with learning problems in the case of single-valued functions (e.g., binaryoutput function for classification or real output for regression). Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7]. Much of this interest has arisen from the need to learn tasks where the target is a complex entity, not a scalar variable. Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].\nIn this paper, we are interested in the problem of functional regression with functional responses in the context of brain-computer interface (BCI) design. More precisely, we are interested in finger movement prediction from electrocorticographic signals [27]. Indeed, from a set of signals measuring brain surface electrical activity on d channels during a given period of time, we want to predict, for any instant of that period whether a finger is moving or not and the amplitude of the finger flexion. Formally, the problem consists in learning a functional dependency between a set of d signals and a sequence of labels (a step function indicating whether a finger is moving or not) and between the same set of signals and vector of real values (the amplitude function). While, it is clear that this problem can be formalized as functional regression problem, from our point of view, such problem can benefit from the multiple operator-valued kernel learning framework. Indeed, for these problems, one of the difficulties arises from the unknown latency between the signal related to the finger movement and the actual movement [22]. Hence, instead of fixing in advance some value for this latency in the regression model, our framework allows to learn it from the data by means of several operator-valued kernels.\nIf we wish to address functional regression problem in the principled framework of reproducing kernel Hilbert spaces (RKHS), we have to consider RKHSs whose elements are operators that map a function to another function space, possibly source and target function spaces being different. Working in such RKHSs, we are able to draw on the important core of work that has been performed on scalar-valued and vector-valued RKHSs [29, 20]. Such a functional RKHS framework and associated operator-valued kernels have been introduced recently [12, 13]. A basic question with reproducing kernels is how to build these kernels and what is the optimal kernel choice for a given application. In order to overcome the need for choosing a kernel before the learning process, several works have tried to address the problem of learning the scalar-valued kernel jointly with the decision function [17, 30]. Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14]. While many works have been devoted to multiple scalar-valued kernel learning, this problem of kernel learning have been barely investigated for operator-valued kernels. One motivation of this work is to bridge the gap between multiple kernel learning (MKL) and operator-valued kernels by proposing a framework and an algorithm for learning a finite linear combination of operator-valued kernels. While each step of the scalar-valued MKL framework can be extended without major difficulties to operator-valued kernels, technical challenges arise at all stages because we deal with infinite dimensional spaces. It should be pointed out that in a recent work [10], the problem of learning the output kernel was\nRR n\u00b0 7900"}, {"heading": "4 Kadri, Rakotomamonjy, Bach & Preux", "text": "formulated as an optimization problem over the cone of positive semidefinite matrices, and proposed a block-coordinate descent method to solve it. However, they did not focus on learning the input kernel. In contrast, our multiple operator-valued kernel learning formulation can be seen as a way of learning simultaneously input and output kernels, although we consider a linear combination of kernels which are fixed in advance.\nIn this paper, we make the following contributions:\n\u2022 we introduce a novel approach to infinite-dimensional multiple operatorvalued kernel learning (MovKL) suitable for learning the functional dependencies and interactions between continuous data,\n\u2022 we extend the original formulation of ridge regression in dual variables to the functional data analysis domain, showing how to perform nonlinear functional regression with functional responses by constructing a linear regression operator in an operator-valued kernel feature space (Section 2),\n\u2022 we derive a dual form of the MovKL problem with functional ridge regression, and show that a solution of the related optimization problem exists (Section 2),\n\u2022 we propose a block-coordinate descent algorithm to solve the MovKL optimization problem which involves solving a challenging linear system with a sum of block operator matrices, and show its convergence in the case of compact operator-valued kernels (Section 3),\n\u2022 we provide an empirical evaluation of MovKL performance which demonstrates its effectiveness on a BCI dataset (Section 4)."}, {"heading": "2 Problem Setting", "text": "Before describing the multiple operator-valued kernel learning algorithm that we will study and experiment with in this paper, we first review notions and properties of reproducing kernel Hilbert spaces with operator-valued kernels, show their connection to learning from multiple response data (multiple outputs; see [20] for discrete data and [12] for continuous data), and describe the optimization problem for learning kernels with functional response ridge regression."}, {"heading": "2.1 Notations and Preliminaries", "text": "We start by some standard notations and definitions used all along the paper. Given a Hilbert space H, \u3008\u00b7, \u00b7\u3009H and \u2016 \u00b7 \u2016H refer to its inner product and norm, respectively. We denote by Gx and Gy the separable real Hilbert spaces of input and output functional data, respectively. In functional data analysis domain, continuous data are generally assumed to belong to the space of square integrable functions L2. In this work, we consider that Gx and Gy are the Hilbert space L2(\u2126) which consists of all equivalence classes of square integrable functions on a finite set \u2126. \u2126 being potentially different for Gx and Gy. We denote by F(Gx,Gy) the vector space of functions f : Gx \u2212\u2192 Gy, and by L(Gy) the set of bounded linear operators from Gy to Gy.\nWe consider the problem of estimating a function f such that f(xi) = yi when observed functional data (xi, yi)i=1,...,n \u2208 (Gx,Gy). Since Gx and Gy are\nINRIA"}, {"heading": "Multiple Operator-valued Kernel Learning 5", "text": "spaces of functions, the problem can be thought of as an operator estimation problem, where the desired operator maps a Hilbert space of factors to a Hilbert space of targets. We can define the regularized operator estimate of f \u2208 F as:\nf\u03bb , argmin f\u2208F\n1\nn\nn\u2211\ni=1\n\u2016yi \u2212 f(xi)\u2016 2 Gy + \u03bb\u2016f\u2016 2 F . (1)\nIn this work, we are looking for a solution to this minimization problem in a function-valued reproducing kernel Hilbert space F . More precisely, we mainly focus on the RKHS F whose elements are continuous linear operators on Gx with values in Gy . The continuity property is obtained by considering a special class of reproducing kernels called Mercer kernels [7, Proposition 2.2]. Note that in this case, F is separable since Gx and Gy are separable [6, Corollary 5.2].\nWe now introduce (function) Gy-valued reproducing kernel Hilbert spaces and show the correspondence between such spaces and positive definite (operator) L(Gy)-valued kernels. These extend the traditional properties of scalarvalued kernels.\nDefinition 1 (function-valued RKHS)\nA Hilbert space F of functions from Gx to Gy is called a reproducing kernel Hilbert space if there is a positive definite L(Gy)-valued kernel KF(w, z) on Gx \u00d7 Gx such that:\ni. the function z 7\u2212\u2192 KF(w, z)g belongs to F , \u2200z \u2208 Gx, w \u2208 Gx, g \u2208 Gy,\nii. \u2200f \u2208 F , w \u2208 Gx, g \u2208 Gy, \u3008f,KF(w, \u00b7)g\u3009F = \u3008f(w), g\u3009Gy (reproducing property).\nDefinition 2 (operator-valued kernel)\nAn L(Gy)-valued kernel KF(w, z) on Gx is a function KF(\u00b7, \u00b7) : Gx \u00d7 Gx \u2212\u2192 L(Gy); furthermore:\ni. KF is Hermitian if KF(w, z) = KF(z, w) \u2217, where \u2217 denotes the adjoint\noperator,\nii. KF is positive definite on Gx if it is Hermitian and for every natural number r and all {(wi, ui)i=1,...,r} \u2208 Gx \u00d7 Gy, \u2211 i,j\u3008KF (wi, wj)uj , ui\u3009Gy \u2265 0.\nTheorem 1 (bijection between function valued RKHS and operator-valued kernel) An L(Gy)-valued kernel KF(w, z) on Gx is the reproducing kernel of some Hilbert space F , if and only if it is positive definite.\nThe proof of Theorem 1 can be found in [20]. For further reading on operatorvalued kernels and their associated RKHSs, see, e.g., [5, 6, 7]."}, {"heading": "2.2 Functional Response Ridge Regression in Dual Variables", "text": "We can write the ridge regression with functional responses optimization problem (1) as follows:\nmin f\u2208F\n1 2 \u2016f\u20162F + 1 2n\u03bb\nn\u2211\ni=1\n\u2016\u03bei\u2016 2 Gy\nwith \u03bei = yi \u2212 f(xi).\n(2)\nRR n\u00b0 7900"}, {"heading": "6 Kadri, Rakotomamonjy, Bach & Preux", "text": "Now, we introduce the Lagrange multipliers \u03b1i, i = 1, . . . , n which are functional variables since the output space is the space of functions Gy. For the optimization problem (2), the Lagrangian multipliers exist and the Lagrangian function is well defined. The method of Lagrange multipliers on Banach spaces, which is a generalization of the classical (finite-dimensional) Lagrange multipliers method suitable to solve certain infinite-dimensional constrained optimization problems, is applied here. For more details, see [15]. Let \u03b1 = (\u03b1i)i=1,...,n \u2208 G n y the vector of functions containing the Lagrange multipliers, the Lagrangian function is defined as\nL(f, \u03b1, \u03be) = 1\n2 \u2016f\u20162F +\n1\n2n\u03bb \u2016\u03be\u20162Gny + \u3008\u03b1, y \u2212 f(x)\u2212 \u03be\u3009Gny , (3)\nwhere \u03b1 = (\u03b11, . . . , \u03b1n) \u2208 G n y , y = (y1, . . . , yn) \u2208 G n y , \u03be = (\u03be1, . . . , \u03ben) \u2208 G n y ,\nf(x) = (f(x1), . . . , f(xn)) \u2208 G n y , and \u2200a, b \u2208 G n y , \u3008a, b\u3009Gny = n\u2211 i=1 \u3008ai, bi\u3009Gy .\nDifferentiating (3) with respect to f \u2208 F and setting to zero, we obtain\nf(.) =\nn\u2211\ni=1\nK(xi, .)\u03b1i, (4)\nwhere K : Gx \u00d7 Gx \u2212\u2192 L(Gy) is the operator-valued kernel of F . Substituting this into (3) and minimizing with respect to \u03be, we obtain the dual of the functional response ridge regression problem\nmax \u03b1\n\u2212 n\u03bb\n2 \u2016\u03b1\u20162Gny \u2212\n1 2 \u3008K\u03b1, \u03b1\u3009Gny + \u3008\u03b1, y\u3009Gny , (5)\nwhere K = [K(xi, xj)] n i,j=1 is the block operator kernel matrix."}, {"heading": "2.3 MovKL in Dual Variables", "text": "Let us now consider that the function f(\u00b7) is sum of M functions {fk(\u00b7)} M k=1 where each fk belongs to a Gy-valued RKHS with kernel Kk(\u00b7, \u00b7). Similarly to scalar-valued multiple kernel learning, we adopt the convention that x0 = 0 if x = 0 and \u221e otherwise, and we can cast the problem of learning these functions fk as\nmin d\u2208D min fk\u2208Fk\nM\u2211\nk=1\n\u2016fk\u2016 2 Fk\n2dk +\n1\n2\u03bb\nn\u2211\ni=1\n\u2016\u03bei\u2016 2 Gy\nwith \u03bei = yi \u2212 \u2211M k=1 fk(xi),\n(6)\nwhere d = [d1, \u00b7 \u00b7 \u00b7 , dM ], D = {d : \u2200k, dk \u2265 0 and \u2211 k d r k \u2264 1} and 1 \u2264 r \u2264 \u221e. Note that this problem can equivalently be rewritten as an unconstrained optimization problem. Before deriving the dual of this problem, it can be shown by means of the generalized Weierstrass theorem [16] that this problem admits a solution (a detailed proof is provided in the supplementary material).\nNow, following the lines of [23], a dualization of this problem leads to the following equivalent one\nmin d\u2208D max \u03b1\u2208Gny\n\u2212 n\u03bb\n2 \u2016\u03b1\u20162Gny \u2212\n1 2 \u3008K\u03b1, \u03b1\u3009Gny + \u3008\u03b1, y\u3009Gny , (7)\nINRIA\nMultiple Operator-valued Kernel Learning 7\nwhere K = M\u2211 k=1 dkKk and Kk is the block operator kernel matrix associated to the operator-valued kernel Kk. The KKT conditions also state that at optimality we have fk(\u00b7) = n\u2211\ni=1\ndkKk(xi, \u00b7)\u03b1i."}, {"heading": "3 Solving the MovKL Problem", "text": "After having presented the framework, we now devise an algorithm for solving this MovKL problem."}, {"heading": "3.1 Block-coordinate descent algorithm", "text": "Since the optimization problem (6) has the same structure as a multiple scalarvalued kernel learning problem, we can build our MovKL algorithm upon the MKL literature. Hence, we propose to borrow from [14], and consider a blockcoordinate descent method. The convergence of a block coordinate descent algorithm which is related closely to the Gauss-Seidel method was studied in works of [31] and others. The difference here is that we have operators and block operator matrices rather than matrices and block matrices, but this doesn\u2019t increase the complexity if the inverse of the operators are computable (typically analytically or by spectral decomposition). Our algorithm iteratively solves the problem with respect to \u03b1 with d being fixed and then with respect to d with \u03b1 being fixed (see Algorithm 1). After having initialized {dk} to non-zero values, this boils down to the following steps :\n1. with {dk} fixed, the resulting optimization problem with respect to \u03b1 has a simple closed-form solution:\n(K+ \u03bbI)\u03b1 = 2y, (8)\nwhere K = \u2211M\nk=1 dkKk. While the form of solution is rather simple, solving this linear system is still challenging in the operator setting and we propose below an algorithm for its resolution.\n2. with {fk} fixed, according to problem (6), we can rewrite the problem as\nmin d\u2208D\nM\u2211\nk=1\n\u2016fk\u2016 2 Fk\ndk (9)\nwhich has a closed-form solution and for which optimality occurs at [19]:\ndk = \u2016fk\u2016\n2 r+1\n( \u2211\nk \u2016fk\u2016 2r\nr+1 )1/r . (10)\nThis algorithm is similar to that of [8] and [14] both being based on alternating optimization. The difference here is that we have to solve a linear system involving a block-operator kernel matrix which is a combination of basic kernel matrices associated to M operator-valued kernels. This makes the system very challenging, and we present an algorithm for solving it in the next paragraph.\nRR n\u00b0 7900"}, {"heading": "8 Kadri, Rakotomamonjy, Bach & Preux", "text": "Algorithm 1 \u2113r-norm MovKL\nInput Kk for k = 1, . . . ,M d1k \u2190\u2212 1\nM for k = 1, . . . ,M\n\u03b1 \u2190\u2212 0\nfor t = 1, 2, . . . do\n\u03b1\u2032 \u2190\u2212 \u03b1 K \u2190\u2212 \u2211\nk d t kKk\n\u03b1 \u2190\u2212 solution of (K+ \u03bbI)\u03b1 = 2y if \u2016\u03b1\u2212 \u03b1\u2032\u2016 < \u01eb then\nbreak\nend if\ndt+1k \u2190\u2212 \u2016fk\u2016\n2 r+1\n( \u2211 k \u2016fk\u2016 2r r+1 )1/r for k = 1, . . . ,M\nend for\nA detailed proof of convergence of the MovKL algorithm, in the case of compact operator-valued kernels, is given in the supplementary material. It proceeds by showing that the sequence of functions {f (n) k } generated by the above alternating optimization produces a non-increasing sequence of objective values of Equation (6). Then, using continuity and boundedness arguments, we can also show that the sequence {f (n) k } is bounded and thus converges to a minimizer of Equation (6). The proof is actually an extension of results obtained by [2] and [24] for scalar-valued kernels. However, the extension is not straighforward since infinite-dimensional Hilbert spaces with operator-valued reproducing kernels raise some technical issues that we have leveraged in the case of compact operators."}, {"heading": "3.2 Solving a linear system with multiple block operatorvalued kernels", "text": "One common way to construct operator-valued kernels is to build scalar-valued ones which are carried over to the vector-valued (resp. function-valued) setting by a positive definite matrix (resp. operator). In this setting an operator-valued kernel has the following form:\nK(w, z) = G(w, z)T,\nwhere G is a scalar-valued kernel and T is an operator in L(Gy). In multi-task learning, T is a finite dimensional matrix that is expected to share information between tasks [11, 5]. More recently and for supervised functional output learning problems, T is chosen to be a multiplication or an integral operator [12, 13]. This choice is motivated by the fact that functional linear models for functional responses [25] are based on these operators and then such kernels provide an interesting alternative to extend these models to nonlinear contexts. In addition, some works on functional regression and structured-output learning consider operator-valued kernels constructed from the identity operator as in [18]\nINRIA"}, {"heading": "Multiple Operator-valued Kernel Learning 9", "text": "Algorithm 2 Gauss-Seidel Method\nchoose an initial vector of functions \u03b1(0)\nrepeat\nfor i = 1, 2, . . . , n\n\u03b1 (t) i \u2190\u2212 sol. of (13):\n[K(xi, xi) + \u03bbI ]\u03b1 (t) i = si\nend for\nuntil convergence\nand [4]. In this work we adopt a functional data analysis point of view and then we are interested in a finite combination of operator-valued kernels constructed from the identity, multiplication and integral operators. A problem encountered when working with operator-valued kernels in infinite-dimensional spaces is that of solving the system of linear operator equations (8). In the following we show how to solve this problem for two cases of operator-valued kernel combinations.\nCase 1: multiple scalar-valued kernels and one operator. This is the simpler case where the combination of operator-valued kernels has the following form\nK(w, z) =\nM\u2211\nk=1\ndkGk(w, z)T, (11)\nwhere Gk is a scalar-valued kernel, T is an operator in L(Gy), and dk are the combination coefficients. In this setting, the block operator kernel matrix K can be expressed as a Kronecker product between the multiple scalar-valued kernel matrix G = \u2211M k=1 dkGk, where Gk = [Gk(xi, xj)] n i,j=1, and the operator T . Thus we can compute an analytic solution of the system of equations (8) by inverting K+ \u03bbI using the eigendecompositions of G and T as in [13].\nCase 2: multiple scalar-valued kernels and multiple operators. This is the general case where multiple operator-valued kernels are combined as follows\nK(w, z) =\nM\u2211\nk=1\ndkGk(w, z)Tk, (12)\nwhere Gk is a scalar-valued kernel, Tk is an operator in L(Gy), and dk are the combination coefficients. Inverting the associated block operator kernel matrix K is not feasible in this case, that is why we propose a Gauss-Seidel iterative procedure (see Algorithm 2) to solve the system of linear operator equations (8). Starting from an initial vector of functions \u03b1(0), the idea is to iteratively compute, until a convergence condition is satisfied, the functions \u03b1i according to the following expression\n[K(xi, xi) + \u03bbI]\u03b1 (t) i = 2yi \u2212\ni\u22121\u2211\nj=1\nK(xi, xj)\u03b1 (t) j \u2212\nn\u2211\nj=i+1\nK(xi, xj)\u03b1 (t\u22121) j , (13)\nwhere t is the iteration index. This problem is still challenging because the kernel K(\u00b7, \u00b7) still involves a positive combination of operator-valued kernels. Our algorithm is based on the idea that instead of inverting the finite combination of\nRR n\u00b0 7900"}, {"heading": "10 Kadri, Rakotomamonjy, Bach & Preux", "text": "operator-valued kernels [K(xi, xi)+\u03bbI], we can consider the following variational formulation of this system\nmin \u03b1\n(t) i\n1 2 \u3008\nM+1\u2211\nk=1\nKk(xi, xi)\u03b1 (t) i , \u03b1 (t) i \u3009Gy \u2212 \u3008si, \u03b1 (t) i \u3009Gy ,\nwhere si = 2yi \u2212 i\u22121\u2211 j=1 K(xi, xj)\u03b1 (t) j \u2212 n\u2211 j=i+1 K(xi, xj)\u03b1 (t\u22121) j , Kk = dkGkTk, \u2200k \u2208 {1, . . . ,M}, and KM+1 = \u03bbI. Now, by means of a variable-splitting approach, we are able to decouple the role of the different kernels. Indeed, the above problem is equivalent to the following one :\nmin \u03b1\n(t) i\n1 2 \u3008K\u0302(xi, xi)\u03b1 (t) i ,\u03b1 (t) i \u3009GMy \u2212 \u3008si,\u03b1 (t) i \u3009GMy\nwith \u03b1 (t) i,1 = \u03b1 (t) i,k for k = 2, . . . ,M + 1,\nwhere K\u0302(xi, xi) is the (M + 1) \u00d7 (M + 1) diagonal matrix [Kk(xi, xi)] M+1 k=1 . \u03b1 (t) i is the vector (\u03b1 (t) i,1, . . . , \u03b1 (t) i,M+1) and the (M + 1)-dimensional vector si = (si, 0, . . . , 0). We now have to deal with a quadratic optimization problem with equality constraints. Writing down the Lagrangian of this optimization problem and then deriving its first-order optimality conditions leads us to the following set of linear equations\n   K1(xi, xi)\u03b1i,1 \u2212 si + \u2211M\nk=1 \u03b3k = 0 Kk(xi, xi)\u03b1i,k \u2212 \u03b3k = 0 \u03b1i,1 \u2212 \u03b1i,k = 0\n(14)\nwhere k = 2, . . . ,M + 1 and {\u03b3k} are the Lagrange multipliers related to the M equality constraints. Finally, in this set of equations, the operator-valued kernels have been decoupled and thus, if their inversion can be easily computed (which is the case in our experiments), one can solve the problem (14) with respect to {\u03b1i,k} and \u03b3k by means of another Gauss-Seidel algorithm after simple reorganization of the linear system."}, {"heading": "4 Experiments", "text": "In order to highlight the benefit of our multiple operator-valued kernel learning approach, we have considered a series of experiments on a real dataset, involving functional output prediction in a brain-computer interface framework. The problem we addressed is a sub-problem related to finger movement decoding from Electrocorticographic (ECoG) signals. We focus on the problem of estimating if a finger is moving or not and also on the direct estimation of the finger movement amplitude from the ECoG signals. The development of the full BCI application is beyond the scope of this paper and our objective here is to prove that this problem of predicting finger movement can benefit from multiple kernel learning.\nTo this aim, the fourth dataset from the BCI Competition IV [21] was used. The subjects were 3 epileptic patients who had platinium electrode grids placed\nINRIA"}, {"heading": "Multiple Operator-valued Kernel Learning 11", "text": "on the surface of their brains. The number of electrodes varies between 48 to 64 depending on the subject, and their position on the cortex was unknown. ECoG signals of the subject were recorded at a 1KHz sampling using BCI2000 [28]. A band-pass filter from 0.15 to 200Hz was applied to the ECoG signals. The finger flexion of the subject was recorded at 25Hz and up-sampled to 1KHz by means of a data glove which measures the finger movement amplitude. Due to the acquisition process, a delay appears between the finger movement and the measured ECoG signal [21]. One of our hopes is that this time-lag can be properly learnt by means of multiple operator-valued kernels. Features from the ECoG signals are built by computing some band-specific amplitude modulation features, which is defined as the sum of the square of the band-specific filtered ECoG signal samples during a fixed time window.\nFor our finger movement prediction task, we have kept 5 channels that have been manually selected and split ECoG signals in portions of 200 samples. For each of these time segments, we have the label of whether at each time sample, the finger is moving or not as well as the real movement amplitudes. The dataset is composed of 487 couples of input-output signals, the output signals being either the binary movement labels or the real amplitude movement. An example of input-output signals are depicted in Figure 1. In a nutshell, the problem boils down to be a functional regression task with functional responses.\nTo evaluate the performance of the multiple operator-valued kernel learning approach, we use both: (1) the percentage of labels correctly recognized (LCR) defined by (Wr/Tn)\u00d7 100%, where Wr is the number of well-recognized labels and Tn the total number of labels to be recognized; (2) the residual sum of squares error (RSSE) as evaluation criterion for curve prediction\nRSSE =\n\u222b \u2211\ni\n{yi(t)\u2212 y\u0302i(t)} 2dt, (15)\nwhere y\u0302i(t) is the prediction of the function yi(t) corresponding to real finger movement or the finger movement state.\nFor the multiple operator-valued kernels having the form (12), we have used a Gaussian kernel with 5 different bandwidths and a polynomial kernel of degree 1 to 3 combined with three operators T : identity Ty(t) = y(t), multiplication operator associated with the function e\u2212t 2 defined by Ty(t) = e\u2212t 2\ny(t), and the integral Hilbert-Schmidt operator with the kernel e\u2212|t\u2212s| proposed in [13], Ty(t) = \u222b e\u2212|t\u2212s|y(s)ds. The inverses of these operators can be computed ana-\nRR n\u00b0 7900"}, {"heading": "12 Kadri, Rakotomamonjy, Bach & Preux", "text": "lytically. While the inverses of the identity and the multiplication operators are easily and directly computable from the analytic expressions of the operators, the inverse of the integral operator is computed from its spectral decomposition as in [13]. The number of eigenfunctions as well as the regularization parameter \u03bb are fixed using \u201cone-curve-leave-out cross-validation\u201d [26] with the aim of minimizing the residual sum of squares error.\nEmpirical results on the BCI dataset are summarized in Tables 1 and 2 . The dataset was randomly partitioned into 65% training and 35% test sets. We compare our approach in the case of \u21131 and \u21132-norm constraint on the combination coefficients with: (1) the baseline scalar-valued kernel ridge regression algorithm by considering each output independently of the others, (2) functional response ridge regression using an integral operator-valued kernel [13], (3) kernel ridge regression with an evenly-weighted sum of operator-valued kernels, which we denote by \u2113\u221e-norm MovKL.\nAs in the scalar case, using multiple operator-valued kernels leads to better results. By directly combining kernels constructed from identity, multiplication and integral operators we could reduce the residual sum of squares error and enhance the label classification accuracy. Best results are obtained using the MovKL algorithm with \u21132-norm constraint on the combination coefficients. RSSE and LCR of the baseline kernel ridge regression are significantly outperformed by the operator-valued kernel based functional response regression. These results confirm that by taking into account the relationship between outputs we can improve performance. This is due to the fact that an operatorvalued kernel induces a similarity measure between two pairs of input/output."}, {"heading": "5 Conclusion", "text": "In this paper we have presented a new method for learning simultaneously an operator and a finite linear combination of operator-valued kernels. We have extended the MKL framework to deal with functional response kernel ridge regression and we have proposed a block coordinate descent algorithm to solve the resulting optimization problem. The method is applied on a BCI dataset to predict finger movement in a functional regression setting. Experimental\nINRIA"}, {"heading": "Multiple Operator-valued Kernel Learning 13", "text": "results show that our algorithm achieves good performance outperforming existing methods. It would be interesting for future work to thoroughly compare the proposed MKL method for operator estimation with previous related methods for multi-class and multi-label MKL in the contexts of structured-output learning and collaborative filtering."}, {"heading": "Appendix", "text": ""}, {"heading": "A Existence of Minimizers", "text": "We discuss in this section the existence of minimizers of problems (1) and (6). Because in both problems, we deal with infinite dimensional spaces in the optimization problem, we have to consider appropriate tools for doing so.\nExistence of f\u03bb in the problem given in Equation (1) is guaranteed, for \u03bb > 0 by the generalized Weierstrass Theorem and one of its corollary that we both remind below [16].\nTheorem 2 Let X be a reflexive Banach space and C \u2286 X a weakly closed and bounded set. Suppose h : C 7\u2192 R is a proper lower semi-continuous function. Then h is bounded from below and has a minimizer on C.\nCorollary 3 Let H be a Hilbert space and h : H 7\u2192 R is a strongly lower semicontinuous, convex and coercive function. Then h is bounded from below and attains a minimizer.\nWe can straighforwadly apply this corollary to Problem (1) by defining\nh(f) =\nn\u2211\ni=1\n\u2016yi \u2212 f(xi)\u2016 2 Gy + \u03bb\u2016f\u2016 2 F\nwith f \u2208 F (which is an Hilbert space). It is easy to note that h is continuous and convex. Besides, h is coercive for \u03bb > 0 since \u2016f\u20162F is coercive and the sum involves only positive terms. Hence f\u03bb exists.\nRegarding the MKL problem given in (6), we show existence of a solution in d and {fk} by defining the function, for fixed {dk}\nh1(f1, \u00b7 \u00b7 \u00b7 , fk; {dk}k) =\nn\u2211\ni=1\n\u2016yi \u2212 \u2211\nk\nfk(xi)\u2016 2 Gy + \u03bb\n\u2211\nk\n\u2016fk\u2016 2 F\ndk\nRR n\u00b0 7900"}, {"heading": "14 Kadri, Rakotomamonjy, Bach & Preux", "text": "and by rewriting problem (6) as\nmin d\u2208D J(d) with J(d) = min {fk} h1(f1, \u00b7, , fk; {dk}k)\nUsing similar arguments as above, it can be shown that h1 is proper, strictly convex and coercive for fixed non-negative {dk} and \u03bb > 0 (remind the convention that x0 = 0 if x = 0 and \u221e otherwise). Hence, minimizers of h1 w.r.t. {f1, \u00b7 \u00b7 \u00b7 , fk} exists and are unique. Since the function J(d) which is equal to h1(f \u22c6 1 , \u00b7 \u00b7 \u00b7 , f \u22c6 k ; {dk}k) is continuous over the compact subset of R\nM defined by the constraints on d, it also attains its minimum. This conclude the proof that a solution of problem (6) exists."}, {"heading": "B Dual Formulation of Functional Ridge Regres-", "text": "sion\nEssential computational details regarding the dual formulation of functional ridge regression presented in Section 2 are discussed here.\nThe functional response ridge regression optimization problem has the following form:\nmin f\u2208F\n1 2 \u2016f\u20162F + 1 2n\u03bb\nn\u2211\ni=1\n\u2016\u03bei\u2016 2 Gy\nwith \u03bei = yi \u2212 f(xi).\n(16)\nwhere (xi, yi)i=1,...,n \u2208 (Gx,Gy). Gx and Gy are the Hilbert space L 2(\u2126) which consists of all equivalence classes of square integrable functions on a finite set \u2126, and F is a RKHS whose elements are continuous linear operators on Gx with values in Gy . K is the L(Gy)-valued reproducing kernel of F .\nSince Gx and Gy are functional spaces, to derive a \u201cdual version\u201d of problem (16) we use the method of Lagrange multipliers on Banach spaces which is suitable to solve certain infinite-dimensional constrained optimization problems. The method is a generalization of the classical method of Lagrange multipliers. The existence of Lagrangian multipliers for the problem (16) which involves an equality constraint is guaranteed by Theorem 4.1 1 in [15]. As consequence, the Lagrangian function associated to (16) is well defined and Fre\u0301chet-differentiable. Let \u03b1 = (\u03b1i)i=1,...,n \u2208 G n y the vector of functions containing the Lagrange multipliers, the Lagrangian function is given by\nL(f, \u03b1, \u03be) = 1\n2 \u2016f\u20162F +\n1\n2n\u03bb \u2016\u03be\u20162Gny + \u3008\u03b1, y \u2212 f(x)\u2212 \u03be\u3009Gny , (17)\nwhere \u03b1 = (\u03b11, . . . , \u03b1n) \u2208 G n y , y = (y1, . . . , yn) \u2208 G n y , \u03be = (\u03be1, . . . , \u03ben) \u2208 G n y ,\nf(x) = (f(x1), . . . , f(xn)) \u2208 G n y , and \u2200a, b \u2208 G n y , \u3008a, b\u3009Gny = n\u2211 i=1 \u3008ai, bi\u3009Gy .\nNow, we compute L\u2032(f) the derivative of L(f, \u03b1, \u03be) with respect to f using the Ga\u0302teaux derivative (generalization of the directional derivative) which can be defined for the direction h \u2208 F by:\nDhL(f) = lim \u03c4\u2212\u21920\nL(f + \u03c4h) \u2212 L(f)\n\u03c4 1This theorem considers only equality constraint, but it is a particular case of Theorem 3.1\nin [15] which deals with more general context.\nINRIA"}, {"heading": "Multiple Operator-valued Kernel Learning 15", "text": "Using the fact that DhL(f) = \u3008L \u2032(f), h\u3009F , we obtain\ni. G(f) = \u2016f\u20162F\nlim \u03c4\u2212\u21920\n\u2016f + \u03c4h\u20162F \u2212 \u2016f\u2016 2 F\n\u03c4 = 2\u3008f, h\u3009 =\u21d2 G\n\u2032\n(f) = 2f\nii. H(f) = \u3008\u03b1, y \u2212 f(x)\u2212 \u03be\u3009Gny\nlim \u03c4\u2212\u21920 \u3008\u03b1, y \u2212 f(x)\u2212 \u03c4h(x) \u2212 \u03be\u3009Gny \u2212 \u3008\u03b1, y \u2212 f(x)\u2212 \u03be\u3009Gny \u03c4 = \u2212\u3008\u03b1, h(x)\u3009Gny = \u2212 \u2211 i\u3008\u03b1i, h(xi)\u3009Gy = \u2212\u3008 \u2211 i KF(xi, \u00b7)\u03b1i, h\u3009F (using the reproducing property)\n=\u21d2 H \u2032 (f) = \u2212 \u2211\niKF(xi, \u00b7)\u03b1i\n(i), (ii), and L \u2032 (f) = 0, we obtain the (representer theorem) solution:\nf(\u00b7) = n\u2211\ni=1\nKF(xi, \u00b7)\u03b1i (18)\nSubstituting this into (17), the problem (16) becomes\nmin \u03be max \u03b1\n\u2212 1\n2 \u3008K\u03b1, \u03b1\u3009Gny +\n1\n2n\u03bb \u2016\u03be\u20162Gny + \u3008\u03b1, y \u2212 \u03be\u3009G n y\n(19)\nwhere K = [K(xi, xj)] n i,j=1 is the block operator kernel matrix.\nDifferentiating (19) with respect to \u03be using the same procedure as described above, we obtain \u03be = n\u03bb\u03b1 and then the dual of the functional response ridge regression problem is given by\nmax \u03b1\n\u2212 n\u03bb\n2 \u2016\u03b1\u20162Gny \u2212\n1 2 \u3008K\u03b1, \u03b1\u3009Gny + \u3008\u03b1, y\u3009Gny (20)"}, {"heading": "C Convergence of Algorithm 1", "text": "In this section, we present a proof of convergence of Algorithm 1. The proof is an extension of results obtained by [2] and [24] to infinite dimensional Hilbert spaces with operator-valued reproducing kernels. Let R(f, d) be the objective function of the MovKL problem defined by (6):\nR(f, d) = L+\nM\u2211\nk=1\n\u2016fk\u2016 2 Fk\ndk\nwhere L = 1\u03bb \u2211 i \u2016yi \u2212 \u2211M k=1 fk(xi)\u2016 2 Gy . Substituting Equation (10) in R we obtain the objective function:\nS(f) := R(f, d(f)) = L+\n( M\u2211\nk=1\n\u2016fk\u2016 2r r+1\nFk\n) r+1 r\nThe function S is strictly convex since L is convex and the function defined by f 7\u2212\u2192 (\u2211M\nk=1 \u2016fk\u2016 2r r+1\n) r+1 r\nis strictly convex (this follows directly from strict\nRR n\u00b0 7900"}, {"heading": "16 Kadri, Rakotomamonjy, Bach & Preux", "text": "convexity of the function x 7\u2212\u2192 xp when x \u2265 0 and p > 1). Thus, S(f) admits a unique minimizer.\nNow let us define the function g by:\ng(f) = min u {R(u, d(f))}.\nThe function g is continuous. This comes from the fact that the function:\nG(d) = min u {R(u, d)}\nis continuous. Indeed, G is the minimal of value of a functional response kernel ridge regression problem in a function-valued RKHS associated to an operatorvalued kernel K. So, G(d) = R(d, u\u2217) with u\u2217 = ( K(d) + \u03bbI )\u22121 y (see Equation (8)). u\u2217 is continuous, and hence G(d) is also continuous. By definition we have S(f) = R(f, d(f)), and since d(f) minimizes R(f, \u00b7), we obtain that: S(f (n+1)) \u2264 g(f (n)) \u2264 S(f (n))\nwhere n is the number of iteration. So, the sequence {S(f (n)), n \u2208 N} is nonincreasing and then it is bounded since L is bounded from below. Thus, as n \u2212\u2192 \u221e, S(f (n)) converges to a number which we denote by S\u2217. {S(f (n))} is convergent and S is a coercive function, then the sequence {\u2016f (n)\u2016, n \u2208 N} is bounded. Consequently, the sequence {f (n), n \u2208 N} is bounded.\nNext we show the following subsequence convergence property which underlies the convergence of Algorithm 1.\nProposition 4 If F is a RKHS associated to a compact-operator-valued kernel, the sequence {f (n) \u2208 F , n \u2208 N}, since it is bounded, has a convergent subsequence.\nProof. The analogue of the Bolzano-Weierstrass theorem2 in Hilbert spaces states that there exists a weakly convergent subsequence {f (nl), l \u2208 N} of the bounded sequence {f (n)} which (weakly) converges to f \u2208 F . By definition of weakly convergence, we have \u2200g \u2208 F (F is a RKHS with the operator-valued kernel K):\nlim nl\u2192\u221e\n\u3008f (nl)(\u00b7), g\u3009F = \u3008f(\u00b7), g\u3009F (21)\nLet g = K(x, \u00b7)\u03b2. Using the reproducing property, we obtain\nlim nl\u2192\u221e \u3008f (nl)(\u00b7), g\u3009F = lim nl\u2192\u221e \u3008f (nl)(x), \u03b2\u3009Gy and \u3008f(\u00b7), g\u3009F = \u3008f(x), \u03b2\u3009Gy\n\u21d2 lim nl\u2192\u221e\n\u3008f (nl)(x), \u03b2\u3009Gy = \u3008f(x), \u03b2\u3009Gy using (21)\nThus the subsequence {f (nl)(\u00b7)} is (weakly) pointwise convergent. Now we show that:\nlim nl\u2192\u221e\n\u2016f (nl)(\u00b7)\u2016F = \u2016f(\u00b7)\u2016F (\u2217)\n2The Bolzano-Weierstrass theorem states that each bounded sequence in Rn has a convergent subsequence. For infinite-dimensional spaces, strong convergence of the subsequence is not reached and only weak convergence is obtained. Proposition 4 shows that strong convergence can be reached for the sequence {f(n), n \u2208 N} solution of our MovKL optimization problem in Hilbert spaces with reproducing compact operator-valued kernels.\nINRIA"}, {"heading": "Multiple Operator-valued Kernel Learning 17", "text": "Since f (nl) \u2208 F is solution of the minimization of the optimization problem (6) with the kernel combination coefficients dk fixed, it can be written as\u2211 i K(xi, \u00b7)\u03b1 (nl) i (representer theorem). We now that f (nl)(x) converges weakly\nto f(x), so \u03b1(nl) = ( \u03b1 (nl) i ) i\u22651 converges weakly to \u03b1 \u2208 Gny . Indeed, \u2200\u03b2 \u2208 Gy we have:\nlim nl\u2192\u221e\n\u3008f (nl)(x), \u03b2\u3009Gy = \u3008f(x), \u03b2\u3009Gy\n\u21d2 lim nl\u2192\u221e\n\u3008 \u2211\ni\nK(xi, x)\u03b1 (nl) i , \u03b2\u3009Gy = \u3008\n\u2211\ni\nK(xi, x)\u03b1i, \u03b2\u3009Gy\n\u21d2 lim nl\u2192\u221e\n\u3008Kx\u03b1 (nl), \u03b2\u3009Gy = \u3008Kx\u03b1, \u03b2\u3009Gy where Kx is the row vector (K(xi, x))i\u22651\n\u21d2 lim nl\u2192\u221e\n\u3008\u03b1(nl),K\u2217 x \u03b2\u3009Gny = \u3008\u03b1,K \u2217 x \u03b2\u3009Gny\n\u21d2 lim nl\u2192\u221e\n\u3008\u03b1(nl), z\u3009Gny = \u3008\u03b1, z\u3009Gny \u2200z \u2208 G n y\n\u21d2 \u03b1(nl) converges weakly to \u03b1\nMoreover\nlim nl\u2192\u221e \u2016f (nl)(\u00b7)\u20162F = limnl\u2192\u221e \u3008 \u2211\ni\nK(xi, \u00b7)\u03b1 (nl) i ,\n\u2211\nj\nK(xj , \u00b7)\u03b1 (nl) j \u3009F\n= lim nl\u2192\u221e\n\u2211\ni,j\n\u3008K(xi, xj)\u03b1 (nl) i , \u03b1 (nl) j \u3009Gy (using the reproducing property)\n= lim nl\u2192\u221e\n\u3008K\u03b1(nl), \u03b1(nl)\u3009Gny\n= \u3008K\u03b1, \u03b1\u3009Gny (because of the compactness 3 of K) = \u2211\ni,j\n\u3008K(xi, xj)\u03b1i, \u03b1j\u3009Gy\n= \u2211\ni,j\n\u3008K(xi, \u00b7)\u03b1i,K(xj , \u00b7)\u03b1j\u3009F = \u2016f\u2016 2 F\nUsing (\u2217) and weak convergence, we obtain the strong convergence of the subsequence {f (nl)}.\nlim nl\u2192\u221e \u2016f (nl) \u2212 f\u20162F = limnl\u2192\u221e \u3008f (nl) \u2212 f, f (nl)\u3009F \u2212 \u3008f (nl) \u2212 f, f\u3009F\n= lim nl\u2192\u221e \u2016f (nl)\u20162F \u2212 limnl\u2192\u221e 2\u3008f, f (nl)\u3009F + \u2016f\u2016 2 F\n= lim nl\u2192\u221e\n\u2016f (nl)\u20162F \u2212 \u2016f\u2016 2 F (using weak convergence)\n= 0 (using(\u2217))\n\u21d2 f (nl) converges strongly to f\nBy Proposition 4, there exists a convergent subsequence {f (nl), l \u2208 N} of the bounded sequence {f (n), n \u2208 N}, whose limit we denote by f\u2217. Since S(f (n+1)) \u2264 g(f (n)) \u2264 S(f (n)) , g(f (n)) converges to S\u2217. Thus, by the continuity\n3Compact operator maps weakly convergent sequences into strongly convergent sequences.\nRR n\u00b0 7900"}, {"heading": "18 Kadri, Rakotomamonjy, Bach & Preux", "text": "of g and S, g(f\u2217) = S(f\u2217). This implies that f\u2217 is a minimizer of R(\u00b7, d(f\u2217)), because R(f\u2217, d(f\u2217)) = S(f\u2217). Moreover, d(f\u2217) is the minimizer of R(\u00b7, f\u2217) subject to the constraints on d. Thus, since the objective function R is smooth, the pair (f\u2217, d(f\u2217)) is the minimizer of R.\nAt this stage, we have shown that any convergent subsequence of {f (n), n \u2208 N} converges to the minimizer of R. Since the sequence {f (n), n \u2208 N} is bounded, it follows that the whole sequence converges to minimizer of R."}, {"heading": "Multiple Operator-valued Kernel Learning 19", "text": "[15] S. Kurcyusz. On the existence and nonexistence of lagrange multipliers in Banach spaces. Journal of Optimization Theory and Applications, 20:81\u2013 110, 1976.\n[16] A. Kurdila and M. Zabarankin. Convex Functional Analysis. Birkhauser Verlag, 2005.\n[17] G. Lanckriet, N. Cristianini, L. El Ghaoui, P. Bartlett, and M. Jordan. Learning the kernel matrix with semi-definite programming. JMLR, 5:27\u2013 72, 2004.\n[18] H. Lian. Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces. The Canadian Journal of Statistics, 35:597\u2013606, 2007.\n[19] C. Micchelli and M. Pontil. Learning the kernel function via regularization. JMLR, 6:1099\u20131125, 2005.\n[20] C. A. Micchelli and M. Pontil. On learning vector-valued functions. Neural Computation, 17:177\u2013204, 2005.\n[21] K. J. Miller and G. Schalk. Prediction of finger flexion: 4th brain-computer interface data competition. BCI Competition IV, 2008.\n[22] T. Pistohl, T. Ball, A. Schulze-Bonhage, A. Aertsen, and C. Mehring. Prediction of arm movement trajectories from ecog-recordings in humans. Journal of Neuroscience Methods, 167(1):105\u2013114, 2008.\n[23] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and S. Canu. SimpleMKL. JMLR, 9:2491\u20132521, 2008.\n[24] A. Rakotomamonjy, R. Flamary, G. Gasso, and S. Canu. l(p)-l(q) penalty for sparse linear and sparse multiple kernel multitask learning. IEEE Trans. Neural Netw., 22(8):1307\u201320, 2011.\n[25] J. O. Ramsay and B. W. Silverman. Functional Data Analysis, 2nd ed. Springer Verlag, New York, 2005.\n[26] John A. Rice and B. W. Silverman. Estimating the mean and covariance structure nonparametrically when the data are curves. Journal of the Royal Statistical Society. Series B, 53(1):233\u2013243, 1991.\n[27] G. Schalk, J. Kubanek, K. J. Miller, N. R. Anderson, E. C. Leuthardt, J. G. Ojemann, D. Limbrick, D. Moran, L. A. Gerhardt, and J. R. Wolpaw. Decoding two-dimensional movement trajectories using electrocorticographic signals in humans. Journal of Neural Engineering, 4(3):264\u2013275, 2007.\n[28] G. Schalk, D. J. McFarland, T. Hinterberger, N. Birbaumer, and J. R. Wolpaw. BCI2000: a general-purpose brain-computer interface system. Biomedical Engineering, IEEE Trans. on, 51:1034\u20131043, 2004.\n[29] B. Scho\u0308lkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA, 2002.\nRR n\u00b0 7900"}, {"heading": "20 Kadri, Rakotomamonjy, Bach & Preux", "text": "[30] S. Sonnenburg, G. Ra\u0308tsch, C. Scha\u0308fer, and B. Scho\u0308lkopf. Large scale multiple kernel learning. JMLR, 7:1531\u20131565, 2006.\n[31] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. J. Optim. Theory Appl., 109:475\u2013494, 2001.\nINRIA\nCentre de recherche INRIA Lille \u2013 Nord Europe Parc Scientifique de la Haute Borne - 40, avenue Halley - 59650 Villeneuve d\u2019Ascq (France)\nCentre de recherche INRIA Bordeaux \u2013 Sud Ouest : Domaine Universitaire - 351, cours de la Lib\u00e9ration - 33405 Talence Cedex Centre de recherche INRIA Grenoble \u2013 Rh\u00f4ne-Alpes : 655, avenue de l\u2019Europe - 38334 Montbonnot Saint-Ismier\nCentre de recherche INRIA Nancy \u2013 Grand Est : LORIA, Technop\u00f4le de Nancy-Brabois - Campus scientifique 615, rue du Jardin Botanique - BP 101 - 54602 Villers-l\u00e8s-Nancy Cedex\nCentre de recherche INRIA Paris \u2013 Rocquencourt : Domaine de Voluceau - Rocquencourt - BP 105 - 78153 Le Chesnay Cedex Centre de recherche INRIA Rennes \u2013 Bretagne Atlantique : IRISA, Campus universitaire de Beaulieu - 35042 Rennes Cedex\nCentre de recherche INRIA Saclay \u2013 \u00cele-de-France : Parc Orsay Universit\u00e9 - ZAC des Vignes : 4, rue Jacques Monod - 91893 Orsay Cedex Centre de recherche INRIA Sophia Antipolis \u2013 M\u00e9diterran\u00e9e : 2004, route des Lucioles - BP 93 - 06902 Sophia Antipolis Cedex\n\u00c9diteur INRIA - Domaine de Voluceau - Rocquencourt, BP 105 - 78153 Le Chesnay Cedex (France)\nhttp://www.inria.fr\nISSN 0249-6399"}], "references": [{"title": "Variable sparsity kernel learning", "author": ["J. Aflalo", "A. Ben-Tal", "C. Bhattacharyya", "J. Saketha Nath", "S. Raman"], "venue": "JMLR, 12:565\u2013592", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73(3):243\u2013272", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Consistency of the group Lasso and multiple kernel learning", "author": ["F. Bach"], "venue": "JMLR, 9:1179\u20131225", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "F", "author": ["C. Brouard"], "venue": "d\u2019Alch\u00e9-Buc, and M. Szafranski. Semi-supervised penalized output kernel regression for link prediction. In Proc. ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal multitask kernels", "author": ["A. Caponnetto", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "JMLR, 68:1615\u20131646", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Vector valued reproducing kernel Hilbert spaces of integrable functions and mercer theorem", "author": ["C. Carmeli", "E. De Vito", "A. Toigo"], "venue": "Analysis and Applications, 4:377\u2013408", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Vector valued reproducing kernel Hilbert spaces and universality", "author": ["C. Carmeli", "E. De Vito", "A. Toigo"], "venue": "Analysis and Applications, 8:19\u201361", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "L2 regularization for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Proc. UAI", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Generalization bounds for learning kernels", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "Proc. ICML", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C.S. Ong", "P. Gehler", "G. Pillonetto"], "venue": "Proc. ICML", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "JMLR, 6:615\u2013637", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonlinear functional regression: a functional RKHS approach", "author": ["H. Kadri", "E. Duflos", "P. Preux", "S. Canu", "M. Davy"], "venue": "Proc. AISTATS, pages 111\u2013 125", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Functional regularized least squares classification with operator-valued kernels", "author": ["H. Kadri", "A. Rabaoui", "P. Preux", "E. Duflos", "A. Rakotomamonjy"], "venue": "Proc. ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "JMLR, 12:953\u2013997", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "On the existence and nonexistence of lagrange multipliers in Banach spaces", "author": ["S. Kurcyusz"], "venue": "Journal of Optimization Theory and Applications, 20:81\u2013 110", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1976}, {"title": "Convex Functional Analysis", "author": ["A. Kurdila", "M. Zabarankin"], "venue": "Birkhauser Verlag", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning the kernel matrix with semi-definite programming", "author": ["G. Lanckriet", "N. Cristianini", "L. El Ghaoui", "P. Bartlett", "M. Jordan"], "venue": "JMLR, 5:27\u2013 72", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonlinear functional models for functional responses in reproducing kernel Hilbert spaces", "author": ["H. Lian"], "venue": "The Canadian Journal of Statistics, 35:597\u2013606", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning the kernel function via regularization", "author": ["C. Micchelli", "M. Pontil"], "venue": "JMLR, 6:1099\u20131125", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Prediction of finger flexion: 4th brain-computer interface data competition", "author": ["K.J. Miller", "G. Schalk"], "venue": "BCI Competition IV", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Prediction of arm movement trajectories from ecog-recordings in humans", "author": ["T. Pistohl", "T. Ball", "A. Schulze-Bonhage", "A. Aertsen", "C. Mehring"], "venue": "Journal of Neuroscience Methods, 167(1):105\u2013114", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "Y. Grandvalet", "S. Canu"], "venue": "JMLR, 9:2491\u20132521", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "l(p)-l(q) penalty for sparse linear and sparse multiple kernel multitask learning", "author": ["A. Rakotomamonjy", "R. Flamary", "G. Gasso", "S. Canu"], "venue": "IEEE Trans. Neural Netw., 22(8):1307\u201320", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Functional Data Analysis", "author": ["J.O. Ramsay", "B.W. Silverman"], "venue": "2nd ed. Springer Verlag, New York", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimating the mean and covariance structure nonparametrically when the data are curves", "author": ["John A. Rice", "B.W. Silverman"], "venue": "Journal of the Royal Statistical Society. Series B,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "Decoding two-dimensional movement trajectories using electrocorticographic signals in humans", "author": ["G. Schalk", "J. Kubanek", "K.J. Miller", "N.R. Anderson", "E.C. Leuthardt", "J.G. Ojemann", "D. Limbrick", "D. Moran", "L.A. Gerhardt", "J.R. Wolpaw"], "venue": "Journal of Neural Engineering, 4(3):264\u2013275", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "BCI2000: a general-purpose brain-computer interface system", "author": ["G. Schalk", "D.J. McFarland", "T. Hinterberger", "N. Birbaumer", "J.R. Wolpaw"], "venue": "Biomedical Engineering, IEEE Trans. on, 51:1034\u20131043", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning with Kernels: Support Vector Machines", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond. MIT Press, Cambridge, MA, USA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "JMLR, 7:1531\u20131565", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Convergence of block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., 109:475\u2013494, 2001. INRIA  Centre de recherche INRIA Lille \u2013 Nord Europe Parc Scientifique de la Haute Borne - 40, avenue Halley - 59650 Villeneuve d\u2019Ascq ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 19, "context": "Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7].", "startOffset": 85, "endOffset": 95}, {"referenceID": 4, "context": "Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7].", "startOffset": 85, "endOffset": 95}, {"referenceID": 6, "context": "Recently, there has been considerable interest in estimating vector-valued functions [20, 5, 7].", "startOffset": 85, "endOffset": 95}, {"referenceID": 10, "context": "Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "Typical learning situations include multi-task learning [11], functional regression [12], and structured output prediction [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 26, "context": "More precisely, we are interested in finger movement prediction from electrocorticographic signals [27].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Indeed, for these problems, one of the difficulties arises from the unknown latency between the signal related to the finger movement and the actual movement [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 28, "context": "Working in such RKHSs, we are able to draw on the important core of work that has been performed on scalar-valued and vector-valued RKHSs [29, 20].", "startOffset": 138, "endOffset": 146}, {"referenceID": 19, "context": "Working in such RKHSs, we are able to draw on the important core of work that has been performed on scalar-valued and vector-valued RKHSs [29, 20].", "startOffset": 138, "endOffset": 146}, {"referenceID": 11, "context": "Such a functional RKHS framework and associated operator-valued kernels have been introduced recently [12, 13].", "startOffset": 102, "endOffset": 110}, {"referenceID": 12, "context": "Such a functional RKHS framework and associated operator-valued kernels have been introduced recently [12, 13].", "startOffset": 102, "endOffset": 110}, {"referenceID": 16, "context": "In order to overcome the need for choosing a kernel before the learning process, several works have tried to address the problem of learning the scalar-valued kernel jointly with the decision function [17, 30].", "startOffset": 201, "endOffset": 209}, {"referenceID": 29, "context": "In order to overcome the need for choosing a kernel before the learning process, several works have tried to address the problem of learning the scalar-valued kernel jointly with the decision function [17, 30].", "startOffset": 201, "endOffset": 209}, {"referenceID": 8, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 126, "endOffset": 132}, {"referenceID": 2, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 126, "endOffset": 132}, {"referenceID": 22, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 177, "endOffset": 188}, {"referenceID": 0, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 177, "endOffset": 188}, {"referenceID": 13, "context": "Since these seminal works, many efforts have been carried out in order to theoretically analyze the kernel learning framework [9, 3] or in order to provide efficient algorithms [23, 1, 14].", "startOffset": 177, "endOffset": 188}, {"referenceID": 9, "context": "It should be pointed out that in a recent work [10], the problem of learning the output kernel was", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "Before describing the multiple operator-valued kernel learning algorithm that we will study and experiment with in this paper, we first review notions and properties of reproducing kernel Hilbert spaces with operator-valued kernels, show their connection to learning from multiple response data (multiple outputs; see [20] for discrete data and [12] for continuous data), and describe the optimization problem for learning kernels with functional response ridge regression.", "startOffset": 318, "endOffset": 322}, {"referenceID": 11, "context": "Before describing the multiple operator-valued kernel learning algorithm that we will study and experiment with in this paper, we first review notions and properties of reproducing kernel Hilbert spaces with operator-valued kernels, show their connection to learning from multiple response data (multiple outputs; see [20] for discrete data and [12] for continuous data), and describe the optimization problem for learning kernels with functional response ridge regression.", "startOffset": 345, "endOffset": 349}, {"referenceID": 19, "context": "The proof of Theorem 1 can be found in [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": ", [5, 6, 7].", "startOffset": 2, "endOffset": 11}, {"referenceID": 5, "context": ", [5, 6, 7].", "startOffset": 2, "endOffset": 11}, {"referenceID": 6, "context": ", [5, 6, 7].", "startOffset": 2, "endOffset": 11}, {"referenceID": 14, "context": "For more details, see [15].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "Before deriving the dual of this problem, it can be shown by means of the generalized Weierstrass theorem [16] that this problem admits a solution (a detailed proof is provided in the supplementary material).", "startOffset": 106, "endOffset": 110}, {"referenceID": 22, "context": "Now, following the lines of [23], a dualization of this problem leads to the following equivalent one", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "Hence, we propose to borrow from [14], and consider a blockcoordinate descent method.", "startOffset": 33, "endOffset": 37}, {"referenceID": 30, "context": "The convergence of a block coordinate descent algorithm which is related closely to the Gauss-Seidel method was studied in works of [31] and others.", "startOffset": 132, "endOffset": 136}, {"referenceID": 18, "context": "which has a closed-form solution and for which optimality occurs at [19]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "This algorithm is similar to that of [8] and [14] both being based on alternating optimization.", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "This algorithm is similar to that of [8] and [14] both being based on alternating optimization.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "The proof is actually an extension of results obtained by [2] and [24] for scalar-valued kernels.", "startOffset": 58, "endOffset": 61}, {"referenceID": 23, "context": "The proof is actually an extension of results obtained by [2] and [24] for scalar-valued kernels.", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "In multi-task learning, T is a finite dimensional matrix that is expected to share information between tasks [11, 5].", "startOffset": 109, "endOffset": 116}, {"referenceID": 4, "context": "In multi-task learning, T is a finite dimensional matrix that is expected to share information between tasks [11, 5].", "startOffset": 109, "endOffset": 116}, {"referenceID": 11, "context": "More recently and for supervised functional output learning problems, T is chosen to be a multiplication or an integral operator [12, 13].", "startOffset": 129, "endOffset": 137}, {"referenceID": 12, "context": "More recently and for supervised functional output learning problems, T is chosen to be a multiplication or an integral operator [12, 13].", "startOffset": 129, "endOffset": 137}, {"referenceID": 24, "context": "This choice is motivated by the fact that functional linear models for functional responses [25] are based on these operators and then such kernels provide an interesting alternative to extend these models to nonlinear contexts.", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "In addition, some works on functional regression and structured-output learning consider operator-valued kernels constructed from the identity operator as in [18]", "startOffset": 158, "endOffset": 162}, {"referenceID": 3, "context": "and [4].", "startOffset": 4, "endOffset": 7}, {"referenceID": 12, "context": "Thus we can compute an analytic solution of the system of equations (8) by inverting K+ \u03bbI using the eigendecompositions of G and T as in [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": "To this aim, the fourth dataset from the BCI Competition IV [21] was used.", "startOffset": 60, "endOffset": 64}, {"referenceID": 27, "context": "ECoG signals of the subject were recorded at a 1KHz sampling using BCI2000 [28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 20, "context": "Due to the acquisition process, a delay appears between the finger movement and the measured ECoG signal [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "For the multiple operator-valued kernels having the form (12), we have used a Gaussian kernel with 5 different bandwidths and a polynomial kernel of degree 1 to 3 combined with three operators T : identity Ty(t) = y(t), multiplication operator associated with the function e 2 defined by Ty(t) = e 2 y(t), and the integral Hilbert-Schmidt operator with the kernel e proposed in [13], Ty(t) = \u222b ey(s)ds.", "startOffset": 378, "endOffset": 382}, {"referenceID": 12, "context": "While the inverses of the identity and the multiplication operators are easily and directly computable from the analytic expressions of the operators, the inverse of the integral operator is computed from its spectral decomposition as in [13].", "startOffset": 238, "endOffset": 242}, {"referenceID": 25, "context": "The number of eigenfunctions as well as the regularization parameter \u03bb are fixed using \u201cone-curve-leave-out cross-validation\u201d [26] with the aim of minimizing the residual sum of squares error.", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "We compare our approach in the case of l1 and l2-norm constraint on the combination coefficients with: (1) the baseline scalar-valued kernel ridge regression algorithm by considering each output independently of the others, (2) functional response ridge regression using an integral operator-valued kernel [13], (3) kernel ridge regression with an evenly-weighted sum of operator-valued kernels, which we denote by l\u221e-norm MovKL.", "startOffset": 306, "endOffset": 310}, {"referenceID": 15, "context": "Existence of f\u03bb in the problem given in Equation (1) is guaranteed, for \u03bb > 0 by the generalized Weierstrass Theorem and one of its corollary that we both remind below [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 14, "context": "1 1 in [15].", "startOffset": 7, "endOffset": 11}, {"referenceID": 14, "context": "1 in [15] which deals with more general context.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "The proof is an extension of results obtained by [2] and [24] to infinite dimensional Hilbert spaces with operator-valued reproducing kernels.", "startOffset": 49, "endOffset": 52}, {"referenceID": 23, "context": "The proof is an extension of results obtained by [2] and [24] to infinite dimensional Hilbert spaces with operator-valued reproducing kernels.", "startOffset": 57, "endOffset": 61}], "year": 2012, "abstractText": "Positive definite operator-valued kernels generalize the well-known notion of reproducing kernels, and are naturally adapted to multi-output learning situations. This paper addresses the problem of learning a finite linear combination of infinite-dimensional operator-valued kernels which are suitable for extending functional data analysis methods to nonlinear contexts. We study this problem in the case of kernel ridge regression for functional responses with an lr-norm constraint on the combination coefficients (r \u2265 1). The resulting optimization problem is more involved than those of multiple scalar-valued kernel learning since operator-valued kernels pose more technical and theoretical issues. We propose a multiple operator-valued kernel learning algorithm based on solving a system of linear operator equations by using a block coordinatedescent procedure. We experimentally validate our approach on a functional regression task in the context of finger movement prediction in brain-computer interfaces. Key-words: Operator-valued kernels, multiple kernel learning, nonparametric functional data analysis, function-valued reproducing kernel Hilbert spaces. \u2217 Sequel Team, INRIA Lille. E-mail: hachem.kadri@inria.fr \u2020 LITIS, Universit\u00e9 de Rouen. E-mail: alain.rakotomamonjy@insa-rouen.fr \u2021 Sierra Team/INRIA, Ecole Normale Sup\u00e9rieure. E-mail: francis.bach@inria.fr \u00a7 Sequel/INRIA-Lille, LIFL/CNRS. E-mail: philippe.preux@inria.fr Apprentissage de Noyaux \u00e0 Valeurs Op\u00e9rateurs Multiples R\u00e9sum\u00e9 : Dans cet article, nous proposons une m\u00e9thode d\u2019apprentissage de noyaux multiples \u00e0 valeurs op\u00e9rateurs dans le cas d\u2019une r\u00e9gression ridge \u00e0 r\u00e9ponse fonctionnelle. Notre m\u00e9thode est bas\u00e9e sur la r\u00e9solution d\u2019un syst\u00e8me d\u2019\u00e9quations lin\u00e9aires d\u2019op\u00e9rateurs en utilisant une proc\u00e9dure de type Iterative Coordinate Descent. Nous validons exp\u00e9rimentalement notre approche sur un probl\u00e8me de pr\u00e9diction de mouvement de doigt dans un contexte d\u2019Interface Cerveau-Machine. Mots-cl\u00e9s : noyaux \u00e0 valeurs op\u00e9rateurs, apprentissage de noyaux multiples, analyse des donn\u00e9es fonctionnelles, espace de Hilbert \u00e0 noyau reproduisant Multiple Operator-valued Kernel Learning 3", "creator": "LaTeX with hyperref package"}}}