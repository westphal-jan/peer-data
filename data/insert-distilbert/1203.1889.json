{"id": "1203.1889", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2012", "title": "Distributional Measures as Proxies for Semantic Relatedness", "abstract": "the automatic ranking of word pairs as per their semantic perceived relatedness and ability to mimic human notions of semantic relatedness specifically has widespread applications. measures however that rely on raw data ( distributional measures ) primarily and those that use knowledge - rich structured ontologies both exist. although extensive studies occasionally have been performed intending to compare ontological measures with human judgment, the distributional measures have primarily been evaluated by indirect means. given this paper is a detailed study of considering some differences of the three major distributional measures ; it lists their respective merits and limitations. new research measures that overcome these drawbacks, properties that are more in line with the human notions of human semantic relatedness, are suggested. the paper concludes with an exhaustive sample comparison of the distributional and ontology - based measures. along the way, significant research problems are identified. much work on these problems may consistently lead to a better understanding of how semantic relatedness is to be measured.", "histories": [["v1", "Thu, 8 Mar 2012 19:03:37 GMT  (66kb)", "http://arxiv.org/abs/1203.1889v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["saif m mohammad", "graeme hirst"], "accepted": false, "id": "1203.1889"}, "pdf": {"name": "1203.1889.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Graeme Hirst"], "emails": ["(smm@cs.toronto.edu)", "(gh@cs.toronto.edu)"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n18 89\nv1 [\ncs .C\nL ]\n8 M\nar 2\n01 2\nKeywords: Distributional similarity/relatedness, semantic similarity/relatedness, word association, relative entropy, asymmetric measures, compositional/non-compositional measures, pseudo-fuzzy metrics\nAbbreviations: PCM \u2013 Primary Compositional Measure; CRM \u2013 Co-occurrence Retrieval Model; KLD \u2013 Kullback-Leibler Divergence; PMI \u2013 Pointwise Mutual Information"}, {"heading": "1. Introduction", "text": "Humans are inherently capable of determining whether one word pair is more semantically related than another. For example, given the word pairs honey\u2013 bee and paper\u2013car, one can easily identify the former pair to be more semantically related than the latter. This, however, is not true for machines. A lot of work has been done in automating the process in the last fifteen years. While some approaches do better than others and have been applied to solving practical problems, none has matched human judgment.\nTypically, automated systems assign a score of semantic relatedness to a given pair of words (target words) calculated from a relatedness measure. The absolute score is usually irrelevant on its own. For example, a relatedness score of 0.7 between a and b, in a possible range of 0 to 1, does not imply that a and b are more related than the average word pair. However, given that the semantic relatedness of c and d is 0.6, the system can conclude that a and b are more related than c and d. Thus even though the absolute score given by a relatedness measure is not of much significance, it is important that\nc\u00a9 2005 Saif Mohammad and Graeme Hirst. This is an unpublished manuscript, first made available on the Web in 2005.\n2 Saif Mohammad and Graeme Hirst\nthe measure give a higher score to word pairs which humans think are more related and comparatively lower scores to word pairs that are less related. This ability to mimic human judgment of semantic relatedness has been used in numerous applications such as automated spelling correction, word sense disambiguation, thesaurus creation, information retrieval, text summarization, and identifying discourse structure.\nExisting measures of semantic relatedness rely either on ontologies and semantic networks or just raw text. Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) do an extensive survey and comparison of the various WordNet-based measures. Measures that use just raw text, known as the distributional measures, have been described individually (for example, in Schu\u0308tze and Pedersen (1997), Hindle (1990), Lin (1998a), Pereira et al. (1993), etc) but have not been extensively compared among each other. This paper focuses on distributional measures and analyzes their strengths and limitations. Particular attention is paid to the different kinds of distributional measures and their components. New measures are suggested that overcome some of their drawbacks. Characteristics of WordNet-based and distributional measures are contrasted and finally, future research directions are suggested which may determine a better understanding of semantic relatedness."}, {"heading": "2. Background", "text": ""}, {"heading": "2.1. CO-OCCURRENCES", "text": "Words that occur within a certain window of a target word are called the cooccurrences of the word. The window size may be a few words on either side, the complete sentence, a paragraph or the entire document. Consider the sentence below:\nthe plane flew through a cloud\nIf we consider the window size to be the complete sentence, flew co-occurs with the, plane, through, a and cloud. The set of words that co-occur with a word constitute the context of the word. They are used in tasks such as information retrieval, word sense disambiguation, and semantic relatedness.\nDistributional Measures as Proxies for Semantic Relatedness 3"}, {"heading": "2.2. WORD ASSOCIATION RATIO", "text": "Given two events x and y with probabilities P(x) and P(y), their pointwise mutual information (Fano, 1961)1, PMI for short, or just I, is defined as follows:\nI(x,y) = log2 P(x,y)\nP(x)P(y) (1)\nP(x,y) is the joint probability of x and y. If I(x,y) evaluates to be close to zero, i,e, P(x,y) \u2248 P(x)\u00d7P(y), then it means that events x and y occur together just as often as is expected from their individual probabilities. If I(x,y) \u226b 0, it implies that x and y occur together more often than would be expected from their individual probabilities and hence have a strong correlation.\nChurch and Hanks (1989)1 introduce word association ratio, which is similar to pointwise mutual information. If x and y are words with probabilities P(x) and P(y) (estimated by corpus counts), their association ratio is defined to be the same as in (1), except that P(x,y) stands for the probability that x appears, within a certain window, before y. It should be noted that P(x,y) is no longer symmetric (P(x,y) 6= P(y,x)) as P(x,y) and P(y,x) represent two different events. If two words have a word association ratio close to zero then they do not share an interesting relationship but if I(w1,w2) \u226b 0, then w2 follows w1 (within a certain window) more often than chance and the words w1 and w2 are strong co-occurrences. Theoretically, word association ratio may yield negative values (word pair occurs less frequently than expected by random chance) but Church and Hanks (1989) show that it is hard to accurately predict negative word association ratios with confidence. Systems which use word association ratio may be adversely affected by this. A common approach to counter this is to equate the negative association values to 0 (for example, Lin (1998a)). This usually means that the system will ignore such words.\nA problem with PMI in general (which is inherited by word association ratio) is that low frequency events get higher scores than expected. Pantel and Lin (2002) try to overcome this by multiplying the PMI value with a correction factor. Although, Pantel and Lin give the correction factor for word association ratio using syntactically related co-occurring words, a more generic form applicable for pointwise mutual information is as shown below:\nIcorrected(x,y) = log2 P(x,y)\nP(x)P(y) \u00d7 min(freq(x), freq(y)) min(freq(x), freq(y))+1\n(2)\nThe correction factor is large (close to 1) if both the events occur a large number of times and small (close to 0) if any of the two events occurs very few times.\n4 Saif Mohammad and Graeme Hirst"}, {"heading": "2.3. RELATEDNESS VS SIMILARITY", "text": "A closely related concept to semantic relatedness is semantic similarity. While there is some overlap in their meanings and they may be used interchangeably in certain contexts, it is important to be aware of their distinction. Budanitsky and Hirst (2001) and Budanitsky and Hirst (2004) point out that semantic similarity is used when similar entities such as apples and bananas or table and furniture are compared. These entities are close to each other in an is-a hierarchy. For example, apples and bananas are hyponyms of fruit and table is a hyponym of furniture. However, even dissimilar entities may be semantically related, for example, door and knob, tree and shade, or gym and weights. In this case the two entities are not similar per se, but are related by some relationship. This relationship may be one of the classical relationships such as meronymy (is part of) as in door\u2013knob or a non-classical one as in tree\u2013shade and gym\u2013weights. Thus two entities are semantically related if they are semantically similar (close together in the is-a hierarchy) or share any other classical or non-classical relationships. As Budanitsky and Hirst (2004) point out, semantic similarity is a subset of semantic relatedness.\nThe concept of semantic distance has traditionally been used in the context of both semantic relatedness and semantic similarity. In the former context, it represents the inverse of semantic relatedness, while in the latter, it is the inverse of semantic similarity. In this paper as well, we shall continue to use the term for both concepts with the confidence that the context will disambiguate the intended meaning."}, {"heading": "2.4. THE DISTRIBUTIONAL HYPOTHESIS", "text": "Given a text corpus, individual words have more or less differing contexts around them. The context of a word is composed of words co-occurring with it within a certain window around it. Distributional measures use statistics acquired from a large text corpora to determine how similar the contexts of two words are. These measures are also used as proxies to measures of semantic similarity as words found in similar contexts tend to be semantically similar. This is known as the distributional hypothesis (Firth (1957) and Harris (1968)) and such measures have traditionally been referred to as measures of distributional similarity.\nThe hypothesis makes intuitive sense as Budanitsky and Hirst (2004) point out. If two words have many co-occurring words then similar things are being said about both of them and so they are likely to be semantically similar. Conversely, if two words are semantically similar then they are likely to be used in a similar fashion in text and thus end up with many common co-occurrences. For example, the semantically similar bug and insect are expected to have a number of common co-occurring words such as crawl, squash, small, woods, and so on, in a large enough text corpus.\nDistributional Measures as Proxies for Semantic Relatedness 5\nLike measures of distributional similarity there exist measures of what we will call distributional relatedness (Schu\u0308tze and Pedersen (1997) and Yoshida et al. (2003)). These measures use raw text and co-occurrence information to determine semantic relatedness between two words. The distributional hypothesis mentioned earlier is generic enough to be the basis for both distributional similarity and distributional relatedness. We propose more specific hypotheses that demarcate the two.\nHypothesis of distributional similarity: Distributionally similar words tend to be semantically similar, where two words (w1 and w2, say) are said to be distributionally similar if they have many common co-occurring words and these co-occurring words are each related to w1 and w2 by the same syntactic relation.\nHypothesis of distributional relatedness: Distributionally related words tend to be semantically related, where two words (w1 and w2, say) are said to distributionally related if they have many common co-occurring words and this set of co-occurring words is not restricted to only those that are related to w1 and w2 by the same syntactic relation.\nThe two hypotheses are based on the fact that semantically similar words belong to the same broad part of speech (noun, verb, etc) and are thus each syntactically related to most common co-occurring words by the same syntactic relation. Further, the more two words are semantically related, the more common co-occurring words they have. Consider the semantically related word pair doctor\u2013operate. In a large enough body of text, the two words are likely to have the following common co-occurring words: patient, scalpel, surgery, recuperate, and so on. All these words will be used by a measure of distributional relatedness and the pair will be assigned a high score. However, a measure of distributional similarity will not use any of these co-occurring words (and likely no other, for that matter) as they are not related to the target words by the same syntactic relation. The word doctor is almost always used as a noun while operate is a verb. Thus doctor and operate will get a very low score of distributional similarity. The word pair doctor\u2013nurse, on the other hand, will get a high score of distributional relatedness and distributed similarity. Thus an important characteristic of any distributional measure is whether it is a measure of distributional similarity or more generally that of distributional relatedness.\nIt should be noted that a measure of distributional similarity will provide a high score for certain closely related but dissimilar words belonging to the same thematic role. For example, homeless and drunk which refer to dissimilar concepts but share a non-classical relationship of association (homeless and drunk tend to occur together in text) will likely get a high score as they belong to the same part of speech (adjective) and may have many common\n6 Saif Mohammad and Graeme Hirst\nco-occurring words such as beggar, person, helped, and so on, related by the same syntactic relation. This is a limitation of the current measures of distributional similarity and the impact of the limitation on the ability of the measures to mimic semantic similarity is worth determining.\nThe relevant literature uses the term distance as inverse of distributional similarity. In order to clearly distinguish between semantic distance, this paper will refer to the inverse of distributional similarity as distributional distance. Like semantic distance, distributional distance will also be used as the inverse of distributional relatedness, and the context should help disambiguate the intended meaning."}, {"heading": "2.5. RELATEDNESS OF WORDS AND CONCEPTS", "text": "Measures of semantic relatedness and similarity are applied to particular concepts (or particular senses of the words); for example, one may determine the semantic relatedness of bank in the financial institution sense and interest in the interest rate sense. Distributional measures, on the other hand, usually assign scores to word pairs irrespective of the nature of their polysemy (how many senses they have) or the particular senses they have been used in. Distributional measures will need a much more knowledge rich source (for example large amounts of sense-tagged corpora) than raw text to assign scores to word-sense pairs."}, {"heading": "2.6. EVALUATION", "text": "The presence of a large number of relatedness measures necessitates a suitable evaluation to determine which methods come closest to the human notions of relatedness and to determine how good they each are. There exist two modes of evaluation. The first involves the creation of two ranked lists of certain word pairs. One list is created using a relatedness measure while the other is ranked by humans. The correlation of the two rankings is indicative of how closely the measure mimics human judgment of relatedness. Rubenstein and Goodenough (1965) were the first to conduct quantitative experiments with human subjects who were asked to rate 65 word pairs on a scale of 0.0 to 4.0 as per their relatedness. The word pairs chosen ranged from very similar and almost synonymous to unrelated. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs. However, lack of large amounts of data from human subject experimentation limits the quality of this mode of evaluation.\nThe second and a more indirect way of evaluating measures of semantic relatedness is by the performance of natural language tasks that use them, for example, automatic spelling correction, word sense disambiguation, estimation of unseen bigram (not found in training data) probabilities, and so on.\nDistributional Measures as Proxies for Semantic Relatedness 7"}, {"heading": "3. Distributional Measures", "text": ""}, {"heading": "3.1. SPATIAL METRICS", "text": "A popular technique to determine distributional relatedness between two words is to map them to points in a multidimensional space such that the distance between the two points is an indicator of distributional and thereby semantic distance between them.\nLarge co-occurrence matrices pertaining to each word, which store the set of words that co-occur with it within a certain window size, are created from a text corpus. Consider a multidimensional space where the number of dimensions is equal to size of vocabulary. A word w1 can be represented by a point in this space such that the vector ~w1 from the origin to this point has equal positive components in all dimensions corresponding to words that co-occur with w1. Similarly, vector ~w2 can be created for word w2. This section describes three distributional distance metrics that quantify the distance between ~w1 and ~w2.\n3.1.1. Cosine The cosine method (denoted by Cos) is one of the earliest distributional measures. Given two words w1 and w2, the cosine measure calculates the cosine of the angle between ~w1 and ~w2. If a large number of words co-occur with both w1 and w2, ~w1 and ~w2 will have a small angle between them, the cosine will be large, and we get a large relatedness value between them. The cosine measure gives scores in the range from 0 (unrelated) to 1 (maximally related).\nCos(w1,w2) = ~w1.~w2\n| ~w1 | \u00d7 | ~w2 | (3)\nA limitation of the cosine method in its original form is that all co-occurring words are treated the same, irrespective of how often they co-occurred with w1 and w2. A popular variation (Yoshida et al. (2003), Lee (1999), and Schu\u0308tze and Pedersen (1997)) that incorporates this information is stated below:\nCos(w1,w2) = \u2211w\u2208C(w1)\u222aC(w2) (P(w|w1)\u00d7P(w|w2)) \u221a\n\u2211w\u2208C(w1) P(w|w1)2 \u00d7 \u221a \u2211w\u2208C(w2) P(w|w2)2 (4)\nC(x) is the set of words that co-occur (within a certain window) with the word x in a corpus. P(x|y) is the probability that a particular co-occurrence is composed of x and y, given that word y is one of the words in the cooccurrence pair. It can be approximated by simple corpus counts. Once again, the formula is the cosine of the angle between the word vectors ~w1 and ~w2 but\n8 Saif Mohammad and Graeme Hirst\nthe word vectors incorporate the strength of association of the co-occurring words with the target words. The component of~x in a dimension (corresponding to word y, say) is equal to the strength of association of y with x. Thus the vectors corresponding to two words are closer together, and thereby get a high distributional relatedness score, if they share many co-occurring words and the co-occurring words have more or less the same strength of association with the two target words. In the above formula conditional probability of the co-occurring words given the target words is used as the strength of association.\nThe cosine is used, among others, by Schu\u0308tze and Pedersen (1997) and Yoshida et al. (2003), who suggest methods of automatically generating thesauri from text corpora. Schu\u0308tze and Pedersen (1997) use the Tipster category B corpus (Harman, 1993) (450,000 unique terms) and the Wall Street Journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency words (frequency rank between 2,000 and 5,000). Latent semantic indexing and single-value decomposition (see Schu\u0308tze and Pedersen (1997) for details) are used to reduce the dimensionality of the matrix and get for each term a word vector of its 20 strongest co-occurrences. The cosine of a word vector (say ~w1) with each of the other word vectors is calculated and the top scores along with the words whose vector generated the top scores is noted. These words form the thesaurus entries for w1.\nYoshida et al. (2003) believe that words that are closely related for one person may be distant for another. They use around 40,000 HTML documents to generate personalized thesauri for six different people. Documents used to create the thesaurus for a person are retrieved from the subject\u2019s home page and a web crawler which accesses linked documents. The authors also suggest a root-mean-squared method to determine the similarity of two different thesaurus entries for the same word.\n3.1.2. Manhattan and Euclidean Distances Distance between two points (words) in multidimensional space can be calculated using the Manhattan distance a.k.a. L1 norm (denoted by L1) or Euclidean distance a.k.a. L2 norm (denoted by L2). In the Manhattan distance (5) (Dagan et al. (1997), Dagan et al. (1999), and Lee (1999)), the disparity in strength of association of w1 and w2 with each word that they co-occur with, is summed. The more the disparity in association, the more is the distributional distance between the two words. The Euclidean distance (6) (Lee (1999)) employs the root mean squared of the disparity in association to get the final distributional distance. Both L1 norm and L2 norm give values in the range 0 (zero distance or maximally related) and infinity (maximally distant or unrelated).\nDistributional Measures as Proxies for Semantic Relatedness 9\nL1(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2) | P(w|w1)\u2212P(w|w2) | (5) L2(w1,w2) = \u221a\n\u2211 w\u2208C(w1)\u222aC(w2) (P(w|w1)\u2212P(w|w2)) 2 (6)\nThe above formulae use conditional probability of the co-occurring words given the target words as the strength of association. The distributional relatedness of words may be found by taking the reciprocal of the distributional distance or similar suitable method.\nLee (1999) compared the ability of all three spatial metrics to determine the probability of an unseen (not found in training data) word pair. The measures in order of their performance (from better to worse) were: L1 norm, cosine, and L2 norm. Weeds (2003) determined the correlation of word pair ranking as per a handful of distributional measures with human rankings (Miller and Charles word pairs Miller and Charles (1991)). Using verb-object pairs from the British National Corpus (BNC), she found the correlation of L1 norm with human rankings to be 0.39."}, {"heading": "3.2. SET OPERATIONS", "text": "Distributional measures, as discussed earlier, aim to determine semantic similarity (or relatedness) using words that co-occur with the target words. The problem can be transformed to finding the similarity of two sets (W1 and W2, say), where each set has as its members the co-occurring words of the two target words (w1 or w2), respectively. One can now use set operations such as Jaccard and Dice coefficient to determine the similarity of the two sets and thereby, the semantic similarity of the target words.\nJaccard(w1,w2) = |W1 \u2229W2| |W1 \u222aW2|\n(7)\nDice(w1,w2) = 2\u00d7|W1 \u2229W2| |W1|+ |W2|\n(8)\nBoth measures give scores in the range from 0 (unrelated) to 1 (maximally related) and will rank word pairs identically.\n10 Saif Mohammad and Graeme Hirst\nTheorem 1. If the similarity of word pair one is less than the similarity of word pair two, as determined by the Jaccard coefficient, then the similarity of word pair one will be less than the similarity of word pair two, as determined by the Dice Coefficient.\nProof. Let x be the number of co-occurrences common to word pair one and y the number of words that co-occur with just one of the two words in word pair one. Let l and m be the corresponding values for word pair two. Therefore,\nJaccard(pair one) = x\nx+ y (9)\nDice(pair one) = 2x\n2x+ y (10)\nJaccard(pair two) = l\nl +m (11)\nDice(pair two) = 2l\n2l +m (12)\nGiven,\nJaccard(pair one) < Jaccard(pair two) (13)\n\u21d2 x\nx+ y < l l +m\n(14)\n\u21d2 xl + xm < xl + yl (15)\n\u21d2 xm < yl (16)\nTo prove,\nDice(pair one) < Dice(pair two) (17)\nor, 2x\n2x+ y < 2l 2l +m\n(18)\nor, 4xl +2xm < 4xl +2yl (19)\nor, xm < yl (20)\nwhich is true (from (16)). \u2737\nThus, in terms of measuring distributional similarity/relatedness, Jaccard and Dice coefficients are identical. Lee (1999) shows that the Jaccard coefficient performs better than L1 norm in an unseen bigram probability estimation task.\nDistributional Measures as Proxies for Semantic Relatedness 11\n3.2.1. Pseudo-Fuzzy Metrics Simple set operations as stated above do not consider the strength of association of the co-occurring word with the target words. The strength of association can be incorporated into the metrics by considering the co-occurrence sets to be pseudo-fuzzy. The degree of membership of each word in the pseudo-fuzzy set corresponding to a target word is its strength of association with the target word. We call the sets pseudo-fuzzy (and not fuzzy) because the range of membership values is now dependent on the measure of association used \u2014 conditional probability: 0 to 1, PMI (ignoring negative values2): 0 to infinity. Even though conditional probability has a range from 0 to 1 like a standard fuzzy set membership function, the conditional probabilities of all the words with respect to a particular target word sum up to 1. This need not be (and usually is not) true of the membership values for a regular fuzzy set.\nUse of conditional probability (denoted by CP) as the strength of association and application of Jaccard and Dice coefficient on the pseudo-fuzzy set results in the following formulae. Similar to the case of regular sets, it can easily be shown that the Dice and Jaccard coefficients of pseudo-fuzzy sets also rank word pairs identically.\nJaccardCP(w1,w2) = \u2211w\u2208C(w1)\u222aC(w2) min(P(w|w1),P(w|w2)) \u2211w\u2208C(w1)\u222aC(w2) max(P(w|w1),P(w|w2))\n(21)\nDiceCP(w1,w2) = 2\u00d7\u2211w\u2208C(w1)\u222aC(w2) min(P(w|w1),P(w|w2))\n\u2211w\u2208C(w1) P(w|w1)+\u2211w\u2208C(w2) P(w|w2) (22)\n= 2\u00d7\u2211w\u2208C(w1)\u222aC(w2) min(P(w|w1),P(w|w2))\n1+1 (23)\n= \u2211 w\u2208C(w1)\u222aC(w2) min(P(w|w1),P(w|w2)) (24)\nObserve that the special nature of the membership function forces the Dice coefficient to equate to simplified form (24) which is also the numerator of the Jaccard coefficient. Since Dice and Jaccard are identical in terms of ranking word pairs, use of this simplified form is computationally optimal if one decides to use the Dice or Jaccard coefficient with conditional probability as the strength of association.\nDagan et al. (1995) use a weighted version of the Jaccard coefficient on pseudo-fuzzy sets with PMI as the strength of association. They do not provide quantitative comparison with other distributional measures and do not derive their measure as shown above. Viewing co-occurrence information as pseudo-fuzzy sets enabling the use of any of the numerous set operations to determine distributional similarity is a novel approach. Part of our future research is to determine how well such measures fare compared to the others.\n12 Saif Mohammad and Graeme Hirst"}, {"heading": "3.3. MUTUAL INFORMATION\u2013BASED MEASURES", "text": "Hindle (1990) was one of the first to factor the strength of association of co-occurring words into a distributional similarity measure. The hypothesis is that the more similar the association of co-occurring words with the two target words, the more semantically similar they are. Hindle3 used pointwise mutual information (PMI) as the strength of association. Consider the nouns n j and nk that exist as objects of verb vi in different instances within a text corpus. Hindle used formula (25) to determine the distributional similarity of n j and nk solely from their occurrences as object of vi. The minimum of the two PMIs captures the similarity in the strength of association of vi with each of the two nouns. Note that in case of negative PMI values, the maximum function captures the PMI which is lower in absolute value.\nHinobj(vi,n j,nk) =\n\n    \n    \nmin(I(vi,n j), I(vi,nk)), if I(vi,n j)> 0 and I(vi,nk)> 0 | max(I(vi,n j), I(vi,nk)) |, if I(vi,n j)< 0 and I(vi,nk)< 0 0, otherwise\n(25)\nI(n,v) stands for the PMI (word association ratio, to be more precise) between the words n and v. Hindle used an analogous formula to calculate the distributional similarity (Hinsub j) using the subject-verb relation. The overall distributional similarity between any two nouns is calculated by the formula (26).\nHin(n1,n2) = N\n\u2211 i=0 (Hinobj(vi,n1,n2)+Hinsubj(vi,n1,n2)) (26)\nThe measure gives similarity scores from 0 (maximally dissimilar) to infinity (maximally similar). Note that in Hindle\u2019s measure, the set of co-occurring words used is restricted to include only those words that have the same syntactic relation with both target words (either verb-object or verb-subject). This is therefore a measure of distributional similarity and not distributional relatedness. A form of Hindle\u2019s measure where all co-occurring words are used, making it a measure of distributional relatedness, is shown below:\nHinrel(w1,w2) = \u2211 w\u2208C(w)\n\n    \n    \nmin(I(w,w1), I(w,w2)), if I(w,w1)> 0 and I(w,w2)> 0 | max(I(w,w1), I(w,w2)) |, if I(w,w1)< 0 and I(w,w2)< 0 0, otherwise\n(27)\nC(x) is the set of words that co-occur with word x. Lin (1998a) suggests a different measure derived from his information theoretic definition of similarity (Lin, 1998b). Further, he uses a broad set\nDistributional Measures as Proxies for Semantic Relatedness 13\nof syntactic relations apart from subject-verb and verb-object relations and shows that using multiple relations is beneficial even by Hindle\u2019s measure. He first extracts triples of the form (x,r,y) from the partially parsed text, where the word x is related to y by the syntactic relation r. If I(x,r,y) is the information contained in the proposition: the triple (x,r,y) occurred a constant c times, then Lin defines the distributional similarity between two words, w1 and w2, as follows:\nLin(w1,w2) = \u2211(r,w)\u2208T (w1)\u2229T (w2) (I(w1,r,w)+ I(w2,r,w))\n\u2211(r,w\u2032)\u2208T (w1) I(w1,r,w\u2032)+\u2211(r,w\u2032\u2032)\u2208T (w2) I(w2,r,w\u2032\u2032) (28)\nT (x) is the set of all word pairs (r,y) such that the pointwise mutual information I(x,r,y), is positive. Note that this is different from Hindle (1990) where even the cases of negative PMI were also considered. As mentioned earlier, Church and Hanks (1989) show that it is hard to accurately predict negative word association ratios with confidence. Thus, co-occurrence pairs with negative PMI are ignored. The measure gives similarity scores from 0 (maximally dissimilar) to 1 (maximally similar).\nLin\u2019s measure distinguishes itself from that of Hindle in two respects. Firstly, he normalizes the distributional similarity between two words (w1 and w2) determined by their PMI with common co-occurring words by the total PMI of w1 and w2 with the rest of the related words. This is a significant improvement as now high PMI of the target words with shared co-occurring words does not guarantee a high distributional similarity score. As an additional requirement, the target words must have low PMI with words they do not both co-occur with. The second difference in the two formulae is that Hindle uses a minimum of the PMI between each of the target words and the shared co-occurring word, while Lin uses the sum. Taking the sum has the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as w1 and w2 both co-occur with a word. We suggest a new measure of distributional similarity (denoted by Saif) which counters this but keeps the normalizing factor of Lin\u2019s measure:\nSaif(w1,w2) = 2\u00d7\u2211(r,w)\u2208T (w1)\u2229T (w2) min(I(w1,r,w), I(w2,r,w)) \u2211(r,w\u2032)\u2208T (w1) I(w1,r,w\u2032)+\u2211(r,w\u2032\u2032)\u2208T (w2) I(w2,r,w\u2032\u2032)\n(29)\nThe multiplication by two is done to get scores in the range of 0 to 1 (note that the sum in Lin\u2019s formula was replaced by a min). The multiplication has no effect on the relative ranking of word pairs by their similarities. Also notice that like Hindle\u2019s measure, both Lin\u2019s and mine are measures of distributional similarity. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes. Lin (1998a) used his measure to generate a thesaurus from a 64-million-word\n14 Saif Mohammad and Graeme Hirst\ncorpus of the Wall Street Journal, San Jose Mercury and AP Newswire. He also provides a framework for evaluating automatically generated thesauri by comparing them with WordNet-based and Roget-based thesauri. He shows that the thesaurus created with his measure is closer to the WordNet and Roget-based thesauri than that of Hindle.\n3.3.1. Mutual Information\u2013Based Spatial and Fuzzy Metrics Variations of the spatial metrics (equations (4), (5), and (6)) that use pointwise mutual information instead of conditional probability as the strength of association are possible. Following are the formulae for mutual information\u2013 based spatial metrics.\nCosMI(w1,w2) = \u2211w\u2208C(w1)\u222aC(w2) (I (w,w1)\u00d7 I (w,w2)) \u221a\n\u2211w\u2208C(w1) I(w,w1)2 \u00d7 \u221a \u2211w\u2208C(w2) I(w,w2)2 (30)\nLMI1 (w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2) | I(w,w1)\u2212 I(w,w2) | (31) LMI2 (w1,w2) = \u221a\n\u2211 w\u2208C(w1)\u222aC(w2) (I(w,w1)\u2212 I(w,w2))2 (32)\nUse of pointwise mutual information as the strength of association in the fuzzy metrics (see equations (22) and (21)) discussed earlier results in the following:\nJaccardMI(w1,w2) = \u2211w\u2208C(w1)\u222aC(w2) min(I(w,w1), I(w,w2)) \u2211w\u2208C(w1)\u222aC(w2) max(I(w,w1), I(w,w2))\n(33)\nDiceMI(w1,w2) = 2\u00d7\u2211w\u2208C(w1)\u222aC(w2) min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w1) I(w,w1)+\u2211w\u2208C(w2) I(w,w2) (34)\nObserve that Saif(w1,w2) (equation (29)) equates to DiceMI(w1,w2) if the restriction to use only positive pointwise mutual information, is lifted.\n3.4. RELATIVE ENTROPY\u2013BASED MEASURES\n3.4.1. Kullback-Leibler divergence Given two probability mass functions p(x) and q(x), their relative entropy (D(p\u2016q)) is:\nD(p\u2016q) = \u2211 x\u2208X p(x) log p(x) q(x)\nfor q(x) 6= 0 (35)\nIntuitively, if p(x) is the accurate probability mass function corresponding to a random variable X , D(p\u2016q) is the information lost on approximating p(x)\nDistributional Measures as Proxies for Semantic Relatedness 15\nby q(x). In other words, D(p\u2016q) is indicative of how different the two distributions are. Relative entropy is also called the Kullback-Leibler divergence or the Kullback-Leibler distance (denoted by KLD).\nPereira et al. (1993) and Dagan et al. (1994) point out that words have probabilistic distributions with respect to neighboring syntactically related words. For example, there exists a certain probabilistic distribution (d1(P(v|n1)), say) of a particular noun n1 being the object of any verb. This distribution can be estimated by corpus counts of parsed or chunked text. Let d2 (P(v|n2)) be the corresponding distribution for noun n2. These distributions (d1 and d2) define the contexts of the two nouns (n1 and n2, respectively). As per the distributional hypothesis (Harris, 1968), the more these contexts are similar, the more are n1 and n2 semantically similar. Thus the Kullback-Leibler distance between the two distributions is indicative of the semantic distance between the nouns n1 and n2.\nKLD(n1,n2) = D(d1\u2016d2)\n= \u2211v\u2208Vb P(v|n1) log P(v|n1) P(v|n2)\nfor P(v|n2) 6= 0\n= \u2211v\u2208Vb\u2019(n1)\u222aVb\u2019(n2) P(v|n1) log P(v|n1) P(v|n2)\nfor P(v|n2) 6= 0 (36)\nwhere Vb is the set of all verbs and Vb\u2019(x) is the set of verbs that have x as the object. The distributional similarity is determined by taking the reciprocal of the Kullback-Leibler distance or similar suitable method. Note that the set of co-occurring words used is restricted to include only verbs that each have the same syntactic relation (verb-object) with both target nouns. This is therefore a measure of distributional similarity and not distributional relatedness.\nIt should be noted that the verb-object relationship is not inherent to the measure and that one or more of any other syntactic relations may be used. The distributional relatedness may even be determined using all words cooccurring with the target words. Thus a more generic expression of the KullbackLeibler divergence is as follows:\nKLD(w1,w2) = D(d1\u2016d2)\n= \u2211w\u2208V P(w|w1) log P(w|w1) P(w|w2)\nfor P(w|w2) 6= 0\n= \u2211w\u2208C(w1)\u222aC(w2) P(w|w1) log P(w|w1) P(w|w2)\nfor P(w|w2) 6= 0 (37)\nV is the vocabulary (all the words found in a corpus). C(x), as mentioned earlier, is the set of words occurring (within a certain window) with word x. The inverse of the distributional distance calculated above yields the distributional relatedness of w1 and w2.\n16 Saif Mohammad and Graeme Hirst\nIt should be noted that the Kullback-Leibler distance is not symmetric, that is, the distance from w1 to w2 is not necessarily, and even not likely, the same as the distance from w2 to w1. This asymmetry is counter-intuitive to the general notion of semantic similarity of words, although Weeds (2003) has argued in favor of asymmetric measures. Further, it is very likely that there be instances such that P(w1|v) is greater than 0 for a particular verb v, while due to data sparseness or grammatical and semantic constraints, the training data has no sentence where v has the object w2. This makes P(w2|v) equal to 0 and the ratio of the two probabilities infinite. Kullback-Leibler divergence is not defined in such cases but approximations may be made by considering smoothed values for the denominator.\nPereira et al. (1993) use relative entropy to create clusters of nouns from verb-object pairs corresponding to a thousand most frequent nouns in the Grolier\u2019s Encyclopedia, June 1991 version (10 million words). Dagan et al. (1994) use Kullback-Leibler distance to estimate the probabilities of bigrams that were not seen in a text corpus. They point out that a significant number of possible bigrams are not seen in any given text corpus. The probabilities of such bigrams may be determined by taking a weighted average of the probabilities of bigrams composed of distributionally similar words. Use of Kullback-Leibler distance as the semantic distance metric yielded a 20% improvement in perplexity on the Wall Street Journal and dictation corpora provided by ARPA\u2019s HLT program (Paul, 1991).\nThe use of distributionally similar words to estimate unseen bigram probabilities will likely lead to erroneous results in case of less-preferred and strongly-preferred collocations (word pairs). Inkpen and Hirst (2002) point out that even though words like task and job are semantically very similar, the collocations they form with other words may have varying degrees of usage. While daunting task is a strongly-preferred collocation, daunting job is rarely used. Thus using the probability of one bigram to estimate that of another will not be beneficial in such cases.\n3.4.2. \u03b1 Skew Divergence \u03b1 skew divergence (ASD) is a slight modification of the Kullback-Leibler divergence, that obviates the need for smoothed probabilities. It has the following formula:\nASD(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1) log P(w|w1)\n\u03b1P(w|w2)+ (1\u2212\u03b1)P(w|w1) (38)\n\u03b1 is a parameter that may be varied but is usually set to 0.99. Note that the denominator within the logarithm is never zero with a non-zero numerator. Also, the measure retains the asymmetric nature of the Kullback-Leibler divergence.\nDistributional Measures as Proxies for Semantic Relatedness 17\nLee (2001) shows that \u03b1 skew divergence performs better than KullbackLeibler divergence in estimating word co-occurrence probabilities. Weeds (2003) achieves a correlation of 0.48 and 0.26 with human judgment on the Miller and Charles word pairs using ASD(w1,w2) and ASD(w2,w1), respectively.\n3.4.3. Jensen-Shannon Divergence A relative entropy\u2013based measure that overcomes the drawback of asymmetry in Kullback-Leibler divergence is the Jensen-Shannon divergence a.k.a. total divergence to the average a.k.a. information radius. It is denoted by JSD and has the following formula:\nJSD(w1,w2) = D\n(\nd1\u2016 1 2 (d1 +d2)\n)\n+D\n(\nd2\u2016 1 2 (d1 +d2)\n)\n(39)\n= \u2211 w\u2208C(w1)\u222aC(w2)\n(\nP(w|w1) log P(w|w1)\n1 2 (P(w|w1)+P(w|w2))\n+\nP(w|w2) log P(w|w2)\n1 2 (P(w|w1)+P(w|w2))\n)\n(40)\nThe Jensen-Shannon divergence is the sum of Kullback-Leibler divergence between each of the individual distributions d1 and d2 with the average distribution (d1+d22 ). Further, it can be shown that the Jensen-Shannon divergence avoids the problem of zero denominator as in Kullback-Leibler divergence. The Jensen-Shannon divergence is therefore always well defined and, like \u03b1 skew divergence, obviates the need for smoothed estimates.\nThe Kullback-Leibler divergence, \u03b1 Skew Divergence, and Jensen-Shannon divergence all give distributional distance scores from 0 (maximally similar/related) to infinity (completely dissimilar/unrelated)."}, {"heading": "3.5. CO-OCCURRENCE RETRIEVAL MODELS", "text": "The distributional measures suggested by Weeds (2003) are based on the notion of substitutability. The more appropriate it is to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they are. The natural language task she focuses on is co-occurrence retrieval (the retrieval of words that co-occur with a target word from text) and depending on the definition of appropriate she suggests six different distributional measures called the co-occurrence retrieval models (CRMs).\nLet N1 be the set of co-occurrences of w1 retrieved from a text corpus and N2 that of w2. In order to determine how appropriate it is to substitute w1 in place of w2 we have to decide how important it is to get as many cooccurrences as possible listed in N2 (recall, denoted by R) and how important\n18 Saif Mohammad and Graeme Hirst\nit is to not get co-occurrences not listed in N2 (precision, denoted by P). Thus Weeds\u2019 distributional measures have a precision component and a recall component. The final score is a weighted sum of the precision, recall and standard F measure (see equation (41)4). The weights determine the importance of precision and recall and are determined empirically. If precision and recall are equally important, then we get a symmetric measure which gives the same scores to the distributional similarity of w1 with w2 and w2 with w1. Otherwise, we get an asymmetric measure which assigns different similarities to the two cases. As substitutability is defined as a measure of distributional similarity, metrics such as precision and recall which quantify how good the substitution is, are used to calculate the distributional similarity.\nCRM(w1,w2) = \u03b3\n[\n2\u00d7P\u00d7R P+R\n]\n+(1\u2212 \u03b3)\n[\n\u03b2[P]+ (1\u2212\u03b2)[R]\n]\n(41)\n\u03b3 and \u03b2 are tuned parameters that lie between 0 and 1. Weeds argues that the asymmetry in substitutability is intuitive as in many cases it may be okay to substitute a word, say dog, with another, say animal, but the reverse is not likely to be acceptable as often. Since substitutability is a measure of semantic similarity, she believes that distributional similarity between two words should reflect this property as well. Hence, like the Kullback-Leibler divergence, all her distributional similarity models are inherently asymmetric.\nA word\u2019s co-occurrence information may be specified by the set of cooccurring words alone, or by specifying the strength of co-occurrences, as well. This strength may be captured by a suitable measure of word association such as conditional probability or pointwise mutual information between the co-occurring words and the target words. Also, the difference in the strength of co-occurrence may or may not be used to penalize the substitutability of one word for another. Weeds (2003) provides six distinct formulae for precision and recall, depending on the the strength of co-occurrence and penalty for differences in strength of association.\nThe precision (or recall) can be considered as the product of a core precision (or recall) formula (denoted by core) and a penalty function (denoted by penalty). The CRMs that use simple counts of the common co-occurrences in N1 and N2 and not the strength of associations as core precision and recall values are called type-based CRMs (denoted by the superscript type). The CRMs that use conditional probabilities of the common co-occurrences in N1 and N2 with the target words as core precision and recall values are called token-based CRMs (denoted by the superscript token). The CRMs that use pointwise mutual information of the common co-occurrences in N1 and N2 with the target words as core precision and recall values are called mutual information\u2013based CRMs (denoted by the superscript mi). The core preci-\nDistributional Measures as Proxies for Semantic Relatedness 19\nsion and recall formulae for type, token and mutual information\u2013based CRMs are listed below:\ncoretypeP (w1,w2) = |C(w1)\u2229C(w2) |\n|C(w1) | (42)\ncoretypeR (w1,w2) = |C(w1)\u2229C(w2) |\n|C(w2) | (43)\ncoretokenP (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w1) (44) coretokenR (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w2) (45)\ncoremiP (w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w1)\n\u2211w\u2208C(w1) I(w,w1) (46)\ncoremiR (w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w2)\n\u2211w\u2208C(w2) I(w,w2) (47)\nThe CRMs that do not penalize difference in strength of co-occurrence are called additive CRMs (denoted by the subscript add). The CRMs that do penalize are called difference-weighted CRMs (subscript dw). The penalty is a conditional probability\u2013based function (48, 49) for the token- and type-based CRMs, and a mutual information\u2013based function (50, 51) for the mutual information\u2013based CRM.\npenaltytypeP = penalty token P = min(P(w|w1),P(w|w2)) P(w|w1)\n(48)\npenaltytypeR = penalty token R = min(P(w|w1),P(w|w2)) P(w|w2)\n(49)\npenaltymiP = min(I(w,w1), I(w,w2))\nI(w,w1) (50)\npenaltymiR = min(I(w,w1), I(w,w2))\nI(w,w2) (51)\nThe precision and recall of additive and difference-weighted CRMs is listed in the appendix.\nWeeds (2003) extracted verb-object pairs of 2,000 nouns from the British National Corpus (BNC). The verbs related to the target words by the verbobject relation were used. Thus each of the co-occurring verbs is related to the target nouns by the same syntactic relation and therefore the measures capture distributional similarity, not relatedness. Correlation with human judgment (Miller and Charles word pairs) showed that difference-weighted (0.61) and additive mutual information\u2013based measures (0.62) performed far better than the rest of the CRMs.\n20 Saif Mohammad and Graeme Hirst"}, {"heading": "4. Discussion and Analysis of Distributional Measures", "text": "The previous section described numerous distributional measures. Variations of the measures are possible depending on certain general properties of a distributional measure. This section discusses a few of the important properties along with an analysis of their effect in assigning semantic relatedness."}, {"heading": "4.1. SIMPLE CO-OCCURRENCES VS SYNTACTICALLY RELATED WORDS", "text": "Harris (1968), one of the early proponents of the distributional hypothesis, used syntactically related words to represent the context of a word. However, the strength of association of any word appearing in the context of the target words may be used to determine their distributional similarity. Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity. Schu\u0308tze and Pedersen (1997) and Yoshida et al. (2003) use all cooccurring words in a pre-decided window size. Although Lin (1998a) shows that the use of multiple syntactic relations is more beneficial as compared to just one, there exist no published results on whether using only syntactically related words (as compared to all co-occurrences) improves or worsens the quality of semantic similarity assignment.\nUse of syntactically related words entails the requirement of chunking or parsing the data. Once the data is suitably parsed, the computational cost of such methods is lower as distributional similarity is determined with much fewer words.\n4.1.1. Use of Multiple Syntactic Relations Lin (1998a) used a subset of words that co-occurred with the target words to determine their distributional similarity. Only those co-occurrences that are syntactically related (by any of the pre-decided list of relations) to the target words are chosen. Once this restricted set of co-occurrences is determined, distributional similarity is determined by formula (28) shown earlier. Observe that the formula does not distinguish between the co-occurrences related by different syntactic relations. An alternative is to calculate a distributional similarity value using each of the syntactic relations individually and then determine the overall distributional similarity from these results. The overall distributional similarity may be as simple as the average similarity (see (52)) or the maximum (see (53)) of individual similarity results. Distributional similarity so calculated is justified in the following two paragraphs, respectively.\nDistributional Measures as Proxies for Semantic Relatedness 21\nSimOverall Avg(w1,w2) = 1 N ( Simr1(w1,w2)+Simr2(w1,w2)+\n. . .+SimrN(w1,w2) )\n(52)\nSimOverall Max(w1,w2) = max(Simr1(w1,w2),Simr2(w1,w2),\n. . . ,SimrN(w1,w2)) (53)\nwhere N is the total number of syntactic relations considered and,\nSimri(w1,w2) = \u2211(ri,w)\u2208T (w1)\u2229T (w2)(I(w1,ri,w)+ I(w2,ri,w))\n\u2211(ri,w\u2032)\u2208T (w1) I(w1,ri,w\u2032)+\u2211(ri,w\u2032)\u2208T (w2) I(w2,ri,w\u2032) (54)\nwhere ri is a particular syntactic relation. Consider the scenario where word w\u2032 has a strong word association ratio (large MI value) with w1 but does not co-occur with w2. The large MI value is added to the denominator as per Lin\u2019s measure (28). This results in a low distributional similarity value. However, a number of words are considered semantically related even though there exist words (exclusive cooccurrences, say) that have strong word association ratios with one or the other target word but not both. A mark of semantically related words is the presence of a number of common co-occurring words with whom they are both strongly associated. One or few strong co-occurrences of a target word that do not co-occur with the other target word do not imply that the target words are semantically unrelated. For example, consider the rather similar pair of nouns bananas and mangoes. The adjective juicy is likely to have a large association ratio with mangoes but not so with bananas. The large MI value of mangoes and juicy may lead to an excessively low distributional similarity value as per Lin\u2019s measure (28). Averaging the different distributional similarity values (as in (52)) calculated from individual syntactic relations instead of Lin\u2019s original method moderates the strongly negative effect of such exclusive co-occurrences by restricting it to a particular syntactic relation (in this case, adjective-noun). It should be noted that the disparity in the strength of association of mangoes and juicy versus banana and juicy, is useful in bringing out the differences between mango and banana which may be used to determine that mango and orange are more semantically related than mango and banana. However, as pointed out earlier, we do not want a strong co-occurrence to have an adverse affect on the estimation of distributional similarity in all other cases.\nTaking the maximum of individual distributional similarity values (53) takes the aforementioned idea one step ahead and is grounded in the following hypothesis:\nDifferent syntactic relations are accurate predictors of the semantic similarity for different pairs of words.\n22 Saif Mohammad and Graeme Hirst\nFor example, fruits tend to have strong word associations with adjectives like sweet, bitter, ripe and juicy, and low association values with verbs that they are related to by the subject-verb relation. For example, consider the sentences:\nthe ripe mango fell to the ground the ripe plum fell to the ground\nThe words ripe and mango are related by the adjective-noun relation and are likely to have a large value of association. On the other hand, mango and fell which are the subject and verb, respectively, are likely to have a low measure of association because almost anything can fall. The adjective-noun relation is thus expected to yield a higher distributional similarity value than the subject-verb relation. Employing equation (53) in this case will mean that cooccurrences related to the target words by the adjective-noun relation will be used to determine the distributional similarity while all other co-occurrences will be ignored. Thus only the relation that has the strongest associated cooccurrences is used to determine the distributional similarity as these cooccurrences are expected to be the best predictors of semantic similarity. A measure where other sets of co-occurrences, which are weak predictors of semantic similarity, are allowed to influence the result may cause more harm than benefit. Flipping the argument on its head, target words predicted to be strongly distributionally similar by two or more syntactic relations should be assigned higher distributional similarity values than in the case of just one. Using the maximum method will loose out on this information.\nPart of our future work will be to determine if calculating individual similarity values from different syntactic relations and then arriving at the final similarity is closer to human judgment or not. Also, as pointed out, both the average or maximum approaches have their advantages and disadvantages. It will be interesting to determine which method gives semantic similarity values closer to the human notion of semantic similarity."}, {"heading": "4.2. COMPOSITIONALITY", "text": "The various measures of distributional similarity may be divided into two kinds as per their composition. In certain measures each co-occurring word contributes to some finite calculable distributional distance between the target words. The final score of distributional distance is the sum of these contributions. We will call such measures compositional measures. The relative entropy\u2013based measures, L1 norm and L2 norm fall in this category. On the other hand, the cosine measure along with Hindle\u2019s and Lin\u2019s mutual information\u2013based measures belong to the category of what we call noncompositional measures. Each co-occurring word shared by both target words contributes a score to the numerator and the denominator. Words that co-occur\nDistributional Measures as Proxies for Semantic Relatedness 23\nwith just one of the two target words contribute scores only to the denominator. The ratio is calculated once all co-occurring words are considered. Thus the distributional distance contributed by individual co-occurrences is not calculable and the final semantic distance cannot be broken down into compositional distances contributed by each of the co-occurrences.\nIt must be noted that it is not clear as to which of the two kinds of measures (compositional or non-compositional) resembles human judgment more closely and how much they differ in their ranking of word pairs. Our future work aims to determine this.\n4.2.1. Primary Compositional Measures The compositional measures of distributional similarity (or relatedness) capture the contribution to distance between the target words (w1 and w2) due to a co-occurring word by three primary mathematical manipulations of the cooccurrence distributions (d1 and d2): the difference, denoted by Dif (as in L1 norm), division, denoted by Div (as in the relative entropy\u2013based measures) and product, denoted by Pdt (as in the conditional probability or mutual information\u2013based cosine method). We will call the three types of compositional measures primary compositional measures (PCM). Their form is depicted below:\nDif = \u2211 w\u2208C(w1)\u222aC(w2) |P(w|w1)\u2212P(w|w2)| (55)\nDiv = \u2211 w\u2208C(w1)\u222aC(w2)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 \u2223\n(56)\nPdt = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1)\u00d7P(w|w2) Scaling Factor\n(57)\nObserve that by taking absolute values in expressions (55) and (56), the variation in the distributions for different co-occurring words has an additive affect and not one of cancellation. This corresponds to our distributional hypothesis \u2014 the more the disparity in distributions, the more is the semantic distance between the target words. The product form (57) also achieves this and is based on the theorem:\nThe product of any two numbers will always be less than or equal to the square of their average.\nIn other words, the more two numbers are close to each other in value, the higher is the ratio of their product to a suitable scaling factor (for example, the square of their average). Note that the difference and division measures give higher values when there is large disparity between the strength of association of co-occurring words with the target words. They are therefore measures of\n24 Saif Mohammad and Graeme Hirst\ndistributional distance and not distributional similarity. The product method gives higher values when the strengths of association are closer, and is a measure of distributional relatedness.\nAlthough all three methods seem intuitive, each produces different distributional similarity values and more importantly, given a set of word pairs, each is likely to rank them differently. For example, consider the division and difference expressions applied to word pairs (w1, w2) and (w3, w4). For simplicity, let there be just one word w\u2032 in the context of all the words. Given:\nP(w\u2032|w1) = 0.91 P(w\u2032|w2) = 0.80 P(w\u2032|w3) = 0.60 P(w\u2032|w4) = 0.50\nThe distributional distance between word pairs as per the difference PCM:\nDif (w1,w2) = |0.91\u22120.8| = 0.11\nDif (w3,w4) = |0.6\u22120.5|= 0.1\nThe distributional distance between word pairs as per the division PCM:\nDiv(w1,w2) =\n\u2223 \u2223 \u2223 \u2223 log 0.91 0.8 \u2223 \u2223 \u2223 \u2223 = 0.056\nDiv(w3,w4) =\n\u2223 \u2223 \u2223 \u2223 log 0.6 0.5 \u2223 \u2223 \u2223 \u2223 = 0.079\nObserve that for the same set of co-occurrence probabilities, the differencebased measure ranks the (w3,w4) pair more distributionally similar (lower distributional distance), while the division-based measure gives lower distributional similarity values for word pairs having large co-occurrence probabilities. This behavior is not intuitive and it remains to be seen, by experimentation, as to which of the three, difference, division or product, yields distributional similarity measures closest to human notions of semantic similarity.\nThe L1 norm is a basic implementation of the difference method. A simple product-based measure of distributional similarity is as proposed below:\nPdtAvg(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\nP(w|w1)\u00d7P(w|w2)\n(12(P(w|w1)+P(w|w2))) 2\n(58)\nThe scaling factor used is the square of the average probability. It can be proved that if the sum of two variables is equal to a constant (k, say). Their\nDistributional Measures as Proxies for Semantic Relatedness 25\nvalues must be equal to k/2 in order to get the largest product. Now, let k be equal to the sum of P(w|w1)/(P(w|w1)+P(w|w2)) and P(w|w2)/(P(w|w1)+ P(w|w2)). This sum will always be equal to 1 and hence the product (Z) will be largest only when the two numbers are equal i,e, P(w|w1) is equal to P(w|w2). In other words, the farther P(w|w1) and P(w|w2) are from their average, the smaller is the product Z. Therefore, the measure gives high scores for low disparity in strengths of co-occurrence and low scores otherwise. The incorporation of 2 in the scaling factor results in a measure that ranges between 0 and 1.\nThe relative entropy\u2013based methods use a weighted division method. Observe that both Kullback-Leibler divergence (formula repeated here for convenience \u2014 equation (59)) and Jensen-Shannon divergence do not take absolute values of the division of co-occurrence probabilities. This will mean that if P(w|w1) > P(w|w2), the logarithm of their ratio will be positive and if P(w|w1) < P(w|w2), the logarithm will be a negative number. Therefore, there will be a cancellation of contributions to distributional distance by words that have higher co-occurrence probability with respect to w1 and words that have a higher co-occurrence probability with respect to w2. Observe however that the weight P(w|w1) multiplied to the logarithm means that in general the positive logarithm values receive higher weight than the negative ones, resulting in a net positive score. Therefore, with no absolute value of the logarithm, as in the KLD, the weight plays a crucial role. A modified Kullback-Leibler divergence (DAbs) which incorporates the absolute value is suggested in equation (60):\nKLD(w1,w2) = D(d1\u2016d2) = \u2211 w\u2208C(w1)\u222aC(w2) P(w|w1) log P(w|w1) P(w|w2)\n(59)\nKLDAbs(w1,w2) = D Abs(d1\u2016d2) = \u2211\nw\u2208C(w1)\u222aC(w2)\nP(w|w1)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 \u2223\n(60)\nThe updated Jensen-Shannon divergence measure will remain the same as in equation (39), except that it is a manipulation of DAbs and not the original Kullback-Leibler divergence (relative entropy).\nJSDAbs(w1,w2) = D Abs(d1\u2016 1 2 (d1 +d2))+D Abs(d2\u2016 1 2 (d1 +d2)) (61)\nNote that once the absolute value of the logarithm is taken, it no longer makes much sense to use an asymmetric weight (P(w|w1)) as in the KLD or as necessary to use a weight at all. Equation (62) shows a simple division-based measure. It is an unweighted form of KLDAbs(w1,w2) and so we will call it KLDAbsUnw.\n26 Saif Mohammad and Graeme Hirst\nKLDAbsUnw(w1,w2) = Div(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 \u2223\n(62)\nExperimental evaluation of these suggested modifications of Kullback-Leibler divergence and informations radius is part of future work.\n4.2.2. Weighting the PCMs The performance of the primary compositional measures may be improved by adding suitable weights to the distributional distance contributed by each co-occurrence. The idea is that some co-occurrences may be better indicators of semantic distance than others. Usually, a formulation of the strength of association of the co-occurring word with the target words is used as weight, the hypothesis being that a strong co-occurrence is likely to be strong indicator of semantic distance.\nWeighting the primary compositional measures results in some of the existing measures. For example, as pointed out earlier, the Kullback-Leibler divergence is a weighted form of the division measure (not considering the absolute value). Here, the conditional probability of a co-occurring word with respect to the first word (P(w|w1)) is used as the weight. Since the weight is dependent on the first word and not the other, we have asymmetry. A more symmetric weight could be the average of the conditional probabilities between the co-occurring word and each of the two target words. A symmetrically weighted division PCM SaifDivAvgWt is shown below:\nSaifDivAvgWt(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\n1 2 (P(w|w1)+P(w|w2))\n\u2223 \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 \u2223\n(63)\nWe can have corresponding, symmetric weighted Jensen-Shannon divergence and \u03b1 skew divergence. L2 norm is a weighted version of the L1 norm, the weight being: P(w|w1)\u2212P(w|w2). A simple product measure with weights is shown below:\nPdtAvgAvgWt = \u2211 w\u2208C(w1)\u222aC(w2) 1 2 (P(w|w1)+P(w|w2))\nP(w|w1)\u00d7P(w|w2)\n(12 (P(w|w1)+P(w|w2))) 2\n= \u2211 w\u2208C(w1)\u222aC(w2) P(w|w1)\u00d7P(w|w2) 1 2(P(w|w1)+P(w|w2))\n(64)\nA better weight (which is also symmetric) may be chosen given the following hypothesis:\nThe stronger the association of a co-occurring word with a target word, the better indicator of semantic properties of the target word it is.\nDistributional Measures as Proxies for Semantic Relatedness 27\nThe co-occurring word is likely to have different strengths of associations with the two target words. Taking the maximum of the two as the weight (Dagan et al. (1995)) will mean that more weight is given to a co-occurring word if it has high strength of association with any of the two target words. As Dagan et al. (1995) point out, there is strong evidence for dissimilarity if the strength of association with the other target word is much lower than the maximum, and strong evidence of similarity if the strength of association with both target words is more or less the same. Equation (65) is a weighted division PCM that captures this intuition.\nSaifDivMaxWt(w1,w2)\n= \u2211 w\u2208C(w1)\u222aC(w2)\nmax(P(w|w1),P(w|w2))\n\u2211w\u2032\u2208C(w1)\u222aC(w2) max(P(w\u2032|w1),P(w\u2032|w2))\n\u2223 \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 \u2223\n(65)\nSimilarly weighted product and difference measures may be created. Both SaifDivMaxWt and Saif Div AvgWt give distributional distance scores from 0 (maximally similar/related) to infinity (completely dissimilar/unrelated). It would be interesting to note the effect of weighting on these measures and also to determine which weight factor is more suitable."}, {"heading": "4.3. MEASURE OF ASSOCIATION", "text": "As mentioned earlier, distributional measures use the disparity in association of the target words with their co-occurring words to determine relatedness. Lin (1998a) and Hindle (1990) use pointwise mutual information as the measure of association. The mutual information\u2013based CRMs of Weeds (2003) also use the same. All other measures studied in this paper use simple conditional probability of the co-occurring words, given the target word. It should be noted that replacing the strength of association in a measure with another can result in a different distributional measure. For example, the mutual information\u2013based spatial and fuzzy metrics discussed earlier. Lin\u2019s measure (28) using conditional probability (CP) is shown below:\nLinCP(w1,w2) = \u2211(r,w)\u2208T (w1)\u2229T (w2)(P(w|w1)+P(w|w2))\n\u2211(r,w\u2032)\u2208T (w1) P(w\u2032|w1)+\u2211(r,w\u2032)\u2208T (w2) P(w\u2032|w2) (66)\nOf course, in case of certain measures, for example the division-based primary compositional measures, use of pointwise mutual information and conditional probability is equivalent.\n28 Saif Mohammad and Graeme Hirst\nDivMI(w1,w2) = \u2211 w\u2208C(w1)\u222aC(w2)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223 log P(w,w1) P(w)P(w1)\nP(w,w2) P(w)P(w2)\n\u2223 \u2223 \u2223 \u2223 \u2223 \u2223\n(67)\n= \u2211 w\u2208C(w1)\u222aC(w2)\n\u2223 \u2223 \u2223 \u2223 log P(w|w1) P(w|w2) \u2223 \u2223 \u2223 \u2223\n(68)\n= Div(w1,w2) (69)\nWeeds (2003) shows that her mutual information\u2013based CRMs exhibit higher correlation with human judgment on the Miller and Charles word pairs compared to the ones that use conditional probability. It remains to be seen if other measures follow the same pattern."}, {"heading": "4.4. PREDICTORS OF SEMANTIC RELATEDNESS", "text": "Given a pair of target words, the vocabulary may be divided into three sets: (1) the set of words that co-occur with both target words (common); (2) words that co-occur with exactly one of the two target words (exclusive); (3) words that do not co-occur with either of the two target words. Hindle (1990) uses evidence only from words that co-occur with both target words to determine the distributional similarity. All the other measures discussed in this paper so far, use words that co-occur with just one target word, as well.\nOne can argue that the more there are common co-occurrences between two words, the more they are related. For example, drink and sip may be considered related as they have a number of common co-occurrences such as water, tea and so on. Similarly, drink and chess can be deemed unrelated as words that co-occur with one, do not with the other. For example, water and tea do not usually co-occur with chess, while castle and move are not found close to drink. Measures that use all co-occurrences (common and exclusive) tap into this intuitive notion. However, certain strong exclusive cooccurrences can adversely effect the measure. Consider the classic strong tea vs powerful tea example (Halliday (1966)). The words strong and powerful are semantically very related. However, the word coffee is likely to co-occur with strong but not with powerful. Further, strong and coffee can be expected to have a large value of association as given by a suitable measure, say PMI. This large PMI value, if used in the distributional relatedness formula, can greatly reduce the final value. Thus it is not clear if the benefit of using all co-occurrences is outweighed by the drawback pointed out.\nA further advantage of using only common co-occurrences is that the Kullback-Leibler divergence can now be used without the need of smoothed probabilities.\nDistributional Measures as Proxies for Semantic Relatedness 29\nKLDCom(w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w1) log P(w|w1) P(w|w2)\n(70)\nObserve that we are taking the intersection of the set of co-occurring words instead of union as in the original formula (37)."}, {"heading": "4.5. CAPITALIZING ON ASYMMETRY", "text": "Given a hypernym-hyponym pair (automobile-car, say) asymmetric distributional measures such as the Kullback-Leibler divergence, \u03b1 skew divergence and the CRMs generate different values as the distributional similarity of w1 with w2 as compared to w2 with w1. Usually, if w1 is a more generic concept than w2, the measures find w1 to be more distributionally similar to w2 than the other way round. Weeds (2003) argues that this behavior is intuitive as it is more often okay to substitute a generic concept in place of a specific one than vice versa, and substitutability is a indicator of semantic similarity. On the other hand, in most cases the notion of asymmetric semantic similarity is counter-intuitive, and possibly detrimental. Further, in case two words share a hypernym-hyponym relation, they are likely to be highly semantically similar. Thus given two words, it may make sense to always choose the higher of the two distributional similarity values suggested by an asymmetric measure as the final distributional similarity between the two. This way an asymmetric measure (SimAsym) can easily be converted into a symmetric one (SimAsym), while still capitalizing on the asymmetry to generate more suitable distributional similarity values for hypernym-hyponym word pairs. Equation (71) states the formula for the proposed conversion. A specific implementation on the KL divergence formula is given in equation (72)\nSimMax(w1,w2) = max(SimAsym(w1,w2),SimAsym(w2,w1)) (71)\nKLDMax(w1,w2) = max(KLD(w1,w2),KLD(w2,w1)) (72)\nAnother method to convert an asymmetric measure of distributional similarity (or relatedness) into a symmetric one is by taking the average (formula 73) of the two possible similarity values. A specific implementation on the KL divergence formula is given in equation (74)\nSimAvg(w1,w2) = 1 2 (SimAsym(w1,w2)+SimAsym(w2,w1)) (73) KLDAvg(w1,w2) = 1 2 (KLD(w1,w2)+KLD(w2,w1)) (74) = 1 2 \u2211w\u2208C(w1)\u222aC(w2) ( P(w|w1) log P(w|w1) P(w|w2) +P(w|w2) log P(w|w2) P(w|w1) ) (75)\n30 Saif Mohammad and Graeme Hirst\n= 1 2 \u2211w\u2208C(w1)\u222aC(w2)\n(\nP(w|w1) log P(w|w1) P(w|w2) \u2212P(w|w2) log P(w|w1) P(w|w2)\n)\n(76)\n= 1 2 \u2211w\u2208C(w1)\u222aC(w2) (P(w|w1)\u2212P(w|w2)) log P(w|w1) P(w|w2)\n(77)\nDetermining the effectiveness of such conversions of existing asymmetric measures is part of our future work."}, {"heading": "4.6. HOW CRMS FIT", "text": "The CRMs suggested by Weeds (2003) are the first distributional measures to be evaluated by comparing ranked word pairs with those ranked by humans (Miller and Charles word pairs). At first glance the CRMs may look quite distinct from the rest of the distributional measures studied so far, owing to their rather complex formulae and multiple optimizing parameters. However, setting the parameters to certain standard values equates a few of the CRMs to other measures. The difference-weighted token-based CRM suggested by Weeds has identical values for precision and recall. She proves that the precision (or recall) is inversely related to the L1 norm measure. This seemingly odd result of equating a distributional distance measure with a precision (or recall) value makes sense due to the following \u2014 as substitutability is defined as a measure of distributional similarity, metrics such as precision and recall which quantify how good the substitution is, reflect the distributional similarity and are inversely related to distributional distance. Thus setting \u03b3 = 0 and \u03b2 = 1 or 0, causes the CRM to behave like the L1 norm. Further, as shown below, setting \u03b3 = 1 (in other words, taking the F measure) makes the difference-weighted mutual information\u2013based CRM identical to the mutual information\u2013based Dice coefficient (34). Following4 is a proof of the same. The precision and recall of the difference-weighted MI-based CRMs are repeated here (equations (78) and (79)) for convenience.\nPMIdw (w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w1) I(w,w1) (78)\nRMIdw(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w2) I(w,w2) (79)\nDistributional Measures as Proxies for Semantic Relatedness 31\nTheorem 2. The difference-weighted mutual information\u2013based CRM equates to the mutual information\u2013based Dice coefficient if its parameter \u03b3 is set to 1.\nProof.\nSimMIdw(w1,w2) = \u03b3\n[\n2\u00d7P\u00d7R P+R\n]\n+(1\u2212 \u03b3)\n[\n\u03b2[P]+ (1\u2212\u03b2)[R]\n]\n= 1\n[\n2\u00d7P\u00d7R P+R\n]\n+(1\u22121)\n[\n\u03b2[P]+ (1\u2212\u03b2)[R]\n]\n= 2\u00d7P\u00d7R\nP+R\nOn substituting values for P and R from equations (78) and (79):\nSimMIdw(w1,w2)\n= 2\u00d7\n(\n\u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1),I(w,w2)) \u2211w\u2208C(w1) I(w,w1)\n) \u00d7 ( \u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1),I(w,w2)) \u2211w\u2208C(w2) I(w,w2) )\n(\n\u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1),I(w,w2)) \u2211w\u2208C(w1) I(w,w1)\n) + ( \u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1),I(w,w2)) \u2211w\u2208C(w2) I(w,w2) )\n=\n2\u00d7\n(\n(\u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1),I(w,w2))) 2\n(\u2211w\u2208C(w1) I(w,w1))(\u2211w\u2208C(w2) I(w,w2))\n)\n(\u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1),I(w,w2)))(\u2211w\u2208C(w1) I(w,w1)+\u2211w\u2208C(w2) I(w,w2)) (\u2211w\u2208C(w1) I(w,w1))(\u2211w\u2208C(w2) I(w,w2))\n= 2\u00d7\u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1), I(w,w2)\n\u2211w\u2208C(w1) I(w,w1)+\u2211w\u2208C(w2) I(w,w2)\n= DiceMI(w1,w2)\n\u2737"}, {"heading": "4.7. HIT AND MISS CO-OCCURRENCES", "text": "Lastly, we examine two kinds of co-occurrences that pose a challenge to existing distributional measures: (1) Word pairs that occur together less number of times than what would be expected by chance. Measures like PMI cannot predict their association values with confidence and as pointed out earlier this is countered by ignoring them completely. This means that the system misses out on evidence from this set of co-occurrence pairs. (2) Co-occurrence pairs\n32 Saif Mohammad and Graeme Hirst\nformed by a word with target words that are near synonyms. Inkpen and Hirst (2002) point out that near synonyms (for example, hidden and concealed) may form strong and anti-collocations, respectively, with the same co-occurring word (for example, agenda). All distributional measures that use strength of association to determine semantic relatedness will consider the large discrepancy in strength of association as evidence of unrelatedness. Therefore, these co-occurrence pairs, which are not ignored (unlike the previous ones), will negatively impact the ability of distributional measures to predict semantic relatedness of near synonyms. It should be noted that we cannot eliminate such co-occurrences in a straightforward manner simply because we are not aware apriori if the target words are near synonyms. It would be interesting to determine the precise quantitative effect of such co-occurrences on the performance of distributional measures."}, {"heading": "4.8. SUMMARIZING THE DISTRIBUTIONAL MEASURES", "text": "In the last two sections we have seen numerous distributional measures. Tables I, II, III, and IV listed in the appendix summarize their properties."}, {"heading": "5. Semantic Network and Ontology-Based Measures", "text": "Creation of electronically available ontologies and semantic networks like WordNet has allowed their use to help solve numerous natural language problems including the measurement of semantic distance between two words. Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) have done an extensive survey of the various WordNet-based measures, their comparisons with human judgment on selected word pairs, and their efficacy in applications such as spelling correction and word sense disambiguation. Hence, this paper provides just a brief summary of the major WordNet-based measures of similarity and focuses on their comparison with distributional ones.\nOne of the earliest and simplest measures is the Rada et al. (1989) edge counting method. The shortest path in the network between the two target words (target path) is determined. The more edges there are between two words, the more distant they are. Elegant as it may be, the measure relies on the unlikely assumption that all the network edges correspond to identical semantic distance between the nodes they connect. Nodes in a network may be connected by numerous relations such as hyponymy, meronymy and so on. Edge counts apart, Hirst and St-Onge (1998) take into account the fact that if the target path consists of edges that belong to a number of such relations, the target words are likely more distant. The idea is that if we start from a particular node (base word) and take a path via a particular relation\nDistributional Measures as Proxies for Semantic Relatedness 33\n(say, hyponymy), to a certain extent the words reached will be quite related to the base word. However, if during the way we take edges belonging to different relations (other than hyponymy), very soon we may reach words that are unrelated. Hirst and St-Onge\u2019s measure of semantic relatedness is listed below:\nHS(c1,c2) =C\u2212path length\u2212 k\u00d7d (80)\nwhere c1 and c2 are the target concepts/words. And, d is the number of times an edge corresponding to a different relation than that of the preceding edge is taken. C and k are empirically determined constants.\nLeacock and Chodrow (1998) used just one relation (hyponymy) and modified the path length formula to reflect the fact that edges lower down in the is-a hierarchy correspond to smaller semantic distance than the ones higher up. For example, sports car and car (low in the hierarchy) are much more similar than transport and instrumentation (higher up in the hierarchy) even though both pairs of words are separated by exactly one edge in the is-a hierarchy of WordNet.\nLC(c1,c2) =\u2212 log len(c1,c2)\n2D (81)\nwhere D is the depth in the taxonomy. Resnik (1995) suggested a measure that used corpus statistics along with the knowledge obtained from a semantic network. The measure is based on the notion that the semantic similarity of two words may be determined from the word that represents their similarity (the lowest common subsumer or lowest super-ordinate (lso)). The more the information contained in this node, the more similar the two words are. Observe that using information content (IC) has the effect of inherently scaling the semantic similarity measure by depth of the taxonomy. Usually, the lower the lowest super-ordinate, the lower is the probability of occurrence of the lso and the concepts subsumed by it, and hence, the higher is its information content.\nRes(c1,c2) =\u2212 logp(lso(c1,c2)) (82)\nAs per the formula, given a particular lowest super-ordinate, the exact positions of the target words below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (denoted by JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso) and IC(c2)\u2212 IC(lso)). The final semantic distance between the two concepts is then taken to be the sum of these differences\n34 Saif Mohammad and Graeme Hirst\n(see Budanitsky (1999) for more details). Lin (1997) points out that the lso is what is common between the two target concepts and that its information content is the common information between the two concepts. Lin\u2019s formula (denoted by Lin) can thus be thought of as taking the Dice coefficient of the information in the two concepts.\nJC(c1,c2) = 2log p(lso(c1,c2))\u2212 (log(p(c1))+ (log(p(c2))) (83)\nLin(c1,c2) = 2\u00d7 log p(lso(c1,c2))\nlog(p(c1))+ (log(p(c2)) (84)\nBudanitsky and Hirst (2001) show that the Jiang-Conrath measure has the highest correlation (0.850) with the Miller and Charles word pairs and performs better than all other measures considered in a spelling correction task. Patwardhan et al. (2003) get similar results using the measure for word sense disambiguation (especially of nouns)."}, {"heading": "6. Comparison of Distributional and Ontology-Based Measures", "text": "Distributional and ontology-based measures use distinct sources of knowledge to achieve the same goal \u2014 the ability to mimic human judgment of semantic relatedness. Owing to the difference in methodology, many interesting comparisons may be made. The next few subsections aim at bringing them to light."}, {"heading": "6.1. KNOWLEDGE SOURCE VERSUS SIMILARITY MEASURE", "text": "Ontologies are much more expensive resources than raw data, which is freely available. Creating an ontology requires human experts, is time intensive and rather brittle to changes in language. Once created, updating an ontology is again expensive and there is usually a lag between the current state of language usage/comprehension and the semantic network representing it. Further, the complexity of human languages makes creation of even a near perfect semantic network of its concepts impossible. Thus in many ways the ontology-based measures are as good as the networks on which they are based. On the other hand, large corpora, trillions of words in size, may now be collected by a simple web crawler. Large corpora of more formal writing are also available (for example, the Wall Street Journal or the American Printing House for the Blind (APHB) corpus). Therefore, using an appropriate distributional measure that best captures the semantic similarity\u2013predicting information, plays a much more vital role in case of distributional measures.\nAs ontologies are a rich source of information where the various concepts are linked together by powerful relations such as hyponymy and meronymy,\nDistributional Measures as Proxies for Semantic Relatedness 35\nthe ontological measures likely correctly identify target words related by edges that belong to just one relation as very similar. However, data sparseness may force distributional measures to assign low similarity values to clearly related word pairs. Assigning appropriate semantic similarity values when target words are connected by different relational edges poses a major challenge to ontological measures."}, {"heading": "6.2. DOMAIN-SPECIFIC SEMANTIC SIMILARITY", "text": "So far, this paper has talked about universal similarity measures. Given a word pair, the measures each give just one similarity value. However, two words may be very semantically similar in a certain domain but not so much in another. For example, the word pair space and time are closely related in the domain of quantum mechanics but not so much in most others. Ontologies have been made for specific domains, which may be used to determine semantic similarity specific to these domains. However, the number of such ontologies is very limited. On the other hand, large amounts of corpora specific to particular domains are much easier to collect, allowing a widespread use of distributional domain-specific similarity."}, {"heading": "6.3. ASSOCIATED WORDS", "text": "Certain word pairs have a special relation with each other. For example, strawberry and cream, doctor and scalpel, and so on. These words are not similar physically or in properties, but strawberries are usually eaten with cream and a doctor uses a scalpel to make an incision. An ontology-based measure will correctly identify the amount of semantic relatedness only if such relations are inherent in the ontology. For example, if the agent-instrument relation does not link concepts in a semantic network (as in WordNet), the ontology-based measures will not identify doctor and scalpel as related.\nOf the various distributional measures discussed, the ones that use simple co-occurrences capture such semantic relatedness, as words that tend to occur together are likely to have large set of common co-occurring words. Measures (e.g., Lin (1998a), Hindle (1990)) that consider a word w to be a shared cooccurrence only if w is related to both target words by the same syntactic relation, will not find such words related, simply because such words that tend to occur in the same sentence are likely to have different thematic roles and thus different syntactic relations with common co-occurring words."}, {"heading": "6.4. MULTI-FACETED CONCEPTS", "text": "The various senses of a word represent distinct concepts. Each of these concepts can usually be described by a number of attributes or features. These attributes may be physical descriptions like color, shape and composition or\n36 Saif Mohammad and Graeme Hirst\nfunction, purpose and role. Two words are adjudged similar if they share a number of such attributes and if the strength of the shared attributes is high. By strength we mean how strongly an attribute helps define the words. The more prominent a shared feature, the more similar the two words are. Further, it is possible that words w1 and w2 are related as they share a certain set of attributes, while w2 and w3 are related because they share a different set of attributes. Thus w1 and w3 are likely not as related as w1 and w2, or w2 and w3. For example, the physical key is closely related to the abstract password, as they are both means of getting access. Password is closely related to encryption as they both pertain to data security. However, the physical key has little to do with encryption and the two are not so much related. Thus semantic relatedness is not necessarily transitive and may be a function of a subset of relevant attributes, not necessarily all.\nHierarchies in an ontology are built by repetitive division of concepts as per their attributes. The order in which these attributes are used to create the tree structure can result in dramatically different hierarchies. For example, consider a scenario depicted in figure 1, where the attributes a1 and a2 are used in different orders to create different hierarchies of the words w1,w2,w3 and w4. Notice that while w1 and w2 are closer to each other than w1 and w3 in hierarchy-1, it is the other way round in hierarchy-2. Thus variations in the order of use of attributes for creating the hierarchy can result in different sets of words being close to each other.\nIt should be noted that real-world semantic hierarchies are created by well formed methodologies and hence the order of attributes used to create the hierarchy is not arbitrary. That said, there is room for variation and further, once a particular hierarchy is chosen, it captures certain semantic relations in its structure, while others are lost.\nIn general, ontology-based measures of similarity capitalize on the property that words that occur close to each other in a hierarchy share a lot of attributes and are therefore similar. However, they usually rely on a fixed\nDistributional Measures as Proxies for Semantic Relatedness 37\nhierarchy. Word pairs that would be closer in variations of the hierarchy are not considered. Thus ontology-based measures are likely to wrongly assign a low semantic similarity value to such word pairs. For example, consider key and password. They are both means to gain access but the is-a hierarchy in WordNet lists them in completely different branches of the network (figure 2). The attribute determining whether the word refers to a physical entity or an abstraction is used first to classify the words and hence key and password fall into different branches at the top of the hierarchy itself. Thus an ontologybased measure is likely to find them unrelated. Distributional measures are not bound by a fixed hierarchy and have a better chance at appropriately identifying the semantic similarity of such word pairs. It would be worth determining the extent to which this is true."}, {"heading": "6.5. EVALUATION AND COMPLEMENTARITY", "text": "Ontology-based and distributional measures of similarity have each been individually shown to be reasonable quantifiers of semantic similarity. WordNetbased measures have been used for applications such as spelling correction and word sense disambiguation, while distributional measures have primarily been used for estimating probabilities of unseen bigrams. Exhaustive comparisons of WordNet-based measures with each other (e.g., Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003)) have found that the Jiang-Conrath measure performs better than the rest.\nDagan et al. (1994) perform experiments with a few relative entropy\u2013 based measures and find that Jensen-Shannon divergence is slightly better than Kullback-Leibler divergence and L1 norm in estimating bigram probabilities of unseen words and in a pseudo\u2013word sense disambiguation experiment. However, the various distributional measures have not been used to rank the Miller-Charles or Rubenstein-Goodenough word pairs, for which\n38 Saif Mohammad and Graeme Hirst\nestimates of human judgment of semantic relatedness are available. Experiments to this end will also enable a comparison of the distributional measures with the ontology-based measures for which this data is already available. Similar to the case of various ontological measures, it is worth determining which distributional measure is closest to human notion of semantic similarity and how well the distributional measures, which rely just on raw data, fare against, the more expensive and knowledge rich, ontology-based measures.\nSince the two kinds of measures rely on different knowledge sources, there is a likelihood that distributional measures more accurately identify the semantic similarity of a certain subset of word pairs, while the ontological methods do so for a different subset. One of the more important problems in the field is to quantify this complementarity. It should be noted that since a similarity measure is evaluated by comparison of ranked word pairs and not by the similarity values alone, capitalizing on the complementarity by creating a combined semantic similarity predictor is a much harder problem."}, {"heading": "7. Conclusions", "text": "The paper has provided a detailed analysis of various corpus-based distributional measures and compared them with measures based on ontologies and semantic networks. Merits and limitations of the various measures were listed. New measures that are likely to overcome the drawbacks of present distributional measures were suggested. Specifically, a distributional measure that keeps the best of Hindle (1990) and Lin (1998a), overcoming their respective drawbacks, was proposed. Variations of Kullback-Leibler divergence and Jensen-Shannon divergence that better capture the disparity in co-occurrence probabilities were suggested. A simple technique to convert asymmetric measures into symmetric ones was suggested. Novel approaches are described to determine distributional similarity by better utilization of co-occurring words related by different syntactic relations.\nThe paper identified significant research problems that need to be answered through experimentation. This will help better understand how statistics from raw data may be manipulated to determine appropriate similarity values between two words. For example, whether the use of syntactically related, rather than plain co-occurrences, significantly improves the measure? Or, are simple co-occurrences just as useful? What kinds of co-occurrences (common, or exclusive, as well) should be used to determine distributional relatedness? Is pointwise mutual information or conditional probability a more suitable measure of association to be used in the various distributional measures? Do compositional or non-compositional distributional measures produce more intuitive semantic similarity values? Which mathematical operation (difference, division, or product) of the co-occurrence distributions yields\nDistributional Measures as Proxies for Semantic Relatedness 39\nvalues that are closest to human judgment, in case of compositional measures? A direct evaluation of the distributional measures (other than L1 norm, \u03b1 skew divergence and the CRMs for which these results exist) by their correlation with the Miller-Charles and Rubenstein-Goodenough word pairs will provide better insight into their relative abilities and will enable a comparison with WordNet-based measures for which the correlation coefficients are already available.\nLastly, the paper pointed out that even though ontological measures are likely to perform better as they rely on a much richer knowledge source, distributional measures have certain distinct advantages. For example, they can easily provide domain-specific similarity values for a large number of domains, their ability to determine similarity of contextually associated word pairs more appropriately, and the flexibility to identify multi-faceted concepts as related from appropriate commonalities that may not be explicitly encoded in a semantic network. Thus it is very likely that ontological measures are better at predicting semantic similarity for certain word pairs, while the distributional measures do so for others. To identify the extent of this complementarity and a suitable combined methodology to assign semantic similarity, remain significant problems in the field. A significant challenge in achieving this is how to reconcile the nature of the two kinds of measures \u2014 while ontological measures predict the semantic similarity of two concepts (or word senses), distributional measures do so for two words. One of the problems we intend to pursue is the development of a methodology that enables the use of distributional measures to predict semantic similarity of concepts, with no or little sense-tagged data.\nAppendix\n.1. CO-OCCURRENCE RETRIEVAL MODELS\nThe precision and recall of additive and difference-weighted CRMs (Weeds, 2003).\nPtypeadd (w1,w2) = |C(w1)\u2229C(w2) |\n|C(w1) | (85)\nRtypeadd (w1,w2) = |C(w1)\u2229C(w2) |\n|C(w2) | (86)\nPtypedw (w1,w2) = \u2211|C(w1)\u2229C(w2)|\nmin(P(w|w1),P(w|w2)) P(w|w1)\n|C(w1) | (87)\nRtypedw (w1,w2) = \u2211|C(w1)\u2229C(w2)|\nmin(P(w|w1),P(w|w2)) P(w|w2)\n|C(w2) | (88)\n40 Saif Mohammad and Graeme Hirst\nPtokenadd (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w1) (89) Rtokenadd (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) P(w|w2) (90) Ptokendw (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) min(P(w|w1),P(w|w2)) (91) Rtokendw (w1,w2) = \u2211 w\u2208C(w1)\u2229C(w2) min(P(w|w2),P(w|w1)) (92)\nPmiadd(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w1)\n\u2211w\u2208C(w1) I(w,w1) (93)\nRmiadd(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) I(w,w2)\n\u2211w\u2208C(w2) I(w,w2) (94)\nPmidw(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w1) I(w,w1) (95)\nRmidw(w1,w2) = \u2211w\u2208C(w1)\u2229C(w2) min(I(w,w1), I(w,w2))\n\u2211w\u2208C(w2) I(w,w2) (96)\nwhere C(x) is the set of all co-occurrences of word x. Note that in case of the difference-weighted token and mutual information\u2013based precision and recall formulae, there is a cancellation of a pair of terms obtained from the core formulae and the penalty.\n.2. SUMMARIZING TABLES\nTables I and II list the measures of distributional distance while tables III and IV list the measures of distributional relatedness/similarity. If a measure is placed in a distributional distance table, it means that the intuition behind the measure lead to its original conception as a distance measure and similarly for a relatedness measure. It should be noted however that a distance measure may be converted into a relatedness measure by taking the inverse or other such mathematical manipulation, and vice versa. Apart from the formula, the tables show whether the measure is compositional (Comp.) or not, and if so then the kind of primary compositional measure (PCM) it is derived from. The last column (Str.) indicates the particular strength of association used (most commonly) in the measure \u2014 conditional probability (CP) or pointwise mutual information (PMI).\nD istributional M easures as Proxies for Sem antic R elatedness 41\n42 Saif M oham m ad and G raem e H irst\nD istributional M easures as Proxies for Sem antic R elatedness 43\n44 Saif M oham m ad and G raem e H irst\nDistributional Measures as Proxies for Semantic Relatedness 45"}, {"heading": "Acknowledgements", "text": "We would like to thank Dr. Suzanne Stevenson, Dr. Gerald Penn, and Dr. Ted Pedersen for their valuable feedback and thought-provoking discussions. This research is financially supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto.\nNotes\n1 In their respective papers, Robert Fano as well as Ken Church and Patrick Hanks refer to pointwise mutual information as mutual information.\n2 It is hard to accurately predict negative word association ratios with confidence (Church and Hanks (1989)).\n3 In their respective papers, Donald Hindle and Dekang Lin refer to pointwise mutual information as mutual information.\n4 P is short for P(w1,w2), while R is short for R(w1,w2). The abbreviations are made due to space constraints and to improve readability."}], "references": [{"title": "Lexical Semantic Relatedness and its Application in Natural Language Processing", "author": ["A. Budanitsky"], "venue": "Technical Report, CSRG-390. Department of Computer Science,", "citeRegEx": "Budanitsky,? \\Q1999\\E", "shortCiteRegEx": "Budanitsky", "year": 1999}, {"title": "Semantic Distance in WordNet: An Experimental, Application-oriented Evaluation of Five Measures", "author": ["A. Budanitsky", "G. Hirst"], "venue": null, "citeRegEx": "Budanitsky and Hirst,? \\Q2001\\E", "shortCiteRegEx": "Budanitsky and Hirst", "year": 2001}, {"title": "Evaluating WordNet-based measures of semantic distance", "author": ["A. Budanitsky", "G. Hirst"], "venue": null, "citeRegEx": "Budanitsky and Hirst,? \\Q2004\\E", "shortCiteRegEx": "Budanitsky and Hirst", "year": 2004}, {"title": "Word Association Norms, Mutual Information and Lexicography", "author": ["K. Church", "P. Hanks"], "venue": "Computational Linguistics", "citeRegEx": "Church and Hanks,? \\Q1989\\E", "shortCiteRegEx": "Church and Hanks", "year": 1989}, {"title": "Similarity-Based Estimation of Word Cooccurrence Probabilities", "author": ["I. Dagan", "L. Lee", "F. Pereira"], "venue": "Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Dagan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "Similarity-Based Methods for Word Sense Disambiguation", "author": ["I. Dagan", "L. Lee", "F. Pereira"], "venue": "Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Dagan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1997}, {"title": "Similarity-Based Models of Cooccurrence Probabilities", "author": ["I. Dagan", "L. Lee", "F. Pereira"], "venue": "Machine Learning", "citeRegEx": "Dagan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1999}, {"title": "Contextual Word Similarity and Estimation from Sparse Data", "author": ["I. Dagan", "S. Marcus", "S. Markovitch"], "venue": "Computer Speech and Language", "citeRegEx": "Dagan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1995}, {"title": "Transmission of Information: A Statistical Theory of Communications", "author": ["R.M. Fano"], "venue": null, "citeRegEx": "Fano,? \\Q1961\\E", "shortCiteRegEx": "Fano", "year": 1961}, {"title": "A synopsis of linguistic theory 1930-55.", "author": ["J.R. Firth"], "venue": null, "citeRegEx": "Firth,? \\Q1957\\E", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "Lexis as a Linguistic Level", "author": ["M.A.K. Halliday"], "venue": "In memory of J.R. Firth. London,", "citeRegEx": "Halliday,? \\Q1966\\E", "shortCiteRegEx": "Halliday", "year": 1966}, {"title": "Overview of the First Text Retrieval Conference", "author": ["D. Harman"], "venue": null, "citeRegEx": "Harman,? \\Q1993\\E", "shortCiteRegEx": "Harman", "year": 1993}, {"title": "Noun Classification from Predicate-Argument Structures", "author": ["D. Hindle"], "venue": "Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90). Pittsburgh,", "citeRegEx": "Hindle,? \\Q1990\\E", "shortCiteRegEx": "Hindle", "year": 1990}, {"title": "Lexical Chains as Representations of Context for the Detection and Correction of Malapropisms", "author": ["G. Hirst", "D. St-Onge"], "venue": "In: C. Fellbaum (ed.): WordNet: An Electronic Lexical Database", "citeRegEx": "Hirst and St.Onge,? \\Q1998\\E", "shortCiteRegEx": "Hirst and St.Onge", "year": 1998}, {"title": "Acquiring collocations for lexical choice between nearsynonyms", "author": ["D. Inkpen", "G. Hirst"], "venue": "SIGLEX Workshop on Unsupervised Lexical Acquisition,", "citeRegEx": "Inkpen and Hirst,? \\Q2002\\E", "shortCiteRegEx": "Inkpen and Hirst", "year": 2002}, {"title": "Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "Proceedings of the International Conference on Research on Computational Linguistics (ROCLING X). Taiwan", "citeRegEx": "Jiang and Conrath,? \\Q1997\\E", "shortCiteRegEx": "Jiang and Conrath", "year": 1997}, {"title": "Combining Local Context and WordNet Similarity for Word Sense Identification", "author": ["C. Leacock", "M. Chodrow"], "venue": "In: C. Fellbaum (ed.): WordNet: An Electronic Lexical Database", "citeRegEx": "Leacock and Chodrow,? \\Q1998\\E", "shortCiteRegEx": "Leacock and Chodrow", "year": 1998}, {"title": "Measures of Distributional Similarity", "author": ["L. Lee"], "venue": "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Lee,? \\Q1999\\E", "shortCiteRegEx": "Lee", "year": 1999}, {"title": "On the Effectiveness of the Skew Divergence for Statistical Language Analysis", "author": ["L. Lee"], "venue": "Artificial Intelligence and Statistics", "citeRegEx": "Lee,? \\Q2001\\E", "shortCiteRegEx": "Lee", "year": 2001}, {"title": "Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity", "author": ["D. Lin"], "venue": "Proceedings of the 8th Conference of the European Chapter of the Association for Computational Linguistics", "citeRegEx": "Lin,? \\Q1997\\E", "shortCiteRegEx": "Lin", "year": 1997}, {"title": "Automatic Retreival and clustering of Similar Words", "author": ["D. Lin"], "venue": "Proceedings of the 17th International Conference on Computational Linguistics (COLING-98). Montreal,", "citeRegEx": "Lin,? \\Q1998\\E", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "An Information-Theoretic Definition of Similarity", "author": ["D. Lin"], "venue": "Proceedings of the 15th International Conference on Machine Learning", "citeRegEx": "Lin,? \\Q1998\\E", "shortCiteRegEx": "Lin", "year": 1998}, {"title": "Contextual Correlates of Semantic Similarity", "author": ["G.A. Miller", "W.G. Charles"], "venue": "Language and Cognitive Processes", "citeRegEx": "Miller and Charles,? \\Q1991\\E", "shortCiteRegEx": "Miller and Charles", "year": 1991}, {"title": "Discovering word senses from text", "author": ["P. Pantel", "D. Lin"], "venue": "Proceedings of the 8th Association of Computing Machinery SIGKDD International Conference On Knowledge Discovery and Data Mining. Edmonton,", "citeRegEx": "Pantel and Lin,? \\Q2002\\E", "shortCiteRegEx": "Pantel and Lin", "year": 2002}, {"title": "Using Measures of Semantic Relatedness for Word Sense Disambiguation", "author": ["S. Patwardhan", "S. Banerjee", "T. Pedersen"], "venue": "Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLING-03). Mexico City,", "citeRegEx": "Patwardhan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2003}, {"title": "Experience with a Stack Decoder-Based HMM CSR and Back-off n-gram Language Models", "author": ["D.B. Paul"], "venue": "Proceedings of the Speech and Natural Language Workshop. Palo Alto, California,", "citeRegEx": "Paul,? \\Q1991\\E", "shortCiteRegEx": "Paul", "year": 1991}, {"title": "Distributional Clustering of English Words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "Proceedings of the 31st Annual Meeting of the Association of Computational Linguistics (ACL-93)", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Development and Application of a Metric on Semantic Nets", "author": ["R. Rada", "H. Mili", "E. Bicknell", "M. Blettner"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics", "citeRegEx": "Rada et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Rada et al\\.", "year": 1989}, {"title": "Using Information Content to Evaluate Semantic Similarity", "author": ["P. Resnik"], "venue": "Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95)", "citeRegEx": "Resnik,? \\Q1995\\E", "shortCiteRegEx": "Resnik", "year": 1995}, {"title": "Contextual Correlates of Synonymy", "author": ["H. Rubenstein", "J.B. Goodenough"], "venue": "Communications of the Association for Computing Machinery (ACM-65)", "citeRegEx": "Rubenstein and Goodenough,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein and Goodenough", "year": 1965}, {"title": "A Cooccurrence-Based Thesaurus and Two Applications to Information Retreival", "author": ["H. Sch\u00fctze", "J.O. Pedersen"], "venue": "Information Processing and Management", "citeRegEx": "Sch\u00fctze and Pedersen,? \\Q1997\\E", "shortCiteRegEx": "Sch\u00fctze and Pedersen", "year": 1997}, {"title": "Measures and Applications of Lexical Distributional Similarity", "author": ["J.E. Weeds"], "venue": "Ph.D. thesis, Department of Informatics,", "citeRegEx": "Weeds,? \\Q2003\\E", "shortCiteRegEx": "Weeds", "year": 2003}, {"title": "Constructing and Examining Personalized Cooccurrence-based Thesauri on Web pages", "author": ["S. Yoshida", "T. Yukawa", "K. Kuwabara"], "venue": "Proceedings of the 12th International World Wide Web Conference. Budapest,", "citeRegEx": "Yoshida et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yoshida et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) do an extensive survey and comparison of the various WordNet-based measures.", "startOffset": 0, "endOffset": 76}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) do an extensive survey and comparison of the various WordNet-based measures. Measures that use just raw text, known as the distributional measures, have been described individually (for example, in Sch\u00fctze and Pedersen (1997), Hindle (1990), Lin (1998a), Pereira et al.", "startOffset": 0, "endOffset": 302}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) do an extensive survey and comparison of the various WordNet-based measures. Measures that use just raw text, known as the distributional measures, have been described individually (for example, in Sch\u00fctze and Pedersen (1997), Hindle (1990), Lin (1998a), Pereira et al.", "startOffset": 0, "endOffset": 317}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) do an extensive survey and comparison of the various WordNet-based measures. Measures that use just raw text, known as the distributional measures, have been described individually (for example, in Sch\u00fctze and Pedersen (1997), Hindle (1990), Lin (1998a), Pereira et al.", "startOffset": 0, "endOffset": 330}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) do an extensive survey and comparison of the various WordNet-based measures. Measures that use just raw text, known as the distributional measures, have been described individually (for example, in Sch\u00fctze and Pedersen (1997), Hindle (1990), Lin (1998a), Pereira et al. (1993), etc) but have not been extensively compared among each other.", "startOffset": 0, "endOffset": 353}, {"referenceID": 8, "context": "Given two events x and y with probabilities P(x) and P(y), their pointwise mutual information (Fano, 1961)1, PMI for short, or just I, is defined as follows:", "startOffset": 94, "endOffset": 106}, {"referenceID": 3, "context": "Church and Hanks (1989)1 introduce word association ratio, which is similar to pointwise mutual information.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "Church and Hanks (1989)1 introduce word association ratio, which is similar to pointwise mutual information. If x and y are words with probabilities P(x) and P(y) (estimated by corpus counts), their association ratio is defined to be the same as in (1), except that P(x,y) stands for the probability that x appears, within a certain window, before y. It should be noted that P(x,y) is no longer symmetric (P(x,y) 6= P(y,x)) as P(x,y) and P(y,x) represent two different events. If two words have a word association ratio close to zero then they do not share an interesting relationship but if I(w1,w2) \u226b 0, then w2 follows w1 (within a certain window) more often than chance and the words w1 and w2 are strong co-occurrences. Theoretically, word association ratio may yield negative values (word pair occurs less frequently than expected by random chance) but Church and Hanks (1989) show that it is hard to accurately predict negative word association ratios with confidence.", "startOffset": 0, "endOffset": 883}, {"referenceID": 3, "context": "Church and Hanks (1989)1 introduce word association ratio, which is similar to pointwise mutual information. If x and y are words with probabilities P(x) and P(y) (estimated by corpus counts), their association ratio is defined to be the same as in (1), except that P(x,y) stands for the probability that x appears, within a certain window, before y. It should be noted that P(x,y) is no longer symmetric (P(x,y) 6= P(y,x)) as P(x,y) and P(y,x) represent two different events. If two words have a word association ratio close to zero then they do not share an interesting relationship but if I(w1,w2) \u226b 0, then w2 follows w1 (within a certain window) more often than chance and the words w1 and w2 are strong co-occurrences. Theoretically, word association ratio may yield negative values (word pair occurs less frequently than expected by random chance) but Church and Hanks (1989) show that it is hard to accurately predict negative word association ratios with confidence. Systems which use word association ratio may be adversely affected by this. A common approach to counter this is to equate the negative association values to 0 (for example, Lin (1998a)).", "startOffset": 0, "endOffset": 1162}, {"referenceID": 3, "context": "Church and Hanks (1989)1 introduce word association ratio, which is similar to pointwise mutual information. If x and y are words with probabilities P(x) and P(y) (estimated by corpus counts), their association ratio is defined to be the same as in (1), except that P(x,y) stands for the probability that x appears, within a certain window, before y. It should be noted that P(x,y) is no longer symmetric (P(x,y) 6= P(y,x)) as P(x,y) and P(y,x) represent two different events. If two words have a word association ratio close to zero then they do not share an interesting relationship but if I(w1,w2) \u226b 0, then w2 follows w1 (within a certain window) more often than chance and the words w1 and w2 are strong co-occurrences. Theoretically, word association ratio may yield negative values (word pair occurs less frequently than expected by random chance) but Church and Hanks (1989) show that it is hard to accurately predict negative word association ratios with confidence. Systems which use word association ratio may be adversely affected by this. A common approach to counter this is to equate the negative association values to 0 (for example, Lin (1998a)). This usually means that the system will ignore such words. A problem with PMI in general (which is inherited by word association ratio) is that low frequency events get higher scores than expected. Pantel and Lin (2002) try to overcome this by multiplying the PMI value with a correction factor.", "startOffset": 0, "endOffset": 1384}, {"referenceID": 0, "context": "Budanitsky and Hirst (2001) and Budanitsky and Hirst (2004) point out that semantic similarity is used when similar entities such as apples and bananas or table and furniture are compared.", "startOffset": 0, "endOffset": 28}, {"referenceID": 0, "context": "Budanitsky and Hirst (2001) and Budanitsky and Hirst (2004) point out that semantic similarity is used when similar entities such as apples and bananas or table and furniture are compared.", "startOffset": 0, "endOffset": 60}, {"referenceID": 0, "context": "Budanitsky and Hirst (2001) and Budanitsky and Hirst (2004) point out that semantic similarity is used when similar entities such as apples and bananas or table and furniture are compared. These entities are close to each other in an is-a hierarchy. For example, apples and bananas are hyponyms of fruit and table is a hyponym of furniture. However, even dissimilar entities may be semantically related, for example, door and knob, tree and shade, or gym and weights. In this case the two entities are not similar per se, but are related by some relationship. This relationship may be one of the classical relationships such as meronymy (is part of) as in door\u2013knob or a non-classical one as in tree\u2013shade and gym\u2013weights. Thus two entities are semantically related if they are semantically similar (close together in the is-a hierarchy) or share any other classical or non-classical relationships. As Budanitsky and Hirst (2004) point out, semantic similarity is a subset of semantic relatedness.", "startOffset": 0, "endOffset": 930}, {"referenceID": 6, "context": "This is known as the distributional hypothesis (Firth (1957) and Harris (1968)) and such measures have traditionally been referred to as measures of distributional similarity.", "startOffset": 48, "endOffset": 61}, {"referenceID": 6, "context": "This is known as the distributional hypothesis (Firth (1957) and Harris (1968)) and such measures have traditionally been referred to as measures of distributional similarity.", "startOffset": 48, "endOffset": 79}, {"referenceID": 0, "context": "The hypothesis makes intuitive sense as Budanitsky and Hirst (2004) point out.", "startOffset": 40, "endOffset": 68}, {"referenceID": 30, "context": "Distributional Measures as Proxies for Semantic Relatedness 5 Like measures of distributional similarity there exist measures of what we will call distributional relatedness (Sch\u00fctze and Pedersen (1997) and Yoshida et al.", "startOffset": 175, "endOffset": 203}, {"referenceID": 30, "context": "Distributional Measures as Proxies for Semantic Relatedness 5 Like measures of distributional similarity there exist measures of what we will call distributional relatedness (Sch\u00fctze and Pedersen (1997) and Yoshida et al. (2003)).", "startOffset": 175, "endOffset": 229}, {"referenceID": 28, "context": "Rubenstein and Goodenough (1965) were the first to conduct quantitative experiments with human subjects who were asked to rate 65 word pairs on a scale of 0.", "startOffset": 0, "endOffset": 33}, {"referenceID": 22, "context": "Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs.", "startOffset": 0, "endOffset": 26}, {"referenceID": 29, "context": "A popular variation (Yoshida et al. (2003), Lee (1999), and Sch\u00fctze and Pedersen (1997)) that incorporates this information is stated below:", "startOffset": 21, "endOffset": 43}, {"referenceID": 17, "context": "(2003), Lee (1999), and Sch\u00fctze and Pedersen (1997)) that incorporates this information is stated below:", "startOffset": 8, "endOffset": 19}, {"referenceID": 17, "context": "(2003), Lee (1999), and Sch\u00fctze and Pedersen (1997)) that incorporates this information is stated below:", "startOffset": 8, "endOffset": 52}, {"referenceID": 11, "context": "Sch\u00fctze and Pedersen (1997) use the Tipster category B corpus (Harman, 1993) (450,000 unique terms) and the Wall Street Journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency words (frequency rank between 2,000 and 5,000).", "startOffset": 62, "endOffset": 76}, {"referenceID": 29, "context": "The cosine is used, among others, by Sch\u00fctze and Pedersen (1997) and Yoshida et al.", "startOffset": 37, "endOffset": 65}, {"referenceID": 29, "context": "The cosine is used, among others, by Sch\u00fctze and Pedersen (1997) and Yoshida et al. (2003), who suggest methods of automatically generating thesauri from text corpora.", "startOffset": 37, "endOffset": 91}, {"referenceID": 29, "context": "The cosine is used, among others, by Sch\u00fctze and Pedersen (1997) and Yoshida et al. (2003), who suggest methods of automatically generating thesauri from text corpora. Sch\u00fctze and Pedersen (1997) use the Tipster category B corpus (Harman, 1993) (450,000 unique terms) and the Wall Street Journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency words (frequency rank between 2,000 and 5,000).", "startOffset": 37, "endOffset": 196}, {"referenceID": 11, "context": "Sch\u00fctze and Pedersen (1997) use the Tipster category B corpus (Harman, 1993) (450,000 unique terms) and the Wall Street Journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency words (frequency rank between 2,000 and 5,000). Latent semantic indexing and single-value decomposition (see Sch\u00fctze and Pedersen (1997) for details) are used to reduce the dimensionality of the matrix and get for each term a word vector of its 20 strongest co-occurrences.", "startOffset": 63, "endOffset": 341}, {"referenceID": 11, "context": "Sch\u00fctze and Pedersen (1997) use the Tipster category B corpus (Harman, 1993) (450,000 unique terms) and the Wall Street Journal to create a large but sparse co-occurrence matrix of 3,000 medium-frequency words (frequency rank between 2,000 and 5,000). Latent semantic indexing and single-value decomposition (see Sch\u00fctze and Pedersen (1997) for details) are used to reduce the dimensionality of the matrix and get for each term a word vector of its 20 strongest co-occurrences. The cosine of a word vector (say ~w1) with each of the other word vectors is calculated and the top scores along with the words whose vector generated the top scores is noted. These words form the thesaurus entries for w1. Yoshida et al. (2003) believe that words that are closely related for one person may be distant for another.", "startOffset": 63, "endOffset": 723}, {"referenceID": 4, "context": "In the Manhattan distance (5) (Dagan et al. (1997), Dagan et al.", "startOffset": 31, "endOffset": 51}, {"referenceID": 4, "context": "In the Manhattan distance (5) (Dagan et al. (1997), Dagan et al. (1999), and Lee (1999)), the disparity in strength of association of w1 and w2 with each word that they co-occur with, is summed.", "startOffset": 31, "endOffset": 72}, {"referenceID": 4, "context": "In the Manhattan distance (5) (Dagan et al. (1997), Dagan et al. (1999), and Lee (1999)), the disparity in strength of association of w1 and w2 with each word that they co-occur with, is summed.", "startOffset": 31, "endOffset": 88}, {"referenceID": 4, "context": "In the Manhattan distance (5) (Dagan et al. (1997), Dagan et al. (1999), and Lee (1999)), the disparity in strength of association of w1 and w2 with each word that they co-occur with, is summed. The more the disparity in association, the more is the distributional distance between the two words. The Euclidean distance (6) (Lee (1999)) employs the root mean squared of the disparity in association to get the final distributional distance.", "startOffset": 31, "endOffset": 336}, {"referenceID": 17, "context": "Lee (1999) compared the ability of all three spatial metrics to determine the probability of an unseen (not found in training data) word pair.", "startOffset": 0, "endOffset": 11}, {"referenceID": 17, "context": "Lee (1999) compared the ability of all three spatial metrics to determine the probability of an unseen (not found in training data) word pair. The measures in order of their performance (from better to worse) were: L1 norm, cosine, and L2 norm. Weeds (2003) determined the correlation of word pair ranking as per a handful of distributional measures with human rankings (Miller and Charles word pairs Miller and Charles (1991)).", "startOffset": 0, "endOffset": 258}, {"referenceID": 17, "context": "Lee (1999) compared the ability of all three spatial metrics to determine the probability of an unseen (not found in training data) word pair. The measures in order of their performance (from better to worse) were: L1 norm, cosine, and L2 norm. Weeds (2003) determined the correlation of word pair ranking as per a handful of distributional measures with human rankings (Miller and Charles word pairs Miller and Charles (1991)).", "startOffset": 0, "endOffset": 427}, {"referenceID": 17, "context": "Lee (1999) shows that the Jaccard coefficient performs better than L1 norm in an unseen bigram probability estimation task.", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "Dagan et al. (1995) use a weighted version of the Jaccard coefficient on pseudo-fuzzy sets with PMI as the strength of association.", "startOffset": 0, "endOffset": 20}, {"referenceID": 12, "context": "MUTUAL INFORMATION\u2013BASED MEASURES Hindle (1990) was one of the first to factor the strength of association of co-occurring words into a distributional similarity measure.", "startOffset": 34, "endOffset": 48}, {"referenceID": 19, "context": "Lin (1998a) suggests a different measure derived from his information theoretic definition of similarity (Lin, 1998b).", "startOffset": 0, "endOffset": 12}, {"referenceID": 11, "context": "Note that this is different from Hindle (1990) where even the cases of negative PMI were also considered.", "startOffset": 33, "endOffset": 47}, {"referenceID": 3, "context": "As mentioned earlier, Church and Hanks (1989) show that it is hard to accurately predict negative word association ratios with confidence.", "startOffset": 22, "endOffset": 46}, {"referenceID": 12, "context": "Also notice that like Hindle\u2019s measure, both Lin\u2019s and mine are measures of distributional similarity. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes.", "startOffset": 22, "endOffset": 117}, {"referenceID": 12, "context": "Also notice that like Hindle\u2019s measure, both Lin\u2019s and mine are measures of distributional similarity. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes. Lin (1998a) used his measure to generate a thesaurus from a 64-million-word", "startOffset": 22, "endOffset": 256}, {"referenceID": 22, "context": "Pereira et al. (1993) and Dagan et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "(1993) and Dagan et al. (1994) point out that words have probabilistic distributions with respect to neighboring syntactically related words.", "startOffset": 11, "endOffset": 31}, {"referenceID": 25, "context": "Use of Kullback-Leibler distance as the semantic distance metric yielded a 20% improvement in perplexity on the Wall Street Journal and dictation corpora provided by ARPA\u2019s HLT program (Paul, 1991).", "startOffset": 185, "endOffset": 197}, {"referenceID": 24, "context": "This asymmetry is counter-intuitive to the general notion of semantic similarity of words, although Weeds (2003) has argued in favor of asymmetric measures.", "startOffset": 100, "endOffset": 113}, {"referenceID": 20, "context": "Pereira et al. (1993) use relative entropy to create clusters of nouns from verb-object pairs corresponding to a thousand most frequent nouns in the Grolier\u2019s Encyclopedia, June 1991 version (10 million words).", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "Dagan et al. (1994) use Kullback-Leibler distance to estimate the probabilities of bigrams that were not seen in a text corpus.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Dagan et al. (1994) use Kullback-Leibler distance to estimate the probabilities of bigrams that were not seen in a text corpus. They point out that a significant number of possible bigrams are not seen in any given text corpus. The probabilities of such bigrams may be determined by taking a weighted average of the probabilities of bigrams composed of distributionally similar words. Use of Kullback-Leibler distance as the semantic distance metric yielded a 20% improvement in perplexity on the Wall Street Journal and dictation corpora provided by ARPA\u2019s HLT program (Paul, 1991). The use of distributionally similar words to estimate unseen bigram probabilities will likely lead to erroneous results in case of less-preferred and strongly-preferred collocations (word pairs). Inkpen and Hirst (2002) point out that even though words like task and job are semantically very similar, the collocations they form with other words may have varying degrees of usage.", "startOffset": 0, "endOffset": 804}, {"referenceID": 17, "context": "Distributional Measures as Proxies for Semantic Relatedness 17 Lee (2001) shows that \u03b1 skew divergence performs better than KullbackLeibler divergence in estimating word co-occurrence probabilities.", "startOffset": 63, "endOffset": 74}, {"referenceID": 17, "context": "Distributional Measures as Proxies for Semantic Relatedness 17 Lee (2001) shows that \u03b1 skew divergence performs better than KullbackLeibler divergence in estimating word co-occurrence probabilities. Weeds (2003) achieves a correlation of 0.", "startOffset": 63, "endOffset": 212}, {"referenceID": 31, "context": "CO-OCCURRENCE RETRIEVAL MODELS The distributional measures suggested by Weeds (2003) are based on the notion of substitutability.", "startOffset": 72, "endOffset": 85}, {"referenceID": 31, "context": "Weeds argues that the asymmetry in substitutability is intuitive as in many cases it may be okay to substitute a word, say dog, with another, say animal, but the reverse is not likely to be acceptable as often. Since substitutability is a measure of semantic similarity, she believes that distributional similarity between two words should reflect this property as well. Hence, like the Kullback-Leibler divergence, all her distributional similarity models are inherently asymmetric. A word\u2019s co-occurrence information may be specified by the set of cooccurring words alone, or by specifying the strength of co-occurrences, as well. This strength may be captured by a suitable measure of word association such as conditional probability or pointwise mutual information between the co-occurring words and the target words. Also, the difference in the strength of co-occurrence may or may not be used to penalize the substitutability of one word for another. Weeds (2003) provides six distinct formulae for precision and recall, depending on the the strength of co-occurrence and penalty for differences in strength of association.", "startOffset": 0, "endOffset": 970}, {"referenceID": 30, "context": "Weeds (2003) extracted verb-object pairs of 2,000 nouns from the British National Corpus (BNC).", "startOffset": 0, "endOffset": 13}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity.", "startOffset": 0, "endOffset": 32}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity.", "startOffset": 0, "endOffset": 50}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity.", "startOffset": 0, "endOffset": 155}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity.", "startOffset": 0, "endOffset": 280}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity. Sch\u00fctze and Pedersen (1997) and Yoshida et al.", "startOffset": 0, "endOffset": 428}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity. Sch\u00fctze and Pedersen (1997) and Yoshida et al. (2003) use all cooccurring words in a pre-decided window size.", "startOffset": 0, "endOffset": 454}, {"referenceID": 4, "context": "Dagan et al. (1997), Lee (1999), and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactic relation), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relation, while Lin (1998a) uses words related to a noun by any of the many pre-decided syntactic relations to determine distributional similarity. Sch\u00fctze and Pedersen (1997) and Yoshida et al. (2003) use all cooccurring words in a pre-decided window size. Although Lin (1998a) shows that the use of multiple syntactic relations is more beneficial as compared to just one, there exist no published results on whether using only syntactically related words (as compared to all co-occurrences) improves or worsens the quality of semantic similarity assignment.", "startOffset": 0, "endOffset": 531}, {"referenceID": 19, "context": "Use of Multiple Syntactic Relations Lin (1998a) used a subset of words that co-occurred with the target words to determine their distributional similarity.", "startOffset": 36, "endOffset": 48}, {"referenceID": 4, "context": "Taking the maximum of the two as the weight (Dagan et al. (1995)) will mean that more weight is given to a co-occurring word if it has high strength of association with any of the two target words.", "startOffset": 45, "endOffset": 65}, {"referenceID": 4, "context": "Taking the maximum of the two as the weight (Dagan et al. (1995)) will mean that more weight is given to a co-occurring word if it has high strength of association with any of the two target words. As Dagan et al. (1995) point out, there is strong evidence for dissimilarity if the strength of association with the other target word is much lower than the maximum, and strong evidence of similarity if the strength of association with both target words is more or less the same.", "startOffset": 45, "endOffset": 221}, {"referenceID": 18, "context": "Lin (1998a) and Hindle (1990) use pointwise mutual information as the measure of association.", "startOffset": 0, "endOffset": 12}, {"referenceID": 12, "context": "Lin (1998a) and Hindle (1990) use pointwise mutual information as the measure of association.", "startOffset": 16, "endOffset": 30}, {"referenceID": 12, "context": "Lin (1998a) and Hindle (1990) use pointwise mutual information as the measure of association. The mutual information\u2013based CRMs of Weeds (2003) also use the same.", "startOffset": 16, "endOffset": 144}, {"referenceID": 11, "context": "Hindle (1990) uses evidence only from words that co-occur with both target words to determine the distributional similarity.", "startOffset": 0, "endOffset": 14}, {"referenceID": 10, "context": "Consider the classic strong tea vs powerful tea example (Halliday (1966)).", "startOffset": 57, "endOffset": 73}, {"referenceID": 31, "context": "Weeds (2003) argues that this behavior is intuitive as it is more often okay to substitute a generic concept in place of a specific one than vice versa, and substitutability is a indicator of semantic similarity.", "startOffset": 0, "endOffset": 13}, {"referenceID": 30, "context": "The CRMs suggested by Weeds (2003) are the first distributional measures to be evaluated by comparing ranked word pairs with those ranked by humans (Miller and Charles word pairs).", "startOffset": 22, "endOffset": 35}, {"referenceID": 14, "context": "Inkpen and Hirst (2002) point out that near synonyms (for example, hidden and concealed) may form strong and anti-collocations, respectively, with the same co-occurring word (for example, agenda).", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al.", "startOffset": 0, "endOffset": 47}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) have done an extensive survey of the various WordNet-based measures, their comparisons with human judgment on selected word pairs, and their efficacy in applications such as spelling correction and word sense disambiguation.", "startOffset": 0, "endOffset": 76}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) have done an extensive survey of the various WordNet-based measures, their comparisons with human judgment on selected word pairs, and their efficacy in applications such as spelling correction and word sense disambiguation. Hence, this paper provides just a brief summary of the major WordNet-based measures of similarity and focuses on their comparison with distributional ones. One of the earliest and simplest measures is the Rada et al. (1989) edge counting method.", "startOffset": 0, "endOffset": 525}, {"referenceID": 0, "context": "Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003) have done an extensive survey of the various WordNet-based measures, their comparisons with human judgment on selected word pairs, and their efficacy in applications such as spelling correction and word sense disambiguation. Hence, this paper provides just a brief summary of the major WordNet-based measures of similarity and focuses on their comparison with distributional ones. One of the earliest and simplest measures is the Rada et al. (1989) edge counting method. The shortest path in the network between the two target words (target path) is determined. The more edges there are between two words, the more distant they are. Elegant as it may be, the measure relies on the unlikely assumption that all the network edges correspond to identical semantic distance between the nodes they connect. Nodes in a network may be connected by numerous relations such as hyponymy, meronymy and so on. Edge counts apart, Hirst and St-Onge (1998) take into account the fact that if the target path consists of edges that belong to a number of such relations, the target words are likely more distant.", "startOffset": 0, "endOffset": 1018}, {"referenceID": 16, "context": "Leacock and Chodrow (1998) used just one relation (hyponymy) and modified the path length formula to reflect the fact that edges lower down in the is-a hierarchy correspond to smaller semantic distance than the ones higher up.", "startOffset": 0, "endOffset": 27}, {"referenceID": 16, "context": "Leacock and Chodrow (1998) used just one relation (hyponymy) and modified the path length formula to reflect the fact that edges lower down in the is-a hierarchy correspond to smaller semantic distance than the ones higher up. For example, sports car and car (low in the hierarchy) are much more similar than transport and instrumentation (higher up in the hierarchy) even though both pairs of words are separated by exactly one edge in the is-a hierarchy of WordNet. LC(c1,c2) =\u2212 log len(c1,c2) 2D (81) where D is the depth in the taxonomy. Resnik (1995) suggested a measure that used corpus statistics along with the knowledge obtained from a semantic network.", "startOffset": 0, "endOffset": 556}, {"referenceID": 15, "context": "Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms.", "startOffset": 0, "endOffset": 40}, {"referenceID": 15, "context": "Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (denoted by JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso) and IC(c2)\u2212 IC(lso)).", "startOffset": 0, "endOffset": 164}, {"referenceID": 0, "context": "(see Budanitsky (1999) for more details).", "startOffset": 5, "endOffset": 23}, {"referenceID": 0, "context": "(see Budanitsky (1999) for more details). Lin (1997) points out that the lso is what is common between the two target concepts and that its information content is the common information between the two concepts.", "startOffset": 5, "endOffset": 53}, {"referenceID": 18, "context": ", Lin (1998a), Hindle (1990)) that consider a word w to be a shared cooccurrence only if w is related to both target words by the same syntactic relation, will not find such words related, simply because such words that tend to occur in the same sentence are likely to have different thematic roles and thus different syntactic relations with common co-occurring words.", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", Lin (1998a), Hindle (1990)) that consider a word w to be a shared cooccurrence only if w is related to both target words by the same syntactic relation, will not find such words related, simply because such words that tend to occur in the same sentence are likely to have different thematic roles and thus different syntactic relations with common co-occurring words.", "startOffset": 15, "endOffset": 29}, {"referenceID": 0, "context": ", Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al.", "startOffset": 2, "endOffset": 20}, {"referenceID": 0, "context": ", Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al.", "startOffset": 2, "endOffset": 49}, {"referenceID": 0, "context": ", Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003)) have found that the Jiang-Conrath measure performs better than the rest.", "startOffset": 2, "endOffset": 78}, {"referenceID": 0, "context": ", Budanitsky (1999), Budanitsky and Hirst (2001) and Patwardhan et al. (2003)) have found that the Jiang-Conrath measure performs better than the rest. Dagan et al. (1994) perform experiments with a few relative entropy\u2013 based measures and find that Jensen-Shannon divergence is slightly better than Kullback-Leibler divergence and L1 norm in estimating bigram probabilities of unseen words and in a pseudo\u2013word sense disambiguation experiment.", "startOffset": 2, "endOffset": 172}, {"referenceID": 12, "context": "Specifically, a distributional measure that keeps the best of Hindle (1990) and Lin (1998a), overcoming their respective drawbacks, was proposed.", "startOffset": 62, "endOffset": 76}, {"referenceID": 12, "context": "Specifically, a distributional measure that keeps the best of Hindle (1990) and Lin (1998a), overcoming their respective drawbacks, was proposed.", "startOffset": 62, "endOffset": 92}, {"referenceID": 31, "context": "CO-OCCURRENCE RETRIEVAL MODELS The precision and recall of additive and difference-weighted CRMs (Weeds, 2003).", "startOffset": 97, "endOffset": 110}, {"referenceID": 3, "context": "2 It is hard to accurately predict negative word association ratios with confidence (Church and Hanks (1989)).", "startOffset": 85, "endOffset": 109}], "year": 2012, "abstractText": "The automatic ranking of word pairs as per their semantic relatedness and ability to mimic human notions of semantic relatedness has widespread applications. Measures that rely on raw data (distributional measures) and those that use knowledge-rich ontologies both exist. Although extensive studies have been performed to compare ontological measures with human judgment, the distributional measures have primarily been evaluated by indirect means. This paper is a detailed study of some of the major distributional measures; it lists their respective merits and limitations. New measures that overcome these drawbacks, that are more in line with the human notions of semantic relatedness, are suggested. The paper concludes with an exhaustive comparison of the distributional and ontology-based measures. Along the way, significant research problems are identified. Work on these problems may lead to a better understanding of how semantic relatedness is to be measured.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}