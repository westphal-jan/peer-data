{"id": "1203.3498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Automated Planning in Repeated Adversarial Games", "abstract": "game theory's prescriptive power typically relies on preventing full rationality and / or self - play interactions. in contrast, this work sets aside below these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or just more agents. specifically, we introduce a new and concise representation for repeated adversarial ( constant - sum ) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game's nash equilibrium, when facing competing heterogeneous adversaries. to this end, we present teamup, a model - based rl algorithm designed for learning and aggressively planning such an abstraction. in essence, it is somewhat similar to r - max with a cleverly engineered reward shaping that treats exploration as an adversarial versus optimization problem. in practice, it attempts to find an ally with which to tacitly collude ( in more than two - player games ) and then collaborates on a joint plan of actions that can consistently score a high utility figure in adversarial than repeated games. we use the inaugural lemonade stand game tournament to demonstrate the effectiveness of our approach, and find that teamup is undoubtedly the best performing agent, demoting the tournament's actual winning strategy into second place. in our experimental analysis, we show hat our strategy successfully and consistently builds collaborations with many arguably different heterogeneous ( and sometimes very sophisticated ) adversaries.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (200kb)", "http://arxiv.org/abs/1203.3498v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["enrique munoz de cote", "archie c chapman", "adam m sykulski", "nicholas r jennings"], "accepted": false, "id": "1203.3498"}, "pdf": {"name": "1203.3498.pdf", "metadata": {"source": "CRF", "title": "Automated Planning in Repeated Adversarial Games", "authors": ["Enrique Munoz de Cote", "Archie C. Chapman", "Adam M. Sykulski", "Nicholas R. Jennings"], "emails": ["jemc@ecs.soton.ac.uk", "acc@ecs.soton.ac.uk", "adam.sykulski@imperial.ac.uk", "nrj@ecs.soton.ac.uk"], "sections": [{"heading": null, "text": "We use the inaugural Lemonade Stand Game Tournament1 to demonstrate the effectiveness of our approach, and find that TeamUP is the best performing agent, demoting the Tournament\u2019s actual winning strategy into second place. In our experimental analysis, we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous (and sometimes very sophisticated) adversaries."}, {"heading": "1 Introduction", "text": "Adversarial games2 have been studied since the birth of game theory. They abstract many real scenarios such as chess, checkers, poker, and even the cold war. Repeated interactions, on the other hand, model situations where two or more agents interact multiple times, building long\u2013run relationships. Their solutions are often represented as a sequence of actions (or plans of actions). Repeated adversarial games are, therefore, a\n1http://users.ecs.soton.ac.uk/acc/LSGT/home.html 2Adversarial games are also known as constant\u2013sum\ngames.\nnatural framework to study sequential decision making in an adversarial setting. It is somewhat surprising, then, to find that artificial intelligence offers few insights on how to interact against heterogeneous agents in repeated adversarial games, particularly since the field is renowned for offering solutions under weaker assumptions than those imposed by game theory (e.g. regarding computation and memory power). Indeed, the problem is further exacerbated if the adversaries\u2019 behaviour is unknown a priori to the planning agent and the latter needs to interact with its adversaries to learn how they behave.\nAgainst this background, we present a study on how an agent should plan a sequence of actions in a repeated adversarial game when the adversaries are unknown. Specifically, in these games, the capability of an agent to compute a good sequence of actions relies on its capacity to forecast the behaviour of its opponents, which may be a hard task against opponents that themselves adapt. The planner, therefore, needs to construct a plan while considering the uncertain effects of its actions on the decisions of its opponents.\nThe repeated nature of the interaction allows us to frame this as a planning adversarial problem, where the unknown behaviour of the opponents is learnt by repeatedly interacting with them. The learning task however, requires a large number of repeated interactions to be effective, which in many scenarios is unacceptable. To this end, we present a new and concise game abstraction that reduces the state space to a size that is tractable for an agent to learn with a small number of samples. Building on this, we present a reinforcement algorithm that learns on such a game abstraction. It is grounded on reward\u2013shaping and model\u2013based learning, techniques that have both been shown to decrease exploration complexity when compared with a straightforward implementation of Q\u2013 learning.\nBy so doing, our work deviates substantially from other work in multiagent learning against heteroge-\nneous opponents. First, instead of assuming that our planner is facing the worst opponent, as (Littman, 1994; Brafman & Tennenholtz, 2002) do, our objective is to design a planner whose resulting strategy can do better than its security (max\u2013min) equilibrium strategy. Second, on the other extreme, our objective is not optimality against a known algorithm, as (Banerjee & Peng, 2005; Munoz de Cote & Jennings, 2010) consider. In that work, the authors derive planning solutions that are optimal only when facing some specific class of opponents. Our analysis, instead, does not put any type of constraints on the class of adversaries our planning agent might face. As a consequence, because an adversary may have any level of sophistication, in our setting it is not possible to produce optimal solutions.\nThe paper is organized as follows: in Section 2 we give useful background and definitions. In Section 3 we define the planning problem more precisely and introduce our novel state abstraction. Following this, in Section 4 we propose an algorithm capable of learning such a state abstraction. In Section 5 we introduce the setting for our experiments, and in Section 6 we test the performance of our algorithm. Finally, in Section 7 we conclude."}, {"heading": "2 Background and Definitions", "text": "Throughout this work we consider n players that face each other and repeatedly play a normal form game."}, {"heading": "A normal form game (NFG) is a n\u2013player", "text": "simultaneous\u2013move game defined by the tuple \u0393 = \u3008N , {Ai}, {ri}\u3009, where N is the set of n players, and for each player i \u2208 N , Ai is player i\u2019s finite set of available actions, A = \u00d7i\u2208NAi is the set of joint actions and ri : A \u2192 R is the player i\u2019s reward function. An agent\u2019s goal is to maximise its reward, and its best response correspondence, BRi(a\u2212i), is the set of i\u2019s best strategies, given the actions a\u2212i \u2208 \u00d7j\u2208N\\iAj of the other players:\nBRi(a\u2212i) = {ai \u2208 Ai : ai = argmax a\u2032\ni \u2208Ai\nri(a \u2032 i,a\u2212i)}\nIn our context, stable points are characterised by the set of pure Nash equilibria (NE), which are those joint action profiles, a\u2217, in which no individual agent has an incentive to change its action:\nri(a \u2217 i ,a \u2217 \u2212i) \u2212 ri(ai,a \u2217 \u2212i) \u2265 0 \u2200 ai, \u2200 i \u2208 N . (1)\nThat is, in a pure NE, a\u2217i \u2208 BRi(a \u2217 \u2212i) \u2200 i \u2208 N . In many games, a player may find it beneficial to collaborate with some subset of the other players. Clearly, this occurs in coordination games; moreover, it can also arise in general\u2013sum games (to choose from different equilibria) and even more surprisingly, in constant\u2013 sum games (we will explain how later in this section). To make this notion of collaboration precise, we now\nintroduce a refinement to the best response correspondence above. First, note that if the size of an agent\u2019s best response set |BRi(a\u2212i)| > 1, then there may exist an ai \u2208 BRi(a\u2212i) such that the payoff to an opponent j is greater for this action than any other element of i\u2019s best response. Call such a set of actions i\u2019s player j\u2013considered best response, BRi\u2192j(a\u2212i), given by:\nrj(BRi\u2192j(a\u2212i), a\u2212i) \u2265 rj(a \u2032 i, a\u2212i) \u2200a \u2032 i \u2208 BRi(a\u2212i).\nGiven this, if, by i playing a j\u2013considered best response, j\u2019s current action becomes an element of its best response set, then we call this a reciprocal best response. Specifically, if the following holds:\naj \u2208 BRj(BRi\u2192j(a\u2212i),a\u2212ij),\nwhere \u2212ij = N \\{i, j}, then we call BRi\u2192j(a\u2212i) an i\u2013 to\u2013j reciprocal best response, written BRi\u2194j(a\u2212ij). Furthermore, this refinement can be generalised to consider sets of players, \u03c7 \u2208 N \\ i, instead of single players j. Although such reciprocal best responses are not guaranteed to exist for every player and for each action profile in a game, such an action does exist at a Nash equilibrium of a game (by definition). Given this, the reciprocal best response concept will be used as a basic building block of the TeamUP algorithm.\nIn our setting, the agents play a repeated game, in which a NFG is repeatedly played. In this context, the NFG is called a stage game. At each time step, the agents simultaneously choose an action, giving a profile a \u2208 A, which is announced to all players, and each player i receives their reward ri(a) in the NFG. This stage game is repeated at each time step.\nRepeated NFGs can have a finite or infinite horizon, depending on the number of stage games played. A player\u2019s objective in a repeated game is to maximize the sum of rewards received. Imagine that at time t, players have seen the history of play (a1, . . . ,at) \u2208 (A)t. In this context, a behavioural policy is a function \u03c0i : (A)\nt \u2192 Ai that maps histories to actions. However, computing an optimal policy at any time t is an adversarial optimization problem of the objective function \u03c0\u2217i = arg max\u03c0i \u2211T k=t ri(\u03c0i,a k \u2212i). It is called adversarial because the term ak\u2212i (i.e. the adversaries\u2019 optimized strategy) is coupled with the term to be maximized, \u03c0i\n3. The capability of a player to compute an optimal strategy therefore relies on its capacity to forecast its counterpart\u2019s behaviour, which may be a hard task against opponents that adapt."}, {"heading": "3 Planning in Adversarial Repeated Games", "text": "Computing a strategy of play in repeated NFGs is an adversarial optimization problem, as explained earlier. And the planner\u2019s optimization capabilities rely on\n3A change in \u03c0i might cause a change in a k \u2212i.\nhow well it can predict the behaviour of its adversaries. Against adaptive opponents, however, predicting their behaviour means learning a mapping from histories to the opponent\u2019s response strategies. Nevertheless, adversarial games involving two or more players are endowed with some underlying structure that can help reasoning using a higher\u2013level representation. Specifically, it is possible to balance the (constant) sum in the planner\u2019s favour by colluding with other player(s) in order to minimize the utility of the excluded adversaries. However, such collaboration is difficult to achieve without explicit communication, correlation or pre\u2013game agreements, which we do not consider.\nWe focus instead on tacit collusion, in which teams of collaborating agents are formed through repeated play. To implement this collusion, we adopt the approach of modelling the high\u2013level decision\u2013making behaviour model of our opponents, rather than focusing on their specific actions. As we show later, this abstraction allows us to accurately forecast the opponent reactions to our current actions, while at the same time it is simple enough for state transitions to be learnt in a very small number of iterations. Together, these two properties allow our planner to collaborate effectively, when such an opportunity exists."}, {"heading": "3.1 State Abstraction", "text": "Our objective is to collaborate with other players by making them an offer they can\u2019t refuse, i.e. an action profile that consistently gives them a score above the game\u2019s Nash equilibrium. We do this by measuring the deviation of the planner\u2019s opponents from ideal types, which the planner can easily predict and collaborate with. In this work, we use the ideas on leading Q\u2013learning algorithms (Littman & Stone, 2002) to generate our ideal types. We now formally describe these ideal types and then show how we generalise these by measuring opponents\u2019 deviations from ideal types, which creates instances of features. Finally, we will describe how the state space is constructed using a tuple of these features. It is this abstraction that will help us build an automatic planner in the spirit of Littman and Stone\u2019s work."}, {"heading": "Ideal types", "text": "There are two obvious ways of initiating collaborations. First, a player could lead simply by sticking to its current strategy, and wait to see if any opponents follow. Second, a player could follow by changing strategy to one that is inside the BR set. Based on these two patterns of play, we define two ideal types of strategies that an agent could easily collaborate with, as follows:\n\u2022 A perfect Lead strategy picks a starting strategy and does not move from it for the duration of the game.\n\u2022 A perfect Follow strategy always selects actions\nthat are a BR to the previous action selected by the opponent that is being followed."}, {"heading": "Features", "text": "The planner classifies the opponents by their proximity to playing either a lead or follow strategy, based on their previous actions. An opponent classified as playing a lead strategy is usually slow moving, or stationary, and is hence very predictable. An opponent classified as playing a follow strategy tends to play actions that are within the best response set from the previous time step (or an appropriately discounted average of recent time steps). Given this, we now discuss these ideal types and how we measure an opponent\u2019s deviation from them, which form the basis for our state abstraction.\nIn order to classify its opponents, the planner maintains a measure of a lead index, li, and a follow index, fij , (where j \u2208 N \\ i), to measure whether player i is following player j. The indices are calculated from the sequence of past actions of each player Ai = (a 1 i , . . . , a t\u22121 i ):\nli = \u2212\nt\u22121 X\nk=2\n\u03b3t\u22121\u2212k\n\u0393 \u2206\n\u201c\na k i , a k\u22121 i\n\u201d\u03c1\n, (2)\nfij = \u2212\nt\u22121 X\nk=2\n\u03b3t\u22121\u2212k\n\u0393 \u2206\n\u201c\na k i , BRi(a k\u22121 j )\n\u201d\u03c1\n, (3)\nwhere \u0393 = \u2211t\u22121\nk=2 \u03b3 t\u22121\u2212k. The function\n\u2206(aki , BRi(a k\u22121 j )) is a metric that captures the distance between the actions of player i at time\u2013step k and the BRi(aj) at k \u2212 1. These indices therefore quantify the closeness of each opponent to an ideal type by looking at the lag\u2013one distance between respective action sequences. The parameter \u03c1 scales the distances between actions: \u03c1 < 1 treats all behaviour that deviates from the ideal types relatively equally, while \u03c1 > 1 places more value on behaviour that is close to ideal type behaviour. As such, with a \u03c1 > 1, our planner can accommodate players that are noisy but select actions close to that of an ideal type. Notice that the indices are always negative \u2014 the greater the value of the index, the more this player follows the ideal type. An index value of 0, indicates an exact ideal type. The parameter \u03b3 \u2208 (0, 1] is the response rate (or discount factor), which exponentially weights past observations. Note that these metrics generalize the idea of \u201cdistance\u201d, for example, in games where actions are physical locations (as is the running example from Section 5), \u2206(\u00b7) can represent the Euclidean distance (if using the Euclidean norm) and in less structured games, for example where any two different actions are treated the same, \u2206(\u00b7) is a boolean 0 if the arguments are equal and 1 otherwise.\nNow we describe how we use these metrics to generate features on a high\u2013level state space so that a learning\nalgorithm, whose objective is to find allies to collaborate with, can plan a sequence of high\u2013level actions that can result in a higher than the NE expected utility in adversarial repeated games."}, {"heading": "4 The TeamUP Algorithm", "text": "TeamUP is a model\u2013based RL algorithm designed for learning and planning against unknown adversaries. In essence, it is reminiscent to R-max (Brafman & Tennenholtz, 2002) (in that it uses a model\u2013based RL paradigm and implicit exploration) however it also incorporate a cleverly engineered reward shaping scheme that treats exploration as an adversarial optimization problem. In practice, TeamUP attempts to find allies with which to tacitly collaborate on a joint plan of actions that can consistently score a high utility in adversarial repeated games. Note that although we motivated this study on adversarial games, the basic properties of our algorithmic approach are general enough to be useful in the broader class of repeated general\u2013sum games.\nIn more detail, using the metrics presented in the section, TeamUP creates feature instances, oi, that take values from the set Oi = {L,F0, Fj , O} (in the next subsection we explain what they mean in detail), and the flow chart in Figure 1 shows the decision process used to generate each instance. The input parameter B is a threshold calculated as\nB = fmin\u03b4 (4)\nwhere fmin = minx,y\u2208Ai \u2212 \u2211t\u22121 k=2 \u03b3t\u22121\u2212k \u0393 \u2206(x, y) \u03c1 is a lower bound on the indices \u2014 note that fmin is an intrinsic parameter of the problem and indices fij , li all take values in the range [fmin, 0]. B is solely \u201ctweakable\u201d by 0 \u2264 \u03b4 \u2264 1 and in essence, this threshold modifies how tolerant to deviations from optimal types the planner is, with \u03b4 = 0 being completely intolerant to deviations from optimal types."}, {"heading": "4.1 States and Transitions", "text": "At a glance, the state representation of TeamUP is based on high\u2013level observations that classify each opponent (and itself) as being either stationary (L), chasing another player (Fx), or unknown (O)\n4. Using this abstraction, the algorithm learns state transitions and expected rewards.\n4Note that we always use the index 0 for self referencing the planner.\nThe planner\u2019s state is composed by a tuple of the form s = (o0, {oi}i\u2208N\\0) \u2208 S = \u00d7i\u2208NOi (which captures the high\u2013level behaviour of all players in the game) and its action space is defined by the set {L,Fi, Fj}. From Eqs. (2,3), note how the discount factor \u03b3 is what succinctly modifies how \u201chigh\u2013level\u201dstrategies are treated. When \u03b3 \u2192 0, only immediate past actions are considered, and when \u03b3 \u2192 1 long sequences of past actions come into play. Also note that by structuring the game using this state abstraction, the planner is actually learning how to play a stochastic game, where state transitions are controlled by high\u2013level strategies. TeamUP learns state\u2013action transitions by counting experience triplets of the form (s, a, s\u2032) (i.e. state, action and next state) and updating the transition function T (s, a, s\u2032) when required5. Note that these transitions are (probably) non\u2013stationary because the process is only partially controlled by TeamUP \u2014 the resulting next state also depends on the opponents\u2019 joint action. This contrasts with Rmax ; It learns the transition function T (s,a, s\u2032), which are stationary transitions. Nevertheless, we aim to learn the (possibly non\u2013stationary) transition function, because accurate transition predictions allow the planner to predict its opponents\u2019 responses and plan the best policy accordingly."}, {"heading": "4.2 Social Reward Shaping Exploration", "text": "Explorative actions in a multiagent context deserve a different treatment to that of a single agent learning problem. This is because an agent\u2019s (typically random) exploratory actions, although unintentional, could be interpreted by its adversaries as deliberate strategic actions, so can contribute to the way its opponents react. This means that exploration in a MAL context is not only a heuristic that allows an agent to learn about its environment, but that it should also be cleverly designed to correctly signal its adversaries. Furthermore, explorative actions might be costly. This fact is exacerbated in finite horizon games where a player\u2019s chance to score high is lost forever.\nSocial reward shaping (Babes et al., 2008) are external rewards presented to a RL algorithm as an addition to the actual stage game rewards. Their purpose is to encourage desirable behavior during the learning process. In more detail, they are designed to try to \u201clock\u201d non\u2013stationary processes by acting as a leader in the early learning phase. They do so by reasoning about the changes in the players\u2019 strategies as a non\u2013linear dynamic system, with probably many absorbing points (equilibria), and each with its basin of attraction. The resulting exploration heuristic that social shaping will induce (especially in the early learning phase) directly\n5Note that the first element of the state tuple is the planner\u2019s last high\u2013level action, which does not need to be measured.\n\u2018shapes\u2019 those basins of attraction, and therefore the probabilities that the system will converge to different equilibrium points. 6 Social shaping extends the well known potential\u2013based shaping framework (Ng et al., 1999) to a multiagent context. Here, the system designer provides a real\u2013valued function \u03a6 : S \u2192 R and the potential function used to modify the reward for a transition from state s to s\u2032 is F (s, s\u2032) = \u03b3\u2032\u03a6(s\u2032)\u2212\u03a6(s), with the discount factor \u03b3\u2032.\nWhen trying to extend potential based shaping to stochastic games (a generalization of Markov decision processes (MDPs) (Puterman, 1994) that allows multiple agents), the potential of a state (i.e. the state value) depends on the joint policy of all players, i.e.,\nV \u03c0(s) = r(\u03c0(s)) + \u03b3\u2032 \u2211\ns\u2032\u2208S\nT (s, \u03c0(s), s\u2032)V \u03c0(s\u2032). (5)\nTherefore, a high potential state for a certain joint policy \u03c0 might be a low potential state for some other joint policy \u03c0\u2032 and there is no hope in defining an optimal joint policy for adversarial games. If this work was concerned with self\u2013play analysis, or interested in strategies when facing fully rational opponents, a worst case solution for adversarial games would suffice (i.e. its max\u2013min policy) and the state potentials would be well defined. However, we are interested in close to optimal policies against unknown adversaries, so state potentials need not be max\u2013min based. Instead, what we define are potentials based on ideal states, that are played against ideal types of opponents.\nTo this end, we consider states, where at least one opponent is a perfect follower and is following the planner, as optimal states. For example, for a three player game, the set of optimal states is S\u0304 = {(L,F1, \u2217), (L, \u2217, F1), (F2, F1, \u2217), (F3, \u2217, F1)}, where \u2217 \u2208 O is a wild\u2013card feature. In the same way, the opposite applies to situations where the planner is left out from being followed as worst states, i.e. s \u2208 S = {(\u2217, F3, L), (\u2217, L, F2), (\u2217, F3, F2)}. Note how the optimal and worse states all express situations where tacit collusion between players exists. In adversarial games, the player(s) that are left out of the coalition will be the \u201csucker\u201d players, achieving lower than max\u2013min utilities. We\u2019ll build our planner around this fact, and therefore, its purpose will be twofold: make an offer the adversary cannot resist (find an ally), and exploit as effectively as possible the deficiencies of the sucker (left out) players in the planner\u2019s favour. Most importantly, note that states where collaborations exist (all states in the set {S\u0304, S}) are likely to be stable configurations, i.e. it is unlikely that there exists any\n6A different approach for equilibrium selection is that of (Wicks & Greenwald, 2005), however that finds a unique stable equilibrium via perturbations, but not necessarily the one wanted.\nprofitable unilateral deviation7. For those states that we know have stable configurations, we can accurately define their state value. We build on this fact to work out our potential\u2013based function. More specifically,\n\u03a6(s\u0304) = V (s\u0304) = Rmax\n1 \u2212 \u03b3\u2032\n\u03a6(s) = V (s) = Rmin\n1 \u2212 \u03b3\u2032\n\u03a6(s\u0303) = V (s\u0303) = Rmax \u2212 \u01eb\n1 \u2212 \u03b3\u2032\nwhere Rmax and Rmin are the largest and smallest rewards respectively; s\u0303 \u2208 S \\ {S\u0304, S} are all other states and are \u01eb lower than optimal, where \u01eb can be chosen arbitrarily small to be optimistic in the face of uncertainty."}, {"heading": "4.3 The Algorithm", "text": "TeamUP takes parameters \u03b3, \u03c1, \u01eb, \u03b4 and K, and learns a model M of the environment by experiencing tuples \u3008s, a, s\u2032, r\u3009 and then computes an optimal policy with respect to this current model. Its execution can be divided in two phases:\nInitialisation: Start with an initial estimate for the model parameters where all state\u2013action pairs yield a reward based on assumptions from its specific shaping function (Section 4.2) and all states lead with probability 1 to the fictitious state s0. Based on this current model, a call to VI(M)8\ncomputes a new optimal discounted policy based on its current model M .\nFor each stage game: (a) observe: at state s, a new joint action a = (a, a1, . . . , an) is observed and the new features are computed using the decision flow (Figure 1), which constructs the next state s\u2032. (b) update:\n\u2022 c(s, a, s\u2032) \u2190 c(s, a, s\u2032) + 1,\n\u2022 u(s, a) \u2190 u(s, a) + r(a) \u2022 R(s, a) \u2190 u(s,a)P s\u2032 c(s,a,s\u2032) ,\n\u2022 if \u2211\ns\u2032 c(s, a, s \u2032) = K, run VI(M) and follow\nthe new optimal discounted policy.\nAs can be seen, TeamUP is similar to R-max but differs in critical ways. A crucial difference is in the way TeamUP updates its model. It keeps counts of experienced tuples \u3008s, a, s\u2032\u3009, therefore, each time it runs VI, it recomputes the complete transition function T (s, a, s\u2032),\u2200s, s\u2032 \u2208 S,\u2200a \u2208 Ai, as opposed to Rmax that only modifies the transition for the newly\n7Under the assumption that left out players do not collude, the best these players can achieve is to optimize against their worst opponent (i.e. the planner and its ally). Any deviation from that max\u2013min strategy results in a further advantage to the colluders.\n8Where VI(M) is a call to the standard value iteration algorithm (Puterman, 1994) on the current model M .\nlabeled \u2018known\u2019 tuple but not the rest. This is a crucial difference if opponents are non\u2013stationary because state transitions depend on the adversaries\u2019 current strategies. The second difference is in the initialisation, where TeamUP uses social shaping to conduct its relaxation search, instead of using a fully optimistic relaxation search. As pointed before, a theoretical analysis of our algorithmic approach will not say much without fixing the type of opponents. We chose the Lemonade Stand Game Tournament (see next section) for driving our experimental analysis to test our initial premise \u2014 i.e. to design a planner that achieves high utilities against unknown adversaries. This tournament provided us the fairest neutral ground for comparison, beside providing us with very interesting and sometimes quite sophisticated adversary algorithms."}, {"heading": "5 The Lemonade Stand Game", "text": "The Lemonade Stand game (LSG)9 is played on an island, where, each day, three players choose a location for their lemonade stand, with the aim of being as far from their opponents as possible10. The game is played for 100 days \u2014 on each day the players choose their locations simultaneously and with no communication.\nIn the stage game, players choose from twelve possible locations, Ai = {1, . . . , 12}. The total payoff sums to 24 and is to be distributed among the players given by the distance to the nearest player clockwise plus the distance to the nearest player anti\u2013clockwise (i.e. customers are assumed to be uniformly distributed around the island). If two players are located in the same position, both receive 6 and the third receives 12. If all three are located in the same position, they all receive 8. Each player\u2019s objective is to maximise their aggregate utility over 100 rounds. As such, a player wishes to be located as far as possible from its two opponents in each stage game.\nThe stage game of the LSG has many pure NE: Figure 2 shows the NE locations for a third player, given players square and star are located as shown. For each configuration, the third player\u2019s best responses are anywhere in the larger segment between the star and square players, while the best responses that are consistent with a NE are those that are on or in\u2013between the positions directly opposite the two players, as indicated by the arrows. This is clear in 2(a). In 2(b), where the opponents play opposite one another, the third player is indifferent between all positions, while in 2(c), where its opponents play on top of one another,\n9The game was invented by Martin Zinkevich. It was designed to specifically test what should a good strategy be in repeated interactions between heterogeneous players, given that game theory\u2019s prescriptive power can only do so much in this domain.\n10We refer the reader to (Sykulski et al., 2010) for a throughout description and analysis of the game.\nthe third player is indifferent between all positions except the location of its opponents. In particular, given the analysis above, it is unlikely that standard game theoretic analysis will suffice in building a good strategy. This motivates our introduction of j\u2013considered and reciprocal best responses in Section 2. In particular, in any adversarial game played between three or more players, a coordinated team of players, \u03c7 \u2282 N can exploit the remainder \u03c7\u2032. This means that in the LSG, two players can collaborate on a sequence of actions that maximize their utilities, at the expense of the third.\nSpecifically, collaboration between two players can be achieved using reciprocal best responses, and this forms the basis of TeamUP. In the LSG, a reciprocal best response is played by i if it chooses to follow an opponent and play directly opposite it (these points are indicated by the arrows in Figure 2). In this situation, the utility of the third player is restricted to 6, which it receives in all of the 12 possible positions \u2014 hence all locations are NE, as shown in Figure 2(b). Thus, the two collaborating players share the remaining utility of 18. Similarly, player i might choose to lead its opponents by sticking in one position and wait for another player to play a reciprocal best response to this action, with the same payoffs resulting. Note that a third strategy that selects locations randomly can easily be defeated this way \u2014 with the two collaborating players receiving an expected utility of 9 and the random strategy only 6. A pair of strategies that consistently play such reciprocal best responses can therefore frequently receive high payoffs."}, {"heading": "6 Results", "text": "In this section we analyse and compare TeamUP to the other entries of the inaugural LSG Tournament, in order to demonstrate the performance of the planner\u2019s resulting strategy against other entrants of the original tournament. Testing TeamUP in this domain means our planner will face a pair of heterogeneous and unknown strategies in a more than two action adversarial game. The successful strategies will therefore be those that can consistently balance the constant sum in their favour."}, {"heading": "6.1 The LSG Tournament", "text": "First, we re\u2013ran the Tournament but included TeamUP (see Table 1). Note that we do not include results for Brown, which was placed 7th in the original tournament11. We repeat the structure of the original tournament, which is a round\u2013robin format with each triplet combination of agents simulated for several repeats. The original Tournament concluded with EA2 shown to be the winner and Pujara and RL3 awarded a statistical tie for second place. In the revised version, however, TeamUP is the overall winner, demoting EA2 into second place. The parameter values that we used for TeamUP in all reported experiments are: \u03b3 = 0.05, \u03b4 = 0.3, \u03c1 = 0.5,K = 15 and these were chosen considering the game length. Also, note that for the LSG, Rmax = 12 (for optimal states) and Rmin = 6 (for worst states). We set \u01eb = 4 such that all other states have a potential of 8, which is a player\u2019s equal share of the total utility.\nThere were several interesting strategies of note. The strategies placed 4th\u20139th, in our revised standings, all select actions similarly to an ideal type (lead or follow), and can therefore be simultaneously used as a collaborative partner (except for the random strategy used by Kuhlmann), or exploited as the sucker player. EA2 and RL3, however, look to adapt their behaviour in order to guarantee forming a collaboration with a particular opponent. These two strategies share some underlying principles with TeamUP \u2014 in particular that actions should be selected according to the general behavioural characteristics of the opponents. It is of no surprise, therefore, that these strategies performed best in this Tournament. The key difference, and contribution of TeamUP, is that it learns reactions of the opponents to the planner\u2019s actions by using sequential planning to select collaborations that are predicted to yield the highest long\u2013term utility dependent on the types of opponent faced. In contrast, EA2 for example, is indifferent between types of collaboration and selects the statistically most likely partnership to succeed myopically, based on the recent behaviour of the opponents, without planning over the rest of the game. RL3 however, selects between potential collaborations based on their historical success, but similarly to EA2, it does not do this by considering the fu-\n11We apologise to the Brown team for this \u2014 their strategy is extremely complex and requires several weeks of computation to analyse, for which we had insufficient time.\nture behaviour of both opponents \u2014 a feature which is inherent in the sequential planning performed by TeamUP.\nTo gain yet more insight, we show some case examples which outline key reasons for TeamUP yielding the highest utility of all strategies. Specifically, in Figure 3 we show a breakdown of the most frequent visited states, aggregated over several repeats of the game, for various combinations of opponents. We classify together all stage game states that belong to the optimal state (s\u0304), worst state (s) or in all other state (s\u0303) sets. We also include the average utility received by each algorithm. From the figure, notice that 85\u2013 95% of the stage games reached belong to the optimal set. Although not noticeable in Figure 3, TeamUP\u2019s collaborative partners are EA2 and RL3, which consistently follow TeamUP \u2014 this reinforces the claim that our strategy is consistent in finding an opponent to collaborate with and exploit the third player. Specifically, not only does TeamUP succeed in collaborating against different types of opponents, but the third player appears to be selecting actions near the opponent and far from our location, such that our utility is the highest of all three players.\nTo explain these results in more detail, in Figure 4 we show an example game run over its 100 stages and plot the high\u2013level strategies played by the triplet at each stage. We choose ACT\u2013R as this strategy cycles between strategies that include following one of the opponents, switching only if the utility received is below some acceptable level. In this game, TeamUP and EA2 have successfully collaborated (as indicated by the average utilities), ACT\u2013R therefore switches between states at every iteration, as its utility is consistently low. Crucially, TeamUP is identifying this change in behaviour from ACT\u2013R. This fact follows from the fast learning time of TeamUP (due to our shaping rewards and efficient use of experience by the model\u2013based RL) and that the response rate, \u03b3, is set suitably low. Notice that occasionally TeamUP then chooses to follow ACT\u2013R (TeamUP\u2019s feature F3), in an attempt by TeamUP to have both opponents play on the opposite side of the island (as EA2 is already following TeamUP).\nWhen taken together, the results in this section\ndemonstrate the ability of TeamUP to not only form collaborations with various heterogeneous opponents, but to also often receive the higher proportion of the joint utility shared with the collaborative partner."}, {"heading": "7 Conclusions and Future Work", "text": "This paper presents a novel way of thinking about multiagent heterogeneous interactions. Specifically, our analysis and results are sustained under the mathematical framework of repeated adversarial games. We propose a new way of analysing any repeated game played between the planner and one or more adversaries by abstracting important features about the interaction between players themselves. Specifically, this paper proposes that such features be defined in terms of \u201cleaders\u201d and \u201cfollowers\u201d and classifying the behaviour of our opponents under these terms. We introduced TeamUP to show how an automatic planning agent, by reasoning in a strategy space (as opposed to an action space), can generate policies that can score high utilities in adversarial games. Note that lead and follow strategies can be used in any repeated interaction game (not just adversarial) and hence our abstraction (and therefore TeamUP) should be general enough to work in these settings. However, our study is based on an adversarial setting given that this is the most challenging (due to the opposing players\u2019 goals).\nOur experimental setting analysed and compared TeamUP to other entries in the inaugural LSG Tournament. Our findings, beside presenting TeamUP as the overall winner, revealed (surprisingly) clever policies that we did not have in mind. TeamUP showed not only that it is successful in building tacit collusion policies with an ally, but that at the same time, it encourages the sucker player to play close to a planner\u2013considered BR (in the LSG this means to stay far way) \u2014 it is this extra component that significantly increased TeamUP\u2019s utility.\nThe study we present takes an important step forward in building good learning algorithms designed specifically to play against other unknown autonomous agents. Our proposed algorithm uses such an abstraction and, besides outperforming every other strategy experimentally, is (to our knowledge) the first automatic planner designed for any type of adversary. In the near future, we plan to work on the theoreti-\ncal bound when facing different classes of adversaries. Also, it would be interesting investigating if our abstraction could be generalized with the idea of clusterbased representations found in (Ficici et al., 2008)."}, {"heading": "Babes, M., Munoz de Cote, E., & Littman, M. L. (2008).", "text": "Social reward shaping in the prisoner\u2019s dilemma. Proceedings of AAMAS (pp. 1389\u20131392).\nBanerjee, B., & Peng, J. (2005). Efficient learning of multistep best response. Proceedings of AAMAS (pp. 60\u201366). The Netherlands: ACM."}, {"heading": "Brafman, R. I., & Tennenholtz, M. (2002). R-MAX - A", "text": "general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3, 213\u2013231."}, {"heading": "Ficici, S., Parkes, D., & Pfeffer, A. (2008). Learning and", "text": "solving many-player games through a cluster-based representation. Proceedings of UAI (pp. 188\u2013195). Corvallis, Oregon: AUAI Press.\nLittman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. Proceedings of the Eleventh International Conference On Machine Learning (pp. 157\u2013163).\nLittman, M. L., & Stone, P. (2002). Implicit negotiation in repeated games, 393\u2013404. LNCS. Springer."}, {"heading": "Munoz de Cote, E., & Jennings, N. R. (2010). Planning", "text": "against fictitious players in repeated normal form games. Proceedings of AAMAS.\nNg, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. Proceedings of ICML (pp. 278\u2013287).\nPuterman, M. L. (1994). Markov decision processes\u2014 discrete stochastic dynamic programming. New York, NY: John Wiley & Sons, Inc."}, {"heading": "Sykulski, A. M., Chapman, A. C., Munoz de Cote, E., &", "text": "Jennings, N. R. (2010). EA2: The winning strategy for the inaugural lemonade stand game tournament. Proceedings of the 2010 European Conference on Artificial Intelligence.\nWicks, J., & Greenwald, A. (2005). An algorithm for computing stochastically stable distributions with applications to multiagent learning in repeated games. Proceedings of UAI (pp. 623\u2013632). Arlington, Virginia: AUAI Press."}], "references": [{"title": "Social reward shaping in the prisoner\u2019s dilemma", "author": ["M. Babes", "E. Munoz de Cote", "M.L. Littman"], "venue": "Proceedings of AAMAS (pp. 1389\u20131392)", "citeRegEx": "Babes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Babes et al\\.", "year": 2008}, {"title": "Efficient learning of multistep best response", "author": ["B. Banerjee", "J. Peng"], "venue": "Proceedings of AAMAS (pp. 60\u201366)", "citeRegEx": "Banerjee and Peng,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2005}, {"title": "R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2002}, {"title": "Learning and solving many-player games through a cluster-based representation", "author": ["S. Ficici", "D. Parkes", "A. Pfeffer"], "venue": "Proceedings of UAI (pp. 188\u2013195)", "citeRegEx": "Ficici et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ficici et al\\.", "year": 2008}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the Eleventh International Conference On Machine Learning (pp. 157\u2013163)", "citeRegEx": "Littman,? \\Q1994\\E", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "Implicit negotiation in repeated games, 393\u2013404", "author": ["M.L. Littman", "P. Stone"], "venue": null, "citeRegEx": "Littman and Stone,? \\Q2002\\E", "shortCiteRegEx": "Littman and Stone", "year": 2002}, {"title": "Planning against fictitious players in repeated normal form games", "author": ["E. Munoz de Cote", "N.R. Jennings"], "venue": "Proceedings of AAMAS", "citeRegEx": "Cote and Jennings,? \\Q2010\\E", "shortCiteRegEx": "Cote and Jennings", "year": 2010}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "Proceedings of ICML (pp. 278\u2013287)", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Markov decision processes\u2014 discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "EA: The winning strategy for the inaugural lemonade stand game tournament", "author": ["A.M. Sykulski", "A.C. Chapman", "E. Munoz de Cote", "N.R. Jennings"], "venue": "Proceedings of the 2010 European Conference on Artificial Intelligence", "citeRegEx": "Sykulski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sykulski et al\\.", "year": 2010}, {"title": "An algorithm for computing stochastically stable distributions with applications to multiagent learning in repeated games", "author": ["J. Wicks", "A. Greenwald"], "venue": "Proceedings of UAI (pp. 623\u2013632)", "citeRegEx": "Wicks and Greenwald,? \\Q2005\\E", "shortCiteRegEx": "Wicks and Greenwald", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "First, instead of assuming that our planner is facing the worst opponent, as (Littman, 1994; Brafman & Tennenholtz, 2002) do, our objective is to design a planner whose resulting strategy can do better than its security (max\u2013min) equilibrium strategy.", "startOffset": 77, "endOffset": 121}, {"referenceID": 0, "context": "Social reward shaping (Babes et al., 2008) are external rewards presented to a RL algorithm as an addition to the actual stage game rewards.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "6 Social shaping extends the well known potential\u2013based shaping framework (Ng et al., 1999) to a multiagent context.", "startOffset": 74, "endOffset": 91}, {"referenceID": 8, "context": "When trying to extend potential based shaping to stochastic games (a generalization of Markov decision processes (MDPs) (Puterman, 1994) that allows multiple agents), the potential of a state (i.", "startOffset": 120, "endOffset": 136}, {"referenceID": 8, "context": "Where VI(M) is a call to the standard value iteration algorithm (Puterman, 1994) on the current model M .", "startOffset": 64, "endOffset": 80}, {"referenceID": 9, "context": "We refer the reader to (Sykulski et al., 2010) for a throughout description and analysis of the game.", "startOffset": 23, "endOffset": 46}, {"referenceID": 3, "context": "Also, it would be interesting investigating if our abstraction could be generalized with the idea of clusterbased representations found in (Ficici et al., 2008).", "startOffset": 139, "endOffset": 160}], "year": 2010, "abstractText": "Game theory\u2019s prescriptive power typically relies on full rationality and/or self\u2013play interactions. In contrast, this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents. Specifically, we introduce a new and concise representation for repeated adversarial (constant\u2013sum) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game\u2019s Nash equilibrium, when facing heterogeneous adversaries. To this end, we present TeamUP, a model\u2013based RL algorithm designed for learning and planning such an abstraction. In essence, it is somewhat similar to R-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem. In practice, it attempts to find an ally with which to tacitly collude (in more than two\u2013player games) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games. We use the inaugural Lemonade Stand Game Tournament to demonstrate the effectiveness of our approach, and find that TeamUP is the best performing agent, demoting the Tournament\u2019s actual winning strategy into second place. In our experimental analysis, we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous (and sometimes very sophisticated) adversaries.", "creator": "dvips(k) 5.97 Copyright 2008 Radical Eye Software"}}}