{"id": "1203.3525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Learning Why Things Change: The Difference-Based Causality Learner", "abstract": "in this paper, we present myself the difference - based causality learner ( dbcl ), currently an algorithm for learning a class of discrete - time dynamic models that truly represents all meaningful causation across time by means of difference equations driving change in a system. we motivate this representation with real - world mechanical systems and prove dbcl's correctness for evaluating learning structure from time series model data, suggesting an endeavour that is complicated by the existence of latent derivatives that have to be detected. we also prove that, under common assumptions for causal discovery, dbcl will identify the presence or presumed absence of feedback loops, making the model more useful for predicting the effects of manipulating local variables when the system is in equilibrium. we argue analytically and show empirically the advantages of dbcl over vector autoregression ( var ) and granger causality models as well as modified forms of bayesian and dynamic constraintbased structure discovery algorithms. finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from eeg data.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (236kb)", "http://arxiv.org/abs/1203.3525v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mark voortman", "denver dash", "marek j druzdzel"], "accepted": false, "id": "1203.3525"}, "pdf": {"name": "1203.3525.pdf", "metadata": {"source": "CRF", "title": "Learning Why Things Change: The Difference-Based Causality Learner", "authors": ["Mark Voortman", "Marek J. Druzdzel"], "emails": ["mark@voortman.name", "denver.h.dash@intel.com", "marek@sis.pitt.edu"], "sections": [{"heading": null, "text": "In this paper, we present the DifferenceBased Causality Learner (DBCL), an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system. We motivate this representation with real-world mechanical systems and prove DBCL\u2019s correctness for learning structure from time series data, an endeavour that is complicated by the existence of latent derivatives that have to be detected. We also prove that, under common assumptions for causal discovery, DBCL will identify the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and show empirically the advantages of DBCL over vector autoregression (VAR) and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms. Finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data."}, {"heading": "1 INTRODUCTION AND MOTIVATION", "text": "In the past 20 years in AI, the practice of learning causal models from data has received considerable attention [cf., Pearl and Verma, 1991, Cooper and Herskovits, 1992, Spirtes et al., 2000]. Existing methods\n\u2217Also Department of Biomedical Informatics, School of Medicine, University of Pittsburgh.\n\u2020 Also Faculty of Computer Science, Bia lystok University of Technology, Wiejska 45A, 15-351 Bia lystok, Poland.\nare based on the formalism of structural equation models (SEMs), which originated in the econometrics literature over 50 years ago [cf., Strotz and Wold, 1960], and Bayesian networks [Pearl, 1987] which started the paradigm shift of graphical models in AI and machine learning 20 years ago. These methods have predominately focused on the learning of equilibrium (static) causal structure, and have recently gained inroads into mainstream scientific research, especially in biology [cf., Sachs et al., 2005].\nDespite the success of these static methods, many realworld systems are dynamic in nature and are accurately modeled by systems of simultaneous differential equations. Temporal causality, in general, has been studied extensively in econometrics over the past four decades: Granger causality and vector autoregression (VAR) methods have become very influential [cf., Granger, 1969, Engle and Granger, 1987, Sims, 1980]. In AI, there has been work on learning Dynamic Bayesian Networks (DBNs) [Friedman et al., 1998] and modified Granger causality [Eichler and Didelez, 2007]. None of these models explicitly take into account the fact that many dynamic systems are based on differential equations. This makes their representations overly general for such systems, allowing arbitrary causal relations across time. In this paper, we show that differential equations impose strict constraints on crosstemporal causal edges, and we present a method that is capable of exploiting that fact.\nThis paper considers Difference-Based Causal Models (DBCMs), a class of discrete-time dynamic models inspired by Iwasaki and Simon [1994] that models all causation across time by means of difference equations driving change in the system. This paper presents the first method to learn DBCMs from data: the Difference-Based Causality Learner (DBCL). This algorithm treats differences as latent variables and conducts an efficient search to find them in the course of constructing a DBCM. This method exploits the fact that unknown derivatives have fixed relationships to\nknown variables and so are easier to find than latent variables in general. We prove that DBCL correctly learns DBCMs given faithfulness and a conditional independence oracle, and show empirically that it is also robust in the sense of avoiding unnecessary calculations of higher-order derivatives, thus preventing mistakes due to numerical errors. We show that compared to Granger causality and VAR models, DBCL output is much more parsimonious and informative. We also show empirically that it outperforms variants of the PC algorithm and greedy Bayesian search algorithms that have been modified to assume DBCM structure. Finally, we prove that DBCL will always identify instantaneous feedback loops when the underlying system is a DBCM, making it easier to detect when an equilibrated model will be causal. To our knowledge, no other method for causal discovery is guaranteed to identify the presence or absence of feedback loops."}, {"heading": "2 DIFFERENCE-BASED CAUSAL MODELS", "text": "Stated briefly, a DBCM is a discrete-time model, based on SEMs, with a graphical interpretation very similar to DBNs. Contemporaneous causation is allowed, i.e., like DBNs, variables can be caused by other variables in the same time slice. The defining characteristic of a DBCM is that all causation across time is due to a derivative (e.g., x\u0307) causing a change in its integral (e.g., x). This cross-temporal restriction makes DBCMs a subset of causal models as defined by Pearl [2000] and structural equation models similar to those discussed 50 years ago by Strotz and Wold [1960]. DBCM-like models were discussed by Iwasaki and Simon [1994] and Dash [2003, 2005] to analyze causality in dynamic systems, but to date no algorithm exists to learn them from data.\nAs an example, consider the set of equations describing the motion of a damped simple harmonic oscillator (SHO). A block of mass m is suspended from a spring in a viscous fluid and several different forces are acting on it, such as the forces resulting from the spring and that of gravity. The harmonic oscillator is an archetypal dynamic system, ubiquitous in nature. Although a linear system, it can form a good approximation to many nonlinear systems close to equilibrium. Furthermore, the F = ma relationship is a canonical example of contemporaneous causation: applying a force to cause a body to accelerate instantly. Thus, although this system is simple, it illustrates many important points. Causal interactions even in such a simple system are problematic when using standard representations for causality, as we will show shortly.\nLike all mechanical systems, the equations of motion\nfor the harmonic oscillator are given by Newton\u2019s 2nd law describing the acceleration a of the mass under the forces (due to the weight, due to the spring, Fx, and due to viscosity, Fv) acting on the block. These forces instantaneously determine a; furthermore, they indirectly determine the values of all integrals of a, in particular the velocity v and the position x, of the block. The longer time passes, the more influence the forces have on those integrals. Writing this continuous time system as a discrete time model, v and x are approximately determined by the difference equations: vt+1 = vt + at\u2206t and xt+1 = xt + vt\u2206t, resulting in the cross-temporal causal links in the graph of Figure 1(a) and (b). Thus, differential equation systems imply cross-temporal arcs with a regular structure. DBCMs assume that all cross-temporal arcs are of this form.\nMore formally, DBCMs are a restricted form of structural equation models (SEMs). We first review these models, and then discuss our additional constraints. We use the notation (A \u22a5 B | C ) to indicate that variable A is conditionally independent on B given a set of variables C .\nDefinition 1 (structural equation model). A SEM is a pair \u3008V ,E\u3009, where V = {V1, . . . , Vn} is a set of variables, and E = {E1, . . . , En} is a set of equations such that each Ei \u2208 E can be written in the form: Vi := fi(W i, \u03b3i) where W i \u2286 V \\ Vi is called the set of causes (or parents) of Vi, denoted by Pa(Vi), and the \u03b3i are noise terms such that (\u03b3i \u22a5 \u03b3j), i 6= j.\nThe noise terms \u03b3i are intended to represent the set of causes of each variable that are not directly accounted for in the model. Historically, SEMs use linear equations with normally distributed noise terms.\nA SEM defines a directed graph such that each variable X \u2208 V is represented by a node and there is an edge Y \u2192 X for each Y \u2208 Pa(X). In this way, SEMs can model relations between variables in a very\ngeneral way. Furthermore, SEMs can be used to represent causality in dynamic systems for a discretetime-setting by defining the set of variables to be a time-series: V = V 0 \u222a V 1 \u222a V 2 , . . ., where V t = {V t1 , . . . , V tn} denotes the set of n variables at time t. We call SEMs that partition their variables according to time indices dynamic SEMs.\nDBCMs are a restricted form of dynamic SEMs. They assume that all causation across time is due to instantaneous causation of the difference of some variables:\nDefinition 2 (Difference variable). Let V = V 0 \u222a V 1 \u222a V 2 \u222a . . . be a time-series. The n-th order difference variable \u2206nV t of variable V t \u2208 V t is defined recursively as:\n\u2206nV t = \u2206n\u22121V t+1 \u2212\u2206n\u22121V t, with \u22060V t = V t.\nIn particular: \u22061V t = V t+1 \u2212 V t, which we sometimes shorten to \u2206V t. When we invert the difference equation to give the value of V t+1 in terms of its past value and its difference, we call it the integral equation of V t+1. I.e., the integral equation of V t+1 is V t+1 = V t + \u2206V t. Integral equations are identities and so are always deterministic. The graphs of Figure 1 use the standard notation from physics such that the derivative of x is velocity (v \u2261 \u22061x) and the derivative of velocity is acceleration (a \u2261 \u22062x).\nA DBCM is a dynamic SEM in which all causation across time is due to the presence of integral equations. Because all DBCMs are based on difference equations that do not vary from time to time, we can restrict ourselves to partitioning the variables into two time slices {0, 1}, where the 0th time slice determines the initial conditions and 1st time slice determines the transitions:\nDefinition 3 (Difference-Based Causal Model). A DBCM M is a dynamic SEM S = \u3008V ,E\u3009 with V = V 0 \u222a V 1 and E = E0 \u222a E1 such that there exists a cross-temporal parent of some variable V 1i \u2208 V 1 if and only if E1i is the integral equation for variable V 1 i .\nThis definition implies that the parent set of a variable X1 that has parents in the previous time slice is Pa(X1) = {X0,\u2206X0}. If this is the case, we callX an integral variable. An integral variable X is part of a chain of causation \u2206jX0 \u2192 \u2206j\u22121X1 \u2192 . . . \u2192 Xj . We call the highest derivative (\u2206jX) of this chain the prime variable of X, which we will also denote as Prime(X). In the example of Figure 1, variables x and v are integral variables, and a is the prime variable of x and v.\nFinally, any variable that is not an integral variable and is not a prime variable is called a static variable. This term does not imply that the variable is\nnot changing from time-step to time-step, because it might have a causal ancestor that is part of an integration chain. However, we use this term to emphasize that the change is not due to a dynamic process involving these variables. In Figure 1, m, Fv and Fx are static variables.\nThe definition of DBCMs does not require that the contemporaneous structure be acyclic; however, in this paper we only consider acyclic DBCMs. It should be emphasized that this assumption does not restrict us to non-feedback systems; rather, this assumption implies that all feedback requires time to occur and thus will only occur through an integral variable. E.g., the position x of the mass in the SHO causes an instantaneous spring force Fx that results in an instantaneous acceleration a. Over time, a causes a change in x via integration. Thus we have the feedback loop: x0 \u2192 F 0x \u2192 a0 \u2192 v1 \u2192 x2. Although the instantaneous part, x0 \u2192 F 0x \u2192 a0, is acyclic, this is still a feedback system. Fb(X) is the set of instantaneous descendants of X which are also ancestors of Prime(X) (in this example, Fb(x) = {Fx}). Another interpretation is that by rejecting instantaneous loops we assume that the observation time-scale is much smaller than any time-scale of the system dynamics.\nSince the contemporaneous structure is not changing over time, the equations in E0 and E1 are partially overlapping: those that correspond to contemporaneous structure are identical in both sets, but E0 contains initial conditions for all integral variables, and E1 contains integral equations for integral variables.\nThe graph in Figure 1(b) is a compressed version of the fully unrolled DBCM. The cycle in the graph caused by the dashed links is really an acyclic structure extending across time."}, {"heading": "2.1 COMPARISON OF REPRESENTATIONS", "text": "Dynamic SEMs, like Granger causality and VAR models, allow arbitrary edges to exist across time. For many real physical systems this representation is too general.\nDBCMs, by contrast, assume that all causation works in the same way as in mechanical systems. This restriction represents a tradeoff between expressibility and tractability. On one hand, DBCMs are only able to represent mechanical systems that are first-order Markovian. On the other hand, DBCMs are in principle easier to learn because, even when the derivatives are unobserved in the data as we will assume (e.g., in the previously introduced example we do not include v and a, or any other derivative in the data set), at least we know something about these latent variables\nthat are required to make the system Markovian.\nWhen confronted with data that was generated by differential equations with some derivatives missing, the distinction between DBCL and the other approaches becomes glaring. Whereas, as we will show shortly, DBCL attempts to search for and identify the latent derivative variables, other approaches would try to marginalize them out. One might suspect that there is not much difference. For example, one might expect that a second order differential equation would simply result in a second-order Markov model when the derivatives are marginalized out. Unfortunately that is not the case, because the causation among the derivatives forms an infinite chain into the past. Thus, any approach that tries to marginalize out the derivatives must include infinite edges in the model, for example, such as those in Figure 1(c). In the harmonic oscillator system with all derivatives marginalized out, all parents of a in time-slice i of the DBCM are parents of x for all time slices j > i+ 1. Thus, the benefits of using the DBCM representation are not merely computational, but in fact, without learning the derivatives directly, the correct model does not have a finite representation."}, {"heading": "3 DBCM LEARNING", "text": "The DBCM Learning problem can be posed in the following way: Given time-series data over a set of variables V , derive a DBCM over a set of variables V \u222a V\u2206, where V\u2206 contains differences of variables that are derived from the original data. In other words, DBCL does not assume all relevant derivatives or the order of those derivatives are known. Instead, it treats these missing derivatives as latent variables and tries to discover them. We assume that, aside from these derivatives, there are no other latent confounding variables present. Note, for example, that this assumption also rules out the existence of structures of the form \u2206X \u2192 X \u2192 Y , where \u2206X and X are latent and Y is observable, because the X process forms a latent chain across time that can confound Y at different times.\nDBCL relies on the standard assumption of faithfulness [Spirtes et al., 2000]. Faithfulness is the converse of the Markov condition, and it is the critical assumption that allows structure to be uncovered from independence relations. However, when a dynamic system goes through equilibrium, by definition, faithfulness is violated. For example, if the motion of the block in the simple harmonic oscillator reaches equilibrium then, by definition, the equation a = (Fx +Fv +mg)/m becomes 0 = Fx+Fv+mg. This means that the values of the forces acting on the block are no longer correlated with the value of a, even though they are direct causes\nof a. Thus, by assuming faithfulness, we are implicitly assuming that no equilibrations have occurred."}, {"heading": "3.1 THE ALGORITHM", "text": "DBCL consists of two steps: (1) detecting prime (and integral) variables (V\u2206), and (2) learning the contemporaneous structure. The first step is achieved by calculating numerical derivatives of all variables and then deciding which ones should be prime variables. This is based on the following theorem, which exploits the fact that only prime variables can always be made independent across time by conditioning on variables in V 1 :\nTheorem 1 (detecting prime variables). Let I be the set of conditional independence relations implied by faithfulness applied to a DBCM M = \u3008V ,E\u3009 with V = V 0 \u222a V 1 and E = E0 \u222a E1 . Let \u2206jX0 denote the j-th order difference of some X0 \u2208 V 0 . Then \u2206jX0 is the prime variable of X0 if and only if it can be d-separated from itself in the future and none of the lower order differences can be d-separated, i.e.:\n1. there exists a W \u2282 V 1 such that (\u2206jX0 \u22a5 \u2206jX1 | W ) \u2208 I, and\n2. there exists no set W \u2032 \u2282 V 1 such that (\u2206kX0 \u22a5 \u2206kX1 | W \u2032) \u2208 I, for all k < j.\nOnce we have found V\u2206, the set of integral and prime variables in the model, learning contemporaneous structure over the two-time-slice model becomes a problem of learning a time-series model from causally sufficient data (i.e., there do not exist any latent common causes).\nTheorem 2 shows that we can learn the contemporaneous structure from time-series data despite the fact that data from time-to-time is not independent. This is because we know by construction that the integral variables will d-separate the time slices, so all structure between variables can be obtained by conditioning only on variables in the same time slice:\nTheorem 2 (learning contemporaneous structure). Let I be the set of conditional independence relations implied by faithfulness applied to a DBCM M = \u3008V ,E\u3009, where V = V 0 \u222a V 1 . There is an edge X1 \u2212 Y 1 if and only if:\n1. Either X1 or Y 1 is not an integral variable, and\n2. there exists no V \u20321 \u2282 V 1 \\ {X1, Y 1} such that (X1 \u22a5 Y 1 | V \u20321) \u2208 I.\nIn addition to having discovered the latent variables in the data, and the structure between non-integral\nvariables, we also know that there can be no contemporaneous edges between two integral variables, and integral variables can have only outgoing contemporaneous edges. We can thus restrict the search space of causal structures. Theorems 1 and 2 together with these constraints form the basis of the DBCL algorithm: Algorithm 1 (DBCL (sketch)). Input: a maximum difference parameter kmax > 0, a time-series dataset D over a set of variables V \u2032 = V 0\n\u2032 \u222aV 1 \u2032 . Output: a set V\u2206 of prime and integral variables, and a partially directed graph G over V = V \u2032 \u222aV\u2206.\n1. Find relevant latent derivatives (Theorem 1):\n(a) Initialize k = 0, and let V\u2206 be all differences up to kmax. (b) Let W be all variables plus their differences up to kth order that are in V\u2206. (c) For all V \u2208 V \u2032 without a prime variable, check to see if there exists a set W \u2032 \u2282 W that renders \u2206iV 0 independent of \u2206iV 1, for the i \u2264 k. If so, remove all \u2206jV 1, j > i\u2032, from V\u2206 where i\u2032 is the lowest i for which the independence occurred.\n(d) Let k = k+1. If not all prime variables have been found and k \u2264 kmax, go to Step 1b.\n2. Learn the structure (Theorem 2):\n(a) Learn the contemporaneous structure by using any correct causal discovery algorithm under causally sufficient data. Impose the following constraints: i. Forbid edges between all integral vari-\nables. ii. If X is an integral variable with an edge\nX \u2212 Y , direct the edge such that X \u2192 Y . (b) Add all cross-temporal links specified by the\nset of integral and prime variables.\nThe output of DBCL will depend on the algorithm used in Step 2. Our implementation is based on the constraint-based search PC algorithm, so the contemporaneous structure will be a partially directed graph that represents the statistical equivalence class in which the true directed graph belongs. One might argue that because there are deterministic relationships (the integral equations) in a DBCM, the faithfulness assumption is not valid. However, all deterministic relations involve exactly 3 variables, e.g., \u2206X0, X0 and X1. However two of those variables are in time slice 0, so DBCL\u2019s conditioning tests never involve all three variables at the same time. The hidden variable thus effectively adds noise to the deterministic relationship."}, {"heading": "3.2 IDENTIFICATION OF EMC VIOLATION", "text": "Given a model output from DBCL, one might be interested in performing causal inference, i.e., prediciting the effects of manipulating components of the system. This operation is complicated by the presence of equilibrations that may have occurred in the system. Dash [2003, 2005] shows that some dynamic systems do not obey Equilibration-Manipulation Commutability (EMC), i.e., the causal graph that results when an equilibrium model is manipulated can be different from the (true) graph that results when the dynamic model is manipulated and then equilibrated. Dash points out two conditions which aid in EMC identification: First, if a variable is self-regulating, meaning thatX \u2208 Pa(Prime(X)), then whenX is equilibrated, the parent set of X and the children set of X are unchanged. Thus, with respect to manipulations on X, the EMC condition is obeyed. Second, a sufficient condition for the violation of EMC exists when the set of feedback variables of some (non-self-regulating) X is nonempty in the equilibrium graph. In this case, there will always exist a manipulation that violates EMC.\nGiven a DBCM with all edges oriented, it is trivial to check these two conditions; however, since DBCL is not guaranteed to find the orientation of every edge in the DBCM structure, it is not obvious that DBCL is useful for identifying EMC violation. The following theorem shows that DBCL output will always identify self-regulating variables:\nTheorem 3. Let D be a DBCM with a variable X that has a prime variable Prime(X). The partially directed graph returned by Algorithm 1 with a perfect independence oracle will have an edge between X and Prime(X) if and only if X is self-regulating.\nIt is easy to show that a feedback set of X is empty if and only if all paths from X to Prime(X) have a collider. Again, since DBCL is not guaranteed to identify all edge orientations, not all colliders are necessarily identified. According to the faithfulness condition, DBCL will detect a correct equivalence class, and so will detect the correct adjacencies and the correct v-structures (unshielded colliders); thus Theorem 4 shows that we can always identify whether or not Fb(X) is empty:\nTheorem 4. Let G be the contemporaneous graph of a DBC model. Then for a variable X in G, Fb(X) = \u2205 if and only if for each undirected path P = \u3008P0, P1, . . . , Pn\u3009 between P0 = X and Pn = Prime(X), there exists a v-structure Pi \u2192 Pj \u2190 Pk in G such that Pi, Pj , Pk \u2208 P .\nTheorem 4 asserts that we can determine whether or\nnot there exists a directed path from X to Prime(X). In fact, this theorem does not make use of the fact that the path terminates on a prime variable, so it actually serves as an identifiability proof for all causal descendants of any integral variable."}, {"heading": "4 RESULTS", "text": "For our empirical studies, we generated data from real physical systems that are representative of the type of systems found in nature. We also applied DBCL to real EEG brain data to reveal the causal propagation of alpha waves.1\nValidation of DBCL is complicated by the fact that, as far as we know, there exist few suitable baseline methods that are even in principle able to correctly learn a DBCM when derivatives are unknown. As discussed in Section 2.1, if one tries to learn causal relations with the latent variables marginalized out, an infinite-order Markov model results (Figure 1(c)). The FCI algorithm [Spirtes et al., 2000], which attempts to take into consideration latent variables, would also result in an infinite-order Markov model because it does not try to isolate and learn the latent variables and the structure between them and the observables. The structural EM algorithm [Friedman, 1998] does try to learn explicit latent variables and structure. However, applying it naively would be unfair since DBCL uses background information about the latent variables that structural EM would not be privy to.\nThus, in order to provide a fair baseline, we chose to adapt some standard algorithms for discovery from causally sufficient data by providing them with known information about the latent derivatives. We used both the PC algorithm and a greedy Bayesian approach on a data set with all differences up to some maximum kmax = 3 calculated a priori, and we applied some heuristics to interpret the output as a DBCM. While perhaps not fully satisfying, we felt that this provided the fairest comparison to a baseline. Essentially, this allows us to assess how well Step 1 of DBCL (the main novel component) performed on learning latent differences. Once those latent differences are found, we used the PC algorithm and the Bayesian search algorithm to recover the contemporaneous structure, but without imposing the structure of a DBCM. In PC and DBCM we used a significance level of 0.01. The Bayesian approach starts with an empty network and then first greedily adds arcs using a Bayesian score with the K2 prior [Cooper and\n1All experiments were performed with SMILE, a Bayesian inference engine developed at the Decision Systems Laboratory and available at http://genie.sis.pitt.edu/.\nHerskovits, 1992], and then greedily removes arcs. For the Bayesian approach we discretized the data into five bins with approximately equal counts. It is possible to use a Bayesian approach without discretizing the data [Geiger and Heckerman, 1994], which we may explore in the future."}, {"heading": "4.1 HARMONIC OSCILLATORS", "text": "We tested DBCL on models of two physical systems, namely a SHO and the more complex coupled SHO shown in Figure 2(a). Although the SHO is a deter-\nministic system, having noise is still realistic: e.g., friction, air pressure, temperature, all of these factors are weak latent causes that add noise when determining the forces of the system. Thus all non-integral equations used Gaussian error terms that were resampled at every time interval. For both systems we selected parameters of our models in such a way that they were stable, i.e., produced measurements within reasonable bounds. We generated 100 data sets of 5,000 records for each system. We should emphasize that, as mentioned earlier, we do not include the derivatives in the data set, but only the original variables.\nWe first computed Granger causality models and VAR models for some of the simulated data for the coupled SHO just to illustrate how uninformative these models\nare when the latent derivatives are unknown. Those results are shown in Figure 2(b) and Figure 2(c), respectively. The Granger graph is more difficult to interpret than the DBCM because of the presence of multiple double-headed edges indicating latent confounders. It was noted that the sole integral variables appeared in the Granger graph with reflexive edges, which might lead to an alternative algorithm for finding prime variables. However, the Granger graph does not provide enough information to perform causal reasoning. The VAR model is also difficult to interpret, as it attempts to learn structure over time of an infinite-order Markov model. The graph of Figure 2(c) shows that variable x1 has 65 parents spread out over time-lags from 1 to 100 (binned into groups of \u2206t = 20) at significance level of 0.05. Thus while VAR models might be useful for prediction, they provide little insight into the causality of DBCMs.\nThere were four algorithms used for quantitative baselines: two based on the PC algorithm and two based on the Bayesian algorithm. We will call them PC1, PC2, B1, and B2, respectively. For all baselines the procedure for detecting the prime variables was the same: all derivatives up to a maximum order were precalculated, and prime variables were determined to be the lowest order derivative that was not connected to itself in the future in the output graph. In the second step PC1 and B1 reported the contemporaneous structure that was found during the search for prime variables. For PC2 and B2, a separate step was made wherein we created a new dataset using only the derivatives found in step 1, and relearned contemporaneous structure from this reduced dataset. The results for the SHO are shown in the following table:\n%\u2206low %\u2206hi %Edel %Eadd %Oerr PC1 0.00 0.50 39 230 26 PC2 0.00 0.50 100 100 1.0 B1 17 72 60 200 20 B2 17 72 78 120 14 DBC 0.00 0.50 0.40 1.2 0.60\nThe first two columns of the table show the percentage of derivatives too low and to high, respectively. The other three columns of the table show the percentage of edges that were deleted, added, and incorrectly oriented. For example, on average, DBCL added 1.2 extra edges for every 100 edges in the correct graph, whereas PC2 added 104 extra edges per 100 original edges.\nThe table below shows the results for the coupled SHO:\n%\u2206low %\u2206hi %Edel %Eadd %Oerr PC1 0.00 12 40 200 23 PC2 0.00 12 84 26 14 B1 0.00 93 64 170 8.5 B2 0.00 93 42 140 21 DBC 0.00 0.25 0.58 1.3 6.4\nThese results show that DBCL is effective at both\nlearning the correct difference variables and of learning contemporaneous structure of these systems. For the SHO, the PC baselines are performing as well as DBCL for discovering prime variables; however, when the network gets more complicated, there is a clear difference. Also, in all cases the second step makes a big difference between baselines and DBCM, most likely because enforcing the DBCM structure is essential. We did try other significance levels besides 0.01, but all results showed the same trend."}, {"heading": "4.2 EEG BRAIN DATA", "text": "In our second experiment, we attempted to learn a DBCM of the causal propagation of alpha waves, an 8-12 Hz signal that typically occurs in the human brain when the subject is in a waking state with eyes closed. Subjects were asked to close their eyes and then an EEG measurement was recorded. The data consisted of 10 subjects and a multivariate time series of 19 variables was recorded for each subject 2, containing over 200,000 time steps at a sampling rate of 256 Hz. Each variable corresponds to a brain region using the standard 10-20 convention for placement of the electrodes on the human scalp.\nAlpha rhythms are known to operate in a specific frequency band peaking at 10 Hz. To focus our results more on this process, we tried learning a DBCM using just the 10 Hz power signal over time. We divided the data into 0.5s segments, performed a FFT on that segment and extracted the power of the 10 Hz bin for each time slice. When learning the DBCM, we used the same significance level and kmax as before. The result for subject 10 is displayed in Figure 3. The circles represent the 19 variables that correspond to the brain regions. The top of this graph represents the front of the brain and the bottom the back. The small squares in each circle represent the derivatives that were found. The lower left is the original EEG signal, the lower right the first derivative, the top right the second derivative, and the top left the third derivative. In some regions, no derivatives were detected, so those squares have been left out.\nHere (and in typical subjects) there are only a few regions that required derivatives to explain their variation. The locations of those regions varied quite a bit from subject to subject, but there were some common patterns. Across all subjects, 16 of 20 occipital regions had at least one derivative present. This contrasts to the frontal lobes where across all subjects only 1 of 70 frontal regions had one derivative or more. When a region had at least one derivative, rarely, if ever, did\n2Data available at http://www.causality.inf.ethz.ch /repository.php?id=17\nit also have an incoming edge from some region that did not have a derivative. This suggests that the regions containing the dynamic processes were the primary drivers of alpha-wave activity. Since most of these drivers occurred in the occipital lobes, this is consistent with the widely accepted view that alpha waves originate from the visual cortex.\nThere were many regions that did not require any derivatives to explain their signals. The alpha wave activity in these regions is quickly (< 0.5s) determined given the state of the generating regions. One hypothesis to explain this is given by Go\u0300mez-Herrero et al. [2008] where they point out that conductivity of the skull can have significant impact on EEG readings by causing local signals to be a superposition of readings across the brain. Thus, if the readings of alpha waves detected in, say, the frontal region of the brain is due merely to conductivity of the skull, we would have effectively instantaneous determination of the alpha signal in those regions given the value in the regions generating the alpha waves.\nWe should note that when DBCL is applied to the raw (unfiltered) data, the resulting DBCM is much less neat: Most regions have at least one derivative present, and connectivity among regions is much higher and more difficult to interpret. This is not surprising given the massively parallel activity occuring in the brain, and it suggests that when seeking to learn causal interactions in the brain, it may be useful to partition brain signals into different frequency bands. We hope to look more fully into different bands and possibly for causal interactions among different bands."}, {"heading": "5 DISCUSSION", "text": "The main contribution of this work is to present a new representation for learning models from time-series data. While DBCMs have been discussed elsewhere in terms of analyzing causality of dynamic systems, there has not as yet been an algorithm to learn them from data. This paper presents such an algorithm and, in the process, makes DBCMs accessible to a wide range of practitioners in econometrics, biology and AI who currently rely on Granger causality, vector autoregression or graphical models to model dynamic systems. We have argued that DBCMs are particularly suited to learning systems which are based on differential equations, and have shown empirically that, for such systems when the relevant derivatives are unknown, DBCL will learn models accurately where existing approaches will fail. We have proven that under common assumptions DBCL will learn the correct equivalence class for a DBCM, and have shown that several important feautures of the underlying DBCM are identifiable from this equivalence class, such as the presence of feedback loops and the set of descendants of integral variables.\nWhile there exist mathematical dynamic systems that cannot be written as DBCMs, we believe that systems based on differential equations are ubiquitous in nature, and, therefore, will be well approximated by DBCMs. Furthermore, we have argued that there does not exist a representation that is capable of learning a finite model of these systems without first finding the correct latent derivative variables. This is because marginalizing out latent derivative variables results in an infinite-order Markov model. Thus our method can be viewed as contributing to the very hard problem of discovering latent common causes in difference equation systems.\nWe have also shown that DBCL can learn parsimonious representations for causal interactions of alpha waves in human brains that are consistent with previous research. We plan to apply this method to understanding causal pathways in the brain more broadly using a combination of EEG and MEG brain data.\nIn general, we find it surprising that after nearly 50 years of developing theories for identification of causes in econometrics, rarely, if ever, have researchers attempted to apply these theories to even the simplest dynamic physical systems. We feel that our work thus exposes a glaring gap in causal discovery and representation, and we hope that by reversing that process\u2014 applying a representation that works well on known mechanical systems to more complicated biological, econometric and AI systems\u2014we can make new inroads to causal understanding in these disciplines."}, {"heading": "Acknowledgments", "text": "Marek Druzdzel was supported in part by the National Institute of Health under grant number U01HL10106601. We would like to the thank the anonymous reviewers for their useful feedback."}, {"heading": "APPENDIX: PROOFS", "text": "Theorem 1 (detecting prime variables). Let I be the set of conditional independence relations implied by faithfulness applied to a DBCM M = \u3008V ,E\u3009 with V = V 0 \u222a V 1 and E = E0 \u222a E1 . Let \u2206jX1 denote the j-th order difference of some X1 \u2208 V 1 . Then \u2206jX1 is the prime variable of X1 if and only if it is d-separated from itself in the future and none of the lower order differences can be d-separated, i.e.:\n1. there exists a W \u2282 V 1 such that (\u2206jX0 \u22a5 \u2206jX1 | W ) \u2208 I, and\n2. there exists no set W \u2032 \u2282 V 1 such that (\u2206kX0 \u22a5 \u2206kX1 | W \u2032) \u2208 I, for all k < j.\nProof. The Markov and faithfulness conditions together imply that an edge exists between any two variables A and B in the model if and only if there exists no setW such that (A \u22a5 B |W ). In a DBCM model, an edge exists across time slices from a difference variable A0 \u2192 A1 iff A is an integral variable. Thus, the first difference variable for which there is no edge must be the prime variable, and Conditions 1 and 2 follow from the Markov and Faithfulness conditions.\nTheorem 2 (learning contemporaneous structure). Let I be the set of conditional independence relations implied by faithfulness applied to a DBCM M = \u3008V ,E\u3009, where V = V 0 \u222a V 1 . There is an edge X1 \u2212 Y 1 if and only if:\n1. Either X1 or Y 1 is not an integral variable, and\n2. there exists no V \u20321 \u2282 V 1 \\ {X1, Y 1} such that (X1 \u22a5 Y 1 | V \u20321) \u2208 I.\nProof. \u21d2 Condition 1 follows from the definition of DBCMs: an integral variable is never connected to another integral variable in a given time slice. Condition 2 follows from the Faithfulness condition. Faithfulness states that an independence relation implies a d-separation in the graph. Therefore the contrapositive states that a conection in the graph implies no conditional independence relation exists when conditioning over any subset of V \\ {X1, Y 1}.\n\u21d0 Assume there exists no conditional independence relation (X1 \u22a5 Y 1 | V \u20321) \u2208 I. We prove by contradiction that there must be an edgeX1\u2212Y 1. Assume there is no such edge. Then by the Markov condition there exists some set V \u2032 \u2282 V such that (X1 \u22a5 Y 1 | V \u2032) \u2208 I. Let V 1\u2206 be the set of integral variables in time slice 1. According to the Markov condition, conditioning on these variables renders V 0 independent of V 1 . Thus, the set {V \u2032 \u222aV 1\u2206}\u2229 {V 1 \\ {X1, Y 1}} \u2282 V 1 will also renderX1 independent from Y 1, which contradicts our original assumption. Therefore, there must be an edge X1 \u2212 Y 1.\nTheorem 3. Let D be a DBCM with a variable X that has a prime variable Prime(X). The partially directed graph returned by Algorithm 1 with a perfect independence oracle will have an edge between X and Prime(X) if and only if X is self-regulating.\nProof. Follows by the correctness of the structure discovery algorithm (all adjacencies in the graph will be recovered) together with the definition of DBCMs (no contemporaneous edge can be oriented into an integral variable).\nTheorem 4. Let G be the contemporaneous graph of a DBC model. Then for a variable X in G, Fb(X) = \u2205 if and only if for each undirected path P between X and Prime(X), there exists a v-structure Pi \u2192 Pj \u2190 Pk in G such that {Pi, Pj , Pk} \u2282 P .\nProof. \u21d2 Assume Fb(X) = \u2205. Let P be an arbitrary path P = P0 \u2192 P1 \u2212 P2 \u2212 . . .\u2212 Pn \u2212 Pn+1 with P0 = X and Pn+1 = Prime(X), and let k be the number of cross-path colliders on that path. The path must have at least one (cross-path) collider, otherwise there will be a directed path from X to Prime(X) which contradicts the fact that Fb(X) = \u2205. If at least one of the cross-path colliders is unshielded the theorem is satisfied, so we only have to consider the case of shielded colliders. Now let Pi \u2192 Pj \u2190 Pk be the first shielded cross-path collider (such that j is the smallest). We consider three cases:\n1. i < j < k: There is a directed path from X to Pi since it is the first collider. Therefore, there can be no edge from Pk to Pi, because that would\ncreate a collider in Pi (and Pj would not be the first). So there must be an edge from Pi to Pk and this implies there is a directed path from X to Pk and we recurse and look for the first shielded cross-path collider after Pk.\n2. i, k < j: Without loss of generality, there is a path X \u2192 . . .\u2192 Pi \u2192 . . .\u2192 Pk \u2192 . . .\u2192 Pj , and edges Pi \u2192 Pj , Pk \u2192 Pj , and Pi\u2212Pk. If Pi \u2190 Pk then there would be a collider in Pi which contradicts that Pj is the first one. Therefore, there must be an edge Pi \u2192 Pk and this implies there is a directed path from X to Pj and we recurse and find the first shielded cross-path collider after Pj .\n3. j < i, k: Without loss of generality, there is a path X \u2192 . . . \u2192 Pj . . . Pi . . . Pk, and edges Pj \u2190 Pi and Pj \u2190 Pk. This results in two cross-path colliders in Pj . Now there are two possibilities, (a) they are both shielded which creates a directed path from X to Pk and we recurse like before, or (b) at least one cross-path collider is unshielded and resulting in the sought after v-structure.\nSince there are only k cross-path colliders, case 1, 2, and 3a reduce the number of colliders towards zero. If there are no cross-path colliders left, there is a directed path from X to Prime(X) which contradicts our assumption that Fb(X) = \u2205. Therefore, eventually we must encounter case 3b and that proves it one way.\n\u21d0 Assume all undirected paths between X and Prime(X) have such a v-structure. We prove by contradiction that there does not exist a directed path from X to Prime(X). Assume that Fb(X) 6= \u2205 and so there must be a path P = X \u2192 P1 \u2192 . . . \u2192 Prime(X), and assume it contains m such vstructures. Now let Pi \u2192 Pj \u2190 Pk be the first vstructure (such that j is the smallest). We consider:\n1. i > j: There is a path Pj \u2192 . . . \u2192 Pi and also an edge Pi \u2192 Pj resulting in a cycle which is a contradiction.\n2. k > j: Analogous to the first case.\n3. i, k < j: Without loss of generality, assume that there is a path X \u2192 . . . \u2192 Pi \u2192 . . . \u2192 Pk \u2192 . . . \u2192 Pj \u2192, and edges Pi \u2192 Pj and Pk \u2192 Pj . So there is a directed path from X to Pj without a v-structure and we recurse to find the first vstructure after Pj .\nSince there are only m cross-path colliders, eventually there will be a path with no colliders left. Since this path contains no v-structures, it contradicts the fact that all paths must have a v-structure and, therefore, Fb(X) = \u2205."}], "references": [{"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["Gregory F. Cooper", "Edward Herskovits"], "venue": "Machine Learning,", "citeRegEx": "Cooper and Herskovits.,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits.", "year": 1992}, {"title": "Caveats for Causal Reasoning with Equilibrium Models", "author": ["Denver Dash"], "venue": "PhD thesis, Intelligent Systems Program,", "citeRegEx": "Dash.,? \\Q2003\\E", "shortCiteRegEx": "Dash.", "year": 2003}, {"title": "Restructuring dynamic causal systems in equilibrium. In AIStats, pages 81\u201388", "author": ["Denver Dash"], "venue": "Society for Artificial Intelligence and Statistics,", "citeRegEx": "Dash.,? \\Q2005\\E", "shortCiteRegEx": "Dash.", "year": 2005}, {"title": "Causal reasoning in graphical time series models", "author": ["Michael Eichler", "Vanessa Didelez"], "venue": "In UAI,", "citeRegEx": "Eichler and Didelez.,? \\Q2007\\E", "shortCiteRegEx": "Eichler and Didelez.", "year": 2007}, {"title": "Cointegration and error-correction: Representation, estimation, and testing", "author": ["Robert E. Engle", "Clive W.J. Granger"], "venue": null, "citeRegEx": "Engle and Granger.,? \\Q1987\\E", "shortCiteRegEx": "Engle and Granger.", "year": 1987}, {"title": "The Bayesian structural EM algorithm", "author": ["Nir Friedman"], "venue": "In UAI,", "citeRegEx": "Friedman.,? \\Q1998\\E", "shortCiteRegEx": "Friedman.", "year": 1998}, {"title": "Learning the structure of dynamic probabilistic networks. In UAI, pages 139\u2013147", "author": ["Nir Friedman", "Kevin Murphy", "Stuart Russell"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1998}, {"title": "Learning Gaussian networks", "author": ["D. Geiger", "D. Heckerman"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Geiger and Heckerman.,? \\Q1994\\E", "shortCiteRegEx": "Geiger and Heckerman.", "year": 1994}, {"title": "Measuring directional coupling between EEG sources", "author": ["Germ\u00e1n G\u00f2mez-Herrero", "Mercedes Atienza", "Karen Egiazarian", "Jose L. Cantero"], "venue": null, "citeRegEx": "G\u00f2mez.Herrero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "G\u00f2mez.Herrero et al\\.", "year": 2008}, {"title": "Investigating causal relations by econometric models and cross-spectral methods", "author": ["Clive W.J. Granger"], "venue": null, "citeRegEx": "Granger.,? \\Q1969\\E", "shortCiteRegEx": "Granger.", "year": 1969}, {"title": "Causality and model abstraction", "author": ["Yumi Iwasaki", "Herbert A. Simon"], "venue": "Artificial Intelligence,", "citeRegEx": "Iwasaki and Simon.,? \\Q1994\\E", "shortCiteRegEx": "Iwasaki and Simon.", "year": 1994}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q2000\\E", "shortCiteRegEx": "Pearl.", "year": 2000}, {"title": "Evidential reasoning using stochastic simulation of causal models", "author": ["Judea Pearl"], "venue": "Artifical Intelligence,", "citeRegEx": "Pearl.,? \\Q1987\\E", "shortCiteRegEx": "Pearl.", "year": 1987}], "referenceMentions": [{"referenceID": 12, "context": ", Strotz and Wold, 1960], and Bayesian networks [Pearl, 1987] which started the paradigm shift of graphical models in AI and machine learning 20 years ago.", "startOffset": 48, "endOffset": 61}, {"referenceID": 6, "context": "In AI, there has been work on learning Dynamic Bayesian Networks (DBNs) [Friedman et al., 1998] and modified Granger causality [Eichler and Didelez, 2007].", "startOffset": 72, "endOffset": 95}, {"referenceID": 3, "context": ", 1998] and modified Granger causality [Eichler and Didelez, 2007].", "startOffset": 39, "endOffset": 66}, {"referenceID": 10, "context": "This paper considers Difference-Based Causal Models (DBCMs), a class of discrete-time dynamic models inspired by Iwasaki and Simon [1994] that models all causation across time by means of difference equations driving change in the system.", "startOffset": 113, "endOffset": 138}, {"referenceID": 8, "context": "This cross-temporal restriction makes DBCMs a subset of causal models as defined by Pearl [2000] and structural equation models similar to those discussed 50 years ago by Strotz and Wold [1960].", "startOffset": 84, "endOffset": 97}, {"referenceID": 8, "context": "This cross-temporal restriction makes DBCMs a subset of causal models as defined by Pearl [2000] and structural equation models similar to those discussed 50 years ago by Strotz and Wold [1960]. DBCM-like models were discussed by Iwasaki and Simon [1994] and Dash [2003, 2005] to analyze causality in dynamic systems, but to date no algorithm exists to learn them from data.", "startOffset": 84, "endOffset": 194}, {"referenceID": 8, "context": "DBCM-like models were discussed by Iwasaki and Simon [1994] and Dash [2003, 2005] to analyze causality in dynamic systems, but to date no algorithm exists to learn them from data.", "startOffset": 35, "endOffset": 60}, {"referenceID": 5, "context": "The structural EM algorithm [Friedman, 1998] does try to learn explicit latent variables and structure.", "startOffset": 28, "endOffset": 44}, {"referenceID": 7, "context": "It is possible to use a Bayesian approach without discretizing the data [Geiger and Heckerman, 1994], which we may explore in the future.", "startOffset": 72, "endOffset": 100}, {"referenceID": 8, "context": "One hypothesis to explain this is given by G\u00f2mez-Herrero et al. [2008] where they point out that conductivity of the", "startOffset": 43, "endOffset": 71}], "year": 2010, "abstractText": "In this paper, we present the DifferenceBased Causality Learner (DBCL), an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system. We motivate this representation with real-world mechanical systems and prove DBCL\u2019s correctness for learning structure from time series data, an endeavour that is complicated by the existence of latent derivatives that have to be detected. We also prove that, under common assumptions for causal discovery, DBCL will identify the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and show empirically the advantages of DBCL over vector autoregression (VAR) and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms. Finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data.", "creator": "TeX"}}}