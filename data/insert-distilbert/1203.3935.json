{"id": "1203.3935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2012", "title": "Distributed Cooperative Q-learning for Power Allocation in Cognitive Femtocell Networks", "abstract": "in this paper, collectively we propose a sophisticated distributed reinforcement learning ( rt rl ) technique called distributed power control using q - driven learning ( dpc - generalized q ) to manage the interference feedback caused by the femtocells on macro - users in forming the downlink. the dpc - series q leverages q - learning theory to identify the intended sub - optimal pattern of power allocation, which strives to maximize femtocell capacity, while guaranteeing macrocell temporal capacity dependency level in an underlay cognitive power setting. we propose two different approaches for computing the dpc - q algorithm : individuals namely, independent, and cooperative. in the former, femtocells learn independently from each some other while in the latter, femtocells share some hidden information during learning in order to enhance their performance. simulation results show that the independent approach is best capable of mitigating the interference generated by feeding the femtocells on macro - users. moreover, modeling the results show that cooperation enhances the performance of the component femtocells in terms of speed of convergence, fairness and aggregate femtocell capacity.", "histories": [["v1", "Sun, 18 Mar 2012 10:30:54 GMT  (168kb)", "http://arxiv.org/abs/1203.3935v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["hussein saad", "amr mohamed", "tamer elbatt"], "accepted": false, "id": "1203.3935"}, "pdf": {"name": "1203.3935.pdf", "metadata": {"source": "CRF", "title": "Distributed Cooperative Q-learning for Power Allocation in Cognitive Femtocell Networks", "authors": ["Hussein Saad", "Amr Mohamed", "Tamer ElBatt"], "emails": ["hussein.saad@nileu.edu.eg", "amrmg@qu.edu.qa", "telbatt@nileuniversity.edu.eg"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 3.\n39 35\nv1 [\ncs .L\nG ]\n1 8\nM ar\n2 01\nI. INTRODUCTION\nFemtocells are considered to be a highly promising solution for the enhancement of the indoor coverage problem. However, femtocells are deployed unpredictably in the macrocell area. Thus, their interference on macro-users and other femtocells is considered to be a daunting problem [1], [2].\nSince femtocells are installed by the end user, their number and positions are random and unknown to the network operator. This makes the centralized approach for solving the interference problem very hard due to the huge overhead needed which in turn calls for a distributed interference management strategy. In the distributed scheme, each femtocell needs to learn how to interact with the dynamic environment created by the coexistence of the femto and macro cells in order to adjust its parameters (carrier frequency and transmission power) to satisfy the QoS of its own users while guaranteeing certain QoS for the macrocell users.\nBased on these observations, in this paper we focus on closed access femtocells [3] working in the same bandwidth with macrocells (cognitive femtocells). We will use a distributed machine learning technique called reinforcement learning (RL) [4] to handle the interference problem generated by the femtocells on the macrocells\u2019 users. One of the most popular RL techniques is Q-learning [5]. The reason we chose Q-learning is because it finds optimal decision policies without any prior model of the environment (in our settings, a prior model can not be achieved due to the unplanned placement of the femtocells). Moreover, Q-learning allows the agents (i.e\nthe femtocells) to take actions while they are learning (i.e no need for a centralized approach). These features make Qlearning very suitable to be applied to the distributed femtocell setting in the form of the so called multi-agent Q-learning (MAQL) [6]. In this paper, MAQL is applied in two different paradigms: independent learning (IL) and cooperative learning (CL). The former assumes that agents are unaware of the other agents\u2019 actions while the latter allows the agents to share some knowledge while they are learning to enhance their performance [6], [7].\nIn literature, RL has been used to perform power allocation in wireless networks. In [8], [9], authors used IL Q-learning to perform power allocation in order to control the aggregate interference generated by multiple secondary users on the primary receiver of a digital TV (DTV) system. In [10], authors addressed the same goal of interference control but in the context of OFDMA-based femtocells. In [11], authors used IL Q-learning in the context of cognitive femtocells and introduced a new concept called docitive femtocells. However, all the papers discussed above were interested in maintaining the QoS of the primary users and ignored the QoS of the femtocells (e.g: fairness, maximizing the femtocell capacity). Moreover, they all used the IL paradigm and did not take into consideration any cooperation between the agents (femtocells) during the learning process.\nMotivated by this, in this paper we apply Q-learning for power control in closed access cognitive femtocells network. The contributions of this paper can be summed up as follows:\n\u2022 A distributed algorithm based on IL paradigm is used to handle the interference problem. A new reward function is introduced and compared to the reward function used in literature [10], [11]. The comparison is applied in two different scenarios:\n1) Maintaining the QoS (i.e. the capacity) of the macrocell without taking into consideration the QoS of the femtocells. 2) Enhancing the capacity of the femtocells while maintaining the QoS of the macrocell.\n\u2022 Cooperation between the femtocells is introduced to enhance the aggregate capacity and fairness amongst all the femtocells, while maintaining the macrocell QoS.\nThe remaining part of this paper is organized as follows.\nSection II gives a brief background for the original single agent Q-learning. In section III, the system model is described. Section IV introduces the proposed distributed Q-learning algorithm and the Q-learning formulation for the cognitive femtocells problem. The simulation scenario and the results are discussed in section V. Finally the conclusion is given in section VI.\nII. BACKGROUND: SINGLE AGENT Q-LEARNING (SAQL)\nIn this section, the idea of Q-learning is presented by introducing the single agent case [5]. The Q-learning model can be defined by the tuple {S,A,R(s, a)} where S = {s1, s2, \u00b7 \u00b7 \u00b7 , sm} is the set of possible states the agent can occupy, A = {a1, a2, \u00b7 \u00b7 \u00b7 , al} is the set of possible actions the agent can perform and R(s, a) is the reward function that determines the reward fed back to the agent by the environment when performing action a in state s. The interaction between the agent and the environment at time t can be described as follows:\n\u2022 The agent senses the environment and observes its current state st \u2208 S. \u2022 Based on st, the agent selects action at \u2208 A. \u2022 Based on at, the environment makes a transition to a\nnew state st+1 \u2208 S and as a result achieves a reward rt = R(st, at) due to this transition. \u2022 The reward is fed back to the agent and the process is repeated.\nThe end goal of the agent is to find an optimal policy \u03c0\u2217(s), which defines the action to be selected for each state s \u2208 S in order to maximize the expected discounted reward over an infinite time:\nV \u03c0(s) = E{ \u221e \u2211\nt=0\n\u03b3tr(st, \u03c0(s))|so = s} (1)\nwhere V \u03c0(s) is the value function of state s which represents the expected discounted infinite reward when the initial state is so and 0 \u2264 \u03b3 \u2264 1 is the discount factor that determines how much effect future rewards have on the decisions at each moment. Furthermore, equation (1) can be expressed as [10]:\nV \u03c0(s) = E{r(s, \u03c0(s))} + \u03b3 \u2211\ns \u2032 \u2208S\nPs,s\u2032 (\u03c0(s))V \u03c0(s\n\u2032\n) (2)\nwhere s \u2032\nis the new state to which the environment transits after taking action a = \u03c0(s) and Ps,s\u2032 is the transition probability from state s to state s \u2032\nafter performing action a = \u03c0(s). From equation (2), the optimal value function V \u2217(s) can be written as:\nV \u2217(s) = max a\u2208A\n(E{r(s, a)} + \u03b3 \u2211\ns \u2032 \u2208S\nPs,s\u2032 (a)V \u2217(s\n\u2032\n)) (3)\nQ-learning aims at finding the optimal policy \u03c0\u2217(s) that corresponds to V \u2217(s) without having any prior knowledge about the transition probabilities Ps,s\u2032 . In order to do this,\na new value called Q-value is defined for each state-action pair, where the optimal Q-value is defined as:\nQ\u2217(s, a) = E{r(s, a)} + \u03b3 \u2211\ns \u2032 \u2208S\nPs,s\u2032 (a)max b\u2208A\nQ\u2217(s \u2032 , b) (4)\nEquation (4) states that the optimal value function can be expressed by V \u2217(s) = maxa\u2208AQ\u2217(s, a). Thus, if the optimal Q-value is known for each state-action pair, the optimal policy can be determined by \u03c0\u2217(s) = argmaxa\u2208A Q\u2217(s, a). The Qlearning algorithm finds Q\u2217(s, a) in a recursive manner using a simple update rule:\nQ(s, a) := (1\u2212\u03b1)Q(s, a) +\u03b1(r(s, a) + \u03b3max b\u2208A\nQ(s \u2032 , b)) (5)\nWhere \u03b1 is the learning rate. It was proved in [5], [12] that this update rule converges to the optimal Q-value under certain conditions. One of these conditions is that each stateaction pair must be visited infinitely often [5]. To address this notion, a random number \u01eb is introduced where at each step of the learning process the action is chosen according to a = argmaxa\u2208A Q(s, a) with probability (1\u2212 \u01eb) or randomly with probability \u01eb. Moreover, in the convergence proof, the reward function is assumed to be bounded and deterministic for each state-action pair [12]. However, in the multi-agent case, this condition is violated since the reward for each state will depend on the joint action of all agents, hence the reward function will not be deterministic from the agent point of view. Thus, in section V, the effect of choosing the reward function will be addressed using simulations.\nIII. SYSTEM MODEL\nIn this paper, a wireless network consisting of one macro cell with one single transmit and receive antenna denoted by Macro Base Station (MBS) underlaid with Nfemto femtocells each with one Femto Base Station (FBS) is considered. Um and Uf macro and femto users are located randomly inside the macro and femto cells respectively. Both MBS and FBS\u2019s transmit over the same Nsub sub-carriers where orthogonal downlink transmission is assumed in each time slot.\nThe transmission powers of the MBS and FBS i in subcarrier n are denoted by P (n)o and P (n) i respectively. Moreover, the maximum transmission powers for the MBS and FBS i are Pmmax and P f max respectively, where \u2211Nsub n=1 P (n) o \u2264 Pmmax and \u2211Nsub\nn=1 P (n) i \u2264 P f max.\nThe system performance is analyzed in terms of the capacity measured in (bits/sec/Hz). The capacity achieved by the MBS at its associated user in subcarrier n is:\nC(n)o = log2(1 + h (n) oo P (n) o\n\u2211Nfemto i=1 h (n) io P (n) i + \u03c3\n2 ) (6)\nwhere h(n)oo denotes the channel gain between the MBS and its associated user in subcarrier n; h(n)io denotes the channel gain between FBS i and the macro user in subcarrier n and\n\u03c32 is the noise power. The capacity achieved by FBS i at its associated user in subcarrier n is:\nC (n) i = log2(1 +\nh (n) ii P (n) i\n\u2211Nfemto j=1,j 6=i h (n) ji P (n) j + h (n) oi P (n) o + \u03c32\n) (7)\nwhere h(n)ii denotes the channel gain between FBS i and its associated user in subcarrier n; h(n)ji denotes the channel gain between FBS j and the femto user associated withe FBS i in subcarrier n.\nIV. DISTRIBUTED POWER CONTROL USING Q-LEARNING (DPC-Q)\nIn this section, a distributed MAQL technique called DPCQ is presented where multiple agents (i.e: femtocells) aim at learning a sub-optimal decision policy (i.e: power allocation) by repeatedly interacting with the environment. First we describe the two different paradigms in which the proposed DPC-Q algorithm is applied: Independent learning (IL) and Cooperative learning (CL). Then, the agents, states, actions and reward functions used during the simulations will be introduced.\n\u2022 Independent learning (IL): In this paradigm, each agent learns independently from other agents (i.e: ignores other agents\u2019 actions and considers other agents as part of the environment). Although, this may lead to oscillations and convergence problems, the IL paradigm showed good results in many applications [13]. The action selection strategy for agent i in the IL paradigm is the same as the SAQL case: ai = argmaxa\u2208Ai Qi(si, a), where Ai is the set of actions available for agent i (in our settings, we assume that Ai is the same for all agents 1 \u2264 i \u2264 N , where N is the number of agents). The only difference here compared to the SAQL case is that the reward function is now dependent on the joint action of all agents ~a. Thus, the update rule can be rewritten as:\nQi(si, ai) := (1\u2212\u03b1)Qi(si, ai)+\u03b1(ri(si,~a)+\u03b3max b\u2208Ai\nQi(s \u2032 i, b))\n(8) However, in the multi-agent case, acting in an independent way is not always the best approach because agents now affect each other in terms of the reward function as shown in equation (8). So, agents will need to know some information about each other (e.g: states, action, Q-tables,\u00b7 \u00b7 \u00b7 ,etc). This information is shared during the learning process in order to enhance the agents\u2019 performance. Motivated by this, we propose a mechanism where each agent shares a portion of its Q-table with all other agents 1(The Q-table is a table with |S|x|A| entries where |S| and |A| are the total number of possible states and actions respectively).\n1We assume that the shared portion of the Q-table is put in the control bits of the packets transmitted between the femtocells. The details of the exact protocol lie out of the scope of this paper.\n\u2022 Cooperative learning (CL): CL is performed as follows: Agent i shares the row of its Q-table that corresponds to its current state with all other agents j. Then agent i selects its action according to the following equation:\nai = argmax a\n( \u2211\n1\u2264j\u2264N\nQj(sj , a)) (9)\nThe main idea behind this strategy depends on two important observations: 1) the meaning of the Q-value Q(s, a), which is an estimate of the value of future rewards if the agent selects action a in state s. For example, if the reward of a femtocell is its capacity, then at a certain instant, if the agent was in state s, has two actions a1, a2 and Q(s, a1) > Q(s, a2), then choosing a1 in state s would achieve higher femtocell capacity than a2. 2) The definition of the global Q-value Q(s,a), which represents the Q-value of the whole system (i.e. if the multi-agent scenario is transformed into a single agent one using a centralized controller with global state s and global joint action a). This global Q-value can be decomposed into a linear combination of local agent-dependent Q-values: Q(s,a) = \u2211\n1\u2264j\u2264N Qj(sj , aj) [14]. Thus, if each agent j maximized its own Q-value, the global Q-value will be maximized. Based on these two observations, choosing the action based on equation 9 would maximize the global Q-value. However, the solution is still not global optimum because based on equation 9, all agents will choose the same action. For example, if there are two agents (femtocells) 1 and 2, each agent has one state s and three actions a1, a2 and a3, the reward for each agent is its capacity and the Q-values for both agents are as follows: Q1(s, a1) = 1, Q1(s, a2) = 2, Q1(s, a3) = 3, Q2(s, a1) = 4, Q2(s, a2) = 6 and Q2(s, a3) = 4.5, then in the IL paradigm, agent 1 will choose action a3, thus maximizing its capacity, while agent 2 will choose action a2, thus maximizing its capacity. However, in the CL paradigm, both agents will choose action a2 (the maximum of the summation of the Q-values is 2 + 6), thus maximizing the aggregate capacity. In terms of overhead, according to our proposed cooperation algorithm each femtocell should only share a row of its Q-table with all its neighbors. This row has a size of 1x|A|. So if the number of femtocells is Nfemto, then the total overhead needed is Nfemto.(Nfemto \u2212 1).|A| per unit time. Finally, it should be noticed that we assume that the information to be shared is put in the control bits in the packets transmitted between the femtocells. The different paradigms of the DPC-Q algorithm are summarized in algorithm 1.\nThe agents, states, actions and reward function are defined as follows:\n\u2022 Agent: FBSi, \u22001 \u2264 i \u2264 Nfemto \u2022 State: At time instant t for femtocell i in subcarrier n,\nthe state is defined as: si,nt = {I n t ,P i t} where I n t \u2208 {0, 1}\nAlgorithm 1 The proposed DPC-Q algorithm\nLet t = 0, Q0i (si, ai) = 0 for all Si \u2208 A and ai \u2208 A Initialize the starting state sti loop send Qti(s t i, :) to all other agents j receive Qtj(s t j , :) from all other agents j if rand < \u01eb then select action randomly else if leaning paradigm == IL then\nchoose action: ati = argmaxa Qi(s t i, a)\nelse choose action: ati = argmaxa( \u2211 1\u2264j\u2264N Q t j(s t j , a))\nend if end if receive reward rti observe next state st+1i update Q-table as in equation 8 sti = s t+1 i end loop\nindicates the level of interference measured at the macrouser in subcarrier n at time t:\nInt =\n{\n1, C (n) o < \u0393o 0, C (n) o \u2265 \u0393o\n(10)\nwhere \u0393o is the target capacity determining the QoS performance of the macrocell. We assume that the macrocell reports the value of Cno to all FBS through the backhaul connection. Pit determines the total power FBS i is transmitting with at time t:\nPit =\n\n \n \n0, \u2211Nsub n=0 p i,n t < (P f max \u2212A1) 1, (P fmax \u2212A2) \u2264 \u2211Nsub n=0 p i,n t \u2264 P f max 2, \u2211Nsub\nn=0 p i,n t > P f max\n(11)\nwhere P fmax, A1 and A2 are set to 15, 5 and 5 dBm respectively in the simulations and pi,nt is the power femtocell i transmitting with on subcarrier n at time t. It should be noticed that other values for A1 and A2 as well as more power levels were tried through the simulations and the performance gain was marginal. \u2022 Action: The set of actions for each agent is the set of possible powers that the FBS can use. In the simulations a range from \u221220 to 15 dBm with step of 2 dBm is used. \u2022 Reward: Two different reward functions were considered in the simulations:\n1)\nr i,n t =\n{\ne(\u2212(C (n) o \u2212\u0393 o)2), \u2211Nsub n=0 p i,n t \u2264 P f max \u22121, \u2211Nsub\nn=0 p i,n t > P f max\n(12)\nThe rationale behind this reward function is to maintain the capacity of the macrocell at the target capacity \u0393o while not exceeding the allowed P fmax. The reason for the small difference between the positive (when P fmax is not exceeded) and negative (when P f max is exceeded) rewards is due to the way the states are defined. Since the state si,nt is defined as {I n t ,P i t} and Pit is defined for certain ranges of powers not for discrete power levels, therefore, large negative numbers can not be assigned as a reward when P fmax is exceeded. For example, if Int = 1 and P i t = 6 dBm, then FBS i is in state {1, 0} in subcarrier n. If FBS i took the action ai,nt = 8 dBm, then the next state would be {1, 1} and FBS i is rewarded positively according to equation 12. Now consider the case when Int = 1 and P i t = 9 dBm, then FBS i is again in state {1, 0} in subcarrier n. If FBS i took the same action ai,nt = 8 dBm, then the next state would {1, 2} and FBS i is rewarded \u22121. So from this example, it can be shown that different rewards could be assigned for the same state-action pair. Thus, the difference between these different rewards must not be large. If the state was defined for discrete power levels (e.g: Pit = \u2211Nsub n=1 p i,n t ), then it would be possible to assign rewards with large differences because the case of having different rewards for the same stateaction pair will not occur. However, defining the states in a discrete manner would dramatically increase the number of possible states which in turn makes it harder to satisfy the condition of visiting all the state-action pairs infinitely many times. Based on this observation, in the next section we compare our reward function to the reward function used in [10]:\nr i,n t =\n{\nK \u2212 (C (n) o \u2212 \u0393o)2, \u2211Nsub n=0 p i,n t \u2264 P f max 0, \u2211Nsub\nn=0 p i,n t > P f max\n(13) where K is a constant value. We will show that our reward function improves the convergence compared to the reward function proposed in the literature. Note that the authors in [10] defined the state for discrete power levels and this proves our point.\n2)\nr i,n t =\n{\ne(\u2212(C (n) o \u2212\u0393 o)2) \u2212 e(\u2212C (n) i ), \u2211Nsub n=0 p i,n t \u2264 P f max \u22123, \u2211Nsub\nn=0 p i,n t > P f max\n(14) The reward function defined by equation (12) does not take into consideration the femtocell capacity. Thus, we define the above reward function with the rationale of maximizing the femtocell capacity while maintaining the macrocell capacity at \u0393o.\nV. PERFORMANCE EVALUATION"}, {"heading": "A. Simulation Scenario", "text": "We consider a wireless network consisting of one macrocell underlaid with Nfemto femtocells. In the simulations, Nfemto ranges from 4 to 15 femtocells. Each femtocell serves Uf = 1 femto-user which is randomly located in the femtocell coverage area. Both the macro and femto cells share the same frequency band composed of Nsub = 6 subcarriers where orthogonal downlink transmission is assumed. The channel gain between transmitter i and receiver j on subcarrier n is assumed to be path-loss dominated and is given by:\nh (n) ij = d (\u2212k) ij (15)\nWhere dij is the physical distance between transmitter i and receiver j, and k is the path loss exponent. In the simulation k = 2 is used. The distances are calculated according to the following assumptions:\n\u2022 The maximum distance between the MBS and its associated user is set to 1000 meters. \u2022 The maximum distance between the MBS and a femtouser is set to 800 meters. \u2022 The maximum distance between a FBS and its associated user is set to 80 meters. \u2022 The maximum distance between a FBS and another femtocell\u2019s user is set to 300 meters. \u2022 The maximum distance between a FBS and the macrouser is set to 800 meters.\nWe used MatLab to simulate such scenario, where in the simulations we set the noise power \u03c32 to 10\u22127, the maximum transmission power of the macrocell Pmmax to 43 dBm, the learning rate \u03b1 to 0.5, the discounted rate \u03b3 to 0.9 and the random number \u01eb to 0.1 during the first 80% of the Q-iterations [8], [9] and [10]."}, {"heading": "B. Numerical Results", "text": "We will refer to the reward functions defined by equations (12), (13) and (14) as RF1, RF2 and RF3 respectively in all the simulations. Figure (1) shows the convergence of the macrocell capacity on a certain subcarrier (C(n)o ) using RF1 and RF2 with K = 80, K = 1000 and K = 10000. It can be observed that RF1 shows better convergence behavior than RF2 with both values of K (i.e: RF1 converges to the target capacity (\u0393o = 6) more accurately). Moreover, the figure shows that the value of K affects the convergence where K = 80 is better than K = 1000 and K = 1000 is better than K = 10000, which proves our point that as the difference between the positive and negative rewards decreases, the convergence is enhanced. Note that in the simulations, the number of Qiterations was 3000 while in the figure only 300 iterations are shown (i.e: The figure is drawn with step = 10) in order to achieve better resolution.\nFigure (2) shows the total femtocell capacity using RF1, RF2 with K = 80 and RF3 in the IL paradigm. It can be observed that introducing C(n)i in RF3 increases the aggregate\nfemtocell capacity compared to RF1. However, since the IL paradigm is used here, the femtocells act in a selfish way, which may reduce the fairness (in terms of capacity) between the femtocells compared to RF1. This is shown in figure (3). Note that the fairness is evaluated using Jain\u2019s fairness index [15]: f(x1, x2, \u00b7 \u00b7 \u00b7 , xn) = ( \u2211n i=1 xi) 2\nn \u2211\nn i=1 x 2 i where 0 \u2264 f(x1, x2, \u00b7 \u00b7 \u00b7 , xn) \u2264 1 and the equality to 1 occurs when all the femtocells achieve the same capacity.\nAs for the cooperation effect, figure (4) shows the total femtocell capacity using RF1 in the IL paradigm and RF3 in both IL and CL paradigms. From the figure, it can be noticed that introducing cooperation increases the total femtocell capacity. Actually, it can be observed that at Nfemto = 11 cooperation increased the capacity by around 2.6 bits/sec/Hz. Figure (5) shows that cooperation not only increases the capacity but also enhances the fairness. Moreover, figure (6) shows that cooperation speeds up the convergence (In the CL paradigm, convergence almost started after 1800 iterations).\nVI. CONCLUSION AND FUTURE WORK\nIn this paper, a distributed Q-learning algorithm based on the multi-agent systems theory called DPC-Q is presented to perform power allocation in cognitive femtocells network. The DPC-Q algorithm is applied in two different paradigms: independent and cooperative. In the independent paradigm, two scenarios were considered. The first scenario is to control the interference generated by the femtocells on the macrouser where the results showed that the proposed algorithm is capable of maintaining the capacity of the macro-user at\na certain threshold. The second scenario is to enhance the aggregate capacity of femtocells while maintaining the QoS\nof the macro-user. Through simulations, we showed that the independent learning paradigm can be used to increase the aggregate femtocell capacity. However, due to the selfishness of the femtocells, fairness is reduced compared to the first scenario. Thus, we proposed a cooperative paradigm, in which, femtocells share a portion of their Q-tables with each other. Simulation results showed that cooperation is capable of increasing the aggregate femtocell capacity and enhancing the fairness compared to the independent paradigm, with a relatively small overhead. Future works will focus on: 1) Devise a numerical framework to study the effect of changing the Q-learning parameters (i.e: \u03b3, \u01eb and \u03b1) on the performance of the proposed algorithm 2) Design a control protocol to exchange the cooperation information amongst all femtocells 3) Other techniques for cooperation 4) Extending cooperation to coordination in which the femtocells try to coordinate their actions with each other to achieve the optimum joint action.\nACKNOWLEDGMENT\nThis work is supported by the Qatar Telecom (Qtel) Grant No.QUEX-Qtel-09/10-10.\nREFERENCES\n[1] V. Chandrasekhar, J. Andrews, and A. Gatherer, \u201cFemtocell networks: a survey,\u201d Communications Magazine, IEEE, vol. 46, no. 9, pp. 59 \u201367, September 2008. [2] A. G. S. Saunders, S. Carlaw et al., Femtocells: Opportunities and Challenges for Business and Technology. Great Britain: John Wiley and Sons Ltd, 2009. [3] P. Xia, V. Chandrasekhar, and J. G. Andrews, \u201cOpen vs closed access femtocells in the uplink,\u201d CoRR, vol. abs/1002.2964, 2010. [4] R. S. Sutton and A. G. Barto, Reinforcement learning: an introduction. Cambridge MA, MIT press, 1998. [5] C. J. C. H. Watkins and P. Dayan, \u201cTechnical note Q-learning,\u201d Journal of Machine Learning, vol. 8, pp. 279\u2013292, 1992. [6] J. R. Kok and N. Vlassis, \u201cCollaborative multiagent reinforcement learning by payoff propagation,\u201d J. Mach. Learn. Res., vol. 7, pp. 1789\u20131828, December 2006. [Online]. Available: http://portal.acm.org/citation.cfm?id=1248547.1248612 [7] M. Ahmadabadi and M. Asadpour, \u201cExpertness based cooperative Qlearning,\u201d Systems, Man, and Cybernetics, Part B: IEEE Transactions on Cybernetics, vol. 32, no. 1, pp. 66 \u201376, Feb 2002. [8] A. Galindo-Serrano and L. Giupponi, \u201cDistributed Q-learning for aggregated interference control in cognitive radio networks,\u201d Vehicular Technology, IEEE Transactions on, vol. 59, no. 4, pp. 1823 \u20131834, May 2010. [9] A. Galindo and L. Giupponi, \u201cDecentralized Q-learning for aggregated interference control in completely and partially observable cognitive radio networks,\u201d in proceedings of the Consumer Communications and Networking Conference (CCNC), 2010 7th IEEE, Jan. 2010, pp. 1 \u20136. [10] A. Galindo-Serrano and L. Giupponi, \u201cDistributed Q-learning for interference control in OFDMA-based femtocell networks,\u201d in Vehicular Technology Conference (VTC 2010-Spring), 2010 IEEE 71st, May 2010, pp. 1 \u20135. [11] A. Galindo-Serrano, L. Giupponi, and M. Dohler, \u201cCognition and docition in OFDMA-based femtocell networks,\u201d in proceeding of GLOBECOM 2010, 2010 IEEE Global Telecommunications Conference, Dec. 2010, pp. 1 \u20136. [12] F. S. Melo, \u201cConvergence of Q-learning: A simple proof,\u201d Institute Of Systems and Robotics, Tech. Rep. [13] L. Panait and S. Luke, \u201cCooperative multi-agent learning: The state of the art,\u201d Autonomous Agents and Multi-Agent Systems, vol. 11, 2005. [14] J. R. Kok, \u201cCoordination and learning in cooperative multiagent systems,\u201d Communication, 2006. [15] R. Jain, D.-M. Chiu, and W. Hawe, \u201cA quantitative measure of fairness and discrimination for resource allocation in shared computer systems,\u201d CoRR, 1998."}], "references": [{"title": "Femtocell networks: a survey,", "author": ["V. Chandrasekhar", "J. Andrews", "A. Gatherer"], "venue": "Communications Magazine, IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "S", "author": ["A.G.S. Saunders"], "venue": "Carlaw et al., Femtocells: Opportunities and Challenges for Business and Technology. Great Britain: John Wiley and Sons Ltd", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "and J", "author": ["P. Xia", "V. Chandrasekhar"], "venue": "G. Andrews, \u201cOpen vs closed access femtocells in the uplink,\u201d CoRR, vol. abs/1002.2964", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge MA, MIT press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Technical note Q-learning,", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Journal of Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "Collaborative multiagent reinforcement learning by payoff propagation,", "author": ["J.R. Kok", "N. Vlassis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Expertness based cooperative Qlearning,", "author": ["M. Ahmadabadi", "M. Asadpour"], "venue": "Systems, Man, and Cybernetics, Part B: IEEE Transactions on Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Distributed Q-learning for aggregated interference control in cognitive radio networks,", "author": ["A. Galindo-Serrano", "L. Giupponi"], "venue": "Vehicular Technology, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Decentralized Q-learning for aggregated interference control in completely and partially observable cognitive radio networks,", "author": ["A. Galindo", "L. Giupponi"], "venue": "in proceedings of the Consumer Communications and Networking Conference (CCNC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Distributed Q-learning for interference control in OFDMA-based femtocell networks,", "author": ["A. Galindo-Serrano", "L. Giupponi"], "venue": "Vehicular Technology Conference (VTC 2010-Spring),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Cognition and docition in OFDMA-based femtocell networks,\u201d in proceeding of GLOBE- COM", "author": ["A. Galindo-Serrano", "L. Giupponi", "M. Dohler"], "venue": "IEEE Global Telecommunications Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Cooperative multi-agent learning: The state of the art,", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Coordination and learning in cooperative multiagent systems,", "author": ["J.R. Kok"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "and W", "author": ["R. Jain", "D.-M. Chiu"], "venue": "Hawe, \u201cA quantitative measure of fairness and discrimination for resource allocation in shared computer systems,\u201d CoRR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Thus, their interference on macro-users and other femtocells is considered to be a daunting problem [1], [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Thus, their interference on macro-users and other femtocells is considered to be a daunting problem [1], [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "Based on these observations, in this paper we focus on closed access femtocells [3] working in the same bandwidth with macrocells (cognitive femtocells).", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "We will use a distributed machine learning technique called reinforcement learning (RL) [4] to handle the interference problem generated by the femtocells on the macrocells\u2019 users.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "One of the most popular RL techniques is Q-learning [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "These features make Qlearning very suitable to be applied to the distributed femtocell setting in the form of the so called multi-agent Q-learning (MAQL) [6].", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "The former assumes that agents are unaware of the other agents\u2019 actions while the latter allows the agents to share some knowledge while they are learning to enhance their performance [6], [7].", "startOffset": 184, "endOffset": 187}, {"referenceID": 6, "context": "The former assumes that agents are unaware of the other agents\u2019 actions while the latter allows the agents to share some knowledge while they are learning to enhance their performance [6], [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "In [8], [9], authors used IL Q-learning to perform power allocation in order to control the aggregate interference generated by multiple secondary users on the primary receiver of a digital TV (DTV) system.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [8], [9], authors used IL Q-learning to perform power allocation in order to control the aggregate interference generated by multiple secondary users on the primary receiver of a digital TV (DTV) system.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "In [10], authors addressed the same goal of interference control but in the context of OFDMA-based femtocells.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [11], authors used IL Q-learning in the context of cognitive femtocells and introduced a new concept called docitive femtocells.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "A new reward function is introduced and compared to the reward function used in literature [10], [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "A new reward function is introduced and compared to the reward function used in literature [10], [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "In this section, the idea of Q-learning is presented by introducing the single agent case [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "Furthermore, equation (1) can be expressed as [10]:", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "It was proved in [5], [12] that this update rule converges to the optimal Q-value under certain conditions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "One of these conditions is that each stateaction pair must be visited infinitely often [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "Although, this may lead to oscillations and convergence problems, the IL paradigm showed good results in many applications [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "1\u2264j\u2264N Qj(sj , aj) [14].", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "Based on this observation, in the next section we compare our reward function to the reward function used in [10]:", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Note that the authors in [10] defined the state for discrete power levels and this proves our point.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "1 during the first 80% of the Q-iterations [8], [9] and [10].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "1 during the first 80% of the Q-iterations [8], [9] and [10].", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "1 during the first 80% of the Q-iterations [8], [9] and [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "fairness index [15]: f(x1, x2, \u00b7 \u00b7 \u00b7 , xn) = ( \u2211n i=1 xi) 2 n \u2211 n i=1 x 2 i where", "startOffset": 15, "endOffset": 19}], "year": 2012, "abstractText": "In this paper, we propose a distributed reinforcement learning (RL) technique called distributed power control using Q-learning (DPC-Q) to manage the interference caused by the femtocells on macro-users in the downlink. The DPC-Q leverages Q-Learning to identify the sub-optimal pattern of power allocation, which strives to maximize femtocell capacity, while guaranteeing macrocell capacity level in an underlay cognitive setting. We propose two different approaches for the DPC-Q algorithm: namely, independent, and cooperative. In the former, femtocells learn independently from each other while in the latter, femtocells share some information during learning in order to enhance their performance. Simulation results show that the independent approach is capable of mitigating the interference generated by the femtocells on macro-users. Moreover, the results show that cooperation enhances the performance of the femtocells in terms of speed of convergence, fairness and aggregate femtocell capacity.", "creator": "LaTeX with hyperref package"}}}