{"id": "1203.4788", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Mar-2012", "title": "Very Short Literature Survey From Supervised Learning To Surrogate Modeling", "abstract": "the past century was era of linear systems. either systems ( especially industrial ones ) were simple ( quasi ) linear reasoning or linear approximations were accurate enough. in a addition, just at the ending decades of the century profusion surveys of computing devices were available, before then due to lack of computational brain resources it was not easy to evaluate available nonlinear system studies. at the moment both these two conditions changed, optimization systems are highly complex and also pervasive amount of computation strength is cheap and easy to achieve. for recent era, a completely new branch of supervised learning well known as surrogate modeling ( blended meta - modeling, surface modeling ) disciplines has been devised which aimed at further answering new needs of integrated modeling realm. this short literature survey is on to introduce surrogate modeling to whom is familiar with the concepts of supervised learning. necessity, challenges and visions of the topic are considered.", "histories": [["v1", "Wed, 21 Mar 2012 17:29:17 GMT  (108kb)", "http://arxiv.org/abs/1203.4788v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["altay brusan"], "accepted": false, "id": "1203.4788"}, "pdf": {"name": "1203.4788.pdf", "metadata": {"source": "CRF", "title": "Very Short Literature Survey From Supervised Learning To Surrogate Modeling", "authors": [], "emails": [], "sections": [{"heading": null, "text": "1. INTRODUCTION\nSince the quality of a model typically determines an upper bound on the quality of the final problem solution, modeling is often the bottleneck in the development of the whole system. The process of model construction is modeling (Learning) which is schematically got represented at Fig. 1.\nInputs, I, simultaneously enters into system and model.\nDue to many reasons, like noises or un-calibrated sensors in industrial plants; round off errors in numerical problems; fraudulent data in social time series, etc., it\u2019s usually impossible to accurately read the true outputs of system. Instead, almost always polluted data are available. The task of a modeling is to construct an analytical tool that mimics the real system as accurate as possible and robust against unwanted disturbances. To reach this, discrepancy between model output and system\u2019s, get feed backed into model. This error helps to improve models performance and the process repeat till suitable model appears.\nWe assumed samples of both inputs and outputs of the system (and model) are available or it\u2019s reachable. Because of this assumption the modeling process is also named \u201cSupervised Learning\u201d, or simply \u201cLearning\u201d process. Currently diverse forms of modeling are under study in literatures among them we may tip adaptive modeling, dynamic models and nonlinear dynamic systems are well established. In this note we will not consider all these, but instead modeling process in its general form has been investigated.\nInstead of giving a train of algorithms, structures and methods without any sense on what\u2019s happening inside them, we prefer in concept introduction in a firm manner to keep the text plain and more meaningful. Materials arrangement is as follow: section two we will have a review over modeling and supervised learning with emphasis on key theorems and challenges; Section three is about active learning and surrogate modeling; section four describes some of the ongoing directions.\n2. MODELLING AND SUPREVISED LEARNING\nFig.2 illustrates steps to construct a model for a system. As the steps goes down the needs to prior knowledge on the system is replaced by experiments, and vice versa.\nThese steps in an agile manner try to find some answers for the the following choices:\n\u2022 Choice of model inputs.\n\u2022 Choice of the excitation signals\n\u2022 Choice of model architecture\n\u2022 Choice of the model structure & complexity\n\u2022 Choice of model parameters.\n\u2022 Model Validation\n2.1 Model Inputs & Excitation Signals Step 1 is typically realized on trial and error approach with the help of prior knowledge. In mechanical processes the influence of the different variables is usually quite clear, however as the process categories become more complex, e.g. chemical, biological economic, the number of potential model inputs typically increases, and the insight into the influence of different variables decreases. To solve the problem bunch of tools has been developed, among them Principle Component Analysis (PCA) and Evolutionary Algorithms (EA) has been widely applied.\nConstructed model is strongly depend on the quality of inputs were used during the modeling (training). If the inputs were such that most hidden characteristics1 of the system get revealed, then the model has the chance to learn them, otherwise model will suffer from serious inaccuracy. Despite this clear and important relation there are a few study in the realm of supervised learning has been dedicated to the topic. As we will see surrogate modeling systematically consider this need.\n2.2 Model Architecture Deciding on the architecture of a model depend on many facts, like: Problem type, Problem dimensionality, resources restrictions, user experiments and customer acceptance. Unfortunately, there is no general method which gives the best architecture for every problem. Its only some guidelines based on reported studies are available that advise for what problem types which sort of modeling architecture may suit more.\nClassically, modeling tools with respect to their information sources are categorized in three main forms: \u201cWhite Box\u201d, \u201cGray Box\u201d and \u201cBlack Box\u201d. Table1 compares these categories with each other. Historically, Those believed in white box modeling confined\n1 System engineers call this \u201cSystem\u2019s Dynamics\u201d\nthemselves to just analytical study of system. Outputs of their works were usually in the form of heavy, expensive but precise simulators. Finite Element Analysis (FEA), Computational Fluid Dynamics (CFD) are among them. These folks are usually strongly suspicious against Black box modeling in which just a bunch of samples are all to make a model. Despite this, tendency toward black box modeling is increasing, recently. As already mentioned increasing in systems complexity and reducing in computation cost are among reasons that make this shift reasonable. In the middle, there are some whom believes in these two simultaneously. Fuzzy movement is most brilient example of this category.\n2.3 Model Structure This step is usually is much harder rather than next step, parameter estimation. It can be automatically carried out if structure optimization techniques such as orthogonal least square (OLS) for linear parameterized models or evolutionary algorithms (EA\u2019s) for non linear parameterized model are applied. An alternative to these general approaches is model specific growing and/or pruning algorithms.\nAt this step one of the most fundamental pillars of machine learning must be concidered: \u201cBias/Variance\u201d trade off. Usually, models are simpler approximate representation of a complex system. Because of this approximation, it\u2019s quite reasonable that models outputs suffer from a bias from reality. Bias reduction is possible by increasing model\u2019s complexity, e.g. putting more\nneurons into ANN, using higher order polynomials in polynomial regressions, etc. In addition models parameters are optimized with respect to limited set of data, which by itself imposes some error in models outputs. This type of error is well known as model variance .To solve this problem one way is to have large amount (infinite count) of samples of system which is not the case for real problems. Other method for variance reduction is to reduce the system\u2019s complexity. So, increasing model complexity reduces model bias but simultaneously cause in model variance and vase versa. This trade off hinder automation of this step and makes it more application dependent.\n2.4 Model Parameters This step is easiest to automate. Parameters of a model estimate with regards to different criterions, e.g. Entropy, Log-Likelihood, minmax, etc. choosing appropriate estimator, consistency of estimation, and type of estimated parameters (linear/nonlinear) are major issues. Currently amount of good optimization toolboxes are available which seems to be useful.\n2.5 Model Validation After model construction, how well it works on unseen inputs? This question is answered by Vapnic, in which he gave an upper bound for generalization error (models error for unseen inputs). He proved that this bound is dominated by problem complexity and total number of samples which used for model construction. This boundary may be count as another pillar of machine learning which must be considered when ones wants to judge generated model, \u201cno pain, no gain, give me more samples, I\u2019ll give you more accurate model!\u201d.\nModel validation is applicable in two forms. Hold out and cross validation. Hold out method is done by partitioning available samples into two categories, \u201cTraining\u201d, \u201cValidation\u201d. Models are trained on training samples and get tested by validations. This methodology suffers from a great weak point, wasting samples. On the other hand, Cross Validation as a family of algorithms which more efficiently uses data has been proposed. k-fold cross validation is a member of this family, widely used in literature. leave-one-out is the one which is used for missing data prediction models. Validation is evaluted by different measures which among them R-square, RMSE, Max are more famous.\n3. ACTIVE LEARNING AND SURROGATE MODELING\nIn modeling process it\u2019s usually assumed enough amount of samples are available or it\u2019s easy to achieve. This assumption is not true always specially for complex systems. Indeed, samples are often expensive, time consuming and rare. Many systems, today have their own simulators. The disadvantage with these simulators are that they are usually expensive and sometimes slow.\nSurrogate modeling2 (in general active learning) is a\n2 surface modeling, meta-modeling\nsupervised machine learning technique in which the learner is in control of the data used for learning. Fig.3 represents its internal loop.\nSurrogate modeling may be accomplished to construct a model which mimic locally or globally the behavior of system. Local models are usually used in conjugation with optimizers which helps a faster way to find optimum point(s). On the other hand global surrogate models give a deep insight into the internal structure of a system.\nDesigne of experiments3: This step is an indispencible parts of surrogate modeling which there is no counter part in standard form of modeling. Originated from a theory with same name, in which the objective is to planning experiments so that the random error in physical experiments has minimum influence in the approval or disaproval of a hypothesis. As example we can name grids, Latin hypercubes, Mont Carlo and uniform designs. Generally DoE algorithms are one of two types of Exploitation or Exploration. The first one tries to choose samples such that get as much information by themselves as possible, while the second type chooses samples uniformly from design space. Recently new algorithms has been proposed such that has pros of both these types, e.g. LOLA-VERONI. In general, more sample points offer more information of function, however, at a higher expense for low order functions, after reaching a certain sample size, increasing the number of sample points does not contribute to the approximation accuracy."}, {"heading": "Numerical Simulation:", "text": "After making decision on next samples which must be evaluate these points sent into system/simulator to receive outputs."}, {"heading": "Construction of surrogate modeling:", "text": "Like classical model design, for surrogate modeling there are a bunch of options available to construct a model based on, e.g. Support Vector Machines (SVM),\n3 In some references this step is also named sampling\nArtificial Neural Networks (ANN),Kriging, Polynomial regressors, spline, Guassian Process, Radial Basis functions, etc.\nAs mentioned earlier, in classical modeling tools choosing appropriate model is not directly considered inside the training loop, and it\u2019s a matter that depends more on personal experiences. Inspite of this, there are some reports on surrogate modeling which automate this task.\nSurrogate modeling is usually considered as black box modeling. In its pure form this claim may be correct but in available toolboxes, SUMO, iSight, etc. its also possible to guid the modeling toolbox by means of some constrains which experts propose. Due to this sorrogate modeling may be either considered as gray box modeling."}, {"heading": "Validation Process:", "text": "This step is almost similar to the one that already dis cussed in the orevious section.\nSurrogate modeling has been riveted many attentions all over the globe. Currently there are many free and commercial tools which its complete list and comparisons are available at [7]. These toolboxes in real world applied to many problems. These problems are usually fall into one of four types of: Model Approximation, Design Space exploration in which visual view over system is aimed, problem formulation in which metamodel helps engineer to choose appropriate optimization criterion and suitable boundaries for constraints and, finally optimization support in which surrogate model is used as a substitute of classical gradient based optimization. Successful implementation of this last usage represented a good picture of surrogate modeling, which led to Metamodelbased Design Optimization (MBDO). Befor end this line, we have to emphesise that Fig.3 is not the only iteration loop. For optimization problems some other loops are also available.\n4. FUTURE WORKS Surrogate modeling has to come with a solution to challenges like curse of dimensionality, computational complexity, noises and meta-model validation. Each of the following directions must be considered with aforementioned challenges.\n4.1 Large scale problems Current tendency in the realm of system identification is toward to linear sub-system approximations of high nonlinear complex systems. Successful structures like Local Linear Model Tree (LOLIMOT), TSK-fuzzy systems, etc. are some of the widely used. Adapting these structures for metamodeling is seems to be an indispensible need for future of surrogate modeling. In this context new methods for splitting models, intelligent sample selection and model\nconcatenation are in horizon challenges\n4.2 Domain expert knowledge A thing that separate fuzzy systems from all available is in its systematic ability to incorporate knowledge of human expert in modeling process. For surrogate modeling this knowledge incorporation is usually appeared as a set of constraints, e.g. optimization constraints in MDO. Increasing studies in this realm may result in more generic, speedy and accurate models."}, {"heading": "4.3 Uncertainity in Metamodeling", "text": "Metamodeling can be used to filter noises in computer simulation. On the other hand the uncertainity in metamodels brings new challenges in design optimization. For constrained optimization problems, if both constraint and objective functions are computation expensive and metamodeling is applied, it is found that the constrained optimum is very sensitive to the accuracy of all metamodels. Mathematically rigorous methods have to developed to qualify the uncertainity of a metamodel."}], "references": [{"title": "Pattern recognition and machine learning,", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Nonlinear System Identification\" springer", "author": ["Oliver Nelles"], "venue": "Verlag Berlin,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "al.,\u201dA novel sequencial design strategy for global surrogate modeling", "author": ["Karl Crombecq", "et"], "venue": "Proceedings of the 2009 Winter Simulation Conference", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Surrogate based analysis and optimisation", "author": ["Nestor V. Queipo", "et.al"], "venue": "Progress in Aerospace Sciences", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "al,\u201dA surrogate Modeling and adaptive sampling toolbox for computer based design", "author": ["Dirk Gorissen", "et"], "venue": "Journal of Machine Learning Research", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Review of metamodeling techniques in support of Engineering Design optimization", "author": ["G. Gary Wang"], "venue": "ASME Trans.,journal of Mechanical design,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "osfeld,\u201dData Driven modelling: some past experiences and new approaches\u201d,journal", "author": ["A. Dimitri P. solomatine"], "venue": "Hydroinformatics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}], "referenceMentions": [{"referenceID": 5, "context": "Currently there are many free and commercial tools which its complete list and comparisons are available at [7].", "startOffset": 108, "endOffset": 111}], "year": 2012, "abstractText": "The past century was era of linear systems. Either systems (especially industrial ones) were simple (quasi)linear or linear approximations were accurate enough. In addition, just at the ending decades of the century profusion of computing devices were available, before then due to lack of computational resources it was not easy to evaluate available nonlinear system studies. At the moment both these two conditions changed, systems are highly complex and also pervasive amount of computation strength is cheap and easy to achieve. For recent era, a new branch of supervised learning well known as surrogate modeling (meta-modeling, surface modeling) has been devised which aimed at answering new needs of modeling realm. This short literature survey is on to introduce surrogate modeling to whom is familiar with the concepts of supervised learning. Necessity, challenges and visions of the topic are considered.", "creator": "PScript5.dll Version 5.2.2"}}}