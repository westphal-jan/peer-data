{"id": "1204.1637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2012", "title": "Characterization of Dynamic Bayesian Network", "abstract": "in this report, we will be interested at dynamic bayesian network ( dbns ) as purely a model that tries to incorporate temporal dimension with uncertainty. we start with basics of dbn where we especially focus in inference and method learning concepts and descriptive algorithms. then we will be present different levels and methods of creating dbns as compared well as some approaches of incorporating temporal network dimension in static bayesian network.", "histories": [["v1", "Sat, 7 Apr 2012 13:55:29 GMT  (367kb)", "http://arxiv.org/abs/1204.1637v1", "9 pages, (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 2, No. 7, 2011"]], "COMMENTS": "9 pages, (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 2, No. 7, 2011", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nabil ghanmy", "mohamed ali mahjoub", "najoua essoukri ben amara"], "accepted": false, "id": "1204.1637"}, "pdf": {"name": "1204.1637.pdf", "metadata": {"source": "CRF", "title": "Characterization of Dynamic Bayesian Network The Dynamic Bayesian Network as temporal network", "authors": ["Nabil Ghanmi", "Mohamed Ali Mahjoub", "Najoua Essoukri Ben Amara"], "emails": ["Nabil.ghnamy@gmail.com", "medali.mahjoub@ipeim.rnu.tn", "Najoua.benamara@eniso.rnu.tn"], "sections": [{"heading": null, "text": "1 | P a g e\nhttp://ijacsa.thesai.org/\nDynamic Bayesian Network (DBNs) as a model that tries to incorporate temporal dimension with uncertainty. We start with basics of DBN where we especially focus in Inference and Learning concepts and algorithms. Then we will present different levels and methods of creating DBNs as well as approaches of incorporating temporal dimension in static Bayesian network. Keywords- DBN, DAG, Inference, Learning, HMM, EM Algorithm, SEM, MLE, coupled HMMs\nI. INTRODUCTION\nThe majority of events encountered in everyday life are not well described based on their occurrence at a particular point in time but rather they are described by a set of observations that can produce a comprehensive final event. Thus, time is an important dimension to take into account in reasoning and in the field of artificial intelligence in general. To add the time dimension in Bayesian networks, different approaches have been proposed. The common names used to describe this new dimension are \"temporal\" and \"dynamic \".\nII. BASICS\nA. Definition\nBayesian networks represent a set of variables in the form of nodes on a directed acyclic graph. It maps the conditional independencies of these variables. They bring us four advantages as a data modeling tool [16,17,18]\nA dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another. Each Network contains a number of random variables representing observations and hidden states of the process. We consider a dynamic Bayesian network composed of a sequence of T hidden state variables (a hidden state of a DBN is represented by a set of hidden state variables) and a sequence of T observable variables where T is time limit of the studied process.\nIn order that the specification of this network is complete, we need to define the following parameters:\n- The transition probability between states\n- The conditional probability of hidden states knowing\nobservation - The probability of the initial state\nThe first two parameters must be determined for each time . These parameters can be invariant or not over time.\nB. Inference\nThe general problem of inference for DBNs is to calculate\nwhere is the hidden variable at time and represents all observations between times and .\nThere are several interesting cases of inference, they are illustrated below. The arrow indicates : that we try to estimate. Shaded regions correspond to observations between\nand\nFiltering: this is to estimate the belief state at time knowing all the observations until this moment:\nDecoding (Viterbi): decoding problem is to determine the most likely sequence of hidden states knowing the observations up to time :\nPrediction: This is to estimate a future observation or state knowing the observations up to the current time t0\nt 0\n\nt 0\nt 0\n2 | P a g e\nhttp://ijacsa.thesai.org/\nSmoothing (offline): is to estimate a past state knowing the observations up to the current time T\nThere are several algorithms for inference in Dynamic Bayesian Networks. We can classify these algorithms according to their accuracy, in two broad classes:"}, {"heading": "1) Exact Inference:", "text": "a) Forward-Backword Algorithm:\nThe algorithm proceeds in two steps:\n1) Forward step: forward propagation of probabilities\n2) Backword step: backward propagation of probabilities\n\u2022 Forward Algorithm:\nWe consider a dynamic Bayesian network B. We wish to\ncalculate the probability ) of occurrence of the sequence of observation . This probability is:\nApplying directly this formula, the computation time is O(TNT). For this, we consider the forward variable defined by:\n(2)\nwhich expresses the probability of observing the sequence while lying in state . This variable can be computed inductively: Initialisation :\nInduction :\n(3)\nThus, we can calculate , this naturally leads us to:\n\u2022 Backward Algorithm:\nIt is also possible to perform the calculation in reverse, using the backward algorithm.\nFor this, we define the backward variable as follows:\n(5)\nThis variable expresses the conditional probability of observation from time t +1 until the last observation time T, given the values of the hidden states at time t. Its calculation follows the following procedure:\nInitialisation :\nInduction :\n(6)\nThus, we can calculate the expected probability:\n(7)\nThe complexity of this algorithm is, as the forward algorithm in O(TN 2 ).\nFrom these two factors (forward and backword) propagation probabilities, we can explore other terms that are useful for inference and learning of Dynamic Bayesian networks:\n Smoothing: this is to calculate where t <\nT. From equations (4) and (6), we can determine the following equation:\n(8)\nis called smoothing operator. We can also derive higher order smoothing equations. For example, a smoothing of the first order is defined as follows:\nThese terms may be used to easily calculate the probabilities of hidden states from the neighboring nodes.\n Prediction: this is to calculate et . We can easily determine:\nSimilarly, one can determine:\nt 0\n3 | P a g e\nhttp://ijacsa.thesai.org/\n Decoding : is to determine the sequence of hidden\nstates\nsuch as :\n(12)\nThis task can be solved using the dynamic programming algorithm of Viterbi. We can start with the following equation:\n(13)\nConsidering the topology of the DBN, we can deduce:\nWe can now easily deduce that:\n(15)\nTo find , we must introduce the argument that maximizes as follows:\n(16)\nAnd we have:\n(17)\nNote that if we want to use the Viterbi algorithm to decode the sequence of hidden states, we must have a complete observation . If the number of observations is not sufficient, a less optimal solution known as the truncated Viterbi algorithm can be used.\nb) Junction Tree Algorithm:\nThe Junction Tree Algorithm [1] is an algorithm similar to the Baum-Welch algorithm used in HMM. It involves transforming the original network into a new structure called junction tree and apply a type inference algorithm used for static Bayesian networks. This tree is obtained by following these steps:\n Moralization: connecting parents and eliminating directions.  Triangularization: selectively adding arcs to the graph morale (not to have cycles of order 4 or more).\n Junction Tree : is obtained from the triangulated graph by connecting the cliques such that all cliques\non the path between two cliques X and Y contain X \u2229 Y"}, {"heading": "2) Approximate inference:", "text": "When the dimension of Bayesian networks increases, the computing time is increasingly important. When the conditional probability tables are derived from data (learning), these tables are not accurate. In this case it is not worth wasting time by making exact inference on probabilities not precise, hence the use of approximate inference methods. Among the approximate inference methods that often work well in practice, we give:\na) Variational methods\nThe simplest example is the approximation by the average (mean-field approximation) [2], which exploits the law of large numbers to approximate large sums of random variables by their average. The approximation by the average product of a lower probability. There are other, more stringent, resulting in a lower and upper.\nb) Monte Carlo\nThe easiest Monte Carlo Method [3] is the Importance Sampling (IS) that produces a large number of samples x from the unconditional distribution of hidden variables) then we give weight to samples based on their likelihood (where y is the observation). This forms the basis of Particulate Filter which is simply the Importance Sampling adapted to a dynamic Bayesian network.\nc) Loopy Belief propagation\nWe apply the algorithm of Pearl [4] to the original graph even if it contains loops. In theory, one runs the risk of double counting certain words but it was shown that in some cases (for example, a single loop), events are counted twice and thus cancel out fairly between them to give the correct answer .\nC. Leaning:\nLearning is to estimate the probability tables and conditional distributions CPTs CPDS. This task is based on the EM algorithm (Expectation Maximization) algorithm or the GEM (General Expectation Maximization) for DBNs.\nLet M be a Dynamic Bayesian network with parameter ,\nlearning aims to determine such the posterior probability of the observations is maximal, then either:\n(18)\n4 | P a g e\nhttp://ijacsa.thesai.org/\nEM Algorithm: This algorithm includes:\n an evaluation step of expectation (E), which calculates the expectation of the likelihood taking\ninto account the recent observed variables,\n a maximization step (M), where an estimated maximum likelihood parameters by maximizing the\nlikelihood found in step E.\nD. Pruning\nThis task is based on the possibility of change in time, of RBD\u2019s structure. This is usually omitted for its complexity. Pruning the network consists in perform one of the following operations:\n- Delete one or more states of a given node - Remove a connection between two nodes\n- Remove one or more network nodes\nThis can be exact (lossless) or approximate\nIII. DIFFERENT LEVELS OF CREATING DBN\nTo describe a dynamic Bayesian network, we must specify its topology (the graph structure) as well as all the tables of conditional probability distribution. You can learn them both (the graph and distributions) from experimental data. However, it is more difficult to learn a structure to learn its parameters.\nIt is possible that some nodes are hidden during the experiments (values that we can\u2019t observe), or missing data. In this case, learning becomes more complicated settings. From these considerations, there are 4 possible cases of learning [5]:\nThe DBN\u2019s structure is known, it remains to estimate the parameters of the network using the method of maximum likelihood estimation. We look for parameters describing the model assumptions that maximize the likelihood of observations Y:\n(19)\nIn general, it instead uses the log likelihood (log-likelihood)\n(20)\nB. Known structure /Partial observability\nWhen certain variables are not observable, the likelihood surface becomes multimodal and we must use iterative methods such as EM or gradient increasing to find local maxima of the function ML / MAP. The principle of the EM algorithm is to associate a problem with an incomplete data problem for which complete data for a simple solution exists for the maximum likelihood estimate. This procedure needs to use an inference algorithm to compute the parameters for each node. These algorithms are explained in section II.3\nC. Unknown structure / Full observability\nThere are several techniques for learning DBN\nstructure from observed data. These techniques help to create the network structure by adding or deleting edges between any two nodes or reversing the direction of an existing arc. These changes must be made in order to maintain and acyclic directed graph.\nTo accomplish the task of structural learning, we need [6]:\n- an algorithm to find the different possible structures - a metric for comparing the possible structures to each other\nThe structure learning algorithms can be classified into two\nbroad categories.\n The first class of algorithms using heuristic search methods to construct the graph and evaluates it using\nscores (scoring methods). This procedure is repeated until the improvement between two consecutive models is not significant.  The second class of algorithms to create the network structure by analyzing the independence relations\nbetween nodes. These independence relations are measured using several types of tests of conditional independence (eg mutual information between two nodes can be considered as a criterion for conditional independence)\nAccording to Cheng et al. [7], when comparing the two types of algorithms, we can conclude that the first class of algorithms are faster than the second if the network is densely connected, but can\u2019t find the best solution for most models corresponding to real processes of the heuristic nature of these algorithms. The second class of algorithms can produce, under some assumptions, an optimal or near optimal solution especially when the data are not numerous.\n5 | P a g e\nhttp://ijacsa.thesai.org/\nD. Unknown Structure /Partial Observability\nThe EM algorithm is developed to make learning network settings, so it must be adjusted to perform structural learning from incomplete data. The structural EM (SEM) is one of the most popular techniques that are developed for this purpose. SEM has the same E-step EM algorithm for completing the data using observations and the current structure of the network. The M-step involves two parts: In the first, it recalculates as already explained, the maximum likelihood to determine the parameters. In the second part, it uses these parameters to evaluate any other candidate structure similar to the current structure."}, {"heading": "IV. DIFFERENT APPROACHES FOR INCORPORATING TIME IN BAYESIAN NETWORK", "text": "Dynamic Bayesian Networks (DBN) are an extension of Bayesian networks that represent the temporal or spatial evolution of random variables. There are several models for incorporating time into network representation. These models can be classified into three broad categories:\n Models that use static BNs and formal grammars to represent the temporal dimension (temporal\nprobabilistic networks (TPNs)\n Models that use a mixture of several probabilistic frameworks  Models that use temporal nodes in the static BNs to represent temporal dependencies\nThe first two models are developed for specific objectives and\nhave a very limited use. We will therefore focus on the third\nmodel.\nA. Probabilistic Temporal Networks (PTN)"}, {"heading": "1) Definition", "text": "A probabilistic temporal network (PTN) is defined as a model, representing the time information while fully embracing probabilistic semantics. In a PTN, the nodes of the graph are the temporal aggregates and the arcs are causal and / or temporal relations\nThis type of network uses grammatical rules to express temporal dependencies in the structure of Bayesian networks: The conservation of the structure of static Bayesian networks allow reuse of powerful techniques for inference of BNs this specific type of networks. Grammar introduce temporal relations between events"}, {"heading": "2) Temporal Reasoning", "text": "In PTN, temporal reasoning is based on interval algebra [8] which was introduced by James F. Allen in 1983. This is a calculation that defines the possible relationships between\ntime intervals and provides a table of composition that can be used as a basis for reasoning on descriptions of temporal events.\nThe 13 following basic relations capture possible relationships between two intervals are illustrated in the following table:\nDynamic Bayesian networks generalize hidden Markov models (HMM) and linear dynamical systems (LDS) by representing the hidden states (and seen) as state variables, with complex interdependencies. The HMMs are used to represent discrete states and the LDS are used to represent states (variables) continuous. The combination of these two structures to create a mixed-state DBN. This type of model was introduced and applied to the recognition of human gestures [9]\nC. Pure Probabilistic DBN\nIn this section we consider a DBN as a graph whose nodes represent states and arcs represent conditional dependencies (causal) between states of a band as well as temporal dependencies between the states belonging to two consecutive time slices\n6 | P a g e\nhttp://ijacsa.thesai.org/"}, {"heading": "1) Extension of BNs toward DBNs", "text": "A static Bayesian network can be extended in many ways to represent temporal process. These extensions can be classified into five categories:\n1- Adding the history of a node to explicitly express the temporal aspect in the Bayesian network. 2- Select from a library of pre-developed Bayesian network, the RB appropriate to the current state. 3- Changing dynamics of the network structure. 4- Repeat the traditional network for each time step by\nintroducing Bayesian networks to represent events.\n5- Repeat the classical Bayesian network by adding arcs representing the time dependencies of a time\nslice to another.\nThe networks of the first category may be regarded as mere static BNs which is added an additional node to represent past information in time. The second class of Bayesian networks is the object of an idea that has been used in early work on DBNs by Singhal et al. They use a bunch of BNs (COBRA) developed locally and every time the system selects the Bayesian network corresponding to its beliefs about the current state of real objects studied, hence the dynamic (temporal) aspect of this class We will describe in more detail, the three other types of extension of BNs to the DBN:"}, {"heading": "2) Dynamic change in the structure of DBNs", "text": "Changes in the structure of a DBN can be:\n- Changing network settings (values of the table of conditional probabilities CPT) of a time slice to another - Adding or deleting new nodes and / or arcs to the structure of BN. The structural changes of a DBN (addition or deletion of edges or nodes) is a complex problem and can not be generalized easily. In the following, we are interested in changing parameters (CPT) system. In [10] Zweig and Russell presented a model that uses decomposition techniques to represent dynamic situations real. These dynamic processes can be decomposed into several sequences. Such decomposition can be used in speech recognition or recognition of manuscripts. They found it more suitable to represent dynamic processes (temporal) creating a RB (a subnet) at each stage in the evolution of the process to model the whole process by a single BN. Each sub-network must be learned from observations at the appropriate time."}, {"heading": "3) DBNs for events representation", "text": "In such networks, we use information obtained from states belonging to two consecutive time slots in order to deduce the events that took place between the two points of time. Structure of these networks is presented in the following diagram:\nIn such networks, there are three types of nodes W, O and E\nwhich represent respectively:\n- The random variables (corresponding to states of the real process) - Observations - Events\nDynamic Bayesian networks are a repetition of the traditional network in which we add a causal link (representing the time dependencies) of a time step to another. The network topology is the same for the different time slots. Arcs and probabilities that form these models have the same interpretations as for a statistical system based on a classic SNL. Thus, a DBN is completely defined by giving the couple , with:\n- is a BN which defines the a priori probability (initial state)\n- is the temporal Bayesian Network with two time slices (2TBN: two-slice Temporal Bayes Net) which\ndefines using a directed acyclic graph DAG as follows:\nWhere\nrepresent le i th node at time t and\nis the parent of in the graph.\nV. FROM HMMS TO DBNS\nThe main difference between the HMM and dynamic Bayesian networks is that in an RBD the hidden states are represented as distributed by a set of random variables\nW(t) W(t-1) W(t+1)\n7 | P a g e\nhttp://ijacsa.thesai.org/\n. Thus, in an HMM, the state space consists\nof a single random variable .\nFigure 2 shows a HMM represented in its graphical form with a dynamic Bayesian network. The gray nodes represent observed nodes and nodes in white are the hidden nodes.\nFigure 2. HMM represented as an instance of RBD unrolled over three time steps\nIn Figure 2, following the notations used in the literature on the HMM, the node represents the initial state with . The transition matrix is represented by tables of transition probabilities between nodes\net with\nFinally, the observation matrix is found in probability\ntables between nodes tX and tY with\nThus, the specification of an HMM as a dynamic Bayesian network is simply given by the probability tables for , et . Assuming that the model is invariant over time (transition matrix and observation are fixed over time) then the givening of , et are sufficient.\nThe major advantage of dynamic Bayesian networks over HMM is that it is very easy to create alternatives to HMM simply giving another structure more or less complex DBN. The formalism and algorithms remain the same [11]. If you change the tables of probability distributions (discrete tables) by continuous distributions (eg Gaussian), then it also becomes possible to represent models based on Kalman filters [12]. It is also possible to combine these different models simply by hanging them DBN and thus provide more complex models.\nVI. REPRESENTATION OF HMMS AS DBNS\nThere are several variants of HMM, which were proposed in response to specific classes of problems and to overcome limitations in traditional HMMs.\nIn this section, we will present the variations of the most widely used HMM (shown in Figure 3). The coupled HMM (Figure 3 (a)) is probably the most natural structure, which can process, simultaneously and with good efficiency, multiple data streams from the same observations. For this, we will briefly introduce other representations and will be presented in more detail the coupled HMMs in the next section. Figure 4 (b) is a specific coupling of HMM described in [13] as an event coupled HMM. The motivation for this representation is to model a class of loosely coupled time series where only the occurrence of events are coupled in time. The representation of events coupled with HMMs is obviously limited by its narrow structure and this structure is for a very specific class of applications.\nInput / Output HMM (Figure 4 (c)) [15] represents a promising alternative to the use of a hidden Markov model. This variant allows to map an input sequence and output sequence. The main difference with traditional HMMs is indeed the first is the distribution of the output sequence when the second shows the distribution conditional of the output sequence given an input sequence . This allows for spot monitoring or recognition of sequences online. The inputs and outputs can be discrete or continuous, scalar or vector.\nY2\nX2\nY3\nX3\nY1\nX1\n8 | P a g e\nhttp://ijacsa.thesai.org/\nThe factorial HMM (Figure 3 (d)) [14] is a model used to represent systems in which the hidden states are made from a set of decoupled dynamical systems and with only one observation available.\nVII. COUPLED HMM"}, {"heading": "A. Definition", "text": "In a coupled HMM, each hidden variable (state) is connected to his own observation. It is also connected to its two nearest neighbors in the time slice with the exception of the following variables belonging to chains border, each with a single nearest neighbor (see Figure 4)."}, {"heading": "B. Parameters of coupled HMMs", "text": "Let a CHMM model formed with L coupled HMMs. This model is fully described giving the following parameters:\n initial probabilities:\nis the number of states (hidden nodes) of the chain\n Transition probabilities:\n Probability of observation"}, {"heading": "C. Extension of the forward-backword algorithm for CHMM:", "text": "In the same way as for traditional HMMs, we use the Forwardbackword algorithm to calculate in the case of L coupled HMMs. There, in this case each observation is a vector ). Since L HMMs are coupled, the variables forward and backword should be defined jointly for all HMMs. In other words, we define the forward variable as follows:\nAnd the backword variable as follows:\nTherefore, we can calculate inductively the two variables as\nfollows:\nAnd the likelihood function can be calculated as\nfollows:"}, {"heading": "D. EM algorithm for learning parameters of CHMM", "text": "As in the case of traditional HMMs, the two basic steps of the\nEM algorithm as described in [3] are:\n Estimation step:\nGiven the observations O, the parameters to estimate\nand the objective function , we construct an\nauxiliary function:\nthat represents the expectation of the objective function\nof all sequences of possible states, given the observations\nO and the current parameters estimated\n9 | P a g e\nhttp://ijacsa.thesai.org/\n Maximization step:\nIn the exact EM algorithm, the role of this step is to\nestimate the new parameters as follows:\nVIII. MOTIVATION OF USING CHMM\nAccording to its definition, a coupled HMM can be viewed as a collection of HMMs, one for each data stream, where the discrete nodes at time t for each HMM are conditioned by the discrete nodes at time t -1 of all HMMs linked.\nThe characteristics of handwritten characters can perform a joint analysis of the image of a character according to the two preferred directions: vertical (\"column\") and horizontal (\"lines\"). So we will use the coupled HMM (CHMM) to couple two HMMs: one can handle comments on the columns and the second will be used to handle comments on the lines\nIX. REFERENCES\n[1] ZWEIG G., Speech Recognition with Dynamic Bayesian\nnetworks,Bayesian networks, PhD thesis, University of California,\nBerkeley, 1998. [2] N. Lawrence. Variational Infererence in Probabilistic Models. PhD thesis,\nUniversity of Cambridge, U.K., 2000.\n[3] W. Gilks, S. Richardson, and D. Spiegelhalter. Markov chain monte carlo in practice. Interdisciplinary Statistics. Chapman & Hall, 1996. [4] J. Pearl. Probabilistic reasoning in intelligent systems: Networks of plausible inference. Morgan Kaufmann, second edition in 1991, 1988. [5] K. Muphy, S.Mian: Modelling Gene Expression Data using Dynamic Bayesian Natwork, Technical Report, Computer Science Division,\nUniversity of California, Berkeley, CA, 1999 [6] T.A. Stephenson, An Introduction to Bayesian Network Theory and\nUsage, IDIAP Research Report 00-03, 2002\n[7] J. Cheng, D. A. Bell, W. Liu, Learnining Belief Networks from Data: An Information Theory Based Approach, Proccedings of the sixth ACm Internatioanal Conference on Information and knowledge Management [8] J.F.Allen, Maintaining Knowledge about Temporal Intevals, Comm. ACM, vol.26, 1983 [9] V. Pavlovic, B. Frey, and T.S. Huang, .Time series classi_cation using mixed-state ynamic Bayesian networks,. in Proc. IEEE CVPR, 1999 [10] G.Zweig, S. Russel, Compositional Modelling With DPNs, Report N. UCB/CSD-97-970, 1997 [11] Smyth P., Heckerman D. and Jordan M. Probabilistic Independence Networks for Hidden Markov Probability Models. Technical Report MSR-TR-96-03, Microsoft Research, 1996 [12] K. Murphy, Dynamic Bayesian Networks: Representation, Inference and\nLeaning, PhD thesis Univesity of Califonia, Berkely, 2002. [13] T. T. Kristjansson, B. J. Frey, and T. Huang. Event-coupled hidden\nMarkov models. In Proc. IEEE Int. Conf. On Multimedia and Exposition,\nvolume 1, pages 385\u2013388, 2000 [14] E. Sanchez, R\u00e9seaux Bay\u00e9siens Dynamiques pour la V\u00e9rification du\nLocuteur, PhD thesis Telecom Paris, Mai 2005\n[15] Y. Bengio and P. Frasconi. Input-Output HMMs for sequence processing. IEEE Trans. Neural Networks, 7(5):1231\u20131249, September 1996. [16] K. Jayech , MA Mahjoub \u201cNew approach using Bayesian Network to improve content based image classification systems\u201d, IJCSI International Journal of Computer Science Issues, Vol. 7, Issue 6, November 2010.\n[17] K. Jayech , MA Mahjoub \u201c\"Clustering and Bayesian Network to improve content based image classification systems\", International Journal of Advanced Computer Science and Applications- a Special Issue on Image Processing and Analysis, May 2011.\n[18] MA Mahjoub, K. kalti \u201cSoftware Comparison dealing with Bayesian networks\u201d Lecture Notes in Computer Science (LNCS), 2011, Volume 6677, Advances in Neural Netwodks \u2013 ISNN 2011 Pages 168-177."}], "references": [{"title": "Speech Recognition with Dynamic Bayesian networks,Bayesian networks", "author": ["G. ZWEIG"], "venue": "PhD thesis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Variational Infererence in Probabilistic Models", "author": ["N. Lawrence"], "venue": "PhD thesis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Markov chain monte carlo in practice", "author": ["W. Gilks", "S. Richardson", "D. Spiegelhalter"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "S.Mian: Modelling Gene Expression Data using Dynamic Bayesian Natwork", "author": ["K. Muphy"], "venue": "Technical Report, Computer Science Division,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "An Introduction to Bayesian Network Theory and Usage", "author": ["T.A. Stephenson"], "venue": "IDIAP Research Report 00-03,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Time series classi_cation using mixed-state ynamic Bayesian networks", "author": ["V. Pavlovic", "B. Frey", "T.S. Huang"], "venue": "in Proc. IEEE CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Compositional Modelling With DPNs", "author": ["G.Zweig", "S. Russel"], "venue": "Report N. UCB/CSD-97-970,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Probabilistic Independence Networks for Hidden Markov Probability Models", "author": ["P. Smyth", "D. Heckerman", "M. Jordan"], "venue": "Technical Report MSR-TR-96-03, Microsoft Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Dynamic Bayesian Networks: Representation, Inference and Leaning", "author": ["K. Murphy"], "venue": "PhD thesis Univesity of Califonia, Berkely,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Event-coupled hidden Markov models", "author": ["T.T. Kristjansson", "B.J. Frey", "T. Huang"], "venue": "In Proc. IEEE Int. Conf. On Multimedia and Exposition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "R\u00e9seaux Bay\u00e9siens Dynamiques pour la V\u00e9rification du Locuteur", "author": ["E. Sanchez"], "venue": "PhD thesis Telecom Paris,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Input-Output HMMs for sequence processing", "author": ["Y. Bengio", "P. Frasconi"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "New approach using Bayesian Network to improve content based image classification systems", "author": ["K. Jayech", "MA Mahjoub"], "venue": "IJCSI International Journal of Computer Science Issues,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Clustering and Bayesian Network to improve content based image classification systems", "author": ["K. Jayech", "MA Mahjoub"], "venue": "International Journal of Advanced Computer Science and Applications- a Special Issue on Image Processing and Analysis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "kalti \u201cSoftware Comparison dealing with Bayesian networks", "author": ["K. MA Mahjoub"], "venue": "Lecture Notes in Computer Science (LNCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "They bring us four advantages as a data modeling tool [16,17,18] A dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another.", "startOffset": 54, "endOffset": 64}, {"referenceID": 14, "context": "They bring us four advantages as a data modeling tool [16,17,18] A dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another.", "startOffset": 54, "endOffset": 64}, {"referenceID": 15, "context": "They bring us four advantages as a data modeling tool [16,17,18] A dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another.", "startOffset": 54, "endOffset": 64}, {"referenceID": 0, "context": "The Junction Tree Algorithm [1] is an algorithm similar to the Baum-Welch algorithm used in HMM.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "a) Variational methods The simplest example is the approximation by the average (mean-field approximation) [2], which exploits the law of large numbers to approximate large sums of random variables by their average.", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "b) Monte Carlo The easiest Monte Carlo Method [3] is the Importance Sampling (IS) that produces a large number of samples x from the unconditional distribution of hidden variables) then we give weight to samples based on their likelihood (where y is the observation).", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "c) Loopy Belief propagation We apply the algorithm of Pearl [4] to the original graph even if it contains loops.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "From these considerations, there are 4 possible cases of learning [5]:", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "To accomplish the task of structural learning, we need [6]: - an algorithm to find the different possible structures - a metric for comparing the possible structures to each other The structure learning algorithms can be classified into two broad categories.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "This type of model was introduced and applied to the recognition of human gestures [9]", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "In [10] Zweig and Russell presented a model that uses decomposition techniques to represent dynamic situations real.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The formalism and algorithms remain the same [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "If you change the tables of probability distributions (discrete tables) by continuous distributions (eg Gaussian), then it also becomes possible to represent models based on Kalman filters [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "Figure 4 (b) is a specific coupling of HMM described in [13] as an event coupled HMM.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Input / Output HMM (Figure 4 (c)) [15] represents a promising alternative to the use of a hidden Markov model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "org/ The factorial HMM (Figure 3 (d)) [14] is a model used to represent systems in which the hidden states are made from a set of decoupled dynamical systems and with only one observation available.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "As in the case of traditional HMMs, the two basic steps of the EM algorithm as described in [3] are: \uf0b7 Estimation step: Given the observations O, the parameters to estimate and the objective function , we construct an auxiliary function:", "startOffset": 92, "endOffset": 95}], "year": 2011, "abstractText": "In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a model that tries to incorporate temporal dimension with uncertainty. We start with basics of DBN where we especially focus in Inference and Learning concepts and algorithms. Then we will present different levels and methods of creating DBNs as well as approaches of incorporating temporal dimension in static Bayesian network. KeywordsDBN, DAG, Inference, Learning, HMM, EM Algorithm, SEM, MLE, coupled HMMs", "creator": "Conv2pdf.com"}}}