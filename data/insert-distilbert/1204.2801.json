{"id": "1204.2801", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Seeing Unseeability to See the Unseeable", "abstract": "we present a framework that allows an observer to determine occluded portions of a structure by finding of the maximum - likelihood estimate of those occluded portions consistent with visible image evidence and a consistency model. doing this requires only determining which portions of the structure are occluded items in the first take place. since each monitoring process relies on the last other, we determine a solution to both problems in tandem. we extend our design framework to determine confidence of balancing one'existing s assessment of which specified portions of an observed structure are occluded, and the estimate of that occluded structure, by determining the sensitivity of one'earlier s assessment to potential new observations. we further extend our framework to helping determine a robotic swarm action whose repetitive execution would allow a new scientific observation that would maximally increase one's confidence.", "histories": [["v1", "Thu, 12 Apr 2012 18:18:35 GMT  (6205kb,D)", "http://arxiv.org/abs/1204.2801v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.RO", "authors": ["siddharth narayanaswamy", "rei barbu", "jeffrey mark siskind"], "accepted": false, "id": "1204.2801"}, "pdf": {"name": "1204.2801.pdf", "metadata": {"source": "CRF", "title": "Seeing Unseeability to See the Unseeable", "authors": ["Siddharth Narayanaswamy", "Andrei Barbu", "Jeffrey Mark Siskind"], "emails": [], "sections": [{"heading": null, "text": "We present a framework that allows an observer to determine occluded portions of a structure by finding the maximum-likelihood estimate of those occluded portions consistent with visible image evidence and a consistency model. Doing this requires determining which portions of the structure are occluded in the first place. Since each process relies on the other, we determine a solution to both problems in tandem. We extend our framework to determine confidence of one\u2019s assessment of which portions of an observed structure are occluded, and the estimate of that occluded structure, by determining the sensitivity of one\u2019s assessment to potential new observations. We further extend our framework to determine a robotic action whose execution would allow a new observation that would maximally increase one\u2019s confidence."}, {"heading": "1 Introduction", "text": "[T]here are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns\u2013the ones we don\u2019t know we don\u2019t know.\nDonald Rumsfeld (12 February 2002)\nAdditional images and videos as well as all code and datasets are available at http://engineering.purdue. edu/\u02dcqobi/arxiv2012c.\nPeople exhibit the uncanny ability to see the unseeable. The colloquial exhortation You have eyes in the back of your head! expresses the assessment that someone is making correct judgements as if they could see what is behind them, but obviously cannot. People regularly determine the properties of occluded portions of objects from observations of visible portions of those objects using general world knowledge about the consistency of object properties. Psychologists have demonstrated that the world knowledge that can influence perception can be high level, abstract, and symbolic, and not just related to low-level image properties such as object class, shape, color, motion, and texture. For example, Freyd et al. (1988) showed that physical forces, such as gravity, and whether such forces are in equilibrium, due to support and attachment relations, influences visual perception of object location in adults. Baillargeon (1986, 1987) showed that knowledge of substantiality, the fact that solid objects cannot interpenetrate, influences visual object perception in young infants. Streri and Spelke (1988) showed that knowledge about object rigidity influences both visual and haptic perception of those objects in young infants. Moreover, such influence is cross modal: observable haptic perception influences visual perception of unobservable properties and observable visual perception influences haptic perception of unobservable properties. Wynn (1998) showed that material properties of objects, such as whether they are countable or mass substances, along with abstract properties, such as the number of countable objects and the quantity of mass substances, and how they are transferred between containers, influences visual perception in young infants. Similar results exist for many physical properties such as relative mass, momentum, etc. These results demonstrate that people can easily integrate information from multiple sources together with world knowledge to see the unseeable.\nar X\niv :1\n20 4.\n28 01\nv1 [\ncs .C\nV ]\n1 2\nPeople so regularly invoke the ability to see the unseeable that we often don\u2019t realize that we do so. If you observe a person entering the front door of a house and later see them appear from behind the house without seeing them exit a door, you easily see the unseeable and conclude that there must be an unseen door to the house. But if one later opens the garage door or the curtain covering a large livingroom bay window in the front of the house so that you see through the house and see the back door you no longer need to invoke the ability to see the unseeable. A more subtle question then arises: when must you invoke the ability to see the unseeable? In other words how can you see unseeability, the inability to see? This question becomes particularly thorny since, as we will see, it can involve a chickenand-egg problem: seeing the unseen can require seeing the unseeability of the unseen and seeing the unseeability of the unseen can require seeing the unseen.\nThe ability to see unseeability and to see the unseeable can further dramatically influence human behavior. We regularly and unconsciously move our heads and use our hands to open containers to render seeable what was previously unseeable. To realize that we need to do so in the first place, we must first see the unseeability of what we can\u2019t see. Then we must determine how to best use our collective perceptual, motor, and reasoning affordances to remedy the perceptual deficiency.\nWe present a general computational framework for seeing unseeability to see the unseeable. We formulate and evaluate a particular instantiation of this general framework in the context of a restricted domain, namely LINCOLN LOGS, a children\u2019s assembly toy where one constructs assemblies from a small inventory of component logs. The two relevant aspects of this domain that facilitate its use for investigating our general computational framework are (a) that LINCOLN LOG assemblies suffer from massive occlusion and (b) that a simple but rich expression of world knowledge, in the form of constraints on valid assemblies, can mitigate the effects of such occlusion.\nWhile LINCOLN LOGS are a children\u2019s toy, this domain is far from a toy when it comes to computer vision. The task of structure estimation, determining, from an image, the correct combination of component logs used to construct an assembly and how they are combined, is well beyond state-of-the-art methods in computer vision. We have not found any general-purpose image segmentation methods that can determine the image boundaries of the visible component logs (see Fig. 1a). Moreover, the uniform matte color and texture of the logs, together with the fact that logs are placed in close proximity and the fact that the majority of any structure is in self shadow means every edge-detection method that we have tried fails to find the boundaries between adjacent logs (see Fig. 1b). This is even before one considers occlusion, which only makes matters worse.\nNot only is the computer-vision problem for this domain immensely difficult, the computational problem is rich as well. We present methods for seeing the unseeable (in section 2) and seeing unseeability (in section 3) based on precise computation of the maximum-likelihood structure estimate conditioned on world knowledge that marginalizes over image evidence. We further present (in section 4) a rational basis for determining confidence in one\u2019s structure estimate despite unseeability based on precise computation of the amount of evidence needed to override a uniform prior on the unseeable. And we finally present (in section 5) an active-vision decision-making process for determining rational behavior in the presence of unseeability based on precise computation of which of several available perception-enhancing actions one should take to maximally improve the confidence in one\u2019s structure estimate. We offer experimental evaluation of each of these methods in section 7, compare against related work in section 8, and conclude with a discussion of potential extensions in section 9."}, {"heading": "2 Structure Estimation", "text": "In previous work Siddharth et al. (2011) presented an approach for using a visual language model for improving recognition accuracy on compositional visual structures in a generative visual domain, over the raw recognition rate of the part detectors\u2014by analogy to the way speech recognizers use a human language model to improve recognition accuracy on utterances in a generative linguistic domain, over the raw recognition rate of the phoneme detectors. In this approach, a complex object is constructed out of a collection of parts taken from a small part inventory. A language model, in the form of a stochastic constraintsatisfaction problem (CSP) (Lauriere, 1978), characterizes the constrained way object parts can combine to yield a whole object and significantly improves the recognition rate of the whole structure over the infinitesimally small recognition rate that would result from unconstrained application of the unreliable part detectors. Unlike the speechrecognition domain, where (except for coarticulation) there is acoustic evidence for all phonemes, in the visual domain\nthere may be components with no image evidence due to occlusion. A novel aspect of applying a language model in the visual domain instead of the linguistic domain is that the language model can additionally help in recovering occluded information.\nThis approach was demonstrated in the domain of LINCOLN LOGS, a children\u2019s assembly toy with a small part inventory, namely, 1-notch, 2-notch, and 3-notch logs, whose CAD models are provided to the system. In this domain, a grammatical LINCOLN LOG structure contains logs that are parallel to the work surface and organized on alternating layers oriented in orthogonal directions. Logs on each layer are mutually parallel with even spacing, thereby imposing a symbolic grid on the LINCOLN LOG assembly. The symbolic grid positions q = (i, j, k) refer to points along log medial axes at notch centers. One can determine the camera-relative pose of this symbolic grid without any knowledge of the assembly structure by fitting the pose to the two predominant directions of image edges that result from the projection of the logs to the image plane.\nEach grid position may be either unoccupied, denoted by \u2205, or occupied with the nth notch, counting from zero, of a log with m notches, denoted by (m,n). Estimating the structure of an assembly reduces to determining the occupancy at each grid position, one of the seven possibilities: \u2205, (1, 0), (2, 0), (2, 1), (3, 0), (3, 1), and (3, 2). This is done by constructing a discrete random variable Zq for each grid position q that ranges over these seven possibilities, mutually constraining these random variables together with other random variables that characterize the image evidence for the component logs using the language model, and finding a maximum-likelihood consistent estimate to the random variables Zq .\nSeveral forms of image evidence are considered for the component logs. LINCOLN LOGS, being cylindrical parts, generate two predominant image (log) features: ellipses that result from the perspective projection of circular log ends and line segments that result from the perspective projection of cylindrical walls. The former are referred to as log ends and the latter as log segments. Log ends can po-\ntentially appear only at a fixed distance on either side of a grid position. Boolean random variables Z+q and Z \u2212 q are constructed to encode the presence or absence of a log end at such positions. There are two kinds of log segments: those corresponding to the portion of a log between two notches and those corresponding to the portions of a log end that extend in front of or behind the two most extreme notches. Given this, three Boolean random variables Zuq , Zvq , and Z w q are constructed for each grid position q that encode the presence or absence of such log segments for the bottoms of logs, i.e. log segments between a grid position and the adjacent grid position below. Fig. 2 depicts the log ends and log segments that correspond to a given grid position as described above.\nA stochastic CSP encodes the validity of an assembly. Image evidence imposes priors on the random variables Z+q , Z\u2212q , Z u q , Z v q , and Z w q and structure estimation is performed by finding a maximum-likelihood solution to this stochastic CSP. When formulating the constraints, the adjacent grid position below q is referred to as b(q) and the adjacent grid position further from the origin along the direction of the grid lines for the layer of q is referred to as n(q). Ignoring boundary conditions at the perimeter of the grid, the grammar of LINCOLN LOGS can be formulated as the following constraints:\na) 2-notch logs occupy two adjacent grid points b) 3-notch logs occupy three adjacent grid points c) 1- and 2-notch logs must be supported at all notches d) 3-notch logs must be supported in at least 2 notches e) log ends must be at the ends of logs f) short log segments indicate occupancy above or below g) long log segments indicate presence of a multi-notch\nlog above or below\nBoundary conditions are handled by stipulating that the grid positions beyond the perimeter are unoccupied, enforcing the support requirement (constraints c\u2013d) only at layers above the lowest layer, and enforcing log-segment constraints (f\u2013g) for the layer above the top of the structure. Structure estimation is performed by first establishing priors over the random variables Z+q , Z \u2212 q , Z u q , Z v q , and Z w q that correspond to log features using image evidence and establishing a uniform prior over the random variables Zq that correspond to the latent structure. This induces a probability distribution over the joint support of these random variables. The random variables that correspond to log features are marginalized and the resulting marginal distribution is conditioned on the language model \u03a6. Finally, the assignment to the collection, Z, of random variables Zq , that maximizes this conditional marginal probability is computed.\nargmax Z \u2211 Z+,Z\u2212,Zu,Zv,Zw\n\u03a6[Z,Z+,Z\u2212,Zu,Zv,Zw]\nPr ( Z,Z+,Z\u2212,Zu,Zv,Zw )\nWhile, in principle, this method can determine the conditional probability distribution over consistent structures given image evidence, doing so is combinatorially intractable. The conditional marginalization process is made tractable by pruning assignments to the random variables that violate the grammar \u03a6 using arc consistency (Mackworth, 1977). The maximization process is made tractable by using a branch-and-bound algorithm (Land and Doig, 1960) that maintains upper and lower bounds on the maximal conditional marginal probability. Thus instead of determining the distribution over structures, this yields a single most-likely consistent structure given the image evidence, along with its probability."}, {"heading": "3 Visibility Estimation", "text": "Image evidence for the presence or absence of each log feature is obtained independently. Each log feature corresponds to a unique local image property when projected to the image plane under the known camera-relative pose. A prior over the random variable associated with a specific log feature can be determined with a detector that is focused on the expected location and shape of that feature in the image given the projection. This assumes that the specific log feature is visible in the image, and not occluded by portions of the structure between the camera and that log feature. When the log feature f , a member of the set {+,\u2212, u, v, w} of the five feature classes defined above, at a position q, is not visible, the prior can be taken as uniform, allowing the constraints in the grammar to fill in unknown information. We represent the visibility of a feature by the boolean variable V fq .\nPr(Z fq = true) \u221d image evidence when V fq = true Pr(Z fq = false) = 1 2 otherwise\nIn order to do so, it is necessary to know which log features are visible and which are occluded so that image evidence is only applied to construct a prior on visible log features and a uniform prior is constructed for occluded log features. Thus, in Rumsfeld\u2019s terminology, one needs to know the known unknowns in order to determine the unknowns. This creates a chicken-and-egg problem. To determine whether a particular log feature is visible, one must know the composition of the structure between that feature and the camera and, to determine the structure composition, one must know which log features are visible. While earlier Siddharth et al. (2011) demonstrated successful automatic determination of log occupancy at occluded log positions, we could only do so given manual annotation of log-feature visibility. In other words, while earlier we were able to automatically infer Zq , it required manual annotation of V fq . Further, determining V fq required knowledge of Zq .\nWe extend this prior work to automatically determine visibility of log features in tandem with log occupancy. Our\nnovel contribution in this section is mutual automatic determination of both Zq and V fq . We solve the chickenand-egg problem inherent in doing so with an iterative algorithm reminiscent of expectation maximization (EM) (Baum, 1972; Baum et al., 1970; Dempster et al., 1977). We start with an initial estimate of the visibility of each log feature. We then apply the structure estimation procedure developed in Siddharth et al. (2011) to estimate the occupancy of each symbolic grid position. We then use the estimated structure to recompute a new estimate of logfeature visibility, and iterate this process until a fixpoint is reached. There are two crucial components of this process: determining the initial log-feature visibility estimate and reestimating log-feature visibility from an estimate of structure.\nWe determine the initial log-feature visibility estimate (i.e. V fq ) by assuming that the structure is a rectangular prism whose top face and two camera-facing front faces are completely visible. In this initial estimate, log features on these three faces are visible and log features elsewhere are not. We use the camera-relative pose of the symbolic grid (which can be determined without any knowledge of the structure) together with maximal extent of each of the three symbolic grid axes (i.e., three small integers which are currently specified manually) to determine the visible faces. This is done as follows. We determine the image positions for four corners of the base of this rectangular prism: the origin (0, 0, 0) of the symbolic grid, the two extreme points (imax, 0, 0) and (0, 0, kmax) of the two horizontal axes in the symbolic grid, and the symbolic grid point (imax, 0, kmax). We select the bottommost three such image positions as they correspond to the endpoints of the lower edges of the two frontal faces. It is possible, however, that one of these faces is (nearly) parallel to the camera axis and thus invisible. We determine that this is the case when the angle subtended by the two lower edges previously determined is less than 110\u25e6 and discard the face whose lower edge has minimal image width.\nWe update the log-feature visibility estimate from a structure estimate by rendering the structure in the context of the known camera-relative pose of the symbolic grid. When rendering the structure, we approximate each log as the bounding cylinder of its CAD model. We characterize each log feature with a fixed number of points, equally spaced around circular log ends or along linear log segments and trace a ray from each such point\u2019s 3D position to the camera center, asking whether that ray intersects some bounding cylinder for a log in the estimated structure. We take a log feature to be occluded when 60% or more of such rays intersect logs in the estimated structure. Our method is largely insensitive to the particular value of this threshold. It only must be sufficiently low to label log features as invisible when they actually are invisible. Structure estimation is not adversely affected by a moderate number of\nlog features that are incorrectly labeled as invisible when they are actually visible because it can use the grammar to determine occupancy of grid positions that correspond to such log features.\nWe can perform such rendering efficiently by rasterization. For each log feature, we begin with an empty bitmap. We iterate over each log feature and each occupied grid position that lies between that log feature and the camera center and render a projection of the bounding cylinder of the log at that grid position on the bitmap. This renders all possible occluders for each log feature allowing one to determine visibility by counting the rendered pixels at points in the bitmap that correspond to the projected rays.\nThe above process might not reach a fixpoint and instead may enter a finite loop of pairs of visibility and structure estimates. In practice, this process either reaches a fixpoint within three to four iterations or enters a loop of length two within three to four iterations, making loop detection straightforward. When a loop is detected, we select the structure in the loop with the highest probability estimate."}, {"heading": "4 Structure-Estimation Confidence", "text": "While the structure estimation process presented by Siddharth et al. (2011) can determine the occupancy of a small number of grid positions when only a single set of occupancy values is consistent with the grammar and the image evidence, it is not clairvoyant; it cannot determine the structure of an assembly when a large part of that assembly is occluded and many different possible structures are consistent with the image evidence. In this case, we again have an issue of unknowns vs. known unknowns: how can one determine one\u2019s confidence in one\u2019s structure estimation. If we could determine the conditional distribution over consistent structures given image evidence, P (Z|I), we could take the entropy of this distribution, H(Z|I), as a measure of confidence. However, as discussed previously, it is intractable to compute this distribution and further intractable to compute its entropy. Thus we adopt an alternate means of measuring confidence in the result of the structure-estimation process.\nGiven a visibility estimate, V fq , a structure estimate, Z, and the priors on the random variables associated with log features computed with image evidence, Zfq , one can marginalize over the random variables associated with visible log features and compute the maximum-likelihood assignment to the random variables associated with occluded log features, Z\u0302 f , that is consistent with a given structure estimate.\nZ\u0302 f = argmax Z fq\nV fq =false\n\u2211 Z fq\nV fq =true\n\u03a6[Z,Z+,Z\u2212,Zu,Zv,Zw]\nPr(Z,Z+,Z\u2212,Zu,Zv,Zw)\nOne can then ask the following question: what is the maximal amount \u03b4 that one can shift the probability mass on the random variables associated with occluded log features away from the uniform prior, reassigning that shifted probability mass to the opposite element of the support of that random variable from the above maximum-likelihood assignment, such that structure estimation yields the same estimated structure. Or in simpler terms,\nHow much hypothetical evidence of occluded log features is needed to cause me to change my mind away from the estimate derived from a uniform prior on such occluded features?\nWe compute this \u03b4 using a modified structure estimation step\nargmax Z \u2211 Z+,Z\u2212,Zu,Zv,Zw\n\u03a6[Z,Z+,Z\u2212,Zu,Zv,Zw]\nPr(Z,Z+,Z\u2212,Zu,Zv Zw) = Z\nwhen, for all q f where V fq = false\nPr(Z fq = \u00acZ\u0302 fq ) = 12 + \u03b4 Pr(Z fq = Z\u0302 f q ) = 1 2 \u2212 \u03b4\nWe call such a \u03b4 the estimation tolerance. Then, for any estimated structure, one can make a confidence judgment by comparing the estimation tolerance to an overall tolerance threshold \u03b4\u2217. One wishes to select a value for \u03b4\u2217 that appropriately trades off false positives and false negatives in such confidence judgements: we want to minimize the cases that result in a positive confidence assessment for an incorrect structure estimate and also minimize the cases that result in a negative confidence assessment for a correct structure estimate. Because the methods we present in the next section can gather additional evidence in light of negative confidence assessment in structure estimation, the former are more hazardous than the latter because the former preclude gathering such additional evidence and lead to an ultimate incorrect structure estimate while the latter simply incur the cost of such additional evidence gathering. Because of this asymmetry, our method is largely insensitive to the particular value of \u03b4\u2217 so long as it is sufficiently high to not yield excessive false positives. We have determined empirically that setting \u03b4\u2217 = 0.2 yields a good tradeoff: only 3/105 false positives and 7/105 false negatives on our corpus.\nOne can determine the estimation tolerance by binary search for the smallest value of \u03b4 \u2208 (0, 0.5) that results in a different estimated structure. However, this process is time consuming. But we don\u2019t actually need the value of \u03b4; we only need to determine whether \u03b4 < \u03b4\u2217. One can do this by simply asking whether the estimated structure, Z, changes when the probabilities are shifted by \u03b4\u2217\nPr(Z fq = \u00acZ\u0302 fq ) = 12 + \u03b4 \u2217 Pr(Z fq = Z\u0302 f q ) = 1 2 \u2212 \u03b4 \u2217\nThis involves only a single new structure estimation. One can make this process even faster by initializing the branchand-bound structure-estimation algorithm with the probability of the original structure estimate given the modified distributions for the random variables associated with occluded log features."}, {"heading": "5 Gathering Additional Evidence to Improve Structure Estimation", "text": "Structure estimation can be made more reliable by integrating multiple sources of image evidence. We perform structure estimation in a novel robotic environment, illustrated in Fig. 3, that facilities automatically gathering multiple sources of image evidence as needed. The structures are assembled in the robot workspace. This workspace is imaged by a camera mounted on a pendulum arm that can rotate 180\u25e6 about the workspace, under computer control, to image the assembly from different viewpoints. This can be used to view portions of the assembly that would otherwise be occluded. Moreover, a robotic arm can disassemble a structure on the workspace. This can be used to reveal the lower layers of a structure that would otherwise be occluded by higher layers. These methods can further be combined. Generally speaking, we seek a method for constraining a single estimate of an initial structure with multiple log features derived from different viewpoints and different stages of disassembly.\nWe can do this as follows. Let Z be a collection of random variables Zq associated with log occupancy for a given initial structure. Given multiple views i = 1, . . . , n with collections Zi of random variables Z+q , Z \u2212 q , Z u q , Z v q , and Z w q\nassociated with the image evidence for log features from those views, we can compute\nargmax Z \u2211 Z1...Zn\n\u03a6[Z,Z1]\u2227...\u2227[Z,Zn]\nPr (Z,Z1, . . . ,Zn)\nOnly two issues arise in doing this. First, we do not know the relative camera angles of the different views. Even though one can estimate the camera-relative pose of the structure independently for each view, this does not yield the registration between these views. There are only four possible symbolic orientations of the structure in each view so for n views we need only consider 4n\u22121 possible combinations of such symbolic orientations. We can search for the combination that yields the maximum-likelihood structure estimate. We do this search greedily, incrementally adding views to the structure-estimation process and registering each added view by searching for the best among the four possible registrations. Second, in the case of partial disassembly, we need to handle the fact that the partially disassembled structure is a proper subset of the initial structure. We do this simply by omitting random variables associated with log features for logs that are known to have been removed in the disassembly process and not instantiating constraints that mention such omitted random variables.\nWe can combine the techniques from section 4 with these techniques to yield an active-vision (Bajcsy, 1988) approach to producing a confident and correct structure estimate. One can perform structure estimation on an initial image and assess one\u2019s confidence in that estimate. If one is not confident, one can plan a new observation, entailing either a new viewpoint, a partial-disassembly operation, or a combination of the two and repeat this process until one is sufficiently confident in the estimated structure. Only one issue arises in doing this. One must plan the new observation. We do so by asking the following question:\nWhich of the available actions maximally increases confidence?\nLike before, if we could determine the conditional distribution over consistent structures given image evidence, we could compute the decrease in entropy that each available action would yield and select the action that maximally decreases entropy. But again, it is intractable to compute this distribution and further intractable to compute its entropy. Thus we adopt an alternate means of measuring increase in confidence.\nGiven visibility estimates V fiq for view i of the n current views along with a structure estimate Z constructed from those views, and priors on the random variables associated with log features computed with image evidence for each of these views Zfiq , one can marginalize over the random\nvariables associated with visible log features, V fiq = true, and compute the maximum-likelihood assignment Z\u0302 f to the random variables associated with occluded log features that is consistent with a given structure estimate:\nZ\u0302 f = argmax Z fiq\nV fiq =false\n\u2211 Z fiq\nV fiq =true\n\u03a6[Z,Z1]\u2227...\u2227\u03a6[Z,Zn]\nPr(Z,Z1, . . . ,Zn)\nWe can further determine those log features that are invisible in all current views but visible in a new view j that would result from a hypothetical action under consideration. One can then ask the following question: what is the maximal amount \u03b4\u2032 that one can shift the probability mass on these random variables away from the uniform prior, reassigning that shifted probability mass to the opposite element of the support of that random variable from the above maximum-likelihood assignment, such that structure estimation when adding the new view yields the same estimated structure. Or in simpler terms,\nFor a given hypothetical action, how much hypothetical evidence of log features that are occluded in all current views is needed in an imagined view resulting from that action where those log features are visible to cause me to change my mind away from the estimate derived from a uniform prior on such features?\nFor an action that yields a new view, j, we compute \u03b4\u2032 as follows\nargmax Z \u2211 Z1...Zn Zj\n\u03a6[Z,Z1]\u2227...\u2227\u03a6[Z,Zn]\u2227\u03a6[Z,Zj ]\nPr(Z,Z1, . . . ,Zn,Zj) = Z\nwhen: Pr(Z fiq = \u00acZ\u0302 f iq) = 1 2 + \u03b4\nPr(Z fiq = Z\u0302 f iq) = 1 2 \u2212 \u03b4\nfor all q f where V fjq = true \u2227 (\u2200i)V f\niq = false. Because we wish to select the action with the smallest \u03b4\u2032, we need its actual value. Thus we perform binary search to find \u03b4\u2032 for each hypothetical action and select the one with the lowest \u03b4\u2032. This nominally requires sufficiently deep binary search to compute \u03b4\u2032 to arbitrary precision. One can make this process even faster by performing binary search on all hypothetical actions simultaneously and terminating when there is only one hypothetical action lower than the branch point. This requires that binary search be only sufficiently deep to discriminate between the available actions."}, {"heading": "6 Natural language", "text": "An interesting feature of our framework is that it allows for elegant inclusion of information from other modalities.\nNatural language, for example, can be integrated into our approach to draw additional evidence for structure estimation from utterances describing the structure in question. A sentence, or set of sentences, describing a structure need not specify the structure unambiguously. Much like additional images from novel viewpoints can provide supplementary but partial evidence for structure estimation, sentences providing incomplete descriptions of structural features also can provide supplementary but partial evidence for structure estimation.\nWe have investigated this possibility via a small domainspecific language for describing some common features present in assembly toys. This language has: two nouns (window and door), four spatial relations (left of, right of, perpendicular to, and coplanar to), and one conjunction (and). Sentences constructed from these words can easily be parsed into logical formulas.\nAnalogous to how a CSP encodes the validity of an assembly through a set of constraints, such logical formulas derived from sentential descriptions can also constrain the structures to be considered. The words in our vocabulary impose the following constraints:\n1. A door is composed of a rectangular vertical coplanar set of grid points. All grid points inside the door must be unoccupied. All grid points on the door posts must be log ends facing away from the door. All grid points on the mantel must be occupied by the same log. The threshold must be unoccupied and at the bottom of the structure. 2. A window is similar to a door whose threshold is occupied by the same log and is not constrained to be at the bottom of the structure. 3. Perpendicular to constrains the grid points of two entities to lie on perpendicular axes. Coplanar to is analogous. 4. Right of or left of constrain the relative coordinates of the grid points of two entities\nWe compute a joint multiple-view and natural-language structure estimate as follows. Let Z be a collection of random variablesZq associated with log occupancy for a given initial structure. Given a set of constraints \u03a8 derived from natural language and multiple views i = 1, . . . , n with collections Zi of random variables Z+q , Z \u2212 q , Z u q , Z v q , and Z w q associated with the image evidence for log features from those views, we compute\nargmax Z\n\u2211 Z1...Zn\n\u03a6[Z,Z1]\u2227...\u2227[Z,Zn]\u2227\u03a8[Z]\nPr (Z,Z1, . . . ,Zn)\nAn example of how such an extension improves results is shown in Fig. 7."}, {"heading": "7 Results", "text": "We gathered a corpus of 5 different images of each of 32 different structures, each from a different viewpoint, for a total of 160 images. The structures were carefully designed so that proper subset relations exist among various pairs of the 32 distinct structures. Our video supplement demonstrates additional examples of our method.\nWe first evaluated automatic visibility estimation. We performed combined visibility and structure estimation on 105 of the 160 images and compared the maximum-likelihood structure estimate to that produced by Siddharth et al. (2011) using manual annotation of visibility. For each image, we compare the maximum-likelihood structure estimate to ground truth and compute the number of errors. We do this as follows. Each 1-, 2-, or 3-notch log in either the ground truth or estimated structure that is replaced with a different, possibly empty, collection of logs in the alternate structure counts as a single error (which may be a deletion, addition, or substitution). Further, each collection of r adjacent logs with the same medial axis in the ground truth that is replaced with a different collection of s logs in the estimated structure counts as min(r, s) errors. We then compute an error histogram of the number of images with fewer than t errors. Fig. 4 shows the error histograms for manual visibility annotation and automatic visibility estimation. Note that the latter performs as well as the former. Thus our automatic visibility-estimation process appears to be reliable.\nWe then evaluated structure-estimation confidence assessment. We computed the false-positive rate and falsenegative rate of our confidence-assessment procedure over\nthe entire corpus of 105 images, where a false positive occurs with a positive confidence assessment for an incorrect structure estimate and a false negative occurs with negative confidence assessment for a correct structure estimate. This resulted in only 3 false positives and 7 false negatives on our corpus.\nWe then evaluated the active-vision process for performing actions to improve structure-estimation confidence on 90 images from our corpus. So as not to render this evaluation dependent on the mechanical reliability of our robot which is tangential to the current paper and focus the evaluation on the computational method, we use the fact that our corpus contains multiple views of each structure from different viewpoints to simulate moving the robot head to gather new views and the fact our corpus contains pairs of structures in a proper-subset relation to simulate using the robot to perform partial disassembly. We first evaluated simulated robot-head motion to gather new views. For each image, we took the other images of the same structure from different viewpoints as potential actions and perform our active-vision process. We next evaluated simulated robotic disassembly. For each image, we took images of propersubset structures taken from the same viewpoint as potential actions and perform our active-vision process. We finally evaluated simulated combined robot-head motion and robotic disassembly. For each image, we took all images of proper-subset structures taken from any viewpoint as potential actions and perform our active-vision process. For each of these, we computed the error histogram at the termination of the active-vision process. Fig. 5 shows the error histograms for each of the active-vision processes together with the error histogram for baseline structure es-\ntimation from a single view on this subset of 90 images. Fig. 6 shows a rendering of the final estimated structure when performing each of the four processes from Fig. 5 on the same initial image. Log color indicates correct (green) or incorrect (red) estimation of log occupancies. Note that our active-vision processes consistently reduce estimation error.\nWe demonstrate natural-language integration in Fig. 7. In Fig. 7(a), structure estimation is performed on a single view, which due to occlusion, is unable to determine the correct structure. A second view is acquired. Note that this second view suffers from far more occlusion than the first view and by itself produces a far worse structure (Fig. 7b) than the first view alone. The information available in these two views is integrated and jointly produces a better structure estimate than either view by itself (Fig. 7c). However, this estimate is still imperfect. To demonstrate the utility and power of integrating visual and linguistic information, we intentionally discard the second view and construct a structure estimate from just a single image together with a single linguistic description, each of which is ambiguous taken in isolation. The user provides the sentence window left of and perpendicular to door. Note this this sentence does not fully describe the assembly. It does not specify the number of windows and doors, their absolute positions, or the contents of the rest of the structure. Yet this sentence together with the single image from the first view is sufficient to correctly estimate the structure (Fig. 7d)."}, {"heading": "8 Related work", "text": "Our work shares three overall objectives with prior work: estimating 3D structure from 2D images, determining when there is occlusion, and active vision. However, our work explores each of these issues from a novel perspective.\nPrior work on structure estimation (e.g. (Changsoo et al., 2009; Gupta et al., 2010; Saxena et al., 2007)) focuses on surface estimation, recovering a 3D surface from 2D images. In contrast, our work focuses on recovering the constituent structure of an assembly: what parts are used to make the assembly and how such parts are combined. Existing state-of-the-art surface reconstruction methods (e.g. Make3D (Saxena et al., 2008)) are unable to determine surface structure of the kinds of LINCOLN LOG assemblies considered here. Ever if such surface estimates were successful, such estimates alone are insufficient to determine the constituent structure.\nPrior work on occlusion determination (e.g. (Gupta et al., 2010; Hoiem et al., 2011)) focuses on finding occlusion boundaries: the 2D image boundaries of occluded regions. In contrast, our work focuses on determining occluded parts in the constituent structure. We see no easy way to determine occluded parts from occlusion boundaries because such boundaries alone are insufficient to determine\neven the number of occluded parts, let alone their types and positions in a 3D structure.\nPrior work on active vision (e.g. (Maver and Bajcsy, 1993)) focuses on integrating multiple views into surface estimation and selecting new viewpoints to facilitate such in the presence of occlusion. In contrast, our work focuses on determining the confidence of constituent structure estimates and choosing an action with maximal anticipated increase in confidence. We consider not only viewpoint changes but also robotic disassembly to view object interiors. Also note that the confidence estimates used in our approach to active vision are mediated by the visual language model. We might not need to perform active vision to observe all occluded structure as it might be possible to infer part of the occluded structure. Prior work selects a new viewpoint to render occluded structure visible. We instead select an action to maximally increase confidence. Such an action might actually not attempt to view an occluded portion of the structure but rather increase confidence in a visible portion of the structure in a way that when mediated by the language model ultimately yields a maximal increase in the confidence assessment of a portion of the structure that remains occluded even with the action taken."}, {"heading": "9 Conclusion", "text": "We have presented a general framework for (a) seeing the unseeable, (b) seeing unseeability, (c) a rational basis for determining confidence in what one sees, and (d) an active-vision decision-making process for determining rational behavior in the presence of unseeability. We instantiated and evaluated our general framework in the LINCOLN LOG domain and found it to be effective. This framework has many potential extensions. One can construct random variables to represent uncertain evidence in other modalities such as language and speech and one can augment the stochastic CSP to mutually constraint these variables together with the current random variables that represent image evidence and latent structure so that a latent utterance describes a latent structure. One can then use the same maximum-likelihood estimation techniques to produce the maximum-likelihood utterance consistent with a structure marginalizing over image evidence. This constitutes producing an utterance that describes a visual observation. One can use the same maximum-likelihood estimation techniques to produce the maximum-likelihood sequence of robotic actions consistent with building a structure marginalizing over utterance evidence or alternatively image evidence. This would constitute building a structure by understanding a linguistic description of that structure or by copying a visually observed assembly. One can combine evidence from an uncertain visual perception of a structure with evidence from an uncertain linguistic description of that structure to reduce the uncertainty of structure estimation. This would constitute using vision\nand language to mutually disambiguate each other. One could augment one\u2019s collection of potential actions to include speech acts as well as robotic-manipulation actions and search for the action that best improves confidence. This would constitute choosing between asking someone to provide you information and seeking that information yourself. One could determine what another agent can see from what that agent says. Likewise one could decide what to say so that another agent can see what is unseeable to that agent yet is seeable to you. Overall, this can lead to a rational basis for cooperative agent behavior and a theory of the perception-cognition-action loop which incorporates mutual belief, goals, and desires where agents seek to assist each other by seeing what their peers cannot, describing such sight, and inferring what their peers can and cannot see. We are currently beginning to investigate potential extensions to our general approach and hope to present them in the future."}, {"heading": "Acknowledgments", "text": "This work was supported, in part, by NSF grant CCF0438806, by the Naval Research Laboratory under Contract Number N00173-10-1-G023, by the Army Research Laboratory accomplished under Cooperative Agreement Number W911NF-10-2-0060, and by computational resources\nprovided by Information Technology at Purdue through its Rosen Center for Advanced Computing. Any views, opinions, findings, conclusions, or recommendations contained or expressed in this document or material are those of the author(s) and do not necessarily reflect or represent the views or official policies, either expressed or implied, of NSF, the Naval Research Laboratory, the Office of Naval Research, the Army Research Laboratory, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein."}], "references": [{"title": "Learning grammatical models for object recognition. In Logic and Probability for Scene Interpretation, number 08091 in Dagstuhl Seminar", "author": ["M. Aycinena Lippow", "L.P. Kaelbling", "T. Lozano-Perez"], "venue": null, "citeRegEx": "Lippow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lippow et al\\.", "year": 2008}, {"title": "Representing the existence and the location of hidden objects: Object permanence in 6- and 8month-old infants", "author": ["R. Baillargeon"], "venue": "Cognition, 23:21\u201341,", "citeRegEx": "Baillargeon.,? \\Q1986\\E", "shortCiteRegEx": "Baillargeon.", "year": 1986}, {"title": "Active perception", "author": ["Ruzena Bajcsy"], "venue": "In Proceedings of IEEE,", "citeRegEx": "Bajcsy.,? \\Q1988\\E", "shortCiteRegEx": "Bajcsy.", "year": 1988}, {"title": "An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process", "author": ["L.E. Baum"], "venue": null, "citeRegEx": "Baum.,? \\Q1972\\E", "shortCiteRegEx": "Baum.", "year": 1972}, {"title": "A maximization technique occuring in the statistical analysis of probabilistic functions of Markov chains", "author": ["L.E. Baum", "T. Petrie", "G. Soules", "N. Weiss"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "Baum et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Baum et al\\.", "year": 1970}, {"title": "Recognition-by-components: A theory of human image understanding", "author": ["I. Biederman"], "venue": "Psychological review,", "citeRegEx": "Biederman.,? \\Q1987\\E", "shortCiteRegEx": "Biederman.", "year": 1987}, {"title": "Geometric reasoning for single image structure recovery", "author": ["L.D. Changsoo", "M. Hebert", "T. Kanade"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Changsoo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Changsoo et al\\.", "year": 2009}, {"title": "A dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image", "author": ["E. Delage", "Honglak Lee", "A.Y. Ng"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Delage et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Delage et al\\.", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm (with discussion)", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "author": ["M.A. Fischler", "Robert C. Bolles"], "venue": "Commun. ACM,", "citeRegEx": "Fischler and Bolles.,? \\Q1981\\E", "shortCiteRegEx": "Fischler and Bolles.", "year": 1981}, {"title": "Representing statics as forces in equilibrium", "author": ["J.J. Freyd", "T.M. Pantzer", "J.L. Cheng"], "venue": "Journal of Experimental Psychology,", "citeRegEx": "Freyd et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Freyd et al\\.", "year": 1988}, {"title": "Blocks world revisited: Image understanding using qualitative geometry and mechanics", "author": ["A. Gupta", "A. Efros", "M. Hebert"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "Gupta et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2010}, {"title": "Learning spatial context: Using stuff to find things", "author": ["G. Heitz", "D. Koller"], "venue": "In Proceedings of the 10th European Conference on Computer Vision,", "citeRegEx": "Heitz and Koller.,? \\Q2008\\E", "shortCiteRegEx": "Heitz and Koller.", "year": 2008}, {"title": "Recovering occlusion boundaries from an image", "author": ["D. Hoiem", "A. Efros", "M. Hebert"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Hoiem et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoiem et al\\.", "year": 2011}, {"title": "An automatic method of solving discrete programming", "author": ["A.H. Land", "A.G. Doig"], "venue": "problems. Econometrica,", "citeRegEx": "Land and Doig.,? \\Q1960\\E", "shortCiteRegEx": "Land and Doig.", "year": 1960}, {"title": "A language and a program for stating and solving combinatorial problems", "author": ["J.-L. Lauriere"], "venue": "Artificial Intelligence,", "citeRegEx": "Lauriere.,? \\Q1978\\E", "shortCiteRegEx": "Lauriere.", "year": 1978}, {"title": "Consistency in networks of relations", "author": ["A.K. Mackworth"], "venue": "Artificial Intelligence,", "citeRegEx": "Mackworth.,? \\Q1977\\E", "shortCiteRegEx": "Mackworth.", "year": 1977}, {"title": "Using contours to detect and localize junctions in natural images", "author": ["M. Maire", "P. Arbelaez", "C. Fowlkes", "J. Malik"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Maire et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maire et al\\.", "year": 2008}, {"title": "Occlusions as a guide for planning the next view", "author": ["J. Maver", "R. Bajcsy"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Maver and Bajcsy.,? \\Q1993\\E", "shortCiteRegEx": "Maver and Bajcsy.", "year": 1993}, {"title": "Active object recognition by view integration and reinforcement learning", "author": ["L. Paletta", "A. Pinz"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Paletta and Pinz.,? \\Q2000\\E", "shortCiteRegEx": "Paletta and Pinz.", "year": 2000}, {"title": "Active recognition through next view planning: a survey", "author": ["S.D. Roy", "S. Chaudhury", "S. Banerjee"], "venue": "Pattern Recognition,", "citeRegEx": "Roy et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2004}, {"title": "Grammar-based object representations in a scene parsing task", "author": ["V. Savova", "F. J\u00e4kel", "J.B. Tenenbaum"], "venue": "In Proceedings of the 31st Annual Meeting of the Cognitive Science Society,", "citeRegEx": "Savova et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Savova et al\\.", "year": 2009}, {"title": "Learning 3-d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A. Y Ng"], "venue": "In ICCV workshop on 3D Representation for Recognition", "citeRegEx": "Saxena et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2007}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "Min Sun", "A.Y. Ng"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Saxena et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2008}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik.,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik.", "year": 2000}, {"title": "A visual language model for estimating object pose and structure in a generative visual domain", "author": ["N. Siddharth", "A. Barbu", "J.M. Siskind"], "venue": "In Proceedings of the IEEE International Conf. on Robotics and Automation,", "citeRegEx": "Siddharth et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Siddharth et al\\.", "year": 2011}, {"title": "Haptic perception of objects in infancy", "author": ["A. Streri", "E.S. Spelke"], "venue": "Cognitive Psychology,", "citeRegEx": "Streri and Spelke.,? \\Q1988\\E", "shortCiteRegEx": "Streri and Spelke.", "year": 1988}, {"title": "A survey of sensor planning in computer vision", "author": ["K.A. Tarabanis", "P.K. Allen", "R.Y. Tsai"], "venue": "IEEE Transactions on Robotics and Automation,,", "citeRegEx": "Tarabanis et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Tarabanis et al\\.", "year": 1995}, {"title": "Psychological foundations of number: Numerical competence in human infants", "author": ["K. Wynn"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "Wynn.,? \\Q1998\\E", "shortCiteRegEx": "Wynn.", "year": 1998}, {"title": "What can missing correspondences tell us about 3d structure and motion", "author": ["C. Zach", "A. Irschara", "H. Bischof"], "venue": "In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "Zach et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zach et al\\.", "year": 2008}, {"title": "A stochastic grammar of images", "author": ["S.-C. Zhu", "D. Mumford"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "Zhu and Mumford.,? \\Q2006\\E", "shortCiteRegEx": "Zhu and Mumford.", "year": 2006}], "referenceMentions": [{"referenceID": 9, "context": "For example, Freyd et al. (1988) showed that physical forces, such as gravity, and whether such forces are in equilibrium, due to support and attachment relations, influences visual perception of object location in adults.", "startOffset": 13, "endOffset": 33}, {"referenceID": 28, "context": "Wynn (1998) showed that material properties of objects, such as whether they are countable or mass substances, along with abstract properties, such as the number of countable objects and the quantity of mass substances, and how they are transferred between containers, influences visual perception in young infants.", "startOffset": 0, "endOffset": 12}, {"referenceID": 24, "context": "Figure 1: (a) A state-of-the-art segmentation method, Normalized Cut (Shi and Malik, 2000), does not segment out the log parts.", "startOffset": 69, "endOffset": 90}, {"referenceID": 17, "context": "(b) A state-of-the-art edge detector, GPB (Maire et al., 2008), does not reliably find edges separating adjacent logs or log ends.", "startOffset": 42, "endOffset": 62}, {"referenceID": 25, "context": "In previous work Siddharth et al. (2011) presented an approach for using a visual language model for improving recognition accuracy on compositional visual structures in", "startOffset": 17, "endOffset": 41}, {"referenceID": 15, "context": "A language model, in the form of a stochastic constraintsatisfaction problem (CSP) (Lauriere, 1978), characterizes the constrained way object parts can combine to yield a whole object and significantly improves the recognition", "startOffset": 83, "endOffset": 99}, {"referenceID": 16, "context": "that violate the grammar \u03a6 using arc consistency (Mackworth, 1977).", "startOffset": 49, "endOffset": 66}, {"referenceID": 14, "context": "The maximization process is made tractable by using a branch-and-bound algorithm (Land and Doig, 1960) that maintains upper and lower bounds on the maximal conditional marginal probability.", "startOffset": 81, "endOffset": 102}, {"referenceID": 25, "context": "While earlier Siddharth et al. (2011) demonstrated successful automatic determination of log occupancy at occluded log positions, we could only do so given manual annotation of log-feature visibility.", "startOffset": 14, "endOffset": 38}, {"referenceID": 3, "context": "We solve the chickenand-egg problem inherent in doing so with an iterative algorithm reminiscent of expectation maximization (EM) (Baum, 1972; Baum et al., 1970; Dempster et al., 1977).", "startOffset": 130, "endOffset": 184}, {"referenceID": 4, "context": "We solve the chickenand-egg problem inherent in doing so with an iterative algorithm reminiscent of expectation maximization (EM) (Baum, 1972; Baum et al., 1970; Dempster et al., 1977).", "startOffset": 130, "endOffset": 184}, {"referenceID": 8, "context": "We solve the chickenand-egg problem inherent in doing so with an iterative algorithm reminiscent of expectation maximization (EM) (Baum, 1972; Baum et al., 1970; Dempster et al., 1977).", "startOffset": 130, "endOffset": 184}, {"referenceID": 25, "context": "We then apply the structure estimation procedure developed in Siddharth et al. (2011) to estimate the occupancy of each symbolic grid position.", "startOffset": 62, "endOffset": 86}, {"referenceID": 25, "context": "While the structure estimation process presented by Siddharth et al. (2011) can determine the occupancy of a small number of grid positions when only a single set of occupancy values is consistent with the grammar and the image evidence, it is not clairvoyant; it cannot determine the structure of an assembly when a large part of that assembly is occluded and many different possible structures are consistent with the image evidence.", "startOffset": 52, "endOffset": 76}, {"referenceID": 2, "context": "We can combine the techniques from section 4 with these techniques to yield an active-vision (Bajcsy, 1988) approach to producing a confident and correct structure estimate.", "startOffset": 93, "endOffset": 107}, {"referenceID": 25, "context": "We performed combined visibility and structure estimation on 105 of the 160 images and compared the maximum-likelihood structure estimate to that produced by Siddharth et al. (2011) using manual annotation of visibility.", "startOffset": 158, "endOffset": 182}, {"referenceID": 6, "context": "(Changsoo et al., 2009; Gupta et al., 2010; Saxena et al., 2007)) focuses on surface estimation, recovering a 3D surface from 2D images.", "startOffset": 0, "endOffset": 64}, {"referenceID": 11, "context": "(Changsoo et al., 2009; Gupta et al., 2010; Saxena et al., 2007)) focuses on surface estimation, recovering a 3D surface from 2D images.", "startOffset": 0, "endOffset": 64}, {"referenceID": 22, "context": "(Changsoo et al., 2009; Gupta et al., 2010; Saxena et al., 2007)) focuses on surface estimation, recovering a 3D surface from 2D images.", "startOffset": 0, "endOffset": 64}, {"referenceID": 23, "context": "Make3D (Saxena et al., 2008)) are unable to determine surface structure of the kinds of LINCOLN LOG assemblies considered here.", "startOffset": 7, "endOffset": 28}, {"referenceID": 11, "context": "(Gupta et al., 2010; Hoiem et al., 2011)) focuses on finding occlusion", "startOffset": 0, "endOffset": 40}, {"referenceID": 13, "context": "(Gupta et al., 2010; Hoiem et al., 2011)) focuses on finding occlusion", "startOffset": 0, "endOffset": 40}, {"referenceID": 18, "context": "(Maver and Bajcsy, 1993)) focuses on integrating multiple views into surface estimation and selecting new viewpoints to facilitate such in the presence of occlusion.", "startOffset": 0, "endOffset": 24}], "year": 2012, "abstractText": "We present a framework that allows an observer to determine occluded portions of a structure by finding the maximum-likelihood estimate of those occluded portions consistent with visible image evidence and a consistency model. Doing this requires determining which portions of the structure are occluded in the first place. Since each process relies on the other, we determine a solution to both problems in tandem. We extend our framework to determine confidence of one\u2019s assessment of which portions of an observed structure are occluded, and the estimate of that occluded structure, by determining the sensitivity of one\u2019s assessment to potential new observations. We further extend our framework to determine a robotic action whose execution would allow a new observation that would maximally increase one\u2019s confidence.", "creator": "LaTeX with hyperref package"}}}