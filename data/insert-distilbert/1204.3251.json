{"id": "1204.3251", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Apr-2012", "title": "Plug-in martingales for testing exchangeability on-line", "abstract": "a standard assumption in machine learning is the exchangeability quality of data, which is equivalent to assuming also that the examples are generated from the same probability distribution independently. this paper is wholly devoted to subsequently testing the assumption of exchangeability on - line : the examples consistently arrive frequently one by one, and after receiving each example we would like to periodically have released a valid measure of the degree component to which the assumption of exchangeability has been falsified. such measures are presently provided by exchangeability martingales. we extend known techniques for constructing exchangeability martingales and obviously show that easily our new method is competitive and with the martingales introduced before. finally we investigate, the performance of our testing method on two benchmark cluster datasets, usps and statlog satellite data ; for the former, the known techniques give satisfactory results, but for the latter criteria our new more flexible method becomes necessary.", "histories": [["v1", "Sun, 15 Apr 2012 10:21:57 GMT  (358kb,D)", "https://arxiv.org/abs/1204.3251v1", "17 pages, 7 figures"], ["v2", "Thu, 28 Jun 2012 09:36:27 GMT  (366kb,D)", "http://arxiv.org/abs/1204.3251v2", "8 pages, 7 figures; ICML 2012 Conference Proceedings"]], "COMMENTS": "17 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG stat.ME", "authors": ["valentina fedorova", "alex j gammerman", "ilia nouretdinov", "volodya vovk"], "accepted": true, "id": "1204.3251"}, "pdf": {"name": "1204.3251.pdf", "metadata": {"source": "META", "title": "Plug-in martingales for testing exchangeability on-line", "authors": ["Valentina Fedorova", "Alex Gammerman", "Ilia Nouretdinov"], "emails": ["valentina@cs.rhul.ac.uk", "alex@cs.rhul.ac.uk", "ilia@cs.rhul.ac.uk", "v.vovk@rhul.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Many machine learning algorithms have been developed to deal with real-life high dimensional data. In order to state and prove properties of such algorithms it is standard to assume that the data satisfy the exchangeability assumption (although some algorithms make different assumptions or, in the case of prediction with expert advice, do not make any statistical assumptions at all). These properties can be violated if the assumption is not satisfied, which makes it important to test the data for satisfying it.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nNote that the popular assumption that the data is i.i.d. (independent and identically distributed) has the same meaning for testing as the exchangeability assumption. A joint distribution of an infinite sequence of examples is exchangeable if it is invariant w.r. to any permutation of examples. Hence if the data is i.i.d., its distribution is exchangeable. On the other hand, by de Finetti\u2019s theorem (see, e.g., Schervish, 1995, p. 28) any exchangeable distribution on the data (a potentially infinite sequence of examples) is a mixture of distributions under which the data is i.i.d. Therefore, testing for exchangeability is equivalent to testing for being i.i.d.\nTraditional statistical approaches to testing are inappropriate for high dimensional data (see, e.g., Vapnik, 1998, pp. 6\u20137). To address this challenge a previous study (Vovk et al., 2003) suggested a way of on-line testing by employing the theory of conformal prediction and calculating exchangeability martingales. Basically testing proceeds in two steps. The first step is implemented by a conformal predictor that outputs a sequence of p-values. The sequence is generated in the on-line mode: examples are presented one by one and for each new example a p-value is calculated from this and all the previous examples. For the second step the authors introduced exchangeability martingales that are functions of the p-values and track the deviation from the assumption. Once the martingale grows up to a large value (20 and 100 are convenient rules of thumb) the exchangeability assumption can be rejected for the data.\nThis paper proposes a new way of constructing martingales in the second step of testing. To construct an exchangeability martingale based on the sequence of p-values we need a betting function, which determines the contribution of a p-value to the value of the martingale. In contrast to the previous studies that use a fixed betting function the new martingale tunes its betting function to the sequence to detect any deviation from\nar X\niv :1\n20 4.\n32 51\nv2 [\ncs .L\nG ]\n2 8\nJu n\n20 12\nthe assumption. We show that this martingale, which we call a plug-in martingale, is competitive with all the martingales covered by the previous studies; namely, asymptotically the former grows faster than the latter."}, {"heading": "1.1. Related work", "text": "The first procedure of testing exchangeability on-line is described in Vovk et al. (2003). The core testing mechanism is an exchangeability martingale. Exchangeability martingales are constructed using a sequence of p-values. The algorithm for generating p-values assigns small p-values to unusual examples. It implies the idea of designing martingales that would have a large value if too many small p-values were generated, and suggests corresponding power martingales. Other martingales (simple mixture and sleepy jumper) implement more complicated strategies, but follow the same idea of scoring on small p-values.\nHo (2005) applies power martingales to the problem of change detection in time-varying data streams. The author shows that small p-values inflate the martingale values and suggests to use the martingale difference as another test for the problem."}, {"heading": "1.2. This paper", "text": "To the best of our knowledge, no study has aimed to find any other ways of translating p-values into a martingale value. In this paper we propose a new more flexible method of constructing exchangeability martingales for a given sequence of p-values.\nThe rest of the paper is organised as follows. Section 2 gives the definition of exchangeability martingales. Section 3 presents the construction of plug-in exchangeability martingales, explains the rationale behind them, and compares them to the power martingales, which have been used previously. Section 4 shows experimental results of testing two real-life datasets for exchangeability; for one of these datasets power martingales work satisfactorily and for the other one the greater flexibility of plug-in martingales becomes essential. Section 5 summarises the paper."}, {"heading": "2. Exchangeability martingales", "text": "This section outlines necessary definitions and results of the previous studies."}, {"heading": "2.1. Exchangeability", "text": "Consider a sequence of random variables ( Z1, Z2, . . .) that all take values in the same example space. Then the joint probability distribution P(Z1, . . . , ZN ) of a\nfinite number of the random variables is exchangeable if it is invariant under any permutation of the random variables. The joint distribution of infinite number of random variables ( Z1, Z2, . . .) is exchangeable if the marginal distribution P(Z1, . . . , ZN ) is exchangeable for every N ."}, {"heading": "2.2. Martingales for testing", "text": "As in Vovk et al. (2003), the main tool for testing exchangeability on-line is a martingale. The value of the martingale reflects the strength of evidence against the exchangeability assumption. An exchangeability martingale is a sequence of non-negative random variables S0, S1, . . . that keep the conditional expectation:\nSn \u2265 0 Sn = E(Sn+1 | S1, . . . , Sn),\nwhere E refers to the expected value with respect to any exchangeable distribution on examples. We also assume S0 = 1. Note that we will obtain an equivalent definition if we replace \u201cany exchangeable distribution on examples\u201d by \u201cany distribution under which the examples are i.i.d.\u201d (remember the discussion of de Finetti\u2019s theorem in Section 1).\nTo understand the idea behind martingale testing we can imagine a game where a player starts from the capital of 1, places bets on the outcomes of a sequence of events, and never risks bankruptcy. Then a martingale corresponds to a strategy of the player, and its value reflects the acquired capital. According to Ville\u2019s inequality (see Ville, 1939, p. 100),\nP { \u2203n : Sn \u2265 C } \u2264 1/C, \u2200C \u2265 1,\nit is unlikely for any Sn to have a large value. For the problem of testing exchangeability, if the final value of a martingale is large then the exchangeability assumption for the data can be rejected with the corresponding probability."}, {"heading": "2.3. On-line calculation of p-values", "text": "Let (z1, z2, . . .) denote a sequence of examples. Each example zi is the vector representing a set of attributes xi and a label yi: zi = (xi, yi). In this paper we use conformal predictors to generate a sequence of p-values that corresponds to the given examples. The general idea of conformal prediction is to test how well a new example fits to the previously observed examples. For this purpose a \u201cnonconformity measure\u201d is defined. This is a function that estimates the strangeness of one example with respect to others:\n\u03b1i = A ( zi, {z1, . . . , zn} ) ,\nAlgorithm 1 Generating p-values on-line\nInput: (z1, z2, . . .) data for testing Output: (p1, p2, . . .) sequence of p-values for i = 1, 2, . . . do\nobserve a new example zi for j = 1 to i do \u03b1j = A ( zj , {z1, . . . , zi} ) end for pi = #{j:\u03b1j>\u03b1i}+\u03b8i#{j:\u03b1j=\u03b1i} i\nend for\nwhere in general {. . .} stands for a multiset (the same element may be repeated more than once) rather than a set. Typically, each example is assigned a \u201cnonconformity score\u201d \u03b1i based on some prediction method. In this paper we deal with the classification problem and the 1-Nearest Neighbor (1-NN) algorithm is used as the underling method to compute the nonconformity scores. The algorithm is simple but it works well enough in many cases (see, e.g., Hastie et al., 2001, pp. 422\u2013427). A natural way to define the nonconformity score of an example is by comparing its distance to the examples with the same label to its distance to the examples with a different label:\n\u03b1i = minj 6=i:yi=yj d(xi, xj)\nminj 6=i:yi 6=yj d(xi, xj) , (1)\nwhere d(xi, xj) is the Euclidean distance. According to the chosen nonconformity measure, \u03b1i is high if the example is close to another example with a different label and far from any examples with the same label.\nUsing the calculated nonconformity scores of all observed examples, the p-value pn that corresponds to an example zn is calculated as\npn = #{i : \u03b1i > \u03b1n}+ \u03b8n#{i : \u03b1i = \u03b1n}\nn ,\nwhere \u03b8n is a random number from [0, 1] and the symbol # means the cardinality of a set. Algorithm 1 summarises the process of on-line calculation of p-values (it is clear that it can also be applied to a finite dataset (z1, . . . , zn) producing a finite sequence (p1, . . . , pn) of p-values).\nThe following is a standard result in the theory of conformal prediction (see, e.g., Vovk et al. 2003, Theorem 1).\nTheorem 1. If examples (z1, z2, . . .) (resp. (z1, z2, . . . , zn)) satisfy the exchangeability assumption, Algorithm 1 produces p-values (p1, p2, . . .) (resp. (p1, p2, . . . , pn)) that are independent and uniformly distributed in [0, 1].\nThe property that the examples generated by an exchangeable distribution provide uniformly and independently distributed p-values allows us to test exchangeability by calculating martingales as functions of the p-values."}, {"heading": "3. Martingales based on p-values", "text": "This section focuses on the second part of testing: given the sequence of p-values a martingale is calculated as a function of the p-values.\nFor each i \u2208 {1, 2, . . .}, let fi : [0, 1]i \u2192 [0,\u221e). Let (p1, p2, . . .) be the sequence of p-values generated by Algorithm 1. We consider martingales Sn of the form\nSn = n\u220f i=1 fi(pi), n = 1, 2, . . . , (2)\nwhere we denote fi(p) = fi(p1, . . . , pi\u22121, p) and call the function fi(p) a betting function.\nTo be sure that (2) is indeed a martingale we need the following constraint on the betting functions fi:\u222b 1\n0\nfi(p)dp = 1, i = 1, 2, . . .\nThen we can check:\nE(Sn+1 | S0, . . . , Sn) = \u222b 1 0 n\u220f i=1 ( fi(pi) ) fn+1(p)dp\n= n\u220f i=1 ( fi(pi) )\u222b 1 0 fn+1(p)dp = n\u220f i=1 fi(pi) = Sn.\nUsing representation (2) we can update the martingale on-line: having calculated a p-value pi for a new example in Algorithm 1 the current martingale value becomes Si = Si\u22121 \u00b7 fi(pi). To define the martingales completely we need to describe the betting functions fi."}, {"heading": "3.1. Previous results: power and simple mixture martingales", "text": "Previous studies (Vovk et al., 2003) have proposed to use a fixed betting function\n\u2200i : fi(p) = \u03b5p\u03b5\u22121,\nwhere \u03b5 \u2208 [0, 1]. Several martingales were constructed using the function. The power martingale for some \u03b5, denoted as M\u03b5n, is defined as\nM\u03b5n = n\u220f i=1 \u03b5p\u03b5\u22121i .\nThe simple mixture martingale, denoted as Mn, is the mixture of power martingales over different \u03b5 \u2208 [0, 1]:\nMn = \u222b 1 0 M\u03b5nd\u03b5.\nSuch a martingale will grow only if there are many small p-values in the sequence. This follows from the shape of the betting functions: see Figure 1. If the generated p-values concentrate in any other part of the unit interval, we cannot expect the martingale to grow. So it might be difficult to reject the assumption of exchangeability for such sequences."}, {"heading": "3.2. New plug-in approach", "text": ""}, {"heading": "3.2.1. Plug-in martingale", "text": "Let us use an estimated probability density function as the betting function fi(p). At each step the probability density function is estimated using the accumulated p-values:\n\u03c1i(p) = \u03c1\u0302(p1, . . . , pi\u22121, p), (3)\nwhere \u03c1\u0302(p1, . . . , pi\u22121, p) is the estimate of the probability density function using the p-values p1, . . . , pi\u22121 output by Algorithm 1.\nSubstituting these betting functions into (2) we get a new martingale that we call a plug-in martingale. The martingale avoids betting if the p-values are distributed uniformly, but if there is any peak it will be used for betting.\nEstimating a probability density function. In our experiments we have used the statistical environment and language R. The density function in its\nStats package implements kernel density estimation with different parameters. But since p-values always lie in the unit interval, the standard methods of kernel density estimation lead to poor results for the points that are near the boundary. To get better results for the boundary points the sequence of p-values is reflected to the left from zero and to the right from one. Then the kernel density estimate is calculated using the extended sample \u222ani=1 { \u2212pi, pi, 2 \u2212 pi } . The estimated density function is set to zero outside the unit interval and then normalised to integrate to one. For the results presented in this paper the parameters used are the Gaussian kernel and Silverman\u2019s \u201crule of thumb\u201d for bandwidth selection. Other settings have been tried as well, but the results are comparable and lead to the same conclusions.\nThe values Sn of the plug-in martingale can be updated recursively. Suppose computing the nonconformity scores (\u03b11, . . . , \u03b1n) from (z1, . . . , zn) takes time g(n) and evaluating (3) takes time h(n). Then updating Sn\u22121 to Sn takes time O(g(n) + n+ h(n)): indeed, it is easy to see that calculating the rank of \u03b1n in the multiset {\u03b11, . . . , \u03b1n} takes time \u0398(n).\nThe performance of the plug-in martingale on real-life datasets will be presented in Section 4. The rest of the current section proves that the plug-in martingale provides asymptotically a better growth rate than any martingale with a fixed betting function. To prove this asymptotical property of the plug-in martingale we need the following assumptions."}, {"heading": "3.2.2. Assumptions", "text": "Consider an infinite sequence of p-values (p1, p2, . . .). (This is simply a deterministic sequence.) For its finite prefix (p1, . . . , pn) define the corresponding empirical probability measure Pn: for a Borel set A in R,\nPn(A) = #{i = 1, . . . , n : pi \u2208 A}\nn .\nWe say that the sequence (p1, p2, . . .) is stable if there exists a probability measure P on R such that:\n1. Pn weak\u2212\u2212\u2212\u2212\u2192 n\u2192\u221e P;\n2. there exists a positive continuous density function \u03c1(p) for P: for any Borel set A in R, P(A) =\u222b A \u03c1(p)dp.\nIntuitively, the stability means that asymptotically the sequence of p-values can be described well by a probability distribution.\nConsider a sequence (f1(p), f2(p), . . .) of betting functions. (This is simply a deterministic sequence of functions fi : [0, 1]\u2192 [0,\u221e), although we are particularly interested in the functions fi(p) = \u03c1i(p), as defined in (3).) We say that this sequence is consistent for (p1, p2, . . .) if\nlog ( fn(p) ) uniformly in p\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 n\u2192\u221e log ( \u03c1(p)).\nIntuitively, consistency is an assumption about the algorithm that we use to estimate the function \u03c1(p); in the limit we want a good approximation."}, {"heading": "3.2.3. Growth rate of plug-in martingale", "text": "The following result says that, under our assumptions, the logarithmic growth rate of the plug-in martingale is better than that of any martingale with a fixed betting function (remember that by a betting function we mean any function mapping [0, 1] to [0,\u221e)). Theorem 2. If a sequence (p1, p2, . . .) \u2208 [0, 1]\u221e is stable and a sequence of betting functions ( f1(p), f2(p), . . .\n) is consistent for it then, for any positive continuous betting function f ,\nlim inf n\u2192\u221e\n( 1\nn n\u2211 i=1 log ( fi(pi) ) \u2212 1 n n\u2211 i=1 log ( f(pi)\n)) \u2265 0\nFirst we explain the meaning of Theorem 2 and then prove it. According to representation (2) after n steps the martingale grows to\nn\u220f i=1 fi(pi). (4)\nNote that if for any p-value p \u2208 [0, 1] we have fi(p) = 0 then the martingale can become zero and will never change after that. Therefore, it is reasonable to consider positive fi(p). Then we can rewrite product (4) as sum of logarithms, which gives us the logarithmic growth of the martingale:\nn\u2211 i=1 log ( fi(pi) ) .\nWe assume that the sequence of p-values is stable and the sequence of estimated probability density functions that is used to construct the plug-in martingale is consistent. Then the limit inequality from Theorem 2 states that the logarithmic growth rate of the plug-in martingale is asymptotically at least as high as that of any martingale with a fixed betting function (which have been suggested in previous studies).\nTo prove Theorem 2 we will use the following lemma.\nLemma 1. For any probability density functions \u03c1 and f (so that \u222b 1 0 \u03c1(p)dp = 1 and \u222b 1 0 f(p)dp = 1),\u222b 1\n0\nlog ( \u03c1(p) ) \u03c1(p)dp \u2265 \u222b 1 0 log ( f(p) ) \u03c1(p)dp.\nProof of Lemma 1. It is well known (Kullback, 1959, p. 14) that the Kullback\u2013Leibler divergence is always non-negative: \u222b 1\n0\nlog ( \u03c1(p) f(p) ) \u03c1(p)dp \u2265 0.\nThis is equivalent to the inequality asserted by Lemma 1.\nProof of Theorem 2. Suppose that, contrary to the statement of Theorem 2, there exists \u03b4 > 0 such that\nlim inf n\u2192\u221e\n( 1\nn n\u2211 i=1 log ( fi(pi) ) \u2212 1 n n\u2211 i=1 log ( f(pi)\n)) < \u2212\u03b4.\n(5) Then choose an \u03b5 satisfying 0 < \u03b5 < \u03b4/4.\nSubstituting the definition of \u03c1(p) into Lemma 1 we obtain \u222b 1\n0\nlog ( \u03c1(p) ) dP \u2265 \u222b 1 0 log ( f(p) ) dP. (6)\nFrom the stability of (p1, p2, . . .) it follows that there exists a number N1 = N1(\u03b5) such that, for all n > N1,\u2223\u2223\u2223\u2223\u222b 1\n0\nlog ( f(p) ) dPn \u2212 \u222b 1 0 log ( f(p) ) dP \u2223\u2223\u2223\u2223 < \u03b5 and \u2223\u2223\u2223\u2223\u222b 1\n0\nlog ( \u03c1(p) ) dPn \u2212 \u222b 1 0 log ( \u03c1(p) ) dP \u2223\u2223\u2223\u2223 < \u03b5. Then inequality (6) implies that, for all n \u2265 N1,\u222b 1\n0\nlog ( \u03c1(p) ) dPn \u2265 \u222b 1 0 log ( f(p) ) dPn \u2212 2\u03b5.\nBy the definition of the probability measure Pn, the last inequality is the same thing as\n1\nn n\u2211 i=1 log ( \u03c1(pi) ) \u2265 1 n n\u2211 i=1 log ( f(pi) ) \u2212 2\u03b5. (7)\nBy the consistency of ( f1(p), f2(p), . . . ) there exists a number N2 = N2(\u03b5) such that, for all i > N2 and all p \u2208 [0, 1], \u2223\u2223\u2223log(fi(p))\u2212 log(\u03c1(p))\u2223\u2223\u2223 < \u03b5. (8)\nLet us define the number\nM = max i,p \u2223\u2223log(fi(p))\u2212 log(\u03c1(p))\u2223\u2223. (9) From (8) and (9) we have\n\u2223\u2223log(fi(p))\u2212 log(\u03c1(p))\u2223\u2223 \u2264 { M, i \u2264 N2\u03b5, i > N2. (10) Denote N3 = max(N1, N2). Then, using (10) and (7), we obtain, for all n > N3,\n1\nn n\u2211 i=1 log ( fi(pi) ) \u2265 1 n n\u2211 i=1 log ( f(pi) ) \u2212 3\u03b5\u2212 MN3 n .\nDenoting N4 = max(N3, MN3 \u03b5 ), we can rewrite the last inequality as\n1\nn n\u2211 i=1 log ( fi(pi) ) \u2265 1 n n\u2211 i=1 log ( f(pi) ) \u2212 4\u03b5,\nfor all n > N4. Finally, recalling that \u03b5 < \u03b4 4 , we have, for all n > N4,\n1\nn n\u2211 i=1 log ( fi(pi) ) \u2212 1 n n\u2211 i=1 log ( f(pi) ) \u2265 \u2212\u03b4.\nThis contradicts (5) and therefore completes the proof of Theorem 2."}, {"heading": "4. Empirical results", "text": "In this section we investigate the performance of our plug-in martingale and compare it with that of the simple mixture martingale. Two real-life datasets have been tested for exchangeability: the USPS dataset and the Statlog Satellite dataset."}, {"heading": "4.1. USPS dataset", "text": "Data The US Postal Service (USPS) dataset consists of 7291 training examples and 2007 test examples of handwritten digits, from 0 to 9. The data were collected from real-life zip codes. Each example is described by the 256 attributes representing the pixels for displaying a digit on the 16\u00d7 16 gray-scaled image and its label. It is well known that the examples in this dataset are not perfectly exchangeable (Vovk et al., 2003), and any reasonable test should reject exchangeability there. In our experiments we merge the training and test sets and perform testing for the full dataset of 9298 examples.\nFigure 2 shows the typical performance of the martingales when the exchangeability assumption is satisfied\nfor sure: all examples have been randomly shuffled before the testing.\nFigure 4 shows the performance of the martingales when the examples arrive in the original order: first 7291 of the training set and then 2007 of the test set. The p-values are generated on-line by Algorithm 1 and the two martingales are calculated from the same sequence of p-values. The final value for the simple mixture martingale is 2.0 \u00d7 1010, and the final value for the plug-in martingale is 3.9\u00d7 108.\nFigure 6 shows the betting functions that correspond to the plug-in martingale and the \u201cbest\u201d power martingale. For the plug-in martingale, the function is the estimated probability density function calculated using the whole sequence of p-values. The betting function for the family of power martingale corresponds to the parameter \u03b5\u2217 that provides the largest final value among all power martingales. It gives a clue why we could not see advantages of the new approach for this dataset: both martingales grew up to approximately the same level. There is not much difference between the best betting functions for the old and new methods, and the new method suffers because of its greater flexibility."}, {"heading": "4.2. Statlog Satellite dataset", "text": "Data The Statlog Satellite dataset (Frank & Asuncion, 2010) consists of 6435 satellite images (divided into 4435 training examples and 2000 test examples). The examples are 3\u00d7 3 pixel sub-areas of the satellite picture, where each pixel is described by four spectral\nvalues in different spectral bands. Each example is represented by 36 attributes and a label indicating the classification of the central pixel. Labels are numbers from 1 to 7, excluding 6. The testing results are described below.\nFigure 3 shows the performance of the martingales for randomly shuffled examples of the dataset. As expected, the martingales do not reject the exchangeability assumption there.\nFigure 5 presents the performance of the martingales when the examples arrive in the original order. The final value for the simple mixture martingale is 5.6\u00d7102 and the final value for the plug-in martingale is 1.8\u00d7 1017. Again, the corresponding betting functions for the plug-in martingale and the \u201cbest\u201d power martingale are presented in Figure 7. For this dataset the generated p-values have a tricky distribution. The family of power betting functions \u03b5p\u03b5\u22121 cannot provide a good approximation. The power martingales lose on p-values close to the second peak of the p-values distribution. But the plug-in martingale is more flexible and ends up with a much higher final value.\nIt can be argued that both methods, old and new, work for the Statlog Satellite dataset in the sense of rejecting the exchangeability assumption at any of the commonly used thresholds (such as 20 or 100). However, the situation would have been different had the dataset consisted of only the first 1000 examples: the final value of the simple mixture martingale would have been 0.013 whereas the final value of the plug-in martingale would have been 3.74\u00d7 1015."}, {"heading": "5. Discussion and conclusions", "text": "In this paper we have introduced a new way of constructing martingales for testing exchangeability online. We have shown that for stable sequences of p-values the new more adaptive martingale provides asymptotically the best result compared with any other martingale with a fixed betting function. The experiments of testing two real-life datasets have been presented. Using the same sequence of p-values the plug-in martingale extracts approximately the same amount or more information about the data-generating distribution as compared to the previously introduced power martingales.\nRemark. The previous studies were based on the natural idea that lack of exchangeability leads to new examples looking strange as compared to the old ones and therefore to small p-values (for example, if the datagenerating mechanism changes its regime and starts producing a different kind of examples). This is, however, a situation where lack of exchangeability makes the p-values cluster around 1: we observe examples that are ideal shapes of several kinds distorted by random noise, and the amount of noise decreases with time. Predicting the kind of a new example using the nonconformity measure (1) will then tend to produce large p-values.\nOur goal has been to find an exchangeability martingale that does not need any assumptions about the p-values generated by the method of conformal prediction. Our proposed martingale adapts to the unknown distribution of the p-values by estimating a good bet-\nting function from the past data. This is an example of the plug-in approach. It is generally believed that the Bayesian approach is more efficient than the plug-in approach (see, e.g., Bernardo & Smith, 2000, p. 483). In our present context, the Bayesian approach would involve choosing a prior distribution on the betting functions and integrating the exchangeability martingales corresponding to these betting functions over the prior distribution. It is not clear yet whether this can be done efficiently and, if yes, whether this can improve the performance of exchangeability martingales."}, {"heading": "Acknowledgments", "text": "We are indebted to Royal Holloway, University of London, for continued support and funding. This work has also been supported by the EraSysBio+ grant SHIPREC from the European Union, BBSRC and BMBF and by the VLA grant on machine learning algorithms.\nWe thank all reviewers for their valuable suggestions for improving the paper."}], "references": [{"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Information Theory and Statistics", "author": ["S. Kullback"], "venue": null, "citeRegEx": "Kullback,? \\Q1959\\E", "shortCiteRegEx": "Kullback", "year": 1959}, {"title": "Etude critique de la notion de collectif", "author": ["J. Ville"], "venue": "GauthierVillars, Paris,", "citeRegEx": "Ville,? \\Q1939\\E", "shortCiteRegEx": "Ville", "year": 1939}, {"title": "Testing exchangeability on-line", "author": ["V. Vovk", "I. Nouretdinov", "A. Gammerman"], "venue": "In Proceedings of the 20th International Conference on Machine Learning (ICML", "citeRegEx": "Vovk et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vovk et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "To address this challenge a previous study (Vovk et al., 2003) suggested a way of on-line testing by employing the theory of conformal prediction and calculating exchangeability martingales.", "startOffset": 43, "endOffset": 62}, {"referenceID": 3, "context": "The first procedure of testing exchangeability on-line is described in Vovk et al. (2003). The core testing mechanism is an exchangeability martingale.", "startOffset": 71, "endOffset": 90}, {"referenceID": 3, "context": "The first procedure of testing exchangeability on-line is described in Vovk et al. (2003). The core testing mechanism is an exchangeability martingale. Exchangeability martingales are constructed using a sequence of p-values. The algorithm for generating p-values assigns small p-values to unusual examples. It implies the idea of designing martingales that would have a large value if too many small p-values were generated, and suggests corresponding power martingales. Other martingales (simple mixture and sleepy jumper) implement more complicated strategies, but follow the same idea of scoring on small p-values. Ho (2005) applies power martingales to the problem of change detection in time-varying data streams.", "startOffset": 71, "endOffset": 629}, {"referenceID": 2, "context": "As in Vovk et al. (2003), the main tool for testing exchangeability on-line is a martingale.", "startOffset": 6, "endOffset": 25}, {"referenceID": 3, "context": "Previous studies (Vovk et al., 2003) have proposed to use a fixed betting function \u2200i : fi(p) = \u03b5p\u03b5\u22121, where \u03b5 \u2208 [0, 1].", "startOffset": 17, "endOffset": 36}, {"referenceID": 3, "context": "It is well known that the examples in this dataset are not perfectly exchangeable (Vovk et al., 2003), and any reasonable test should reject exchangeability there.", "startOffset": 82, "endOffset": 101}], "year": 2012, "abstractText": "A standard assumption in machine learning is the exchangeability of data, which is equivalent to assuming that the examples are generated from the same probability distribution independently. This paper is devoted to testing the assumption of exchangeability on-line: the examples arrive one by one, and after receiving each example we would like to have a valid measure of the degree to which the assumption of exchangeability has been falsified. Such measures are provided by exchangeability martingales. We extend known techniques for constructing exchangeability martingales and show that our new method is competitive with the martingales introduced before. Finally we investigate the performance of our testing method on two benchmark datasets, USPS and Statlog Satellite data; for the former, the known techniques give satisfactory results, but for the latter our new more flexible method becomes necessary.", "creator": "LaTeX with hyperref package"}}}