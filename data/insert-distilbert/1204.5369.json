{"id": "1204.5369", "review": {"conference": "ACL", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2012", "title": "Ecological Evaluation of Persuasive Messages Using Google AdWords", "abstract": "in recent these years there has been a growing public interest in suggesting crowdsourcing methodologies to be openly used in experimental research for nlp tasks. in particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. in this paper ) we present a new cheap and fast methodology that allows multiple fast experiment building and evaluation with fully - automated analysis at a low cost. the central idea is exploiting existing commercial tools for advertising on the web, such as google adwords, to measure user message impact in an ecological setting. the academic paper includes a description concept of the approach, tips for how strategies to use adwords recognition for conducting scientific research, and a results of seven pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach.", "histories": [["v1", "Tue, 24 Apr 2012 13:22:44 GMT  (81kb,DS)", "http://arxiv.org/abs/1204.5369v1", "To appear at ACL 2012. 9 pages, 2 figures"]], "COMMENTS": "To appear at ACL 2012. 9 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL cs.SI", "authors": ["marco guerini", "carlo strapparava", "oliviero stock"], "accepted": true, "id": "1204.5369"}, "pdf": {"name": "1204.5369.pdf", "metadata": {"source": "CRF", "title": "Ecological Evaluation of Persuasive Messages Using Google AdWords", "authors": ["Marco Guerini", "Carlo Strapparava"], "emails": ["marco.guerini@trentorise.eu", "strappa@fbk.eu", "stock@fbk.eu"], "sections": [{"heading": null, "text": "To appear at ACL 2012"}, {"heading": "1 Introduction", "text": "In recent years there has been a growing interest in finding new cheap and fast methodologies to be used in experimental research, for, but not limited to, NLP tasks. In particular, approaches to NLP that rely on the use of web tools - for crowdsourcing long and tedious tasks - have emerged. Amazon Mechanical Turk, for example, has been used for collecting annotated data (Snow et al., 2008). However approaches a la Mechanical Turk might not be suitable for all tasks.\nIn this paper we focus on evaluating systems and theories about persuasion, see for example (Fogg,\n2009) or the survey on persuasive NL generation studies in (Guerini et al., 2011a). Measuring the impact of a message is of paramount importance in this context, for example how affective text variations can alter the persuasive impact of a message.\nThe problem is that evaluation experiments represent a bottleneck: they are expensive and time consuming, and recruiting a high number of human participants is usually very difficult.\nTo overcome this bottleneck, we present a specific cheap and fast methodology to automatize largescale evaluation campaigns. This methodology allows us to crowdsource experiments with thousands of subjects for a few euros in a few hours, by tweaking and using existing commercial tools for advertising on the web. In particular we make reference to the AdWords Campaign Experiment (ACE) tool provided within the Google AdWords suite. One important aspect of this tool is that it allows for realtime fully-automated data analysis to discover statistically significant phenomena. It is worth noting that this work originated in the need to evaluate the impact of short persuasive messages, so as to assess the effectiveness of different linguistic choices. Still, we believe that there is further potential for opening an interesting avenue for experimentally exploring other aspects of the wide field of pragmatics.\nThe paper is structured as follows: Section 2 discusses the main advantages of ecological approaches using Google ACE over traditional lab settings and state-of-the-art crowdsourcing methodologies. Section 3 presents the main AdWords features. Section 4 describes how AdWords features can be used for defining message persuasiveness metrics and what\nar X\niv :1\n20 4.\n53 69\nv1 [\ncs .C\nL ]\n2 4\nA pr\nkind of stimulus characteristics can be evaluated. Finally Sections 5 and 6 describe how to build up an experimental scenario and some pilot studies to test the feasibility of our approach."}, {"heading": "2 Advantages of Ecological Approaches", "text": "Evaluation of the effectiveness of persuasive systems is very expensive and time consuming, as the STOP experience showed (Reiter et al., 2003): designing the experiment, recruiting subjects, making them take part in the experiment, dispensing questionnaires, gathering and analyzing data.\nExisting methodologies for evaluating persuasion are usually split in two main sets, depending on the setup and domain: (i) long-term, in the field evaluation of behavioral change (as the STOP example mentioned before), and (ii) lab settings for evaluating short-term effects, as in (Andrews et al., 2008). While in the first approach it is difficult to take into account the role of external events that can occur over long time spans, in the second there are still problems of recruiting subjects and of time consuming activities such as questionnaire gathering and processing.\nIn addition, sometimes carefully designed experiments can fail because: (i) effects are too subtle to be measured with a limited number of subjects or (ii) participants are not engaged enough by the task to provoke usable reactions, see for example what reported in (Van Der Sluis and Mellish, 2010). Especially the second point is awkward: in fact, subjects can actually be convinced by the message to which they are exposed, but if they feel they do not care, they may not \u201creact\u201d at all, which is the case in many artificial settings. To sum up, the main problems are:\n1. Time consuming activities 2. Subject recruitment 3. Subject motivation 4. Subtle effects measurements"}, {"heading": "2.1 Partial Solution - Mechanical Turk", "text": "A recent trend for behavioral studies that is emerging is the use of Mechanical Turk (Mason and Suri, 2010) or similar tools to overcome part of these limitations - such as subject recruitment. Still we believe\nthat this poses other problems in assessing behavioral changes, and, more generally, persuasion effects. In fact:\n1. Studies must be as ecological as possible, i.e. conducted in real, even if controlled, scenarios.\n2. Subjects should be neither aware of being observed, nor biased by external rewards.\nIn the case of Mechanical Turk for example, subjects are willingly undergoing a process of being tested on their skills (e.g. by performing annotation tasks). Cover stories can be used to soften this awareness effect, nonetheless the fact that subjects are being paid for performing the task renders the approach unfeasible for behavioral change studies. It is necessary that the only reason for behavior induction taking place during the experiment (filling a form, responding to a questionnaire, clicking on an item, etc.) is the exposition to the experimental stimuli, not the external reward. Moreover, Mechanical Turk is based on the notion of a \u201cgold standard\u201d to assess contributors reliability, but for studies concerned with persuasion it is almost impossible to define such a reference: there is no \u201cright\u201d action the contributor can perform, so there is no way to assess whether the subject is performing the action because induced to do so by the persuasive strategy, or just in order to receive money. On the aspect of how to handle subject reliability in coding tasks, see for example the method proposed in (Negri et al., 2010)."}, {"heading": "2.2 Proposed Solution - Targeted Ads on the Web", "text": "Ecological studies (e.g. using Google AdWords) offer a possible solution to the following problems:\n1. Time consuming activities: apart from experimental design and setup, all the rest is automatically performed by the system. Experiments can yield results in a few hours as compared to several days/weeks. 2. Subject recruitment: the potential pool of subjects is the entire population of the web. 3. Subject motivation: ads can be targeted exactly to those persons that are, in that precise moment throughout the world, most interested in the topic of the experiment, and so potentially more prone to react.\n4. Subject unaware, unbiased: subjects are totally unaware of being tested, testing is performed during their \u201cnatural\u201d activity on the web. 5. Subtle effects measurements: if the are not enough subjects, just wait for more ads to be displayed, or focus on a subset of even more interested people.\nNote that similar ecological approaches are beginning to be investigated: for example in (Aral and Walker, 2010) an approach to assessing the social effects of content features on an on-line community is presented. A previous approach that uses AdWords was presented in (Guerini et al., 2010), but it crowdsourced only the running of the experiment, not data manipulation and analysis, and was not totally controlled for subject randomness."}, {"heading": "3 AdWords Features", "text": "Google AdWords is Google\u2019s advertising program. The central idea is to let advertisers display their messages only to relevant audiences. This is done by means of keyword-based contextualization on the Google network, divided into:\n\u2022 Search network: includes Google search pages, search sites and properties that display search results pages (SERPs), such as Froogle and Earthlink. \u2022 Display network: includes news pages, topicspecific websites, blogs and other properties - such as Google Mail and The New York Times.\nWhen a user enters a query like \u201ccruise\u201d in the Google search network, Google displays a variety of relevant pages, along with ads that link to cruise trip businesses. To be displayed, these ads must be associated with relevant keywords selected by the advertiser.\nEvery advertiser has an AdWords account that is structured like a pyramid: (i) account, (ii) campaign and (iii) ad group. In this paper we focus on ad groups. Each grouping gathers similar keywords together - for instance by a common theme - around an ad group. For each ad group, the advertiser sets a cost-per-click (CPC) bid. The CPC bid refers to the amount the advertiser is willing to pay for a click on his ad; the cost of the actual click instead is based\non its quality score (a complex measure out of the scope of the present paper).\nFor every ad group there could be multiple ads to be served, and there are many AdWords measurements for identifying the performance of each single ad (its persuasiveness, from our point of view):\n\u2022 CTR, Click Through Rate: measures the number of clicks divided by the number of impressions (i.e. the number of times an ad has been displayed in the Google Network). \u2022 Conversion Rate: if someone clicks on an ad, and buys something on your site, that click is a conversion from a site visit to a sale. Conversion rate equals the number of conversions divided by the number of ad clicks. \u2022 ROI: Other conversions can be page views or signups. By assigning a value to a conversion the resulting conversions represents a return on investment, or ROI. \u2022 Google Analytics Tool: Google Analytics is a web analytics tool that gives insights into website traffic, like number of visited pages, time spent on the site, location of visitors, etc.\nSo far, we have been talking about text ads, - Google\u2019s most traditional and popular ad format - because they are the most useful for NLP analysis. In addition there is also the possibility of creating the following types of ads:\n\u2022 Image (and animated) ads \u2022 Video ads \u2022 Local business ads \u2022 Mobile ads\nThe above formats allow for a greater potential to investigate persuasive impact of messages (other than text-based) but their use is beyond the scope of the present paper1."}, {"heading": "4 The ACE Tool", "text": "AdWords can be used to design and develop various metrics for fast and fully-automated evaluation experiments, in particular using the ACE tool.\nThis tool - released in late 2010 - allows testing, from a marketing perspective, if any change made to\n1For a thorough description of the AdWords tool see: https://support.google.com/adwords/\na promotion campaign (e.g. a keyword bid) had a statistically measurable impact on the campaign itself. Our primary aim is slightly different: we are interested in testing how different messages impact (possibly different) audiences. Still the ACE tool goes exactly in the direction we aim at, since it incorporates statistically significant testing and allows avoiding many of the tweaking and tuning actions which were necessary before its release.\nThe ACE tool also introduces an option that was not possible before, that of real-time testing of statistical significance. This means that it is no longer necessary to define a-priori the sample size for the experiment: as soon as a meaningful statistically significant difference emerges, the experiment can be stopped.\nAnother advantage is that the statistical knowledge to evaluate the experiment is no longer necessary: the researcher can focus only on setting up proper experimental designs2.\nThe limit of the ACE tool is that it only allows A/B testing (single split with one control and one experimental condition) so for experiments with more than two conditions or for particular experimental settings that do not fit with ACE testing boundaries (e.g. cross campaign comparisons) we suggest taking (Guerini et al., 2010) as a reference model, even if the experimental setting is less controlled (e.g. subject randomness is not equally guaranteed as with ACE).\nFinally it should be noted that even if ACE allows only A/B testing, it permits the decomposition of almost any variable affecting a campaign experiment in its basic dimensions, and then to segment such dimensions according to control and experimental conditions. As an example of this powerful option, consider Tables 3 and 6 where control and experimental conditions are compared against every single keyword and every search network/ad position used for the experiments."}, {"heading": "5 Evaluation and Targeting with ACE", "text": "Let us consider the design of an experiment with 2 conditions. First we create an ad Group with 2 competing messages (one message for each condition).\n2Additional details about ACE features and statistics can be found at http://www.google.com/ads/innovations/ace.html\nThen we choose the serving method (in our opinion the rotate option is better than optimize, since it guarantees subject randomness and is more transparent) and the context (language, network, etc.). Then we activate the ads and wait. As soon as data begins to be collected we can monitor the two conditions according to:\n\u2022 Basic Metrics: the highest CTR measure indicates which message is best performing. It indicates which message has the highest initial impact. \u2022 Google Analytics Metrics: measures how much the messages kept subjects on the site and how many pages have been viewed. Indicates interest/attitude generated in the subjects. \u2022 Conversion Metrics: measures how much the messages converted subjects to the final goal. Indicates complete success of the persuasive message. \u2022 ROI Metrics: by creating specific ROI values for every action the user performs on the landing page. The more relevant (from a persuasive point of view) the action the user performs, the higher the value we must assign to that action. In our view combined measurements are better: for example, there could be cases of messages with a lower CTR but a higher conversion rate.\nFurthermore, AdWords allows very complex targeting options that can help in many different evaluation scenarios:\n\u2022 Language (see how message impact can vary in different languages). \u2022 Location (see how message impact can vary in different cultures sharing the same language). \u2022 Keyword matching (see how message impact can vary with users having different interests). \u2022 Placements (see how message impact can vary among people having different values - e.g. the same message displayed on Democrat or Republican web sites). \u2022 Demographics (see how message impact can vary according to user gender and age)."}, {"heading": "5.1 Setting up an Experiment", "text": "To test the extent to which AdWords can be exploited, we focused on how to evaluate lexical variations of a message. In particular we were interested\nin gaining insights about a system for affective variations of existing commentaries on medieval frescoes for a mobile museum guide that attracts the attention of visitors towards specific paintings (Guerini et al., 2008; Guerini et al., 2011b). The various steps for setting up an experiment (or a series of experiments) are as follows:\nChoose a Partner. If you have the opportunity to have a commercial partner that already has the infrastructure for experiments (website, products, etc.) many of the following steps can be skipped. We assume that this is not the case.\nChoose a scenario. Since you may not be equipped with a VAT code (or with the commercial partner that furnishes the AdWords account and infrastructure), you may need to \u201cinvent something to promote\u201d without any commercial aim. If a \u201csocial marketing\u201d scenario is chosen you can select \u201cpersonal\u201d as a \u201ctax status\u201d, that do not require a VAT code. In our case we selected cultural heritage promotion, in particular the frescoes of Torre Aquila (\u201cEagle Tower\u201d) in Trento. The tower contains a group of 11 frescoes named \u201cCiclo dei Mesi\u201d (cycle of the months) that represent a unique example of non-religious medieval frescoes in Europe.\nChoose an appropriate keyword on which to advertise, \u201cmedieval art\u201d in our case. It is better to choose keywords with enough web traffic in order to speed up the experimental process. In our case the search volume for \u201cmedieval art\u201d (in phrase match) was around 22.000 hits per month. Another suggestion is to restrict the matching modality on Keywords in order to have more control over the situations in which ads are displayed and to avoid possible extraneous effects (the order of control for matching modality is: [exact match], \u201cphrase match\u201d and broad match).\nNote that such a technical decision - which keyword to use - is better taken at an early stage of development because it affects the following steps.\nWrite messages optimized for that keyword (e.g. including it in the title or the body of the ad). Such optimization must be the same for control and experimental condition. The rest of the ad can be designed in such a way to meet control and experimental condition design (in our case a message with slightly affective terms and a similar message with more affectively loaded variations)\nBuild an appropriate landing page, according to the keyword and populate the website pages with relevant material. This is necessary to create a \u201ccredible environment\u201d for users to interact with.\nIncorporate meaningful actions in the website. Users can perform various actions on a site, and they can be monitored. The design should include actions that are meaningful indicators of persuasive effect/success of the message. In our case we decided to include some outbound links, representing:\n\u2022 general interest: \u201cBuonconsiglio Castle site\u201d \u2022 specific interest: \u201cEagle Tower history\u201d \u2022 activated action: \u201cTimetable and venue\u201d \u2022 complete success: \u201cBook a visit\u201d\nFurthermore, through new Google Analytics features, we set up a series of time spent on site and number of visited pages thresholds to be monitored in the ACE tool."}, {"heading": "5.2 Tips for Planning an Experiment", "text": "There are variables, inherent in the Google AdWords mechanism, that from a research point of view we shall consider \u201cextraneous\u201d. We now propose tips for controlling such extraneous variables.\nAdd negative matching Keywords: To add more control, if in doubt, put the words/expressions of the control and experimental conditions as negative keywords. This will prevent different highlighting between the two conditions that can bias the results. It is not strictly necessary since one can always control which queries triggered a click through the report menu. An example: if the only difference between control and experimental condition is the use of the adjectives \u201cgentle knights\u201d vs. \u201cvalorous knights\u201d, one can use two negative keyword matches: -gentle and -valorous. Obviously if you are using a keyword in exact matching to trigger your ads, such as [knight], this is not necessary.\nFrequency capping for the display network: if you are running ads on the display network, you can use the \u201cfrequency capping\u201d option set to 1 to add more control to the experiment. In this way it is assured that ads are displayed only one time per user on the display network.\nPlacement bids for the search network: unfortunately this option is no longer available. Basically the option allowed to bid only for certain positions\non the SERPs to avoid possible \u201cextraneous variables effect\u201d given by the position. This is best explained via an example: if, for whatever reason, one of the two ads gets repeatedly promoted to the premium position on the SERPs, then the CTR difference between ads would be strongly biased. From a research point of view \u201cpremium position\u201d would then be an extraneous variable to be controlled (i.e. either both ads get an equal amount of premium position impressions, or both ads get no premium position at all). Otherwise the difference in CTR is determined by the \u201cpremium position\u201d rather than by the independent variable under investigation (presence/absence of particular affective terms in the text ad). However even if it is not possible to rule out this \u201cposition effect\u201d it is possible to monitor it by using the report (Segment > Top vs. other + Experiment) and checking how many times each ad appeared in a given position on the SERPs, and see if the ACE tool reports any statistical difference in the frequencies of ads positions.\nExtra experimental time: While planning an experiment, you should also take into account the ads reviewing time that can take up to several days, in worst case scenarios. Note that when ads are in eligible status, they begin to show on the Google Network, but they are not approved yet. This means that the ads can only run on Google search pages and can only show for users who have turned off SafeSearch filtering, until they are approved. Eligible ads cannot run on the Display Network. This status will provide much less impressions than the final \u201capproved\u201d status.\nAvoid seasonal periods: for the above reason, and to avoid extra costs due to high competition, avoid seasonal periods (e.g. Christmas time).\nDelivery method: if you are planning to use the Accelerated Delivery method in order to get the results as quick as possible (in the case of \u201cquick and dirty\u201d experiments or \u201cfast prototyping-evaluation cycles\u201d) you should consider monitoring your experiment more often (even several times per day) to avoid running out of budget during the day."}, {"heading": "6 Experiments", "text": "We ran two pilot experiments to test how affective variations of existing texts alter their persuasive im-\npact. In particular we were interested in gaining initial insights about an intelligent system for affective variations of existing commentaries on medieval frescoes.\nWe focused on adjective variations, using a slightly biased adjective for the control conditions and a strongly biased variation for the experimental condition. In these experiments we took it for granted that affective variations of a message work better than a neutral version (Van Der Sluis and Mellish, 2010), and we wanted to explore more finely grained tactics that involve the grade of the variation (i.e. a moderately positive variation vs. an extremely positive variation). Note that this is a more difficult task than the one proposed in (Van Der Sluis and Mellish, 2010), where they were testing long messages with lots of variations and with polarized conditions, neutral vs. biased. In addition we wanted to test how quickly experiments could be performed (two days versus the two week suggestion of Google).\nAdjectives were chosen according to MAX bigram frequencies with the modified noun, using the Web 1T 5-gram corpus (Brants and Franz, 2006). Deciding whether this is the best metric for choosing adjectives to modify a noun or not (e.g. also pointwise mutual-information score can be used with a different rationale) is out of the scope of the present paper, but previous work has already used this approach (Whitehead and Cavedon, 2010). Top ranked adjectives were then manually ordered - according to affective weight - to choose the best one (we used a standard procedure using 3 annotators and a reconciliation phase for the final decision)."}, {"heading": "6.1 First Experiment", "text": "The first experiment lasted 48 hour with a total of 38 thousand subjects and a cost of 30 euros (see Table 1 for the complete description of the experimental setup). It was meant to test broadly how affective variations in the body of the ads performed. The two variations contained a fragment of a commentary of the museum guide; the control condition contained \u201cgentle knight\u201d and \u201cAfrican lion\u201d, while in the experimental condition the affective loaded variations were \u201cvalorous knight\u201d and \u201cindomitable lion\u201d (see Figure 1, for the complete ads). As can be seen from Table 2, the experiment did not yield any significant\nresult, if one looks at the overall analysis. But segmenting the results according to the keyword that triggered the ads (see Table 3) we discovered that on the \u201cmedieval art\u201d keyword, the control condition performed better than the experimental one.\nDiscussion. As already discussed, user motivation is a key element for success in such finegrained experiments: while less focused keywords did not yield any statistically significant differences, the most specialized keyword \u201cmedieval art\u201d was the one that yielded results (i.e. if we display messages like those in Figure 1, that are concerned with medieval art frescoes, only those users really interested in the topic show different reaction patterns to the affective variations, while those generically interested in medieval times behave similarly in the two conditions). In the following experiment we tried to see\nwhether such variations have different effects when modifying a different element in the text."}, {"heading": "6.2 Second Experiment", "text": "The second experiment lasted 48 hours with a total of one thousand subjects and a cost of 17 euros (see Table 4 for the description of the experimental setup). It was meant to test broadly how affective variations introduced in the title of the text Ads performed. The two variations were the same as in the first experiment for the control condition \u201cgentle knight\u201d, and for the experimental condition \u201cvalorous knight\u201d (see Figure 2 for the complete ads). As can be seen from Table 5, also in this case the experiment did not yield any significant result, if one looks at the overall analysis. But segmenting the results according to the search network that triggered the ads (see Table 6) we discovered that on the search partners at the \u201cother\u201d position, the control condition performed better than the experimental one. Unlike the first experiment, in this case we segmented according to the ad position and search network typology since we were running our experiment only on one keyword in exact match.\nDiscussion. From this experiment we can confirm that at least under some circumstances a mild affective variation performs better than a strong variation. This mild variations seems to work better when user attention is high (the difference emerged when ads are displayed in a non-prominent position). Furthermore it seems that modifying the title of the ad rather than the content yields better results: 0.9% vs. 1.83% CTR (\u03c72 = 6.24; 1 degree of freedom; \u03b1 < 0,01) even if these results require further assessment with dedicated experiments.\nAs a side note, in this experiment we can see the problem of extraneous variables: according to AdWords\u2019 internal mechanisms, the experimental condition was displayed more often in the Google\nsearch Network on the \u201cother\u201d position (277 vs. 219 impressions - and overall 524 vs. 462), still from a research perspective this is not a interesting statistical difference, and ideally should not be present (i.e. ads should get an equal amount of impressions for each position).\nConclusions and future work\nAdWords gives us an appropriate context for evaluating persuasive messages. The advantages are fast experiment building and evaluation, fully-automated analysis, and low cost. By using keywords with a low CPC it is possible to run large-scale experiments for just a few euros. AdWords proved to be very accurate, flexible and fast, far beyond our expectations. We believe careful design of experiments will yield important results, which was unthinkable before this opportunity for studies on persuasion appeared.\nThe motivation for this work was exploration of the impact of short persuasive messages, so to assess the effectiveness of different linguistic choices. The experiments reported in this paper are illustrative examples of the method proposed and are concerned with the evaluation of the role of minimal affective variations of short expressions. But there is enormous further potential in the proposed approach to ecological crowdsourcing for NLP: for instance, different rhetorical techniques can be checked in practice with large audiences and fast feedback. The assessment of the effectiveness of a change in the title as opposed to the initial portion of the text body provides a useful indication: one can investigate if variations inside the given or the new part of an expression or in the topic vs. comment (Levinson, 1983) have different effects. We believe there is potential for a concrete extensive exploration of different linguistic theories in a way that was simply not realistic before."}, {"heading": "Acknowledgments", "text": "We would like to thank Enrique Alfonseca and Steve Barrett, from Google Labs, for valuable hints and discussion on AdWords features. The present work was partially supported by a Google Research Award."}], "references": [{"title": "Argumentative human computer dialogue for automated persuasion", "author": ["Andrews et al.2008] P. Andrews", "S. Manandhar", "M. De Boni"], "venue": "In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue,", "citeRegEx": "Andrews et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2008}, {"title": "Creating social contagion through viral product design: A randomized trial of peer influence in networks", "author": ["Aral", "Walker2010] S. Aral", "D. Walker"], "venue": "In Proceedings of the 31th Annual International Conference on Information Systems", "citeRegEx": "Aral et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Aral et al\\.", "year": 2010}, {"title": "Web 1t 5-gram corpus version 1.1", "author": ["Brants", "Franz2006] T. Brants", "A. Franz"], "venue": null, "citeRegEx": "Brants et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Brants et al\\.", "year": 2006}, {"title": "Creating persuasive technologies: An eight-step design process", "author": ["BJ Fogg"], "venue": "Proceedings of the 4th International Conference on Persuasive Technology", "citeRegEx": "Fogg.,? \\Q2009\\E", "shortCiteRegEx": "Fogg.", "year": 2009}, {"title": "Valentino: A tool for valence shifting of natural language texts", "author": ["Guerini et al.2008] M. Guerini", "O. Stock", "C. Strapparava"], "venue": "In Proceedings of LREC", "citeRegEx": "Guerini et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2008}, {"title": "Evaluation metrics for persuasive nlp with google adwords", "author": ["Guerini et al.2010] M. Guerini", "C. Strapparava", "O. Stock"], "venue": "In Proceedings of LREC-2010", "citeRegEx": "Guerini et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2010}, {"title": "Approaches to verbal persuasion in intelligent user interfaces", "author": ["Guerini et al.2011a] M. Guerini", "O. Stock", "M. Zancanaro", "D.J. O\u2019Keefe", "I. Mazzotta", "F. Rosis", "I. Poggi", "M.Y. Lim", "R. Aylett"], "venue": "EmotionOriented Systems,", "citeRegEx": "Guerini et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2011}, {"title": "Slanting existing text with Valentino", "author": ["Guerini et al.2011b] M. Guerini", "C. Strapparava", "O. Stock"], "venue": "In Proceedings of the 16th international conference on Intelligent user interfaces,", "citeRegEx": "Guerini et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Guerini et al\\.", "year": 2011}, {"title": "Divide and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora", "author": ["Negri et al.2010] M. Negri", "L. Bentivogli", "Y. Mehdad", "D. Giampiccolo", "A. Marchetti"], "venue": "Proc. of EMNLP", "citeRegEx": "Negri et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Negri et al\\.", "year": 2010}, {"title": "Lesson from a failure: Generating tailored smoking cessation letters", "author": ["Reiter et al.2003] E. Reiter", "R. Robertson", "L. Osman"], "venue": "Artificial Intelligence,", "citeRegEx": "Reiter et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Reiter et al\\.", "year": 2003}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks", "author": ["Snow et al.2008] R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": "In Proceedings of the Conference on Empirical", "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Towards empirical evaluation of affective tactical nlg", "author": ["Van Der Sluis", "Mellish2010] I. Van Der Sluis", "C. Mellish"], "venue": "In Empirical methods in natural language generation,", "citeRegEx": "Sluis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sluis et al\\.", "year": 2010}, {"title": "Generating shifting sentiment for a conversational agent", "author": ["Whitehead", "Cavedon2010] S. Whitehead", "L. Cavedon"], "venue": "In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,", "citeRegEx": "Whitehead et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Whitehead et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Amazon Mechanical Turk, for example, has been used for collecting annotated data (Snow et al., 2008).", "startOffset": 81, "endOffset": 100}, {"referenceID": 3, "context": "In this paper we focus on evaluating systems and theories about persuasion, see for example (Fogg, 2009) or the survey on persuasive NL generation studies in (Guerini et al.", "startOffset": 92, "endOffset": 104}, {"referenceID": 9, "context": "Evaluation of the effectiveness of persuasive systems is very expensive and time consuming, as the STOP experience showed (Reiter et al., 2003): designing the experiment, recruiting subjects, making them take part in the experiment, dispensing questionnaires, gathering and analyzing data.", "startOffset": 122, "endOffset": 143}, {"referenceID": 0, "context": "ing short-term effects, as in (Andrews et al., 2008).", "startOffset": 30, "endOffset": 52}, {"referenceID": 8, "context": "ample the method proposed in (Negri et al., 2010).", "startOffset": 29, "endOffset": 49}, {"referenceID": 5, "context": "A previous approach that uses AdWords was presented in (Guerini et al., 2010), but it crowd-", "startOffset": 55, "endOffset": 77}, {"referenceID": 5, "context": "cross campaign comparisons) we suggest taking (Guerini et al., 2010) as a reference model, even if the experimental setting is less controlled (e.", "startOffset": 46, "endOffset": 68}, {"referenceID": 4, "context": "in gaining insights about a system for affective variations of existing commentaries on medieval frescoes for a mobile museum guide that attracts the attention of visitors towards specific paintings (Guerini et al., 2008; Guerini et al., 2011b).", "startOffset": 199, "endOffset": 244}], "year": 2012, "abstractText": "In recent years there has been a growing interest in crowdsourcing methodologies to be used in experimental research for NLP tasks. In particular, evaluation of systems and theories about persuasion is difficult to accommodate within existing frameworks. In this paper we present a new cheap and fast methodology that allows fast experiment building and evaluation with fully-automated analysis at a low cost. The central idea is exploiting existing commercial tools for advertising on the web, such as Google AdWords, to measure message impact in an ecological setting. The paper includes a description of the approach, tips for how to use AdWords for scientific research, and results of pilot experiments on the impact of affective text variations which confirm the effectiveness of the approach. To appear at ACL 2012", "creator": "LaTeX with hyperref package"}}}