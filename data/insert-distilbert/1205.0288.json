{"id": "1205.0288", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2012", "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning", "abstract": "we consider the problem of repeatedly learning a particular predictor by combining possibly infinitely multiple many linear predictors whose output weights are to be indirectly learned, too, an instance of multiple kernel learning. to control overfitting a group partial p - norm exclusion penalty is used to slowly penalize the empirical loss. we consider a reformulation of the problem that lets us implement a randomized version of the proximal point algorithm. the key idea of the new algorithm is to use randomized computation to alleviate the problem of dealing with possibly uncountably many predictors. finite - time performance bounds are derived that show that under equally mild conditions the method finds the optimum of the penalized criterion in an efficient manner. experimental results confirm precisely the effectiveness of the new algorithm.", "histories": [["v1", "Tue, 1 May 2012 23:42:57 GMT  (197kb,S)", "https://arxiv.org/abs/1205.0288v1", null], ["v2", "Mon, 7 Jan 2013 17:42:46 GMT  (120kb,D)", "http://arxiv.org/abs/1205.0288v2", null]], "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["arash afkanpour", "andr\u00e1s gy\u00f6rgy", "csaba szepesv\u00e1ri", "michael bowling"], "accepted": true, "id": "1205.0288"}, "pdf": {"name": "1205.0288.pdf", "metadata": {"source": "CRF", "title": "A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning", "authors": ["Arash Afkanpour", "Andr\u00e1s Gy\u00f6rgy", "Csaba Szepesv\u00e1ri", "Michael Bowling"], "emails": ["afkanpou@ualberta.ca", "gyorgy@ualberta.ca", "szepesva@ualberta.ca", "mbowling@ualberta.ca"], "sections": [{"heading": "1 Introduction", "text": "We look into the computational challenge of finding a good predictor in a multiple kernel learning (MKL) setting where the number of kernels is very large. In particular, we are interested in cases where the base kernels come from a space with combinatorial structure and thus their number d could be exponentially large. Just like some previous works (e.g. Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach that views the MKL problem as a nested, large scale convex optimization problem, where the first layer optimizes the weights of the kernels to be combined. More specifically, as the\nar X\niv :1\n20 5.\n02 88\nv2 [\ncs .L\nobjective we minimize the group p-norm penalized empirical risk. However, as opposed to these works whose underlying iterative methods have a complexity of \u2126(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richta\u0301rik and Taka\u0301c\u0302, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1). The role of randomization in our method is to use it to build an unbiased estimate of the gradient at the most recent iteration. The issue then is how the variance (and so the number of iterations required) scales with d. As opposed to the above mentioned works, in this paper we propose to make the distribution over the updated coordinate dependent on the history. We will argue that sampling from a distribution that is proportional to the magnitude of the gradient vector is desirable to keep the variance (actually, second moment) low and in fact we will show that there are interesting cases of MKL (in particular, the case of combining kernels coming from a polynomial family of kernels) when efficient sampling (i.e., sampling at a cost of O(log d)) is feasible from this distribution. Then, the variance is controlled by the a priori weights put on the kernels, making it potentially independent of d. Under these favorable conditions (and in particular, for the polynomial kernel set with some specific prior weights), the complexity of the method as a function of d becomes logarithmic, which makes our MKL algorithm feasible even for large scale problems. This is to be contrasted to the approach of Nesterov (2010, 2012) where a fixed distribution is used and where the a priori bounds on the method\u2019s convergence rate, and, hence, its computational cost to achieve a prescribed precision, will depend linearly on d (note that we are comparing upper bounds here, so the actual complexity could be smaller). Our algorithm is based on the mirror descent (or mirror descent) algorithm (similar to the work of Richta\u0301rik and Taka\u0301c\u0302 (2011) who uses uniform distributions).\nIt is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d. The algorithm of Bach (2008), though practically very efficient, suffers from the same deficiency. A very interesting proposal by Cortes et al. (2009) considers learning to combine a large number of kernels and comes with guarantees, though their algorithm restricts the family of kernels in a specific way.\nThe rest of the paper is organized as follows. The problem is defined formally in Section 2. Our new algorithm is presented and analyzed in Section 3, while its specialized version for learning polynomial kernels is given in Section 4. Finally, experiments are provided in Section 5."}, {"heading": "2 Preliminaries", "text": "In this section we give the formal definition of our problem. Let I denote a finite index set, indexing the predictors (features) to be combined, and define the set of predictors considered over the input space X as F = { fw : X \u2192 R : fw(x) = \u2211 i\u2208I \u3008wi, \u03c6i(x)\u3009 , x \u2208 X } . HereWi is a Hilbert space over the reals, \u03c6i : X \u2192 Wi is a feature-map, \u3008x, y\u3009 is the inner product over the Hilbert space that x, y belong to and w = (wi)i\u2208I \u2208 W . = \u00d7i\u2208IWi (as an example, Wi may just be a finite dimensional Euclidean space). The problem we consider is to solve\nthe optimization problem\nminimize Ln(fw) + Pen(fw) subject to w \u2208 W , (1)\nwhere Pen(fw) is a penalty that will be specified later, and Ln(fw) = 1 n \u2211n t=1 `t(fw(xt))is the empirical risk of predictor fw, defined in terms of the convex losses `t : R \u2192 R (1 \u2264 t \u2264 n) and inputs xt \u2208 X (1 \u2264 t \u2264 n). The solution w\u2217 of the above penalized empirical risk minimization problem is known to have favorable generalization properties under various conditions, see, e.g., Hastie et al. (2009). In supervised learning problems `t(y) = `(yt, y) for some loss function ` : R \u00d7 R \u2192 R, such as the squared-loss, `(yt, y) = 12(y \u2212 yt)\n2, or the hinge-loss, `t(yt, y) = max(1 \u2212 yyt, 0), where in the former case yt \u2208 R, while in the latter case yt \u2208 {\u22121,+1}. We note in passing that for the sake of simplicity, we shall sometimes abuse notation and write Ln(w) for Ln(fw) and even drop the index n when the sample-size is unimportant.\nAs mentioned above, in this paper we consider the special case in (1) when the penalty is a so-called group p-norm penalty with 1 \u2264 p \u2264 2, a case considered earlier, e.g., by Kloft et al. (2011). Thus our goal is to solve\nminimize w\u2208W\nLn(w) + 1\n2 (\u2211 i\u2208I \u03c1pi \u2016wi\u2016 p 2 ) 2 p , (2)\nwhere the scaling factors \u03c1i > 0, i \u2208 I, are assumed to be given. We introduce the notation u = (ui) \u2208 RI to denote the column vector obtained from the values ui.\nThe rationale of using the squared weighted p-norm is that for 1 \u2264 p < 2 it is expected to encourage sparsity at the group level which should allow one to handle cases when I is very large (and the case p = 2 comes for free from the same analysis). The actual form, however, is also chosen for reasons of computational convenience. In fact, the reason to use the 2-norm of the weights is to allow the algorithm to work even with infinite-dimensional feature vectors (and thus weights) by resorting to the kernel trick. To see how this works, just notice that the penalty in (2) can also be written as(\u2211\ni\u2208I \u03c1pi \u2016wi\u2016 p 2\n) 2 p\n= inf {\u2211 i\u2208I \u03c12i \u2016wi\u201622 \u03b8i : \u03b8 \u2208 \u2206 p 2\u2212p } ,\nwhere for \u03bd \u2265 1, \u2206\u03bd = {\u03b8 \u2208 [0, 1]|I| : \u2016\u03b8\u2016\u03bd \u2264 1} is the positive quadrant of the |I|-dimensional `\u03bd-ball (see, e.g., Micchelli and Pontil, 2005, Lemma 26). Hence, defining\nJ(w, \u03b8) = L(w) + 1\n2 \u2211 i\u2208I \u03c12i \u2016wi\u201622 \u03b8i\nfor any w \u2208 W, \u03b8 \u2208 [0, 1]|I|, an equivalent form of (2) is\nminimize w\u2208W,\u03b8\u2208\u2206\u03bd J(w, \u03b8) (3)\nwhere \u03bd = p/(2 \u2212 p) \u2208 [1,\u221e) and we define 0/0 = 0 and u/0 = \u221e for u > 0, which implies that wi = 0 if \u03b8i = 0. That this minimization problem is indeed equivalent to our original task (2) for the chosen value of \u03bd follows from the fact that J(w, \u03b8) is jointly convex in (w, \u03b8).1\n1Here and in what follows by equivalence we mean that the set of optimums in terms of w (the primary optimization variable) is the same in the two problems.\nLet \u03bai : X \u00d7 X \u2192 R be the reproducing kernel underlying \u03c6i: \u03bai(x, x\u2032) = \u3008\u03c6i(x), \u03c6i(x\u2032)\u3009 (x, x\u2032 \u2208 X ) and let Hi = H\u03bai the corresponding reproducing kernel Hilbert space (RKHS). Then, for any given fixed value of \u03b8, the above problem becomes an instance of a standard penalized learning problem in the RKHS H\u03b8 underlying the kernel \u03ba\u03b8 = \u2211 i\u2208I \u03b8i\u03c1 \u22122 i \u03bai. In particular, by the theorem on page 353 in Aronszajn (1950), the problem of finding w \u2208 W for fixed \u03b8 can be seen to be equivalent to minimizef\u2208H\u03b8 L(f) + 1 2\u2016f\u2016 2 H\u03b8 , and thus (2) is seen to be equivalent to minimizef\u2208H\u03b8,\u03b8\u2208\u2206\u03bd L(f) + 1 2\u2016f\u2016 2 H\u03b8 . Thus, we see that the method can be thought of as finding the weights of a kernel \u03ba\u03b8 and a predictor minimizing the H\u03b8norm penalized empirical risk. This shows that our problem is an instance of multiple kernel learning (for an exhaustive survey of MKL, see, e.g., Go\u0308nen and Alpayd\u0131n, 2011 and the references therein)."}, {"heading": "3 The new approach", "text": "When I is small, or moderate in size, the joint-convexity of J allows one to use off-the-shelf solvers to find the joint minimum of J . However, when I is large, off-the-shelf solvers might be slow or they may run out of memory. Targeting this situation we propose the following approach: Exploiting again that J(w, \u03b8) is jointly convex in (w, \u03b8), find the optimal weights by finding the minimizer of\nJ(\u03b8) . = inf\nw J(w, \u03b8),\nor, alternatively, J(\u03b8) = J(w\u2217(\u03b8), \u03b8), where w\u2217(\u03b8) . = arg minw J(w, \u03b8) (here we have slightly abused notation by reusing the symbol J). Note that J(\u03b8) is convex by the joint convexity of J(w, \u03b8). Also, note that w\u2217(\u03b8) exists and is well-defined as the minimizer of J(\u00b7, \u03b8) is unique for any \u03b8 \u2208 \u2206\u03bd (see also Proposition 3.2 below). Again, exploiting the joint convexity of J(w, \u03b8), we find that if \u03b8\u2217 is the minimizer of J(\u03b8), then w\u2217(\u03b8\u2217) will be an optimal solution to the original problem (2). To optimize J(\u03b8) we propose to use stochastic gradient descent with artificially injected randomness to avoid the need to fully evaluate the gradient of J . More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled."}, {"heading": "3.1 A randomized mirror descent algorithm", "text": "Before giving the algorithm, we need a few definitions. Let d = |I|, A \u2282 Rd be nonempty with a convex interior A\u25e6. We call the function \u03a8 : A\u2192 R a Legendre (or barrier) potential if it is strictly convex, its partial derivatives exist and are continuous, and for every sequence {xk} \u2282 A approaching the boundary of A, limk\u2192\u221e \u2016\u2207\u03a8(xk)\u2016 = \u221e. Here \u2207 is the gradient operator: \u2207\u03a8(x) = ( \u2202\u2202x\u03a8(x))\n> is the gradient of \u03a8. When \u2207 is applied to a non-smooth convex function J \u2032(\u03b8) (J may be such without additional assumptions) then \u2207J \u2032(\u03b8) is defined as any subgradient of J \u2032 at \u03b8. The corresponding Bregman-divergence D\u03a8 : A \u00d7 A\u25e6 \u2192 R is defined as D\u03a8(\u03b8, \u03b8\n\u2032) = \u03a8(\u03b8) \u2212 \u03a8(\u03b8\u2032) \u2212 \u3008\u2207\u03a8(\u03b8\u2032), \u03b8 \u2212 \u03b8\u2032\u3009. The Bregman projection \u03a0\u03a8,K : A\u25e6 \u2192 K corresponding to the Legendre potential \u03a8 and a closed convex set K \u2282 Rd such that K \u2229A 6= \u2205 is defined, for all \u03b8 \u2208 A\u25e6 as \u03a0\u03a8,K(\u03b8) = arg min\u03b8\u2032\u2208K\u2229AD\u03c8(\u03b8\u2032, \u03b8).\nAlgorithm 1 shows a randomized version of the standard mirror descent method with an unbiased gradient estimate. By assumption, \u03b7k > 0 is deterministic. Note that step 1 of the\nAlgorithm 1 Randomized mirror descent algorithm\n1: Input: A,K \u2282 Rd, where K is closed and convex with K \u2229A 6= \u2205, \u03a8 : A\u2192 R Legendre, step sizes {\u03b7k}, a subroutine, GradSampler, to sample the gradient of J at an arbitrary vector \u03b8 \u2265 0 2: Initialization: \u03b8(0) = arg min\u03b8\u2208K\u2229A \u03a8(\u03b8), k = 0. 3: repeat 4: k = k + 1. 5: Obtain g\u0302k = GradSampler(\u03b8\n(k\u22121)) 6: \u03b8\u0303(k) = arg min\u03b8\u2208A { \u03b7k\u22121\u3008g\u0302k, \u03b8\u3009+D\u03a8(\u03b8, \u03b8(k\u22121)) } . 7: \u03b8(k) = \u03a0\u03a8,K(\u03b8\u0303 (k)). 8: until convergence.\nalgorithm is well-defined since \u03b8\u0303(k) \u2208 A\u25e6 by the assumption that \u2016\u2207\u03a8(x)\u2016 tends to infinity as x approaches the boundary of A. The performance of Algorithm 1 is bounded in the next theorem. The analysis follows the standard proof technique of analyzing the mirror descent algorithm (see, e.g., Beck and Teboulle, 2003), however, in a slightly more general form than what we have found in the literature. In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richta\u0301rik and Taka\u0301c\u0302, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent. The proof is included in Section A in the appendix.\nTheorem 3.1. Assume that \u03a8 is \u03b1-strongly convex with respect to some norm \u2016 \u00b7 \u2016 (with dual norm \u2016 \u00b7 \u2016\u2217) for some \u03b1 > 0, that is, for any \u03b8 \u2208 A\u25e6, \u03b8\u2032 \u2208 A\n\u03a8(\u03b8\u2032)\u2212\u03a8(\u03b8) \u2265 \u2329 \u2207\u03a8(\u03b8), \u03b8\u2032 \u2212 \u03b8 \u232a + \u03b12 \u2016\u03b8 \u2032 \u2212 \u03b8\u20162. (4)\nSuppose, furthermore, that Algorithm 1 is run for T time steps. For 0 \u2264 k \u2264 T \u2212 1 let Fk denote the \u03c3-algebra generated by \u03b81, . . . , \u03b8k. Assume that, for all 1 \u2264 k \u2264 T , g\u0302k \u2208 Rd is an unbiased estimate of \u2207J(\u03b8(k\u22121)) given Fk\u22121, that is,\nE [ g\u0302k| Fk\u22121] = \u2207J(\u03b8(k\u22121)). (5)\nFurther, assume that there exists a deterministic constant B \u2265 0 such that for all 1 \u2264 k \u2264 T ,\nE [ \u2016g\u0302k\u20162\u2217 \u2223\u2223Fk\u22121] \u2264 B a.s. (6) Finally, assume that \u03b4 = sup\u03b8\u2032\u2208K\u2229A \u03a8(\u03b8 \u2032) \u2212 \u03a8(\u03b8(0)) is finite. Then, if \u03b7k\u22121 = \u221a 2\u03b1\u03b4 BT for all k \u2265 1, it holds that\nE [ J ( 1\nT T\u2211 k=1 \u03b8(k\u22121) )] \u2212 inf \u03b8\u2208K\u2229A J(\u03b8) \u2264 \u221a 2B\u03b4 \u03b1T . (7)\nFurthermore, if \u2016g\u0302k\u20162\u2217 \u2264 B\u2032 a.s. (8)\nfor some deterministic constant B\u2032 and \u03b7k\u22121 = \u221a 2\u03b1\u03b4 B\u2032T for all k \u2265 1 then, for any 0 < < 1, it holds with probability at least 1\u2212 that\nJ\n( 1\nT T\u2211 k=1 \u03b8(k\u22121) ) \u2212 inf \u03b8\u2208K\u2229A J(\u03b8) \u2264 \u221a 2B\u2032\u03b4 \u03b1T + 4 \u221a B\u2032\u03b4 log 1 \u03b1T . (9)\nThe convergence rate in the above theorem can be improved if stronger assumptions are made on J , for example if J is assumed to be strongly convex, see, for example, (Hazan et al., 2007; Hazan and Kale, 2011).\nEfficient implementation of Algorithm 1 depends on efficient implementations of steps 1-1, namely, computing an estimate of the gradient, solving the minimization for \u03b8\u0303(k), and projecting it into K. The first problem is related to the choice of gradient estimate we use, which, in turn, depends on the structure of the feature space, while the last two problems depend on the choice of the Legendre function. In the next subsections we examine how these choices can be made to get a practical variant of the algorithm."}, {"heading": "3.2 Application to multiple kernel learning", "text": "It remains to define the gradient estimates g\u0302k in Algorithm 1. We start by considering importance sampling based estimates. First, however, let us first verify whether the gradient exist. Along the way, we will also derive some explicit expressions which will help us later.\nClosed-form expressions for the gradient. Let us first consider how w\u2217(\u03b8) can be calculated for a fixed value of \u03b8. As it will turn out, this calculation will be useful not only when the procedure is stopped (to construct the predictor fw\u2217(\u03b8) but also during the iterations when we will need to calculate the derivative of J with respect to \u03b8i. The following proposition summarizes how w\u2217(\u03b8) can be obtained. Note that this type of result is standard (see, e.g., Shawe-Taylor and Cristianini, 2004; Scho\u0308lkopf and Smola, 2002), thus we include it only for the sake of completeness (the proof is included in Section A in the appendix).\nProposition 3.2. For 1 \u2264 t \u2264 n, let `\u2217t : R\u2192 R denote the convex conjugate of `t: `\u2217t (v) = sup\u03c4\u2208R {v\u03c4 \u2212 `t(\u03c4)}, v \u2208 R. For i \u2208 I, recall that \u03bai(x, x\u2032) = \u3008\u03c6i(x), \u03c6i(x\u2032)\u3009, and let Ki = (\u03bai(xt, xs))1\u2264t,s\u2264n be the n \u00d7 n kernel matrix underlying \u03bai and let K\u03b8 = \u2211 i\u2208I\n\u03b8i \u03c12i Ki be the kernel matrix underlying \u03ba\u03b8 = \u2211\ni\u2208I \u03b8i \u03c12i \u03bai. Then, for any fixed \u03b8, the minimizer w \u2217(\u03b8) of\nJ(\u00b7, \u03b8) satisfies\nw\u2217i (\u03b8) = \u03b8i \u03c12i n\u2211 t=1 \u03b1\u2217t (\u03b8)\u03c6i(xt), i \u2208 I , (10)\nwhere\n\u03b1\u2217(\u03b8) = arg min \u03b1\u2208Rn\n{ 1\n2 \u03b1>K\u03b8\u03b1+\n1\nn n\u2211 t=1 `\u2217t (\u2212n\u03b1t)\n} . (11)\nBased on this proposition, we can compute the predictor fw\u2217(\u03b8) using the kernels {\u03bai}i\u2208I and the dual variables (\u03b1\u2217t (\u03b8))1\u2264t\u2264n: fw\u2217(\u03b8)(x) = \u2211 i\u2208I \u3008w\u2217i (\u03b8), \u03c6i(x)\u3009 = \u2211n t=1 \u03b1 \u2217 t (\u03b8)\u03ba\u03b8(xt, x) .\nLet us now consider the differentiability of J = J(\u03b8) and how to compute its derivatives. Under proper conditions with standard calculations (e.g., Rakotomamonjy et al., 2008) we find that J is differentiable over \u2206 and its derivative can be written as2\n\u2202\n\u2202\u03b8 J(\u03b8) = \u2212\n( \u03b1\u2217(\u03b8)>Ki\u03b1\u2217(\u03b8)\n\u03c12i\n) i\u2208I . (12)\nImportance sampling based estimates. Let d = |I| and let ei, i \u2208 I denote the ith unit vector of the standard basis of Rd, that is, the ith coordinate of ei is 1 while the others are 0. Introduce\ngk,i = \u2329 \u2207J(\u03b8(k\u22121)), ei \u232a , i \u2208 I (13)\nto denote the ith component of the gradient of J in iteration k (that is, gk,i can be computed based on (12)). Let sk\u22121 \u2208 [0, 1]I be a distribution over I, computed in some way based on the information available up to the end of iteration k \u2212 1 of the algorithm (formally, sk\u22121 is Fk\u22121-measurable). Define the importance sampling based gradient estimate to be\ng\u0302k,i = I{Ik=i} sk\u22121,Ik gk,Ik , i \u2208 I, where Ik \u223c sk\u22121,\u00b7 . (14)\nThat is, the gradient estimate is obtained by first sampling an index from sk\u22121,\u00b7 and then setting the gradient estimate to be zero at all indices i \u2208 I except when i = Ik in which case its value is set to be the ratio\ngk,Ik sk\u22121,Ik . It is easy to see that as long as sk\u22121,i > 0 holds\nwhenever gk,i 6= 0, then it holds that E [ g\u0302k| Fk\u22121] = \u2207J(\u03b8(k\u22121)) a.s. Let us now derive the conditions under which the second moment of the gradient estimate\nstays bounded. Define Ck\u22121 = \u2225\u2225\u2207J(\u03b8(k\u22121))\u2225\u2225\n1 . Given the expression for the gradient of J\nshown in (12), we see that supk\u22651Ck\u22121 <\u221e will always hold provided that \u03b1\u2217(\u03b8) is continuous since (\u03b8(k\u22121))k\u22651 is guaranteed to belong to a compact set (the continuity of \u03b1\n\u2217 is discussed in Section B in the appendix).\nDefine the probability distribution qk\u22121,\u00b7 as follows: qk\u22121,i = 1 Ck\u22121 |gk,i| , i \u2208 I. Then\nit holds that \u2016g\u0302k\u20162\u2217 = 1s2k\u22121,Ik g2k,Ik \u2016eIk\u2016 2 \u2217 = q2k\u22121,Ik s2k\u22121,Ik C2k\u22121 \u2016eIk\u2016 2 \u2217. Therefore, it also holds\nthat E [ \u2016g\u0302k\u20162\u2217 \u2223\u2223Fk\u22121] = C2k\u22121\u2211i\u2208I q2k\u22121,isk\u22121,i \u2016ei\u20162\u2217 \u2264 C2k\u22121 maxi\u2208I qk\u22121,isk\u22121,i \u2016ei\u20162\u2217. This shows that supk\u22651 E [ \u2016g\u0302k\u20162\u2217\n\u2223\u2223Fk\u22121] <\u221e will hold as long as supk\u22651 maxi\u2208I qk\u22121,isk\u22121,i <\u221e and supk\u22651Ck\u22121 < \u221e. Note that when sk\u22121 = qk\u22121, the gradient estimate becomes g\u0302k,i = Ck\u22121I{It=i}. That is, in this case we see that in order to be able to calculate g\u0302k,i, we need to be able to calculate Ck\u22121 efficiently.\nChoosing the potential \u03a8. The efficient sampling of the gradient is not the only practical issue, since the choice of the Legendre function and the convex set K may also cause some complications. For example, if \u03a8(x) = \u2211 i\u2208I xi(lnxi \u2212 1), then the resulting algorithm is exponential weighting, and one needs to store and update |I| weights, which is clearly infeasible if |I| is very large (or infinite). On the other hand, if \u03a8(x) = 12\u2016x\u2016 2 2 and we project\n2For completeness, the calculations are given in Section B in the appendix.\nAlgorithm 2 Projected stochastic gradient algorithm.\n1: Initialization: \u03a8(x) = 12\u2016x\u2016 2 2, \u03b8 (0) i = 0 for all i \u2208 I, k = 0, step sizes {\u03b7k}. 2: repeat 3: k = k + 1. 4: Sample a gradient estimate g\u0302k of g(\u03b8\n(k\u22121) randomly according to (14). 5: \u03b8(k) = \u03a0\u03a8,\u22062(\u03b8\n(k\u22121) \u2212 \u03b7k\u22121g\u0302k). 6: until convergence.\nto K = \u22062, the positive quadrant of the ` 2-ball (with A = [0,\u221e)I), we obtain a stochastic projected gradient method, shown in Algorithm 2. This is in fact the algorithm that we use in the experiments. Note that in (2) this corresponds to using p = 4/3. The reason we made this choice is because in this case projection is a simple scaling operation. Had we chosen K = \u22061, the `\n2-projection would very often cancel many of the nonzero components, resulting in an overall slow progress. Based on the above calculations and Theorem 3.1 we obtain the following performance bound for our algorithm.\nCorollary 3.3. Assume that \u03b1\u2217(\u03b8) is continuous on \u22062. Then there exists a C > 0 such that \u2016 \u2202\u2202\u03b8J(\u03b8)\u20161 \u2264 C for all \u03b8 \u2208 \u22062. Let B = 1 2C 2 maxi\u2208I,1\u2264k\u2264T qk\u22121,i sk\u22121,i . If Algorithm 2 is run\nfor T steps with \u03b7k\u22121 = \u03b7 = 1/ \u221a BT, k = 1, . . . , T , then, for all \u03b8 \u2208 \u22062,\nE [ J ( 1\nT T\u2211 k=1 \u03b8(k\u22121)\n)] \u2212 J(\u03b8) \u2264 \u221a B\nT .\nNote that to implement Algorithm 2 efficiently, one has to be able to sample from sk\u22121,\u00b7 and compute the importance sampling ratio gk,i/sk,i efficiently for any k and i."}, {"heading": "4 Example: Learning polynomial kernels", "text": "In this section we show how our method can be applied in the context of multiple kernel learning. We provide an example when the kernels in I are tensor products of a set of base kernels (this we shall call learning polynomial kernels). The importance of this example follows from the observation of Go\u0308nen and Alpayd\u0131n (2011) that the non-linear kernel learning methods of Cortes et al. (2009), which can be viewed as a restricted form of learning polynomial kernels, are far the best MKL methods in practice and can significantly outperform state-of-the-art SVM with a single kernel or with the uniform combination of kernels.\nAssume that we are given a set of base kernels {\u03ba1, . . . , \u03bar}. In this section we consider the setKD of product kernels of degree at mostD: Choose I = {(r1, . . . , rd) : 0 \u2264 d \u2264 D, 1 \u2264 ri \u2264 r} and the multi-index r1:d = (r1, . . . , rd) \u2208 I defines the kernel \u03bar1:d(x, x\u2032) = \u220fd i=1 \u03bari(x, x\n\u2032). For d = 0 we define \u03bar1:0(x, x\n\u2032) = 1. Note that indices that are the permutations of each other define the same kernel. On the language of statistical modeling, \u03bar1:d models interactions of order d between the features underlying the base kernels \u03ba1, . . . , \u03bar. Also note that |I| = \u0398(rD), that is, the cardinality of I grows exponentially fast in D.\nWe assume that \u03c1r1:d depends only on d, the order of interactions in \u03bar1:d . By abusing\nAlgorithm 3 Polynomial kernel sampling. The symbol denotes the Hadamard product/power.\n1: Input: \u03b1 \u2208 Rn, the solution to the dual problem; kernel matrices {K1, . . . ,Kr}; the degree D of the polynomial kernel, the weights (\u03c120, . . . , \u03c1 2 D).\n2: S \u2190 \u2211r\nj=1Kj , M \u2190 \u03b1\u03b1> 3: \u03b4(d\u2032)\u2190 \u03c1\u22122d\u2032 \u2329 M,S d \u2032 \u232a , d\u2032 \u2208 {0, . . . , D}\n4: Sample d from \u03b4(\u00b7)/ \u2211D\nd\u2032=0 \u03b4(d \u2032)\n5: for i = 1 to d do 6: \u03c0(j)\u2190 tr(M S\n(d\u2212i) Kj) tr(M S (d\u2212i+1))\n, j \u2208 {1, . . . , r} 7: Sample zi from \u03c0(\u00b7) 8: M \u2190M Kzi 9: end for\n10: return (z1, . . . , zd)\nnotation, we will write \u03c1d in the rest of this section to emphasize this. 3 Our proposed algorithm to sample from qk\u22121,\u00b7 is shown in Algorithm 3. The algorithm is written to return a multi-index (z1, . . . , zd) that is drawn from qk\u22121,\u00b7. The key idea underlying the algorithm is to exploit that ( \u2211r j=1 \u03baj) d = \u2211\nr1:d\u2208I \u03bar1:d . The correctness of the algorithm is shown in Section 4.1. In the description of the algorithm denotes the matrix entrywise product (a.k.a. Schur, or Hadamard product) and A s denotes A . . . A\ufe38 \ufe37\ufe37 \ufe38\ns\n, and we set the priority\nof to be higher than that of the ordinary matrix product (by definition, all the entries of A 0 are 1).\nLet us now discuss the complexity of Algorithm 3. For this, first note that computing all the Hadamard products S d \u2032 , d\u2032 = 0, . . . , D requires O(Dn2) computations. Multiplication with Mk\u22121 can be done in O(n 2) steps. Finally, note that each iteration of the for loop takes O(rn2) steps, which results in the overall worst-case complexity of O(rn2D) if \u03b1\u2217(\u03b8k\u22121) is readily available. The computational complexity of determining \u03b1\u2217(\u03b8k\u22121) depends on the exact form of `t, and can be done efficiently in many situations: if, for example, `t is the squared loss, then \u03b1\u2217 can be computed in O(n3) time. An obvious improvement to the approach described here, however, would be to subsample the empirical loss Ln, which can bring further computational improvements. However, the exploration of this is left for future work.\nFinally, note that despite the exponential cardinality of |I|, due to the strong algebraic structure of the space of kernels, Ck\u22121 can be calculated efficiently. In fact, it is not hard to see that with the notation of the algorithm, Ck\u22121 = \u2211D d\u2032=0 \u03b4(d\n\u2032). This also shows that if \u03c1d decays \u201cfast enough\u201d, Ck\u22121 can be bounded independently of the cardinality of I."}, {"heading": "4.1 Correctness of the sampling procedure", "text": "In this section we prove the correctness of Algorithm 3. As said earlier, we assume that \u03c1r1:d depends only on d, the order of interactions in \u03bar1:d\n3Using importance sampling, more general weights can also be accommodated, too without effecting the results as long as the range of weights (\u03c1r1:d) is kept under control for all d.\nand, by abusing notation, we will write \u03c1d to emphasize this. Let us now consider how one can sample from qk\u22121,\u00b7. The implementation relies on the fact that ( \u2211r j=1 \u03baj) d = \u2211\nr1:d\u2208I \u03bar1:d . Remember that we denoted the kernel matrix underlying some kernel k by Kk, and recall that Kk is an n\u00d7 n matrix. For brevity, in the rest of this section for \u03ba = \u03bar1:d we will write Kr1:d instead of K\u03bar1:d . Define Mk\u22121 = \u03b1 \u2217(\u03b8k\u22121)\u03b1 \u2217(\u03b8k\u22121)\n>. Thanks to (12) and the rotation property of trace, we have\ngk,r1:d = \u2212\u03c1 \u22122 d tr(Mk\u22121Kr1:d) . (15) The plan to sample from qk\u22121,\u00b7 = |gk,\u00b7|/ \u2211\nr1:d\u2208I |gk,r1:d | is as follows: We first draw the order of interactions, 0 \u2264 d\u0302 \u2264 D. Given d\u0302 = d, we restrict the draw of the random multi-index R1:d to the set {r1:d \u2208 I}. A multi-index will be sampled in a d\u0302-step process: in each step we will randomly choose an index from the indices of base kernels according to the following distributions. Let S = K1 + . . .+Kr, let\nP ( d\u0302 = d|Fk\u22121 ) = \u03c1\u22122d tr(Mk\u22121S d)\u2211D\nd\u2032=0 \u03c1 \u22122 d\u2032 tr(Mk\u22121S d\u2032)\nand, with a slight abuse of notation, for any 1 \u2264 i \u2264 d define P ( Ri = ri|Fk\u22121, d\u0302 = d,R1:i\u22121 = r1:i\u22121 ) = tr ( Mk\u22121 ( ij=1Krj ) S (d\u2212i) ) \u2211r\nr\u2032i=1 tr ( Mk\u22121\n( i\u22121j=1Krj ) Kr\u2032i S (d\u2212i) )\nwhere we used the sequence notation (namely, s1:p denotes the sequence (s1, . . . , sp)). We have, by the linearity of trace and the definition of S that\nr\u2211 r\u2032i=1 tr ( Mk\u22121\n( i\u22121j=1Krj ) Kr\u2032i S (d\u2212i) )\n= tr ( Mk\u22121\n( i\u22121j=1Krj ) S (d\u2212i+1) ) Thus, by telescoping,\nP ( d\u0302 = d,R1:d = r1:d|Fk\u22121 ) =\n\u03c1\u22122d tr(Mk\u22121Kr1 . . . Krd\u22121 Krd)\u2211D d\u2032=0 \u03c1 \u22122 d\u2032 tr(Mk\u22121S d\u2032) .\nas desired. An optimized implementation of drawing these random variables is shown as Algorithm 3. The algorithm is written to return the multi-index R1:d."}, {"heading": "5 Experiments", "text": "In this section we apply our method to the problem of multiple kernel learning in regression with the squared loss: L(w) = 12 \u2211n t=1(fw(xt) \u2212 yt)2, where (xt, yt) \u2208 Rr \u00d7 R are the inputoutput pairs in the data. In these experiments our aim is to learn polynomial kernels (cf. Section 4).\nWe compare our method against several kernel learning algorithms from the literature on synthetic and real data. In all experiments we report mean squared error over test sets. A constant feature is added to act as offset, and the inputs and output are normalized to have zero mean and unit variance. Each experiment is performed with 10 runs in which we randomly choose training, validation, and test sets. The results are averaged over these runs."}, {"heading": "5.1 Convergence speed", "text": "In this experiment we examine the speed of convergence of our method and compare it against one of the fastest standard multiple kernel learning algorithms, that is, the p-norm multiple kernel learning algorithm of Kloft et al. (2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richta\u0301rik and Taka\u0301c\u0302, 2011). We aim to learn polynomial kernels of up to degree 3 with all algorithms. Our method uses Algorithm 3 for sampling with D = 3. The set of provided base kernels is the linear kernels built from input variables, that is, \u03ba(i)(x, x \u2032) = x(i)x \u2032 (i), where x(i) denotes the i\nth input variable. For the other two algorithms the kernel set consists of product kernels from monomial terms for D \u2208 {0, 1, 2, 3} built from r base kernels, where r is the number of input variables. The number of distinct product kernels is ( r+D D ) . In this experiment for all algorithms we use ridge regression with its regularization parameter set to 10\u22125. Experiments with other values of the regularization parameter achieved similar results.\nWe compare these methods in four datasets from the UCI machine learning repository (Frank and Asuncion, 2010) and the Delve datasets5. The specifications of these datasets are shown in Table 1. We run all algorithms for a fixed amount of time and measure the value\nof the objective function (1), that is, the sum of the empirical loss and the regularization term. Figure 1 shows the performance of these algorithms. In this figure Stoch represents our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents the uniform coordinate descent algorithm. The results show that our method consistently outperforms the other algorithms in convergence speed. Note that our stochastic method updates one kernel coefficient per iteration, while Kloft updates ( r+D D ) kernel coefficients per iteration. The difference between the two methods is analogous to the difference between stochastic gradient vs. full gradient algorithms. While UCD also updates one kernel\n4Note that p = 2 in Kloft et al. (2011) notation corresponds to p = 4/3 or \u03bd = 2 in our notation, which gives the same objective function that we minimize with Algorithm 2.\n5See, www.cs.toronto.edu/~delve/data/datasets.html\ncoefficient per iteration its naive method of selecting coordinates results in a slower overall convergence compared to our algorithm. In the next section we compare our algorithm against several representative methods from the MKL literature."}, {"heading": "5.2 Synthetic data", "text": "In this experiment we examine the effect of the size of the kernel space on prediction accuracy and training time of MKL algorithms. We generated data for a regression problem. Let r denote the number of dimensions of the input space. The inputs are chosen uniformly at random from [\u22121, 1]r. The output of each instance is the uniform combination of 10 monomial terms of degree 3 or less. These terms are chosen uniformly at random among all possible terms. The outputs are noise free. We generated data for r \u2208 {5, 10, 20, . . . , 100}, with 500 training and 1000 test points. The regularization parameter of the ridge regression algorithm was tuned from {10\u22128, . . . , 102} using a separate validation set with 1000 data points.\nWe compare our method (Stoch) against the algorithm of Kloft et al. (2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai)\nD, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.\nThe results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets\n6While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011) has the best performance overall. Hence, we decided to compare against only this algorithm. Also note that the memory and computational cost of all these methods still scale linearly with the number of kernels, making them unsuitable for the case we are most interested in. Furthermore, to keep the focus of the paper we compare our algorithm to methods with sound theoretical guarantees. As such, it remains for future work to compare with other methods, such as the infinite kernel learning of Gehler and Nowozin (2008), which lack such guarantees but exhibit promising performance in practice.\nindicate the total number of distinct product kernels for each value of r. This is the number of kernels fed to the Kloft algorithm. Since this method deals with a large number of kernels, it was possible to precompute and keep the kernels in memory (8GB) for r \u2264 25. Therefore, we ran this algorithm for r \u2264 25. For r > 25, we could use on-the-fly implementation of this algorithm, however that further increases the training time. Note that the computational cost of this method depends linearly on the number of kernels, which in this experiment, is cubic in the number of input variables since D = 3. While the standard MKL algorithms, such as Kloft, cannot handle such large kernel spaces, in terms of time and space complexity, the other three algorithms can efficiently learn kernel combinations. However their predictive accuracies are quite different. Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases. This is due to the restricted family of kernels that this method considers. The method of Bach (2008), which is well-suited to learn sparse combination of product kernels, performs better than Cortes et al. (2009) for higher input dimensions. Among all methods, our method performs best in predictive accuracy while its computational cost is close to that of the other two competitors."}, {"heading": "5.3 Real data", "text": "In this experiment we aim to compare several MKL methods in real datasets. We compare our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of Cortes et al. (2009) (Cortes). For each algorithm we consider learning polynomial kernels of degree 2 and 3. We also include uniform combination of product kernels of degree D, i.e. \u03baD = ( \u2211r i=1 \u03bai)\nD, for D \u2208 {1, 2, 3} (Uniform). To find out if considering higherorder interaction of input variables results in improved performance we also included a MKL algorithm to which we only feed linear kernels (D = 1). We use the MKL algorithm of Kloft et al. (2011) with p \u2208 {1, 2} (Kloft).\nWe compare these methods on six datasets from the UCI machine learning repository\nand Delve datasets. In these datasets the number of dimensions of the input space is 20 and above. The specifications of these datasets are shown in Table 1. The regularization parameter is selected from the set {10\u22124, . . . , 103} for all methods using a validation set. The results are shown in Figure 3.\nOverall, we observe that methods that consider non-linear variable interactions (Stoch, Bach, and Cortes) perform better than linear methods (Kloft). Among non-linear methods, Cortes performs worse than the other two. We believe that this is due to the restricted kernel space considered by this method. The performance of Stoch and Bach methods is similar overall.\nWe observe that our method overfits when it considers kernels of degree 3. However, one can easily prevent overfitting by assigning larger \u03c1 values to higher-degree kernels such that the stochastic algorithm selects lower-degree kernels more often. For this purpose, we repeat this experiment for D = 3 with a modified set of \u03c1 values, where we use \u03c12d = 1 for kernels of degree 2 or less and \u03c12d = 4 for kernels of degree 3. With the new \u03c1 coefficients we observe an improvement in algorithm\u2019s performance. See Stoch (D = 3, prior) error values in Figure 3."}, {"heading": "6 Conclusion", "text": "We introduced a new method for learning a predictor by combining exponentially many linear predictors using a randomized mirror descent algorithm. We derived finite-time performance bounds that show that the method efficiently optimizes our proposed criterion. Our proposed method is a variant of a randomized stochastic coordinate descent algorithm, where the main trick is the careful construction of an unbiased randomized estimate of the gradient vector that keeps the variance of the method under control, and can be computed efficiently when the base kernels have a certain special combinatorial structure. The efficiency of our method\nwas demonstrated for the practically important problem of learning polynomial kernels on a variety of synthetic and real datasets comparing to a representative set of algorithms from the literature. For this case, our method is able to compute an optimal solution in polynomial time as a function of the logarithm of the number of base kernels. To our knowledge, ours is the first method for learning kernel combinations that achieve such an exponential reduction in complexity while satisfying strong performance guarantees, thus opening up the way to apply it to extremely large number of kernels. Furthermore, we believe that our method is applicable beyond the case studied in detail in our paper. For example, the method seems extendible to the case when infinitely many kernels are combined, such as the case of learning a combination of Gaussian kernels. However, the investigation of this important problem remains subject to future work."}, {"heading": "Acknowledgements", "text": "This work was supported by Alberta Innovates Technology Futures and NSERC."}, {"heading": "A Proofs", "text": "In this section we present the proofs of Theorem 3.1 and Proposition 3.2. The proof of Theorem 3.1 is based on the standard proof of the convergence rate of the proximal point algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.2 of Nemirovski et al. (2009b), which carry over the same argument to solve very similar but less general problems. We also provide some improvements and simplifications at the end. Before giving the actual proof, we need the following standard lemma:\nLemma A.1 (Lemma 2.1 of Nemirovski et al. 2009b). Assume that \u03a8 is \u03b1-strongly convex with respect to some norm \u2016 \u00b7 \u2016 (i.e., (4) holds). Let \u03b81 \u2208 K \u2229 A\u25e6, \u03b8 \u2208 K \u2229 A, and g \u2208 Rd. Define \u03b82 = arg min\u03b8\u2032\u2208K\u2229A {\u3008g, \u03b8\u2032\u3009+D\u03a8(\u03b8\u2032, \u03b81)}. Then\n\u3008g, \u03b81 \u2212 \u03b8\u3009 \u2264 D\u03a8(\u03b8, \u03b81)\u2212D\u03a8(\u03b8, \u03b82) + \u2016g\u20162\u2217 2\u03b1 .\nWe provide an alternate proof that is based on the so-called 3-DIV lemma. The 3-DIV lemma (e.g., Lemma 11.1, Cesa-Bianchi and Lugosi, 2006) allows one to express the sum of the divergences between the vectors u, v and v, w in terms of the divergence between u and w and an additional \u201cerror term\u201d, where u \u2208 A, v, w \u2208 A\u25e6:\nD\u03a8(u, v) +D\u03a8(v, w) = D\u03a8(u,w) + \u3008\u2207\u03c8(w)\u2212\u2207\u03c8(v), u\u2212 v\u3009 .\nProof. Note that \u03b82 \u2208 A\u25e6 due to behavior of \u03a8 at the boundary of A. Thus, \u03a8 is differentiable at \u03b82 and\n\u22071D\u03a8(\u03b82, \u03b81) = \u2207\u03c8(\u03b82)\u2212\u2207\u03c8(\u03b81) , (16)\nwhere\u22071 denotes differentiation of D\u03a8 w.r.t. its first variable. Let f(\u03b8\u2032) = \u3008g, \u03b8\u2032\u3009+D\u03a8(\u03b8\u2032, \u03b81). By the optimality property of \u03b82 and since \u03b8 \u2208 K \u2229A, we have\n\u3008\u2207f(\u03b82), \u03b82 \u2212 \u03b8\u3009 \u2264 0 .\nPlugging in the definition of f together with the identity (16) gives\n\u3008g +\u2207\u03c8(\u03b82)\u2212\u2207\u03c8(\u03b81), \u03b82 \u2212 \u03b8\u3009 \u2264 0 . (17)\nNow, by the 3-DIV Lemma,\nD\u03a8(\u03b8, \u03b82) +D\u03a8(\u03b82, \u03b81) = D\u03a8(\u03b8, \u03b81) + \u3008\u2207\u03a8(\u03b81)\u2212\u2207\u03a8(\u03b82), \u03b8 \u2212 \u03b82\u3009 = D\u03a8(\u03b8, \u03b81) + \u3008g +\u2207\u03a8(\u03b82)\u2212\u2207\u03a8(\u03b81), \u03b82 \u2212 \u03b8\u3009+ \u3008g, \u03b8 \u2212 \u03b82\u3009 .\nHence, by reordering and using the inequality (17) we get\nD\u03a8(\u03b8, \u03b82)\u2212D\u03a8(\u03b8, \u03b81) \u2264 \u3008g, \u03b8 \u2212 \u03b82\u3009 \u2212D\u03a8(\u03b82, \u03b81) = \u3008g, \u03b81 \u2212 \u03b82\u3009 \u2212D\u03a8(\u03b82, \u03b81) + \u3008g, \u03b8 \u2212 \u03b81\u3009\n\u2264 \u2016g\u2016 2 \u2217\n2\u03b1 + \u3008g, \u03b8 \u2212 \u03b81\u3009 ,\nwhere in the last line we used Young\u2019s inequality7 and that due to the strong convexity of \u03a8, D\u03a8(\u03b82, \u03b81) \u2265 \u03b12 \u2016\u03b82 \u2212 \u03b81\u2016 2.\nTheorem 3.1. Assume that \u03a8 is \u03b1-strongly convex with respect to some norm \u2016 \u00b7 \u2016 (with dual norm \u2016 \u00b7 \u2016\u2217) for some \u03b1 > 0, that is, for any \u03b8 \u2208 A\u25e6, \u03b8\u2032 \u2208 A\n\u03a8(\u03b8\u2032)\u2212\u03a8(\u03b8) \u2265 \u2329 \u2207\u03a8(\u03b8), \u03b8\u2032 \u2212 \u03b8 \u232a + \u03b12 \u2016\u03b8 \u2032 \u2212 \u03b8\u20162. (4)\nSuppose, furthermore, that Algorithm 1 is run for T time steps. For 0 \u2264 k \u2264 T \u2212 1 let Fk denote the \u03c3-algebra generated by \u03b81, . . . , \u03b8k. Assume that, for all 1 \u2264 k \u2264 T , g\u0302k \u2208 Rd is an unbiased estimate of \u2207J(\u03b8(k\u22121)) given Fk\u22121, that is,\nE [ g\u0302k| Fk\u22121] = \u2207J(\u03b8(k\u22121)). (5)\nFurther, assume that there exists a deterministic constant B \u2265 0 such that for all 1 \u2264 k \u2264 T , E [ \u2016g\u0302k\u20162\u2217 \u2223\u2223Fk\u22121] \u2264 B a.s. (6) Finally, assume that \u03b4 = sup\u03b8\u2032\u2208K\u2229A \u03a8(\u03b8 \u2032) \u2212 \u03a8(\u03b8(0)) is finite. Then, if \u03b7k\u22121 = \u221a 2\u03b1\u03b4 BT for all k \u2265 1, it holds that\nE [ J ( 1\nT T\u2211 k=1 \u03b8(k\u22121) )] \u2212 inf \u03b8\u2208K\u2229A J(\u03b8) \u2264 \u221a 2B\u03b4 \u03b1T . (7)\nFurthermore, if \u2016g\u0302k\u20162\u2217 \u2264 B\u2032 a.s. (8)\nfor some deterministic constant B\u2032 and \u03b7k\u22121 = \u221a 2\u03b1\u03b4 B\u2032T for all k \u2265 1 then, for any 0 < < 1, it holds with probability at least 1\u2212 that\nJ\n( 1\nT T\u2211 k=1 \u03b8(k\u22121) ) \u2212 inf \u03b8\u2208K\u2229A J(\u03b8) \u2264 \u221a 2B\u2032\u03b4 \u03b1T + 4 \u221a B\u2032\u03b4 log 1 \u03b1T . (9)\n7Young\u2019s inequality states that for any x, y vectors and \u03b1 > 0, \u3008x, y\u3009 \u2264 \u2016x\u2016\u2217\u2016y\u2016 \u2264 12 ( \u2016x\u20162\u2217 \u03b1 + \u03b1\u2016y\u20162 ) .\nProof. Introduce the average learning rates \u03b7 (T ) k = \u03b7k/ \u2211T k=1 \u03b7k\u22121, k = 1, . . . , T , the averaged parameter estimates\n\u03b8\u0304(T\u22121) = T\u2211 k=1 \u03b7 (T ) k\u22121\u03b8 (k\u22121)\nand choose some \u03b8\u2217 \u2208 K \u2229A. To prove the first part of the theorem, it suffices to show that the bound holds for J(\u03b8\u0304(T\u22121)) \u2212 J(\u03b8\u2217). Define gk = \u2207J ( \u03b8(k\u22121) ) . By the convexity of J(\u03b8), we have\nJ ( \u03b8\u0304(T\u22121) ) \u2212 J(\u03b8\u2217) \u2264 T\u2211 k=1 \u03b7 (T ) k\u22121 ( J ( \u03b8(k\u22121) ) \u2212 J(\u03b8\u2217) ) \u2264\nT\u2211 k=1 \u03b7 (T ) k\u22121 \u2329 gk, \u03b8 (k\u22121) \u2212 \u03b8\u2217 \u232a\n= T\u2211 k=1 \u03b7 (T ) k\u22121 \u2329 g\u0302k, \u03b8 (k\u22121) \u2212 \u03b8\u2217 \u232a + T\u2211 k=1 \u03b7 (T ) k\u22121 \u2329 gk \u2212 g\u0302k, \u03b8(k\u22121) \u2212 \u03b8\u2217 \u232a (18)\nNotice that the first term on the right hand side above is the sum of linearized losses appearing in the standard analysis of the proximal point algorithm with loss functions g\u0302k and learning rates \u03b7 (T ) k\u22121, and the second sum contains the term that depends on how well g\u0302k estimates the gradient gk. Thus, in this way, it is separated how the proximal point algorithm and the gradient estimate effect the convergence rate of the algorithm. The first sum can be bounded by invoking the standard bound for the proximal point algorithm (we will give the very short proof for completeness, based on Lemma A.1), while the second sum can be analyzed by noticing that, by assumption (5), its elements form an {Fk}-adapted martingale-difference sequence.\nTo bound the first sum, first note that the conditions of Lemma A.1 are satisfied for\n\u03b81 = \u03b8 (k\u22121), \u03b8 = \u03b8\u2217, g = \u03b7 (T ) k\u22121g\u0302k, since \u03b81 \u2208 K \u2229 A \u25e6 (as mentioned beforehand, this follows from the behavior of \u03a8 at the boundary of A). Further, note that due to the so-called projection lemma (i.e., the D\u03a8-projection of the unconstrained optimizer is the same as the optimizer of the constrained optimization problem),we can conclude that \u03b8(k) = \u03b82, where \u03b82 is defined in Lemma A.1. Thus, Lemma A.1 gives\n\u03b7k\u22121 \u2329 g\u0302k, \u03b8 (k\u22121) \u2212 \u03b8\u2217 \u232a \u2264 D\u03a8(\u03b8\u2217, \u03b8(k\u22121))\u2212D\u03a8(\u03b8\u2217, \u03b8(k) +\n\u03b72k\u22121\u2016g\u0302k\u20162\u2217 2\u03b1 .\nSumming the above inequality for k = 1, . . . , T , the divergence terms cancel each other, yielding T\u2211 k=1 \u03b7 (T ) k\u22121 \u2329 g\u0302k, \u03b8 (k\u22121) \u2212 \u03b8\u2217 \u232a \u2264 1\u2211T k=1 \u03b7k\u22121 ( D\u03a8(\u03b8 \u2217, \u03b8(0))\u2212D\u03a8(\u03b8\u2217, \u03b8(T )) + 1 2\u03b1 T\u2211 k=1 \u03b72k\u22121\u2016g\u0302k\u20162\u2217 ) .\n(19) Let us now turn to the second sum. We start with developing a bound on the expected regret. For any 1 \u2264 k \u2264 T , by construction \u03b7(T )k\u22121 and \u03b8 (k\u22121) are Fk\u22121-measurable. This, together with (5) gives\nE [ \u03b7\n(T ) k\u22121 \u2329 gk \u2212 g\u0302k, \u03b8\u2217 \u2212 \u03b8(k\u22121) \u232a\u2223\u2223\u2223Fk\u22121] = \u03b7(T )k\u22121 \u2329gk \u2212 E [ g\u0302k| Fk\u22121] , \u03b8\u2217 \u2212 \u03b8(k\u22121)\u232a = 0 . (20)\nCombining this result with (18) and (19) yields E [ J ( \u03b8\u0304(T ) ) \u2212 J(\u03b8\u2217) ] \u2264 1\u2211T\nk=1 \u03b7k\u22121\n( D\u03a8(\u03b8 \u2217, \u03b8(0))\u2212D\u03a8(\u03b8\u2217, \u03b8(T )) + 1\n2\u03b1 T\u2211 k=1 \u03b72k\u22121E [ E [ \u2016g\u0302k\u20162\u2217\n\u2223\u2223Fk\u22121]] )\n\u2264 \u03b4 + 12\u03b1\n\u2211T k=1 \u03b7\n2 k\u22121B\u2211T\nk=1 \u03b7k\u22121 , (21)\nwhere we used the tower rule to bring in the bound (6), the nonnegativity of Bregman divergences, and D\u03a8(\u03b8, \u03b8 (0)) \u2264 \u03a8(\u03b8) \u2212 \u03a8(\u03b8(0)); the latter holds as \u2329 \u2207\u03a8(\u03b8(0)), \u03b8 \u2212 \u03b8(0) \u232a \u2265 0\nsince \u03b8(0) minimizes \u03a8 on K. Substituting \u03b7k\u22121 = \u03b7 = \u221a 2\u03b1\u03b4 BT , k = 1, . . . , T finishes the proof of (7). To prove the high probability result (9), notice that thanks to (5) { \u03b7k\u22121 \u2329 gk \u2212 g\u0302k, \u03b8\u2217 \u2212 \u03b8(k\u22121)\n\u232a} is an {Fk}-adapted martingale-difference sequence (cf. (20)). By the strong convexity of \u03a8 we have\n\u03b1 2 \u2016\u03b8(k\u22121) \u2212 \u03b8\u2217\u20162 \u2264 \u03a8(\u03b8(k\u22121))\u2212\u03a8(\u03b8\u2217) \u2264 \u03b4.\nFurthermore, conditions (5) and (8) imply that \u2016gk\u20162\u2217 \u2264 B\u2032 a.s., and so by (8) we have \u2016gk \u2212 g\u0302k\u2016\u2217 \u2264 2 \u221a B\u2032 a.s. Then by Ho\u0308lder\u2019s inequality\n\u2223\u2223\u2223\u2329gk \u2212 g\u0302k, \u03b8\u2217 \u2212 \u03b8(k\u22121)\u232a\u2223\u2223\u2223 \u2264 \u2016gk \u2212 g\u0302k\u2016\u2217 \u2016\u03b8\u2217 \u2212 \u03b8(k\u22121)\u2016 \u2264 2 \u221a 2B\u2032\u03b4\n\u03b1 .\nThus, by the Hoeffding-Azuma inequality (see, e.g., Lemma A.7, Cesa-Bianchi and Lugosi, 2006), for any 0 < < 1 we have, with probability at least 1\u2212 ,\nT\u2211 k=1 \u03b7 (T ) k\u22121 \u2329 gk \u2212 g\u0302k, \u03b8\u2217 \u2212 \u03b8(k\u22121) \u232a \u2264 4\u2211T k=1 \u03b7k\u22121 \u221a\u221a\u221a\u221aB\u2032\u03b4 \u03b1 ( T\u2211 k=1 \u03b72k\u22121 ) ln 1 . (22)\nCombining (19) with (8) implies an almost sure upper bound on the first sum on the right hand side of (18) as in (21) with B\u2032 in place of B. This, together with (22) proves the required high probability bound (9) when substituting \u03b7k\u22121 = \u03b7 \u2032 = \u221a 2\u03b1\u03b4 B\u2032T .\nProposition 3.2. For 1 \u2264 t \u2264 n, let `\u2217t : R\u2192 R denote the convex conjugate of `t: `\u2217t (v) = sup\u03c4\u2208R {v\u03c4 \u2212 `t(\u03c4)}, v \u2208 R. For i \u2208 I, recall that \u03bai(x, x\u2032) = \u3008\u03c6i(x), \u03c6i(x\u2032)\u3009, and let Ki = (\u03bai(xt, xs))1\u2264t,s\u2264n be the n \u00d7 n kernel matrix underlying \u03bai and let K\u03b8 = \u2211 i\u2208I\n\u03b8i \u03c12i Ki be the kernel matrix underlying \u03ba\u03b8 = \u2211\ni\u2208I \u03b8i \u03c12i \u03bai. Then, for any fixed \u03b8, the minimizer w \u2217(\u03b8) of\nJ(\u00b7, \u03b8) satisfies\nw\u2217i (\u03b8) = \u03b8i \u03c12i n\u2211 t=1 \u03b1\u2217t (\u03b8)\u03c6i(xt), i \u2208 I , (10)\nwhere\n\u03b1\u2217(\u03b8) = arg min \u03b1\u2208Rn\n{ 1\n2 \u03b1>K\u03b8\u03b1+\n1\nn n\u2211 t=1 `\u2217t (\u2212n\u03b1t)\n} . (11)\nProof. By introducing the variables \u03c4 = (\u03c4t)1\u2264t\u2264n \u2208 Rn and using the definition of L we can write the optimization problem (3) as the constrained optimization problem\nminimize w\u2208W,\u03c4\u2208Rn\n1\nn n\u2211 t=1 `t(\u03c4t) + 1 2 \u2211 i\u2208I \u03c12i \u2016wi\u201622 \u03b8i\ns.t. \u03c4t = \u2211 i\u2208I \u3008wi, \u03c6i(xt)\u3009 , (23)\nIn what follows, we call this problem the primal problem. The Lagrangian of this problem is\nL(w, \u03c4, \u03b1) .= 1 n n\u2211 t=1 `t(\u03c4t) + 1 2 \u2211 i\u2208I \u03c12i \u2016wi\u201622 \u03b8i + n\u2211 t=1 \u03b1t\n{ \u03c4t \u2212\n\u2211 i\u2208I \u3008wi, \u03c6i(xt)\u3009\n} ,\nwhere \u03b1 = (\u03b1t)1\u2264t\u2264n \u2208 Rn is the vector of Lagrange multipliers (or dual variables) associated with the n equality constraints. The Lagrange dual function, g(\u03b1) . = infw,\u03c4 L(w, \u03c4, \u03b1), can be readily seen to satisfy\ng(\u03b1) = \u2212\n( 1\n2 \u03b1>K\u03b8\u03b1+\n1\nn n\u2211 t=1 `\u2217t (\u2212n\u03b1t)\n) .\nNow, since the objective function of the primal problem is convex and the primal problem involves only affine equality constraints and the primal problem is clearly feasible, by Slater\u2019s condition (p.226, Boyd and Vandenberghe, 2004), if \u03b1\u2217(\u03b8) is the maximizer of g(\u03b1) then\nw\u2217(\u03b8) = arg min w\u2208W inf \u03c4\u2208Rn L(w, \u03c4, \u03b1\u2217(\u03b8))\n= arg min w\u2208W \u2211 i\u2208I\n{ \u03c12i \u2016wi\u201622\n2\u03b8i \u2212 n\u2211 t=1 \u03b1t \u3008wi, \u03c6i(xt)\u3009\n} .\nThe minimum of the last expression is readily seen to be equal to the expression given in (10), thus finishing the proof."}, {"heading": "B Calculating the derivative of J(\u03b8)", "text": "In this section we show that under mild conditions the derivative of J exist and we also give explicit forms. These derivations are quite standard and a similar argument can be found in the paper by (e.g.) Rakotomamonjy et al. (2008) specialized to the case when `t is the hinge loss.\nAs it is well-known, thanks to the implicit function theorem (e.g., Brown and Page,\n1970, Theorem 7.5.6), provided that J = J(w, \u03b8) is such that \u2202 2 \u2202\u03b8\u2202wJ(w, \u03b8) and \u2202 \u2202wJ(w, \u03b8) are continuous, the gradient of J(\u03b8) can be computed by evaluating the partial derivative \u2202 \u2202\u03b8J(w, \u03b8) of J(w, \u03b8) with respect to \u03b8 at (w \u2217(\u03b8), \u03b8)), that is, \u2202\u03b8J(\u03b8) = \u2202 \u2202\u03b8 J(w, \u03b8)|w=w\u2217(\u03b8).Note that the derivative is well-defined only if \u03b8 > 0, that is, when no coordinates of \u03b8 is zero, in which case\n\u2202\n\u2202\u03b8 J(w\u2217(\u03b8), \u03b8) = \u2212\n( \u03c12i \u2016w\u2217i (\u03b8)\u201622\n\u03b82i\n) i\u2208I . (24)\nIf \u03b8i = 0 for some i \u2208 I, we define the derivative in a continuous manner as \u2202\n\u2202\u03b8 J(\u03b8) = lim\n\u03b8\u2032\u2192\u03b8 \u03b8\u2032\u2208\u2206,\u03b8\u2032>0\n\u2202\n\u2202\u03b8 J(\u03b8\u2032) (25)\nassuming that the limit exists. From (10) we get, for any i \u2208 I, \u2016w\u2217i (\u03b8)\u201622 = \u03b82i \u03c14i \u03b1\u2217(\u03b8)>Ki\u03b1\u2217(\u03b8). Combining with (24) we obtain\n\u2202\n\u2202\u03b8 J(w\u2217(\u03b8), \u03b8) = \u2212\n( \u03b1\u2217(\u03b8)>Ki\u03b1\u2217(\u03b8)\n\u03c12i\n) i\u2208I .\nNow, by (25) and the implicit function theorem, \u03b1\u2217(\u03b8) is a continuous function of \u03b8 provided that the functions `\u2217t (1 \u2264 t \u2264 n) are twice continuously differentiable. This shows that under the conditions listed so far, the limit in (25) exists. In the application we shall be concerned with, these conditions can be readily verified."}], "references": [{"title": "Learning convex combinations", "author": ["A. Argyriou", "C. Micchelli", "M. Pontil"], "venue": null, "citeRegEx": "Argyriou et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Argyriou et al\\.", "year": 2005}, {"title": "Mirror descent and nonlinear projected subgradient meth", "author": ["A. Beck", "M. Teboulle"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Beck and Teboulle,? \\Q2003\\E", "shortCiteRegEx": "Beck and Teboulle", "year": 2003}, {"title": "Learning non-linear combinations", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": null, "citeRegEx": "Cortes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2009}, {"title": "Infinite kernel learning", "author": ["A. Asuncion"], "venue": "Neural Information Processing Systems,", "citeRegEx": "A. and Asuncion,? \\Q2010\\E", "shortCiteRegEx": "A. and Asuncion", "year": 2010}, {"title": "Multiple kernel learning algorithms", "author": ["M. Planck Institute For Biological Cybernetics. G\u00f6nen", "E. Alpayd\u0131n"], "venue": "Journal of Machine", "citeRegEx": "G\u00f6nen and Alpayd\u0131n,? 2011", "shortCiteRegEx": "G\u00f6nen and Alpayd\u0131n", "year": 2011}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Learning Research,", "citeRegEx": "Hastie et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning Journal, 69(2-3):169\u2013192.", "citeRegEx": "Hazan et al\\.,? 2007", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["E. Hazan", "S. Kale"], "venue": "Proceedings of the 24th Annual Conference on Learning Theory, volume 19 of JMLR Workshop and Conference Proceedings, pages 421\u2013436.", "citeRegEx": "Hazan and Kale,? 2011", "shortCiteRegEx": "Hazan and Kale", "year": 2011}, {"title": "lp-norm multiple kernel learning", "author": ["M. Kloft", "U. Brefeld", "S. Sonnenburg", "A. Zien"], "venue": "Journal of Machine Learning Research, 12:953\u2013997.", "citeRegEx": "Kloft et al\\.,? 2011", "shortCiteRegEx": "Kloft et al\\.", "year": 2011}, {"title": "Perturbation des m\u00e9thodes d\u2019optimisation", "author": ["B. Martinet"], "venue": "Applications. RAIRO Analyse Num\u00e9rique, 12:153\u2013171.", "citeRegEx": "Martinet,? 1978", "shortCiteRegEx": "Martinet", "year": 1978}, {"title": "Learning the kernel function via regularization", "author": ["C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:1099\u20131125.", "citeRegEx": "Micchelli and Pontil,? 2005", "shortCiteRegEx": "Micchelli and Pontil", "year": 2005}, {"title": "On the algorithmics and applications of a mixed-norm based kernel learning formulation", "author": ["J. Nath", "G. Dinesh", "S. Raman", "C. Bhattacharyya", "A. Ben-Tal", "K. Ramakrishnan"], "venue": "Advances in Neural Information Processing Systems, volume 22, pages 844\u2013852.", "citeRegEx": "Nath et al\\.,? 2009", "shortCiteRegEx": "Nath et al\\.", "year": 2009}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM J. Optimization, 4:1574\u20131609.", "citeRegEx": "Nemirovski et al\\.,? 2009a", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "venue": "SIAM Journal on Optimization, 19(4):1574\u2013 1609.", "citeRegEx": "Nemirovski et al\\.,? 2009b", "shortCiteRegEx": "Nemirovski et al\\.", "year": 2009}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A. Nemirovski", "D. Yudin"], "venue": "Wiley.", "citeRegEx": "Nemirovski and Yudin,? 1998", "shortCiteRegEx": "Nemirovski and Yudin", "year": 1998}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion paper, (2010/2).", "citeRegEx": "Nesterov,? 2010", "shortCiteRegEx": "Nesterov", "year": 2010}, {"title": "Subgradient methods for huge-scale optimization problems", "author": ["Y. Nesterov"], "venue": "CORE Discussion paper, (2012/2).", "citeRegEx": "Nesterov,? 2012", "shortCiteRegEx": "Nesterov", "year": 2012}, {"title": "Ultra-fast optimization algorithm for sparse multi kernel learning", "author": ["F. Orabona", "J. Luo"], "venue": "Proceedings of the 28th International Conference on Machine Learning, pages 249\u2013256.", "citeRegEx": "Orabona and Luo,? 2011", "shortCiteRegEx": "Orabona and Luo", "year": 2011}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet"], "venue": "Journal of Machine Learning Research, 9:2491\u20132521.", "citeRegEx": "Rakotomamonjy et al\\.,? 2008", "shortCiteRegEx": "Rakotomamonjy et al\\.", "year": 2008}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["P. Richt\u00e1rik", "M. Tak\u00e1\u0109"], "venue": "(revised July 4, 2011) submitted to Mathematical Programming.", "citeRegEx": "Richt\u00e1rik and Tak\u00e1\u0109,? 2011", "shortCiteRegEx": "Richt\u00e1rik and Tak\u00e1\u0109", "year": 2011}, {"title": "Monotone operators and the proximal point algorithm", "author": ["R. Rockafellar"], "venue": "SIAM Journal on Control and Optimization, 14(1):877\u2013898. 21", "citeRegEx": "Rockafellar,? 1976", "shortCiteRegEx": "Rockafellar", "year": 1976}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press, Cambridge, MA, USA.", "citeRegEx": "Sch\u00f6lkopf and Smola,? 2002", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Stochastic methods for l1-regularized loss minimization", "author": ["S. Shalev-Shwartz", "A. Tewari"], "venue": "Journal of Machine Learning Research, 12:1865\u20131892.", "citeRegEx": "Shalev.Shwartz and Tewari,? 2011", "shortCiteRegEx": "Shalev.Shwartz and Tewari", "year": 2011}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge Univ Press.", "citeRegEx": "Shawe.Taylor and Cristianini,? 2004", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "The Journal of Machine Learning Research, 7:1531\u20131565.", "citeRegEx": "Sonnenburg et al\\.,? 2006", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2006}, {"title": "An extended level method for efficient multiple kernel learning", "author": ["Z. Xu", "R. Jin", "I. King", "M. Lyu"], "venue": "Advances in Neural Information Processing Systems, volume 21, pages 1825\u20131832.", "citeRegEx": "Xu et al\\.,? 2008", "shortCiteRegEx": "Xu et al\\.", "year": 2008}, {"title": "Simple and efficient multiple kernel learning by group lasso", "author": ["Z. Xu", "R. Jin", "H. Yang", "I. King", "M.R. Lyu"], "venue": "Proceedings of the 27th International Conference on Machine Learning, pages 1175\u20131182. 22", "citeRegEx": "Xu et al\\.,? 2010", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 25, "context": "Just like some previous works (e.g. Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach that views the MKL problem as a nested, large scale convex optimization problem, where the first layer optimizes the weights of the kernels to be combined.", "startOffset": 30, "endOffset": 99}, {"referenceID": 11, "context": "Just like some previous works (e.g. Rakotomamonjy et al., 2008; Xu et al., 2008; Nath et al., 2009) we start with the approach that views the MKL problem as a nested, large scale convex optimization problem, where the first layer optimizes the weights of the kernels to be combined.", "startOffset": 30, "endOffset": 99}, {"referenceID": 22, "context": "However, as opposed to these works whose underlying iterative methods have a complexity of \u03a9(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1).", "startOffset": 134, "endOffset": 216}, {"referenceID": 19, "context": "However, as opposed to these works whose underlying iterative methods have a complexity of \u03a9(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1).", "startOffset": 134, "endOffset": 216}, {"referenceID": 13, "context": "However, as opposed to these works whose underlying iterative methods have a complexity of \u03a9(d) for just any one iteration, following (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011) we use a randomized coordinate descent method, which was effectively used in these works to decrease the per iteration complexity to O(1). The role of randomization in our method is to use it to build an unbiased estimate of the gradient at the most recent iteration. The issue then is how the variance (and so the number of iterations required) scales with d. As opposed to the above mentioned works, in this paper we propose to make the distribution over the updated coordinate dependent on the history. We will argue that sampling from a distribution that is proportional to the magnitude of the gradient vector is desirable to keep the variance (actually, second moment) low and in fact we will show that there are interesting cases of MKL (in particular, the case of combining kernels coming from a polynomial family of kernels) when efficient sampling (i.e., sampling at a cost of O(log d)) is feasible from this distribution. Then, the variance is controlled by the a priori weights put on the kernels, making it potentially independent of d. Under these favorable conditions (and in particular, for the polynomial kernel set with some specific prior weights), the complexity of the method as a function of d becomes logarithmic, which makes our MKL algorithm feasible even for large scale problems. This is to be contrasted to the approach of Nesterov (2010, 2012) where a fixed distribution is used and where the a priori bounds on the method\u2019s convergence rate, and, hence, its computational cost to achieve a prescribed precision, will depend linearly on d (note that we are comparing upper bounds here, so the actual complexity could be smaller). Our algorithm is based on the mirror descent (or mirror descent) algorithm (similar to the work of Richt\u00e1rik and Tak\u00e1\u0109 (2011) who uses uniform distributions).", "startOffset": 135, "endOffset": 2002}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d.", "startOffset": 140, "endOffset": 196}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d.", "startOffset": 140, "endOffset": 332}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d. The algorithm of Bach (2008), though practically very efficient, suffers from the same deficiency.", "startOffset": 140, "endOffset": 387}, {"referenceID": 0, "context": "It is important to mention that there are algorithms designed to handle the case of infinitely many kernels, for example, the algorithms by Argyriou et al. (2005, 2006); Gehler and Nowozin (2008). However, these methods lack convergence rate guarantees, and, for example, the consistency for the method of Gehler and Nowozin (2008) works only for \u201csmall\u201d d. The algorithm of Bach (2008), though practically very efficient, suffers from the same deficiency. A very interesting proposal by Cortes et al. (2009) considers learning to combine a large number of kernels and comes with guarantees, though their algorithm restricts the family of kernels in a specific way.", "startOffset": 140, "endOffset": 509}, {"referenceID": 5, "context": ", Hastie et al. (2009). In supervised learning problems `t(y) = `(yt, y) for some loss function ` : R \u00d7 R \u2192 R, such as the squared-loss, `(yt, y) = 1 2(y \u2212 yt) 2, or the hinge-loss, `t(yt, y) = max(1 \u2212 yyt, 0), where in the former case yt \u2208 R, while in the latter case yt \u2208 {\u22121,+1}.", "startOffset": 2, "endOffset": 23}, {"referenceID": 5, "context": ", Hastie et al. (2009). In supervised learning problems `t(y) = `(yt, y) for some loss function ` : R \u00d7 R \u2192 R, such as the squared-loss, `(yt, y) = 1 2(y \u2212 yt) 2, or the hinge-loss, `t(yt, y) = max(1 \u2212 yyt, 0), where in the former case yt \u2208 R, while in the latter case yt \u2208 {\u22121,+1}. We note in passing that for the sake of simplicity, we shall sometimes abuse notation and write Ln(w) for Ln(fw) and even drop the index n when the sample-size is unimportant. As mentioned above, in this paper we consider the special case in (1) when the penalty is a so-called group p-norm penalty with 1 \u2264 p \u2264 2, a case considered earlier, e.g., by Kloft et al. (2011). Thus our goal is to solve", "startOffset": 2, "endOffset": 654}, {"referenceID": 20, "context": "More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled.", "startOffset": 110, "endOffset": 173}, {"referenceID": 9, "context": "More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled.", "startOffset": 110, "endOffset": 173}, {"referenceID": 14, "context": "More precisely, our proposed algorithm is an instance of a randomized version of the mirror descent algorithm (Rockafellar, 1976; Martinet, 1978; Nemirovski and Yudin, 1998), where in each time step only one coordinate of the gradient is sampled.", "startOffset": 110, "endOffset": 173}, {"referenceID": 12, "context": "In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent.", "startOffset": 27, "endOffset": 135}, {"referenceID": 22, "context": "In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent.", "startOffset": 27, "endOffset": 135}, {"referenceID": 19, "context": "In particular, compared to (Nemirovski et al., 2009a; Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011), our analysis allows for the conditional distribution of the noise in the gradient estimate to be history dependent.", "startOffset": 27, "endOffset": 135}, {"referenceID": 6, "context": "The convergence rate in the above theorem can be improved if stronger assumptions are made on J , for example if J is assumed to be strongly convex, see, for example, (Hazan et al., 2007; Hazan and Kale, 2011).", "startOffset": 167, "endOffset": 209}, {"referenceID": 7, "context": "The convergence rate in the above theorem can be improved if stronger assumptions are made on J , for example if J is assumed to be strongly convex, see, for example, (Hazan et al., 2007; Hazan and Kale, 2011).", "startOffset": 167, "endOffset": 209}, {"referenceID": 21, "context": "Note that this type of result is standard (see, e.g., Shawe-Taylor and Cristianini, 2004; Sch\u00f6lkopf and Smola, 2002), thus we include it only for the sake of completeness (the proof is included in Section A in the appendix).", "startOffset": 42, "endOffset": 116}, {"referenceID": 3, "context": "The importance of this example follows from the observation of G\u00f6nen and Alpayd\u0131n (2011) that the non-linear kernel learning methods of Cortes et al.", "startOffset": 63, "endOffset": 89}, {"referenceID": 2, "context": "The importance of this example follows from the observation of G\u00f6nen and Alpayd\u0131n (2011) that the non-linear kernel learning methods of Cortes et al. (2009), which can be viewed as a restricted form of learning polynomial kernels, are far the best MKL methods in practice and can significantly outperform state-of-the-art SVM with a single kernel or with the uniform combination of kernels.", "startOffset": 136, "endOffset": 157}, {"referenceID": 22, "context": "(2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011).", "startOffset": 127, "endOffset": 209}, {"referenceID": 19, "context": "(2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011).", "startOffset": 127, "endOffset": 209}, {"referenceID": 8, "context": "1 Convergence speed In this experiment we examine the speed of convergence of our method and compare it against one of the fastest standard multiple kernel learning algorithms, that is, the p-norm multiple kernel learning algorithm of Kloft et al. (2011) with p = 2,4 and the uniform coordinate descent algorithm that updates one coordinate per iteration uniformly at random (Nesterov, 2010, 2012; Shalev-Shwartz and Tewari, 2011; Richt\u00e1rik and Tak\u00e1\u0109, 2011).", "startOffset": 235, "endOffset": 255}, {"referenceID": 8, "context": "In this figure Stoch represents our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents the uniform coordinate descent algorithm.", "startOffset": 82, "endOffset": 102}, {"referenceID": 8, "context": "In this figure Stoch represents our algorithms, Kloft represents the algorithm of Kloft et al. (2011), and UCD represents the uniform coordinate descent algorithm. The results show that our method consistently outperforms the other algorithms in convergence speed. Note that our stochastic method updates one kernel coefficient per iteration, while Kloft updates ( r+D D ) kernel coefficients per iteration. The difference between the two methods is analogous to the difference between stochastic gradient vs. full gradient algorithms. While UCD also updates one kernel Note that p = 2 in Kloft et al. (2011) notation corresponds to p = 4/3 or \u03bd = 2 in our notation, which gives the same objective function that we minimize with Algorithm 2.", "startOffset": 82, "endOffset": 609}, {"referenceID": 7, "context": "We compare our method (Stoch) against the algorithm of Kloft et al. (2011) (Kloft), the nonlinear kernel learning method of Cortes et al.", "startOffset": 55, "endOffset": 75}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).", "startOffset": 56, "endOffset": 77}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).", "startOffset": 56, "endOffset": 149}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel.", "startOffset": 56, "endOffset": 290}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.", "startOffset": 56, "endOffset": 665}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3.", "startOffset": 56, "endOffset": 711}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al.", "startOffset": 56, "endOffset": 1046}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al.", "startOffset": 56, "endOffset": 1075}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al.", "startOffset": 56, "endOffset": 1093}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al.", "startOffset": 56, "endOffset": 1117}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al.", "startOffset": 56, "endOffset": 1138}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011) has the best performance overall.", "startOffset": 56, "endOffset": 1262}, {"referenceID": 2, "context": "(2011) (Kloft), the nonlinear kernel learning method of Cortes et al. (2009) (Cortes), and the hierarchical kernel learning algorithm of Bach (2008) (Bach).6 The set of base kernels consists of r linear kernels built from the input variables. Recall that the method of Cortes et al. (2009) only considers kernels of the form \u03ba\u03b8 = ( \u2211r i=1 \u03b8i\u03bai) D, where D is a predetermined integer that specifies the degree of nonlinear kernel. Note that adding a constant feature is equivalent to adding polynomial kernels of degree less than D to the combination too. We provide all possible product kernels of degree 0 to D to the kernel learning method of Kloft et al. (2011). For our method and the method of Bach (2008) we set the maximum kernel degree to D = 3. The results are shown in Figure 2, the mean squared errors are on the left plot, while the training times are on the right plot. In the training-time plot the numbers inside brackets While several fast MKL algorithms are available in the literature, such as those of Sonnenburg et al. (2006); Rakotomamonjy et al. (2008); Xu et al. (2010); Orabona and Luo (2011); Kloft et al. (2011), a comparison of the reported experimental results shows that from among these algorithms the method of Kloft et al. (2011) has the best performance overall. Hence, we decided to compare against only this algorithm. Also note that the memory and computational cost of all these methods still scale linearly with the number of kernels, making them unsuitable for the case we are most interested in. Furthermore, to keep the focus of the paper we compare our algorithm to methods with sound theoretical guarantees. As such, it remains for future work to compare with other methods, such as the infinite kernel learning of Gehler and Nowozin (2008), which lack such guarantees but exhibit promising performance in practice.", "startOffset": 56, "endOffset": 1784}, {"referenceID": 2, "context": "Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases.", "startOffset": 43, "endOffset": 64}, {"referenceID": 2, "context": "Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases. This is due to the restricted family of kernels that this method considers. The method of Bach (2008), which is well-suited to learn sparse combination of product kernels, performs better than Cortes et al.", "startOffset": 43, "endOffset": 200}, {"referenceID": 2, "context": "Note that the performance of the method of Cortes et al. (2009) starts to degrade as r increases. This is due to the restricted family of kernels that this method considers. The method of Bach (2008), which is well-suited to learn sparse combination of product kernels, performs better than Cortes et al. (2009) for higher input dimensions.", "startOffset": 43, "endOffset": 312}, {"referenceID": 2, "context": "We compare our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of Cortes et al. (2009) (Cortes).", "startOffset": 96, "endOffset": 117}, {"referenceID": 2, "context": "We compare our new algorithm (Stoch), the algorithm of Bach (2008) (Bach), and the algorithm of Cortes et al. (2009) (Cortes). For each algorithm we consider learning polynomial kernels of degree 2 and 3. We also include uniform combination of product kernels of degree D, i.e. \u03baD = ( \u2211r i=1 \u03bai) , for D \u2208 {1, 2, 3} (Uniform). To find out if considering higherorder interaction of input variables results in improved performance we also included a MKL algorithm to which we only feed linear kernels (D = 1). We use the MKL algorithm of Kloft et al. (2011) with p \u2208 {1, 2} (Kloft).", "startOffset": 96, "endOffset": 556}, {"referenceID": 1, "context": "1 is based on the standard proof of the convergence rate of the proximal point algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.", "startOffset": 108, "endOffset": 133}, {"referenceID": 1, "context": "1 is based on the standard proof of the convergence rate of the proximal point algorithm, see, for example, (Beck and Teboulle, 2003), or the proof of Proposition 2.2 of Nemirovski et al. (2009b), which carry over the same argument to solve very similar but less general problems.", "startOffset": 109, "endOffset": 196}, {"referenceID": 18, "context": ") Rakotomamonjy et al. (2008) specialized to the case when `t is the hinge loss.", "startOffset": 2, "endOffset": 30}], "year": 2013, "abstractText": "We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show the surprising result that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows the implementation of sampling from this distribution to run in O(log(d)) time, making the total computational cost of the method to achieve an optimal solution to be O(log(d)/ ), thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives.", "creator": "LaTeX with hyperref package"}}}