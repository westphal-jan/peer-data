{"id": "1205.0622", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2012", "title": "No-Regret Learning in Extensive-Form Games with Imperfect Recall", "abstract": "counterfactual regret minimization ( cfr ) is an efficient no - regret learning algorithm aiming for decision planning problems modeled as extensive games. cfr's term regret bounds depend on the requirement of perfect recall : players always remember information that was revealed to them and the order in which it surrendered was revealed. in formal games without perfect recall, however, cfr's guarantees don't apply. in this paper, we present the first regret bound for cfr when applied to compute a general practice class of games with imperfect recall. in addition, we show that cfr applied to any abstraction belonging to our general class results in a regret proof bound not just complete for the typical abstract game, yes but for the full game experience as well. we verify our theory and show how imperfect recall equations can be comfortably used to trade a small increase in regret for a significant reduction in memory in three domains : magic die - roll poker, dynamic phantom virtual tic - tac - nap toe, and bluff.", "histories": [["v1", "Thu, 3 May 2012 05:54:14 GMT  (79kb,D)", "http://arxiv.org/abs/1205.0622v1", "21 pages, 4 figures, expanded version of article to appear in Proceedings of the Twenty-Ninth International Conference on Machine Learning"]], "COMMENTS": "21 pages, 4 figures, expanded version of article to appear in Proceedings of the Twenty-Ninth International Conference on Machine Learning", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["marc lanctot", "richard g gibson", "neil burch", "michael bowling"], "accepted": true, "id": "1205.0622"}, "pdf": {"name": "1205.0622.pdf", "metadata": {"source": "CRF", "title": "No-Regret Learning in Extensive-Form Games with Imperfect Recall", "authors": ["Marc Lanctot", "Richard Gibson", "Neil Burch", "Martin Zinkevich", "Michael Bowling"], "emails": [], "sections": [{"heading": null, "text": "gorithm for decision problems modeled as extensive games. CFR\u2019s regret bounds depend on the requirement of perfect recall: players always remember information that was revealed to them and the order in which it was revealed. In games without perfect recall, however, CFR\u2019s guarantees do not apply. In this paper, we present the first regret bound for CFR when applied to a general class of games with imperfect recall. In addition, we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game, but for the full game as well. We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains: die-roll poker, phantom tic-tac-toe, and Bluff."}, {"heading": "1 Introduction", "text": "Many real-world problems can be modeled as a repeated decision-making task. For problems involving multiple agents, one can model the repeated task as a normal-form game. When the task incorporates sequential decisions involving imperfect information or stochastic events, an extensive game is a useful alternative. In such decision problems, a typical goal is to minimize regret: the amount of utility lost by playing a past sequence of strategies, versus playing the best, stationary strategy in hindsight.\nIn this paper, we consider the problem of minimizing regret in an extensive game. A common approach to achieving low regret in extensive games is the Counterfactual Regret Minimization (CFR) [Zinkevich et al., 2008] algorithm. CFR uses a regret minimizer at every decision point with an alternative notion of regret, which provably minimizes regret in the entire extensive game. However, convergence is limited to games exhibiting perfect recall: players never forget information that was revealed to them, nor the order in the which the information was revealed. For games with imperfect recall, CFR\u2019s original analysis provides no general guarantees.\nImperfect recall brings about a number of complications. In games with perfect recall, every mixed strategy (probability distribution over pure strategies) has a utility-equivalent behavioral strategy (probability distribution over actions at each decision point) [Kuhn, 1953]. While certain lossless imperfect recall games share this\nar X\niv :1\n20 5.\n06 22\nv1 [\ncs .G\nT ]\n3 M\nproperty [Kaneko and Kline, 1995], it is not true for imperfect recall games in general [Piccione and Rubinstein, 1996]. In addition, the decision problem of determining if a player can assure themself a certain payoff in an imperfect recall game is NPcomplete [Koller and Megiddo, 1992]. Two-player zero-sum games can be solved by constructing an appropriate linear program [Koller et al., 1994] or minimizing regret [Zinkevich et al., 2008], provided the game has perfect recall. Without perfect recall, however, the problem becomes exponential in the worst case [Koller et al., 1994].\nOn the other hand, imperfect recall extensive games are more versatile than perfect recall games for modelling large real-world problems. While perfect recall requires all past information to be remembered, imperfect recall allows irrelevant information to be forgotten so that the size of the game is smaller. As CFR\u2019s memory requirements are linear in the size of the game, more games become feasible through imperfect recall. Despite the complications above, CFR has empirically been shown to work well when applied to imperfect recall abstractions of Texas Hold\u2019em poker [Waugh et al., 2009b], but there is currently no theory to suggest why this is so.\nThis paper presents theoretical groundings for applying CFR to games exhibiting imperfect recall. We define a general class of imperfect recall games and provide a bound on CFR\u2019s regret in such games. For a subset of this class, CFR minimizes average regret in the extensive game. Moreover, our results also provide regret guarantees when applying CFR to an abstract game, provided the abstract game belongs to our general class. We test our theory in three different domains: die-roll poker, phantom tic-tac-toe, and Bluff. To the best of our knowledge, this work demonstrates the first theoretically-grounded, practical use of imperfect recall in extensive games."}, {"heading": "2 Background", "text": "An extensive-form game \u0393 with imperfect information [Osborne and Rubinstein, 1994] is a tuple \u3008N,A,H,Z, P, \u03c3c, u, I\u3009, where N is a finite set of players. A is a finite set of actions. H is a finite set of histories: a subset of the set of sequences of elements in A. A prefix of a history h\u2032 \u2208 H is a history h \u2208 H where h\u2032 begins with the sequence h; we denote prefix histories by h v h\u2032. For every h \u2208 H , define A(h) = {a : a \u2208 A, ha \u2208 H}, the set of valid actions at history h; P (h) \u2208 N \u222a {c} is the player to act at the history h, or chance if P (h) = c; and Hi = {h | h \u2208 H,P (h) = i}. Z \u2286 H is the set of terminal histories. A terminal history z \u2208 Z is a history where there does not exist any history h \u2208 H , h 6= z such that z v h. The utility function ui : Z \u2192 R gives the utility to player i \u2208 N , for each terminal history. If |N | = 2 and for all z \u2208 Z, \u2211 i\u2208N ui(z) = 0, we say the game is zero-sum.\nFor each player i \u2208 N , Ii is a partition of Hi with the property that A(h) = A(h\u2032) whenever h and h\u2032 are in the same member of the partition. We call Ii the information partition of player i, and a set I \u2208 Ii is an information set of player i. A player, when taking actions, cannot distinguish between two histories in the same information set. For I \u2208 Ii we denote A(I) as the set A(h) for any h \u2208 I . Define I(h) to be the information set containing h. In this paper, we restrict ourselves to games where players cannot reach the same information set twice in a single game. Thus, we assume\nthat for all i \u2208 N and h, h\u2032 \u2208 Hi,\nh v h\u2032, h 6= h\u2032 \u21d2 I(h) 6= I(h\u2032). (1)\nFinally, \u03c3c is the fixed \u201cstrategy\u201d of the special player chance. \u03c3c(h, a) gives the probability that chance event a occurs at h. For all h \u2208 Hc, \u2211 a\u2208A(h) \u03c3c(h, a) = 1 and the decisions at any h are independent of the decision at any other h\u2032 6= h. Given a history h, define Xi(h) to be the sequence of information set, action pairs such that (I, a) \u2208 Xi(h) if I \u2208 Ii and there exists h\u2032 v h such that h\u2032 \u2208 I and h\u2032a v h. The order of the pairs in Xi(h) is the order in which they occur in h. Define X(h) to be the sequence of information set, action pairs belonging to all players in the order in which they occur in h, and X\u2212i(h) similarly, by removing player i\u2019s information set, action pairs from X(h). Also, define X(h, h\u2032) to be the sequence of information set, action pairs belonging to all players that start at h and end at h\u2032 when h v h\u2032; if h 6v h\u2032, X(h, h\u2032) is defined to be the empty sequence. Finally, Xi(h, h\u2032) and X\u2212i(h, h\u2032) are similarly defined.\nDefinition 1. An extensive game has perfect recall if for every player i \u2208 N , for every information set I \u2208 Ii, for any h, h\u2032 \u2208 I : Xi(h) = Xi(h\u2032). Otherwise, the game has imperfect recall.\nIntuitively, with perfect recall every player has an infallible memory: they cannot \u201cforget\u201d anything during a play of the game that they once knew. Hence, what a player knows at I is a composition of what the player has discovered in the past up to this point and the precise order in which information was discovered. Note that every perfect recall game satisfies equation (1), but not every imperfect recall game does.\nA (behavioral) strategy \u03c3i for player i is a function such that for each history h \u2208 Hi, \u03c3i(h) is a probability distribution over A(h). Furthermore, it is required that \u03c3i(h) = \u03c3i(h\n\u2032) for all h, h\u2032 \u2208 I , and we denote that as \u03c3i(I). The set of all such strategies for player i is denoted by \u03a3i. A strategy profile \u03c3 \u2208 \u03a3 is a collection of strategies, one for each player, i.e. in a two-player game \u03c3 = (\u03c31, \u03c32). By notational convention, \u03c3\u2212i refers to the set of strategies including every strategy in \u03c3 except player i\u2019s strategy.\nFor any \u03c3 \u2208 \u03a3, i \u2208 N \u222a{c}, and h \u2208 H , define \u03c0\u03c3i (h) = \u220f h\u2032avh,P (h\u2032)=i \u03c3i(h\n\u2032, a) to be the probability that player i plays to reach history h under \u03c3. We can then define \u03c0\u03c3(h) = \u220f i\u2208N\u222a{c} \u03c0 \u03c3 i (h) to be the probability that history h is reached under \u03c3. Let \u03c0\u03c3\u2212i(h) be the product of all players\u2019 contribution (including chance) except that of player i. Furthermore, let \u03c0\u03c3i (h, h\n\u2032) be the probability of player i playing to reach history h\u2032 after h, given h has occurred. Let \u03c0\u03c3(h, h\u2032) and \u03c0\u03c3\u2212i(h, h\n\u2032) be defined similarly. Finally, we can define the expected utility of a strategy profile \u03c3 for player i to be\nui(\u03c3) = Ez\u2208Z [ui(z)] = \u2211 z\u2208Z ui(z)\u03c0 \u03c3(z).\nWe will say that a game \u0393\u2032 = \u3008N,A\u2032, H, Z, P, \u03c3c, u, I \u2032\u3009 is an abstraction, or an abstract game, of \u0393 = \u3008N,A,H,Z, P, \u03c3c, u, I\u3009 if for all i \u2208 N and h, k \u2208 Hi, A\u2032(h) \u2286 A(h) and I(h) = I(k) implies I \u2032(h) = I \u2032(k). In this paper, we only consider abstractions where A = A\u2032. A typical use of abstraction is to reduce the size of the game by ensuring that |I \u2032| < |I|."}, {"heading": "3 Example: Die-Roll Poker", "text": "We now introduce a game that we will use as a running example throughout the paper.\nDie-roll poker (DRP) is a simplified two-player poker game that uses dice rather than cards. To begin, each player antes one chip to the pot. There are two betting rounds, where at the beginning of each round, players roll a private six-sided die. The game has imperfect information due to the players not seeing the result of the opponent\u2019s die rolls. During a betting round, a player may fold (forfeit the game), call (match the current bet), or raise (increase the current bet) by a fixed number of chips, with a maximum of two raises per round. In the first round, raises are worth two chips, whereas in the second round, raises are worth four chips. If both players have not folded by the end of the second round, a showdown occurs where the player with the largest sum of their two dice wins all of the chips in the pot.\nDRP is naturally a game with perfect recall; players remember the exact sequence of bets made and the exact outcome of each die roll from both rounds. However, consider an imperfect recall version of DRP, DRP-IR, where at the beginning of the second round, both players \u201cforget\u201d their first die roll and only know the sum of their two dice. In other words, DRP-IR is an abstraction of DRP where any two histories are in the same abstract information set if and only if the sum of the player\u2019s private dice is the same and the sequence of betting is the same. DRP-IR has imperfect recall since histories that were distinguishable in the first round (for example, a roll of 1 and a roll of 4) are no longer distinguishable in the second round (for example, a roll of 1 followed by a roll of 5, and a roll of 4 followed by a roll of 2)."}, {"heading": "4 Counterfactual Regret Minimization", "text": "Given a sequence of strategy profiles \u03c31, \u03c32, ..., \u03c3T , the (external) regret for player i,\nRTi = max \u03c3\u2032\u2208\u03a3i T\u2211 t=1 ( ui(\u03c3 \u2032, \u03c3t\u2212i)\u2212 ui(\u03c3ti , \u03c3t\u2212i) ) ,\nis the amount of utility player i could have gained had she played the best single strategy in hindsight for all time steps t \u2208 {1, 2, ..., T}. An algorithm minimizes regret, or is a no-regret algorithm, for player i if the average positive regret approaches zero; i.e., limT\u2192\u221eR T,+ i /T = 0, where x\n+ = max{x, 0}. Having no regret is a desirable property. For example, it is well known that in a zero-sum game, if both players\u2019 average regret is bounded above by , then the average of the strategy profiles generated is a 2 -Nash equilibrium.\nCounterfactual Regret Minimization (CFR) is an iterative no-regret learning algorithm for extensive-form games having perfect recall. On each iteration t, CFR recursively traverses the entire game tree, computing the expected utility for player i at each information set I \u2208 Ii under the current profile \u03c3t, assuming player i plays to reach I . This expectation is the counterfactual value for player i,\nvi(\u03c3, I) = \u2211 z\u2208ZI ui(z)\u03c0 \u03c3 \u2212i(z[I])\u03c0 \u03c3(z[I], z),\nwhere ZI is the set of terminal histories passing through I and z[I] is the prefix of z contained in I (z[I] is unique by equation (1)). For each action a \u2208 A(I), these values determine the counterfactual regret at iteration t, rti(I, a) = vi(\u03c3tI\u2192a, I)\u2212 vi(\u03c3t, I), where \u03c3I\u2192a is the profile \u03c3 except at I , action a is always taken. The regret rti(I, a) measures how much player i would rather play action a at I than play \u03c3t. Finally, \u03c3t is updated by applying regret matching [Hart and Mas-Colell, 2000, Zinkevich et al., 2008] to the immediate counterfactual regrets,RTi (I, a) = \u2211T t=1 r t i(I, a), according to\n\u03c3T+1(I, a) = RT,+i (I, a)\u2211\nb\u2208A(I)R T,+ i (I, b)\n,\nwith actions chosen uniformly at random when the denominator is zero. Regret matching is a no-regret learner that minimizes the per-information set immediate counterfactual regret,\nmax a\u2208A(I)\nRTi (I, a)\nT \u2264 \u2206i \u221a |A(I)|\u221a T , (2)\nwhere \u2206i = maxz,z\u2032\u2208Z ui(z)\u2212ui(z\u2032). In games having perfect recall, minimizing the immediate counterfactual regrets at every information set in turn minimizes average regret, RTi /T . This is because perfect recall implies that the regret is bounded by the sum of the positive parts of the immediate counterfactual regrets [Zinkevich et al., 2008],\nRTi \u2264 \u2211 I\u2208Ii max a\u2208A(I) RT,+i (I, a), (3)\nand thus RTi T \u2264\n\u2206i|Ii| \u221a |Ai|\u221a\nT , (4)\nwhere |Ai| = maxI\u2208Ii |A(I)|. CFR must store the immediate counterfactual regret for each information set, action pair, and thus CFR\u2019s memory requirements are O(|Ii||Ai|).\nWhile equation (2) still holds in imperfect recall games, equation (3) and consequently equation (4) are not guaranteed to hold. An example game where CFR would exhibit high regret is provided in Section 7. Consequently, the regret for playing according to the CFR algorithm is unknown in general for imperfect recall games. However, the advantage of applying CFR to DRP-IR, for example, is that this imperfect recall game contains fewer information sets than the full game, and thus less memory is required by CFR. Although DRP is a toy example and is small enough to run CFR on the full game, this example is useful for understanding the concepts in the rest of this paper."}, {"heading": "5 CFR with Imperfect Recall", "text": "In this section, we investigate the application of CFR to games with imperfect recall. We begin by showing that CFR minimizes regret for a class of games that we call \u201cwellformed games.\u201d We then present a bound on the average regret for a more general class of imperfect recall games that we call \u201cskew well-formed games.\u201d"}, {"heading": "5.1 Well-formed Games", "text": "For games \u0393 = \u3008N,A,H,Z, P, \u03c3c, u, I\u3009 and \u0393\u0306 = \u3008N,A,H,Z, P, \u03c3c, u, I\u0306\u3009, we say that \u0393\u0306 is a perfect recall refinement of \u0393 if \u0393\u0306 has perfect recall and \u0393 is an abstraction of \u0393\u0306. So, the information available to players in \u0393\u0306 is never forgotten, and is at least as informative as the information available to them in \u0393. For example, DRP is a perfect recall refinement of DRP-IR. Every game has at least one perfect recall refinement by simply making \u0393\u0306 a perfect information game (I\u0306 = {h} for all I\u0306 \u2208 I\u0306i). Furthermore, a perfect recall game is a perfect recall refinement of itself. For I \u2208 Ii, we define\nP\u0306(I) = {I\u0306 | I\u0306 \u2208 I\u0306i, I\u0306 \u2286 I}\nto be the set of all information sets in I\u0306i that are subsets of I . Note that our notion of refinement is similar to the one described by Kaneko & Kline (1995). Our definition differs in that we consider any possible refinement, whereas Kaneko & Kline consider only the coarsest such refinement.\nDefinition 2. For a game \u0393 and a perfect recall refinement \u0393\u0306, we say that \u0393 is a wellformed game with respect to \u0393\u0306 if for all i \u2208 N , I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), there exists a bijection \u03c6 : ZI\u0306 \u2192 ZI\u0306\u2032 and constants kI\u0306,I\u0306\u2032 , `I\u0306,I\u0306\u2032 \u2208 [0,\u221e) such that for all z \u2208 ZI\u0306 :\n(i) ui(z) = kI\u0306,I\u0306\u2032ui(\u03c6(z)),\n(ii) \u03c0c(z) = `I\u0306,I\u0306\u2032\u03c0c(\u03c6(z)),\n(iii) In \u0393, X\u2212i(z) = X\u2212i(\u03c6(z)), and\n(iv) In \u0393, Xi(z[I\u0306], z) = Xi(\u03c6(z)[I\u0306 \u2032], \u03c6(z)).\nWe say that \u0393 is a well-formed game if it is well-formed with respect to some perfect recall refinement.\nRecall that ZI is the set of terminal histories containing a prefix in the information set I , and that z[I] is that prefix. Intuitively, a game is well-formed if for each information set I \u2208 Ii, the structures around each I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I) of some perfect recall refinement are isomorphic across four conditions. Conditions (i) and (ii) state that the corresponding utilities and chance frequencies at each terminal history are proportional. Condition (iii) asserts that the opponents can never distinguish the corresponding histories at any point in \u0393. Finally, condition (iv) states that player i cannot distinguish between corresponding histories from I\u0306 and I\u0306 \u2032 until the end of the game.\nConsider again DRP as a perfect recall refinement of DRP-IR. In DRP, the available actions are independent of dice outcomes, and the final utilities are only dependent on the final sum of the players\u2019 dice. Therefore, in DRP the utilities are equivalent between, for example, the terminal histories where player i rolled a 1 followed by a 5, and the terminal histories where player i rolled a 4 followed by a 2 (condition (i)). In addition, the chance probabilities of reaching each terminal history are equal (condition (ii)). Furthermore, the opponents can never distinguish between two isomorphic histories since player i\u2019s rolls are private (condition (iii)). Finally, in DRP-IR, player i\nnever remembers the outcome of the first roll from the second round on (condition (iv)). Thus, DRP-IR is well-formed with respect to DRP, with constants kI\u0306,I\u0306\u2032 = `I\u0306,I\u0306\u2032 = 1.\nAny perfect recall game is well-formed with respect to itself since P\u0306(I) = {I}, \u03c6 equal to the identity bijection, and kI\u0306,I\u0306\u2032 = `I\u0306,I\u0306\u2032 = 1 satisfies Definition 2. However, many imperfect recall games are also well-formed, with DRP-IR being one example. An additional example is presented in Section 6.\nWe now show that CFR can be applied to any well-formed game to minimize average regret. A sketch of the proof is described below, while a full proof is provided as supplementary material.\nTheorem 1. If \u0393 is well-formed with respect to \u0393\u0306, then the average regret in \u0393\u0306 for player i of choosing strategies according to CFR in \u0393 is bounded by\nR\u0306Ti T \u2264\n\u2206iK \u221a |Ai|\u221a\nT ,\nwhere K = \u2211 I\u2208Ii maxI\u0306,I\u0306\u2032\u2208P\u0306(I) kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 .\nProof sketch. One can show that conditions (i) to (iv) of Definition 2 imply that the positive regrets are proportional between any two information sets in \u0393\u0306 that are merged in the well-formed game, \u0393. In other words, for all I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), and a \u2208 A(I),\nRT,+i (I\u0306 , a) = kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (I\u0306 \u2032, a).\nSince regrets between \u0393 and \u0393\u0306 are additive, i.e.,\nRTi (I, a) = \u2211\nI\u0306\u2208P\u0306(I)\nRTi (I\u0306 , a) for all I \u2208 Ii,\nthe proportionality implies that minimizing regret at each I \u2208 Ii minimizes regret at each I\u0306 \u2208 I\u0306i. Because \u0393\u0306 has perfect recall, applying equation (3) gives the result.\nSince the strategy space is more expressive in \u0393\u0306 than in \u0393 (\u03a3 \u2286 \u03a3\u0306), RTi \u2264 R\u0306Ti and thus it immediately follows that the average regret in \u0393 is minimized. In the case when \u0393 has perfect recall, because \u0393 is well-formed with respect to itself, Theorem 1 with K = |Ii| is a direct generalization of the original CFR bound in equation (4). Theorem 1 not only guarantees regret minimization for perfect recall games, but also for well-formed imperfect recall games."}, {"heading": "5.2 Skew Well-formed Games", "text": "We now present a generalization of well-formed games to which a regret bound can still be derived.\nDefinition 3. For a game \u0393 and a perfect recall refinement \u0393\u0306, we say that \u0393 is a skew well-formed game with respect to \u0393\u0306 if for all i \u2208 N , I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), there exists a bijection \u03c6 : ZI\u0306 \u2192 ZI\u0306\u2032 and constants kI\u0306,I\u0306\u2032 , \u03b4I\u0306,I\u0306\u2032 , `I\u0306,I\u0306\u2032 \u2208 [0,\u221e) such that for all z \u2208 ZI\u0306 :\n(i) \u2223\u2223\u2223ui(z)\u2212 kI\u0306,I\u0306\u2032ui(\u03c6(z))\u2223\u2223\u2223 \u2264 \u03b4I\u0306,I\u0306\u2032 ,\n(ii) \u03c0c(z) = `I\u0306,I\u0306\u2032\u03c0c(\u03c6(z)),\n(iii) In \u0393, X\u2212i(z) = X\u2212i(\u03c6(z)), and\n(iv) In \u0393, Xi(z[I\u0306], z) = Xi(\u03c6(z)[I\u0306 \u2032], \u03c6(z)).\nWe say that \u0393 is a skew well-formed game if it is skew well-formed with respect to some perfect recall refinement.\nThe only difference between Definitions 2 and 3 is in condition (i). While utilities must be exactly proportional in a well-formed game, utilities in a skew well-formed game must only be proportional up to a constant \u03b4I\u0306,I\u0306\u2032 . Note that any well-formed game is skew well-formed by setting \u03b4I\u0306,I\u0306\u2032 = 0.\nFor example, consider a new version of DRP called Skew-DRP(\u03b4) with slightly modified payouts at the end of the game. Whenever the game reaches a showdown, player 1 receives a bonus \u03b4 times the number of chips in the pot from player 2 if player 1\u2019s second die roll was even; otherwise, no bonus is awarded. The pot is then awarded to the player with the highest dice sum as usual. Analogously, define SkewDRP-IR(\u03b4) to be the imperfect recall abstraction of Skew-DRP(\u03b4) where in the second round, players only remember the sum of their two dice. Now, Skew-DRP-IR(\u03b4) is not well-formed with respect to Skew-DRP(\u03b4). To see this, note that the utilities resulting from the rolls 1,5 and the rolls 4,2 and the same sequence of betting are not exactly proportional because the second roll 5 is odd but 2 is even (utilities are off by \u03b4 times the pot size). However, Skew-DRP-IR(\u03b4) is skew well-formed with respect to SkewDRP(\u03b4) with \u03b4I\u0306,I\u0306\u2032 = \u03b4 times the maximum pot size attainable from I .\nUnfortunately, there is no guarantee that regret will be minimized by CFR in a skew well-formed game. However, we can still bound regret in a predictable manner according to the degree that the utilities are skewed:\nTheorem 2. If \u0393 is skew well-formed with respect to \u0393\u0306, then the average regret in \u0393\u0306 for player i of choosing strategies according to CFR in \u0393 is bounded by\nR\u0306Ti T \u2264\n\u2206iK \u221a |Ai|\u221a T + \u2211 I\u2208Ii |P\u0306(I)|\u03b4I ,\nwhere K = \u2211 I\u2208Ii maxI\u0306,I\u0306\u2032\u2208P\u0306(I) kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 and \u03b4I = maxI\u0306,I\u0306\u2032\u2208P\u0306(I) \u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 .\nThe proof is similar to that of Theorem 1. Theorem 8 shows that as T approaches infinity, the bound on our regret approaches \u2211 I\u2208Ii |P\u0306(I)|\u03b4I . Our experiments in Section 6 demonstrate that as the skew \u03b4 grows, so does our regret in Skew-DRP(\u03b4) after a fixed number of iterations.\nRemarks. Theorems 1 and 8 are, to our knowledge, the first to provide such theoretical guarantees in imperfect recall settings. However, these results are also relevant with regards to regret in the full game when CFR is applied to an abstraction. Recall that if \u0393 has perfect recall, then \u0393 is a perfect recall refinement of any (skew) wellformed abstract game. Thus, if we choose an abstraction that yields a (skew) wellformed game, then applying CFR to the abstract game achieves a bound on the average\nregret in the full game, \u0393. This is true regardless of whether the abstraction exhibits perfect recall or imperfect recall. Previous counterexamples show that abstraction in general provides no guarantees in the full game [Waugh et al., 2009a]. In contrast, our results show that applying CFR to an abstract game leads to bounded regret in the full game, provided we restrict ourselves to (skew) well-formed abstractions. If such an abstract game is much smaller than the full game, a significant amount of memory is saved when running CFR."}, {"heading": "6 Empirical Evaluation", "text": "To complement our theoretical results, we apply CFR to both players simultaneously in several zero-sum imperfect recall (abstract) games, and measure the sum of the average regrets for both players in a perfect recall refinement (the full game). Along with the small DRP domain and its variants, we also consider the challenging domains of phantom tic-tac-toe and Bluff, which we now describe.\nPhantom tic-tac-toe. As in regular tic-tac-toe, phantom tic-tac-toe (PTTT) is played on a 3-by-3 board, initially empty, where the goal is to claim three squares along the same row, column, or diagonal. However, in PTTT, players\u2019 actions are private. Each turn, a player attempts to take a square of their choice. If they fail due to the opponent having taken that square on a previous turn, the same player keeps trying to take an alternative square until they succeed. Players are not informed about how many attempts the opponent made before succeeding. The game ends immediately if there is ever a connecting line of squares belonging to the same player. The winner receives a payoff of +1, while the losing player receives\u22121. In PTTT, the total number of histories |H| \u2248 1010.\nBluff. Bluff, also known as Liar\u2019s Dice, Dudo, and Perudo, is a dice-bidding game. In our version, Bluff(D1,D2), each die has six sides with faces 1 to 6. Each player i rolls Di of these dice and looks at them without showing them to the opponent. Each round, players alternate by bidding on the outcome of all dice in play until one player claims that the other is bluffing (i.e., claims that the bid does not hold). A bid consists of a quantity of dice and a face value. A face of 6 is considered \u201cwild\u201d and counts as matching any other face. For example, the bid 2x5 represents the claim that there are at least two dice with a face of 5 (or 6) among both players\u2019 dice. To place a new bid, the player must increase either the quantity or face value of the current bid; in addition, lowering the face is allowed if the quantity is increased. The player calling bluff wins the round if the opponent\u2019s last bid is incorrect, and loses otherwise. The losing player removes one of their dice from the game and a new round begins, starting with the player who won the previous round. When a player has no more dice left, they have lost the game. A utility of +1 is given for a win and \u22121 for a loss. In this paper, we restrict ourselves to the case where D1 = D2 = 2. Note that since Bluff(2,2) is a multi-round game, the expected values of Bluff(1,1) are precomputed for payoffs at the leaves of Bluff(2,1), which is then solved for leaf payoffs in the full Bluff(2,2) game. In Bluff(2,2), the total number of histories |H| \u2248 1010."}, {"heading": "6.1 Results", "text": "We consider several different imperfect recall abstractions for DRP, Skew-DRP(\u03b4), PTTT, and Bluff. For the DRP games, we apply DRP-IR and Skew-DRP-IR(\u03b4) respectively as described in Section 5. Our PTTT and Bluff experiments, however, also investigate the effects of imperfect recall beyond skew well-formed games. In the full, perfect recall version of PTTT, each player remembers the order of every failed and every successful move she makes throughout the entire game. In our first abstract game, FOSF, players forget the order of successive failures within the same turn. Clearly, there is an isomorphism between any two merged information sets I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I) since the order of the actions does not affect the available future moves or utilities. Players still remember which turn each success and each failure occurred, and so the opponent\u2019s sequences of actions must be equal across the isomorphism. Thus, FOSF is wellformed. Our remaining PTTT abstractions, however, are not even skew well-formed. In FOI, players independently remember the sequence of failures and the sequence of successful actions, but not how the actions interleave. In FOS, players remember the order of failed actions, but not the order of successes. Finally, in FOE, players only know what actions they have taken and remember nothing about the order in which they were taken. FOI, FOS, and FOE are not skew well-formed because no isomorphism can preserve the order of the opponent\u2019s previous information set, action pairs (breaking condition (iii) of Definitions 2 and 3). In Bluff, we use abstractions described by Neller and Hnath (2011) that force players to forget everything except the last r bids. Similarly, these abstract games are not skew well-formed because the players forget information that the opponent could previously distinguish. The size of each DRP, PTTT, and Bluff game is given in Table 1. Here, A = {(I, a) : i \u2208 N, I \u2208 Ii, a \u2208 A(I)} is the set of all information set, action pairs. Note that Skew-DRP(\u03b4) is the same size as\nDRP regardless of the skew, and recall that CFR requires space linear in |A|. For each game, we ran CFR1 on both players, meaning that each player\u2019s opponent was an identical copy of the same no-regret learner. The sum of the average regrets for each player over number of iterations is shown in Figure 1. The SkewDRP-IR(\u03b4) experiments show that as \u03b4 increases, so does the regret as predicted by Theorem 8, though \u2211 I\u2208Ii\n\u2223\u2223\u2223P\u0306(I)\u2223\u2223\u2223 \u03b4I appears to be a very loose bound on the final regret. In PTTT, regret diverges from zero for FOI, FOS, and FOE, where FOS appears to provide slightly better strategies than FOI and FOE. While our theory cannot explain why FOS performs better, this does match our intuition that remembering information about the opponent\u2019s moves is important. For a small increase in average regret, FOS reduces the space required by 87% compared to FOSF\u2019s 20% reduction. Note that for both DRP and PTTT, running CFR on the full, perfect recall game achieves the same regret as in the well-formed abstractions (Skew-DRP-IR(0) and FSOF) and is thus not shown. In Bluff, we see that regret consistently worsens as fewer previous bids are remembered. This suggests that a result similar to Theorem 8 for skew-well-formed games may hold if condition (iii) of Definition 2 is less constrained, though the proper formulation for such a relaxation remains unclear. Nonetheless, choosing r = 8 saves 85% of the memory with only a very small increase in average regret after millions of iterations."}, {"heading": "7 Discussion", "text": "Well-formed games are described by four conditions provided in Definition 2. Recall that Koller & Megiddo (1992) prove that determining a player\u2019s guaranteed payoff in an imperfect recall game is NP-complete. However, Koller & Megiddo\u2019s NP-hardness reduction creates an imperfect recall game that breaks conditions (i), (iii), and (iv) of Definition 2. In this section, we discuss the following question: For minimizing regret, how important is it to satisfy each individual condition of Definition 2?\nSkew well-formed games and Theorem 8 show that one can relax condition (i) of Definition 2 and still derive a bound on the average regret. In addition, most of our PTTT and Bluff abstractions from the previous section do not satisfy condition (iii), but CFR still produces reliable results. This suggests that it may be possible to relax condition (iii) in a similar manner to the relaxation of condition (i) introduced by skew well-formed games. While we leave this question open, we now demonstrate that breaking condition (iii) can lead CFR to a dead-lock situation where one player has constant average regret.\nLet us walk through the process of applying CFR to the game in Figure 2. Note that this game satisfies all of the conditions of Definition 2, except for condition (iii). To begin, the current strategy profile \u03c31 is set to be uniform random at every information set. Under this profile, when player 1 is at I3, each of the four histories are equally likely. Thus, vi(\u03c31(I3\u2192l), I3) = vi(\u03c3 1 (I3\u2192r), I3) = vi(\u03c3\n1, I3) = 0, and so r11(I3, l) = r 1 1(I3, r) = 0. In addition, under \u03c3 1, the counterfactual value of the pass\n1Similar to Zinkevich et al. (2008), we used the chance sampling variant of CFR.\n(p) and continue (c) actions at both I1 and I2 is zero, and thus the immediate counterfactual regrets at I1 and I2 on iteration 1 are also zero. Player 2, however, has positive immediate counterfactual regret for passing (p) at histories ac and ec (to always receive \u03be utility) and for continuing (c) at bc and de (to always avoid receiving \u2212\u03be utility), and has negative immediate counterfactual regret for continuing at ac and ec and for passing at bc and de. Therefore, the next profile \u03c32 still has player 1 playing uniformly random everywhere, but player 2 now always passes at ac and ec, and always continues at bc and dc. On the second iteration of CFR, the positive regrets for player 1 at I3 remain the same because the histories bcc and dcc are equally likely. Also, player 2\u2019s positive regrets remain the same at all four histories in H2. However, player 1\u2019s expected utility for continuing at I1 or I2 is now negative since player 2 now passes at ac and ec. Thus, player 1 gains positive regret for passing at both I1 and I2. This leads us to the next profile \u03c33 = {(I1, p) = 1, (I2, p) = 1, (ac, p) = 1, (bc, p) = 0, (dc, p) = 0, (ec, p) = 1, (I3, l) = 0.5}. One can check that running CFR for more iterations yields \u03c3t = \u03c33 for all t \u2265 3. The average regret for playing this way will be constant and hence does not approach zero because player 1 would rather play \u03c3\u20321 = {(I1, p) = 1, (I2, p) = 0, (I3, l) = 0} and get u1(\u03c3\u20321, \u03c332) = (1\u2212 \u03be)/4 > u1(\u03c33) for \u03be \u2208 (0, 1). A similar example can be constructed where condition (iii) holds, but chance\u2019s probabilities are not proportional (breaking condition (ii)).\nDespite the problem of breaking condition (iii), condition (iv) of Definition 2 can be relaxed. Rather than enforcing player i\u2019s future information to be the same across the bijection \u03c6, we only require that the corresponding subtrees be isomorphic, allowing\nplayer i to re-remember information that was previously forgotten. The details for this relaxation are in the supplementary material. However, it is not clear that this relaxation is possible in skew well-formed games, nor does it seem to provide any practical advantage."}, {"heading": "8 Conclusion", "text": "We have provided the first set of theoretical guarantees for CFR in imperfect recall games. We defined well-formed and skew well-formed games and provided bounds on the average regret that results from applying CFR to such games. In addition, our theory shows that we can achieve low average regret in a full, perfect recall game when employing CFR on an abstract version of the game, provided the abstract game is skew well-formed (with or without imperfect recall). Our DRP experiments confirm these theoretical results, while our PTTT and Bluff experiments hint that it may be possible to still bound regret in other types of imperfect recall games. Future work will look to expand on the set of imperfect recall games to which CFR can be reliably applied. In particular, it may be possible to derive regret bounds for a new class of games where conditions (ii) and (iii) of Definition 2 are relaxed."}, {"heading": "Acknowledgments", "text": "We would like to thank the Computer Poker Research Group at the University of Alberta for their helpful discussions that contributed to this work. This work was supported by NSERC, Alberta Innovates \u2013 Technology Futures, and the use of computing resources provided by WestGrid and Compute Canada."}, {"heading": "Appendix A", "text": "In this section, we will prove Theorems 1 and 2 of the main paper. Note that by the definition of counterfactual value, the regrets between \u0393 and a perfect recall refinement \u0393\u0306 are additive; specifically, for I \u2208 Ii in \u0393,\nRTi (I, a) = \u2211\nI\u0306\u2208P\u0306(I)\nRTi (I\u0306 , a). (5)\nFirst, we provide a lemma that generalizes Theorem 4 of (Zinkevich et al., 2008) by showing that if the immediate counterfactual regrets of each I\u0306 \u2208 P\u0306(I) are proportional up to some difference D, then the average regret can be bounded above:\nLemma A. Let \u0393\u0306 be a perfect recall refinement of a game \u0393. If for all I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), and a \u2208 A(I), there exist constants CI\u0306,I\u0306\u2032,a, DI\u0306,I\u0306\u2032,a \u2208 [0,\u221e) such that\n1\nT\n\u2223\u2223\u2223RT,+i (I\u0306 , a)\u2212 CI\u0306,I\u0306\u2032,aRT,+i (I\u0306 \u2032, a)\u2223\u2223\u2223 \u2264 DI\u0306,I\u0306\u2032,a, (6)\nthen the average regret in \u0393\u0306 is bounded by\nR\u0306Ti T \u2264 \u2206iC \u221a |Ai|\u221a T + \u2211 I\u2208I |P\u0306(I)|DI ,\nwhere C = \u2211 I\u2208Ii max I\u0306,I\u0306\u2032\u2208P\u0306(I),a\u2208A(I) CI\u0306,I\u0306\u2032,a\nand DI = max\nI\u0306,I\u0306\u2032\u2208P\u0306(I),a\u2208A(I) DI\u0306,I\u0306\u2032,a.\nProof. R\u0306Ti \u2264 \u2211 I\u0306\u2208I\u0306i max a\u2208A(I) RT,+i (I\u0306 , a) by Theorem 3 of (Zinkevich et al., 2008)\n= \u2211 I\u2208Ii \u2211 I\u0306\u2208P\u0306(I) max a\u2208A(I) RT,+i (I\u0306 , a) by definition of a perfect recall refinement\n\u2264 \u2211 I\u2208Ii |P\u0306(I)|RT,+i (I\u0306 \u2217, a\u2217) where I\u0306\u2217 = arg max I\u0306\u2208P\u0306(I) max a\u2208A(I) RT,+i (I\u0306 , a)\nand a\u2217 = arg max a\u2208A(I) RT,+i (I\u0306 \u2217, a)\n\u2264 \u2211 I\u2208Ii |P\u0306(I)| ( CI\u0306\u2217,I\u0306\u2217\u2217,a\u2217R T,+ i (I\u0306 \u2217\u2217, a\u2217) + TDI\u0306\u2217,I\u0306\u2217\u2217,a\u2217 ) by (6),\nwhere I\u0306\u2217\u2217 = arg min I\u0306\u2208P\u0306(I) RTi (I\u0306 , a \u2217)\n\u2264 \u2211 I\u2208Ii |P\u0306(I)|CI\u0306\u2217,I\u0306\u2217\u2217,a\u2217  1 |P\u0306(I)| \u2211 I\u0306\u2208P\u0306(I) RTi (I\u0306 , a \u2217) + + T \u2211 I\u2208Ii |P\u0306(I)|DI\nbecause the minimum is less than the average and (\u00b7)+ is monotone increasing = \u2211 I\u2208Ii CI\u0306\u2217,I\u0306\u2217\u2217,a\u2217R T,+ i (I, a \u2217) + T \u2211 I\u2208Ii |P\u0306(I)|DI by (5) \u2264 \u2211 I\u2208Ii CI\u0306\u2217,I\u0306\u2217\u2217,a\u2217T \u221a\u221a\u221a\u221a\u221a \u2211 a\u2208A(I) ( RT,+i (I, a) T )2 + T \u2211 I\u2208Ii |P\u0306(I)|DI\n\u2264 \u2211 I\u2208Ii CI\u0306\u2217,I\u0306\u2217\u2217,a\u2217\u2206i \u221a |A(I)| \u221a T + T \u2211 I\u2208Ii |P\u0306(I)|DI\nby Theorem 6 of (Lanctot et al., 2009) \u2264 \u2206iC \u221a |Ai| \u221a T + T \u2211 I\u2208Ii |P\u0306(I)|DI .\nDividing both sides by T establishes the lemma.\nNote that if \u0393 has perfect recall, then the constants CI,I,a = 1 and DI,I,a = 0 for all I \u2208 Ii and a \u2208 A(I) satisfy the condition of Lemma A. In this case, C = |Ii| and DI = 0, and so RTi /T \u2264 \u2206i|Ii| \u221a |Ai|/ \u221a T , recovering Theorem 4 of (Zinkevich et al., 2008). We now use Lemma A to prove Theorems 1 and 2:\nTheorem 2. If \u0393 is skew well-formed with respect to \u0393\u0306, then the average regret in \u0393\u0306 for player i of choosing strategies according to CFR in \u0393 is bounded by\nR\u0306Ti T \u2264\n\u2206iK \u221a |Ai|\u221a T + \u2211 I\u2208Ii |P\u0306(I)|\u03b4I ,\nwhere K = \u2211 I\u2208Ii maxI\u0306,I\u0306\u2032\u2208P\u0306(I) kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 and \u03b4I = maxI\u0306,I\u0306\u2032\u2208P\u0306(I) \u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 .\nProof. We will show that for all I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), and a \u2208 A(I),\n1\nT \u2223\u2223\u2223RT,+i (I\u0306 , a)\u2212 kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032RT,+i (I\u0306 \u2032, a)\u2223\u2223\u2223 \u2264 \u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 , (7) which, by Lemma A, proves the theorem.\nFix I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), and a \u2208 A(I). Firstly, for all z \u2208 ZI\u0306 and \u03c3 \u2208 \u03a3, by conditions (ii) and (iii) of Definition 3, we have\n\u03c0\u03c3\u2212i(z) = \u03c0c(z) \u220f\n(I,a)\u2208X\u2212i(z)\n\u03c3(I, a)\n= `I\u0306,I\u0306\u2032\u03c0c(\u03c6(z)) \u220f\n(I,a)\u2208X\u2212i(\u03c6(z))\n\u03c3(I, a)\n= `I\u0306,I\u0306\u2032\u03c0 \u03c3 \u2212i(\u03c6(z)) (8)\nand by condition (iv) of Definition 3, we similarly have\n\u03c0\u03c3i (z[I\u0306], z) = \u03c0 \u03c3 i (\u03c6(z)[I\u0306 \u2032], \u03c6(z)) (9)\nand \u03c0\u03c3i (z[I\u0306]a, z) = \u03c0 \u03c3 i (\u03c6(z)[I\u0306 \u2032]a, \u03c6(z)). (10)\nWe can then bound the positive part of the immediate counterfactual regretRT,+i (I\u0306 , a) above by\nRT,+i (I\u0306 , a) = ( T\u2211 t=1 rti(I\u0306 , a) )+\n=  T\u2211 t=1 \u2211 z\u2208ZI\u0306 \u03c0\u03c3\u2212i(z)(\u03c0 \u03c3 i (z[I\u0306]a, z)\u2212 \u03c0\u03c3i (z[I\u0306], z))ui(z) + \u2264 ( T\u2211 t=1 \u2211 z\u2208ZI\u0306 `I\u0306,I\u0306\u2032\u03c0 \u03c3 \u2212i(\u03c6(z))(\u03c0 \u03c3 i (\u03c6(z)[I\u0306 \u2032]a, \u03c6(z))\n\u2212 \u03c0\u03c3i (\u03c6(z)[I\u0306 \u2032], \u03c6(z)))(kI\u0306,I\u0306\u2032ui(\u03c6(z)) + \u03b4I\u0306,I\u0306\u2032)) +\nby equations (8), (9), (10), and condition (i) of Definition 3\n= ( T\u2211 t=1 \u2211 z\u2208ZI\u0306\u2032 `I\u0306,I\u0306\u2032\u03c0 \u03c3 \u2212i(z)(\u03c0 \u03c3 i (z[I\u0306 \u2032]a, z)\n\u2212 \u03c0\u03c3i (z[I\u0306 \u2032], z))(kI\u0306,I\u0306\u2032ui(z) + \u03b4I\u0306,I\u0306\u2032)) +\nsince \u03c6 is a bijection\n\u2264  T\u2211 t=1 \u2211 z\u2208ZI\u0306\u2032 kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032\u03c0 \u03c3 \u2212i(z)(\u03c0 \u03c3 i (z[I\u0306]a, z)\u2212 \u03c0\u03c3i (z[I\u0306], z))ui(z) +\n+  T\u2211 t=1 \u2211 z\u2208ZI\u0306\u2032 \u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032\u03c0 \u03c3 \u2212i(z)(\u03c0 \u03c3 i (z[I\u0306]a, z)\u2212 \u03c0\u03c3i (z[I\u0306], z)) +\n\u2264 kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (I\u0306 \u2032, a) + T\u2211 t=1 \u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032\u03c0 \u03c3 \u2212i(I\u0306 \u2032) \u2264 kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (I\u0306 \u2032, a) + T\u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 , (11)\nwhere the last line follows because \u03c0\u03c3\u2212i(I\u0306 \u2032) = \u2211 z\u2208ZI\u0306\u2032 \u03c0\u03c3\u2212i(z[I\u0306 \u2032]) \u2264 1 in a perfect\nrecall game \u0393\u0306. Similarly,\nRT,+i (I\u0306 , a) \u2265 kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (I\u0306 \u2032, a)\u2212 T\u03b4I\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032 , (12)\nwhich together with equation (11) and dividing by T establishes (7), completing the proof. Note that Theorem 1 immediately follows from Theorem 2 since a well-formed game is skew well-formed with \u03b4I\u0306,I\u0306\u2032 = 0 for all I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I)."}, {"heading": "Appendix B", "text": "In this section, we consider an alternative extension of well-formed games that relaxes condition (iv) of Definition 2. For a subset of histories S \u2286 Hi, define\nDi(S) = {I | I \u2208 Ii,\u2203h \u2208 S, h\u2032 \u2208 I such that h v h\u2032}\nto be the set of all information sets descending from any history in S.\nDefinition 4. For a game \u0393 and a perfect recall refinement \u0393\u030c, we say that \u0393 is a nearly well-formed game with respect to \u0393\u030c if for all i \u2208 N , I \u2208 Ii, I\u030c , I\u030c \u2032 \u2208 P\u030c(I), J \u2208 Di(I\u030c), there exist bijections \u03c6 : ZI\u030c \u2192 ZI\u030c\u2032 , \u03c8 : Di(I\u030c) \u2192 Di(I\u030c \u2032), \u03c9 : A(J) \u2192 A(\u03c8(J)) and constants kI\u030c,I\u030c\u2032 , `I\u030c,I\u030c\u2032 \u2208 [0,\u221e) such that for all z \u2208 ZI\u030c :\n(i) ui(z) = kI\u030c,I\u030c\u2032ui(\u03c6(z)),\n(ii) \u03c0c(z) = `I\u030c,I\u030c\u2032\u03c0c(\u03c6(z)),\n(iii) In \u0393, X\u2212i(z) = X\u2212i(\u03c6(z)), and\n(iv) Xi(z[I\u030c], z) = (J1, a1), ..., (Jm, am) if and only if Xi(\u03c6(z)[I\u030c \u2032], \u03c6(z)) = (\u03c8(J1), \u03c9(a1)), ..., (\u03c8(Jm), \u03c9(am)).\nWe say that \u0393 is a nearly well-formed game if it is nearly well-formed with respect to some perfect recall refinement.\nIn a nearly well-formed game, condition (iv) says that player i may now remember information that was once forgotten, provided the descendants from I\u030c and I\u030c \u2032 are isomorphic across \u03c6. This relaxes the corresponding condition for a well-formed game where player i could never remember information once it was forgotten. Clearly, any well-formed game is nearly well-formed by choosing \u03c8 and \u03c9 to be the identity bijections.\nFor example, consider a longer version of DRP, DRP-3, that consists of three betting rounds instead of two where a third die is rolled at the beginning of round 3. We then define DRP-IR-3 to be the imperfect recall abstraction of DRP-3 where during round 2, players only know the sum of their two dice. In round 3, players once again know the outcome of each individual die roll, recovering information from the first round that was forgotten in the second. For instance, corresponding histories where player i\u2019s first two rolls were 1,5 and where her first two rolls were 4,2 will be in the same information set during round 2, but will be in different information sets in round 3. However, betting is independent of dice rolls and utilities are only dependent on the final sum of the three dice. Therefore, the descendants from these histories are isomorphic across \u03c6 and thus DRP-IR-3 is nearly well-formed with respect to DRP-3.\nCFR guarantees that the average regret is also minimized in nearly well-formed games:\nTheorem 3. If \u0393 is nearly well-formed with respect to \u0393\u0306, then the average regret in \u0393\u0306 for player i of choosing strategies according to CFR in \u0393 is bounded by\nR\u030cTi T \u2264\n\u2206iK \u221a |Ai|\u221a\nT ,\nwhere K = \u2211 I\u2208Ii maxI\u030c,I\u030c\u2032\u2208P\u030c(I) kI\u030c,I\u030c\u2032`I\u030c,I\u030c\u2032 .\nProof. Fix I \u2208 Ii, I\u0306 , I\u0306 \u2032 \u2208 P\u0306(I), and a \u2208 A(I). By conditions (ii) and (iii) of Definition 4, equation (8) holds.\nClaim: RTi (J, b) = kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T i (\u03c8(J), \u03c9(b)) for all J \u2208 Di(I\u0306), b \u2208 A(J), T \u2265 0.\nProvided the claim is true, we have\n\u03c3T+1(J, b) =  RT,+i (J,b)\u2211 d\u2208A(J) R T,+ i (J,d) if \u2211 d\u2208A(J)R T,+ i (J, d) > 0\n1 |A(J)| otherwise\n=  kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (\u03c8(J),\u03c9(b))\u2211 d\u2208A(J) kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (\u03c8(J),\u03c9(b)) if \u2211 d\u2208A(J) kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T,+ i (\u03c8(J), \u03c9(b)) > 0\n1 |A(\u03c8(J))| otherwise\nsince \u03c9 is a bijection\n= \u03c3T+1(\u03c8(J), \u03c9(b)) (13)\nfor all J \u2208 Di(I\u0306), b \u2208 A(J), T \u2265 0. Therefore, for t \u2265 1,\n\u03c0\u03c3 t i (z[I\u0306], z) = \u220f\n(J,b)\u2208Xi(z[I\u0306],z)\n\u03c3t(J, b)\n= \u220f\n(J,b)\u2208Xi(z[I\u0306],z)\n\u03c3t(\u03c8(J), \u03c9(b))\n= \u220f\n(J,b)\u2208Xi(\u03c6(z)[I\u0306\u2032],\u03c6(z))\n\u03c3t(J, b) by condition (iv) of Definition 4\n= \u03c0\u03c3 t i (\u03c6(z)[I\u0306 \u2032], \u03c6(z)),\nand thus equation (9) and similarly equation (10) hold for \u03c3 = \u03c3t. By following the proof of Theorem 2, we then have that equations (11) and (12) with \u03b4I\u0306,I\u0306\u2032 = 0 hold, and hence equation (7) with \u03b4I\u0306,I\u0306\u2032 = 0 holds. This establishes the theorem by Lemma A.\nTo complete the proof, we are left to show that the claim holds. We will do so by induction on T . The base case T = 0 holds since R0i (I, a) = 0 for all I \u2208 Ii, a \u2208 A(I). For the inductive step, assume that RT\u22121i (J, b) = kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032R T\u22121 i (\u03c8(J), \u03c9(b)) for all J \u2208 Di(I\u0306), b \u2208 A(J). We will show that RTi (J, b) = kI\u0306,I\u0306\u2032`I\u0306,I\u0306\u2032RTi (\u03c8(J), \u03c9(b)) for all J \u2208 Di(I\u0306), b \u2208 A(J).\nFix J \u2208 Di(I\u0306) and b \u2208 A(J). By equation (13), we have for all z \u2208 ZJ ,\n\u03c0\u03c3 T i (z[J ], z) = \u220f\n(J\u2032,b\u2032)\u2208Xi(z[J],z)\n\u03c3T (J \u2032, b\u2032)\n= \u220f\n(J\u2032,b\u2032)\u2208Xi(z[J],z)\n\u03c3T (\u03c8(J \u2032), \u03c9(b\u2032)) by equation (13)\n= \u220f\n(J\u2032,b\u2032)\u2208Xi(\u03c6(z)[\u03c8(J)],\u03c6(z))\n\u03c3T (J \u2032, b\u2032)\nby condition (iv) of Definition 4 since Xi(z[J ], z) is a subsequence\n(more precisely, a suffix) of Xi(z[I\u0306], z)\n= \u03c0\u03c3 T\ni (\u03c6(z)[\u03c8(J)], \u03c6(z)) (14)\nand similarly \u03c0\u03c3 T\ni (z[J ]b, z) = \u03c0 \u03c3T i (\u03c6(z)[\u03c8(J)]\u03c9(b), \u03c6(z)). (15)\nNow consider the counterfactual regret at time T ,\nrTi (J, b) = \u2211 z\u2208ZJ \u03c0\u03c3 T \u2212i (z)(\u03c0 \u03c3T i (z[J ]b, z)\u2212 \u03c0\u03c3 T i (z[J ], z))ui(z)\n= \u2211 z\u2208ZJ `I\u0306,I\u0306\u2032\u03c0 \u03c3T \u2212i (\u03c6(z))(\u03c0 \u03c3T i (\u03c6(z)[\u03c8(J)]\u03c9(b), \u03c6(z))\n\u2212 \u03c0\u03c3 T\ni (\u03c6(z)[\u03c8(J)], \u03c6(z)))kI\u0306,I\u0306\u2032ui(\u03c6(z))\nby equations (14), (15) and conditions (i), (ii), and (iii) of Definition 4\n= `I\u0306,I\u0306\u2032kI\u0306,I\u0306\u2032r T i (\u03c8(J), \u03c9(b)).\nFinally,\nRTi (J, b) = T\u2211 t=1 rti(J, b)\n= RT\u22121i (J, b) + r T i (J, b) = `I\u0306,I\u0306\u2032kI\u0306,I\u0306\u2032(R T\u22121 i (\u03c8(J), \u03c9(b)) + r T i (\u03c8(J), \u03c9(b)))\nby the induction hypothesis and the above\n= `I\u0306,I\u0306\u2032kI\u0306,I\u0306\u2032 T\u2211 t=1 rti(\u03c8(J), \u03c9(b)) = `I\u0306,I\u0306\u2032kI\u0306,I\u0306\u2032R T i (\u03c8(J), \u03c9(b)),\nestablishing the inductive step. This completes the proof."}], "references": [{"title": "A simple adaptive procedure leading to correlated", "author": ["Sergiu Hart", "Andreu Mas-Colell"], "venue": "equilibrium. Econometrica,", "citeRegEx": "Hart and Mas.Colell.,? \\Q2000\\E", "shortCiteRegEx": "Hart and Mas.Colell.", "year": 2000}, {"title": "Behavior strategies, mixed strategies and perfect recall", "author": ["Mamoru Kaneko", "J. Jude Kline"], "venue": "International Journal of Game Theory,", "citeRegEx": "Kaneko and Kline.,? \\Q1995\\E", "shortCiteRegEx": "Kaneko and Kline.", "year": 1995}, {"title": "The complexity of two-person zero-sum games in extensive form", "author": ["Daphne Koller", "Nimrod Megiddo"], "venue": "Games and Economic Behavior,", "citeRegEx": "Koller and Megiddo.,? \\Q1992\\E", "shortCiteRegEx": "Koller and Megiddo.", "year": 1992}, {"title": "Fast algorithms for finding randomized strategies in game trees", "author": ["Daphne Koller", "Nimrod Megiddo", "Bernhard von Stengel"], "venue": "In Proceedings of the 26th ACM Symposium on Theory of Computing (STOC", "citeRegEx": "Koller et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1994}, {"title": "Extensive games and the problem of information", "author": ["Harold W. Kuhn"], "venue": "Contributions to the Theory of Games,", "citeRegEx": "Kuhn.,? \\Q1953\\E", "shortCiteRegEx": "Kuhn.", "year": 1953}, {"title": "Monte carlo sampling for regret minimization in extensive games. Technical Report TR09-15", "author": ["Marc Lanctot", "Kevin Waugh", "Martin Zinkevich", "Michael Bowling"], "venue": "University of Alberta,", "citeRegEx": "Lanctot et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Lanctot et al\\.", "year": 2009}, {"title": "Approximating optimal Dudo play with fixedstrategy iteration counterfactual regret minimization", "author": ["Todd W. Neller", "Steven Hnath"], "venue": "In Computers and Games,", "citeRegEx": "Neller and Hnath.,? \\Q2011\\E", "shortCiteRegEx": "Neller and Hnath.", "year": 2011}, {"title": "A Course in Game Theory", "author": ["Martin J. Osborne", "Ariel Rubinstein"], "venue": null, "citeRegEx": "Osborne and Rubinstein.,? \\Q1994\\E", "shortCiteRegEx": "Osborne and Rubinstein.", "year": 1994}, {"title": "On the interpretation of decision problems with imperfect recall", "author": ["Michele Piccione", "Ariel Rubinstein"], "venue": "In Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge,", "citeRegEx": "Piccione and Rubinstein.,? \\Q1996\\E", "shortCiteRegEx": "Piccione and Rubinstein.", "year": 1996}, {"title": "Abstraction pathologies in extensive games", "author": ["Kevin Waugh", "Dave Schnizlein", "Michael Bowling", "Duane Szafron"], "venue": "In he Eight International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Waugh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Waugh et al\\.", "year": 2009}, {"title": "A practical use of imperfect recall", "author": ["Kevin Waugh", "Martin Zinkevich", "Michael Johanson", "Morgan Kan", "David Schnizlein", "Michael Bowling"], "venue": "In Proceedings of SARA 2009: The Eighth Symposium on Abstraction, Reformulation and Approximation,", "citeRegEx": "Waugh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Waugh et al\\.", "year": 2009}, {"title": "Regret minimization in games with incomplete information", "author": ["Martin Zinkevich", "Michael Johanson", "Michael Bowling", "Carmelo Piccione"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zinkevich et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zinkevich et al\\.", "year": 2008}, {"title": "We now use Lemma A to prove Theorems 1 and 2: Theorem 2. If \u0393 is skew well-formed with respect to \u0393\u0306, then the average regret in \u0393\u0306", "author": [], "venue": "(Zinkevich et al.,", "citeRegEx": "\u221a,? \\Q2008\\E", "shortCiteRegEx": "\u221a", "year": 2008}], "referenceMentions": [{"referenceID": 11, "context": "A common approach to achieving low regret in extensive games is the Counterfactual Regret Minimization (CFR) [Zinkevich et al., 2008] algorithm.", "startOffset": 109, "endOffset": 133}, {"referenceID": 4, "context": "In games with perfect recall, every mixed strategy (probability distribution over pure strategies) has a utility-equivalent behavioral strategy (probability distribution over actions at each decision point) [Kuhn, 1953].", "startOffset": 207, "endOffset": 219}, {"referenceID": 1, "context": "property [Kaneko and Kline, 1995], it is not true for imperfect recall games in general [Piccione and Rubinstein, 1996].", "startOffset": 9, "endOffset": 33}, {"referenceID": 8, "context": "property [Kaneko and Kline, 1995], it is not true for imperfect recall games in general [Piccione and Rubinstein, 1996].", "startOffset": 88, "endOffset": 119}, {"referenceID": 2, "context": "In addition, the decision problem of determining if a player can assure themself a certain payoff in an imperfect recall game is NPcomplete [Koller and Megiddo, 1992].", "startOffset": 140, "endOffset": 166}, {"referenceID": 3, "context": "Two-player zero-sum games can be solved by constructing an appropriate linear program [Koller et al., 1994] or minimizing regret [Zinkevich et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 11, "context": ", 1994] or minimizing regret [Zinkevich et al., 2008], provided the game has perfect recall.", "startOffset": 29, "endOffset": 53}, {"referenceID": 3, "context": "Without perfect recall, however, the problem becomes exponential in the worst case [Koller et al., 1994].", "startOffset": 83, "endOffset": 104}, {"referenceID": 7, "context": "An extensive-form game \u0393 with imperfect information [Osborne and Rubinstein, 1994] is a tuple \u3008N,A,H,Z, P, \u03c3c, u, I\u3009, where N is a finite set of players.", "startOffset": 52, "endOffset": 82}, {"referenceID": 11, "context": "This is because perfect recall implies that the regret is bounded by the sum of the positive parts of the immediate counterfactual regrets [Zinkevich et al., 2008], R i \u2264 \u2211", "startOffset": 139, "endOffset": 163}, {"referenceID": 6, "context": "In Bluff, we use abstractions described by Neller and Hnath (2011) that force players to forget everything except the last r bids.", "startOffset": 43, "endOffset": 67}, {"referenceID": 11, "context": "In addition, under \u03c3 , the counterfactual value of the pass 1Similar to Zinkevich et al. (2008), we used the chance sampling variant of CFR.", "startOffset": 72, "endOffset": 96}], "year": 2012, "abstractText": "Counterfactual Regret Minimization (CFR) is an efficient no-regret learning algorithm for decision problems modeled as extensive games. CFR\u2019s regret bounds depend on the requirement of perfect recall: players always remember information that was revealed to them and the order in which it was revealed. In games without perfect recall, however, CFR\u2019s guarantees do not apply. In this paper, we present the first regret bound for CFR when applied to a general class of games with imperfect recall. In addition, we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game, but for the full game as well. We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains: die-roll poker, phantom tic-tac-toe, and Bluff.", "creator": "LaTeX with hyperref package"}}}