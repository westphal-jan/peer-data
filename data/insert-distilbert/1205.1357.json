{"id": "1205.1357", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-May-2012", "title": "Detecting Spammers via Aggregated Historical Data Set", "abstract": "the battle between email service providers and senders of advanced mass unsolicited emails ( spam ) continues to gain traction. vast numbers of spam emails are therefore sent mainly from automatic botnets distributed over the world. one example method for mitigating spam results in a computationally efficient centralized manner is fast and accurate blacklisting times of the senders. in attempting this work we propose a new sender reputation mechanism that automatically is based on an aggregated large historical records data - set entity which encodes the behavior of mail transfer agents over unspecified time. a combined historical data - set is periodically created visually from labeled log logs of received emails. we use machine learning algorithms to build a model that predicts the \\ emph { spammingness } of identified mail transfer agents in the near sighted future. the proposed mechanism is targeted mainly at large enterprises and email service providers and can be used for updating well both the black and the white lists. we evaluate the proposed secrecy mechanism using 9. 5m anonymized log entries obtained from the biggest internet service provider in europe. experiments show that proposed method detects that more than 94 % blocks of the message spam emails that allegedly escaped the blacklist ( i. os e., tpr ), while having less than 0. 5 % false - alarms. therefore, the effectiveness of the proposed method is much higher than of various previously reported reputation mechanisms, which rely directly on emails logs. in sum addition, the proposed method, when used for updating both the black and white lists, eliminated the need in automatic content inspection of 4 out of 5 incoming emails, since which resulted only in dramatic reduction difficulties in the filtering computational load.", "histories": [["v1", "Mon, 7 May 2012 12:16:27 GMT  (429kb,D)", "http://arxiv.org/abs/1205.1357v1", "This is a conference version of the HDS research. 13 pages 10 figures"]], "COMMENTS": "This is a conference version of the HDS research. 13 pages 10 figures", "reviews": [], "SUBJECTS": "cs.CR cs.LG", "authors": ["eitan menahem", "rami puzis"], "accepted": false, "id": "1205.1357"}, "pdf": {"name": "1205.1357.pdf", "metadata": {"source": "CRF", "title": "Detecting Spammers via Aggregated Historical Data Set", "authors": ["Eitan Menahem", "Rami Puzis"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Surveys show that in recent years 76% to 90% of all email traffic can be considered abusive [20, 1]. A major portion of those billions of Spam emails annually are automatically produced by botnets [22, 14]. Bots create and exploit free web-mail accounts or deliver Spam emails\ndirectly to victim mailboxes by exploiting the computational power and network bandwidth of their hosts and sometimes even user credentials. Many Spam mitigation methods are used by email service providers and organizations to protect mail boxes of their customers and employees respectively. There are three main approaches for Spam mitigation; content-based filtering (CBF), realtime blacklisting or DNS based blacklists, and sender reputation mechanisms (SRM). All three approaches are briefly described in Section 2.\nWhile CBF are considered as the most accurate Spam mitigation methods, they are also the most computationally intensive and sometimes considered as privacy infringing. In order to speed up the filtering process, organizations maintain blacklists of repeated Spam senders [28, 27, 26]. Those blacklists usually complement existing CBF methods by performing the first rough filtering of incoming emails. Organizations that choose to maintain their own blacklists gain flexibility in blocking / unblocking certain addresses and the ability to respond to emerging Spam attacks in real-time. Flexibility in managing the blacklist is also very important for large email service providers that must react immediately if they receive complaints about emails not reaching their destination.\nSender reputation mechanisms are used to refine blacklisting strategies by learning the liability of mail transfer agents (MTA). Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5]. In Section 3 we elaborate on sender reputation mechanisms. Beside estimating the liability of MTAs, SRMs also help to respond to emerging Spam attacks in a timely manner. By analyzing MTA behavior they try to identify patterns which indicate that the particular network address or a subnet is exploited by Spammers. SRMs are especially important when one requires a quick detection of sending pattern changes of mail transfer agents. For example, a white-listed address\nar X\niv :1\n20 5.\n13 57\nv1 [\ncs .C\nR ]\n7 M\nay 2\nbelonging to a small bankrupted firm that once maintained legitimate email servers is a lickerish target for exploitation by Spammers. A Good SRM should be able to detect a change in the behavior of such address and suggest removing it from a white list and adding it to a black list.\nIn this paper we investigate methods for updating the reputation of sender MTAs from the perspective of a single email service provider. Based on an anonymized log of 9.5M emails, obtained from a large email service provider, we created a historical data set (HDS) that encodes the behavior of sender MTAs over time, as described in Section 5. Machine learning (ML) algorithms were applied on both the features extracted from the email log and on the HDS in order to create a sender behavior models for deducing the black and white lists. We empirically compare email log (EL) based models, HDS based models, and commonly used blacklisting heuristic in a setup where these SRMs are applied on emails that have passed the provider\u2019s blacklist.\nBased on experiment results, described in Section 7, we show that an analysis of past behavior of a sender MTAs encoded in HDS enables the filtering out of as much as 94% Spam emails that have passed the blacklist while having only 0.5% false positives. Finally, we also show that by frequent updating of both the black and white-list it is possible to spare content analysis of roughly 82% of incoming emails. We discuss the results and limitations of this study and propose directions for future research in Section 8."}, {"heading": "2 Background", "text": "In this section we discuss in further details three prominent approaches for Spam mitigation and some important previous works focusing on machine learning and sender reputation mechanisms. Spam-blocking approaches can be roughly divided into three main categories: (1) content-based filtering (CBF), (2) real-time blacklisting (RBL), and (3) sender reputation mechanisms (SRM) (see Table 1). Next, we briefly describe the different categories.\nContent-based filtering (CBF) refers to techniques in which emails\u2019 body, attached executables, pictures\nor other files are analyses and processed for producing some features upon which email classification is made [3, 9, 6, 16, 33]. The email\u2019s content is related to the application-level, the highest level of the Open Systems Interconnection (OSI) model. Content-based features have a lot of useful information for classification, however, in an Internet Service Provider\u2019s (ISP) perspective, there are some disadvantages. First, in order to classify the incoming emails, each email must be put through a relatively heavy-weight content-based filter. This leads to a lot of computational resources wasted on filtering, and thus makes it fairly costly when compare to other approaches, such as real-time blacklisting, which will be discussed later. A second disadvantage of CBF is that Spammers continuously improve their CBF evading techniques. For example, instead of sending plain textual Spam emails, they use Spam-images or smarter textual content that obfuscates the unwanted content.\nReal-time blacklists (RBL) are IP-based lists that contain IP prefixes of spamming MTAs and are regarded as network-level filters. Using the RBL, large firms such as ISPs can filter out emails originating from spamming IPs. The filtering is very fast since the decision to accept or reject the email does not require receiving the full email (saving network resources) nor processing its content (saving computational resources). In order to avoid misclassification, RBLs must be updated systematically. For example, Spamhaus [28], Spam-Cop [27], and SORBS [26] are some initiatives that keep RBL systems updated by tracking and reporting spammer IPs. RBL methods, however, cannot solve the Spam email problem entirely, as spammers can escape them by, for example, repeatedly changing their IPs by stealing local network IPs [14], or by temporarily stealing IPs using BGP hijacking attacks [22]. Another shortcoming of RBL is that whenever an IP prefix is blacklisted, both spammers and benign senders who share the same prefix might be rejected. Benign senders can also be blocked because of inaccurate blacklisting heuristics. In order to lower the false-positive rates, blacklisting heuristics limit their true positive rates, allowing many Spam-mails to pass the filter and block mainly repeated spammers. RBL usually has lower accuracy than CBF, which is an acceptable trade-off given its real-time nature and low utilization of computational resources.\nSender reputation mechanisms (SRM) for Spam mitigation are methods for computing a liability score for email senders. The computation is usually based on information extracted from the network or transport level, social network information, or other useful information sources. According to Alperovitch et. al [2], sender reputation systems should react quickly to changes in sender\u2018s behavioral patterns. That is, when sender\u2018s sending patterns take a shape of a spammer, his reputa-\ntion should decrease. If the reputation of a sender is below the specified threshold, the system should reject his mails, at least until the sender gains up some reputation by changing his sending properties. One of the strong advantages of sender reputation mechanisms is that they complement and improve RBLs both in terms of detection accuracy and response to changes.\nDespite the fact that the addresses of MTS which were spotted repeatedly sending Spam will usually not start sending legitimate emails all of a studden, there are several reasons to have accurate SRMs that react quickly to changes in sender behavior. First, addresses of once legitimate email servers that are no longer used are exploited by spammers due to their high reputation in databases of large email service providers. Spammers can use the window of opportunity created by such addresses for as long as it takes for SRMs to detect the change in behavior of these addresses. Quick reaction of SRM is also required when small legitimate email service providers are used to launch massive Spam attacks. When such an attack is detected, operations may decide to temporarily blacklist the provider in order to avoid being overwhelmed by Spam emails. However, as soon as the attack is over, the provider addresses should be removed from the blacklist.\nFinally, the importance of SRMs will further increase with the prevalence of IPv6 in the Internet. The impact of botnets on the prevalence of Spam is significantly reduced by blacklisting all known dynamic IP ranges. This simple heuristic assumes that legitimate MTAs are not hosted by end users, which are given dynamic IPs by their Internet service providers. Currently, in order to simplify the DHCP configuration, dynamic IPv4 addresses are arranged in continuous ranges which are very easy to blacklist. However, with IPv6, there will not be a necessity for dynamic addresses and the whole address space is expected to become more fragmented, making it difficult to blacklist large IP ranges with simple heuristics. Furthermore, the extremely large address space and auto-configuration functionality of IPv6 are expected to increase the cost of Spam mitigation [17].\nCurrent paper describes SRM that is based on aggregated spatio-temporal and application level features extracted from logs of incoming email."}, {"heading": "3 Related Works", "text": "In this section we discuss some sender reputation mechanisms that share a similar problem domain as the proposed HDS based method. Several works have used machine learning algorithms to compute sender reputation from data sets which are not based on email content analysis.\nRamachandran et al. [23], research a sender reputation and blacklisting approach. They present a blacklisting system, SpamTracker, to classify email senders based on their sending behavior rather than the MTA\u2019s IP addresses. The authors assume that spammers abuse multiple benign MTAs at different times, but their sending patterns tend to remain mostly similar even when shifting from one abused IP to another. SpamTracker extracts network-level features, such as received time, remote IP, targeted domain, whether rejected, and uses spectrum clustering algorithms to cluster email servers. The clustering is performed on the email destination domains. The authors reported a 10.4% TPR when using SpamTracker on a data set of 620 Spam mails which were missed by the organization filter and eventually were blacklisted in the following weeks.\nTang et al. [29] addressed the Spam imbalanced classification task with a new version of SVM, the GSVMBA. The imbalance problem exists in the Spam detection domain due to the fact that there are around 10 Spam mails for each non-Spam mail [1]. The main two attributes GSVM-BA adds to SVM are a granular computing, which makes it more computationally efficient and a mechanism for under sampling the data set positive instances, so that a good classifier could be learned in a highly unbalanced domain. The authors use the proposed GSVN-BA to classify IPs into Spam-IP or non-SpamIPs, (i.e., learns IPs reputation). They used two types of aggregate features, which are both derived from sender\u2019s IP, receiver\u2019s IPs, and a sending time. Their experiments showed a very high precision rate at 99.87% with a recall of 47%.\nSender reputation mechanisms are not only limited to network-level features; reputation can also be learned from a much higher communication level, such as the social network level. The social-network-based filtering approach takes advantage of the natural trust system of social networks. For example, if the sender and the receiver belong to the same social group, the communication is probably legitimate. On the other hand, if the sender does not belong to any trustworthy social group, he is more likely to be blacklisted. There are many methods which make a good use of social networks to create both black and white lists. For example, J. Balthrop et al. [4], used email address books to compute sender trust degree.\nBoykin and Roychowdhury [7] extract social network\u2019s features from the email header fields such as From, To, and CC, from which they construct a social graph, as can be observed from a single user\u2019s perspective. Later, they find cluster of users who can be regarded as trusted. Finally, they train a classifier on the email addresses in the white list and black list. The authors reported 56% TPR with the black list and 34% TPR with\nthe white list. Their method, empirically tested on three data sets of several thousands of emails, did not have any false positives. The downside of the proposed algorithm is that both the black and white lists can be made erroneous. For example, the black list can be fooled by attackers which use spyware to learn the user\u2019s frequently used address list and have one or more of them added to the Spam mail so that the co-recipients (the Spam victims) will look like they belong to the user\u2019s social network. The white list can also be fooled by sending Spam mail by using one or more of the user\u2019s friends accounts. This can be done, for example, if the friend\u2019s computer has been compromised by a bot which selectively send Spam mails.\nA supervised collaborative approach for learning the reputation of networks is presented by Golbeck and Hendler [11]. The proposed mechanism is aided by the user\u2019s own scores for email senders. For example, a user can assign a high reputation score to his closest friends, and they in their turn may assign a high reputation rank to their friends. In this way, a reputation network is created. The reputation network may be used as a white-list as a recommendation system with very low false positive rate or allow the user to sort emails by reputation score. Similar approach is presented by Xie and Wang [32]. They focus on collaboration among several email domains in order to increase the coverage of senders. Each provider compares the email histories obtained from its peers via the proposed Simple Email Reputation Protocol (SERP) with its own records in order to establish trustworthiness of the received data.\nBeverly and Sollins [5] investigated the effectiveness of using transport-level features, i.e., round trip time, FIN count, Jitter, and several more. The best features were selected using the forward fitting method to train a SVM-based classifier. They reported 90% accuracy on 60 training examples. However, one of the weaknesses of their method when compared to RBL and other reputation mechanisms is that emails (both Spam and Ham) must be fully received in order to extract their features, thus making the Spam mitigation process less effective.\nIn addition to the above mentioned SRMs, there is another line of research that focuses on inferring the reputation of senders from spatial and temporal features extracted from emails logs.\nSNARE, by Hao et al. [13] presented a method based solely on network-level and geodesic features, such as distance in IP space to other email senders or the geographic distance between sender and receiver. SNARE classification accuracy has been shown to be comparable with existing static IP blacklists on data extracted from McAfee\u2019s TrustedSource system [19].\nLiu [18] proposed a policy that assigned reputation to senders according to the results returned by a contend\nbased filter. The author proposes several simple rules that, nevertheless, are able to reduce the number of non caught low volume spammers and accurately selects the set of mixed senders such that the ham/Spam ratio is maximized. Finally, West et al. [30] introduce a reputation model named PRESTA (Preventive Spatio-Temporal Aggregation) that is used to predict behavior potential spam senders based on email logs. The authors report that PreSTA identifies up to 50% spam emails that have passed the blacklist having 0.66% false positives.\nIn the current work, we report more than 94% true positive detection rate and up to 0.6% false positives in a similar scenario with one week of email logs obtained from a large email service provider."}, {"heading": "4 Learning From the Email Log", "text": "The data set used in this research contained 168 hours (7 days) of anonymized email log obtained from a large email service provider. The provider maintains its own Spam mitigation solution which includes black-and white lists and a CBF. The email log includes only emails that have passed the black-list and are labeled by a CBF named eXpurgate [10]. The developers of eXpurgate claim \u201cA Spam recognition rate of over 99%\u201d and \u201czero false positives\u201d with unpublished false negative rate.\nThe email log was parsed to create a relational data set in the following way. Let IP denote the Internet address of the Sender MTA. In the following discussions we will assume an implicit partition of the IP address into four fields: two fields (MSB and LSB) of in CIDR (Classless Inter-Domain Routing) notation, the subnet identifiers IP/8, IP/16, IP/24, and the host identifier IP/32. Let EL = {IP,T,NR,AE,PT,SpamClass}m be the relational data set representing the email log. It contains one line for each received mail where T is the receiving time, NR is the number of recipients, AE is the number of addressing errors, PT is the time spent by eXpurgate for processing the email, SpamClass is the binary mail classification (Spam = 1 or Ham = 0) obtained from eXpurgate, and m is the number of emails. Table 4 presents the properties of the EL data set, used in the experiment. Table 2 is an example of EL. This example will be used to demonstrate the construction of HDS in Section 5.\nWe used machine learning (ML) algorithms to create sender reputation models based on EL. As can be seen from Table 3, the subnet identifiers play significant role in ML models based on EL. This fact suggests that ML algorithms are able to identify spamming addresses even based on emails that have passed the blacklist of the email service provider. We will denote the model built by ML from the EL data set as EL-Based SRM.\nLet BLT and WLT be the blacklisting and the whitelisting thresholds, respectively. If the EL-Based\nSRM classifies an EL instance as Spam with confidence above BLT the respective IP address is added to the black-list. The white-list is updated symmetrically. A similar approach using a different set of features, was taken in [13]. In Section 7 we will evaluate the effectiveness of EL-Based SRM for updating the black and the white-lists and use it as a baseline for comparison to the proposed SRM. It should be noted that the performance of the EL-Based SRM roughly matches the performance of state-of-the-art methods.\nAnother simple SRM which can be applied directly on the EL data set is a heuristic that detects repeated spammers and trusted legitimate email senders. This Heuristic-SRM is based solely on the fraction of Spam mails sent by the IP in the past. Let [Tstart ,Tend) denote a continuous time range starting at Tstart (inclusive) and ending at Tend (exclusive).\nDefinition 1 Spammingness of a sender IP (denoted by YIP,[Tstart ,Tend)) is the fraction of Spam emails sent by the IP during the time window [Tstart ,Tend).\nIf the spammingness of an IP in the past is above BLT , we blacklist it. Symmetrically, if the spammingness of an IP is below WLT , we put it in the white-list. Since most of the IP addresses are either legitimate email senders or potential spammers this heuristic performs quite well in practice for large time windows. However, it can hardly detect changes in the sender behavior and therefore cannot react to it in a timely manner.\nMaking aggregations which capture the entire history of an IP may not be optimal as it releases the focus from the most recent statistics that may indicate behavior changes. A better approach is to split the history on an IP into several non-congruent time windows, as described in the next section."}, {"heading": "5 Historical Data Set", "text": "The primary objective of the presented work is to automatically learn a classification model for updating the black and the white lists in a timely manner. We assume that there are differences in the behavioral patterns of spammers and legitimate IPs and try to capture these differences. We also assume that the behavioral patterns of IPs may change over time due to adversary actions, such as stealing local network IPs of benign MTAs, by temporarily stealing IPs using BGP hijacking attacks, or by installing a bot agent at a end-user\u2019s device. For example, a small business that runs its own legitimate email servers may be subverted by some malware and become a Spam sender. In this case the IP may temporary enter the blacklists of large email service providers until the problem is mitigated. We further assume that an analysis of a long period of time is important for blacklisting repeated spammers. However the recent behavior is important for rapid reaction to changes in the sender behavior.\nThe idea of HDS in a nutshell is to aggregate email log records across multiple variable length historical time windows in order to create informative statistical records.\nFor each historical time window, HDS records contain multiple aggregations of the attributes in the EL data set. The statistical learning algorithm is applied on the aggre-\ngated features to predict the behavior of an IP in the near future. Based on this prediction, HDS-based SRM will temporarily update the black and white lists.\nIntuitively, one special property of the proposed method is its ability to model the subject behavior over time (which is more informative than only its current state). In particular, it extracts features for a sender MTA by aggregating past transactions, which are then labeled using its behavior on future transactions. An HDS-based classification model, therefore, can estimate MTA behavior in the near future based on its past transactions. This information allows detection of \u2019Spammer\u2019 behavior patterns even with only very few Spam emails sent. This unique property can speed-up the blacklisting process, and hence improve the TPR.\nNext, we define the HDS building blocks. Figure 1 depicts the general work flow of HDS based SRM. HDS records are uniquely identified by a reference time T0 and IP. EL records preceding T0 are denoted with negative time indexes e.g. T\u22121. An Historical time window is a continuous range [T\u2212w0\u00b72i ,T0) where w0 is the length of the smallest historical time window and i is a positive integer. Using exponentially growing historical time windows gives us two benefits. First, we are able to capture a long history of an IP without exploding the size of the HDS records. Second, the size of the most recent time windows is small enough to detect the slightest change in the behavior of an IP. In Section 7 we will show that the number of historical time windows should be carefully chosen in order to obtain the best performance. Choosing the best length of the smallest time window (w0) is not intuitive either, however, will not be covered in this paper.\nLet FSIP,T0,i be a set of aggregated features of a particular IP, computed using EL records in a historical time window [Tw0\u00b72i ,T0). Each HDS record contains n feature sets (FSIP,T0,0,FSIP,T0,1, . . . ,FSIP,T0,n\u22121). Every feature set FSIP,T0,i includes aggregates of all features extracted from the email logs. The actual set of features depends on the email service provider and the nature of the email logs. In this paper we have constructed feature sets for the HDS records by taking the sum, mean, and variance of the number of recipients (NR), the number of addressing errors (AE), the CBF processing time (PT ), and the SpamClass. Note that the mean of SpamClass is in fact the spammingness of the IP in the respective time window. In addition, for each time window we have also included the total number of emails sent and the number of times the sender changed their behavior from sending Spam to sending legitimate emails, and vice versa. The last feature plays a significant role in the classification of senders as can be seen from Table 5.\nDefinition 2 Erraticness of a sender IP (denoted by ZIP,[Tstart ,Tend)) is the number times the sender changed his\nbehavior from sending Spam to sending legitimate emails and vice versa during the time window [Tstart ,Tend).\nThe goal of the Erraticness method is to detect MTAs that are about to change behavior to \u2018Spamming\u2019 and will stay that way for a long period of time, i.e., have a small Erraticness value. Note that the underlying notion of Erraticness, that MTAs sending behavior can alternate, i.e., MTA can send ham emails for a while, then send some Spam emails for a while and later go back to send ham emails once again, is indeed realistic. In fact, our EL-dataset (discussed in section 6.2) shows that 25.6% of the MTAs changed their sending behavior during a single day (8,804 IPs change their behavior more than 4 times). In section 7 we show that the HDS (Erraticness) can learn to avoid blacklisting such MTAs to avoid false-positives.\nLet [T0,TPred) be the prediction time window where Pred is its length. Let ClassIP,[T0,TPred) be the target attribute of the HDS records used to train machine learning classifiers. HDS records are identified by an IP and a reference time T0. They contain n feature sets that correspond to n historical time windows and a target attribute. Let HDS be a relational data set derived from EL:\nHDS = (IP,T0,FSIP,T0,1, . . . ,FSIP,T0,n,ClassIP,[T0,TPred)) l\nwhere l is the number of records. Table 6 depicts the historical data set structure. Each HDS record is identified by IP and a reference time T0 and contains n feature sets.\nNext, we describe two variants of an HDS-Based SRM. The target attribute of the first variant is the future Spammingness of an IP and the target attribute of the second variant is its future Erraticness. We will denote these two variants as HDS-Based (Spammingness) SRM and HDS-Based (Erraticness) SRM, respectively."}, {"heading": "5.1 HDS-Based (Spammingness) SRM", "text": "In order to train the HDS-Based (Spammingness) classifier, we set the target attribute of every HDS record to be the Spammingness of the IP in a time period following T0 (YIP,[T0,TPred)). Table 7 shows an example of an historical data set derived from the email log in Table 2. HDS contains one instance per IP per time unit, where time unit was defined as the size of the smallest time window (w0). In this example, w0 = 1, n = 4, and TPred = 4. EC stands for email count and AE stands for the sum of addressing errors. Both are examples of aggregated features that are calculated for each one the four feature sets, while Spammingness is calculated according to Definition 1. In our experiments, each feature set contained 13 different features (see Table 5).\nGiven the HDS with a target attribute set to Spammingness, we can train a machine learning based classifier to predict the future Spammingness of IP addresses. These predictions are used by the HDS-Based (Spammingness) SRM to roughly distinguish between spammers and non spammers. If the predicted Spammingness is higher than a given threshold (e.g., 0.5), we then apply the BLT threshold on Spammingness (i.e SpamClassmean) in the largest historical time window. Symmetrically, if the predicted Spammingness is lower than the given threshold the WLT , is applied to determine whether or not the IP is added to the white-list.\nThe operation of HDS-Based (Spammingness) SRM resembles the Heuristic SRM described in Section 4. Here, however, the heuristic rule is augmented by the prediction of a machine learning classifier. In Section 7 we will show that this combination produces a high quality SRM. Preliminary experiments (not presented in this paper) show that using the predicted Spammingness only, without applying the BLT and WLT thresholds on historical Spammingness, results in poor classification."}, {"heading": "5.2 HDS-Based (Erraticness) SRM", "text": "Another variant of the HDS-Based SRM uses a machine learning classifier to predict the stability or Erraticness of the IP behavior in the prediction time window. In order to train the HDS-Based (Erraticness) classifier we set\nthe target attribute of every HDS record to be the Erraticness of the IP in a time period following T0 (ZIP,[T0,TPred)). Classifiers trained on this data are used in a slightly different way than classifiers trained to predict the Spammingness of an IP. The difference is mainly in the rule we use to arrive at the second part of the black-listing / white-listing decision process. If the ML model predicts an unvarying behavior (Erraticness << \u03b5), after T0, meaning that the IP is not expected to change its behavior in the nearest future, we apply the same rule that guided the heuristic-SRM.\nFirst, we check whether the predicted Erraticness is very close to zero, meaning that IP is not expected to change its behavior in the nearest future. If it is, then the same rule that guides the Heuristic-SRM is applied.\nThat is, if the Spammingness of the IP in the longest historical time window is above the blacklisting threshold BLT , then the IP is blacklisted. Symmetrically, if changes in the IP behavior are not predicted and the IP Spammingness is below the whitelisting threshold, then the IP is added to the white-list.\nExperiment results presented in Section 7 show that, in terms of AUC score, the HDS-Based (Spammingness) SRM consistently performs better than all the other examined SRMs. In fact, the former SRM receives the highest performance metrics and is the most tolerant to configuration changes."}, {"heading": "6 Evaluation Methodology", "text": ""}, {"heading": "6.1 Evaluation Environment", "text": "It is not possible to directly compare ML models trained on the EL and the HDS data sets. The main difficulty is that the number of instances and the target-variable in both data sets is different. Each EL instance corresponds to a single email, while each HDS instance represents email aggregates for a time period. Therefore, we implemented a unique evaluation environment around WEKA machine-learning tool [12] that allows for an evaluation of the aforementioned SRMs on a common ground. In addition to the HDS-based and the EL-based SRMs, we implemented an Huristic-SRM. The HuristicSRM blacklists IPs that, during a past time window, had a spam fraction value greater than the BLT value. As opposed to HDS and EL SRMs, the Huristic SRM does not use machine-learning, and therefore, does not need training. The Hueristic-SRM is currently deployed at the ISP, from which our data is originated, and is used for updating their black-list. The evaluation environment was designed to simulate the general filtering process of incoming emails.\nThe HDS evaluation environment, (see Figure 3) contains four modules: controller, while-list, black-list, and\nTable 7: Example of an HDS\n[T\u22128,T0) [T\u22124,T0) [T\u22122,T0) [T\u22121,T0) [T0,T+4)\nIP T0 EC AE EC AE EC AE EC AE Spammingness IP1 2 - - - - 2 8 1 2 0.333 IP1 4 - - 3 11 1 3 0 0 0.200 IP1 6 - - 3 5 2 2 1 2 0.400 IP1 8 8 120 5 109 3 107 2 105 0.500\na reputation mechanism. The sender IP of every incoming email (i.e., an EL instance) is first looked up in the white-list (steps 1: and 2: in Figure 3). A positive result cause the email to be accepted (steps 3: and 9:). EL instances that have been accepted are passed to the SRM (step 6:) in order to update the reputation model. Emails arriving from blacklisted MTAs are rejected without further processing (steps 4:, 5:, and 9:). If neither the whitelist nor the black-list contain the IP, the classification result of the SRM is used to make the final decision and to update the lists, if necessary (steps 6:, 7:, 8:, and 9:). Note that in the case of the HDS-Based and the Heuristic SRMs, emails received from blacklisted IPs are ignored since they never reach the reputation mechanism. The general experimental settings are depicted in Figure 2.\nIn contrast to continuous classification where each email received from non blacklisted IP is passed to the SRM, the evaluation environment can operate in a batch mode. In this mode the incoming emails are logged, but the reputation mechanism is activated once in a predefined time period. After processing the logged emails, the SRM returns the controller two sets of addresses. One set contains the addresses that should be blacklisted and the other contains the addresses that should be whitelisted. Either set, of course, can be empty.\nEvaluation Environment Performance Metrics\nE-mail Logs (Train/Test)\n\u2022 Size\n\u2022 Accuracy \u2022 TPR \u2022 AUC\n2: Exist IP?\n\u22053: Ham /\n1: EL instance\n\u22057: Spam / Ham /\n6: EL instance\n4: Exist IP?\n\u22055: Spam /\nReputation Mechanism\nC ontroller\n8: Add IP\n9: Reject / Accept\nBlack List\n\u2022 SizeWhite List\nFigure 3: HDS evaluation environment"}, {"heading": "6.2 Dataset", "text": "To evaluate the proposed SRMs, we made use of a single email log datasets which contained 9.507 million anonymized log entries (emails headers) of 678,509 distincs IPs, which were received during a 168 hours (7 days) period at T-Online ISP. The dataset is comprised of 9 attribues and 12.25% \u2018Spam\u2019 labeled instances. The un-received emails, which were blocked by the T-Online black-list, were not logged and therefore their headers are not included in the dataset. The dataset was fully labeled by an automatic content-based filtering device, eXpurgate 1. The eXpurgate claims \u201dA Spam recognition rate of over 99%\u201d and \u201dzero false positives\u201d with unpublished false negative rate. The dataset was partitioned into training and validation sets, each containing the instances of all the emails sent by 200k randomly selected sender IPs. The training set (2,835,214 instances) and validation sets (2,864,208 instances) are mutually exclusive, meaning that IPs that exist in the training set do not appear in the validation set, and vice versa."}, {"heading": "6.3 Performance Metrics", "text": "In order to evaluate the sender reputation mechanisms discussed in this paper, we use the following performance metrics: classification error rate, true positive rate, false positive rate, area under the ROC (Re-\n1Eleven, eXpurgate Anti Spam, http://www.eleven.de/ overview-antispam.html\nceiver Operating Characteristic) curve, black-list size, and number of white-list hits. These performance metrics provide enough information to assess how well the HDS-Based SRM could be used to both reduce the load from mail servers, and reduce the number of potential customer complaints.\nClassification error rate is the rate of incorrect predictions made by a classifier and is computed by the following equation:\nError = FP+FN\nT P+T N +FP+FN\nwhere TP, TN, FP and FN stand for the number of true positive, true negative, false positive, and false negative rejections of emails respectively.\nWe will use the Area Under the ROC Curve (AUC) measure in the evaluation process. The ROC curve is a graph produced by plotting the true positive rate (T PR = T P/(T P+ FN)) versus the false positive rate (FPR = FP/(FP+ T N)). The AUC value of the best possible classifier will be equal to unity. This would imply that it is possible to configure the classifier so that it will have 0% false positive and 100% true positive classifications. The worst possible binary classifier (obtained by flipping a coin for example) has an AUC of 0.5. The AUC is considered as an objective performance metric as it does not depend on the specific discrimination threshold used by a classifier.\nBlack list size is the number of IPs added to the black list during each experiment execution. The blacklist size mainly affects the IP lookup time and the amount of computational resources spent on its maintenance [24]. Faster lookup times mean less delay in email delivery, while the computational resources required to maintain the black-list directly translate into cost.\nThe number of white-list hits is an indication of the number of emails that were delivered without content inspection. A Higher number of white-list hits means less computational resources spent on Spam filtering."}, {"heading": "7 Experimental Results", "text": "In order to assess the effectiveness of HDS-Based SRMs, we implemented HDS-Based (Erraticness), HDS-Based (Spammingness), EL-Based, and Heuristic SRMs within WEKA machine-learning framework 2 [31]. The evaluation environment presented in Figure 3 was also implemented within WEKA as a special classifier which uses the SRMs to update the black and white lists. The training of the EL-Based and HDS-Based sender reputation algorithms was made using a cross validation process. The data sets were ordered chronologically to preserve the order in which the emails were received. The same cross-validation procedure was applied to all tested settings, shown in Table 8.\nThe blacklisting threshold BLT and the whitelisting threshold WLT , as described in Sections 4 and 5, are parameters of the evaluation environment. In the following experiments these two threshold were fixed and equal for all SRMs. The value of BLT and WLT were empirically chosen to be 0.5 and 0.05 respectively. These values assure low false positive rates while resulting in relatively high true positive rates.\nBoth the black and the white lists were empty in the beginning of the experiments. It is also possible to initialize the black and white lists by executing the evaluated SRM on the train data. In this scenario the address\n2The code, dataset, and operation instructions can be downloaded from #Anonymized#\nlists already contain valuable information in the beginning of the testing phase. In this paper, however, we focus on the more challenging task that involves the construction of the address lists from scratch."}, {"heading": "7.1 The Value of HDS Aggregations", "text": "The first and most important question in this study is whether the HDS-based SRM outperforms other related SRMs. To answer this we compare HDS-based SRM to EL-based SRM on an identical test bed. We argue that in general HDS-based features are at least as informative as EL-based features because they are derived directly from EL-based features and thus HDS features should produce a superior classification model, with respect to that produced using the corresponding EL dataset. In this experiment we simulated a condition in which every email, whose sender\u2019s IP is not included in either the black or white lists, is classified by the tested SRMs on arrival. We denote this classification mode as \u2019continuous mode\u2019.\nIn order to increase the reliability of the experimental results, we evaluated the HDS-Based and ELBased sender reputation mechanisms on four different machine-learning algorithms from separated machinelearning families. The algorithms used were: Na\u0131\u0308ve Bayes (Bayes) [15], C4.5 (Decision Trees) [21], Logistic Regression (Function) [8], and BayesNet (Bayes). Due to memory constraints, in this experiment the train and validation set contained the email sending of 50,000 randomly selected IPs.\nThe HDS instances were generated using the following parameters: w0 = 60 minutes, n = 5, and TPred = 60 minutes. The total history length is:\n\u2206T = w0 \u00b72(n\u22121) = 960 minutes (16 hours)\nTable 9 presents the results of this experiment. We also applied the Heuristic SRM as a baseline to compare with other techniques. The time window used by the Heuristic SRM for computing the spammingness of the IP addresses was set to 960 minutes. The blacklisting and the whitelisting thresholds were set to BLT = 0.5 and WLT = 0.05, respectively for all SRMs.\nJudging the best results for each of the SRMs, we can see from Table 9 that the HDS-SRM (Erraticness) had the best IP classification performance. Not far behind at second place is the HDS-SRM (Spammingness). Third place, with noticeable AUC difference, is the ELbased SRM. The worst performance was achieved by the Heuristic-based SRM. Interestingly, the best results for each SRM were obtained by different learning algorithms. Moreover, it seems that EL-based SRM is much more sensitive to the learning algorithm choice, compared with the HDS-Based SRMs. The LogisticRegression, which worked very well for both HDS-\nSRMs, produced the worst results when applied to ELBased. In this cases, very few IPs were blacklisted by the EL-based SRM, consequencly, obtaining a very poor AUC score.\nThe highest true-positive rate was acheived by the HDS-SRM (Spammingness), whereas the lowest falsepositive and error rates were obtained by the HDS-SRM (Erraticness). The EL-based SRM blacklisted the most IPs. However, many of these IPs were of benign senders, which is reflected in the very high false-positive rate obtained by this SRM.\nFinally, it is noticeable that in some configurations the very simple Heuristic and the EL-based SRMs achieved comparable performance. We also noticed that both the Heuristic SRM and the EL-Based SRM roughly match the reported state-of-the-art performance."}, {"heading": "7.2 Batch IP Classification", "text": "In the previous subsection we discussed continuous classification mode where the sender reputation is computed and updated each time a new email is received. This mode of operation may not be realistic due to relatively high resource consumption of machine learning based classifiers compared to black and white lists data structures. Moreover, classifying every incoming email would also mean placing the classifier in the critical path and turn it into a bottleneck during the process of handling incoming emails. Another drawback of the continuous classification mode is that most black-lists are optimized for fast information retrieval but do not tolerate frequent updates. Updating the black-list data structure may be a very expensive operation in terms of computational resources [24].\nIt is therefore a good practice to minimize the number of updates and to make them as infrequent as possible. In practice, SRM can be activated once in a while in order to save computational resources. The payoff for a periodic activation of SRM is a window of opportunity during which spammers who are not yet blacklisted can send large amounts of Spam without being blocked.\nIn order to investigate the impact of black and white lists\u2019 update frequency on the accuracy of Spam filtering, we executed the reputation mechanisms in a batch mode with various update frequencies. The SRMs were executed each k minutes where k was set to 0.5, 1, 2, 5, 15, or 60 minutes. HDS parameters were: w0 = 60 minutes, n = 5, and TPred = 60 minutes. In this experiment we used the BayesNet algorithm to train classifiers for both the HDS-Based SRMs and the EL-Based SRM.\nThe results presented in Figures 4, 5, and 6 depict the superiority of HDS-Based SRMs also in batch mode.\nFigure 4 shows that the HDS-SRM (Erraticness) has the lowest false reject rate (FPR), while the EL-based SRM has the highest. Looking at the predictability results (TPR), we see that the Heuristic SRM had the worst results, whereas HDS-based SRM (Spammingness) had the highest results among the SRMs. Figure 6 shows that for all update frequencies, the HDS-SRM (Spammingness) has a considrebly high AUC when compared to the Heurist-based and El-based SRMs. Since the AUC metric is an objective performance metric that does not depend on a configurable threshold, it better reflects the\nsuperiority of the HDS over the other tested SRMs. In general, all four SRMs\u2018 performance detereorated, more or less at the same rate, as the black and white lists update frequency was reduced. This was well demonstrated by the drop in predictability, and AUC scores. Interestingly, the SRMs\u2019 false-positive rates were not sensitive to the update rate, and tend to stay constant."}, {"heading": "7.3 The Effect of History Length", "text": "The evidence presented in previous subsections suggests that aggregating sender MTA behavior over time is worthwhile and yields good classification models. The models created from the HDS when trying to predict the Spammingness of IP addresses are the least sensitive both to the choice of the machine learning algorithm and to the frequency of address-lists updates. We therefore focus this subsection on the HDS-Based (Spammingness) SRM, investigating its performance as the function of the number of time windows used to construct the HDS.\nWe compared multiple models of the HDS-Based (Spammingness) SRM trained on different HDS train sets that were constructed from one to fourteen time windows. The models were induced by the Bayes-Net algorithm on HDS instances, generated for every incoming email using: w0 = 15 seconds, n = 1, . . . ,14 and TPred = 60 minutes. Both the black and white list were cleared once per 1,440 minutes (1 day).\nThe experiment shows a mixed trend in the classification performance as a function of the number of historical time windows. The results presented in Figures 7 and 8, and Table 10 show that up to the ninth historical time-window the performance of the HDS-Based SRMs improves, whereas, when using more historical time-windows, the AUC score ceased to improve, indicating a change of trend.\nNote that the experiment settings were not optimal w.r.t. initial historical time window (W0 = 15s) resulting in lower TPR and higher FPR that the respective performance metrics reported in Section 7.1. Yet, this experiment illustrates the effect of the number of time windows on performance of HDS-based SRM. Due to the very small initial historical time window (i.e., only 15 seconds) we are able to notice the decrease in false positive and error rates as more time windows were used. Surprisingly, the true positive rate was also gradually decreased as the number of historical time windows grew. This can be explained by both the decrease in the blacklist size and the growing number of features that added additional dimensions to the machine-learning problems, and therefore, made it more and more complex to learn from (a.k.a. \u201cthe course of dimensionality\u201d). Notice that the FPR decreased faster then the TRP as the number of historical time windows increased.\nThe blacklist size declined at a constant rate, as more historical time windows were used. Note that the blacklist\u2019s size decline had probably affected (i.e., reduced) both the TPR and the FPR, since less IPs were classified as spammers. In contrast to the black list, the white list average size increased until the tenth time window, and then the trend was reversed, were it begun an accelerated decline in size. Interestingly, the number of whitelist hits remained more or less stable from the 9th\nFigure 8: The effect of history length on the black and white lists\nto the 14th time windows, even when the white list average size dropped. Since the AUC score during these time windows (9th to the 14th) was constant, we conclude that for the whitelisting task, the number of history time windows should be greater than nine. Overall, we see that as the more history is used beyond the 9th time window, the effectivess of the black and white lists increases."}, {"heading": "8 Discussion and Future Work", "text": ""}, {"heading": "8.1 IP Classification Performance", "text": "Experiment results presented in this paper show that aggregating behavior of MTAs over time is an effective way to elicit valuable information from email logs. The proposed method was found to be more effective than emaillog-based and heuristic-based SRMs, tested under the exact same conditions. In fact, the ISP whose dataset we evaluated uses a blacklisting method which is similar to the Heuristic SRM, while the Email-log-based SRM is similar to state-of-the-art methods [23, 13, 5].\nThe same machine learning algorithms applied on HDS produce much more effective models than if applied on the non-aggregated data extracted from the raw email logs. The best results were obtained using HDSBased (Erraticness) SRM with nine time windows: AUC 0.907, TPR 90.7%, and FPR 0.4%. To the best of our\nknowledge these results are better than previously reported SRMs evaluated on data sets of similar scale. In fact, the accuracy of HDS based SRMs approaches the accuracy of content-based filtering. Another interesting fact is that HDS-based SRM blacklisted roughly the same number of IPs as the EL-based SRM, while incurring, by far, fewer classifications errors.\nSome related works (e.g. [13]) reported roughly the same performance as the EL-Based SRM reported here. Despite the differences in the particular features, we believe that aggregations over multiple time windows can boost the performance of sender reputation mechanisms based on statistical learning. ON our data set the upgrading of EL-based SRM to HDS-based SRM resulted in an elevated performance. We suggest that our results are general enough to motivate \u201dupgrading\u201d EL to HDS in other data set too.\nIn the second experiment we studied the impact of periodical execution of SRM on its effectiveness. The results show a clear trade-off between batch size and the effectiveness of Spam filtering. A less frequent execution of SRM results in less predictability power, as expected. The main reason for inefficient blacklisting when sender reputation is computed once in a long time period is the tendency of spamming bots to send a number of Spam emails during a very short time period and go silent afterwards [22, 14]. The observed deficiency of periodical activation of the evaluated SRMs could also be explained by a reduced marginal benefit when compared to the email service provider\u2018s own SRM that is activated once in a while.\nIn Section 7.3 we studied the influence of history length on the performance of the reputation classifier. The results show that in general, the longer history is used the better classification model is produced.\nIncreasing the number of time windows (and hence the number of features) above a certain point have resulted in an increasing efficiency of both the black and white lists, and thus a decreased workload of the entire filtering system. At the same time, instead of growing further, the AUC remained constant more or less. This phenomenon occurs probably due to the \u201ccourse of dimensionality\u201d effect, in which the growing dimensionality of the dataset plays a negative role, making the learned concept more and more complex.\nIn order to capture a longer mail sending history using fewer historical time windows, the size of the smallest historical time window w0can be increased. Unfortunately, in this case the most recent behavior of the MTAs would be diluted and damage the ability of the HDSbased SRM to respond to sudden behavioral changes. The \u201ccourse of dimensionality\u201d phenomenon can also be tackled by selecting the most informative features. Currently, we leave the optimal configuration of HDS con-\nstruction as an open issue. Computing the reputation of a sender IP using HDSbased SRM is a computationally intensive task due to both the construction of HDS records and the classification using machine learning models. The HDS construction could be optimized by using past HDS records to compute the aggregated features of a new one. This should be further studied in a future work. On the machine-learning end, for lowering the overall computational requirements of the HDS-based SRM we suggest using a non-complex classification models, e.g., Na\u0131\u0308ve Bayes or Bayes-Net."}, {"heading": "8.2 Reducing the Filtering Workload With HDS", "text": "To insure that the end-users receive only very few spammails, ISPs usually employ a filtering mechanism based on black and white lists. These lists need to be updated frequently, so to insure minimal filtering errors. Currently, there are three methods for updateing these lists: real-time black listing (RBL), SRMs, and contentbased filters (CFBs). The SRMs and CBFs are much slower, and computation-power demanding, compared to the black and white lists. However, the CBF and SRMs only filter emails that were filtered-in by the black and white lists. Thus, as the black and white lists hit-ratio increase, fewer emails are need to be processed by the CBF and SRMs, and hence the filtering process becomes more computationally efficient. While a very high black and white list hit ratios might incur a very computentionally efficient filtering, it can result with high filtering errors, and so there is a constant trade-off between filtering efficiency and accuracy. In order to increase both the filtering computationally efficiency and accuracy, we propose using HDS-base SRMs, as method for updating both the black and white lists.\nOn our experiments the black and the white lists had on average 457,120 and 1,916,636 hits respectively. Each hit corresponded to a spam or benign MTA\u2019s email that was not put through a content-based filter. Therefore, the filtering workload decrease (FGain) is:\nFGain = black and white lists hits\nemails in TestSet =\n= 2,373,756 2,864,208 = 0.828 = 82.7%\nThis means that more than 4 out of 5 emails had hit one of the lists, and therefore, skipped the content-based filter, thanks to the HDS-RM black and white listing. This is a very significant contribution for ISPs, as their entire filtering workload, and conseqencly the energy consumed during the email filtering process can significanly be reduced."}, {"heading": "9 Acknowledgments", "text": "This work was partially supported by Deutshce Telekom AG. The authors would like to thank Yevgenia Gorodtzki, Igor Dvorkin, and Polina Zilberman for coding first versions of HDS and to Danny Hendler and Yuval Elovici for useful remarks."}], "references": [{"title": "Taxonomy of email reputation systems", "author": ["D. Alperovitch", "P. Judge", "S. Krasser"], "venue": "In ICDCS Workshops\u201907,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "An experimental comparison of naive bayesian and keyword-based antispam filtering with personal e-mail messages", "author": ["I. Androutsopoulos", "J. Koutsias", "K. Chandrinos", "C.D. Spyropoulos"], "venue": "In ACM SIGIR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Technological networks and the spread of computer viruses", "author": ["J. Balthrop", "S. Forrest", "M.E.J. Newman", "M.M. Williamson"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Exploiting the transportlevel characteristics of am", "author": ["R. Beverly", "K. Sollins"], "venue": "In 5th Conference on Email and Anti-Spam (CEAS),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Scalable centralized bayesian spam mitigation with bogofilter", "author": ["J. Blosser", "D. Josephsen"], "venue": "In USENIX LISA,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Leveraging social networks to fight spam", "author": ["P. Boykin", "V. Roychowdhury"], "venue": "IEEE Computer,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Ridge estimators in logistic regression", "author": ["S.L. Cessie", "J.C.V. Houwelingen"], "venue": "Applied Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "A neural network based approach to automated email classification", "author": ["J. Clark", "I. Koprinska", "J. Poon"], "venue": "In IEEE/WIC International Conference on Web Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Reputation network analysis for email filtering", "author": ["J. Golbeck", "J. Hendler"], "venue": "In First Conference on Email and Anti-Spam,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Detecting spammers with snare: Spatiotemporal network-level automated reputation engine", "author": ["S. Hao", "N.A. Syed", "N. Feamster", "A.G. Gray", "S. Krasser"], "venue": "In 18th USENIX Security Symposium,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Botnets battle over turf", "author": ["K.J. Higgins"], "venue": "http://www.darkreading.com/document.asp?doc id =122116,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Estimating continuous distributions in bayesian classifiers", "author": ["G.H. John", "P. Langley"], "venue": "In Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Learning to classify email", "author": ["I. Koprinska", "J. Poon", "J. Clark", "J. Chan"], "venue": "Inf. Sci.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Rajabiun. Ipv6 and spam", "author": ["P. Kosik", "P. Ostrihon"], "venue": "In MIT Spam Conference,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Identifying and addressing rogue servers in countering internet email misuse", "author": ["W. Liu"], "venue": "In IEEE SADFE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Securelist spam report", "author": ["M. Namestnikova"], "venue": "http://www.securelist. com/en/analysis/204792212/Spam_report_ December_2011,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "C4.5: programs for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Understanding the network-level behavior of spammers", "author": ["A. Ramachandran", "N. Feamster"], "venue": "In ACM SIGCOMM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2006}, {"title": "Filtering spam with behavioral blacklisting", "author": ["A. Ramachandran", "N. Feamster", "S. Vempala"], "venue": "In ACM CCS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Survey and taxonomy of ip address lookup algorithms", "author": ["M.A. Ruiz-Sanchez", "E.W. Biersack"], "venue": "IEEE Network,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Predictive blacklisting as an implicit recommendation system", "author": ["F. Soldo", "A. Le", "A. Markopoulou"], "venue": "In IEEE INFOCOM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Fast and effective spam sender detection with granular svm on highly imbalanced mail server havior data", "author": ["Y. Tang", "S. Krasser", "P. Judge", "Y.-Q. Zhang"], "venue": "In 2nd International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Preventing malicious behavior using spatio-temporal reputation", "author": ["A.G. West", "A.J. Aviv", "J. Chang", "I. Lee"], "venue": "In ACM EUROSYS\u201910,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["I.H. Witten", "E. Frank"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "A collaboration-based autonomous reputation system for email services", "author": ["M. Xie", "H. Wang"], "venue": "In IEEE INFOCOM,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Improved spam filtering by extraction of information from text embedded image email", "author": ["S. Youn", "D. McLeod"], "venue": "In ACM SAC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}], "referenceMentions": [{"referenceID": 16, "context": "Surveys show that in recent years 76% to 90% of all email traffic can be considered abusive [20, 1].", "startOffset": 92, "endOffset": 99}, {"referenceID": 18, "context": "A major portion of those billions of Spam emails annually are automatically produced by botnets [22, 14].", "startOffset": 96, "endOffset": 104}, {"referenceID": 11, "context": "A major portion of those billions of Spam emails annually are automatically produced by botnets [22, 14].", "startOffset": 96, "endOffset": 104}, {"referenceID": 0, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 10, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 22, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 2, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 5, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 8, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 3, "context": "Most work in this research field focuses on extracting meaningful features from communication patterns of MTAs and from the social network structure [2, 13, 29, 4, 7, 11, 5].", "startOffset": 149, "endOffset": 173}, {"referenceID": 1, "context": "CBF [3, 6, 9, 16, 33]", "startOffset": 4, "endOffset": 21}, {"referenceID": 4, "context": "CBF [3, 6, 9, 16, 33]", "startOffset": 4, "endOffset": 21}, {"referenceID": 7, "context": "CBF [3, 6, 9, 16, 33]", "startOffset": 4, "endOffset": 21}, {"referenceID": 13, "context": "CBF [3, 6, 9, 16, 33]", "startOffset": 4, "endOffset": 21}, {"referenceID": 26, "context": "CBF [3, 6, 9, 16, 33]", "startOffset": 4, "endOffset": 21}, {"referenceID": 18, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 18, "endOffset": 30}, {"referenceID": 22, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 18, "endOffset": 30}, {"referenceID": 19, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 18, "endOffset": 30}, {"referenceID": 3, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 69, "endOffset": 83}, {"referenceID": 5, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 69, "endOffset": 83}, {"referenceID": 8, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 69, "endOffset": 83}, {"referenceID": 15, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 69, "endOffset": 83}, {"referenceID": 10, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 100, "endOffset": 112}, {"referenceID": 23, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 100, "endOffset": 112}, {"referenceID": 21, "context": "SRM Network level [22, 29, 23] Transport level [5] Application level [4, 7, 11, 18] Spatio-temporal [13, 30, 25]", "startOffset": 100, "endOffset": 112}, {"referenceID": 1, "context": "Content-based filtering (CBF) refers to techniques in which emails\u2019 body, attached executables, pictures or other files are analyses and processed for producing some features upon which email classification is made [3, 9, 6, 16, 33].", "startOffset": 215, "endOffset": 232}, {"referenceID": 7, "context": "Content-based filtering (CBF) refers to techniques in which emails\u2019 body, attached executables, pictures or other files are analyses and processed for producing some features upon which email classification is made [3, 9, 6, 16, 33].", "startOffset": 215, "endOffset": 232}, {"referenceID": 4, "context": "Content-based filtering (CBF) refers to techniques in which emails\u2019 body, attached executables, pictures or other files are analyses and processed for producing some features upon which email classification is made [3, 9, 6, 16, 33].", "startOffset": 215, "endOffset": 232}, {"referenceID": 13, "context": "Content-based filtering (CBF) refers to techniques in which emails\u2019 body, attached executables, pictures or other files are analyses and processed for producing some features upon which email classification is made [3, 9, 6, 16, 33].", "startOffset": 215, "endOffset": 232}, {"referenceID": 26, "context": "Content-based filtering (CBF) refers to techniques in which emails\u2019 body, attached executables, pictures or other files are analyses and processed for producing some features upon which email classification is made [3, 9, 6, 16, 33].", "startOffset": 215, "endOffset": 232}, {"referenceID": 11, "context": "RBL methods, however, cannot solve the Spam email problem entirely, as spammers can escape them by, for example, repeatedly changing their IPs by stealing local network IPs [14], or by temporarily stealing IPs using BGP hijacking attacks [22].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "RBL methods, however, cannot solve the Spam email problem entirely, as spammers can escape them by, for example, repeatedly changing their IPs by stealing local network IPs [14], or by temporarily stealing IPs using BGP hijacking attacks [22].", "startOffset": 238, "endOffset": 242}, {"referenceID": 0, "context": "al [2], sender reputation systems should react quickly to changes in sender\u2018s behavioral patterns.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "Furthermore, the extremely large address space and auto-configuration functionality of IPv6 are expected to increase the cost of Spam mitigation [17].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "[23], research a sender reputation and blacklisting approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[29] addressed the Spam imbalanced classification task with a new version of SVM, the GSVMBA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[4], used email address books to compute sender trust degree.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Boykin and Roychowdhury [7] extract social network\u2019s features from the email header fields such as From, To, and CC, from which they construct a social graph, as can be observed from a single user\u2019s perspective.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "A supervised collaborative approach for learning the reputation of networks is presented by Golbeck and Hendler [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Similar approach is presented by Xie and Wang [32].", "startOffset": 46, "endOffset": 50}, {"referenceID": 3, "context": "Beverly and Sollins [5] investigated the effectiveness of using transport-level features, i.", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "[13] presented a method based solely on network-level and geodesic features, such as distance in IP space to other email senders or the geographic distance between sender and receiver.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Liu [18] proposed a policy that assigned reputation to senders according to the results returned by a contend based filter.", "startOffset": 4, "endOffset": 8}, {"referenceID": 23, "context": "[30] introduce a reputation model named PRESTA (Preventive Spatio-Temporal Aggregation) that is used to predict behavior potential spam senders based on email logs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "A similar approach using a different set of features, was taken in [13].", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "Therefore, we implemented a unique evaluation environment around WEKA machine-learning tool [12] that allows for an evaluation of the aforementioned SRMs on a common ground.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "The blacklist size mainly affects the IP lookup time and the amount of computational resources spent on its maintenance [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 24, "context": "In order to assess the effectiveness of HDS-Based SRMs, we implemented HDS-Based (Erraticness), HDS-Based (Spammingness), EL-Based, and Heuristic SRMs within WEKA machine-learning framework 2 [31].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "The algorithms used were: Na\u0131\u0308ve Bayes (Bayes) [15], C4.", "startOffset": 47, "endOffset": 51}, {"referenceID": 17, "context": "5 (Decision Trees) [21], Logistic Regression (Function) [8], and BayesNet (Bayes).", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "5 (Decision Trees) [21], Logistic Regression (Function) [8], and BayesNet (Bayes).", "startOffset": 56, "endOffset": 59}, {"referenceID": 20, "context": "Updating the black-list data structure may be a very expensive operation in terms of computational resources [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "In fact, the ISP whose dataset we evaluated uses a blacklisting method which is similar to the Heuristic SRM, while the Email-log-based SRM is similar to state-of-the-art methods [23, 13, 5].", "startOffset": 179, "endOffset": 190}, {"referenceID": 10, "context": "In fact, the ISP whose dataset we evaluated uses a blacklisting method which is similar to the Heuristic SRM, while the Email-log-based SRM is similar to state-of-the-art methods [23, 13, 5].", "startOffset": 179, "endOffset": 190}, {"referenceID": 3, "context": "In fact, the ISP whose dataset we evaluated uses a blacklisting method which is similar to the Heuristic SRM, while the Email-log-based SRM is similar to state-of-the-art methods [23, 13, 5].", "startOffset": 179, "endOffset": 190}, {"referenceID": 10, "context": "[13]) reported roughly the same performance as the EL-Based SRM reported here.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The main reason for inefficient blacklisting when sender reputation is computed once in a long time period is the tendency of spamming bots to send a number of Spam emails during a very short time period and go silent afterwards [22, 14].", "startOffset": 229, "endOffset": 237}, {"referenceID": 11, "context": "The main reason for inefficient blacklisting when sender reputation is computed once in a long time period is the tendency of spamming bots to send a number of Spam emails during a very short time period and go silent afterwards [22, 14].", "startOffset": 229, "endOffset": 237}], "year": 2012, "abstractText": "The battle between email service providers and senders of mass unsolicited emails (Spam) continues to gain traction. Vast numbers of Spam emails are sent mainly from automatic botnets distributed over the world. One method for mitigating Spam in a computationally efficient manner is fast and accurate blacklisting of the senders. In this work we propose a new sender reputation mechanism that is based on an aggregated historical data-set which encodes the behavior of mail transfer agents over time. A historical data-set is created from labeled logs of received emails. We use machine learning algorithms to build a model that predicts the spammingness of mail transfer agents in the near future. The proposed mechanism is targeted mainly at large enterprises and email service providers and can be used for updating both the black and the white lists. We evaluate the proposed mechanism using 9.5M anonymized log entries obtained from the biggest Internet service provider in Europe. Experiments show that proposed method detects more than 94% of the Spam emails that escaped the blacklist (i.e., TPR), while having less than 0.5% false-alarms. Therefore, the effectiveness of the proposed method is much higher than of previously reported reputation mechanisms, which rely on emails logs. In addition, the proposed method, when used for updating both the black and white lists, eliminated the need in automatic content inspection of 4 out of 5 incoming emails, which resulted in dramatic reduction in the filtering computational load.", "creator": "LaTeX with hyperref package"}}}