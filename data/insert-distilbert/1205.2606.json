{"id": "1205.2606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Exploring compact reinforcement-learning representations with linear regression", "abstract": "this presented paper presents exactly a new algorithm for online linear regression whose efficiency guarantees learners satisfy the requirements of the kwik ( knows what it knows ) framework. the algorithm improves on the complexity bounds of performing the current state - of - the - art procedure in this setting. we explore several applications of this algorithm for learning inherently compact reinforcement - learning representations. we show that kwik linear regression can be used to learn the reward function of a factored mdp and the probabilities of action outcomes in stochastic strips and digital object oriented mdps, none of which have been proven to be efficiently learnable in the rl setting before. we help also combine kwik classical linear regression with other kwik programming learners to learn larger portions of these conceptual models, including experiments on learning factored mdp transition and reward functions together.", "histories": [["v1", "Wed, 9 May 2012 18:40:40 GMT  (283kb)", "http://arxiv.org/abs/1205.2606v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["thomas j walsh", "istvan szita", "carlos diuk", "michael l littman"], "accepted": false, "id": "1205.2606"}, "pdf": {"name": "1205.2606.pdf", "metadata": {"source": "CRF", "title": "Exploring compact reinforcement-learning representations with linear regression", "authors": ["Thomas J. Walsh", "Istv\u00e1n Szita", "Carlos Diuk", "Michael L. Littman"], "emails": [], "sections": [{"heading": null, "text": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together."}, {"heading": "1 Introduction", "text": "Linear regression has, for decades, been a powerful tool in the kits of machine-learning researchers. While the field of Reinforcement Learning (RL) [16] has certainly made use of linear regression in approximating value functions [3], using online regression to learn parameters of a model has been limited to environments with linear dynamics (e.g. [7]), and has often been unable to make guarantees about the behavior of the resulting learning agent without strict assumptions. One of the great hindrances in applying linear regression to learn models in any RL environment was that the computational and sample efficiency guarantees of online regression learners (such as those that rely on distributional assumptions or do not maintain explicit confidences) did not port to the reinforcement-learning\nsetting, which is not i.i.d. and where realizing what portion of the model needs to be explored is crucial to optimizing reward.\nRecently, the introduction of the KWIK (Knows What It Knows) framework [11] has provided a characterization of sufficient conditions for a model-learning algorithm to induce sample-efficient behavior in a reinforcement-learning agent. One of the first algorithms developed for this framework was a KWIK linear regression algorithm [15], which was used to learn the transition function of an MDP with linear dynamics. In this paper, we present an algorithm that improves on both the sample and computational bounds of this previous algorithm and apply it to a stable of learning problems for reinforcement-learning agents that employ \u201ccompact\u201d representations. Specifically, we use KWIK linear regression (KWIK-LR) to learn the reward function in a factored MDP and the transition probabilities in domains encoded using Stochastic STRIPS [12] or Object Oriented MDPs (OOMDP) [5]. We note that learning these parameters is not typically associated with linear regression\u2014this paper shows that KWIK-LR can be used to help learn models beyond its standard usage in learning linear dynamics. Because of the KWIK guarantees, agents using this algorithm in these settings are guaranteed to make at most a polynomial (in the parameters of the learned model) number of sub-optimal steps with high probability. We present algorithms and theoretical arguments to this effect, including a general reinforcementlearning algorithm for agents that need to learn the probabilities of action outcomes when these effects may be ambiguous in sample data. Experimental evidence is also presented for benchmark problems in the application areas mentioned above.\nOur major contributions are an improved and simplified algorithm for KWIK linear regression, and the sample-efficient algorithms that use KWIK-LR to efficiently learn portions of compact reinforcementlearning representations, none of which have previ-\nously been shown to be efficiently learnable. We also discuss extensions, including combining KWIKLR with other KWIK algorithms to learn more parameters of these compact representations."}, {"heading": "2 KWIK Linear Regression", "text": "KWIK [11] (Knows What It Knows) is a framework for studying supervised learning algorithms and was designed to unify the analysis of model-based reinforcement-learning algorithms. Formally, a KWIK learner operates over an input space X and an output space Y . At every timestep t, an input xt \u2208 X is chosen and presented to the learner. If the learner can make an accurate prediction on this input, it can predict y\u0302t, otherwise it must admit it does not know by returning \u22a5 (\u201cI don\u2019t know\u201d), allowing it to see the true yt or a noisy version zt. An algorithm is said to be KWIK if and only if, with high (1 \u2212 \u03b4) probability, ||y\u0302t \u2212 yt|| < \u01eb and the number of \u22a5s returned over the agent\u2019s lifetime is bounded by a polynomial function over the size of the input problem. It has been shown [10] that in the model-based reinforcement-learning setting, if the underlying model learner is KWIK, then it is possible to build an RL agent around it by driving exploration of the \u22a5 areas using an R-max [4] style manipulation of the value function. Such an agent will, with high probability, take no more than a polynomial (in the parameters of the model being learned) number of suboptimal actions. In this paper, which deals with compact representations where the parameter sizes are far smaller than the enumerated state space, we will call agents that satisfy these conditions, but may require superpolynomial (in the size of the compact parameters) computation for planning, PAC-compact-MDP.\nOne of the first uses of the KWIK framework was in the analysis of an online linear regression algorithm used to learn linear transitions in continuous state MDPs [15]. This algorithm uses the least squares estimate of the weight vector for inputs where the output is known with high certainty. Certainty is measured by two terms representing (1) the number and proximity of previous samples to the current point and (2) the appropriateness of the previous samples for making a least squares estimate. When certainty is low for either measure, the algorithm reports \u22a5. In this work, we present a KWIK-LR algorithm that is more sample and computationally efficient than this previous work. First though, we define some notation.\nLet X := {~x \u2208 Rn | \u2016~x\u2016 \u2264 1}, and let f : X \u2192 R be a linear function with slope \u03b8\u2217 \u2208 Rn, \u2016\u03b8\u2217\u2016 \u2264 M , i.e. f(~x) := ~xT \u03b8\u2217. Fix a timestep t. For each i \u2208 {1, . . . , t}, denote the stored samples by\n~xi, their (unknown) expected values by yi := ~x T t \u03b8 \u2217, and their observed values by zi := ~x T i \u03b8\n\u2217 + \u03b7i, where the noise \u03b7i is assumed to form a martingale, i.e., E(\u03b7i|\u03b71, . . . , \u03b7i\u22121) = 0, and bounded: |\u03b7i| \u2264 S. Define the matrix Dt := [~x1, ~x2, . . . , ~xt]\nT \u2208 Rt\u00d7n and vectors ~yt := [y1; . . . ; yt] \u2208 R t and ~zt := [z1; . . . ; zt] \u2208 R t, and let I be an n\u00d7 n identity matrix."}, {"heading": "2.1 A New KWIK Linear Regression Algorithm", "text": "Suppose that a new query ~x arrives. If we were able to solve the linear regression problem Dt\u03b8 = ~zt, then we could predict y\u0302 = ~xT \u03b8, where \u03b8 is the least-squares solution to the system. However, solving this system directly is problematic because: (1) if Dt is rankdeficient the least-squares solution may not be unique and (2) even if we have a solution, we have no information on its confidence.\nWe can avoid the first problem by regularization, i.e. by augmenting the system with I\u03b8 = ~v, where ~v is some arbitrary vector. Regularization certainly distorts the solution, but this gives us a measure of confidence: if the distortion is large, the predictor should have low confidence and output \u22a5. On the other hand, if the distortion is low, it has two important consequences. First, the choice of ~v has little effect, and second, the fluctuations caused by using ~zt instead of ~yt are also minor.\nLet At := [I;D T t ] T . The solution of the system At\u03b8 = [(\u03b8\u2217)T ; ~yTt ]\nT is unique, and equal to \u03b8\u2217. However, the right-hand side of this system includes the unknown \u03b8\u2217, so we use the approximate system At\u03b8 = [~0 T ;~zTt ] T , which has a solution \u03b8\u0302 = (ATt At) \u22121ATt [~0 T ;~zTt ] T . Define Qt := (A T t At) \u22121. The prediction error for ~x is\ny\u0302 \u2212 y = ~xT (\u03b8\u0302 \u2212 \u03b8\u2217) (1)\n= ~xTQtA T t ([ ~0 ~zt ] \u2212 [ \u03b8\u2217 ~yt ]) = ~xTQtA T t ([ ~0 ~\u03b7t ] \u2212 [ \u03b8\u2217 ~0 ]) .\nIf \u2016Qt~x\u2016 is small, then both terms of (1) will be small, although for different reasons. If \u2016Qt~x\u2016 is larger than a suitable threshold \u03b10, our algorithm will output \u22a5, otherwise it outputs y\u0302, which will be an accurate prediction with high probability.\nAlgorithm 1 describes our method for KWIK-learning a linear model. Notice it avoids the problem of storing At and ~zt, which grow without bound as t \u2192 \u221e. The quantities Qt = (A T t At) \u22121 and ~wt = A T t [~0 T , ~zTt ] T are sufficient for calculating the predictions, and can be updated incrementally (see Algorithm 1). The algorithm is KWIK, as shown next.\nTheorem 2.1 Let \u03b4 > 0 and \u01eb > 0. If Algorithm 1 is executed with \u03b10 := min { c\u00b7\u01eb2\nlog n \u03b4\u01eb\n, \u01eb2M\n} with\nAlgorithm 1 Learning of linear model\ninput: \u03b10 initialize: t := 0, m := 0, Q := I, ~w := ~0 repeat\nobserve ~xt if \u2016Q~xt\u2016 < \u03b10 then predict y\u0302t = ~x T t Q~w //known state else\npredict y\u0302t =\u22a5 //unknown state observe zt Q := Q\u2212 (Q~xt)(Q~xt) T\n1+~xTt Q~xt , ~w := ~w + ~xtzt\nt := t+ 1 until there are more samples\na suitable constant c, then the number of \u22a5s will be O ( n \u01eb2 max { 1 \u01eb2 log 2 n \u03b4\u01eb ,M 2 })\n, and with probability at least 1 \u2212 \u03b4, for each sample ~xt for which a prediction y\u0302t is made, |y\u0302t \u2212 f(~xt)| \u2264 \u01eb holds. 1\nThis result is a \u0398(n2/ log2 n) improvement over the sample complexity of Strehl & Littman\u2019s KWIK online linear regression, and requires \u0398(n2) operations per timestep, in contrast to their O(tn2) complexity."}, {"heading": "2.2 Proof sketch of Theorem 2.1", "text": "The second term of the prediction error (1) is ~xTQ\u03b8\u2217, which can be bounded by \u2223\u2223~xTQ\u03b8\u2217 \u2223\u2223 \u2264 \u2016Q~x\u2016 \u2016\u03b8\u2217\u2016 \u2264 \u03b10M. This will be no more than \u01eb/2 if \u03b10 \u2264 \u01eb/(2M).\nWe now consider the first term of (1). Fix a constant \u03b1 \u2208 R, and let m be the number of timesteps when \u2016Qt~xt\u2016 > \u03b1. We show that m < 2n/\u03b1\n2. We proceed by showing that the traces of the Qt matrices are positive, monotonically decreasing, and decrease considerably when \u2016Qt~xt\u2016 is large. Q1 = I, so trace(Q1) = n. For t \u2265 1, Qt+1 \u2212 Qt = \u2212 (Qt~xt)(Qt~xt) T\n1+~xTt Qt~xt . To lower-\nbound the update, note that ATt At = (I+D T t Dt) \u2265 I, so Qt = (A T t At)\n\u22121 \u2264 I. Therefore, 1 + ~xTt Qt~xt \u2264 1 + ~xTt ~xt \u2264 2.\ntrace(Qt+1)\u2212 trace(Qt) = \u2212 trace((Qt~xt)(Qt~xt)T )\n1+~xTt Qt~xt\n\u2264 \u2212 12 trace ( (Qt~xt)(Qt~xt) T ) = \u2212 12 \u2016Qt~xt\u2016 2 , (2)\nWhen \u2016Qt~xt\u2016 > \u03b1, this expression is at most \u2212 \u03b12\n2 and at most 0 otherwise. Applying (2) iteratively, we get that trace(Qt+1) \u2264 trace(Q1) \u2212m \u03b12 2 = n\u2212m \u03b12\n2 . On the other hand, Qt+1 is positive definite, which implies that trace(Qt+1) is positive. So, m < 2n/\u03b1 2.\nLet {\u03b1k} (k = 0, 1, . . .) be a monotonically decreasing, 0-limit sequence of constants to be defined later, and\n1The theorem also holds if the noise is Gaussian. There will be a difference only in the constant terms.\nlet mk be the number of samples where \u2016Qt~xt\u2016 > \u03b1k is true.\nFix t and drop the subscripts of ~xt, At and Qt. Suppose a new sample ~x arrives and the algorithm decides that it is \u201cknown\u201d, i.e., \u2016Q~x\u2016 \u2264 \u03b10. The first term of the prediction error (1) is a weighted sum of the noise, \u2211t i=1(~x\nTQ~xi)\u03b7i. If \u2016Q~x\u2016 = 0 then the prediction error is 0. Otherwise let k be the index for which \u03b1k+1 < \u2016Q~x\u2016 \u2264 \u03b1k. The number of ~x inputs falling in the range (\u03b1k+1, \u03b1k] is at most mk+1. We can bound the squared sum of the weights of the noise terms:\nt\u2211\ni=1\n(~xTQ~xi) 2 =\nn+t\u2211\ni=n+1\n( [~xTQAT ]i )2 < (AQ~x)T (AQ~x)\n= ~xTQ(ATAQ)~x = ~xTQ~x \u2264 \u2016Q~x\u2016 \u2016~x\u2016 \u2264 \u03b1k .\nLet \u03b41 > 0 be a constant to be determined later. We can apply Azuma\u2019s inequality to the weighted sum of random variables \u03b7i, with weights ~x\nTQ~xi, which gives the result that the probability that the first term of the error is larger than \u01eb/2 is at most 2 exp ( \u2212 \u01eb 2\n16S2\u03b1k\n) . This probability will be no more\nthan \u03b41/2 k if \u03b1k \u2264\n\u01eb2\n16S2 log(2k+1/\u03b41) . Putting together\nthe two terms, if \u03b1k := min {\n\u01eb2\n16S2 log(2k+1/\u03b41) , \u01eb2M\n} ,\nthen |y\u0302 \u2212 y| < \u01eb with probability at least 1 \u2212 \u03b41/2 k. With these settings, the number of \u22a5s is at most m0 \u2264 2n \u03b12\n0\n= \u0398 ( max { n log2(1/\u03b41)\n\u01eb4 , nM2 \u01eb2\n}) . The to-\ntal probability of an error is at most \u2211\u221e\nk=0 mk \u03b41 2k\n\u2264 \u2211\u221e\nk=0 2n \u03b12\nk\n\u03b41 2k = \u0398\n( max { n\u03b41 log\n2(1/\u03b41) \u01eb4 , n\u03b41M 2 \u01eb2\n}) . Let\nus set \u03b41 so that the above probability is less than \u03b4. For this, the following assignment is sufficient: \u03b41 := O ( min { \u01eb4\u03b42 n , \u01eb2\u03b4 nM2 }) ."}, {"heading": "3 Application 1: Learning Rewards in a Factored-State MDP", "text": "A Markov decision process (MDP) [16] is characterized by a quintuple (X, A,R, P, \u03b3), where X is a finite set of states; A is a finite set of actions; R : X \u00d7 A \u2192 R is the reward function of the agent; P : X\u00d7A\u00d7X \u2192 [0, 1] is the transition function; and finally, \u03b3 \u2208 [0, 1) is the discount rate on future rewards. A factored-state Markov decision process (fMDP) is a structured MDP, where X is the Cartesian product of m smaller components: X = X1 \u00d7X2 \u00d7 . . . \u00d7Xm. A function f is a local-scope function if it is defined over a subspace X[Z] of the state space, where Z is a (presumably small) index set. We make the standard assumption [9] that for each i there exist sets \u0393i of size O(1) such that ~xt+1[i] depends only on ~xt[\u0393i] and at. The transition probabilities are then\nAlgorithm 2 Reward learning in fMDPs with optimistic initialization input: R0 M\u2212r = ( {Xi} m 1 ;A; {Pi} m 1 ; ~xs; \u03b3; {\u0393i} m 1 ; {Zj} J 1 )\nt := 0, ~x0 := ~xs, m := 0, Q := I, ~w := R0~1 repeat\n~\u0302r := Q~w at := greedy action in fMDP(M \u2212r, ~\u0302r), state ~xt execute at, observe reward rt, next state ~xt+1 ~u := \u03c7(~xt, at) Q := Q\u2212 (Q~u)(Q~u) T\n1+~uTQ~u , ~w := ~w + ~uT rt\nt := t+ 1 until there are more samples\nP (~y | ~x, a) = \u220fm\ni=1 Pi(~y[i] | ~x[\u0393i], a) for each ~x, ~y \u2208 X, a \u2208 A, so each factor is determined by a local-scope function. We make the analogous assumption that the reward function is the sum of J local-scope functions with scopes Zj : R(~x, a) = \u2211J j=1 Rj(~x[Zj ], a). An fMDP is fully characterized by the tuple M =( {Xi} m i=1;A; {Pi} m i=1; {Rj} J j=1; ~xs; \u03b3; {\u0393i} m i=1; {Zj} J j=1 ) . Algorithms exist for learning transition probabilities [9] and dependency structure [14], but until now, no algorithm existed for learning the reward functions.\nFor J > 1, we can only observe the sums of unknown quantities, not the quantities themselves. Doing so requires the solution of an online linear regression problem with a suitable encoding of the reward model. Let Nr be the total number of parameters describing the reward model (Nr \u2264 J |A|n\nnf ), and consider the indicator function \u03c7(~x, a) \u2208 RNr , which is 1 on indices corresponding to Rj(~x[Zj ], a) and 0 elsewhere.\nOur solution, Algorithm 2, is a modification of Algorithm 1. We initialize all unknown rewards to some constant R0 (analogous to the common maximum reward parameter Rmax [4]). If R0 is sufficiently high, the algorithm outputs optimistic reward estimates for unknown states (instead of \u22a5), and otherwise gives near-accurate predictions with high probability. This property follows from standard arguments [17]: for unknown states, the noise term of the prediction error can be bounded by Azuma\u2019s inequality, and R0 can be set high enough so that the second term is positive and dominates. This form of optimistic initialization has proven consistently better than R-max in flat MDPs [17]. For known states, the KWIK guarantees suffice to ensure near-optimal behavior. Because it combines a KWIK model-learner with R-max style exploration, this algorithm is PAC-compact-MDP\u2014the first efficient algorithm for this task. In Section 5.1 we combine this algorithm with another KWIK learner that learns the transition dependency structure and probabilities [11] to learn the full fMDP model."}, {"heading": "3.1 Experiments", "text": "We carried out reward-learning experiments on the Stocks domain [14], with 3 sectors and 2 stocks per sector. Rewards were uniformly random in the interval [0.5, 1.5] for owning a rising stock, and random in [\u22121.5,\u22120.5] for owning a decreasing stock. We compared six algorithms: (1) Algorithm 2; (2) Algorithm 1 modified to output Rmax in unknown states; (3) the previous state-of-the-art KWIK-LR algorithm [15] modified to output Rmax in unknown states; (4) a flat tabular reward learner; and to demonstrate the need for efficient exploration, (5) linear regression without exploration and (6) linear regression with epsilon-greedy exploration.\nEach algorithm was run 20 times for 250 steps, updating the model every 5 steps. For the Stocks(3,2) domain, Rmax = 6. We used R0 = 10 for Algorithm 2, \u03b10 = 1.0 for the second approach and \u03b11 = \u03b12 = 1.0 for the third one. The values of the learned policies are shown in Figure 1. All curves except (1) and (2) differ significantly on a 95% confidence level. Notice (3) and (5) take longer to learn the model, (4) takes far longer, and (6) fails to explore and find the correct model."}, {"heading": "4 Learning Transition Probabilities", "text": "We consider another novel application of KWIK linear regression\u2014learning action-effect probabilities in environments where these effects may be ambiguous. Specifically, we consider environments where actions are encoded as stochastic action schemas (e.g. travel(X, Y) rather than travel(Paris, Rome)) and the effects of these actions are stochastic. For instance, the action travel(X, Y) may result in the effect at(Y) with probability .9 and the effect at(X) with probability .1. More formally, every action a \u2208 A is of the\nform a = [(\u03c90, p0) \u00b7 \u00b7 \u00b7 (\u03c9n, pn)] where each \u03c9i \u2208 \u2126 a is a possible effect. When the action is taken, one of these effects occurs according to the probability distribution induced by the pis. The schemas may also contain conditional effect distributions, the nature of which is determined by the specific language used (as discussed in the following subsections). This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13]. Learning these probabilities is non-trivial because for a given state action pair (s, a), the effects are partitioned into equivalence classes E(s, a) = {{\u03c9i, \u03c9j , \u03c9k}, {\u03c9l, \u03c9m}, ...} where each e \u2208 E(s, a) contains effects that are identical given state s. For instance, if an action has two possible effects Holding(X) and Holding(Y), but is taken in a state where X and Y are both already held, we cannot tell which actually occurred. Notice that the probability of any equivalence class is equal to the sum of the probabilities of the effects it contains, hence the link to linear regression.\nThe standard \u201ccounting\u201d method for learning probabilities cannot be used in such a setting because it is unclear which effect\u2019s count (Holding(X)s or Holding(Y)s) we should increment. However, KWIK-LR can be used to learn the probabilities. While standard linear regression could also be used if transition data was available in batch form, an RL agent using KWIKLR can learn the probabilities online and maintain the PAC-compact-MDP guarantee needed for effective exploration. Algorithm 3 presents a PAC-compactMDP algorithm for an agent that is given full actionoperator specifications, except for the probabilities. That is, since our focus is on learning the probabilities, we assume that each \u03c9i is known, but not the pis. We also assume the preconditions and reward structure of the problem are known. We discuss methods for relaxing these assumptions in Section 5. This algorithm can be used with several representations including those mentioned above.\nIntuitively, Algorithm 3 computes an optimistically optimal action at using a planner (Line 7, detailed below) and then gathers experience indicating which of the equivalence classes et \u2208 E(st, at) actually occurred. For instance, it may see that Holding(X) occurred, instead of Holding(Y). This equivalence class contains one or more effects (\u03c9i, \u03c9j ... \u2208 \u2126\na), and an indicator vector ~x is created where ~xi = 1 if \u03c9i \u2208 et (Line 10). For instance, if the agent was not holding anything, and then Holding(X) occurred and not Holding(Y), the vector would be [1; 0], but in a state where both were already held, ~x would always come out [1; 1]. Note that each equivalence class in E(s, a)\nAlgorithm 3 Transition Probability Learner\n1: input: S, A (action schemas sans pis), R, \u03b1 2: \u2200a \u2208 A, instantiate a learner (Algorithm 1) La(\u03b1) 3: for each current state st do 4: for each e \u2208 E(s, a), s \u2208 S and a \u2208 A do 5: Construct ~x where xj = 1 if j \u2208 e, else 0. 6: P\u0302 (e) = Prediction of La(~x) // can be \u22a5 7: Perform modified value iteration on {S,A,E, P\u0302 , R}, to get greedy policy \u03c0 8: Perform action at = \u03c0(st), observe et \u2208 E(st, at) 9: for equivalence classes e \u2208 E(st, at) do 10: Construct ~x where xj = 1 if j \u2208 e, else 0. 11: Update Lat with ~x and y = 1 if e = et, else\ny = 0\ninduces a unique (and disjoint) ~x. Each of these is used to update the KWIK-LR learner (Lines 9-11), with an output (y) of 1 associated with the ~x that actually happened (which may have 1s in multiple places, as in the ambiguous Holding case). Given any possible action and equivalence class in the state/action space, the KWIK-LR agent can now be queried to determine the probability of the equivalent transition (Lines 5-6), though it may return \u22a5, identifying transitions from equivalence partitions that have unlearned probability.\nPlanning in Algorithm 3 is done by constructing a transition model in the grounded state space (Lines 4-6). KWIK-LR determines for each E(s, a), the probability of the possible next states (one for each e \u2208 E(s, a)). A modified version of value iteration [16] (Line 7) is then used to plan the optimal next action. The modification is that at every iteration, for every state-action pair that has effects with known probabilities K = {\u03c9i, \u03c9j ...} and unknown probabilities U = {\u03c9k, \u03c9l...}, the effect in U that leads to the highest value next state is considered to have probability 1 \u2212\n\u2211 \u03c9i\u2208K\nP (\u03c9i). This change is designed to force value iteration to be optimistic\u2014it considers the most rosiest of all models consistent with what has been learned. We note that planning in the flat state space can require exponential computation time, but this is often unavoidable, and since the model is still learned in a sample efficient manner, it satisfies the conditions for PAC-compact-MDP."}, {"heading": "4.1 Application 2: Stochastic STRIPS", "text": "STRIPS domains [8] are made up of a set of objects O, a set of predicates P , and a set of actions A. The actions have conjunctive (over P ) preconditions and effects specified by ADD and DELETE (DEL) lists, which specify what predicates are added and deleted from the world state when the action occurs. Stochastic STRIPS operators generalize this representation\nby considering multiple possible action effects specified by \u3008ADD, DEL, PROB\u3009 tuples as in Table 1. Notice this representation is an instance of the general action schemas defined above. While others have looked at learning similar operators [12], their work attempted to heuristically learn the full operators (including structure), and could not give any guarantees (as we do) on the behavior of their algorithm, nor did they identify an efficient algorithm for learning the probabilities, as we have with Algorithm 3. To make the planning step well defined, we consider Stochastic STRIPS with rewards [18].\nTable 1 illustrates a variant of the familiar Paint/Polish domain in the Stochastic STRIPS setting. There are several ambiguous effects. For instance, executing paint on a scratched but not painted object, and observing it is now scratched and painted, one cannot tell which of the first two effects occurred. We used this domain with a single object to empirically test Algorithm 3 against a version of the algorithm that does not use KWIK-LR, instead attempting to learn every possible equivalence class partition distribution separately (Partition). Because the focus here is on learning the transition probabilities (pis), both learners were given the preconditions and effects (\u03c9i) of each action. We discuss relaxing these assumptions later. Figure 2 shows the results (with the known/unknown thresholds empirically tuned) averaged over 1000 runs with randomized initial states for each episode (both learners receive the same initial states). Algorithm 3 learns much faster because it effectively shares information between partitions."}, {"heading": "4.2 Application 3: Stochastic Object Oriented RL", "text": "Object-oriented MDPs [5] consist of a set of objects O, a set of actions A that take elements of O as parameters, and (in the original deterministic description) a set of condition-effect pairs \u3008c, \u03c9\u3009 associated with each action. Objects have attributes, and the set of all objects and their attribute values at a given time t constitute the state st. When an action a is executed from st, the environment checks which condition ci \u2208 C is satisfied (if any), and applies the corresponding effect wi \u2208 \u2126\na, which updates the attributes of the affected objects. Stochastic OOMDP effects generalize this representation so that a given condition induces not just a single effect (\u03c9) but a distribution over possible effects (\u2126ca), only one of which (\u03c9i) will actually occur on a single timestep. In the parlance of action schemas as defined above, each {c,\u2126} pair for action a can be thought of as its own action with preconditions specified by c. Again, this model can be viewed as a specific version of the action schemas presented above.\nPrevious work [5] presented an efficient algorithm for learning deterministic effects. Here, we demonstrate Algorithm 3 learning the probabilities associated with each effect in the stochastic setting when the possible effects given a condition and the conditions themselves are known in advance. Methods for relaxing these assumptions are discussed in later sections.\nWe demonstrate Algorithm 3 for stochastic OOMDPs on a simple 5\u00d75 Maze domain, illustrated in Figure 3. The agent starts at location S and the goal is to arrive at G. Each step has a cost of \u22120.01, and arriving at the goal results in a reward of +1. The agent\u2019s actions are N, S, E and W. When executing an action, the agent will attempt to move in the desired direction with probability 0.8 and will slip to either side with probability 0.1. If it hits a wall, it stays put. This rule is what produces ambiguity in the effects. For\nexample, imagine the agent has a wall to its North and East. If it attempts the N action, it could move to the West (with probability 0.1), or stay in place. If it stays in place, it might be because it attempted to move North (with probability 0.8) and hit the North wall, or it attempted to move East (with probability 0.1) and hit the East wall.\nFigure 4 presents the results. As in the previous experiment, we can see that the KWIK-RL algorithm learns much faster than the \u201cPartition\u201d learner by sharing information between equivalence partitions."}, {"heading": "5 Extensions", "text": "We now discuss extensions for learning more of the compact representations discussed in this work. We will outline how to learn full fMDPs and larger parts of STRIPS and OOMDPmodels by combining KWIKLR with other KWIK learners, and provide empirical support in the fMDP case. We also discuss a variation of KWIK-LR that can be used to learn stochastic action-schema outcomes (\u2126a)."}, {"heading": "5.1 Combining with Other KWIK Learners", "text": "Existing work [11] describes methods for combining simple KWIK agents to learn in increasingly com-\nplex domains. We follow this \u201cbuilding blocks\u201d approach and show how to combine the fMDP rewardlearning algorithm (Algorithm 2) with an implementation of the Noisy-Union [11; 10] algorithm (which is also KWIK) to learn the transition structure (\u0393), transition probabilities (P ), and reward function (R) of an fMDP all at once. The only knowledge given to the agent is the number of parents a factor may have (|Z|) and the reward-function structure, resulting in the only algorithm to date that learns all of these parameters efficiently. Experience is fed to both building block algorithms in parallel. The reward learner outputs an optimistic approximation of the reward function, which is given to Noisy-Union, which then learns the transition structure and probabilities. We conducted experiments to validate this algorithm in the Stocks domain. For comparison, we also ran the NoisyUnion algorithm with the rewards given a priori. Figure 5 displays the results, which show that all three quantities can be learned with small overhead.\nWe note that there are many other settings that could benefit from combining KWIK-LR with other KWIK learners for different parts of the model. For instance, in stochastic STRIPS and OOMDPs, the preconditions of actions (STRIPS) or the conditional effects (OOMDP) can be learned using an existing KWIK adaptation of Noisy Union [6] as long as their size is bounded by some known constant. Together, these learners could be used to learn all but the effects (\u2126a) of each action operator, a problem we now consider."}, {"heading": "5.2 Future Work: Learning Effects", "text": "All of the effect distribution / condition learning variations require the possible effects (\u2126a) as input. Unfortunately, relaxing this assumption in the stochastic case is unlikely, since the effect learning problem is known to be NP-Hard [12]. When the number of possible effects is very small, or each \u03c9i is of constant size,\nenumeration techniques could be used. But, these assumptions are often violated, so researchers have concentrated on heuristic solutions [12]. Here, we suggest a novel heuristic that extends KWIK-LR for probability learning to the setting where the whole action schema (including \u2126a) needs to be learned.\nWe propose using sparsification; we consider all possible effects and use KWIK-LR to learn their probabilities, with an extra constraint that the number of \u201cactive\u201d (non-zero) probabilities should be small. This minimization can be efficiently computed via linear programming, and techniques such as column generation may be used to keep the number of active constraints small in most cases. Together with the other learners discussed in this paper, the implementation of such an extension would form a complete solution to the action-schema learning problem: the probabilities and preconditions can be learned with KWIK-learners and the effects themselves can be learned heuristically using this sparsification extension. It remains a matter for future work to compare this system to other heuristic solutions [12] for such problems."}, {"heading": "6 Related Work", "text": "Online linear regression has also been studied in the regret minimization framework (see e.g. [1]). Applications to restricted RL problems also exist [2], but with different types of bounds. Furthermore, regret analysis seems to lack the modularity (the ability to combine different learners) of the KWIK framework.\nPrevious work on linear regression for model-based RL has focussed on learning linear transition functions in continuous spaces. However, these approaches often lacked theoretical guarantees or placed restrictions on the environment regarding noise and the reward structure [7]. In this paper, we have both improved on the current state-of-the-art algorithm for linear regression in RL [15], and used it in applications beyond the standard linear transition models. Our theoretical results and experiments illustrate the potential of KWIK-LR in model-based RL. In the future, we intend to identify other compact models where this technique can facilitate efficient learning and perform expanded empirical and theoretical studies of the extensions mentioned above."}, {"heading": "Acknowledgements", "text": "We thank Alexander L. Strehl and the reviewers for helpful comments. Support was provided by the Fullbright Foundation, DARPA IPTO FA8750-05-2-0249, FA8650-06-C-7606, and NSF IIS-0713435."}], "references": [{"title": "An improved on-line algorithm for learning linear evaluation functions", "author": ["P. Auer"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Adaptive and self-confident on-line learning", "author": ["P. Auer", "N. Cesa-Bianchi", "C. Gentile"], "venue": "algorithms. JCSS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "R-MAX\u2013a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["C. Diuk", "A. Cohen", "M.L. Littman"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "The adaptive kmeteorologists problem and its application to structure learning and feature selection in reinforcement learning", "author": ["C. Diuk", "L. Li", "B. Leffler"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "PAC adaptive control of linear systems", "author": ["C.-N. Fiechter"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "STRIPS: A new approach to the application of theorem proving to problem solving", "author": ["R. Fikes", "N.J. Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1971}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["M.J. Kearns", "D. Koller"], "venue": "In IJCAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "A Unifying Framework for Computational Reinforcement Learning Theory", "author": ["L. Li"], "venue": "PhD thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Knows what it knows: A framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Learning symbolic models of stochastic domains", "author": ["H.M. Pasula", "L.S. Zettlemoyer", "L.P. Kaelbling"], "venue": "JAIR, 29:309\u2013352,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Improving action selection in MDP\u2019s via knowledge transfer", "author": ["A.A. Sherstov", "P. Stone"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Efficient structure learning in factored-state MDPs", "author": ["A.L. Strehl", "C. Diuk", "M.L. Littman"], "venue": "In AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Online linear regression and its application to model-based reinforcement learning", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "The many faces of optimism: a unifying approach", "author": ["I. Szita", "A. L\u0151rincz"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "The First Probabilistic Track of the International Planning Competition", "author": ["H.L.S. Younes", "M.L. Littman", "D. Weissman", "J. Asmuth"], "venue": "JAIR, 24:851\u2013887,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}], "referenceMentions": [{"referenceID": 15, "context": "While the field of Reinforcement Learning (RL) [16] has certainly made use of linear regression in approximating value functions [3], using online regression to learn parameters of a model has been limited to environments with linear dynamics (e.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "While the field of Reinforcement Learning (RL) [16] has certainly made use of linear regression in approximating value functions [3], using online regression to learn parameters of a model has been limited to environments with linear dynamics (e.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "[7]), and has often been unable to make guarantees about the behavior of the resulting learning agent without strict assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Recently, the introduction of the KWIK (Knows What It Knows) framework [11] has provided a characterization of sufficient conditions for a model-learning algorithm to induce sample-efficient behavior in a reinforcement-learning agent.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "One of the first algorithms developed for this framework was a KWIK linear regression algorithm [15], which was used to learn the transition function of an MDP with linear dynamics.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Specifically, we use KWIK linear regression (KWIK-LR) to learn the reward function in a factored MDP and the transition probabilities in domains encoded using Stochastic STRIPS [12] or Object Oriented MDPs (OOMDP) [5].", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "Specifically, we use KWIK linear regression (KWIK-LR) to learn the reward function in a factored MDP and the transition probabilities in domains encoded using Stochastic STRIPS [12] or Object Oriented MDPs (OOMDP) [5].", "startOffset": 214, "endOffset": 217}, {"referenceID": 10, "context": "KWIK [11] (Knows What It Knows) is a framework for studying supervised learning algorithms and was designed to unify the analysis of model-based reinforcement-learning algorithms.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "It has been shown [10] that in the model-based reinforcement-learning setting, if the underlying model learner is KWIK, then it is possible to build an RL agent around it by driving exploration of the \u22a5 areas using an R-max [4] style manipulation of the value function.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "It has been shown [10] that in the model-based reinforcement-learning setting, if the underlying model learner is KWIK, then it is possible to build an RL agent around it by driving exploration of the \u22a5 areas using an R-max [4] style manipulation of the value function.", "startOffset": 224, "endOffset": 227}, {"referenceID": 14, "context": "One of the first uses of the KWIK framework was in the analysis of an online linear regression algorithm used to learn linear transitions in continuous state MDPs [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "A Markov decision process (MDP) [16] is characterized by a quintuple (X, A,R, P, \u03b3), where X is a finite set of states; A is a finite set of actions; R : X \u00d7 A \u2192 R is the reward function of the agent; P : X\u00d7A\u00d7X \u2192 [0, 1] is the transition function; and finally, \u03b3 \u2208 [0, 1) is the discount rate on future rewards.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "A Markov decision process (MDP) [16] is characterized by a quintuple (X, A,R, P, \u03b3), where X is a finite set of states; A is a finite set of actions; R : X \u00d7 A \u2192 R is the reward function of the agent; P : X\u00d7A\u00d7X \u2192 [0, 1] is the transition function; and finally, \u03b3 \u2208 [0, 1) is the discount rate on future rewards.", "startOffset": 213, "endOffset": 219}, {"referenceID": 8, "context": "We make the standard assumption [9] that for each i there exist sets \u0393i of size O(1) such that ~xt+1[i] depends only on ~xt[\u0393i] and at.", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Algorithms exist for learning transition probabilities [9] and dependency structure [14], but until now, no algorithm existed for learning the reward functions.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Algorithms exist for learning transition probabilities [9] and dependency structure [14], but until now, no algorithm existed for learning the reward functions.", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "We initialize all unknown rewards to some constant R0 (analogous to the common maximum reward parameter Rmax [4]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 16, "context": "This property follows from standard arguments [17]: for unknown states, the noise term of the prediction error can be bounded by Azuma\u2019s inequality, and R0 can be set high enough so that the second term is positive and dominates.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "This form of optimistic initialization has proven consistently better than R-max in flat MDPs [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "1 we combine this algorithm with another KWIK learner that learns the transition dependency structure and probabilities [11] to learn the full fMDP model.", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "We carried out reward-learning experiments on the Stocks domain [14], with 3 sectors and 2 stocks per sector.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "We compared six algorithms: (1) Algorithm 2; (2) Algorithm 1 modified to output Rmax in unknown states; (3) the previous state-of-the-art KWIK-LR algorithm [15] modified to output Rmax in unknown states; (4) a flat tabular reward learner; and to demonstrate the need for efficient exploration, (5) linear regression without exploration and (6) linear regression with epsilon-greedy exploration.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13].", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 15, "context": "A modified version of value iteration [16] (Line 7) is then used to plan the optimal next action.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "STRIPS domains [8] are made up of a set of objects O, a set of predicates P , and a set of actions A.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "While others have looked at learning similar operators [12], their work attempted to heuristically learn the full operators (including structure), and could not give any guarantees (as we do) on the behavior of their algorithm, nor did they identify an efficient algorithm for learning the probabilities, as we have with Algorithm 3.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "To make the planning step well defined, we consider Stochastic STRIPS with rewards [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Object-oriented MDPs [5] consist of a set of objects O, a set of actions A that take elements of O as parameters, and (in the original deterministic description) a set of condition-effect pairs \u3008c, \u03c9\u3009 associated with each action.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Previous work [5] presented an efficient algorithm for learning deterministic effects.", "startOffset": 14, "endOffset": 17}, {"referenceID": 10, "context": "Existing work [11] describes methods for combining simple KWIK agents to learn in increasingly com0 500 1000 1500 2000 2500 3000 3500 4000 \u22121000 0 1000 2000 3000 4000 5000 6000", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "For instance, in stochastic STRIPS and OOMDPs, the preconditions of actions (STRIPS) or the conditional effects (OOMDP) can be learned using an existing KWIK adaptation of Noisy Union [6] as long as their size is bounded by some known constant.", "startOffset": 184, "endOffset": 187}, {"referenceID": 11, "context": "Unfortunately, relaxing this assumption in the stochastic case is unlikely, since the effect learning problem is known to be NP-Hard [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "But, these assumptions are often violated, so researchers have concentrated on heuristic solutions [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "It remains a matter for future work to compare this system to other heuristic solutions [12] for such problems.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Applications to restricted RL problems also exist [2], but with different types of bounds.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "However, these approaches often lacked theoretical guarantees or placed restrictions on the environment regarding noise and the reward structure [7].", "startOffset": 145, "endOffset": 148}, {"referenceID": 14, "context": "In this paper, we have both improved on the current state-of-the-art algorithm for linear regression in RL [15], and used it in applications beyond the standard linear transition models.", "startOffset": 107, "endOffset": 111}], "year": 2009, "abstractText": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}