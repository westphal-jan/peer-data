{"id": "1205.2619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Regret-based Reward Elicitation for Markov Decision Processes", "abstract": "the specification of amarkov decision process ( pronounced mdp ) can be difficult. reward function specification complexity is especially problematic ; in practice, it is often cognitively complex and time - spent consuming for users wishing to still precisely specify rewards. this work casts the problem of clearly specifying rewards as one of preference elicitation and aims to minimize the degree of objective precision with which a reward function must be specified while constraints still allowing optimal or near - optimal policies judgments to be produced. we first discuss how robust policies outcomes can be computed together for mdps given only partial reward desired information using the null minimax regret criterion. we then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret - reduction as a means for choosing suitable queries. empirical results first demonstrate that regret - based proportional reward elicitation offers an effective way to equally produce near - optimal policies without resorting backwards to the precise specification of the entire reward function.", "histories": [["v1", "Wed, 9 May 2012 18:23:30 GMT  (317kb)", "http://arxiv.org/abs/1205.2619v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kevin regan", "craig boutilier"], "accepted": false, "id": "1205.2619"}, "pdf": {"name": "1205.2619.pdf", "metadata": {"source": "CRF", "title": "Regret-based Reward Elicitation for Markov Decision Processes", "authors": ["Kevin Regan", "Craig Boutilier"], "emails": ["kmregan@cs.toronto.edu", "cebly@cs.toronto.edu"], "sections": [{"heading": null, "text": "The specification of a Markov decision process (MDP) can be difficult. Reward function specification is especially problematic; in practice, it is often cognitively complex and time-consuming for users to precisely specify rewards. This work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced. We first discuss how robust policies can be computed for MDPs given only partial reward information using the minimax regret criterion. We then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret-reduction as a means for choosing suitable queries. Empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function."}, {"heading": "1 Introduction", "text": "Markov decision processes (MDPs) have proven to be an extremely useful formalism for decision making in stochastic environments. However, the specification of an MDP by a user or domain expert can be difficult, e.g., cognitively demanding, computationally costly, or time consuming. For this reason, much work has been devoted to learning the dynamics of stochastic systems from transition data, both in offline [11] and online (i.e., reinforcement learning) settings [19]. While model dynamics are often relatively stable in many application domains, MDP reward functions are muchmore variable, reflecting the preferences and goals of specific users in that domain. This makes reward function specification more difficult: they can\u2019t generally be specified a priori, but must be elicited or otherwise assessed for individual users. Even online RL methods require the specification of a user\u2019s reward function in some form: unlike state transitions, it is impossible to directly observe a reward function except in very specific settings with simple, objectively definable, observable performance\ncriteria. The \u201cobservability\u201d of reward is a convenient fiction often assumed in the RL literature.\nReward specification is difficult for three reasons. First, it requires the translation of user preferences\u2014which states and actions are \u201cgood\u201d and \u201cbad\u201d\u2014into precise numerical rewards. As has been well-recognized in decision analysis, people find it extremely difficult to quantify their strength of preferences precisely using utility functions (and, by extension, reward functions) [10]. Second, the requirement to assess rewards and costs for all states and actions imposes an additional burden (one that can be somewhat alleviated by the use of multiattributemodels in factoredMDPs [5]). Finally, the elicitation problem in MDPs is further exacerbated by the potential conflation of immediate reward (i.e., r(s, a)) with long-term value (eitherQ(s, a) or V (s)): states can be viewed as good or bad based on their ability to make other good states reachable.\nIn this paper, we tackle the problem of reward elicitation in MDPs by treating it as a preference elicitation problem. Recent research in preference elicitation for non-sequential decision problems exploits the fact that optimal or nearoptimal decisions can often be made with relatively imprecise specification of a utility function [6, 8]. Interactive elicitation and optimization techniques take advantage of feasibility restrictions on actions or outcomes to focus their elicitation efforts on only the most relevant aspects of a utility function. We adopt a similar perspective in the MDP setting, demonstrating that optimal and near-optimal policies can be often found with limited reward information. For instance, reward bounds in conjunction with MDP dynamics can render certain regions of state space provably dominated by others (w.r.t. value).\nWe make two main contributions that allow effective elicitation of reward functions. First, we develop a novel robust optimization technique for solving MDPs with imprecisely specified rewards. Specifically, we adopt the minimax regret decision criterion [6, 18] and develop a formulation for MDPs: intuitively, this determines a policy that has minimum regret, or loss w.r.t. the optimal policy, over all possible reward function realizations consistent with the cur-\nrent partial reward specification. Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty. We describe an exact computational technique for minimax regret and suggest several approximations. Second, we develop a simple elicitation procedure that exploits the information provided by the minimax-regret solution to guide the querying process. In this work, we focus on simple schemes that refine the upper and lower bounds of specific reward values. We show that good or optimal policies can be determined with very imprecise reward functions when elicitation effort is focused in this way. Our work thus tackles the problem of reward function precision directly. While we do not address the issue of reward-value conflation in this model, we will discuss it further below."}, {"heading": "2 Notation and Problem Formulation", "text": "We begin by reviewing MDPs and defining the minimax regret criterion for MDPs with imprecise rewards."}, {"heading": "2.1 Markov Decision Processes", "text": "Let \u3008S,A, {Psa}, \u03b3, \u03b1, r\u3009 be an infinite horizon MDP with: finite state set S of size n; finite action setA of size k; transition distributions Psa(\u00b7), with Psa(t) denoting the probability of reaching state t when action a is taken at s; reward function r(s, a); discount factor \u03b3 < 1; and initial state distribution \u03b1(\u00b7). Let r be the n\u00d7k-vector with entries r(s, a) and P the n \u00d7 k \u00d7 n transition matrix. We use ra and Pa to denote the obvious restrictions of these to action a. We define E to be the nk\u00d7n-matrix with a row for each stateaction pair and one column per state, with Esa,t = Psa(t) if t 6= s, and Esa,t = Psa(t)\u2212 1 if t = s.\nOur aim is to find an optimal policy that maximizes expected discounted reward. A deterministic policy \u03c0 : S \u2192 A has value function V \u03c0 satisfying:\nV \u03c0(s) = r(s, \u03c0(s)) + \u03b3 \u2211\ns\u2032\nPs\u03c0(s)(s \u2032)V \u03c0(s\u2032)\nor equivalently (slightly abusing subscript \u03c0):\nV\u03c0 = ra\u03c0 + \u03b3Pa\u03c0V \u03c0 (1)\nWe also define the Q-functionQ : S \u00d7A \u2192 R as:\nQ\u03c0 a = ra + \u03b3PaV \u03c0 ,\ni.e., the value of executing \u03c0 forward after taking action a.\nA policy \u03c0 induces a visitation frequency function f\u03c0, where f\u03c0(s, a) is the total discounted joint probability of being in state s and taking action a. The policy can readily be recovered from f\u03c0, via \u03c0(s, a) = f\u03c0(s, a)/ \u2211 a\u2032 f \u03c0(s, a\u2032). (For deterministic policies, f\u03c0 sa =\n0 for all a other than \u03c0(s).) We use F to denote the set of valid visitation frequency functions (w.r.t. a fixed MDP), i.e., those satisfying [17]:\n\u03b3E\u22a4f + \u03b1 = 0. (2)\nThe optimal value function V \u2217 satisfies:\n\u03b1V\u2217 = r\u22a4f\u2217 (3)\nwhere f\u2217 = sup f r\u22a4f [17]. Thus, determining an optimal policy is equivalent to finding optimal frequencies f\u2217."}, {"heading": "2.2 Minimax Regret for Imprecise MDPs", "text": "A number of researchers have considered the problem of solving imprecisely specified MDPs (see below). Here we focus on the solution of MDPs with imprecise reward functions. Since fully specifying reward functions is difficult, we will often be faced with the problem of computing policies with an incomplete reward specification. Indeed, as we see below, we often explicitly wish to leave parts of a reward function unelicited (or otherwise unassessed). Formally we assume that r \u2208 R, where the feasible reward set R reflects current knowledge of the reward. These could reflect: prior bounds specified by a user or domain expert; constraints that emerge from an elicitation process (as discussed below); or constraints that arise from observations of user behavior (as in inverse RL [15]). In all of these situations, we are unlikely to have full reward information. Thus we require a criterion by which to compare policies in an imprecise-reward MDP.\nWe adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7]. Let R be the set of feasible reward functions. Minimax regret can be defined in three stages:\nR(f , r) = max g\u2208F r \u00b7 g \u2212 r \u00b7 f (4)\nMR(f ,R) = max r\u2208R R(f , r) (5)\nMMR(R) = min f\u2208F MR(f ,R) (6)\nR(f , r) is the regret of policy f (as represented by its visitation frequencies) relative to reward function r: it is simply the loss or difference in value between f and the optimal policy under r. MR(f ,R) is the maximum regret of f w.r.t. feasible reward set R. Should we chose a policy with visitation frequencies f , MR(f ,R) represents the worst-case loss over all possible realizations of the reward function; i.e., the regret incurred in the presence of an adversarywho chooses the r from R to maximize our loss. Finally, in the presence of such an adversary, we wish to minimize this max regret: MMR(R) is the minimax regret of feasible reward set R. This can be viewed as a game between a decision maker choosing f who wants to minimize loss relative to the optimal policy, and an adversary who chooses a reward to maximize this loss given the decision maker\u2019s\nchoice of policy. Any f\u2217 that minimizes max regret is a minimax optimal policy, while the r that maximizes its regret is the witness or adversarial reward function, and the optimal policy g for r is the witness or adversarial policy.\nMinimax regret has a variety of desirable properties relative to other robust decision criteria [6]. Compared to Bayesian methods that compute expected value using a prior overR [3, 8], minimax regret provides worst-case bounds on loss. Specifically, let f be the minimax regret optimal visitation frequencies and let \u03b4 be the max regret achieved by f ; then, given any instantiation of r, no policy will outperform f by more than \u03b4 w.r.t. expected value. Minimax optimal decisions can often be computed more effectively than decisions that maximize expected value w.r.t. to some prior. Finally, it has been shown to be a very effective criterion for driving elicitation in one-shot problems [6, 7]."}, {"heading": "2.3 Robust Optimization for Imprecise MDPs", "text": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16]. Restricting attention to imprecise rewards, the maximin value is given by:\nMMN(R) = max f\u2208F min r\u2208R r \u00b7 f (7)\nMost models are defined for uncertainty in any MDP parameters, but algorithmic work has focused on uncertainty in the transition function, and the of eliciting information about transition functions or rewards is left unaddressed. Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16]. McMahan, Gordon, and Blum [14] develop a linear programming approach to efficiently compute the maximin value of an MDP (we empirically compare this approach to ours below). Delage and Mannor [9] address the problem of uncertainty over reward functions (and transition functions) in the presence of prior information, using a percentile criterion, which can be somewhat less pessimistic than maximin. They also contribute a method for eliciting rewards using sampling to approximate the expected value of information of noisy information about a point in reward space. The percentile approach is neither fully Bayesian nor does it offer a bound on performance. Zhang and Parkes ([20]) also adopt maximin in a model that assumes an inverse reinforcement learning setting for policy teaching. The approach is essentially a form of reward elicitation which the queries are changes to a student\u2019s reward, and information is gained by observing change in the student\u2019s behavior.\nGenerally, the maximin criterion leads to conservative policies by optimizing against the worst possible instantiation of r (as we will see below). Minimax regret offers a more\nintuitive measure of performance by assessing the policy ex post and making comparisons only w.r.t. specific reward realizations. Thus, policy \u03c0 is penalized on reward r only if there exists a \u03c0\u2032 that has higher value w.r.t. r itself."}, {"heading": "3 Minimax Regret Computation", "text": "As discussed above, maximin is amenable to dynamic programming since it can be decomposed over decision stages. This decomposition does not appear tenable for minimax regret since it grants the adversary too much power by allowing rewards to be set independently at each stage (though see our discussion of future work below). Following the formulations for non-sequential problems developed in [6, 7], we instead formulate the optimization using a series of linear (LPs) and mixed integer programs (MIPs) that enforce a consistent choice of reward across time.\nAssume feasible reward set R is represented by a convex polytope Cr \u2264 d, which we assume to be bounded. The constraints on r arise as discussed above (prior bounds, elicitation, or behavioral observation). Minimax regret can then be expressed as following minimax program:\nmin f max g max r r \u00b7 g \u2212 r \u00b7 f\nsubject to: \u03b3E \u22a4 f + \u03b1 = 0\n\u03b3E \u22a4 g + \u03b1 = 0\nCr \u2264 d\nThis is equivalent to a minimization:\nminimize f,\u03b4 \u03b4 (8)\nsubject to: r \u00b7 g \u2212 r \u00b7 f \u2264 \u03b4 \u2200 g \u2208 F , r \u2208 R\n\u03b3E \u22a4 f + \u03b1 = 0\nThis corresponds to the standard dual LP formulation of an MDP with the addition of adversarial policy constraints. The infinite number of constraints can be reduced: first we need only retain as potentially active those constraints for vertices of polytopeR; and for any r \u2208 R, we only require the constraint corresponding to its optimal policy g\u2217r . However, vertex enumeration is not feasible; so we apply Benders\u2019 decomposition [2] to iteratively generate constraints.\nAt each iteration, two optimizations are solved. Themaster problem solves a relaxation of program (8) using only a small subset of the constraints, corresponding to a subset Gen of all \u3008g, r\u3009 pairs; we call these generated constraints. Initially, this set is arbitrary (e.g., empty). Intuitively, in the game against the adversary, this restricts the adversary to choosing witnesses (i.e., \u3008g, r\u3009 pairs) from Gen.\nLet f be the solution to the current master problem and MMR\u2032(R) its objective value (i.e., minimax regret in the presence of the restricted adversary). The subproblem generates the maximally violated constraint relative to f . In other words, we compute MR(f ,R); its solution determines the witness points \u3008g, r\u3009 by removing restrictions\non the adversary. If MR(f ,R) = MMR\u2032(R) then the constraint for \u3008g, r\u3009 is satisfied at the current solution, and indeed all unexpressed constraints must be satisfied as well. The process then terminates with minimax optimal solution f . Otherwise, MR(f ,R) > MMR\u2032(R), implying that the constraint for \u3008g, r\u3009 is violated in the current relaxation (indeed, it is the maximally violated such constraint). So it is added to Gen and the process repeats.\nComputation ofMR(f ,R) is realized by the followingMIP, using value and Q-functions:1\nmaximize Q,V,I,r \u03b1 \u00b7 V \u2212 r \u00b7 f (9)\nsubject to: Qa = ra + \u03b3PaV \u2200 a \u2208 A\nV \u2265 Qa \u2200 a \u2208 A (10) V \u2264 (1 \u2212 Ia)Ma + Qa \u2200 a \u2208 A (11) Cr \u2264 d X\na\nIa = 1 (12)\nIa(s) \u2208 {0, 1} \u2200a, s (13) Ma = M \u22a4 \u2212 M\u22a5a\nHere I represents the adversary\u2019s policy, with Ia(s) denoting the probability of action a being taken at state s (constraints (12) and (13) restrict it to be deterministic). Constraints (10) and (11) ensure that the optimal value V (s) = Q(s, a) for a single action a. We ensure a tight M\u22a5a by setting M \u22a4 to be the optimal value functionV\u22a4 of the optimal policy with respect to the best setting of each individual reward point and M\u22a5a to be the Q-value Q \u22a5 a of the optimal policy with respect to the worst point-wise setting of rewards (the resulting rewards need not be feasible).\nThe subproblem does not directly produce a witness pair \u3008gi, ri\u3009 for the master constraint set; instead it provides ri and Vi. However, we do not need access to gi directly; the constraint can be posted using the reward function ri and the value \u03b1 \u00b7Vi, since \u03b1 \u00b7Vi = ri \u00b7gi (and gi is required to determine this adversarial value in the posted constraint).\nIn practice we have found that the iterative constraint generation converges quickly, with relatively few constraints required to determine minimax regret (see Sec. 5). However, the computational cost per iteration can be quite high. This is due exclusively to the subproblem optimization, which requires the solution of a MIP with a large number of integer variables, one per state-action pair. The master problem optimization, by contrast, is extremely effective (since it is basically a standard MDP linear program). This suggests examination of approximations to the subproblem, i.e., the computation of max regret MR(f ,R). This is also motivated by our focus on reward elicitation. We wish to use minimax regret to drive query selection: our aim is not\n1Specifying max regret in terms of visitation frequencies (i.e., the standard dual MDP formulation) gives rise to a non-convex quadratic program. Regret maximization does not lend itself to a natural, linear primal formulation.\nto compute minimax regret for its own sake, but to determine which state-action pairs should be queried, i.e., which have the potential to reduce minimax regret. The visitation frequencies used by our heuristics need not correspond to exact minimax optimal policy.\nWe have explored several promising alternatives, including an alternating optimization model that computes an adversarial policy (for a fixed reward) and an adversarial reward (for a fixed policy). This reduces the quadratic optimization for max regret to a sequence of LPs. An simpler approximation is explored here (which performs as well in practice): we solve the LP relaxation of the MIP by removing the integrality constraints (13) on the binary policy indicators. The value function V resulting from this relaxation does not accurately reflect the (now stochastic) adversarial policy: V may include a fraction of the big-M term due to constraint (10). However, the reward function r selected remains in the feasible set, and, empirically, the optimal value function for r yields a solution to the subproblem that is close to optimal.2 Since the reward is valid choice, this solution is guaranteed to be a lower bound on the solution to the subproblem. When this approximate subproblem solution is used in constraint generation, convergence is no longer guaranteed; however, the solution to the master problem represents a valid lower bound on minimax regret."}, {"heading": "4 Reward Elicitation", "text": "Reward elicitation and assessment can proceed in a variety of ways. Many different query forms can be adopted for user interaction. Similarly, observed user behavior can be used to induce constraints on the reward function under assumptions of user \u201coptimality\u201d [15]. In this work, we focus on simple bound queries, though our strategies can be adapted to more general query types. We discuss some of these below.3\nWe assume that R is given by upper and lower bounds on r(s, a) for each state-action pair. A bound query takes the form \u201cIs r(s, a) \u2265 b?\u201d where b lies between the upper and lower bound on r(s, a). While this appears to require a direct, quantitative assessment of value/reward by the user, it can be recast as a standard gamble [10], a device used in decision analysis to reduce this to preference query over two outcomes (one of which is stochastic). For simplicity, we express it in this bound form. Unlike reward queries [9], which require a direct assessment of r(s, a), bound queries require only a yes-no response and are less cognitively demanding. A response tightens either the upper or lower\n2Finding the optimal value function for r requires solving a standard MDP LP.\n3We allow reward queries about any state-action pair, in contrast to online RL formalisms, in which information can be gleaned only about the reward (and dynamics) at the current state. As such, we face no exploration-exploitation tradeoff.\nbound on r(s, a).4\nBound queries offer a natural starting point for the investigation of reward elicitation. Of course, many alternative query modes can be used, with the sequential nature of the MDP setting opening up choices that don\u2019t exist in oneshot settings. These include the direct comparison of policies; comparison of (full or partial) state-action trajectories or distributions over trajectories; and comparisons of outcomes in factored reward models. Trajectory comparisons can be facilitated by using counts of relevant (or rewardbearing) events as dictated by a factored reward model for example. These query forms should prove useful and even more cognitively penetrable. However, the principles and heuristics espoused below can be adapted to these settings.\nThere are many ways to select the point (s, a) at which to ask a bound query. We explore some simple myopic heuristic criteria that are very easy to compute, are based on criteria suggested in [6]. The first selection heuristic is called halve largest gap (HLG), which selects the point (s, a)with the largest gap between its upper and lower bound. Formally, we define the gap\u2206(s, a) and largest gap by:\n\u2206(s, a) = max r\u2032\u2208R\nr \u2032(s, a)\u2212min\nr\u2208R r(s, a)\nargmax a\u2217\u2208A,s\u2217\u2208S\n\u2206(s\u2217, a\u2217)\nThe second selection heuristic is the current solution (CS) strategy, and uses the visitation frequencies from the minimax optimal solution f or the adversarial witness g to weight each gap. Intuitively, if a query involves a reward parameter that influences the value of neither f nor g, minimax regret will not be reduced, and visitation frequencies quantify the degree of influence. Formally CS selects the point:\nargmax a\u2217\u2208A,s\u2217\u2208S\nmax{f(s\u2217, a\u2217)\u2206(s\u2217, a\u2217), g(s\u2217, a\u2217)\u2206(s\u2217, a\u2217)}.\nGiven the selected (s\u2217, a\u2217), bound b in the query is set to the midpoint of the interval for r(s\u2217, a\u2217). Thus either response will reduce the interval by half. It is easy to apply CS to the maximin criterion as well, using the visitation frequencies associated with the maximin policy."}, {"heading": "5 Experiments", "text": "We assess the general performance of our approach using a set of randomly generated MDPs and specific MDPs arising in an autonomic computing setting. We assess scalability of our procedures, as well as the effectiveness of minimax regret as a driver of elicitation.\nWe first consider randomly generated MDPs. We impose structure on the MDP by creating a semi-sparse transition function: for each (s, a)-pair, \u2308logn\u2309 reachable states are drawn uniformly and a Gaussian is used to generate transition probabilities. We use a uniform initial state distribution \u03b1 and discount factor \u03b3 = 0.95. The true reward\n4Indifference (e.g., \u201cI\u2019m not sure\u201d) can also be handled by constraining bounds to be within \u03b5 of the query point.\nis drawn uniformly from a fixed interval and uncertainty w.r.t. this true (but unknown) reward is created by bounding each (s, a)-pair independently with bounds drawn randomly: thus the set of feasible rewards forms a hyperrectangle. 5"}, {"heading": "5.1 Computational Efficiency", "text": "To measure the performance of minimax regret computation, we first examine the constraint generation procedure. Fig. 1 plots the regret gap between the master problem value and subproblem value at each iteration versus the time (in ms.) to reach that iteration. Results are shown for 20 randomly generated MDPs with ten states and five actions. Fig. 2 shows how minimax regret computation time increases with the size of theMDP (5 actions, varying number of states). Constraint generation using the MIP formulation scales super-linearly, hence computing minimax regret exactly is only feasible for small MDPs using this formulation; by comparison the linear relaxation is far more efficient.6 On the other hand, minimax regret computation has very favorable anytime behavior, as exhibited in Fig. 1. During constraint generation, the regret gap shrinks very quickly early on. If exact minimax regret is not needed, this property allows for fast approximation."}, {"heading": "5.2 Approximation Error", "text": "To evaluate the linear relaxation scheme for max regret, we generated randomMDPs, varying the number of states. Fig. 3 shows average relative error over 100 runs. The approximation performs well and, encouragingly, error does not increase with the size of the MDP. We also evaluate its impact on minimax regret when used to generate violated constraints. Fig. 3 also shows relative error for minimax regret to be small, well under 10% on average.\n5CPLEX 11 is used for all MIPS and LPs, and all code run on a PowerEdge 2950 server with dual quad-core Intel E5355 CPUs.\n6Of note, the computations shown here are using the initial reward uncertainty. As queries refine the reward polytope, regret computation becomes faster in general. This has positive impli-"}, {"heading": "5.3 Elicitation Effectiveness", "text": "We analyzed the effectiveness of our regret-based elicitation procedure by comparing it with the maximin criterion. We implemented a variation of the Double Oracle maximin algorithm developed by McMahan, Gordon & Blum [14]. The computation time for maximin is significantly less the that of minimax regret\u2014this is expected since maximin requires only the solution of a pair of linear programs.\nWe use both maximin and minimax regret to compute policies at each step of preference elicitation, and paired each with the current solution (CS) and halve largest gap (HLG) query strategies, giving four elicitation procedures: MMRHLG (policies are computed using regret, queries generated by HLG); MMR-CS (regret policies, CS queries); MM-HLG (maximin policies, HLG queries); and MM-CS (maximin policies, CS queries). We assess each procedure by measuring the quality of the policies produced after each query, using the following metrics: (a) its maximin value given the current (remaining) reward uncertainty; (b) its max regret given the current (remaining) reward uncertainty; and (c) its true regret (i.e., loss w.r.t. the optimal policy for the true reward function r, where r is used to generate query responses). Minimax regret is the most critical since it provides the strongest guarantees; but we compare to maximin value as well, since maximin policies are optimizing against a very different robustness measure. True\ncations for anytime computation.\nregret is not available in practice; but it gives an indication of how good the resulting policies actually are (as opposed to a worst-case bound).\nFig. 4 show the results of the comparison on each measure. MMR-CS performs extremely well on all measures. Somewhat surprisingly, it outperforms MM-CS and MMHLG w.r.t. maximin value (except at the very early stages). Even though the maximin procedures are optimizing maximin value, MMR-CS asks much more informative queries, allowing for a larger reduction in reward uncertainty at the most relevant state-action pairs. This ability ofMMR-CS to identify the highest impact reward points becomes clearer still when we examine how much reduction there is in reward intervals over the course of elicitation. Let \u03c7measure the sum of the length of the reward intervals. At the end of elicitation, MMR-HLG reduces \u03c7 to 15.6% of its original value (averaged over the 20 MDPs), while MMR-CS only reduces \u03c7 to 67.8 % of its original value. MMR-CS is effectively eliminating regret while leaving a large amount of uncertainty. Fig. 5 illustrates this using a histogram of the number of queries asked by MMR-CS about each of the 1000 possible state-action pairs.7 We see that MMRCS asks no queries about the majority of state-action pairs, and asks quite a few queries (up to eight) about a small number of \u201chigh impact\u201d pairs.\nFig. 4(b) shows that MMR-CS is able to reduce regret to zero (i.e., find an optimal policy) after less than 100 queries on average. Recall that the MDP has 50 reward parameters (state-action pairs), so on average, less than two queries per parameter are required to find a provably optimal policy. The minimax regret policies also outperform the maximin policies by a wide margin with respect to true regret (Fig. 4(c)). With the CS heuristic, a near-optimal policy is found after fewer than 50 queries (less than one query per parameter), though to prove that the policy is near-optimal requires further queries (to reduce minimax regret).\nIt is worth noting that during preference elicitation, HLG does not require that minimax regret actually be computed.\n720 MDPs with 10 states, 5 actions each.\nMinimax regret is only necessary to assess when to stop the elicitation process (i.e., to determine if minimax regret has dropped to an acceptable level). One possible modification to reduce the time between queries is to only compute minimax regret after every k queries. Of course, the HLG strategy will lead to a slower reduction in true regret and minimax regret as shown in Figs. 4(b) and 4(c).\nTo further evaluate our approach we elicit the reward function for an autonomic computing scenario [4] in which we must allocate computing or storage resources to application servers as their client demands change over time. We assume k application server elements and N units of resource available to be assigned to the servers (plus a \u201czero resource\u201d). An allocation n = \u3008n1 . . . nk\u3009 must satisfy\u2211k i ni < N . There are D demand levels at which each server can operate, reflecting client demands. A demand state d = \u3008d1 . . . dk\u3009 specifies the current demand for each server. A state of the MDP comprises the current resource allocation and the current demand state: s = \u3008n,d\u3009. Actions are new allocations m = \u3008m1 . . .mk\u3009 of the N resources to the k servers. Reward r(n,d,m) = u(n,d) \u2212 c(n,d,m) decomposes as follows. Utility u(n,d) is the sum of server utilities ui(ni, di). The MDP is initially specified with strict uncertainty over the utilities ui however, we assume that each utility function ui is monotonic non-decreasing in demand and resource level. The cost c(n,d,m) is the sum of the costs of taking away one unit of resource from each server at any stage. Uncertainty in demand is exogenous and the action in the current state uniquely determines the allocation in the next state. Thus the transition function is composed of k Markov chains Pr(d\u2032i | di), i \u2264 k. Reward specification in this context is inherently distributed and quite difficult: the local utility function ui for server i has no convenient closed form. Server i can respond only to queries about the utility it gets from a specific resource allocation level, and this requires intensive optimization and simulation on the part of the server [4]; hence minimizing the number of such queries is critical.\nWe constructed a small instance of the autonomic computing scenario with 2 servers, 3 demand levels and 3 (indi-\nvisible) units of resource. The combined state space of both servers includes 32 demand levels and 10 possible allocations of resources leading to 90 states and 10 actions. We modeled the uncertainty over rewards using a hyperrectangle as with the random MDPs. We compared elicitation approaches as above, this time using the linear relaxation to compute minimax regret (each minimax computation takes under 3s.). Fig. 6 shows that MMR-CS again outperforms the maximin criterion on each measure. Minimax regret and true regret fall to almost zero after 200 queries. Recall that the autonomic MDP had 900 stateaction pairs\u2014the additional problem structure results in fewer than 0.25 queries being asked for each state-action pair. In fact, on average MMR-CS only asks about 106.5 distinct state-action pairs, only examining 12% of the reward space. By comparison, the queries chosen by the MM-CS strategy cover just over 68% of the reward space. As with randomMDPs, minimax regret quickly reduces regret because it focuses queries on the \u201chigh impact\u201d stateaction pairs.\nOverall, our regret-based approach is quite appealing from the perspective of reward elicitation. While the regret computation is more computationally intensive than other criteria, it provides arguably much more natural decisions in the face of reward uncertainty. More importantly, from the perspective of elicitation, it is much more attractive than maximin w.r.t. the number of queries required to produce high-quality policies. As long as interaction time (time between queries) remains reasonable, reducing user burden (or other computational costs required to answer queries) is our primary goal."}, {"heading": "6 Conclusions & Future Work", "text": "We have developed an approach to reward elicitation in MDPs that eases the burden of reward function elicitation. Minimax regret not only offers robust policies in the face of reward uncertainty, but we\u2019ve shown it also allows one to focus elicitation attention on the most important aspects of the reward function. While the computational costs are significant, it is an extremely effective driver of elicitation,\nthus reducing the (more important) cognitive or computational cost of reward determination. Furthermore, it lends itself to anytime approximation.\nThe somewhat preliminary nature of this work leaves many interesting directions for future research. Perhaps most interesting is the development of more informative and intuitive queries that capture the sequential nature of the elicitation problem. Direct comparison of policies allows one to distinguish value from reward, but are cognitively demanding. Trajectory comparison similar distinguishes value, but may contain irrelevant detail. However, trajectory summaries (e.g., counts of relevant reward bearing events) may be more perspicuous, and could be generated to reflect expected \u201cevent counts\u201d given a policy. Other forms of queries should also prove valuable, but all exploit the basic idea embodied by minimax regret and the current solution heuristic. Another direction for improving elicitation is to incorporate implicit information in a manner similar to policy teaching [20]. Inverse RL [15] can be also used to translate observed behavior into constraints on reward. Some Bayesian models [6, 8] allow noisy query responses and adding this to our regret model is another important direction. Two approaches include: approximate indifference constraints and regret-based sensitivity analysis. The efficiency of the minimax regret computation remains an important research topic. We are exploring the use of dynamic programming to generate linear representations of the best policies over all regions of reward space (much like POMDPs) which can greatly assist max regret computation. We are also exploring techniques that exploit factored MDP structure using LP approaches [12]."}], "references": [{"title": "Solving uncertain Markov decision problems", "author": ["J. Bagnell", "A. Ng", "J. Schneider"], "venue": "Tech Report,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Partitioning procedures for solving mixedvariables programming problems", "author": ["J. Benders"], "venue": "Numerische Math.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1962}, {"title": "A POMDP formulation of preference elicitation problems", "author": ["C. Boutilier"], "venue": "AAAI-02, pp.239\u2013246, Edmonton", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Cooperative negotiation in autonomic systems using  incremental utility elicitation UAI-03", "author": ["C. Boutilier", "R. Das", "J.O. Kephart", "G. Tesauro", "W.E. Walsh"], "venue": "pp.89\u201397, Acapulco", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Decision theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "J. Artif. Intel. Res., 11:1\u201394", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Constraint-based optimization and utility elicitation using the minimax decision criterion", "author": ["C. Boutilier", "R. Patrascu", "P. Poupart", "D. Schuurmans"], "venue": "Artificial Intelligence, 170:686\u2013713", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Eliciting bid taker non-price preferences in (combinatorial) auctions", "author": ["C. Boutilier", "T. Sandholm", "R. Shields"], "venue": "AAAI-04, pp.204\u2013211, San Jose, CA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["U. Chajewska", "D. Koller", "R. Parr"], "venue": "AAAI-00, pp.363\u2013 369, Austin, TX", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Percentile optimization in uncertain markov decision processes with application to efficient exploration", "author": ["E. Delage", "S. Mannor"], "venue": "ICML-07, pp.225\u2013232, Corvalis, OR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Decision Theory: An Introduction to the Mathematics of Rationality", "author": ["S. French"], "venue": "Halsted Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning the structure of dynamic probabilistic networks", "author": ["N. Friedman", "K.Murphy", "S. Russell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Efficient solution algorithms for factored mdps", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "J. Artif. Intel. Res. 19:399-468", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust dynamic programming", "author": ["G. Iyengar"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Planning in the presence of cost functions controlled by an adversary", "author": ["H. McMahan", "G .Gordon", "A. Blum"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Ng", "S. Russell"], "venue": "ICML-00, pp.663\u2013670, Stanford, CA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Robustness in Markov decision problems with uncertain transition matrices", "author": ["A. Nilim", "L. El Ghaoui"], "venue": "NIPS-03, Vancouver", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": "Wiley, New York", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "The Foundations of Statistics", "author": ["L. Savage"], "venue": "Wiley", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1954}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Value-based policy teaching with active indirect elicitation", "author": ["H. Zhang", "D. Parkes"], "venue": "AAAI-08, pp.208\u2013214", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "For this reason, much work has been devoted to learning the dynamics of stochastic systems from transition data, both in offline [11] and online (i.", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": ", reinforcement learning) settings [19].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "As has been well-recognized in decision analysis, people find it extremely difficult to quantify their strength of preferences precisely using utility functions (and, by extension, reward functions) [10].", "startOffset": 199, "endOffset": 203}, {"referenceID": 4, "context": "Second, the requirement to assess rewards and costs for all states and actions imposes an additional burden (one that can be somewhat alleviated by the use of multiattributemodels in factoredMDPs [5]).", "startOffset": 196, "endOffset": 199}, {"referenceID": 5, "context": "Recent research in preference elicitation for non-sequential decision problems exploits the fact that optimal or nearoptimal decisions can often be made with relatively imprecise specification of a utility function [6, 8].", "startOffset": 215, "endOffset": 221}, {"referenceID": 7, "context": "Recent research in preference elicitation for non-sequential decision problems exploits the fact that optimal or nearoptimal decisions can often be made with relatively imprecise specification of a utility function [6, 8].", "startOffset": 215, "endOffset": 221}, {"referenceID": 5, "context": "Specifically, we adopt the minimax regret decision criterion [6, 18] and develop a formulation for MDPs: intuitively, this determines a policy that has minimum regret, or loss w.", "startOffset": 61, "endOffset": 68}, {"referenceID": 17, "context": "Specifically, we adopt the minimax regret decision criterion [6, 18] and develop a formulation for MDPs: intuitively, this determines a policy that has minimum regret, or loss w.", "startOffset": 61, "endOffset": 68}, {"referenceID": 0, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 12, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 13, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 15, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 16, "context": ", those satisfying [17]:", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "where f = sup f rf [17].", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "These could reflect: prior bounds specified by a user or domain expert; constraints that emerge from an elicitation process (as discussed below); or constraints that arise from observations of user behavior (as in inverse RL [15]).", "startOffset": 225, "endOffset": 229}, {"referenceID": 17, "context": "We adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7].", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "We adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7].", "startOffset": 163, "endOffset": 169}, {"referenceID": 6, "context": "We adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Minimax regret has a variety of desirable properties relative to other robust decision criteria [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Compared to Bayesian methods that compute expected value using a prior overR [3, 8], minimax regret provides worst-case bounds on loss.", "startOffset": 77, "endOffset": 83}, {"referenceID": 7, "context": "Compared to Bayesian methods that compute expected value using a prior overR [3, 8], minimax regret provides worst-case bounds on loss.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "Finally, it has been shown to be a very effective criterion for driving elicitation in one-shot problems [6, 7].", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "Finally, it has been shown to be a very effective criterion for driving elicitation in one-shot problems [6, 7].", "startOffset": 105, "endOffset": 111}, {"referenceID": 0, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 12, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 13, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 15, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 0, "context": "Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16].", "startOffset": 246, "endOffset": 257}, {"referenceID": 12, "context": "Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16].", "startOffset": 246, "endOffset": 257}, {"referenceID": 15, "context": "Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16].", "startOffset": 246, "endOffset": 257}, {"referenceID": 13, "context": "McMahan, Gordon, and Blum [14] develop a linear programming approach to efficiently compute the maximin value of an MDP (we empirically compare this approach to ours below).", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Delage and Mannor [9] address the problem of uncertainty over reward functions (and transition functions) in the presence of prior information, using a percentile criterion, which can be somewhat less pessimistic than maximin.", "startOffset": 18, "endOffset": 21}, {"referenceID": 19, "context": "Zhang and Parkes ([20]) also adopt maximin in a model that assumes an inverse reinforcement learning setting for policy teaching.", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "Following the formulations for non-sequential problems developed in [6, 7], we instead formulate the optimization using a series of linear (LPs) and mixed integer programs (MIPs) that enforce a consistent choice of reward across time.", "startOffset": 68, "endOffset": 74}, {"referenceID": 6, "context": "Following the formulations for non-sequential problems developed in [6, 7], we instead formulate the optimization using a series of linear (LPs) and mixed integer programs (MIPs) that enforce a consistent choice of reward across time.", "startOffset": 68, "endOffset": 74}, {"referenceID": 1, "context": "However, vertex enumeration is not feasible; so we apply Benders\u2019 decomposition [2] to iteratively generate constraints.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "Similarly, observed user behavior can be used to induce constraints on the reward function under assumptions of user \u201coptimality\u201d [15].", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "While this appears to require a direct, quantitative assessment of value/reward by the user, it can be recast as a standard gamble [10], a device used in decision analysis to reduce this to preference query over two outcomes (one of which is stochastic).", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "Unlike reward queries [9], which require a direct assessment of r(s, a), bound queries require only a yes-no response and are less cognitively demanding.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "We explore some simple myopic heuristic criteria that are very easy to compute, are based on criteria suggested in [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 13, "context": "We implemented a variation of the Double Oracle maximin algorithm developed by McMahan, Gordon & Blum [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "To further evaluate our approach we elicit the reward function for an autonomic computing scenario [4] in which we must allocate computing or storage resources to application servers as their client demands change over time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "Server i can respond only to queries about the utility it gets from a specific resource allocation level, and this requires intensive optimization and simulation on the part of the server [4]; hence minimizing the number of such queries is critical.", "startOffset": 188, "endOffset": 191}, {"referenceID": 19, "context": "Another direction for improving elicitation is to incorporate implicit information in a manner similar to policy teaching [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Inverse RL [15] can be also used to translate observed behavior into constraints on reward.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Some Bayesian models [6, 8] allow noisy query responses and adding this to our regret model is another important direction.", "startOffset": 21, "endOffset": 27}, {"referenceID": 7, "context": "Some Bayesian models [6, 8] allow noisy query responses and adding this to our regret model is another important direction.", "startOffset": 21, "endOffset": 27}, {"referenceID": 11, "context": "We are also exploring techniques that exploit factored MDP structure using LP approaches [12].", "startOffset": 89, "endOffset": 93}], "year": 2009, "abstractText": "The specification of a Markov decision process (MDP) can be difficult. Reward function specification is especially problematic; in practice, it is often cognitively complex and time-consuming for users to precisely specify rewards. This work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced. We first discuss how robust policies can be computed for MDPs given only partial reward information using the minimax regret criterion. We then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret-reduction as a means for choosing suitable queries. Empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function.", "creator": "dvips(k) 5.96 Copyright 2005 Radical Eye Software"}}}