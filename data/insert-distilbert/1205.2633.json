{"id": "1205.2633", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts", "abstract": "we consider the task of obtaining the maximum a posteriori estimate of discrete pairwise random fields with arbitrary unary potentials uniformly and equal semimetric order pairwise potentials. for this problem, we propose an accurate hierarchical move estimate making strategy where each move is computed efficiently simply by solving an adaptive st - mincut problem. unlike previous move making approaches, e. g. the widely used a - expansion algorithm, our method obtains the guarantees of the standard linear programming ( lp ) relaxation for the important special case of metric labeling. unlike the existing lp optimization relaxation solvers, e. g. interior - point algorithms or tree - reweighted symmetric message passing, our method is significantly faster as it uses only the efficient st - mincut algorithm in enabling its design. using both synthetic and real data experiments, we show that our technique outperforms somewhat several commonly used algorithms.", "histories": [["v1", "Wed, 9 May 2012 15:47:50 GMT  (403kb)", "http://arxiv.org/abs/1205.2633v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI cs.DS", "authors": ["m pawan kumar", "daphne koller"], "accepted": false, "id": "1205.2633"}, "pdf": {"name": "1205.2633.pdf", "metadata": {"source": "CRF", "title": "MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts", "authors": ["M. Pawan Kumar", "Daphne Koller"], "emails": ["pawan@cs.stanford.edu", "koller@cs.stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "Markov random fields (MRFs) offer an expressive and intuitive framework for several important problems in artificial intelligence and machine learning. Given a set of random variables along with a neighborhood relationship defined over them, an MRF offers a concise representation of the probability of each labeling (i.e. a particular assignment of labels to the variables) in terms of potentials defined over the cliques of random variables. Due to the central role of MRFs in various applications, algorithms that perform efficient and accurate inference on them are highly desirable. One important and well-studied class of inference, called maximum a posteriori (MAP) estimation, seeks the labeling with the maximum probability.\nWe consider a special case of MAP estimation, known as semi-metric labeling [4], where (i) the size of the maximal clique is 2 (a pairwise MRF); and (ii) the pairwise potentials are defined by a semi-metric distance function over labels. Although these may seem like very restrictive assumptions, several problems in computer vision and related areas can\nbe expressed using semi-metric labeling, from low level tasks like image denoising and stereo reconstruction [30] to high level tasks like pose estimation [9] and scene segmentation [27]. Hence, the semi-metric labeling problem merits special attention.\nWe describe a novel algorithm for semi-metric labeling which approximates a given semi-metric distance function using a mixture of r-hierarchically well-separated tree (rHST) metrics [1]. The r-HST metrics form an amenable class of distance functions which admit elegant divide-andconquer approaches for several problems [1, 8]. In our case, they not only result in easier-to-solve instances of MAP estimation, they also provide an accurate approximation of the original problem. Given a mixture of r-HSTs, we reformulate semi-metric labeling using a set of r-HST metric labeling problems (i.e. MAP estimation for r-HST metric pairwise potentials), where each problem is specified by one component of the mixture. We show how each resulting r-HST metric labeling problem can be solved accurately using an iterative procedure that only employs the efficient st-MINCUT algorithm [3] in its design. Unlike previous st-MINCUT based approaches, our method provides the best known approximation bound for the important special case of metric labeling (i.e. when the pairwise potentials are defined by a metric distance function). In practice, our technique outperforms several state of the art algorithms on both synthetic and real data experiments."}, {"heading": "2 Related Work", "text": "The most commonly used algorithms for semi-metric labeling can be broadly divided into two categories: message passing and move making. Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36]. Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33]. Although message passing algorithms provide accurate MAP estimates, they can be computationally expensive in certain cases [30].\nMove making approaches refer to a large class of iterative algorithms which move from one labeling to the other\nwhile ensuring that the probability of the labeling never decreases. The move space, i.e. the search space for the new labeling, is restricted to a subspace of the original search space that can be explored efficiently. Typically, the move space is explored using the st-MINCUT algorithm, e.g. in \u03b1\u03b2-swap [4] and \u03b1-expansion [5]. However, recently researchers have also used more sophisticated algorithms such as quadratic pseudo-boolean optimization [2] (e.g. see [13, 21, 35]). Move making algorithms are generally preferred in applications which involve a large number of random variables, e.g. on image-sized MRFs in computer vision, due to their efficiency."}, {"heading": "3 Preliminaries", "text": "Consider an MRF defined over a set of random variables V = {v1, v2, \u00b7 \u00b7 \u00b7 , vN}, each of which can take a value from a discrete label set L = {l1, l2, \u00b7 \u00b7 \u00b7 , lH}. Furthermore, let E define the neighborhood such that va and vb are neighbors if and only if (va, vb) \u2208 E . A labeling of the MRF is a function f : {1, 2, \u00b7 \u00b7 \u00b7 , N} \u2192 {1, 2, \u00b7 \u00b7 \u00b7 , H} such that variable va takes label lf(a). Associated with each labeling is its probability Pr(f |\u03b8) = exp(\u2212Q(f ;\u03b8))/Z , where Z is the partition function and Q(\u00b7;\u03b8) is the Gibbs energy:\nQ(f ;\u03b8) = \u2211\nva\u2208V\n\u03b8a(f(a))+ \u2211\n(va,vb)\u2208E\n\u03b8ab(f(a), f(b)). (1)\nHere \u03b8a(f(a)) and \u03b8ab(f(a), f(b)) denote unary and pairwise potentials respectively. For semi-metric labeling, the pairwise potentials are of the form \u03b8ab(f(a), f(b)) = wabd(f(a), f(b)), where wab \u2265 0 and d(\u00b7, \u00b7) is a semimetric distance function. Recall that d(\u00b7, \u00b7) is semi-metric if and only if: (i) d(i, i) = 0, \u2200i; and (ii) d(i, j) = d(j, i) > 0, \u2200i 6= j. Examples of commonly used semimetric distance measures include the truncated linear function, d(i, j) = min{|i \u2212 j|,M} where the truncation factor M \u2265 0, the truncated quadratic function, d(i, j) = min{(i \u2212 j)2,M}, and the uniform metric (a special case of truncated linear/quadratic function with M = 1). Within this setting, the problem of MAP estimation is formally specified as: f\u2217 = argminf Q(f ;\u03b8).\n4 The r-HST Metric Labeling Problem As mentioned earlier, there are two key ingredients to our MAP estimation algorithm: (i) approximating a given semimetric by a mixture of r-HST metrics; and (ii) solving each resulting r-HST metric labeling problem. We begin by defining r-HST metrics and designing an efficient move making algorithm for the corresponding labeling problem. The next section describes a simple yet accurate procedure for approximating semi-metrics.\n4.1 The r-HST Metric An r-HST metric [1] dt(\u00b7, \u00b7) is specified by a rooted tree whose edge lengths are non-negative and satisfy the following properties: (i) the edge lengths from any node to all of its children are the same; and (ii) the edge lengths\nalong any path from the root to a leaf decrease by a factor of at least r > 1. Given such a tree, known as r-HST, the distance dt(i, j) is the sum of the edge lengths on the unique path between them. Note that, as the name suggests, an r-HST specifies a metric distance. In other words, it is a semi-metric distance function that satisfies the triangular inequality: d(i, j) \u2212 d(j, k) \u2264 d(i, k), \u2200i, j, k. In this paper, we consider only those r-HSTs where all the labels in the set L are at the leaves of the r-HST. As observed in several earlier works [1, 6, 8], r-HSTs satisfying this assumption are sufficient to provide an accurate approximation of a given semi-metric distance function. Fig. 1 shows an example r-HST over H = 6 labels with r = 2."}, {"heading": "4.2 The Move Making Algorithm", "text": "For a given r-HST metric dt(\u00b7, \u00b7), we define an MRF parameterized by \u03b8t with arbitrary unary potentials \u03b8ta(i) and pairwise potentials of the form \u03b8tab(i, j) = wabd\nt(i, j). We show how to obtain an accurate MAP estimate for the parameter \u03b8t, known as the r-HST metric labeling problem, using a novel approach based on the st-MINCUT algorithm. Our approach is a divide-and-conquer method consisting of two steps: (i) replace the original problem by a series of subproblems that are easier to solve; and (ii) combine the solutions of the subproblems to obtain an accurate solution of the original problem. Each of the subproblems is specified by a node in the given r-HST, and their solutions are combined using the standard \u03b1-expansion algorithm [5].\nIn more detail, consider a node p of the given r-HST. We say that a label li belongs to the node p (denoted by i \u2208 p) if and only if it is a leaf node in the subtree rooted at p. Let Lp denote the set of labels that belong to p, i.e.Lp = {i|i \u2208 p}. The subproblem defined at node p is to find the labeling fp of the random variables V that minimizes the energy under the constraint that each variable va \u2208 V takes a label from the set Lp. Note that if p is the root node of the given r-HST, then the subproblem is the same as the original problem. On the other hand, if p is the leaf node, then the solution to the subproblem is trivial, i.e. fp(a) = p for all va \u2208 V . In fact, as one moves from the root towards the leaves of the r-HST, the label set of the subproblem keeps reducing in size thereby making the subproblems easier to solve. This observation suggests the following hierarchical approach: solve the easier subproblems at level m + 1 of the r-HST and use their labelings to solve the subproblem defined by their parent nodes at level m.\nIt remains to be seen how exactly a subproblem at node p can benefit from the labelings of its child nodes. To answer this we consider the stage of the above hierarchical approach where we have to solve the subproblem defined at node p, having already obtained the labelings of the subproblems associated with the children of p. We denote these labelings by f1, \u00b7 \u00b7 \u00b7 , fC where C is the number of child nodes of p. To find the labeling fp for the subproblem at p efficiently, we restrict the label of each variable va to be one of the C labels specified by the child nodes. In other words, fp(a) = fi(a) where i \u2208 {1, \u00b7 \u00b7 \u00b7 , C} is the index of the child node from which va takes its label. Note that different variables can take labels from different child nodes. In order to find the indices of the child nodes for all variables, we define a parameter \u03b8p such that the corresponding unary and pairwise potentials are given by\n\u03b8pa(i) = \u03b8 t a(fi(a)), \u03b8 p ab(i, j) = wabd t(fi(a), fj(b)), \u2200(va, vb) \u2208 E , i, j \u2208 {1, \u00b7 \u00b7 \u00b7 , C}. (2)\nWe obtain an approximate MAP estimate f \u2032 for the parameter \u03b8p using \u03b1-expansion (see [18] for details). Using the properties of r-HST metrics, it follows that each move of \u03b1-expansion results in a submodular problem that can be solved exactly. The labeling f \u2032 provides the required indices of the child nodes to obtain the labeling fp as\nfp(a) = fi(a) where i = f \u2032(a), \u2200va \u2208 V . (3) The hierarchical approach for solving the r-HST metric labeling problem terminates when the subproblem corresponding to the root node is solved. Our method is not only easy to implement and effective in practice, it also provides the approximation bounds of the LP relaxation [7, 11]. Specifically, the following property holds true:\nTheorem 4.1 [18]: For r-HST metric labeling we obtain an approximation bound of O(1)."}, {"heading": "5 The Semi-Metric Labeling Problem", "text": "Similar to MAP estimation, several problems specified on r-HST metrics are well-known to be amenable to efficient divide-and-conquer approaches [1, 8]. However, their use in the AI community has been very limited thus far. The main reason for this would appear to be their restrictive form which may not offer an accurate model for real-world applications. A natural way to address this deficiency is to use a mixture of r-HST metrics instead of a single r-HST.\n5.1 Learning a Mixture of r-HSTs. Given a distance function d(\u00b7, \u00b7), we would like to learn a set of r-HST metrics D = {dt(\u00b7, \u00b7), t = 1, \u00b7 \u00b7 \u00b7 , T } along with a probability distribution \u03c1 on them such that the distortion is minimized, that is\n(D\u2217,\u03c1\u2217) = argmin D,\u03c1\n(\nmax i6=j\n\u2211\nt \u03c1 tdt(i, j)\nd(i, j)\n)\n. (4)\nWhen the distance function is a metric, Fakcharoenphol et al. [8] provide a simple yet accurate randomized algorithm for sampling r-HST metrics. Below, we describe their\nmethod for r = 2 while noting that it can be easily extended for any value of r.\nIt is helpful to think of each level of an r-HST as a clustering of labels such that a node p defines a cluster of labels Lp = {i|i \u2208 p} (i.e. i is a leaf node of the subtree rooted at p). In other words, an r-HST defines a hierarchical clustering of labels. Let the clustering at level m be denoted by Cm. The root node denotes the trivial clustering which consists of all the labels C1 = {1, \u00b7 \u00b7 \u00b7 , H}. Given the clusters Cm\u22121, Cm is obtained by further clustering the labels {j|j \u2208 p} for each node p \u2208 Cm\u22121.\nWithout loss of generality, we assume that the diameter \u2206 = maxi6=j d(i, j) = 2\n\u03b4 for an integer \u03b4 \u2265 1 and mini6=j d(i, j) > 1. Due to the above assumptions, the rHST would consist of at most \u03b4 levels. The algorithm is initialized by: (i) picking a random permutation \u03c0 of the label indices {1, 2, \u00b7 \u00b7 \u00b7 , H}, which defines a priority ordering on the cluster centers; and (ii) choosing a value of \u03b2 \u2208 [1, 2] from the distribution Pr(x) = 1/(x log 2). Note that both the permutation \u03c0 and value of \u03b2 are fixed throughout the process, i.e. they are selected once before running the clustering algorithm for all levels. Given Cm\u22121, the clustering at level m is obtained as follows:\n\u2022 Consider a node p \u2208 Cm\u22121. Define \u03b2m = 2\u03b4\u2212m\u03b2.\n\u2022 For a label i \u2208 p, find the first label j according to the permutation \u03c0 such that d(i, j) \u2264 \u03b2m.\n\u2022 Assign the label i to the cluster centered at j. \u2022 Repeat for all labels i \u2208 p and nodes p \u2208 Cm\u22121.\nThe edge length ep from a node p to each of its children is given by \u2206p/2 where \u2206p denotes the diameter of the cluster of labels Lp specified by p. Fakcharoenphol et al. [8] showed that \u2206p reduces by at least a factor of r = 2 for metric distances, thereby providing us with an r-HST.\nImportantly, the method of [8] can also be applied for approximating semi-metric distance functions. However, as the triangular inequality is not satisfied, the resulting tree will not be an r-HST. Nonetheless, the tree obtained using this method would provide a metric distance function which can then be approximated to r-HST metrics by applying the above procedure again. The only question that remains is the number of r-HSTs T to be employed. In order to answer this question, we note that [8] also provided a deterministic version of the above algorithm for solving a related problem that we call the dual procedure (DP):\nDP(y): min dt(\u00b7,\u00b7)\n\u2211\ni,j\nyijd t(i, j), s.t. dt(i, j) \u2265 d(i, j), (5)\nfor some values of yij \u2265 0. In other words, DP provides one r-HST that minimizes the non-negatively weighted sum of distances that dominate the original distance d(\u00b7, \u00b7) (i.e. dt(i, j) \u2265 d(i, j), for all i, j). Note that each r-HST sampled from the randomized procedure described above dominates d(\u00b7, \u00b7). Briefly, DP works by derandomizing the\nabove procedure using conditional expectation. In other words, the elements of the permutation \u03c0 are obtained sequentially by computing the expectation of equation (5) given the previously selected elements of the permutation. Using DP, Charikar et al. [6] provided an iterative algorithm to obtain a small mixture of r-HST metrics (with O(H logH) r-HSTs). The algorithm initializes the mixture to one r-HST. In our implementation, we found the r-HST obtained by solving the DP (5) for the values yij = 1 for all i, j to be a good initialization. Let (D,\u03c1) denote the mixture after n iterations which consists of n r-HSTs. The (n+ 1)st r-HST is obtained by defining\nyij =\n{\n1 d(i,j) exp\n( P\nt \u03c1tdt(i,j) \u03bbd(i,j)\n)\nif i 6= j,\n0 otherwise, (6)\nand solving DP(y). Here \u03bb > 0 is the user defined learning rate. Note that in the above values of yij , the pairs of labels li and lj which result in a bigger distortion are given more weight while solving the DP. The probability distribution over the n+1 r-HSTs is updated to ((1\u2212\u03bb)\u03c1;\u03bb) where (; ) denotes vector concatenation. Although more sophisticated clustering algorithms may be used, the above method is appealing due to its ease of implementation. Furthermore, it also provides an accurate approximation as evidenced by the following result from [8] and its simple extension.\nTheorem 5.1 [8]: When d(\u00b7, \u00b7) is a metric distance function, the above approach provides a mixture of r-HST metrics with distortion of O(logH).\nTheorem 5.2: Let d(\u00b7, \u00b7) be a semi-metric which satisfies the following relaxed version of triangular inequality:\nd(i, j)\u2212 d(j, k) \u2264 \u03b3d(i, k), \u2200i, j, k, (7)\nfor some value of \u03b3 \u2265 1. The above approach provides a mixture of r-HST metrics with a distortion of O((\u03b3 logH)2) with respect to d(\u00b7, \u00b7). Note that any distance function defined over a finite number of labels will admit a finite \u03b3."}, {"heading": "5.2 Approximating Semi-Metric Labeling", "text": "Once the mixture of r-HSTs is learnt, the original semimetric labeling problem parameterized by \u03b8 can be approximated by a set of r-HST metric labeling problems specified by parameters \u03b8t, t = 1, \u00b7 \u00b7 \u00b7 , T , where\n\u03b8ta(i) = \u03b8a(i), \u03b8 t ab(i, j) = wabd t(i, j). (8)\nAs shown in the previous section, r-HST metric labeling can be solved efficiently and accurately using an stMINCUT based approach. Hence, in order to solve semimetric labeling, we solve the set of r-HST metric labeling problems to obtain the labelings f t. We then combine these labelings to obtain the final labeling f , using the same approach as the one used to combine the labelings of the children of node p of the r-HST (see \u00a7 4.2). Note that, unlike the\nproblem of combining labelings of child nodes, in this case the moves of \u03b1-expansion are no longer necessarily submodular. In other words, we are not guaranteed to obtain the optimal move at each iteration. In order to overcome this problem, we solve the \u03b1-expansion procedure by using the primal dual scheme of [16]. This has two advantages: (i) it reduces the run-time of \u03b1-expansion; and (ii) it handles non-submodular moves by truncating the edges with negative capacities in the st-MINCUT graph to 0. At each iteration of \u03b1-expansion, we move to a new labeling only if it decreases the energy. Otherwise we retain the old labeling and repeat the procedure until we can no longer decrease the energy for any iteration of \u03b1-expansion. We initialize the labeling by the lowest energy labeling amongst the set {f t, t = 1, \u00b7 \u00b7 \u00b7 , T }. The \u03b1-expansion procedure described above guarantees that the energy is not increased at any iteration. In other words, the energy of the labeling obtained by our approach is bounded from above by the energy of the best labeling provided by solving the set of r-HST metric labeling problems. Using this observation along with Theorems 5.1 and 5.2 allows us to prove the following approximation bounds for our overall approach.\nTheorem 5.3 [18]: For the metric and semi-metric labeling problems, we obtain an approximation bound of O(logH) and O((\u03b3 logH)2) respectively.\nIn practice, when solving a subproblem at node p of an r-HST, we use the given distance function d(\u00b7, \u00b7) to specify the pairwise potentials of \u03b8p instead of r-HST metric dt(\u00b7, \u00b7). This tends to improve the quality of the labelings whilst retaining the approximation bound. Specifically,\nObservation 5.4 [18]: Theorem 5.3 also holds true if the rHST metric dt(\u00b7, \u00b7) is replaced by the given distance d(\u00b7, \u00b7) in equation (2) for all subproblems defined by the r-HSTs.\nNote that our algorithm provides the guarantees of the LP relaxation for the metric labeling problem. Together with the results for truncated convex models [5, 20], this implies that there exist moving making algorithms which match all known LP relaxation guarantees when the number of labels is smaller than the number of variables (i.e. H < N ). Although the above theorem shows that our approach provides a tight approximation, we can further improve its accuracy by using a hard EM strategy described below."}, {"heading": "5.3 Refining the Labeling", "text": "For a given semi-metric labeling problem, consider the labeling f obtained using the method described above. The energy defined by f is given by\nQ(f ;\u03b8) = \u2211\nva\u2208V\n\u03b8a(f(a)) + \u2211\n(va,vb)\u2208E\nwabd(f(a), f(b))\n(9) We define a set of non-negative weights y as\nyij = \u2211\n(va,vb)\u2208E,f(a)=i,f(b)=j\nwab, (10)\ni.e. yij is the contribution of labels li and lj to the energy (9). Specifically, using y, the energy of the labeling f can be rewritten as\nQ(f ;\u03b8) = \u2211\nva\u2208V\n\u03b8a(f(a)) + \u2211\ni,j\nyijd(i, j). (11)\nWe obtain an r-HST metric dt(\u00b7, \u00b7) by solving DP (5) for the values of y defined above. The metric dt(\u00b7, \u00b7) provides an MRF parameterized by \u03b8t as defined in equation (8). Since the metric dt(\u00b7, \u00b7) dominates the given distance d(\u00b7, \u00b7) it follows that \u2211\ni,j\nyijd(i, j) \u2264 \u2211\ni,j\nyijd t(i, j) \u21d2 Q(f ;\u03b8) \u2264 Q(f ;\u03b8t). (12)\nNow consider the case when the above inequality holds with an equality. In other words, DP provides an r-HST metric which exactly models the weighted sum of distances where the weights are specified by y. We can now solve the r-HST metric labeling problem corresponding to \u03b8t in order to obtain a new labeling f \u2032. If the labeling f \u2032 is such that Q(f \u2032;\u03b8t) \u2264 Q(f ;\u03b8t) then we are guaranteed not to increase the energy of the solution by moving from labeling f to labeling f \u2032 since\nQ(f \u2032;\u03b8t) \u2264 Q(f ;\u03b8t) = Q(f ;\u03b8) \u21d2 Q(f \u2032;\u03b8) \u2264 Q(f ;\u03b8). (13) The process of obtaining a new r-HST metric followed by a new labeling f \u2032 can be repeated till we reach a local minima. Note that the above inequality is obtained by assuming that the DP can be solved exactly. However, this cannot be guaranteed for general semi-metric distance functions. Nonetheless, in practice we use the above procedure to refine the labeling obtained by solving each r-HST metric labeling problem. As the results in the next section show, it helps further decrease the energy of the labelings obtained by our method at the cost of more computation time."}, {"heading": "6 Experiments", "text": "We compare our approach to several state of the art MAP estimation algorithms using both synthetic and real data experiments. In all our experiments we set r = 2. Empirically, we found that the accuracy of our approach saturates after using T = 50 r-HSTs to define the mixture.\nSynthetic Data. We consider the following cases of the MAP estimation problem: (i) truncated linear metrics; (ii) truncated quadratic semi-metrics; (iii) r-HST metrics; (iv) general metrics; and (v) general semi-metrics. Note that for uniform metric labeling, our approach reduces to \u03b1expansion and hence, we do not consider such problems in our evaluation. In each of the five cases above, we generated 100 random 4-connected grid structured MRFs of size 100 \u00d7 100 with H = 20. The unary potentials were randomly sampled from the uniform distribution defined over the interval [0, 10] (denoted by u(0, 10)). The pairwise potentials for the five cases were generated as follows. For the truncated convex models (cases (i) and (ii)) the truncation factor was sampled from u(0, 10). For r-HST metrics we defined a random hierarchical clustering of labels with the edge lengths at the root sampled from u(0, 10). The edge lengths at other levels were sampled while ensuring that the properties of the r-HST metric hold true. In order to generate a general metric distance function, we defined a complete graph over the labels with random edge lengths from u(0, 10). The distance function d(i, j) between labels i and j is given by the shortest path from i to j. A general semi-metric distance was defined by randomly sampling the values of d(i, j) where i 6= j from u(0, 10) and setting d(i, i) = 0 for all i.\nThe MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20]. We used publicly available code for these approaches to compare them with the two variants of our method: with and without using the hard EM strategy described in \u00a7 5.3.\nThe \u03b1-expansion algorithm was solved using the primaldual scheme of [16] (for both the original problem as well as the various subproblems used in our approach). Recall that [16] also handles non-submodular moves and hence, is capable of solving semi-metric labeling problems like cases (ii) and (v). All the move making algorithms were initialized to the constant labeling f(a) = 1 for all va. For the truncated convex models (cases (i) and (ii)) the messages of TRW-S and BP-S were computed efficiently using the dis-\ntance transform technique [9]. We report the results of the methods described in [20, 32] (denoted by R-exp and Rswap respectively) only for truncated convex models since these approaches are not applicable to the other cases.\nTable 1 lists the average time required and the average value of the energy obtained for various methods. Our approach is slower than previous move making algorithms (\u03b1expansion and \u03b1\u03b2-swap) as it solves a set of r-HST metric labeling problems. However, in terms of the energy values, it significantly outperforms them in all cases. It even provides similar results to the methods of [20, 32] which were specifically designed for the truncated convex models. The energy values obtained by our approach also compare favorably with TRW-S. In terms of speed, our method is significantly faster than TRW-S, especially in cases where the distance transform trick cannot be employed. As mentioned earlier, the computational efficiency of our method is due to the fact that it only uses the efficient st-MINCUT algorithm in its design. Finally, we also note that the hard EM strategy decreases the energy of the labeling. However, it is slower since it has to solve at least one instance of the DP (5) for each r-HST in the mixture.\nScene Registration. Given two images of different scenes with some common elements (e.g. both scenes contain buildings, see Fig. 2), scene registration requires us to find a point to point correspondence from one image to the other. In this work, we follow the framework of [22] and define an MRF whose variables correspond to the pixels of the first image. The labels of the variables denote the displacement that the pixel undergoes from the first image to its corresponding pixel in the second image. The neighborhood is defined such that the MRF forms a 4-connected grid graph. The unary potentials are given by the \u21131 difference between the SIFT features [23] of corresponding points. The pairwise potentials, which enforce smoothness of the displacement map, are defined as\n\u03b8ab(i, j) = \u03ba (min{|u(i) \u2212 u(j)|,M} + min{|v(i) \u2212 v(j)|,M}) , (14) where (u(i), v(i)) and (u(j), v(j)) are the horizontal and vertical displacements specified by labels li and lj respectively, M is the truncation factor and \u03ba is the scaling factor. Since the above pairwise potential forms a metric distance, our approach can be applied to obtain the solution.\nIn our experiments, we use the values of u(i) \u2208 [\u22125, 5] and v(i) \u2208 [\u22125, 5], i.e. the total number of labels for each random variable is H = 121. The truncation factor M was set to 5 and the scaling factor \u03ba = 1. Fig. 2 shows the results obtained for three pairs of images using six different MAP estimation algorithms along with the corresponding energy values and timings. Similar to the synthetic data experiments, our approach outperforms other move making approaches in terms of accuracy, and it outperforms TRW-S in terms of speed. In fact, the accuracy of our method is very similar to TRW-S. Note that TRW-S and BP-S\ncan be speeded up by using the decomposable model [26]. However, this makes the approximations to the free energy weaker thereby providing less accurate results.\nA related problem to scene registration, known as stereo reconstruction, is concerned with obtaining correspondences between two images of the same scene. The image pairs are epipolar rectified, i.e. the vertical displacement of each pixel is known to be 0. The unary potentials are computed using the difference in the RGB values of the corresponding pixels (instead of the SIFT feature), and the pairwise potentials are given by equation (14) with M = 5 and \u03ba = 20. We compared our approach with other algorithms on two standard stereo pairs used in computer vision, namely \u2018teddy\u2019 and \u2018tsukuba\u2019. Our method provides a labeling with lower energy than \u03b1-expansion and \u03b1\u03b2-swap using H = 40 labels, as shown in Fig. 3. Image Denoising Image denoising is a classic problem in low-level computer vision. Given an image with noise and/or missing pixels, the task is to obtain a \u2018clean\u2019 version of the image, i.e. remove the noise and fill up the missing pixels. The problem is modeled as an MRF whose variables correspond to the image pixels and whose edges define a 4-connected grid graph. The labels are the 256 possible intensity values that lie in the interval [0, 255]. The unary potentials are given by the squared difference between the intensity corresponding to the label and the observed intensity in the image. Since natural images are smooth, i.e. neighboring pixels tend to have similar intensity values, it is common practice to employ truncated convex pairwise potentials. In this work, we use\n\u03b8ab(i, j) = 30min{|i\u2212 j|, 50}. (15)\nWe compared our method with the state of the art MAP estimation algorithms on two standard images, namely \u2018house\u2019 and \u2018penguin\u2019. Fig. 4 shows the results obtained. Similar to other synthetic and real data experiments, our approach obtains labelings with lower energy values than the other move making algorithms (although it takes a longer time since it solves a series of r-HST metric labeling problems). In terms of the energy values, our method is outperformed by TRW-S but is computationally more efficient.\nThe results for scene segmentation are provided in [18]."}, {"heading": "7 Discussion", "text": "We presented a move making approach for the semi-metric labeling problem which approximates the given semimetric into a mixture of r-HST metrics and solves each of the resulting problems using an efficient st-MINCUT based algorithm. Our approach provides the guarantees of the LP relaxation for the metric labeling problem. Together with the work of [5, 20], this provides further evidence of a link between randomized rounding techniques used with convex relaxations and move making algorithms. We believe that further investigations in this direction would help\ndesign move making algorithms for more complex relaxations such as [19, 28]. In practice, the results on both synthetic and real data experiments show that our method reduces the gap in performance between move making algorithms and message passing approaches. This is particularly true for applications where the unary potentials do not dominate the pairwise potentials, i.e. the prior specified by the MRF plays a vital role in obtaining good results (e.g. in scene registration). Such scenarios occur not only during testing, but during parameter learning of MRFs as well (for example, structured SVMs [31] solve a series of MAP estimation problems to learn log-linear models). An interesting direction for future research would be to generalize our move making approach to other hierarchical distance functions that approximate semi-metric distances accurately and can be learnt efficiently. Similar to the existing move making algorithms [12], the possibility of extending our approach to solve special cases of higher order potentials should also be explored. Acknowledgments. We thank Stephen Gould for helpful discussions and careful proofreading of the manuscript. The first author is funded by the DARPA grant SA499610929-5."}], "references": [{"title": "On approximating arbitrary metrics by tree metrics", "author": ["Y. Bartal"], "venue": "In STOC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Pseudo-boolean optimization", "author": ["E. Boros", "P. Hammer"], "venue": "Discrete Applied Mathematics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision", "author": ["Y. Boykov", "V. Kolmogorov"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Markov random fields with efficient approximations", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "In CVPR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Fast approximate energy minimization via graph cuts", "author": ["Y. Boykov", "O. Veksler", "R. Zabih"], "venue": "In ICCV,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Approximating a finite metric by a small number of tree metrics", "author": ["M. Charikar", "C. Chekuri", "A. Goel", "S. Guha", "S. Plotkin"], "venue": "In FOCS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A linear programming formulation and approximation algorithms for the metric labelling problem", "author": ["C. Chekuri", "S. Khanna", "J. Naor", "L. Zosin"], "venue": "SIAM Journal on Disc. Math.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "A tight bound on approximating arbitrary metrics by tree metrics", "author": ["J. Fakcharoenphol", "S. Rao", "K. Talwar"], "venue": "In STOC,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Efficient matching of pictorial structures", "author": ["P. Felzenszwalb", "D. Huttenlocher"], "venue": "In CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Convergent message-passing algorithms for inference over general graphs with convex free energy", "author": ["T. Hazan", "A. Shashua"], "venue": "In UAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields", "author": ["J. Kleinberg", "E. Tardos"], "venue": "In STOC,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Robust higher order potentials for enforcing label consistency", "author": ["P. Kohli", "L. Ladicky", "P. Torr"], "venue": "In CVPR,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "On partial optimality in multi-label MRFs", "author": ["P. Kohli", "A. Shekhovtsov", "C. Rother", "V. Kolmogorov", "P. Torr"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "MRF optimization via dual decomposition: Message-passing revisited", "author": ["N. Komodakis", "N. Paragios", "G. Tziritas"], "venue": "In ICCV,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Fast, approximately optimal solutions for single and dynamic MRFs", "author": ["N. Komodakis", "G. Tziritas", "N. Paragios"], "venue": "In CVPR,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "The partial constraint satisfaction problem: Facets and lifting theorems", "author": ["A. Koster", "C. van Hoesel", "A. Kolen"], "venue": "Operations Research Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "MAP estimation of semi-metric MRFs via hierarchical graph cuts", "author": ["M.P. Kumar", "D. Koller"], "venue": "Technical report, Stanford University,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "An analysis of convex relaxations for MAP estimation", "author": ["M.P. Kumar", "V. Kolmogorov", "P. Torr"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Improved moves for truncated convex models", "author": ["M.P. Kumar", "P. Torr"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "FusionFlow: Discrete-continuous optimization for optical flow estimation", "author": ["V. Lempitsky", "S. Roth", "C. Rother"], "venue": "In CVPR,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "SIFT flow: Dense correspondence across different scenes", "author": ["C. Liu", "J. Yuen", "A. Torralba", "J. Sivic", "W. Freeman"], "venue": "In ECCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Object recognition from local scale-invariant features", "author": ["D. Lowe"], "venue": "In ICCV,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Syntactic analysis of two-dimensional visual signals in noisy conditions", "author": ["M. Schlesinger"], "venue": "Kibernetika,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1976}, {"title": "Efficient MRF deformation model for non-rigid image matching", "author": ["A. Shekhovstov", "I. Kovtun", "V. Hlavac"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Texton- Boost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation", "author": ["J. Shotton", "J. Winn", "C. Rother", "A. Criminisi"], "venue": "In ECCV, pages I:", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2006}, {"title": "New outer bounds for the marginal polytope", "author": ["D. Sontag", "T. Jaakkola"], "venue": "In NIPS,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Tightening LP relaxations for MAP using message passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "T. Jaakkola", "Y. Weiss"], "venue": "In UAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "A comparative study of energy minimization methods for Markov random fields with smoothness-based priors", "author": ["R. Szeliski", "R. Zabih", "D. Scharstein", "O. Veksler", "V. Kolmogorov", "A. Agarwala", "M. Tappen", "C. Rother"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Support vector learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Graph cut based optimization for MRFs with truncated convex priors", "author": ["O. Veksler"], "venue": "In CVPR,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "MAP estimation via agreement on trees: Message passing and linear programming", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "Info. Th.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "MAP estimation, linear programming and belief propagation with convex free energies", "author": ["Y. Weiss", "C. Yanover", "T. Meltzer"], "venue": "In UAI,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Global stereo reconstruction under second order smoothness priors", "author": ["O. Woodford", "P. Torr", "I. Reid", "A. Fitzgibbon"], "venue": "In CVPR,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Generalized belief propagation", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "In NIPS, pages 689\u2013695,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "We consider a special case of MAP estimation, known as semi-metric labeling [4], where (i) the size of the maximal clique is 2 (a pairwise MRF); and (ii) the pairwise potentials are defined by a semi-metric distance function over labels.", "startOffset": 76, "endOffset": 79}, {"referenceID": 29, "context": "Although these may seem like very restrictive assumptions, several problems in computer vision and related areas can be expressed using semi-metric labeling, from low level tasks like image denoising and stereo reconstruction [30] to high level tasks like pose estimation [9] and scene segmentation [27].", "startOffset": 226, "endOffset": 230}, {"referenceID": 8, "context": "Although these may seem like very restrictive assumptions, several problems in computer vision and related areas can be expressed using semi-metric labeling, from low level tasks like image denoising and stereo reconstruction [30] to high level tasks like pose estimation [9] and scene segmentation [27].", "startOffset": 272, "endOffset": 275}, {"referenceID": 26, "context": "Although these may seem like very restrictive assumptions, several problems in computer vision and related areas can be expressed using semi-metric labeling, from low level tasks like image denoising and stereo reconstruction [30] to high level tasks like pose estimation [9] and scene segmentation [27].", "startOffset": 299, "endOffset": 303}, {"referenceID": 0, "context": "We describe a novel algorithm for semi-metric labeling which approximates a given semi-metric distance function using a mixture of r-hierarchically well-separated tree (rHST) metrics [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "The r-HST metrics form an amenable class of distance functions which admit elegant divide-andconquer approaches for several problems [1, 8].", "startOffset": 133, "endOffset": 139}, {"referenceID": 7, "context": "The r-HST metrics form an amenable class of distance functions which admit elegant divide-andconquer approaches for several problems [1, 8].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "We show how each resulting r-HST metric labeling problem can be solved accurately using an iterative procedure that only employs the efficient st-MINCUT algorithm [3] in its design.", "startOffset": 163, "endOffset": 166}, {"referenceID": 9, "context": "Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36].", "startOffset": 105, "endOffset": 129}, {"referenceID": 13, "context": "Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36].", "startOffset": 105, "endOffset": 129}, {"referenceID": 14, "context": "Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36].", "startOffset": 105, "endOffset": 129}, {"referenceID": 28, "context": "Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36].", "startOffset": 105, "endOffset": 129}, {"referenceID": 33, "context": "Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36].", "startOffset": 105, "endOffset": 129}, {"referenceID": 35, "context": "Message passing algorithms attempt to minimize approximations of the free energy associated with the MRF [10, 14, 15, 29, 34, 36].", "startOffset": 105, "endOffset": 129}, {"referenceID": 13, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 32, "endOffset": 44}, {"referenceID": 14, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 32, "endOffset": 44}, {"referenceID": 33, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 32, "endOffset": 44}, {"referenceID": 6, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 131, "endOffset": 146}, {"referenceID": 16, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 131, "endOffset": 146}, {"referenceID": 24, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 131, "endOffset": 146}, {"referenceID": 32, "context": "Amongst them, the algorithms of [14, 15, 34] are closely related to the linear programming (LP) relaxation of semi-metric labeling [7, 17, 25, 33].", "startOffset": 131, "endOffset": 146}, {"referenceID": 29, "context": "Although message passing algorithms provide accurate MAP estimates, they can be computationally expensive in certain cases [30].", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "in \u03b1\u03b2-swap [4] and \u03b1-expansion [5].", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "in \u03b1\u03b2-swap [4] and \u03b1-expansion [5].", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "However, recently researchers have also used more sophisticated algorithms such as quadratic pseudo-boolean optimization [2] (e.", "startOffset": 121, "endOffset": 124}, {"referenceID": 12, "context": "see [13, 21, 35]).", "startOffset": 4, "endOffset": 16}, {"referenceID": 20, "context": "see [13, 21, 35]).", "startOffset": 4, "endOffset": 16}, {"referenceID": 34, "context": "see [13, 21, 35]).", "startOffset": 4, "endOffset": 16}, {"referenceID": 0, "context": "1 The r-HST Metric An r-HST metric [1] d(\u00b7, \u00b7) is specified by a rooted tree whose edge lengths are non-negative and satisfy the following properties: (i) the edge lengths from any node to all of its children are the same; and (ii) the edge lengths along any path from the root to a leaf decrease by a factor of at least r > 1.", "startOffset": 35, "endOffset": 38}, {"referenceID": 0, "context": "As observed in several earlier works [1, 6, 8], r-HSTs satisfying this assumption are sufficient to provide an accurate approximation of a given semi-metric distance function.", "startOffset": 37, "endOffset": 46}, {"referenceID": 5, "context": "As observed in several earlier works [1, 6, 8], r-HSTs satisfying this assumption are sufficient to provide an accurate approximation of a given semi-metric distance function.", "startOffset": 37, "endOffset": 46}, {"referenceID": 7, "context": "As observed in several earlier works [1, 6, 8], r-HSTs satisfying this assumption are sufficient to provide an accurate approximation of a given semi-metric distance function.", "startOffset": 37, "endOffset": 46}, {"referenceID": 4, "context": "Each of the subproblems is specified by a node in the given r-HST, and their solutions are combined using the standard \u03b1-expansion algorithm [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 17, "context": "We obtain an approximate MAP estimate f \u2032 for the parameter \u03b8 using \u03b1-expansion (see [18] for details).", "startOffset": 85, "endOffset": 89}, {"referenceID": 6, "context": "Our method is not only easy to implement and effective in practice, it also provides the approximation bounds of the LP relaxation [7, 11].", "startOffset": 131, "endOffset": 138}, {"referenceID": 10, "context": "Our method is not only easy to implement and effective in practice, it also provides the approximation bounds of the LP relaxation [7, 11].", "startOffset": 131, "endOffset": 138}, {"referenceID": 17, "context": "1 [18]: For r-HST metric labeling we obtain an approximation bound of O(1).", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "Similar to MAP estimation, several problems specified on r-HST metrics are well-known to be amenable to efficient divide-and-conquer approaches [1, 8].", "startOffset": 144, "endOffset": 150}, {"referenceID": 7, "context": "Similar to MAP estimation, several problems specified on r-HST metrics are well-known to be amenable to efficient divide-and-conquer approaches [1, 8].", "startOffset": 144, "endOffset": 150}, {"referenceID": 7, "context": "[8] provide a simple yet accurate randomized algorithm for sampling r-HST metrics.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The algorithm is initialized by: (i) picking a random permutation \u03c0 of the label indices {1, 2, \u00b7 \u00b7 \u00b7 , H}, which defines a priority ordering on the cluster centers; and (ii) choosing a value of \u03b2 \u2208 [1, 2] from the distribution Pr(x) = 1/(x log 2).", "startOffset": 199, "endOffset": 205}, {"referenceID": 1, "context": "The algorithm is initialized by: (i) picking a random permutation \u03c0 of the label indices {1, 2, \u00b7 \u00b7 \u00b7 , H}, which defines a priority ordering on the cluster centers; and (ii) choosing a value of \u03b2 \u2208 [1, 2] from the distribution Pr(x) = 1/(x log 2).", "startOffset": 199, "endOffset": 205}, {"referenceID": 7, "context": "[8] showed that \u2206 reduces by at least a factor of r = 2 for metric distances, thereby providing us with an r-HST.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Importantly, the method of [8] can also be applied for approximating semi-metric distance functions.", "startOffset": 27, "endOffset": 30}, {"referenceID": 7, "context": "In order to answer this question, we note that [8] also provided a deterministic version of the above algorithm for solving a related problem that we call the dual procedure (DP):", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "[6] provided an iterative algorithm to obtain a small mixture of r-HST metrics (with O(H logH) r-HSTs).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Furthermore, it also provides an accurate approximation as evidenced by the following result from [8] and its simple extension.", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "1 [8]: When d(\u00b7, \u00b7) is a metric distance function, the above approach provides a mixture of r-HST metrics with distortion of O(logH).", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": "In order to overcome this problem, we solve the \u03b1-expansion procedure by using the primal dual scheme of [16].", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "3 [18]: For the metric and semi-metric labeling problems, we obtain an approximation bound of O(logH) and O((\u03b3 logH)) respectively.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": "4 [18]: Theorem 5.", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "Together with the results for truncated convex models [5, 20], this implies that there exist moving making algorithms which match all known LP relaxation guarantees when the number of labels is smaller than the number of variables (i.", "startOffset": 54, "endOffset": 61}, {"referenceID": 19, "context": "Together with the results for truncated convex models [5, 20], this implies that there exist moving making algorithms which match all known LP relaxation guarantees when the number of labels is smaller than the number of variables (i.", "startOffset": 54, "endOffset": 61}, {"referenceID": 9, "context": "The unary potentials were randomly sampled from the uniform distribution defined over the interval [0, 10] (denoted by u(0, 10)).", "startOffset": 99, "endOffset": 106}, {"referenceID": 4, "context": "The MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20].", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "The MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20].", "startOffset": 104, "endOffset": 107}, {"referenceID": 13, "context": "The MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "The MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20].", "startOffset": 203, "endOffset": 207}, {"referenceID": 31, "context": "The MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20].", "startOffset": 220, "endOffset": 224}, {"referenceID": 19, "context": "The MRFs were used to test several state of the art MAP estimation algorithms: \u03b1-expansion [5], \u03b1\u03b2-swap [4], sequential tree-reweighted message passing (TRW-S) [14], sequential belief propagation (BP-S) [24], range swap [32], and range expansion [20].", "startOffset": 246, "endOffset": 250}, {"referenceID": 15, "context": "The \u03b1-expansion algorithm was solved using the primaldual scheme of [16] (for both the original problem as well as the various subproblems used in our approach).", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Recall that [16] also handles non-submodular moves and hence, is capable of solving semi-metric labeling problems like cases (ii) and (v).", "startOffset": 12, "endOffset": 16}, {"referenceID": 8, "context": "tance transform technique [9].", "startOffset": 26, "endOffset": 29}, {"referenceID": 19, "context": "We report the results of the methods described in [20, 32] (denoted by R-exp and Rswap respectively) only for truncated convex models since these approaches are not applicable to the other cases.", "startOffset": 50, "endOffset": 58}, {"referenceID": 31, "context": "We report the results of the methods described in [20, 32] (denoted by R-exp and Rswap respectively) only for truncated convex models since these approaches are not applicable to the other cases.", "startOffset": 50, "endOffset": 58}, {"referenceID": 19, "context": "It even provides similar results to the methods of [20, 32] which were specifically designed for the truncated convex models.", "startOffset": 51, "endOffset": 59}, {"referenceID": 31, "context": "It even provides similar results to the methods of [20, 32] which were specifically designed for the truncated convex models.", "startOffset": 51, "endOffset": 59}, {"referenceID": 21, "context": "In this work, we follow the framework of [22] and define an MRF whose variables correspond to the pixels of the first image.", "startOffset": 41, "endOffset": 45}, {"referenceID": 22, "context": "The unary potentials are given by the l1 difference between the SIFT features [23] of corresponding points.", "startOffset": 78, "endOffset": 82}, {"referenceID": 25, "context": "Note that TRW-S and BP-S can be speeded up by using the decomposable model [26].", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "The results for scene segmentation are provided in [18].", "startOffset": 51, "endOffset": 55}, {"referenceID": 4, "context": "Together with the work of [5, 20], this provides further evidence of a link between randomized rounding techniques used with convex relaxations and move making algorithms.", "startOffset": 26, "endOffset": 33}, {"referenceID": 19, "context": "Together with the work of [5, 20], this provides further evidence of a link between randomized rounding techniques used with convex relaxations and move making algorithms.", "startOffset": 26, "endOffset": 33}, {"referenceID": 21, "context": "The image pairs are obtained from [22].", "startOffset": 34, "endOffset": 38}, {"referenceID": 18, "context": "design move making algorithms for more complex relaxations such as [19, 28].", "startOffset": 67, "endOffset": 75}, {"referenceID": 27, "context": "design move making algorithms for more complex relaxations such as [19, 28].", "startOffset": 67, "endOffset": 75}, {"referenceID": 30, "context": "Such scenarios occur not only during testing, but during parameter learning of MRFs as well (for example, structured SVMs [31] solve a series of MAP estimation problems to learn log-linear models).", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "Similar to the existing move making algorithms [12], the possibility of extending our approach to solve special cases of higher order potentials should also be explored.", "startOffset": 47, "endOffset": 51}], "year": 2009, "abstractText": "We consider the task of obtaining the maximum a posteriori estimate of discrete pairwise random fields with arbitrary unary potentials and semimetric pairwise potentials. For this problem, we propose an accurate hierarchical move making strategy where each move is computed efficiently by solving an st-MINCUT problem. Unlike previous move making approaches, e.g. the widely used \u03b1-expansion algorithm, our method obtains the guarantees of the standard linear programming (LP) relaxation for the important special case of metric labeling. Unlike the existing LP relaxation solvers, e.g. interior-point algorithms or tree-reweighted message passing, our method is significantly faster as it uses only the efficient st-MINCUT algorithm in its design. Using both synthetic and real data experiments, we show that our technique outperforms several commonly used algorithms.", "creator": null}}}