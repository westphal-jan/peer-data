{"id": "1205.2652", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Complexity Analysis and Variational Inference for Interpretation-based Probabilistic Description Logic", "abstract": "this laboratory paper presents complexity analysis and variational methods specific for inference in probabilistic description logics featuring boolean operators, quantification, integer qualified number restrictions, nominals, inverse roles and binary role hierarchies. inference is shown to be pexp - complete, and variational learning methods are systematically designed so as to exploit logical inference theories whenever possible.", "histories": [["v1", "Wed, 9 May 2012 15:05:48 GMT  (251kb)", "http://arxiv.org/abs/1205.2652v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["fabio gagliardi cozman", "rodrigo bellizia polastro"], "accepted": false, "id": "1205.2652"}, "pdf": {"name": "1205.2652.pdf", "metadata": {"source": "CRF", "title": "Complexity Analysis and Variational Inference for Interpretation-based Probabilistic Description Logics", "authors": ["Fabio Gagliardi", "Rodrigo Bellizia Polastro"], "emails": [], "sections": [{"heading": null, "text": "This paper presents complexity analysis and variational methods for inference in probabilistic description logics featuring Boolean operators, quantification, qualified number restrictions, nominals, inverse roles and role hierarchies. Inference is shown to be PEXP-complete, and variational methods are designed so as to exploit logical inference whenever possible."}, {"heading": "1 Introduction", "text": "In this paper we investigate probabilistic description logics that are based on the well-known description logicALC. In ALC one deals with individuals, concepts, roles, Boolean operators, and restricted forms of quantification [1]. For example, the concept Fireman denotes the set of firemen, and Vegetarian u \u2200buyFrom.Fireman denotes a set of individuals who are vegetarian and buy only from firemen. We can also have assertions such as Fireman(John), stating that John belongs to Fireman.\nWe start from credalALC, a probabilistic description logic we have introduced previously [8, 33]. Credal ALC, referred to as CRALC, mixes constructs of ALC with features of relational Bayesian networks. Indeed most of this paper can be read as the study of those relational Bayesian networks that can be expressed with (variants) of ALC. In CRALC one can have probabilistic assessments such as P (C|D) = \u03b1 for concepts C and D, or P (r) = \u03b2 for role r. The semantics of these assessments is roughly given by:\n\u2200x : P (C(x)|D(x)) = \u03b1, \u2200x, y : P (r(x, y)) = \u03b2. Credal ALC is attractive because its semantics allows reasonably flexible probabilistic assessments and the calculation of probabilities over assertions; the language stays both close to the power of ALC and to the clarity of relational Bayesian networks. Section 2 reviews work on probabilistic description logics and in particular CRALC.\nTo illustrate the main features of CRALC, consider an example whose syntax should be reasonably easy to grasp. Take the Kangaroo ontology, a small collection of facts about animals.1 Consider a probabilistic version as follows [33]: P (Animal)=0.9, P (Rational) =0.6, P (hasChild)=0.3, Human \u2261 Animal u Rational, Beast \u2261 Animal u \u00acRational, Parent \u2261 Human u \u2203hasChild.Human, P (Kangaroo|Beast) = 0.4, P (Kangaroo|\u00acBeast) = 0.0, MaternityKangaroo \u2261 Kangaroo u \u2203hasChild.Kangaroo. The most basic problem in CRALC is to compute the probability of an assertion, possibly conditional on other assertions. For instance, we might be interested in computing P (MaternityKangaroo(Tweety)) given the probabilistic Kangaroo ontology. In Section 3 we present complexity analysis for this inference problem. Variants of CRALC are also analyzed in Section 3, both by removing some constructs from the language, and by adding constructs such as numeric restrictions, inverse roles, and nominals.\nIn Section 4 we propose a variational scheme for inference in CRALC and its variants, designed to exploit Boolean operators (through logical inference) in a set of sentences. We often adopt a Bayesian assumption that guarantees uniqueness of probability values, but we also briefly examine failures of this assumption. Experiments described in Section 4 show the value of the algorithms in practice."}, {"heading": "2 Probabilistic description logics", "text": "The combination of probability and logic has a long history, with much recent activity [16, 17, 34]. In particular, there has been significant interest in probabilistic description logics [29]. The next two paragraphs summarize recent activity in this topic; a more detailed review and comparison can be found in our previous publication [8, Sec.2].\nWe can divide probabilistic description logics into logics\n1Distributed with the CEL System for logical reasoning, at http://lat.inf.tu-dresden.de/\u02dcmeng/ontologies/kangaroo.cl.\nthat assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37]. In a domain-based semantics an assessment such as P (Fireman) = 1/2 means that the probability mass over the set of all firemen is half. The challenge then is to define probabilities for an assertion: the probability of the set of individuals who are firemen does not constrain the probability that a particular individual is a fireman. This is indeed the old philosophical problem of direct inference [26]. Hence logics with domainbased semantics either do not allow probabilities of assertions to be expressed, or resort to non-standard forms of entailment [21, 28]. This is the reason why we developed CRALC as an interpretation-based probabilistic logic. We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28]. Logics in the first group usually assume some Markov condition, and assume that probabilities are uniquely defined for any valid sentence. Logics in the second group allow probability intervals (sometimes sets of probabilities) to be associated with sentences. We think that independence relations are powerful constraints that should be used whenever possible, and for this reason CRALC has many similarities to logics in the first group, even though it does not mandate uniqueness for probabilities. In particular, CRALC shares many features with PR-OWL [7] as both have interpretation-based semantics and use graph-theoretical tools.\nWe now turn to a more precise description of CRALC, starting with background onALC [36]. Throughout, a, b are individuals; A, B, C, D are concepts; r, s are roles. A terminology contains inclusions C v D and definitions C \u2261 D. A concept can be a concept name or, recursively, a conjunction (C uD), disjunction (C tD), negation (\u00acC), existential restriction (\u2203r.C), value restriction (\u2200r.C). Concept C directly uses D if they appear respectively in the left and right hand sides of an inclusion/definition. The relation uses is always the transitive closure of directly uses. A terminology is acyclic if no concept uses itself in an inclusion/definition. An Abox contains assertions such as C(a), r(a, b). An assertion C(a) directly uses assertions of concepts and roles directly used by C instantiated respectively by elements and pairs of elements from D. The semantics of ALC is given by a set D, the domain, and a mapping I, the interpretation. This mapping takes each individual to an element of the domain, each concept name to a subset of the domain, each role name to a binary relation on D \u00d7D. An interpretation is extended to other concepts: I(CuD) = I(C)\u2229 I(D), I(C tD) = I(C)\u222a I(D), I(\u00acC) = D\\I(C), I(\u2203r.C) = {x \u2208 D|\u2203y : (x, y) \u2208 I(r) \u2227 y \u2208 I(C)}, I(\u2200r.C) = {x \u2208 D|\u2200y : (x, y) \u2208 I(r) \u2192 y \u2208 I(C)}. We have C v D if and only if I(C) \u2286 I(D), and C \u2261 D if and only if I(C) = I(D). There are translations of these con-\nstructs into modal and first-order logic [1]; we often treat a concept C as a unary predicate C(x), a role r as a binary predicate r(x, y), and restrictions as quantifiers.\nIn CRALC, we have all constructs of ALC but we only allow concept names in the left hand side of inclusions/definitions. Additionally, we allow three kinds of probabilistic assessments, where C is a concept name, D is a concept, r is a role name:\nP (C) \u2208 [\u03b1, \u03b1], (1) P (C|D) \u2208 [\u03b1, \u03b1], (2)\nP (r) \u2208 [\u03b2, \u03b2]. (3) In Expression (2), C directly uses conditioning concepts. We write P (C|D) = \u03b1 when \u03b1 = \u03b1, P (C|D) \u2265 \u03b1 when \u03b1 < \u03b1 = 1, and so on. No concept is allowed to use itself, neither through deterministic nor through probabilistic inclusions/definitions; this guarantees acyclicity.\nThe semantics is based on probabilities over interpretations; that is, we take that measures are assigned to the set of all interpretations. The semantics of Expression (1) is: for any x \u2208 D, the probability that x belongs to the interpretation of C is in [\u03b1, \u03b1]. That is,\n\u2200x \u2208 D : P ( { I : x \u2208 I(C) } ) \u2208 [\u03b1, \u03b1].\nAn informal and intuitive way to express the semantics is\n\u2200x \u2208 D : P (C(x)) \u2208 [\u03b1, \u03b1]. The semantics of Expressions (2) and (3) is then:\n\u2200 x \u2208 D : P (C(x)|D(x)) \u2208 [\u03b1, \u03b1], \u2200 (x, y) \u2208 D \u00d7D : P (r(x, y)) \u2208 [\u03b2, \u03b2].\nAs usual in interpretation-based probabilistic logics [3], CRALC requires that all individuals be rigid (an individual corresponds to the same element of the domain across interpretations). Similarly to other probabilistic description logics [7, 13, 25], CRALC adopts a (two-part) Markov condition that is best formulated using indicator functions.2 First, for every concept C and for every x \u2208 D, the indicator function of C(x) is independent of the indicator function of every assertion of a concept that does not use C(x), given the indicator function of assertions of concepts that C directly uses. Second, for every role r and for every (x, y) \u2208 D \u00d7 D, the indicator function of r(x, y) is independent of every indicator function of assertions, except those assertions of concepts that use r(x, y). This closes the specification of CRALC. We define the t-network of a terminology to be a directed acyclic graph where each node is a concept name or a restriction or a role name. If concept C directly uses other\n2The indicator function of a grounded relation C(x) or r(x, y) yields 1 if the grounded relation holds and 0 otherwise.\n\u00ae  \u00a9 \u00aaAnimal\u00ae\n \u00a9 \u00aaHuman\u00ae\n \u00a9 \u00aaParent\n\u00ae  \u00a9 \u00aahasChild \u00ae \n\u00a9 \u00aaKangaroo\u00ae\n \u00a9 \u00aaMaternityKangaroo\n\u00ae  \u00a9 \u00aaRational\u00ae\n \u00a9 \u00aaBeastj\u2203 j\u2203 ? ? ? ? ? ?- \u00be \u00be X XXz\u00bb\u00bb\u00bb9\n\u00a1\u00a1\u00aa @@R\njr \u00a1\u00b5 @R\n\u00b1\u00b0 \u00b2\u00af A \u00b1\u00b0 \u00b2\u00af B \u00b1\u00b0 \u00b2\u00af C ? ? \u00ae  \u00a9 \u00aa\u2203r.D \u00b1\u00b0 \u00b2\u00af D \u00ae  \u00a9 \u00aa\u2200r.A ? ? - \u00be\nFigure 1: Left: t-network for Kangaroo ontology (Section 1). Right: t-network for terminology Tu (Example 1).\nconcept/role names, these names appear as parents of C in the t-network. To simplify the presentation, we assume that only a concept name C can appear in a restriction \u2200r.C or \u2203r.C (without loss of generality, as C can be defined through other concepts). A restriction \u2200r.C or \u2203r.C has parents r and C in the t-network. Figure 1 shows the tnetworks for the Kangaroo ontology (Section 1) and for the following simple example.\nExample 1 (from [8]) Consider terminology Tu: P (A) = 0.9, B v A, P (B|A) = 0.45, D \u2261 \u2200r.A, C \u2261 B t \u2203r.D, P (r) = 0.3. Consider a terminology T, a set C containing the concept names and the restrictions in T, a set R containing the role names inT, and a finite domainD. Denote by pa(C(a)) the set of all indicator functions of assertions that C(a) directly uses (C may be a restriction). Under the Markov condition, any joint probability distribution for the set X of indicator functions of assertions from T and D has the form P (X) = \u220f\nC\u2208C;x\u2208D P (C(x)|pa(C(x)))\u00d7\n\u220f\nr\u2208R;(x,y)\u2208D\u00d7D P (r(x, y)) . (4)\nThroughout the paper we do not differentiate a grounded relation from its indicator function: C(x) and r(x, y) stand for indicator functions of binary and unary relations in Expression (4). We assume throughout the unique-name assumption: if a and b are distinct names for individuals, their interpretations are distinct. We also contemplate two other assumptions. The confined-domain assumption says that D has finite known cardinality, denoted by N . The uniqueness assumption says that there is a precise assessment for each role name (P (r) = \u03b2), and for each concept name we have either a definition C \u2261 D, or a single assessment P (C) = \u03b1, or a pair of assessments {P (C|C \u2032) = \u03b1\u2032, P (C|\u00acC \u2032) = \u03b1\u2032\u2032} where C \u2032 must be a concept name (that can be defined elsewhere). Under these three assumptions, collectively referred to as the Bayesian assumption, every CRALC terminology defines a unique relational Bayesian network [22] whose grounding is a Bayesian network given by Expression (4). Logics such as P-CLASSIC and PR-OWL adopt similar assumptions.\nAn inference in CRALC is the computation of P (A0(a0)|A) given a terminology and an Abox A = {Aj(aj)}Mj=1. For instance, given the terminology Tu in Example 1 we have the trivial inferences P (A(a)) = 0.9 and P (A(a)|A(a)) = 1 for any N .\n3 Expressivity, complexity, and CRHOrIQ We now examine the interplay between expressivity and complexity in CRALC and its extensions. Besides usual classes P, NP, PSPACE and NEXP, we use three other classes. A language L is in class PP if there is a nondeterministic Turing machine M such that x \u2208 L if and only if more than half of the computations of M on input x end up accepting, when M has a polynomial-time bound [32]; L is in PPSPACE if the same definition is used but we replace polynomial-time by polynomial-space [31]; and L is in PEXP if the same definition is used but we replace polynomial-time by exponential-time [5].\nDefine Q .= P (A0(a0)|A) for an Abox A, and denote by INFB(Q) the decision problem that returns YES if Q > 1/2 and NO otherwise, under the Bayesian assumption. The assumption that numeric parameters are coded in unary is common in description logic research [1]. Even though in the present context unary specification seems rather artificial because probabilities are normally coded in binary, the following easy proposition is worth stating.\nProposition 1 If N is given in unary and probabilities in binary, INFB(Q) is a PP-complete problem in CRALC. Proof. Membership: Propositionalize terminology into Bayesian network with N |C| + N2|R| variables, and run probabilistic inference. Hardness: for N = 1 the inference is PP-complete (Bayesian network inference) [27]. 2\nA much more interesting question is the complexity of inferences for N in binary, where the domain is exponentially larger than its description. One might conjecture that, as satisfiability in ALC is in PSPACE, inference in CRALC should be in PPSPACE. One the other hand, as Jaeger\u2019s important previous analysis [24] indicates that (unless ETIME=NETIME) there must be model representation systems for which inference is not in P with respect to N in unary, one might suspect that inference with N in binary should take us to exponential time complexity of some sort. The next theorem offers the precise completeness result for inference; the proof, summarized in the Appendix, is somewhat long and contains ideas that may be of general interest.\nTheorem 1 If all numbers are given in binary, INFB(Q) is a PEXP-complete problem in CRALC.\nWe thus have a clear difference between probabilistic reasoning with an enumerated domain (Proposition 1) and with a compactly specified domain (Theorem 1). One might try to reduce the complexity of INFB(Q) by starting with a description logic simpler thanALC, for instance by discarding some operators and negation [2]. However, one can \u201cprobabilistically negate\u201d a concept C by creating a new concept C \u2032 that has probability 0 when C obtains and probability 1 otherwise. Thus the proof of Theorem 1 can\nbe reconstructed if we restrict CRALC even to the logical constructs of the simple logic EL (conjunction and existential quantification) [2]. Hence ALC seems to be the minimal start for a probabilistic description logic, and the gap between PP and PEXP does seem to be the minimal gap between an essentially propositional and a truly relational probabilistic representation system.\nWe now consider the opposite direction; that is, the complexity of inference in logics that extend CRALC. Consider the following constructs. A qualified number restriction (\u2265 k r.C) has semantics I(\u2265 k r.C) = {x \u2208 D : #{y \u2208 D : (x, y) \u2208 I(r) \u2227 y \u2208 I(C)} \u2265 k}, where #(\u00b7) yields the cardinality of the set. Number restrictions (\u2264 k r.C) and (= k r.C) are defined similarly. An inverse role r\u2212 is interpreted by replacing r\u2212 and (x, y) respectively by r and (y, x) in the semantics. A role hierarchy is based on inclusions r v s, and a probabilistic version consists of assessments P (r|s) \u2208 [\u03b2\u2032, \u03b2\u2032] and P (r|\u00acs) \u2208 [\u03b2\u2032\u2032, \u03b2\u2032\u2032]. Such probabilistic role hierarchies demand some strenghtening of the uniqueness condition: we must assume that every role r must be either associated with an assessment P (r) = \u03b2 or a pair {P (r|s) = \u03b2\u2032, P (r|\u00acs) = \u03b2\u2032\u2032}. It is also possible to add nominals to CRALC; a nominal is an individual name identified with a concept. Nominals would add enormous expressivity to CRALC: for instance, one might express probabilities for a particular individual a through the assessment P (C|{a}) = \u03b1. However it does not seem reasonable to ask for assessments such as P ({a}) = \u03b1, meaning \u2200x \u2208 D : P ({a}(x)) = \u03b1, because this sentence is clearly false for every x if \u03b1 \u2208 (0, 1). Either the syntax regarding nominals and probabilities must be significantly changed, so that nominals cannot be assigned probabilities, or the inference algorithms presented later would have to be changed so as to detect inconsistencies between assessments and nominals. In this paper we do not attempt to model nominals in their full generality; instead we only allow nominals in restrictions such as \u2200r.{a}, \u2203r.{a} and \u2265 kr.{a}. Such constructs seem to capture significant portion of the practical use of nominals.\nAs usual in description logics, we indicate numeric restrictions by the letter Q; inverse roles by I; role hierarchies by H. We use Or to indicate the restricted use of nominals described in the previous paragraph. Whenever possible we remove the letters ALC from names, so CRALC with the additional features just mentioned is referred to as CRHOrIQ. This logic contains most of SHOIQ, a logical basis for the OWL language [19].3 We have:\nTheorem 2 If all numbers are given in binary, INFB(Q) is a PEXP-complete problem in logics whose features contain CREL and are contained in CRHOrIQ.\n3To reach SHOIQwe would need transitive roles; this seems to require new ideas as transitivity violates the Markov condition (groundings of the same role are dependent given transitivity).\nProof. Membership: argument in Theorem 1 is not affected by the new features. Hardness follows from Theorem 1. 2\nTo conclude this section, we briefly comment on failures of the confined-domain and uniqueness assumptions. By retaining the confined-domain assumption and dropping the uniqueness assumption, the decision problem \u201cmin Q > 1/2?\u201d belongs to NEXPPEXP, as there are exponentially many choices of probabilities (only the vertices of the sets of distributions can generate minimizing/maximizing probabilities [15], and there are exponentially many vertices) and for each one of these choices, an oracle in PEXP yields the answer. One might adopt an homogeneity assumption [8] prescribing that the selection of a probability (within a probability interval) should be constant across individuals; this assumption moves the decision problem to NPPEXP, as there are polynomially many choices concerning probabilities, followed by an oracle in PEXP. Other situations are more difficult to analyze. For instance, suppose we adopt uniqueness and take N to be countably infinite. Using results by Jaeger [23], we know that CRALC has a 0/1-law such that the probability of every restriction goes to 0 or 1 as N grows without bound [8]; if we could determine the limiting value of probabilities for quantifiers, then the grounded network would decompose into a set of independent Bayesian networks and inference would be PPcomplete. Of course, determining the limiting values for restrictions is not an easy matter [23], so our point is just to describe it as a difficult open problem. Likewise, we do not have complexity results for the challenging problems that arise when N is unconstrained. In Section 4.3 we produce approximate methods for such inferences. Indeed, we focus on approximate inference in the remainder of the paper, as it should be clear that applications will often have to resort to approximations.\n4 Variational inference for CRHOrIQ\nWe now look into methods that approximate Q = P (A0(a0)|A), where the assertions in A and the nominals in the terminology refer to individuals {a1, . . . , aM}. Define D\u2032 .= {a1, . . . , aM} and assume, without loss of generality, that a0 \u2208 D\u2032. If we ground a terminology, we obtain a directed acyclic graph with N slices: The slice of a is the set of indicator functions for grounded concepts C(a) and grounded roles r(a, x) as x ranges over D. Only M slices refer to named individuals in D\u2032; the other N \u2212 M slices are identical for all purposes, and we can lump them into a single parameterized slice. The network with a slice per named individual ai and an additional slice for a \u201cgeneric\u201d element x \u2208 D\\D\u2032 is called the shattered network [8].4 Figure 2 (left) shows the shattered network for terminol-\n4This network is a representation for the shattering operation in first-order variable elimination [11]; the relationship between shattering and shattered networks is given by [8, Thm. 1].\nogy Tu in Example 1 and query P (A(a0)|C(a0)); there is a slice for a0 (the only named individual in the query) and a single slice for x \u2208 D\\D\u2032. An additional variable y \u2208 D\\D\u2032 is used to represent groundings of roles in the parameterized slice.\nParts of the shattered network can be eliminated using dseparation, possibly leading to substantial savings. For instance, if d-separation eliminates all quantifiers and interval-valued assessments, we face a Bayesian network inference. We assume that such easy cases are detected before our variational method is employed.\nIn what follows we focus on universal/existential restrictions for the sake of space. Numeric restrictions and role hierarchies can be handled with straightforward modifications, while inverse roles can also be handled by the same methods but the equations presented later must be changed significantly. Finally, each nominal a that appears in restrictions can be dealt with by creating a concept {a} such that {a}(a) holds while {a}(x) does not hold for x 6= a; the shattered network now has all nodes {a}(x) clamped to false in its parameterized portion, and all techniques presented later apply. (A different scheme for inference in CRALC with restricted nominals can be found in [33].)"}, {"heading": "4.1 Inference under the Bayesian assumption", "text": "Under the Bayesian assumption every grounded network is a Bayesian network. Such networks tend to be densely connected due to the presence of restrictions. Indeed, even the state-of-art inference engine ACE [6] fails to handle the grounded network of Example 1 for N = 10. We have previously suggested the use of Loopy Belief Propagation (LBP) for approximate inference in large grounded networks [8, 33]. The key observation is that many messages in LBP are repeated across the grounded network, and can therefore be lumped into parameterized messages. The resulting parameterized LBP is similar to the lifted propagation scheme of Singla and Domingos [38]. The difference is that Singla and Domingos\u2019 algorithm applies to a general logic and offers few guarantees regarding the number of lifted messages to be exchanged in each iteration, while the number of parameterized messages is fixed in our previ-\nous method, due to the regularity of CRALC terminologies. To understand the idea behind parameterized LBP, suppose that in Example 1 we are interested in P (D(a0)|B(a0)). In LBP we need only to send messages from groundings of A and r to (\u2200r.A)(a0), and from this latter node to D(a0). Node A(a0) sends a single message to (\u2200r.A)(a0), while node A(x) sends N \u2212 1 identical messages to (\u2200r.A)(a0). Then P ((\u2200r.A)(a0)) is\n(1\u2212P (r)(1\u2212P (A)))N\u22121(1\u2212P (r)(1\u2212P (A(a0)|B(a0))).\nThis parameterized LBP scheme, where parameterized messages are raised to appropriate powers because they stand for sets of messages, is iterated until convergence.\nIn this section we present improvements to this parameterized LBP scheme, motivated by two observations. First, while LBP usually displays excellent empirical performance, in LBP all nodes are treated alike, thus forsaking the possible exploitation of logical reasoning in a tnetwork. We expect that applications for probabilistic description logics will contain quite a few deterministic definitions and Boolean operators. Hence it is important to develop probabilistic inference that can exploit logical inference whenever appropriate. Second, we note that most terminologies in the literature are represented as relatively sparse graphs; we expect that applications will contain tnetworks such that the case N = 1 admits exact inference.\nThe natural strategy is to examine variational methods that improve LBP by processing regions (sets of nodes) [41]. In fact we may ignore the variational justification of our algorithm and view it as a clustered propagation method, where messages are exchanged amongst regions. As there is no systematic way in the literature to define such regions optimally, we propose a scheme that is well suited to our purposes, by grouping nodes in the shattered network into larger conglomerates, and then running LBP on a set of regions that emerges easily from these conglomerates. In order to group nodes without concerns about directed cycles, we use factor graphs; that is, undirected bipartide graphs where circular nodes correspond to variables and rectangular nodes correspond to factors of the joint distribution, and where an edge connects a circular node and a rectangular node when the variable in the former defines the domain of\nthe factor in the latter. Each factor fj(Xj) is a conditional probability distribution, where Xj is a set of indicator functions of grounded relations. Figure 2 (middle) depicts the factor graph for a fragment of Figure 2 (left).\nOur proposal is to transform each slice of the grounded network into a small set of factors, and run LBP in the resulting factor graph, while running exact inference inside some of the factors. We now elaborate this idea.\nConsider the slice for individual a. Multiply together every probability distribution in the slice for a, except the distributions for indicator variables of restrictions (\u2200r.A)(a) and (\u2203r.A)(a).5 The resulting product is called the factor for a, and denoted by fa. We now construct a factor graph where all factors used to produce fa are replaced by the factor fa itself, and run LBP in this compact factor graph.\nDenote by Ba the Bayesian network with all nodes used in fa, and refer to nodes in Ba that are not grounded at a as extraneous node. For instance, if (\u2200r.B)(a) belongs to Ba, then B(b) is an extraneous node in Ba. Denote by Ea the variables in Ba that correspond to members of the Abox of interest (that is, the evidence in the query).\nTo simplify the calculation of messages, we introduce an individual b 6\u2208 D\u2032. So we have the individuals in D\u2032 plus b and then x standing for all other elements of the domain. Figure 2 (right) depicts a fragment of the resulting factor graph for Example 1, corresponding to the slice of a0. We must propagate the following messages until convergence [41, Eqs. (4), (5)], for a concept A and a \u2208 D\u2032 \u222a {b}:\nnA(a)\u2192fa\u2032 (A(a)) = \u220f\nf \u2208 G(A(a))\\fa\u2032 mf\u2192A(a)(A(a)),\nmfa\u2032\u2192A(a)(A(a)) = \u2211\nXa\u2032\\A(a) fa\u2032(Xa\u2032)\n\u220f A\u0302(a\u0302) \u2208 G(fa\u2032 )\\A(a) nA\u0302(a\u0302)\u2192fa\u2032 (A\u0302(a\u0302)),\nwhere G(\u00b7) denotes neighbors in the compact factor graph. Consider the computation of message mfa\u2192A(a), sent to node A(a) from fa. The distribution of an extraneous node is either a message nB(ai)\u2192fa from individuals ai, or a message nB(b)\u2192fa from the generic individual b, or a parameterized message nB(x)\u2192fa . The only question is how to handle the N \u2212M messages nB(x)\u2192fa at once, without actually writing them all. Note that nB(x)\u2192fa is actually equal to nB(b)\u2192fa , because any particular x behaves like b. So, we compute the message for b and just use the result for the remaining elements represented by x (this is the reason to keep an individual b separately). As LBP takes these N \u2212 M \u2212 1 messages related to x to be independent, and we can use this approximation to compute\n5In our implementation we decompose each such restriction into conjunction/disjunction of two pieces: one is entirely related to a and its factors are kept within fa; the other is related to other elements of the domain and is kept outside of fa. This increases the cluster related to fa, improving accuracy and performance.\nin closed-form the effect of these messages. A simple example: Take (\u2200r.B)(a); we need P (\u2227x r(a, x) \u2192 B(x)), approximated by (1 \u2212 P (r(a, x)) (1 \u2212 nB(b)\u2192fa))N\u2212M (the exponent N \u2212 M is needed because the calculation stands for this many elements of the domain). In short, the message mfa\u2192A(a) is the result of probabilistic inference in Ba with a particular set of distributions for extraneous nodes. Similar reasoning yields messages mfb\u2192A(b) and mfa\u2192A(b). Messages nA(a)\u2192fa can be interpreted as P (A(a),Ea)\u00d7 \u220f f \u2208 N(A(a)\\fa P (Eb|A(a)), where terms are computed in Ba. The point is that messages can be computed as inferences in appropriate Bayesian networks.\nAs factor fa and its incoming messages represent a Bayesian network in each iteration, we can use recent algorithms that exploit determinism within probabilistic inference [6, 12, 35]. The use of larger regions leads to improvements in accuracy compared to LBP, while the use of logical inference within regions leads to gains in speed. Of course, it may happen that a particular problem has a tnetwork so complex that Bayesian network inference fails even for N = 1. In this case the method can be applied by breaking factors more finely; that is, by selecting smaller regions for the propagation [41]."}, {"heading": "4.2 Experiments under the Bayesian assumption", "text": "We now describe experiments with the proposed inference algorithm. We have used the ACE engine for the probabilistic calculations as it can exploit logical inference [6], running in a dual core Pentium 2GHz with 2GBytes of memory. As we have indicated, the current technology on exact probabilistic inference can only deal with small N ; it does not seem that an extensive comparison between approximations and exact results is possible at the moment. We analyze a few examples that illustrate well our method.\nIn the following tables we present results of exact inference, then LBP, then our proposed method. Exact inference always produced results within two seconds but failed for N > 9 due to memory exhaustion. The results of LBP (on the grounded network) and parameterized LBP are identical, but their running times are different. We only present results in cases where (grounded) LBP converged in less than 45 minutes. Overall, paremeterized LBP runs in milliseconds and is slightly slower (about 15%) than our proposed method. For the latter we present the result of the inference and the running time in milliseconds (respectively the last two rows of the tables). We always initialize messages with the results of probabilistic inference in a grounded Bayesian network for N = 1.\nFirst, we present inferences P (C(a0)) for Example 1. Note first that the proposed method is more accurate than LBP. It is also remarkable that the proposed method always converged within two iterations, while the number of iterations for LBP varied but was always much larger. We have:\nN 1 5 9 50 200 500 Exact: 0.5535 0.8445 0.9210 \u2014 \u2014 \u2014 LBP: 0.5781 0.8558 0.9421 0.9798 \u2014 \u2014\nProposed: 0.5335 0.8445 0.9285 0.9739 0.4723 0.4050 Runtime(ms): 32 30 30 29 29 29\nOur next experiment uses the probabilistic Kangaroo ontology (Section 1), as this ontology leads to very dense grounded networks. We compute P (Parent(a0)) (again, convergence with LBP took dozens of iterations, while our proposed method always converged almost instantly):\nN 1 5 9 20 50 200 Exact: 0.1620 0.3536 0.4481 \u2014 \u2014 \u2014 LBP: 0.0875 0.3196 0.4299 0.5243 \u2014 \u2014\nProposed: 0.1620 0.3536 0.4481 0.5268 0.5399 0.5400 Runtime(ms): 32 32 34 30 36 32\nWe finish with a larger terminology containing substantial deterministic information. We have randomly generated a terminology whose t-network has 20 nodes, 15 of which are associated with deterministic definitions and 2 others are associated with restrictions. We compute inference P (A(a0)) where A(a0) is the node whose inference requires most computation:\nN 1 5 9 20 50 200 Exact: 0.7240 0.7544 0.7694 \u2014 \u2014 \u2014 LBP: 0.6452 0.6991 0.7257 0.7479 \u2014 \u2014\nProposed: 0.7240 0.7544 0.7694 0.7819 0.7840 0.7840 Runtime(ms): 68 71 69 68 74 70\nTo conclude this section, we note that inferences such as P (A(a0)|A) = \u03b1 are not the only ones our method can produce. To illustrate this, consider the completely different question, \u201cWhat is the tighest interval [\u03b1, \u03b1] such that\n\u2200x \u2208 D : P (A(x)|A) \u2208 [\u03b1, \u03b1]?\u2032\u2032.\nWe answer this question by collecting probabilities across the domain P (A(x)|A) after running inference. For instance, consider Example 1 with N = 9 and evidence A = {\u00acC(a0),\u00acD(a1), B(a2),\u00acB(a3)}. We then obtain \u2200x \u2208 D : P (A(x)|A) \u2208 [0.598, 1]."}, {"heading": "4.3 Dropping assumptions", "text": "In this very brief section we comment on failures of the Bayesian assumption. Suppose we have the uniqueness assumption but N is countably infinite, thus failing the confined-domain assumption. As already remarked in Section 3, results by Jaeger [23] show that a unique joint distribution exists in this case. To obtain approximate inferences for N = \u221e, run the propagation method with increasing values of N : there will be a point where probabilities raised to N \u2212 M become smaller than machine precision, and at that point we reach approximations valid for N = \u221e. For instance, for the network of Example 1, all messages related to quantifiers are deterministic for N = 500, so the approximate value for P (C(a0)) is\n0.4050 for N = \u221e. This is a nice result because in this example one can show that P (C(a0)) is exactly 0.4050 for N = \u221e [8]. Note that it would be misleading to take the result for N = 50 as an approximation for the case N = \u221e: while the correct value is P (C(a0)) = 0.4050, LBP produces P (C(a0)) = 0.9798 for N = 50.\nWhen N is unconstrained we may have different inferences for varying N : uniqueness of probabilities is not guaranteed. Thus it is reasonable to analyze unconstrained N together with failure of the uniqueness assumption. We should expect most applications to stay within uniqueness, but there are several reasons why uniqueness may fail in a probabilistic description logics. One reason is that a standard inclusion C v D does imply P (C|\u00acD) = 0, but nothing is implied about P (C|D). Another reason is that a precise assessment such as P (C|A tB) = \u03b1 does not necessarily constrain probabilities such as P (C|A uB) down to a single value, so uniqueness may fail. A version of LBP that handles probability intervals can be used when uniqueness fails [8]. This version of LBP, called L2U [20] is similar to LBP but it propagates the interval-valued messages derived in the 2U algorithm [15]. We have only two observations to make in this setting. First, as handling unconstrained N is akin to handling probability intervals, messages that are propagated must only be adapted by conducting, for each message, a minimization and a maximization with respect to N . Second, we emphasize that our method allows each slice to be processed in isolation, and specific methods for inference with probability intervals can be used inside such factors [9].\nTo illustrate these techniques, consider again the terminology in Example 1. Suppose we discard the assessment P (B|A) = \u03b12, leaving only the standard inclusion B v A for B. By propagating messages with N = 10, we obtain P (C(a0)) \u2208 [0.9179, 0.9917]. If we only impose that N \u2264 20, leaving N otherwise unconstrained, we obtain P (C(a0)) \u2208 [0.2910, 0.9831]."}, {"heading": "5 Conclusion", "text": "This paper has contributed with novel complexity analysis and algorithms for inference in a family of probabilistic description logics with interpretation-based semantics and assumptions of acyclicity and independence. Variants of Proposition 1 and Theorem 1 should apply to other relational/first-order probabilistic models [16, 34]; we note that currently there are relatively few results on complexity for these models. Also, our analysis produces a meaningful class of PEXP-complete problems that may be useful to other applications.\nConcerning algorithms, the contributions here center around a variational scheme that, essentially, breaks the shattered network into smaller units, so as to employ logical inference whenever possible. This idea should be useful\nto other probabilistic logics. Experiments demonstrate that our techniques are better than existing ones and in fact can handle problems of practical significance. The method can be adapted to handle infinite and unconstrained domains, as well as situations where uniqueness is violated. Clearly, many improvements and extensions can be contemplated; in particular, most of this paper focuses on the Bayesian assumption, and more work is to be devoted to situations where this assumption fails.\nIn this paper we have increased substantially the expressivity of CRALC, while keeping intact both the worst-case complexity and the basic variational method. We feel justified in starting from CRALC as it seems to be a minimal starting point for an interpretation-based probabilistic description logic, as noted in Section 3. A challenge for the future is to include transitivity and cyclic definitions; these features may require a move to undirected t-networks (alas, the usual Markov condition for undirected graphs does not guarantee factorization when deterministic constraints are present [30], so the move to undirected graphs may require substantially new ideas). Other extensions would be desirable, such as unrestricted nominals, but they appear to be quite challenging. As practical applications should be valuable in indicating which constructs are really useful, we plan to devote some effort to applications before we attempt to study all of these extensions."}, {"heading": "Acknowledgements", "text": "Thanks to Cassio Polpo de Campos for discussions on computational complexity. This work was partially funded by FAPESP (grants 04/09568-0 and 08/03995-5); the first author is partially supported by CNPq and the second author is supported by FAPESP."}, {"heading": "A Proof of Theorem 1", "text": "Membership: Propositionalize the terminology into an exponentially large Bayesian network and run inference (a PP-complete problem in the exponentially large network). Hardness: We resort to bounded domino problems. A domino system consists of a finite set D of tiles and a pair of compatibility relations on D \u00d7D, respectively expressing horizontal and vertical constraints between tiles. Some tiles, the initial conditions, are assigned to a torus U(s, t), and the torus has a tiling if it is possible to cover the whole torus while respecting the constraints and the initial conditions. Bo\u0308rger et al [4, Thm. 6.1.2] show that given a (time/space) bounded Turing machine one can construct a bounded domino system that reproduces its behavior. However their reduction is not parsimonious [32, Sec. 18.1] as the number of accepting paths and the number of tilings may differ. A parsimonious reduction can be constructed by enlarging the original bounded Turing machine into a\nmachine that visits every cell in its tape (within the space bound) and that reaches the final accepting state only after a prescribed number of operations (by counting operations via auxiliary counters). Consider then a Turing machine solving some selected NEXP-complete problem of size O(n) in such a way that its translation into a domino system involves a torus of size 2n \u00d7 2n. We wish to encode this domino system using a CRALC terminology. Tobies shows how to encode such a torus U(2n, 2n) with ALCQ plus several cardinality constraints [39]. We can adapt his construction to ALC with a single cardinality constraint N = 2n \u00d7 2n. To do so, we take the definitions of concepts C0,0, Deast and Dnorth exactly as in Tobies\u2019 work; additionally, the constraints (\u2203C0,0), (\u2200Deast), (\u2200Dnorth), (\u2200(\u2203east.>)), (\u2200(\u2203north.>)), where: > denotes A t \u00acA for some A not in the terminology; (\u2203C) denotes \u2203x \u2208 D : C(x); (\u2200C) denotes \u2200x \u2208 D : C(x). Using these constraints it is possible to construct an isomorphism between any model of the terminology and a torus U(2n, 2n) (by Tobies\u2019 finite induction argument plus the fact that the constraint on N forces a single element to be associated with each point in the torus [39, pp. 205-206]).\nThe key insight now is to \u201csimulate\u201d constraints such as (\u2200C) and (\u2203C) using probabilities. To do so, assign probability 1/2 to every free concept/role in Tobies\u2019s construction (P (Xi) = P (Yi) = P (east) = P (north) = 1/2) and introduce a new role r and assessment P (r) = 1. The probability P (C \u2032(a0)) for a concept C \u2032 \u2261 \u2200r.C u \u2203r.C0,0, where C is the conjunction Deast uDnorth u (\u2203east.>) u (\u2203north.>), is the probability that a torus is built under the probabilistic assessments. We must still encode the compatibility relations and the initial conditions on the torus; this is done exactly as in Tobies\u2019 construction, by using concepts Cd, Ci,0 and associated definitions and constraints of the form (\u2200C). We again simulate these latter constraints probabilistically: introduce P (Cd) = 1/2 for all Cd, define C\u0302 \u2261 C \u2032 u \u2200r.C \u2032\u2032 where C \u2032\u2032 is the conjunction of all concepts used in the compatibility constraints and initial conditions. Now \u03b3 .= P (C\u0302(a0)) is the probability that a torus satisfying all conditions is built. If we can recover the number of tilings of the torus from \u03b3, we obtain the number of accepting computations of the original exponentially-bounded Turing machine. Note that \u03b3 \u00d7 2\u03b4 is the number of truth assignments that build the torus satisfying horizontal and vertical relations and initial conditions, where \u03b4 is the number of logically independent random variables in the grounding of the terminology (we have \u03b4 = 22N (2N +22N+1 + |D|)). This number is not equal to the number of tilings of the torus; to produce the number of tilings of the torus, we must compute \u03b3 \u00d7 2\u03b4/22n!, where we divide the number of satisfying truth assignments by the number of repeated tilings. Consequently we obtain the number of accepting computations of the original Turing machine just by processing the inference P (C\u0302(a0)). This shows that INFB(Q) is PEXP-hard.2"}], "references": [{"title": "Description Logic Handbook", "author": ["F. Baader", "D. Calvanese", "D.L. McGuinness", "D. Nardi", "P.F. Patel-Schneider"], "venue": "Cambridge University Press,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Terminological cycles in a description logic with existential restrictions", "author": ["F. Baader"], "venue": "Int. Joint Conf. on AI, pages 325\u2013 330,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Representing and Reasoning with Probabilistic Knowledge: A Logical Approach", "author": ["F. Bacchus"], "venue": "MIT Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "The Classical Decision Problem", "author": ["E. B\u00f6rger", "E. Gr\u00e4del", "Y. Gurevich"], "venue": "Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonrelativizing separations", "author": ["H. Buhrman", "L. Fortnow", "T. Thierauf"], "venue": "Proc. of IEEE Complexity, pages 8\u201312,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artificial Intelligence, 172(6- 7):772\u2013799,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "PR-OWL: A framework for probabilistic ontologies", "author": ["P.C.G. Costa", "K.B. Laskey"], "venue": "Conf. on Formal Ontology in Information Systems,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Loopy propagation in a probabilistic description logic", "author": ["F.G. Cozman", "R.B. Polastro"], "venue": "Int. Conf. on Scalable Uncertainty Management (LNAI 5291), pages 120\u2013133. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "The inferential complexity of Bayesian and credal networks", "author": ["C. Polpo de Campos", "F.G. Cozman"], "venue": "Int. Joint Conf. on AI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Tractable reasoning with Bayesian description logics", "author": ["C. D\u2019Amato", "N. Fanizzi", "T. Lukasiewicz"], "venue": "Int. Conf. on Scalable Uncertainty Management (LNAI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "Int. Joint Conf. on AI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "AND/OR search spaces for graphical models. Artificial Intelligence,171:73\u2013106,2007", "author": ["R. Dechter", "R. Mateescu"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "BayesOWL: Uncertainty modeling in semantic web ontologies", "author": ["Z. Ding", "Y. Peng", "R. Pan"], "venue": "Soft Computing in Ontologies and Semantic Web, pages 3\u201329. Springer, Berlin/Heidelberg,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic ABox reasoning: preliminary results. Description Logics,pages104\u2013111,2005", "author": ["M. D\u00fcrig", "T. Studer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "2U: An exact interval propagation algorithm for polytrees with binary variables", "author": ["E. Fagiuoli", "M. Zaffalon"], "venue": "Artificial Intelligence, 106(1):77\u2013107,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Introduction to Statistical Relational Learning", "author": ["L. Getoor", "B. Taskar"], "venue": "MIT Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Reasoning about Uncertainty", "author": ["J.Y. Halpern"], "venue": "MIT Press, Cambridge, Massachusetts,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Probabilistic description logics", "author": ["J. Heinsohn"], "venue": "Conf. Uncertainty in AI, page 311-318,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "From SHIQ and RDF to OWL: The making of a web ontology language", "author": ["I. Horrocks", "P.F. Patel-Schneider", "F. van Harmelen"], "venue": "Journal of Web Semantics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Approximate algorithms for credal networks with binary variables", "author": ["J.S. Ide", "F.G. Cozman"], "venue": "Int. Journal of Approximate Reasoning, 48(1):275\u2013296,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic reasoning in terminological logics", "author": ["M. Jaeger"], "venue": "Principles of Knowledge Representation, pages 461\u2013 472,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1994}, {"title": "Relational Bayesian networks", "author": ["M. Jaeger"], "venue": "Conf. Uncertainty in AI, pages 266\u2013273, San Francisco, California,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Reasoning about infinite random structures with relational Bayesian networks", "author": ["M. Jaeger"], "venue": "Knowledge Representation, San Francisco, California,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "On the complexity of inference about probabilistic relational models", "author": ["M. Jaeger"], "venue": "Artificial Intelligence, 117(2):297\u2013 308,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "P-CLASSIC: A tractable probablistic description logic", "author": ["D. Koller", "A.Y. Levy", "A. Pfeffer"], "venue": "AAAI Conf. on AI, pages 390\u2013397,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "and C", "author": ["H.E. Kyburg Jr"], "venue": "M. Teng. Uncertain Inference. Cambridge University Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic Boolean satisfiability", "author": ["M.L. Littman", "S.M. Majercik", "T. Pitassi"], "venue": "Journal of Automated Reasoning, 27(3):251\u2013296,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Expressive probabilistic description logics", "author": ["T. Lukasiewicz"], "venue": "Artificial Intelligence, 172(6-7):852\u2013883, April", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Managing uncertainty and vagueness in description logics for the semantic web", "author": ["T. Lukasiewicz", "U. Straccia"], "venue": "Journal of Web Semantics, 6(4):291\u2013308, November", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Gibbs and Markov random systems with constraints", "author": ["J. Moussouris"], "venue": "Journal of Statistical Physics,10(1):11\u201333,1974", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1974}, {"title": "Games against nature (extended abstract)", "author": ["C.H. Papadimitriou"], "venue": "24th Annual Symposium on Foundations of Computer Science, pages 446\u2013450,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1983}, {"title": "Computational Complexity", "author": ["C.H. Papadimitriou"], "venue": "Addison- Wesley Publishing,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Inference in probabilistic ontologies with attributive concept descriptions and nominals", "author": ["R.B. Polastro", "F.G. Cozman"], "venue": "4th Int. Workshop on Uncertainty Reasoning for the Semantic Web, Karlsruhe, Germany,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Logical and Relational Learning", "author": ["L. De Raedt"], "venue": "Springer,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Solving Bayesian networks by weighted model counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "AAAI Conf. on AI, Pittsburgh, PA,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Attributive concept descriptions with complements", "author": ["M. Schmidt-Schauss", "G. Smolka"], "venue": "Artificial Intelligence, 48:1\u2013 26,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1991}, {"title": "A probabilistic terminological logic for modelling information retrieval", "author": ["F. Sebastiani"], "venue": "17th Conf. on Research and Development in Information Retrieval, pages 122\u2013130, Dublin, Ireland,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1994}, {"title": "Lifted first-order belief propagation", "author": ["P. Singla", "P. Domingos"], "venue": "AAAI Conf. on AI,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "The complexity of reasoning with cardinality restrictions and nominals in expressive description logics", "author": ["S. Tobies"], "venue": "Journal of Artificial Intelligence Research, 12:199\u2013217,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Languages represented by Boolean formulas", "author": ["H. Veith"], "venue": "Information Processing Letters, pages 251\u2013256,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1997}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory, 51:2282\u20132312,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "In ALC one deals with individuals, concepts, roles, Boolean operators, and restricted forms of quantification [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "We start from credalALC, a probabilistic description logic we have introduced previously [8, 33].", "startOffset": 89, "endOffset": 96}, {"referenceID": 32, "context": "We start from credalALC, a probabilistic description logic we have introduced previously [8, 33].", "startOffset": 89, "endOffset": 96}, {"referenceID": 32, "context": "1 Consider a probabilistic version as follows [33]: P (Animal)=0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "The combination of probability and logic has a long history, with much recent activity [16, 17, 34].", "startOffset": 87, "endOffset": 99}, {"referenceID": 16, "context": "The combination of probability and logic has a long history, with much recent activity [16, 17, 34].", "startOffset": 87, "endOffset": 99}, {"referenceID": 33, "context": "The combination of probability and logic has a long history, with much recent activity [16, 17, 34].", "startOffset": 87, "endOffset": 99}, {"referenceID": 28, "context": "there has been significant interest in probabilistic description logics [29].", "startOffset": 72, "endOffset": 76}, {"referenceID": 12, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 51, "endOffset": 75}, {"referenceID": 13, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 51, "endOffset": 75}, {"referenceID": 17, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 51, "endOffset": 75}, {"referenceID": 20, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 51, "endOffset": 75}, {"referenceID": 24, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 51, "endOffset": 75}, {"referenceID": 27, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 51, "endOffset": 75}, {"referenceID": 6, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 110, "endOffset": 117}, {"referenceID": 36, "context": "that assign probabilities to subsets of the domain [13, 14, 18, 21, 25, 28] and to subsets of interpretations [7, 10], with some logics in between [37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "This is indeed the old philosophical problem of direct inference [26].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "Hence logics with domainbased semantics either do not allow probabilities of assertions to be expressed, or resort to non-standard forms of entailment [21, 28].", "startOffset": 151, "endOffset": 159}, {"referenceID": 27, "context": "Hence logics with domainbased semantics either do not allow probabilities of assertions to be expressed, or resort to non-standard forms of entailment [21, 28].", "startOffset": 151, "endOffset": 159}, {"referenceID": 6, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 135, "endOffset": 146}, {"referenceID": 12, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 135, "endOffset": 146}, {"referenceID": 24, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 135, "endOffset": 146}, {"referenceID": 13, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 204, "endOffset": 220}, {"referenceID": 17, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 204, "endOffset": 220}, {"referenceID": 20, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 204, "endOffset": 220}, {"referenceID": 27, "context": "We can alternatively divide probabilistic description logics into logics that allow independence relations to be organized into graphs [7, 13, 25], and logics that do not resort to independence relations [14, 18, 21, 28].", "startOffset": 204, "endOffset": 220}, {"referenceID": 6, "context": "In particular, CRALC shares many features with PR-OWL [7] as both have interpretation-based semantics and use graph-theoretical tools.", "startOffset": 54, "endOffset": 57}, {"referenceID": 35, "context": "We now turn to a more precise description of CRALC, starting with background onALC [36].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "There are translations of these constructs into modal and first-order logic [1]; we often treat a concept C as a unary predicate C(x), a role r as a binary predicate r(x, y), and restrictions as quantifiers.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "As usual in interpretation-based probabilistic logics [3], CRALC requires that all individuals be rigid (an individ-", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "Similarly to other probabilistic description logics [7, 13, 25], CRALC adopts a (two-part) Markov condition that is best formulated using indicator functions.", "startOffset": 52, "endOffset": 63}, {"referenceID": 12, "context": "Similarly to other probabilistic description logics [7, 13, 25], CRALC adopts a (two-part) Markov condition that is best formulated using indicator functions.", "startOffset": 52, "endOffset": 63}, {"referenceID": 24, "context": "Similarly to other probabilistic description logics [7, 13, 25], CRALC adopts a (two-part) Markov condition that is best formulated using indicator functions.", "startOffset": 52, "endOffset": 63}, {"referenceID": 7, "context": "Example 1 (from [8]) Consider terminology Tu: P (A) = 0.", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "Under these three assumptions, collectively referred to as the Bayesian assumption, every CRALC terminology defines a unique relational Bayesian network [22] whose grounding is a Bayesian network given by Expression (4).", "startOffset": 153, "endOffset": 157}, {"referenceID": 31, "context": "A language L is in class PP if there is a nondeterministic Turing machine M such that x \u2208 L if and only if more than half of the computations of M on input x end up accepting, when M has a polynomial-time bound [32]; L is in PPSPACE if the same definition is used but we replace polynomial-time by polynomial-space [31]; and L is in PEXP if the same definition is used but we replace polynomial-time by exponential-time [5].", "startOffset": 211, "endOffset": 215}, {"referenceID": 30, "context": "A language L is in class PP if there is a nondeterministic Turing machine M such that x \u2208 L if and only if more than half of the computations of M on input x end up accepting, when M has a polynomial-time bound [32]; L is in PPSPACE if the same definition is used but we replace polynomial-time by polynomial-space [31]; and L is in PEXP if the same definition is used but we replace polynomial-time by exponential-time [5].", "startOffset": 315, "endOffset": 319}, {"referenceID": 4, "context": "A language L is in class PP if there is a nondeterministic Turing machine M such that x \u2208 L if and only if more than half of the computations of M on input x end up accepting, when M has a polynomial-time bound [32]; L is in PPSPACE if the same definition is used but we replace polynomial-time by polynomial-space [31]; and L is in PEXP if the same definition is used but we replace polynomial-time by exponential-time [5].", "startOffset": 420, "endOffset": 423}, {"referenceID": 0, "context": "The assumption that numeric parameters are coded in unary is common in description logic research [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 26, "context": "Hardness: for N = 1 the inference is PP-complete (Bayesian network inference) [27].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "One the other hand, as Jaeger\u2019s important previous analysis [24] indicates that (unless ETIME=NETIME) there must be model representation systems for which inference is not in P with respect to N in unary, one might suspect that inference with N in binary should take us to exponential time complexity of some sort.", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "might try to reduce the complexity of INFB(Q) by starting with a description logic simpler thanALC, for instance by discarding some operators and negation [2].", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "be reconstructed if we restrict CRALC even to the logical constructs of the simple logic EL (conjunction and existential quantification) [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 18, "context": "This logic contains most of SHOIQ, a logical basis for the OWL language [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "By retaining the confined-domain assumption and dropping the uniqueness assumption, the decision problem \u201cmin Q > 1/2?\u201d belongs to NEXPPEXP, as there are exponentially many choices of probabilities (only the vertices of the sets of distributions can generate minimizing/maximizing probabilities [15], and there are exponentially many vertices) and for each one of these choices, an oracle in PEXP yields the answer.", "startOffset": 295, "endOffset": 299}, {"referenceID": 7, "context": "One might adopt an homogeneity assumption [8] prescribing that the selection of a probability (within a probability interval) should be constant across individuals; this assumption moves the decision problem to NPPEXP, as there are polynomially many choices concerning probabilities, followed by an oracle in PEXP.", "startOffset": 42, "endOffset": 45}, {"referenceID": 22, "context": "Using results by Jaeger [23], we know that CRALC has a 0/1-law such that the probability of every restriction goes to 0 or 1 as N grows without bound [8]; if we could determine the limiting value of probabilities for quantifiers, then the grounded network would decompose into a set of independent Bayesian networks and inference would be PPcomplete.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "Using results by Jaeger [23], we know that CRALC has a 0/1-law such that the probability of every restriction goes to 0 or 1 as N grows without bound [8]; if we could determine the limiting value of probabilities for quantifiers, then the grounded network would decompose into a set of independent Bayesian networks and inference would be PPcomplete.", "startOffset": 150, "endOffset": 153}, {"referenceID": 22, "context": "Of course, determining the limiting values for restrictions is not an easy matter [23], so our point is just to describe it as a difficult open problem.", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "The network with a slice per named individual ai and an additional slice for a \u201cgeneric\u201d element x \u2208 D\\D\u2032 is called the shattered network [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 10, "context": "This network is a representation for the shattering operation in first-order variable elimination [11]; the relationship between shattering and shattered networks is given by [8, Thm.", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "(A different scheme for inference in CRALC with restricted nominals can be found in [33].", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "Indeed, even the state-of-art inference engine ACE [6] fails to handle the grounded network of Example 1 for N = 10.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "previously suggested the use of Loopy Belief Propagation (LBP) for approximate inference in large grounded networks [8, 33].", "startOffset": 116, "endOffset": 123}, {"referenceID": 32, "context": "previously suggested the use of Loopy Belief Propagation (LBP) for approximate inference in large grounded networks [8, 33].", "startOffset": 116, "endOffset": 123}, {"referenceID": 37, "context": "sulting parameterized LBP is similar to the lifted propagation scheme of Singla and Domingos [38].", "startOffset": 93, "endOffset": 97}, {"referenceID": 40, "context": "The natural strategy is to examine variational methods that improve LBP by processing regions (sets of nodes) [41].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "As factor fa and its incoming messages represent a Bayesian network in each iteration, we can use recent algorithms that exploit determinism within probabilistic inference [6, 12, 35].", "startOffset": 172, "endOffset": 183}, {"referenceID": 11, "context": "As factor fa and its incoming messages represent a Bayesian network in each iteration, we can use recent algorithms that exploit determinism within probabilistic inference [6, 12, 35].", "startOffset": 172, "endOffset": 183}, {"referenceID": 34, "context": "As factor fa and its incoming messages represent a Bayesian network in each iteration, we can use recent algorithms that exploit determinism within probabilistic inference [6, 12, 35].", "startOffset": 172, "endOffset": 183}, {"referenceID": 40, "context": "In this case the method can be applied by breaking factors more finely; that is, by selecting smaller regions for the propagation [41].", "startOffset": 130, "endOffset": 134}, {"referenceID": 5, "context": "We have used the ACE engine for the probabilistic calculations as it can exploit logical inference [6], running in a dual core Pentium 2GHz with 2GBytes of memory.", "startOffset": 99, "endOffset": 102}, {"referenceID": 22, "context": "As already remarked in Section 3, results by Jaeger [23] show that a unique joint distribution exists in this case.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "4050 for N = \u221e [8].", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "A version of LBP that handles probability intervals can be used when uniqueness fails [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 19, "context": "This version of LBP, called L2U [20] is similar to LBP but it propagates the interval-valued messages derived in the 2U algorithm [15].", "startOffset": 32, "endOffset": 36}, {"referenceID": 14, "context": "This version of LBP, called L2U [20] is similar to LBP but it propagates the interval-valued messages derived in the 2U algorithm [15].", "startOffset": 130, "endOffset": 134}, {"referenceID": 8, "context": "Second, we emphasize that our method allows each slice to be processed in isolation, and specific methods for inference with probability intervals can be used inside such factors [9].", "startOffset": 179, "endOffset": 182}, {"referenceID": 15, "context": "Variants of Proposition 1 and Theorem 1 should apply to other relational/first-order probabilistic models [16, 34]; we note that currently there are relatively few results on complexity for these models.", "startOffset": 106, "endOffset": 114}, {"referenceID": 33, "context": "Variants of Proposition 1 and Theorem 1 should apply to other relational/first-order probabilistic models [16, 34]; we note that currently there are relatively few results on complexity for these models.", "startOffset": 106, "endOffset": 114}, {"referenceID": 29, "context": "A challenge for the future is to include transitivity and cyclic definitions; these features may require a move to undirected t-networks (alas, the usual Markov condition for undirected graphs does not guarantee factorization when deterministic constraints are present [30], so the move to undirected graphs may require substantially new ideas).", "startOffset": 269, "endOffset": 273}, {"referenceID": 38, "context": "Tobies shows how to encode such a torus U(2, 2) with ALCQ plus several cardinality constraints [39].", "startOffset": 95, "endOffset": 99}], "year": 2009, "abstractText": "This paper presents complexity analysis and variational methods for inference in probabilistic description logics featuring Boolean operators, quantification, qualified number restrictions, nominals, inverse roles and role hierarchies. Inference is shown to be PEXP-complete, and variational methods are designed so as to exploit logical inference whenever possible.", "creator": " TeX output 2009.05.26:1020"}}}