{"id": "1205.2655", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Mean Field Variational Approximation for Continuous-Time Bayesian Networks", "abstract": "continuous - time bayesian networks operator is a natural elementary structured representation language for multicomponent stochastic processes that evolve continuously smoothly over time. despite the compact representation, operator inference in most such models is intractable even in relatively large simple structured networks. here we introduce a mean field variational approximation in which we use a product of continuously inhomogeneous markov processes to approximate a distribution over trajectories. this generalized variational approach leads to a globally consistent distribution, which can be efficiently queried. additionally, it provides a rather lower bound on the probability significance of observations, thus making it attractive for learning tasks. we furthermore provide the theoretical modeling foundations for the approximation, an efficient implementation that exploits the wide wide range of highly optimized ordinary differential equations ( ode ) solvers, experimentally explore geometric characterizations representations of processes for which this parameter approximation is suitable, and show applications to a large - scale realworld stability inference problem.", "histories": [["v1", "Wed, 9 May 2012 14:57:02 GMT  (1353kb)", "http://arxiv.org/abs/1205.2655v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ido cohn", "tal el-hay", "nir friedman", "raz kupferman"], "accepted": false, "id": "1205.2655"}, "pdf": {"name": "1205.2655.pdf", "metadata": {"source": "CRF", "title": "Mean Field Variational Approximation for Continuous-Time Bayesian Networks", "authors": ["Ido Cohn"], "emails": ["cohn@cs.huji.ac.il", "tale@cs.huji.ac.il", "nir@cs.huji.ac.il", "raz@math.huji.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Many real-life processes can be naturally thought of as evolving continuously in time. Examples cover a diverse range, including server availability, changes in socioeconomic status, and genetic sequence evolution. To realistically model such processes, we need to reason about systems that are composed of multiple components (e.g., many servers in a server farm, multiple residues in a protein sequence) and evolve in continuous time. Continuous-time Bayesian networks (CTBNs) provide a representation language for such processes, which allows to naturally exploit sparse patterns of interactions to compactly represent the dynamics of such processes [9].\nInference in multi-component temporal models is a notoriously hard problem [1]. Similar to the situation in dis-\ncrete time processes, inference is exponential in the number of components, even in a CTBN with sparse interactions [9]. Thus, we have to resort to approximate inference methods. The recent literature has adapted several strategies from discrete graphical models to CTBNs. These include sampling-based approaches, where Fan and Shelton [5] introduced a likelihood-weighted sampling scheme, and more recently we [4] introduced a Gibbs-sampling procedure. Such sampling-based approaches yield more accurate answers with the investment of additional computation. However, it is hard to bound the required time in advance, tune the stopping criteria, or estimate the error of the approximation. An alternative class of approximations is based on variational principles.\nRecently, Nodelman et al. [11] introduced an Expectation Propagation approach, which can be roughly described as a local message passing scheme, where each message describes the dynamics of a single component over an interval. This message passing procedure can automatically refine the number of intervals according to the complexity of the underlying system [14]. Nonetheless, it does suffer from several caveats. On the formal level, the approximation has no convergence guaranties. Second, upon convergence, the computed marginals do not necessarily form a globally consistent distribution. Third, it is restricted to approximations in the form of piecewisehomogeneous messages on each interval. Thus, the refinement of the number of intervals depends on the fit of such homogeneous approximations to the target process. Finally, the approximation of Nodelman et al does not provide a provable approximation on the likelihood of the observation\u2014a crucial component in learning procedures.\nHere, we develop an alternative variational approximation, which provides a different trade-off. We use the strategy of structured variational approximations in graphical models [8], and specifically by the variational approach of Opper and Sanguinetti [12] for approximate inference in Markov Jump Processes, a related class of models (see below). The resulting procedure approximates the posterior distribution of the CTBN as a product of independent components, each of which is an inhomogeneous continuous-\ntime Markov process. As we show, by using a natural representation of these processes, we derive a variational procedure that is both efficient, and provides a good approximation both for the likelihood of the evidence and for the expected sufficient statistics. In particular, the approximation provides a lower-bound on the likelihood, and thus is attractive for use in learning."}, {"heading": "2 Continuous-Time Bayesian Networks", "text": "Consider a D-component Markov process X(t) = (X(t)1 , X (t) 2 , . . . X (t) D ) with state space S = S1\u00d7S2\u00d7\u00b7 \u00b7 \u00b7\u00d7 SD. A notational convention: vectors are denoted by boldface symbols, e.g., X , and matrices are denoted by blackboard style characters, e.g., Q. The states in S are denoted by vectors of indexes, x = (x1, . . . , xD). We use indexes 1 \u2264 i, j \u2264 D for enumerating components and X(t) and X\n(t) i to denote the random variable describing the state of the process and its i\u2019th components at time t. The dynamics of a time-homogeneous continuous-time Markov process are fully determined by the Markov transition function,\npx,y(t) = Pr(X(t+s) = y|X(s) = x),\nwhere time-homogeneity implies that the right-hand side does not depend on s. These dynamics are fully captured by a matrix Q\u2014the rate matrix with non-negative offdiagonal entries qx,y and diagonal qx,x = \u2212 \u2211 y 6=x qx,y . This rate matrix defines the transition probabilities\npx,y(h) = \u03b4x,y + qx,y \u00b7 h+ o(h)\nwhere \u03b4x,y is a multivariate Kronecker delta and o(\u00b7) means decay to zero faster than its argument. Using the rate matrix Q, we can express the Markov transition function as px,y(t) = [exp(tQ)]x,y where exp(tQ) is a matrix exponential [2, 7].\nA continuous-time Bayesian network is defined by assigning each component i a set of components Pai \u2286 {1, . . . , D} \\ {i}, which are its parents in the network [9]. With each component i we then associate a set of conditional rate matrix Qi|Pai\u00b7|ui for each state ui of Pai. The off-diagonal entries qi|Paixi,yi|ui represent the rate at which Xi transitions from state xi to state yi given that its parents are in state ui. The dynamics of X(t) are defined by a rate matrix Q with entries qx,y , which amalgamates the conditional rate matrices as follows:\nqx,y =  q i|Pai xi,yi|ui \u03b4(x,y) = {i}\u2211 i q i|Pai xi,xi|ui x = y\n0 otherwise,\n(1)\nwhere \u03b4(x,y) = {i|xi 6= yi}. This definition implies that changes are one component at a time.\nGiven a continuous-time Bayesian network, we would like to evaluate the likelihood of evidence, to compute the\nprobability of various events given the evidence (e.g., that the state of the system at time t is x), and to compute conditional expectations (e.g., the expected amount of time Xi was in state xi). Direct computations of these quantities involve matrix exponentials of the rate matrix Q, whose size is exponential in the number of components, making this approach infeasible beyond a modest number of components. We therefore have to resort to approximations."}, {"heading": "3 Variational Principle for Continuous Time Markov Processes", "text": "We start by defining a variational approximations principle in terms of a general continuous-time Markov process (that is, without assuming any network structure). For convenience we restrict our treatment to a time interval [0, T ] with end-point evidence X(0) = e0 and X(T ) = eT . We discuss more general types of evidence below. Here we aim to define a lower bound on lnPQ(eT |e0) as well as to approximate the posterior probability PQ(\u00b7 | e0, eT ).\nMarginal Density Representation Variational approximations cast inference as an optimization problem of a functional which approximates the log probability of the evidence by introducing an auxiliary set of variational parameters. Here we define the optimization problem over a set of mean parameters [15], representing possible values of expected sufficient statistics.\nAs discussed above, the prior distribution of the process can be characterized by a time-independent rate matrix Q. It is easy to show that if the prior is a Markov process, then the posterior is also a Markov process, albeit not necessarily a homogeneous one. Such a process can be represented by a time-dependent rate matrix that describes the instantaneous transition rates. Here, rather than representing the target distribution by a time-dependent rate matrix, we consider a representation that is more natural for variational approximations. Let Pr be the distribution of a Markov process. We define a family of functions:\n\u00b5x(t) = Pr(X(t) = x)\n\u03b3x,y(t) = lim h\u21930 Pr(X(t) = x,X(t+h) = y) h , x 6= y\n\u03b3x,x(t) = \u2212 \u2211 y 6=x \u03b3x,y(t).\n(2)\nThe function \u00b5x(t) is the probability that X(t) = x. The function \u03b3x,y(t) is the probability density that X transitions from state x to y at time t. Note that this parameter is not a transition rate, but rather a product of a pointwise probability with the point-wise transition rate of the approximating probability, i.e., \u03b3x,y(t)/\u00b5x(t) is the x,y entry of the time-dependent rate matrix. Hence, unlike the (inhomogeneous) rate matrix at time t, \u03b3x,y(t) takes into account the probability of being in state x and not only the\nrate of transitions. This definition implies that\nPr(X(t) = x,X(t+h) = y) = \u00b5x(t)\u03b4x,y+\u03b3x,y(t)h+o(h),\nWe aim to use the family of functions \u00b5 and \u03b3 as a representation of a Markov process. To do so, we need to characterize the set of constraints that these functions should satisfy.\nDefinition 3.1: A family \u03b7 = {\u00b5x(t), \u03b3x,y(t) : 0 \u2264 t \u2264 T} of continuous functions is a Markov-consistent density set if the following constraints are fulfilled:\n\u00b5x(t) \u2265 0, \u2211 x \u00b5x(0) = 1,\n\u03b3x,y(t) \u2265 0 \u2200y 6= x, \u03b3x,x(t) = \u2212 \u2211 y 6=x \u03b3x,y(t),\nd dt \u00b5x(t) = \u2211 y \u03b3y,x(t).\nLetM be the set of all Markov-consistent densities. Using standard arguments we can show that there exists a correspondence between (generally inhomogeneous) Markov processes and density sets \u03b7. Specifically:\nLemma 3.2: Let \u03b7 = {\u00b5x(t), \u03b3x,y(t)}. If \u03b7 \u2208 M, then there exists a continuous-time Markov process P\u03b7 for which \u00b5x and \u03b3x,y satisfy (2).\nThe processes we are interested in, however, have additional structure, as they correspond to the posterior distribution of a time-homogeneous process with end-point evidence. This additional structure implies that we should only consider a subset ofM: Lemma 3.3: Let Q be a rate matrix, and e0, eT be states of X . Then the representation \u03b7 corresponding to the posterior distribution PQ(\u00b7|e0, eT ) is in the setMe \u2282M that contains Markov-consistent density sets satisfying \u00b5x(0) = \u03b4x,e0 , \u00b5x(T ) = \u03b4x,eT .\nThus, from now on we can restrict our attention to density sets from Me. The constraint that \u00b5x(0) and \u00b5x(T ) also has consequences on \u03b3x,y at these points.\nLemma 3.4: If \u03b7 \u2208 Me then \u03b3x,y(0) = 0 for all x 6= e0 and \u03b3x,y(T ) = 0 for all y 6= eT .\nVariational Principle We can now state the variational principle for continuous processes, which closely tracks similar principles for discrete processes.\nWe define a free energy functional,\nF(\u03b7; Q) = E(\u03b7; Q) +H(\u03b7),\nwhich, as we will see, measures the quality of \u03b7 as an approximation of PQ(\u00b7|e). (For succinctness, we will assume that the evidence e is clear from the context.) The two\nterms in the continuous functional correspond to an entropy, H(\u03b7) = \u222b T\n0 \u2211 x \u2211 y 6=x \u03b3x,y(t)[1 + ln\u00b5x(t)\u2212 ln \u03b3x,y(t)]dt,\nand an energy, E(\u03b7; Q) = \u222b T\n0 \u2211 x \u00b5x(t)qx,x + \u2211 y 6=x \u03b3x,y(t) ln qx,y  dt. Theorem 3.5: Let Q be a rate matrix, e = (e0, eT ) be states of X , and \u03b7 \u2208Me. Then\nF(\u03b7; Q) = lnPQ(eT |e0)\u2212 ID(P\u03b7(\u00b7)||PQ(\u00b7|e))\nwhere ID(P\u03b7(\u00b7||PQ(\u00b7|e)) is the KL divergence between the two processes.\nWe conclude that F(\u03b7; Q) is a lower bound of the loglikelihood of the evidence, and that the closer the approximation to the target posterior, the tighter the bound.\nProof Outline The basic idea is to consider discrete approximations of the functional. Let K be an integer. We define the K-sieve XK to be the set of random variables X(t0),X(t1), . . . ,X(tK) where tk = kTK . We can use the variational principle [8] on the marginal distributions PQ(XK |e) and P\u03b7(XK). More precisely, define\nFK(\u03b7; Q) = EP\u03b7 [ ln PQ(XK , eT | e0)\nP\u03b7(XK)\n] ,\nwhich can, by using simple arithmetic manipulations, be recast as\nFK(\u03b7; Q) = lnPQ(eT |e0)\u2212 ID(P\u03b7(XK)||PQ(XK |e)).\nWe get the desired result by letting K \u2192 \u221e. By definition limK\u2192\u221e ID(P\u03b7(XK)||PQ(XK |e)) is ID(P\u03b7(\u00b7)||PQ(\u00b7|e)). The crux of the proof is in proving the following lemma.\nLemma 3.6: F(\u03b7; Q) = limK\u2192\u221e FK(\u03b7; Q).\nProof: Since both PQ and P\u03b7 are Markov processes,\nFK(\u03b7; Q) = K\u22121\u2211 k=0 EP\u03b7 [ lnPQ(X(tk+1)|X(tk)) ] \u2212 K\u22121\u2211 k=0 EP\u03b7 [ lnP\u03b7(X(tk),X(tk+1))\n] + K\u22121\u2211 k=1 EP\u03b7 [ lnP\u03b7(X(tk))\n] We now express these terms as functions of \u00b5x(t), \u03b3x,y(t) and qx,y . By definition, P\u03b7(X(tk) = x) = \u00b5x(tk). Each\nof the expectations either depend on this term, or on the joint distribution P\u03b7(X(tk\u22121),X(tk)). Using the continuity of \u03b3x,y(t) we write\nP\u03b7(X(tk) = x,X(tk+1) = y) = \u03b4x,y\u00b5x(tk) + \u2206K \u00b7 \u03b3x,y(tk) + o(\u2206K)\nwhere \u2206K = T/K. Similarly, we can also write\nPQ(X(tk+1) = y|X(tk) = x) = \u03b4x,y +\u2206K \u00b7qx,y +o(\u2206K)\nFinally, using properties of logarithms we have that\nln (1 + \u2206K \u00b7 z + o(\u2206K)) = \u2206K \u00b7 z + o(\u2206K).\nUsing these relations, we can rewrite after tedious yet straightforward manipulations,\nFK(\u03b7; Q) = EK(\u03b7; Q) +HK(\u03b7),\nwhere EK(\u03b7; Q) = K\u22121\u2211 k=0 \u2206KeK(tk), HK(\u03b7) = K\u22121\u2211 k=0 \u2206KhK(tk),\nand eK(t) = \u2211 x \u2211 y 6=x \u03b3x,y(t)[1 + ln\u00b5x(t)\u2212 ln \u03b3x,y(t)] + o(\u2206K) hK(t) = \u2211 x \u00b5x(t)qxx + \u2211 y 6=x \u03b3x,y(t) log qx,y\n+ o(\u2206K) Letting K \u2192 \u221e we have that\n\u2211 k \u2206k[f(tk) + o(\u2206K)] \u2192\u222b T\n0 f(t)dt, hence EK(\u03b7; Q) and HK(\u03b7) converge to\nE(\u03b7; Q) andH(\u03b7), respectively."}, {"heading": "4 Factored Approximation", "text": "The variational principle we discussed is based on a representation that is as complex as the original process\u2014the number of functions \u03b3x,y(t) we consider is equal to the size of the original rate matrix Q. To get a tractable inference procedure we make additional simplifying assumptions on the approximating distribution.\nGiven a D-component process we consider approximations that factor into products of independent processes. More precisely, we define Mie to be the continuous Markov-consistent density sets over the component Xi, that are consistent with the evidence on Xi at times 0 and T . Given a collection of density sets \u03b71, . . . , \u03b7D for the different components, the product density set \u03b7 = \u03b71 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u03b7D is defined as\n\u00b5x(t) = \u220f i \u00b5ixi(t)\n\u03b3x,y(t) =  \u03b3ixi,yi(t)\u00b5 \\i x (t) \u03b4(x,y) = {i}\u2211 i \u03b3 i xi,xi(t)\u00b5 \\i x (t) x = y\n0 otherwise\nwhere \u00b5\\ix (t) = \u220f j 6=i \u00b5 j xj (t) is the joint distribution at time t of all the components other than the i\u2019th. (It is not hard to see that if \u03b7i \u2208 Mie for all i, then \u03b7 \u2208 Me.) We define the setMFe to contain all factored density sets. From now on we assume that \u03b7 = \u03b71 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 \u03b7D \u2208MFe .\nAssuming that Q is defined by a CTBN, and that \u03b7 is a factored density set, we can rewrite E(\u03b7; Q) = \u2211 i \u222b T 0 \u2211 xi \u00b5ixi(t)E\u00b5\\i(t) [ qxi,xi|Ui ] dt\n+ \u2211 i \u222b T 0 \u2211 xi,yi 6=xi \u03b3ixi,yi(t)E\u00b5\\i(t) [ ln qxi,yi|Ui ] dt,\nand H(\u03b7) = \u2211 i H(\u03b7i).\nThis decomposition involves only local terms that either include the i\u2019th component, or include the i\u2019th component and its parents in the CTBN defining Q. Note that terms such as E\u00b5\\i(t) [ qxi,xi|Ui ] involve only \u00b5j(t) for j \u2208 Pai.\nTo make the factored nature of the approximation explicit in the notation, we write henceforth,\nF(\u03b7; Q) = FF (\u03b71, . . . , \u03b7D; Q).\nFixed Point Characterization We can now pose the optimization problem we wish to solve:\nFixing i, and given \u03b71, . . . , \u03b7i\u22121, \u03b7i+1, . . . , \u03b7D, in M1e, . . .Mi\u22121e ,Mi+1e , . . . ,MDe , respectively, find arg max\u03b7i\u2208Mie F F (\u03b71, . . . , \u03b7D; Q).\nIf for all i, we have a \u00b5i \u2208 Mie, which is a solution to this optimization problem with respect to each component, then we have a (local) stationary point of the energy functional withinMFe .\nTo solve this optimization problem, we define a Lagrangian, which includes the constraints in the form of Def. 3.1. The Lagrangian is a functional of the functions \u00b5ixi(t) and \u03b3 i xi,yi(t) and Lagrange multipliers (which are functions of t as well). The stationary point of the functional satisfies the Euler-Lagrange equations, namely the functional derivatives of L vanish. Writing these equations in explicit form we get a fixed point characterization of the solution in term of the following set of ODEs:\nd dt \u00b5ixi(t) = \u2211 yi 6=xi ( \u03b3iyi,xi(t)\u2212 \u03b3 i xi,yi(t) ) d dt \u03c1ixi(t) = \u2212\u03c1 i xi(t)(q\u0304 i xi,xi(t) + \u03c8 i xi(t))\n\u2212 \u2211 yi 6=xi \u03c1iyi(t)q\u0303 i xi,yi(t)\n(3)\nwhere \u03c1i are the exponents of the Lagrange multipliers.\nIn addition we have the following algebraic constraint\n\u03c1ixi(t)\u03b3 i xi,yi(t) = \u00b5 i xi(t)q\u0303 i xi,yi(t)\u03c1 i yi(t), xi 6= yi. (4)\nIn these equations we use the following shorthand notations for the average rates\nq\u0304ixi,yi(t) = E\u00b5\\i(t) [ q i|Pai xi,yi|Ui ] q\u0304ixi,yi|xj (t) = E\u00b5\\i(t) [ q i|Pai xi,yi|Ui | xj ] ,\nSimilarly, we have the following shorthand notations for the geometrically-averaged rates,\nq\u0303ixi,yi(t) = exp { E\u00b5\\i(t) [ ln qi|Paixi,yi|Ui ]} q\u0303ixi,yi|xj (t) = exp { E\u00b5\\i(t) [ ln qi|Paixi,yi|Ui | xj ]} ,\nThe last auxiliary term is \u03c8ixi(t) = \u2211\nj\u2208Childreni \u2211 xj\n\u00b5jxj (t)q\u0304 j xj ,xj |xi(t)+\u2211\nj\u2208Childreni \u2211 xj 6=yj \u03b3jxj ,yj (t) ln q\u0303 j xj ,yj |xi(t).\nThe two differential equations (3) for \u00b5ixi(t) and \u03c1 i xi(t) describe, respectively, the progression of \u00b5ixi forward, and the progression of \u03c1ixi backward. To uniquely solve these equations we need to set the boundary conditions. The boundary condition for \u00b5ixi is defined explicitly inM F e as\n\u00b5ixi(0) = \u03b4xi,ei,0 (5)\nThe boundary condition at T is slightly more involved. The constraints inMFe imply that \u00b5ixi(T ) = \u03b4xi,ei,T . As stated by Lemma 3.4, we have that \u03b3iei,T ,xi(T ) = 0 when xi 6= ei,T . Plugging these values into (4), and assuming that Q is irreducible we get that \u03c1xi(T ) = 0 for all xi 6= ei,T . In addition, we notice that \u03c1ei,T (T ) 6= 0, for otherwise the whole system of equations for \u03c1 will collapse to 0. Finally, notice that the solution of (3) for \u00b5i and \u03b3i is insensitive to the multiplication of \u03c1i by a constant. Thus, we can arbitrarily set \u03c1ei,T (T ) = 1, and get the boundary condition\n\u03c1ixi(T ) = \u03b4xi,ei,T . (6)\nTheorem 4.1: \u03b7i \u2208 Mie is a stationary point (e.g., local maxima) of FF (\u03b71, . . . , \u03b7D; Q) subject to the constraints of Def. 3.1 if and only if it satisfies (3\u20136).\nIt is straightforward to extend this result to show that at a maximum with respect to all the component densities, this fixed-point characterization must hold for all components simultaneously.\nExample 4.2: Consider the case of a single component, for which our procedure should be exact, as no simplifying assumptions are made on the density set. In this case, the\naveraged rates q\u0304i and the geometrically-averaged rates q\u0303i both reduce to the unaveraged rates q, and \u03c8 \u2261 0. Thus, the system of equations to be solved is\nd dt \u00b5x(t) = \u2211 y 6=x (\u03b3y,x(t)\u2212 \u03b3x,y(t))\nd dt \u03c1x(t) = \u2212 \u2211 y qx,y\u03c1y(t),\nalong with the algebraic equation\n\u03c1x(t)\u03b3x,y(t) = qx,y\u00b5x(t)\u03c1y(t), y 6= x.\nIn this case, it is straightforward to show that the backward propagation rule for \u03c1x implies that\n\u03c1x(t) = Pr(eT |X(t)).\nThis system of ODEs is similar to forward-backward propagation, except that unlike classical forward propagation (which would use a function such as \u03b1x(t) = Pr(X(t) = x|e0)), here the forward propagation already takes into account the backward messages, to directly compute the posterior. Given this interpretation, it is clear that integrating \u03c1x(t) from T to 0 followed by integrating \u00b5x(t) from 0 to T computes the exact posterior of the processes.\nThis interpretation of \u03c1x(t) also allows us to understand the role of \u03b3x,y(t). Recall that \u03b3x,y(t)/\u00b5x(t) is the instantaneous rate of transition from x to y at time t. Thus,\n\u03b3x,y(t) \u00b5x(t) = qx,y \u03c1y(t) \u03c1x(t) .\nThat is, the instantaneous rate combines the original rate with the relative likelihood of the evidence at T given y and x. If y is much more likely to lead to the final state, then the rates are biased toward y. Conversely, if y is unlikely to lead to the evidence the rate of transitions to it are lower. This observation also explains why the forward propagation of \u00b5x will reach the observed \u00b5x(T ) even though we did not impose it explicitly.\nExample 4.3: We define an Ising chain to be a CTBN X1 \u2194 X2 \u2194 \u00b7 \u00b7 \u00b7 \u2194 XD such that each binary component prefers to be in the same state as its neighbor. These models are governed by two parameters: a coupling parameter \u03b2 which determines the strength of the coupling between two neighboring components, and a rate parameter \u03c4 that determines the propensity of each component to change its state. More formally, we define the conditional\nrate matrices as qi|Paixi,yi|ui = \u03c4 ( 1 + e\u22122yi\u03b2 P \u2208Pai xj )\u22121 where xj \u2208 {\u22121, 1}. As an example, we consider a two-component Ising chain with initial state X(0)1 = \u22121 and X (0) 2 = 1, and a reversed state at the final time, X(T )1 = 1 and X (T ) 2 = \u22121. For a large value of \u03b2, this evidence is unlikely as at\nboth end points the components are in a undesired configurations. The exact posterior is one that assigns higher probabilities to trajectories where one of the components switches relatively fast to match the other, and then toward the end of the interval, they separate to match the evidence. Since the model is symmetric, these trajectories are either ones in which both components are most of the time in state \u22121, or ones where both are most of the time in state 1 (Fig. 1 top). Due to symmetry, the marginal probability of each component is around 0.5 throughout most of the interval (Fig. 1 middle). The variational approximation cannot capture the dependency between the two components, and thus converges to one of two local maxima, corresponding to the two potential subsets of trajectories. Examining the value of \u03c1i, we see that close to the end of the interval they bias the instantaneous rates significantly (Fig. 1 bottom).\nThis example also allows to examine the implications of modeling the posterior by inhomogeneous Markov processes. In principle, we might have used as an approximation Markov processes with homogeneous rates, and conditioned on the evidence. To examine whether our approx-\nimation behaves in this manner, we notice that in the single component case we have\nqx,y = \u03c1x(t)\u03b3x,y(t) \u03c1y(t)\u00b5x(t) ,\nwhich should be constant. Consider the analogous quantity in the multi-component case: q\u0303ixi,yi(t), the geometric average of the rate of Xi, given the probability of parents state. Not surprisingly, this is exactly a mean field approximation, where the influence of interacting components is approximated by their average influence. Since the distribution of the parents (in the two-component system, the other component) changes in time, these rates change continuously, especially near the end of the time interval. This suggests that a piecewise homogeneous approximation cannot capture the dynamics without a loss in accuracy.\nOptimization Procedure If Q is irreducible, then \u03c1ixi and \u00b5xi are non-zero throughout the open interval (0, T ). As a result, we can solve (4) to express \u03b3ixi,yi as a function of \u00b5i and \u03c1i, thus eliminating it from (3) to get evolution equations solely in terms of \u00b5i and \u03c1i. Abstracting the details, we obtain a set of ODEs of the form\nd dt \u00b5i(t) = \u03b1(\u00b5i(t), \u03c1i(t), \u00b5\\i(t)) \u00b5i(0) = given\nd dt \u03c1i(t) = \u2212\u03b2(\u03c1i(t), \u00b5\\i(t)) \u03c1i(T ) = given.\nwhere \u03b1 and \u03b2 can be inferred from (3) and (4). Since the evolution of \u03c1i does not depend on \u00b5i, we can integrate backward from time T to solve for \u03c1i. Then, integrating forward from time 0, we compute \u00b5i. After performing a single iteration of backward-forward integration, we obtain a solution that satisfies the fixed-point equation (3) for the i\u2019th component. (This is not surprising once we have identified our procedure to be a variation of a standard forwardbackward algorithm for a single component.) Such a solution will be a local maximum of the functional w.r.t. to \u03b7i (reaching a local minimum or a saddle point requires very specific initialization points).\nThis suggests that we can use the standard procedure of asynchronous update, where we update each component in a round-robin fashion. Since each of these singlecomponent updates converges in one backward-forward step, and since it reaches a local maximum, each step improves the value of the free energy over the previous one. Since the free energy functional is bounded by the probability of the evidence, this procedure will always converge.\nAnother issue is the initialization of this procedure. Since the iteration on the i\u2019th component depends on \u00b5\\i, we need to initialize \u00b5 by some legal assignment. To do so, we create a fictional rate matrix Q\u0303i for each component and initialize \u00b5i to be the posterior of the process given the evidence ei,0 and ei,T . As a reasonable initial guess, we choose at random one of the conditional rates in Q to determine the fictional rate matrix.\nThe continuous time update equations allow us to use standard ODE methods with an adaptive step size (here we use the Runge-Kutta-Fehlberg (4,5) method). At the price of some overhead, these procedure automatically tune the trade-off between error and time granularity."}, {"heading": "5 Perspective & Related Works", "text": "Variational approximations for different types of continuous-time processes have been recently proposed [12, 13]. Our approach is motivated by results of Opper and Sanguinetti [12] who developed a variational principle for a related model. Their model, which they call a Markov jump process, is similar to an HMM, in which the hidden chain is a continuous-time Markov process and there are (noisy) observations at discrete points along the process. They describe a variational principle and discuss the form of the functional when the approximation is a product of independent processes. There are two main differences between the setting of Opper and Sanguinetti and ours. First, we show how to exploit the structure of the target CTBN to reduce the complexity of the approximation. These simplifications imply that the update of the i\u2019th process depends only on its Markov blanket in the CTBN, allowing us to develop efficient approximations for large models. Second, and more importantly, the structure of the evidence in our setting is quite different, as we assume deterministic evidence at the end of intervals. This setting typically leads to a posterior Markov process in which the instantaneous rates used by Opper and Sanguinetti diverge toward the end point\u2014the rates of transition into the observed state go to infinity, leading to numerical problems at the end points. We circumvent this problem by using the marginal density representation which is much more stable numerically.\nTaking the general perspective of Wainwright and Jordan [15], the representation of the distribution uses the natural sufficient statistics. In the case of a continuous-time Markov process, the sufficient statistics are Tx, the time\nspent in state x, and Mx,y , the number of transitions from state x to y. In a discrete-time model, we can capture the statistics for every random variable. In a continuous-time model, however, we need to consider the time derivative of the statistics. Indeed, it is not hard to show that\nd dt E [Tx(t)] = \u00b5x(t) and\nd dt E [Mx,y(t)] = \u03b3x,y(t).\nThus, our marginal density sets \u03b7 provide what we consider a natural formulation for variational approaches to continuous-time Markov processes.\nOur presentation focused on evidence at two ends of an interval. Our formulation easily extends to deal with more elaborate types of evidence: (1) If we do not observe the initial state of the i\u2019th component, we can set \u00b5ix(0) to be the prior probability of X(0) = x. Similarly, if we do not observe Xi at time T , we set \u03c1ix(T ) = 1 as initial data for the backward step. (2) In a CTBN where one (or more) components are fully observed, we simply set \u00b5i for these components to be a distribution that assigns all the probability mass to the observed trajectory. Similarly, if we observe different components at different times, we may update each component on a different time interval. Consequently, maintaining for each component a marginal distribution \u00b5i throughout the interval of interest, we can update the other ones using their evidence patterns."}, {"heading": "6 Experimental Evaluation", "text": "To gain better insight into the quality of our procedure, we performed numerical tests on models that challenge the approximation. Specifically, we use Ising chains where we explore regimes defined by the degree of coupling between the components (the parameter \u03b2) and the rate of transitions (the parameter \u03c4 ). We evaluate the error in two ways. The first is by the difference between the true log-likelihood and our estimate. The second is by the average relative error in the estimate of different expected sufficient statistics defined by \u2211 j |\u03b8\u0302j\u2212\u03b8j | \u03b8j where \u03b8j is exact value of the j\u2019th ex-\npected sufficient statistics and \u03b8\u0302j is the approximation. Applying our procedure on an Ising chain with 8 components, for which we can still perform exact inference, we evaluated the relative error for different choices of \u03b2 and \u03c4 . The evidence in this experiment is e0 = {+,+,+,+,+,+,\u2212,\u2212}, T = 0.64 and eT = {\u2212,\u2212,\u2212,+,+,+,+,+}. As shown in Fig. 2a, the error is larger when \u03c4 and \u03b2 are large. In the case of a weak coupling (small \u03b2), the posterior is almost independent, and our approximation is accurate. In models with few transitions (small \u03c4 ), most of the mass of the posterior is concentrated on a few canonical \u201ctypes\u201d of trajectories that can be captured by the approximation (as in Example 4.3). At high transition rates, the components tend to transition often, and in a coordinated manner, which leads to a posterior that is hard to approximate by a product distribution. Moreover, the resulting free energy landscape is rough with many local maxima. Examining the error in likelihood estimates (Fig. 2b,c) we see a similar trend.\nNext, we examine the run time of our approximation when using fairly standard ODE solver with few optimizations and tunings. The run time is dominated by the time needed to perform the backward-forward integration when updating a single component, and by the number of such updates until convergence. Examining the run time for different choices of \u03b2 and \u03c4 (Fig. 3), we see that the run time of our procedure scales linearly with the number of components in the chain. Moreover, the run time is generally insensitive to the difficulty of the problem in terms of \u03b2. It does depend to some extent on the rate \u03c4 , suggesting that processes with more transitions require more iterations to converge. Indeed, the number of iterations required to achieve convergence in the largest chains under consideration are mildly affected by parameter choices. The scalability of the run time stands in contrast to the Gibbs sampling procedure [4], which scales roughly with the number in transitions in the sampled trajectories. Comparing our method to the Gibbs sampling procedure we see (Fig. 4) that the faster Mean Field approach dominates the Gibbs procedure over short run times. However, as opposed to Mean Field, the Gibbs procedure is asymptotically unbiased, and with longer run times it ultimately prevails. This evaluation also shows that adaptive integration procedure in our methods strikes a better trade-off than using a fixed time granularity integration."}, {"heading": "7 Inference on Trees", "text": "The abovementioned experimental results indicate that our approximation is accurate when reasoning about weaklycoupled components, or about time intervals involving few transitions (low transition rates). Unfortunately, in many domains we face strongly-coupled components. For example, we are interested in modeling the evolution of biological sequences (DNA, RNA, and proteins). In such systems, we have a phylogenetic tree that represents the branching\nprocess that leads to current day sequences (see Fig. 5a). It is common in sequence evolution to model this process as a continuous-time Markov process over a tree [6]. More precisely, the evolution along each branch is a standard continuous-time Markov process, and branching is modeled by a replication, after which each replica evolves independently along its sub-branch. Common applications are forced to assume that each character in the sequence evolves independently of the other.\nIn some situations, assuming an independent evolution of each character is highly unreasonable. Consider the evolution of an RNA sequence that folds onto itself to form a functional structure. This folding is mediated by complementary base-pairing (A-U, C-G, etc) that stabilizes the structure. During evolution, we expect to see compensatory mutations. That is, if a A changes into C then its basedpaired U will change into a G soon thereafter. To capture such coordinated changes, we need to consider the joint evolution of the different characters. In the case of RNA structure, the stability of the structure is determined by stacking potentials that measure the stability of two adjacent pairs of interacting nucleotides. Thus, if we consider a factor network to represent the energy of a fold, it will have structure as shown in Fig. 5b. We can convert this factor graph into a CTBN using procedures that consider the energy function as a fitness criteria in evolution [3, 16]. Unfortunately, inference in such models suffers from computational blowup, and so the few studies that deal with it explicitly resort to sampling procedures [16].\nTo consider trees, we need to extend our framework to deal with branching processes. In a linear-time model, we view the process as a map from [0, T ] into random variables X(t). In the case of a tree, we view the process as a map from a point t = \u3008b, t\u3009 on a tree T (defined by branch b and the time t within it) into a random variable X(t). Similarly, we generalize the definition of the Markov-consistent density set \u03b7 to include functions on trees. We define continuity of functions on trees in the obvious manner.\nThe variational approximation on trees is thus similar\nto the one on intervals. Within each branch, we deal with the same update formulas as in linear time. We denote by \u00b5ixi(b, t) and \u03c1 i xi(b, t) the messages computed on branch b at time t. The only changes occur at vertices. Suppose we have a branch b1 of length T1 incoming into vertex v, and two outgoing branches b2 and b3 (see Fig. 5c). Then we use the following updates for \u00b5ixi and \u03c1 i xi\n\u00b5ixi(bk, 0) = \u00b5 i xi(b1, T1) k = 2, 3,\n\u03c1ixi(b1, T1) = \u03c1 i xi(b2, 0)\u03c1 i xi(b3, 0).\nThe forward propagation of \u00b5i simply uses the value at the end of the incoming branch as initial value for the outgoing branches. In backward propagation of \u03c1i the value at the end of b1 is the product of the values at the start of the two outgoing branches. This is the natural operation when we recall the interpretation of \u03c1i as the probability of the downstream evidence given the current state.\nWhen switching to trees, we increase the amount of evidence about intermediate states. Consider for example the\ntree of Fig. 5a. We can view the span from C to D as an interval with evidence at its end. When we add evidence at the tip of other branches we gain more information about intermediate points between C and D. To evaluate the impact of these changes on our approximation, we considered the tree of Fig. 5a, and compared it to inference in the backbone between C and D (Fig. 2). Comparing the true marginal to the approximate one along the main backbone (see Fig. 6a) we see a major difference in the quality of the approximation. The evidence in the tree leads to a much tighter approximation of the marginal distribution. A more systematic comparison (Fig. 6b,c) demonstrates that the additional evidence reduces the magnitude of the error throughout the parameter space.\nAs a more demanding test, we applied our inference procedure to the model introduced by Yu and Thorne [16] for a stem of 18 interacting RNA nucleotides in 8 species in the phylogeny of Fig. 5a. We compared our estimate of the expected sufficient statistics of this model to these obtained\nby the Gibbs sampling procedure. The results, shown in Fig. 7, demonstrate that over all, the two approximate inference procedures are in good agreement about the value of the expected sufficient statistics."}, {"heading": "8 Discussion", "text": "In this paper we formulate a general variational principle for continuous-time Markov processes (by reformulating and extending the one proposed by Opper and Sanguinetti [12]), and use it to derive an efficient procedure for inference in CTBNs. In this mean field-type approximation, we use a product of independent inhomogeneous processes to approximate the multi-component posterior. Our procedure enjoys the same benefits encountered in discrete time mean field procedure [8]: it provides a lower-bound on the likelihood of the evidence and its run time scales linearly with the number of components. Using asynchronous updates it is guaranteed to converge, and the approximation represents a consistent joint distribution. It also suffers from expected shortcomings: there are multiple local maxima, and it cannot captures certain complex interactions in the posterior. By using a time-inhomogeneous representation, our approximation does capture complex patterns in the temporal progression of the marginal distribution of each component. Importantly, the continuous time parametrization enables straightforward implementation using standard ODE integration packages that automatically tune the trade-off between time granularity and approximation quality. We show how to extend it to perform inference on phylogenetic trees, and show that it provides fairly accurate answers in the context of a real application.\nOne of the key developments here is the shift from (piecewise) homogeneous parametric representations to continuously inhomogeneous representations based on marginal density sets. This shift increases the flexibility of the approximation and, somewhat surprisingly, also significantly simplifies the resulting formulation.\nA possible extension of the ideas set here is to use\nour variational procedure to generate initial distribution for Gibbs sampling skip the initial burn-in phase and produce accurate samples. Another attractive aspect of this new variational approximation is its potential use for learning model parameters from data. It can be easily combined with the EM procedure for CTBNs [10], to obtain a Variational-EM procedure for CTBNs, which monotonically increases the likelihood by alternating between steps that improve the approximation \u03b7 (the updates discussed here) and steps that improve the model parameters \u03b8."}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for helpful remarks on previous versions of the manuscript. This research was supported in part by a grant from the Israel Science Foundation. Tal El-Hay is supported by the Eshkol fellowship from the Israeli Ministry of Science."}], "references": [{"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In UAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Markov chains with stationary transition probabilities", "author": ["K.L. Chung"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1960}, {"title": "Continuous time markov networks", "author": ["T. El-Hay", "N. Friedman", "D. Koller", "R. Kupferman"], "venue": "In UAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Gibbs sampling in factorized continuous-time markov processes", "author": ["T. El-Hay", "N. Friedman", "R. Kupferman"], "venue": "In UAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Sampling for approximate inference in continuous time Bayesian networks", "author": ["Y. Fan", "C.R. Shelton"], "venue": "In AI and Math,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Handbook of stochastic methods", "author": ["C.W. Gardiner"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "An introduction to variational approximations methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T. Jaakkola", "L.K. Saul"], "venue": "In Learning in Graphical Models", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Expectation maximization and complex duration distributions for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Expectation propagation for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Variational inference for Markov jump processes", "author": ["M. Opper", "G. Sanguinetti"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Variational inference for Diffusion Processes", "author": ["C. Archambeau", "M. Opper", "Y. Shen", "D. Cornford", "J. Shawe-Taylor"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Reasoning at the right time granularity", "author": ["S. Saria", "U. Nodelman", "D. Koller"], "venue": "In UAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M. Jordan"], "venue": "Found. Trends Mach. Learn.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Dependence among sites in RNA evolution", "author": ["J. Yu", "J. L Thorne"], "venue": "Mol. Biol. Evol.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}], "referenceMentions": [{"referenceID": 7, "context": "Continuous-time Bayesian networks (CTBNs) provide a representation language for such processes, which allows to naturally exploit sparse patterns of interactions to compactly represent the dynamics of such processes [9].", "startOffset": 216, "endOffset": 219}, {"referenceID": 0, "context": "Inference in multi-component temporal models is a notoriously hard problem [1].", "startOffset": 75, "endOffset": 78}, {"referenceID": 7, "context": "Similar to the situation in discrete time processes, inference is exponential in the number of components, even in a CTBN with sparse interactions [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "These include sampling-based approaches, where Fan and Shelton [5] introduced a likelihood-weighted sampling scheme, and more recently we [4] introduced a Gibbs-sampling procedure.", "startOffset": 63, "endOffset": 66}, {"referenceID": 3, "context": "These include sampling-based approaches, where Fan and Shelton [5] introduced a likelihood-weighted sampling scheme, and more recently we [4] introduced a Gibbs-sampling procedure.", "startOffset": 138, "endOffset": 141}, {"referenceID": 9, "context": "[11] introduced an Expectation Propagation approach, which can be roughly described as a local message passing scheme, where each message describes the dynamics of a single component over an interval.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "This message passing procedure can automatically refine the number of intervals according to the complexity of the underlying system [14].", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "We use the strategy of structured variational approximations in graphical models [8], and specifically by the variational approach of Opper and Sanguinetti [12] for approximate inference in Markov Jump Processes, a related class of models (see below).", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "We use the strategy of structured variational approximations in graphical models [8], and specifically by the variational approach of Opper and Sanguinetti [12] for approximate inference in Markov Jump Processes, a related class of models (see below).", "startOffset": 156, "endOffset": 160}, {"referenceID": 1, "context": "Using the rate matrix Q, we can express the Markov transition function as px,y(t) = [exp(tQ)]x,y where exp(tQ) is a matrix exponential [2, 7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 5, "context": "Using the rate matrix Q, we can express the Markov transition function as px,y(t) = [exp(tQ)]x,y where exp(tQ) is a matrix exponential [2, 7].", "startOffset": 135, "endOffset": 141}, {"referenceID": 7, "context": ", D} \\ {i}, which are its parents in the network [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "Here we define the optimization problem over a set of mean parameters [15], representing possible values of expected sufficient statistics.", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "We can use the variational principle [8] on the marginal distributions PQ(XK |e) and P\u03b7(XK).", "startOffset": 37, "endOffset": 40}, {"referenceID": 10, "context": "Variational approximations for different types of continuous-time processes have been recently proposed [12, 13].", "startOffset": 104, "endOffset": 112}, {"referenceID": 11, "context": "Variational approximations for different types of continuous-time processes have been recently proposed [12, 13].", "startOffset": 104, "endOffset": 112}, {"referenceID": 10, "context": "Our approach is motivated by results of Opper and Sanguinetti [12] who developed a variational principle for a related model.", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Taking the general perspective of Wainwright and Jordan [15], the representation of the distribution uses the natural sufficient statistics.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "The scalability of the run time stands in contrast to the Gibbs sampling procedure [4], which scales roughly with the number in transitions in the sampled trajectories.", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "We can convert this factor graph into a CTBN using procedures that consider the energy function as a fitness criteria in evolution [3, 16].", "startOffset": 131, "endOffset": 138}, {"referenceID": 14, "context": "We can convert this factor graph into a CTBN using procedures that consider the energy function as a fitness criteria in evolution [3, 16].", "startOffset": 131, "endOffset": 138}, {"referenceID": 14, "context": "Unfortunately, inference in such models suffers from computational blowup, and so the few studies that deal with it explicitly resort to sampling procedures [16].", "startOffset": 157, "endOffset": 161}, {"referenceID": 14, "context": "As a more demanding test, we applied our inference procedure to the model introduced by Yu and Thorne [16] for a stem of 18 interacting RNA nucleotides in 8 species in the phylogeny of Fig.", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "In this paper we formulate a general variational principle for continuous-time Markov processes (by reformulating and extending the one proposed by Opper and Sanguinetti [12]), and use it to derive an efficient procedure for inference in CTBNs.", "startOffset": 170, "endOffset": 174}, {"referenceID": 6, "context": "Our procedure enjoys the same benefits encountered in discrete time mean field procedure [8]: it provides a lower-bound on the likelihood of the evidence and its run time scales linearly with the number of components.", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "It can be easily combined with the EM procedure for CTBNs [10], to obtain a Variational-EM procedure for CTBNs, which monotonically increases the likelihood by alternating between steps that improve the approximation \u03b7 (the updates discussed here) and steps that improve the model parameters \u03b8.", "startOffset": 58, "endOffset": 62}], "year": 2009, "abstractText": "Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation, inference in such models is intractable even in relatively simple structured networks. Here we introduce a mean field variational approximation in which we use a product of inhomogeneous Markov processes to approximate a distribution over trajectories. This variational approach leads to a globally consistent distribution, which can be efficiently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for learning tasks. We provide the theoretical foundations for the approximation, an efficient implementation that exploits the wide range of highly optimized ordinary differential equations (ODE) solvers, experimentally explore characterizations of processes for which this approximation is suitable, and show applications to a large-scale realworld inference problem.", "creator": "TeX"}}}