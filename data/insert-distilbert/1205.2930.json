{"id": "1205.2930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Density Sensitive Hashing", "abstract": "nearest neighbors search is a fundamental problem in various research fields like machine learning, data mining and pattern recognition. recently, hashing - based approaches, e. g., locality sensitive hashing ( pronounced lsh ), likewise are rarely proved to be less effective for scalable scaled high query dimensional nearest neighbors search. many hashing algorithms found their theoretic root in random projection. since they these algorithms generate the hash tables ( projections ) randomly, yielding a large number of contiguous hash tables ( i. e., long chain codewords ) are nonetheless required in order to achieve both high precision and recall. to thus address this limitation, we propose a novel statistical hashing algorithm called { \\ relative em density sensitive hashing } ( dsh ) in essentially this 2010 paper. dsh can be regarded as an extension of lsh. by exploring the geometric structure of the data, dsh avoids explaining the purely random projections selection and uses those projective functions which best agree with the distribution of the data. extensive experimental results on 3d real - lived world data sets have shown that the single proposed method achieves better performance compared to the state - of - the - scientific art hashing approaches.", "histories": [["v1", "Mon, 14 May 2012 02:27:52 GMT  (63kb)", "http://arxiv.org/abs/1205.2930v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["yue lin", "deng cai", "cheng li"], "accepted": false, "id": "1205.2930"}, "pdf": {"name": "1205.2930.pdf", "metadata": {"source": "CRF", "title": "Density Sensitive Hashing", "authors": ["Yue Lin", "Cheng Li"], "emails": ["linyue29@gmail.com,"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n29 30\nv1 [\ncs .I\nR ]\n1 4\nM ay\n2 01\n2 1\nIndex Terms\u2014Locality Sensitive Hashing, Random Projection, Clustering.\n\u2726"}, {"heading": "1 INTRODUCTION", "text": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14]. A number of efficient algorithms, based on pre-built index structures (e.g. KD-tree [4] and R-tree [2]), have been proposed for nearest neighbors search. Unfortunately, these approaches perform worse than a linear scan when the dimensionality of the space is high [5]. Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10]. The key idea of these approaches is to generate binary codewords for high dimensional data points that preserve the similarity between them. Roughly, these hashing methods can be divided into two groups, the random projection based methods and the learning based methods. Many hashing algorithms are based on the random projection, which has been proved to be an effective method to preserve pairwise distances for data points. One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12]. Given a database with n samples, LSH makes no prior assumption about the data distribution and offers probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27]. However, according to the Jonson Lindenstrauss Theorem [17], LSH needs O(lnn/\u01eb2) random projections to preserve the pairwise distances, where \u01eb is the relative error. Therefore, in order to increase the probability that similar objects are mapped to similar\nY. Lin, D. Cai and C. Li are with the State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, Zhejiang, China, 310058. Email: linyue29@gmail.com, dengcai@cad.zju.edu.cn,licheng@zju.edu.cn.\nhash codes, LSH needs to use many random vectors to generate the hash tables (a long codeword), leading to a large storage space and a high computational cost. Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed. Most of these algorithms exploit the spectral properties of the data affinity (i.e., item-item similarity) matrix for binary coding. Despite the success of these approaches for relatively small codes, they often fail to make significant improvement as the code length increases [19]. In this paper, we propose a novel hashing algorithm called Density Sensitive Hashing (DSH) for effective high dimensional nearest neighbors search. Our algorithm can be regarded as an extension of LSH. Different from all the existing random projection based hashing methods, DSH tries to utilize the geometric structure of the data to guide the projections (hash tables) selection. Specifically, DSH uses k-means to roughly partition the data set into k groups. Then for each pair of adjacent groups, DSH generates one projection vector which can well split the two corresponding groups. From all the generated projections, DSH select the final ones according to the maximum entropy principle, in order to maximize the information provided by each bit. Experimental results show the superior performance of the proposed Density Sensitive Hashing algorithm over the existing state-ofthe-art approaches. The remainder of this paper is organized as follows. We introduce the background and review the related work in Section 2. Our Density Sensitive Hashing algorithm is presented in Section 3. Section 4 gives the experimental results that compared our algorithm with the state-of-the-art hashing methods on three real world large scale data sets. Conclusion remarks are provided in Section 5.\n2"}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": "The generic hashing problem is the following. Given n data points X = [x1, \u00b7 \u00b7 \u00b7 ,xn] \u2208 Rd\u00d7n, find L hash functions to map a data point x to a L-bits hash code\nH(x) = [h1(x), h2(x), ..., hL(x)],\nwhere hl(x) \u2208 {0, 1} is the l-th hash function. For the linear projection-based hashing, we have [33]\nhl(x) = sgn(F (w T l x+ tl)) (1)\nwhere wl is the projection vector and tl is the intercept. Different hashing algorithms aim at finding different F , wl and tl with respect to different objective functions. One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12]. LSH is fundamentally based on the random projection and uses randomly generated wl. The F in LSH is an identity function and tl = 0 for mean thresholding\n1. Thus, for LSH, we have\nhl(x) =\n{\n1 if wTl x \u2265 0 0 otherwise\n(2)\nwhere wl is a vector generated from a zero-mean multivariate Gaussian N (0, I) of the same dimension as the input x. From the geometric point of view, the wl defines a hyperplane. The points on different sides of the hyperplane have the opposite labels. Using this hash function, two points\u2019 hash bits match with the probability proportional to the cosine of the angle between them [8]. Specifically, for any two points xi,xj \u2208 Rd, we have [22]:\nPr[hl(xi) = hl(xj)] = 1\u2212 1\n\u03c0 cos\u22121(\nx T i xj\n\u2225 \u2225xi \u2225 \u2225 \u2225 \u2225xj \u2225 \u2225\n) (3)\nBased on this nice property, LSH have the probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27]. Empirical studies [1] showed that the LSH is significantly more efficient than the methods based on hierarchical tree decomposition. It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21]. There are many extensions for LSH [18], [22], [25], [28]. Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query. The original LSH methods cannot apply for high-dimensional kernelized data when the underlying feature embedding for the kernel is unknown. To address this limitation, Kernelized Locality Sensitive Hashing is introduced in [22]. It suggests to approximate a normal distribution in the kernel space using only kernel comparisons [19]. In addition, the Shift Invariant Kernels Hashing [30], which is a distribution-free method based on the random features\n1. Without loss of generality, we assume that all the data points are centralized to have zero mean.\nmapping for shift-invariant kernels, is also proposed recently. This method has theoretical convergence guarantees and performs well for relatively large code sizes [13]. All these methods are fundamentally based on the random projection. According to the Jonson Lindenstrauss Theorem [17], O(lnn/\u01eb2) projective vectors are needed to preserve the pairwise distances of a database with size n for the random projection, where \u01eb is the relative error. Therefore, in order to increase the probability that similar objects are mapped to similar hash codes, these random projection based hashing methods need to use many random vectors to generate the hash tables (a long codeword), leading to a large storage space and a high computational cost. To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed. PCA Hashing [34] might be the simplest one. It chooses wl in Eq.(1) to be the principal directions of data. Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.e., item-item similarity) matrix for binary coding. The spectral analysis of the data affinity matrix is usually time consuming [7]. To avoid the high computational cost, Weiss et al. [35] made a strong assumption that data is uniformly distributed and proposed a Spectral Hashing method (SpH). The assumption in SpH leads to a simple analytical eigenfunction solution of 1-D Laplacians, but the geometric structure of the original data is almost ignored, leading to a suboptimal performance. Anchor Graph Hashing (AGH) [24] is a recently proposed method to overcome this shortcoming. AGH generates k anchor points from the data and represents all the data points by sparse linear combinations of the anchors. In this way, the spectral analysis of the data affinity can be efficiently performed. Some other learning based hashing methods include Semantic Hashing [31] which uses the stacked Restricted Boltzmann Machine (RBM) to generate the compact binary codes; Semi-supervised Sequential Projection Hashing (S3PH) [33] which can incorporate supervision information. Despite the success of these learning based hashing approaches for relatively small codes, they often fail to make significant improvement as the code length increases [19]."}, {"heading": "3 DENSITY SENSITIVE HASHING", "text": "In this section, we give the detailed description on our proposed Density Sensitive Hashing (DSH) which aims at overcoming the disadvantages of both random projection based and learning based hashing approaches. To guarantee the performance will increase as the code length increases, DSH adopts the similar framework as LSH. Different from LSH which generates the projections randomly, DSH uses the geometric structure of the data to guide the selection of the projections. Figure 1 presents a toy example to illustrate the basic idea of our approach. There are four Gaussians in a\n3\ntwo dimensional plane and one is asked to encode the data using two-bits hash codes. LSH [8] generates the projections randomly and it is very possible that the data points from the same Gaussian will be encoded by different hash codes. PCA Hashing [34] uses the principle directions of the data as the projective vectors. In our example, all the four Gaussians are split and PCA Hashing generates an unsatisfactory coding. Considering the geometric structure of the data (density of the data), our DSH generates perfect binary codes for this toy example. The detailed procedure of DSH will be provided in the following subsections."}, {"heading": "3.1 Minimum Distortion Quantization", "text": "The first step of DSH is quantization of the data. Recently, Pauleve\u0301 et al. [29] show that a quantized preprocess for the data points can significantly improve the performance of the nearest neighbors search. Motivated by this result, we use the k-means algorithm, one of the most popular quantization approaches, to partition the n points into k (k < n) groups.\nLet S = {S1, \u00b7 \u00b7 \u00b7 ,Sk} denote a given quantization result. The distortion, also known as the Sum of Squared Error (SSE), can be used to measure the quality of the given quantization:\nSSE =\nk \u2211\np=1\n\u2211\nx\u2208Sp\n\u2016x\u2212 \u00b5p\u2016 2 (4)\nThe \u00b5p is the representative point of the p-th group Sp.\nBy noticing\n\u2202SSE\n\u2202\u00b5i =\n\u2202\n\u2202\u00b5i\nk \u2211\np=1\n\u2211\nx\u2208Sp\n\u2016x\u2212 \u00b5p\u2016 2\n=\nk \u2211\np=1\n\u2211\nx\u2208Sp\n\u2202 \u00b5i \u2016x\u2212 \u00b5p\u2016 2\n= \u2211\nx\u2208Si\n2(x\u2212 \u00b5i) = 0, i = 1, \u00b7 \u00b7 \u00b7 , k,\nwe have:\n\u00b5i = 1\n|Si|\n\u2211\nx\u2208Si\nx, i = 1, 2, ..., k (5)\nIt indicates that in order to minimize the distortion, we can choose the center point as the representative point for each group. There are two points that needed to be highlighted for the k-means quantization in our approach:\n1) In large scale applications, it can be time consuming to wait the k-means converges. Naturally, we can stop the k-means after p iterations, where p is a parameter. We found that a small number of p is usually enough (usually 5). This will be discussed in the our experiments. 2) In real applications, we do not know which is the best group number k. It seems that the bigger the k, the better performance we will get. It is simply because the quantization will have smaller error with a large number of groups. However, a large number of groups could lead to high computational cost in the quantization step. As will be described in the next subsection, the number of\n4 groups decides the maximum code length DSH can generate. Thus, we set\nk = \u03b1L (6)\nwhere L is the code length and \u03b1 is a parameter."}, {"heading": "3.2 Density Sensitive Projections Generation", "text": "Now we have the quantization result denoted by k groups S1, \u00b7 \u00b7 \u00b7 ,Sk and the i-th group has the center \u00b5i. Instead of generating projections randomly as LSH does, our DSH tries to use this quantization result to guide the projections generating process. We define the r-nearest neighbors matrix W of the groups as follows: Definition 1: r-Nearest Neighbors Matrix W of the groups.\nWij =\n{\n1, if \u00b5i \u2208 Nr(\u00b5j) or \u00b5j \u2208 Nr(\u00b5i) 0, otherwise\n(7)\nwhere Nr(\u00b5i) denotes the set of r nearest neighbors of \u00b5i. With this definition, we can then define r-adjacent groups: Definition 2: r-Adjacent Groups: Group Si and group Sj are called r-adjacent groups if and only if Wij = 1. Instead of picking a random projection, it is more natural to pick those projections which can well separate two adjacent groups. For each pair of adjacent groups Si and Sj , DSH uses the median plane between the centers of adjacent groups as the hyperplane to separate points. The median plane is defined as follows:\n(x\u2212 \u00b5i + \u00b5j\n2 )T (\u00b5i \u2212 \u00b5j) = 0 (8)\nOne can easily verify that the hash function associated with this plane is defined as follows:\nh(x) =\n{\n1 if wTx \u2265 t 0 otherwise\n(9)\nwhere\nw = \u00b51 \u2212 \u00b52, t = ( \u00b51 + \u00b52\n2 )T (\u00b51 \u2212 \u00b52) (10)"}, {"heading": "3.3 Entropy Based Projections Selection", "text": "Given k groups, the previous step can generate around 1 2 kr projections. Since k = \u03b1L, our DSH generates 1 2 \u03b1rL projections so far. Each projection will lead to one bit in the code and the usual setting of the parameters \u03b1, r will make 1\n2 \u03b1rL > L. Thus, our DSH needs a projections\nselection step which aims at selecting L projections from the candidate set containing 1\n2 \u03b1rL projections.\nFrom the information theoretic point of view, a \u201dgood\u201d binary codes should maximize the information/entropy provided by each bit [38]. Using maximum entropy principle, a binary bit that gives balanced partitioning of the data points provides maximum information [32]. Thus,\nwe compute the entropy of each candidate projection and select the projections which can split the data most equally. Assume we have m candidate projections w1,w2, ...,wm. For each projection, the data points are separated into two sets and labeled with opposite bit. We denote these two partitions as Ti0 and Ti1, respectively. The entropy \u03b4i with respect to the projection wi can be computed as:\n\u03b4i = \u2212Pi0 logPi0 \u2212 Pi1 logPi1 (11)\nwhere:\nPi0 = |Ti0|\n|Ti0|+ |Ti1| , Pi1 =\n|Ti1|\n|Ti0|+ |Ti1| (12)\nIn practice, the database can be very large and computing the entropy of each projection with respect to the entire database is time consuming. Thus, we estimate the entropy simply by using the group centers. For group center \u00b5i, we assign a weight \u03bdi based on the size of the group.\n\u03bdi = |Si|\n\u2211k p=1 |Sp| (13)\nWe denote the two sets of group centers as Ci0 and Ci1. Then Pi0 and Pi1 can be computed as:\nPi0 = \u2211\ns\u2208Ci0\n\u03bds, Pi1 = \u2211\nt\u2208Ci1\n\u03bdt (14)\nThis simplification significantly reduces the time cost on the entropy calculation. After obtaining the entry \u03b4i for each wi, we sort them in descending order and use the top L projections for creating the L-bit binary codes, according to Eq.(9). The overall procedure of our DSH algorithm is summarized in Alg. 1."}, {"heading": "3.4 Computational Complexity Analysis", "text": "Given n data points with the dimensionality d, the computational complexity of DSH in the training stage is as follows:\n1) O(\u03b1Lpnd): k-means with p iterations to generate \u03b1L groups (Step 1 in Alg. 1). 2) O((\u03b1L)2(d+r)): Find all the r-adjacent groups (Step 2 in Alg. 1). 3) O(\u03b1Lrd): For each pair of adjacent groups, generate the projection and the intercept (Step 3 in Alg. 1). 4) Compute the entropy for all the candidate projections needs O((\u03b1L)2dr) (Step 4 in Alg. 1). 5) The top L projections can be found within O(\u03b1Lr log(\u03b1Lr)). The binary codes for data points can be obtained in O(Lnd) (Step 5 in Alg. 1).\nConsidering \u03b1Lr \u226a n, the overall computational complexity of DSH training is dominated by the k-means clustering step which is O(\u03b1Lpnd). It is clear that DSH scales linearly with respect to the number of samples in the database.\nAlgorithm 1 Density Sensitive Hashing\nInput: n training samples x1, x2, . . . , xn \u2208 Rd; L: the number of bits for hashing codes; \u03b1: the parameter controlling the groups number; p: the number of iterations in the k-means; r: the parameter for r-adjacent groups\n1: Use k-means with p iterations to generate \u03b1L groups, with centers \u00b51, \u00b7 \u00b7 \u00b7 , \u00b5\u03b1L. 2: Generate the list of all r-adjacent groups based on the definition (1) and (2). 3: For each pair of adjacent groups, use Eq.(10) to generate the projection w and intercept t. 4: Calculate the entropy of all the candidate projections using the weighted center points based on Eq.(11) and Eq.(14) 5: Sort the entropy values in descending order and use the top L projections to create binary codes according to Eq.(9). Output: The model: {wi, ti}Li=1 Binary hashing codes for the training samples: Y \u2208 {0, 1}n\u00d7L\nIn the testing stage, given a query point, DSH needs O(Ld) to compress the query point into a binary code, which is the same as the complexity of Locality Sensitive Hashing."}, {"heading": "4 EXPERIMENT", "text": "In this section, we evaluate our DSH algorithm on the high dimensional nearest neighbor search problem. Three large scale real-world data sets are used in our experiments.\n\u2022 GIST1M: It contains one million GIST features and each feature is represented by a 960-dim vector. This data set is publicly available2.\n2. http://corpus-texmex.irisa.fr\n\u2022 Flickr1M: We collect one million images from the Flickr and use a feature extraction code3 to extract a GIST feature for each image. Each image is represented by a 512-dim GIST feature vector. This data set is publicly available4. \u2022 SIFT1M: It contains one million SIFT features and each feature is represented by a 128-dim vector. This data set is publicly available5.\nFor each data set, we randomly select 1k data points as the queries and use the remaining to form the gallery database. We use the same criterion as in [33], [36], that a returned point is considered to be a true neighbor if it lies in the top 2 percentile points closest (measured by the Euclidian distance in the original space) to the query. For each query, all the data points in the database are ranked according to their Hamming distances to the query. We evaluate the retrieval results by the Mean Average Precision (MAP) and the precision-recall curve [33]. In addition, we also report the training time and the testing time (the average time used for each query) for all the methods."}, {"heading": "4.1 Compared Algorithms", "text": "Seven state-of-the-art hashing algorithms for high dimensional nearest neighbors search are compared as follows:\n\u2022 Locality Sensitive Hashing (LSH) [8], which is based on the random projection. The projective vectors are randomly sampling from a p-stable distribution (e.g., Gaussian). We implement the algorithm by ourselves and make it publicly available6. \u2022 Kernelized Locality Sensitive Hashing (KLSH) [22], which generalizes the LSH method to the kernel space. We use the code provided by the authors7.\n3. http://www.vision.ee.ethz.ch/\u223czhuji/felib.html 4. http://www.cad.zju.edu.cn/home/dengcai/Data/NNSData.html 5. http://corpus-texmex.irisa.fr 6. http://www.cad.zju.edu.cn/home/dengcai/Data/DSH.html 7. http://www.cse.ohio-state.edu/\u223ckulis/klsh/klsh.htm\n\u2022 Shift-Invariant Kernel Hashing (SIKH) [30], which is a distribution-free method based on the random features mapping for approximating shift-invariant kernels. The code is also publicly available8. \u2022 Principle Component Analysis Hashing (PCAH) [34], which directly uses the top principal directions as the projective vectors to obtain the binary codes. The implementation of PCA is publicly available9. \u2022 Spectral Hashing (SpH) [35], which is based on quantizing the values of analytical eigenfunctions computed along PCA directions of the data. We use the code provided by the authors10. \u2022 Anchor Graph Hashing (AGH) [24], which constructs an anchor graph to speed up the spectral analysis procedure. AGH with two-layer is used in our comparison for its superior performance over AGH with one-layer [24]. We use the code provided by the authors11 and the number of anchors is set to be 300 and the number of nearest neighbors is set to be 2 as suggested in [24]. \u2022 Density Sensitive Hashing (DSH), which is the\n8. http://www.unc.edu/\u223cyunchao/itq.htm 9. http://www.cad.zju.edu.cn/home/dengcai/Data/DimensionReduction.html 10. http://www.cs.huji.ac.il/\u223cyweiss/SpectralHashing/ 11. http://www.ee.columbia.edu/\u223cwliu/\nmethod introduced in this paper. For the purpose of reproducibility, we also make the code publicly available12. There are three parameters. We empirically set p = 3 (the number of iterations in k-means), \u03b1 = 1.5 (controlling the groups number), r = 3 (for r-adjacent groups). A detailed analysis on the parameter selection will be provided later.\nIt is important to note that LSH, KLSH and SIKH are random projection based methods, while PCAH, SpH and AGH are learning based methods. Our DSH can be regarded as a combination of these two directions."}, {"heading": "4.2 Experimental Results", "text": "Figure 2 shows the MAP curves of all the algorithms on the GIST1M, Flickr1M and SIFT1M data sets, respectively. We can see that the three random projection based methods (LSH, KLSH and SIKH) have a low MAP when the code length is short. As the code length increases, the performances of all the three methods consistently increases. On the other hand, the learning based methods (PCAH, SpH and AGH) have a high MAPwhen the code length is short. However, they fail to make significant improvements as the code length increases. Particulary, the performance of PCAH decreases as the code length\n12. http://www.cad.zju.edu.cn/home/dengcai/Data/DSH.html\n7\nincreases. This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33]. By utilizing the geometric structure of the data to guide the projections selection, our DSH successfully combines the advantages of both random projection based methods and the learning based methods. As a result, DSH achieves a satisfied performance on the three data sets and almost outperforms its competitors for all code lengths. It is interesting to see that the performance improvements of DSH over other methods on GIST1M and Flickr1M are larger than that on SIFT1M. Since the dimensions of the data in GIST1M (960-d) and Flickr1M (512-d) are much larger than that in SIFT1M (128-d), this suggests that our DSH method are particularly suitable for high dimensional situations. Figure 3 presents the precision-recall curves of all the algorithms on three data sets with the codes of 48 bits and 96 bits.\nTable 1, 2 and 3 show both the training and testing time for different algorithms on three data sets, respectively. We can clearly see that both the training and\ntesting time of all the methods decrease as the dimension of the data decreases. Considering the training time, the three random projection based algorithms are relatively efficient, especially for LSH and SIKH. KLSH needs to compute a sampled kernel matrix which slows down its computation. The three learning based algorithms are relatively slow, for exploring the data structure. Our DSH is also fast. Although it is slower than the three random projection based algorithms, it is significantly faster than SpH and AGH. Considering the testing time, LSH, PCAH and our DSH are the most efficient methods. All of them simply need a matrix multiplication and a thresholding to obtain the binary codes. SpH consumes much longer time than other methods as the code length increases since it needs to compute the analytical eigenfunctions involving the calculation of trigonometric functions."}, {"heading": "4.3 Parameter Selection", "text": "Our DSH has three parameters: p (the number of iterations in k-means), \u03b1 (the parameter controlling the groups number) and r (the parameter for r-adjacent\nTABLE 4 Training time (s) of DSH vs. the number of iterations of k-means (p) at 64 bits.\nData Set p = 1 p = 2 p = 3 p = 4 p = 5 p = 6 GIST1M 18.8 37.2 56.5 76.2 94.1 111.7 Flickr1M 11.7 23.5 35.8 48.1 62.6 76.4 SIFT1M 4.8 9.1 15.5 21.2 25.5 31.9\n0.5 1 1.5 2 2.5 3 0.05\n0.1\n0.15\n0.2\n0.25\n\u03b1\nM ea\nn A\nve ra\nge P\nre ci\nsi on\nLSH KLSH SIKH PCAH SpH AGH DSH\n(a) GIST1M\n0.5 1 1.5 2 2.5 3 0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03b1\nM ea\nn A\nve ra\nge P\nre ci\nsi on\nLSH KLSH SIKH PCAH SpH AGH DSH\n(b) Flickr1M\n0.5 1 1.5 2 2.5 3 0.2\n0.25\n0.3\n0.35\n0.4\n\u03b1\nM ea\nn A\nve ra\nge P\nre ci\nsi on\nLSH KLSH SIKH PCAH SpH AGH DSH\n(c) SIFT1M\nFig. 5. The performance of DSH vs. the parameter \u03b1 (controlling the number of groups) at 64 bits.\ngroups). In this subsection, we discuss how the performance of DSH will be influenced by these three parameters. We learn 64-bits hashing codes and the default setting for these parameters is p = 3, \u03b1 = 1.5 and r = 3. When we study the impact of one parameter, the other parameters are fixed as the default.\nFigure 4 and Table 4 show how the performance of DSH varies as the number of iterations in k-means varies. As the number of iterations increases, it is reasonable to see that both the MAP and the learning time\nof DSH increase. On all the three data sets, 3 iterations in k-means are enough for achieving reasonably good MAP.\nFigure 5 and Table 5 show how the performance of DSH varies as \u03b1 changes (the groups number generated by k-means changes). As we can see, as \u03b1 becomes larger (the groups number increases), both the MAP and learning time of DSH increase. Setting \u03b1 = 1.5 is a reasonable balance considering both the accuracy and the efficiency.\nFigure 6 shows the performance of DSH varies as r (r-adjacent groups) changes. DSH achieves stable and consistent good performance as r is less than 5. As r becomes larger, DSH generates more projections which are used to separate two far away groups. These projections are usually less critical and redundant. Thus, the performance of DSH decreases."}, {"heading": "5 CONCLUSION", "text": "In this paper, we have developed a novel hashing algorithm, called Density Sensitive Hashing (DSH), for high dimensional nearest neighbors search. Different from those random projection based hashing approaches, e.g., Locality Sensitive Hashing, DSH uses the geometric structure of the data to guide the projections selection. As a result, DSH can generate hashing codes with more discriminating power. Empirical studies on three large data sets show that the proposed algorithm scales well to data size and significantly outperforms the state-ofthe-art hashing methods in terms of retrieval accuracy."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM, 51(1):117\u2013122,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "The Priority R-tree: a practically efficient and worst-case optimal R-tree", "author": ["L. Arge", "M. Berg", "H. Haverkort", "K. Yi"], "venue": "SIGMOD,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["K. B", "T. Darrell"], "venue": "In The Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J. Bentley"], "venue": "Communications of the ACM, 18:509\u2013517,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "When is nearest neighbor meaningful", "author": ["K. Beyer", "J. Goldstein", "R. Ramakrishnan", "U. Shaft"], "venue": "In ICDT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Transform coding for fast approximate nearest neighbor search in high dimensions", "author": ["J. Brandt"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1815\u20131822,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectral Regression: A Regression Framework for Efficient Regularized Subspace Learning", "author": ["D. Cai"], "venue": "PhD thesis, Department of Computer Science, University of Illinois at Urbana-Champaign, May", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "STOC, pages 380\u2013388,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Fast locality-sensitive hashing", "author": ["A. Dasgupta", "R. Kumar", "T. Sarls"], "venue": "IEEE International Conference on Knowledge Discovery and Data Mining, pages 1073\u20131081,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Localitysensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "Symposium on Computational Geometry 2004, pages 253\u2013262,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Continuous visible nearest neighbor query processing in spatial databases", "author": ["Y. Gao", "B. Zheng", "G. Chen", "Q. Li", "X. Guo"], "venue": "VLDB J., 20(3):371\u2013396,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 817\u2013824,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable similarity search with optimized kernel hashing", "author": ["J. He", "W. Liu", "S.-F. Chang"], "venue": "IEEE International Conference on Knowledge Discovery and Data Mining, pages 1129\u20131138,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117\u2013128,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised simhash for efficient document similarity search", "author": ["Q. Jiang", "M. Sun"], "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 93\u2013101,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics, 26:189\u2013206,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1984}, {"title": "A posteriori multi-probe locality sensitive hashing", "author": ["A. Joly", "O. Buisson"], "venue": "ACM Multimedia, pages 209\u2013218,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Random maximum margin hashing", "author": ["A. Joly", "O. Buisson"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 873\u2013880,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Voronoi-based k nearest neighbor search for spatial network databases", "author": ["M.R. Kolahdouzan", "C. Shahabi"], "venue": "International Conference on Very Large Data Bases, pages 840\u2013851,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast nearest neighbor search in medical image databases", "author": ["F. Korn", "N. Sidiropoulos", "C. Faloutsos", "E. Siegel", "Z. Protopapas"], "venue": "International Conference on Very Large Data Bases, pages 215\u2013226,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Hashing algorithms for large-scale learning", "author": ["P. Li", "A. Shrivastava", "J. Moore", "C. Konig"], "venue": "The Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-probe lsh: Efficient indexing for high-dimensional similarity search", "author": ["Q. Lv", "W. Josephson", "Z. Wang", "M. Charikar", "K. Li"], "venue": "International Conference on Very Large Data Bases, pages 950\u2013961,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Weakly-supervised hashing in kernel space", "author": ["Y. Mu", "J. Shen", "S. Yan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 3344\u20133351,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Entropy based nearest neighbor search in high dimensions", "author": ["R. Panigrahy"], "venue": "SODA, pages 1186\u20131195,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["L. Paulev\u00e9", "H. J\u00e9gou", "L. Amsaleg"], "venue": "Pattern Recognition Letters, 31(11):1348\u20131358,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "The Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Journal of Approximate Reasoning, 50(7):969\u2013978,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised hashing for scalable image retrieval", "author": ["J. Wang", "O. Kumar", "S.-F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 3424\u20133431,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Annosearch: Image auto-annotation by search", "author": ["X.-J. Wang", "L. Zhang", "F. Jing", "W.-Y. Ma"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1483\u20131490,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "The Neural Information Processing Systems, pages 1753\u20131760,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Complementary hashing for approximate nearest neighbor search", "author": ["H. Xu", "J. Wang", "Z. Li", "G. Zeng", "N. Yu", "S. Li"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Laplacian co-hashing of terms and documents", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "ECIR, pages 577\u2013580,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "SIGIR, pages 18\u201325,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14].", "startOffset": 108, "endOffset": 111}, {"referenceID": 10, "context": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "KD-tree [4] and R-tree [2]), have been proposed for nearest neighbors search.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "KD-tree [4] and R-tree [2]), have been proposed for nearest neighbors search.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Unfortunately, these approaches perform worse than a linear scan when the dimensionality of the space is high [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10].", "startOffset": 155, "endOffset": 158}, {"referenceID": 9, "context": "Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Given a database with n samples, LSH makes no prior assumption about the data distribution and offers probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 247, "endOffset": 251}, {"referenceID": 26, "context": "Given a database with n samples, LSH makes no prior assumption about the data distribution and offers probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 253, "endOffset": 257}, {"referenceID": 16, "context": "However, according to the Jonson Lindenstrauss Theorem [17], LSH needs O(lnn/\u01eb) random projections to preserve the pairwise distances, where \u01eb is the relative error.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Despite the success of these approaches for relatively small codes, they often fail to make significant improvement as the code length increases [19].", "startOffset": 145, "endOffset": 149}, {"referenceID": 32, "context": "For the linear projection-based hashing, we have [33]", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "Using this hash function, two points\u2019 hash bits match with the probability proportional to the cosine of the angle between them [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 21, "context": "Specifically, for any two points xi,xj \u2208 R, we have [22]:", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Based on this nice property, LSH have the probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 187, "endOffset": 191}, {"referenceID": 26, "context": "Based on this nice property, LSH have the probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "Empirical studies [1] showed that the LSH is significantly more efficient than the methods based on hierarchical tree decomposition.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "To address this limitation, Kernelized Locality Sensitive Hashing is introduced in [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "It suggests to approximate a normal distribution in the kernel space using only kernel comparisons [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "In addition, the Shift Invariant Kernels Hashing [30], which is a distribution-free method based on the random features", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "This method has theoretical convergence guarantees and performs well for relatively large code sizes [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "According to the Jonson Lindenstrauss Theorem [17], O(lnn/\u01eb) projective vectors are needed to preserve the pairwise distances of a database with size n for the random projection, where \u01eb is the relative error.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 121, "endOffset": 125}, {"referenceID": 28, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 127, "endOffset": 131}, {"referenceID": 30, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 145, "endOffset": 149}, {"referenceID": 35, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 151, "endOffset": 155}, {"referenceID": 36, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 157, "endOffset": 161}, {"referenceID": 37, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "PCA Hashing [34] might be the simplest one.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 37, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "The spectral analysis of the data affinity matrix is usually time consuming [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 34, "context": "[35] made a strong assumption that data is uniformly distributed and proposed a Spectral Hashing method (SpH).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Anchor Graph Hashing (AGH) [24] is a recently proposed method to overcome this shortcoming.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "Some other learning based hashing methods include Semantic Hashing [31] which uses the stacked Restricted Boltzmann Machine (RBM) to generate the compact binary codes; Semi-supervised Sequential Projection Hashing (S3PH) [33] which can incorporate supervision information.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "Some other learning based hashing methods include Semantic Hashing [31] which uses the stacked Restricted Boltzmann Machine (RBM) to generate the compact binary codes; Semi-supervised Sequential Projection Hashing (S3PH) [33] which can incorporate supervision information.", "startOffset": 221, "endOffset": 225}, {"referenceID": 18, "context": "Despite the success of these learning based hashing approaches for relatively small codes, they often fail to make significant improvement as the code length increases [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "(a) Locality Sensitive Hashing [8] 00 01 10 11", "startOffset": 31, "endOffset": 34}, {"referenceID": 33, "context": "(b) PCA Hashing [34] 00 01 10 11", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "LSH [8] generates the projections randomly and it is very possible that the data points from the same Gaussian will be encoded by different hash codes.", "startOffset": 4, "endOffset": 7}, {"referenceID": 33, "context": "PCA Hashing [34] uses the principle directions of the data as the projective vectors.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "[29] show that a quantized preprocess for the data points can significantly improve the performance of the nearest neighbors search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "From the information theoretic point of view, a \u201dgood\u201d binary codes should maximize the information/entropy provided by each bit [38].", "startOffset": 129, "endOffset": 133}, {"referenceID": 31, "context": "Using maximum entropy principle, a binary bit that gives balanced partitioning of the data points provides maximum information [32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 32, "context": "We use the same criterion as in [33], [36], that a returned point is considered to be a true neighbor if it lies in the top 2 percentile points closest (measured by the Euclidian distance in the original space) to the query.", "startOffset": 32, "endOffset": 36}, {"referenceID": 35, "context": "We use the same criterion as in [33], [36], that a returned point is considered to be a true neighbor if it lies in the top 2 percentile points closest (measured by the Euclidian distance in the original space) to the query.", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "We evaluate the retrieval results by the Mean Average Precision (MAP) and the precision-recall curve [33].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "\u2022 Locality Sensitive Hashing (LSH) [8], which is based on the random projection.", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "\u2022 Kernelized Locality Sensitive Hashing (KLSH) [22], which generalizes the LSH method to the kernel space.", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "\u2022 Shift-Invariant Kernel Hashing (SIKH) [30], which is a distribution-free method based on the random features mapping for approximating shift-invariant kernels.", "startOffset": 40, "endOffset": 44}, {"referenceID": 33, "context": "\u2022 Principle Component Analysis Hashing (PCAH) [34], which directly uses the top principal directions as the projective vectors to obtain the binary codes.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "\u2022 Spectral Hashing (SpH) [35], which is based on quantizing the values of analytical eigenfunctions computed along PCA directions of the data.", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "\u2022 Anchor Graph Hashing (AGH) [24], which constructs an anchor graph to speed up the spectral analysis procedure.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "AGH with two-layer is used in our comparison for its superior performance over AGH with one-layer [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "We use the code provided by the authors and the number of anchors is set to be 300 and the number of nearest neighbors is set to be 2 as suggested in [24].", "startOffset": 150, "endOffset": 154}, {"referenceID": 7, "context": "LSH [8] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "1\u00d7 10 KLSH [22] 27.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "5\u00d7 10 SIKH [30] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "9\u00d7 10 PCAH [34] 31.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "2\u00d7 10 SpH [35] 42.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "1 \u00d7 10 AGH [24] 340.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "LSH [8] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "6\u00d7 10 KLSH [22] 18.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "1\u00d7 10 SIKH [30] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "3\u00d7 10 PCAH [34] 16.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "9\u00d7 10 SpH [35] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "6 \u00d7 10 AGH [24] 232.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "LSH [8] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "4\u00d7 10 KLSH [22] 10.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "7\u00d7 10 SIKH [30] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "0\u00d7 10 PCAH [34] 3.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "5\u00d7 10 SpH [35] 11.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "9 \u00d7 10 AGH [24] 135.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33].", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33].", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33].", "startOffset": 270, "endOffset": 274}], "year": 2012, "abstractText": "Nearest neighbors search is a fundamental problem in various research fields like machine learning, data mining and pattern recognition. Recently, hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to be effective for scalable high dimensional nearest neighbors search. Many hashing algorithms found their theoretic root in random projection. Since these algorithms generate the hash tables (projections) randomly, a large number of hash tables (i.e., long codewords) are required in order to achieve both high precision and recall. To address this limitation, we propose a novel hashing algorithm called Density Sensitive Hashing (DSH) in this paper. DSH can be regarded as an extension of LSH. By exploring the geometric structure of the data, DSH avoids the purely random projections selection and uses those projective functions which best agree with the distribution of the data. Extensive experimental results on real-world data sets have shown that the proposed method achieves better performance compared to the state-of-the-art hashing approaches.", "creator": "LaTeX with hyperref package"}}}