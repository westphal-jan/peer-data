{"id": "1205.3054", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Approximate Modified Policy Iteration", "abstract": "modified policy recovery iteration ( formerly mpi ) is a dynamic programming ( dp ) algorithm that contains the two celebrated policy and estimated value data iteration elimination methods. despite its generality, mpi sampling has not now been thoroughly studied, especially its approximation form decomposition which is used when the invariant state constraint and / whether or action spaces are large or infinite. in this paper, we propose simply three approximate mpi ( ampi ) algorithms that are extensions of the well - many known other approximate dp algorithms : fitted - value iteration, fitted - q iteration, and classification - based policy iteration. we provide an error propagation analysis for ampi that unifies those for approximate policy preparation and value iteration. we do also even provide a finite - sample analysis for the classification - based implementation of ampi ( cbmpi ), which is more visually general ( and somehow contains ) than the analysis of the other presented ampi algorithms. an interesting observation is that the mpi's parameter allows us today to control the balance of errors ( in value function approximation and in estimating the greedy policy ) reflected in the final performance of testing the cbmpi algorithm.", "histories": [["v1", "Mon, 14 May 2012 15:01:31 GMT  (66kb,D)", "https://arxiv.org/abs/1205.3054v1", null], ["v2", "Fri, 18 May 2012 06:56:47 GMT  (68kb,D)", "http://arxiv.org/abs/1205.3054v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bruno scherrer", "victor gabillon", "mohammad ghavamzadeh", "matthieu geist"], "accepted": true, "id": "1205.3054"}, "pdf": {"name": "1205.3054.pdf", "metadata": {"source": "META", "title": "Approximate Modified Policy Iteration", "authors": ["Bruno Scherrer", "Victor Gabillon", "Mohammad Ghavamzadeh"], "emails": ["Bruno.Scherrer@inria.fr", "Victor.Gabillon@inria.fr", "Mohammad.Ghavamzadeh@inria.fr", "Matthieu.Geist@supelec.fr"], "sections": [{"heading": "1. Introduction", "text": "Modified Policy Iteration (MPI) (Puterman & Shin, 1978) is an iterative algorithm to compute the optimal policy and value function of a Markov Decision Process (MDP). Starting from an arbitrary value function v0, it generates a sequence of value-policy pairs\n\u03c0k+1 = G vk (greedy step) (1) vk+1 = (T\u03c0k+1) mvk (evaluation step) (2)\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nwhere G vk is a greedy policy w.r.t. vk, T\u03c0k is the Bellman operator associated to the policy \u03c0k, and m \u2265 1 is a parameter. MPI generalizes the well-known dynamic programming algorithms Value Iteration (VI) and Policy Iteration (PI) for values m = 1 and m =\u221e, respectively. MPI has less computation per iteration than PI (in a way similar to VI), while enjoys the faster convergence of the PI algorithm (Puterman & Shin, 1978). In problems with large state and/or action spaces, approximate versions of VI (AVI) and PI (API) have been the focus of a rich literature (see e.g. Bertsekas & Tsitsiklis 1996; Szepesva\u0301ri 2010). The aim of this paper is to show that, similarly to its exact form, approximate MPI (AMPI) may represent an interesting alternative to AVI and API algorithms.\nIn this paper, we propose three implementations of AMPI (Sec. 3) that generalize the AVI implementations of Ernst et al. (2005); Antos et al. (2007); Munos & Szepesva\u0301ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al. (2011). We then provide an error propagation analysis of AMPI (Sec. 4), which shows how the Lp-norm of its performance loss can be controlled by the error at each iteration of the algorithm. We show that the error propagation analysis of AMPI is more involved than that of AVI and API. This is due to the fact that neither the contraction nor monotonicity arguments, that the error propagation analysis of these two algorithms rely on, hold for AMPI. The analysis of this section unifies those for AVI and API and is applied to the AMPI implementations presented in Sec. 3. We detail the analysis of the classification-based implementation of MPI (CBMPI) of Sec. 3 by providing its finite sample analysis in Sec. 5. Our analysis indicates that the parameter m allows us to balance the estimation error of the classifier with the overall quality of the value approximaar X iv :1 20 5.\n30 54\nv2 [\ncs .A\nI] 1\n8 M\nay 2\ntion. We report some preliminary results of applying CBMPI to standard benchmark problems and comparing it with some existing algorithms in (Scherrer et al., 2012, Appendix G)."}, {"heading": "2. Background", "text": "We consider a discounted MDP \u3008S,A, P, r, \u03b3\u3009, where S is a state space, A is a finite action space, P (ds\u2032|s, a), for all (s, a), is a probability kernel on S, the reward function r : S \u00d7 A \u2192 R is bounded by Rmax, and \u03b3 \u2208 (0, 1) is a discount factor. A deterministic policy is defined as a mapping \u03c0 : S \u2192 A. For a policy \u03c0, we may write r\u03c0(s) = r ( s, \u03c0(s) ) and P\u03c0(ds \u2032|s) = P ( ds\u2032|s, \u03c0(s) ) . The value of policy \u03c0 in a state s is defined as the expected discounted sum of rewards received starting from state s and following the policy \u03c0, i.e.,v\u03c0(s) = E [ \u2211\u221e t=0 \u03b3\ntr\u03c0(st)| s0 = s, st+1 \u223c P\u03c0(\u00b7|st) ] . Similarly, the action-value function of a policy \u03c0 at a state-action pair (s, a), Q\u03c0(s, a), is the expected discounted sum of rewards received starting from state s, taking action a, and then following the policy. Since the rewards are bounded by Rmax, the values and action-values should be bounded by Vmax = Qmax = Rmax/(1 \u2212 \u03b3). The Bellman operator T\u03c0 of policy \u03c0 takes a function f on S as input and returns the function T\u03c0f defined as \u2200s, [T\u03c0f ](s) = E [ r\u03c0(s) + \u03b3f(s \u2032) | s\u2032 \u223c P\u03c0(.|s) ] , or in compact form, T\u03c0f = r\u03c0 + \u03b3P\u03c0f . It is known that v\u03c0 is the unique fixed-point of T\u03c0. Given a function f on S, we say that a policy \u03c0 is greedy w.r.t. f , and write it as \u03c0 = G f , if \u2200s, (T\u03c0f)(s) = maxa(Taf)(s), or equivalently T\u03c0f = max\u03c0\u2032(T\u03c0\u2032f). We denote by v\u2217 the optimal value function. It is also known that v\u2217 is the unique fixed-point of the Bellman optimality operator T : v \u2192 max\u03c0 T\u03c0v = TG(v)v, and that a policy \u03c0\u2217 that is greedy w.r.t. v\u2217 is optimal and its value satisfies v\u03c0\u2217 = v\u2217."}, {"heading": "3. Approximate MPI Algorithms", "text": "In this section, we describe three approximate MPI (AMPI) algorithms. These algorithms rely on a function space F to approximate value functions, and in the third algorithm, also on a policy space \u03a0 to represent greedy policies. In what follows, we describe the iteration k of these iterative algorithms."}, {"heading": "3.1. AMPI-V", "text": "For the first and simplest AMPI algorithm presented in the paper, we assume that the values vk are represented in a function space F \u2286 R|S|. In any state s, the action \u03c0k+1(s) that is greedy w.r.t. vk can be\nestimated as follows:\n\u03c0k+1(s) = arg max a\u2208A\n1\nM ( M\u2211 j=1 r(j)a + \u03b3vk(s (j) a ) ) , (3)\nwhere \u2200a \u2208 A and 1 \u2264 j \u2264 M , r(j)a and s(j)a are samples of rewards and next states when action a is taken in state s. Thus, approximating the greedy action in a state s requires M |A| samples. The algorithm works as follows. It first samples N states from a distribution \u00b5, i.e., {s(i)}Ni=1 \u223c \u00b5. From each sampled state s(i), it generates a rollout of size\nm, i.e., ( s(i), a\n(i) 0 , r (i) 0 , s (i) 1 , . . . , a (i) m\u22121, r (i) m\u22121, s (i) m\n) , where\na (i) t is the action suggested by \u03c0k+1 in state s (i) t , computed using Eq. 3, and r (i) t and s (i) t+1 are the reward and next state induced by this choice of action. For each s(i), we then compute a rollout estimate v\u0302k+1(s (i)) = \u2211m\u22121 t=0 \u03b3 tr (i) t +\u03b3 mvk(s (i) m ), which is an un-\nbiased estimate of [( T\u03c0k+1 )m vk ]\n(s(i)). Finally, vk+1 is computed as the best fit in F to these estimates, i.e.,\nvk+1 = FitF ({( s(i), v\u0302k+1(s (i)) )}N\ni=1\n) .\nEach iteration of AMPI-V requires N rollouts of size m, and in each rollout any of the |A| actions needs M samples to compute Eq. 3. This gives a total of Nm(M |A|+1) transition samples. Note that the fitted value iteration algorithm (Munos & Szepesva\u0301ri, 2008) is a special case of AMPI-V when m = 1."}, {"heading": "3.2. AMPI-Q", "text": "In AMPI-Q, we replace the value function v : S \u2192 R with an action-value function Q : S \u00d7 A \u2192 R. The Bellman operator for a policy \u03c0 at a state-action pair (s, a) can then be written as\n[T\u03c0Q](s, a) = E [ r\u03c0(s, a) +\u03b3Q(s \u2032, \u03c0(s\u2032))|s\u2032 \u223c P (\u00b7|s, a) ] ,\nand the greedy operator is defined as\n\u03c0 = GQ \u21d4 \u2200s \u03c0(s) = arg max a\u2208A Q(s, a).\nIn AMPI-Q, action-value functions Qk are represented in a function space F \u2286 R|S\u00d7A|, and the greedy action w.r.t. Qk at a state s, i.e., \u03c0k+1(s), is computed as\n\u03c0k+1(s) \u2208 arg max a\u2208A Qk(s, a). (4)\nThe evaluation step is similar to that of AMPI-V, with the difference that now we work with stateaction pairs. We sample N state-action pairs from a distribution \u00b5 on S \u00d7 A and build a rollout set\nDk = {(s(i), a(i))}Ni=1, (s(i), a(i)) \u223c \u00b5. For each (s(i), a(i)) \u2208 Dk, we generate a rollout of size m, i.e., ( s(i), a(i), r\n(i) 0 , s (i) 1 , a (i) 1 , \u00b7 \u00b7 \u00b7 , s (i) m , a (i) m\n) , where the\nfirst action is a(i), a (i) t for t \u2265 1 is the action suggested by \u03c0k+1 in state s (i) t computed using Eq. 4, and r (i) t and s (i) t+1 are the reward and next state induced by this choice of action. For each (s(i), a(i)) \u2208 Dk, we then compute the rollout estimate\nQ\u0302k+1(s (i), a(i)) = m\u22121\u2211 t=0 \u03b3tr (i) t + \u03b3 mQk(s (i) m , a (i) m ),\nwhich is an unbiased estimate of[ (T\u03c0k+1) mQk ] (s(i), a(i)). Finally, Qk+1 is the best fit to these estimates in F , i.e.,\nQk+1 = FitF ({( (s(i), a(i)), Q\u0302k+1(s (i), a(i)) )}N\ni=1\n) .\nEach iteration of AMPI-Q requires Nm samples, which is less than that for AMPI-V. However, it uses a hypothesis space on state-action pairs instead of states. Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1."}, {"heading": "3.3. Classification-Based MPI", "text": "The third AMPI algorithm presented in this paper, called classification-based MPI (CBMPI), uses an ex-\nplicit representation for the policies \u03c0k, in addition to the one used for value functions vk. The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).\nIn order to describe CBMPI, we first rewrite the MPI formulation (Eqs. 1 and 2) as\nvk = (T\u03c0k) mvk\u22121 (evaluation step) (5) \u03c0k+1 = G [ (T\u03c0k) mvk\u22121 ] (greedy step) (6)\nNote that in the new formulation both vk and \u03c0k+1 are functions of (T\u03c0k)\nmvk\u22121. CBMPI is an approximate version of this new formulation. As described in Fig. 1, CBMPI begins with arbitrary initial policy \u03c01 \u2208 \u03a0 and value function v0 \u2208 F .1 At each iteration k, a new value function vk is built as the best approximation of the m-step Bellman operator (T\u03c0k)\nmvk\u22121 in F (evaluation step). This is done by solving a regression problem whose target function is (T\u03c0k)\nmvk\u22121. To set up the regression problem, we build a rollout set Dk by sampling n states i.i.d. from a distribution \u00b5.2 For each state s(i) \u2208 Dk, we generate a rollout ( s(i), a\n(i) 0 , r (i) 0 , s (i) 1 , . . . , a (i) m\u22121, r (i) m\u22121, s (i) m\n) of size m,\nwhere a (i) t = \u03c0k(s (i) t ), and r (i) t and s (i) t+1 are the reward and next state induced by this choice of action. From this rollout, we compute an unbiased estimate v\u0302k(s\n(i)) of [ (T\u03c0k) mvk\u22121 ] (s(i)) as\nv\u0302k(s (i)) = m\u22121\u2211 t=0 \u03b3tr (i) t + \u03b3 mvk\u22121(s (i) m ), (7)\nand use it to build a training set {( s(i), v\u0302k(s (i)) )}n i=1\n. This training set is then used by the regressor to compute vk as an estimate of (T\u03c0k) mvk\u22121.\nThe greedy step at iteration k computes the policy \u03c0k+1 as the best approximation of G [ (T\u03c0k) mvk\u22121 ]\nby solving a cost-sensitive classification problem. From the definition of a greedy policy, if \u03c0 = G [ (T\u03c0k) mvk\u22121 ] , for each s \u2208 S, we have[ T\u03c0(T\u03c0k) mvk\u22121 ] (s) = max\na\u2208A\n[ Ta(T\u03c0k) mvk\u22121 ] (s). (8)\nBy defining Qk(s, a) = [ Ta(T\u03c0k) mvk\u22121 ] (s), we may\n1Note that the function space F and policy space \u03a0 are automatically defined by the choice of the regressor and classifier, respectively.\n2Here we used the same sampling distribution \u00b5 for both regressor and classifier, but in general different distributions may be used for these two components.\nrewrite Eq. 8 as Qk ( s, \u03c0(s) ) = max\na\u2208A Qk(s, a). (9)\nThe cost-sensitive error function used by CBMPI is of the form\nL\u03a0\u03c0k,vk\u22121(\u00b5;\u03c0) = \u222b S [ max a\u2208A Qk(s, a)\u2212Qk ( s, \u03c0(s) )] \u00b5(ds).\nTo simplify the notation we use L\u03a0k instead of L\u03a0\u03c0k,vk\u22121 . To set up this cost-sensitive classification problem, we build a rollout set D\u2032k by sampling N states i.i.d. from a distribution \u00b5. For each state s(i) \u2208 D\u2032k and each action a \u2208 A, we build M independent rollouts of size m+ 1, i.e.,3( s(i), a, r\n(i,j) 0 , s (i,j) 1 , a (i,j) 1 , . . . , a (i,j) m , r (i,j) m , s (i,j) m+1 )M j=1 ,\nwhere for t \u2265 1, a(i,j)t = \u03c0k(s (i,j) t ), and r (i,j) t and s (i,j) t+1 are the reward and next state induced by this choice of action. From these rollouts, we compute an unbiased estimate of Qk(s (i), a) as Q\u0302k(s (i), a) =\n1 M \u2211M j=1R j k(s (i), a) where\nRjk(s (i), a) = m\u2211 t=0 \u03b3tr (i,j) t + \u03b3 m+1vk\u22121(s (i,j) m+1).\nGiven the outcome of the rollouts, CBMPI uses a costsensitive classifier to return a policy \u03c0k+1 that minimizes the following empirical error\nL\u0302\u03a0k (\u00b5\u0302;\u03c0) = 1\nN N\u2211 i=1 [ max a\u2208A Q\u0302k(s (i), a)\u2212 Q\u0302k ( s(i), \u03c0(s(i)) )] ,\nwith the goal of minimizing the true error L\u03a0k (\u00b5;\u03c0).\nEach iteration of CBMPI requires nm+M |A|N(m+1) (or M |A|N(m + 1) in case we reuse the rollouts, see Footnote 3) transition samples. Note that when m tends to \u221e, we recover the DPI algorithm proposed and analyzed by Lazaric et al. (2010)."}, {"heading": "4. Error propagation", "text": "In this section, we derive a general formulation for propagation of error through the iterations of an AMPI algorithm. The line of analysis for error propagation is different in VI and PI algorithms. VI analysis is based on the fact that this algorithm computes the fixed point of the Bellman optimality operator, and this operator is a \u03b3-contraction in max-norm (Bertsekas & Tsitsiklis, 1996; Munos, 2007). On the other\n3We may implement CBMPI more sample efficient by reusing the rollouts generated for the greedy step in the evaluation step.\nhand, it can be shown that the operator by which PI updates the value from one iteration to the next is not a contraction in max-norm in general. Unfortunately, we can show that the same property holds for MPI when it does not reduce to VI (i.e., m > 1).\nProposition 1. If m > 1, there exists no norm for which the operator that MPI uses to update the values from one iteration to the next is a contraction.\nProof. Consider a deterministic MDP with two states {s1, s2}, two actions {change, stay}, rewards r(s1) = 0, r(s2) = 1, and transitions Pch(s2|s1) = Pch(s1|s2) = Pst(s1|s1) = Pst(s2|s2) = 1. Consider the following two value functions v = ( , 0) and v\u2032 = (0, ) with > 0. Their corresponding greedy policies are \u03c0 = (st, ch) and \u03c0\u2032 = (ch, st), and the next iterates of v and v\u2032 can\nbe computed as (T\u03c0) mv =\n( \u03b3m\n1 + \u03b3m\n) and (T\u03c0\u2032)\nmv\u2032 =( \u03b3\u2212\u03b3m 1\u2212\u03b3 + \u03b3\nm 1\u2212\u03b3m 1\u2212\u03b3 + \u03b3 m\n) . Thus, (T\u03c0\u2032) mv\u2032\u2212(T\u03c0)mv = ( \u03b3\u2212\u03b3m 1\u2212\u03b3 \u03b3\u2212\u03b3m 1\u2212\u03b3 ) while v\u2032\u2212 v = ( \u2212 ) . Since can be arbitrarily small,\nthe norm of (T\u03c0\u2032) mv\u2032\u2212(T\u03c0)mv can be arbitrarily larger than the norm of v \u2212 v\u2032 as long as m > 1.\nWe also know that the analysis of PI usually relies on the fact that the sequence of the generated values is non-decreasing (Bertsekas & Tsitsiklis, 1996; Munos, 2003). Unfortunately, it can easily be shown that for m finite, the value functions generated by MPI may decrease (it suffices to take a very high initial value). It can be seen from what we just described and Proposition 1 that for m 6= 1 and\u221e, MPI is neither contracting nor non-decreasing, and thus, a new line of proof is needed for the propagation of error in this algorithm.\nTo study error propagation in AMPI, we introduce an abstract algorithmic model that accounts for potential errors. AMPI starts with an arbitrary value v0 and at each iteration k \u2265 1 computes the greedy policy w.r.t. vk\u22121 with some error \u2032 k, called the greedy step error. Thus, we write the new policy \u03c0k as\n\u03c0k = G\u0302 \u2032kvk\u22121. (10)\nEq. 10 means that for any policy \u03c0\u2032,\nT\u03c0\u2032vk\u22121 \u2264 T\u03c0kvk\u22121 + \u2032k.\nAMPI then generates the new value function vk with some error k, called the evaluation step error\nvk = (T\u03c0k) mvk\u22121 + k. (11)\nBefore showing how these two errors are propagated through the iterations of AMPI, let us first define them\nin the context of each of the algorithms presented in Section 3 separately.\nAMPI-V: k is the error in fitting the value function vk. This error can be further decomposed into two parts: the one related to the approximation power of F and the one due to the finite number of samples/rollouts. \u2032k is the error due to using a finite number of samples M for estimating the greedy actions.\nAMPI-Q: \u2032k = 0 and k is the error in fitting the state-action value function Qk.\nCBMPI: This algorithm iterates as follows:\nvk = (T\u03c0k) mvk\u22121 + k\n\u03c0k+1 = G\u0302 \u2032k+1 [(T\u03c0k) mvk\u22121]\nUnfortunately, this does not exactly match with the model described in Eqs. 10 and 11. By introducing the auxiliary variable wk \u2206 = (T\u03c0k)\nmvk\u22121, we have vk = wk + k, and thus, we may write\n\u03c0k+1 = G\u0302 \u2032k+1 [wk] . (12)\nUsing vk\u22121 = wk\u22121 + k\u22121, we have\nwk = (T\u03c0k) mvk\u22121 = (T\u03c0k) m(wk\u22121 + k\u22121)\n= (T\u03c0k) mwk\u22121 + (\u03b3P\u03c0k) m k\u22121. (13)\nNow, Eqs. 12 and 13 exactly match Eqs. 10 and 11 by replacing vk with wk and k with (\u03b3P\u03c0k) m k\u22121.\nThe rest of this section is devoted to show how the errors k and \u2032 k propagate through the iterations of an AMPI algorithm. We only outline the main arguments that will lead to the performance bound of Thm. 1 and report most proofs in (Scherrer et al., 2012). We follow the line of analysis developped by Thiery & Scherrer (2010). The results are obtained using the following three quantities:\n1) The distance between the optimal value function and the value before approximation at the kth iteration: dk \u2206 = v\u2217 \u2212 (T\u03c0k)mvk\u22121 = v\u2217 \u2212 (vk \u2212 k).\n2) The shift between the value before approximation and the value of the policy at the kth iteration: sk \u2206 = (T\u03c0k) mvk\u22121 \u2212 v\u03c0k = (vk \u2212 k)\u2212 v\u03c0k .\n3) The Bellman residual at the kth iteration: bk \u2206 = vk \u2212 T\u03c0k+1vk.\nWe are interested in finding an upper bound on the loss lk \u2206 = v\u2217 \u2212 v\u03c0k = dk + sk. To do so, we will upper bound dk and sk, which requires a bound on the Bellman residual bk. More precisely, the core of our analysis is to prove the following point-wise inequalities for our three quantities of interest.\nLemma 1 (Proof in (Scherrer et al., 2012, Appendix A)). Let k \u2265 1, xk \u2206 = (I \u2212 \u03b3P\u03c0k) k + \u2032k+1 and yk \u2206 = \u2212\u03b3P\u03c0\u2217 k + \u2032k+1. We have:\nbk \u2264 (\u03b3P\u03c0k)mbk\u22121 + xk,\ndk+1 \u2264 \u03b3P\u03c0\u2217dk + yk + m\u22121\u2211 j=1 (\u03b3P\u03c0k+1) jbk,\nsk = (\u03b3P\u03c0k) m(I \u2212 \u03b3P\u03c0k)\u22121bk\u22121.\nSince the stochastic kernels are non-negative, the bounds in Lemma 1 indicate that the loss lk will be bounded if the errors k and \u2032 k are controlled. In fact, if we define as a uniform upper-bound on the errors | k| and | \u2032k|, the first inequality in Lemma 1 implies that bk \u2264 O( ), and as a result, the second and third inequalities gives us dk \u2264 O( ) and sk \u2264 O( ). This means that the loss will also satisfy lk \u2264 O( ).\nOur bound for the loss lk is the result of careful expansion and combination of the three inequalities in Lemma 1. Before we state this result, we introduce some notations that will ease our formulation.\nDefinition 1. For a positive integer n, we define Pn as the set of transition kernels that are defined as follows:"}, {"heading": "1) for any set of n policies {\u03c01, . . . , \u03c0n},", "text": "(\u03b3P\u03c01)(\u03b3P\u03c02) . . . (\u03b3P\u03c0n) \u2208 Pn,\n2) for any \u03b1 \u2208 (0, 1) and (P1, P2) \u2208 Pn \u00d7 Pn, \u03b1P1 + (1\u2212 \u03b1)P2 \u2208 Pn.\nFurthermore, we use the somewhat abusive notation \u0393n for denoting any element of Pn. For example, if we write a transition kernel P as P = \u03b11\u0393 i + \u03b12\u0393 j\u0393k = \u03b11\u0393 i+\u03b12\u0393\nj+k, it should be read as there exist P1 \u2208 Pi, P2 \u2208 Pj, P3 \u2208 Pk, and P4 \u2208 Pk+j such that P = \u03b11P1 + \u03b12P2P3 = \u03b11P1 + \u03b12P4.\nUsing the notation introduced in Definition 1, we now derive a point-wise bound on the loss.\nLemma 2 (Proof in (Scherrer et al., 2012, Appendix B)). After k iterations, the losses of AMPI-V and AMPI-Q satisfy\nlk \u2264 2 k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393j | k\u2212i|+ k\u22121\u2211 i=0 \u221e\u2211 j=i \u0393j | \u2032k\u2212i|+ h(k),\nwhile the loss of CBMPI satisfies\nlk \u2264 2 k\u22122\u2211 i=1 \u221e\u2211 j=i+m \u0393j | k\u2212i\u22121|+ k\u22121\u2211 i=0 \u221e\u2211 j=i \u0393j | \u2032k\u2212i|+ h(k),\nwhere h(k) \u2206 = 2 \u2211\u221e j=k \u0393 j |d0| or h(k) \u2206 = 2 \u2211\u221e j=k \u0393 j |b0|.\nRemark 1. A close look at the existing point-wise error bounds for AVI (Munos, 2007, Lemma 4.1) and API (Munos, 2003, Corollary 10) shows that they do not consider error in the greedy step (i.e., \u2032k = 0) and that they have the following form:\nlim supk\u2192\u221elk \u2264 2 lim supk\u2192\u221e k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393j | k\u2212i|.\nThis indicates that the bound in Lemma 2 not only unifies the analysis of AVI and API, but it generalizes them to the case of error in the greedy step and to a finite horizon k. Moreover, our bound suggests that the way the errors are propagated in the whole family of algorithms VI/PI/MPI does not depend on m at the level of the abstraction suggested by Definition 1.4\nThe next step is to show how the point-wise bound of Lemma 2 can turn to a bound in weighted Lp-norm, which for any function f : S \u2192 R and any distribution \u00b5 on S is defined as \u2016f\u2016p,\u00b5 \u2206 = (\u222b |f(x)|p\u00b5(dx) )1/p . Munos (2003; 2007); Munos & Szepesva\u0301ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients. We will discuss the potential advantage of this new class in Remark 4. We will also show through the proofs of Thms. 1 and 3, how the result of Lemma 3 provides us with a flexible tool for turning point-wise bounds into Lp-norm bounds. Thm. 3 in (Scherrer et al., 2012, Appendix D) provides an alternative bound for the loss of AMPI, which in analogy with the results of Farahmand et al. (2010) shows that the last iterations have the highest impact on the loss (the influence exponentially decreases towards the initial iterations).\nLemma 3 (Proof in (Scherrer et al., 2012, Appendix C)). Let I and (Ji)i\u2208I be sets of positive integers, {I1, . . . , In} be a partition of I, and f and (gi)i\u2208I be functions satisfying\n|f | \u2264 \u2211 i\u2208I \u2211 j\u2208Ji \u0393j |gi| = n\u2211 l=1 \u2211 i\u2208Il \u2211 j\u2208Ji \u0393j |gi|.\nThen for all p, q and q\u2032 such that 1q + 1 q\u2032 = 1, and for all distributions \u03c1 and \u00b5, we have\n\u2016f\u2016p,\u03c1 \u2264 n\u2211 l=1 ( Cq(l) )1/p sup i\u2208Il \u2016gi\u2016pq\u2032,\u00b5 \u2211 i\u2208Il \u2211 j\u2208Ji \u03b3j ,\n4Note however that the dependence on m will reappear if we make explicit what is hidden in the terms \u0393j .\nwith the following concentrability coefficients\nCq(l) \u2206 =\n\u2211 i\u2208Il \u2211 j\u2208Ji \u03b3\njcq(j)\u2211 i\u2208Il \u2211 j\u2208Ji \u03b3 j ,\nwith the Radon-Nikodym derivative based quantity\ncq(j) \u2206 = max \u03c01,\u00b7\u00b7\u00b7 ,\u03c0j \u2225\u2225\u2225\u2225d(\u03c1P\u03c01P\u03c02 \u00b7 \u00b7 \u00b7P\u03c0j )d\u00b5 \u2225\u2225\u2225\u2225 q,\u00b5\n(14)\nWe now derive a Lp-norm bound for the loss of the AMPI algorithm by applying Lemma 3 to the pointwise bound of Lemma 2.\nTheorem 1 (Proof in (Scherrer et al., 2012, Appendix D)). Let \u03c1 and \u00b5 be distributions over states. Let p, q, and q\u2032 be such that 1q + 1 q\u2032 = 1. After k iterations, the loss of AMPI satisfies\n\u2016lk\u2016p,\u03c1 \u2264 2(\u03b3 \u2212 \u03b3k)\n( C1,k,0q ) 1 p\n(1\u2212 \u03b3)2 sup1\u2264j\u2264k\u22121 \u2016 j\u2016pq\u2032,\u00b5 (15)\n+ (1\u2212 \u03b3k)\n( C0,k,0q ) 1 p\n(1\u2212 \u03b3)2 sup1\u2264j\u2264k \u2016 \u2032j\u2016pq\u2032,\u00b5 + g(k),\nwhile the loss of CBMPI satisfies\n\u2016lk\u2016p,\u03c1 \u2264 2\u03b3m(\u03b3 \u2212 \u03b3k\u22121)\n( C2,k,mq ) 1 p\n(1\u2212 \u03b3)2 sup1\u2264j\u2264k\u22122 \u2016 j\u2016pq\u2032,\u00b5\n(16)\n+ (1\u2212 \u03b3k)\n( C1,k,0q ) 1 p\n(1\u2212 \u03b3)2 sup1\u2264j\u2264k \u2016 \u2032j\u2016pq\u2032,\u00b5 + g(k),\nwhere for all q, l, k and d, the concentrability coefficients Cl,k,dq are defined as\nCl,k,dq \u2206 =\n(1\u2212 \u03b3)2 \u03b3l \u2212 \u03b3k k\u22121\u2211 i=l \u221e\u2211 j=i \u03b3jcq(j + d),\nwith cq(j) given by Eq. 14, and g(k) is defined as\ng(k) \u2206 = 2\u03b3\nk 1\u2212\u03b3 ( Ck,k+1q ) 1 p min ( \u2016d0\u2016pq\u2032,\u00b5, \u2016b0\u2016pq\u2032,\u00b5 ) .\nRemark 2. When p tends to infinity, the first bound of Thm. 1 reduces to \u2016lk\u2016\u221e \u2264 2(\u03b3 \u2212 \u03b3k) (1\u2212 \u03b3)2 sup1\u2264j\u2264k\u22121 \u2016 j\u2016\u221e + 1\u2212 \u03b3k (1\u2212 \u03b3)2 sup1\u2264j\u2264k \u2016 \u2032j\u2016\u221e\n+ 2\u03b3k\n1\u2212 \u03b3 min(\u2016d0\u2016\u221e, \u2016b0\u2016\u221e). (17)\nWhen k goes to infinity, Eq. 17 gives us a generalization of the API (m = \u221e) bound of Bertsekas & Tsitsiklis (1996, Prop. 6.2), i.e.,\nlim sup k\u2192\u221e\n\u2016lk\u2016\u221e \u2264 2\u03b3 supj \u2016 j\u2016\u221e + supj \u2016 \u2032j\u2016\u221e\n(1\u2212 \u03b3)2 .\nMoreover, since our point-wise analysis generalizes those of API and AVI (as noted in Remark 1), the Lp-bound of Eq. 15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).\nRemark 3. Canbolat & Rothblum (2012) recently (and independently) developped an analysis of an approximate form of MPI. Also, as mentionned, the proof technique that we used is based on that of Thiery & Scherrer (2010). While Canbolat & Rothblum (2012) only consider the error in the greedy step and Thiery & Scherrer (2010) that in the value update, our work is more general in that we consider both sources of error \u2013 this is required for the analysis of CBMPI. Thiery & Scherrer (2010) and Canbolat & Rothblum (2012) provide bounds when the errors are controlled in maxnorm, while we consider the more general Lp-norm. At a more technical level, Th. 2 in (Canbolat & Rothblum, 2012) bounds the norm of the distance v\u2217 \u2212 vk while we bound the loss v\u2217\u2212v\u03c0k . If we derive a bound on the loss (using e.g., Th. 1 in (Canbolat & Rothblum, 2012)), this leads to a bound on the loss that is looser than ours. In particular, this does not allow to recover the standard bounds for AVI/API, as we managed to (c.f. Remark 2).\nRemark 4. We can balance the influence of the concentrability coefficients (the bigger the q, the higher the influence) and the difficulty of controlling the errors (the bigger the q\u2032, the greater the difficulty in controlling the Lpq\u2032 -norms) by tuning the parameters q and q\u2032, given the condition that 1q + 1 q\u2032 = 1. This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesva\u0301ri (2008), and q = q\u2032 = 2 in Farahmand et al. (2010).\nRemark 5. For CBMPI, the parameter m controls the influence of the value function approximator, cancelling it out in the limit when m tends to infinity (see Eq. 16). Assuming a fixed budget of sample transitions, increasing m reduces the number of rollouts used by the classifier and thus worsens its quality; in such a situation, m allows to make a trade-off between the estimation error of the classifier and the overall value function approximation."}, {"heading": "5. Finite-Sample Analysis of CBMPI", "text": "In this section, we focus on CBMPI and detail the possible form of the error terms that appear in the bound of Thm. 1. We select CBMPI among the proposed algorithms because its analysis is more general than the others as we need to bound both greedy and evaluation step errors (in some norm), and also because it displays an interesting influence of the parameter m (see Remark 5). We first provide a bound on the greedy step error. From the definition of \u2032k for CBMPI (Eq. 12) and the description of the greedy step in CBMPI, we can easily observe that \u2016 \u2032k\u20161,\u00b5 = L\u03a0k\u22121(\u00b5;\u03c0k).\nLemma 4 (Proof in (Scherrer et al., 2012, Appendix E)). Let \u03a0 be a policy space with finite VCdimension h = V C(\u03a0) and \u00b5 be a distribution over the state space S. Let N be the number of states in D\u2032k\u22121 drawn i.i.d. from \u00b5, M be the number of rollouts per state-action pair used in the estimation of Q\u0302k\u22121, and \u03c0k = argmin\u03c0\u2208\u03a0 L\u0302\u03a0k\u22121(\u00b5\u0302, \u03c0) be the policy computed at iteration k \u2212 1 of CBMPI. Then, for any \u03b4 > 0, we have\n\u2016 \u2032k\u20161,\u00b5 = L\u03a0k\u22121(\u00b5;\u03c0k) \u2264 inf \u03c0\u2208\u03a0 L\u03a0k\u22121(\u00b5;\u03c0) + 2( \u20321 + \u20322),\nwith probability at least 1\u2212 \u03b4, where\n\u20321(N, \u03b4) = 16Qmax\n\u221a 2\nN\n( h log eN\nh + log\n32\n\u03b4\n) ,\n\u20322(N,M, \u03b4) = 8Qmax\n\u221a 2\nMN\n( h log eMN\nh + log\n32\n\u03b4\n) .\nWe now consider the evaluation step error. The evaluation step at iteration k of CBMPI is a regression problem with the target (T\u03c0k)\nmvk\u22121 and a training set {( s(i), v\u0302k(s (i)) )}n i=1 in which the states s(i) are i.i.d. samples from \u00b5 and v\u0302k(s (i)) are unbiased estimates of the target computed according to Eq. 7. Different function spaces F (linear or non-linear) may be used to approximate (T\u03c0k)\nmvk\u22121. Here we consider a linear architecture with parameters \u03b1 \u2208 Rd and bounded (by L) basis functions {\u03d5j}dj=1, \u2016\u03d5j\u2016\u221e \u2264 L. We denote by \u03c6 : X \u2192 Rd, \u03c6(\u00b7) = ( \u03d51(\u00b7), . . . , \u03d5d(\u00b7)\n)> the feature vector, and by F the linear function space spanned by the features \u03d5j , i.e., F = {f\u03b1(\u00b7) = \u03c6(\u00b7)>\u03b1 : \u03b1 \u2208 Rd}. Now if we define vk as the truncation (by Vmax) of the solution of the above linear regression problem, we may bound the evaluation step error using the following lemma.\nLemma 5 (Proof in (Scherrer et al., 2012, Appendix F)). Consider the linear regression setting described above, then we have\n\u2016 k\u20162,\u00b5 \u2264 4 inf f\u2208F \u2016(T\u03c0k)mvk\u22121 \u2212 f\u20162,\u00b5 + 1 + 2,\nwith probability at least 1\u2212 \u03b4, where\n1(n, \u03b4) = 32Vmax\n\u221a 2 n log (27(12e2n)2(d+1) \u03b4 ) ,\n2(n, \u03b4) = 24 ( Vmax + \u2016\u03b1\u2217\u20162 \u00b7 sup\nx \u2016\u03c6(x)\u20162 )\u221a 2 n log 9 \u03b4 ,\nand \u03b1\u2217 is such that f\u03b1\u2217 is the best approximation (w.r.t. \u00b5) of the target function (T\u03c0k) mvk\u22121 in F .\nFrom Lemmas 4 and 5, we have bounds on \u2016 \u2032k\u20161,\u00b5 and \u2016 k\u20161,\u00b5 \u2264 \u2016 k\u20162,\u00b5. By a union bound argument,\nwe thus control the r.h.s of Eq. 16 in L1 norm. In the context of Th. 1, this means p = 1, q\u2032 = 1 and q =\u221e, and we have the following bound for CBMPI:\nTheorem 2. Let d\u2032 = supg\u2208F,\u03c0\u2032 inf\u03c0\u2208\u03a0 L\u03a0\u03c0\u2032,g(\u00b5;\u03c0) and dm = supg\u2208F,\u03c0 inff\u2208F \u2016(T\u03c0)mg \u2212 f\u20162,\u00b5. With the notations of Th. 1 and Lemmas 4-5, after k iterations, and with probability 1 \u2212 \u03b4, the expected loss E\u00b5[lk] = \u2016lk\u20161,\u00b5 of CBMPI is bounded by\n2\u03b3m(\u03b3 \u2212 \u03b3k\u22121)C2,k,m\u221e (1\u2212 \u03b3)2\n( dm + 1(n, \u03b4\n2k ) + 2(n,\n\u03b4\n2k ) ) +\n(1\u2212 \u03b3k)C1,k,0\u221e (1\u2212 \u03b3)2\n( d\u2032 + \u20321(N, \u03b4\n2k ) + \u20322(N,M,\n\u03b4\n2k )\n) + g(k).\nRemark 6. This result leads to a quantitative version of Remark 5. Assume that we have a fixed budget for the actor and the critic B = nm = NM |A|m. Then, up to constants and logarithmic factors, the bound has the form \u2016lk\u20161,\u00b5 \u2264\nO ( \u03b3m ( dm + \u221a m B ) + d\u2032 + \u221a M |A|m B ) . It shows the\ntrade-off in the tuning of m: a big m can make the influence of the overall (approximation and estimation) value error small, but that of the estimation error of the classifier bigger."}, {"heading": "6. Summary and Extensions", "text": "In this paper, we studied a DP algorithm, called modified policy iteration (MPI), that despite its generality that contains the celebrated policy and value iteration methods, has not been thoroughly investigated in the literature. We proposed three approximate MPI (AMPI) algorithms that are extensions of the wellknown ADP algorithms: fitted-value iteration, fittedQ iteration, and classification-based policy iteration. We reported an error propagation analysis for AMPI that unifies those for approximate policy and value iteration. We also provided a finite-sample analysis for the classification-based implementation of AMPI (CBMPI), whose analysis is more general than the other presented AMPI methods. Our results indicate that the parameter of MPI allows us to control the balance of errors (in value function approximation and estimation of the greedy policy) in the final performance of CBMPI. Although AMPI generalizes the existing AVI and classification-based API algorithms, additional experimental work and careful theoretical analysis are required to obtain a better understanding of the behaviour of its different implementations and their relation to the competitive methods. Extension of CBMPI to problems with continuous action space is another interesting direction to pursue."}, {"heading": "Ernst, D., Geurts, P., and Wehenkel, L. Tree-based batch", "text": "mode reinforcement learning. Journal of Machine Learning Research, 6:503\u2013556, 2005."}, {"heading": "Farahmand, A., Munos, R., and Szepesva\u0301ri, Cs. Error", "text": "propagation for approximate policy and value iteration. In Proceedings of NIPS, pp. 568\u2013576, 2010.\nFern, A., Yoon, S., and Givan, R. Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes. Journal of Artificial Intelligence Research, 25:75\u2013118, 2006.\nGabillon, V., Lazaric, A., Ghavamzadeh, M., and Scherrer, B. Classification-based policy iteration with a critic. In Proceedings of ICML, pp. 1049\u20131056, 2011."}, {"heading": "Lagoudakis, M. and Parr, R. Reinforcement Learning as", "text": "Classification: Leveraging Modern Classifiers. In Proceedings of ICML, pp. 424\u2013431, 2003."}, {"heading": "Lazaric, A., Ghavamzadeh, M., and Munos, R. Analysis", "text": "of a Classification-based Policy Iteration Algorithm. In Proceedings of ICML, pp. 607\u2013614, 2010."}, {"heading": "Munos, R. Error Bounds for Approximate Policy Iteration.", "text": "In Proceedings of ICML, pp. 560\u2013567, 2003.\nMunos, R. Performance Bounds in Lp-norm for Approximate Value Iteration. SIAM J. Control and Optimization, 46(2):541\u2013561, 2007."}, {"heading": "Munos, R. and Szepesva\u0301ri, Cs. Finite-Time Bounds for", "text": "Fitted Value Iteration. Journal of Machine Learning Research, 9:815\u2013857, 2008.\nPuterman, M. and Shin, M. Modified policy iteration algorithms for discounted Markov decision problems. Management Science, 24(11), 1978."}, {"heading": "Scherrer, Bruno, Gabillon, Victor, Ghavamzadeh, Mohammad, and Geist, Matthieu. Approximate Modified Policy", "text": "Iteration. Technical report, INRIA, May 2012."}, {"heading": "Szepesva\u0301ri, Cs. Reinforcement Learning Algorithms for", "text": "MDPs. In Wiley Encyclopedia of Operations Research. Wiley, 2010."}, {"heading": "Thiery, Christophe and Scherrer, Bruno. Performance", "text": "bound for Approximate Optimistic Policy Iteration. Technical report, INRIA, 2010.\nSupplementary Material for Approximate Modified Policy Iteration"}, {"heading": "A. Proof of Lemma 1", "text": "Before we start, we recall the following definitions:\nbk = vk\u2212T\u03c0k+1vk, dk = v\u2217\u2212 (T\u03c0k)mvk\u22121 = v\u2217\u2212 (vk\u2212 k), sk = (T\u03c0k)mvk\u22121\u2212 v\u03c0k = (vk\u2212 k)\u2212 v\u03c0k ."}, {"heading": "Bounding bk", "text": "bk = vk \u2212 T\u03c0k+1vk = vk \u2212 T\u03c0kvk + T\u03c0kvk \u2212 T\u03c0k+1vk (a) \u2264 vk \u2212 T\u03c0kvk + \u2032k+1\n= vk \u2212 k \u2212 T\u03c0kvk + \u03b3P\u03c0k k + k \u2212 \u03b3P\u03c0k k + \u2032k+1 (b) = vk \u2212 k \u2212 T\u03c0k(vk \u2212 k) + (I \u2212 \u03b3P\u03c0k) k + \u2032k+1. (18)\nUsing the definition of xk, i.e.,\nxk \u2206 = (I \u2212 \u03b3P\u03c0k) k + \u2032k+1, (19)\nwe may write Eq. (18) as\nbk \u2264 vk \u2212 k \u2212 T\u03c0k(vk \u2212 k) + xk (c) = (T\u03c0k) mvk\u22121 \u2212 T\u03c0k(T\u03c0k)mvk\u22121 + xk = (T\u03c0k)mvk\u22121 \u2212 (T\u03c0k)m(T\u03c0kvk\u22121) + xk = (\u03b3P\u03c0k) m(vk\u22121 \u2212 T\u03c0kvk\u22121) + xk = (\u03b3P\u03c0k)mbk\u22121 + xk. (20)\n(a) From the definition of \u2032k+1, we have \u2200\u03c0\u2032 T\u03c0\u2032vk \u2264 T\u03c0k+1vk + \u2032k+1, thus this inequality holds also for \u03c0\u2032 = \u03c0k. (b) This step is due to the fact that for every v and v\u2032, we have T\u03c0k(v + v \u2032) = T\u03c0kv + \u03b3P\u03c0kv \u2032.\n(c) This is from the definition of k, i.e., vk = (T\u03c0k) mvk\u22121 + k."}, {"heading": "Bounding dk", "text": "dk+1 = v\u2217 \u2212 (T\u03c0k+1)mvk = T\u03c0\u2217v\u2217 \u2212 T\u03c0\u2217vk + T\u03c0\u2217vk \u2212 T\u03c0k+1vk + T\u03c0k+1vk \u2212 (T\u03c0k+1)mvk (a)\n\u2264 \u03b3P\u03c0\u2217(v\u2217 \u2212 vk) + \u2032k+1 + gk+1 = \u03b3P\u03c0\u2217(v\u2217 \u2212 vk) + \u03b3P\u03c0\u2217 k \u2212 \u03b3P\u03c0\u2217 k + \u2032k+1 + gk+1\n(b) = \u03b3P\u03c0\u2217 ( v\u2217 \u2212 (vk \u2212 k) ) + yk + gk+1 = \u03b3P\u03c0\u2217dk + yk + gk+1 (c) = \u03b3P\u03c0\u2217dk + yk + m\u22121\u2211 j=1 (\u03b3P\u03c0k+1) jbk. (21)\n(a) This step is from the definition of \u2032k+1 (see step (a) in bounding bk) and by defining gk+1 as follows:\ngk+1 \u2206 = T\u03c0k+1vk \u2212 (T\u03c0k+1)mvk. (22)\n(b) This is from the definition of yk, i.e.,\nyk \u2206 = \u2212\u03b3P\u03c0\u2217 k + \u2032k+1. (23)\n(c) This step comes from rewriting gk+1 as\ngk+1 = T\u03c0k+1vk \u2212 (T\u03c0k+1)mvk = m\u22121\u2211 j=1 [ (T\u03c0k+1) jvk \u2212 (T\u03c0k+1)j+1vk ] = m\u22121\u2211 j=1 [ (T\u03c0k+1) jvk \u2212 (T\u03c0k+1)j(T\u03c0k+1vk) ]\n= m\u22121\u2211 j=1 (\u03b3P\u03c0k+1) j(vk \u2212 T\u03c0k+1vk) = m\u22121\u2211 j=1 (\u03b3P\u03c0k+1) jbk. (24)\nBounding sk With some slight abuse of notation, we have\nv\u03c0k = (T\u03c0k) \u221evk\nand thus:\nsk = (T\u03c0k) mvk\u22121 \u2212 v\u03c0k (a) = (T\u03c0k) mvk\u22121 \u2212 (T\u03c0k)\u221evk\u22121 = (T\u03c0k)mvk\u22121 \u2212 (T\u03c0k)m(T\u03c0k)\u221evk\u22121\n= (\u03b3P\u03c0k) m ( vk\u22121 \u2212 (T\u03c0k)\u221evk\u22121 ) = (\u03b3P\u03c0k) m \u221e\u2211 j=0 [ (T\u03c0k) jvk\u22121 \u2212 (T\u03c0k)j+1vk\u22121 ]\n= (\u03b3P\u03c0k) m ( \u221e\u2211 j=0 [ (T\u03c0k) jvk\u22121 \u2212 (T\u03c0k)jT\u03c0kvk\u22121 ] = (\u03b3P\u03c0k) m ( \u221e\u2211 j=0 (\u03b3P\u03c0k) j ) (vk\u22121 \u2212 T\u03c0kvk\u22121) = (\u03b3P\u03c0k) m(I \u2212 \u03b3P\u03c0k)\u22121(vk\u22121 \u2212 T\u03c0kvk\u22121) = (\u03b3P\u03c0k)m(I \u2212 \u03b3P\u03c0k)\u22121bk. (25)\n(a) For any v, we have v\u03c0k = (T\u03c0k) \u221ev. This step follows by setting v = vk\u22121, i.e., v\u03c0k = (T\u03c0k) \u221evk\u22121."}, {"heading": "B. Proof of Lemma 2", "text": "We begin by focusing our analysis on AMPI. Here we are interested in bounding the loss lk = v\u2217\u2212v\u03c0k = dk +sk.\nBy induction, from Eqs. (20) and (21), we obtain\nbk \u2264 k\u2211 i=1 \u0393m(k\u2212i)xi + \u0393 mkb0, (26)\ndk \u2264 k\u22121\u2211 j=0 \u0393k\u22121\u2212j ( yj + m\u22121\u2211 l=1 \u0393lbj ) + \u0393kd0. (27)\nin which we have used the notation introduced in Definition 1. In Eq. (27), we also used the fact that from Eq. (24), we may write gk+1 = \u2211m\u22121 j=1 \u0393 jbk. Moreover, we may rewrite Eq. (25) as\nsk = \u0393 m \u221e\u2211 j=0 \u0393jbk\u22121 = \u221e\u2211 j=0 \u0393m+jbk\u22121. (28)\nBounding lk From Eqs. (26) and (27), we may write\ndk \u2264 k\u22121\u2211 j=0 \u0393k\u22121\u2212j\n( yj +\nm\u22121\u2211 l=1 \u0393l ( j\u2211 i=1 \u0393m(j\u2212i)xi + \u0393 mjb0 )) + \u0393kd0\n= k\u2211 i=1 \u0393i\u22121yk\u2212i + k\u22121\u2211 j=0 m\u22121\u2211 l=1 j\u2211 i=1 \u0393k\u22121\u2212j+l+m(j\u2212i)xi + zk, (29)\nwhere we used the following definition\nzk \u2206 = k\u22121\u2211 j=0 m\u22121\u2211 l=1 \u0393k\u22121+l+j(m\u22121)b0 + \u0393 kd0 = mk\u22121\u2211 i=k \u0393ib0 + \u0393 kd0.\nThe triple sum involved in Eq. (29) may be written as\nk\u22121\u2211 j=0 m\u22121\u2211 l=1 j\u2211 i=1 \u0393k\u22121\u2212j+l+m(j\u2212i)xi = k\u22121\u2211 i=1 k\u22121\u2211 j=i m\u22121\u2211 l=1 \u0393k\u22121+l+j(m\u22121)\u2212mixi = k\u22121\u2211 i=1 mk\u22121\u2211 j=mi+k\u2212i \u0393j\u2212mixi\n= k\u22121\u2211 i=1 m(k\u2212i)\u22121\u2211 j=k\u2212i \u0393jxi = k\u22121\u2211 i=1 mi\u22121\u2211 j=i \u0393jxk\u2212i. (30)\nUsing Eq. (30), we may write Eq. (29) as\ndk \u2264 k\u2211 i=1 \u0393i\u22121yk\u2212i + k\u22121\u2211 i=1 mi\u22121\u2211 j=i \u0393jxk\u2212i + zk. (31)\nSimilarly, from Eqs. (28) and (26), we have\nsk \u2264 \u221e\u2211 j=0 \u0393m+j ( k\u22121\u2211 i=1 \u0393m(k\u22121\u2212i)xi + \u0393 m(k\u22121)b0 ) = \u221e\u2211 j=0 ( k\u22121\u2211 i=1 \u0393m+j+m(k\u22121\u2212i)xi + \u0393 m+j+m(k\u22121)b0 )\n= k\u22121\u2211 i=1 \u221e\u2211 j=0 \u0393j+m(k\u2212i)xi + \u221e\u2211 j=0 \u0393j+mkb0 = k\u22121\u2211 i=1 \u221e\u2211 j=0 \u0393j+mixk\u2212i + \u221e\u2211 j=mk \u0393jb0 = k\u22121\u2211 i=1 \u221e\u2211 j=mi \u0393jxk\u2212i + z \u2032 k, (32)\nwhere we used the following definition\nz\u2032k \u2206 = \u221e\u2211 j=mk \u0393jb0.\nFinally, using the bounds in Eqs. (31) and (32), we obtain the following bound on the loss\nlk \u2264 dk + sk \u2264 k\u2211 i=1 \u0393i\u22121yk\u2212i + k\u22121\u2211 i=1 (mi\u22121\u2211 j=i \u0393j + \u221e\u2211 j=mi \u0393j ) xk\u2212i + zk + z \u2032 k\n= k\u2211 i=1 \u0393i\u22121yk\u2212i + k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393jxk\u2212i + \u03b7k, (33)\nwhere we used the following definition\n\u03b7k \u2206 = zk + z \u2032 k = \u221e\u2211 j=k \u0393jb0 + \u0393 kd0. (34)\nNote that we have the following relation between b0 and d0\nb0 = v0 \u2212 T\u03c01v0 = v0 \u2212 v\u2217 + T\u03c0\u2217v\u2217 \u2212 T\u03c0\u2217v0 + T\u03c0\u2217v0 \u2212 T\u03c01v0 \u2264 (I \u2212 \u03b3P\u03c0\u2217)(\u2212d0) + \u20321, (35)\nIn Eq. (35), we used the fact that v\u2217 = T\u03c0\u2217v\u2217, 0 = 0, and T\u03c0\u2217v0 \u2212 T\u03c01v0 \u2264 \u20321 (this is because the policy \u03c01 is \u20321-greedy w.r.t. v0). As a result, we may write |\u03b7k| either as\n|\u03b7k| \u2264 \u221e\u2211 j=k \u0393j [ (I\u2212\u03b3P\u03c0\u2217)|d0|+ | \u20321| ] +\u0393k|d0| \u2264 \u221e\u2211 j=k \u0393j [ (I+\u03931)|d0|+ | \u20321| ] +\u0393k|d0| = 2 \u221e\u2211 j=k \u0393j |d0|+ \u221e\u2211 j=k \u0393j | \u20321|, (36)\nor using the fact that from Eq. (35), we have d0 \u2264 (I \u2212 \u03b3P\u03c0\u2217)\u22121(\u2212b0 + \u20321), as\n|\u03b7k| \u2264 \u221e\u2211 j=k \u0393j |b0|+ \u0393k \u221e\u2211 j=0 (\u03b3P\u03c0\u2217) j ( |b0|+ | \u20321| ) = \u221e\u2211 j=k \u0393j |b0|+ \u0393k \u221e\u2211 j=0 \u0393j ( |b0|+ | \u20321| ) = 2 \u221e\u2211 j=k \u0393j |b0|+ \u221e\u2211 j=k \u0393j | \u20321|. (37)\nNow, using the definitions of xk and yk in Eqs. (19) and (23), the bound on |\u03b7k| in Eq. (36) or (37), and the fact that 0 = 0, we obtain |lk| \u2264 k\u2211 i=1 \u0393i\u22121 [ \u03931| k\u2212i|+ | \u2032k\u2212i+1| ] + k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393j [ (I + \u03931)| k\u2212i|+ | \u2032k\u2212i+1| ] + |\u03b7k|\n= k\u22121\u2211 i=1 ( \u0393i + \u221e\u2211 j=i (\u0393j + \u0393j+1) ) | k\u2212i|+ \u0393k| 0|+ k\u22121\u2211 i=1 ( \u0393i\u22121 + \u221e\u2211 j=i \u0393j ) | \u2032k\u2212i+1|+ \u0393k\u22121| \u20321|+ \u221e\u2211 j=k \u0393j | \u20321|+ h(k)\n= 2 k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393j | k\u2212i|+ k\u22121\u2211 i=1 \u221e\u2211 j=i\u22121 \u0393j | \u2032k\u2212i+1|+ \u221e\u2211 j=k\u22121 \u0393j | \u20321|+ h(k) = 2 k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393j | k\u2212i|+ k\u22121\u2211 i=0 \u221e\u2211 j=i \u0393j | \u2032k\u2212i|+ h(k),\n(38)\nwhere we used the following definition\nh(k) \u2206 = 2 \u221e\u2211 j=k \u0393j |d0|, or h(k) \u2206 = 2 \u221e\u2211 j=k \u0393j |b0|.\nWe end this proof by adapting the error propagation to CBMPI. As expressed by Eqs. 12 and 13 in Sec. 4, an analysis of CBMPI can be deduced from that we have just done by replacing vk with the auxiliary variable\nwk = (T\u03c0k) mvk\u22121 and k with (\u03b3P\u03c0k) m k\u22121 = \u0393 m k\u22121. Therefore, using the fact that 0 = 0, we can rewrite the bound of Eq. 38 for CBMPI as follows:\nlk \u2264 2 k\u22121\u2211 i=1 \u221e\u2211 j=i \u0393j+m| k\u2212i\u22121|+ k\u22121\u2211 i=0 \u221e\u2211 j=i \u0393j | \u2032k\u2212i|+ h(k)\n= 2 k\u22122\u2211 i=1 \u221e\u2211 j=m+i \u0393j | k\u2212i\u22121|+ k\u22121\u2211 i=0 \u221e\u2211 j=i \u0393j | \u2032k\u2212i|+ h(k). (39)"}, {"heading": "C. Proof of Lemma 3", "text": "For any integer t and vector z, the definition of \u0393t and the Ho\u0308lder\u2019s inequality imply that\n\u03c1\u0393t|z| = \u2225\u2225\u0393t|z|\u2225\u2225\n1,\u03c1 \u2264 \u03b3tcq(t)\u2016z\u2016q\u2032,\u00b5 = \u03b3tcq(t)\n( \u00b5|z|q \u2032 ) 1 q\u2032 . (40)\nWe define\nK \u2206 = n\u2211 l=1 \u03bel \u2211 i\u2208Il \u2211 j\u2208Ji \u03b3j  , where {\u03bel}nl=1 is a set of non-negative numbers that we will specify later. We now have\n\u2016f\u2016pp,\u03c1 = \u03c1|f |p\n\u2264 Kp\u03c1\n(\u2211n l=1 \u2211 i\u2208Il \u2211 j\u2208Ji \u0393\nj |gi| K\n)p = Kp\u03c1 \u2211nl=1 \u03bel\u2211i\u2208Il\u2211j\u2208Ji \u0393j ( |gi| \u03bel ) K p\n(a) \u2264 Kp\u03c1\n\u2211n l=1 \u03bel \u2211 i\u2208Il \u2211 j\u2208Ji \u0393 j ( |gi| \u03bel )p K = Kp \u2211n l=1 \u03bel \u2211 i\u2208Il \u2211 j\u2208Ji \u03c1\u0393 j ( |gi| \u03bel )p K\n(b) \u2264 Kp \u2211n l=1 \u03bel \u2211 i\u2208Il \u2211 j\u2208Ji \u03b3 jcq(j) ( \u00b5 ( |gi| \u03bel )pq\u2032) 1q\u2032 K\n= Kp\n\u2211n l=1 \u03bel \u2211 i\u2208Il \u2211 j\u2208Ji \u03b3 jcq(j) ( \u2016gi\u2016pq\u2032,\u00b5 \u03bel )p K\n\u2264 Kp \u2211n l=1 \u03bel (\u2211 i\u2208Il \u2211 j\u2208Ji \u03b3 jcq(j) )( supi\u2208Il \u2016gi\u2016pq\u2032,\u00b5 \u03bel )p K\n(c) = Kp\n\u2211n l=1 \u03bel (\u2211 i\u2208Il \u2211 j\u2208Ji \u03b3 j ) Cq(l) ( supi\u2208Il \u2016gi\u2016pq\u2032,\u00b5 \u03bel )p K ,\nwhere (a) results from Jensen\u2019s inequality, (b) from Eq. 40, and (c) from the definition of Cq(l). Now, by setting \u03bel = ( Cq(l) )1/p supi\u2208Il \u2016gi\u2016pq\u2032,\u00b5, we obtain\n\u2016f\u2016pp,\u03c1 \u2264 Kp \u2211n l=1 \u03bel (\u2211 i\u2208Il \u2211 j\u2208Ji \u03b3 j )\nK = Kp,\nwhere the last step follows from the definition of K."}, {"heading": "D. Proof of Theorem 1 & other Bounds on the Loss", "text": "Proof. We only detail the proof for AMPI (the proof being similar for CBMPI). We define I = {1, 2, \u00b7 \u00b7 \u00b7 , 2k}, the partition I = {I1, I2, I3} as I1 = {1, . . . , k \u2212 1}, I2 = {k, . . . , 2k \u2212 1}, and I3 = {2k}, and for each i \u2208 I\ngi =  2 k\u2212i if 1 \u2264 i \u2264 k \u2212 1, \u2032k\u2212(i\u2212k) if k \u2264 i \u2264 2k \u2212 1, 2d0 (or 2b0) if i = 2k,\nand Ji =  {i, i+ 1, \u00b7 \u00b7 \u00b7 } if 1 \u2264 i \u2264 k \u2212 1,{i\u2212 k, i\u2212 k + 1, \u00b7 \u00b7 \u00b7 } if k \u2264 i \u2264 2k \u2212 1,{k, k + 1, \u00b7 \u00b7 \u00b7 } if i = 2k. Note that here we have divided the terms in the point-wise bound of Lemma 2 into three groups: the evaluation error terms { j}k\u22121j=1 , the greedy step error terms { \u2032j}kj=1, and finally the residual term h(k). With the above definitions and the fact that the loss lk is non-negative, Lemma 2 may be rewritten as\n|lk| \u2264 3\u2211 l=1 \u2211 i\u2208Il \u2211 j\u2208Ji \u0393j |gi|.\nThe result follows by applying Lemma 3 and noticing that \u2211k\u22121 i=i0 \u2211\u221e j=i \u03b3 j = \u03b3 i0\u2212\u03b3k (1\u2212\u03b3)2 .\nHere in oder to show the flexibility of Lemma 3, we group the terms differently and derive an alternative Lpbound for the loss of AMPI and CBMPI. In analogy with the results of Farahmand et al. (2010), this new bound shows that the last iterations have the highest influence on the loss (the influence exponentially decreases towards the initial iterations).\nTheorem 3. With the notations of Theorem 1, after k iterations, the loss of AMPI satisfies\n\u2016lk\u2016p,\u03c1 \u2264 2 k\u22121\u2211 i=1 \u03b3i 1\u2212 \u03b3 ( Ci,i+1q ) 1 p \u2016 k\u2212i\u2016pq\u2032,\u00b5 + k\u22121\u2211 i=0 \u03b3i 1\u2212 \u03b3 ( Ci,i+1q ) 1 p \u2016 \u2032k\u2212i\u2016pq\u2032,\u00b5 + g(k).\nwhile the loss of CBMPI satisfies\n\u2016lk\u2016p,\u03c1 \u2264 2\u03b3m k\u22122\u2211 i=1 \u03b3i 1\u2212 \u03b3 ( Ci,i+1q ) 1 p \u2016 k\u2212i\u22121\u2016pq\u2032,\u00b5 + k\u22121\u2211 i=0 \u03b3i 1\u2212 \u03b3 ( Ci,i+1q ) 1 p \u2016 \u2032k\u2212i\u2016pq\u2032,\u00b5 + g(k).\nProof. Again, we only detail the proof for AMPI (the proof being similar for CBMPI). We define I, (gi) and (Ji) as in the proof of Theorem 1. We then make as many groups as terms, i.e., for each n \u2208 {1, 2, . . . , 2k \u2212 1}, we define In = {n}. The result follows by application of Lemma 3."}, {"heading": "E. Proof of Lemma 4", "text": "The proof of this lemma is similar to the proof of Theorem 1 in Lazaric et al. (2010). Before stating the proof, we report the following two lemmas that are used in the proof.\nLemma 6. Let \u03a0 be a policy space with finite VC-dimension h = V C(\u03a0) < \u221e and N be the number of states in the rollout set Dk\u22121 drawn i.i.d. from the state distribution \u00b5. Then we have\nPDk\u22121 [\nsup \u03c0\u2208\u03a0 \u2223\u2223\u2223L\u03a0k\u22121(\u00b5\u0302;\u03c0)\u2212 L\u03a0k\u22121(\u00b5;\u03c0)\u2223\u2223\u2223 > ] \u2264 \u03b4 , with = 16Qmax \u221a 2 N ( h log eNh + log 8 \u03b4 ) .\nProof. This is a restatement of Lemma 1 in Lazaric et al. (2010).\nLemma 7. Let \u03a0 be a policy space with finite VC-dimension h = V C(\u03a0) <\u221e and s(1), . . . , s(N) be an arbitrary sequence of states. At each state we simulate M independent rollouts of the form , then we have\nP sup \u03c0\u2208\u03a0 \u2223\u2223\u2223 1 N N\u2211 i=1 1 M M\u2211 j=1 Rjk\u22121 ( s(i,j), \u03c0(s(i,j)) ) \u2212 1 N N\u2211 i=1 Qk\u22121 ( s(i,j), \u03c0(s(i,j)) )\u2223\u2223\u2223 >  \u2264 \u03b4 ,\nwith = 8Qmax\n\u221a 2\nMN ( h log eMNh + log 8 \u03b4 ) .\nProof. The proof is similar to the one for Lemma 6.\nProof. (Lemma 4) Let a\u2217(\u00b7) = argmaxa\u2208AQk\u22121(\u00b7, a) be the greedy action. To simplify the notation, we remove the dependency of a\u2217 on states and use a\u2217 instead of a\u2217(xi) in the following. We prove the following series of inequalities:\nL\u03a0k\u22121(\u00b5;\u03c0k) (a) \u2264 L\u03a0k\u22121(\u00b5\u0302;\u03c0k) + \u20321 w.p. 1\u2212 \u03b4\u2032\n= 1\nN N\u2211 i=1 [ Qk\u22121(xi, a \u2217)\u2212Qk\u22121 ( xi, \u03c0k(xi) )] + \u20321\n(b) \u2264 1 N N\u2211 i=1 [ Qk\u22121(xi, a \u2217)\u2212 Q\u0302k\u22121 ( xi, \u03c0k(xi) )] + \u20321 + \u2032 2 w.p. 1\u2212 2\u03b4\u2032\n(c) \u2264 1 N N\u2211 i=1 [ Qk\u22121(xi, a \u2217)\u2212 Q\u0302k\u22121 ( xi, \u03c0 \u2217(xi) )] + \u20321 + \u2032 2\n\u2264 1 N N\u2211 i=1 [ Qk\u22121(xi, a \u2217)\u2212Qk\u22121 ( xi, \u03c0 \u2217(xi) )] + \u20321 + 2 \u2032 2 w.p. 1\u2212 3\u03b4\u2032 = L\u03a0k\u22121(\u00b5\u0302;\u03c0\u2217) + \u20321 + 2 \u20322 \u2264 L\u03a0k\u22121(\u00b5;\u03c0\u2217) + 2( \u20321 + \u20322) w.p. 1\u2212 4\u03b4\u2032 = inf \u03c0\u2208\u03a0 L\u03a0k\u22121(\u00b5;\u03c0) + 2( \u20321 + \u20322).\nThe statement of the theorem is obtained by \u03b4\u2032 = \u03b4/4.\n(a) This follows from Lemma 6.\n(b) Here we introduce the estimated action-value function Q\u0302k\u22121 by bounding\nsup \u03c0\u2208\u03a0\n[ 1\nN N\u2211 i=1 Q\u0302k\u22121 ( s(i), \u03c0(s(i)) ) \u2212 1 N N\u2211 i=1 Qk\u22121 ( s(i), \u03c0(s(i)) )]\nusing Lemma 7. (c) From the definition of \u03c0k in CBMPI, we have\n\u03c0k = argmin \u03c0\u2208\u03a0 L\u0302\u03a0k\u22121(\u00b5\u0302;\u03c0) = argmax \u03c0\u2208\u03a0\n1\nN N\u2211 i=1 Q\u0302k\u22121 ( s(i), \u03c0(s(i)) ) ,\nthus, \u22121/N \u2211N i=1 Q\u0302k\u22121 ( s(i), \u03c0k(s (i)) ) can be maximized by replacing \u03c0k with any other policy, particularly with\n\u03c0\u2217 = argmin \u03c0\u2208\u03a0 \u222b S ( max a\u2208A Qk\u22121(s, a)\u2212Qk\u22121 ( s, \u03c0(s) )) \u00b5(ds)."}, {"heading": "F. Proof of Lemma 5", "text": "Let us define two n-dimensional vectors z = ([\n(T\u03c0k) mvk\u22121\n] (s(1)), . . . , [ (T\u03c0k) mvk\u22121 ] (s(n)) )> and y =(\nv\u0302k(s (1)), . . . , v\u0302k(s (n)) )>\nand their orthogonal projections onto the vector space Fn as z\u0302 = \u03a0\u0302z and y\u0302 = \u03a0\u0302y = ( v\u0303k(s (1)), . . . , v\u0303k(s (n)) )>\n, where v\u0303k is the result of linear regression and its truncation (by Vmax) is vk, i.e., vk = T(v\u0303k) (see Figure 2). What we are interested is to find a bound on the regression error \u2016z \u2212 y\u0302\u2016 (the difference between the target function z and the result of the regression y\u0302). We may decompose this error as\n\u2016z \u2212 y\u0302\u2016n \u2264 \u2016z\u0302 \u2212 y\u0302\u2016n + \u2016z \u2212 z\u0302\u2016n = \u2016\u03be\u0302\u2016n + \u2016z \u2212 z\u0302\u2016n, (41)\nwhere \u03be\u0302 = z\u0302 \u2212 y\u0302 is the projected noise (estimation error) \u03be\u0302 = \u03a0\u0302\u03be, with the noise vector \u03be = z \u2212 y defined as \u03bei = [ (T\u03c0k) mvk\u22121 ] (s(i)) \u2212 v\u0302k(s(i)). It is easy to see that noise is zero mean, i.e., E[\u03bei] = 0 and is bounded by 2Vmax, i.e., |\u03bei| \u2264 2Vmax. We may write the estimation error as\n\u2016z\u0302 \u2212 y\u0302\u20162n = \u2016\u03be\u0302\u20162n = \u3008\u03be\u0302, \u03be\u0302\u3009 = \u3008\u03be, \u03be\u0302\u3009,\nwhere the last equality follows from the fact that \u03be\u0302 is the orthogonal projection of \u03be. Since \u03be\u0302 \u2208 Fn, let f\u03b1 \u2208 F be any function whose values at {s(i)}ni=1 equals to {\u03bei}ni=1. By application of a variation of Pollard\u2019s inequality (Gyo\u0308rfi et al., 2002), we obtain\n\u3008\u03be, \u03be\u0302\u3009 = 1 n n\u2211 i=1 \u03beif\u03b1(s (i)) \u2264 4Vmax\u2016\u03be\u0302\u2016n\n\u221a 2\nn log\n( 3(9e2n)d+1\n\u03b4\u2032\n) ,\nwith probability at least 1\u2212 \u03b4\u2032. Thus, we have\n\u2016z\u0302 \u2212 y\u0302\u2016n = \u2016\u03be\u0302\u2016n \u2264 4Vmax\n\u221a 2\nn log\n( 3(9e2n)d+1\n\u03b4\u2032\n) . (42)\nFrom Eqs. 41 and 42, we have\n\u2016(T\u03c0k)mvk\u22121 \u2212 v\u0303k\u2016\u00b5\u0302 \u2264 \u2016(T\u03c0k)mvk\u22121 \u2212 \u03a0\u0302(T\u03c0k)mvk\u22121\u2016\u00b5\u0302 + 4Vmax\n\u221a 2\nn log\n( 3(9e2n)d+1\n\u03b4\u2032\n) , (43)\nwhere \u00b5\u0302 is the empirical norm induced from the n i.i.d. samples from \u00b5. Now in order to obtain a random design bound, we first define f\u03b1\u0302\u2217 \u2208 F as f\u03b1\u0302\u2217(s(i)) = [ \u03a0\u0302(T\u03c0k) mvk\u22121 ] (s(i)), and then define f\u03b1\u2217 = \u03a0(T\u03c0k) mvk\u22121 that is the best approximation (w.r.t. \u00b5) of the target function (T\u03c0k)\nmvk\u22121 in F . Since f\u03b1\u0302\u2217 is the minimizer of the empirical loss, any function in F different than f\u03b1\u0302\u2217 has a bigger empirical loss, thus we have\n\u2016f\u03b1\u0302\u2217 \u2212 (T\u03c0k)mvk\u22121\u2016\u00b5\u0302 \u2264 \u2016f\u03b1\u2217 \u2212 (T\u03c0k)mvk\u22121\u2016\u00b5\u0302 \u2264 2\u2016f\u03b1\u2217 \u2212 (T\u03c0k)mvk\u22121\u2016\u00b5\nApproximate Modified Policy Iteration\n+ 12 ( Vmax + \u2016\u03b1\u2217\u20162 sup\nx \u2016\u03c6(x)\u20162 )\u221a 2 n log 3 \u03b4\u2032 , (44)\nwith probability at least 1\u2212 \u03b4\u2032, where the second inequality is the application of a variation of Theorem 11.2 in the book by Gyo\u0308rfi et al., (2002) with \u2016f\u03b1\u2217 \u2212 (T\u03c0k)mvk\u22121\u2016\u221e \u2264 Vmax + \u2016\u03b1\u2217\u20162 supx \u2016\u03c6(x)\u20162. Similarly, we can write the left-hand-side of Equation 43 as\n2\u2016(T\u03c0k)mvk\u22121 \u2212 v\u0303k\u2016\u00b5\u0302 \u2265 2\u2016(T\u03c0k)mvk\u22121 \u2212 T(v\u0303k)\u2016\u00b5\u0302 \u2265 \u2016(T\u03c0k)mvk\u22121 \u2212 T(v\u0303k)\u2016\u00b5 \u2212 24Vmax\n\u221a 2\nn \u039b(n, d, \u03b4\u2032), (45)\nwith probability at least 1\u2212 \u03b4\u2032, where \u039b(n, d, \u03b4\u2032) = 2(d+ 1) log n+ log e\u03b4\u2032 + log ( 9(12e)2(d+1) ) . Putting together Equations 43, 44, and 45 and using the fact that T(v\u0303k) = vk, we obtain\n\u2016\u03b7k\u20162,\u00b5 = \u2016(T\u03c0k)mvk\u22121 \u2212 vk\u2016\u00b5 \u2264 2 ( 2\u2016(T\u03c0k)mvk\u22121 \u2212 f\u03b1\u2217\u2016\u00b5 + 12 ( Vmax + \u2016\u03b1\u2217\u20162 sup\nx \u2016\u03c6(x)\u20162 )\u221a 2 n log 3 \u03b4\u2032\n+ 4Vmax\n\u221a 2\nn log\n( 3(9e2n)d+1\n\u03b4\u2032\n)) + 24Vmax \u221a 2\nn \u039b(n, d, \u03b4\u2032).\nThe result follows by setting \u03b4 = 3\u03b4\u2032 and some simplification."}, {"heading": "G. Experimental Results", "text": "In this section, we report the empirical evaluation of CBMPI and compare it to DPI and LSPI. In the experiments, we show that CBMPI, by combining policy and value function approximation, can improve over DPI and LSPI. In these experiments, we are using the same setting as in Gabillon et al. (2011) to facilitate the comparison."}, {"heading": "G.1. Setting", "text": "We consider the mountain car (MC) problem with its standard formulation in which the action noise is bounded in [\u22121, 1] and \u03b3 = 0.99. The value function is approximated using a linear space spanned by a set of radial basis functions (RBFs) evenly distributed over the state space.\nEach CBMPI-based algorithm is run with the same fixed budget B per iteration. CBMPI splits the budget into a rollout budget BR = B(1 \u2212 p) used to build the training set of the greedy step and a critic budget BC = Bp used to build the training set of the evaluation step , where p \u2208 (0, 1) is the critic ratio. The rollout budget is divided into M rollouts of length m for each action in A and each state in the rollout set D\u2032, i.e., BR = mMN |A|. The critic budget is divided into one rollout of length m for each action in A and each state in the rollout set D, i.e., BC = mn|A|.\nIn Fig. 3, we report the performance of DPI, CBMPI, and LSPI. In MC, the performance is evaluated as the number of steps-to-go with a maximum of 300. The results are averaged over 1000 runs. We report the performance of DPI and LSPI at p = 0 and p = 1, respectively. DPI can be seen as a special case of CBMPI where p = 0. We tested the performance of DPI and CBMPI on a wide range of parameters (m,M,N, n) but we only report their performance for the best choice of M (M = 1 was the best choice in all the experiments) and different values of m."}, {"heading": "G.2. Experiments", "text": "As discussed in Remark 5, the parameter m balances between the error in evaluating the value function and the error in evaluating the policy. The value function approximation error tends to zero for large values of m. Although this would suggest to have large values for m, the size of the rollout sets would correspondingly decrease as N = O(B/m) and n = O(B/m), thus decreasing the accuracy of both the regression and classification problems. This leads to a trade-off between long rollouts and the number of states in the rollout sets. The solution to this trade-off strictly depends on the capacity of the value function space F . A rich value function space would lead to solve the trade-off for small values of m. On the other hand, when the value function space is poor, or as in the DPI case, m should be selected in a way to guarantee a sufficient number of informative rollouts, and\nat the same time, a large enough rollout sets.\nFigure 3 shows the learning results in MC with budget B = 200. On the left panel, the function space is rich enough to approximate v\u2217. Therefore LSPI has almost optimal results (about 80 steps to reach the goal). On the other hand, DPI achieves a poor performance of about 150 steps, which is obtained by setting m = 12 and N = 5. We also report the performance of CBMPI for different values of m and p. When p is large enough, the value function approximation becomes accurate enough so that the best solution is to have m = 1. This both corresponds to rollouts built almost entirely on the basis of the approximated value function and to a large number of states in the training set N . For m = 1 and p \u2248 0.8, CBMPI achieves a slightly better performance than LSPI.\nIn the next experiment, we show that CBMPI is able to outperform both DPI and LSPI when F has a lower accuracy. The results are reported on the right panel of Figure 3. The performance of LSPI now worsens to 190 steps. Simultaneously one can notice m = 1 is no longer the best choice for CBMPI. Indeed in the case where m = 1, CBMPI becomes an approximated version of the value iteration algorithm relying on a function space not rich enough to approximate v\u2217. Notice that relying on this space is still better than setting the value function to zero which is the case in DPI. Therefore, we notice an improvement of CBMPI over DPI for m = 4 which trade-off between the estimates of the value function and the rewards collected by the rollouts. Combining those two, CBMPI also improves upon LSPI."}], "references": [{"title": "Fitted Qiteration in continuous action-space MDPs", "author": ["A. Antos", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of NIPS, pp", "citeRegEx": "Antos et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2007}, {"title": "approximate) iterated successive approximations algorithm for sequential decision processes", "author": ["Canbolat", "Pelin", "Rothblum", "Uriel"], "venue": "Annals of Operations Research, pp", "citeRegEx": "Canbolat et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Canbolat et al\\.", "year": 2012}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ernst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ernst et al\\.", "year": 2005}, {"title": "Error propagation for approximate policy and value iteration", "author": ["A. Farahmand", "R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Farahmand et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2010}, {"title": "Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes", "author": ["A. Fern", "S. Yoon", "R. Givan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fern et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fern et al\\.", "year": 2006}, {"title": "Classification-based policy iteration with a critic", "author": ["V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "B. Scherrer"], "venue": "In Proceedings of ICML,", "citeRegEx": "Gabillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gabillon et al\\.", "year": 2011}, {"title": "Reinforcement Learning as Classification: Leveraging Modern Classifiers", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Analysis of a Classification-based Policy Iteration Algorithm", "author": ["A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Lazaric et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lazaric et al\\.", "year": 2010}, {"title": "Error Bounds for Approximate Policy Iteration", "author": ["R. Munos"], "venue": "In Proceedings of ICML, pp", "citeRegEx": "Munos,? \\Q2003\\E", "shortCiteRegEx": "Munos", "year": 2003}, {"title": "Performance Bounds in Lp-norm for Approximate Value Iteration", "author": ["R. Munos"], "venue": "SIAM J. Control and Optimization,", "citeRegEx": "Munos,? \\Q2007\\E", "shortCiteRegEx": "Munos", "year": 2007}, {"title": "Finite-Time Bounds for Fitted Value Iteration", "author": ["R. Munos", "Szepesv\u00e1ri", "Cs"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Munos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Munos et al\\.", "year": 2008}, {"title": "Modified policy iteration algorithms for discounted Markov decision problems", "author": ["M. Puterman", "M. Shin"], "venue": "Management Science,", "citeRegEx": "Puterman and Shin,? \\Q1978\\E", "shortCiteRegEx": "Puterman and Shin", "year": 1978}, {"title": "Approximate Modified Policy Iteration", "author": ["Scherrer", "Bruno", "Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Geist", "Matthieu"], "venue": "Technical report,", "citeRegEx": "Scherrer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Scherrer et al\\.", "year": 2012}, {"title": "Reinforcement Learning Algorithms for MDPs", "author": ["Szepesv\u00e1ri", "Cs"], "venue": "In Wiley Encyclopedia of Operations Research. Wiley,", "citeRegEx": "Szepesv\u00e1ri and Cs.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Performance bound for Approximate Optimistic Policy Iteration", "author": ["Thiery", "Christophe", "Scherrer", "Bruno"], "venue": "Technical report,", "citeRegEx": "Thiery et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thiery et al\\.", "year": 2010}, {"title": "Let \u03a0 be a policy space with finite VC-dimension h = V C(\u03a0) <\u221e and s", "author": ["Lazaric"], "venue": null, "citeRegEx": "Lazaric,? \\Q2010\\E", "shortCiteRegEx": "Lazaric", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "3) that generalize the AVI implementations of Ernst et al. (2005); Antos et al.", "startOffset": 46, "endOffset": 66}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.", "startOffset": 8, "endOffset": 126}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al.", "startOffset": 8, "endOffset": 146}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al.", "startOffset": 8, "endOffset": 169}, {"referenceID": 0, "context": "(2005); Antos et al. (2007); Munos & Szepesv\u00e1ri (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al. (2011). We then provide an error propagation analysis of AMPI (Sec.", "startOffset": 8, "endOffset": 193}, {"referenceID": 2, "context": "Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1.", "startOffset": 43, "endOffset": 83}, {"referenceID": 0, "context": "Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1.", "startOffset": 43, "endOffset": 83}, {"referenceID": 4, "context": "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).", "startOffset": 62, "endOffset": 151}, {"referenceID": 7, "context": "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).", "startOffset": 62, "endOffset": 151}, {"referenceID": 5, "context": "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space \u03a0 (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).", "startOffset": 62, "endOffset": 151}, {"referenceID": 7, "context": "Note that when m tends to \u221e, we recover the DPI algorithm proposed and analyzed by Lazaric et al. (2010).", "startOffset": 83, "endOffset": 105}, {"referenceID": 9, "context": "VI analysis is based on the fact that this algorithm computes the fixed point of the Bellman optimality operator, and this operator is a \u03b3-contraction in max-norm (Bertsekas & Tsitsiklis, 1996; Munos, 2007).", "startOffset": 163, "endOffset": 206}, {"referenceID": 8, "context": "We also know that the analysis of PI usually relies on the fact that the sequence of the generated values is non-decreasing (Bertsekas & Tsitsiklis, 1996; Munos, 2003).", "startOffset": 124, "endOffset": 167}, {"referenceID": 12, "context": "1 and report most proofs in (Scherrer et al., 2012).", "startOffset": 28, "endOffset": 51}, {"referenceID": 12, "context": "1 and report most proofs in (Scherrer et al., 2012). We follow the line of analysis developped by Thiery & Scherrer (2010). The results are obtained using the following three quantities: 1) The distance between the optimal value function and the value before approximation at the k iteration: dk \u2206 = v\u2217 \u2212 (T\u03c0k)vk\u22121 = v\u2217 \u2212 (vk \u2212 k).", "startOffset": 29, "endOffset": 123}, {"referenceID": 7, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al.", "startOffset": 0, "endOffset": 46}, {"referenceID": 3, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP.", "startOffset": 70, "endOffset": 94}, {"referenceID": 3, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients.", "startOffset": 70, "endOffset": 407}, {"referenceID": 3, "context": "Munos (2003; 2007); Munos & Szepesv\u00e1ri (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients. We will discuss the potential advantage of this new class in Remark 4. We will also show through the proofs of Thms. 1 and 3, how the result of Lemma 3 provides us with a flexible tool for turning point-wise bounds into Lp-norm bounds. Thm. 3 in (Scherrer et al., 2012, Appendix D) provides an alternative bound for the loss of AMPI, which in analogy with the results of Farahmand et al. (2010) shows that the last iterations have the highest impact on the loss (the influence exponentially decreases towards the initial iterations).", "startOffset": 70, "endOffset": 853}, {"referenceID": 8, "context": "15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).", "startOffset": 41, "endOffset": 54}, {"referenceID": 9, "context": "15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).", "startOffset": 63, "endOffset": 76}, {"referenceID": 7, "context": "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008), and q = q\u2032 = 2 in Farahmand et al.", "startOffset": 175, "endOffset": 188}, {"referenceID": 7, "context": "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008), and q = q\u2032 = 2 in Farahmand et al.", "startOffset": 175, "endOffset": 215}, {"referenceID": 3, "context": "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = \u221e and q\u2032 = 1 in Munos (2007); Munos & Szepesv\u00e1ri (2008), and q = q\u2032 = 2 in Farahmand et al. (2010). Remark 5.", "startOffset": 234, "endOffset": 258}, {"referenceID": 3, "context": "In analogy with the results of Farahmand et al. (2010), this new bound shows that the last iterations have the highest influence on the loss (the influence exponentially decreases towards the initial iterations).", "startOffset": 31, "endOffset": 55}, {"referenceID": 7, "context": "Proof of Lemma 4 The proof of this lemma is similar to the proof of Theorem 1 in Lazaric et al. (2010). Before stating the proof, we report the following two lemmas that are used in the proof.", "startOffset": 81, "endOffset": 103}, {"referenceID": 7, "context": "This is a restatement of Lemma 1 in Lazaric et al. (2010). Lemma 7.", "startOffset": 36, "endOffset": 58}, {"referenceID": 5, "context": "In these experiments, we are using the same setting as in Gabillon et al. (2011) to facilitate the comparison.", "startOffset": 58, "endOffset": 81}], "year": 2012, "abstractText": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI\u2019s main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.", "creator": "LaTeX with hyperref package"}}}