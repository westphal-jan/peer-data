{"id": "1205.4891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2012", "title": "Clustering is difficult only when it does not matter", "abstract": "numerous papers ask how difficult it is to cluster hierarchical data. we suggest that beyond the more relevant and interesting question is that how difficult it is to cluster discrete data formula sets { \\ em that can be clustered well }. more generally, despite the ubiquity and the great societal importance of clustering, we still rather do need not have written a satisfactory mathematical theory of clustering. in order to properly fully understand clustering, it is clearly necessary to develop a solid theoretical basis for the area. for example, from around the perspective of computational complexity theory the rational clustering problem altogether seems very hard. numerous papers introduce various criteria scales and calculate numerical empirical measures systematically to quantify the quality of a given clustering. the resulting conclusions are pessimistic, since it is computationally difficult to find exactly an optimal clustering of a given data set, if we go by any of these popular criteria. in contrast, the practitioners'perspective is much more optimistic. our explanation for this disparity of rational opinions is that complexity theory concentrates on the worst case, whereas in reality we only care for data sets that can be roughly clustered fairly well.", "histories": [["v1", "Tue, 22 May 2012 12:25:01 GMT  (18kb,D)", "http://arxiv.org/abs/1205.4891v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["amit daniely", "nati linial", "michael saks"], "accepted": false, "id": "1205.4891"}, "pdf": {"name": "1205.4891.pdf", "metadata": {"source": "CRF", "title": "Clustering is difficult only when it does not matter\u2217", "authors": ["Amit Daniely", "Michael Saks"], "emails": ["amit.daniely@math.huji.ac.il", "nati@cs.huji.ac.il", "saks@math.rutgers.edu."], "sections": [{"heading": null, "text": "We introduce a theoretical framework of clustering in metric spaces that revolves around a notion of \u201dgood clustering\u201d. We show that if a good clustering exists, then in many cases it can be efficiently found. Our conclusion is that contrary to popular belief, clustering should not be considered a hard task.\nKeywords: Cluster Analysis, Hardness of clustering, Theoretical Framework for clustering, Stability.\n\u2217Credit for this title goes to Tali Tishby who stated this in a conversation with one of us many years ago. \u2020Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. amit.daniely@math.huji.ac.il \u2021School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. nati@cs.huji.ac.il \u00a7Department of Mathematics, Rutgers University, Piscataway, NJ 08854. Supported in part by NSF under grant CCF-0832787 and by a binational Israel-USA grant 2008368. saks@math.rutgers.edu.\nar X\niv :1\n20 5.\n48 91\nv1 [\ncs .L\nG ]\n2 2\nM ay"}, {"heading": "1 Introduction", "text": "Clustering is the task of partitioning a set of objects in a meaningful way. Notwithstanding several recent attempts to develop a theory of clustering (e.g. [1, 4, 9]), our foundational understanding of the matter is still quite unsatisfactory.\nThe clustering problem deals with a set of objects X that is equipped with some additional structure, such as a dissimilarity (or similarity) function w : X\u00d7X \u2192 [0,\u221e). Informally, we are seeking a partition of X into clusters, such that objects are placed in the same cluster iff they are sufficiently similar. Here are some concrete popular manifestations of this general problem.\n1. A very popular optimization criterion is k-means. Aside from X and w one is given an integer k. The goal is partition X into k parts C1, . . . , Ck and find a center xi \u2208 Ci in each part so as to minimize \u2211 i \u2211 y\u2208Ci w\n2(y, xi). Other popular criteria of similar nature are k-medians, min-sum and others.\n2. Many clustering algorithms work \u201cbottom up\u201d. Initially, every singleton in X is considered as a separate cluster, and the algorithm proceeds by repeatedly merging nearby clusters. Other popular algorithms work \u201ctop down\u201d: Here we start with a single cluster that consists of the whole space. Subsequently, existing clusters get split to improve some objective function.\n3. Several successful methods use spectral methods. One associates a matrix (e.g. a Laplacian) to (X,w), and partitions X according to the eigenvectors of this matrix.\nApproaches to the clustering problem that focus on some objective function, usually result in NP -hard optimization problems. Consequently, most existing theoretical studies concentrate on designing approximation algorithms for such optimization problems and proving appropriate hardness results.\nHowever, the practical purpose of clustering is not to optimize such objectives. Rather, our goal is to find a meaningful partition of the data (provided, of course, that such a partition exists). The point that we advocate is that a satisfactory theory of clustering, should start with a definition of a good clustering and proceed to determine when a good clustering can be found efficiently. In this paper, we follow this approach when the underlying space in a metric1 space.\nThis perspective leads to conclusions which are at odds with common beliefs regarding clustering. This applies, in particular, to the computational hardness of clustering. The infeasibility of optimizing most of the popular objectives led many theoreticians, to the bleak view that clustering is hard. However, we show that in many circumstances a good clustering can be efficiently found, leading to the opposite conclusion. From the practitioner\u2019s viewpoint, \u201dclustering is either easy or pointless\u201d \u2013 that is, whenever\n1The assumption that d is a metric is not too strict. E.g., much of what we do applies even if we weaken the triangle inequality to \u03bb \u00b7 d(x, z) \u2264 d(x, y) + d(y, z) for \u03bb bounded away from zero.\nthe input admits a good clustering, finding it is feasible. Our analysis provides some support to this view.\nThis work is one of several recent attempts to develop a mathematical theory of clustering. For more on the relevant literature, see Section 4."}, {"heading": "1.1 A Theoretical Framework for Clustering in Metric Spaces", "text": "There are numerous notions of clusters in data sets and clustering methods to be found in the literature. Although not necessarily stated explicitly, these methods are guided by an ideal (in the Platonic sense) notion of a good cluster in a space X. This is a subset C \u2286 X such that if x \u2208 C and y 6\u2208 C, then x is substantially closer to C than y is. To rule out trivialities we usually require C to be big enough. This, in particular, eliminates the possibility of trivial singleton clusters. Even more emphasis is put on problems of clustering. Here we seek partitions of the space X into clusters such that every x \u2208 X is substantially closer to the cluster containing it than to any other cluster. The problem is specified in terms of a proximity measure \u2206(x,A) between elements x \u2208 X and subsets A \u2286 X. Numerous natural choices for \u2206(\u00b7, \u00b7) suggest themselves. For example, if X is a metric space, it is reasonable to define \u2206(x,A) in terms of x\u2019s distances from members of A.\nIn the present paper we consider a metric space (X, d) from which data points are sampled2 according to a probability distribution P . The definition we adopt here is \u2206(x,A) = Ey\u223cP [d(x, y)|y \u2208 A]. Other interesting definitions suggest themselves, e.g., \u2206\u2032(x,A) = infy\u2208A\\{x} d(x, y).\nA technical comment: The definition of \u2206(x,A) depends on the distribution P . To simplify notations we omit subscripts such as P when they are clear from the context.\nFormally, we say that C \u2282 X is an (\u03b1, \u03b3)-cluster for \u03b1 > 0, \u03b3 > 1 if P (C) \u2265 \u03b1 and for (almost-)every3 x \u2208 C, y /\u2208 C,\n\u2206(y, C) \u2265 \u03b3 \u00b7\u2206(x,C).\nLikewise, a partition C = {C1, . . . , Ck} of X is an (\u03b1, \u03b3)-clustering for some \u03b1 > 0, \u03b3 > 1 if\n\u2206(x,Cj) \u2265 \u03b3 \u00b7\u2206(x,Ci)\nfor every i 6= j and (almost-)every x \u2208 Ci and, in addition, P (Ci) \u2265 \u03b1 for every i. A few technical points are in place.\n\u2022 We study (\u03b1, \u03b3)-clusterings of a space as well as partitions of a space into (\u03b1, \u03b3)clusters. We note that although these two notions are similar, they are not identical.\n2In certain cases it is inappropriate to assume that points of X are drawn at random. It is also possible that we do not know how X is sampled. In such circumstances, we consider P as the uniform distribution on X.\n3Almost means, as usual, that we are allowing an exceptional set of measure zero.\n\u2022 Our results hold if we choose instead to define \u2206(x,A) as E[d(x, y)|y \u2208 A \\ {x}]. This definition is perfectly reasonable, but it leads to certain minor technical complications that the current definition avoids. Moreover, the difference between the two definitions is rather insignificant, since our main interest is in cases where P ({x}) P (A).\nOur main focus here is on efficient algorithms for finding (\u03b1, \u03b3)-clusters and clusterings. The analysis of these algorithms rely on the structural properties of such clusters. We can now present our main results. To simplify matters without compromising the big picture, we state our theorems in the case when X is a given finite metric space.\nTheorem 1.1 For every fixed \u03b3 > 1, \u03b1 > 0 there is an algorithm that finds all (\u03b1, \u03b3)clusterings of a given finite metric space X and runs in time poly(|X|).\nTheorem 1.2 There is a polynomial time algorithm that on input a finite metric space X and \u03b1 > 0 finds all \u03b3-clusters in X with \u03b3 > 3 and a partition of X into (\u03b1, \u03b3)clusters with \u03b3 > 3, provided one exists. Moreover, the latter problem is NP -hard for \u03b3 = 5/2."}, {"heading": "1.2 An overview", "text": "Our discussion splits according to the value of the parameter \u03b1. When \u03b1 is bounded away from zero we work by exhaustive sampling (e.g. as in [2]). We first sample a small set of points S from the space. Since |S| is small (logarithmic in an error parameter), it is computationally feasible to consider all possible partitions \u03a0 of S. To each partition \u03a0 of S we associate a clustering that can be viewed as the corresponding \u201cVoronoi diagram\u201d. If the space has an (\u03b1, \u03b3)-clustering C, let \u03a0\u2217 be the partition of S that is consistent with C. We show that the \u201cVoronoi diagram\u201d of \u03a0\u2217 nearly coincides with C provided that \u03b3 is bounded away from 1. Concretely, Lemma 2.2 controls the distances between points that reside in distinct clusters in an (\u03b1, \u03b3)-clustering. Together with Hoeffding\u2019s inequality this yields Lemma 2.3 and Corollary 2.4 which show that the \u201cVoronoi diagram\u201d of an appropriate partition of a small sample is nearly an (\u03b1, \u03b3)clustering. Lemma 2.5 speaks about the collection of all possible (\u03b1, \u03b3)-clusterings of the space. It shows that every two distinct (\u03b1, \u03b3)-clusterings must differ substantially. Consequently (Corollary 2.6) there is a bound on the number of (\u03b1, \u03b3)-clusterings that any space can have. All of this is then used to derive an efficient algorithm that can find all (\u03b1, \u03b3)-clusterings of the space, proving Theorem 1.1.\nIn section 3 we deal with the case of small \u03b1. This affects the analysis, since we require that the dependency of the algorithm\u2019s runtime on \u03b1 be poly( 1\n\u03b1 ). We show that\n(\u03b1, 3+ )-clusters are very simple: Such a cluster is a ball and any two such clusters that intersect are (inclusion) comparable. These structural properties are used to derive an efficient algorithm that partitions the space into (\u03b1, 3 + )-clusters (provided that such a partition exists), proving the positive part of Theorem 1.2. To match this result, we\nshow that finding a partition of the space into (\u03b1, 2.5)-clusters is NP-Hard, proving Theorem 1.2 in full.\nLastly, in section 4 we discuss some connection to other work, both old and new, as well as some open questions arising from our work."}, {"heading": "2 Clustering into Few Clusters \u2013 \u03b1 is bounded away", "text": "from zero\nThroughout the section, X is a metric space endowed with a probability measure P . To avoid confusion, other probability measures that are used throughout, are denoted by Pr. We define a metric d between two collections of subsets of X, say C = {C1, . . . , Ck} and C \u2032 = {C \u20321, . . . , C \u2032k}. Namely, d(C, C \u2032) = minP (\u222aki=1Ci\u2295C \u2032\u03c3(i)) where A\u2295B denotes symmetric difference, and the minimum is over all permutations \u03c3 \u2208 Sk. The definition of d(C, C \u2032) extends naturally to the case where C and C \u2032 have k resp. l sets and, say l \u2264 k. The only change is that now \u03c3 : [l]\u2192 [k] is 1 : 1.\nWe define \u2206 also on sets. If A,B \u2286 X, we define \u2206(A,B) as the expectation of d(x, y) where x and y are drawn from the distribution P restricted to A and B respectively. It is easily verified that \u2206 is symmetric and satisfies the triangle inequality. It is usually not a metric, since \u2206(A,A) is usually positive.\nProposition 2.1 For every A,B,C \u2282 X,\n\u2206(A,B) = \u2206(B,A) and \u2206(A,B) \u2264 \u2206(A,C) + \u2206(C,B)\nAs the following lemma shows, distances in an (\u03b1, \u03b3)-clustering are fairly regular\nLemma 2.2 Let C1, . . . , Ck be an (\u03b1, \u03b3)-clustering and let i 6= j. Then\n1. For almost every x \u2208 Ci, y \u2208 Cj, \u03b3\u22121\u03b3 \u2206(y, Ci) \u2264 d(x, y) \u2264 \u03b32+1 \u03b3(\u03b3\u22121)\u2206(y, Ci)\n2. For almost every x, y \u2208 Ci, d(x, y) \u2264 2\u03b3\u22121 \u00b7\u2206(x,Cj)\nProof. Let x \u2208 Ci, y \u2208 Cj. For the left inequality in part 1, note that\nd(x, y) \u2265 \u2206(y, Ci)\u2212\u2206(x,Ci)\n\u2265 \u2206(y, Ci)\u2212 1\n\u03b3 \u00b7\u2206(x,Cj)\n\u2265 \u2206(y, Ci)\u2212 1\n\u03b3 \u00b7 [d(x, y) + \u2206(y, Cj)]\n\u2265 \u2206(y, Ci)\u2212 1 \u03b3 \u00b7 [d(x, y) + 1 \u03b3 \u00b7\u2206(y, Ci)]\nFor the right inequality,\nd(x, y) \u2264 \u2206(x,Ci) + \u2206(y, Ci)\n\u2264 1 \u03b3 \u00b7\u2206(x,Cj) + \u2206(y, Ci) \u2264 1 \u03b3 \u00b7 (d(x, y) + \u2206(y, Cj)) + \u2206(y, Ci) \u2264 1 \u03b3 \u00b7 (d(x, y) + 1 \u03b3 \u00b7\u2206(y, Ci)) + \u2206(y, Ci)\nFor part 2,\nd(x, y) \u2264 \u2206(x,Ci) + \u2206(y, Ci)\n\u2264 1 \u03b3 \u00b7 [\u2206(x,Cj) + \u2206(y, Cj)] \u2264 1 \u03b3 \u00b7 [2 \u00b7\u2206(x,Cj) + d(x, y)]\nNote that for \u03b3 \u2192\u221e all distances d(x, y) with x \u2208 Ci and y \u2208 Cj are roughly equal and d(x1, x2) d(x1, y) for all x1, x2 \u2208 Ci and y \u2208 Cj with i 6= j. We show next how to recover an (\u03b1, \u03b3)-clustering by sampling. For x \u2208 X and A \u2286 X finite, we denote the average distance from x to A\u2019s elements by \u2206U(x,A) := 1 |A| \u2211 y\u2208A d(x, y). A finite sample set S provides us with an estimate for the distance of a point x from a (not necessarily finite) C \u2286 X. Namely, we define the empirical proximity of x to C as \u2206emp(x,C) := \u2206U(x,C \u2229 S).\nWe turn to explain how we recover an unknown (\u03b1, \u03b3)-clustering of X with \u03b1 > 0 and \u03b3 > 1. Consider a collection C = {C1, . . . ,Ck} \u2286 X of disjoint subsets of X. We define a \u201cVoronoi diagram\u201d corresponding to S, denoted C\u03b3 = {C\u03b31 , . . . , C \u03b3 k}. Here\nC\u03b3i = {x \u2208 X : \u2200j 6= i, \u03b3 \u00b7\u2206emp(x,Ci) < \u2206emp(x,Cj)}. If C is a (\u03b1, \u03b3)-clustering of X, we expect C\u03b3 to be a good approximation of C.\nLemma 2.3 Let C = {C1, . . . , Ck} be an (\u03b1, \u03b3)-clustering of X. Let S = {Z1, . . . , Zm} be an i.i.d. sample with distribution P and let q 6= p. Then, for every x \u2208 Cq, > 0,\nP (\u2206emp(x,Cp) \u2265 (\u03b3 \u2212 ) \u00b7\u2206emp(x,Cq)) \u2265 1\u2212 3 exp ( \u2212 (\n(\u03b3 \u2212 1)\u03b1\u221a 8\u03b3(\u03b32 + 1)\n)2 \u00b7m ) The proof follows by a standard application of the Hoeffding bound and is deferred to the appendix.\nCorollary 2.4 Let S = {Z1, . . . , Zm} be an i.i.d. sample with distribution P . Then,\nfor every (\u03b1, \u03b3)-clustering C, Pr(d(C, C\u03b3\u2212\u03b4) > t) \u2264 3 t\u03b1 \u00b7 exp\n( \u2212 (\n(\u03b3\u22121)\u03b4\u03b1\u221a 8\u03b3(\u03b32+1)\n)2 \u00b7m ) .\nProof. Denote C = {C1, . . . , Ck}. By lemma 2.3, with = \u03b4, we have\nE[d(C,C\u03b3\u2212\u03b4)] = E[P (\u222aki=1Ci \u2295 C \u03b3\u2212\u03b4 i )]\n= k\u2211 i=1 \u222b Ci Pr(x /\u2208 C\u03b3\u2212\u03b4i )dP (x)\n= k\u2211 i=1 \u2211 j 6=i \u222b Ci Pr(x \u2208 C\u03b3\u2212\u03b4j )dP (x)\n\u2264 k\u2211 i=1 (k \u2212 1) \u00b7 P (Ci) \u00b7 3 \u00b7 exp\n( \u2212 (\n(\u03b3 \u2212 1)\u03b4\u03b1\u221a 8\u03b3(\u03b32 + 1)\n)2 \u00b7m )\n= (k \u2212 1) \u00b7 3 \u00b7 exp ( \u2212 (\n(\u03b3 \u2212 1)\u03b4\u03b1\u221a 8\u03b3(\u03b32 + 1)\n)2 \u00b7m )\nThus, the lemma follows from Markov\u2019s inequality and the fact that k \u2212 1 \u2264 k \u2264 1 \u03b1\nWe next turn to investigate the collection of all (\u03b1, \u03b3)-clusterings of the given space. We observe first that every two distinct (\u03b1, \u03b3)-clusterings must differ substantially.\nLemma 2.5 If C, C \u2032 are two (\u03b1, \u03b3)-clusterings with d(C, C \u2032) > 0, then d(C, C \u2032) \u2265 \u03b1\u00b7(\u03b3\u22121)2 2\u03b32\u2212\u03b3+1 .\nProof. Denote C = {C1, . . . , Ck}, C \u2032 = {C \u20321, . . . , C \u2032k\u2032} and = d(C, C \u2032). By adding empty clusters if needed, we can assume that k = k\u2032. By reordering the clusters, if necessary, we can assume that P (\u222aki=1Ci \u2295 C \u2032i) = and P (C \u20321 \u2295 C1) > 0. Again by selecting the ordering we can assume the existence of some point x that is in C \u20321 \\ C1 and in C2 \\ C \u20322.\n\u2206(x,C \u20321) = 1 P (C \u20321) \u00b7 \u222b C\u20321 d(x, y)dP (y)\n\u2265 1 P (C \u20321) \u00b7 \u222b C1 d(x, y)dP (y)\u2212 1 P (C \u20321) \u00b7 \u222b C1\\C\u20321 d(x, y)dP (y) \u2265 P (C1) P (C \u20321) \u00b7\u2206(x,C1)\u2212 P (C1 \\ C \u20321) \u03b1 \u00b7 max y\u2208C1\\C\u20321 d(x, y) (1)\n\u2265 (\n1\u2212 \u03b1\n) \u00b7\u2206(x,C1)\u2212\n\u03b1 \u00b7 \u03b3\n2 + 1\n\u03b3(\u03b3 \u2212 1) \u2206(x,C1) \u2265 (\n1\u2212 \u03b1 \u00b7 2\u03b3 2 \u2212 \u03b3 + 1 \u03b3(\u03b3 \u2212 1)\n) \u00b7 \u03b3 \u00b7\u2206(x,C2)\nFor the second inequality note that P (C\u20321) P (C1) \u2265 P (C1)\u2212P (C1\\C \u2032 1) P (C1) \u2265 1\u2212 \u03b1 . The third inequality follows from lemma 2.2.\nAs we just saw \u2206(x,C\u20321) \u2206(x,C2) \u2265 ( 1\u2212 \u03b1 \u00b7 2\u03b32\u2212\u03b3+1 \u03b3(\u03b3\u22121) ) \u00b7 \u03b3. The same argument yields as well\n\u2206(x,C2) \u2206(x,C\u20321)\n\u2265 (\n1\u2212 \u03b1 \u00b7 2\u03b32\u2212\u03b3+1 \u03b3(\u03b3\u22121)\n) \u00b7 \u03b3. Consequently 1 \u2265 ( 1\u2212\n\u03b1 \u00b7 2\u03b32\u2212\u03b3+1 \u03b3(\u03b3\u22121)\n) \u00b7 \u03b3 which proves\nthe lemma. As we observe next, for every \u03b1 > 0 and \u03b3 > 1 the number of (\u03b1, \u03b3)-clusterings that any space can have does not exceed f(\u03b1, \u03b3), where f depends only on \u03b1 and \u03b3 but not on the space. We find this somewhat surprising, although the proof is fairly easy.\nCorollary 2.6 There is a function f = f(\u03b1, \u03b3) defined for \u03b1 > 0 and \u03b3 > 1 with the following property. The number of (\u03b1, \u03b3)-clusterings of any metric probability space X is at most f(\u03b1, \u03b3). This works in particular with f(\u03b1, \u03b3) = 2 \u00b7(\n12(2\u03b32\u2212\u03b3+1) \u03b12(\u03b3\u22121)2\n)(\u221a8\u03b3(\u03b32+1) (\u03b3\u22121)2\u03b1 )2 \u00b7ln( 1 \u03b1 )\nProof. Consider the following experiment. We take an i.i.d. sample Z1, . . . , Zm of points from the distribution P with\nm >\n(\u221a 8\u03b3(\u03b32 + 1)\n(\u03b3 \u2212 1)2\u03b1\n)2 \u00b7 ln (\n12(2\u03b32 \u2212 \u03b3 + 1) \u03b12(\u03b3 \u2212 1)2\n) .\nand partition them randomly into k \u2264 ( 1 \u03b1\n) parts T1, . . . , Tk. This induces a partition C\u2217 = {C1, . . . , Ck} of the space X defined by\nCi = {x \u2208 X : \u2200j 6= i, \u2206U(x, Ti) < \u2206U(x, Tj)}\nFor every (\u03b1, \u03b3)-clustering C of X we consider the event AC that the induced partition of X satisfies d(C, C\u2217) < \u03b1 \u00b7 (\u03b3\u22121) 2\n2\u00b7(2\u03b32\u2212\u03b3+1) . Let us consider the events AC over distinct\n(\u03b1, \u03b3)-clusterings of the space. By Lemma 2.5, these events AC are disjoint. Now consider the event B that the Ti\u2019s are consistent with C. There are at most ( 1\u03b1)\nm ways to partition the sampled points into 1\n\u03b1 parts or less, so that Pr(B) \u2265 \u03b1m. By the choice\nof m and by Corollary 2.4 Pr(AC|B) \u2265 12 . Thus, Pr(AC) \u2265 Pr(B) \u00b7 Pr(AC|B) \u2265 1 2 \u03b1m. Consequently, X has at most f(\u03b1, \u03b3) = 2( 1 \u03b1 )m distinct (\u03b1, \u03b3)-clusterings, as claimed.\nNote 2.7 Fix \u03b1 > 0. The number of (\u03b1, \u03b3)-clusterings might be quite large when \u03b3 is close to 1. For example, let X be an n-point space, with uniform metric and uniform probability measure. Every partition in which each part has cardinality \u2265 \u03b1 \u00b7 n is an (\u03b1, n\nn\u22121)-clustering 4.\n4Note that this example is not valid if we define \u2206(x,A) = E[d(x, y)|y \u2208 A \\ {x}]. To overcome this point, we can replace every point x \u2208 X by many copies, where two copies of x are distance and a copy of x and a copy of y 6= x are at distance d(x, y)."}, {"heading": "Algorithmic Aspects", "text": "Fix \u03b1 > 0, \u03b3 > 1. We shall now show that an (\u03b1, \u03b3)-clustering can be well approximated efficiently. By lemma 2.4, (\u03b1, \u03b3)-clustering can be approximated by a small sample, where the approximation is with respect to the symmetric difference metric. A major flaw of this approximation scheme is that we have no verification method to accompany it. We do not know how to check whether a given partition is close to an (\u03b1, \u03b3)clustering w.r.t. the symmetric difference metric. To this end, we introduce another notion of approximation. A family of subsets of X, C = {C1, . . . , Ck}, is an ( , \u03b1, \u03b3)clustering if\n\u2022 For every i \u2208 [k], P (Ci) \u2265 \u03b1\n\u2022 There is a set N \u2282 X with P (N) \u2264 such that every x \u2208 X \\ N , belongs to exactly one Ci and for every j 6= i, \u2206(x,Cj) \u2265 \u03b3 \u00b7\u2206(x,Ci).\nWe consider next a partition that is attained by the method of Corollary 2.4. We show that if it is -close to an (\u03b1, \u03b3)-clustering w.r.t. symmetric differences, then it is necessarily an (\u03b1\u2212 , \u03b3 \u2212O( ), )-clustering.\nWe associate with every collection A = {A1, . . . , Ak} of finite subsets5 of X the following collection of subsets C\u03b3(A) = {C\u03b31 (A), . . . , C \u03b3 k (A)}:\nC\u03b3i (A) = {x \u2208 X : \u2200j 6= i, \u03b3 \u00b7\u2206U(x,Ai) < \u2206U(x,Aj)} (2)\nwhere, as above, \u2206U(x,A) := 1 |A| \u2211 z\u2208A d(x, z).\nProposition 2.8 Let C = {C1, . . . , Ck} be an (\u03b1, \u03b3)-clustering. Let A = {A1, . . . , Ak} where \u2200i, Ai \u2282 Ci and d(C\u03b3(A), C) < . Then C\u03b3(A) is an (\u03b1\u2212 , \u03b3\u2212O( ), )-clustering. The unspecified coefficients in the O-term depend on \u03b1 and \u03b3.\nThe main idea of the proof is rather simple: The assumption d(C\u03b3(A), C) < implies that for all i the set Ci\u2295C\u03b3i (A) is small. This suggests that \u2206(x,Ci) \u2248 \u2206(x,C \u03b3 i (A)) for most points x \u2208 X. The only difficulty in realizing this idea is that points in Ci\u2295C\u03b3i (A) might have a large effect on either \u2206(x,Ci) or \u2206(x,C \u03b3 i (A)). But the assumption that Ai \u2282 Ci gives us control over the distances between x to these points. The full proof can be found in the appendix.\nTo recap, the above discussion suggests a randomized algorithm that for a given > 0 runs in time poly(1 ) and finds w.h.p. an (\u03b1 \u2212 , \u03b3 \u2212 O( ), )-clustering of X provided that X has an (\u03b1, \u03b3)-clustering C. We take m = \u0398(log(1 )) i.i.d. samples from X and go over all possible partitions of the sample points into at most 1 \u03b1 sets.\nThere are only ( 1 )O(log( 1 \u03b1 )) such partitions. We next check whether the clustering of X that is induced as in Equation (2) is an (\u03b1 \u2212 , \u03b3 \u2212 O( ), )-clustering (this can be easily done by standard statistical estimates).\n5In fact, we will allow A1, . . . , Ak to have multiple points. Formally, then, A1, . . . , Ak are multisets.\nTo see that the algorithm accomplishes what it should, note that the failure probability in corollary 2.4 with \u03b4 = \u03b3\u22121 can be \u2264 1\n2 for m = \u0398(log(1 )). Thus, w.p. > 1 2 one\nof the considered partitions induces a partition of X which is -close in the symmetric difference sense to C. By Proposition 2.8, this partition is an (\u03b1 \u2212 , \u03b3 \u2212 O( ), )- clustering.\nThis also proves Theorem 1.1: If our input is a finite metric space X, we can apply the above algorithm with = 1|X|+1 and examples that are being sampled from X uniformly at random. As explained, w.h.p., the algorithm will consider every partition which is -close in the symmetric difference sense to any of X\u2019s (\u03b1, \u03b3)-clusterings. However, since = 1|X|+1 , two -close partitions must be identical. This proves Theorem 1.1.\nNote that by corollary 2.6, all the (\u03b1, \u03b3)-clusterings can be approximated. A similar algorithm can efficiently find an approximate (\u03b1, \u03b3, )-clustering, provided that one exists6. Also, similar techniques yield an algorithm to approximate an individual (\u03b1, \u03b3)cluster."}, {"heading": "3 Clustering into Many Clusters", "text": "To simplify matters we consider only finite metric spaces endowed with a uniform probability distribution7.\nLemma 3.1 Let X be a metric space and let > 0.\n1. Let C1, C2 \u2286 X be two (3 + )-clusters. Then C1 \u2229C2 = \u2205, C1 \u2282 C2 or C2 \u2282 C1.\n2. Every (3 + )-cluster is a ball around one of its points.\n3. The claim is sharp and the above claims need not hold for = 0.\nProof. We prove the first claim by contradiction and assume that P (C1 \\C2), P (C2 \\ C1), P (C1 \u2229 C2) are positive. Let x \u2208 C1 \u2229 C2, y \u2208 C1 \u2295 C2 be such that d(x, y) is as small as possible. Say that y \u2208 C2. Clearly, \u2206(x,C1 \\ C2) \u2265 d(x, y).\n6The main difference is that here we do not consider partitions of the whole sample set. Rather, we seek first those sample points that belong to the exceptional set, and only partitions of the remaining sample points are considered.\n7As in the previous section, it\u2019s a fairly easy matter to accommodate general metric spaces and arbitrary probability distributions.\nWe first deal with the case P (C1 \\C2) \u2265 P (C1 \u2229C2), and arrive at a contradiction as follows:\n\u2206(x,C1) = P (C1 \\ C2) P (C1) \u2206(x,C1 \\ C2) + P (C1 \u2229 C2) P (C1) \u2206(x,C1 \u2229 C2)\n\u2265 1 2 \u2206(x,C1 \\ C2) \u2265 1 2 d(x, y) \u2265 1 2 [\u2206(y, C1)\u2212\u2206(x,C1)] \u2265 3 + \u2212 1 2 \u2206(x,C1)\nWhen P (C1 \\ C2) \u2264 P (C1 \u2229 C2), a contradiction is reached as follows. By the choice of x, y, for every z \u2208 C1 \\ C2, there holds \u2206(z, C1 \u2229 C2) \u2265 d(x, y). Therefore,\n\u2206(z, C1) = P (C1 \\ C2) P (C1) \u2206(z, C1 \\ C2) + P (C1 \u2229 C2) P (C1) \u2206(z, C1 \u2229 C2)\n\u2265 1 2 \u2206(z, C1 \u2229 C2) \u2265 1 2 d(x, y) \u2265 1 2 [\u2206(y, C1)\u2212\u2206(x,C1)] \u2265 1 2 \u00b7 (1\u2212 1 3 + ) \u00b7\u2206(y, C1) \u2265 1 2 \u00b7 (1\u2212 1 3 + ) \u00b7 (3 + ) \u00b7\u2206(z, C1)\nTo prove the second part, let C be a (3 + )-cluster of diameter r, and let x, y \u2208 C satisfy d(x, y) = r. Since d(x, y) \u2264 \u2206(x,C) + \u2206(y, C), we may assume w.l.o.g. that \u2206(x,C) \u2265 d(x,y)\n2 . We show now that C = B(x, r) and C is a ball, as claimed. Indeed\nd(x, z) \u2264 r for every z \u2208 C, and if z /\u2208 C, then d(x, z) \u2265 \u2206(z, C) \u2212 \u2206(x,C) \u2265 (3 + \u2212 1)\u2206(x,C) > d(x, y) = r. The conclusion follows.\nTo show that the result is sharp, consider the graph G that is a four-vertex cycle and its graph metric. It is not hard to check that every two consecutive vertices in G constitute a 3-cluster which is not a ball. Moreover a pair of intersecting edges in G yield an example for which the first part of the lemma fails to hold.\nAn (\u03b1, \u03b3)-cluster in a space X is called minimal if it contains no (\u03b1, \u03b3)-cluster other than itself. Such clusters are of interest, since they can be viewed as \u201catoms\u201d in clustering X.\nCorollary 3.2 For every \u03b1, > 0 and every space X there is at most one partition of X into minimal (\u03b1, 3 + )-clusters.\nTo see this, consider two (\u03b1, 3 + )-clusters C and C \u2032 that belong to two different such partitions and have a nonempty intersection. By Lemma 3.1, they must be comparable. By the minimality assumption, C = C \u2032 which proves the claim.\nNote 3.3 We note that the previous Corollary may fail badly without the minimality assumption. Let X = {x1, . . . , xn}\u222a\u0307{y1, . . . , yn}, where d(xi, yi) = 1 for all i and all other distance equal \u03b3. It is not hard to see that the following are (\u03b1, \u03b3)-clusters in X where \u03b1 = 1 2n : A singleton and a pair {xi, yi}. There are 2n = 2 1 2\u03b1 ways to partition"}, {"heading": "X into such clusters.", "text": ""}, {"heading": "Algorithmic Aspects", "text": "We next discuss several algorithmic aspects of clustering into arbitrarily many clusters. Our input consists of a finite metric space X and the parameter \u03b1 > 0. Lemma 3.1 suggests an algorithm for finding (\u03b1, 3 + )-clusters and for partitioning the space into (\u03b1, 3+ )-clusters. The runtime of this algorithm is polynomial in |X|, and independent of \u03b1. The second part of the lemma suggests how to find all the (\u03b1, 3 + )-clusters. As the first part of the lemma shows, the inclusion relation among the (\u03b1, 3 + )-clusters has a tree structure. Thus, we can use dynamic programming to find a partition of the space into (\u03b1, 3 + )-clusters, provided that such a partition exists. This proves the positive part of Theorem 1.2.\nTo match the above positive result, we show\nTheorem 3.4 The following problems are NP-Hard.\n1. (\u03b1, 2.5)-CLUSTERING: Given an n-point metric space X and \u03b1 > 0, decide whether X has a (\u03b1, 2.5)-clustering.\n2. PARTITION-INTO-(\u03b1, 2.5)-CLUSTERS: Given an n-point metric space X and \u03b1 > 0, decide whether X has a partition into (\u03b1, 2.5)-clusters.\nThe proof of this Theorem, which also proves the negative part of Theorem 1.2, is deferred to the appendix."}, {"heading": "4 Conclusion", "text": ""}, {"heading": "4.1 Relation to other work", "text": "As we explain below, our work is inspired by the classical VC/PAC theory. In addition we refer to several recent papers that contribute to the development of a theory of clustering."}, {"heading": "VC/PAC theory", "text": "The VC/PAC setting offers the following formal description of the classification problem. We are dealing with a space X of instances. The problem is to recover an unknown member h\u2217 in a known class H of hypotheses. Here H \u2282 YX , where Y is a finite set of labels. We seek to recover the unknown h\u2217 by observing a sample S = {(xi, h\u2217(xi)}mi=1 \u2282 X \u00d7 Y . These samples come from some fixed but unknown distribution over X .\nOur description of the clustering problem is similar. We consider a space X of instances and a class G of good clusterings (P, C) of X, where P is probability measure over X and C is a partition of X. We are given a sample {X1, . . . , Xm} \u2282 X that comes from some unknown P , where (P, C) \u2208 G for some partition C, and our purpose is to recover C. Specifically, here X is a metric space, G is the class of probability measures P that admit a partition which is a (\u03b1, \u03b3)-clustering and the corresponding partition is the associated (\u03b1, \u03b3)-clustering.\nBoth theories seek conditions on G or H under which there are no information theoretic or computational obstacles that keep us from performing the above mentioned tasks."}, {"heading": "Alternative Notions of Good Clustering", "text": "Our approach is somewhat close in spirit to [4], see also [6]. These papers assume that the space under consideration has a clustering with some structural properties, and show how to find it efficiently. In particular, a key notion in these papers is the \u03b3-average attraction property, which is conceptually similar to our notion of \u03b3clustering. Given a partition C = {C1, . . . , Ck} of a space X it is possible to compare between clusters either additively or through multiplication. In [4] the requirement is that \u2206(x,Ci) + \u03b3 \u2264 \u2206(x,Cj) for every x \u2208 Ci and j 6= i, whereas our condition is \u2206(x,Ci) \u00b7 \u03b3 \u2264 \u2206(x,Cj). A clear advantage of our notion is its scale invariance. On the other hand, their algorithms work even if X is not a metric space and is only endowed with an arbitrary dissimilarity function.\nWe mention two more papers that share a similar spirit. Consider a data set that resides in the unit ball of a Hilbert Space. It is shown in [8] how to efficiently find a large margin classifier for the data provided that one exists. In [1] several additional possible notions of good clustering are introduced and analyzed."}, {"heading": "Stability", "text": "The notion of instance stability was introduced in [5] (See also [3]). An instance for an optimization problem in called stable if the optimal solution does not change (or changes only slightly) upon a small perturbation of the input. The point is made that instances of clustering problems are of practical interest only if they are stable. The notion of an (\u03b1, \u03b3)-clustering has a similar stability property. Namely, if we slightly perturb a metric, an (\u03b1, \u03b3)-clustering is still (\u03b1\u2032, \u03b3\u2032)-clustering for \u03b1\u2032 \u2248, \u03b1, \u03b3\u2032 \u2248 \u03b3.\nThus, a good clustering remains a good clustering under a slight perturbation of the input\nIn fact, the present paper is an outgrowth of our work on stable instances for MAXCUT, which we view as a clustering problem. We recall that the input to the MAXCUT problem is an n\u00d7n nonnegative symmetric matrix W . We seek an S \u2286 [n] which maximizes \u2211 i\u2208S,j 6\u2208S wij. Even METRIC-MAXCUT problem (i.e., when wij form a metric) is NP -Hard []. We say that W \u2032 is a \u03b3-perturbation of W some \u03b3 > 1 if \u2200i, j, \u03b3\u2212 12wij \u2264 w\u2032i,j \u2264 \u03b3 1 2wij. The instance W of MAXCUT is called \u03b3-stable if the optimal solution S for W coincides with the optimal solution for every \u03b3-perturbation W \u2032 of W . The methods presented in this paper can be used to give, for every > 0, an efficient algorithm that correctly solves all (1 + )-stable instances of METRICMAXCUT.\nThese developments will be elaborated in a future publication."}, {"heading": "4.2 Future Work and Open Questions", "text": "In view of this article and papers such as [1, 8, 4] it is clear that there is still much interest in new notions of a good clustering and the relevant algorithms. Still, on the subjects discussed here several natural questions remain open.\n1. We believe that it should be possible to improve the dependence on \u03b1 and \u03b3 of the run time of the algortihm in Theorem 1.1.\n2. We gave an efficient method for partitioning a space into 3-clusters, and showed (theorem 3.4) that it is NP -Hard to find a partition into 2.5-clusters. Can this gap be closed?\n3. As Lemma 3.1 shows, (3 + )-clusters are just balls. It is not hard to see that Lemma 2.3 implies that given an (\u03b1, \u03b3)-clustering of an n-point metric space, it is possible to find O\u03b3(log n) representative points in every cluster so that the clustering is nothing but the Voronoi diagram of the (bunched) representative sets. Presumably, there is still some interesting structural theory of (\u03b1, \u03b3)-clustering waiting to be discovered here. Specifically, can the above O\u03b3(log n) be replaced by O\u03b3(1)? A positive answer would give a deterministic version of our algorithm from section 2, with no dependency of \u03b1, but only on the maximal number of clusters.\n4. Consider the following statement \u201cEvery n-point metric space X has a partition X = A\u222a\u0307B such that for every x \u2208 A, y \u2208 B, it holds that \u03b3(n)\u00b7\u2206(x,A) \u2264 \u2206(x,B) and \u03b3(n) \u00b7\u2206(y,B) \u2264 \u2206(y, A)\u201d. How large can \u03b3(n) be for this statement to be true?"}, {"heading": "A Proofs omitted from the text", "text": "Proof. (of Lemma 2.3) For A \u2282 X, denote IA = 1m |{j : Zj \u2208 A}|, JA =\u2211 j:Zj\u2208A d(x, Zj). For every j \u2208 [m] define\nYj =  1 P (Cp) \u00b7 d(x, Zj) Zj \u2208 Cp \u2212 \u03b3\u2212 2 P (Cq) \u00b7 d(x, Zj) Zj \u2208 Cq\n0 otherwise\nWe have EYj = \u2206(x,Cp)\u2212 (\u03b3\u2212 2) \u00b7\u2206(x,Cq) \u2265 2\u03b3 \u00b7\u2206(x,Cp). Moreover, by lemma 2.2, |Yj| \u2264 (\u03b3\u2212 2 ) \u03b1 \u00b7 \u03b32+1 \u03b3(\u03b3\u22121) \u00b7\u2206(x,Cp) \u2264 \u03b32+1 \u03b1(\u03b3\u22121) \u00b7\u2206(x,Cp). Thus, by Hoeffding\u2019s bound,\nP ( JCp P (Cp) \u2264 ( \u03b3 \u2212 2 ) \u00b7 JCq P (Cq) ) = P ( m\u2211 j=1 Yj \u2264 0 ) \u2264 exp ( \u2212 ( (\u03b3 \u2212 1)\u03b1\u221a 8\u03b3(\u03b32 + 1) )2 \u00b7m )\nAgain by Hoeffding\u2019s bound, we have\nP\n( ICq\nP (Cq) \u2264 1\u2212 4\u03b3\n) \u2264 exp ( \u2212 ( \u03b1\u221a 8\u03b3 )2 \u00b7m )\nP\n( ICp\nP (Cp) \u2265 1 + 4\u03b3\n) \u2264 exp ( \u2212 ( \u03b1\u221a 8\u03b3 )2 \u00b7m ) Combining the inequalities, we conclude that, with probability \u2265 1 \u2212\n3 exp ( \u2212 (\n(\u03b3\u22121)\u03b1\u221a 8\u03b3(\u03b32+1)\n)2 \u00b7m ) ,\nJCp ICp\n(\u03b3 \u2212 )JCq ICq\n=\nJCp P (Cp)\n(\u03b3 \u2212 2 ) JCq P (Cq)\n\u00b7 P (Cp) ICp\nP (Cq)\nICq\n\u00b7 \u03b3 \u2212 2\n\u03b3 \u2212\n\u2265 1\u2212 4\u03b3\n1 + 4\u03b3\n\u00b7 \u03b3 \u2212 2\n\u03b3 \u2212 \u2265 1\nProof (of Proposition 2.8) It is very suggestive how to select the exceptional set in the (\u03b1\u2212 , \u03b3\u2212O( ), )-clustering that we seek. Namely, let N = \u222ai (Ci \\ C\u03b3i (A)). As needed, P (N) < , since d(C\u03b3(A), C) < . To prove our claim, note that \u2200i, P (C\u03b3i ) \u2265 \u03b1 \u2212 since d(C, C\u03b3\u2217 (A)) < . Consider some x \u2208 X \\ N and the unique index i for which x \u2208 C\u03b3i (A) \u2229 Ci. If j 6= i, we need to show that\n\u2206(x,C\u03b3j (A)) \u2265 (\u03b3 \u2212O( ))\u2206(x,C \u03b3 i (A))\nAs in the proof of lemma 2.5, we have \u2206(x,C\u03b3j (A)) \u2265 (\n1\u2212 \u03b1\n) \u2206(x,Cj)\u2212\n\u03b1 max y\u2208Cj\\C\u03b3j (A) d(x, y) \u2265 (\n1\u2212 \u03b1 \u00b7 2\u03b3 2 \u2212 \u03b3 + 1 \u03b3(\u03b3 \u2212 1)\n) \u00b7\u2206(x,Cj) (3)\n=: (1\u2212 a1 \u00b7 ) \u00b7\u2206(x,Cj)\nSimilarly, again as in the proof of lemma 2.5, we have \u2206(x,Ci) \u2265 (\n1\u2212 \u03b1\n) \u2206(x,C\u03b3i (A))\u2212\n\u03b1 max y\u2208C\u03b3i (A)\\Ci d(x, y) (4)\nNow, for y \u2208 C\u03b3i (A), we have\nd(x, y) \u2264 \u2206U(x,Ai) + \u2206U(y, Ai)\n\u2264 1 \u03b3 \u2206U(x,Aj) + 1 \u03b3 \u2206U(y, Aj) \u2264 2 \u03b3 \u2206U(x,Aj) + 1 \u03b3 d(x, y)\nNow, since Aj \u2282 Cj, by lemma 2.2, \u2206U(x,Aj) \u2264 \u03b3 2+1\n\u03b3(\u03b3\u22121)\u2206(x,Cj) and we have,\nd(x, y) \u2264 \u03b3 \u03b3 \u2212 1 \u00b7 \u03b3 2 + 1 \u03b3(\u03b3 \u2212 1) \u00b7 2 \u03b3 \u2206(x,Cj)\nSo, by equation (4) we have,\n\u2206(x,Ci) \u2265 (1\u2212 a2 \u00b7 ) \u00b7\u2206(x,C\u03b3i (A))\u2212 a3 \u00b7 \u00b7\u2206(x,Cj) (5)\nFor some positive constants a2, a3 which depend only of \u03b3 and \u03b1. Now by equations (3) and (5) we conclude that\n\u2206(x,C\u03b3j (A)) \u2265 (1\u2212 (a1 + \u03b3a3) \u00b7 ) \u00b7\u2206(x,Cj) + \u03b3a3 \u00b7 \u00b7\u2206(x,Cj) \u2265 (1\u2212 (a1 + \u03b3a3) \u00b7 ) \u00b7 \u03b3 \u00b7\u2206(x,Ci) + \u03b3a3 \u00b7 \u00b7\u2206(x,Cj) \u2265 (1\u2212 (a1 + \u03b3a3) \u00b7 )(1\u2212 a2 \u00b7 )\u03b3 \u00b7\u2206(x,C\u03b3i (A)) = (\u03b3 \u2212O( )) \u00b7\u2206(x,C\u03b3i (A))\nProof. (of Theorem 3.4) Both claims are proved by the same reduction from 3- DIMENSIONAL-MATCHING (e.g., [7] pp. 221). The input to this problem is a subset M \u2282 Y \u00d7Z\u00d7W , where Y, Z,W are three disjoint q-element sets. A three dimensional matching (=3DM) is a q-element subset M \u2032 \u2282M that covers all elements in Y \u222a\u0307Z\u222a\u0307W . The problem is to decide whether a 3DM exists.\nWe associate with this instance of the problem a graph on vertex set Y \u222a\u0307Z\u222a\u0307W , and edge set the union of all triangles {y, z, w} over (y, z, w) \u2208 M . It is not hard to see that 3DM remains NP -Hard under the restriction that this graph is connected.\nHere is our reduction. Given an instance M \u2282 Y \u00d7 Z \u00d7W of 3DM , we construct a graph GM = (V M , EM) as follows: Associated with every m = (y, z, w) \u2208 M is a gadget below. We consider the clustering problem on GM with its natural graph metric.\n\u2022y\n\u2022m1 \u2022m2\n\u2022m3 \u2022m4\n\u2022m5 \u2022m6 \u2022m7\n\u2022m8 \u2022m9\u2022z \u2022w We say that a triangle T in a graph is isolated if every vertex outside it has at most one neighbor in T . The above gadget is useful for the reduction since it\u2019s easy to verify that:\nClaim 1 The graph GM can be partitioned into isolated triangles iff M has a 3DM .\nProof(sketch). If M has a 3DM , we can construct a partition of V into isolated triangle by taking the triangles\n{y,m1,m2}, {z,m6,m8}, {w,m7,m9}, {m3,m4,m5} (6)\nfor every m in the 3DM and the triangles\n{m1,m3,m6}, {m2,m4,m7}, {m5,m8,m9} (7)\nform outside it. On the other hand, consider any partition ofGM into isolated triangles. Its restriction to every gadget must coincide with one of the above two choices, so that the corresponding 3DM is readily apparent\nBoth NP -Hardness claims in Theorem 3.4 follow from the above discussion and the following claim\nClaim 2 Let G = (V,E) be a connected graph in which all vertex degrees are \u2265 2. For every partition of the vertex set V = \u222a\u0307k1Ci, the following are equivalent\n1. Each Ci induces an isolated triangle.\n2. Each Ci is a ( 3 |V | , 2.5)-cluster.\n3. The partition C1, . . . , Ck is a ( 3 |V | , 2.5)-clustering.\nProof The implication 1. \u21d2 2. and 1. \u21d2 3. are easily verified. We turn to prove 3. \u21d2 1. Let i \u2208 [k]. We need to show that each Ci is an isolated triangle. Clearly, |Ci| \u2265 3 by definition of ( 3|V | , 2.5)-clustering. But G is connected, so there are two neighbors xy with x \u2208 Ci, y 6\u2208 Ci. By proposition 2.2 we have\n1 = d(x, y) \u2265 (2.5\u2212 1)\u2206(x,Ci) \u2265 1.5 \u00b7 |Ci| \u2212 1 |Ci| ,\nso that |Ci| = 3. Consider now x, y \u2208 Ci which are nonadjacent. Since d(x) \u2265 2, it has a neighbor z 6\u2208 Ci. Using Proposition 2.2 we arrive at the following contradiction: 1 = d(x, z) \u2265 (2.5 \u2212 1)\u2206(x,Ci) \u2265 1.5 \u00b7 2+13 = 1.5. We already know that each Ci is a triangle, but why is it isolated? If z \u2208 Cj, j 6= i has at least two neighbors in Ci, then\n2.5 \u00b7\u2206(z, Cj) \u2264 \u2206(z, Ci) \u2264 4 3 < 2.5 \u00b7 2 3 = 2.5 \u00b7\u2206(z, Cj).\nThe proof of 2.\u21d2 1. is similar. Let Ci be cluster in the partition. Using the same argument as before, where the the fact that \u2200x \u2208 Ci, y /\u2208 Ci, d(x, y) \u2265 \u2206(y, Ci) \u2212 \u2206(y, Ci) \u2265 (2.5 \u2212 1)\u2206(x,Ci) replace Proposition 2.2, we deduce that Ci induces a triangle. To show that Ci is isolated, suppose that there exists a vertex z /\u2208 Ci with \u2265 2 neighbors in Ci. Let x \u2208 Ci be an arbitrary vertex. To obtain a contradiction, we note that\n2.5\u2206(x,Ci) \u2264 \u2206(z, Ci) \u2264 4 3 < 2.5 \u00b7 2 3 = 2.5\u2206(x,Ci)\nNote A.1 Theorem 3.4 is tight in the following sense: As the proof shows, the above problems are hard even for graph metrics. On the other hand, given a graph G = (V,E), the following polynomial time algorithms find (i) A partition into (\u03b1, 2.5 + )-clusters, and (ii) A (\u03b1, 2.5 + )-clustering. (provided, of course, that one exists).\n1. If \u03b1 > 1|V | then, as in the proof of theorem 3.4, one shows that a partition into\n(\u03b1, 2.5 + )-clusters / (\u03b1, 2.5 + )-clustering is equivalent to a perfect matching, no edge of which is contained in a triangle. This can be done by first eliminating every edge that belongs to a triangle and then running an arbitrary matching algorithm.\n2. If \u03b1 < 1|V | then clearly there is no partition into (\u03b1, 2.5+ )-clusters / (\u03b1, 2.5+ )-\nclustering. If \u03b1 = 1|V | , the singletons constitute a partition of V into (\u03b1, 2.5 + )- clusters and a (\u03b1, 2.5 + )-clustering.\nNote A.2 As in Note 2.7, by replacing each vertex with many points at distance from each other, the above reduction applies as well with the definition \u2206(x,A) = E[d(x, y)|y \u2208 A \\ {x}]."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Numerous papers ask how difficult it is to cluster data. We suggest that the<lb>more relevant and interesting question is how difficult it is to cluster data sets<lb>that can be clustered well. More generally, despite the ubiquity and the great<lb>importance of clustering, we still do not have a satisfactory mathematical theory<lb>of clustering. In order to properly understand clustering, it is clearly necessary to<lb>develop a solid theoretical basis for the area. For example, from the perspective<lb>of computational complexity theory the clustering problem seems very hard.<lb>Numerous papers introduce various criteria and numerical measures to quantify<lb>the quality of a given clustering. The resulting conclusions are pessimistic, since<lb>it is computationally difficult to find an optimal clustering of a given data set, if<lb>we go by any of these popular criteria. In contrast, the practitioners\u2019 perspective<lb>is much more optimistic. Our explanation for this disparity of opinions is that<lb>complexity theory concentrates on the worst case, whereas in reality we only care<lb>for data sets that can be clustered well.<lb>We introduce a theoretical framework of clustering in metric spaces that<lb>revolves around a notion of \u201dgood clustering\u201d. We show that if a good clustering<lb>exists, then in many cases it can be efficiently found. Our conclusion is that<lb>contrary to popular belief, clustering should not be considered a hard task.<lb>", "creator": "LaTeX with hyperref package"}}}