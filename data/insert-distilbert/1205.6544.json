{"id": "1205.6544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2012", "title": "A Brief Summary of Dictionary Learning Based Approach for Classification (revised)", "abstract": "this note typically presents notably some representative methods applications which are based on dictionary learning ( dl ) for classification. yet we don't review the sophisticated methods or frameworks that involve dl for classification, such a as online dynamic dl and spatial pyramid matching ( spm ), but rather, we concentrate on criticizing the direct dl - based classification methods. here, the \" so - called dynamic direct dl - based identification method \" is the approach directly deals with dl framework by adding arguably some meaningful penalty terms., by listing some representative methods, we can roughly divide them into two categories, i. e. ( 1 ) directly making the dictionary discriminative and ( 2 ) forcing the sparse id coefficients discriminative to clearly push the discrimination power of the dictionary. from this taxonomy, we definitely can expect some extensions of them as future digital researches.", "histories": [["v1", "Wed, 30 May 2012 05:07:55 GMT  (33kb)", "http://arxiv.org/abs/1205.6544v1", "a note revised from a withdrawn one"]], "COMMENTS": "a note revised from a withdrawn one", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["shu kong", "donghui wang"], "accepted": false, "id": "1205.6544"}, "pdf": {"name": "1205.6544.pdf", "metadata": {"source": "CRF", "title": "A Brief Summary of Dictionary Learning Based Approach for Classification", "authors": [], "emails": ["dhwang}@zju.edu.cn"], "sections": [{"heading": null, "text": "ar X\niv :1\n20 5.\n65 44\nv1 [\ncs .C\nV ]"}, {"heading": "1 Introduction", "text": "Dictionary learning (DL), as a particular sparse signal model, aims to learn a set of atoms, or called visual words in the computer vision community, in which a few atoms can be linearly combined to well approximate a given signal. From the view of compression sensing, it is originally designed to learn an adaptive codebook to faithfully represent the signals with sparsity constraint. In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.\nIt is well-known that the conventional DL framework is not adapted to classification as a result that the learned dictionary is merely used for signal reconstruction. Therefore, to circumvent this problem, researchers have developed several approaches to learn a classification-oriented dictionary in a supervised learning fashion by exploring the label information. In this note, we review the some existing representative DL-based classification methods. Through comparison, we can roughly divide them into two categories: (1) directly forcing the dictionary discriminative, or (2) making the sparse coefficients discriminative (usually through simultaneously learning a classifier) to promote the discrimination of the dictionary. The first category, named Track I in this note, mainly uses representation error for the final classification, whereas, the second category (Track II) can utilize the sparse coefficients as new feature representation for classification.\nTrack I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11]. The abbreviations of these methods are listed in Table 1.\nThe organization of this note is as follows. In the end of this section, we review an important method called sparse representation-based classification [10], then introduce the general dictionary learning framework with notations used in this note. Note that even though SRC do not learn dictionaries, it opens the prologue of classification based on sparse coding technique. In Section 2, we introduce Meta-face learning [12] and DLSI [8] as two specific examples of Track I, which uses the reconstruction error for the final classification like what SRC does. Its counterpart, i.e. Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11]. In Section 4, we give a brief summary on DL-based classification methods, and expect some extensions in the future work."}, {"heading": "1.1 Sparse Representation-Based Classification", "text": "Wright et al. [10] propose the sparse representation based classification (SRC) method for robust face recognition, and achieve very impressive results. Suppose there are C classes of individual faces, let D = [X1, . . . ,Xc, . . . ,XC ] \u2208 R d\u00d7N be the set of original training samples, where Xc \u2208 R d\u00d7Nc is the sub-set of all the Nc vector-represented training samples from class c. SRC treats the original data set as an overall dictionary. Denote by x \u2208 Rd a query facial image, then SRC identifies x as the following two-stage procedure:\n1. sparsely code x over X via \u21131-norm minimization\na = argmin a \u2016x\u2212Da\u20162 2 + \u03bb\u2016a\u20161, (1)\nwhere \u03bb is a scalar constant.\n2. identify x to the cth class that\nc = argmin i \u2016x\u2212Xi\u03b4i(a)\u2016 2 2 , (2)\nwhere \u03b4i(\u00b7) is a vector indicator function that extract the elements corresponding to the i th class.\nSRC achieves very impressive performance in face recognition, and robust to noises such as occlusion, lighting, etc. Even if SRC learns no dictionaries for classification, it acts as one vanguard to open the prologue of classification with the help of sparse coding. In this view, we can see SRC naively uses all the training samples as one dictionary, wherein the class-specific training sets are sub-dictionaries contributing to discrimination."}, {"heading": "1.2 Dictionary Learning Framework", "text": "Learning an adaptive dictionary (possible overcomplete) aims to provide a basis pool in which a few bases can be linearly combined to approximate a novel signal. Suppose there are a set of signals, denoted by X = [x1, . . . ,xi, . . . ,xN ], where xi is the i\nth signal. Then the conventional dictionary learning framework learns the dictionary as below:\n{A,D} = argmin D\u2208Rd\u00d7K\nA\u2208RK\u00d7N\nN \u2211\ni=1\n\u2016xi \u2212Dai\u2016 2 2 + \u03bb\u2016ai\u20161\n= argmin D\u2208Rd\u00d7K\nA\u2208RK\u00d7N\n\u2016X\u2212DA\u20162F + \u03bb\u2016A\u20161\ns.t. \u2016di\u2016 2 2 \u2264 1, for \u2200i = 1, . . . , N ,\n(3)\nwhere A = [a1, . . . , aN ] is the coefficient matrix and \u2016A\u20161 = \u2211N i \u2016ai\u20161.\nIt is widely known that classic dictionary learning framework is designed for a reconstruction task instead of classification tasks, even if good classification results are achieved in the literature. It is believed that classification performance will be further improved if we carefully learn a classificationoriented dictionary. In next section, we will have a look at several DL-based classification methods belonging to Track I."}, {"heading": "2 Track I: Directly Making the Dictionary Discriminative", "text": "The methods from Track I use the reconstruction error for the final classification, thus the learned dictionary ought to be as discriminative as possible. Inspired by SRC, Yang et al. propose meta-face learning [12] to learn an adaptive dictionary for each class, and Ramirez et al. add a sophisticated term to derive more delicate classification-oriented dictionaries. Now, we present the two methods."}, {"heading": "2.1 Meta-Face Learning", "text": "SRC directly adopts the original facial images as the dictionary, however, as discussed in [12], this pre-defined dictionary will incorporate much redundancy as well as noise and trivial information that can can be negative to the face recognition. Additionally, when the training data grows, the computation of sparse coding will become a main bottleneck. Focusing on this problem, Yang et al. [12] propose a Metaface learning method to learn a class-specific dictionary for each object:\nDi = argmin Di \u2016Xi \u2212DiAi\u2016 2 2 + \u03bb\u2016Ai\u20161,\ns.t. \u2016dij\u20162 \u2264 1, \u2200j = 1, . . . ,K, (4)\nwhere matrix Xi \u2208 R d\u00d7Ni contains all the training images from the ith class as its columns, dij is the jth column of the ith class-specific sub-dictionary Di = [d i 1 , . . . ,diK ] \u2208 R\nd\u00d7K , and \u2016Ai\u20161 is defined as the summation of \u21131-norm of all the columns of Ai = [a\ni 1 , . . . , aiNi ] \u2208 R K\u00d7Ni , i.e.\n\u2016Ai\u20161 = \u2211Ni j \u2016a i j\u20161. Metaface learning method concatenates all the sub-dictionaries as an overall dictionary D = [D1, . . . ,DC ] for classification, the same as the second stage of SRC."}, {"heading": "2.2 Dictionary Learning with Structured Incoherence", "text": "Ramirez et al. note that the learned sub-dictionaries may share some common bases, i.e. some visual words from different sub-dictionaries can be very coherent [8]. Undoubtedly, the coherence of the atoms can be used for reconstructing the query image interchangeably, and the reconstruction error based classifier will fail in identifying some queries. To circumvent this problem, they add an incoherence term term to drive the dictionaries associated to different classes as independent as possible.\nThe incoherence term is denoted as Q(Di,Dj) = \u2016D T i Dj\u2016 2 F . It is easy to see this term drives the atoms from different sub-dictionaries to be as independent/incoherent as possible. Therefore, Ramirez et al. derive the final dictionary learning method with structured incoherence as below:\nmin {Di,Ai}i=1,...,C\nC \u2211\ni=1\n{\n\u2016Xi \u2212DiAi\u2016 2 F + \u03bb\u2016Ai\u20161\n}\n+ \u03b7 \u2211\ni6=j\n\u2016DTi Dj\u2016 2 F , (5)\nwhere Ai = [a 1 i , . . . , a ni i ] \u2208 R ki\u00d7ni , each column aji is the sparse code corresponding to the signal j \u2208 [1, . . . , ni] in class i.\nThey empirically note that even though the incoherence term is imposed in the dictionaries, atoms representing common features in all classes tend to appear repeated almost exactly in dictionaries corresponding to different classes [8]. Being so common, these atoms are used often and their associated reconstruction coefficients have a high absolute value |ar|, r \u2208 {1, . . . , ki}, thus making the reconstruction costs similar. They further propose to detect such atoms is to inspect the already available DTi Dj matrices, whose absolute values represent the inner products between atoms. By ignoring the coefficients associated to these common atoms when computing the reconstruction error, they improve the discriminatory power of the system."}, {"heading": "3 Track II: Making the Coefficients Discriminative", "text": "Track II is different from Track I in the way of discrimination. Contrary to Track I, it forces the sparse coefficients to be discriminative, and indirectly propagates the discrimination power to the overall dictionary. Track II only need to learn an overall dictionary, instead of class-specific dictionaries. In this section, we list several recent-proposed methods belonging to Track II."}, {"heading": "3.1 Supervised Dictionary Learning", "text": "Before presenting this method, we have to clarify that the Supervised DL (SupervisedDL) method is a specific approach proposed in [6], regardless of other possible supervised DL framework.\nMairal et al. propose to combine the logistic regression with conventional dictionary learning framework as below:\n(A,D) = argmin \u03b8\nD\u2208Rd\u00d7K A\u2208RK\u00d7N\nN \u2211\ni=1\n(C(yif(xi, ai, \u03b8)) + \u03bb0\u2016xi \u2212Dai\u2016 2 2 + \u03bb1\u2016ai\u20161) + \u03bb2\u2016\u03b8\u2016 2 2 ,\ns.t. \u2016di\u2016 2 2 \u2264 1, for \u2200i = 1, . . . , N ,\n(6)\nwhere C is the logistic loss function (C(x) = log(1 + e\u2212x)), which enjoys properties similar to that of the hinge loss from the SVM literature, while being differentiable, and \u03bb2 is a regularization\nparameter which prevents overfitting. This is the approach chosen in [7]. And f is a classification function \u2014 linear in a: f(x, a, \u03b8) = \u03b8Ta + b wherein \u03b8 \u2208 RK , or bilinear in a and x: f(x, a, \u03b8) = xTWa+ b wherein \u03b8 = {W \u2208 Rd\u00d7K , b \u2208 R}."}, {"heading": "3.2 Discriminative K-SVD for Dictionary Learning", "text": "Zhang and Li propose discriminative K-SVD (D-KSVD) to simultaneously achieve a desired dictionary which has good representation power while supporting optimal discrimination of the classes [13]. D-KSVD adds a simple linear regression as a penalty term to the conventional DL framework:\n(D,W,A) = argmin D,W,A \u2016X\u2212DA\u20162F + \u03bb1\u2016H\u2212WA\u2016 2 F + \u03bb2\u2016A\u20161 + \u03bb3\u2016W\u2016 2 F , (7)\nwhereH = [h1, . . . ,hN ] \u2208 R C\u00d7N is the label of the training images, in which hn = [0, . . . , 0, 1, 0, . . . , 0]: the position of non-zero element indicates the class. And W is the parameter of the classifier, \u03bb1, \u03bb2 and \u03bb3 are scalars controlling the relative contribution of the corresponding terms.\nNote that the first two terms can be fused into one, and the term \u2016W\u20162F can be dropped during computation owing to the protocol of the original K-SVD algorithm(details in [13]). After obtaining the classifier parameter W and the dictionary, the final classification can be very fast for a query image."}, {"heading": "3.3 Label Consistent K-SVD", "text": "Jiang et al. propose a label consistent K-SVD (LC-KSVD) method to learn a discriminative dictionary for sparse coding [5]. They introduce a label consistent constraint called \u201cdiscriminative sparse-code error\u201d, and combine it with the reconstruction error and the classification error to form a unified objective function as below:\n(D,W,A) = argmin D,W,A \u2016X\u2212DA\u20162F + \u03bb1\u2016Q\u2212GA\u2016 2 F + \u03bb2\u2016H\u2212WA\u2016 2 F + \u03bb3\u2016A\u20161\ns.t. \u2016di\u2016 2 2 \u2264 1, for \u2200i = 1, . . . , N ,\n(8)\nwhere H and W are the same as that of D-KSVD described in the previous subsection, Q = [q1, . . . ,qN ] \u2208 R K\u00d7N is the label consistence term. Here qn = [0, . . . , 1, . . . , 1, 0, . . . , 0] T \u2208 RK is an indicator corresponding to the input signal xn from suitable class: the non-zero values of qn occur at those indices where the input signal xn and the dictionary codeword dk share the same label.\nThe term \u2016Q\u2212GA\u20162F represents the discriminative sparse-code error, which enforces that the sparse codes A approximate the discriminative sparse codes Q. It forces the signals from the same class to have very similar sparse representations, i.e. encouraging label consistency in resulting sparse codes. At the same time, the linear regression term \u2016H\u2212WA\u20162F is added, which is the same as that of D-KSVD [13]. Intuitively, the final classification mechanism is very fast owing to the classifier parameter matrix W."}, {"heading": "3.4 Fisher Discriminant Dictionary Learning", "text": "Yang et al. propose Fisher discrimination dictionary learning (FisherDL) method based on the Fisher criterion to learn a structured dictionary [11], whose atom has correspondence to the class label. The structured dictionary is denoted as D = [D1, . . . ,DC ], where Dc is the class-specific sub-dictionary associated with the cth class. Denote the data set X = [X1, . . . ,XC ], where Xc is the sub-set of the\ntraining samples from the cth class. Then they solve the following formulation over the dictionary and the coefficients to derive the desired discriminative dictionary:\n(D,A) = argmin D\u2208Rd\u00d7K\nA\u2208RK\u00d7N\nC(X,D,A) + \u03bb1\u2016A\u20161 + \u03bb2f(A),\ns.t. \u2016di\u2016 2 2 \u2264 1, for \u2200i = 1, . . . , N ,\n(9)\nwhere C(X,D,A) is the discriminative fidelity term (pending to discuss it as below); \u2016A\u20161 is the sparsity constraint; f(A) is a discrimination constraint (as discussed below) imposed on the coefficient matrix A.\nThe discriminative fidelity term We can write Ai, the representation of Xi over D, as Ai = [A1i ; . . . ;A c i ; . . . ;A C i ], where A c i is the coding coefficient of Xi over the sub-dictionary Dc. Denote the representation of Dc to Xi as Rc = DcA c i . First of all, the dictionary D should be able to well representXi, and there is Xi \u2248 DAi = D1A 1 i +\u00b7 \u00b7 \u00b7+DjA j i+\u00b7 \u00b7 \u00b7+DCA C i = R1+\u00b7 \u00b7 \u00b7+Rj+\u00b7 \u00b7 \u00b7+RC . Second, since Di is associated with the i th class, it is expected that Xi should be well represented by Di but not by Dj , j 6= i. This implies that A i should have some significant coefficients such that Xi \u2212DiA i i is small, while A j i should have nearly zero coefficients such that DjA j i is small. Thus the discriminative fidelity term is defined as:\nC(Xi,D,Ai) =\u2016Xi \u2212DAi\u2016 2 F + \u2016Xi \u2212DiA i i\u2016 2 F + \u2211\nj 6=i\n\u2016DjA j i\u2016 2 F , (10)\nThe discriminative coefficient term To make dictionary D be discriminative for the samples in X, we can make the coding coefficient of X over D, i.e. A, be discriminative. Based on Fisher Criterion, this can be achieved by minimizing the within-class scatter of A, denoted by SW and maximizing the between-class scatter of A, denoted by SB. SW and SB are defined as:\nSW =\nC \u2211\nc=1\n\u2211\nxi\u2208Xc\n(ai \u2212mc)(ai \u2212mc) T\nSB =\nC \u2211\nc=1\n(mc \u2212m)(mc \u2212m) T\nIntuitively, we can define f(A) as tr(SW ) \u2212 tr(SB). However, such an f(A) is non-convex and unstable. To solve this problem, we propose to add an elastic term \u2016A\u20162F into f(A):\nf(A) = tr(SW )\u2212 tr(SB) + \u03b7\u2016A\u2016 2 F (11)\nIncorporating all the terms, we have the following FDDL model:\n(D,A) = argmin D,A\n{\nC \u2211\nc=1\nC(Xi,D,Ai) + \u03bb2(tr(SW )\u2212 tr(SB) + \u03b7\u2016A\u2016 2 F ) + \u03bb1\u2016A\u20161\n}\n(12)\nThere are some crucial issues related to their model, such as the convexity of f(A) and sparse coding, and they discuss these issue in depth [11]. As for classification, they still utilize the reconstruction error as that of Track I."}, {"heading": "4 Summary", "text": "In previous two sections, we review some representative DL-based classification approaches, both from Track I and Track II. Obviously, it is intuitive but effective to add some sophisticated discrimination term to the conventional DL framework to derive a well-learned dictionary for classification.\nIf we check these methods, we can anticipate a general framework here:\nmin D,W,A C(Y,X,D,A) + \u03b7f(W,A,Y) + \u03bbAhA(A) + \u03bbWhW(W)\ns.t. constraint on D, (13)\nwhere C(Y,X,D,A) is the conventional DL framework, f(W,A,Y) is the discrimination term on the sparse coefficients, hA and hW are the Lagrange constraints on the sparse coefficient matrix A and the projector W, \u03b7 and \u03bb\u2019s are scalars to balance their weights. Note W does not necessarily mean only one projector, but rather represents several ones. From Eq. 13, we can see that, by employing the label matrix Y, the discriminative dictionary can be learned directly in the term C(Y,X,D,A), at the same time, the term f(W,A,Y) can also propagate the discrimination power of the coefficients to the dictionary, making the dictionary even more discriminative and reliable for classification. Obviously, if we set \u03b7 = 0, Eq. 13 degrades to Track I; if we omit the label information in term C(Y,X,D,A), Eq. 13 degenerates to Track II. Note that FisherDL [11] can also be cast as a specific example of Eq. 13, which drives the dictionary to be as discriminative as possible from two directions (direct push and indirect push by the coefficients).\nBesides, the main concern seems to be the trade-off between the classification accuracy and the complexity of formulation. Furthermore, when meeting large scale database, these methods will be time consuming in learning the dictionary. Therefore, how to extend these method to online version is an interesting but significant research."}], "references": [{"title": "Differentiable sparse coding", "author": ["D.M. Bradley", "J.A. Bagnell"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Learning with l1-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang"], "venue": "IEEE Trans. Img. Proc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Image denoising via learned dictionaries and sparse representation", "author": ["M. Elad", "M. Aharon"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On the role of sparse and redundant representations in image processing", "author": ["M. Elad", "M. Figueiredo", "Y. Ma"], "venue": "proceedings of IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent k-svd", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "sparse representation for computer vision and pattern recognition", "author": ["J. Wright", "Y. Ma", "J. Mairal", "G. Sapiro", "T. Huang", "S. Yan"], "venue": "proceedings of IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Fisher discrimination dictionary learning for sparse representation", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "metaface learning for sparse representation based face recognition", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Discriminative k-svd for dictionary learning in face recognition", "author": ["Q. Zhang", "B. Li"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 180, "endOffset": 186}, {"referenceID": 8, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 180, "endOffset": 186}, {"referenceID": 0, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 203, "endOffset": 209}, {"referenceID": 5, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 203, "endOffset": 209}, {"referenceID": 11, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 99, "endOffset": 102}, {"referenceID": 12, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 111, "endOffset": 115}, {"referenceID": 4, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 173, "endOffset": 176}, {"referenceID": 10, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 9, "context": "In the end of this section, we review an important method called sparse representation-based classification [10], then introduce the general dictionary learning framework with notations used in this note.", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "In Section 2, we introduce Meta-face learning [12] and DLSI [8] as two specific examples of Track I, which uses the reconstruction error for the final classification like what SRC does.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "In Section 2, we introduce Meta-face learning [12] and DLSI [8] as two specific examples of Track I, which uses the reconstruction error for the final classification like what SRC does.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "[10] propose the sparse representation based classification (SRC) method for robust face recognition, and achieve very impressive results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "propose meta-face learning [12] to learn an adaptive dictionary for each class, and Ramirez et al.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "1 Meta-Face Learning SRC directly adopts the original facial images as the dictionary, however, as discussed in [12], this pre-defined dictionary will incorporate much redundancy as well as noise and trivial information that can can be negative to the face recognition.", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "[12] propose a Metaface learning method to learn a class-specific dictionary for each object: Di = argmin Di \u2016Xi \u2212DiAi\u2016 2 2 + \u03bb\u2016Ai\u20161, s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "some visual words from different sub-dictionaries can be very coherent [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "They empirically note that even though the incoherence term is imposed in the dictionaries, atoms representing common features in all classes tend to appear repeated almost exactly in dictionaries corresponding to different classes [8].", "startOffset": 232, "endOffset": 235}, {"referenceID": 5, "context": "1 Supervised Dictionary Learning Before presenting this method, we have to clarify that the Supervised DL (SupervisedDL) method is a specific approach proposed in [6], regardless of other possible supervised DL framework.", "startOffset": 163, "endOffset": 166}, {"referenceID": 6, "context": "This is the approach chosen in [7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 12, "context": "2 Discriminative K-SVD for Dictionary Learning Zhang and Li propose discriminative K-SVD (D-KSVD) to simultaneously achieve a desired dictionary which has good representation power while supporting optimal discrimination of the classes [13].", "startOffset": 236, "endOffset": 240}, {"referenceID": 12, "context": "Note that the first two terms can be fused into one, and the term \u2016W\u20162F can be dropped during computation owing to the protocol of the original K-SVD algorithm(details in [13]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "propose a label consistent K-SVD (LC-KSVD) method to learn a discriminative dictionary for sparse coding [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "At the same time, the linear regression term \u2016H\u2212WA\u20162F is added, which is the same as that of D-KSVD [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "propose Fisher discrimination dictionary learning (FisherDL) method based on the Fisher criterion to learn a structured dictionary [11], whose atom has correspondence to the class label.", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "There are some crucial issues related to their model, such as the convexity of f(A) and sparse coding, and they discuss these issue in depth [11].", "startOffset": 141, "endOffset": 145}], "year": 2012, "abstractText": "This note presents some representative methods which are based on dictionary learning (DL) for classification. We do not review the sophisticated methods or frameworks that involve DL for classification, such as online DL and spatial pyramid matching (SPM), but rather, we concentrate on the direct DL-based classification methods. Here, the \u201cso-called direct DL-based method\u201d is the approach directly deals with DL framework by adding some meaningful penalty terms. By listing some representative methods, we can roughly divide them into two categories, i.e. (1) directly making the dictionary discriminative and (2) forcing the sparse coefficients discriminative to push the discrimination power of the dictionary. From this taxonomy, we can expect some extensions of them as future researches.", "creator": "LaTeX with hyperref package"}}}