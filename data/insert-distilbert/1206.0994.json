{"id": "1206.0994", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2012", "title": "An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers", "abstract": "unsupervised models can provide supplementary soft constraints to help colleagues classify new, \" target \" data since similar instances in the target set are more likely to share the same class representation label. such models can also help us detect possible differences between training and target distributions, work which is useful nowadays in applications where concept drift may take place, as in transfer learning settings. this paper describes a general optimization framework that takes as input class membership estimates from existing type classifiers learnt on previously encountered \" source \" data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, analyses and yields a consensus labeling of the target data. nowadays this framework admits successfully a wide range scheme of loss functions approximation and classification / clustering methods. it exploits properties of bregman divergences in conjunction with legendre duality to yield a principled and scalable data approach. a variety of competing experiments generally show that the proposed framework can yield results achieved substantially superior notably to those provided exclusively by popular transductive learning techniques or by naively applying substitute classifiers learnt on the original task data to process the target data.", "histories": [["v1", "Fri, 20 Apr 2012 01:58:40 GMT  (3895kb)", "http://arxiv.org/abs/1206.0994v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ayan acharya", "eduardo r hruschka", "joydeep ghosh", "sreangsu acharyya"], "accepted": false, "id": "1206.0994"}, "pdf": {"name": "1206.0994.pdf", "metadata": {"source": "CRF", "title": "An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers", "authors": ["Ayan Acharya", "Eduardo R. Hruschka", "Joydeep Ghosh", "Sreangsu Acharyya"], "emails": [], "sections": [{"heading": null, "text": "ar X\niv :1\n20 6.\n09 94\nv1 [\ncs .L\nG ]\n2 0\nA pr\n2 01\n2\n01"}, {"heading": "An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers", "text": "Ayan Acharya, University of Texas at Austin, USA Eduardo R. Hruschka, University of Texas at Austin, USA; University of Sao Paulo at Sao Carlos, Brazil Joydeep Ghosh, University of Texas at Austin, USA Sreangsu Acharyya, University of Texas at Austin, USA\nUnsupervised models can provide supplementary soft constraints to help classify new, \u201ctarget\u201d data since similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers learnt on previously encountered \u201csource\u201d data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by popular transductive learning techniques or by na\u0131\u0308vely applying classifiers learnt on the original task to the target data.\nCategories and Subject Descriptors: I.5.2 [Pattern Recognition]: Design Methodology\u2014Classifier design and evaluation; I.5.3 [Pattern Recognition]: Clustering\u2014Algorithms; I.5.4 [Pattern Recognition]: Applications\u2014Computer vision; Text processing\nGeneral Terms: Algorithms, Design, Performance, Theory\nAdditional Key Words and Phrases: Classification; Clustering; Ensembles; Transductive Learning; Semisupervised Learning; Transfer Learning\nACM Reference Format: Acharya, A., Hruschka, E.R., Ghosh, J., Acharyya, S. 2012. ACM Trans. Knowl. Discov. Data. 2, 4, Article 01 (April 2012), 33 pages. DOI = 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000"}, {"heading": "1. INTRODUCTION", "text": "In several data mining applications, ranging from identifying distinct control regimes in complex plants to characterizing different types of stocks in terms of price and volume movements, one builds an initial classification model that needs to be applied to unlabeled data acquired subsequently. Since the statistics of the underlying phenomena being modeled often changes with time, these classifiers may also need to be occasionally rebuilt if performance degrades beyond an acceptable level. In such sit-\nThis work has been supported by NSF Grants (IIS-0713142 and IIS-1016614) and by the Brazilian Research Agencies FAPESP and CNPq. Author\u2019s addresses: A. Acharya, Department of Electrical and Computer Engineering, University of Texas at Austin; Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c\u00a9 2012 ACM 1556-4681/2012/04-ART01 $10.00 DOI 10.1145/0000000.0000000 http://doi.acm.org/10.1145/0000000.0000000\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nuations, it is desirable that the classifier functions well with as little labeling of new data as possible, since labeling can be expensive in terms of time and money, and it is a potentially error-prone process. Moreover, the classifier should be able to adapt to changing statistics to some extent, given the afore-mentioned constraints. This paper addresses the problem of combining multiple classifiers and clusterers in a fairly general setting, that includes the scenario sketched above. An ensemble of classifiers is first learnt on an initial labeled training dataset which can conveniently be denoted by \u201csource\u201d dataset. At this point, the training data can be discarded. Subsequently, when new, unlabeled target data is encountered, a cluster ensemble is applied to it to yield a similarity matrix. In addition, the previously learnt classifier(s) can be used to obtain an estimate of the class probability distributions for this data. The heart of our technique is an optimization framework that combines both sources of information to yield a consensus labeling of the target data. General properties of a large class of loss functions described by Bregman divergences are exploited in this framework in conjunction with Legendre duality and a notion of variable splitting that is also used in alternating direction method of multipliers [Boyd et al. 2011]) to yield a principled and scalable solution. Note that the setting described above is different from transductive learning setups where both labeled and unlabeled data are available at the same time for model building [Silver and Bennett 2008], as well as online methods where decisions are made on one new example at a time, and after each such decision, the true label of the example is obtained and used to update the model parameters [Blum 1998]. Additional differences from existing approaches are described in the section on related works. For the moment we note that the underlying assumption is that similar new instances in the target set are more likely to share the same class label. Thus, the supplementary constraints provided by the cluster ensemble can be useful for improving the generalization capability of the resulting classifier system, specially when labeled data for training the base classifiers is scarce. Also, these supplementary constraints provided by unsupervised models can be useful for designing learning methods that help determine differences between training and target distributions, making the overall system more robust against concept drift. To highlight these additional capabilities that are useful for transfer learning, we provide a separate set of empirical studies where the target data is related to but significantly different from the initial training data. The remainder of this paper is organized as follows. After addressing related work in Section 2, the proposed optimization framework and its associated algorithm \u2014 namedOAC3, fromOptimization Algorithm for Combining Classifiers and Clusterers \u2014 are described in Section 3. This particular algorithm has been briefly introduced in [Acharya et al. 2011]. A convergence analysis of OAC3 is reported in Section 4, while Section 5 analyses its convergence rate. An experimental study illustrating the potential of the proposed framework for a variety of applications is reported in Section 6. Finally, Section 7 concludes the paper. Notation. Vectors and matrices are denoted by bold faced lowercase and capital letters, respectively. Scalar variables are written in italic font. A set is denoted by a calligraphic uppercase letter. The effective domain of a function f(y), i.e., the set of all y such that f(y) < +\u221e is denoted by dom(f), while the interior and the relative interior of a set Y are denoted by int(Y) and ri(Y), respectively. For yi,yj \u2208 Rk, \u3008yi,yj\u3009 denotes their inner product. A function f \u2208 Ck \u2032\nif all of its first k\u2032 derivatives exist and are continuous.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012."}, {"heading": "2. RELATED WORK", "text": "This contribution leverages the theory of classifier and cluster ensemble to solve transfer and semi-supervised learning problems. Also, the underlying optimization framework inherits properties from alternating optimization type of algorithms. In this section, a brief introduction to each of these different research areas is provided. The combination of multiple single or base classifiers to generate a more capable ensemble classifier has been an active area of research for the past two decades [Kuncheva 2004; Oza and Tumer 2008]. Several papers provide both theoretical results [Tumer and Ghosh 1996] and empirical evidence showing the utility of such approaches for solving difficult classification problems. For instance, an analytical framework to mathematically quantify the improvements in classification results due to combining multiple models has been addressed in [Tumer and Ghosh 1996]. A survey of traditional ensemble techniques\u2014 including their applications to many difficult real-world problems such as remote sensing, person recognition, one vs. all recognition, and medicine \u2014 is presented in [Oza and Tumer 2008]. In summary, the extensive literature on the subject has shown that an ensemble created from diversified classifiers is typically more accurate than its individual components. Analogously, several research efforts have shown that cluster ensembles can improve the quality of results as compared to a single clustering solution \u2014 e.g., see [Wang et al. 2011; Ghosh and Acharya 2011] and references therein. Indeed, the potential motivations and benefits for using cluster ensembles are much broader than those for using classifier ensembles, for which improving the predictive accuracy is usually the primary goal. More specifically, cluster ensembles can be used to generate more robust and stable clustering results (compared to a single clustering approach), perform distributed computing under privacy or sharing constraints, or reuse existing knowledge [Strehl and Ghosh 2002]. We note however that:\n\u2022Like single classifiers/clusterers, with very few exceptions [Polikar 2007], ensemble methods assume that the test or scoring data comes from the same underlying distribution as the training (and validation) data. Thus their performance degrades if the underlying input-output map changes over time. \u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].\nTransfer learning emphasizes the transfer of knowledge across related domains, tasks and distributions that are similar but not the same. The domain from which the knowledge is transferred is called the \u201csource\u201d domain and the domain to which the knowledge is transferred is called the \u201ctarget\u201d domain. In transfer learning scenarios, the source and target distributions are somewhat different, as they represent (potentially) related but not identical tasks. The literature on transfer learning is fairly rich and varied (e.g., see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997]. The tasks may be learnt simultaneously [Caruana 1997] or sequentially [Bollacker and Ghosh 2000]. The novelty of our approach lies in the utilization of the theory of both classifier and cluster ensembles to address the challenge when there is very few labeled examples from the target class. There are certain application domains such as the problem of land-cover classification of spatially separated regions, where the setting is appropri-\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nate. Moreover, one does not always need to know a priori whether the target is similar to the source domain. Though there is a recent paper that uses a single clustering to modify the weights of base classifiers in an ensemble in order to provide some transfer learning capability [Gao et al. 2008], that algorithm is completely different from ours. Semi-supervised learning is a domain of machine learning where both labeled and unlabeled data are used to train a model \u2013 typically with lot of unlabeled data and only a small amount of labeled data (see [Bengio et al. 2006; Zhu and Goldberg 2009] and the references therein for more details). There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011]. These approaches are typically non-parametric and transductive, needing both the labeled and unlabeled data to be simultaneously available for the entire training process. OAC3 can use parametric classifiers so that old labeled data can be discarded once the classifier parameters are obtained, leading to additional savings in speed and storage. A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence. OAC3 uses certain Bregman divergences [Censor and Zenios 1997], among which the KL divergence and squared loss constitute just a subset (further details are provided later, in Section 4). This facilitates one to use well-defined functions of measures for a specific problem in order to improve performance. Additionally, the techniques of variable splitting [Boyd et al. 2011] and alternating minimization procedure [Bezdek and Hathaway 2002] are invoked to provide a more scalable solution. The work that comes closest to ours is by Gao et al. [Gao et al. 2009; Gao et al. 2011], which also combines the outputs of multiple supervised and unsupervised models. Here, it is assumed that each model partitions the target dataset X into groups, so that the instances in the same group share either the same predicted class label or the same cluster label. The data, models and outputs are summarized by a bipartite graph with connections only between group nodes and instance nodes. A group node and an instance node are connected if the instance is assigned to the group \u2014 no matter if it comes from a supervised or unsupervised model. The authors cast the final consensus labeling as an optimization problem on this bipartite graph. To solve the optimization problem, they introduce the Bipartite Graph-based Consensus Maximization (BGCM) Algorithm, which is essentially a block coordinate descent based algorithm that performs an iterative propagation of probability estimates among neighboring nodes. Note that their formulation requires hard classification and clustering inputs. In contrast, OAC3 essentially processes only two fused models, namely an ensemble of classifiers and an ensemble of clusterers, the constituents of both of which can be either hard or soft. Moreover,OAC3 avoids solving a difficult correspondence problem\u2014 i.e., aligning cluster labels to class labels \u2014 implicitly tackled by BGCM, and has a lower computational complexity as well."}, {"heading": "3. DESCRIPTION OF OAC3", "text": "The proposed framework that combines classifiers and clusterers to generate a more consolidated classification is depicted in Fig. 1. It is assumed that a set of classifiers (consisting of one or more classifiers) have been previously induced from a training set. Such classifiers could have been derived from labeled and unlabeled data, and they are part of the framework that will be used for classifying new data \u2014 i.e., instances from\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nthe target set X = {xi}ni=1. The target set is a test set that has not been used to build the classifiers. The classifiers are employed to estimate initial class probabilities for every instance xi \u2208 X . These probability distributions are stored as a set of vectors {\u03c0i}ni=1 and will be refined with the help of the clusterer(s). From this point of view, the clusterers provide supplementary constraints for classifying the instances of X , with the rationale that similar instances are more likely to share the same class label. Given k classes, denoted by C = {C\u2113}k\u2113=1\n1, each of \u03c0i\u2019s is of dimension k. In order to capture the similarities between the instances of X , OAC3 also takes as input a similarity matrix S, which can be computed from a cluster ensemble, in such a way that each matrix entry corresponds to the relative co-occurrence of two instances in the same cluster [Strehl and Ghosh 2002] \u2014 considering all the data partitions that form the cluster ensemble induced from X . Alternatively, S can be obtained from computing pair-wise similarities between instances, or from a cophenetic matrix resulting from running a hierarchical clustering algorithm. To summarize, OAC3 receives as inputs a set of vectors {\u03c0i}ni=1 and a similarity matrix S for the target set. After processing these inputs, OAC3 outputs a consolidated classification \u2014 represented by a set of vectors {yi}ni=1 \u2208 S \u2286 R\nk, where yi \u221d P\u0302 (C | xi) (estimated posterior class probability assignment) \u2014 for every instance in X . This procedure is described in more detail in the sequel."}, {"heading": "3.1. Optimization Algorithm \u2014 OAC3", "text": "Consider that r1 (r1 \u2265 1) classifiers, indexed by q1, and r2 (r2 \u2265 1) clusterers, indexed by q2, are employed to obtain a consolidated classification. The following steps (I-III) outline the proposed approach. Steps I and II can be seen as preliminary steps to get the inputs for OAC3, while Step III is the optimization algorithm, which will be discussed in more detail.\n1C, with an overload of notation, is used here to denote a collection of classes and should not be confused with Ck \u2032 which is used to denote smoothness of a function.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nStep I - Obtain input from classifiers. The output of classifier q1 for instance xi is a k-dimensional class probability vector \u03c0\n(q1) i . This probability vector denotes the\nprobabilities for xi being assigned to the corresponding classes (which might be soft or hard assignments). From the set of such vectors {\u03c0 (q1) i } r1 q1=1\n, an average vector can be computed for xi as:\n\u03c0i = 1\nr1\nr1 \u2211\nq1=1\n\u03c0 (q1) i . (1)\nStep II - Obtain a similarity matrix. A similarity matrix can be obtained in a number of ways, such as computing pair-wise similarities between instances from the original space of features. For high-dimensional data, it is usually more appropriate to use a cluster ensemble for computing similarities between instances of the target set. In this case, after applying r2 clustering algorithms (clusterers) to X , a similarity matrix S is computed. Assuming that each clustering is a hard data partition (possibly obtained from a particular subspace), the similarity between two instances is simply the fraction of the r2 clustering solutions in which those two instances lie in the same cluster2. Note that such similarity matrices are byproducts of several cluster ensemble solutions, e.g., the CSPA algorithm in [Strehl and Ghosh 2002]. Step III - Obtain consolidated results fromOAC3. Having defined the inputs for OAC3, namely the set {\u03c0i}ni=1 and the similarity matrix, S, the problem of combining classifiers and clusterers can be posed as an optimization problem whose objective is to minimize J in (2) with respect to the set of probability vectors {yi} n i=1, where yi is the new and hopefully improved estimate of the aposteriori class probability distribution for a given instance in X .3\nJoriginal = \u2211\ni\u2208X\nL(\u03c0i,yi) + \u03b1 \u2211\n(i,j)\u2208X\nsijL(yi,yj) (2)\nThe quantity L(\u00b7, \u00b7) denotes a loss function. Informally, the first term in Eq. (2) captures dissimilarities between the class probabilities provided by the ensemble of classifiers and the output vectors {yi}ni=1. The second term encodes the cumulative weighted dissimilarity between all possible pairs (yi,yj). The weights to these pairs are assigned in proportion to the similarity values sij \u2208 [0, 1] of matrix S. The coefficient \u03b1 \u2208 R+ controls the relative importance of classifier and cluster ensembles. Therefore, minimizing the objective function over {yi}ni=1 involves combining the evidence provided by the ensembles in order to build a more consolidated classification. The approach taken in this paper is quite general in the sense that any Bregman divergence that satisfies some specific properties (these properties will be introduced in more detail in section 4 where the discussion is more relevant) can be used as a loss function L(\u00b7, \u00b7) in Eq. (2). So, before going into further details, the formal definition of Bregman divergence is provided.\nDefinition 3.1 ([Bregman 1967], [Banerjee et al. 2005]). Let \u03c6 : S \u2192 R,S = dom(\u03c6) be a strictly convex function defined on a convex set S \u2286 Rk such that \u03c6 is differentiable on ri(S), which is assumed to be nonempty. The Bregman divergence d\u03c6 : S \u00d7 ri(S) \u2192 [0,\u221e) is defined as d\u03c6(p, q) = \u03c6(p) \u2212 \u03c6(q) \u2212 \u3008p \u2212 q,\u2207\u03c6(q)\u3009, where \u2207\u03c6(q) represents the gradient vector of \u03c6 evaluated at q.\n2A similarity matrix can also be defined for soft clusterings \u2014 e.g., see [Punera and Ghosh 2008]. 3From now on, for generality, we assume that we have two ensembles (a classifier ensemble and a cluster ensemble), but note that each of these ensembles may be formed by a single component.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nA specific Bregman Divergence (e.g. KL-divergence) between two vectors yi and yj can be identified by a corresponding strictly convex function \u03c6 (e.g. negative entropy for KL-divergence), and hence be written as d\u03c6(yi,yj). Following from Definition 3.1, d\u03c6(yi,yj) \u2265 0 \u2200yi \u2208 S,yj \u2208 ri(S) and equality holds if and only if yi = yj . Using this notation, the objective function of OAC3, that is going to be minimized over {yi}ni=1, can be rewritten as:\nJ0 =\n\n\n\u2211\ni\u2208X\nd\u03c6(\u03c0i,yi) + \u03b1 \u2211\n(i,j)\u2208X\nsijd\u03c6(yi,yj)\n\n . (3)\nAll Bregman divergences have the remarkable property that the single best (in terms of minimizing the net loss) representative of a set of vectors, is simply the expectation of this set (!) provided the divergence is computed with this representative as the second argument of d\u03c6(\u00b7, \u00b7) \u2014 see Theorem 3.2 in the sequel for a more formal statement of this result. Unfortunately, this simple form of the optimal solution is not valid if the variable to be optimized occurs as the first argument. In that case, however, one can work in the (Legendre) dual space, where the optimal solution has a simple form \u2014 see [Banerjee et al. 2005] for details. Re-examining Eq. (3), we notice that the yi\u2019s to be minimized over occur both as first and second arguments of a Bregman divergence. Hence optimization over {yi}ni=1 is not available in closed form. We circumvent this problem by creating two copies for each yi \u2014 the left copy, y (l) i , and the right copy, y (r) i . The left (right) copies are used whenever the variables are encountered in the first (second) argument of the Bregman divergences. In what follows, it will be clear that the right and left copies are updated iteratively, and an additional soft constraint is used to ensure that the two copies of a variable remain \u201cclose enough\u201d during the updates. With this modification, we propose minimizing the following objective J : Sn \u00d7 Sn \u2192 [0,\u221e):\nJ(y(l),y(r)) =\n\n\nn \u2211\ni=1\nd\u03c6(\u03c0i,y (r) i ) + \u03b1\nn \u2211\ni,j=1\nsijd\u03c6(y (l) i ,y (r) j ) + \u03bb\nn \u2211\ni=1\nd\u03c6(y (l) i ,y (r) i )\n\n , (4)\nwhere, y(l) = (\ny (l) i\n)n\ni=1 \u2208 Sn and y(r) =\n(\ny (r) i\n)n\ni=1 \u2208 Sn.\nTo solve the optimization problem in an efficient way, we first keep {y (l) i } n i=1 and\n{y (r) i } n i=1 \\ {y (r) j } fixed, and minimize the objective w.r.t. y (r) j only. The problem can, therefore, be written as:\nmin y (r) j\n[\nd\u03c6(\u03c0 (r) j ,y (r) j ) + \u03b1\n\u2211\ni(l)\u2208X\nsi(l)j(r)d\u03c6(y (l) i ,y (r) j ) + \u03bb (r) j d\u03c6(y (l) j ,y (r) j )\n]\n, (5)\nwhere \u03bb (r) j is the corresponding penalty parameter that is used to keep y (r) j and y (l) j\nclose to each other. For every valid assignment of {y (l) i } n i=1, it can be shown that there is a unique minimizer y (r) j \u2217 for the optimization problem in (5). For that purpose, a new Corollary is developed from the results of Theorem 3.2 [Banerjee et al. 2005] that is stated below.\nTHEOREM 3.2 ([BANERJEE ET AL. 2005]). Let Y be a random variable that takes values in Y = {yi}ni=1 \u2282 S \u2286 R k following a probability measure v such that Ev[Y ] \u2208\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nri(S). Given a Bregman divergence d\u03c6 : S \u00d7 ri(S) \u2192 [0,\u221e), the optimization problem mins\u2208ri(S) Ev[d\u03c6(Y, s)] has a unique minimizer given by s \u2217 = \u00b5 = Ev[Y ].\nTo solve the problem formulated in Eq. (5), the following corollary is required:\nCOROLLARY 3.3. Let {Yi} n i=1 be a set of random variables, each of which takes\nvalues in Yi = {yij} ni j=1 \u2282 S \u2286 R k following a probability measure vi such that Evi [Yi] \u2208 ri(S). Consider a Bregman divergence d\u03c6 : S \u00d7 ri(S) \u2192 [0,\u221e) and an objective\nfunction of the form J\u03c6(s) =\nm \u2211\ni=1\n\u03b1iEvi [d\u03c6(Yi, s)] with \u03b1i \u2208 R+ \u2200i. This objective function\nhas a unique minimizer given by s\u2217 = \u00b5 =\n[\nm \u2211\ni=1\n\u03b1iEvi [Yi]\n]\n/\n[\nm \u2211\ni=1\n\u03b1i\n]\n.\nPROOF. Since Evi [Yi] \u2208 ri(S) \u2200i, their convex combination should also belong to ri(S), implying that \u00b5 \u2208 ri(S). Now \u2200s \u2208 ri(S) we have:\nJ\u03c6(s)\u2212 J\u03c6(\u00b5) = m \u2211\ni=1\n\u03b1iEvi [d\u03c6(Yi, s)]\u2212 m \u2211\ni=1\n\u03b1iEvi [d\u03c6(Yi,\u00b5)]\n=\nm \u2211\ni=1\n\u03b1i [\u03c6(\u00b5)\u2212 \u03c6(s)] \u2212 m \u2211\ni=1\n\u03b1i \u2329\nn \u2211\nj=1\nvijyij \u2212 s,\u2207\u03c6(s) \u232a\n+\nm \u2211\ni=1\n\u03b1i \u2329\nn \u2211\nj=1\nvijyij \u2212 \u00b5,\u2207\u03c6(\u00b5) \u232a\n=\nm \u2211\ni=1\n\u03b1i [\u03c6(\u00b5)\u2212 \u03c6(s)\u2212 \u3008\u00b5\u2212 s,\u2207\u03c6(s)\u3009] = d\u03c6(\u00b5, s) m \u2211\ni=1\n\u03b1i \u2265 0\nwith equality only when s = \u00b5 following the strict convexity of \u03c6. Hence, \u00b5 is the unique minimizer of the objective function J\u03c6.\nFrom the results of Corollary 3.3, the unique minimizer of the optimization problem in (5) is obtained as:\ny (r) j\n\u2217 =\n\u03c0 (r) j + \u03b3 (r) j\n\u2211\ni(l)\u2208X\n\u03b4i(l)j(r)y (l) i + \u03bb (r) j y (l) j\n1 + \u03b3 (r) j + \u03bb (r) j\n, (6)\nwhere \u03b3 (r) j = \u03b1 \u2211 i(l)\u2208X si(l)j(r) and \u03b4i(l)j(r) = si(l)j(r)/ [ \u2211 i(l)\u2208X si(l)j(r) ] . The same optimization in (5) is repeated over all the y (r) j \u2019s. After the right copies are updated, the objective function is (sequentially) optimized with respect to all the y (l) i \u2019s. Like in the first step, {y (l) j } n j=1 \\ {y (l) i } and {y (r) j } n j=1 are kept fixed, and the difference between the left and right copies of yi is penalized, so that the optimization with respect to y (l) i can be rewritten as:\nmin y (l) i\n\n\u03b1 \u2211\nj(r)\u2208X\nsi(l)j(r)d\u03c6(y (l) i ,y (r) j ) + \u03bb (l) i d\u03c6(y (l) i ,y (r) i )\n\n , (7)\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nwhere \u03bb (l) i is the corresponding penalty parameter. As mentioned earlier, one needs to work in the dual space now, using the convex function \u03c8 (Legendre dual of \u03c6) which is defined as:\n\u03c8(yi) = \u3008yi,\u2207 \u22121 \u03c6 (yi)\u3009 \u2212 \u03c6(\u2207 \u22121 \u03c6 (yi)). (8)\nOne can show that \u2200yi,yj \u2208 int(dom(\u03c6)), d\u03c6(yi,yj) = d\u03c8(\u2207\u03c6(yj),\u2207\u03c6(yi)) \u2014 see [Banerjee et al. 2005] for more details. Thus, the optimization problem in (7) can be rewritten in terms of the Bregman divergence associated with \u03c8 as follows:\nmin \u2207\u03c6(y (l) i )\n[\n\u03b1 \u2211\nj(r)\u2208X\nsi(l)j(r)d\u03c8(\u2207\u03c6(y (r) j ),\u2207\u03c6(y (l) i )) + \u03bb (l) i d\u03c8(\u2207\u03c6(y (r) i ),\u2207\u03c6(y (l) i ))\n]\n. (9)\nThe unique minimizer of the problem in (9) can be computed using Corollary 3.3. \u2207\u03c6 is monotonic and invertible for \u03c6 being strictly convex and hence the inverse of the unique minimizer for the problem in (9) is also unique and equals to the unique minimizer for the problem in (7). Therefore, the unique minimizer of the problem in (7) with respect to y (l) i is given by:\ny (l) i \u2217 = \u2207\u22121\u03c6\n\n   \n\u03b3 (l) i\n\u2211\nj(r)\u2208X\n\u03b4i(l)j(r)\u2207\u03c6(y (r) j ) + \u03bb (l) i \u2207\u03c6(y (r) i )\n\u03b3 (l) i + \u03bb (l) i\n\n    , (10)\nwhere \u03b3 (l) i = \u03b1 \u2211 j(r)\u2208X si(l)j(r) and \u03b4i(l)j(r) = si(l)j(r)/ [ \u2211 j(r)\u2208X si(l)j(r) ] . For the experi-\nments reported in this paper, the generalized I-divergence, defined as:\nd\u03c6(yi,yj) =\nk \u2211\n\u2113=1\nyi\u2113log\n(\nyi\u2113 yj\u2113\n) \u2212 k \u2211\n\u2113=1\n(yi\u2113 \u2212 yj\u2113), \u2200yi,yj \u2208 R k +, (11)\nhas been used. The underlying convex function is then given by \u03c6(yi) = k \u2211\n\u2113=1\nyi\u2113log(yi\u2113)\nso that \u2207\u03c6(yi) = (1 + log(yi\u2113)) k \u2113=1. Thus, Eq. (10) can be rewritten as:\ny (l) i\n\u2217,I = exp\n\n   \n\u03b3 (l) i\n\u2211\nj(r)\u2208X\n\u03b4i(l)j(r)\u2207\u03c6(y (r) j ) + \u03bb (l) i \u2207\u03c6(y (r) i )\n\u03b3 (l) i + \u03bb (l) i\n\n    \u2212 1, (12)\nwhere part of the superscript \u201c, I\u201d indicates that the optimal value corresponds to Idivergence. Optimization over the left and right arguments of all the instances constitutes one pass (iteration) of the algorithm, and these two steps are repeated till convergence (a detailed proof for convergence will be given in Section 4). Upon convergence, all the yi\u2019s are normalized to unit L1 norm after averaging over the respective left and right copies, to yield the individual class probability distributions for every instance xi \u2208 X . The main steps of OAC3 are summarized in Algorithm 1. The update procedure captured by Eq. (10) deserves some special attention. Depending on the divergence used, the update might not ensure that the left copies returned are in the correct domain. For example, if KL divergence is used, Eq. (10) will not\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nALGORITHM 1: \u2014 OAC3\nInputs: {\u03c0i},S. Output: {yi}. Step 0: Initialize {y (r) i }, {y (l) i } so that y (r) i\u2113 = y (l) i\u2113 = 1 k \u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}, \u2200\u2113 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , k}. Loop until convergence: Step 1: Update y (r) j using Eq. (6) \u2200j \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}. Step 2: Update y (l) i using Eq. (10) \u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}. End Loop Step 3: Compute yi = 0.5[y (l) i + y (r) i ] \u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}. Step 4: Normalize yi \u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n}.\nnecessarily produce probabilities. In that case, one needs to use another Lagrangian multiplier to make sure that the returned values lie on simplex as has been done in [Subramanya and Bilmes 2011]."}, {"heading": "3.2. Time Complexity Analysis of OAC3", "text": "Considering that a trained ensemble of classifiers is available, the computation of the set of vectors {\u03c0i}ni=1 requires O(nr1k), where n is the number of instances in the target set, r1 is the number of components of the classifier ensemble, and k is the number of class labels. Computing the similarity matrix, S, is O(r2n2), where r2 is the number of components of the cluster ensemble. Finally, having {\u03c0i}ni=1 and S available, the computational cost (per iteration) of OAC3 is O(kn2). Actually, the computational bottleneck of OAC3 is not the optimization algorithm itself, whose main steps (1 and 2) can be parallelized (this can be identified by a careful inspection of Eq. (6) and (10)), but the computation of the similarity matrix. Note that low values in the similarity matrix can often be zeroed out to further speed up the computation, without having much impact on the results."}, {"heading": "4. CONVERGENCE ANALYSIS OF OAC3", "text": "We claim thatOAC3 makes the objective J in Eq. 4 converge to some uniqueminimizer when Bregman divergences with the following properties are used as loss functions:\n(a) d\u03c6(p,q) is strictly convex in p and q separately. (b) d\u03c6(p,q) is jointly convex w.r.t p and q. (c) The level sets {q : d\u03c6(p,q) \u2264 r} are bounded for any given p \u2208 S. (d) d\u03c6(p,q) is lower-semi-continuous in p and q jointly. (e) If d\u03c6(pt,qt) \u2192 0 and pt or qt is bounded, then pt \u2192 qt and qt \u2192 pt. (f) If p \u2208 S and qt \u2192 p, then d\u03c6(p,qt) \u2192 0.\nBregman divergences that satisfy the above properties include a large number of useful loss functions such as the well-known squared loss, KL-divergence, generalized I-divergence, logistic loss, Itakura-Saito distance and Bose-Einstein entropy [Wang and Schuurmans 2003a]. These divergences along with their associated strictly convex functions \u03c6(.) and domains are listed in Table I. An alternating optimization algorithm, in general, is not guaranteed to converge. Even if it converges it might not converge to the locally optimal solution. Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized. Others have used information geometry [Csisza\u0301r and Tusna\u0301dy 1984; Wang and Schuurmans 2003b; Subramanya and Bilmes 2011] to analyze the convergence as well as a combination of both information geometry and topological properties\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nof the objective [Gunawardana and Byrne 2005]. In this paper, the information geometry approach is utilized to show that the proposed optimization procedure converges to the global minima of the objective J in 4. At this point it is worth mentioning the connection of the optimization framework with other related approaches. The algorithms in [Zhu and Ghahramani 2002; Belkin et al. 2005] are based on minimizing squared-loss and are only suitable for binary classification problems. Multi-class extension of these algorithms is entirely based on one-vs-all strategy. MP [Subramanya and Bilmes 2011], on the other hand, is suitable for multi-class problems and additionally provides guard against degenerate solutions (those that assign equal confidence to all classes). OAC3 does not guard against degenerate solutions but can easily be extended to alleviate the same problem with the addition of a single tuning parameter. In the experiments reported, no significant difference in performance is observed with this extension and hence it is discarded to help tune one less model parameter. Label Propagation ([Zhu 2005] \u2013 LP) is another related algorithm and has been shown to converge to the optimal solution. In [Subramanya and Bilmes 2011], the authors also proved that their algorithm converges but the convergence rate (for KL divergence) is not proven and only empirical evidence is given for a linear rate. In this paper, apart from generalizing these algorithms with a larger class of Bregman divergences, we provide proofs for linear rate of convergence for generalized I divergence and KL divergence (the proof for squared loss follows directly from the analysis of [Subramanya and Bilmes 2011]). Spectral graph transduction Joachims 2003 is an approximate solution to the NP-hard norm-cut problem. However, this algorithm requires eigen-decomposition of a matrix of size n \u00d7 n, where n is the number of instances, which is inefficient for very large data sets. Manifold regularization [Belkin et al. 2005] is a general framework in which a parametric loss function is defined over the labeled samples and is regularized by graph smoothness term defined over both the labeled and unlabeled samples. In the algorithms proposed therein, one either needs to invert an n \u00d7 n matrix or use optimization techniques for general SVM in case there is no closed form solution. Both OAC3 and MP, on the other hand, have closed form solutions corresponding to each update and hence are perfectly suitable for large scale applications. Information regularization [Corduneanu and Jaakkola 2003], in essence, works on the same intuition as OAC3, but does not provide any proof of convergence and one of the steps of the optimization does not have a closed form solution \u2013 a concern for large data applications. [Tsuda 2005] extended the works of [Corduneanu and Jaakkola 2003] to hyper-graphs and used closed form solutions in both steps of the alternating minimization procedure which, surprisingly, can be seen as a special case of MP.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nWe now give a sketch of the proof of convergence of OAC3. The so-called 5-points property (5-pp) of the objective function J is essential to analyze the convergence. If J satisfies the 3-points property (3-pp) and the 4-points property (4-pp), then it satisfies the 5-pp. Therefore, to prove 5-pp of J , we will try to prove that it satisfies both 3-pp and 4-pp. However, this proof is not easy for any arbitrary Bregman divergence. In [Subramanya and Bilmes 2011], the authors followed the procedure of [Csisza\u0301r and Tusna\u0301dy 1984] to prove the convergence of a slightly different objective that involves KL-divergence as a loss function. The proof there is specific to KLdivergence and does not generalize to Bregman divergences with properties (a) to (f). Therefore, we take a more subtle route in proving the 3-pp and 4-pp of J . We show that the objective function J , which is a sum of Bregman divergences of different pairs of variables, can itself be thought of as a Bregman divergence in some joint space. This Bregman divergence also satisfies the properties (a) to (f), which then allows one to use the convergence tools developed by [Wang and Schuurmans 2003a]. The formal proof for convergence is placed in appendix A to facilitate an easy perusal of the paper."}, {"heading": "5. ANALYSIS OF RATE OF CONVERGENCE FOR OAC3", "text": "In practical applications, the rate of convergence of any optimization algorithm is of great importance. To analyze the same, we use some formulations that were derived in [Bezdek and Hathaway 2003] to characterize the local convergence rate of alternating minimization type of algorithms in general. In this section, we will first explain the tools and then show that the analysis applies to the objective function J seamlessly. The details of the tools are skipped here though and only the main lemmata and theorems are provided."}, {"heading": "5.1. Tools for Analyzing Local Rate of Convergence", "text": "Let us consider a variable z \u2208 S2n where z = (zn\u2032)2nn\u2032=1 and zn\u2032 \u2208 S \u2200n \u2032. Assume functions Mn\u2032 : S2n\u22121 \u2192 S \u2200n\u2032 which are defined as:\nMn\u2032(z\u0303n\u2032) = argmin zn\u2032\u2208S f(z1, \u00b7 \u00b7 \u00b7 , zn\u2032\u22121, zn\u2032 , zn\u2032+1, \u00b7 \u00b7 \u00b7 , z2n) (13)\nHere, z\u0303n\u2032 = (z1, \u00b7 \u00b7 \u00b7 , zn\u2032\u22121, zn\u2032+1, \u00b7 \u00b7 \u00b7 , z2n). Corresponding to each Mn\u2032 we also define a function Cn\u2032 : S2n \u2192 S2n as:\nCn\u2032(z1, \u00b7 \u00b7 \u00b7 , zn\u2032\u22121, zn\u2032 , zn\u2032+1, \u00b7 \u00b7 \u00b7 , z2n) = (z1, \u00b7 \u00b7 \u00b7 , zn\u2032\u22121,Mn\u2032(z\u0303n\u2032), zn\u2032+1, \u00b7 \u00b7 \u00b7 , z2n) (14)\nMoreover, one complete execution of alternating minimization step can conveniently be represented by a function S : S2n \u2192 S2n:\nS(z) = C1 \u25e6 C2 \u25e6 \u00b7 \u00b7 \u00b7C2n(z). (15)\nLEMMA 5.1. Let f : S2n \u2192 R satisfy the following conditions:\n(a) f is C2 in a neighborhood of z\u2217, z\u2217 being a local minimizer of f ; (b) \u22072f(z\u2217) is positive definite; (c) There is a neighborhood N of z\u2217 on which f is strictly convex, and such that for\nn\u2032 \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , 2n} if z = z\u2217 zn\u2032 locally minimizes gn\u2032(zzn\u2032 ) = f(zzn\u2032 ) with zzn\u2032 indicating that all variables except zn\u2032 are held fixed, then z \u2217 zn\u2032 is also the unique global minimizer of gn\u2032(zzn\u2032 ).\nThen in some neighborhood of z\u2217, the minimizing function Mn\u2032 exists and is continuously differentiable \u2200n\u2032 \u2208 {1, 2, 3, \u00b7 \u00b7 \u00b7 , 2n}.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nLEMMA 5.2. Let f : Sn \u2192 R be differentiable and satisfy the conditions of Lemma 5.1. Then \u03c1(\u2207S(z \u2217)) < 1 where \u2207S(z \u2217) is the Jacobian of the mapping S evaluated at z\u2217 and \u03c1 is the spectral radius of the Jacobian.\nBefore presenting the main theorem from [Bezdek and Hathaway 2003], the formal definition of q-linear rate of convergence is provided below. The \u201cq\u201d in this definition stands for quotient.\nDefinition 5.3 (q-linear rate of convergence). A sequence {z(t)} \u2192 z\u2217 q-linearly iff \u2203t0 \u2265 0 and \u2203\u03c1 \u2208 [0, 1) such that \u2200t \u2265 t0, ||z(t+1) \u2212 z\u2217|| \u2264 \u03c1||z(t) \u2212 z\u2217||\nTHEOREM 5.4. Let z\u2217 be a local minimizer of f : Sn \u2192 R for which \u22072f(z\u2217) is positive definite and let f be C2 in a neighborhood of z\u2217. Also let assumption (c) of Lemma 5.2 hold for z\u2217. Then there is a neighborhoodN of z\u2217 such that for any z(0) \u2208 N , the corresponding iteration sequence {z(t+l) = S(z(t)) : t = 0, 1, ...} converges q-linearly to z\u2217."}, {"heading": "5.2. Hessian Calculation of J", "text": "From the theorems and lemmata presented in the previous subsection, one can observe that the Hessian of the objective being positive definite is a critical condition. Therefore, we will try to show that \u22072J is positive definite for some of the Bregman divergences. According to Eq. (4), \u2207J involves the following terms:\n\u2207 y (l) i\nJ = \u03b1\nn \u2211\nj=1;j 6=i\nsij\n[\n\u2207\u03c6(y (l) i )\u2212\u2207\u03c6(y (r) j )\n] + \u03bb [\n\u2207\u03c6(y (l) i )\u2212\u2207\u03c6(y (r) i )\n]\n\u2207 y (r) j\nJ =\n\n(\u2207\u03c6(y (r) j )\u2212\u2207\u03c6(\u03c0j)) + \u03b1\nn \u2211\nj=1;j 6=i\n( \u2207\u03c6(y (r) j )\u2212\u2207\u03c6(y (l) i ) )\n+\u03bb [\n\u2207\u03c6(y (r) j )\u2212\u2207\u03c6(y (l) i )\n]]\u2020\n\u2207 2 \u03c6(y (r) j ).\n\u2207 2J , derived from the above equations, has the following terms:\n\u2207 2\ny (l) i ,y (l) i\nJ = ( \u03b1\nn \u2211\nj=1;j 6=i\nsij + \u03bb ) \u2207 2 \u03c6(y (l) i )\n\u2207 2\ny (r) j ,y (r) j\nJ =\n\n\n(\n1 + \u03b1\nn \u2211\ni=1;j 6=i\nsij + \u03bb ) y (r) j \u2212 \u03c0j \u2212 \u03b1\nn \u2211\ni=1;j 6=i\nsijy (l) i \u2212 \u03bby (l) j\n\n\n\u2020\n\u2207 3 \u03c6(y (r) j )\n+ ( 1 + \u03b1\nn \u2211\ni=1;j 6=i\nsij + \u03bb ) \u2207 2 \u03c6(y (r) j )\n\u2207 2\ny (r) j ,y (l) i J = \u22072 y (l) i ,y (r) j J = \u2212\u03b1sij\u2207 2 \u03c6(y (r) j )(i 6= j)\n\u2207 2\ny (r) i ,y (l) i J = \u22072 y (l) i ,y (r) i J = \u2212\u03bb\u22072\u03c6(y (r) i )\n\u2207 2\ny (l) i ,y (l) j J = \u22072 y (r) i ,y (r) j J = 0, (i 6= j)\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nNote that this calculation is valid for any Bregman divergence within the assumed family."}, {"heading": "5.3. Hessian Calculation for KL and Generalized I divergence", "text": "We are now in a position to show that the Hessian of the objective J is positive definite when KL or I-divergence is used as Bregman divergence. Recall from table I that the generating functions \u03c6(.)\u2019s for KL and I-divergence differ only by a linear term and hence the Hessian of the objective J would be the same for these two cases. We list different terms of the Hessian here:\n\u2207 2\ny (l) i ,y (l) i\nJ =\n \u03b1 n \u2211\nj=1;j 6=i\nsij + \u03bb\n\n diag ( (1/y (l) i\u2113 ) k \u2113=1 )\n(16)\n\u2207 2\ny (r) j ,y (r) j\nJ = diag\n\n     \n\n    \n\u03c0j\u2113 + \u03b1\nn \u2211\ni=1;j 6=i\nsijy (l) i\u2113 + \u03bby (l) j\u2113\ny (r\u2113) j\n2\n\n    \nk\n\u2113=1\n\n     \n(17)\n\u2207 2\ny (r) j ,y (l) i J = \u22072 y (l) i ,y (r) j J = \u2212\u03b1sijdiag ( (1/y (r) j\u2113 ) k \u2113=1 ) (i 6= j) (18)\n\u2207 2\ny (r) i ,y (l) i J = \u22072 y (l) i ,y (r) i J = \u2212\u03bbdiag ( (1/y (r) i\u2113 ) k \u2113=1 )\n(19)\n\u2207 2\ny (l) i ,y (l) j J = \u22072 y (r) i ,y (r) j J = 0, (i 6= j). (20)\nUsing Eqs. (16) to (20) and some simple algebra, the following lemma can be proved.\nLEMMA 5.5. H = \u22072J is positive definite over the domain of J under the assump-\ntion\nn \u2211\ni=1\nk \u2211\n\u2113=1\n\u03c0i\u2113 > 0 when KL or generalized I divergence is used as a Bregman diver-\ngence.\nThe proof is placed in Appendix B."}, {"heading": "5.4. Convergence Rate of OAC3 with KL and I-divergence", "text": "From Lemma 5.5, we have H is positive definite if n \u2211\ni=1\nk \u2211\n\u2113=1\n\u03c0i\u2113 > 0. This is always the\ncase as \u03c0i represents some probability assignment. Also, if generalized I divergence or KL divergence is used as the Bregman divergence, J \u2208 C\u221e (i.e. J is a smooth function). From Lemma A.1, we have that J is jointly strictly convex and hence has a unique minimizer. From the same Lemma, J is separately strictly convex w.r.t each of its arguments. Therefore, with other variables fixed at some value, J has a unique minimizer w.r.t one particular variable. Hence, all the conditions mentioned in Lemma 5.1 are satisfied for J in its entire domain. Therefore, following Theorem 5.4 we can conclude that J converges globally (implying that N = dom(J)) to its unique minimizer q-linearly using OAC3. Note that when the Bregman divergence is the squared Euclidean distance, variable splitting is not required at all. The updates involve only\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\none set of copies (i.e. there is no need to maintain left and right copies) and the q-linear rate of convergence of the objective J can be proved following the same method as done in [Subramanya and Bilmes 2011]. The proof uses Perron-Frobenius theorem to bound the maximum eigen-value of the transformation matrix used to update the values of the probability assignments. Thus, OAC3 converges q-linearly at least when squared Euclidean, KL or I divergence is used as loss function. One needs to compute the Hessian or use some other tricks for other Bregman divergences having properties (a) to (f)."}, {"heading": "6. EXPERIMENTAL EVALUATION", "text": "First we provide a simple pedagogical example that illustrates how the supplementary constraints provided by clustering algorithms can be useful for improving the generalization capability of classifiers. Section 6.2 reports sensitivity analyses on the OAC3 parameters. Then, in Section 6.3, we compare the performance of OAC3 with the recently proposedBGCM [Gao et al. 2009; Gao et al. 2011]. This comparison is straightforward and fair, since it uses the same datasets, as well as the same outputs of the base models, which were kindly provided by the authors of this paper. For a comparison with other semi-supervised methods, the design space is much larger, since we are now faced with a variety of classification and clustering algorithms to choose from as the base models in OAC3, as well as a variety of semi-supervised methods to compare with. Given the space available, in Section 6.4 we use simple (linear) base methods, and pick the popular Semi-Supervised Linear Support Vector Machine (S3VM) [Sindhwani and Keerthi 2006] for comparison. Finally, in Section 6.5 we report empirical results for transfer learning settings."}, {"heading": "6.1. Pedagogical Example", "text": "Consider the two-dimensional dataset known as Half-Moon, which has two classes, each of which represented by 400 instances. From this dataset, 2% of the instances are used for training, whereas the remaining instances are used for testing (target set). A classifier ensemble formed by three well-known classifiers (Decision Tree, Linear Discriminant, and Generalized Logistic Regression) are adopted. In order to get a cluster ensemble, a single linkage (hierarchical) clustering algorithm is chosen. The cluster ensemble is then obtained from five data partitions represented in the dendrogram, which is cut for different number of clusters (from 4 to 8). Fig. 2 shows the target data class labels obtained from the standalone use of the classifier ensemble, whereas Fig. 3 shows the corresponding results achieved by OAC3. The parameter values were set\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nby using cross-validation. In particular, we set \u03b1 = 0.0001 and \u03bb (r) i = \u03bb (l) i = \u03bb = 0.1 for all i. Comparing Fig. 2 to Fig. 3, one can see that OAC3 does a better job, especially with the most difficult objects to be classified, showing that the information provided by the similarity matrix can improve the generalization capability of classifiers. We also evaluate the performance ofOAC3 for different proportions (from 1% to 50%) of training data. Fig. 4 summarizes the average accuracies (over 10 trials) achieved by OAC3. The accuracies provided by the classifier ensemble, as well as by its best individual component, are also shown for comparison purposes. The results obtained by OAC3 are consistently better than those achieved by the classifier ensemble. As expected, the curve for OAC3 shows that the less the amount of labeled objects, the greater are the benefits of using the information provided by the cluster ensemble. With 2% of training data, the accuracies observed are 100% in nine trials and 95% in one trial. The mean and standard deviation are 99.5 and 1.59 respectively. This explains why the error bar exceeds 100%."}, {"heading": "6.2. Sensitivity Analysis", "text": "We perform a sensitivity analysis on the OAC3 parameters by using the same classification datasets employed in [Gao et al. 2009]. These datasets represent eleven classification tasks from three real-world applications (20 Newsgroups, Cora, and DBLP). There are six datasets (News1 \u2014 News6) for 20 Newsgroups and four datasets (Cora1 \u2014 Cora4) for Cora. In each task, there is a target set on which the class labels should be predicted. In [Gao et al. 2009], two supervised models and two unsupervised models were used to obtain (on the target sets) class and cluster labels, respectively. These\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nsame class and cluster labels are used as inputs to OAC3. Then, we vary the OAC3 parameters and observe their respective accuracies. In order to analyze the influence of the parameters \u03b1 and \u03bb (recall that we set \u03bb (r) i = \u03bb (l) i = \u03bb for all i), we consider that the algorithm converges when the relative difference of the objective function in two consecutive iterations is less than \u03b5 = 10\u221210. By adopting this criterion, OAC3 usually converges after nine iterations (on average). The algorithm has shown to be robust with respect to \u03bb. As far as \u03b1 is concerned, for most of the datasets \u2014 News1, News3, News4, News6, Cora1, Cora3, Cora4, and DBLP\u2014 the classification accuracies achieved fromOAC3 are better than those found by the classifier ensemble \u2014 no matter the value chosen for \u03b1. Figure 5 illustrates a typical accuracy surface for different values of \u03bb and \u03b1. It is worth mentioning that the accuracy surface tends to keep steady for \u03b1 > 1 (i.e., the accuracies do not change significantly). In particular, OAC3 was run for \u03b1 = {10; 20; ...; 100; 200; ...; 1000; 100000}, for which the obtained results are the same as those achieved for \u03b1 = 1 for any value of \u03bb. This same observation holds for all the assessed datasets. The interpretation for such results is that there is a threshold value for \u03b1 that makes the second term of the objective function in (2) dominating \u2014 i.e., the information provided by the cluster ensemble is much more important than the information provided by the classifier ensemble. We observed that for five datasets (News3, News6, Cora1, Cora3, and DBLP) any value of \u03b1 > 0.30 provides the best classification accuracy. Thus, the algorithm can be robust with respect to the choice of its parameters for some datasets. For the datasets News2 and News5, some \u03b1 values yield to accuracy deterioration, thereby suggesting that, depending on the value chosen for \u03b1, the information provided by the cluster ensemble may hurt \u2014 e.g., see Figure 7. Finally, for Cora2, accuracy improvements were not observed, i.e., the accuracies provided by the classifier ensemble were always the best ones. This result suggests that the assumption that classes can be represented by means of clusters does not hold. As expected, our experiments also show that the number of iterations may influence the performance of the algorithm. In particular, depending on the values chosen for \u03b1, a high number of iterations may prejudice the obtained accuracies. Considering the best values obtained for \u03b1 in our sensitivity analysis, we observed that, for all datasets, the best accuracies were achieved for less than 10 iterations. By taking into account the results obtained in our sensitivity analyses, and recalling that fine tuning of the OAC3 parameters can be done by means of cross-validation, in the next section we compare the performance of OAC3 with the recently proposed BGCM [Gao et al. 2009; Gao et al. 2011]."}, {"heading": "6.3. Comparison with BGCM", "text": "As discussed in Section 2, BGCM is the algorithm most closely related to OAC3. We evaluate OAC3 on the same classification datasets employed to assess BGCM [Gao et al. 2009; Gao et al. 2011]. These datasets are those addressed in Section 6.2. In [Gao et al. 2009], two supervised models (M1 and M2) and two unsupervised models (M3 and M4) were used to obtain (on the target sets) class and cluster labels, respectively. These same labels are used as inputs to OAC3. In doing so, comparisons between OAC3 and BGCM are performed using exactly the same base models, which were trained in the same datasets4. In other words, bothOAC3 andBGCM receive the\n4For these datasets, comparisons with S3VM [Sindhwani and Keerthi 2006] have not been performed because the raw data required for learning is not available.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nsame inputs with respect to the components of the ensembles, from which consolidated classification solutions for the target sets are generated. For the sake of compactness, the description of the datasets and learning models used in [Gao et al. 2009] are not reproduced here, and the interested reader is referred to that paper for further details. However, the results for their four base models (M1,...,M4), forBGCM, and for two well-known cluster ensemble approaches\u2014MCLA [Strehl and Ghosh 2002] and HBGF [Fern and Brodley 2004] \u2014 are reproduced here for comparison purposes. Being cluster ensemble approaches, MCLA and HBGF ignore the class labels, considering that the four base models provide just cluster labels. Therefore, to evaluate classification accuracy obtained by these ensembles, the cluster labels are matched to the classes through an Hungarian method which favors the best possible class predictions. In order to run OAC3, the supervised models (M1 and M2) are fused to obtain class probability estimates for every instance in the target set. Also, the similarity matrix used by OAC3 is calculated by fusing the unsupervised models (M3 and M4). The parameters of OAC3 have been chosen from the sensitivity analysis performed in Section 6.2. However, for the experiments reported in this section we do not set particular values for each of the (eleven) studied datasets. Instead, we have chosen a set of parameter values that result in good accuracies across related datasets. In particular the following pairs of (\u03b1, \u03bb) are respectively used for the datasets News, Cora, and DBLP: (4\u00d7 10\u22122,10\u22122); (10\u22124,10\u22122); (10\u22127, 10\u22123). Such choices will hopefully show that one can get good results by using OAC3 without being (necessarily) picky about its parameter values \u2014 thus these results are also complementary to the ones provided in Section 6.2. The classification accuracies achieved by the studied methods are summarized in Table II, where one can see that OAC3 shows the best accuracies for all datasets. In order to provide some reassurance about the validity and non-randomness of the obtained results, the outcomes of statistical tests, following the study in [Demsar 2006],\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nare also reported. In brief, multiple algorithms are compared on multiple datasets by using the Friedman test, with a corresponding Nemenyi post-hoc test. The Friedman test is a non-parametric statistic test equivalent to the repeated-measures ANOVA. If the null hypothesis, which states that the algorithms under study have similar performances, is rejected, then the Nemenyi post-hoc test is used for pairwise comparisons between algorithms. The adopted statistical procedure indicates that the null hypothesis of equal accuracies \u2014 considering the results obtained by the ensembles \u2014 can be rejected at 10% significance level. In pairwise comparisons, significant statistical differences are only observed between OAC3 and the other ensembles, i.e., there is no evidence that the accuracies of MCLA, HBGF, and BGCM are statistically different from one another."}, {"heading": "6.4. Comparison with S3VM", "text": "We also compare OAC3 to a popular semi-supervised algorithm known as S3VM [Sindhwani and Keerthi 2006]. This algorithm is essentially a Transductive Linear Support Vector Machine (SVM) which can be viewed as a large scale implementation of the algorithm introduced in [Joachims 1999b]. For dealing with unlabeled data, it appends an additional term in the SVM objective function whose role is to drive the classification hyperplane towards low data density regions [Sindhwani and Keerthi 2006]. The default parameter values have been used for S3VM. Six datasets are used in our experiments:Half-Moon (see Section 6.1),Circles (which is a synthetic dataset that has two-dimensional instances that form two concentric circles \u2014 one for each class), and four datasets from the Library for Support Vector Machines5 \u2014 Pima Indians Diabetes, Heart, German Numer, and Wine. In order to simulate real-world classification problems where there is a very limited amount of labeled instances, small percentages (e.g., 2%) of the instances are randomly selected for training, whereas the remaining instances are used for testing (target set). The amount of instances for training is chosen so that the pooled covariance matrix of the training set is positive definite. This restriction comes from the use of an LDA classifier in the ensemble, and it imposes a lower bound on the number of training instances (7% for Heart and 10% for German Numer). We perform 10 trials for every proportion of instances in the training/target sets. The number of features are 2, 2, 8, 13, 24, 24 for Half-moon, Circles, Pima, Heart, German Numer and Wine respectively. Considering OAC3, the components of the classifier ensemble are chosen as previously described in Section 6.1. Cluster ensembles are generated by means of multiple runs of k-means (10 data partitions for the two-dimensional datasets and 50 data partitions for Pima, Heart, German Numer, and Wine). The parameters of OAC3 (\u03b1 and \u03bb) are optimized for better performance in each dataset using 5-fold cross-validation. The optimal values of (\u03b1, \u03bb) for Half-moon, Cir-\n5http://www.csie.ntu.edu.tw/~cjlin/libsvm/\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\ncles, Pima, Heart, German Numer, and Wine are (0.05,0.1), (0.01,0.1), (0.002,0.1), (0.01,0.2), (0.01,0.1) and (0.01,0.1) respectively. Table III shows that the accuracies obtained by OAC3 are good and consistently better than those achieved by both the classifier ensemble and its best individual component. In addition, OAC3 shows better accuracies than both S3VM and BGCM \u2014 from the adopted statistical procedure [Demsar 2006], OAC3 exhibits significantly better accuracies at a significance level of 10%."}, {"heading": "6.5. Transfer Learning", "text": "Transfer learning emphasizes the transfer of knowledge across domains, tasks, and distributions that are similar but not the same [Silver and Bennett 2008]. We focus on learning scenarios where training and test distributions are different, as they represent (potentially) related but not identical tasks. It is assumed that the training and test domains involve the same class labels. The real-world datasets employed in our experiments are: a) Text Documents \u2014 [Pan and Yang 2010]: From the well-known text collections 20 newsgroup and Reuters-21758, nine cross-domain learning tasks are generated. The two-level hierarchy in both of these datasets is exploited to frame a learning task involving a top category classification problem with training and test data drawn from different sub categories \u2014 e.g., to distinguish documents from two top newsgroup categories (rec and talk), the training set is built from \u201crec.autos\u201d, \u201crec.motorcycles\u201d, \u201ctalk.politics\u201d, and \u201ctalk.politics.misc\u201d, and the test set is formed from the sub-categories \u201crec.sport.baseball\u201d, \u201crec.sport.hockey\u201d, \u201ctalk.politics.mideast\u201d, and \u201ctalk.religions.misc\u201d. The Email spam data set, released by ECML/PKDD 2006 discovery challenge, contains a training set of publicly available messages and three sets of email messages from individual users as test sets. The 4000 labeled examples in the training set and the 2500 test examples for each of the three different users differ in the word distribution. A spam filter learned from public sources are used to test transfer capability on each of the users.\nb) Botswana \u2014 [Rajan et al. 2006]: This is an application of transfer learning to the pixel-level classification of remotely sensed images, which provides a real-life scenario where such learning will be useful \u2014 in contrast to the contrived setting of text classification, which is chosen as it has been used previously in [Dai et al. 2007]. It is\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nrelatively easy to acquire an image, but expensive to label each pixel manually, where images typically have about a million pixels and represent inaccessible terrain. Thus typically only part of an image gets labeled. Moreover, when the satellite again flies over the same area, the new image can be quite different due to change of season, thus a classifier induced on the previous image becomes significantly degraded for the new task. These hyperespectral data sets used are from a 1476\u00d7256 pixel study area located in the Okavango Delta, Botswana. It has nine different land-cover types consisting of seasonal swamps, occasional swamps, and drier woodlands located in the distal portion of the delta. Data from this region for different months (May, June and July) were obtained by the Hyperion sensor of the NASA EO-1 satellite for the calibration/validation portion of the mission in 2001. Data collected for each month was further segregated into two different areas. While the May scene (Fig. 8) is characterized by the onset of the annual flooding cycle and some newly burned areas, the progression of the flood and the corresponding vegetation responses are seen in the June (Fig. 9) and July (Fig. 10) scenes. The acquired raw data was further processed to produce 145 features. From each area of Botswana, different transfer learning tasks are generated: the classifiers are trained on either May, June or {May \u222a June} data and tested on either June or July data. For text data, we use logistic regression (LR), SVM, and Winnow (WIN) [Gao et al. 2008] as baseline classifiers. The CLUTO package (http://www.cs.umn.edu/~karypis/cluto) is used for clustering the target data into two clusters. We also compare OAC3 with two transfer learning algorithms from the literature \u2014 Transductive Support Vector Machines (TSVM) [Joachims 1999a] and the Locally Weighted Ensemble (LWE) [Gao et al. 2008]. We use Bayesian Logistic Regression http://www.bayesianregression.org/ for running the logistic regression classifier, LIBSVM (http://www.csie.ntu.edu.tw/~cjlin/libsvm/) for SVM, SNoW Learning Architecture http://cogcomp.cs.illinois.edu/page/software_view/1 for Winnow, and SVMlight http://svmlight.joachims.org/ for transductive SVM. The posterior class probabilities from SVM are also obtained using the LIBSVM package with linear kernel. For SNoW, \u201c-S 3 -r 5\u201d is used and the remaining parameters of all the packages are set to their default values. The values of (\u03b1, \u03bb), obtained by 10-fold cross-validation in source domain, are set as (0.008, 0.1) and (0.11, 0.1) for the transfer learning tasks corresponding to 20 Newsgroup and Spam datasets, respectively. For Reuters-21578, the best values of the parameters (\u03b1, \u03bb) are found as (0.009, 0.1), (0.0001, 0.1), and (0.08, 0.1) for O vs Pe, O vs Pl, and Pe vs Pl, respectively (see Table\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nV). For the hyperspectral data, we use two baseline classifiers: the well-known Na\u0131\u0308ve Bayes Wrapper (NBW) and the Maximum Likelihood (ML) classifier, which performs well when used with a best bases feature extractor [Kumar et al. 2001]. The target set instances are clustered by k-means, varying k from 50 to 70. PCA is also used for reducing the number of features employed by ML. In particular, for the hyperspectral data, cross-validation in the source domain does not result in very good performance. Therefore, we take 5% labeled examples from each of the nine classes of the target data and tune the values of \u03b1 and \u03bb based on the performance on these examples. The classifiers NBW or ML, however, are not retrained with these examples from the target domain and the accuracies reported in Table V are on the unlabeled examples only from the target domain.\nThe results for text data are reported in Table IV. The different learning tasks corresponding to different pairs of categories are listed as \u201cMode\u201d. OAC3 improves the performance of the classifier ensemble (formed by combining WIN, LR and SVM via output averaging) for all learning tasks, except forO vs Pl, where apparently the training and test distributions are similar. Also, the OAC3 accuracies are better than those achieved by both TSVM and LWE in most of the datasets. Except for WIN, the performances of the base classifiers and clustereres (and hence of OAC3) are quite invariant, thereby resulting in very low standard deviations. The OAC3 accuracies are significantly better than those obtained by both TSVM and LWE (at 10% significance level).\nTable V reports the results for the hyperspectral data. The parameter values (\u03b1, \u03bb) for best performance of OAC3 are also presented alongside. Note that OAC3 provides consistent accuracy improvements for both NBW and ML6. In pairwise comparisons,\n6Standard deviations of the accuracies from NBW and ML are close to 0 and hence not shown.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nthe accuracies provided by OAC3 are significantly better than those obtained by both NBW and ML (at 10% significance level). The column \u201cPCs\u201d indicates the number of principal components used to project the data."}, {"heading": "7. CONCLUDING REMARKS", "text": "We presented a general framework for combining classifiers and clusterers to address semi-supervised and transfer learning problems. The optimization algorithm assumes closed form updates, facilitates parallelization of the same and, therefore, is extremely convenient in handling large scale data \u2013 specially with a linear rate of convergence. The proofs for the convergence are quite novel and generalize across a wide variety of Bregman divergences, facilitating one to use proper divergence measure based on the application domain and subsuming many other existing graph based semi-supervised learning algorithms as special cases. The proposed framework has been empirically shown to outperform a variety of algorithms [Gao et al. 2011; Sindhwani and Keerthi 2006; Gao et al. 2008] in both semi-supervised and transfer learning problems. There are few aspects that can be further explored. For example, the impact of the number of classifiers and clusterers in OAC3 deserves further investigation. In addition, a more extensive study across a wide variety of problem domains will reveal the capabilities as well as potential limitations of the framework.\nAPPENDIX"}, {"heading": "A. PROOFS FOR CONVERGENCE OF OAC3", "text": "LEMMA A.1. The objective function J used in Eq. (4) is separately and jointly strictly convex over Sn \u00d7 Sn. Also, J is jointly lower-semi-continuous w.r.t y(l) and y(r).\nPROOF.\n(a) From the property (a) in Section 4, one can see that J is strictly convex w.r.t y(l) and y(r) separately. From the same property the first term f1(y(r)) = n \u2211\ni=1\nd\u03c6(\u03c0i,y (r) i ) in J is strictly convex w.r.t. y (r). The 2nd and 3rd terms in the ob-\njective function can collectively be represented by f2(y(l),y(r)). This function is jointly convex by property (b) but is not necessarily jointly strictly convex. Suppose (y1,(l),y1,(r)), (y2,(l),y2,(r)) \u2208 Sn \u00d7 Sn and 0 < w < 1. Then, we have:\nf1(wy 1,(r) + (1\u2212 w)y2,(r)) < wf1(y 1,(r)) + (1\u2212 w)f1(y 2,(r))\nf2(w(y 1,(l),y1,(r)) + (1\u2212 w)(y2,(l),y2,(r))) \u2264 wf2(y 1,(l),y1,(r)) + (1\u2212 w)f2(y 2,(l),y2,(r)).\nNow, it follows that:\nJ(w(y1,(l),y1,(r)) + (1\u2212 w)(y2,(l),y2,(r))) (21)\n= f1(wy 1,(r) + (1 \u2212 w)y2,(r)) + f2(w(y 1,(l),y1,(r)) + (1 \u2212 w)(y2,(l),y2,(r)))\n< wf1(y 1,(r)) + (1\u2212 w)f1(y 2,(r)) + wf2(y 1,(l),y1,(r)) + (1 \u2212 w)f2(y 2,(l),y2,(r))\n= wJ(y1,(l),y1,(r)) + (1 \u2212 w)J(y2,(l),y2,(r)),\nwhich implies that J is jointly strictly convex.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\n(b) To prove that J(y(l),y(r)) is lower-semi-continuous in y(l) and y(r) jointly, we observe that\nlim inf (y(l),y(r))\u2192(y0,(l),y0,(r))\nJ(y0,(l),y0,(r)) (22)\n=\n\n\nn \u2211\ni=1 lim inf y (r) i \u2192y 0,(r) i\nd\u03c6(\u03c0i,y (r) i ) + \u03b1\nn \u2211\ni,j=1\nsij lim inf (y\n(l) i ,y (r) j )\u2192(y 0,(l) i ,y 0,(r) j )\nd\u03c6(y (l) i ,y (r) j )\n+ \u03bb\nn \u2211\ni=1\nlim inf (y\n(l) i ,y (r) i )\u2192(y 0,(l) i ,y 0,(r) i )\nd\u03c6(y (l) i ,y (r) i )\n]\n\u2265\n\n\nn \u2211\ni=1\nd\u03c6(\u03c0i,y 0,(r) i ) + \u03b1\nn \u2211\ni,j=1\nsijd\u03c6(y 0,(l) i ,y 0,(r) j ) + \u03bb\nn \u2211\ni=1\nd\u03c6(y 0,(l) i ,y 0,(r) i )\n\n\n= J(y0,(l),y0,(r)).\nThe inequality in the 3rd step follows from the lower semi continuity of d\u03c6(., .) in Section 4 (Property (d)).\nThe following theorem helps prove that the objective function J can be seen as part of a Bregman divergence.\nTHEOREM A.2 ([BANERJEE ET AL. 2005]). A divergence d : S \u00d7 ri(S) \u2192 [0,\u221e) is a Bregman divergence if and only if \u2203a \u2208 ri(S) such that the function \u03c6a(p) = d(p, a) satisfies the following conditions:\n(a) \u03c6a is strictly convex on S. (b) \u03c6a is differentiable on ri(S). (c) d(p,q) = d\u03c6a(p,q), \u2200p \u2208 S,q \u2208 ri(S) where d\u03c6a is the Bregman divergence associ-\nated with \u03c6a.\nWe now introduce a function J\u0303 : Sn \u00d7 Sn \u2192 [0,\u221e) that is defined as follows:\nJ\u0303(y(r)\u2032,y(r)) =\n\n\nn \u2211\ni=1\nd\u03c6(y (r)\u2032 i ,y (r) i ) + \u03b1\nn \u2211\ni,j=1\nsijd\u03c6(y (r)\u2032 i ,y (r) j ) + \u03bb\nn \u2211\ni=1\nd\u03c6(y (r)\u2032 i ,y (r) i )\n\n . (23)\nNote that J\u0303 is different from J defined in Eq. (4). The left arguments in the divergences of the first term of J are \u03c0i\u2019s which are assumed to be fixed.\nLEMMA A.3. J\u0303 satisfies properties (a) and (b) in Section 4.\nPROOF. The proof is direct from the definition of J\u0303 .\nFurther assume:\np = ( (\u03c0i) n i=1, (y (l) i ) n i=1, ((y (l) i ) n\u22121 j=1 ) n i=1 ) ,\nq = ( (y (r) i ) n i=1, (y (r) i ) n i=1, ((y (r) j ) n\u22121 j=1,j 6=i) n i=1 ) ,\nq\u2032 = ( (y (r)\u2032 i ) n i=1, (y (r)\u2032 i ) n i=1, ((y (r)\u2032 j ) n\u22121 j=1,j 6=i) n i=1 ) ,\nwith y (l) i ,y (r) i ,y (r)\u2032 i \u2208 S \u2200i. The vectors p, q and q \u2032 are each of dimension kn(n+ 1) and formed by concatenating vectors from the set S. (y)ni=1 implies that a new vector is\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\ncreated by repeating y for n times. For ease of understanding, we also define A = {p : y(l) \u2208 Sn}, B = {q : y(r) \u2208 Sn}. We will assume that whenever a point y(l) \u2208 Sn is mapped to a point p \u2208 A, p = A(y(l)). Similarly, q = B(y(r)) whenever y(r) \u2208 Sn is mapped to q \u2208 B. Indeed, both A and B are bijective mappings.\nExample A.4. To explain the mappings A and B more clearly, we consider the fol-\nlowing example. Let n = 3 and y(l),y(r),y(r)\u2032 \u2208 S3. Here, y(l) = ( y (l) 1 ,y (l) 2 ,y (l) 3 ) \u2013 a concatenation of three vectors y (l) 1 , y (l) 2 and y (l) 3 (corrpesponding to three instances) each of which belongs to S \u2286 Rk. Similarly, y(r) = (\ny (r) 1 ,y (r) 2 ,y (r) 3 ) and y(r)\u2032 = ( y (r)\u2032 1 ,y (r)\u2032 2 ,y (r)\u2032 3 ) .\nThe vector p, formed by the transformation A on y(l), takes the following form:\np = ( \u03c01,\u03c02,\u03c03,y (l) 1 ,y (l) 2 ,y (l) 3 ,y (l) 1 ,y (l) 1 ,y (l) 2 ,y (l) 2 ,y (l) 3 ,y (l) 3 )\nNote that this vector has 12 elements each of dimension k and hence the dimension of the whole vector is of the form kn(n+ 1). Similarly,\nq = B(y(r)) = ( y (r) 1 ,y (r) 2 ,y (r) 3 ,y (r) 1 ,y (r) 2 ,y (r) 3 ,y (r) 2 ,y (r) 3 ,y (r) 1 ,y (r) 3 ,y (r) 1 ,y (r) 2 ) ,\nand,\nq\u2032 = B(y(r)\u2032) = ( y (r)\u2032 1 ,y (r)\u2032 2 ,y (r)\u2032 3 ,y (r)\u2032 1 ,y (r)\u2032 2 ,y (r)\u2032 3 ,y (r)\u2032 2 ,y (r)\u2032 3 ,y (r)\u2032 1 ,y (r)\u2032 3 ,y (r)\u2032 1 ,y (r)\u2032 2 ) .\nNow, in light of Theorem A.2, the following corollary is introduced.\nCOROLLARY A.5. If a mapping d : (A \u222a B)\u00d7 B \u2192 [0,\u221e) is defined as:\nd(r,q) =\n{\nd(p,q) = J(y(l),y(r)) if r = p \u2208 A d(q\u2032,q) = J\u0303(y(r)\u2032,y(r)) if r = q\u2032 \u2208 B\n(24)\nthen d is a Bregman divergence.\nPROOF. We show that conditions (a), (b) and (c) of Theorem A.2 are satisfied for d.\n(a) Since d\u03c6 is a Bregman divergence, \u2203a \u2208 ri(S) such that conditions (a), (b) and (c) are satisfied in corollary A.2 pertaining to this divergence. Note that p \u2208 A and q\u2032,q \u2208 B. Assume a\u2032 = B((a)ni=1) \u2208 B \u2282 (A \u222a B). We now define\n\u03c8a\u2032(r) =\n{\n\u03c8a\u2032(p) = d(p, a \u2032) = J(y(l),y(r)) if r = p \u2208 A \u03c8a\u2032(q \u2032) = d(q\u2032, a\u2032) = J\u0303(y(r)\u2032,y(r)) if r = q\u2032 \u2208 B\n(25)\nSince each of d\u03c6(., a) is strictly convex over S n in Eq. (4) and Eq. (23), \u03c8a\u2032 is also strictly convex on A \u222a B. Note the emphasis on B \u2282 (A \u222a B) in the definition of a\u2032\nwhich just ensures that all conditions in Theorem A.2 are satisfied. (b) Again, this is a direct consequence from Eq. (4) and Eq. (23). Since, by the strict\nconvexity of \u03c6(.), each of d\u03c6(., a) is differentiable over ri(Sn), \u03c8a\u2032 is also differentiable over ri(A\u222aB). Note that we have a bijective mapping of elements from Sn to A \u222a B, and hence ri(Sn) gets mapped to ri(A \u222a B).\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\n(c) We have \u2200 (p,q) \u2208 A\u00d7 B,\nd\u03c8 a \u2032 (p,q) =\n[\n\u03c8a\u2032(p)\u2212 \u03c8a\u2032(q) \u2212 \u3008\u2207\u03c8 a \u2032 (q), (p \u2212 q)\u3009\n]\n= n \u2211\ni=1\n[\nd\u03c6(\u03c0i, a)\u2212 d\u03c6(y (r) i , a)\n] + \u03b1 n \u2211\ni,j=1\nsij\n[\nd\u03c6(y (l) i , a)\u2212 d\u03c6(y (r) j , a)\n]\n+ \u03bb\nn \u2211\ni=1\n[\nd\u03c6(y (l) i , a)\u2212 d\u03c6(y (r) i , a)\n]\n\u2212 \u3008\u2207\u03c8 a \u2032 (q), (p \u2212 q)\u3009\n=\nn \u2211\ni=1\n\nd\u03c6(\u03c0i,y (r) i ) + \u03b1\nn \u2211\ni,j=1\nsijd\u03c6(y (l) i ,y (r) j ) + \u03bb\nn \u2211\ni=1\nd\u03c6(y (l) i ,y (r) i )\n\n\n= d(p,q).\nThe second step follows from the definition of \u03c8(.) in Eq. (25) and the last step follows from the definition of d(p,q) in Eq. (24). The equality d\u03c8 a \u2032 (q\u2032,q) = d(q\u2032,q) \u2200(q\u2032,q) \u2208 B \u00d7 B can similarly be proved. Therefore, combining the two results, we have d\u03c8\na \u2032 (r,q) = d(r,q) \u2200(r,q) \u2208 (A \u222a B) \u00d7 B. With a slight abuse of notation,\nhenceforth, we will denote the mapping \u03c8a\u2032 by \u03c8 with an implicit assumption of the existence of an a\u2032 \u2208 B as described before.\nWe will see next that we require some definition of \u03c8a\u2032(q) for q \u2208 B and this explains the definition of d(r,q) in Eq. (24) for the case when r = q\u2032 \u2208 B.\nLEMMA A.6. d\u03c8 satisfies properties (a) and (b) in Section 4.\nPROOF.\n(a) One can see that d\u03c8 is strictly convex separately w.r.t its arguments from its def-\ninition in Eq. (24). Since each of J and J\u0303 is strictly convex separately w.r.t the arguments and A and B are bijective mappings, d\u03c8 is strictly convex separately w.r.t. r and q. (b) The joint convexity of d\u03c8 also follows directly from its definition and the joint con-\nvexity of J and J\u0303 .\nAt this point, we reiterate that defining d\u03c8 as in Eq. (24) helps in proving some interesting properties of J in a very elegant way. We, in fact, treat d\u03c8 as a surrogate for J , establish two specific properties of d\u03c8 and then show that these properties, by the definition of d\u03c8, translates to the same properties of J . The first of them is the 3-Points Property (3-pp) which is introduced in the following definition.\nDefinition A.7 (3-pp). Let P and Q be closed convex sets of finite measures. A function d : P \u00d7 Q \u2192 R \u222a {\u2212\u221e,+\u221e} is said to satisfy the 3-points property (3-pp) if for a given q \u2208 Q for which d(p, q) < \u221e \u2200p \u2208 P , \u03b4(p, p\u2217) \u2264 d(p, q) \u2212 d(p\u2217, q) where p\u2217 = argmin\np\u2208P\nd(p, q) and \u03b4 : P \u00d7 P \u2192 R+ with \u03b4(p, p\u2032) = 0 iff p = p\u2032.\nLEMMA A.8. J satisfies 3-pp.\nPROOF. The proof is based on the works of [Wang and Schuurmans 2003a]. First, we will show that 3-pp is valid for d\u03c8(., .) over A \u00d7 B. As mentioned earlier, this is where the introduction of d\u03c8 becomes useful and elegant. Assume that p = A(y(l)) \u2208 A corresponding to some y(l) \u2208 Sn, q = B(y(r)) \u2208 B corresponding to some y(r) \u2208 Sn and\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\np\u2217 = argmin p\u2208A d\u03c8(p,q) = argmin y(l)\u2208Sn\nJ(y(l),y(r)) = A ( y(l) \u2217 ) (the fact that the minimizers are\njust transformations of each other under A or A\u22121 follows directly from the separately strict convexity of J and d\u03c8). Therefore,\nd\u03c8(p,q) \u2212 d\u03c8(p \u2217,q)\n= \u03c8(p)\u2212 \u03c8(p\u2217)\u2212 \u3008\u2207\u03c8(q),p \u2212 p \u2217\u3009 = \u03b4\u03c8(p,p \u2217) + \u3008\u2207\u03c8(p \u2217)\u2212\u2207\u03c8(q),p \u2212 p \u2217\u3009\nwhere, \u03b4\u03c8 : A\u00d7A \u2192 R is defined as follows:\n\u03b4\u03c8(p,p \u2217) = \u03c8(p)\u2212 \u03c8(p\u2217)\u2212 \u3008\u2207\u03c8(p \u2217),p\u2212 p\u2217\u3009. (26)\nSince p\u2217 = argmin p\u2208A d\u03c8(p,q), \u3008\u2207pd\u03c8(p\u2217,q), (p \u2212 p\u2217)\u3009 \u2265 0, then \u3008\u2207\u03c8(p\u2217) \u2212 \u2207\u03c8(q), (p \u2212 p\u2217)\u3009 \u2265 0 which implies d\u03c8(p,q)\u2212d\u03c8(p \u2217,q) \u2265 \u03b4(p,p\u2217). Now, by some simple algebra, we\ncan show \u03b4\u03c8(p,p\u2217) = n \u2211\ni=1\n\n\u03bb+ \u03b1\nn \u2211\nj=1;j 6=i\n\n d\u03c6(y (l) i ,y (l) i \u2217 ). By assumption, d\u03c6 ( y (l) i ,y (l) i \u2217) \u2265\n0 and hence \u03b4\u03c8(p,p\u2217) \u2265 0 with 0 achieved iff y(l) = y(l) \u2217 . If we define \u03b4\u03c8(p,p\u2217) = \u03b4J(A \u22121(p),A\u22121(p\u2217)) then \u03b4J(y(l),y(l) \u2217 ) \u2265 0 with 0 achieved iff y(l) = y(l) \u2217 . Note that\n\u03b4J(y 1,(l),y2,(l) =\nn \u2211\ni=1\n\n\u03bb+ \u03b1\nn \u2211\nj=1;j 6=i\n\n d\u03c6(y 1,(l) i ,y 2,(l) i ). (27)\nTherefore, following 3-pp of d\u03c8 over A\u00d7 B, we can conclude that\nJ(y(l),y(r))\u2212 J(y(l) \u2217 ,y(r)) \u2265 \u03b4J(y (l),y(l) \u2217 ), (28)\nwhich is the 3-pp for J .\nLEMMA A.9. \u03b4J satisfies properties (c) and (f) mentioned in Section 4.\nPROOF.\n(a) Since level sets of each of the terms in Eq. (27) are bounded following the property (c) in Section 4, we conclude that the level set {y(r) : \u03b4J(y(l),y(r)) \u2264 \u2113} for a given y(l) \u2208 Sn is also bounded. (b) We refer to Eq. (27). As, y2,(l) \u2192 y1,(l), each of the d\u03c6(., .)\u2019s goes to 0 by the property (f) in Section 4. Therefore, \u03b4J \u2192 0 as y2,(l) \u2192 y1,(l).\nNext, 4-Points Property (4-pp) is introduced.\nDefinition A.10 (4-pp). Let P and Q be closed convex sets of finite measures. A function d : P \u00d7 Q \u2192 R \u222a {\u2212\u221e,+\u221e} is said to satisfy 4-pp if for a given p \u2208 P , d(p, q\u2217) \u2264 \u03b4(p, p\u2217)+d(p, q)where q\u2217 = argmin\nq\u2208Q\nd(p\u2217, q) and \u03b4 : P\u00d7P \u2192 R+ with \u03b4(p, p\u2032) = 0\niff p = p\u2032.\nLEMMA A.11. J satisfies 4-pp.\nPROOF. Assume u = A(y1,(l)) \u2208 A, p = A(y2,(l)) \u2208 A, q = B(y3,(r)) \u2208 B,\nand q\u2217 = argmin q\u2208B\nd\u03c8(p,q) = B(y 4,(r)\u2217). Here, y1,(l),y2,(l),y3,(r) \u2208 Sn and y4,(r) \u2217 =\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nargmin y(r)\u2208Sn\nJ(y2,(l),y(r)). From the joint convexity of d\u03c8 (established in Lemma A.6) w.r.t\nboth of its arguments we have:\nd\u03c8(u,v) \u2265 d\u03c8(p,q \u2217) + \u3008\u2207pd\u03c8(p,q \u2217),u\u2212 p\u3009+ \u3008\u2207qd\u03c8(p,q \u2217),v \u2212 q\u2217\u3009. (29)\nSince q\u2217 minimizes d\u03c8(p,q) over q \u2208 B, we have \u3008\u2207qd\u03c8(p,q\u2217),v \u2212 q\u2217\u3009 \u2265 0 which, in turn, implies:\nd\u03c8(u,p)\u2212 d\u03c8(p,q \u2217)\u2212 \u3008\u2207pd\u03c8(p,q \u2217),u\u2212 p\u3009 \u2265 0.\nNow we have:\n\u03b4\u03c8(u,p)\u2212 d\u03c8(u,q \u2217)\n= \u03c8(q\u2217)\u2212 \u03c8(p)\u2212 \u3008\u2207\u03c8(q \u2217),u\u2212 q\u2217\u3009 \u2212 \u3008\u2207\u03c8(p),u\u2212 p\u3009 = \u2212d\u03c8(p,q \u2217)\u2212 \u3008\u2207\u03c8(p)\u2212\u2207\u03c8(q \u2217),u\u2212 p\u3009 = \u2212d\u03c8(p,q \u2217)\u2212 \u3008\u2207pd\u03c8(p,q \u2217),u\u2212 p\u3009\nCombining the above two equations, we have,\n\u03b4\u03c8(u,p) + d\u03c8(u,v) \u2265 d\u03c8(u,q \u2217) (30)\nEq. (30) gets translated for J as follows (using definitions of \u03b4\u03c8 and d\u03c8):\n\u03b4J (y 1,(l),y2,(l)) + J(y1,(l),y3,(r)) \u2265 J(y1,(l),y4,(r) \u2217 ) (31)\nHence, J satisfies 4-pp.\nWe now introduce the main theorem that establishes the convergence guarantee of OAC3.\nTHEOREM A.12. If y(l,t) = argmin y(l)\u2208Sn J(y(l),y(r,t\u22121)), y(r,t) = argmin y(r)\u2208Sn J(y(l,t),y(r)), then\nlim t\u2192\u221e J(y(l,t),y(r,t)) = inf y(l),y(r)\u2208Sn J(y(l),y(r)).\nPROOF. The proof here follows the same line of argument as given in [Wang and Schuurmans 2003a] and [Eggermont and LaRiccia 1998]. Since, y(r,t+1) = argmin y(r)\u2208Sn J(y(l,t),y(r)), we have, J(y(l,t),y(r,t)) \u2212 J(y(l,t),y(r,t+1)) \u2265 0. By the 3-pp, J(y(l,t),y(r,t+1))\u2212 J(y(l,t+1),y(r,t+1)) \u2265 \u03b4J(y(l,t),y(l,t+1)). Then,\nJ(y(l,t),y(r,t))\u2212 J(y(l,t+1),y(r,t+1))\n= J(y(l,t),y(r,t))\u2212 J(y(l,t),y(r,t+1)) + J(y(l,t),y(r,t+1))\u2212 J(y(l,t+1),y(r,t+1))\n\u2265 \u03b4J(y (l,t),y(l,t+1)) \u2265 0.\nThis implies that the sequence J(y(l,t),y(r,t)) is non-increasing and non-negative. Let, (y(l,\u221e),y(r,\u221e)) = argmin\ny(l),y(r)\u2208Sn J(y(l),y(r)). From 4-pp and 3-pp, we can derive the follow-\ning two inequalities:\nJ(y(l,\u221e),y(r,t+1)) \u2264 \u03b4J(y (l,\u221e),y(l,t)) + J(y(l,\u221e),y(r,\u221e)) (32)\n\u03b4J(y (l,\u221e),y(l,t+1)) \u2264 J(y(l,\u221e),y(r,t+1))\u2212 J(y(l,t+1),y(r,t+1)). (33)\nCombining the above two inequalities, we get:\n\u03b4J (y (l,\u221e),y(l,t))\u2212 \u03b4J (y (l,\u221e),y(l,t+1)) \u2265 J(y(l,t+1),y(r,t+1))\u2212 J(y(l,\u221e),y(r,\u221e)) \u2265 0, (34)\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nwhich is the 5-points property (5-pp) of J . From (34), the sequence \u03b4J(y(l,\u221e),y(l,t)) is non-increasing and non-negative. Therefore, it must have a limit (from the Monotone Convergence Theorem) and consequently the left hand side of (34) approaches 0 as t \u2192 \u221e. Hence, lim\nt\u2192\u221e J(y(l,t),y(r,t)) = J(y(l,\u221e),y(r,\u221e)) (by the Pinching Theorem).\nFinally, we must show that y(l,t) and y(r,t) themselves converge. From the boundedness of \u03b4J(y(l,\u221e),y(l,t)) (established in Lemma A.9), it follows that y(l,t) is bounded. Therefore, it has a convergent subsequence {y(l,ti)} \u2013 the limit of which can be denoted by y0,(l) (by the Bolzano-Weierstrass Theorem). Similarly, it can be shown that the subsequence {y(r,ti)} also converges to some limit. Let that limit be denoted by y0,(r). By the lower-semi-continuity of J (established in Lemma A.1), we have:\nJ(y0,(l),y0,(r)) \u2264 lim inf i J(y(l,ti),y(r,ti)) = J(y(l,\u221e),y(r,\u221e)). (35)\nWe denote Y\u221el = {y (l) : arg min y(l),y(r)\u2208Sn J(y(l),y(r))} and Y\u221er = {y (r) : arg min y(l),y(r)\u2208Sn J(y(l),y(r))}. Therefore, from the joint strict convexity of J , we should have Y\u221el = {y 0,(l)} = {y(l,\u221e)} and Y\u221er = {y 0,(r)} = {y(r,\u221e)}.\nTo prove the convergence of the entire sequence, we apply the same logic as above with y(l,\u221e) replaced by y0,(l). Then the sequence {\u03b4J(y0,(l),y(l,t))} is bounded and nonincreasing and by using Lemma A.9, we conclude that it has a convergent subsequence {\u03b4J(y\n0,(l),y(l,ti))} that goes to 0 as y(l,ti) \u2192 y0,(l). This, from Monotone Convergence Theorem, implies that {\u03b4J(y0,(l),y(l,t))} \u2192 0 and again using Lemma A.9, we can conclude that y(l,t) \u2192 y0,(l). Since y(r,t) is also bounded, it should have a convergent subsequence (by the Bolzano-Weierstrass Theorem). We denote this limit by y(0),(r). Again, by the lower-semi-continuity of J , we have:\nJ(y0,(l),y(0),(r)) \u2264 J(y(l,\u221e),y(r,\u221e)). (36)\nHence, y(0),(r) = arg min y(r)\u2208Sn J(y(l),y(r,\u221e)) and y(r,t) \u2192 y(0),(r) = y0,(r).\nThere is another interesting aspect of J that was discovered in [Subramanya and Bilmes 2011] for a slightly different objective function with KL divergence used as a loss function. The same property also holds for J if the loss function is constructed from the assumed family of Bregman divergences. This property is concerned with the equality of solutions of J and J0 and explores under what conditions these two objectives become equal. To establish the theorem that explores this condition, the following lemmata are essential.\nLEMMA A.13. If y(r) = y(l) = y then J0 = J .\nPROOF. This proof immediately follows from the definitions of J0 and J in Eq. (3) and Eq. (4) respectively.\nLEMMA A.14. arg min (y(l),y(r))\u2208Sn\u00d7Sn J(y(l),y(r);\u03bb = 0) \u2264 arg min y\u2208Sn J0(y).\nPROOF.\nmin y\u2208Sn J0(y) = min (y(l),y(r))\u2208Sn\u00d7Sn;y(r)=y(l) J(y(l),y(r);\u03bb = 0) \u2265 min (y(l),y(r))\u2208Sn\u00d7Sn J(y(l),y(r);\u03bb = 0)\nThe last step is due to the fact that the unconstrained minima is never larger than the constrained minima.\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012.\nLEMMA A.15. Given any y(l), y(r), y \u2208 Sn such that y(l), y(r), y > 0 and y(l) 6= y(r) (i.e. not all components are equal) then there exists a finite \u03bb such that J(y(l),y(r)) \u2265 J(y,y) = J0(y).\nPROOF. For J(y(l),y(r)) \u2265 J(y,y), we should have:\n\n\nn \u2211\ni=1\nd\u03c6(\u03c0i,y (r) i ) + \u03b1\nn \u2211\ni,j=1\nsijd\u03c6(y (l) i ,y (r) j ) + \u03bb\nn \u2211\ni=1\nd\u03c6(y (l) i ,y (r) i )\n\n\u2212 J(y,y) \u2265 0\n\u21d2 \u03bb \u2265\nJ(y,y) \u2212 n \u2211\ni=1\nd\u03c6(\u03c0i,y (r) i )\u2212 \u03b1\nn \u2211\ni,j=1\nsijd\u03c6(y (l) i ,y (r) j )\nn \u2211\ni=1\nd\u03c6(y (l) i ,y (r) i )\n\u21d2 \u03bb \u2265 J0(y) \u2212 J(y (l),y(r);\u03bb = 0) n \u2211\ni=1\nd\u03c6(y (l) i ,y (r) i )\n\u2265 0.\nwhere the last inequality follows from Lemma A.14.\nThe theorem that formulates the conditions for equality of solutions of J and J0 is given below:\nTHEOREM A.16 (EQUALITY OF SOLUTIONS OF J AND J0). Let y\u2217 = arg min y\u2208Sn J0(y)\nand (y\u03bb\u0303,(l) \u2217 ,y\u03bb\u0303,(r) \u2217 ) = arg min\ny\u2208Sn J(y(l),y(r); \u03bb\u0303) for an arbitrary \u03bb = \u03bb\u0303 > 0. Then there\nexists a finite \u03bb\u0302 such that at convergence of OAC3, we have y\u2217 = y\u03bb\u0302,(l) \u2217 = y\u03bb\u0302,(r) \u2217 . Further, if y\u03bb\u0303,(l) \u2217 6= y\u03bb\u0303,(r) \u2217 , then\n\u03bb\u0302 \u2265 J0(y\n\u2217)\u2212 J(y\u03bb\u0303,(l) \u2217 ,y\u03bb\u0303,(r) \u2217 ;\u03bb = 0)\nn \u2211\ni=1\nd\u03c6(y \u03bb\u0303,(l) i ,y \u03bb\u0303,(r) i )\nand if y\u03bb\u0303,(l) \u2217 = y\u03bb\u0303,(r) \u2217 , then \u03bb\u0302 \u2265 \u03bb\u0303.\nPROOF. If y\u03bb\u0303,(l) \u2217 = y\u03bb\u0303,(r) \u2217 , then from the strict convexity of both J0 and J , J0(y\u2217) =\nJ(y\u03bb\u0303,(l) \u2217 ,y\u03bb\u0303,(r) \u2217 ;\u03bb = 0). Also, since for any y(l) 6= y(r), J(y(l),y(r); \u03bb\u0302) > J(y(l),y(r); \u03bb\u0303), whenever \u03bb\u0302 \u2265 \u03bb\u0303, then \u2200\u03bb\u0302 \u2265 \u03bb\u0303 J0(y\u2217) = J(y\u03bb\u0302,(l) \u2217 ,y\u03bb\u0302,(r) \u2217 ;\u03bb = 0). Also, if y\u03bb\u0303,(l) \u2217 6= y\u03bb\u0303,(r) \u2217 , then from Lemma A.15, if\n\u221e > \u03bb\u0302 \u2265 J0(y\n\u2217)\u2212 J(y\u03bb\u0303,(l) \u2217 ,y\u03bb\u0303,(r) \u2217 ;\u03bb = 0)\nn \u2211\ni=1\nd\u03c6(y \u03bb\u0303,(l) i ,y \u03bb\u0303,(r) i )\nthen it is guaranteed that y\u03bb\u0302,(l) \u2217 = y\u03bb\u0302,(r) \u2217 .\nACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Publication date: April 2012."}, {"heading": "B. PROOF FOR ANALYSIS OF RATE OF CONVERGENCE", "text": "LEMMA B.1. H = \u22072J is positive definite over the domain of J under the assump-\ntion\nn \u2211\ni=1\nk \u2211\n\u2113=1\n\u03c0i\u2113 > 0 when KL or generalized I divergence is used as a Bregman diver-\ngence.\nPROOF. Assume z =\n((\ny (l) i\n\u2020 )n\ni=1\n,\n(\ny (r) i\n\u2020 )n\ni=1\n)\u2020\n. Now,\nz\u2020Hz (37)\n=\nn \u2211\ni=1\ny (l) i\n\u2020 \u2207\ny (l) i ,y (l) i\ny (l) i +\nn \u2211\nj=1\ny (r) j\n\u2020 \u2207\ny (r) j ,y (r) j\ny (r) j + 2\nn \u2211\ni,j=1;i6=j\ny (l) i\n\u2020 \u2207\ny (l) i ,y (r) i\ny (r) j\n+ 2\nn \u2211\ni=1\ny (l) i\n\u2020 \u2207\ny (l) i ,y (r) i\ny (r) i\n=\nn \u2211\ni=1\n(\n\u03b1\nn \u2211\nj=1;j 6=i\nsij + \u03bb )\nk \u2211\n\u2113=1\ny (l) i\u2113 +\nn \u2211\nj=1\nk \u2211\n\u2113=1\n(\n\u03c0j\u2113 + \u03b1\nn \u2211\ni=1;i6=j\nsijy (l) i\u2113 + \u03bby (l) j\u2113 )\n\u2212 2\u03bb n \u2211\ni=1\nk \u2211\n\u2113=1\ny (l) i\u2113\n\u2212 2\u03b1 n \u2211\ni,j=1;i6=j\nsij\nk \u2211\n\u2113=1\ny (l) i\u2113\n=\nn \u2211\ni=1\nk \u2211\n\u2113=1\n\u03c0i\u2113 > 0.\nTherefore, if n \u2211\ni=1\nk \u2211\n\u2113=1\n\u03c0i\u2113 > 0, \u2207 2J is positive definite over the domain of J ."}, {"heading": "ACKNOWLEDGMENTS", "text": "We are grateful to Luiz F. S. Coletta for running the experiments described in Section 6.2. We also thank Ambuj Tewari and Ali Jalali for pointing us to relevant literarure for analyzing the rate of convergence of the optimization framework."}], "references": [{"title": "C3E: A Framework for Combining Ensembles of Classifiers and Clusterers", "author": ["A. ACHARYA", "E.R. HRUSCHKA", "J. GHOSH", "S. ACHARYYA"], "venue": "MCS. 269\u2013278.", "citeRegEx": "ACHARYA et al\\.,? 2011", "shortCiteRegEx": "ACHARYA et al\\.", "year": 2011}, {"title": "Clustering with Bregman Divergences", "author": ["A. BANERJEE", "S. MERUGU", "I.S. DHILLON", "J. GHOSH"], "venue": "JMLR 6, 1705\u20131749.", "citeRegEx": "BANERJEE et al\\.,? 2005", "shortCiteRegEx": "BANERJEE et al\\.", "year": 2005}, {"title": "On Manifold Regularization", "author": ["M. BELKIN", "P. NIYOGI", "V. SINDHWANI"], "venue": "AISTAT.", "citeRegEx": "BELKIN et al\\.,? 2005", "shortCiteRegEx": "BELKIN et al\\.", "year": 2005}, {"title": "Label Propagation and Quadratic Criterion", "author": ["Y. BENGIO", "O. DELALLEAU", "N. LE ROUX"], "venue": "SemiSupervised Learning, O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, Eds. MIT Press, 193\u2013216.", "citeRegEx": "BENGIO et al\\.,? 2006", "shortCiteRegEx": "BENGIO et al\\.", "year": 2006}, {"title": "Some notes on alternating optimization", "author": ["J. BEZDEK", "R. HATHAWAY"], "venue": "Advances in Soft Computing AFSS 2002, N. Pal and M. Sugeno, Eds. Lecture Notes in Computer Science Series, vol. 2275. Springer Berlin / Heidelberg, 187\u2013195.", "citeRegEx": "BEZDEK and HATHAWAY,? 2002", "shortCiteRegEx": "BEZDEK and HATHAWAY", "year": 2002}, {"title": "Convergence of alternating optimization", "author": ["J.C. BEZDEK", "R.J. HATHAWAY"], "venue": "Neural, Parallel Sci. Comput. 11, 4, 351\u2013368.", "citeRegEx": "BEZDEK and HATHAWAY,? 2003", "shortCiteRegEx": "BEZDEK and HATHAWAY", "year": 2003}, {"title": "On-line algorithms in machine learning", "author": ["A. BLUM"], "venue": "Online Algorithms: The State of the Art, Fiat and Woeginger, Eds. LNCS Vol.1442, Springer.", "citeRegEx": "BLUM,? 1998", "shortCiteRegEx": "BLUM", "year": 1998}, {"title": "Knowledge transfer mechanisms for characterizing image datasets", "author": ["K.D. BOLLACKER", "J. GHOSH"], "venue": "Soft Computing and Image Processing. Physica-Verlag, Heidelberg.", "citeRegEx": "BOLLACKER and GHOSH,? 2000", "shortCiteRegEx": "BOLLACKER and GHOSH", "year": 2000}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. BOYD", "N. PARIKH", "E. CHU", "B. PELEATO", "J. ECKSTEIN"], "venue": "Tech Report.", "citeRegEx": "BOYD et al\\.,? 2011", "shortCiteRegEx": "BOYD et al\\.", "year": 2011}, {"title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "author": ["L.M. BREGMAN"], "venue": "USSR Computational Mathematics and Mathematical Physics 7, 3, 200 \u2013 217.", "citeRegEx": "BREGMAN,? 1967", "shortCiteRegEx": "BREGMAN", "year": 1967}, {"title": "A simultaneous learning framework for clustering and classification", "author": ["W. CAI", "S. CHEN", "D. ZHANG"], "venue": "Pattern Recognition 42, 1248\u20131259.", "citeRegEx": "CAI et al\\.,? 2009", "shortCiteRegEx": "CAI et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["R. CARUANA"], "venue": "Mach. Learn. 28, 41\u201375.", "citeRegEx": "CARUANA,? 1997", "shortCiteRegEx": "CARUANA", "year": 1997}, {"title": "Parallel Optimization: Theory, Algorithms and Applications", "author": ["Y.A. CENSOR", "S.A. ZENIOS"], "venue": "Oxford University Press.", "citeRegEx": "CENSOR and ZENIOS,? 1997", "shortCiteRegEx": "CENSOR and ZENIOS", "year": 1997}, {"title": "Semi-Supervised Learning", "author": ["O. CHAPELLE", "B. SCH\u00d6LKOPF", "A. ZIEN"], "venue": "MIT Press.", "citeRegEx": "CHAPELLE et al\\.,? 2006", "shortCiteRegEx": "CHAPELLE et al\\.", "year": 2006}, {"title": "Semi-supervised classification based on clustering ensembles", "author": ["S. CHEN", "G. GUO", "L. CHEN"], "venue": "Proc. of AICI \u201909. Springer-Verlag, 629\u2013638.", "citeRegEx": "CHEN et al\\.,? 2009", "shortCiteRegEx": "CHEN et al\\.", "year": 2009}, {"title": "Proximity maps for convex sets", "author": ["W. CHENEY", "A.A. GOLDSTEIN"], "venue": "Proceedings of the American Mathematical Society 10, 3, pp. 448\u2013450.", "citeRegEx": "CHENEY and GOLDSTEIN,? 1959", "shortCiteRegEx": "CHENEY and GOLDSTEIN", "year": 1959}, {"title": "On information regularization", "author": ["A. CORDUNEANU", "T. JAAKKOLA"], "venue": "UAI. 151\u2013158.", "citeRegEx": "CORDUNEANU and JAAKKOLA,? 2003", "shortCiteRegEx": "CORDUNEANU and JAAKKOLA", "year": 2003}, {"title": "Information geometry and alternatingminimization procedures", "author": ["I. CSISZ\u00c1R", "G. TUSN\u00c1DY"], "venue": "Statistics aand Decisions, Supplement Issue 1, 1, 205\u2013237.", "citeRegEx": "CSISZ\u00c1R and TUSN\u00c1DY,? 1984", "shortCiteRegEx": "CSISZ\u00c1R and TUSN\u00c1DY", "year": 1984}, {"title": "Co-clustering based classification for out-of-domain documents", "author": ["DAI W.", "XUE G.", "YANG Q.", "YU", "Y."], "venue": "Proc. of KDD. New York, NY, USA, 210\u2013219.", "citeRegEx": "W. et al\\.,? 2007", "shortCiteRegEx": "W. et al\\.", "year": 2007}, {"title": "Statistical comparison of classifiers over multiple data sets", "author": ["J. DEMSAR"], "venue": "Journal of Machine Learning Research 7, 7, 1\u201330.", "citeRegEx": "DEMSAR,? 2006", "shortCiteRegEx": "DEMSAR", "year": 2006}, {"title": "On em-like algorithms for minimum distance estimation", "author": ["P. EGGERMONT", "V. LARICCIA"], "venue": "Unpublished manuscript, University of Delaware.", "citeRegEx": "EGGERMONT and LARICCIA,? 1998", "shortCiteRegEx": "EGGERMONT and LARICCIA", "year": 1998}, {"title": "Solving cluster ensemble problems by bipartite graph partitioning", "author": ["X. FERN", "C. BRODLEY"], "venue": "Proc. of ICML. 281\u2013288.", "citeRegEx": "FERN and BRODLEY,? 2004", "shortCiteRegEx": "FERN and BRODLEY", "year": 2004}, {"title": "Collaborative clustering with background knowledge", "author": ["G. FORESTIER", "P. GAN\u00c7ARSKI", "C. WEMMERT"], "venue": "Data Knowl. Eng. 69, 211\u2013228.", "citeRegEx": "FORESTIER et al\\.,? 2010", "shortCiteRegEx": "FORESTIER et al\\.", "year": 2010}, {"title": "Knowledge transfer via multiple model local structure mapping", "author": ["GAO J.", "FAN W.", "JIANG J.", "HAN", "J."], "venue": "Proc. of KDD. 283\u2013291.", "citeRegEx": "J. et al\\.,? 2008", "shortCiteRegEx": "J. et al\\.", "year": 2008}, {"title": "Graph-based consensus maximization among multiple supervised and unsupervised models", "author": ["GAO J.", "LIANG F.", "FAN W.", "SUN Y.", "HAN", "J."], "venue": "Proc. of NIPS. 1\u20139.", "citeRegEx": "J. et al\\.,? 2009", "shortCiteRegEx": "J. et al\\.", "year": 2009}, {"title": "A graph-based consensus maximization approach for combining multiple supervised and unsupervised models", "author": ["GAO J.", "LIANG F.", "FAN W.", "SUN Y.", "HAN", "J."], "venue": "IEEE Transactions on Knowledge and Data Engineering accepted for publication.", "citeRegEx": "J. et al\\.,? 2011", "shortCiteRegEx": "J. et al\\.", "year": 2011}, {"title": "Cluster ensembles", "author": ["J. GHOSH", "A. ACHARYA"], "venue": "WIREs Data Mining and Knowledge Discovery 1, 1\u201312.", "citeRegEx": "GHOSH and ACHARYA,? 2011", "shortCiteRegEx": "GHOSH and ACHARYA", "year": 2011}, {"title": "Convergence theorems for generalized alternating minimization procedures", "author": ["A. GUNAWARDANA", "W. BYRNE"], "venue": "J. Mach. Learn. Res. 6, 2049\u20132073.", "citeRegEx": "GUNAWARDANA and BYRNE,? 2005", "shortCiteRegEx": "GUNAWARDANA and BYRNE", "year": 2005}, {"title": "Making large-scale SVM learning practical", "author": ["T. JOACHIMS"], "venue": "Advances in Kernel Methods: Support Vector Learning, C. B. B. Scholkopf and A. Smola, Eds. MIT Press, 169\u2013184.", "citeRegEx": "JOACHIMS,? 1999a", "shortCiteRegEx": "JOACHIMS", "year": 1999}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. JOACHIMS"], "venue": "Proc. of ICML. 200\u2013209.", "citeRegEx": "JOACHIMS,? 1999b", "shortCiteRegEx": "JOACHIMS", "year": 1999}, {"title": "Transductive learning via spectral graph partitioning", "author": ["T. JOACHIMS"], "venue": "Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003).", "citeRegEx": "JOACHIMS,? 2003", "shortCiteRegEx": "JOACHIMS", "year": 2003}, {"title": "Best-bases feature extraction algorithms for classification of hyperspectral data", "author": ["S. KUMAR", "J. GHOSH", "M.M. CRAWFORD"], "venue": "IEEE TGRS 39, 7, 1368\u201379.", "citeRegEx": "KUMAR et al\\.,? 2001", "shortCiteRegEx": "KUMAR et al\\.", "year": 2001}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. KUNCHEVA"], "venue": "Wiley, Hoboken, NJ.", "citeRegEx": "KUNCHEVA,? 2004", "shortCiteRegEx": "KUNCHEVA", "year": 2004}, {"title": "Classifier ensembles: Select real-world applications", "author": ["N.C. OZA", "K. TUMER"], "venue": "Inf. Fusion 9, 4\u201320.", "citeRegEx": "OZA and TUMER,? 2008", "shortCiteRegEx": "OZA and TUMER", "year": 2008}, {"title": "A survey on transfer learning", "author": ["S.J. PAN", "Q. YANG"], "venue": "IEEE TKDE 22, 1345\u20131359.", "citeRegEx": "PAN and YANG,? 2010", "shortCiteRegEx": "PAN and YANG", "year": 2010}, {"title": "Bootstrap-inspired techniques in computational intelligence", "author": ["R. POLIKAR"], "venue": "IEEE SIGNAL PROCESSING MAGAZINE.", "citeRegEx": "POLIKAR,? 2007", "shortCiteRegEx": "POLIKAR", "year": 2007}, {"title": "Consensus based ensembles of soft clusterings", "author": ["K. PUNERA", "J. GHOSH"], "venue": "Applied Artificial Intelligence. Vol. 22. 109\u2013117.", "citeRegEx": "PUNERA and GHOSH,? 2008", "shortCiteRegEx": "PUNERA and GHOSH", "year": 2008}, {"title": "Exploiting class hierarchies for knowledge transfer in hyperspectral data", "author": ["S. RAJAN", "J. GHOSH", "M.M. CRAWFORD"], "venue": "IEEE TGRS 44, 11, 3408\u20133417.", "citeRegEx": "RAJAN et al\\.,? 2006", "shortCiteRegEx": "RAJAN et al\\.", "year": 2006}, {"title": "Guest editor\u2019s introduction: special issue on inductive transfer learning", "author": ["D.L. SILVER", "K.P. BENNETT"], "venue": "Mach. Learn. 73, 215\u2013220.", "citeRegEx": "SILVER and BENNETT,? 2008", "shortCiteRegEx": "SILVER and BENNETT", "year": 2008}, {"title": "Large scale semi-supervised linear SVMs", "author": ["V. SINDHWANI", "S.S. KEERTHI"], "venue": "Proc. of the 29th Annual International ACM SIGIR Conf. on Research and Development in Information Retrieval. NY, USA, 477\u2013484.", "citeRegEx": "SINDHWANI and KEERTHI,? 2006", "shortCiteRegEx": "SINDHWANI and KEERTHI", "year": 2006}, {"title": "Cluster ensembles \u2013 a knowledge reuse framework for combining multiple partitions", "author": ["A. STREHL", "J. GHOSH"], "venue": "JMLR 3, 583\u2013617.", "citeRegEx": "STREHL and GHOSH,? 2002", "shortCiteRegEx": "STREHL and GHOSH", "year": 2002}, {"title": "Semi-supervised learning with measure propagation", "author": ["A. SUBRAMANYA", "J. BILMES"], "venue": "Journal of Machine Learning. Research 12, 3311\u20133370.", "citeRegEx": "SUBRAMANYA and BILMES,? 2011", "shortCiteRegEx": "SUBRAMANYA and BILMES", "year": 2011}, {"title": "Entropic graph regularization in non-parametric semisupervised classification", "author": ["A. SUBRAMANYA", "J.A. BILMES"], "venue": "Proc. of NIPS. Vancouver, Canada.", "citeRegEx": "SUBRAMANYA and BILMES,? 2009", "shortCiteRegEx": "SUBRAMANYA and BILMES", "year": 2009}, {"title": "Learning To Learn", "author": ["S. THRUN", "L. PRATT"], "venue": "Kluwer Academic, Norwell, MA.", "citeRegEx": "THRUN and PRATT,? 1997", "shortCiteRegEx": "THRUN and PRATT", "year": 1997}, {"title": "Propagating distributions on a hypergraph by dual information regularization", "author": ["K. TSUDA"], "venue": "Proceedings of the 22nd international conference on Machine learning. ICML \u201905. ACM, New York, NY, USA, 920\u2013927.", "citeRegEx": "TSUDA,? 2005", "shortCiteRegEx": "TSUDA", "year": 2005}, {"title": "Analysis of decision boundaries in linearly combined neural classifiers", "author": ["K. TUMER", "J. GHOSH"], "venue": "Pattern Recognition 29, 2, 341\u2013348.", "citeRegEx": "TUMER and GHOSH,? 1996", "shortCiteRegEx": "TUMER and GHOSH", "year": 1996}, {"title": "Bayesian cluster ensembles", "author": ["H. WANG", "H. SHAN", "A. BANERJEE"], "venue": "Statistical Analysis and Data Mining 1, 1\u201317.", "citeRegEx": "WANG et al\\.,? 2011", "shortCiteRegEx": "WANG et al\\.", "year": 2011}, {"title": "Learning continuous latent variable models with bregman divergences", "author": ["S. WANG", "D. SCHUURMANS"], "venue": "2842, 190\u2013204.", "citeRegEx": "WANG and SCHUURMANS,? 2003a", "shortCiteRegEx": "WANG and SCHUURMANS", "year": 2003}, {"title": "Learning latent variable models with Bregman divergences", "author": ["S. WANG", "D. SCHUURMANS"], "venue": "IEEE International Symposium on Information Theory.", "citeRegEx": "WANG and SCHUURMANS,? 2003b", "shortCiteRegEx": "WANG and SCHUURMANS", "year": 2003}, {"title": "On the convergence properties of the EM algorithm", "author": ["WU C.F.J."], "venue": "Annals of Statistics.", "citeRegEx": "J.,? 1982", "shortCiteRegEx": "J.", "year": 1982}, {"title": "Nonlinear Programming: a Unified Approach", "author": ["W. ZANGWILL"], "venue": "Prentice-Hall International Series in Management, Englewood Cliffs: N.J.", "citeRegEx": "ZANGWILL,? 1969", "shortCiteRegEx": "ZANGWILL", "year": 1969}, {"title": "Linear prediction models with graph regularization for web-page categorization", "author": ["T. ZHANG", "A. POPESCUL", "DOM", "B."], "venue": "Proc. of the 12th ACM SIGKDD. ACM, New York, NY, USA, 821\u2013826.", "citeRegEx": "ZHANG et al\\.,? 2006", "shortCiteRegEx": "ZHANG et al\\.", "year": 2006}, {"title": "Semi-supervised learning with graphs", "author": ["ZHU X."], "venue": "Ph.D. thesis, Pittsburgh, PA, USA. Chair-Lafferty, John and Chair-Rosenfeld, Ronald.", "citeRegEx": "X.,? 2005", "shortCiteRegEx": "X.", "year": 2005}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. ZHU", "Z. GHAHRAMANI"], "venue": "Tech. rep., Carnegie Mellon University.", "citeRegEx": "ZHU and GHAHRAMANI,? 2002", "shortCiteRegEx": "ZHU and GHAHRAMANI", "year": 2002}, {"title": "Introduction to Semi-Supervised Learning", "author": ["X. ZHU", "A.B. GOLDBERG"], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "ZHU and GOLDBERG,? 2009", "shortCiteRegEx": "ZHU and GOLDBERG", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": "General properties of a large class of loss functions described by Bregman divergences are exploited in this framework in conjunction with Legendre duality and a notion of variable splitting that is also used in alternating direction method of multipliers [Boyd et al. 2011]) to yield a principled and scalable solution.", "startOffset": 256, "endOffset": 274}, {"referenceID": 38, "context": "Note that the setting described above is different from transductive learning setups where both labeled and unlabeled data are available at the same time for model building [Silver and Bennett 2008], as well as online methods where decisions are made on one new example at a time, and after each such decision, the true label of the example is obtained and used to update the model parameters [Blum 1998].", "startOffset": 173, "endOffset": 198}, {"referenceID": 6, "context": "Note that the setting described above is different from transductive learning setups where both labeled and unlabeled data are available at the same time for model building [Silver and Bennett 2008], as well as online methods where decisions are made on one new example at a time, and after each such decision, the true label of the example is obtained and used to update the model parameters [Blum 1998].", "startOffset": 393, "endOffset": 404}, {"referenceID": 0, "context": "This particular algorithm has been briefly introduced in [Acharya et al. 2011].", "startOffset": 57, "endOffset": 78}, {"referenceID": 32, "context": "The combination of multiple single or base classifiers to generate a more capable ensemble classifier has been an active area of research for the past two decades [Kuncheva 2004; Oza and Tumer 2008].", "startOffset": 163, "endOffset": 198}, {"referenceID": 33, "context": "The combination of multiple single or base classifiers to generate a more capable ensemble classifier has been an active area of research for the past two decades [Kuncheva 2004; Oza and Tumer 2008].", "startOffset": 163, "endOffset": 198}, {"referenceID": 45, "context": "Several papers provide both theoretical results [Tumer and Ghosh 1996] and empirical evidence showing the utility of such approaches for solving difficult classification problems.", "startOffset": 48, "endOffset": 70}, {"referenceID": 45, "context": "For instance, an analytical framework to mathematically quantify the improvements in classification results due to combining multiple models has been addressed in [Tumer and Ghosh 1996].", "startOffset": 163, "endOffset": 185}, {"referenceID": 33, "context": "all recognition, and medicine \u2014 is presented in [Oza and Tumer 2008].", "startOffset": 48, "endOffset": 68}, {"referenceID": 46, "context": ", see [Wang et al. 2011; Ghosh and Acharya 2011] and references therein.", "startOffset": 6, "endOffset": 48}, {"referenceID": 26, "context": ", see [Wang et al. 2011; Ghosh and Acharya 2011] and references therein.", "startOffset": 6, "endOffset": 48}, {"referenceID": 40, "context": "More specifically, cluster ensembles can be used to generate more robust and stable clustering results (compared to a single clustering approach), perform distributed computing under privacy or sharing constraints, or reuse existing knowledge [Strehl and Ghosh 2002].", "startOffset": 243, "endOffset": 266}, {"referenceID": 35, "context": "We note however that: \u2022Like single classifiers/clusterers, with very few exceptions [Polikar 2007], ensemble methods assume that the test or scoring data comes from the same underlying distribution as the training (and validation) data.", "startOffset": 84, "endOffset": 98}, {"referenceID": 13, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 54, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 10, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 22, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 14, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 34, "context": ", see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997].", "startOffset": 6, "endOffset": 50}, {"referenceID": 38, "context": ", see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997].", "startOffset": 6, "endOffset": 50}, {"referenceID": 43, "context": ", see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997].", "startOffset": 117, "endOffset": 139}, {"referenceID": 11, "context": "The tasks may be learnt simultaneously [Caruana 1997] or sequentially [Bollacker and Ghosh 2000].", "startOffset": 39, "endOffset": 53}, {"referenceID": 7, "context": "The tasks may be learnt simultaneously [Caruana 1997] or sequentially [Bollacker and Ghosh 2000].", "startOffset": 70, "endOffset": 96}, {"referenceID": 3, "context": "Semi-supervised learning is a domain of machine learning where both labeled and unlabeled data are used to train a model \u2013 typically with lot of unlabeled data and only a small amount of labeled data (see [Bengio et al. 2006; Zhu and Goldberg 2009] and the references therein for more details).", "startOffset": 205, "endOffset": 248}, {"referenceID": 54, "context": "Semi-supervised learning is a domain of machine learning where both labeled and unlabeled data are used to train a model \u2013 typically with lot of unlabeled data and only a small amount of labeled data (see [Bengio et al. 2006; Zhu and Goldberg 2009] and the references therein for more details).", "startOffset": 205, "endOffset": 248}, {"referenceID": 51, "context": "There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011].", "startOffset": 233, "endOffset": 308}, {"referenceID": 42, "context": "There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011].", "startOffset": 233, "endOffset": 308}, {"referenceID": 41, "context": "There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011].", "startOffset": 233, "endOffset": 308}, {"referenceID": 53, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 30, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 2, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 3, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 41, "context": "2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 53, "endOffset": 81}, {"referenceID": 16, "context": "2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 110, "endOffset": 140}, {"referenceID": 44, "context": "2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 145, "endOffset": 157}, {"referenceID": 12, "context": "OAC uses certain Bregman divergences [Censor and Zenios 1997], among which the KL divergence and squared loss constitute just a subset (further details are provided later, in Section 4).", "startOffset": 37, "endOffset": 61}, {"referenceID": 8, "context": "Additionally, the techniques of variable splitting [Boyd et al. 2011] and alternating minimization procedure [Bezdek and Hathaway 2002] are invoked to provide a more scalable solution.", "startOffset": 51, "endOffset": 69}, {"referenceID": 4, "context": "2011] and alternating minimization procedure [Bezdek and Hathaway 2002] are invoked to provide a more scalable solution.", "startOffset": 45, "endOffset": 71}, {"referenceID": 40, "context": "In order to capture the similarities between the instances of X , OAC also takes as input a similarity matrix S, which can be computed from a cluster ensemble, in such a way that each matrix entry corresponds to the relative co-occurrence of two instances in the same cluster [Strehl and Ghosh 2002] \u2014 considering all the data partitions that form the cluster ensemble induced from X .", "startOffset": 276, "endOffset": 299}, {"referenceID": 40, "context": ", the CSPA algorithm in [Strehl and Ghosh 2002].", "startOffset": 24, "endOffset": 47}, {"referenceID": 9, "context": "1 ([Bregman 1967], [Banerjee et al.", "startOffset": 3, "endOffset": 17}, {"referenceID": 1, "context": "1 ([Bregman 1967], [Banerjee et al. 2005]).", "startOffset": 19, "endOffset": 41}, {"referenceID": 36, "context": ", see [Punera and Ghosh 2008].", "startOffset": 6, "endOffset": 29}, {"referenceID": 1, "context": "In that case, however, one can work in the (Legendre) dual space, where the optimal solution has a simple form \u2014 see [Banerjee et al. 2005] for details.", "startOffset": 117, "endOffset": 139}, {"referenceID": 1, "context": "2 [Banerjee et al. 2005] that is stated below.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": "2 ([BANERJEE ET AL. 2005]).", "startOffset": 3, "endOffset": 25}, {"referenceID": 1, "context": "(8) One can show that \u2200yi,yj \u2208 int(dom(\u03c6)), d\u03c6(yi,yj) = d\u03c8(\u2207\u03c6(yj),\u2207\u03c6(yi)) \u2014 see [Banerjee et al. 2005] for more details.", "startOffset": 80, "endOffset": 102}, {"referenceID": 41, "context": "In that case, one needs to use another Lagrangian multiplier to make sure that the returned values lie on simplex as has been done in [Subramanya and Bilmes 2011].", "startOffset": 134, "endOffset": 162}, {"referenceID": 47, "context": "Bregman divergences that satisfy the above properties include a large number of useful loss functions such as the well-known squared loss, KL-divergence, generalized I-divergence, logistic loss, Itakura-Saito distance and Bose-Einstein entropy [Wang and Schuurmans 2003a].", "startOffset": 244, "endOffset": 271}, {"referenceID": 15, "context": "Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized.", "startOffset": 13, "endOffset": 90}, {"referenceID": 50, "context": "Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized.", "startOffset": 13, "endOffset": 90}, {"referenceID": 5, "context": "Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized.", "startOffset": 13, "endOffset": 90}, {"referenceID": 48, "context": "Others have used information geometry [Csisz\u00e1r and Tusn\u00e1dy 1984; Wang and Schuurmans 2003b; Subramanya and Bilmes 2011] to analyze the convergence as well as a combination of both information geometry and topological properties", "startOffset": 38, "endOffset": 119}, {"referenceID": 41, "context": "Others have used information geometry [Csisz\u00e1r and Tusn\u00e1dy 1984; Wang and Schuurmans 2003b; Subramanya and Bilmes 2011] to analyze the convergence as well as a combination of both information geometry and topological properties", "startOffset": 38, "endOffset": 119}, {"referenceID": 27, "context": "of the objective [Gunawardana and Byrne 2005].", "startOffset": 17, "endOffset": 45}, {"referenceID": 53, "context": "The algorithms in [Zhu and Ghahramani 2002; Belkin et al. 2005] are based on minimizing squared-loss and are only suitable for binary classification problems.", "startOffset": 18, "endOffset": 63}, {"referenceID": 2, "context": "The algorithms in [Zhu and Ghahramani 2002; Belkin et al. 2005] are based on minimizing squared-loss and are only suitable for binary classification problems.", "startOffset": 18, "endOffset": 63}, {"referenceID": 41, "context": "MP [Subramanya and Bilmes 2011], on the other hand, is suitable for multi-class problems and additionally provides guard against degenerate solutions (those that assign equal confidence to all classes).", "startOffset": 3, "endOffset": 31}, {"referenceID": 41, "context": "In [Subramanya and Bilmes 2011], the authors also proved that their algorithm converges but the convergence rate (for KL divergence) is not proven and only empirical evidence is given for a linear rate.", "startOffset": 3, "endOffset": 31}, {"referenceID": 41, "context": "In this paper, apart from generalizing these algorithms with a larger class of Bregman divergences, we provide proofs for linear rate of convergence for generalized I divergence and KL divergence (the proof for squared loss follows directly from the analysis of [Subramanya and Bilmes 2011]).", "startOffset": 262, "endOffset": 290}, {"referenceID": 2, "context": "Manifold regularization [Belkin et al. 2005] is a general framework in which a parametric loss function is defined over the labeled samples and is regularized by graph smoothness term defined over both the labeled and unlabeled samples.", "startOffset": 24, "endOffset": 44}, {"referenceID": 16, "context": "Information regularization [Corduneanu and Jaakkola 2003], in essence, works on the same intuition as OAC, but does not provide any proof of convergence and one of the steps of the optimization does not have a closed form solution \u2013 a concern for large data applications.", "startOffset": 27, "endOffset": 57}, {"referenceID": 44, "context": "[Tsuda 2005] extended the works of [Corduneanu and Jaakkola 2003] to hyper-graphs and used closed form solutions in both steps of the alternating minimization procedure which, surprisingly, can be seen as a special case of MP.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[Tsuda 2005] extended the works of [Corduneanu and Jaakkola 2003] to hyper-graphs and used closed form solutions in both steps of the alternating minimization procedure which, surprisingly, can be seen as a special case of MP.", "startOffset": 35, "endOffset": 65}, {"referenceID": 41, "context": "In [Subramanya and Bilmes 2011], the authors followed the procedure of [Csisz\u00e1r and Tusn\u00e1dy 1984] to prove the convergence of a slightly different objective that involves KL-divergence as a loss function.", "startOffset": 3, "endOffset": 31}, {"referenceID": 47, "context": "This Bregman divergence also satisfies the properties (a) to (f), which then allows one to use the convergence tools developed by [Wang and Schuurmans 2003a].", "startOffset": 130, "endOffset": 157}, {"referenceID": 5, "context": "To analyze the same, we use some formulations that were derived in [Bezdek and Hathaway 2003] to characterize the local convergence rate of alternating minimization type of algorithms in general.", "startOffset": 67, "endOffset": 93}, {"referenceID": 5, "context": "Before presenting the main theorem from [Bezdek and Hathaway 2003], the formal definition of q-linear rate of convergence is provided below.", "startOffset": 40, "endOffset": 66}, {"referenceID": 41, "context": "there is no need to maintain left and right copies) and the q-linear rate of convergence of the objective J can be proved following the same method as done in [Subramanya and Bilmes 2011].", "startOffset": 159, "endOffset": 187}, {"referenceID": 39, "context": "4 we use simple (linear) base methods, and pick the popular Semi-Supervised Linear Support Vector Machine (SVM) [Sindhwani and Keerthi 2006] for comparison.", "startOffset": 112, "endOffset": 140}, {"referenceID": 39, "context": "4For these datasets, comparisons with SVM [Sindhwani and Keerthi 2006] have not been performed because the raw data required for learning is not available.", "startOffset": 42, "endOffset": 70}, {"referenceID": 40, "context": ",M4), forBGCM, and for two well-known cluster ensemble approaches\u2014MCLA [Strehl and Ghosh 2002] and HBGF [Fern and Brodley 2004] \u2014 are reproduced here for comparison purposes.", "startOffset": 71, "endOffset": 94}, {"referenceID": 21, "context": ",M4), forBGCM, and for two well-known cluster ensemble approaches\u2014MCLA [Strehl and Ghosh 2002] and HBGF [Fern and Brodley 2004] \u2014 are reproduced here for comparison purposes.", "startOffset": 104, "endOffset": 127}, {"referenceID": 19, "context": "In order to provide some reassurance about the validity and non-randomness of the obtained results, the outcomes of statistical tests, following the study in [Demsar 2006],", "startOffset": 158, "endOffset": 171}, {"referenceID": 39, "context": "Comparison with SVM We also compare OAC to a popular semi-supervised algorithm known as SVM [Sindhwani and Keerthi 2006].", "startOffset": 92, "endOffset": 120}, {"referenceID": 29, "context": "This algorithm is essentially a Transductive Linear Support Vector Machine (SVM) which can be viewed as a large scale implementation of the algorithm introduced in [Joachims 1999b].", "startOffset": 164, "endOffset": 180}, {"referenceID": 39, "context": "For dealing with unlabeled data, it appends an additional term in the SVM objective function whose role is to drive the classification hyperplane towards low data density regions [Sindhwani and Keerthi 2006].", "startOffset": 179, "endOffset": 207}, {"referenceID": 19, "context": "In addition, OAC shows better accuracies than both SVM and BGCM \u2014 from the adopted statistical procedure [Demsar 2006], OAC exhibits significantly better accuracies at a significance level of 10%.", "startOffset": 105, "endOffset": 118}, {"referenceID": 38, "context": "Transfer Learning Transfer learning emphasizes the transfer of knowledge across domains, tasks, and distributions that are similar but not the same [Silver and Bennett 2008].", "startOffset": 148, "endOffset": 173}, {"referenceID": 34, "context": "The real-world datasets employed in our experiments are: a) Text Documents \u2014 [Pan and Yang 2010]: From the well-known text collections 20 newsgroup and Reuters-21758, nine cross-domain learning tasks are generated.", "startOffset": 77, "endOffset": 96}, {"referenceID": 37, "context": "b) Botswana \u2014 [Rajan et al. 2006]: This is an application of transfer learning to the pixel-level classification of remotely sensed images, which provides a real-life scenario where such learning will be useful \u2014 in contrast to the contrived setting of text classification, which is chosen as it has been used previously in [Dai et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 28, "context": "We also compare OAC with two transfer learning algorithms from the literature \u2014 Transductive Support Vector Machines (TSVM) [Joachims 1999a] and the Locally Weighted Ensemble (LWE) [Gao et al.", "startOffset": 124, "endOffset": 140}, {"referenceID": 31, "context": "For the hyperspectral data, we use two baseline classifiers: the well-known Na\u0131\u0308ve Bayes Wrapper (NBW) and the Maximum Likelihood (ML) classifier, which performs well when used with a best bases feature extractor [Kumar et al. 2001].", "startOffset": 213, "endOffset": 232}, {"referenceID": 39, "context": "The proposed framework has been empirically shown to outperform a variety of algorithms [Gao et al. 2011; Sindhwani and Keerthi 2006; Gao et al. 2008] in both semi-supervised and transfer learning problems.", "startOffset": 88, "endOffset": 150}, {"referenceID": 1, "context": "2 ([BANERJEE ET AL. 2005]).", "startOffset": 3, "endOffset": 25}, {"referenceID": 47, "context": "The proof is based on the works of [Wang and Schuurmans 2003a].", "startOffset": 35, "endOffset": 62}, {"referenceID": 47, "context": "The proof here follows the same line of argument as given in [Wang and Schuurmans 2003a] and [Eggermont and LaRiccia 1998].", "startOffset": 61, "endOffset": 88}, {"referenceID": 20, "context": "The proof here follows the same line of argument as given in [Wang and Schuurmans 2003a] and [Eggermont and LaRiccia 1998].", "startOffset": 93, "endOffset": 122}, {"referenceID": 41, "context": "There is another interesting aspect of J that was discovered in [Subramanya and Bilmes 2011] for a slightly different objective function with KL divergence used as a loss function.", "startOffset": 64, "endOffset": 92}], "year": 2012, "abstractText": "Unsupervised models can provide supplementary soft constraints to help classify new, \u201ctarget\u201d data since similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers learnt on previously encountered \u201csource\u201d data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by popular transductive learning techniques or by na\u0131\u0308vely applying classifiers learnt on the original task to the target data.", "creator": "LaTeX with hyperref package"}}}