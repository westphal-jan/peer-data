{"id": "1206.1106", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "No more pesky learning rates", "abstract": "the performance of stochastic gradient descent ( sgd ) depends critically on comparing how average learning rates are tuned and decreased over time. we propose a similar method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. the method relies constantly on local gradient variations across samples. using a number of convex and overlapping non - convex learning tasks, we show independently that essentially the resulting algorithm matches the performance indicators of the best settings consistently obtained through systematic search, and effectively removes the need for learning rate tuning.", "histories": [["v1", "Wed, 6 Jun 2012 02:06:57 GMT  (629kb,D)", "http://arxiv.org/abs/1206.1106v1", "Submitted to NIPS 2012"], ["v2", "Mon, 18 Feb 2013 16:09:50 GMT  (1097kb,D)", "http://arxiv.org/abs/1206.1106v2", null]], "COMMENTS": "Submitted to NIPS 2012", "reviews": [], "SUBJECTS": "stat.ML cs.LG", "authors": ["tom schaul", "sixin zhang", "yann lecun"], "accepted": true, "id": "1206.1106"}, "pdf": {"name": "1206.1106.pdf", "metadata": {"source": "CRF", "title": "No More Pesky Learning Rates", "authors": ["Tom Schaul", "Sixin Zhang", "Yann LeCun"], "emails": ["schaul@cims.nyu.edu", "zsx@cims.nyu.edu", "yann@cims.nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "Large-scale learning problems require algorithms that scale benignly (e.g. sub-linearly) with the size of the dataset and the number of trainable parameters. This has lead to a recent resurgence of interest in stochastic gradient descent methods (SGD). Besides fast convergence, SGD has sometimes been observed to yield significantly better generalization errors than batch methods [1].\nIn practice, getting good performance with SGD requires some manual adjustment of the initial value of the learning rate (or step size) for each model and each problem, as well as the design of an annealing schedule for stationary data. The problem is particularly acute for non-stationary data.\nThe contribution of this paper is a novel method to automatically adjust learning rates (possibly different learning rates for different parameters), so as to minimize some estimate of the expectation of the loss at any one time.\nStarting from an idealized scenario where every sample\u2019s contribution to the loss is quadratic and separable, we derive a formula for the optimal learning rates for SGD, based on estimates of the variance of the gradient. The formula has two components: one that captures variability across samples, and one that captures the local curvature, both of which can be estimated in practice. The method can be used to derive a single common learning rate, or local learning rates for each parameter, or each block of parameters, leading to five variations of the basic algorithm, none of which need any parameter tuning.\nThe performance of the methods obtained without any manual tuning are reported on a variety of convex and non-convex learning models and tasks. They compare favorably with an \u201cideal SGD\u201d, where the best possible learning rate was obtained through systematic search."}, {"heading": "2 Background", "text": "SGD methods have a long history in adaptive signal processing, neural networks, and machine learning, with an extensive literature (see [2, 1] for recent reviews). While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of\nar X\niv :1\n20 6.\n11 06\nv1 [\nst at\n.M L\n] 6\nJ un\n2 01\nthe Hessian was used within SGD. Still, practitioners need to deal with a sensitive hyper-parameter tuning phase to get top performance: each of the PASCAL tasks used very different parameter settings. This tuning is very costly, as every parameter setting is typically tested over multiple epochs.\nLearning rates in SGD are generally decreased according a schedule of the form \u03b7(t) = \u03b70(1 + \u03b3t)\u22121. Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.\nThe main contribution of the present paper is a formula that gives the value of the learning rate that will maximally decrease the expected loss after the next update. For efficiency reasons, some terms in the formula must be approximated using such quantities as the mean and variance of the gradient. As a result, the learning rate is automatically decreased to zero when approaching an optimum of the loss, without requiring a pre-determined annealing schedule."}, {"heading": "3 Optimal Adaptive Learning Rates", "text": "In this section, we briefly review SGD, and derive an optimal learning rate schedule, using an idealized quadratic and separable loss function. We show that using this learning rate schedule preserves convergence guarantees of SGD. We then find how the optimal learning rate values can be estimated from available information, and describe a couple of possible approximations.\nThe samples, indexed by j, are drawn i.i.d. from a data distribution P . Each sample contributes a per-sample loss L(j)(\u03b8) to the expected loss:\nJ (\u03b8) = Ej\u223cP [ L(j)(\u03b8) ] (1)\nwhere \u03b8 \u2208 Rd is the trainable parameter vector, whose optimal value is denoted \u03b8\u2217 = argmin\u03b8 J (\u03b8). The SGD parameter update formula is of the form \u03b8(t+1) = \u03b8(t) \u2212 \u03b7(t)\u2207(j)\u03b8 , where \u2207(j)\u03b8 = \u2202 \u2202\u03b8L\n(j)(\u03b8) is the gradient of the the contribution of example j to the loss, and the learning rate \u03b7(t) is a suitably chosen sequence of positive scalars (or positive definite matrices)."}, {"heading": "3.1 Noisy Quadratic Loss", "text": "We assume that the per-sample loss functions are smooth around minima, and can be locally approximated by a quadratic function. We also assume that the minimum value of the per-sample loss functions are zero:\nL(j)(\u03b8) = 1 2\n( \u03b8 \u2212 c(j) )> H(j) ( \u03b8 \u2212 c(j) ) +Q(j) ; \u2207(j)\u03b8 = H (j) ( \u03b8 \u2212 c(j) ) (2)\nwhere Hi is the (positive semi-definite) Hessian matrix of the per-sample loss of sample j, and c(j) is the optimum for that sample. The distribution of per-sample optima c(j) has mean \u03b8\u2217 and variance \u03a3. Figure 1 illustrates the scenario in one dimension.\nTo simplify the analysis, we assume for the remainder of this section that the Hessians of the persample losses are identical for all samples, and that the problem is separable, i.e., the Hessians\nare diagonal, with diagonal terms denoted {h1, . . . , hi, . . . , hd}. Further, we will ignore the offdiagonal terms of \u03a3, with and denote the diagonal {\u03c321 , . . . , \u03c32i , . . . , \u03c32d}. Then, for any of the d dimensions, we thus obtain a one-dimensional problem (all indices i omitted).\nJ(\u03b8) = Ei\u223cP [ 1\n2 h(\u03b8 \u2212 c(j))2\n] = 1 2 h [ (\u03b8 \u2212 \u03b8\u2217)2 + \u03c32 ] (3)\nThe gradient components are\u2207(j)\u03b8 = h ( \u03b8 \u2212 c(j) ) , with\nE[\u2207\u03b8] = h(\u03b8 \u2212 \u03b8\u2217) V ar[\u2207\u03b8] = h2\u03c32 (4)\nand we can rewrite the SGD update equation as \u03b8(t+1) = \u03b8(t) \u2212 \u03b7h ( \u03b8(t) \u2212 c(j) ) = (1\u2212 \u03b7h)\u03b8(t) + \u03b7h\u03b8\u2217 + \u03b7h\u03c3\u03be(j) (5)\nwhere the \u03be(j) are i.i.d. samples from a zero-mean and unit-variance Gaussian distribution. Inserting this into equation 3, we obtain the expected loss after an SGD update\nE [ J ( \u03b8(t+1) ) | \u03b8(t) ] = 1 2 h \u00b7 [ (1\u2212 \u03b7h)2(\u03b8(t) \u2212 \u03b8\u2217)2 + \u03b72h2\u03c32 + \u03c32 ] We can now derive the optimal (greedy) learning rates for the current time t as the value \u03b7\u2217(t) that minimizes the expected loss after the next update\n\u03b7\u2217(t) = argmin \u03b7\n[ (1\u2212 \u03b7h)2(\u03b8(t) \u2212 \u03b8\u2217)2 + \u03c32 + \u03b72h2\u03c32 ] (6)\n\u03b7\u2217(t) = argmin \u03b7\n[ \u03b72 ( h(\u03b8(t) \u2212 \u03b8\u2217)2 + h\u03c32 ) \u2212 2\u03b7(\u03b8(t) \u2212 \u03b8\u2217)2 ] = 1\nh \u00b7 (\u03b8\n(t) \u2212 \u03b8\u2217)2\n(\u03b8(t) \u2212 \u03b8\u2217)2 + \u03c32 (7)\nIn the classical (noiseless or batch) derivation of the optimal learning rate, the best value is simply \u03b7\u2217(t) = h\u22121. The above formula inserts a corrective term that reduces the learning rate whenever the sample pulls the parameter vector in different directions, as measured by the gradient variance \u03c32. The reduction of the learning rate is larger near an optimum, when (\u03b8(t) \u2212 \u03b8\u2217)2 is small relative to \u03c32. In effect, this will reduce the expected error due to the noise in the gradient. Overall, this will have the same effect as the usual method of progressively decreasing the learning rate as we get closer to the optimum, but it makes this annealing schedule automatic."}, {"heading": "3.2 Convergence", "text": "If we use \u03b7\u2217(t), then almost surely, the above algorithm converges (for the quadratic model). To prove that, we follow classical techniques based on Lyapunov stability theory [9]. Notice that the expected loss follows\nE [ J ( \u03b8(t+1) ) | \u03b8(t) ] = 1 2 h \u00b7 [\n\u03c32\n(\u03b8(t) \u2212 \u03b8\u2217)2 + \u03c32 (\u03b8(t) \u2212 \u03b8\u2217)2 + \u03c32\n] \u2264 J ( \u03b8(t) )\nThus J(\u03b8(t)) is a positive super-martingale, indicating that almost surely J(\u03b8(t))\u2192 J\u221e. We are to prove that almost surely J\u221e = J(\u03b8\u2217) = 12h\u03c3 2. Observe that\nJ(\u03b8(t))\u2212 E[J(\u03b8(t+1)) | \u03b8(t)] = 1 2 h\u03b7\u2217(t) , and E[J(\u03b8(t))]\u2212 E[J(\u03b8(t+1)) | \u03b8(t)] = 1 2 hE[\u03b7\u2217(t)]\nSince E[J(\u03b8(t))] is bounded below by 0, the telescoping sum gives us E[\u03b7\u2217(t)] \u2192 0, which in turn implies that in probability \u03b7\u2217(t)\u2192 0. We can rewrite this as\n\u03b7\u2217(t) = J(\u03b8t)\u2212 12h\u03c3 2\nJ(\u03b8t) \u2192 0\nBy uniqueness of the limit, almost surely, J \u221e\u2212 12h\u03c3 2\nJ\u221e = 0. Given that J is strictly positive everywhere, we conclude that J\u221e = 12h\u03c3 2 almost surely, i.e J(\u03b8(t))\u2192 12h\u03c3 2 = J(\u03b8\u2217)."}, {"heading": "3.3 Global vs. Parameter-specific Learning Rates", "text": "The previous subsections looked at the optimal learning rate in the one-dimensional case, which can be trivially generalized to d dimensions if we assume that all parameters are separable, namely by using an individual learning rate \u03b7\u2217i for each dimension i. This is especially useful if the problem is badly conditioned.\nAlternatively, we can derive an optimal global learning rate \u03b7\u2217g as follows.\n\u03b7\u2217g(t) = argmin \u03b7\nE [ J ( \u03b8(t+1) ) | \u03b8(t) ] = argmin\n\u03b7 [ d\u2211 i=1 hi ( (1\u2212 \u03b7hi)2(\u03b8(t)i \u2212 \u03b8 \u2217 i ) 2 + \u03c32i + \u03b7 2h2i\u03c3 2 i )]\n= argmin \u03b7\n[ \u03b72\nd\u2211 i=1 ( h3i (\u03b8 (t) i \u2212 \u03b8 \u2217 i ) 2 + h3i\u03c3 2 i ) \u2212 2\u03b7 d\u2211 i=1 h2i (\u03b8 (t) i \u2212 \u03b8 \u2217 i ) 2 ] which gives\n\u03b7\u2217g(t) =\n\u2211d i=1 h 2 i (\u03b8\n(t) i \u2212 \u03b8\u2217i )2\u2211d\ni=1 ( h3i (\u03b8 (t) i \u2212 \u03b8\u2217i )2 + h3i\u03c32i ) (8) In-between a global and a component-wise learning rate, it is of course possible to have common learning rates for blocks of parameters. In the case of multi-layer learning systems, the blocks may regroup the parameters of each single layer, the biases, etc."}, {"heading": "4 Approximations", "text": "In practice, we are not given the quantities \u03c3i, hi and (\u03b8 (t) i \u2212 \u03b8\u2217i )2. However, based on equation 4, we can estimate them from the observed samples of the gradient:\n\u03b7\u2217i = 1 hi \u00b7 (E[\u2207\u03b8i ])\n2\n(E[\u2207\u03b8i ]) 2 + V ar[\u2207\u03b8i ]\n= 1 hi \u00b7 (E[\u2207\u03b8i ])\n2\nE[\u22072\u03b8i ] (9)\nThe situation is slightly different for the global learning rate \u03b7\u2217g . Here we assume that it is feasible to estimate the maximal curvature h+ = maxi(hi) (which can be done efficiently, for example using the diagonal Hessian computation method described in [3]). Then we have the bound\n\u03b7\u2217g(t) \u2265 1\nh+ \u00b7\n\u2211d i=1 h 2 i (\u03b8\n(t) i \u2212 \u00b5i)2\u2211d\ni=1 ( h2i (\u03b8 (t) i \u2212 \u00b5i)2 + h2i\u03c32i ) = 1 h+ \u00b7 \u2016E[\u2207\u03b8]\u2016 2 E [ \u2016\u2207\u03b8\u20162 ] (10) because E [ \u2016\u2207\u03b8\u20162 ] = E [\u2211d i=1(\u2207\u03b8i)2 ] = \u2211d i=1 E [ (\u2207\u03b8i)2 ] . In both cases (equations 9 and 10),\nthe optimal learning rate is decomposed into two factors, one which is the inverse curvature (as is the case for batch second-order methods), and one novel term that depends on the noise in the gradient, relative to the expected square norm of the gradient. Below, we approximate these terms separately. For the investigations where we use the true values instead of a practical algorithm, we speak of the \u2018oracle\u2019 variant (e.g. in Figure 2)."}, {"heading": "4.1 Approximate Variability", "text": "We use an exponential moving average with time-constant \u03c4 (the approximate number of samples considered from recent memory) for online estimates:\ngi(t+ 1) = (1\u2212 \u03c4\u22121i ) \u00b7 gi(t) + \u03c4 \u22121 i \u00b7 \u2207\u03b8i(t) vi(t+ 1) = (1\u2212 \u03c4\u22121i ) \u00b7 vi(t) + \u03c4 \u22121 i \u00b7 (\u2207\u03b8i(t)) 2\nl(t+ 1) = (1\u2212 \u03c4\u22121) \u00b7 l(t) + \u03c4\u22121 \u00b7 \u2016\u2207\u03b8\u20162\nAlgorithm 1: Stochastic gradient descent with (component-wise) adaptive learning rates (SGD-ld) input : , n0, C initialization from n0 samples, but no updates for i \u2208 {1, . . . , d} do\n\u03b8i \u2190 \u03b8init gi \u2190 1n0 \u2211n0 j=1\u2207 (j) \u03b8i\n\u03c4i \u2190 n0 vi \u2190 C \u00b7 1n0 \u2211n0 j=1 ( \u2207(j)\u03b8i )2 hi \u2190 min [ , C \u00b7 1n0 \u2211n0 j=1 h (j) i\n] end repeat\ndraw a sample c(j), compute the gradient\u2207(j)\u03b8 , and compute the diagonal Hessian estimates h (j) i using the \u201cbbprop\u201d method for i \u2208 {1, . . . , d} do\nupdate moving averages\ngi \u2190 (1\u2212 \u03c4\u22121i ) \u00b7 gi + \u03c4 \u22121 i \u00b7 \u2207 (j) \u03b8i vi \u2190 (1\u2212 \u03c4\u22121i ) \u00b7 vi + \u03c4 \u22121 i \u00b7 ( \u2207(j)\u03b8i )2 hi \u2190 (1\u2212 \u03c4\u22121i ) \u00b7 hi + \u03c4 \u22121 i \u00b7min [ , |bbprop(\u03b8)(j)i |\n] estimate learning rate \u03b7\u2217i \u2190 (gi) 2 hi \u00b7 vi update parameter \u03b8i \u2190 \u03b8i \u2212 \u03b7\u2217i\u2207 (j) \u03b8i\nupdate memory size \u03c4i \u2190 ( 1\u2212 (gi) 2\nvi\n) \u00b7 \u03c4i + 1\nend until stopping criterion is met\nwhere gi estimates the average gradient component i, vi estimates the uncentered variance on gradient component i, and l estimates the squared length of the gradient vector:\ngi \u2248 E[\u2207\u03b8i ] vi \u2248 E[\u22072\u03b8i ] l \u2248 E [ \u2016\u2207\u03b8\u20162 ] and we need vi only for an element-wise adaptive learning rate and l only in the global case.\nWe want the size of the memory to increase when the steps taken are small (increment by 1), and to decay quickly if a large step is taken, which is obtained naturally, by the following update\n\u03c4i(t+ 1) =\n( 1\u2212 gi(t) 2\nvi(t)\n) \u00b7 \u03c4i(t) + 1\nand the equivalent in the global case\n\u03c4(t+ 1) = ( 1\u2212 \u2211d i=1 gi 2\nl(t)\n) \u00b7 \u03c4(t) + 1\nTo initialize these estimates, we compute the arithmetic averages over a handful n0 of samples before starting to update the parameters, and overestimate vi (or l) by a factor C, which has the effect of slowing down the initial updates, until the moving averages become reliable. The parameters n0 and C have only a transient initialization effect on the algorithm, and thus do not need to be tuned."}, {"heading": "4.2 Approximate Curvature", "text": "There exist a number of methods for obtaining an online estimates of the diagonal Hessian. We adopt the \u201cbbprop\u201d method, which computes positive estimates of the diagonal Hessian terms for a single sample h(j)i , using a back-propagation formula [3]. The diagonal estimates are used in an exponential moving average procedure\nhi(t+ 1) = (1\u2212 \u03c4\u22121i ) \u00b7 hi(t) + \u03c4 \u22121 i \u00b7 h (t) i\nIf the curvature is close to zero for some component, this can drive \u03b7\u2217 to infinity. Thus, to avoid numerical instability (to bound the condition number of the approximated Hessian), we enforce a lower bound hi \u2265 . This trick is not necessary in the global case, because h+ = maxi(hi) is rarely zero."}, {"heading": "5 Adaptive SGD", "text": "The simplest version of the method views each component in isolation. This form of the algorithm will be called \u201cvSGD\u201d (for \u201cvariance-based SGD\u201d). In realistic setting with high-dimensional parameter vector, it is not clear a priori whether it is best to have a single, global learning rate (that can be estimated robustly), or a set of local , dimension-specific, or block-specific learning rates (whose estimation will be less robust). The local vs global choice can be done independently to estimate the diagonal Hessian terms, and the correction term due to the variance of the gradient. The various combinations (all of which have linear complexity in time and space) are as follows:\nvSGD-ld uses local gradient variance terms (\u201cl\u201d for local) and the local diagonal Hessian estimates (\u201cd\u201d for diagonal), leading to \u03b7\u2217i = (gi)\n2/(hi \u00b7vi) (full pseudocode shown in Algorithm 1), vSGD-lu uses local gradient variance terms, and an upper bound on the diagonal Hessian (\u201cu\u201d for\nupper bound), so \u03b7\u2217i = (gi) 2/(h+ \u00b7 vi),\nvSGD-gd uses global gradient variance terms (\u201cg\u201d for global) and diagonal Hessian estimates, thus \u03b7\u2217i = \u2211 (gi)\n2/(hi \u00b7 l) vSGD-gu uses a global gradient variance term and an upper bound on diagonal Hessian terms:\n\u03b7\u2217 = \u2211 (gi) 2/(h+ \u00b7 l),\nvSGD-b operates like vSGD-gu, but being only global across multiple (architecture-specific) blocks of parameters, with a different learning rate per block. Similar ideas are adopted in TONGA [10]. In the experiments, the parameters connecting every two layers of the network are regard as a block.\nvSGD-bb same as vSGD-b, except that the weight matrices and the bias parameters in the same module are placed in separate blocks."}, {"heading": "6 Experiments", "text": ""}, {"heading": "6.1 Noisy Quadratic", "text": "To form an intuitive understanding of the effects of the optimal adaptive learning rate method, and the effect of the approximation, we illustrates the oscillatory behavior of SGD, and compare the decrease in the loss function and the accompanying change in learning rates on toy model of Section 3.1 (see Figure 2)"}, {"heading": "6.2 Non-stationary Quadratic", "text": "In realistic on-line learning scenarios, the curvature or noise level in any given dimension changes over time (for example because of the effects of updating other parameters), and thus the learning rates need to decrease as well as increase. Of course, no fixed learning rate or fixed cooling schedule can achieve this. Figure 3 illustrates how vSGD with its adaptive memory-size appropriately handles such cases. Its initially large learning rate allows it to quickly approach the optimum, then it gradually reduces the learning rate as the gradient variance increases relative to the square norm of the average gradient, thus allowing the parameter to continually approach the optimum. When the data distribution changes abruptly, the algorithm automatically detects that the norm of the average gradient increases relative to the variance. The learning rate jumps back up and adapts to the new circumstances."}, {"heading": "6.3 Experiments with Standard Benchmarks and Architectures", "text": "Experiments used a variety of architectures trained on the widely tested MNIST dataset [11] (with 60,000 training samples, and 10,000 test samples), and the \u201cbatch1\u201d subset of the CIFAR-10 dataset [12], which contains 20% of the full training set (10,000 training samples, 10,000 test samples). The architectures tested include logistic regression (MNIST-1C, CIFAR-1C), which are convex in the parameters, and fully-connected multi-layer neural nets with 2 and 3 layers (-2C and -3C). These architectures used the tanh non-linearity for hidden units, and were trained with the cross-entropy loss. The loss is non-convex in the parameters for the multi-layer architectures. The last experiment consisted of training a narrow auto-encoder with a single hidden layer, trained to minimize the square reconstruction error (CIFAR-2R.\nFor all experiments, the training data were normalized to have mean zero in each input dimension. Experiments were repeated with 10 random initializations of the weights [13] and stopped after 6 epochs (see Table 1).\nIn Table 2 and Table 3, we compare the best SGD with our vSGD variants. vSGD-lu (local gradient variance, upper bound on diagonal Hessian) seems to be good in \u2019shallow\u2019 network like MNIST1C,CIFAR-1/2C. But for more challenging architectures, such as CIFAR-2R, vSGD-bb (layers and biases in different blocks) produces better results. However, on MNIST-3C vSGD-bb does not perform as good as expected. The issue was traced over-fitting due to the large size of the network, the relatively small size of the training set, and the absence of regularization term. A separate set of experiments were carried out with L2 regularizations on the parameters. Figure 4, shows the learning curves of two runs with an L2 regularizer \u03bb2 ||\u03b8|| 2 2. When \u03bb = 0, the L\n2 norm of the parameters goes up very quickly and ends up much bigger than that of best SGD; but with \u03bb = 0.0001, it ends up at a similar level as that of best SGD."}, {"heading": "7 Conclusion and Future Work", "text": "Starting from the idealized case of quadratic loss contributions from each sample, we derived a method to compute an optimal learning rate at each update, and (possibly) for each parameter, that optimizes the expected loss after the next update. The method relies on the square norm of the expectation of the gradient, and the expectation of the square norm of the gradient. A proof of the conversion in probability is given. We showed different ways of approximating those learning rates in linear time and space in practice. The experimental results confirm the theoretical prediction: the adaptive learning rate method completely eliminates the need for manual tuning of the learning rate, or for systematic search of its best value.\nIn a set of experiments (not included from this paper for lack of space), the method proved very effective when used in on-line training scenarios with non-stationary signals. The adaptive learning rate automatically increases when the distribution changes, so as to adjust the model to the new distribution, and automatically decreases in stable periods when the system fine-tunes itself within an attractor."}, {"heading": "Acknowledgments", "text": "The authors want to thank Camille Couprie, Cle\u0301ment Farabet and Arthur Szlam for helpful discussions, and Shane Legg for the paper title. This work was funded in part through AFR postdoc grant number 2915104, of the National Research Fund Luxembourg, and ONR Grant 5-74100-F6510."}], "references": [{"title": "The tradeoffs of large scale learning", "author": ["L Bottou", "O. Bousquet"], "venue": "Optimization for Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": "Online Learning and Neural Networks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Efficient backprop", "author": ["Y LeCun", "L Bottou", "G Orr", "K. Muller"], "venue": "Neural Networks: Tricks of the trade. Springer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Large scale online learning", "author": ["L Bottou", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Sgd-qn: Careful quasi-newton stochastic gradient descent", "author": ["A Bordes", "L Bottou", "P. Gallinari"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A stochastic approximation method", "author": ["H Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1951}, {"title": "Towards optimal one pass large scale learning with averaged stochastic gradient descent", "author": ["W. Xu"], "venue": "ArXiv-CoRR, abs/1107.2490,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["F Bach", "E. Moulines"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Stability and positive supermartingales", "author": ["R.S. Bucy"], "venue": "Journal of Differential Equations,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1965}, {"title": "The mnist dataset of handwritten digits", "author": ["Y LeCun", "C. Cortes"], "venue": "http://yann.lecun.com/exdb/mnist/", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "Technical report, Department of Computer Science, University of Toronto,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Besides fast convergence, SGD has sometimes been observed to yield significantly better generalization errors than batch methods [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "SGD methods have a long history in adaptive signal processing, neural networks, and machine learning, with an extensive literature (see [2, 1] for recent reviews).", "startOffset": 136, "endOffset": 142}, {"referenceID": 0, "context": "SGD methods have a long history in adaptive signal processing, neural networks, and machine learning, with an extensive literature (see [2, 1] for recent reviews).", "startOffset": 136, "endOffset": 142}, {"referenceID": 2, "context": "While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of", "startOffset": 268, "endOffset": 271}, {"referenceID": 4, "context": "While the practical advantages of SGD for machine learning applications have been known for a long time [3], interest in SGD has increased in recent years due to the ever-increasing amounts of streaming data, to theoretical optimality results for generalization error [4], and to competitions being won by SGD methods, such as the PASCAL Large Scale Learning Challenge [5], where Quasi-Newton approximation of", "startOffset": 369, "endOffset": 372}, {"referenceID": 5, "context": "Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.", "startOffset": 40, "endOffset": 43}, {"referenceID": 6, "context": "Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.", "startOffset": 80, "endOffset": 86}, {"referenceID": 7, "context": "Originally proposed as \u03b7(t) = O(t\u22121) in [6], this form was recently analysed in [7, 8] from a non-asymptotic perspective to understand how hyper-parameters like \u03b70 and \u03b3 affect the convergence speed.", "startOffset": 80, "endOffset": 86}, {"referenceID": 8, "context": "To prove that, we follow classical techniques based on Lyapunov stability theory [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "Here we assume that it is feasible to estimate the maximal curvature h = maxi(hi) (which can be done efficiently, for example using the diagonal Hessian computation method described in [3]).", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "We adopt the \u201cbbprop\u201d method, which computes positive estimates of the diagonal Hessian terms for a single sample h i , using a back-propagation formula [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 9, "context": "Experiments used a variety of architectures trained on the widely tested MNIST dataset [11] (with 60,000 training samples, and 10,000 test samples), and the \u201cbatch1\u201d subset of the CIFAR-10 dataset [12], which contains 20% of the full training set (10,000 training samples, 10,000 test samples).", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "Experiments used a variety of architectures trained on the widely tested MNIST dataset [11] (with 60,000 training samples, and 10,000 test samples), and the \u201cbatch1\u201d subset of the CIFAR-10 dataset [12], which contains 20% of the full training set (10,000 training samples, 10,000 test samples).", "startOffset": 197, "endOffset": 201}], "year": 2012, "abstractText": "The performance of stochastic gradient descent (SGD) depends critically on how learning rates are tuned and decreased over time. We propose a method to automatically adjust multiple learning rates so as to minimize the expected error at any one time. The method relies on local gradient variations across samples. Using a number of convex and non-convex learning tasks, we show that the resulting algorithm matches the performance of the best settings obtained through systematic search, and effectively removes the need for learning rate tuning.", "creator": "LaTeX with hyperref package"}}}