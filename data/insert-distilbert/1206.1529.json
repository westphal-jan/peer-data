{"id": "1206.1529", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2012", "title": "Sparse projections onto the simplex", "abstract": "the past decade has seen the rise of $ \\ ell _ 1 $ - rate relaxation methods to typically promote sparsity for better interpretability and generalization of learning results. however, there are several important learning applications, such as markowitz portolio partition selection and sparse mixture finite density response estimation, that feature simplex constraints, which disallow the application of the original standard $ \\ at ell _ r 1 $ - penalty. in this setting, we show how to efficiently obtain sparse projections onto the positive compound and general simplex with sparsity constraints. we provide an exact sparse projector for the positive simplex correction constraints, and derive a novel approach with online optimality goals and approximation guarantees for sparse projections stacked onto the underlying general simplex processing constraints. even for small sized maze problems, computing this new approach is three orders of magnitude faster frequently than the alternative, state - of - the - art branch - and - bound based cplex solver counterparts with no sacrifice in solution quality. we also empirically demonstrate that our projectors provide substantial numerical benefits in portfolio crowd selection and density estimation.", "histories": [["v1", "Thu, 7 Jun 2012 15:33:12 GMT  (399kb,D)", "http://arxiv.org/abs/1206.1529v1", "13 Pages"], ["v2", "Thu, 14 Jun 2012 07:21:01 GMT  (399kb,D)", "http://arxiv.org/abs/1206.1529v2", "13 Pages"], ["v3", "Thu, 10 Jan 2013 16:33:23 GMT  (361kb,D)", "http://arxiv.org/abs/1206.1529v3", "10 Pages"], ["v4", "Thu, 28 Mar 2013 15:01:33 GMT  (1182kb,D)", "http://arxiv.org/abs/1206.1529v4", "9 Pages"], ["v5", "Wed, 10 Apr 2013 08:39:10 GMT  (1350kb,D)", "http://arxiv.org/abs/1206.1529v5", "9 Pages"]], "COMMENTS": "13 Pages", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["anastasios kyrillidis", "stephen becker", "volkan cevher", "christoph koch 0001"], "accepted": true, "id": "1206.1529"}, "pdf": {"name": "1206.1529.pdf", "metadata": {"source": "CRF", "title": "Sparse projections onto the simplex", "authors": ["Anastasios Kyrillidis", "Stephen Becker", "Volkan Cevher"], "emails": ["volkan.cevher}@epfl.ch", "stephen.becker@upmc.fr"], "sections": [{"heading": "1 Introduction", "text": "Learning applications typically boil down to optimization of a loss function f(\u03b2) in order to obtain a (model or feature) vector \u03b2 \u2208 Rp. In this setting, we prefer sparse solutions even if there exist other solutions that might obtain better loss values. By sparse, we mean that \u03b2 has at most s nonzero coefficients where s p. This choice is well-justified as sparsity-based learning not only shows great empirical success with improved interpretability, but also is backed up with rigorous theoretical analysis. For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].\nHowever, sparsity inherently introduces non-convexity into learning problems, which is undesirable according to conventional wisdom. For instance, to obtain a minimizer of f(\u03b2) with a specific sparsity s, we have to deal with a non-convex constraint: \u2016\u03b2\u20160 \u2264 s, where \u2016 \u00b7 \u20160 counts the sparsity of \u03b2. In turn, the resulting minimization is typically NP-Hard (even if f(\u03b2) is convex) if we seek a global minimizer (cf., [3]). Surprisingly, it is possible to obtain sparse critical points of general loss functions with the projected gradient algorithm [4]. This algorithm iteratively uses the Euclidean projection onto the constraint \u2016\u03b2\u20160 \u2264 s, which is efficiently obtained via hard-thresholding.\nLuckily, we can almost always achieve approximately sparse solutions via a simple convexification. For instance, we simply replace the \u2016\u03b2\u20160 \u2264 s constraint with an `1-norm constraint as \u2016\u03b2\u20161 \u2264 \u03bb, where \u03bb \u2208 R+. The Euclidean projection onto the convex constraint \u2016\u03b2\u20161 \u2264 \u03bb can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions. Moreover, if f(\u03b2) is convex, then we can leverage decades of research in convex optimization, which not only provide computationally efficient algorithms to obtain accurate solutions, but also strong analytical tools to establish consistency and prediction efficiency of the solutions as \u03bb varies [2].\nWhile the `1-norm is now a de facto standard in sparsity related problems, several important learning problems involve simplex constraints that mar its application in further sparsifying the solutions. Let us first recall the definitions\nar X\niv :1\n20 6.\n15 29\nv1 [\ncs .L\nG ]\n7 J\nof the two simplex variants. The standard simplex in Rp with parameter \u03bb is given by \u2206+\u03bb = { \u03b2 \u2208 Rp : \u03b2i \u2265\n0, \u2211 i \u03b2i = \u03bb } . The general simplex in Rp with parameter \u03bb is given by \u2206\u03bb = { \u03b2 \u2208 Rp : \u2211 i \u03b2i = \u03bb } . It is clear that direct applications of the `1-norm (regularization, constraint, or otherwise) cannot achieve further sparsification beyond what the standard simplex constraint obtains. Unfortunately, many applications require further sparsification.\nAs a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector \u03b2 that minimizes a return-adjusted empirical risk objective [7, 8]. Here, we immediately have simplex constraints, i.e., \u2206+1 with no-short positions and \u22061 with short positions due to limited capital resources. Moreover, we typically need solutions sparser than the ones with the simplex constraint due to two reasons. The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8]. The second reason is cost: transactions fees increase with sparsity. Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].\nContributions: Within this context, our contributions can be summarized as follows:\n1. We prove a new result that states greedy sparse selection followed by standard simplex projections provides efficient and optimal sparse Euclidean projections onto \u2206+\u03bb .\n2. We introduce a new projection algorithm for efficiently obtaining approximate sparse Euclidean projections onto \u2206\u03bb. This new scheme is based on a principled, iterative selection method that searches over relaxations of the general simplex constraint to obtain the best approximate projection. We establish the exact convergence of the scheme, and show that we can certify the optimality of our projections or obtain approximation guarantees online.\n3. We apply our projections in iterative optimization algorithms to real-life and synthetic examples. In Markowitz portfolio selection problems, we can reduce transaction costs while still improving average return. In mixture density learning, our results naturally reveal the underlying model order. Finally, in general simplex projection problems, our approach outperforms the state-of-the-art branch-and-bound approaches by three orders of magnitude in efficiency even in small size problems, while obtaining exactly the same solutions.\nTo the best of our knowledge, sparse Euclidean projections onto the simplex constraints have not been considered before. The closest works to ours are the papers [12, 13], where the authors recover a vector \u03b2 in regression setting, where \u03b2 is already sparse and within a convex norm-ball constraint. In this paper, we consider the problem of projecting an arbitrary given vector w onto convex-based and sparse constraints, jointly. While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.\nNotation: Given a set S \u2286 N = {1, . . . , p}, the complement Sc is defined with respect to N , and the cardinality is written |S|. The support set of w is defined as supp(w) = {i : wi 6= 0}. Given a vector w \u2208 Rp, wS is either the projection (in Rp) of w onto S, i.e. (wS)Sc = 0, or a vector in R|S| depending on context. The all-ones column vector is 1, with dimensions apparent from the context. We define \u03a3s as the set of all s-sparse subsets of N , and we sometimes write \u03b2 \u2208 \u03a3s to mean supp(\u03b2) \u2208 \u03a3s."}, {"heading": "2 Preliminaries", "text": "Problem statement: In this paper, we mainly study the following problem and its variant:\nProblem 1. Given a vector w \u2208 Rp, find a Euclidean projection of w onto the intersection of s-sparse signals and the standard simplex \u2206+\u03bb :\n\u03b2? \u2208 arg min \u03b2:supp(\u03b2)\u2208\u03a3s,\u03b2\u2208\u2206+\u03bb \u2016\u03b2 \u2212 w\u201622. (1)\nProblem 2. Same as Problem 1 but use the general simplex \u2206\u03bb instead of \u2206+\u03bb .\nRemark 1. Problems 1\u20132 are special instances of mixed-integer programming and are in general combinatorially hard. However, there exist well-built solvers relying on good branch-and-bound heuristics, such as CPLEX [14]. In the sequel, we prove that Problem 1 can be solved optimally. Therefore, we use CPLEX as a benchmark against our relaxed solution to Problem 2.\nProjections: We will write P\u03bb for the projector onto either the standard or general simplex (it will be clear from context which we mean). Without loss of generality, let w be sorted in descending order so that w1 is the largest element, and write [wi]+ to mean max(wi, 0).\nDefinition 1 (Greedy basis algorithm P\u03a3s ). The greedy basis algorithm calculates the optimal projection P\u03a3s onto \u03a3s itself in O(pmax(s, log(p)))-time, by picking the s-largest entries of w. The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15]. This particular operation is also known as hard thresholding.\nDefinition 2 (Projection onto the standard simplex P\u03bb). The projector onto the standard simplex is given by\n(P\u03bb(w))i = [wi \u2212 \u03c4 ]+ where \u03c4 = \u03c4\u03c1, \u03c4j := 1\nj ( j\u2211 i=1 wi \u2212 \u03bb ) and \u03c1 := max{j : wj > \u03c4j}. (2)\nThe projector onto the convex hull of the standard simplex (that is, require \u2211 \u03b2i \u2264 \u03bb rather than \u2211 \u03b2i = \u03bb) is\nwritten P\u0304\u03bb and is identical except we set replace \u03c4 with [\u03c4 ]+.\nDefinition 3 (Projection onto the general simplex P\u03bb). The projector onto the general simplex is [16]\n(P\u03bb(w))i = wi + \u03c4 where \u03c4 = 1\np (\u03bb\u2212 p\u2211 i=1 wi). (3)\nA reformulation: The following observation is convenient in the sequel:\nRemark 2. The Problem 1 statement (1) can be equivalently transformed into the following nested minimization problem:\n{S?, \u03b2?S?} = arg min S:S\u2208\u03a3s\n[ min\n\u03b2:\u03b2S\u2208\u2206+\u03bb , \u03b2Sc=0\n\u2016(\u03b2 \u2212 w)S\u201622 + \u2016(w)Sc\u201622 ] . (4)\nwhere supp(\u03b2?) = S? and \u03b2? \u2208 \u2206+\u03bb ."}, {"heading": "3 Efficient sparse projections onto the simplex", "text": "Let \u03b2? be a projection of w onto \u03a3s \u2229 K\u03bb, where K\u03bb is either \u2206+\u03bb or \u2206\u03bb. Let C(\u03b2) be a mapping that encodes the constraint K\u03bb. For instance, C(\u03b2) = \u2211 i \u03b2i so C(\u03b2) = \u03bb evaluates the simplex constraint.\nWe first make an elementary observation based on Remark 2. Given S? = supp(\u03b2?), we can find \u03b2? by projecting wS? onto K\u03bb within the s-dimensional space. Thus, the difficulty is finding S?.\nAlgorithm 1 suggests an obvious greedy approach. We select the set S? by naively projecting w onto \u03a3s. Remarkably, this gives the correct support set for Problem 1, as we prove in \u00a74.1. After finding the support, we project (restricted to the support) back to K\u03bb. We call this algorithm the greedy selector and simplex projector (GSSP).\nAlgorithm 1: Greedy selector and simplex projector (GSSP) 1: S? = supp(P\u03a3s(w)) {Select support} 2: (\u03b2)S? = P\u03bb(wS?), (\u03b2)S?,c = 0 {Final projection}\nUnfortunately, the greedy approach fails for Problem 2. Here we propose Algorithm 2 which is non-obvious. The algorithm first projects w onto K\u03c4 , which we call a relaxed version of K\u03bb since it is possible that \u03c4 6= \u03bb. Projecting once more onto \u03a3s gives\n\u03b2\u0302\u03c4 := P\u03a3s(P\u03c4 (w)).\nLet C(\u03c4) := C(\u03b2\u0302\u03c4 ). If C(\u03c4) = \u03bb then \u03b2\u0302\u03c4 is feasible. In \u00a74.2, we will prove that if \u03c4 is chosen such that C(\u03c4) = \u03bb, then remarkably supp(\u03b2\u0302\u03c4 ) = supp(\u03b2?) for some optimal solution \u03b2?. Finding this correct value of \u03c4 can be efficiently done with a bisection search, leading to an algorithm with O(ps log s) complexity. When there is no \u03c4 such that C(\u03c4) = \u03bb, the algorithm still provides an estimate which has a provable bound on its sub-optimality. Due to overwhelming numerical evidence, we also conjecture that supp(\u03b2\u0302\u03c4 ) is optimal in this case as well: in low-dimensional tests, the algorithm has always returned the exact global solution.\nAlgorithm 2: Relaxed selector and simplex projector (RSSP)\n1: t? = arg mint |\u03bb\u2212 C(\u03b2\u0302t)| {One-dimensional optimization using relaxed constraints} 2: S? = supp(\u03b2\u0302t?) {Select support} 3: (\u03b2)S? = P\u03bb(wS?), (\u03b2)S?,c = 0 {Final projection}"}, {"heading": "4 Main results", "text": "Remark 3. When the symbol S is used as S = supp(\u03b2\u0304) where \u03b2\u0304 = P\u03a3s(w\u0304) for any w\u0304, then if |S| < s, we enlarge S until it has s elements by taking the first s \u2212 |S| elements that are not already in S and defining \u03b2\u0304 = 0 on these elements. The lexicographic approach is used to break ties when there are more than one solution to Problem 1."}, {"heading": "4.1 Projection onto the standard simplex with cardinality constraints", "text": "Theorem 1. Algorithm 1 exactly solves Problem 1.\nProof. By Remark 2, we split the problem into the task of finding the support and then finding the values on the support. Given any support S, the unique corresponding estimator is \u03b2\u0302S = P\u03bb(wS).\nWe may conclude that \u03b2? satisfies (\u03b2?)S? = (P\u03bb(w))S? and (\u03b2?)(S?)c = 0, where\nS? \u2208 arg min S:S\u2208\u03a3s \u2016(P\u03bb(w))S \u2212 w\u20162 = arg max S:S\u2208\u03a3s\n[ \u2016(w)S\u201622 \u2212 \u2016(P\u03bb(w)\u2212 w)S\u201622 ] = arg max S:S\u2208\u03a3s \u2211 i\u2208S ( w2i \u2212 ((P\u03bb(w))i \u2212 wi)2\n) = arg max S:S\u2208\u03a3s F (S) (5)\nwith F (S) := \u2211 i\u2208S ( w2i \u2212 ((P\u03bb(w))i \u2212 wi)2 ) . We now introduce a simple yet powerful lemma.\nLemma 1. Let \u03b2 = P\u03bb(w) where \u03b2i = [wi \u2212 \u03c4 ]+. Then wi \u2265 \u03c4 for all i \u2208 S = supp(\u03b2). Furthermore, \u03c4 = 1|S| (\u2211 i\u2208S wi \u2212 \u03bb ) .\nProof. Directly from the definition of \u03c4j in (2). The intuition is quite simple: the \u201cthreshold\u201d \u03c4 should be smaller than the smallest entry in the selected support. Otherwise, we unnecessarily shrink the coefficients that are larger without introducing any new support to the solution.\nLet S be any support set, and let |S| = l \u2264 s. If S is not chosen greedily, then there exists some wk /\u2208 S and some wj \u2208 S such that wk > wj . Define S\u0302 = (S \\ {j}) \u222a {k}. We will show that F (S\u0302) \u2265 F (S), and therefore it is never\nharmful to keep adding the largest elements of w, and hence the greedy set is at least as good as all other possible sets. For convenience, define r = \u2211 i\u2208S wi and r\u0302 = \u2211 i\u2208S\u0302 wi. First, we note that\nF (S) = \u2211 i\u2208S (w2i \u2212 \u03c42) = (\u2211 i\u2208S w2i ) \u2212 1 l (r \u2212 \u03bb)2\nwhich follows from Lemma 1. Now,\nF (S\u0302)\u2212 F (S) = (w2k \u2212 w2j )\u2212 1\nl\n( (r\u0302 \u2212 \u03bb)2 \u2212 (r \u2212 \u03bb)2 ) = (wk \u2212 wj)(wk + wj)\u2212 1\nl (r\u0302 \u2212 r)(r\u0302 + r \u2212 2\u03bb)\n\u2265 (wk \u2212 wj) [(wk \u2212 (r\u0302 \u2212 \u03bb)/l) + (wj \u2212 (r \u2212 \u03bb)/l)] \u2265 0\nsince wk \u2212 wj > 0 and wk \u2265 (r\u0302 \u2212 \u03bb)/l and wj \u2265 (r \u2212 \u03bb)/l by Lemma 1.\nRemark 4. The same algorithm works for projecting onto the convex hull of the standard simplex, since forcing \u03c4 \u2265 0 does not change the proof."}, {"heading": "4.2 Projection onto the general simplex with cardinality constraints", "text": "We are now in position to specialize Algorithm 2 to project onto \u2206\u03bb \u2229 \u03a3s.\nTheorem 2. For all \u03bb except on a set \u039b of finite measure, Algorithm 2 computes the projection of w onto \u2206\u03bb \u2229 \u03a3s exactly. If \u03bb \u2208 \u039b, the algorithm returns a solution with online optimality guarantees.\nEmpirically, the algorithm is exact even when \u03bb \u2208 \u039b, but we defer this proof for later work. The rest of this subsection proves the theorem.\nConsider the nested form of the problem, as in (4). Given a support set S, the inner minimization in (4) has an analytical solution as a function of \u03b2 using (3):\n\u03b2S = wS + \u03c4(S) \u00b7 1S , (6)\nwith \u03c4(S) := (\u03bb\u22121 T SwS) |S| \u2208 R. We sometimes write \u03c4 when the set S is clear from context. Replacing (6) in (4), we obtain the following subset selection problem:\nS? \u2208 arg min S:|S|\u2264s\n[ \u2016\u03c4 \u00b7 1S\u201622 + \u2016(w)Sc\u201622 ] = arg max S:|S|\u2264s [ \u2016w\u201622 \u2212 ( \u2016\u03c4 \u00b7 1S\u201622 + \u2016(w)Sc\u201622 )] = arg max S:|S|\u2264s \u2211 i\u2208S ( w2i \u2212 \u03c42 ) . (7)\nWrite F (S) = \u2211 i\u2208S ( w2i \u2212 \u03c42 ) , and thus we seek S \u2208 \u03a3s that maximizes F (S).\nWrite \u03b2\u03c4 = w + \u03c4 , so that \u03b2\u03c4 = PK\u03bb\u03c4 (w) for some \u03bb\u03c4 , depending on \u03c4 . Then, let \u03b2\u0302\u03c4 = P\u03a3s(\u03b2\u03c4 ) be the s-sparse projection of \u03b2\u03c4 , and define C(\u03c4) = 1T \u03b2\u0302\u03c4 . Algorithm 2 suggests that finding the minimizer of |C(\u03c4)\u2212 \u03bb| will define the correct support. We start with a lemma about C(\u03c4).\nLemma 2. The map C(\u03c4) is monotonically increasing in \u03c4 , and is piecewise linear apart from s \u201cturning points\u201d \u03c4k, k = 1, . . . , s, where it is multi-valued.\nProof. Fix \u03c4 , and let \u03c3(j) be a permutation such that |w\u03c3(1) + \u03c4 | \u2265 |w\u03c3(2) + \u03c4 | \u2265 . . . \u2265 |w\u03c3(n) + \u03c4 |. Let S = {\u03c3(1), . . . , \u03c3(s)}, so PS(\u03b2\u03c4 ) is a valid projection of \u03b2\u03c4 onto \u03a3s. Thus a value for C(\u03c4) is C(\u03c4) = \u2211 i\u2208S(wi + \u03c4). The set S need not be unique; if w\u03c3(s) = w\u03c3(s+1), then there there are other valid S but it will result in the same value\nof C(\u03c4), and if w\u03c3(s) 6= w\u03c3(s+1) but |w\u03c3(s) + \u03c4 | = |w\u03c3(s+1) + \u03c4 | then there is another valid S that gives a different value of C(\u03c4). This latter case only happens at s-many points which we call \u201cturning points.\u201d\nNow examine C(\u03c4+\u2206\u03c4) for any \u2206\u03c4 > 0. If there is no turning point \u03c4k \u2208 [\u03c4, \u03c4+\u2206\u03c4 ], then C(\u03c4+\u2206\u03c4)\u2212C(\u03c4) = s\u2206\u03c4 . If there is a turning point, assume \u2206\u03c4 is sufficiently small so that only one turning point, \u03c4k, is in [\u03c4, \u03c4 + \u2206\u03c4 ]. Let S be the index set corresponding to \u03c4 ; then the index set for \u03c4 + \u2206\u03c4 is the same, except it has an additional index w(new) and it lacks some index w(old). Then C(\u03c4 + \u2206\u03c4) \u2212 C(\u03c4) = s\u2206\u03c4 + w(new) \u2212 w(old). Since w(new) has been added, this means |w(new) + \u03c4 + \u2206\u03c4 | > |w(new) + \u03c4 | and thus w(new) > 0. Likewise, w(old) < 0, and hence C(\u03c4 + \u2206\u03c4)\u2212 C(\u03c4) > 0, so C is a monotonically increasing function.\nThe s turning points are easily found by sorting w (so assume w1 \u2265 w2 \u2265 . . .) and finding the points \u03c4 such that |wk + \u03c4 | = |wp\u2212s+k + \u03c4 | for k = 1, . . . , s, i.e. \u03c4k = (w2(p\u2212s+k) \u2212 w 2 k)/(2(wk \u2212 wp\u2212s+k)).\nNow, we approach the problem from the combinatorial side. Let S\u0302 be the unique support defined by any \u03c4\u0302(S\u0302) that is not a turning point (for simplicity, we exclude the possibility that w has identical entries). The support set does not change as \u03c4\u0302(S\u0302) changes except when \u03c4\u0302(S\u0302) crosses a turning point. If S\u0302 is fixed, then consider projecting wS\u0302 onto \u2206\u03bb\u03c4\u0302(S\u0302) where \u03bb\u03c4\u0302(S\u0302) = s\u03c4\u0302(S\u0302) + 1 T S\u0302 wS\u0302 . Here, we use Remark 3, where |S\u0302| = s. The nonzero entries of \u03b2 over S\u0302 are given by (6) for\n\u03c4\u0302(S\u0302) = \u03bb\u03c4\u0302(S\u0302) \u2212 1 T S\u0302 wS\u0302\ns .\nThe algorithm proceeds by picking \u03c4 \u2208 (\u03c4k, \u03c4k+1), with the convention that \u03c40 = \u2212\u221e and \u03c4s+1 = +\u221e, which defines a support S\u03c4 and hence \u03bb\u03c4 := C(\u03c4). We search for arg min\u03c4 |\u03bb \u2212 \u03bb\u03c4 |. Because C(\u03c4) is monotonic, we can compute bounds on \u03bb\u03c4 using the bisection method. Let \u03c4l and \u03c4u be values of \u03c4 such that \u03bbl = \u03bb\u03c4l < \u03bb < \u03bb\u03c4u = \u03bbu.\nUsing (7), the optimal s-sparse set can be found by maximizing F (S). The final step in our proof shows that \u03bb\u03c4 ' \u03bb implies F (S\u03c4 ) ' F (S?).\nLemma 3. If C(\u03c4) := C(\u03b2\u0302\u03c4 ) = \u03bb, then \u03b2\u0302\u03c4 = \u03b2?.\nProof. Since \u03b2\u0302\u03c4 is defined as a projection of w + \u03c4 , we have\n\u03b2\u0302\u03c4 = arg min \u03b2\u2208\u03a3s\n\u2016\u03b2 \u2212 (w + \u03c41)\u201622\n= arg min \u03b2\u2208\u03a3s,C(\u03b2)=\u03bb\u03c4\n\u2016\u03b2 \u2212 (w + \u03c41)\u201622 since C(\u03b2\u0302\u03c4 ) = \u03bb\u03c4\n= arg min \u03b2\u2208\u03a3s,C(\u03b2)=\u03bb\u03c4 \u2016\u03b2 \u2212 w\u201622 + \u2016\u03c41\u20162 \u2212 2\u03c41T (\u03b2 \u2212 w)\ufe38 \ufe37\ufe37 \ufe38 c .\nBecause of the constraint C(\u03b2) = \u03bb\u03c4 , then 1T\u03b2, and hence c, is a constant over the feasible set, and so this is the same as Problem 2 using \u03bb\u03c4 . Thus if \u03bb\u03c4 = \u03bb, we have solved the original problem.\nLemma 4. Let S\u0302 = supp(\u03b2\u0302t?) be the solution index set of t? = arg min\u03c4 |\u03bb \u2212 \u03bb\u03c4 |, and \u03bb\u0302 = \u03bb\u03c4 . Define S\u0302 = G(\u03bb, \u03bb\u0302)/F (S\u0302) and G(\u03bb, \u03bb\u0302) = \u03bb\n2\u2212\u03bb\u03022 s + 2(\u03bb\u0302\u2212\u03bb) s \u2211 i\u2208S? wi. Then, we have the following approximation guarantee\nF (S\u0302) \u2265 ( 1\u2212 S\u0302 ) F (S?). (8)\nProof. We observe the following:\nF (S\u0302, w) = \u2211 i\u2208S\u0302 [w]2i \u2212 1 s\n( \u03bb\u0302\u2212\n\u2211 i\u2208Sl [w]i\n)2\n\u2265 \u2211 i\u2208S? [w]2i \u2212 1 s\n( \u03bb\u0302\u2212\n\u2211 i\u2208S? [w]i\n)2\n= \u2211 i\u2208S? [w]2i \u2212 1 s\n( \u03bb\u0302+ \u03bb\u2212 \u03bb\u2212\n\u2211 i\u2208S? [w]i\n)2\n= F (S?, w)\u2212 2\u03bb(\u03bb\u0302\u2212 \u03bb) s + 2(\u03bb\u0302\u2212 \u03bb) s \u2211 i\u2208S? [w]i \u2212 (\u03bb\u0302\u2212 \u03bb)2 s\n= F (S?, w) + \u03bb 2 \u2212 \u03bb\u03022\ns + 2(\u03bb\u0302\u2212 \u03bb) s \u2211 i\u2208S? [w]i,\nwhere we simply use the fact that given \u03bb\u0302, the support S\u0302 obtains the maximum objective.\nWe obtain S\u0302 values to be much smaller than 1 after the termination of RSSP (e.g., typical values for the simplex projection experiments in Section 5.3 vary between 10\u22124 to 10\u22127). Given this epsilon value for RSSP, it is possible to obtain an online approximation guarantee for iterative optimization algorithms that can use RSSP (cf., [12]).\nRemark 5. The time complexity of the algorithm is O(ps log2(s)) because we need only test \u03c4 that define unique support, and there are s + 1 of these. Because C is monotonic, we can test just log2(s + 1) supports by using the bisection method. For each support, we compute a projection onto \u03a3s which takes O(ps) complexity.\nRemark 6. We conjecture that it is possible to relax the constraint from \u2211 \u03b2i = \u03bb (which is non-convex) to \u2211 \u03b2i \u2264 \u03bb (which is convex), but we leave a proof for future work."}, {"heading": "5 Experiments", "text": ""}, {"heading": "5.1 Portfolio optimization", "text": "Given a sample covariance matrix \u03a3 and mean \u00b5, the Markowitz mean-variance (MV) framework selects a portfolio \u03b2? = Rp (without short positions) via \u03b2? \u2208 arg min\u03b2\u2208\u2206+1 [ \u03b2T\u03a3\u03b2 \u2212 \u03b1\u00b5T\u03b2 ] , where \u2206+1 encodes the normalized capital constraint, and \u03b1 trades off risk and return [7, 8]. The solution \u03b2? \u2208 \u2206+1 indicates the fractional investments over the p available assets. In mathematical terms, this leads to the following quadratic optimization problem:1\n\u03b2? = arg min \u03b2\u2208\u2206+1\n[ \u03b2T\u03a3\u03b2 \u2212 \u03b1\u00b5T\u03b2 ] , (9)\nfor a given regularization parameter \u03b1 \u2265 0. In practice, the preferences of the investor may lead to further constraints in the optimization problem. Additional fees for asset trading (transaction costs) and costs of monitoring and portfolio re-weighting naturally lead to cardinality constraints in the optimization procedure. This additional flavor leads to mixed integer quadratic programming formulation which is difficult to solve by standard optimization techniques. Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein. Most of the investigations on cardinality\n1There is an extensive list of works in the literature where the mean-variance efficient portfolios are compared with global minimum variance portfolios. The primary reason is that estimating the expected return from past data is a difficult task where large outliers lead to imprecise estimations. We refer the reader to [7, 17] for an excellent discussion.\nconstrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).\nHere, we are interested in the MV optimization with the added twist that the solution satisfies \u03b2? \u2208 \u03a3s:\n\u03b2? \u2208 arg min \u03b2\u2208\u2206+1 \u2229\u03a3s\n[ \u03b2T\u03a3\u03b2 \u2212 \u03b1\u00b5T\u03b2 ] , (10)\nfor a given level of sparsity s. Out-of-sample performance: We use a publicly available dataset compiled by Farma and French2. In this dataset, we monitor 49 diverse industry assets and consider both monthly and daily recordings. Procedure: We evaluate the out-of-sample performances of the estimated portfolios over various time periods. For instance, during each year from 1971 to 2011, we estimate expected monthly returns of the stocks and their covariance values using the available data from the preceding 5 years. Finally, we evaluate the estimated portfolio \u03b2? by computing the monthly returns and risks each year keeping \u03b2? fixed. The supplementary material has daily return values as well as a synthetic experiment to also illustrate Pareto trade-offs achieved by sparsity assumptions.\nWe compare the following approaches: (i) the constrained quadratic optimization as described in (10) without the cardinality constraint \u03a3s using quadprog in MATLAB [22], (ii) the cardinality-constrained projected gradient descent algorithm that solves (10) using GSSP for s = {4, 10} and, (iii) the naive 1/p-strategy where we use the same weight over the portfolio, i.e. \u03b2i = 1/p for all i.\nResults: We provide some typical return evaluations with \u03b1 = 1 in Table 1 (as \u03b1 varies, the results qualitatively remain the same). Our approach with GSSP performs quite well, especially for smaller active portfolio sizes as constrained by s.3 We observe that as s decreases, the expected return \u00b5\u0302 as well as the standard deviation \u03c3\u0302 of the returns increase. Surprisingly, the GSSP solutions exhibit competitive Sharpe ratios \u00b5\u0302/\u03c3\u0302, which measures the risk adjusted return, as compared to the MV portfolio, and with much lower transactions costs. Overall, the quadratic programming approach has a median sparsity level of 14 and a mean sparsity level of 14.78. The 1/p baseline strategy has the worst returns and worst Sharpe ratios for most years.\nInterestingly, the naive baseline strategy does well in recession years like \u201981 to \u201986 and \u201906 to \u201911. In these years, presumably the model (\u03a3, \u00b5) is less accurate, and hence the quadratic programming solution does much worse than the naive strategy. The sparsity-constrained solution does better than the quadratic programming solution, suggesting that sparsity helps buffer against inaccurate models.\nEfficient frontier with cardinality constraints: To provide further numerical evidence, we generate random expected returns and covariance quantities of p = 100 assets. In Figure 2, we depict the unconstrained Pareto frontier\n2 http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html 3Moreover, our approach is extremely efficient as explored in supplementary material.\nby solving the optimization problem (9). Without any cardinality constraints, the MV framework suggests dense portfolio solutions for low risk investments (additional selections lower the risk) while sparser solutions can be obtained for riskier investments. In practice, dense portfolios are difficult to administrate and have higher transactions costs. To this end, we propose sparser portfolio strategies using a projected gradient descent solver for (10) where we use GSSP. The corresponding frontiers are depicted in Figure 2 for various s.\nAdditional comparisons: We perform a comparative study between the commercial toolbox CPLEX Studio V12.3 [14] using YALMIP [23] and our positive simplex, cardinality constrained, projected gradient method. The table in Figure 2 depicts a summary of the results. Even within this relatively small-scale configuration, our projected gradient descent algorithm admits fast performance (due to the first-order gradient statistics) and almost equivalent estimation performance compared to the state-of-the-art CPLEX algorithm."}, {"heading": "5.2 Sparse kernel density estimation", "text": "Let x(1), x(2), . . . , x(n) \u2208 Rp be a n-size corpus of p-dimensional samples, drawn from an unknown probability density function (pdf) f(x). Given this dataset, we are interested in estimating f(x). We focus on kernel estimation techniques that employ the Integrated Squared Error (ISE) criterion, ISE = E\u2016f\u0302(x) \u2212 f(x)\u201622, to design a weight vector \u03b2? \u2208 \u2206+1 such that f\u0302(x) := \u2211n i=1 \u03b2 ? i \u03ba\u03c3(x, x (i)) and f\u0302(x) minimizes the ISE. The resulting problem can be\nwritten as follows [10, 24]\n\u03b2? \u2208 arg min \u03b2\u2208\u2206+1\n[ \u03b2T\u03a3\u03b2 \u2212 cT\u03b2 ] , (11)\nwhere \u03ba\u03c3(x, y) is a Gaussian kernel, \u03a3 \u2208 Rn\u00d7n with \u03a3ij = \u03ba\u221a2\u03c3(x(i), x(j)) and c \u2208 Rn\u00d71 with\nci = 1 n\u2212 1 \u2211 j 6=i \u03ba\u03c3(x (i), x(j)), \u2200i, j (12)\nWhile the term \u2212cT\u03b2 induces sparsity in the solution for \u03b2i \u2265 0,\u2200i, in several cases \u03b2? is not sparse enough even if f(x) is a linear combination of a few components; even sparser weight solutions are required for instance to avoid overfitting or obtain interpretable results. In this context, we extend (11) to include cardinality constraints, i.e. \u03b2? \u2208 \u2206+1 \u2229 \u03a3s.\nSparse estimation of Gaussian mixtures: We consider the following one-dimensional mixture of 5 Gaussians: f(x) = 15 \u22115 i=1 \u03ba\u03c3i(\u00b5i, x) where \u03c3i = (7/9)\ni and \u00b5i = 14(\u03c3i\u22121). A sample of 1000 points is drawn from f(x). We compare the density estimation performance of: (i) the Parzen window method [25], (ii) the quadratic programming formulation in (11), and (iii) our cardinality-constrained version of (11) using GSSP. While f(x) is constructed by kernels with various widths, we assume a constant width during the kernel estimation. In practice, the width is not known a priori but can be found using cross-validation techniques, i.e., leave-one-out technique; for simplicity, we assume kernels with width \u03c3 = 1.\nFigure 3(a) depicts the true pdf and the estimated densities using the Parzen window method and the quadratic programming approach. Moreover, the figure also include a scaled plot of 1/\u03c3i, indicating the height of the individual Gaussian mixtures. By default, the Parzen window method estimation interpolates 1000 Gaussian kernels with centers\naround the sampled points to compute the estimate f\u0302(x); unfortunately, neither the quadratic programming approach (as Figure 3(b) illustrates) nor the Parzen window estimator results are easily interpretable while both approaches provide a good approximation of the true pdf.\nIn stark contrast, using our cardinality-constrained approach, we can significantly enhance the interpretability. This is because the sparsity-constrained approach approximately reveals the number of Gaussian components. To see this, we first show the coefficient profile of the proposed approach for s = 5 in Figure 3(b) (middle-bottom). Moreover, Figure 3(c) shows the estimated pdf for s = 5 along with the positions of weight coefficients obtained by our sparsity enforcing approach. Note that most of the weights obtained concentrate around the true means, providing some prior information about the ingredients of f(x)\u2014this happens with rather high frequency in the experiments we conducted. Figure 4 illustrates further estimated pdf\u2019s using our approach for various s. From the plot, it is also clear that if s > 5, the resulting solutions are still approximately 5-sparse as the over estimated coefficients exhibit extremely small values."}, {"heading": "5.3 Sparse projections onto the general simplex", "text": "While the general simplex constraints also have applications with Markowitz portfolio selection, it is better to use synthetic data to numerically benchmark the quality and efficiency of our projection onto the general simplex \u2206\u03bb. Here, we generate random vector realizations w \u2208 Rp with p = {102, 104} and compare the projection performance of the following approaches over various s and \u03bb: (i) the GSSP on \u03a3s\u2229\u2206\u03bb, (ii) the proximal alternating minimization projection (PAMP) [26], (iii) the commercial mixed integer quadratic programming toolbox CPLEX Studio V12.3 [14], and (iv) the RSSP on \u03a3s \u2229 \u2206\u03bb. The PAMP approach is based on a general proximal scheme that tries to solve a coupled problem and requires turning parameters. For a fair comparison, we tuned PAMP parameters for best performance; we note that deviations from this fine tuning leads to performance degradation.\nTable 2 depicts the efficiency of our approaches where both GSSP and RSSP efficiently decrease the distance to the original vector w compared to state-of-the-art algorithms with total computational costs that scale well as p increases. Note that while RSSP requires more computation, its solution quality is consistently (and strictly, in our experiments) better, where the difference can be quite large. Some illustrative instance is depicted in Figure 5. In Figure 5(b), let \u03b2?i , i \u2208 I = {GSSP,RSSP,Prox.Alter.} be the solutions obtained by the algorithms in comparison and let ei,j denote the distance \u2016\u03b2?i \u2212w\u20162, i \u2208 I at the j-th MC iteration. Then, we use the performance profile notion [27] where \u03c1(\u03b1) is the probability for a solver that the performance ratio ei,jmin{ei,j :i\u2208I} is within a factor \u03b1 \u2208 R of the best possible ratio. Note that RSSP performs exactly the same as CPLEX within numerical precision for small size problems while being three orders of magnitude faster. The PAMP approach is quite competitive but it requires some level of tuning."}, {"heading": "6 Conclusions", "text": "While non-convexity in learning algorithms is undesirable according to conventional wisdom, avoiding it might be difficult in many problems. In particular, sparse learning problems with simplex constraints disallow the application of the `1-relaxation for sparsification of the solution. In this setting, we show how to efficiently obtain sparse projections onto positive and general simplex and sparsity constraints. To this end, we provide an exact sparse projector for the positive simplex constraints and a relaxation based approach for approximate sparse projections onto the general simplex constraints. Interestingly, the latter approach can feature online optimality guarantees. We also empirically demonstrate that our projectors provide substantial benefits in generalization and interpretability in sparse portfolio selection and density estimation applications."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the European Commission under Grant MIRG-268398, ERC Future Proof, SNF 200021 132548, and DARPA KeCoM program #11-DARPA-1055. VC also would like to acknowledge Rice University for his Faculty Fellowship, and SB would like to acknowledge the Fondation Sciences Mathe\u0301matiques de Paris for his fellowship."}], "references": [{"title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational learning theory, pages 23\u201337. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Statistics for High-Dimensional Data: Methods, Theory and Applications", "author": ["P. B\u00fchlmann", "S. Van De Geer"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Sparse approximate solutions to linear systems", "author": ["B.K. Natarajan"], "venue": "SIAM J. Comput., 24(2):227\u2013234", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms", "author": ["H. Attouch", "J. Bolte", "B.F. Svaiter"], "venue": "forward\u2013backward splitting, and regularized Gauss-Seidel methods. Mathematical Programming, pages 1\u201339", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "An O(n) algorithm for quadratic knapsack problems", "author": ["P. Brucker"], "venue": "Operations Research Letters, 3(3):163\u2013166", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1984}, {"title": "Efficient projections onto the `1-ball for learning in high dimensions", "author": ["J. Duchi", "S. Shalev-Shwartz", "Y. Singer", "T. Chandra"], "venue": "ICML", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "A generalized approach to portfolio optimization: Improving performance by constraining portfolio norms", "author": ["V. DeMiguel", "L. Garlappi", "F.J. Nogales", "R. Uppal"], "venue": "Management Science, 55(5):798\u2013812", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Sparse and stable Markowitz portfolios", "author": ["J. Brodie", "I. Daubechies", "C. De Mol", "D. Giannone", "I. Loris"], "venue": "Proceedings of the National Academy of Sciences, 106(30):12267\u201312272", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiscale poisson intensity and density estimation", "author": ["R.M. Willett", "R.D. Nowak"], "venue": "Information Theory, IEEE Transactions on, 53(9):3171\u20133187", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "SPADES and mixture models", "author": ["F. Bunea", "A.B. Tsybakov", "M.H. Wegkamp", "A. Barbu"], "venue": "The Annals of Statistics, 38(4):2525\u20132558", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust ensemble learning", "author": ["G. Ratsch", "B. Scholkopf", "A.J. Smola", "S. Mika", "T. Onoda", "K.R. Muller"], "venue": "Advances in Neural Information Processing Systems, pages 207\u2013220", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Combinatorial selection and least absolute shrinkage via the CLASH algorithm", "author": ["A. Kyrillidis", "V. Cevher"], "venue": "EPFL Technical Report", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Hard thresholding with norm constraints", "author": ["A. Kyrillidis G. Puy", "V. Cevher"], "venue": "In International Conference on Acoustics, Speech, and Signal Processing", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Integer and combinatorial optimization", "author": ["G.L. Nemhauser", "L.A. Wolsey"], "venue": "volume 18. Wiley New York", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Convex Optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Risk reduction in large portfolios: Why imposing the wrong constraints helps", "author": ["R. Jagannathan", "T. Ma"], "venue": "The Journal of Finance, 58(4):1651\u20131684", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Algorithm for cardinality-constrained quadratic optimization", "author": ["D. Bertsimas", "R. Shioda"], "venue": "Computational Optimization and Applications, 43(1):1\u201322", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Portfolio selection problems in practice: a comparison between linear and quadratic optimization models", "author": ["F. Cesarone", "A. Scozzari", "F. Tardella"], "venue": "Arxiv preprint arXiv:1105.3594", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Heuristics for cardinality constrained portfolio optimisation", "author": ["T.J. Chang", "N. Meade", "J.E. Beasley", "Y.M. Sharaiha"], "venue": "Computers and Operations Research, 27(13):1271\u20131302", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Hybrid search for cardinality constrained portfolio optimization", "author": ["M.A. Gomez", "C.X. Flores", "M.A. Osorio"], "venue": "Proceedings of the 8th annual conference on Genetic and evolutionary computation, pages 1865\u20131866. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "YALMIP: A toolbox for modeling and optimization in MATLAB", "author": ["J. Lofberg"], "venue": "Computer Aided Control Systems Design, 2004 IEEE International Symposium on, pages 284\u2013289. IEEE", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Least squares mixture decomposition estimation", "author": ["D. Kim"], "venue": "PhD thesis", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "On estimation of a probability density function and mode", "author": ["E. Parzen"], "venue": "The annals of mathematical statistics, 33(3):1065\u20131076", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1962}, {"title": "Alternating minimization and projection methods for nonconvex problems", "author": ["H. Attouch", "J. Bolte", "P. Redont", "A. Soubeyran"], "venue": "Arxiv preprint arXiv:0801.1780", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Benchmarking optimization software with performance profiles", "author": ["E.D. Dolan", "J.J. Mor\u00e9"], "venue": "Mathematical Programming, 91(2):201\u2013213", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 1, "context": "For instance, sparsity provably avoids over-fitting for better generalization of learning algorithms, and provably circumvents the ill-posed nature of regression problems [1, 2].", "startOffset": 171, "endOffset": 177}, {"referenceID": 2, "context": ", [3]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "Surprisingly, it is possible to obtain sparse critical points of general loss functions with the projected gradient algorithm [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The Euclidean projection onto the convex constraint \u2016\u03b2\u20161 \u2264 \u03bb can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions.", "startOffset": 111, "endOffset": 117}, {"referenceID": 5, "context": "The Euclidean projection onto the convex constraint \u2016\u03b2\u20161 \u2264 \u03bb can be efficiently obtained via soft-thresholding [5, 6], which automatically sparsifies solutions.", "startOffset": 111, "endOffset": 117}, {"referenceID": 1, "context": "Moreover, if f(\u03b2) is convex, then we can leverage decades of research in convex optimization, which not only provide computationally efficient algorithms to obtain accurate solutions, but also strong analytical tools to establish consistency and prediction efficiency of the solutions as \u03bb varies [2].", "startOffset": 297, "endOffset": 300}, {"referenceID": 6, "context": "As a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector \u03b2 that minimizes a return-adjusted empirical risk objective [7, 8].", "startOffset": 165, "endOffset": 171}, {"referenceID": 7, "context": "As a stylized example, consider the Markowitz portfolio selection, where we try to learn a design vector \u03b2 that minimizes a return-adjusted empirical risk objective [7, 8].", "startOffset": 165, "endOffset": 171}, {"referenceID": 6, "context": "The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8].", "startOffset": 135, "endOffset": 140}, {"referenceID": 7, "context": "The first reason is robustness: since empirical covariance and return estimates are rather noisy, sparser portfolios generalize better [7,8].", "startOffset": 135, "endOffset": 140}, {"referenceID": 8, "context": "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].", "startOffset": 81, "endOffset": 88}, {"referenceID": 9, "context": "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].", "startOffset": 81, "endOffset": 88}, {"referenceID": 10, "context": "Other example learning problems include sparse mixture/kernel density estimation [9, 10], and boosting/leveraging weak classifiers [11].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "The closest works to ours are the papers [12, 13], where the authors recover a vector \u03b2 in regression setting, where \u03b2 is already sparse and within a convex norm-ball constraint.", "startOffset": 41, "endOffset": 49}, {"referenceID": 12, "context": "The closest works to ours are the papers [12, 13], where the authors recover a vector \u03b2 in regression setting, where \u03b2 is already sparse and within a convex norm-ball constraint.", "startOffset": 41, "endOffset": 49}, {"referenceID": 11, "context": "While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.", "startOffset": 18, "endOffset": 25}, {"referenceID": 12, "context": "While the results [12,13] only apply to linear regression problems with the restricted isometry property, our projectors can be used in general minimization problems.", "startOffset": 18, "endOffset": 25}, {"referenceID": 11, "context": "The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15].", "startOffset": 95, "endOffset": 102}, {"referenceID": 13, "context": "The optimality of this algorithm follows from the matroid structure of cardinality constraints [12,15].", "startOffset": 95, "endOffset": 102}, {"referenceID": 14, "context": "The projector onto the general simplex is [16]", "startOffset": 42, "endOffset": 46}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "1 Portfolio optimization Given a sample covariance matrix \u03a3 and mean \u03bc, the Markowitz mean-variance (MV) framework selects a portfolio \u03b2 = R (without short positions) via \u03b2 \u2208 arg \u03b2\u2208\u22061 [ \u03b2\u03a3\u03b2 \u2212 \u03b1\u03bc\u03b2 ] , where \u22061 encodes the normalized capital constraint, and \u03b1 trades off risk and return [7, 8].", "startOffset": 285, "endOffset": 291}, {"referenceID": 7, "context": "1 Portfolio optimization Given a sample covariance matrix \u03a3 and mean \u03bc, the Markowitz mean-variance (MV) framework selects a portfolio \u03b2 = R (without short positions) via \u03b2 \u2208 arg \u03b2\u2208\u22061 [ \u03b2\u03a3\u03b2 \u2212 \u03b1\u03bc\u03b2 ] , where \u22061 encodes the normalized capital constraint, and \u03b1 trades off risk and return [7, 8].", "startOffset": 285, "endOffset": 291}, {"referenceID": 16, "context": "Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein.", "startOffset": 79, "endOffset": 87}, {"referenceID": 17, "context": "Numerous approaches have been proposed in the literature to solve this problem [18, 19] and references therein.", "startOffset": 79, "endOffset": 87}, {"referenceID": 6, "context": "We refer the reader to [7, 17] for an excellent discussion.", "startOffset": 23, "endOffset": 30}, {"referenceID": 15, "context": "We refer the reader to [7, 17] for an excellent discussion.", "startOffset": 23, "endOffset": 30}, {"referenceID": 16, "context": "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).", "startOffset": 184, "endOffset": 196}, {"referenceID": 18, "context": "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).", "startOffset": 184, "endOffset": 196}, {"referenceID": 19, "context": "constrained porfolio optimization focus on finding local solutions using greedy techniques, simulated annealing and evolution methods, genetic algorithms, and branch and bound ideas ( [18, 20, 21] and references therein).", "startOffset": 184, "endOffset": 196}, {"referenceID": 7, "context": "Red solid curve denotes the quadratic programming solution as obtained by (9) and blue squares represent a variation of `1-norm regularized solver in [8].", "startOffset": 150, "endOffset": 153}, {"referenceID": 20, "context": "3 [14] using YALMIP [23] and our positive simplex, cardinality constrained, projected gradient method.", "startOffset": 20, "endOffset": 24}, {"referenceID": 9, "context": "written as follows [10, 24]", "startOffset": 19, "endOffset": 27}, {"referenceID": 21, "context": "written as follows [10, 24]", "startOffset": 19, "endOffset": 27}, {"referenceID": 22, "context": "We compare the density estimation performance of: (i) the Parzen window method [25], (ii) the quadratic programming formulation in (11), and (iii) our cardinality-constrained version of (11) using GSSP.", "startOffset": 79, "endOffset": 83}, {"referenceID": 23, "context": "Here, we generate random vector realizations w \u2208 R with p = {10, 10} and compare the projection performance of the following approaches over various s and \u03bb: (i) the GSSP on \u03a3s\u2229\u2206\u03bb, (ii) the proximal alternating minimization projection (PAMP) [26], (iii) the commercial mixed integer quadratic programming toolbox CPLEX Studio V12.", "startOffset": 242, "endOffset": 246}, {"referenceID": 24, "context": "Then, we use the performance profile notion [27] where \u03c1(\u03b1) is the probability for a solver that the performance ratio ei,j min{ei,j :i\u2208I} is within a factor \u03b1 \u2208 R of the best possible ratio.", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "The past decade has seen the rise of `1-relaxation methods to promote sparsity for better interpretability and generalization of learning results. However, there are several important learning applications, such as Markowitz portolio selection and sparse mixture density estimation, that feature simplex constraints, which disallow the application of the standard `1-penalty. In this setting, we show how to efficiently obtain sparse projections onto the positive and general simplex with sparsity constraints. We provide an exact sparse projector for the positive simplex constraints, and derive a novel approach with online optimality and approximation guarantees for sparse projections onto the general simplex constraints. Even for small sized problems, this new approach is three orders of magnitude faster than the alternative, state-of-the-art branch-and-bound based CPLEX solver with no sacrifice in solution quality. We also empirically demonstrate that our projectors provide substantial benefits in portfolio selection and density estimation.", "creator": "LaTeX with hyperref package"}}}