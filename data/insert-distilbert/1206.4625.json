{"id": "1206.4625", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Optimizing F-measure: A Tale of Two Approaches", "abstract": "improved f - measures are popular performance metrics, particularly for elementary tasks with imbalanced data sets. algorithms for formal learning to maximize f - measures follow two approaches : the empirical utility maximization ( eum ) approach closely learns a classifier having optimal performance on training data, while the decision - theoretic approach learns a probabilistic regression model and then predicts labels with maximum expected f - measure. in this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach prediction is preferable to the other using extremely synthetic and real datasets. given accurate models, our results suggest that the two approaches are asymptotically equivalent especially given large training and test sets. nevertheless, empirically, the eum approach appears to be more robust against strict model misspecification, and given a good model, the decision - theoretic approach appears to be looking better for handling rare classes and a common domain adaptation scenario.", "histories": [["v1", "Mon, 18 Jun 2012 15:07:04 GMT  (378kb)", "http://arxiv.org/abs/1206.4625v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nan ye", "kian ming adam chai", "wee sun lee", "hai leong chieu"], "accepted": true, "id": "1206.4625"}, "pdf": {"name": "1206.4625.pdf", "metadata": {"source": "META", "title": "Optimizing F-Measures: A Tale of Two Approaches", "authors": ["Nan Ye", "Kian Ming A. Chai", "Wee Sun Lee", "Hai Leong Chieu"], "emails": ["yenan@comp.nus.edu.sg", "ckianmin@dso.org.sg", "leews@comp.nus.edu.sg", "chaileon@dso.org.sg"], "sections": [{"heading": "1. Introduction", "text": "F-measures (van Rijsbergen, 1974) or F-scores have been commonly used in tasks in which it is important to retrieve elements belonging to a particular class correctly without including too many elements of other\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nclasses. F-measures are usually preferred to accuracies as standard performance measures in information retrieval (Manning et al., 2008), particularly, when relevant items are rare. They are also popular in information extraction tasks such as named entity recognition (Tjong Kim Sang & De Meulder, 2003) where most of the elements do not belong to a named class.\nVarious methods have been proposed for optimizing Fmeasures. They fall into two paradigms. The empirical utility maximization (EUM) approach learns a classifier having optimal F-measure on the training data. Optimizing the F-measure directly is often difficult as the F-measure is non-convex. Thus approximation methods are often used instead. Joachims (2005) gave an efficient algorithm for maximizing a convex lower bound of F-measures for support vector machines, and showed it worked well on text classification. Jansche (2005) gave an efficient algorithm to maximize a non-convex approximation to F-measures using logistic regression models, and showed it works well on a text summarization problem. A simpler method is to optimize the F-measure in two stages: First learn a score function using standard methods such as logistic regression or support vector machines, then select a threshold for the score function to maximize the empirical F-measure. Though simple, this method has been found to be effective and is commonly applied, for example, in text categorization (Yang, 2001).\nThe decision-theoretic approach (DTA), advocated by Lewis (1995), estimates a probability model first, and then computes the optimal predictions (in the sense of having highest expected F-measure) according to the model. This method has not been commonly applied for F-measures, possibly due to the high computa-\ntional complexity of existing algorithms for the prediction step. Assuming the independence of labels, Lewis showed that, in the optimal prediction, the probabilities of being positive for irrelevant items are not more than those for relevant items. He also gave a bound for expected F-measures, which can be computed in O(n) time, but can be very loose. Based on Lewis\u2019s characterization, Chai (2005) gave an O(n3) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA. Apparently unaware of Chai\u2019s work, Jansche (2007) solved the same problem in O(n4) time. For the general case when the labels are not necessarily independent, Dembczynski et al. (2011) gave an O(n3) time algorithm given n2+1 parameters of the label distribution, but the parameters can be expensive to compute. They also showed that the independence assumption can lead to bad performance in the worst case, but on the practical datasets used in their experiments, methods assuming the independence assumption are at least as good as those not assuming independence.\nWe have only discussed works on binary classification. There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daume\u0301 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al., 2010; Petterson & Caetano, 2010).\nOptimality in EUM and DTA are different. EUM considers only instance classifiers (functions mapping instances to labels), and roughly speaking, an optimal classifier is an instance classifier having highest F-measure on a very large test set among all instance classifiers. On the other hand, DTA considers set classifiers (functions mapping sets of instances to sets of labels), and an optimal classifier in DTA is a set classifier having maximum expected F-measure among all set classifiers. Optimality in these two approaches are also achieved differently using different learning objectives. Unless otherwise stated, optimal classifiers refer to EUM-optimal classifiers, and optimal predictions refer to predictions by DTA-optimal classifiers.\nIn this paper, we study the relative effectiveness of the two approaches, and develop theories and algorithms for this purpose. We focus on binary classification, assuming the data is independently and identically distributed (i.i.d.). The contributions of this paper are as follows. In Section 2, we establish a consistency result for empirical maximization of F-measures, together with bounds on the rate of convergence. This provides some insights into the factors affecting the convergence rate in EUM. In particular, our bounds suggest that rare classes require more data for performance guar-\nantee, which is consistent with our intuition. We then show that thresholding the true conditional distribution on a large i.i.d. test set can perform as well as the best instance classifier, justifying the popular hybrid approach of learning a conditional distribution followed by learning a threshold. We also show that an EUM-optimal classifier and a DTA-optimal classifier are asymptotically equivalent if the probability measure for any set of instances with the same conditional probability of being relevant is negligible.\nIn Section 3, we give a new O(n2) time algorithm for computing optimal predictions, assuming independence of labels. Our algorithm can compute optimal predictions on tens of thousand instances within seconds, significantly faster than previous algorithms which require hours or more. 1\nIn Section 4, we compare EUM and DTA on synthetic and real datasets. Our theoretical results are useful in explaining the experimental results. Empirically, EUM seems more robust against model misspecification, but given a good model, DTA seems better for handling rare classes on small datasets and a common scenario of domain adaptation."}, {"heading": "2. Theoretical Analysis", "text": "Let X and Y denote the input and output random variables. We assume there is a fixed but unknown distribution P (X,Y ) that generates i.i.d. (X,Y ) pairs during training and testing. We use X and Y to denote their domains as well. In this paper, Y = {0, 1}, with 0 for the negative or irrelevant class and 1 for the positive or relevant class. I(\u00b7) is the indicator function. Let Dn = {(x1, y1), . . . , (xn, yn)} be a set of n (possibly non-i.i.d.) examples, and let x and y denote (x1, . . . , xn) and (y1, . . . , yn) respectively. If the predicted labels are s = (s1, . . . , sn), then precision p(s,y) is the number of true positives over the number of predicted positives, and recall r(s,y) is the number of true positives over the number of positives. F\u03b2measure (van Rijsbergen, 1974) F\u03b2(s,y) is a weighted harmonic mean of precision and recall. Formally,\nF\u03b2(s,y) = (1 + \u03b22)\n\u2211 i siyi\n\u03b22 \u2211 i yi + \u2211 i si , (1)\np(s,y) = \u2211 i siyi/ \u2211 i si and r(s,y) = \u2211 i siyi/ \u2211 i yi. Thus, F\u03b2 = (1 +\u03b2 2)/(\u03b22/r+p). In addition, F0 is the precision and F\u221e is the recall. F1 is most frequently used in practice. Henceforth, we assume \u03b2 \u2208 (0,\u221e).\n1See http://www.comp.nus.edu.sg/\u223cyenan/."}, {"heading": "2.1. Uniform Convergence and Consistency for EUM", "text": "Consider an arbitrary classifier \u03b8 : X 7\u2192 Y . Let F\u03b2,n(\u03b8) denote the F\u03b2 score of \u03b8 on Dn. Let pij,n(\u03b8) be the empirical probability that a class i instance is observed and predicted as class j by \u03b8; that is, pij,n(\u03b8) = \u2211n k=1 I(yk = i \u2227 \u03b8(xk) = j)/n. Then\nF\u03b2,n(\u03b8) = (1 + \u03b22)p11,n(\u03b8)\n\u03b22(p11,n(\u03b8) + p10,n(\u03b8)) + p11,n(\u03b8) + p01,n(\u03b8)\nLet pij(\u03b8) = E(I(Y = i\u2227\u03b8(X) = j)), that is, the probability that a class i instance is predicted as class j by \u03b8. Under the i.i.d. assumption, for large i.i.d. sample, the law of large numbers implies that pij,n(\u03b8)\u2019s converge to pij(\u03b8)\u2019s. Thus F\u03b2,n(\u03b8) is expected to converge to\nF\u03b2(\u03b8) = (1 + \u03b22)p11(\u03b8)\n\u03b22\u03c01 + p11(\u03b8) + p01(\u03b8) , (2)\nwhere \u03c0Y denotes P (Y ). Hence we can define this to be the F\u03b2-measure of the classifier \u03b8. The above heuristic argument is formalized below. We often omit \u03b8 from the notations whenever there is no ambiguity. All proofs are in the supplement (See foonote 1).\nLemma 1. For any > 0, lim n\u2192\u221e P(|F\u03b2,n(\u03b8)\u2212F\u03b2(\u03b8)| < ) = 1.\nBy using a concentration inequality, such as the Hoeffding\u2019s inequality, in place of the law of large numbers, we can obtain a bound on the convergence rate.\nLemma 2. Let r(n, \u03b7) = \u221a\n1 2n ln 6 \u03b7 . When r(n, \u03b7) <\n\u03b22\u03c01 2(1+\u03b22) , then with probability at least 1\u2212 \u03b7, |F\u03b2,n(\u03b8)\u2212 F\u03b2(\u03b8)| < 3(1+\u03b2 2)r(n,\u03b7) \u03b22\u03c01\u22122(1+\u03b22)r(n,\u03b7) .\nWe now show that training to maximize the empirical F\u03b2 is consistent, using VC-dimension (Vapnik, 1995) to quantify the complexity of the classifier class. Theorem 3. Let \u0398 \u2286 X 7\u2192 Y , d = V C(\u0398), \u03b8\u2217 = arg max\u03b8\u2208\u0398 F\u03b2(\u03b8), and \u03b8n = arg max\u03b8\u2208\u0398 F\u03b2,n(\u03b8).\nLet r\u0304(n, \u03b7) = \u221a\n1 n (ln 12 \u03b7 + d ln 2en d ). If n is such that\nr\u0304(n, \u03b7) < \u03b2 2\u03c01\n2(1+\u03b22) , then with probability at least 1\u2212 \u03b7, F\u03b2(\u03b8n) > F\u03b2(\u03b8 \u2217)\u2212 6(1+\u03b2 2)r\u0304(n,\u03b7)\n\u03b22\u03c01\u22122(1+\u03b22)r\u0304(n,\u03b7) .\nThe above bound indicates that for smaller \u03c01 and \u03b2, more samples are probably required for convergence to start occurring. When r(n, \u03b7) < \u03b2 2\u03c01\n4(1+\u03b22) , the difference\nbetween F\u03b2,n(\u03b8) and F\u03b2(\u03b8) is at most 6(1+\u03b22) \u03b22\u03c01 r(n, \u03b7)."}, {"heading": "2.2. Optimality of Thresholding in EUM", "text": "We now consider a common EUM approach: learning a score function and then using a fixed threshold\non the score function. This threshold is obtained by optimizing the F-measure on the training data.\nAssume we know the true conditional distribution P (Y |X). Consider the class T of probability thresholding classifiers of the form I\u03b4(x) = I(P (1|x) > \u03b4), and the class T \u2032 containing I\u2032\u03b4(x) = I(P (1|x) \u2265 \u03b4). 2 T \u222a T \u2032 has VC dimension 1, so empirical maximization of F-measure for this class is consistent. Although T \u222a T \u2032 does not contain all possible classifiers on X, an optimal classifier can be found in this class. Let t\u2217 = arg maxh\u2208T \u222aT \u2032 F\u03b2(h). Theorem 4. For any classifier \u03b8, F\u03b2(\u03b8) \u2264 F\u03b2(t\u2217).\nThresholding is often applied on a score function f : X 7\u2192 R, rather than on the true conditional distribution. For example, output of a support vector machine is commonly thresholded. Let f\u03b4(x) = I(f(x) > \u03b4) and f \u2032 \u03b4(x) = I(f(x) \u2265 \u03b4). Function f is called an optimal score function if there is a \u03b4 such that F\u03b2(f \u2032 \u03b4) = F\u03b2(t\n\u2217). We give a sufficient condition for a score function to be optimal. A score function f is rank-preserving if it satisfies f(x1) > f(x2) iff P (1|x1) > P (1|x2) for all x1, x2 \u2208 X. The sufficient condition relates rank-preservation to optimality:\nTheorem 5. A rank-preserving function is an optimal score function.\nBy Theorem 5, we can sidestep learning the true distribution and instead try to learn a function which is likely to be rank-preserving. An optimal score function may not be rank-preserving. For example, we can swap the scores of x\u2019s above the optimal threshold."}, {"heading": "2.3. An Asymptotic Equivalence Result", "text": "We now investigate the connections between EUMoptimal classifiers and DTA-optimal classifiers when the true distribution P (X,Y ) is known. By definition, a DTA-optimal classifier is expected to be better than an EUM-optimal classifier if tested on many i.i.d. test sets. We shall give an asymptotic equivalence result for EUM-optimal classifiers and DTA-optimal classifiers on large i.i.d test sets. In light of Theorem 4, we only need to consider an optimal probability-thresholding classifier as a representative EUM-optimal classifier.\nIn the following, let x = (x1, . . . , xn) \u2208 Xn be an i.i.d. sequence of observations. For any classifier \u03b8, let \u03b8(x) = (\u03b8(xi))i. All expectations, denoted by E(\u00b7), are taken under the conditional distribution P (y|x). The following theorem says that for an arbitrary classifier\n2 Any \u03b8 \u2208 T can be approximated by members in T \u2032 with arbitrary close F\u03b2 , and vice versa, but T \u2032 may contain \u03b8\u2032 such that F\u03b2(\u03b8) 6= F\u03b2(\u03b8\u2032) for all \u03b8 \u2208 T , implying T \u2032 6= T .\n\u03b8, when n is large enough, then for any x, the expected F-measure of \u03b8(x) is close to F\u03b2(\u03b8).\nTheorem 6. For any classifier \u03b8, any , \u03b7 > 0, there exists N\u03b2, ,\u03b7 such that for all n > N\u03b2, ,\u03b7, with probability at least 1\u2212 \u03b7, |E[F\u03b2(\u03b8(x),y)]\u2212 F\u03b2(\u03b8)| < .\nSuch approximation holds uniformly for the class T .3 Lemma 7. For any , \u03b7 > 0, there exists N\u03b2, ,\u03b7 such that for all n > N\u03b2, ,\u03b7, with probability at least 1\u2212 \u03b7, for all \u03b4 \u2208 [0, 1], |E[F\u03b2(I\u03b4(x),y)]\u2212 F\u03b2(I\u03b4)| < .\nThe above uniform approximation result leads to the following asymptotic equivalence result. Theorem 8. Let s\u2217(x) = maxs E[F\u03b2(s,y)], with s satisfying {P (1|xi) | si = 1} \u2229 {P (1|xi) | si = 0} = \u2205. Let t\u2217 = arg maxt\u2208T F\u03b2(t). Then for any , \u03b7 > 0, (a) There exists N\u03b2, ,\u03b7 such that for all n > N\u03b2, ,\u03b7, with probability at least 1 \u2212 \u03b7, E[F\u03b2(t\u2217(x),y)] \u2264 E(F\u03b2(s \u2217(x),y)) < E[F\u03b2(t \u2217(x),y)] + . (b) There exists N\u03b2, ,\u03b7 such that for all n > N\u03b2, ,\u03b7, with probability at least 1 \u2212 \u03b7, |F\u03b2(t\u2217(x),y) \u2212 F\u03b2(s \u2217(x),y))| < .\nPart (a) says that the t\u2217(x) and s\u2217(x) have almost the same expected F\u03b2 , and Part (b) says that for a large i.i.d. test set (x,y), t\u2217 and s\u2217 have almost identical F\u03b2 .\nThe constraint on s ensures that instances with the same probability of being positive are placed in the same class. In general, optimal predictions may not satisfy this constraint (Lewis, 1995). However, if the underlying distribution satisfies that P (P (1|X) = \u03b4) = 0 for any \u03b4, then the above result is essentially this: given P , an optimal prediction and the prediction using the optimal threshold are asymptotically equivalent. This is demonstrated empirically in Section 4."}, {"heading": "3. Algorithms", "text": "We first discuss approximations to EUM, then discuss DTA and present a new efficient prediction algorithm."}, {"heading": "3.1. Approximations to the EUM Approach", "text": "Exact empirical optimization of F-measures for a parametric family is difficult due to its complex piecewise linear nature, and typically only approximations of the F-measures are maximized. We discuss three methods.\nIn view of the optimality of probability thresholding classifiers, it is natural to first learn an estimate p(Y |X) for P (Y |X), and then learn an optimal threshold \u03b4. If p(Y |X) is chosen from a parametric family\n3Both Lemma 7 and Theorem 8 hold for T \u222a T \u2032 as well. We consider T to simplify the presentation.\nusing the maximum likelihood (ML) principle, then under very general conditions, the learned distribution follows an asymptotically normal convergence to the model with smallest KL-divergence to the true distribution (White, 1982). Thus when the model family is well-specified, the resulting classifier is asymptotically optimal. We call this the ML\u03b4approximation. Strictly, this is a combination of the conditional probability estimation and F-measure optimization of the threshold, and the convergence rate in Theorem 3 does not apply.\nJansche (2005) learned a logistic regression model p(Y |X,\u03c6) by maximizing the empirical F\u03b2 in eq. 1, but with each binary decision si replaced by the predictive probabilities pi = p(1|xi, \u03c6). The eventual classifier uses the rule h(x) = I(p(1|x) > 0.5). It is unknown whether this method is consistent or whether it follows any asymptotic convergence. There is also no apparent reason to use 0.5 as the threshold, so we shall optimize the threshold on the training data in addition to estimating \u03c6. We call this the F\u03b4 approximation.\nWe considered learning a rule h(x) = I(p(1|x, \u03c6) > \u03b4) directly, where \u03c6, \u03b4 are parameters, by approximating the empirical F\u03b2 in eq. 1 using si = I\n\u03b3(p(1|xi, \u03c6)\u2212 \u03b4), where I\u03b3(t) = 1/(1 + e\u2212\u03b3t) approximates I(t > 0) for large \u03b3. However, this seemed to overfit easily, and it rarely yielded better performance than the ML\u03b4 and F\u03b4 approximations in our preliminary experiments. We will not consider it further."}, {"heading": "3.2. Maximizing Expected F-measure", "text": "Given a utility function U(s,y), the decision-theoretic optimal prediction for x maximizes Ey\u223cP (\u00b7|x)(U(s,y)). In general, the true distribution P is not known and is estimated. The approach that involves first estimating true distributions using maximum likelihood (ML) and then making decision-theoretic optimal predictions will be called the MLE approach. We discuss the two steps in MLE , then we present an efficient algorithm for computing the optimal predictions.\nFirst, the asymptotic convergence of ML (White, 1982) implies the MLE approach is asymptotically optimal when estimating with sufficient training examples in a well-specified family. In practice, we will not know whether the model family is well-specified. Nevertheless, as we shall see in Section 4.1, the MLE approach can yield results indistinguishable from the optimal if the model family is misspecified but contains a reasonable approximation to the true distribution.\nSecond, for arbitrary utility function U , computing the expectation can be computationally difficult. But for the case when the utility function is an F-measure, and\nAlgorithm 1 Compute f\u03b2;1, . . . , f\u03b2;n, where \u03b2 2 = q/r\n1: For 0 \u2264 i \u2264 n, set C[i] as the coefficient of xi in [p1x+ (1\u2212 p1)] . . . [pNx+ (1\u2212 pN )]; 2: For 1 \u2264 i \u2264 (q + r)n, S[i]\u2190 q/i; 3: for k = n to 1 do 4: f\u03b2;k \u2190 \u2211n k1=0\n(1 + r/q)k1C[k1]S[rk + qk1]; 5: Divide C by pkx+ (1\u2212 pk); 6: for i = 1 to (q + r)(k \u2212 1) do 7: S[i]\u2190 (1\u2212 pk)S[i] + pkS[i+ q]; 8: end for 9: end for\nP (y|x) = \u220fni=1 P (yi|xi), efficient algorithms can be designed by exploiting the following characterization of an optimal prediction. Let pi = P (1|xi). Theorem 9. (Probability Ranking Principle for Fmeasure, Lewis 1995) Suppose s\u2217 = maxs E(F\u03b2(s,y)). Then min{pi | s\u2217i = 1} \u2265 max{pi | s\u2217i = 0}.\nThus the decision-theoretic optimal prediction contains the top k instances that are most likely to be positive for some k \u2208 {0, . . . , n}. This reduces the number of candidate predictions from 2n to n+1. We shall use this result to give an efficient algorithm for computing the optimal predictions."}, {"heading": "3.2.1. A Quadratic Time Algorithm", "text": "We give an O(n3) time algorithm for computing the optimal predictions, then improve it to O(n2) when \u03b22 is rational, which is often the case.\nLet F\u03b2;k(y) be the F\u03b2-measure when the first k instances are predicted as positive, then we have F\u03b2;k(y) = (1 + \u03b2 2) \u2211k i=1 yi/[k + \u03b2 2 \u2211n i=1 yi]. Let\nf\u03b2;k = \u2211 y P (y)F\u03b2;k(y), and Si:j = \u2211j l=i yl. For y\u2019s satisfying S1:k = k1 and Sk+1:n = k2, their F\u03b2 \u2019s are (1 + \u03b22)k1/(k + \u03b2\n2(k1 + k2)), and the probability this happens is P (S1:k = k1)P (Sk+1:n = k2), thus\nf\u03b2;k = \u2211\n0\u2264k1\u2264k 0\u2264k2\u2264n\u2212k\nP (S1:k = k1)P (Sk+1:n = k2)(1 + \u03b2 2)k1\nk + \u03b22(k1 + k2) .\nOne can show that P (S1:k = i) and P (Sk+1:n = i) are the coefficients of xi in \u220fk j=1[pjx + (1 \u2212 pj)] and\u220fn\nj=k+1[pjx+(1\u2212pj)] respectively. Thus, each f\u03b2;k can be computed in O(n2) time using O(n) space. Hence computing all f\u03b2;k\u2019s takes O(n 3) time and O(n) space.\nFor rational \u03b22, we can improve the computation to O(n2) time and O(n) space. The key is to note that\nf\u03b2;k = k\u2211 k1=0 (1 + \u03b2\u22122)k1P (S1:n = k1)s(k, k\u03b2 \u22122 + k1),\nwhere s(k, \u03b1) = \u2211n\u2212k k2=0\nP (Sk+1:n = k2)/(\u03b1 + k2). For rational \u03b2, the s values required for the f\u03b2;k\u2019s are shared. To compute s, use s(n, \u03b1) = 1/\u03b1, and\ns(k \u2212 1, \u03b1) = pks(k, \u03b1+ 1) + (1\u2212 pk)s(k, \u03b1),\nwhich follows from P (Sk:n = i) = pkP (Sk+1:n = i \u2212 1) + (1\u2212 pk)P (Sk+1:n = i). The pseudo-code is given in Algorithm 1, with q/r as the reduced fraction of \u03b22. Correctness can be seen by observing that at line 3, S[i] = s(k, i/q), and C[k1] = P (S1:k = k1). In practice, polynomial division can be numerically unstable, and it is preferred to precompute all the C[i]\u2019s using O(n2) time and space first."}, {"heading": "4. Experiments", "text": "We empirically demonstrate that EUM can be more robust against model misspecification, but DTA can be better for rare classes on small datasets and a common scenario of domain-adaptation. We use a synthetic dataset, the Reuters-21578 dataset, and four multilabel classification datasets."}, {"heading": "4.1. Mixtures of Gaussians", "text": "We consider a mixture of Gaussians on D dimensions: P (X,Y ) = \u03c0YN(X;\u00b5Y ,\u03a3Y ), with \u03a31 = \u03a30 = ID, \u00b51 = (S +O)1/ \u221a 4D and \u00b50 = \u2212(S \u2212O)1/ \u221a 4D, where S and O are non-negative constants. Thus S is the distance between the centers. We shall vary S, O, D, \u03c01 and the number of training examples Ntr. All instances are i.i.d. The optimal F1 achievable by a classifier \u03b8 can be computed (see eq. 2), and it depends only on S and \u03c01. Ntr determines how close the estimated distribution is to the optimal model; and the number of test examples, Nts, affects the gap in the performance between the thresholding method and the expectation method (Theorem 8).\nWe train logistic regression (LR) models using three different attribute vector representations: R0 consists of the coordinates only, R1 is R0 with an additional dummy attribute fixed at 1, and R2 is R1 with additional all degree two monomials of the coordinates. LR with R2 includes the true distribution. The methods compared are ML\u03b4, F\u03b4, MLE , Truth\u03b4 and TruthE , where last two methods use the true model P (X,Y ) for thresholding and expectation.\nThe first column in Table 1 lists the parameter settings. For the row headed by Default, we use D = 10, S = 4, O = 0, Ntr = 1000, Nts = 3000, and \u03c01 = 0.5. This dataset is low dimensional, almost noiseless, balanced and has sufficiently many train and test in-\nstances.4 Each of the remaining rows uses the same set of parameters, except for the one parameter indicated on the first column. LR with R0 or R1 contains a good approximation to the true distribution for all settings except \u03c01 = 0.05 and O = 50. For \u03c01 = 0.05, the class is imbalanced and such imbalance cannot be modelled without the dummy attribute. Thus R0 will not give a good model, but R1 will. For O = 50, the centers are far from the origin, and this makes both R0 and R1 inadequate for density estimation.\nIn Table 1, the F1 results for Truth E , Truth\u03b4 and Theory (the theoretical optimal F1) are similar. These are expected according to Theorem 8. Most other scores are close to the optimal scores. For MLE and ML\u03b4, these scores are expected due to the presence of a good approximation to the true distribution in the model family, and the asymptotic convergence property of ML\u03b4 and MLE given sufficiently many examples, as discussed in Section 3. For F\u03b4, although we lack its theoretical convergence to an optimal classifier, the results suggest that such convergence may hold.\nThe scores obtained using R2 are generally lower than scores obtained using R0 and R1 under the settings Default, S = 0.4, D = 100, and Ntr = 100, though R2 gives a well-specified model class while R1 and R0 do not. Thus, a well-specified model class is not necessarily better. This is because a misspecified model class with a small VC dimension can converge to the optimal model within the class using fewer samples than a well-specified model class with a higher VC dimension. To choose a class of the right complexity, one may follow the structural risk minimization principle (Vapnik, 1995). This requires bounds like those in Lemma 2 and Theorem 3. However, the given bounds cannot be used because they only apply for large samples.\nThe gaps between R2 scores and the optimal score for Default is significantly smaller than the gaps for S =\n4We have verified that the sizes are large enough to give the same conclusions for other i.i.d. data of the same sizes.\n0.4, D = 100, Ntr = 100, and \u03c01 = 0.05. This suggests that higher noise level, higher model class complexity, smaller training size, and smaller positive ratio make it harder to learn a good classifier. Note that Theorem 3 already suggests that in EUM, smaller positive ratio can make learning more difficult.\nFor the setting \u03c01 = 0.05, using R0, ML E performs poorly, while ML\u03b4 is close to optimal. MLE \u2019s poor performance is expected due to poor quality of the learned distribution, and ML\u03b4\u2019s performance can be justified by Theorem 5: the thresholding method can remain optimal when the score function is rank-preserving but not close to the true probability distribution. For the setting O = 50, both MLE and ML\u03b4 perform poorly using R0, but ML\n\u03b4 is much better than MLE using R1. Thus although ML\n\u03b4 can still be severely affected by model misspecification, it is still relatively robust. In addition, for \u03c01 = 0.05 and O = 50, F\n\u03b4 has much higher or at least comparable scores than MLE and ML\u03b4. This suggests that if the model class is severely misspecified, then EUM can be more robust than DTA.\nWe also compare MLE and ML\u03b4 on small test sets with Nts = 100 (Theorem 8 only holds for large test set size). We observed similar performances from ML\u03b4 and MLE when \u03c01 is high, but ML E seems significantly better than ML\u03b4 when \u03c01 is small. To illustrate, Table 2 gives the results when the same setting as \u03c01 = 0.05 in Table 1 is used to generate the data. It shows that, with a sufficiently accurate model, MLE can be better than ML\u03b4 and F\u03b4 on rare classes."}, {"heading": "4.1.1. Effect of Model Quality", "text": "We also perform experiments to study the effect of incorrect probability models on MLE . We use the Default setting in the previous section, with \u03c01 = 0.5 and S = 4 changed to S = 2, as the true distribution, to generate a set of 3000 i.i.d. test instances. We make optimal predictions using an assumed distribution which is the same as the true one except that we vary \u03c01. For each \u03c01, we compute the F1 and the Kullback-\nLeibler-divergence (KL) from the true to the assumed distribution on the test set. These are plotted in Figure 1(b), where 1\u2212 F1 is plotted instead of F1. Figures 1(a) and 1(c) plot for similar experiments, but using 0.1 and 0.9 as the true \u03c01 instead. Our choice of S = 2 instead of S = 4 for the true distribution has made the difference between the true and assumed distributions more pronounced in the plots. Comparing the curves for KL and 1\u2212 F1 within each figure, we see that the F-measure of DTA is roughly positively correlated with the model quality. The plot for 1\u2212 F1 in Figure 1(a) exhibits higher curvature around the true \u03c01 than those in the other two figures. This suggests that if the true distribution has a small positive ratio, the performance is more sensitive to model quality."}, {"heading": "4.1.2. Domain Adaptation", "text": "In domain adaptation, the test distribution differs from the training one. One common scenario is when P (X) changes but P (Y |X) does not. Using the mixture of Gaussians with D = 10, S = 4, O = 0 and \u03c01 = 0.5, we generate 5000 i.i.d. training instances, and 5000 test instances with P (Y |X) < 0.5. The F1 scores for Truth\u03b4, TruthE , ML\u03b4 and MLE (using R1) are 21%, 38%, 11% and 36% respectively. Similar results are obtained under similar settings. Under such conditions, DTA is more robust than EUM."}, {"heading": "4.2. Text Classification", "text": "We evaluate on the Reuters-21578 dataset5 using the ModApte partition, which has 9603 training docu-\n5This is from http://www.daviddlewis.com/resources/ testcollections/reuters21578/.\nments and 3299 test documents. We train two models: the standard multinomial na\u0308\u0131ve Bayes (NB) model and a LR model, using word occurrence counts and a dummy attribute fixed at one. Both models are regularized. For NB, we use the Laplace corrector with one count for class and word counts. For LR, we use the Gaussian norm on the parameters. We use only those topics with at least C positive instances in both the train and test sets, and we vary C. Table 3 reports macro-F1 scores (the F1 averaged over topics), where ML.5 uses 0.5 to threshold the probabilities,\nIn Table 3, although NB generally does not provide good probability estimates, MLE is still at least comparable to ML.5 and ML\u03b4. With LR, MLE is a few percents better for rare classes. Chai (2005) used Gaussian process and obtained similar conclusion."}, {"heading": "4.3. Multilabel Datasets", "text": "We evaluate on four standard multilabel classification datasets.6 We train regularized LR, with the regu-\n6These are available at http://mulan.sourceforge.net/.\nlarization parameter for each class selected using two fold cross validation. Macro-F1 scores are shown in Table 4. The bracketed scores are obtained by choosing the regularization parameter giving a model with minimum empirical KL divergence on the test data. Each bracketed score is higher than its non-bracketed counterpart, thus models closer to the true one perform better for both MLE and ML\u03b4. Comparing the scores for MLE with those for ML\u03b4 and F\u03b4, bracketed or not, we see that MLE performs better, especially for smaller C, suggesting MLE is better for rare classes."}, {"heading": "5. Conclusion", "text": "We gave theoretical justifications and connections for optimizing F-measures using EUM and DTA. We empirically demonstrated that EUM seems more robust against model misspecification, while given a good model, DTA seems better for handling rare classes and a common domain adaptation scenario.\nA few important questions are unanswered yet: existence of interesting classifiers for which EUM can be done exactly, quantifying the effect of inaccurate models on optimal predictions, identifying conditions under which one method is preferable to another, and practical methods for selecting the best method on a dataset. Results in this paper only hold for large data sets, and it is important to consider the case for small number of instances. Experiments with and analyses of other methods may yield additional insights as well."}, {"heading": "Acknowledgement", "text": "This work is supported by DSO grant DSOL11102."}], "references": [{"title": "Expectation of F-measures: tractable exact computation and some empirical observations of its properties", "author": ["K.M.A. Chai"], "venue": "In SIGIR,", "citeRegEx": "Chai,? \\Q2005\\E", "shortCiteRegEx": "Chai", "year": 2005}, {"title": "Search-based structured prediction", "author": ["H. Daum\u00e9", "J. Langford", "D. Marcu"], "venue": "Machine learning,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2009}, {"title": "An exact algorithm for F-measure maximization", "author": ["K. Dembczynski", "W. Waegeman", "W. Cheng", "E. Hullermeier"], "venue": "In NIPS,", "citeRegEx": "Dembczynski et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dembczynski et al\\.", "year": 2011}, {"title": "A study on threshold selection for multi-label classification", "author": ["R.E. Fan", "C.J. Lin"], "venue": "Technical report,", "citeRegEx": "Fan and Lin,? \\Q2007\\E", "shortCiteRegEx": "Fan and Lin", "year": 2007}, {"title": "Maximum expected F-measure training of logistic regression models", "author": ["M. Jansche"], "venue": "In HLT/EMNLP,", "citeRegEx": "Jansche,? \\Q2005\\E", "shortCiteRegEx": "Jansche", "year": 2005}, {"title": "A maximum expected utility framework for binary sequence labeling", "author": ["M. Jansche"], "venue": "In ACL,", "citeRegEx": "Jansche,? \\Q2007\\E", "shortCiteRegEx": "Jansche", "year": 2007}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "In ICML, pp", "citeRegEx": "Joachims,? \\Q2005\\E", "shortCiteRegEx": "Joachims", "year": 2005}, {"title": "Evaluating and optimizing autonomous text classification systems", "author": ["D.D. Lewis"], "venue": "In SIGIR, pp", "citeRegEx": "Lewis,? \\Q1995\\E", "shortCiteRegEx": "Lewis", "year": 1995}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Schutze"], "venue": "CUP,", "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Reverse multi-label learning", "author": ["J. Petterson", "T. Caetano"], "venue": "In NIPS, pp. 1912\u20131920,", "citeRegEx": "Petterson and Caetano,? \\Q2010\\E", "shortCiteRegEx": "Petterson and Caetano", "year": 2010}, {"title": "Training conditional random fields with multivariate evaluation measures", "author": ["J. Suzuki", "E. McDermott", "H. Isozaki"], "venue": "In ACL, pp", "citeRegEx": "Suzuki et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2006}, {"title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "In HLT-NAACL,", "citeRegEx": "Sang and Meulder,? \\Q2003\\E", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "JMLR, 6(2):1453\u20131484,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Foundation of evaluation", "author": ["C.J. van Rijsbergen"], "venue": "Journal of Documentation,", "citeRegEx": "Rijsbergen,? \\Q1974\\E", "shortCiteRegEx": "Rijsbergen", "year": 1974}, {"title": "The nature of statistical learning theory", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}, {"title": "Maximum likelihood estimation of misspecified models", "author": ["H. White"], "venue": null, "citeRegEx": "White,? \\Q1982\\E", "shortCiteRegEx": "White", "year": 1982}, {"title": "A study of thresholding strategies for text categorization", "author": ["Y. Yang"], "venue": "In SIGIR, pp", "citeRegEx": "Yang,? \\Q2001\\E", "shortCiteRegEx": "Yang", "year": 2001}, {"title": "Bayesian online learning for multi-label and multi-variate performance measures", "author": ["X. Zhang", "T. Graepel", "R. Herbrich"], "venue": "In AISTATS, pp", "citeRegEx": "Zhang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "F-measures are usually preferred to accuracies as standard performance measures in information retrieval (Manning et al., 2008), particularly, when relevant items are rare.", "startOffset": 105, "endOffset": 127}, {"referenceID": 16, "context": "Though simple, this method has been found to be effective and is commonly applied, for example, in text categorization (Yang, 2001).", "startOffset": 119, "endOffset": 131}, {"referenceID": 4, "context": "Joachims (2005) gave an efficient algorithm for maximizing a convex lower bound of F-measures for support vector machines, and showed it worked well on text classification.", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "Jansche (2005) gave an efficient algorithm to maximize a non-convex approximation to F-measures using logistic regression models, and showed it works well on a text summarization problem.", "startOffset": 0, "endOffset": 15}, {"referenceID": 7, "context": "The decision-theoretic approach (DTA), advocated by Lewis (1995), estimates a probability model first, and then computes the optimal predictions (in the sense of having highest expected F-measure) according to the model.", "startOffset": 52, "endOffset": 65}, {"referenceID": 0, "context": "Based on Lewis\u2019s characterization, Chai (2005) gave an O(n) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA.", "startOffset": 35, "endOffset": 47}, {"referenceID": 0, "context": "Based on Lewis\u2019s characterization, Chai (2005) gave an O(n) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA. Apparently unaware of Chai\u2019s work, Jansche (2007) solved the same problem in O(n) time.", "startOffset": 35, "endOffset": 223}, {"referenceID": 0, "context": "Based on Lewis\u2019s characterization, Chai (2005) gave an O(n) time algorithm to compute optimal predictions, and he gave empirical demonstration for the effectiveness of DTA. Apparently unaware of Chai\u2019s work, Jansche (2007) solved the same problem in O(n) time. For the general case when the labels are not necessarily independent, Dembczynski et al. (2011) gave an O(n) time algorithm given n+1 parameters of the label distribution, but the parameters can be expensive to compute.", "startOffset": 35, "endOffset": 357}, {"referenceID": 12, "context": "There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum\u00e9 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al.", "startOffset": 85, "endOffset": 155}, {"referenceID": 10, "context": "There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum\u00e9 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al.", "startOffset": 85, "endOffset": 155}, {"referenceID": 1, "context": "There are also algorithms for optimizing F-measures for tasks with structured output (Tsochantaridis et al., 2005; Suzuki et al., 2006; Daum\u00e9 et al., 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al.", "startOffset": 85, "endOffset": 155}, {"referenceID": 17, "context": ", 2009) and multilabel tasks (Fan & Lin, 2007; Zhang et al., 2010; Petterson & Caetano, 2010).", "startOffset": 29, "endOffset": 93}, {"referenceID": 14, "context": "We now show that training to maximize the empirical F\u03b2 is consistent, using VC-dimension (Vapnik, 1995) to quantify the complexity of the classifier class.", "startOffset": 89, "endOffset": 103}, {"referenceID": 7, "context": "In general, optimal predictions may not satisfy this constraint (Lewis, 1995).", "startOffset": 64, "endOffset": 77}, {"referenceID": 15, "context": "using the maximum likelihood (ML) principle, then under very general conditions, the learned distribution follows an asymptotically normal convergence to the model with smallest KL-divergence to the true distribution (White, 1982).", "startOffset": 217, "endOffset": 230}, {"referenceID": 15, "context": "First, the asymptotic convergence of ML (White, 1982) implies the ML approach is asymptotically optimal when estimating with sufficient training examples in a well-specified family.", "startOffset": 40, "endOffset": 53}, {"referenceID": 14, "context": "To choose a class of the right complexity, one may follow the structural risk minimization principle (Vapnik, 1995).", "startOffset": 101, "endOffset": 115}, {"referenceID": 0, "context": "Chai (2005) used Gaussian process and obtained similar conclusion.", "startOffset": 0, "endOffset": 12}], "year": 2012, "abstractText": "F-measures are popular performance metrics, particularly for tasks with imbalanced data sets. Algorithms for learning to maximize F-measures follow two approaches: the empirical utility maximization (EUM) approach learns a classifier having optimal performance on training data, while the decision-theoretic approach learns a probabilistic model and then predicts labels with maximum expected F-measure. In this paper, we investigate the theoretical justifications and connections for these two approaches, and we study the conditions under which one approach is preferable to the other using synthetic and real datasets. Given accurate models, our results suggest that the two approaches are asymptotically equivalent given large training and test sets. Nevertheless, empirically, the EUM approach appears to be more robust against model misspecification, and given a good model, the decision-theoretic approach appears to be better for handling rare classes and a common domain adaptation scenario.", "creator": "LaTeX with hyperref package"}}}