{"id": "1206.4628", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Robust PCA in High-dimension: A Deterministic Approach", "abstract": "we consider principal component analysis for contaminated solid data - set in the high dimensional regime, where the dimensionality of each observation is comparable or even no more than the normal number of observations. we propose a deterministic high - dimensional statistical robust pca algorithm which inherits all theoretical properties of each its randomized counterpart, though i. e., it necessarily is tractable, robust applicable to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness - - a breakdown point function of 50 %. more crucial importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large - scale real applications.", "histories": [["v1", "Mon, 18 Jun 2012 15:07:55 GMT  (699kb)", "http://arxiv.org/abs/1206.4628v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["jiashi feng", "huan xu", "shuicheng yan"], "accepted": true, "id": "1206.4628"}, "pdf": {"name": "1206.4628.pdf", "metadata": {"source": "META", "title": "Robust PCA in High-dimension: A Deterministic Approach", "authors": ["Jiashi Feng", "Huan Xu", "Shuicheng Yan"], "emails": ["a0066331@nus.edu.sg", "mpexuh@nus.edu.sg", "eleyans@nus.edu.sg"], "sections": [{"heading": "1. Introduction", "text": "This paper is about robust principal component analysis (PCA) for high-dimensional data, a topic that has drawn surging attention in recent years. PCA is one of the most widely used data analysis methods (Pearson, 1901). It constructs a low-dimensional subspace based on a set of principal components (PCs) to approximate the observations in the least-square sense. Standard PCA computes PCs as eigenvectors of the sample covariance matrix. Due to the quadratic error criterion, PCA is notoriously sensitive and fragile, and the quality of its output can suffer severely in the face of even few corrupted samples. Therefore, it is not surprising that many works have been dedicated to robustifying PCA (Hubert et al., 2005; Croux\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\n& Ruiz-Gazen, 2005; Candes et al., 2009).\nAnalyzing high dimensional data \u2013 data sets where the dimensionality of each observation is comparable to or even larger than the number of observations \u2013 has become a critical task in modern statistics and machine learning (Donoho, 2000). Practical high dimensional data, such as DNA microarray data, financial data, consumer data, and climate data, easily have dimensionality ranging from thousand to billions. Partly due to the fact that extending traditional statistical tools (designed for the low dimensional case) into this highdimensional regime are often unsuccessful, tremendous research efforts have been made to design fresh statistical tools to cope with such \u201cdimensionality explosion\u201d.\nThe work of Xu et al. (2010a) is among the first to analyze robust PCA algorithms in the high-dimensional setup. They identified three pitfalls, namely diminishing breakdown point, noise explosion and algorithmic intractability, where previous robust PCA algorithms stumble. They then proposed the high-dimensional robust PCA (HR-PCA) algorithm that can effectively overcome these problems, and showed that HR-PCA is tractable, provably robust and easily kernelizable. In particular, in contrast to standard PCA and existing robust PCA algorithms, HR-PCA is able to robustly estimate the PCs in the high-dimensional regime even in the face of a constant fraction of outliers and extremely low Signal Noise Ratio (SNR) \u2013 the breakdown point of HR-PCA is 50%, 1 which is the highest breakdown point can ever be achieved, whereas other existing methods all have breakdown points diminishing to zero. Indeed, to the best of our knowledge, HRPCA appears to be the only algorithm having these\n1Breakdown point is a robustness measure defined as the percentage of corrupted points that can make the output of the algorithm arbitrarily bad.\nproperties in the high-dimensional regime.\nBriefly speaking, HR-PCA is an iterative method which in each iteration performs standard PCA, and then randomly remove one point in a way that outliers are more likely to be removed, so that the algorithm converges to a good output. Because in each iteration, only one point is removed, the number of iterations required to find a good solution is at least as much as the number of outliers. This, combined with the fact that PCA is computationally expensive itself, prevents HR-PCA from effectively handling large-scale datasets with many outliers. In addition, the performance of HR-PCA depends on the ability of the built-in random removal to eliminate outliers correctly, which is only guaranteed in a probabilistic manner.\nTo address these two issues, we propose a deterministic high dimensional robust PCA algorithm (DHRPCA). Specifically, instead of removing one point, the proposed algorithm decreases the weights of all observations in each iteration, in a way that the total weight of the outliers will decrease faster than that of the true samples. We show that DHR-PCA inherits all desirable theoretical properties of HR-PCA, including tractability, kernelizability, the maximal breakdown point, provable performance guarantee and asymptotical optimality. Moreover, DHR-PCA can be much more computationally efficient than (randomized) HRPCA. As we show below, the number of iterations for DHR-PCA to converge is nearly constant, in sharp contrast to HR-PCA whose number of iterations required increases linearly with the number of outliers. Simulations in Section 4 show that for any fixed number of iterations, the solution to DHR-PCA is at least as good as HR-PCA, and is significantly better when the number of iterations is small. This is very appealing in practice, as both algorithms are \u201cany-time\u201d algorithms, i.e., one can terminate the algorithms at any time and obtain the best solution so-far."}, {"heading": "2. Related Work", "text": "Besides HR-PCA, there have been abundant works on robust PCA, which we briefly discuss in this section. Robust PCA algorithms focusing on the low-dimensional setup (e.g., Rousseeuw, 1984; Croux & Ruiz-Gazen, 2005; Hubert et al., 2005) can be roughly categorized into two groups. The first group of algorithms pursue robust estimation of the covariance matrix, e.g., M -estimator (Maronna, 1976), S-estimator (Rousseeuw & Leroy, 1987), and Minimum Covariance Determinant (MCD) estimator (Rousseeuw, 1984). These algorithms generally provide more robust results, but their applicability\nis severely limited to small or moderate dimensions, as there are not enough observations to robustly estimate a high-dimensional covariance matrix. The second group of algorithms directly maximize certain robust estimation of univariate variance for the projected observations and then obtain maximizers as the candidate principal components (Li & Chen, 1985; Croux & Ruiz-Gazen, 1996; 2005; Hubert et al., 2002). These algorithms inherit the robustness characteristics of the adopted estimators and are qualitatively robust. However, all of these algorithms run into unsolvable issues in the high dimensional regime incurred by the curse of dimensionality as stated in the followings.\nThe targeted high-dimensional regime poses three main challenges to existing robust PCA methods. First, some robust PCA algorithms have breakdown point inversely proportional to the dimensionality, e.g., M -estimator (Maronna, 1976), in the highdimensional regime their breakdown points will diminish and the results will be arbitrarily bad in presence of even few outliers. Second, widely used outlyingness indicators, including Mahalanobis distance and StahelDonoho outlyingness (Donoho, 1982) are no longer valid, due to a phenomenon termed \u2013 \u201cnoise explosion\u201d (Xu et al., 2010a). This causes the algorithms relying on such outlyingness measures (Hubert et al., 2005) to collapse. The third problem is that the dimensionality may be larger than the number of data points and thus some robust estimators including Minimum Volume Ellipsoid (MVE) and Minimum Covariance Determinant (MCD) (Rousseeuw, 1984) become degenerated. Furthermore, the extremely high computational complexity of these estimators and projection pursuit methods for high dimensional data prevents them from being tractable.\nFinally, we discuss recent works addressing robust PCA using low-rank technique. (Candes et al., 2009) developed a framework to perform robust PCA using low-rank matrix decomposition. Yet, their method focuses on the scenario that random entries of the observation matrix are arbitrarily corrupted, which differs from our setup where one corrupted data point may change the whole column of the observation matrix. The later setup is then investigated in Xu et al. (2010b). While their proposed method performs well under a small fraction of outliers, it breaks down for larger fraction of outliers \u2013 in particular, the breakdown point is far from 50%. Moreover, the performance scales unfavorably with the magnitude of noise, which makes it not suitable for the high-dimensional setup, due to \u201cnoise-explosion\u201d."}, {"heading": "3. The Algorithm", "text": "In this section, we first formally state the problem setup of the high dimensional robust PCA. Then we provide the details of the proposed DHR-PCA algorithm and finally present the main theoretic results on the performance guarantees of the algorithm."}, {"heading": "3.1. Problem Setup", "text": "In this subsection, we present the formal problem description of PCA for the high dimensional data with contamination. Our setup, detailed below for completeness, largely follows that of Xu et al. (2010a).\nGiven n observations, there are t observations not corrupted, called authentic samples. The authentic samples zi \u2208 Rm are generated through a linear mapping: zi = Axi + ni. Here, noise ni is sampled from normal distribution N (0, Im); and the signal xi \u2208 Rd are i.i.d. samples of a random variable x with mean zero and variance Id. The matrix A \u2208 Rm\u00d7d and the distribution \u00b5 of x are unknown. We assume \u00b5 is absolutely continuous w.r.t. the Borel measure and spherically symmetric. And \u00b5 has light tails, i.e., there exist constants K,C > 0 such that Pr(\u2016x\u2016 \u2265 x) \u2264 K exp(\u2212Cx) for all x \u2265 0. We are interested in the case where n \u2248 m d, i.e., the dimensionality of observations is much larger than that of signals and of the same order as the number of observations.\nThe outliers (the corrupted data) are denoted as o1, . . . ,on\u2212t \u2208 Rm and they are with arbitrary values. We only require that n\u2212 t \u2264 t, i.e., the number of outliers are not more than that of authentic samples. Let \u03bb , (n\u2212 t)/n be the fraction of corrupted points. We observe the contaminated dataset\nY , {y1, . . . ,yn} = {z1, . . . , zt} \u22c3 {o1, . . . ,on\u2212t},\nand aim to recover the principal components of A, i.e., the top eigenvectors w\u03041, . . . , w\u0304d of AA\nT . That is, we seek a collection of orthogonal vectors w1, . . . ,wd, that maximize the following performance metric called the Expressed Variance (E.V.):\nE.V.(w1, . . . ,wd) ,\n\u2211d j=1 w T j AA\nTwj\u2211d j=1 w\u0304 T j AA T w\u0304j .\nThe E.V. represents the portion of signal Ax being expressed by w1, . . . ,wd. Thus, 1\u2212E.V. is the reconstruction error of the signal. The E.V. is a commonly used evaluation metric for the PCA algorithms (Xu et al., 2010a; d\u2019Aspremont et al., 2008). It is always less than one, with equality achieved by a perfect recovery, i.e., the vectors w1, . . . ,wd have the same span as the true principal components {w\u03041, . . . , w\u0304d}.\nThe distribution \u00b5 affects the performance of the algorithms through its tail. We hence adapt the following tail weight function V : [0, 1] \u2192 [0, 1] from Xu et al. (2010a), which essentially represents how the tail of \u00b5\u0304 contributes to its variance,\nV(\u03b1) , \u222b c\u03b1 \u2212c\u03b1 x2\u00b5\u0304(dx),\nwhere \u00b5\u0304 is the one-dimensional margin of \u00b5 and c\u03b1 is such that \u00b5\u0304 ([\u2212c\u03b1, c\u03b1]) = \u03b1. Notice that V(0) = 0,V(1) = 1, and V(\u00b7) is continuous."}, {"heading": "3.2. Deterministic HR-PCA Algorithm", "text": "Our main algorithm is given in Algorithm 1. Here, a Robust Variance Estimator (RVE) V\u0304t\u0302(\u00b7) is adopted to identify the candidate principal components. For w \u2208 Sm, the RVE is defined as V\u0304t\u0302(w) , 1n \u2211t\u0302 i=1 |wTy|2(i), where the subscript (\u00b7) denotes a non-decreasing order of the variables. And it can be seen that the RVE stands for the following statistics: project yi onto the direction w, replace the furthest n \u2212 t\u0302 samples by 0, and then compute the variance. If the variance is large, it is likely that a correct principal component direction is found. Otherwise, a number of points with largest variance may be corrupted. Notice that the RVE is always performed on the original observed set Y. We find that RVE coincides with the robust L-estimator, which is defined as a linear combination of order statistics: Tn = \u2211n i=1 anih(x(i)) for some function h.\nWe now explain our innovation compared to HR-PCA, and its intuition. In HR-PCA, steps 4 and 5 are replaced by a random removal \u2013 the probability y\u0302i being\nremoved is proportional to \u2211d j=1 ( wTj y\u0302i )2 . It has been shown in Xu et al. (2010a) that in expectation (and in probability), either the number of outliers will decrease faster, or the algorithm will find a good solution. Since in each iteration, only one point is removed, the number of iterations required to find a satisfactory output depends linearly on the number of outliers.\nInstead of resorting to a random mechanism, DHRPCA deterministically reduce the effect of corrupted data points. In particular, Moreover, DHR-PCA operates on all the data points in each iteration, which decouples the dependence of the computational cost on the number of outliers and enhances the efficiency significantly compared with HR-PCA. We consider an artificial example to illustrate this: assume both HRPCA and DHR-PCA requires M iterations for a dataset Y0. Now suppose a new data-set Y contains J identical copies of data-set Y0. Then the number of iterations for DHR-PCA remains unchanged, while HR-PCA requires JM iterations. Simulation results\nAlgorithm 1 DHR-PCA.\nInput: Contaminated sample set Y = {y1, . . . ,yn} \u2282 Rm, parameters d, t\u0302. Output: Recovered PCs: w\u22171, . . . ,w \u2217 d. Initialize y\u0302i := yi, \u03b1i = 1,\u2200i = 1, . . . , n; Opt := 0. repeat\n1. Compute the empirical variance matrix\n\u03a3\u0302 := 1\nn n\u2211 i=1 \u03b1iy\u0302iy\u0302 T i ;\n2. Perform PCA on \u03a3\u0302. Let w1, . . . ,wd be the d principle components of \u03a3\u0302;\n3. If \u2211d j=1 V\u0304t\u0302(wj) > Opt, then let Opt :=\u2211d\nj=1 V\u0304t\u0302(wj) and let w \u2217 j := wj for j =\n1, . . . , d;\n4. Calculate\n\u03b7 = min i 1\u2211d j=1 ( wTj y\u0302i )2 ,\u2200i : \u03b1i 6= 0. 5. Update the sample weight \u03b1i := \u03b1i \u2212 \u2206\u03b1i,\n\u2200i : \u03b1i 6= 0, where \u2206\u03b1i = \u03b7\u03b1i \u2211d j=1 ( wTj y\u0302i )2 ;\nuntil Convergence\nfor more realistic setups, reported in Section 4, also demonstrate that the deterministic algorithm provides higher efficiency than HR-PCA.\nTheorem 1 and Theorem 2 below show that the proposed algorithm achieves the same performance guarantees as HR-PCA. The proofs are shown in Section 5.\nTheorem 1. (Finite Sample Performance) Let the Algorithm 1 output {w1, . . . ,wd}. Fix a \u03ba > 0, and let \u03c4 = max(m/n, 1). There exists a universal constant c0 and a constant C which can possibly depend on t\u0302/t, \u03bb, d, \u00b5 and \u03ba, such that for any \u03b3 < 1, if n/ log4 n \u2265 log6(1/\u03b3), then with probability 1 \u2212 \u03b3 the following holds\nE.V.{w1, . . . ,wd}\n\u2265\nV ( 1\u2212 \u03bb(1+\u03ba)(1\u2212\u03bb)\u03ba )\n(1 + \u03ba)\n\u00d7 V ( t\u0302 t \u2212 \u03bb 1\u2212\u03bb ) V ( t\u0302 t ) \n\u2212 8\u221ac0\u03c4d V ( t\u0302 t )  (trace(AAT ))\u22121/2\n\u2212  2c0\u03c4 V ( t\u0302 t )  (trace(AAT ))\u22121 \u2212 C log2 n log3(1/\u03b3)\u221a n .\nWe also consider the asymptotic performance of the proposed algorithm when the dimension and the number of data points grow together to infinity. Our asymptotic setting is similar to (Xu et al., 2010a). Suppose there exists a sequence of sample sets {Y(j)} = {Y(1),Y(2), . . .}, where Y(j), n(j),m(j), A(j), d(j), etc., denote the corresponding values of the quantities defined above, the following must hold for some positive constants c1, c2:\nlim j\u2192\u221e\nn(j) m(j) = c1; d(j) \u2264 c2;m(j) \u2191 +\u221e;\ntrace ( A(j)TA(j) ) \u2191 \u221e. (1)\nWhile trace ( A(j)TA(j) ) \u2191 \u221e, if it scales slowly than\u221a\nm(j), the SNR will asymptotically decrease to zero.\nThe last three terms in Theorem 1 go to zero as the dimension and number of points scale to infinity, i.e., as n and m\u2192\u221e. Therefore, we immediately obtain: Theorem 2. (Asymptotic Performance) Given a sequence of {Y(j)}, if the asymptotic scaling in Expression (1) holds, and lim sup\u03bb(j) \u2264 \u03bb\u2217, then the following holds in probability when j \u2191 \u221e (i.e., when n and m \u2191 \u221e),\nlim inf j\nE.V.{w1(j), . . . ,wd(j)}\n\u2265 max \u03ba\nV ( 1\u2212 \u03bb \u2217(1+\u03ba) (1\u2212\u03bb\u2217)\u03ba ) (1 + \u03ba) \u00d7 V ( t\u0302 t \u2212 \u03bb\u2217 1\u2212\u03bb\u2217 ) V ( t\u0302 t )  .(2)\nObserve that when \u03bb\u2217 = 0, i.e., the number of outliers scales sublinearly, the right-hand-side converges to 1 by taking \u03ba(j) = \u221a \u03bb(j), implying that the algorithm is asymptotically optimal. On the other hand, for any \u03bb < 0.5, the right hand side is strictly positive (picking \u03ba large enough), implying that the breakdown point converges to 50%.\nFor small \u03bb, we can make use of the light tail condition on \u00b5\u0304, to establish the following bound that simplifies (2). The proof is deferred to the supplementary material.\nCorollary 1. Under the settings of the above theorem, the following holds in probability when j \u2191 \u221e (i.e., when n, p \u2191 \u221e),\nlim inf j\nE.V.{w1(j), . . . ,wd(j)} \u2265 1\u2212 C \u2032 \u221a \u03b1\u03bb\u2217 log(1/\u03bb\u2217)\nV(0.5) .\nBefore concluding this section, we remark that DHRPCA is easily kernelizable. Specifically, given a mapping function \u03c6(\u00b7) : Rm \u2192 H and kernel function k(\u00b7, \u00b7)\nsatisfying k(x,y) = \u3008\u03c6(x), \u03c6(y)\u3009 for all x,y \u2208 Rm, we can perform dimension reduction without requiring the explicit form of \u03c6(\u00b7) in the kernel PCA (Scho\u0308lkopf et al., 1997). In particular, for the centered mapped features {\u03c6(y1), \u00b7 \u00b7 \u00b7 , \u03c6(yn)}, the output PCs can be represented as\nwq = n\u2211 j=1 aj(k)\u03c6(y\u0302j).\nAnd the feature projection can be calculated by\n\u3008wq, \u03c6(v)\u3009 = n\u2211 j=1 aj(q)k(y\u0302j ,v),\nwhere a(q) is the qth eigenvector of the kernel matrix. Note that Algorithm 1 only involves calculating \u3008wq, \u03c6(yi)\u3009 (in RVE evaluation) and \u3008wq, \u03c6( \u221a \u03b1iyi)\u3009 (in decreasing values of \u03b1i\u2019s). Since the kernelization of both these two steps are obtained, the DHR-PCA algorithm can be kernelized easily."}, {"heading": "4. Simulations", "text": "We devote this section to experimentally comparing the proposed DHR-PCA with HR-PCA. Since HRPCA has shown superior robustness (against the dimensionality and number of outliers) over several robust PCA algorithms and standard PCA (Xu et al., 2010a), we skip simulations for them here.\nThe numerical study is aimed to illustrate that DHRPCA is much more efficient than HR-PCA, and meanwhile it achieves competitive performance. Here, we report the results for d = 1. We follow the data generation method in (Xu et al., 2010a) to randomly generate an m\u00d71 matrix and then scale its leading singular value to \u03c3. A \u03bb fraction of outliers are generated on a line with a uniform distribution over [\u2212\u03c3\u00b7mag, \u03c3\u00b7mag]. Thus, \u201cmag\u201d represents the ratio between the magnitude of the outliers and that of the signal Axi and is fixed as 10. The value of t\u0302 is set as (1 \u2212 \u03bb)n, if \u03bb is known exactly. Otherwise, t\u0302 can be simply set as 0.5n. For each parameter setup, we report the average result of 20 tests and standard deviation.\nFigure 4 shows the results form = 100, 1000 and 10000 cases respectively with \u03c3 = 5. From the figure, we can make following observations. Firstly, DHR-PCA converges much faster than HR-PCA, especially for a large number of outliers. For example, when m = 10000 and \u03bb = 0.4, the proposed algorithm converges using less than 2 iterations in average while HR-PCA needs more than 4000 iterations to converge. Secondly, the computational time for DHR-PCA in each iteration is\nalways in the same order as HR-PCA. These results well demonstrate that DHR-PCA is much more efficient than HR-PCA.\nAs for the performance, i.e., the E.V. of the recovered PCs, Figure 4 shows that DHR-PCA performs competitively to HR-PCA. For all the cases, the E.V. of final solution of DHR-PCA is always larger than that of HR-PCA. Moreover, if we terminate both algorithms at any early iteration, DHR-PCA always perform better than HR-PCA. This is appealing in practice, as we can terminate DHR-PCA at any time and obtain a satisfactory result in practical implementation. In addition, both DHR-PCA and HR-PCA perform quite well even in presence of varying number of outliers (\u03bb = 0.05 to 0.4) and small signal magnitude (\u03c3 = 5), which coincides with the results in (Xu et al., 2010a).\nWe then investigate the relationship between the number of iterations before convergence and the number of outliers for the two methods. As shown in Figure 2, the number of iterations taken by HR-PCA is approximately proportional to the number of corrupted points. This is not surprising, since in each iteration HR-PCA removes at most one outlier. In a stark contrast, the number of required iterations of DHR-PCA remains nearly constant, shown by the flat curve in the figures. This demonstrates that DHR-PCA has good scalability and can potentially be applied to large real applications. We provide more simulations under numerous settings in the appendix."}, {"heading": "5. Proof of Theorem 1", "text": "In this section, we sketch the proof of Theorem 1. In what follows, we let d,m/n, \u03bb, t\u0302/t, and \u00b5 be fixed. We can fix a \u03bb \u2208 (0, 0.5) w.l.o.g. due to the fact that if a result is shown to hold for \u03bb, then it holds for \u03bb\u2032 < \u03bb. The letter c is used to represent a constant, and is a constant that decreases to zero as n and m increase to infinity. Let w1(s), . . . ,wd(s) be the candidate solution at stage s. Let Z and O be the sets of indices of authentic samples and corrupted samples respectively. We let Bd , {w \u2208 Rd|\u2016w\u2016 \u2264 1}, and Sd be its boundary. Here Theorems 3 and 4 are directly adapted from (Xu et al., 2010a)."}, {"heading": "5.1. Validity of the Robust Variance Estimator", "text": "We first show that the following condition holds with high probability. The detailed proof can be found in (Xu et al., 2010a).\nCondition 1. There exists 1, 2, c\u0304 such that\n(I)supw\u2208Sd \u2223\u2223\u2223 1t \u2211t\u2032i=1 \u2223\u2223wTx\u2223\u22232(i) \u2212 V ( t\u2032t )\u2223\u2223\u2223 \u2264 1;\nDeterministic High-dimensional Robust PCA\n(II) supw\u2208Sd \u2223\u2223\u2223 1t \u2211ti=1 \u2223\u2223wTxi\u2223\u22232 \u2212 1\u2223\u2223\u2223 \u2264 2; (III) supw\u2208Sm 1 t \u2211t i=1\n\u2223\u2223wTni\u2223\u22232 \u2264 c\u0304. Theorem 3. Fix any \u03b7 < 1. With probability at least 1 \u2212 3\u03b3, Condition 1 holds uniformly for all t\u2032 \u2264 \u03b7t, with c\u0304 = c\u03c4(1 + log(1/\u03b3)n ), 2 = c log 2 n log3(1/\u03b3)/ \u221a n,\nand 1 = c \u221a logn+log(1/\u03b3) n + c log2.5 n log3.5(1/\u03b3) n , for a constant c possibly depends on d, \u00b5 and \u03b7.\nUnder Condition 1, RVE is a good estimator. Theorem 4. Let t\u2032 \u2264 t. Suppose Condition 1 holds. Then for all w \u2208 Sm the following holds:\n(1\u2212 1)\u2016wTA\u20162V ( t\u2032\nt\n) \u2212 2\u2016wTA\u2016 \u221a (1 + 2)c\u0304\n\u2264 1 t t\u2032\u2211 i=1 |wT z|2(i)\n\u2264 (1 + 1)\u2016wTA\u20162V ( t\u2032\nt\n) + 2\u2016wTA\u2016 \u221a (1 + 2)c\u0304+ c\u0304.\nFrom the above theorem, we can immediately obtain the following corollary. Corollary 2. Let t\u2032 \u2264 t. Suppose Condition 1 holds. Then for all any w1, \u00b7 \u00b7 \u00b7 ,wd \u2208 Sm the following holds:\n(1\u2212 1)V ( t\u2032\nt\n) H(w)\u2212 2 \u221a (1 + 2)c\u0304dH(w)\n\u2264 d\u2211 j=1 1 t t\u2032\u2211 i=1 |wTj z|2(i)\n\u2264 (1 + 1)V ( t\u2032\nt\n) H(w) + 2 \u221a (1 + 2)c\u0304dH(w) + c\u0304,\nand\n(1\u2212 )H(w)\u2212 2 \u221a (1 + )c\u0304dH(w)\n\u2264 d\u2211 j=1 1 t t\u2211 i=1 |wTj zi|2\n\u2264 (1 + )H(w) + 2 \u221a (1 + )c\u0304dH(w) + c\u0304,\nwhere H(w) , \u2211d j=1 \u2016wTj A\u20162."}, {"heading": "5.2. Finite Steps for a Good Solution", "text": "In this step, we show that the algorithm finds a good solution in a small number of steps. Proving this involves showing that at any given step, either the algorithm finds a good solution, or the weight adjusting step decreases weights of corrupted points more than the authentic points. Let \u03b1 (s) i denote the weight of the ith data point in the sth stage. These points are\na good solution if the variance of the points projected onto their span is mainly due to the authentic samples rather than the corrupted points. We denote this \u201cgood output event at step s\u201d by E(s), defined as:\nE(s) =\n{\u2211 i\u2208Z \u03b1 (s) i vi(s) \u2265 1 \u03ba \u2211 i\u2208O \u03b1 (s) i vi(s) } ,\nwhere the variance vi(s) = \u2211d j=1 ( wj(s) Tyi )2\n. The intuition is that there cannot be too many steps without finding a good solution, since too many weights of the corrupted points will have been decreased to zero.\nTheorem 5. The event E(s) is true for some 1 \u2264 s \u2264 s0, where s0 \u2264 \u03bbn(1+\u03ba)\u03ba .\nThe proof of the above theorem is provided in the supplementary material. We compare Theorem 5 with its randomized counterpart, Theorem 9 of Xu et al. (2010a). The latter states that for HR-PCA, E(s) succeeds with high probability for some s \u2264 (1 + )(1 + \u03ba)\u03bbn/\u03ba, where depends on \u03ba and \u03bb, and decreases to 0 when n \u2191 \u221e (for fixed \u03ba and \u03bb). Thus, the advantage of Theorem 5 is two-fold: it is deterministic as opposed to probabilistic, and it does not require the decreasing ."}, {"heading": "5.3. Bounds on the Solution Performance", "text": "Let w\u03041, . . . , w\u0304d be the eigenvectors corresponding to the d largest eigenvalues of AAT , namely the optimal solution, w\u22171, . . . ,w \u2217 d be the output of the Algorithm 1 and w1(s), . . . ,wd(s) be the candidate solution at stage s. We define H(w1, . . . ,wd) ,\u2211d j=1 \u2016wTj A\u20162, and for notational simplification, let H\u0304 , H(w\u03041, . . . , w\u0304d), Hs , H(w1(s), . . . ,wd(s)), and H\u2217 , H(w\u22171, . . . ,w \u2217 d).\nThe statement of the finite-sample and asymptotic theorems (Theorem 1 and Theorem 2, respectively) lower bound the expressed variance, E.V., which is the ratio H\u2217/H\u0304. The final part of the proof accomplishes this in two main steps. First, we lower bound Hs in terms of H\u0304 where s is some step for which E(s) is true, i.e., the principal components found by the sth step of the algorithm are \u201cgood\u201d. By Theorem 5, we know that there is a \u201csmall\u201d such s. Based on the true E(s) and the algorithm definition, we can conclude the bound via some algebraic manipulations. The final output of the algorithm, however, is only guaranteed to have a high value of the robust variance estimator, V\u0304 - that is, even if there is a \u201cgood\u201d solution at some intermediate step s, we do not necessarily have a way of identifying it. Thus, the next step lower bounds the value of H\u2217 in terms of the value H of any output w\u20321, . . . ,w \u2032 d that has a smaller value of the robust\nvariance estimator. The details of these two steps are deferred to the supplementary material. Combining the results of above two steps, we can obtain the following theorem providing a lower bound of the ratio H\u2217/H\u0304, i.e., the expressed variance.\nTheorem 6. If \u22c3s0 s=1 E(s) is true, and there exist 1 < 1, 2, c\u0304 such that\nsupw\u2208Sd \u2223\u2223\u2223 1t \u2211t\u2212s0i=1 |wTx|2(i) \u2212 V ( t\u2212s0t )\u2223\u2223\u2223 \u2264 1 and Condition 1 holds, then\nH\u2217\nH\u0304 \u2265\n(1\u2212 1)2V ( t\u0302 t \u2212 \u03bb 1\u2212\u03bb ) V ( t\u2212s0 t ) (1 + 1)(1 + 2)(1 + \u03ba)V ( t\u0302 t\n) \u2212 [ (D1 +D2) \u221a (1 + 2)c\u0304d\n(1 + 1)(1 + 2)(1 + \u03ba)\n] (H\u0304)\u22121/2 (3)\n\u2212\n (1\u2212 1)V ( t\u0302 t \u2212 \u03bb 1\u2212\u03bb ) c\u0304+ (1 + 2)c\u0304\n(1 + 1)(1 + 2)V ( t\u0302 t\n)  (H\u0304)\u22121,\nwhere D1 = (2\u03ba + 4)(1 \u2212 1)V ( t\u0302 t \u2212 \u03bb 1\u2212\u03bb ) and D2 = 4(1 + 2)(1 + \u03ba).\nBy bounding all diminishing terms in the right hand side of (3), it reduces to Theorem 1. And Theorem 2 follows immediately. The proofs of Theorem 6 and Theorem 1 are similar to those in (Xu et al., 2010a) and we omit it here."}, {"heading": "6. Conclusions", "text": "In this work, we proposed a deterministic robust PCA algorithm for high-dimensional data corrupted by arbitrary outliers. The algorithm alternates between a classical PCA and decrease of weight coefficients on all the data points. Theoretical analysis showed that the proposed algorithm is tractable, robust to corrupt points, easily kernelizable, asymptotic consistent and achieving maximal breakdown point of 50% \u2013 to the best of our knowledge, the first deterministic algorithm that achieves these properties in the high-dimensional setup. More importantly, simulation results demonstrated that the proposed algorithm improves computational efficiency over its randomized counterpart HR-PCA \u2013 indeed, the number of iterations required to find a satisfactory solution appears to approximate constant, in sharp contrast to HR-PCA whose number of iterations required increases linearly with the number of outliers."}, {"heading": "Acknowledgements", "text": "H. Xu is partially supported by National University of Singapore startup grant R-265-000-384-133. S. Yan is\npartially supported by Singapore Ministry of Education under research Grant MOE2010-T2-1-087."}], "references": [{"title": "Robust principal component analysis", "author": ["E.J. Candes", "X. Li", "Y. Ma", "J. Wright"], "venue": null, "citeRegEx": "Candes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Candes et al\\.", "year": 2009}, {"title": "A fast algorithm for robust principal components based on projection pursuit", "author": ["C. Croux", "A. Ruiz-Gazen"], "venue": "In COMPSTAT: Proceedings in computational statistics,", "citeRegEx": "Croux and Ruiz.Gazen,? \\Q1996\\E", "shortCiteRegEx": "Croux and Ruiz.Gazen", "year": 1996}, {"title": "High breakdown estimators for principal components: the projection-pursuit approach revisited", "author": ["C. Croux", "A. Ruiz-Gazen"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Croux and Ruiz.Gazen,? \\Q2005\\E", "shortCiteRegEx": "Croux and Ruiz.Gazen", "year": 2005}, {"title": "Optimal solutions for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F. Bach", "L. Ghaoui"], "venue": null, "citeRegEx": "d.Aspremont et al\\.,? \\Q2008\\E", "shortCiteRegEx": "d.Aspremont et al\\.", "year": 2008}, {"title": "Breakdown properties of multivariate location estimators", "author": ["D.L. Donoho"], "venue": "Technical report,", "citeRegEx": "Donoho,? \\Q1982\\E", "shortCiteRegEx": "Donoho", "year": 1982}, {"title": "High-dimensional data analysis: The curses and blessings of dimensionality", "author": ["D.L. Donoho"], "venue": "AMS Math Challenges Lecture,", "citeRegEx": "Donoho,? \\Q2000\\E", "shortCiteRegEx": "Donoho", "year": 2000}, {"title": "A fast method for robust principal components with applications to chemometrics", "author": ["M. Hubert", "P.J. Rousseeuw", "S. Verboven"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "Hubert et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hubert et al\\.", "year": 2002}, {"title": "Robpca: a new approach to robust principal component analysis", "author": ["M. Hubert", "P.J. Rousseeuw", "K.V. Branden"], "venue": null, "citeRegEx": "Hubert et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hubert et al\\.", "year": 2005}, {"title": "Projection-pursuit approach to robust dispersion matrices and principal components: primary theory and monte carlo", "author": ["G. Li", "Z. Chen"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Li and Chen,? \\Q1985\\E", "shortCiteRegEx": "Li and Chen", "year": 1985}, {"title": "Robust m-estimators of multivariate location and scatter", "author": ["R.A. Maronna"], "venue": "The annals of statistics,", "citeRegEx": "Maronna,? \\Q1976\\E", "shortCiteRegEx": "Maronna", "year": 1976}, {"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": null, "citeRegEx": "Pearson,? \\Q1901\\E", "shortCiteRegEx": "Pearson", "year": 1901}, {"title": "Least median of squares regression", "author": ["P.J. Rousseeuw"], "venue": "Journal of the American statistical association,", "citeRegEx": "Rousseeuw,? \\Q1984\\E", "shortCiteRegEx": "Rousseeuw", "year": 1984}, {"title": "Robust regression and outlier detection", "author": ["P.J. Rousseeuw", "A.M. Leroy"], "venue": null, "citeRegEx": "Rousseeuw and Leroy,? \\Q1987\\E", "shortCiteRegEx": "Rousseeuw and Leroy", "year": 1987}, {"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.R. M\u00fcller"], "venue": "Artificial Neural Networks,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1997}, {"title": "Principal component analysis with contaminated data: The high dimensional case", "author": ["H. Xu", "C. Caramanis", "S. Mannor"], "venue": "In COLT,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "In NIPS,", "citeRegEx": "Xu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "PCA is one of the most widely used data analysis methods (Pearson, 1901).", "startOffset": 57, "endOffset": 72}, {"referenceID": 5, "context": "Analyzing high dimensional data \u2013 data sets where the dimensionality of each observation is comparable to or even larger than the number of observations \u2013 has become a critical task in modern statistics and machine learning (Donoho, 2000).", "startOffset": 224, "endOffset": 238}, {"referenceID": 14, "context": "The work of Xu et al. (2010a) is among the first to analyze robust PCA algorithms in the high-dimensional setup.", "startOffset": 12, "endOffset": 30}, {"referenceID": 7, "context": "Robust PCA algorithms focusing on the low-dimensional setup (e.g., Rousseeuw, 1984; Croux & Ruiz-Gazen, 2005; Hubert et al., 2005) can be roughly categorized into two groups.", "startOffset": 60, "endOffset": 130}, {"referenceID": 9, "context": ", M -estimator (Maronna, 1976), S-estimator (Rousseeuw & Leroy, 1987), and Minimum Covariance Determinant (MCD) estimator (Rousseeuw, 1984).", "startOffset": 15, "endOffset": 30}, {"referenceID": 11, "context": ", M -estimator (Maronna, 1976), S-estimator (Rousseeuw & Leroy, 1987), and Minimum Covariance Determinant (MCD) estimator (Rousseeuw, 1984).", "startOffset": 122, "endOffset": 139}, {"referenceID": 6, "context": "The second group of algorithms directly maximize certain robust estimation of univariate variance for the projected observations and then obtain maximizers as the candidate principal components (Li & Chen, 1985; Croux & Ruiz-Gazen, 1996; 2005; Hubert et al., 2002).", "startOffset": 194, "endOffset": 264}, {"referenceID": 9, "context": ", M -estimator (Maronna, 1976), in the highdimensional regime their breakdown points will diminish and the results will be arbitrarily bad in presence of even few outliers.", "startOffset": 15, "endOffset": 30}, {"referenceID": 4, "context": "Second, widely used outlyingness indicators, including Mahalanobis distance and StahelDonoho outlyingness (Donoho, 1982) are no longer valid, due to a phenomenon termed \u2013 \u201cnoise explosion\u201d (Xu et al.", "startOffset": 106, "endOffset": 120}, {"referenceID": 7, "context": "This causes the algorithms relying on such outlyingness measures (Hubert et al., 2005) to collapse.", "startOffset": 65, "endOffset": 86}, {"referenceID": 11, "context": "The third problem is that the dimensionality may be larger than the number of data points and thus some robust estimators including Minimum Volume Ellipsoid (MVE) and Minimum Covariance Determinant (MCD) (Rousseeuw, 1984) become degenerated.", "startOffset": 204, "endOffset": 221}, {"referenceID": 0, "context": "(Candes et al., 2009) developed a framework to perform robust PCA using low-rank matrix decomposition.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "(Candes et al., 2009) developed a framework to perform robust PCA using low-rank matrix decomposition. Yet, their method focuses on the scenario that random entries of the observation matrix are arbitrarily corrupted, which differs from our setup where one corrupted data point may change the whole column of the observation matrix. The later setup is then investigated in Xu et al. (2010b). While their proposed method performs well under a small fraction of outliers, it breaks down for larger fraction of outliers \u2013 in particular, the breakdown point is far from 50%.", "startOffset": 1, "endOffset": 391}, {"referenceID": 14, "context": "Our setup, detailed below for completeness, largely follows that of Xu et al. (2010a).", "startOffset": 68, "endOffset": 86}, {"referenceID": 3, "context": "is a commonly used evaluation metric for the PCA algorithms (Xu et al., 2010a; d\u2019Aspremont et al., 2008).", "startOffset": 60, "endOffset": 104}, {"referenceID": 3, "context": ", 2010a; d\u2019Aspremont et al., 2008). It is always less than one, with equality achieved by a perfect recovery, i.e., the vectors w1, . . . ,wd have the same span as the true principal components {w\u03041, . . . , w\u0304d}. The distribution \u03bc affects the performance of the algorithms through its tail. We hence adapt the following tail weight function V : [0, 1] \u2192 [0, 1] from Xu et al. (2010a), which essentially represents how the tail of \u03bc\u0304 contributes to its variance,", "startOffset": 9, "endOffset": 386}, {"referenceID": 14, "context": "It has been shown in Xu et al. (2010a) that in expectation (and in probability), either the number of outliers will decrease faster, or the algorithm will find a good solution.", "startOffset": 21, "endOffset": 39}, {"referenceID": 13, "context": "satisfying k(x,y) = \u3008\u03c6(x), \u03c6(y)\u3009 for all x,y \u2208 R, we can perform dimension reduction without requiring the explicit form of \u03c6(\u00b7) in the kernel PCA (Sch\u00f6lkopf et al., 1997).", "startOffset": 147, "endOffset": 171}, {"referenceID": 14, "context": "We compare Theorem 5 with its randomized counterpart, Theorem 9 of Xu et al. (2010a). The latter states that for HR-PCA, E(s) succeeds with high probability for some s \u2264 (1 + )(1 + \u03ba)\u03bbn/\u03ba, where depends on \u03ba and \u03bb, and decreases to 0 when n \u2191 \u221e (for fixed \u03ba and \u03bb).", "startOffset": 67, "endOffset": 85}], "year": 2012, "abstractText": "We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness \u2013 a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications.", "creator": "LaTeX with hyperref package"}}}