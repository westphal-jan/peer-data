{"id": "1206.4665", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Nonparametric variational inference", "abstract": "variational methods are widely used for approximate posterior inference. however, their use is typically limited to families of distributions that enjoy particular conjugacy properties. to circumvent this limitation, we propose offering a family of variational approximations thoroughly inspired by nonparametric kernel density estimation. setting the locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. using multiple kernels they allows the approximation to capture precisely multiple modes of the posterior, unlike most other variational approximations. we demonstrate the efficacy of the nonparametric approximation with a polynomial hierarchical logistic regression model and a nonlinear matrix image factorization model. we obtain predictive performance as good as or better coverage than more specialized variational methods and sample - based approximations. the method is easy to apply to more comprehensive general comparison graphical models for which standard variational methods are difficult to derive.", "histories": [["v1", "Mon, 18 Jun 2012 15:32:05 GMT  (655kb)", "http://arxiv.org/abs/1206.4665v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["samuel gershman", "matthew d hoffman", "david m blei"], "accepted": true, "id": "1206.4665"}, "pdf": {"name": "1206.4665.pdf", "metadata": {"source": "META", "title": "Nonparametric Variational Inference", "authors": ["Samuel J. Gershman", "Matthew D. Hoffman", "David M. Blei"], "emails": ["sjgershm@princeton.edu", "mdhoffma@cs.princeton.edu", "blei@cs.princeton.edu"], "sections": [{"heading": "1. Introduction", "text": "Approximate posterior inference\u2014estimating the conditional distribution of hidden variables given some observations\u2014is an important problem in many settings. In this paper, we develop a new variational inference algorithm for complex probabilistic mod-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nels. Compared to traditional variational methods, our method can capture more expressive distributions and be applied to a wider class of models.\nVariational inference methods define some restricted family of distributions over the hidden variables \u03b8 and try to find the member of that family that is closest to the posterior. The family is chosen so that the problem of finding the distribution q that best approximates the posterior becomes a tractable optimization problem.\nVariational methods are effective and widely used. These methods usually find a unimodal approximation of the posterior, especially when the variational family is the commonly chosen mean-field family (Jordan et al., 1999; Beal, 2003). Such an approximation is inadequate when the posterior is multimodal. Furthermore, variational inference algorithms are challenging to derive for models that lack conditional distributions in tractable exponential families (i.e., for models without conditional conjugacy).\nWe develop a variational inference method for continuous hidden variables that captures multimodality and can be applied to many non-conjugate models. The variational family is a mixture of Gaussians, where the variational parameters are the locations and variances of each mixture component. This family of distributions resembles classical kernel density estimators from nonparametric statistics (Silverman, 1986). To approximate the variational objective function, we use Taylor series approximations of the log joint distribution and a bound on the entropy. We call this method nonparametric variational inference (NPV). In contrast to traditional unimodal variational distributions, the multiple components of the mixture can capture different aspects of the posterior.\nWhile mixture approximations have been studied in the variational inference literature (Bishop et al., 1998;\nJaakola & Jordan, 1998), we develop this idea into a more generally applicable framework. (We discuss other approaches to mixture approximations in Section 4.) NPV is \u201cgeneral\u201d in the sense that it is not tailored to a specific model, only requiring that the first and second derivatives of the log joint probability log p(\u03b8, y) be computable. Thus, it can be used in non-conjugate settings, i.e., where conditionals of the the individual hidden variables cannot be computed, such as in Bayesian models with non-conjugate priors. While previous methods for variational inference in non-conjugate models rely on mathematics tailored to the problem at hand, NPV is easily adapted to many settings.\nIn the following sections, we describe the variational objective function using this family and a generalpurpose algorithm to approximately optimize it. We illustrate its performance on two models. First, we show that it performs as well in Bayesian logistic regression as the method of Jaakkola & Jordan (2000), which is tailored to that specific model. Second, we show that it outperforms several MCMC methods for a non-conjugate matrix factorization model of brain activity data (Gershman et al., 2011). Nonparametric variational inference is a promising strategy for approximating posterior distributions in complex probabilistic models."}, {"heading": "2. Variational inference", "text": "We consider the problem of computing the posterior distribution of hidden variables \u03b8 \u2208 RD given observed data y,\np(\u03b8|y) = p(y|\u03b8)p(\u03b8) p(y) . (1)\nThis computation is analytically intractable for many models of interest because the denominator is difficult to compute.\nThe idea behind variational methods is to approximate p(\u03b8|y) with a distribution q(\u03b8) that belongs to a constrained family of distributions, indexed by a variational parameter (Jordan et al., 1999; Beal, 2003). The goal is to choose a member of that family that is \u201dclosest\u201d to the posterior. In variational inference, closeness is measured by Kullback-Leibler (KL) divergence,\nKL[q(\u03b8)||p(\u03b8|y)] = Eq [ log q(\u03b8)\np(\u03b8|y)\n] . (2)\nThus, inference becomes an optimization problem: we choose the variational parameter to minimize the KL\ndivergence. The family of distributions is chosen to make this optimization tractable.\nThe KL divergence is difficult to optimize because it requires knowing the distribution that we are trying to approximate. In variational inference, we maximize an objective that is equal to the negative KL divergence plus a constant. Recall that KL[q(\u03b8)||p(\u03b8|y)] \u2265 0. We define a lower bound on the log marginal likelihood (evidence) log p(y) through the relation\nlog p(y) = F [q] + KL[q(\u03b8)||p(\u03b8|y)], (3)\nwhere F [q] = Eq [ log p(y, \u03b8)\nq(\u03b8)\n] = H[q] + Eq [f(\u03b8)] (4)\nis the negative free energy, also known as the evidence lower bound (ELBO). HereH[q] is the entropy of q and f(\u03b8) = log p(y, \u03b8). The ELBO is equal to the negative KL divergence plus the marginal distribution of the observations, which is constant with respect to the family q. It therefore reaches a maximum when p(\u03b8|y) = q(\u03b8), where the KL is zero. Note that this is only attainable when the target posterior p(\u03b8|y) is in the variational family, which it usually is not. Typically, q will be constrained to a family of simpler distributions, and F [q] is optimized to find the distribution in this family that is closest (in KL) to the true posterior.\nThe most commonly used variational inference algorithm is mean-field variational inference. Mean-field methods find q from the family of factorized posteriors: q(\u03b8) = \u220f i qi(\u03b8i), where it is often convenient to choose qi(\u03b8i) to have the same functional form as the conditional distribution p(\u03b8i|\u03b8\u2212i, y). When p(\u03b8i) is chosen to be conjugate to p(y|\u03b8), the calculus of variations leads to closed-form coordinate ascent updates that converge to a local maximum of F [q] (Beal, 2003).\nDespite the computational convenience of the meanfield approximation, it can be overly restrictive if there are strong dependencies between the hidden variables in the posterior distribution. Moreover, the closedform updates are only available when using conjugate priors; many likelihood models of interest, such as logistic regression and the multilayer perceptron, cannot be paired with conjugate priors, making the application of mean-field methods more difficult."}, {"heading": "3. Nonparametric variational inference", "text": "We now consider a flexible family of variational approximations that admits an efficient inference algorithm. Our algorithm is appropriate for models with continuous-valued hidden random variables, and does not require conjugacy between pairs of variables.\nWe choose the distribution q(\u03b8) to be a uniformlyweighted Gaussian mixture with isotropic covariances,\nq(\u03b8) = 1\nN N\u2211 n=1 N (\u03b8;\u00b5n, \u03c32nI), (5)\nwhere \u00b5n is the mean of the nth Gaussian component and \u03c32n is its variance. We call this a \u201cnonparametric\u201d family: We are making a weak set of assumptions about the shape of the posterior, since the Gaussian mixture family can approximate arbitrarily complex posteriors given a sufficient number of components. Further, this family resembles kernel density estimators used in classical nonparametric statistics (Silverman, 1986), with \u00b5n playing the role of a kernel center and \u03c32n playing the role of a bandwidth parameter."}, {"heading": "3.1. The Evidence Lower Bound", "text": "If q is in the family defined by Eq. 5, we cannot compute the ELBO F [q]; in general there is no closedform expression either for the expectation of a nonlinear function under a Gaussian distribution or for the entropy of a mixture of Gaussians. However, we can approximate the ELBO and optimize this approximation (see Lawrence, 2000; Honkela et al., 2007, for other approaches to this problem). First, we lower bound the entropy term H[q]. Then, we approximate the expected log joint Eq[log p(y, \u03b8)].\nWe lower bound the entropy (the first term in Eq. 4) using Jensen\u2019s inequality (Huber et al., 2008),\nH[q] = \u2212 \u222b \u03b8 q(\u03b8) log q(\u03b8)d\u03b8\n= \u2212 \u222b \u03b8 q(\u03b8) log 1 N N\u2211 n=1 N (\u03b8;\u00b5n, \u03c32nI)d\u03b8\n\u2265 \u2212 1 N N\u2211 n=1 log \u222b \u03b8 q(\u03b8)N (\u03b8;\u00b5n, \u03c32nI)d\u03b8. (6)\nEach integral in Eq. 6 is the sum of N convolved Gaussians, each component convolved with the nth. We obtain the final bound by using the fact that the convolution of two Gaussians is another Gaussian,\nH[q] \u2265 \u2212 1 N N\u2211 n=1 log qn, (7)\nwhere qn = 1 N \u2211N j=1N (\u00b5n;\u00b5j , (\u03c32n + \u03c32j )I).\nWe now turn to the expected log joint f(\u03b8), which is the second term in Eq. 4,\nEq[f(\u03b8)] = 1\nN N\u2211 n=1 \u222b \u03b8 N (\u03b8;\u00b5n, \u03c32nI)f(\u03b8)d\u03b8.\nWe approximate each term in this sum with a secondorder Taylor series expansion of f(\u03b8) around \u00b5n,\nf(\u03b8) \u2248 f\u0302n(\u03b8) =f(\u00b5n) +\u2207f(\u00b5n)(\u03b8 \u2212 \u00b5n)+ 1\n2 (\u03b8 \u2212 \u00b5n)>Hn(\u03b8 \u2212 \u00b5n), (8)\nwhere Hn = \u22072\u03b8f(\u03b8) is the Hessian matrix of second derivatives. The approximate expectation is\nEq[f(\u03b8)] \u2248 1\nN N\u2211 n=1 \u222b \u03b8 N (\u03b8;\u00b5n, \u03c32nI)f\u0302n(\u03b8)d\u03b8\n= 1\nN N\u2211 n=1 f(\u00b5n) + \u03c32n 2 Tr(Hn). (9)\nThis approximation is known as the multivariate delta method for moments (Bickel & Doksum, 2007), and is often used within variational inference schemes for models that cannot exploit conjugacy (e.g., Braun & McAuliffe, 2010).\nFinally, we add the bound in Eq. 7 to the approximation in Eq. 9. This gives the approximate ELBO1\nL2[q] = 1\nN N\u2211 n=1 f(\u00b5n) + \u03c32n 2 Tr(Hn)\u2212 log qn. (10)\nIntuitively, the likelihood term, f(\u00b5n), encourages placing samples in areas of high probability density, while the entropy term, log qn, penalizes \u201covercrowded\u201d locations (i.e., where many samples are near each other). The Hessian term captures the local curvature of the posterior, discouraging the algorithm from placing samples in areas with high probability density but low volume (and therefore low mass).\nWe note two attractive properties of the approximate ELBO in Eq. 10. First, we have made no conjugacy assumptions; our only requirement is that the log joint f(\u03b8) = log p(\u03b8, y) is twice differentiable (or thrice differentiable if one wishes to use gradient ascent; but see below). Second, although the objective function involves a Hessian term, it only requires the calculation of the diagonal components; the cost of computing the diagonal of the Hessian is comparable to the cost of computing the gradient."}, {"heading": "3.2. Optimizing the ELBO", "text": "Eq. 10 is a tractable approximation of the ELBO in Eq. 4. Our goal is now to maximize Eq. 10 with respect\n1When some parameters are bounded, one can use nonlinear transformations to map an unbounded parameterization to a bounded range (e.g., the logistic function for variables in [0, 1]). In this case, one should add log |J| to the approximate ELBO, where J is the Jacobian matrix of first derivatives of the transformation.\nAlgorithm 1 Nonparametric variational inference\nInput: data y, number of components N . Initialize \u03b81:N randomly. repeat\nfor n = 1 to N do \u00b5n \u2190 argmax\u00b5n L1[q]. end for \u03c321:N \u2190 argmax\u03c321:N L2[q].\nuntil change of L2[q] is less than 0.0001.\nto the variational parameters \u00b5n and \u03c3n. One option is to use a gradient-based solver. However, there is a serious computational problem with this approach\u2014 computing the gradient of Eq. 10 requires computing a matrix of third derivatives, since we must compute the gradient of the Hessian trace Tr(Hn). This leads to a cost that is quadratic in the number of parameters.\nTo avoid the calculation of third derivatives, we use both first- and second-order approximations of the ELBO. The first-order approximation is\nL1[q] = 1\nN N\u2211 n=1 f(\u00b5n)\u2212 log qn. (11)\nThis is obtained in the same way as Eq. 10, but using a first-order approximation of f(\u03b8), rather than the second-order approximation in Eq. 8. We iterate between optimizing the variances \u03c3 using the secondorder approximation in Eq. 10 and optimizing the means \u00b5 using the Eq. 11. Each optimization is done using L-BFGS. We found that it is more efficient to optimize L1[q] with respect to one mean at a time, holding the others fixed, and iterating over components. This coordinate ascent procedure converges faster than batch optimization of \u00b51:N , but coordinate and batch optimization produce similar results. Our algorithm is summarized in Algorithm 1.\nBoth L1[q] and L2[q] are approximations of F [q]. Splitting the optimization problem into these two steps allows us to avoid the cost of calculating the gradient of \u03c32n 2 Tr(Hn) with respect to the means \u00b5. In our experiments, 3 iterations typically proved sufficient to achieve convergence. Although the first-order approximation may appear drastic, it still achieves our main goal: placing kernels in areas of high probability mass. Further simulation work is needed to assess the tradeoffs involved in this approximation.\nAs an illustration, we constructed a synthetic multimodal \u201cposterior\u201d f(\u03b8) using a mixture of skewed bivariate t-distributions. Figure 1 shows f(\u03b8) alongside the NPV approximation with several settings of N .\nWith N = 1, the approximation is only able to capture a single mode, but with N = 2 it is able to capture the two modes with high fidelity, though it cannot capture the true covariance structure or the heavy tails. With N = 10, the approximation better captures the skew by placing several low-variance components along the diagonal. This illustration demonstrates some strengths and weaknesses of the NPV approximation: it can capture multi-modality, but the isotropic covariance of the components makes it difficult to capture skew in the posterior. This problem can be ameliorated by using more components.\nNote that the number of parameters that need to be fit with NPV increases linearly with N (the number of components in the mixture). This may pose challenges for models with a large number of hidden variables. On the other hand, it may only be necessary to use a small number of components (e.g., less than 10) to capture the major aspects of the posterior (as suggested by Figure 1). We note also that the KL divergence between the mixture distribution q and the true posterior decreases at best logarithmically in the number of mixture components N , suggesting that there may be diminishing returns to using very large values of N (Jaakola & Jordan, 1998)."}, {"heading": "3.3. Relationship to other algorithms", "text": "The NPV objective relates to several other methods. When there is one component N = 1, the entropy term log q1 does not depend on the mean \u00b51, and when \u03c3 2 1 becomes sufficiently small, the Hessian term of Eq. 10 goes to 0. Consequently, the NPV objective when N = 1 and \u03c31 \u2192 0 is\nL[q] = log p(y, \u00b5) + const. = log p(\u03b8 = \u00b5|y) + const.\nThe maximum of this function is the maximum a posteriori (MAP) solution.\nWhen N = 1 and \u03c321 is allowed to vary, we obtain a Gaussian approximation centered around the MAP solution. This can be understood as a diagonalized Laplace approximation (MacKay, 1995), i.e., where we ignore correlations between the dimensions of \u03b8. The Laplace approximation has drawbacks: for example, it is not invariant to reparameterization, it performs badly when the mean and mode of the posterior are far apart, and it cannot capture multiple modes (Beal, 2003).\nWhen N > 1 and \u03c32n \u2192 0, we obtain a quasiMonte Carlo approximation of the posterior, q(\u03b8) = 1 N \u2211N n=1 \u03b4\u00b5n(\u03b8), where \u03b4\u00b5n(\u00b7) is a Dirac point mass located at \u00b5n. Thus one way to look at the NPV algorithm is as a deterministic sampling method."}, {"heading": "4. Related work", "text": "Approximate inference for non-conjugate models is an active area of research. Some authors have used numerical or Monte Carlo methods to approximate intractable integrals. For example, Lawrence et al. (2004) used importance sampling to approximate the expectations required for inference in a Bayesian model of microarray images. Ihler et al. (2009) generalized particle filtering for approximate inference in factor graphs with continuous variables. Honkela et al. (2007) used numerical quadrature to approximate expectations in a nonlinear factor analysis model. These techniques are useful, but may fail in high dimensions.\nSeveral researchers use specialized approximations for certain classes of models, such as those with logistic nonlinearities (e.g., Jaakkola & Jordan, 2000; Khan et al., 2010). In contrast, our goal is to develop an algorithm for inference in general non-conjugate models with continuous hidden variables.\nClosely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations. Recently, Bouchard & Zoeter (2009) revisited this approach using soft-binning functions. NPV can be viewed as a special case of MMF because each component factorizes into a collection of one-dimensional Gaussian sub-components (due to the isotropic covariances). Our innovation is that we exploited the functional form of the Gaussian mixture to derive an efficient approximate inference algorithm. NPV requires no user input beyond specifying the joint likelihood function, its gradient, optionally the diagonal of its Hessian, and the number of components. These modest requirements give NPV a practical advantage in situations where it is difficult to derive the MMF up-\ndates."}, {"heading": "5. Applications", "text": "In this section, we apply the NPV algorithm to several probabilistic models and compare its performance to other widely-used methods."}, {"heading": "5.1. Logistic regression", "text": "In this section, we ask whether NPV produces reasonable approximations for models where closed-form updates can be applied. We focus on a hierarchical logistic regression model and compare its accuracy to a standard variational treatment (Jaakkola & Jordan, 2000, henceforth \u201cJJ\u201d).\nGenerative model. The observed data y = {c,X} consist of T binary class labels, ct \u2208 {\u22121, 1}, and K covariates for each datapoint, xt \u2208 RK . The hidden variables \u03b8 = {w, \u03b1} consist of K regression coefficients wk \u2208 R, and a precision parameter \u03b1 \u2208 R+. We assume the following model (MacKay, 1995):\np(\u03b1) = Gamma(\u03b1; a, b) (12) p(wk|\u03b1) = N (wk; 0, \u03b1\u22121) (13)\np(ct = 1|xt,w) = 1\n1 + exp(\u2212w>xt) . (14)\nHere a and b are hyperparameters (shape and inverse scale, respectively) that we assume to be fixed.\nResults. We evaluated NPV and JJ on 13 binary classification data sets compiled by Mika et al. (1999).2 The number of covariates in these data sets ranges from 2 to 60, and the number of observations ranges from 24 to 7400. We used split-half training/testing. We used the following hyperparameter settings: a = 1, b = 0.01, N = 5 (similar results were obtained with N = 10).\nThe predictive distribution for NPV was approximated using a Monte Carlo estimate. We drew 1000 samples from the fitted variational mixture of Gaussians and estimated the log-likelihood of the test data as an average of the log-likelihoods under each sample. Figure 2 (top) compares the log-likelihood of the test data under the NPV and JJ approximations. NPV and JJ achieve statistically indistinguishable accuracy. Figure 2 (bottom) shows the same comparison for the ELBO, confirming that NPV closely mimics the JJ approximation. We emphasize that JJ exploits special properties of the generative model (i.e., a clever lower bound on the logistic sigmoid function), whereas NPV only uses\n2http://theoval.cmp.uea.ac.uk/matlab/default. html\nthe derivatives of the joint distribution.\nWe also fit the model using an MCMC algorithm, Hamiltonian (or Hybrid) Monte Carlo algorithm (HMC; Neal, 2011), which takes the same inputs as NPV (the log joint probability and its gradient). HMC uses the gradient of f(\u03b8) to efficiently explore the posterior, making it one of the most effective samplers for models with continuous variables. With 1000 samples, we found that this algorithm predicts held-out data significantly worse (p < 0.00001, Wilcoxon signed-rank test) compared to NPV and JJ. Presumably the inferior performance of HMC could be improved by running the sampler for longer, but this would result in greater computational overhead."}, {"heading": "5.2. Topographic latent source analysis", "text": "We now study our method with a more complicated model, for which standard variational algorithms are inapplicable. We apply the NPV approximation to a nonlinear latent variable model of functional magnetic resonance imaging (fMRI) data. Data from fMRI experiments contain measurements of brain activity that are collected while a subject performs a task, such as labeling images. The goal of these experiments is to understand the relationship between cognitive pro-\ncesses and brain activity. One reason this problem is complicated is that fMRI data is spatial. Brain activity is measured in 3D brain-space (a grid of \u201cvoxels\u201d). Measurements made on nearby voxels are dependent.\nGershman et al. (2011) developed a factorization model of spatial patterns in fMRI data, topographic latent source analysis (TLSA). TLSA decomposes voxel activations into a set of spatial functions (topographic latent sources). These functions are related to task and cognitive variables (called \u201ccovariates\u201d) through a weight matrix that is also inferred from the data. We can evaluate the quality of a fitted model by using it to predict held-out brain data, conditional on covariates. Unlike traditional probabilistic matrix factorization models, TLSA is not conditionally conjugate and closed-form mean-field inference is not available. Gershman et al. (2011) approximated the posterior with MCMC, but their method was too slow to analyze large data sets.\nGenerative model. Each datapoint t in an fMRI experiment consists of a vector of V voxel activations, ut \u2208 RV , and a vector of C covariates, xt \u2208 RC . The intuition behind TLSA is that the spatial organization of voxel activations arises from a small number of anatomically localized brain regions involved in processing the task. Formally, TLSA decomposes the voxel activations into a covariate-dependent superposition of K latent sources:\nutv = C\u2211 c=1 xtc K\u2211 k=1 wckgkv + tv, (15)\nwhere tv \u223c N (0, \u03c4\u22121) is a Gaussian noise term, wck is a weight that specifies how covariate c influences source k, and gkv is the activation of source k in voxel v. This generative process (illustrated in Figure 3)\ncan be viewed as a probabilistic matrix factorization model where {gk} are basis images that are combined to produce the observed neural activity.\nEach basis image is constructed by evaluating a parameterized spatial basis function at each voxel location. Following Gershman et al. (2011), we chose this function to be a radial basis function with parameters \u03c9k = {r\u0304k, \u03bbk}:\ngkv = exp { \u2212\u03bb\u22121k ||rv \u2212 r\u0304k|| 2 } , (16)\nwhere r\u0304k \u2208 [0, 1]M is the source center (in normalized coordinates), \u03bbk \u2208 R+ is a width parameter, and rv \u2208 [0, 1] is the location of voxel v. In the notation of Section 2, the observed variables are y = {X,U,R} and the hidden variables are \u03b8 = {W,G}.\nTo complete the generative model, we placed the following priors on the parameters:\nwck \u223c N (0, \u03c32w), r\u0304kd \u223c Beta(1, 1), \u03bbk \u223c Exp(\u03c1).\nIn all our analyses, we used the following hyperparameter settings: \u03c4 = 1, \u03c32w = 5, \u03c1 = 1.\nResults. We fit TLSA to data collected by Mason and Just (unpublished), involving subjects viewing words. Each word was either the name of a type of tool or of a type of building (i.e., there were 2 classes), and the subject\u2019s task was to think about the word and its properties. There were a total of 84 trials per subject (see Gershman et al., 2011, for more details). We restricted our analysis to a 1,323 voxels (a single slice of the brain activity data) from a single subject.\nWe trained the model on one half and then generated predictions of the neural data for the other half, conditioning on the test covariates. For NPV, we approximated the predictive distribution using a Monte Carlo estimate, as described in the previous section. As an illustration of the model fits, Figure 4 shows the average data and reconstructions derived from the MAP estimate and the NPV approximation. While the MAP estimate captures the global pattern of activity, each component of the NPV approximation captures small\nidiosyncrasies that may be difficult to extract using a single point estimate. In other words, the NPV approximation captures several local maxima of the posterior; we next show that this translates into better predictive accuracy.\nWe evaluated the quality of the reconstruction by calculating the mean-squared reconstruction error of held-out neural data, a quantity proportional to the negative log-likelihood of the held-out data. We also fit TLSA using HMC (see above); we collected 5000 samples, keeping the last 200 for the predictive distribution. We repeated this procedure for the MetropolisHastings (MH) sampler used in the original TLSA paper (Gershman et al., 2011). The results are shown in Figure 5. NPV works well with a varying number of components (though best when N > 3), substantially outperforming the MAP and MCMC estimators.\nWe re-emphasize here that TLSA is non-conjugate, and hence MMF cannot be applied without using specially-tailored approximations (Lawrence, 2000). Note that while both MH and HMC are asymptotically guaranteed to perfectly approximate the posterior, these algorithms require tuning and are often slow to converge. In our experiments, NPV was about 3 times faster than HMC."}, {"heading": "6. Discussion", "text": "We developed an approximate inference method for posteriors that do not necessarily enjoy the conjugacy properties that make common variational approximations (e.g., mean-field) possible. Our algorithm is easy to apply to new probabilistic models; all that is required is the likelihood function and its gradient (a requirement shared by many other algorithms, including MAP estimation and HMC). When applied to a hierarchical logistic regression model, we found that NPV incurs little loss in accuracy compared to a more specialized variational algorithm (Jaakkola & Jordan, 2000). We further showed, using a nonlinear latent variable model of fMRI data, that NPV can find an approximation of the posterior that improves predictive performance over MAP estimation and MCMC.\nNPV has limitations. First, it assumes a simple approximating family. This could be improved by introducing a full covariance matrix into the component distributions or by allowing the components to be nonuniformly weighted. Further, NPV only applies to continuous variables. We plan to extend it to models with discrete hidden variables.\nIn summary, NPV is a posterior inference algorithm that is a step towards generically applicable variational approximations. The need for such approximations is increasing, as researchers begin to explore more and more complicated probabilistic models to cope with the increasing complexity of large data sets. Our hope is that by employing generic inference algorithms, the hard work of inference can proceed \u201cinvisibly,\u201d and researchers can devote more time to testing and refining the assumptions of their models.\nAcknowledgements. Samuel J. Gershman is supported by a graduate research fellowship from the NSF. Matthew D. Hoffman is supported by NSF ATM-0934516, DOE DE-SC0002099, and IES R305D100017. David M. Blei is supported by ONR N00014-11-1-0651, NSF CAREER 0745520, NSF IIS-1009542, AFOSR FA9550-09-1-0668, the Alfred P. Sloan foundation, and a grant from Google."}], "references": [{"title": "Variational algorithms for approximate Bayesian inference", "author": ["M.J. Beal"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Beal,? \\Q2003\\E", "shortCiteRegEx": "Beal", "year": 2003}, {"title": "Mathematical statistics: Basic ideas and selected topics, 2nd edn.(Vol", "author": ["P.J. Bickel", "K.A. Doksum"], "venue": null, "citeRegEx": "Bickel and Doksum,? \\Q2007\\E", "shortCiteRegEx": "Bickel and Doksum", "year": 2007}, {"title": "Approximating posterior distributions in belief networks using mixtures", "author": ["C.M. Bishop", "N. Lawrence", "T. Jaakkola", "M.I. Jordan"], "venue": null, "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "Split variational inference", "author": ["G. Bouchard", "O. Zoeter"], "venue": "In ICML, pp. 57\u201364", "citeRegEx": "Bouchard and Zoeter,? \\Q2009\\E", "shortCiteRegEx": "Bouchard and Zoeter", "year": 2009}, {"title": "Variational inference for largescale models of discrete choice", "author": ["M. Braun", "J. McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Braun and McAuliffe,? \\Q2010\\E", "shortCiteRegEx": "Braun and McAuliffe", "year": 2010}, {"title": "A topographic latent source model for fMRI data", "author": ["S.J. Gershman", "D.M. Blei", "F. Pereira", "K.A. Norman"], "venue": null, "citeRegEx": "Gershman et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gershman et al\\.", "year": 2011}, {"title": "Blind separation of nonlinear mixtures by variational bayesian learning", "author": ["A. Honkela", "H. Valpola", "A. Ilin", "J. Karhunen"], "venue": "Digital Signal Processing,", "citeRegEx": "Honkela et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Honkela et al\\.", "year": 2007}, {"title": "On entropy approximation for gaussian mixture random vectors", "author": ["M.F. Huber", "T. Bailey", "H. Durrant-Whyte", "U.D. Hanebeck"], "venue": "In Multisensor Fusion and Integration for Intelligent Systems,", "citeRegEx": "Huber et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Huber et al\\.", "year": 2008}, {"title": "Particle-based variational inference for continuous systems", "author": ["A.T. Ihler", "A.J. Frank", "P. Smyth"], "venue": null, "citeRegEx": "Ihler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2009}, {"title": "Bayesian parameter estimation via variational methods", "author": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "Statistics and Computing,", "citeRegEx": "Jaakkola and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 2000}, {"title": "Improving the mean field approximation via the use of mixture distributions", "author": ["T.S. Jaakola", "M.I. Jordan"], "venue": "In Learning Graphical Models", "citeRegEx": "Jaakola and Jordan,? \\Q1998\\E", "shortCiteRegEx": "Jaakola and Jordan", "year": 1998}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Variational bounds for mixed-data factor analysis", "author": ["M.E. Khan", "B. Marlin", "G. Bouchard", "K. Murphy"], "venue": "In NIPS", "citeRegEx": "Khan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Khan et al\\.", "year": 2010}, {"title": "Variational inference in probabilistic models", "author": ["N.D. Lawrence"], "venue": "PhD thesis,", "citeRegEx": "Lawrence,? \\Q2000\\E", "shortCiteRegEx": "Lawrence", "year": 2000}, {"title": "Reducing the variability in cDNA microarray image processing by Bayesian inference", "author": ["N.D. Lawrence", "M. Milo", "M. Niranjan", "P. Rashbass", "S. Soullier"], "venue": null, "citeRegEx": "Lawrence et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lawrence et al\\.", "year": 2004}, {"title": "Probable networks and plausible predictions-a review of practical Bayesian methods for supervised neural networks. Network: Computation", "author": ["D.J.C. MacKay"], "venue": "Neural Systems,", "citeRegEx": "MacKay,? \\Q1995\\E", "shortCiteRegEx": "MacKay", "year": 1995}, {"title": "Fisher discriminant analysis with kernels", "author": ["S. Mika", "G. Ratsch", "J. Weston", "B. Scholkopf", "Mullers", "KR"], "venue": "In Neural Networks for Signal Processing IX,", "citeRegEx": "Mika et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Mika et al\\.", "year": 1999}, {"title": "MCMC using Hamiltonian dynamics", "author": ["R.M. Neal"], "venue": "Handbook of Markov Chain Monte Carlo", "citeRegEx": "Neal,? \\Q2011\\E", "shortCiteRegEx": "Neal", "year": 2011}, {"title": "Density Estimation for Statistics and Data Analysis", "author": ["B.W. Silverman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Silverman,? \\Q1986\\E", "shortCiteRegEx": "Silverman", "year": 1986}], "referenceMentions": [{"referenceID": 11, "context": "These methods usually find a unimodal approximation of the posterior, especially when the variational family is the commonly chosen mean-field family (Jordan et al., 1999; Beal, 2003).", "startOffset": 150, "endOffset": 183}, {"referenceID": 0, "context": "These methods usually find a unimodal approximation of the posterior, especially when the variational family is the commonly chosen mean-field family (Jordan et al., 1999; Beal, 2003).", "startOffset": 150, "endOffset": 183}, {"referenceID": 18, "context": "This family of distributions resembles classical kernel density estimators from nonparametric statistics (Silverman, 1986).", "startOffset": 105, "endOffset": 122}, {"referenceID": 5, "context": "Second, we show that it outperforms several MCMC methods for a non-conjugate matrix factorization model of brain activity data (Gershman et al., 2011).", "startOffset": 127, "endOffset": 150}, {"referenceID": 11, "context": "The idea behind variational methods is to approximate p(\u03b8|y) with a distribution q(\u03b8) that belongs to a constrained family of distributions, indexed by a variational parameter (Jordan et al., 1999; Beal, 2003).", "startOffset": 176, "endOffset": 209}, {"referenceID": 0, "context": "The idea behind variational methods is to approximate p(\u03b8|y) with a distribution q(\u03b8) that belongs to a constrained family of distributions, indexed by a variational parameter (Jordan et al., 1999; Beal, 2003).", "startOffset": 176, "endOffset": 209}, {"referenceID": 0, "context": "When p(\u03b8i) is chosen to be conjugate to p(y|\u03b8), the calculus of variations leads to closed-form coordinate ascent updates that converge to a local maximum of F [q] (Beal, 2003).", "startOffset": 164, "endOffset": 176}, {"referenceID": 18, "context": "Further, this family resembles kernel density estimators used in classical nonparametric statistics (Silverman, 1986), with \u03bcn playing the role of a kernel center and \u03c3 n playing the role of a bandwidth parameter.", "startOffset": 100, "endOffset": 117}, {"referenceID": 7, "context": "4) using Jensen\u2019s inequality (Huber et al., 2008),", "startOffset": 29, "endOffset": 49}, {"referenceID": 15, "context": "This can be understood as a diagonalized Laplace approximation (MacKay, 1995), i.", "startOffset": 63, "endOffset": 77}, {"referenceID": 0, "context": "The Laplace approximation has drawbacks: for example, it is not invariant to reparameterization, it performs badly when the mean and mode of the posterior are far apart, and it cannot capture multiple modes (Beal, 2003).", "startOffset": 207, "endOffset": 219}, {"referenceID": 11, "context": "For example, Lawrence et al. (2004) used importance sampling to approximate the expectations required for inference in a Bayesian model of microarray images.", "startOffset": 13, "endOffset": 36}, {"referenceID": 7, "context": "Ihler et al. (2009) generalized particle filtering for approximate inference in factor graphs with continuous variables.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Honkela et al. (2007) used numerical quadrature to approximate expectations in a nonlinear factor analysis model.", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "Several researchers use specialized approximations for certain classes of models, such as those with logistic nonlinearities (e.g., Jaakkola & Jordan, 2000; Khan et al., 2010).", "startOffset": 125, "endOffset": 175}, {"referenceID": 2, "context": "Closely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations.", "startOffset": 68, "endOffset": 129}, {"referenceID": 13, "context": "Closely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations.", "startOffset": 68, "endOffset": 129}, {"referenceID": 2, "context": "Closely related to our method is the mixture meanfield (MMF) method (Bishop et al., 1998; Jaakola & Jordan, 1998; Lawrence, 2000), which models the posterior as a mixture of mean-field approximations. Recently, Bouchard & Zoeter (2009) revisited this approach using soft-binning functions.", "startOffset": 69, "endOffset": 236}, {"referenceID": 15, "context": "We assume the following model (MacKay, 1995):", "startOffset": 30, "endOffset": 44}, {"referenceID": 16, "context": "We evaluated NPV and JJ on 13 binary classification data sets compiled by Mika et al. (1999). The number of covariates in these data sets ranges from 2 to 60, and the number of observations ranges from 24 to 7400.", "startOffset": 74, "endOffset": 93}, {"referenceID": 16, "context": "Each point represents one of 13 data sets compiled by Mika et al. (1999). For NPV, N = 5 components were used (similar results were obtained with N = 10).", "startOffset": 54, "endOffset": 73}, {"referenceID": 17, "context": "We also fit the model using an MCMC algorithm, Hamiltonian (or Hybrid) Monte Carlo algorithm (HMC; Neal, 2011), which takes the same inputs as NPV (the log joint probability and its gradient).", "startOffset": 93, "endOffset": 110}, {"referenceID": 5, "context": "Following Gershman et al. (2011), we chose this function to be a radial basis function with parameters \u03c9k = {r\u0304k, \u03bbk}:", "startOffset": 10, "endOffset": 33}, {"referenceID": 5, "context": "We repeated this procedure for the MetropolisHastings (MH) sampler used in the original TLSA paper (Gershman et al., 2011).", "startOffset": 99, "endOffset": 122}, {"referenceID": 13, "context": "We re-emphasize here that TLSA is non-conjugate, and hence MMF cannot be applied without using specially-tailored approximations (Lawrence, 2000).", "startOffset": 129, "endOffset": 145}], "year": 2012, "abstractText": "Variational methods are widely used for approximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approximations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data. Unlike most other variational approximations, using multiple kernels allows the approximation to capture multiple modes of the posterior. We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model. We obtain predictive performance as good as or better than more specialized variational methods and MCMC approximations. The method is easy to apply to graphical models for which standard variational methods are difficult to derive.", "creator": "LaTeX with hyperref package"}}}