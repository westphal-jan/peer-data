{"id": "1206.4666", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices", "abstract": "we present a bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. a gibbs sampler is derived to simulate samples of the common eigenvectors r and the eigenvalues for these matrices. several synthetic examples are used to illustrate the performance of the proposed gibbs sampler and how we then provide comparisons to several other joint diagonalization algorithms, which shows that the calculated gibbs sampler slowly achieves the state - profile of - the - art performance reflected on the examples considered. as a byproduct, the linear output of the gibbs sampler could be used just to effectively estimate the log marginal likelihood, however we employ the approximation based on the bayesian information criterion ( cp bic ) which stated in the diagram synthetic examples considered correctly we located the number parameter of common eigenvectors. we then succesfully applied the sampler to the source separation problem as hell well as towards the common principal component analysis and the common distributed spatial frequency pattern analysis problems.", "histories": [["v1", "Mon, 18 Jun 2012 15:32:46 GMT  (472kb)", "http://arxiv.org/abs/1206.4666v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "stat.CO cs.LG stat.ME", "authors": ["mingjun zhong", "mark a girolami"], "accepted": true, "id": "1206.4666"}, "pdf": {"name": "1206.4666.pdf", "metadata": {"source": "META", "title": "A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices", "authors": ["Mingjun Zhong", "Mark Girolami"], "emails": ["mingjun.zhong@gmail.com", "m.girolami@ucl.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "Joint diagonalization is a technique to simultaneously diagonalize a series of K square symmetric matrices C = {Ck}Kk=1 where Ck \u2208 RN\u00d7N (Friedland, 1983; Cardoso & Souloumiac, 1996; Pham, 2001;\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nvan der Veen, 2001; Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009). The standard joint diagonalization scheme finds a common matrix W \u2208 RN\u00d7N such that WCkW\nT for all k are diagonal matrices. There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al., 2008). However, it is well known that for more than two matrices it may not be possible to achieve joint diagonalization exactly and one must resort to approximate joint diagonalization for such applications. Furthermore, for some applications such as BSS, the matrices Ck may not even be symmetric.\nIn this paper, we generalize the standard joint diagonalization scheme by proposing an underlying statistical model. Rather than directly seeking W, we infer a global matrix B \u2208 RN\u00d7M , which is not necessarily square, and a series of K diagonal matrices \u039b = {\u039bk}Kk=1 where \u039bk = diag(\u03bbk1 , \u03bbk2 , \u00b7 \u00b7 \u00b7 , \u03bbkM ) are the eigenvalues for every k\nCk = B\u039bkB T + Ek (1)\nwhere Ek = (e k 1 , e k 2 , \u00b7 \u00b7 \u00b7 , ekN ) are assumed errors. A Gaussian error model ekn \u223c N (0, \u03c32kI) is assumed, therefore each element ekij of Ek follows a Gaussian distribution with zero mean and variance \u03c32k. We note that in this formulation the matrices Ck may not be symmetric. In this paper we restrict ourselves to the case BTB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009). The columns of B are then recognized as the common eigenvectors.\nThis model is a generalization of the subspace fitting models of (van der Veen, 2001; Yeredor, 2002), where here the errors Ek are represented explicitly.\nVarious joint diagonalization algorithms have been proposed in the literature. The Jacobi method of (Cardoso & Souloumiac, 1996) seeks an orthogonal matrix W for jointly diagonalizing Ck whilst Pham\u2019s algorithm (Pham, 2001) is restricted to positive definite matrices Ck. Compared to these methods, our method is not restricted to positive definite Ck furthermore B could be a non-square matrix. Our approach is most similar to the subspace fitting models (van der Veen, 2001; Yeredor, 2002) although we also treat the error variances as parameters to be inferred. Flurry (Flury, 1984) and Hoff (Hoff, 2009b) proposed CPCA for a series of covariance matrices whose eigenvalues were assumed positive, a restriction which we relax.\nPoint estimate methods such as the expectationmaximisation (EM) algorithm (Dempster et al., 1977) or the AC-DC algorithm (Yeredor, 2002) could be derived to estimate these parameters. In this paper we prefer a Bayesian formulation for the model (1) where the posterior distributions for parameters is provided. A Gibbs sampler is derived to simulate model parameters from their posterior distribution. Based on some data, we compared the Gibbs sampler to the Jacobi method (Cardoso & Souloumiac, 1996), AC-DC (Yeredor, 2002) and the FFDiag algorithm (Ziehe et al., 2004). Across all the results the Gibbs sampler is comparable to both the Jacobi and FFDiag methods whilst outperforming the AC-DC algorithm in terms of the Amari performance index (Amari et al., 1996)."}, {"heading": "2. The Model", "text": ""}, {"heading": "2.1. The Data Likelihood", "text": "Given the model in (1) and the assumed error distribution the likelihood, p(C|\u0398), has the following form\nK\u220f k=1 |2\u03c0\u03c32kI|\u2212 1 2 exp { \u2212 1 2\u03c32k ||Ck \u2212B\u039bkBT ||2F } where \u0398 = (B,\u039b, \u03c3) and ||X||2F = trace(XTX) is the Frobenius norm. For simplicity of representing the conditional distributions, we give an alternative representation of the model (1). Let vec(\u00b7) denote the transformation of a matrix to a vector by concatenation of the columns. Denote xk = vec(Ck),\nA = (B \u2297 1) (1 \u2297 B) where 1 denotes a column vector of size N \u00d7 1 with all the elements being 1, uk = (\u03bb k 1 , \u03bb k 2 , \u00b7 \u00b7 \u00b7 , \u03bbkM )T and k = vec(Ek). Then the model (1) could alternatively be represented as xk = Auk + k. We denote the data X = (x1,x2, \u00b7 \u00b7 \u00b7 ,xK) with dimension N2 \u00d7K and likelihood accordingly p(X|\u0398) = K\u220f k=1 |2\u03c0\u03c32kIN2 |\u2212 1 2 exp { \u2212 1 2\u03c32k ||xk \u2212Auk||22\n} Given the data likelihood and associated priors on the parameters the posterior distribution takes the form\np(B,U, \u03c3|X) \u221d p(X|B,U, \u03c3)p(B)p(U)p(\u03c3)\nwhich is detailed in the following subsection."}, {"heading": "2.2. The Priors and Posteriors", "text": "We employ a Gaussian prior on uk depending on \u03c3 2 k and a hyperparameter \u03c52k such that\np(uk|\u03c32k, \u03c52k) = |2\u03c0\u03c32k\u03c52kIM |\u2212 1 2 exp\n{ \u2212 1\n2\u03c32k\u03c5 2 k\nuTk uk } The conditional posterior for uk has the exact form\np(uk|xk,A, \u03c3k, \u03c5k) \u223c N (\u00b5uk , \u03c32k\u03a3uk)\nwhere \u03a3uk = (\u03c5 \u22122 k IM + A TA)\u22121 & \u00b5uk = \u03a3ukA Txk.\nAn inverse Gamma distribution is employed for \u03c32k such that p(\u03c32k|ak, bk) \u221d (\u03c32k)\u2212ak\u22121 exp { \u2212bk\u03c3\u22122k } , thus the conditional posterior is\np(\u03c32k|xk,A,uk, \u03c5k) \u223c InvGamma(a\u03c32k , b\u03c32k) (2)\nwhere a\u03c32 k = ak + 1 2N 2 + 12M and b\u03c32k = bk + 1 2 ||xk\u2212Auk|| 2 2+ 1 2\u03c5 \u22122 k u T k uk. So p(\u03c3 \u22122 k |xk,A,uk, \u03c5k) \u223c Gamma(a\u03c32 k , 1/b\u03c32 k ) is a Gamma distribution. We also require an inverse Gamma prior for \u03c52k such that p(\u03c52k|ak, bk) \u223c InvGamma(ak, bk), so the posterior for \u03c52k is\np(\u03c52k|uk, \u03c32k) \u223c InvGamma(a\u03c52k , b\u03c52k)\nwhere a\u03c52 k = ak + 1 2M and b\u03c52k = bk + 1 2\u03c3 \u22122 k u T k uk. So p(\u03c5\u22122k |uk, \u03c32k) \u223c Gamma(a\u03c52k , 1/b\u03c52k).\nTo derive the conditional posterior for B, it is convenient to employ the Frobenius norm form of the data likelihood. Since BTB = I, we assume a uniform distribution on the Stiefel manifold for the random matrix\nAlgorithm 1 Gibbs Sampler\nInput: Matrices Ck (k = 1, \u00b7 \u00b7 \u00b7 ,K), number of eigenvectors M , number of samples NSamps Initialize B, U, \u03c3\u22122k , \u03c5 \u22122 k for j = 1 to NSamps do for k = 1 to K do\n(a) Sample uk \u223c N (\u00b5uk , \u03c32k\u03a3uk) (b) Sample \u03c3\u22122k \u223c Gamma(a\u03c32k , 1/b\u03c32k) (c) Sample \u03c5\u22122k \u223c Gamma(a\u03c52k , 1/b\u03c52k)\nend for Sample B \u223c LB(\u039b,\u03a5)\nend for\nB (Chikuse, 2003). The posterior for B then has the following form, p(B|C,\u039b, \u03c3) \u221d K\u220f k=1 exp { tr ( \u039bkB T C T k + Ck 2\u03c32k B )}\n= M\u220f m=1 exp { bTm K\u2211 k=1 \u03bbkm(C T k + Ck) 2\u03c32k bm }\nThis is the matrix Langevin-Bingham (LB) distribution or the matrix Bingham-von Mises-Fisher distribution (Khatri & Mardia, 1977). For simplicity, denote \u03a5k = CTk +Ck\n2\u03c32 k and \u03a5 = {\u03a5k}Kk=1. We denote the matrix LB distribution as LB(\u039b,\u03a5). It should be noted that even though the distribution is represented as a product form, since BTB = I, the columns of B are not statistical independent. Given those conditional posterior distributions, a Gibbs sampler in Algorithm 1 is derived for simulating the parameters. We require to simulate the matrix LB distribution which is described in the following section."}, {"heading": "3. Sampling from the matrix Langevin-Bingham distribution", "text": ""}, {"heading": "3.1. The vector Bingham distribution", "text": "To sample from the matrix LB distribution, we need to draw samples from the following vector Bingham distribution\np(x|\u03a3) = c\u22121(\u03a3) exp(xT\u03a3x)\nwhere \u03a3 is symmetric, and x \u2208 RM such that xTx = 1. This distribution is defined under the uniform measure dSM\u22121(x) in the sphere SM\u22121. The uniform measure dSM\u22121(x) is invariant under the orthogonal transformation. Therefore, for any orthogonal matrix \u0393 \u2208 O(M), set y = \u0393Tx, then y has a Bing-\nham distribution p(y|\u0393T\u03a3\u0393) with respect to the uniform measure in SM\u22121. The eigenvalue decomposition could be applied to \u03a3, then \u03a3 = U\u039bUT , where \u039b = diag(\u03bb1, \u03bb2, \u00b7 \u00b7 \u00b7 , \u03bbM ). Set y = UTx, then y has a simplified Bingham distribution p(y|\u039b). Instead of sampling p(x|\u03a3) directly, we sample the density of y\np(y|\u039b) = c\u22121(\u039b) exp(yT\u039by)\nwith respect to the uniform measure in SM\u22121 such that\u2211M i=1 y 2 i = 1. The uniform measure in SM\u22121 has the following form with respect to the Lebesgue measure in RM (Kume & Walker, 2006),\ndSM\u22121(y) \u221d\n( 1\u2212\nM\u22121\u2211 i=1 y2i\n)\u2212 12 1 ( M\u22121\u2211 i=1 y2i < 1 ) M\u22121\u220f i=1 dyi\nThe random vector y on SM\u22121 has the following well known representations (Chikuse, 2003)\ny1 = cos \u03b81 y2 = sin \u03b81 cos \u03b82 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 yM\u22121 = sin \u03b81 sin \u03b82 \u00b7 \u00b7 \u00b7 sin \u03b8M\u22122 cos \u03b8M\u22121 yM = sin \u03b81 sin \u03b82 \u00b7 \u00b7 \u00b7 sin \u03b8M\u22122 sin \u03b8M\u22121\nInstead of directly drawing samples for y1, \u00b7 \u00b7 \u00b7 , yM\u22121 we could draw samples for \u03b81, \u00b7 \u00b7 \u00b7 , \u03b8M\u22121. However, for simplicity, given \u03b8 = cos2 \u03b81, we could employ a new parameterisation of the form y21 = \u03b8 and y 2 i = (1 \u2212 \u03b8)ui, where i = 2, \u00b7 \u00b7 \u00b7 ,M , which induces the following Lebesgue measure\ndSM\u22121(y) \u221d s1\u03b8\u2212 1 2 (1\u2212 \u03b8) M\u22123 2 ( 1\u2212\nM\u22121\u2211 i=2 ui\n)\u2212 12\n1 ( M\u22121\u2211 i=1 y2i < 1 ) d\u03b8 M\u22121\u220f i=2 siu \u2212 12 i dui\nwhere si \u2208 {\u22121,+1} is the sign of the variable yi. Consequently, the vector Bingham density has the following form with respect to \u03b8, ui where i = 2, \u00b7 \u00b7 \u00b7 ,M\np(\u03b8, ui|\u039b) \u221d exp { \u03bb1\u03b8 + (1\u2212 \u03b8)\nM\u2211 i=2 \u03bbiu 2 i\n}\n\u00d7s1\u03b8\u2212 1 2 (1\u2212 \u03b8) M\u22123 2 ( 1\u2212\nM\u22121\u2211 i=2 ui )\u2212 12 M\u22121\u220f i=2 siu \u2212 12 i\nConditional on u2, \u00b7 \u00b7 \u00b7 , uM\u22121, we draw samples for \u03b8. Given the current known values of y, ui =\ny2i 1\u2212y21 for\ni = 2, \u00b7 \u00b7 \u00b7 ,M . We draw samples for \u03b8 proportional to p(\u03b8) \u221d \u03b8\u2212 12 (1\u2212 \u03b8)M\u221232 exp { \u03bb1\u03b8 + (1\u2212 \u03b8) \u2211M i=2 \u03bbiu 2 i } .\nWe could employ a rejection scheme as in (Hoff, 2009a). Since this distribution has a simple form and \u03b8 \u2208 (0, 1), a grid or Slice sampling (Neal, 2003) could also be employed. For the si, it is just sampled uniformly from {\u22121,+1}. Given the sampled \u03b8\u2217, a new sample for y is then y1 = s1 \u221a \u03b8\u2217 and\nyi = si \u221a\n(1\u2212 \u03b8\u2217)ui for i = 2, \u00b7 \u00b7 \u00b7 ,M . Finally a new sample x\u2217 = Uy\u2217."}, {"heading": "3.2. The matrix Langevin-Bingham distribution", "text": "We employ the procedures of Hoff (2009a) to simulate the matrix LB distribution. When M < N , we denote\nGm = \u2211K k=1 \u03bbkm 2\u03c32\nk\n(CTk + Ck). We then need to sample\nthe matrix LB distribution of the following form,\np(B|C,\u039b, \u03c3) \u221d M\u220f m=1 exp { bTmGmbm } Since the columns of B are orthogonal to each other, bm are not statistically independent. To draw any bi, we denote the rest of the columns of B by B\u2212i. Denote Q having the size M\u00d7 (M\u22121) as the orthonormal basis for the Null space of B\u2212i. Then we could have the representation bi = Q\u03b2 for \u03b2 \u2208 SM\u22122. So the conditional distribution of \u03b2 given B\u2212i has the following representation\np(\u03b2|B\u2212i,Gm) \u221d exp { \u03b2T G\u0303m\u03b2 } where G\u0303m = Q TGmQ. Since \u03b2 T\u03b2 = 1, it is a vector Bingham distribution. We can in the first step draw a sample \u03b2\u2217, and then the new sample for b\u2217i is obtained by the transformation Q\u03b2\u2217.\nFor M = N , we need to simulate two columns of B at the same time (Hoff, 2009a). Without loss of generality, we look at sampling the first two columns of B. We denote B\u2212(1,2) as the matrix B without the first two columns. Let Q be the orthonormal basis for the Null space of B\u2212(1,2). Then (b1,b2) = QZ for a proper 2 \u00d7 2 orthonormal matrix Z = (zij). Instead of sampling (b1,b2) directly, we sample Z. Denote A\u0303k = Q T 1\n2\u03c32 k\n(CTk + Ck)Q and \u039b\u0303 k 1,2 = diag(\u03bb k 1 , \u03bb k 2).\nThe posterior for Z is p(Z|B\u2212(1,2), A\u0303k, \u039b\u0303k1,2) \u221d K\u220f k=1 exp { tr ( \u039b\u0303k1,2Z T A\u0303kZ )} = exp { ZT.1A1Z.1 + Z T .2A2Z.2\n} where Z.i denotes the ith column of Z and Ai =\u2211K k=1 \u03bb k i A\u0303k where i = 1, 2. Given a sample Z\n\u2217, a new sample is then (b\u22171,b \u2217 2) = QZ \u2217."}, {"heading": "4. Simulation Results", "text": "In this section we evaluate the proposed Gibbs sampler based on a few data sets. In the first example, based on a toy data set we compare the Rejection, Slice and Grid sampling schemes for simulating the vector Bingham distribution. In the second example, the Gibbs sampler was applied to data with size N = 10, M = 5 and K = 100 to seek the common eigenvectors. We also considered data with size N = 10, M = 10 and K = 100. Given these results, we compare the Gibbs sampler to the Jacobi method, AC-DC and FFDiag algorithms. We finally applied the Gibbs sampler to BSS, CPCA and CSPA. Note that all the Gibbs samplers in our experiments were monitored for convergence. For each sampler we used 10 chains to calculate theR statistics defined in (Gelman & Rubin, 1992) and the Gibbs sampler was considered converged when R < 1.2. All the results shown here were the samples drawn after convergence to the invariant measure."}, {"heading": "4.1. Simulating vector Bingham distribution", "text": "In this experiment, we compare the three sampling schemes, namely rejection, slice and grid samplings, for simulating the vector Bingham distributions p(x|A) \u221d exp(xTAx). A symmetric real matrix A with size 10\u00d710 was randomly generated which has 10 different eigenvalues. We calculated the effective sample sizes (ESS) for all the methods which are shown in Table 1. The simulations of the log likelihood for the three methods are plotted in Figure 1. As expected, all the values of log likelihood lie in between the minimum and maximum eigenvalues of A. We conclude\nthat all the three schemes perform similarly in terms of ESS. We then applied all the schemes to the Gibbs sampler for the joint diagonalization problems."}, {"heading": "4.2. Gibbs sampler for joint diagonalization", "text": "In this section we evaluate the proposed Gibbs sampler for the joint diagonaliztion problems based on two data sets. For the data considered, K = 100. The matrices were generated according to Ck = B\u039bkB\nT +Ek where BTB = I, and each matrix Ck has size 10\u00d7 10. All the diagonal elements of the diagonal matrix \u039bk were randomly generated. For the first data set, the matrix B has size 10\u00d7 5. For the second data set, the matrix B has size 10\u00d7 10. Since Gaussian errors were assumed the matrices Ck may not be symmetric. All the algorithms considered here were designed to seek B and \u039bk. If B\u0302 is denoted as the estimation of B, for comparison purposes we calculate Amari\u2019s performance index (API) (Amari et al., 1996) for the matrix P = B\u0302\u22121B. Note that when P is a permutation of the identity matrix, the API is zero. The API is defined\nas \u2211 i (\u2211 j |Pij | maxl |Pil| \u2212 1 ) + \u2211 j (\u2211 i |Pij | maxl |Plj | \u2212 1 ) ."}, {"heading": "4.2.1. The 10\u00d7 5 matrix B", "text": "For the first experiment when M < N , we compare the performance of the Gibbs sampler based on rejection, Slice and grid sampling schemes for sampling from the matrix LB distribution. We calculated the ESS for the three sampling schemes which are shown in the Table 2. We see that both the rejection and slice sampling are more efficient than the grid sampling in terms of ESS. We also plotted the log likelihood for the three schemes (see Figure 2), which shows that the grid sampling scheme performs better than the other two in searching for the modes of the model. We now employ the grid sampling scheme for the model when\nN > M . The ESS of grid sampling was very small, which suggests that grid sampling may be performing a random walk in the local mode of the posterior.\nSince our model is similar to that of (Yeredor, 2002), we compare grid sampling to the AC-DC algorithm. Both the grid sampling and AC-DC algorithm were applied to the data to infer B and \u039bk. We calculated the API values for the estimates from both methods when various errors were added to the model. The API statistics for grid sampling and the minimum API values for the AC-DC algorithm are shown in the Table 3. Note that we also show the API computed by using the maximum a posteriori (MAP) sample. The results show that the proposed scheme outperforms the AC-DC algorithm across all error levels. We note here that one of the advantages of the Bayesian approach compared to point estimate methods is that the sampling approach accommodates naturally the model selection issue. As the first step to model selection, we used the Gibbs sampling output to approximate the log marginal likelihood of various models using the BIC (Schwarz, 1978). We calculated the log marginal likelihood for the models M = 1, 2, \u00b7 \u00b7 \u00b7 , N where M denotes the number of columns of B. The estimates are shown in the Figure 3 which indicates that BIC prefers M = 5 which correctly located the model. Note that besides the approximate BIC method considered here more rigorous (i.e. unbiased estimation schemes) could be used for model selection as in (Zhong et al., 2011)"}, {"heading": "4.2.2. The 10\u00d7 10 matrix B", "text": "For the second experiment, we applied grid sampling, AC-DC, Jacobi and FFDiag methods to the data set where the matrix B has size 10 \u00d7 10. The grid sampling scheme was used to sample the matrix B and all the rest algorithms were used to provide point estimates for B. The API statistics for grid sampling are shown in the Table 4. The minimum API values\nfor AC-DC, Jacobi method and FFDiag are shown in Table 5. These results show that the proposed method is comparable to the Jacobi method and outperforms both the AC-DC and FFDiag in terms of the minimum API. However, in practice we usually do not know the true B. So the MAP sample could be used for comparison purposes, which shows that the Gibbs sampler is comparable to FFDiag and outperforms AC-DC."}, {"heading": "4.3. Applications to BSS", "text": "In this section we apply the proposed approximate joint diagonalization scheme to a BSS problem. We employ the \u2018ACsin10d\u2019 data from the ICAlab benchmark data sets (http:// www.bsp.brain.riken.jp/ ICALAB/). This data set contains 10 sine-wave sources denoted by S, and each source is sampled 1000 equally spaced data points. We randomly generated a mixing matrix A of size 10 \u00d7 10 and then the mixtures were generated by X = AS + E where E were the Gaussian error realisations with \u03c3 = 0.1. The data X were whitened by multiplying the matrix W as defined in (Belouchrani et al., 1997) and then we obtain 100 co-\nvariance matrices C(\u03c4) = E { X\u0303(t+ \u03c4)X\u0303(t)T } . For\ncomparison purposes, we applied the Jacobi method, FFDiag algorithm, AC-DC algorithm as well as grid sampling of C(\u03c4) to search for the diagonalizing matrix B. We then calculated the API values for the matrix P = B\u22121WA. The API values for the Jacobi method, FFDiag and AC-DC were 1.0333, 1.4951 and 1.5018. The API value statistics obtained from grid sampling were 1.1869 (Min), 1.3046\u00b10.0361 (Mean\u00b1std) , 1.4408 (Max) and 1.3254 (MAP), indicating that the\nproposed method produced smaller API values than both the FFDiag and AC-DC with the Jacobi method always achieving the best API."}, {"heading": "4.4. Applications to CPCA and CSPA", "text": "In this example, we show that the Gibbs sampler could be applied to both CPCA and CSPA. Given K covariance matrices Ck, CPCA seeks to find a common orthogonal matrix B such that BCkB = \u039bk are diagonal (Flury, 1984). The columns of B are the common principal components (CPC). However, for non-trivial problems it would be unlikely that the \u039bk are exactly diagonal. We employ the model (1) with Gaussian errors to infer the CPC. CPCA can be applied to signal processing, such as the CSPA which has been applied to the preprocessing of EEG data in Brain Computer Interfaces (BCI) (Blankertz et al., 2008). We first apply the Gibbs sampler to a toy data set and then some EEG data."}, {"heading": "4.4.1. A Synthetic Data", "text": "We employ the procedure in (Blankertz et al., 2008) to generate data in demonstrating the Gibbs sampler in CPCA and CSPA. The data for analysis is generated according to the following linear mixing model\nY j = ASj\nwhere j = 1, 2 are class labels, and S1 \u223c N (0,\u039b1) where \u039b1 = diag(0.1, 0.9) and S2 \u223c N (0,\u039b2) where \u039b2 = diag(0.9, 0.1). Since S1 and S2 are from different distributions, they are data from two classes. We can see that S1 has the largest variance in one direction and S2 is in the opposite direction. The linear mixing matrix is denoted as A, and the observations Y 1 and Y 2 are the data labeled as class 1 and 2.\nEssentially, CSPA seeks the common spatial patterns \u0393 = A\u22121 to filter the observed data Y j , such that the filtered data \u0393Y j are uncorrelated in both classes. We generated 200 samples for both classes Sj , and A was then randomly generated to form the mixtures Y j for j = 1, 2. Denote Y = (Y 1, Y 2). In the first step, we whiten the data Y via matrix W as in the BSS problem and denote the whitened data as Y\u0303 . To use the Gibbs sampler, we form the sample covariance matrices C1 and C2 by the samples of Y\u0303 1 and Y\u0303 2, respectively. We then applied the Gibbs sampler to C1 and C2 to seek an orthogonal matrix B which simultaneously diagonalised the covariance matrices. The filtered data are Y \u2032 = BTWY , which shows that the filtered data of the two classes have largest variances in opposite directions (See Figure 4).\nMixtures Before CSPA Filtering\nAfter CSPA Fitering\nTwo\u2212Class Gaussian Data"}, {"heading": "4.4.2. EEG Data", "text": "In this section we apply the Gibbs sampler to EEG data in performing CSPA. The data used for this study corresponds to the EEG data set IIIb of the BCI competition III (Blankertz et al., 2006). This data set gathers the EEG signals recorded for three subjects who had to perform motor imagery, i.e. to imagine\nleft or right hand movements. Hence, the two classes to be identified were Left and Right. Before doing the CSPA, we extract features from these EEG signals. We choose to use Band Power (BP) features and finally we have four features for each subject which are denoted as C3-\u03b1, C3-\u03b2, C4-\u03b1 and C4-\u03b2. So each data point having four features is labeled as one of the two classes. We do the CSPA for these data points. We denote the two class data as X1 and X2. In the first step we whitened the data by using a whitening matrix W as usual. Then we form the sample covariance matrices C1 and C2 for the two classes, respectively. Note that the covariance matrix has size 4 \u00d7 4. Then the Gibbs sampler was used to seek an orthogonal matrix B to diagonalize the covariance matrices simultaneously. Finally the filtered data of the two classes are then X\u03031 = BTWX1 and X\u03032 = BTWX2. For demonstration, we plotted the scatter plots for the features C4-\u03b1 and C4-\u03b2 before filtering and after filtering (see Figure 5). From the plots in Figure 5 (a), we see that the data in the two classes have large variance in the same direction, and after the filtering in Figure 5 (b) they have large variances in opposite directions. This can also be demonstrated by the inferred eigenvalues plotted in Figure 6, where the squares denote the eigenvalues for one class and circles for the other. We observe that one class has larger eigenvalues in features C3-\u03b1 and C4-\u03b1 and the other class has larger eigenvalues in features C3-\u03b2 and C4-\u03b2. This would be useful for the further classification purpose. It is interesting to note that the sample covariance matrices are positive definite matrices, and thus the inferred eigenvalues finally converged to positive values even though they were initialized in negative values. This could be an advantage for our Gibbs sampler comparing to the algorithms of (Hoff, 2009b) and (Pham, 2001) where the eigenvalues were restricted to positive values."}, {"heading": "5. Conclusions", "text": "We have proposed a Gibbs sampler to simultaneously diagonalize several matrices which are not necessarily symmetric. All the required conditional distributions are known standard distributions. We have compared the Gibbs sampler to some other joint diagonalization algorithms and shown that the Gibbs sampler achieves the state-of-the-art performance on the small examples considered. We also applied the algorithm to BSS, CPCA and CSPS. The Gibbs sampler could be extended to the case where the diagonalizatioin matrix is nonorthogonal."}, {"heading": "Acknowledgments", "text": "MG is grateful for financial support from the Engineering and Physical Sciences Research Council (EPSRC) UK, grants EP/J007617/1, EP/E052029/2, EP/H024875/2. MZ is supported by the fundamental research funds for the central universities in China."}], "references": [{"title": "A new learning algorithm for blind source separation", "author": ["Amari", "S.-I", "A. Cichocki", "H.H. Yang"], "venue": "In NIPS,", "citeRegEx": "Amari et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Amari et al\\.", "year": 1996}, {"title": "A blind source separation technique based on second order statistics", "author": ["A. Belouchrani", "Meraim", "K. Abed", "Cardoso", "J.-F", "E. Moulines"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "Belouchrani et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Belouchrani et al\\.", "year": 1997}, {"title": "Optimizing spatial filters for robust eeg single-trial analysis", "author": ["B. Blankertz", "R. Tomioka", "S. Lemm", "M.Kawanabe", "K.-R.Mller"], "venue": "IEEE Signal Proc. Magazine,", "citeRegEx": "Blankertz et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blankertz et al\\.", "year": 2008}, {"title": "Jacobi angles for simultaneous diagonalization", "author": ["Cardoso", "J.-F", "A. Souloumiac"], "venue": "SIAM J. Mat. Anal. Appl.,", "citeRegEx": "Cardoso et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cardoso et al\\.", "year": 1996}, {"title": "maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Common principal components in k groups", "author": ["B. Flury"], "venue": "J. Amer. Statist. Assoc.,", "citeRegEx": "Flury,? \\Q1984\\E", "shortCiteRegEx": "Flury", "year": 1984}, {"title": "Simultaneous similarity of matrices", "author": ["S. Friedland"], "venue": "Adv. in Math.,", "citeRegEx": "Friedland,? \\Q1983\\E", "shortCiteRegEx": "Friedland", "year": 1983}, {"title": "Inference from iterative simulation using multiple sequences", "author": ["A. Gelman", "D. Rubin"], "venue": "Statist. Sci.,", "citeRegEx": "Gelman and Rubin,? \\Q1992\\E", "shortCiteRegEx": "Gelman and Rubin", "year": 1992}, {"title": "Simulation of the matrix Bingham-von Mises-Fisher distribution, with applications to multivariate and relational data", "author": ["P.D. Hoff"], "venue": "J. Comput. Graph. Statist.,", "citeRegEx": "Hoff,? \\Q2009\\E", "shortCiteRegEx": "Hoff", "year": 2009}, {"title": "A hiearchical eigenmodel for pooled covariance estimation", "author": ["P.D. Hoff"], "venue": "J. R. Statist. Soc. B,", "citeRegEx": "Hoff,? \\Q2009\\E", "shortCiteRegEx": "Hoff", "year": 2009}, {"title": "The von mises-fisher matrix distribution in orientation statistics", "author": ["C.G. Khatri", "K.V. Mardia"], "venue": "J. Roy. Statist. Soc. Ser. B,", "citeRegEx": "Khatri and Mardia,? \\Q1977\\E", "shortCiteRegEx": "Khatri and Mardia", "year": 1977}, {"title": "The quantitative extraction and topographic mapping of the abnormal components in the clinical EEG. Electroencephalogr", "author": ["Z.J. Koles"], "venue": "Clin. Neurophysiol.,", "citeRegEx": "Koles,? \\Q1991\\E", "shortCiteRegEx": "Koles", "year": 1991}, {"title": "Sampling from compositional and directional distributions", "author": ["A. Kume", "S.G. Walker"], "venue": "Statist. and Comput.,", "citeRegEx": "Kume and Walker,? \\Q2006\\E", "shortCiteRegEx": "Kume and Walker", "year": 2006}, {"title": "Joint approximate diagonalization of positive definite matrices", "author": ["Pham", "D.-T"], "venue": "SIAM J. on Matrix Anal. and Appl.,", "citeRegEx": "Pham and D..T.,? \\Q2001\\E", "shortCiteRegEx": "Pham and D..T.", "year": 2001}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Statist.,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "Nonorthogonal joint diagonalization by combining givens and hyperbolic rotations", "author": ["A. Souloumiac"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "Souloumiac,? \\Q2009\\E", "shortCiteRegEx": "Souloumiac", "year": 2009}, {"title": "Joint diagonalization via subspace fitting techniques", "author": ["A. van der Veen"], "venue": "In ICASSP, pp. 2773\u20132776,", "citeRegEx": "Veen,? \\Q2001\\E", "shortCiteRegEx": "Veen", "year": 2001}, {"title": "Non-orthogonal joint diagonalization in the least-squares sense with application in blind source separation", "author": ["A. Yeredor"], "venue": "IEEE Trans. on Sig. Proc.,", "citeRegEx": "Yeredor,? \\Q2002\\E", "shortCiteRegEx": "Yeredor", "year": 2002}, {"title": "Bayesian methods to detect dye-labelled dna oligonucleotides in multiplexed raman spectra", "author": ["M. Zhong", "M. Girolami", "K. Faulds", "D. Graham"], "venue": "J. R. Statist. Soc. C,", "citeRegEx": "Zhong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2011}, {"title": "A fast algorithm for joint diagonalization with nonorthogonal transformations and its application to blind source separation", "author": ["A. Ziehe", "P. Laskov", "G. Nolte", "Muller", "K-R"], "venue": null, "citeRegEx": "Ziehe et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ziehe et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 17, "context": "There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 19, "context": "There are various applications for the joint diagonalization technique, such as blind source separation (BSS) (Belouchrani et al., 1997; Yeredor, 2002; Ziehe et al., 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 110, "endOffset": 171}, {"referenceID": 5, "context": ", 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al.", "startOffset": 52, "endOffset": 65}, {"referenceID": 11, "context": ", 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al., 2008).", "startOffset": 110, "endOffset": 147}, {"referenceID": 2, "context": ", 2004), common principal component analysis (CPCA) (Flury, 1984), and common spatial pattern analysis (CSPA) (Koles, 1991; Blankertz et al., 2008).", "startOffset": 110, "endOffset": 147}, {"referenceID": 17, "context": "In this paper we restrict ourselves to the case BB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009).", "startOffset": 127, "endOffset": 180}, {"referenceID": 19, "context": "In this paper we restrict ourselves to the case BB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009).", "startOffset": 127, "endOffset": 180}, {"referenceID": 15, "context": "In this paper we restrict ourselves to the case BB = I, however further work could consider the case where B is non-orthogonal (Yeredor, 2002; Ziehe et al., 2004; Souloumiac, 2009).", "startOffset": 127, "endOffset": 180}, {"referenceID": 17, "context": "This model is a generalization of the subspace fitting models of (van der Veen, 2001; Yeredor, 2002), where here the errors Ek are represented explicitly.", "startOffset": 65, "endOffset": 100}, {"referenceID": 17, "context": "Our approach is most similar to the subspace fitting models (van der Veen, 2001; Yeredor, 2002) although we also treat the error variances as parameters to be inferred.", "startOffset": 60, "endOffset": 95}, {"referenceID": 5, "context": "Flurry (Flury, 1984) and Hoff (Hoff, 2009b) proposed CPCA for a series of covariance matrices whose eigenvalues were assumed positive, a restriction which we relax.", "startOffset": 7, "endOffset": 20}, {"referenceID": 4, "context": "Point estimate methods such as the expectationmaximisation (EM) algorithm (Dempster et al., 1977) or the AC-DC algorithm (Yeredor, 2002) could be derived to estimate these parameters.", "startOffset": 74, "endOffset": 97}, {"referenceID": 17, "context": ", 1977) or the AC-DC algorithm (Yeredor, 2002) could be derived to estimate these parameters.", "startOffset": 31, "endOffset": 46}, {"referenceID": 17, "context": "Based on some data, we compared the Gibbs sampler to the Jacobi method (Cardoso & Souloumiac, 1996), AC-DC (Yeredor, 2002) and the FFDiag algorithm (Ziehe et al.", "startOffset": 107, "endOffset": 122}, {"referenceID": 19, "context": "Based on some data, we compared the Gibbs sampler to the Jacobi method (Cardoso & Souloumiac, 1996), AC-DC (Yeredor, 2002) and the FFDiag algorithm (Ziehe et al., 2004).", "startOffset": 148, "endOffset": 168}, {"referenceID": 0, "context": "Across all the results the Gibbs sampler is comparable to both the Jacobi and FFDiag methods whilst outperforming the AC-DC algorithm in terms of the Amari performance index (Amari et al., 1996).", "startOffset": 174, "endOffset": 194}, {"referenceID": 8, "context": "We employ the procedures of Hoff (2009a) to simulate the matrix LB distribution.", "startOffset": 28, "endOffset": 41}, {"referenceID": 0, "context": "If B\u0302 is denoted as the estimation of B, for comparison purposes we calculate Amari\u2019s performance index (API) (Amari et al., 1996) for the matrix P = B\u0302\u22121B.", "startOffset": 110, "endOffset": 130}, {"referenceID": 17, "context": "Since our model is similar to that of (Yeredor, 2002), we compare grid sampling to the AC-DC algorithm.", "startOffset": 38, "endOffset": 53}, {"referenceID": 14, "context": "As the first step to model selection, we used the Gibbs sampling output to approximate the log marginal likelihood of various models using the BIC (Schwarz, 1978).", "startOffset": 147, "endOffset": 162}, {"referenceID": 18, "context": "unbiased estimation schemes) could be used for model selection as in (Zhong et al., 2011)", "startOffset": 69, "endOffset": 89}, {"referenceID": 1, "context": "The data X were whitened by multiplying the matrix W as defined in (Belouchrani et al., 1997) and then we obtain 100 covariance matrices C(\u03c4) = E { X\u0303(t+ \u03c4)X\u0303(t) } .", "startOffset": 67, "endOffset": 93}, {"referenceID": 5, "context": "Given K covariance matrices Ck, CPCA seeks to find a common orthogonal matrix B such that BCkB = \u039bk are diagonal (Flury, 1984).", "startOffset": 113, "endOffset": 126}, {"referenceID": 2, "context": "CPCA can be applied to signal processing, such as the CSPA which has been applied to the preprocessing of EEG data in Brain Computer Interfaces (BCI) (Blankertz et al., 2008).", "startOffset": 150, "endOffset": 174}, {"referenceID": 2, "context": "We employ the procedure in (Blankertz et al., 2008) to generate data in demonstrating the Gibbs sampler in CPCA and CSPA.", "startOffset": 27, "endOffset": 51}], "year": 2012, "abstractText": "We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-theart performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems.", "creator": "LaTeX with hyperref package"}}}