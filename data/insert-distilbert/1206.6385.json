{"id": "1206.6385", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Improved Estimation in Time Varying Models", "abstract": "locally adapted parameterizations of effectively a model ( constructions such as locally weighted regression ) are expressive but often suffer from high variance. we describe an approach provided for reducing the variance, based on the incorrect idea of estimating simultaneously a transformed space for the model, as well as locally adapted parameterizations in this new integration space. we present a new problem formulation that captures this transformation idea strongly and illustrate it in the important context of time varying models. we develop an innovative algorithm for learning a set of bases for approximating a time continuously varying sparse network ; each learned basis constitutes an internal archetypal sparse network structure. we also provide an extension for learning nonlinear task - driven bases. concurrently we present empirical results data on synthetic data sets, as well as calculations on a bci eeg classification task.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (631kb)", "http://arxiv.org/abs/1206.6385v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ME stat.ML", "authors": ["doina precup", "philip bachman"], "accepted": true, "id": "1206.6385"}, "pdf": {"name": "1206.6385.pdf", "metadata": {"source": "META", "title": "Improved Estimation in Time Varying Models", "authors": ["Philip Bachman", "Doina Precup"], "emails": ["PHIL.BACHMAN@GMAIL.COM", "DPRECUP@CS.MCGILL.CA"], "sections": [{"heading": "1. Introduction", "text": "Locally adapted parameterizations can produce flexible representations from relatively rigid components; locally weighted regression serves as a canonical example of this approach. Such models reduce bias but increase variance, due to reduced effective sample sizes used for each estimation. We tackle this problem using a natural machine learning idea: using a transformed (more restricted or simpler) space in which to find local parameterizations.\nA common approach to improving model efficacy in machine learning is to first transform the data into an alternate representation prior to model estimation, ideally in a way that amplifies useful information while attenuating noise. Algorithms exemplifying this approach include: PCA, ICA (Hyva\u0308rinen & Oja, 2000), nonlinear-dimension reduction, e.g. (Tenenbaum et al., 2000), and dimension reduction for regression (Fukumizu et al., 2004; Cook & Forzani, 2009).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nAnother line of work considers transformations of the model used to describe the data, either by reducing the number of degrees of freedom, or by seeking a model form amenable to more powerful estimation procedures. Examples of the first approach include DiscLDA (Lacoste-Julien et al., 2008) and supervised dimensionality reduction using Bayesian mixture models (Sajama & Orlitsky, 2005), which seek useful linear reductions of the parameters of a generative model. The second approach includes the application of spectral methods to learning transformed representations of HMMs (Siddiqi et al., 2010) and PSRs (Boots & Gordon, 2011).\nIn this paper, we provide a different lens through which to view model transformations. In Sec. 2, we present a general formulation of the problem of estimating useful transformations of model parameters, which encompasses several of the previously mentioned methods for both data and model transformation. Our problem depends on the simultaneous estimation of a transformation of the parameter space of a model and of the parameters within the transformed space. We formulate the problem primarily for use with multiply parameterized models (such as locally weighted linear regression or mixture models), which distinguishes it from the spectral methods for HMM and PSR learning, which seek single transformed parameterizations of a given model. We illustrate our problem formulation in the context of familiar models (locally weighted regression and Gaussian mixtures) in Sec. 3. In Sec. 4 we present a novel algorithm for modeling time varying sparse network structures underlying sequential observations. In Sec. 5 and 6, we use synthetic data and data drawn from real-world BCI EEG experiments to showcase our algorithm."}, {"heading": "2. A General Problem Formulation", "text": "The problem investigated in this paper arises as a generalization of the following optimization:\nB\u2217 = arg min B [`(f,X,B)] (1)\nwhere the loss ` measures the \u201cgoodness\u201d of fit of the model f to the data X = {(x1, y1), ..., (xm, ym)} given\na set of parameterizations B = {\u03b21, ..., \u03b2m\u2032} of f , and an optimal set of parameterizations B\u2217 is sought.\nThe idea of using multiple model parameterizations is not often explored in machine learning. As motivation for this view, we begin by expressing standard linear regression in the form of (1). In this case, f measures the residuals produced by a parameter vector:\nf((x, y), \u03b2) = \u03b2Tx\u2212 y\nFor a set of parameter vectors \u03b2i, ` is proportional to the log-likelihood of observing the residuals assuming they are normally distributed with variance \u03c32:\n`(f,X,B) = 1 \u03c32 m\u2032\u2211 i=1 m\u2211 j=1 f((xj , yj), \u03b2i)2 (2)\nWe usually think of the loss in this case as having m\u2032 = 1. However, note that considering m\u2032 > 1 does not modify the solution, as loss is measured equally over all (xj , yj), which implies that \u03b2i = \u03b2j ,\u2200\u03b2i, \u03b2j \u2208 B\u2217 (i.e., there still is, in effect, one optimal parameter vector).\nUsing this view, we can transform standard linear regression into kernel weighted linear regression as follows:\nf((x, y), \u03b2) = \u03b2Tx\u2212 y\n`(f,X,B) = 1 \u03c32 m\u2032\u2211 i=1 m\u2211 j=1 k(\u03b2xi , xj)f((xj , yj), \u03b2 w i ) 2 (3)\nwhere the kernel weighting function k(x, x\u2032) measures similarity between locations in the input space, and each \u03b2i consists of two components: \u03b2i = (\u03b2wi , \u03b2 x i ). The localization component \u03b2xi associates \u03b2i with a location in the input space and the coefficient component \u03b2wi associates \u03b2i with a set of regression coefficients.\nIntroducing the kernel k allows the \u03b2i in (3) to have local rather than global effect, which leads to different parameterizations at each location in observation space. However, there is no need to optimize jointly over B\u2217, as there is no constraint linking different elements in a parameter set. Allowing multiple local parameterizations of f is useful for increasing the power of simple models; estimation of time varying covariance matrices in financial modeling and estimation of time varying auto-regressions in econometrics are two well-studied examples of this idea.\nWhile locally weighted regression is typically thought of as a \u201cnon-parametric\u201d method, in the context of our work it is more fruitfully viewed as an approach based on multiple parameterization, in which the implied infinite set B\u2217 can be queried \u201clazily\u201d for specific parameter locations \u03b2xi , rather than computed monolithically.\nTo illustrate a problem in the form of (1) in which the elements of B\u2217 are not independent, for \u03b2i = (\u03b2 \u00b5 i , \u03b2 \u03a3 i , \u03b2 \u03c0 i ), consider the following optimization:\nf((x, y), \u03b2) = \u03b2\u03c0p(x|\u03b2\u00b5, \u03b2\u03a3)\n`(f,X,B) = \u2212 log  m\u220f j=1  m\u2032\u2211 i=1 f((xj , yj), \u03b2i)  , (4) in which 0 \u2264 \u03b2\u03c0i \u2264 1,\u2200\u03b2i and p(x|\u03b2\u00b5, \u03b2\u03a3) is the probability of observing x given a Gaussian distribution with mean \u03b2\u00b5 and covariance \u03b2\u03a3. Minimizing (4) corresponds to estimating a Gaussian mixture model for the data X = {x1, ..., xm}. Interdependence among the \u03b2i \u2208 B\u2217 is induced by the negative log-likelihood loss, together with a constraint on the set of mixture weights: \u2211 i \u03b2 \u03c0 i = 1. Note that in the last two examples, the estimation of B\u2217 may be subject to high variance. To tackle this problem, and to exploit possible structure in the parameterizations, we introduce a \u201cgenerating\u201d function g, which takes inputs \u03b2\u0302 \u2208 Rp (with p chosen a priori) and transforms them into outputs \u03b2. This function can be used to express both regularities and restrictions in the space of parameterization. For instance, in the case of a time varying model, the optimal, temporally local parameterizations of f may lie on a low-dimensional manifold embedded in the full parameter space of f . The structure of such a manifold could be of interest, and restricting the estimation could significantly reduce variance in the resulting parameter estimates with only a small increase in bias.\nWe can now rephrase (1) as an optimization problem involving g. Given dimension p, a model f , a loss `, and a set of inputs X , our optimization becomes:\narg min g [ min B\u0302 [ `(f |g,X, B\u0302) ]] (5)\nin which B\u0302 = {\u03b2\u03021, ..., \u03b2\u0302m\u2032} is a set of inputs to g and f |g denotes the restriction of parameterizations of f to the output space of g.\nIf we define g(\u03b2\u0302) \u2261 \u03b2\u0302, then (5) exactly reproduces (1). If we allow g to take an arbitrarily complex form, then we similarly recover the optimization in (1), as we can define g(\u03b2\u0302i) \u2261 \u03b2i for each \u03b2i \u2208 B\u2217. Thus, interesting cases of (5) arise when g is more carefully chosen. The next section illustrates some useful problems that arise from different definitions of g, f , and `."}, {"heading": "3. Illustrations of the Problem Formulation", "text": "As a first example, consider performing a locally weighted regression analogous to that in (3), but with the local parameterizations of f restricted to a linear subspace. Let\ng(\u03b2\u0302) = A\u03b2\u0302w, where A is the matrix of the parameters of g. We can re-write (5) as follows:\narg min A min B\u0302  m\u2032\u2211 i=1 m\u2211 j=1 k(\u03b2\u0302xi , xj)(x T j A\u03b2\u0302 w i \u2212 yj)2  (6) in which we now split each \u03b2\u0302i into a localization subcomponent \u03b2\u0302xi and a coefficient subcomponent \u03b2\u0302 w i . If one views xTj A\u03b2\u0302 w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002). However, minor modifications, like regularizing the \u03b2\u0302is via \u03bb \u2211 i ||\u03b2\u0302wi ||1, weaken this link.\nAs a second example, we restate the mixture of Gaussians model under the constraint that the means {\u03b2\u00b51 , ..., \u03b2 \u00b5 m\u2032} of the parameterizations {\u03b21, ...\u03b2m\u2032} lie within a linear subspace of the observation space, i.e. \u03b2i = (g(\u03b2\u0302 \u00b5 i ), \u03b2\u0302 \u03a3 i , \u03b2\u0302 \u03c0 i )\n1, with g defined as for (6). The resulting optimization can be written as follows:\narg min A min B\u0302 \u2212 log  m\u220f j=1  m\u2032\u2211 i=1 \u03b2\u0302\u03c0i p(xj |A\u03b2\u0302 \u00b5 i , \u03b2\u0302 \u03a3 i )  (7) Performing the optimization in (7) was shown to be useful for classification tasks in (Sajama & Orlitsky, 2005).\nWe can similarly generate optimization problems in the form of (5) whose solutions correspond to PCA and sparse coding, which are left out due to space constraints."}, {"heading": "4. Learning Compact Representations of", "text": "Time Varying Network Structure\nIn this section, we use our new problem formulation to derive a novel algorithm for estimating time varying network structure, using a time-dependent sparse combination of learned basis structures. Through an analogy between our algorithm and sparse coding (Olshausen & Field, 1996), we then extend our algorithm to learning of task-driven basis structures, guided by the work in (Mairal et al., 2011). We begin by reviewing existing work on network structure estimation, before describing the new algorithms."}, {"heading": "4.1. Sparse Network Structure Estimation", "text": "In recent years, much effort has gone into developing effective methods for estimating sparsely structured Gaussian graphical models. A Gaussian graphical model (GGM) explains a set of m n-dimensional observations X =\n1Note that we have not transformed the covariances \u03b2\u0302\u03a3i\n{x1, ..., xm}, xi \u2208 Rn using a set of n vertices (each corresponding to one dimension) and a set of edges, each describing the strength of the relationship between its incident vertices. A GGM implies a covariance \u03a3 and is equivalent to modeling X with a normal distribution N (~0,\u03a3). Typically, prior to estimating a GGM, the observations are standardized to have mean 0.\nMany existing methods addressing GGMs focus on estimating their structure, i.e. the pattern of zero/non-zero edges. These methods typically work with the precision matrix (i.e. \u03a3\u22121) implied by a GGM, as non-zero entries in \u03a3\u22121 correspond to non-zero edges in the GGM. Estimating the structure of \u03a3\u22121 is facilitated by the following relationship:\n\u03c1ij = \u03c3\u0303ij\u221a \u03c3\u0303ii\u03c3\u0303jj , (8)\nin which \u03c1ij indicates the partial correlation between the ith and jth dimension conditioned on the values of all other dimensions, and \u03c3\u0303ij is the entry in the ith row and jth column of \u03a3\u22121. The relationship between partial correlations and GGM structure leads to efficient methods for GGM structure estimation, as partial correlations can be directly estimated by \u201cself-regression\u201d.\nThe use of self-regression for network structure estimation is based on the following results (Lauritzen, 1996):\nxit = \u2211 j 6=i xjt \u03c1\u0303ij + i t, (9)\nin which xit represents the value of the i th dimension of the tth observation in X , \u03c1\u0303ij is a real-valued scalar, and it is uncorrelated with xit if and only if:\n\u03c1\u0303ij = \u2212 \u03c3\u0303ij \u03c3\u0303ii = \u03c1ij \u221a \u03c3\u0303jj \u03c3\u0303ii , from which (10)\n\u03c1ij = sign(\u03c1\u0303ij) \u221a \u03c1\u0303ij \u03c1\u0303ji. (11)\nHence, \u03c1\u0303ij can be efficiently estimated for any given i using linear regression of the response variables {xi1, ..., xim} on the covariates {x\\i1 , ...x \\i m}, in which x\\it indicates a vector including all dimensions except i; \u03c1ij can then be computed as well.\nMost existing methods for GGM structure estimation assume that \u03a3\u22121 is sparse. Value estimation methods estimate each entry in \u03a3\u22121 (Zhou et al., 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al., 2008; Song et al., 2009b; Kolar & Xing, 2011)). The sparsity assumption can be incorporated into the self-regression process by using sparsifying regression techniques, such as the well known Lasso (Tibshirani, 1996). Self-regression methods using sparsity have been shown to produce consistent estimates of structure in\n\u03a3\u22121 under suitable conditions (Meinshausen & Bu\u0308hlmann, 2006; Wainwright et al., 2007; Kolar & Xing, 2011).\nA recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011). We focus on the KELLER algorithm from (Song et al., 2009a), as our algorithm can be seen as its natural generalization using the problem formulation in (5). The KELLER algorithm is predicated on two assumptions: sparsity in the time varying network structure, and smoothness in the changes of these structures over time. This second assumption distinguishes KELLER from methods such as (Ahmed & Xing, 2009) and (Kolar et al., 2009), which assume abrupt changes in the network structure.\nTo estimate the structure of a network at time t, given a sequence of T observations X = {x1, .., xT |xi \u2208 Rn}, KELLER performs a set of n independent `1-regularized locally weighted regressions, with the ith regression estimating the values \u03c1\u0303ij ,\u2200j 6= i as described above. By using locally weighted regression, these values are specifically adapted to the predominant network structure affecting the observation at time t. For time t, these regressions can be written compactly as follows:\nA\u2217t = arg min A\u2208Rn\u00d7n T\u2211 t\u2032=1 k(t, t\u2032) ||xt\u2032 \u2212Axt\u2032 ||22 + \u03bb||A||1, (12)\nin which k(t, t\u2032) computes a kernel weight measuring temporal proximity, diagonal entries of A are fixed at 0, ||A||1 is the entry-wise matrix 1-norm (i.e. \u2211 i \u2211 j |Aij |), and \u03bb controls the `1 regularization, which determines the sparsity of A. After estimating A\u2217 according to (12), KELLER performs a simple procedure to make the implied structure estimate coherent with the assumption of an undirected GGM (i.e. A\u2217 should be symmetric), which consists of inferring an edge between any pair of vertices (i, j) such that Aij 6= 0 or Aji 6= 0. An estimation similar to (12) is used in (Song et al., 2009b), without the additional symmetrization, for networks with directed edges.\nAs used in (12), the weighting kernel makes the estimate of A\u2217t at time t effectively independent from observations at times remote from t. This can lead to high variance, and ignores potential structure in the way in which the network structure changes over time. We will now state our algorithm, which addresses these problems."}, {"heading": "4.2. Estimating Network Structures as Combinations of Basis Structures", "text": "We reformulate the optimization in (12) similarly to the way in which we generalized locally weighted regression from (3) to the form (6). At each time t, the optimal A\u2217t\nis estimated as a linear combination of a set of k basis matrices A\u0302 = {A1, ..., Ak| diag(Ai) = 0}. Our proposed estimation procedure revolves around the following optimization:\n\u03b2\u0302t = arg min \u03b2\u0302\u2208Rk T\u2211 t\u2032=1 k(t, t\u2032) ||xt\u2032 \u2212 k\u2211 i=1 \u03b2\u0302iAixt\u2032 ||22 + \u03bb r(\u03b2\u0302)\n(13) in which \u03b2\u0302i is the ith element of \u03b2\u0302, r(\u03b2\u0302) is a regularization term, and \u03bb controls the strength of regularization. Given \u03b2\u0302t, we estimateA\u2217t as \u2211k i=1 \u03b2\u0302 i tA\ni. The optimization in (13) involves a fixed set of basis matrices A\u0302, but what we really want is to jointly optimize the loss in (13) over all times 1 \u2264 t \u2264 T , with respect to both the \u03b2\u0302t \u2208 B\u0302 = {\u03b2\u03021, ...\u03b2\u0302T } and the Ai \u2208 A\u0302. By doing so, information extracted from the entire sequence is allowed to affect the estimation of each A\u2217t when each A \u2217 t is constructed from the bases in A\u0302, which helps mitigate problems with high variance.\nThe desired joint optimization over B\u0302 and A\u0302 is easy to express in the terms of (5). Let g(\u03b2\u0302) = \u2211k i=1 \u03b2\u0302\niAi, where Ai \u2208 A\u0302 and ||Ai||1 \u2264 c. The constraint on the entry-wise 1-norm of each Ai enforces the structural sparsity assumption. Next, we define f(x, g(\u03b2\u0302)) = ||x \u2212 g(\u03b2\u0302)x||2. Then, we define `(f |g,X, B\u0302) as: `(f |g,X, B\u0302) = T\u2211 t=1 T\u2211 t\u2032=1 k(t, t\u2032) f(xt\u2032 , g(\u03b2\u0302t))2+\u03bb T\u2211 t=1 r(\u03b2\u0302t) (14) Finally, we express the full joint optimization as follows:\nA\u0302\u2217 = arg min A\u0302 min B\u0302 T\u2211 t=1 T\u2211 t\u2032=1 k(t, t\u2032) ||xt\u2032 \u2212 k\u2211 i=1 \u03b2\u0302itA ixt\u2032 ||22\n+\u03bb\u03b2 T\u2211 t=1 r(\u03b2\u0302t) + \u03bbA k\u2211 i=1 ||Ai||1 (15)\nin which we changed the entry-wise 1-norm constraint on each Ai for a functionally similar entry-wise 1-norm regularization term. Intuitively, our method produces a set of basis network structures, i.e. A\u0302\u2217, with which the temporally local network structures can be effectively approximated.\nThe joint optimization in (15) is closely analogous to the following sparse coding objective:\nA\u2217 = arg min A\u2208Rn\u00d7k [ min B m\u2211 i=1 ( ||xi \u2212A\u03b2i||22 + \u03bb||\u03b2i||1 )] ,\n(16) in whichB = {\u03b21, ...\u03b2m |\u03b2i \u2208 Rk}, \u03bb controls the tradeoff between reconstruction accuracy and representational sparsity, and the columns of A are constrained to unit norm. We can emphasize this by introducing the concept of time varying pseudo-dictionaries Dt \u2208 Rn\u00d7k, in which the ith\ncolumn of Dt is Aixt. Using pseudo-dictionaries, we can rewrite (15) as follows:\nA\u0302\u2217 = arg min A\u0302\n(17)\nmin B\u0302 T\u2211 t=1 [( T\u2211 t\u2032=1 k(t, t\u2032) ||xt\u2032 \u2212Dt\u2032 \u03b2\u0302t||22 ) + \u03bb\u03b2r(\u03b2\u0302t) ]\nin which we dropped the sparsifying penalty on Ai \u2208 A\u0302\u2217 for notational brevity. From (17), it can be seen that the inner optimization over B\u0302 in (15) can be addressed as a set of sparse coding problems. For our purposes, we set the regularization term \u03bb\u03b2r(\u03b2\u0302t) to:\n\u03b1\u03bb\u03b2 2 ||\u03b2\u0302t||22 + (1\u2212 \u03b1)\u03bb\u03b2 ||\u03b2\u0302t||1; 0 \u2264 \u03b1 \u2264 1, (18)\nwhich corresponds to elastic-net regularization (Zou & Hastie, 2005). We use this form to meet the assumptions required for the task-driven dictionary learning described in (Mairal et al., 2011), used in the further extension of our algorithm.\nThe analogy between our method and sparse coding leads naturally to a method for effecting the joint optimization in (15). As in sparse coding, we can jointly optimize over A\u0302 and B\u0302 using an EM-like block coordinate descent process that alternates between optimizing B\u0302 while holding A\u0302 fixed and optimizing A\u0302 while holding B\u0302 fixed (each of these is a convex problem). When optimizing B\u0302 with A\u0302 held fixed, we compute the optimal \u03b2\u0302t for each t via elastic-net regressions solved with the publicly available, highly optimized glmnet package (Friedman et al., 2009). When optimizing A\u0302 with B\u0302 held fixed, given current estimates of each basis Ai \u2208 A\u0302, we compute the partial gradients of the objective in (17) w.r.t. the entries of each pseudo-dictionary Dt, and then backpropagate these partial gradients through the pseudo-dictionary formation process to get partial gradients w.r.t. each entry of each basis structure Ai. We symmetrize the partial gradient of (17) w.r.t. each Ai by setting \u2202Aiuv = 1 2 (\u2202A i uv + \u2202A i vu). We also set \u2202A i uu = 0,\u2200u to maintain the zero-diagonal constraint on Ai \u2208 A\u0302. In the next subsection we refer to these (unsupervised) partial gradients as \u2207Ai`u. Using the computed gradients, we then take a single gradient descent step to update each Ai.\nThe full joint optimization process iterates between updating the \u03b2\u0302i \u2208 B\u0302 via the regression in (13) and performing a single gradient descent update of the entries in each Ai \u2208 A\u0302. We dynamically select the step size for gradient descent updates in each iteration by line search and iterate until convergence. We perform the iterative optimization using subsampled batches of the available observations, which yields a stochastic gradient descent approach to jointly optimizing (15)/(17)."}, {"heading": "4.3. Supervised Basis Structure Learning", "text": "We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of basis network structures. We consider the task of minimizing differentiable supervised loss functions that can be written as:\nLs(X, B\u0302, w) = T\u2211 t=1 `s(\u03c9>\u03b2\u0302t, yt) + \u03bd 2 ||\u03c9||22, (19)\nwhere \u03c9 \u2208 Rk, yt is the target output at time t, and the \u03b2\u0302t \u2208 B\u0302 were produced to minimize (17). This includes any differentiable linear function of the \u03b2\u0302t \u2208 B\u0302. In this paper, we focus on classification tasks and thus use the binomial deviance loss of logistic regression, i.e. `s(\u03c9>\u03b2\u0302t, yt) = log(1 + e\u2212yt\u03c9 >\u03b2\u0302t), yt \u2208 {\u22121,+1}.\nThe crux of task-driven dictionary learning is converting the readily available gradients of `s w.r.t. the structure codes \u03b2\u0302t into gradients w.r.t. the pseudo-dictionaries Dt with which they were computed to minimize (17), as gradients w.r.t. theDt easily produce gradients w.r.t. theAi \u2208 A\u0302. Unfortunately, the optimization producing the \u03b2\u0302t makes the conversion\u2207\u03b2\u0302t \u2192 \u2207Dt non-trivial. However, Mairal et al. (2011) show that if elastic-net regularization is used to produce each \u03b2\u0302t from the (xt, Dt), the gradient of the perinstance supervised loss `s w.r.t. Dt can be computed as follows:\n\u2207Dt`s(\u03c9T \u03b2\u0302t, yt) = \u2212Dt\u03c6t\u03b2\u0302>t + (xt \u2212Dt\u03b2\u0302t)\u03c6>t , (20)\nin which \u03c6t \u2208 Rk is defined as follows:\n\u03c6t\u039bC = 0, \u03c6t\u039b = (D > t\u039bDt\u039b+\u03b1\u03bb\u03b2I) \u22121\u2207\u03b2\u0302t\u039b`s(\u03c9 >\u03b2\u0302t, yt) (21) where \u039b denotes the indices of non-zero entries in the sparse \u03b2\u0302t, \u039bC indicates the complementary set of indices, and \u03b1\u03bb\u03b2 is the `2 regularization weight from (18). Once gradients of `s w.r.t. each Dt (i.e. \u2207Dt`s) have been computed for each time t, they can be backpropagated through the pseudo-dictionary formation process and summed across time points to get gradients w.r.t. each Ai \u2208 A\u0302 (i.e. \u2207Ai`s).\nGiven unsupervised gradients \u2207Ai`u, computed as described at the end of Sec. 4.2, and supervised gradients \u2207Ai`s, we define the final gradients for stochastic descent optimization of the combined unsupervised/supervised objective as follows:\n\u2202Ai = \u03b3\u2207Ai`u + (1\u2212 \u03b3)\u2207Ai`s; 0 \u2264 \u03b3 \u2264 1, (22)\nwhere \u03b3 is a mixing parameter controlling the tradeoff between supervised and unsupervised learning. As before, we enforce symmetry and zero-diagonal constraints prior to using the joint gradients for basis updates."}, {"heading": "5. Synthetic Network Analysis", "text": "This section presents tests based on simulated observation sequences which show the ability of our algorithm to recover recurring elements of time varying network structures. We generated each observation sequence by drawing the observation xt at time t from a normal distribution N (0,\u03a3t), in which \u03a3t was a convex combination of four covariance matrix bases: \u03a3t = \u22114 i=1 \u03b1 i t\u03a3\ni, with\u22114 i=1 \u03b1 i t = 1 and 0 \u2264 \u03b1it \u2264 1. We generated smooth trajectories for the \u03b1it (an example set of trajectories is shown in Fig. 1). We generated each \u03a3i by symmetrically removing two thirds of the off-diagonal entries (the ones with the smallest magnitude) from a random covariance matrix with eigenvalues uniformly distributed in (0, 1), and then rescaling diagonal entries to ensure positive definiteness. An example of the sparse basis structures used in our tests can be seen in the right panel of Fig. 2. The inputs ranged from 10-dimensional to 40-dimensional. For each tested dimensionality, we generated 25 sequences of 5000 observations, with the first 3000 reserved for training and the last 2000 reserved for testing; each sequence was based on different basis matrices \u03a3i and different \u03b1it trajectories. Results are averaged over the 25 sequences.\nMethods based on (12) are much better suited for this task than methods expecting abrupt \u201cchange point\u201d structure. Hence, we tested three methods for estimating time varying network structure in our sequences: locally weighted `1-\nregularized self-regression (as described in (12)), the same self-regression followed by projection of the inferred structures onto the principal components of structures estimated for each time point in the training set, and our iterative approach to learning task-driven basis structures.\nThe self-regression-based method used in our tests can be considered equivalent to KELLER (Song et al., 2009a). Using the principal components of the set of A\u2217t produced by this method is itself novel, and can be seen as an approximation to our method. When executing our method, we initialized the set A\u0302 using these principal structures. In our tests, we used six principal structures with the PCA-based method and learned six basis structures with our algorithm.\nWe measured test performance for a classification task in which the class of each xt was set as follows: yt = 1 if \u03b11t + \u03b1 2 t \u2265 \u03b13t + \u03b14t and yt = \u22121 otherwise. We also estimated a similarity score between the sets of estimated structures and the true precision matrices underlying each sequence, as explained below.\nClassification was performed using the parameterization produced by each method for a given xt (i.e. a matrix A\u2217t for the self-regression method, the same matrix projected onto a set of principal structures for the PCA method, and the inferred vector \u03b2\u0302t for our algorithm) as input features to a regularized logistic regression classifier, with the target class determined by yt. Fig. 3 presents the results. The basis structures learned by our method, and the codes they induce, offer an informative representation of regularities in time varying sparse network structure.\nWe measured similarity between learned bases and the true precision matrices using a form of pairwise matrix correlation. First we set the diagonal entries of each matrix to zero, then their off-diagonal entries to zero mean and unit norm, and finally \u201cvectorize\u201d each matrix and compute the\n660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714\n720 721 722 723 724 725 726 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769\nImproved Estimation in Time Varying Models\n0 500 1000 1500 100\n50\n0\n50\nAc cu\nm ul\nat ed\nE vi\nde nc\ne k6b\n0 500 1000 1500\n200\n100\n0\n100\n200\nk3b\n0 500 1000 1500\n80\n40\n0\n40\n80\nl1b\nFigure 4. Behavior of the evidence accumulation classifier learned using features produced by our algorithm, averaged over all trials for each subject. As indicated by Table 1, subject k6b proved difficult for both our method and RCSP. With the other two subjects, the discriminative capacity of our bases can be clearly seen in the rapid bifurcation of their induced classifier response after the cue time at x = 500. Lines trending upwards indicate left hand trials and lines trending downwards represent right hand trials.\nsubject during a set of test trials, given a labeled set of training trials. In each trial, a cue is given to the subject indicating a motor action, after which the subject visualizes that action for several seconds. Cortical activity during each trial was measured by a set of 60 electrodes placed on the scalp, taking measurements at 250Hz. Data collected from these electrodes was the subject of our analysis.\nWe used left hand and right hand trials from this dataset for the subjects l1b, k3b, and k6b. Several trials from each subject were discarded due to significant artifacts, as measured by deviation from a Gaussian model of the mean behavior of the joint set of trials for a subject. We also applied a whitening transform V D\u2212 1 2 V T to each subject\u2019s data prior to analysis, where D was a diagonal matrix containing the eigenvalues of the data and the columns of V were the corresponding eigenvectors. We set kernel widths and regularization weights for the optimization in (14) uniformly for all subjects and trials, following a brief manual search.\nWe learned a set of 20 sparse basis structures for each subject using our algorithm in an unsupervised fashion (i.e. \u03b3 = 1). Afterwards, we performed 20 rounds of randomized cross-validation in which we split the trials for each subject 4/1 into training/test sets. We trained three classifiers in each round of cross-validation: a classifier built on the \u03b2\u0302t inferred by our algorithm after a period of supervised basis updates (i.e. \u03b3 = 0.75) using the training set, a classifier built on the output of a set of 20 RCSP filters (Lotte & Guan, 2011), and a classifier built on the combination of both feature sets. The regularization parameter for RCSP was selected to maximize expected performance across all subjects.\nWe built our classifier by considering the \u03b2\u0302t and class labels for each time point in each training trial as inferred feature/label pairs for training an 2-regularized logistic regression classifier. Given the encoding of a particular trial in terms of a set of \u03b2\u0302t, an overall output for the trial was\ncomputed by accumulating (i.e. summing) the output of the learned single time-point classifier over the first three postcue seconds of the trial. After this evidence accumulation phase, the classification for each trial was determined by the sign of its overall output. RCSP filters were trained as described in (Lotte & Guan, 2011), after which the squared responses of these filters to the observations were used as input features to an 2-regularized logistic regression classifier, trained as above We also trained an analogous classifier using the combined features produced by our algorithm and the RCSP filters at each time point. Classification results for each subject are shown in Table 1, and a visual representation of the evidence accumulation process based on our features is shown in Fig. 4. Classifiers constructed in this fashion have the advantage of being amenable to \u201cearly exit\u201d, in the spirit of drift-diffusion decision making.\nThese results show that our approach produces informative features in a real-world scenario, with the results for the combined features suggesting that our features supplement, rather than replace, the commonly used RCSP features.\n6. Conclusion We introduced a problem formulation in the context of multiply parameterized models. Using this formulation, we developed a novel algorithm for learning representations of sparse structure in time varying networks with recurring structural motifs. We used tests on synthetic data to show that our algorithm behaves as desired under suitable conditions, while an application to BCI EEG data showed the potential value of our algorithm in real world conditions.\nWe plan to investigate more in-depth the performance of our approach by applying it to other types of tasks, such as analysis of time varying weather and traffic patterns. Our algorithm is also readily extensible to the estimation of time varying structure in Dynamic Bayesian Networks. We also plan to look at alternative parameter transformation methods, beyond the linear transforms considered in this paper.\nFigure 4. Behavior of the evidence accumulation classifier learned using features produced by our algorithm, averaged over all trials for each subject. As indicated by Table 1, subject k6b proved difficult for both our method and RCSP. With the other two subjects, the discriminative capacity of our bases can be clearly seen in the rapid bifurcation of their induced classifier response after the cue time at x = 500. Lines trending upwards indicate left hand trials and lines trending downwards represent right hand trials.\ndot product betwe n the resulting vectors. This measure ranges from \u22121 to 1, with larg r magnitudes indicating greater similarity. For each sequence and each method, we found the best match to each (\u03a3i)\u22121, as determined by the magnitude of our correlation score, among th set of bases produced by that method. We then averaged best match scores for each method over both true bases and sequences, to get a final score for each dimensionality. Fig. 3 shows the similarity scores achieved by the PCA-based method and our method, with the bases produced by our method consistently displaying greater similarity to the true bases than those produced by PCA alone. Fig. 2 shows a typical example of a best match produced by our method during these tests; as can be seen, the learned basis is qualitatively very similar to the true basis."}, {"heading": "6. BCI EEG Analysis", "text": "We applied our algorithm to the analysis of EEG data from a Brain Computer Interface (abbr. BCI) motor imagery experiment available as task 3a from BCI competition III (Schlo\u0308gl et al., 2005; Blankertz et al., 2006). In this task, the objective is to infer the motor action visualized by a subject during a set of test trials, given a labeled set of training trials. In each trial, a cue is given to the subject indicating a motor action, after which the subject visualizes that action for several seconds. Cortical activity during each trial was measured by a set of 60 electrodes placed on the scalp, taking measurements at 250Hz. Data collected from these electrodes was the subject of our analysis.\nWe used left hand and right hand trials from this dataset for the subjects l1b, k3b, and k6b. Several trials from each subject were discarded due to significant artifacts, as measured by deviation from a Gaussian model of the mean behavior of the joint set of trials for a subject. We also applied a whitening transform V D\u2212 1 2V T to each subject\u2019s data prior to analysis, where D was a diagonal matrix containing the\neigenvalues of the data and the columns of V were the corresponding eigenvectors. We set kernel widths and regularization weights for the optimization in (15) uniformly for all subjects and trials, following a brief manual search.\nWe lear ed a set of 20 sparse ba is structures for each subject using our algorithm in an unsupervis d fashion (i.e. \u03b3 = 1). Afterwards, we performed 20 rounds of randomized cross-validation in which we split the trials for ach subject 4/1 into training/test sets. We trained three classifiers in each round of cross-validation: a classifier built on the \u03b2\u0302t inferred by our algorithm after a period of supervised basis updates (i.e. \u03b3 = 0.75) using the training set, a classifier built on the output of a set of 20 RCSP filters (Lotte & Guan, 2011), and a classifier built on the combination of both feature sets. The regularization parameter for RCSP was selected to maximize expected performance across all subjects.\nWe built our classifier by considering the \u03b2\u0302t and class labels for each time point in each training trial as inferred feature/label pairs for training an `2-regularized logistic regression classifier. Given the encoding of a particular trial in terms of a set of \u03b2\u0302t, an overall output for the trial was computed by accumulating (i.e. summing) the output of the learned single time-point classifier over the first three postcue seconds of the trial. After this evidence accumulation phase, the classification for each trial was determined by the sign of its overall output. RCSP filters were trained as described in (Lotte & Guan, 2011), after which the squared responses of these filters to the observations were used as input features to an `2-regularized logistic regression classifier, trained as for our algorithm. We also trained an analogous classifier using the combined features produced by our algorithm and the RCSP filters at each time point. Classification results for each subject are shown in Table 1, and a visual representation of the evidence accumulation process based on our features is shown in Fig. 4. Classifiers constructed in this fashion have the advantage of being amenable to \u201cearly exit\u201d, in the spirit of drift-diffusion decision making.\nThese results show that our approach produces informative\nfeatures in a real-world scenario, with the results for the combined features suggesting that our features supplement, rather than replace, the commonly used RCSP features."}, {"heading": "7. Conclusion", "text": "We introduced a problem formulation in the context of multiply parameterized models. Using this formulation, we developed a novel algorithm for learning representations of sparse structure in time varying networks with recurring structural motifs. We used tests on synthetic data to show that our algorithm behaves as desired under suitable conditions, while an application to BCI EEG data showed the potential value of our algorithm in real world conditions.\nWe plan to apply our approach to other types of tasks, such as analysis of time varying weather and traffic patterns, in addition to investigating alternative parameter transformation methods, beyond the linear transforms considered in this paper. Our algorithm is readily extensible to the estimation of time varying structure in Dynamic Bayesian Networks.\nAcknowledgements: Funded by NSERC and ONR."}], "references": [{"title": "Recovering time-varying networks of dependencies in social and biological studies", "author": ["A. Ahmed", "E.P. Xing"], "venue": null, "citeRegEx": "Ahmed and Xing,? \\Q2009\\E", "shortCiteRegEx": "Ahmed and Xing", "year": 2009}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["B. Boots", "G. Gordon"], "venue": "In AAAI,", "citeRegEx": "Boots and Gordon,? \\Q2011\\E", "shortCiteRegEx": "Boots and Gordon", "year": 2011}, {"title": "Likelihood-based sufficient dimension reduction", "author": ["R.D. Cook", "L. Forzani"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Cook and Forzani,? \\Q2009\\E", "shortCiteRegEx": "Cook and Forzani", "year": 2009}, {"title": "Sparse inverse covariance estimation with the graphical lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2008}, {"title": "Regularization paths for generalized linear models via coordinate descent", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Technical report, Stanford University,", "citeRegEx": "Friedman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 2009}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": null, "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Independent component analysis: algorithms and applications", "author": ["A. Hyv\u00e4rinen", "E. Oja"], "venue": "Neural Networks,", "citeRegEx": "Hyv\u00e4rinen and Oja,? \\Q2000\\E", "shortCiteRegEx": "Hyv\u00e4rinen and Oja", "year": 2000}, {"title": "On time varying undirected graphs", "author": ["M. Kolar", "E.P. Xing"], "venue": "In AISTATS,", "citeRegEx": "Kolar and Xing,? \\Q2011\\E", "shortCiteRegEx": "Kolar and Xing", "year": 2011}, {"title": "Sparsistent Learning of Varying-coefficient Models with Structural Changes", "author": ["M. Kolar", "L. Song", "E.P. Xing"], "venue": "In NIPS 22,", "citeRegEx": "Kolar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kolar et al\\.", "year": 2009}, {"title": "Disclda: Discriminative learning for dimensionality reduction and classification", "author": ["S. Lacoste-Julien", "F. Sha", "M.I. Jordan"], "venue": "In NIPS", "citeRegEx": "Lacoste.Julien et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Lacoste.Julien et al\\.", "year": 2008}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": null, "citeRegEx": "Lauritzen,? \\Q1996\\E", "shortCiteRegEx": "Lauritzen", "year": 1996}, {"title": "Regularizing common spatial patterns to improve bci designs: Unified theory and new algorithms", "author": ["F. Lotte", "C. Guan"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "Lotte and Guan,? \\Q2011\\E", "shortCiteRegEx": "Lotte and Guan", "year": 2011}, {"title": "Task-driven dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Mairal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mairal et al\\.", "year": 2011}, {"title": "High-dimensional graphs and variable selection with the lasso", "author": ["N Meinshausen", "P. B\u00fchlmann"], "venue": "Annals of Statistics,", "citeRegEx": "Meinshausen and B\u00fchlmann,? \\Q2006\\E", "shortCiteRegEx": "Meinshausen and B\u00fchlmann", "year": 2006}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "Olshausen and Field,? \\Q1996\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1996}, {"title": "Supervised dimensionality reduction using mixture models", "author": ["Sajama", "A. Orlitsky"], "venue": "In ICML,", "citeRegEx": "Sajama and Orlitsky,? \\Q2005\\E", "shortCiteRegEx": "Sajama and Orlitsky", "year": 2005}, {"title": "Exploring regression structure using nonparametric functional estimation", "author": ["A.M. Samarov"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Samarov,? \\Q1993\\E", "shortCiteRegEx": "Samarov", "year": 1993}, {"title": "Characterization of four-class motor-imagery eeg data for the bcicompetition", "author": ["A. Schl\u00f6gl", "F. Lee", "H. Bischof", "G. Pfurtscheller"], "venue": "Journal of Neural Engineering,", "citeRegEx": "Schl\u00f6gl et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schl\u00f6gl et al\\.", "year": 2005}, {"title": "Reduced-rank hidden markov models", "author": ["S. Siddiqi", "B. Boots", "G. Gordon"], "venue": "In AISTATS,", "citeRegEx": "Siddiqi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Siddiqi et al\\.", "year": 2010}, {"title": "Keller: estimating timevarying interactions between genes", "author": ["L. Song", "M. Kolar", "E.P. Xing"], "venue": "Bioinformatics, 25:i128\u2013", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "Time-varying dynamic bayesian networks", "author": ["L. Song", "M. Kolar", "E.P. Xing"], "venue": "In NIPS 22,", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science, 290:2319\u20132323,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statiscal Society. Series B,", "citeRegEx": "Tibshirani,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani", "year": 1996}, {"title": "Highdimensional graphical model selection using l1-regularized logistic regression", "author": ["M.J. Wainwright", "P. Ravikumar", "J. Lafferty"], "venue": "In NIPS", "citeRegEx": "Wainwright et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2007}, {"title": "An adaptive estimation of dimension reduction space", "author": ["Y. Xia", "H. Tong", "W.K. Li", "Zhu", "L-X"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Xia et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xia et al\\.", "year": 2002}, {"title": "Time varying undirected graphs", "author": ["S. Zhou", "J. Lafferty", "L. Wasserman"], "venue": "Machine Learning,", "citeRegEx": "Zhou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2010}, {"title": "Regularization and variable selection via the elastic net", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Zou and Hastie,? \\Q2005\\E", "shortCiteRegEx": "Zou and Hastie", "year": 2005}], "referenceMentions": [{"referenceID": 21, "context": "(Tenenbaum et al., 2000), and dimension reduction for regression (Fukumizu et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 5, "context": ", 2000), and dimension reduction for regression (Fukumizu et al., 2004; Cook & Forzani, 2009).", "startOffset": 48, "endOffset": 93}, {"referenceID": 9, "context": "Examples of the first approach include DiscLDA (Lacoste-Julien et al., 2008) and supervised dimensionality reduction using Bayesian mixture models (Sajama & Orlitsky, 2005), which seek useful linear reductions of the parameters of a generative model.", "startOffset": 47, "endOffset": 76}, {"referenceID": 18, "context": "The second approach includes the application of spectral methods to learning transformed representations of HMMs (Siddiqi et al., 2010) and PSRs (Boots & Gordon, 2011).", "startOffset": 113, "endOffset": 135}, {"referenceID": 16, "context": "If one views xj A\u03b2\u0302 w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002).", "startOffset": 284, "endOffset": 317}, {"referenceID": 24, "context": "If one views xj A\u03b2\u0302 w i as a reduction of xj into the subspace spanned by the columns of A, followed by a linear regression in that subspace, the objective in (6) is closely related to methods developed for linear dimension reduction for regression based on non-parametric estimators (Samarov, 1993; Xia et al., 2002).", "startOffset": 284, "endOffset": 317}, {"referenceID": 12, "context": "Through an analogy between our algorithm and sparse coding (Olshausen & Field, 1996), we then extend our algorithm to learning of task-driven basis structures, guided by the work in (Mairal et al., 2011).", "startOffset": 182, "endOffset": 203}, {"referenceID": 10, "context": "The use of self-regression for network structure estimation is based on the following results (Lauritzen, 1996):", "startOffset": 94, "endOffset": 111}, {"referenceID": 25, "context": "Value estimation methods estimate each entry in \u03a3\u22121 (Zhou et al., 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al.", "startOffset": 52, "endOffset": 71}, {"referenceID": 3, "context": ", 2010), while structure estimation determines the pattern of zero/non-zero entries (Friedman et al., 2008; Song et al., 2009b; Kolar & Xing, 2011)).", "startOffset": 84, "endOffset": 147}, {"referenceID": 22, "context": "The sparsity assumption can be incorporated into the self-regression process by using sparsifying regression techniques, such as the well known Lasso (Tibshirani, 1996).", "startOffset": 150, "endOffset": 168}, {"referenceID": 23, "context": "\u03a3\u22121 under suitable conditions (Meinshausen & B\u00fchlmann, 2006; Wainwright et al., 2007; Kolar & Xing, 2011).", "startOffset": 30, "endOffset": 105}, {"referenceID": 8, "context": "A recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011).", "startOffset": 128, "endOffset": 229}, {"referenceID": 25, "context": "A recent line of work focuses on extending methods for network structure estimation for the case when structures vary over time (Ahmed & Xing, 2009; Kolar et al., 2009; Song et al., 2009a;b; Zhou et al., 2010; Kolar & Xing, 2011).", "startOffset": 128, "endOffset": 229}, {"referenceID": 8, "context": "This second assumption distinguishes KELLER from methods such as (Ahmed & Xing, 2009) and (Kolar et al., 2009), which assume abrupt changes in the network structure.", "startOffset": 90, "endOffset": 110}, {"referenceID": 12, "context": "We use this form to meet the assumptions required for the task-driven dictionary learning described in (Mairal et al., 2011), used in the further extension of our algorithm.", "startOffset": 103, "endOffset": 124}, {"referenceID": 4, "context": "When optimizing B\u0302 with \u00c2 held fixed, we compute the optimal \u03b2\u0302t for each t via elastic-net regressions solved with the publicly available, highly optimized glmnet package (Friedman et al., 2009).", "startOffset": 172, "endOffset": 195}, {"referenceID": 12, "context": "We can adapt the work of (Mairal et al., 2011) to enable our algorithm to learn task-driven sets of basis network structures.", "startOffset": 25, "endOffset": 46}, {"referenceID": 12, "context": "However, Mairal et al. (2011) show that if elastic-net regularization is used to produce each \u03b2\u0302t from the (xt, Dt), the gradient of the perinstance supervised loss `s w.", "startOffset": 9, "endOffset": 30}, {"referenceID": 17, "context": "BCI) motor imagery experiment available as task 3a from BCI competition III (Schl\u00f6gl et al., 2005; Blankertz et al., 2006).", "startOffset": 76, "endOffset": 122}], "year": 2012, "abstractText": "Locally adapted parameterizations of a model (such as locally weighted regression) are expressive but often suffer from high variance. We describe an approach for reducing this variance, based on the idea of estimating simultaneously a transformed space for the model and locally adapted parameterizations expressed in the new space. We present a new problem formulation that captures this idea and illustrate it in the important context of time varying models. We develop an algorithm for learning a set of bases for approximating a time varying sparse network; each learned basis constitutes an archetypal sparse network structure. We also provide an extension for learning task-specific bases. We present empirical results on synthetic data sets, as well as on a BCI EEG classification task.", "creator": "LaTeX with hyperref package"}}}