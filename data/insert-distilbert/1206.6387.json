{"id": "1206.6387", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Fast classification using sparse decision DAGs", "abstract": "in this important paper we propose an algorithm that builds sparse decision dags ( directed random acyclic graphs ) from a list of base classifiers provided by an external learning method such as adaboost. ostensibly the basic idea approach is to cast the dag dynamic design task as a markov decision process. each instance can decide to use or skip to necessarily skip each base classifier, based on the arbitrary current state of the classifier being built. the result is a sparse decision dag where the base classifiers are selected in a data - dependent way. the method has maintained a single hyperparameter with therefore a clear semantics ordering of components controlling the performance accuracy / reward speed trade - off. the algorithm is competitive with state - of - the - farm art cascade detectors on three object - detection benchmarks, and despite it clearly outperforms them drastically when there is a small number of base classifiers. unlike large cascades, it is also readily applicable for multi - class classification. using indeed the continuous multi - class setup, we rapidly show on a benchmark web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (398kb)", "http://arxiv.org/abs/1206.6387v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["r\u00f3bert busa-fekete", "djalel benbouzid", "bal\u00e1zs k\u00e9gl"], "accepted": true, "id": "1206.6387"}, "pdf": {"name": "1206.6387.pdf", "metadata": {"source": "META", "title": "Fast classification using sparse decision DAGs", "authors": ["Djalel Benbouzid", "R\u00f3bert Busa-Fekete", "Bal\u00e1zs K\u00e9gl"], "emails": ["djalel.benbouzid@gmail.com", "busarobi@gmail.com", "balazs.kegl@gmail.com"], "sections": [{"heading": "1. Introduction", "text": "There are numerous applications where the computational requirements of classifying a test instance are as important as the performance of the classifier itself. Object detection in images (Viola & Jones, 2004) and web page ranking (Chapelle & Chang, 2011) are well-known examples. A more recent application domain with similar requirements is trigger design in\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nhigh energy physics (Gligorov, 2011). Most of these applications come with another common feature: the negative class (usually called noise or background) sometimes has orders of magnitudes higher probability than the positive class. Besides the testing time constraints, this also makes training difficult: traditional classification-error-based measures are not adequate, and using prior class probabilities in constructing training samples leads to either enormous data sizes or little representativity of the positive class.\nA common solution to these problems is to design cascade classifiers (Viola & Jones, 2004). A cascade classifier consists of stages. In each stage a binary classifier attempts to eliminate background instances by classifying them negatively. Positive classification in inner stages sends the instance to the next stage, so detection can only be made in the last stage. By using simple and fast classifiers in the first stages, \u201ceasy\u201d background instances can be rejected fast, shortening the expected testing time. The cascade structure also allows us to use different training sets in different stages, having more difficult background samples in later stages.\nCascade classifiers, however, have many disadvantages in both the training and test phases. The training process requires a lot of hand-tuning of control parameters, and it is non-trivial how to handle the tradeoff between the performance and the complexity of the cascade. Also, each individual stage needs to be trained with examples that have been classified positively by all the previous stages, which becomes difficult to satisfy in the later stages. Moreover, during test time, the cascade structure itself has several drawbacks. First, for a given stage, the margin information of a test example is lost and is not exploited in the subsequent stages. Second, all the positive instances have to pass through all the stages for a correct clas-\nsification. Finally, extending the cascade architecture to the multi-class case is non-trivial. For example in web page ranking, it is just as crucial to make a fast prediction on the relevance of a web page to a query as in object detection (Chapelle et al., 2011a), but unlike in object detection, human annotation often provides more than two relevance levels.\nIn this paper we propose a method intended to overcome these problems. In our setup we assume that we are given a sequence of low-complexity, possibly multiclass base classifiers (or features) sorted by importance or quality. Our strategy is to design a controller or a decision maker which decides which base classifiers should be evaluated for a given instance. The controller makes its decision sequentially based on the output of the base classifiers evaluated so far. It has three possibilities in each step: 1) it can decide to continue the classification by evaluating the next classifier, 2) skip a classifier by jumping over it, or 3) quit and use the current combined classifier. The goal of the controller is to achieve a good performance with as few base classifier evaluations as possible. This flexible setup can accommodate any performance evaluation metric and an arbitrary computational cost function. Designing the controller can be naturally cast into a Markov decision process (MDP) framework where the roles are the following: the policy is the controller, the index of the base classifier and the output of classifier constitute the states, the alternatives correspond to the actions, and the rewards are defined based on the target metric and the cost of evaluating the base classifiers.\nOur approach has several advantages over cascades. First, we can eliminate stages. Similar to SoftCascade (Bourdev & Brandt, 2005), the base classifiers do not have to be organized into a small number of stages before or while learning the cascade. Second, we can easily control the trade-off between the average number of evaluated base classifiers and the quality of the classification by combining these two competing goals into an appropriate reward. The form of the reward can also easily accommodate costsensitivity (Saberian & Vasconcelos, 2010) of the base classifiers although we will not investigate this issue here. The fact that some base classifiers can be skipped has an important consequence: the resulting classifier is sparse, moreover, the number and identities of base classifiers depend on the particular instances. Third, eliminating stages allows each instance to \u201cdecide\u201d its own path within the list of base-classifiers. Theoretically, we could have as many different paths as training instances, but, a-posteriori, we observe clustering in the \u201cpath-space\u201d. Fourth, eliminating stages\nalso greatly simplifies the design. Our algorithm is basically turn-key: it comes with an important design parameter (the trade-off coefficient between accuracy and speed) and a couple of technical hyperparameters of the MDP algorithm that can be kept constant across the benchmark problems we use. Finally, the multi-class extension of the technique is quite straightforward.\nAllowing skipping is an important feature of the algorithm. The result of this design choice is that the structure of the learned classifier is not a cascade, but a more general directed acyclic graph or a decision DAG. In fact, the main reason for sticking to the cascade design is that it is easy to control with semi-manual heuristics. Once the construction is automatic, keeping the cascade architecture is no longer a necessary constraint. Allowing skipping is also a crucial difference compared to the approach of (Po\u0301czos et al., 2009) who also proposed to learn a cascade in an MDP setup. While their policy simply designs optimal thresholds in stages of a classical cascade, MDDAG outputs a classifier with a different structure. Our method can also be related to the sequential classifier design of (Dulac-Arnold et al., 2011). In their approach the action space is much larger: at any state the controler can decide to jump to any of the base classifiers, and so the action space grows with the number of base learners. Whereas this design choice makes feature selection more flexible, it also generates a harder learning problem for the MDP.\nThe paper is organized as follows. In Section 2 we describe the algorithm, then in Section 3 we present our experimental results. In Section 4 we discuss the algorithm and its connection with existing methods, and in Section 5 we draw some pertinent conclusions."}, {"heading": "2. The MDDAG algorithm", "text": "We will assume that we are given a sequence of N base classifiers H = (h1, . . . ,hN ). Although in most cases cascades are built for binary classification, we will describe the method for the more general multiclass case, which means that hj : X \u2192 RK , where X is the input space and K is the number of classes. The semantics of h is that, given an observation x \u2208 X , it votes for class ` if its `th element h`(x) is positive, and votes against class ` if h`(x) is negative. The absolute value |h`(x)| can be interpreted as the confidence of the vote. This assumption is naturally satisfied by the output of AdaBoost.MH (Schapire & Singer, 1999), but in principle any algorithm that builds its final classifier as a linear combination of simpler functions can be used to provide H. In the case of AdaBoost.MH or multi-class neural networks, the final (or strong or\naveraged) classifier defined by the full sequence H is f(x) = \u2211N j=1 hj(x), and its prediction for the class in-\ndex of x is \u0302\u0300= argmax` f`(x). In binary detection, f is usually used as a scoring function. The observation x is classified as positive if f1(x) = \u2212f2(x) > \u03b8 and background otherwise. The threshold \u03b8 is a free parameter that can be tuned to achieve, for instance, a given false positive rate.\nThe goal of the MDDAG (Markov decision direct acyclic graph) algorithm is to build a sparse final classifier from H that does not use all the base classifiers, and which selects them in a way depending on the instance x to be classified. For a given observation x, we process the base classifiers in their original order. For each base classifier hj , we choose from among three possible actions: 1) we Evaluate hj and continue, 2) we Skip hj and continue, or 3) we Quit and return the classifier built so far. Let\nbj(x) = 1\u2212 I {aj = Skip \u2228 \u2203j\u2032 < j : aj\u2032 = Quit} (1)\nbe the indicator that hj is evaluated on x, where aj \u2208 {Eval,Skip,Quit} is the action taken at step j and the indicator function I {A} is 1 if its argument A is true and 0 otherwise. Then the final classifier built by the procedure is\nf (N)(x) = N\u2211 j=1 bj(x)hj(x). (2)\nThe decision on action aj will be made based on the index of the base classifier j and the output vector of the classifier\nf (j)(x) = j\u2211\nj\u2032=1\nbj\u2032(x)hj\u2032(x). (3)\nbuilt up to step j.1 Formally, aj = \u03c0 ( (sj(x) ) , where\nsj(x) = ( f (j\u22121) 1 (x), . . . , f (j\u22121) K (x), j \u2212 1 ) \u2208 RK \u00d7 N+\n(4) is the state we are in before visiting hj , and \u03c0 is a policy that determines the action in state sj . The initial state s1 is the zero vector with K + 1 elements.\nThis setup formally defines a Markov decision process (MDP). An MDP is a 4-tupleM = (S,A,P,R), where S is the (possibly infinite) state space and A is the\n1When using AdaBoost.MH, the base classifiers are binary hj(x) = {\u00b1\u03b1j}K , and we normalize the output (3) by PN j=1 \u03b1j , but since this factor is constant, the only reason to do so is to make the range of the state space uniform across experiments.\ncountable set of actions. P : S \u00d7 S \u00d7A \u2192 [0, 1] is the transition probability kernel which defines the random transitions s(t+1) \u223c P(\u00b7|s(t), a(t)) from a state s(t) applying the action a(t), and R : R \u00d7 S \u00d7 A \u2192 [0, 1] defines the distribution R(\u00b7|s(t), a(t)) of the immediate reward r(t) for each state-action pair. A deterministic policy \u03c0 assigns an action to each state \u03c0 : S \u2192 A. We will only use undiscounted and episodic MDPs where the policy \u03c0 is evaluated using the expected sum of rewards\n% = E { T\u2211 t=1 r(t) } (5)\nwith a finite horizon T . In the episodic setup we also have an initial state (s1 in our case) and a terminal state s\u221e which is impossible to leave. In our setup, the state s(t) is equivalent to sj(x) (4) with j = t. The action Quit brings the process to the terminal state s\u221e. Note that in s(T ) only the Quit action is allowed."}, {"heading": "2.1. The rewards", "text": "As our primary goal is to achieve a good performance in terms of the evaluation metric of interest, we will penalize the error of f (t) when the action a(t) = Quit is applied. The setup can handle any loss function. Here, we will use the multi-class 0-1 loss function\nLI(f , (x, `)) = I { f`(x)\u2212max\n`\u2032 6=` f`\u2032(x) < 0 } and the multi-class exponential loss function\nLexp(f , (x, `)) = exp  K\u2211 `\u2032 6=` f`\u2032(x)\u2212 f`(x)  , where the training observations (x, `) \u2208 Rd \u00d7 {1, . . . ,K} are drawn from a distribution D. Note that in the binary case, LI and Lexp recover the classical binary notions.\nWith these notations, the reward for the Quit action comes from the distribution\nR(r|s(t),Quit) = P(x,`)\u223cD ( \u2212 L(f , (x, `))|s(t) = (f (t\u22121)(x), t\u2212 1) ) . (6)\nFrom now on we will refer to our algorithm as MDDAG.I or MDDAG.EXP when we use 0-1 loss or exponential loss, respectively. In principle, any of the usual convex upper bounds (e.g., logistic, hinge, quadratic) could be used in the MDP framework. The exponential loss function was inspired by the setup of AdaBoost (Freund & Schapire, 1997; Schapire & Singer, 1999).\nTo encourage sparsity, we will also penalize each evaluated base classifier h by a uniform fixed negative reward R(r|s,Eval) = \u03b4(\u2212\u03b2 \u2212 r), (7) where \u03b4 is the Dirac delta and \u03b2 is a hyperparameter that represents the accuracy-speed trade-off. Note that, again, this flexible setup can accommodate any cost function penalizing the evaluation of base classifiers. Finally, choosing the Skip action does not incur any reward, so R(r|s,Skip) = \u03b4(0).\nThe goal of reinforcement learning (RL) in our case is to learn a policy which maximizes the expected sum of rewards (5). Since in our setup, the transition P is deterministic given the observation x, the expectation in (5) is taken with respect to the random input point (x, `). This means that the global objective of the MDP is to minimize\nE(x,`)\u223cD L(f , (x, `))+ \u03b2 N\u2211 j=1 bj(x) . (8)"}, {"heading": "2.2. Learning the policy", "text": "There are several efficient algorithms available for learning the policy \u03c0 using an iid sample D =( (x1, `1), . . . , (xn, `n) ) drawn from D (Sutton & Barto, 1998). When P and R are unknown, model-free methods are commonly used for learning the policy \u03c0. These methods directly learn a value function (the expected reward in a state or for a state-action pair) and derive a policy from it. Among model-free RL algorithms, temporal-difference (TD) learning algorithms are the most widely used. They can be divided into two groups: off-policy and on-policy methods. In the case of off-policy methods the policy search method learns about one policy while following another, whereas in the on-policy case the policy search algorithm seeks to improve the current policy by maintaining sufficient exploration. On-policy methods have an appealing practical advantage: they usually converge faster to the optimal policy than off-policy methods.\nWe shall use the SARSA(\u03bb) algorithm (Rummery & Niranjan, 1994) with replacing traces to learn the policy \u03c0. For more details, we refer the reader to (Szepesva\u0301ri, 2010). SARSA(\u03bb) is an on-policy method, so to make sure that all policies can be visited with nonzero probability, we use an -greedy exploration strategy. To be precise, we apply SARSA in an episodic setup: we use a random training instance x from D per episode. The instance follows the current policy with probability 1 \u2212 and chooses a random action with probability . The instance observes the\nimmediate rewards defined based on some loss function, or (7) after each action. The policy is updated during the episode according to SARSA(\u03bb).\nIn our experiments we used AdaBoost.MH2 to obtain a pool of weak classifiers H, and the RL Toolbox 2.03 for training the MDDAG. We ran AdaBoost.MH for N = 1000 iterations, and then trained SARSA(\u03bb) on the same training set. The hyperparameters of SARSA(\u03bb) were kept constant throughout the experiments. We set \u03bb to 0.95. In principle, the learning rate should decrease to 0, but we found that this setting forced the algorithm to converge too fast to suboptimal solutions. Instead we set the learning rate to a constant 0.2, we evaluated the current policy after every 10000 episodes, and we selected the best policy based on their performance also on the training set (overfitting the MDP was a nonissue). The exploration term was decreased gradually as 0.3 \u00d7 1/d 10000\u03c4 e, where \u03c4 is the number of training episodes. We trained SARSA(\u03bb) for 106 episodes.\nAs a final remark, note that maximizing (8) over the data set D is equivalent to minimizing a margin-based loss with an L0 constraint. If rI (6) is used as a reward, the loss is also non-convex, but minimizing a loss with an L0 constraint is NP-hard even if the loss is convex (Davis et al., 1997). So, what we are aiming at is an MDP-based heuristic to solve an NP-hard problem, something that is not without precedent (Ejov et al., 2004). This equivalence implies that even though the algorithm would converge in the ideal case (with a decreasing learning rate), in principle, convergence can be exponentially slow in n. In practice, however, we had no problem finding good policies in reasonable training time."}, {"heading": "3. Experiments", "text": "In Section 3.1 we first verify the sparsity and heterogeneity hypotheses on a synthetic toy example. In Section 3.2, we compare MDDAG with state-of-theart cascade detectors on three object detection benchmarks. After, in Section 3.3 we show how the multiclass version of MDDAG performs on a benchmark web page ranking problem."}, {"heading": "3.1. Synthetic data", "text": "The aim of this experiment was to verify whether MDDAG can learn the subset of \u201cuseful\u201d base classifiers in a data-dependent way. We created a twodimensional binary dataset with real-valued features\n2http://www.multiboost.org (Benbouzid et al., 2012). 3http://www.igi.tugraz.at/ril-toolbox/general/\noverview.html\nwhere the positive class was composed of two easily separable clusters (see Figure 1(a)). This is a typical case where AdaBoost or a traditional cascade is suboptimal since they both have to use all the base classifiers for all the positive instances (Bourdev & Brandt, 2005).\nWe ran MDDAG.I with \u03b2 = 0.01 on the 1000 decision stumps learned by AdaBoost.MH. In Figure 1(b), we plot the number of base classifiers used for each individual positive instance as a function of the twodimensional instance itself. As expected, the \u201ceasier\u201d the instance, the smaller the number of base classifiers are needed for classification. Figure 1(c) confirms our second hypothesis: base classifiers are used selectively, depending on whether the positive instance is in the blue or red cluster.\nThe lower panel of Figure 1 shows a graphical representation of the MDDAG classifier f acting on a data set D. The nodes of the directed acyclic graph (DAG) are the base classifiers in H. Each observation (x, `) \u2208 D determines a set of edges\nUx = {(j, j\u2032) : bj(x) = bj\u2032(x) = 1\u2227 bj\u2032\u2032(x) = 0 for all j < j\u2032\u2032 < j\u2032}.\nIn other words, we take all the base classifiers that are evaluated on the instance (x, `) and connect the nodes representing these base classifiers with a direct edge. The edge set Ux is called the classification path of x which constitutes a directed path by definition. The DAG we plot in Figure 1(d) includes all of the edges U = \u22c3 (x,1)\u2208D Ux generated by the positive instances taken from the training data D. The width of an edge (j, j\u2032) is proportional to its multiplicity #{x : (j, j\u2032) \u2208 Ux, (x, 1) \u2208 D}. The color of an edge (j, j\u2032) represents the proportion of observations taken from the blue and red sub-classes, whose classification path includes (j, j\u2032). Similarly, the size of the node is proportional to #{x : bj(x) = 1, (x, 1) \u2208 D}, and the color of the nodes represent sub-class proportions. The structure of the DAG also agrees with our original intuition, namely that the bulk of the two subclasses are separated early and follow different classification paths. It is also worth noting that even though the number of possible classification paths is exponentially large, the number of realized paths is quite small. Some \u201cnoisy\u201d points along the main diagonal (border between the subclasses) generate rare subpaths, but the bulk of the data mostly follows two paths."}, {"heading": "3.2. Binary detection benchmarks", "text": "In these experiments we applied MDDAG on three image data sets often used for benchmarking object detection cascades. VJ (Viola & Jones, 2004) and CBCL\nare face recognition benchmarks, and DPED (Munder & Gavrila, 2006) is a pedestrian recognition data set. We divided the data sets into training and test sets.\nWe compared MDDAG to three state-of-the-art object detection algorithms (the original Viola-Jones cascade VJCascade (Viola & Jones, 2004), FCBoost (Saberian & Vasconcelos, 2010), and SoftCascade (Bourdev & Brandt, 2005)). VJCascade builds the cascade stage-by-stage by running AdaBoost in each stage. It stops adding base classifiers to the mth stage when the false positive rate (FPR) falls below pmfpr and true positive rate (TPR) exceeds pmtpr, where ptpr and pftr are hyperparameters of the algorithm. The total number of stages is also a hyperparameter. FCBoost also adds base classifiers iteratively to the cascade, but the base classifier can be inserted into any of the stages. The goal is to minimize a global criterion which, similarly to (8), is composed of a performance-based term and a complexity-based term. The number of iterations and the parameter \u03b7 that determines the trade-off between the two competing objectives are hyperparameters of the algorithm. SoftCascade, like MDDAG, builds a cascade on the output of AdaBoost, where each stage consists of exactly one base classifier. The final number of base classifiers is decided beforehand by the hyperparameter N . In each iteration j, the base classifier with the highest balanced edge is selected, and the detection threshold \u03b8j is set to achieve a TPR of 1\u2212 exp (\u03b1j/N \u2212 \u03b1I {\u03b1 < 0}), where \u03b1 is a second hyperparameter of the algorithm. Both the TPR and the number of base classifiers increase with \u03b1, so the choice of \u03b1 influences the speed/accuracy trade-off (although not as explicitly as our \u03b2 or FCBoost\u2019s \u03b7).\nComparing test-time-constrained detection algorithms is quite difficult. The usual trade-off between the false positive rate (FPR) and the true positive rate (TPR) can be captured by ROC curves, but here we also have to take into account the computational efficiency of the detector. In (Bourdev & Brandt, 2005) this problem is solved by displaying three-dimensional FPR/TPR/number-of-features surfaces. Here, we decided to show two-dimensional slices of these surfaces: we fix the FPR to reasonable values and plot the TPR against the detection time. In each of the algorithms we used Haar features as base classifiers, so the detection time can be uniformly measured in terms of the average number of base classifiers needed for detection. In typical detection problems the number of background (negative) instances is orders of magnitudes higher than the number of signal (positive) instances, so we computed this average only over the negative test set. It turns out that using this measure,\nvanilla AdaBoost is fairly competitive with tailormade cascade detectors, so we also included it in the comparison.\nComputing the TPR versus number-of-features curve at a fixed FPR cannot be done in a generic algorithmindependent way. For AdaBoost, in each iteration j (that is, for each number j of base classifiers) we tune the detection threshold \u03b8 to achieve the given test FPR, and plot the achieved test TPR versus j. In the other three algorithms we have 2-3 hyperparameters that explicitly or implicitly influence the average number of base classifiers and the overall performance. We ran the algorithms using different hyperparameter combinations. In each run we set the detection threshold \u03b8 to achieve the given test FPR. With this threshold, each run k determines a TPR/averagenumber-of-features pair (pk, Nk) on the training set and (p\u2032k, N \u2032 k) on the test set. For each N , we find the run k\u2217(N) = argmaxk:Nk\u2264N pk that achieves the best training pk using at most N base classifiers, and plot the test TPR p\u2032k\u2217(N) versus N . Although overfitting is not an issue here (the complexity of the classifiers is relatively low), this setup is important for a fair comparison. If overfitting were an issue, the optimization could also be carried out on a validation set, independent of both the test and the training sets. Optimizing the TPR on the training set and plotting it on the test set also explains why the curves are non-monotonic.\nFigure 2 shows the results we obtained. Although the differences are quite small, MDDAG outperforms the three benchmarks algorithms consistently in the regime of low number base classifiers, and it is competitive with them over the full range. MDDAG.I is slightly better at low complexities, whereas MDDAG.EXP is more effective at a slightly higher number of base classifiers. This is not surprising as in the low complexity regime the 0-1 error is more aggressive and closer to the measured TPR, whereas when most of the instances are classified correctly, without the margin information it is impossible to improve the policy any further."}, {"heading": "3.3. Ranking with multi-class DAGs", "text": "Although object detection is arguably the best-known test-time-constrained problem, it is far from being unique. In web page ranking, the problem is similar: training time can be almost unlimited, but the learned ranker must be fast to execute. State-of-theart techniques often use thousands of trained models in an ensemble setup (Chapelle et al., 2011b), so extracting lean rankers from the full models is an important practical issue. One of the difficulties in this case is that relevance labels may be non-binary, so classical object-detection cascades cannot be applied. At the same time, the principles used to design cascades re-surface also in this domain, although the setup is rather new and the algorithms require a fair amount\nof manual tuning (Cambazoglu et al., 2010). Despite this, MDDAG can be used for this task as is.\nTo evaluate MDDAG on a multi-class classification/ranking problem, we present results on the MQ2007 and MQ2008 data sets taken from LETOR 4.0. In web page ranking, observations come in the form of query-document pairs, and the performance of the ranker is evaluated using tailor-made loss or gain functions that take as input the ordering of all the documents given a query. To train a ranker, query-document pairs come with manually annotated relevance labels that are usually multi-valued ({0, 1, 2} in our case). One common performance measure is the Normalized Discounted Cumulative Gain (NDCGm) (Ja\u0308rvelin & Keka\u0308la\u0308inen, 2002) which is based on the first m documents in the order output by the ranker. We used the averaged NDCG score ndcg, provided by LETOR 4.0, that takes an average of the query-wise NDCGm values to evaluate the algorithms. In these experiments the base learners were decision trees with eight leaves.\nThe goal of MDDAG is similar to the binary case, namely to achieve a comparable performance to AdaBoost.MH using fewer base learners. To make the comparison fair, we employed the same calibration method to convert the output of the multi-class clas-\nsifiers to a scoring function and then to a ranking (Li et al., 2007). Since the goal this time was not detection, we simply evaluated the average NDCG for each run k to obtain (ndcgk, Nk) on the training set and (ndcg\u2032k, N \u2032 k) on the test set. We then selected k\u2217(N) = argmaxk:Nk\u2264N ndcgk and plotted the test ndcg\u2032k\u2217(N) against N . Figure 3 tells us that MDDAG performs as well as AdaBoost.MH with roughly twofold savings in the number of base classifiers."}, {"heading": "4. Related works", "text": "Besides (Po\u0301czos et al., 2009) and (Dulac-Arnold et al., 2011), MDDAG has several close relatives in the family of supervised methods. It is obviously related to algorithms taken from the vast array of sparse meth-\nods. The main advantage here is that the MDP setup allows one to achieve sparsity in a dynamical datadependent way. This feature relates the technique to unsupervised sparse coding (Lee et al., 2007; Ranzato et al., 2007) rather than to sparse classification or regression. On a more abstract level, MDDAG is also similar to (Larochelle & Hinton, 2010)\u2019s approach to \u201clearn where to look\u201d. Their goal is to find a sequence of two-dimensional features for classifying images in a data-dependent way, whereas we do a similar search in a one-dimensional ordered sequence of features."}, {"heading": "5. Conclusions", "text": "In this paper, we introduced an MDP-based design of decision DAGs. The output of the algorithm is a data-dependent sparse classifier, which means that every instance \u201cchooses\u201d the base classifiers or features that it needs to predict its class index. The algorithm is competitive with state-of-the-art cascade detectors on object detection benchmarks, and it is also directly applicable to test-time-constrained problems involving multi-class classification, such as web page ranking. However, in our view, the main benefit of the algorithm is not necessarily its performance, but its simplicity and versatility. First, MDDAG is basically a turn-key procedure: it comes with one userprovided hyperparameter with a clear semantics of directly determining the accuracy-speed trade-off. Second, MDDAG can be readily applied to problems different from classification by redefining the rewards on the Quit and Eval actions. For example, one can easily design regression or cost-sensitive classification DAGs by using an appropriate reward in (6), or add a weighting to (7) if the features have different evaluation costs."}, {"heading": "Acknowledgments", "text": "This work was supported by the ANR-2010-COSI-002 grant of the French National Research Agency."}], "references": [{"title": "MultiBoost: a multi-purpose boosting", "author": ["D. Benbouzid", "R. Busa-Fekete", "N. Casagrande", "Collin", "F.-D", "B. K\u00e9gl"], "venue": "package. JMLR,", "citeRegEx": "Benbouzid et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Benbouzid et al\\.", "year": 2012}, {"title": "Robust object detection via soft cascade", "author": ["L. Bourdev", "J. Brandt"], "venue": "In CVPR,", "citeRegEx": "Bourdev and Brandt,? \\Q2005\\E", "shortCiteRegEx": "Bourdev and Brandt", "year": 2005}, {"title": "Early exit optimizations for additive machine learned ranking systems", "author": ["Cambazoglu"], "venue": "In WSDM,", "citeRegEx": "Cambazoglu,? \\Q2010\\E", "shortCiteRegEx": "Cambazoglu", "year": 2010}, {"title": "Future directions in learning to rank", "author": ["O. Chapelle", "Y. Chang", "T.Y. Liu"], "venue": "In JMLR W&CP,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Yahoo! Learning-toRank Challenge overview", "author": ["Chapelle", "Olivier", "Chang", "Yi"], "venue": "In JMLR W&CP,", "citeRegEx": "Chapelle et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2011}, {"title": "Adaptive greedy approximations", "author": ["G. Davis", "S. Mallat", "M. Avellaneda"], "venue": "Constructive Approximation,", "citeRegEx": "Davis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Davis et al\\.", "year": 1997}, {"title": "Datum-wise classification: A sequential approach to sparsity", "author": ["G. Dulac-Arnold", "L. Denoyer", "P. Preux", "P. Gallinari"], "venue": "In ECML,", "citeRegEx": "Dulac.Arnold et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dulac.Arnold et al\\.", "year": 2011}, {"title": "An interior point heuristic for the Hamiltonian cycle problem via Markov Decision Processes", "author": ["V. Ejov", "J. Filar", "J. Gondzio"], "venue": "JGO, 29(3):315\u2013334,", "citeRegEx": "Ejov et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ejov et al\\.", "year": 2004}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "JCSS, 55:119\u2013139,", "citeRegEx": "Freund and Schapire,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire", "year": 1997}, {"title": "A single track HLT1 trigger", "author": ["V. Gligorov"], "venue": "Technical Report LHCb-PUB-2011-003, CERN,", "citeRegEx": "Gligorov,? \\Q2011\\E", "shortCiteRegEx": "Gligorov", "year": 2011}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM TIS,", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen", "year": 2002}, {"title": "Learning to combine foveal glimpses with a third-order Boltzmann machine", "author": ["H. Larochelle", "G. Hinton"], "venue": "In NIPS, pp", "citeRegEx": "Larochelle and Hinton,? \\Q2010\\E", "shortCiteRegEx": "Larochelle and Hinton", "year": 2010}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A.Y. Ng"], "venue": "In NIPS, pp", "citeRegEx": "Lee et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2007}, {"title": "McRank: Learning to rank using multiple classification and gradient boosting", "author": ["P. Li", "C. Burges", "Q. Wu"], "venue": "In NIPS,", "citeRegEx": "Li et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Li et al\\.", "year": 2007}, {"title": "An experimental study on pedestrian classification", "author": ["S. Munder", "D.M. Gavrila"], "venue": "IEEE PAMI,", "citeRegEx": "Munder and Gavrila,? \\Q2006\\E", "shortCiteRegEx": "Munder and Gavrila", "year": 2006}, {"title": "Learning when to stop thinking and do something", "author": ["B. P\u00f3czos", "Y. Abbasi-Yadkori", "Szepesv\u00e1ri", "Cs", "R. Greiner", "N. Sturtevant"], "venue": "In ICML, pp", "citeRegEx": "P\u00f3czos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "P\u00f3czos et al\\.", "year": 2009}, {"title": "Efficient learning of sparse representations with an energybased model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "On-line Q-learning using connectionist systems", "author": ["G.A. Rummery", "M. Niranjan"], "venue": "Technical Report CUED/FINFENG/TR 166,", "citeRegEx": "Rummery and Niranjan,? \\Q1994\\E", "shortCiteRegEx": "Rummery and Niranjan", "year": 1994}, {"title": "Boosting classifier cascades", "author": ["M. Saberian", "N. Vasconcelos"], "venue": "In NIPS, pp. 2047\u20132055,", "citeRegEx": "Saberian and Vasconcelos,? \\Q2010\\E", "shortCiteRegEx": "Saberian and Vasconcelos", "year": 2010}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer", "year": 1999}, {"title": "Reinforcement learning: an introduction. Adaptive computation and machine learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["Szepesv\u00e1ri", "Cs"], "venue": null, "citeRegEx": "Szepesv\u00e1ri and Cs.,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri and Cs.", "year": 2010}, {"title": "Robust real-time face detection", "author": ["P. Viola", "M. Jones"], "venue": "IJCV, 57:137\u2013154,", "citeRegEx": "Viola and Jones,? \\Q2004\\E", "shortCiteRegEx": "Viola and Jones", "year": 2004}], "referenceMentions": [{"referenceID": 9, "context": "high energy physics (Gligorov, 2011).", "startOffset": 20, "endOffset": 36}, {"referenceID": 15, "context": "Allowing skipping is also a crucial difference compared to the approach of (P\u00f3czos et al., 2009) who also proposed to learn a cascade in an MDP setup.", "startOffset": 75, "endOffset": 96}, {"referenceID": 6, "context": "Our method can also be related to the sequential classifier design of (Dulac-Arnold et al., 2011).", "startOffset": 70, "endOffset": 97}, {"referenceID": 5, "context": "If rI (6) is used as a reward, the loss is also non-convex, but minimizing a loss with an L0 constraint is NP-hard even if the loss is convex (Davis et al., 1997).", "startOffset": 142, "endOffset": 162}, {"referenceID": 7, "context": "So, what we are aiming at is an MDP-based heuristic to solve an NP-hard problem, something that is not without precedent (Ejov et al., 2004).", "startOffset": 121, "endOffset": 140}, {"referenceID": 0, "context": "org (Benbouzid et al., 2012).", "startOffset": 4, "endOffset": 28}, {"referenceID": 13, "context": "To make the comparison fair, we employed the same calibration method to convert the output of the multi-class classifiers to a scoring function and then to a ranking (Li et al., 2007).", "startOffset": 166, "endOffset": 183}, {"referenceID": 15, "context": "Besides (P\u00f3czos et al., 2009) and (Dulac-Arnold et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 6, "context": ", 2009) and (Dulac-Arnold et al., 2011), MDDAG has several close relatives in the family of supervised methods.", "startOffset": 12, "endOffset": 39}, {"referenceID": 12, "context": "This feature relates the technique to unsupervised sparse coding (Lee et al., 2007; Ranzato et al., 2007) rather than to sparse classification or regression.", "startOffset": 65, "endOffset": 105}, {"referenceID": 16, "context": "This feature relates the technique to unsupervised sparse coding (Lee et al., 2007; Ranzato et al., 2007) rather than to sparse classification or regression.", "startOffset": 65, "endOffset": 105}], "year": 2012, "abstractText": "In this paper we propose an algorithm that builds sparse decision DAGs (directed acyclic graphs) from a list of base classifiers provided by an external learning method such as AdaBoost. The basic idea is to cast the DAG design task as a Markov decision process. Each instance can decide to use or to skip each base classifier, based on the current state of the classifier being built. The result is a sparse decision DAG where the base classifiers are selected in a data-dependent way. The method has a single hyperparameter with a clear semantics of controlling the accuracy/speed trade-off. The algorithm is competitive with state-of-the-art cascade detectors on three object-detection benchmarks, and it clearly outperforms them when there is a small number of base classifiers. Unlike cascades, it is also readily applicable for multi-class classification. Using the multi-class setup, we show on a benchmark Web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker.", "creator": "LaTeX with hyperref package"}}}