{"id": "1206.6394", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Nonparametric Link Prediction in Dynamic Networks", "abstract": "we propose a non - parametric link prediction algorithm for a sequence of graph snapshots modified over time. the model predicts links based on the features of its endpoints, as well as those of plotting the local continuous neighborhood around the endpoints. this allows for different types of neighborhoods in every a graph, each with its own continuous dynamics ( e. g, continuous growing homes or shrinking communities ). we prove the consistency of our estimator, and give players a fast implementation based on strong locality - sensitive hashing. experiments with simulated as tall well as five real - world dynamic graphs show that soon we outperform the state of the art, especially when sharp fluctuations or relative non - linearities that are present.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (423kb)", "http://arxiv.org/abs/1206.6394v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG cs.SI stat.ML", "authors": ["purnamrita sarkar", "deepayan chakrabarti", "michael i jordan"], "accepted": true, "id": "1206.6394"}, "pdf": {"name": "1206.6394.pdf", "metadata": {"source": "CRF", "title": "Nonparametric Link Prediction in Dynamic Networks", "authors": ["Purnamrita Sarkar", "Deepayan Chakrabarti", "Michael I. Jordan"], "emails": ["psarkar@eecs.berkeley.edu", "deepay@fb.com", "jordan@eecs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "The problem of predicting links in a graph occurs in many settings\u2014recommending friends in social networks, predicting movies or songs to users, market analysis, and so on. However, state-of-the-art methods suffer from two weaknesses. First, most methods rely on heuristics such as counting common neighbors, etc. while these often work well in practice, their theoretical properties have not been thoroughly analyzed. (Sarkar et al. (2010) is one step in this direction). Second, most of the heuristics are meant for predicting links from one static snapshot of the graph. However, graph datasets often carry additional temporal information such as the creation and deletion times of nodes and edges, so the data is better viewed as a sequence of snapshots of an evolving graph or as a continuous time process (Vu et al., 2011). In this paper, we focus on link prediction in the sequential snapshot setting, and propose a nonparametric method that (a) makes weak model assumptions about the graph generation process, (b) leads to formal guarantees of consistency,\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nand (c) offers a fast and scalable implementation via locality sensitive hashing (LSH).\nOur approach falls under the framework of nonparametric time series prediction, which models the evolution of a sequence xt over time (Masry & Tj\u00f8stheim, 1995). Each xt is modeled as a function of a moving window (xt\u22121, . . . , xt\u2212p), and so xt is assumed to be independent of the rest of the time series given this window; the function itself is learned via kernel regression. In our case, however, there is a graph snapshot in each timestep. The obvious extension of modeling each graph as a multi-dimensional xt quickly runs into problems of high dimensionality, and is not scalable. Instead, we appeal to the following intuition: the graphs can be thought of as providing a \u201cspatial\u201d dimension that is orthogonal to the time axis. In the spirit of the time series model discussed above, our model makes the additional assumption that the linkage behavior of any node i is independent of the rest of the graph given its \u201clocal\u201d neighborhood or cluster N(i); in effect, local neighborhoods are to the spatial dimension what moving windows are to the time dimension. Thus, the out-edges of i at time t are modeled as a function of the local neighborhood of i over a moving window, resulting in a much more tractable problem. This model also allows for different types of neighborhoods to exist in the same graph, e.g., regions of slow versus fast change in links, assortative versus disassortative regions (where high-degree nodes are more/less likely to connect to other high-degree nodes), densifying versus sparsifying regions, and so on. An additional advantage of our nonparametric model is that it can easily incorporate node and link features which are not based on the graph topology (e.g., labels in labeled graphs).\nOur contributions are as follows: (1) Nonparametric problem formulation: We offer, to our knowledge, the first nonparametric model for link prediction in dynamic graphs. The model is powerful enough to accommodate regions with very different evolution profiles, which would be impossible for any single link prediction rule or heuristic. It also enables\nlearning based on both topological as well as other externally available features (such as labels).\n(2) Consistency of the estimator: Using arguments from the literature on Markov chains and strong mixing, we prove consistency of our estimator.\n(3) Fast implementation via LSH: Nonparametric methods such as kernel regression can be very slow when the kernel must be computed between a query and all points in the training set. We adapt the locality sensitive hashing algorithm of Indyk & Motwani (1998) for our particular kernel function, which allows the link prediction algorithm to scale to large graphs and long sequences.\n(4) Empirical improvements over previous methods: We show that on graphs with nonlinearities, such as seasonally fluctuating linkage patterns, we outperform all of the state-of-the-art heuristic measures for static and dynamic graphs. This result is confirmed on a real-world sensor network graph as well as via simulations. On other real-world datasets whose evolution is far smoother and simpler, we perform as well as the best competitor. Finally, on simulated datasets, our LSH-based kernel regression is shown to be much faster than the exact version while yielding almost identical accuracy. For larger real-world datasets, the exact kernel regression did not even finish in a day.\nThe rest of the paper is organized as follows. We present the model and prove consistency in Sections 2 and 3. We discuss our LSH implementation in Section 4. We give empirical results in Section 5, followed by related work and conclusions in Sections 6 and 7."}, {"heading": "2. Proposed Method", "text": "Consider the link prediction problem in static graphs. Simple heuristics like picking node pairs that were linked most recently (i.e., had small time to last-link), or that have the most common neighbors, have been shown empirically to be good indicators of future links between node pairs (Liben-Nowell & Kleinberg, 2003). An obvious extension to dynamic graphs is to compute the fraction of pairs that had lastlink = k at time t and formed an edge at time t + 1, aggregated over all timesteps t, and use the value of k with the highest fraction as the best predictor. This can easily be extended to multiple features. Thus, modulo fraction estimation errors, the dynamic link prediction problem reduces to the computation and analysis of multidimensional histograms, or datacubes.\nHowever, this simple solution suffers from two critical problems. First, it does not allow for local variations in the link-formation fractions. This can be addressed\nby computing a separate datacube for each local neighborhood (made more precise later). The second, more subtle, problem is that the above method implicitly assumes stationarity, i.e., a node\u2019s link-formation probabilities are time-invariant functions of the datacube features. This is clearly inaccurate: it does not allow for seasonal changes in linkage patterns, or for a transition from slow to fast evolution, etc. The solution is to use the datacubes not to directly predict future links, but as a signature of the recent evolution of the neighborhood. We can then find historical neighborhoods from some previous time t that had the same signature, and use their evolution from t to t+1 to predict link formation in the next timestep for the current neighborhood. Thus, seasonalities and other arbitrary patterns can be learned. Also, this combats sparsity by aggregating data across similarly-evolving communities even if they are separated by graph distance and time. Finally, note that the signature encodes the recent evolution of a neighborhood, and not just the distribution of features in it. Thus, it is evolution that drives the estimation of linkage probabilities.\nWe now formalize these ideas. Let the observed sequence of directed graphs be G = {G1, G2, . . . , Gt}. Let Yt(i, j) = 1 if the edge i \u2192 j exists at time t, and let Yt(i, j) = 0 otherwise. Let Nt(i) be the local neighborhood of node i in Gt; in our experiments, we define it to be the set of nodes within 2 hops of i, and all edges between them. Note that the neighborhoods of nearby nodes can overlap. Let ~Nt,p(i) = {Nt(i), . . . , Nt\u2212p+1(i)}. Then, our model is:\nYt+1(i, j)|G \u223c Bernoulli(g(\u03c8t(i, j))) \u03c8t(i, j) = {st (i, j) , dt (i)},\nwhere 0 \u2264 g(.) \u2264 1 is a function of two sets of features: those specific to the pair of nodes (i, j) under consideration (st (i, j)), and those for the local neighborhood of the endpoint i (dt (i)). We require that both of these be functions of ~Nt,p(i). Thus, Yt+1(i, j) is assumed to be independent of G given ~Nt,p(i), limiting the dimensionality of the problem. Also, two pairs of nodes (i, j) and (i\u2032, j\u2032) that are close to each other in terms of graph distance are likely to have overlapping neighborhoods, and hence higher chances of sharing neighborhood-specific features. Thus, link prediction probabilities for pairs of nodes from the same graph region are likely to be dependent, as expected.\nAssume that the pair-specific features st (i, j) come from a finite set S; if not, they are discretized into such a set. For example, one may use st (i, j) = {cnt(i, j), ``t(i, j)} (i.e., number of common neighbors and the last time a link appeared between nodes i and\nj). Let dt (i) = {\u03b7it (s) , \u03b7+it (s) \u2200s \u2208 S}, where \u03b7it (s) are the number of node pairs in Nt\u22121(i) with feature vector s, and \u03b7+it (s) the number of such pairs which were also linked by an edge in the next timestep t. In a nutshell, dt (i) tells us the chances of an edge being created in t given its features in t \u2212 1, averaged over the whole neighborhood Nt\u22121(i) \u2014 in other words, it captures the evolution of the neighborhood around i over one timestep.\nOne can think of dt (i) as a multi-dimensional histogram, or a \u201cdatacube\u201d, which is indexed by the features s. Hence, now onwards we will often refer to dt (i) as a \u201cdatacube\u201d, and a feature vector s as the \u201ccell\u201d s in the datacube with contents (\u03b7it (s) , \u03b7 + it (s)). Finiteness of S is necessary to ensure that datacubes are finite-dimensional, which allows us to index them and quickly find nearest-neighbor datacubes.\nEstimator. Our estimator of the function g(.) is:\ng\u0302(\u03c8t(i, j)) =\n\u2211 i\u2032,j\u2032,t\u2032\nSim(\u03c8t(i, j), \u03c8t\u2032(i \u2032, j\u2032)) \u00b7 Yt\u2032+1(i\u2032, j\u2032)\u2211\ni\u2032,j\u2032,t\u2032 Sim(\u03c8t(i, j), \u03c8t\u2032(i\u2032, j\u2032))\n.\nTo reduce dimensionality, we factor Sim(\u03c8t(i, j), \u03c8t\u2032(i\n\u2032, j\u2032)) into neighborhood-specific and pair-specific parts: K(dt (i) , dt\u2032 (i\n\u2032)) \u00b7 I{st (i, j) = st\u2032 (i\n\u2032, j\u2032)}. In other words, the similarity measure Sim(.) computes the similarity between the two neighborhood evolutions (i.e., the datacubes), but only for pairs (i\u2032, j\u2032) at time t\u2032 that had exactly the same features as the query pair (i, j) at t (i.e., pairs belonging to the cell s = st (i, j)). This yields a different interpretation of the estimator:\u2211 i\u2032,t\u2032 K (dt (i) , dt\u2032 (i \u2032)) \u00b7 \u2211 j\u2032\n[I{st (i, j) = st\u2032 (i\u2032, j\u2032)} \u00b7 Yt\u2032+1(i\u2032, j\u2032)]\u2211 i\u2032,t\u2032 K (dt (i) , dt\u2032 (i\u2032)) \u00b7 \u2211 j\u2032 I{st (i, j) = st\u2032 (i\u2032, j\u2032)}\n=\n\u2211 i\u2032,t\u2032\nK (dt (i) , dt\u2032 (i \u2032)) \u00b7 \u03b7+i\u2032t\u2032+1 (st (i, j))\u2211 i\u2032,t\u2032 K (dt (i) , dt\u2032 (i\u2032)) \u00b7 \u03b7i\u2032t\u2032+1 (st (i, j)) .\nIntuitively, given the query pair (i, j) at time t, we look only inside cells for the query feature s = st (i, j) in all neighborhood datacubes, compute the average \u03b7+i\u2032t\u2032 (s) and \u03b7i\u2032t\u2032 (s) in these cells after accounting for the similarities of the datacubes to the query neighborhood datacube, and use their quotient as the estimate of linkage probability. Thus, the probabilities are computed from historical instances where (a) the feature vector of the historical node pair matches the query, and (b) the local neighborhood was similar as well.\nNow, we need a measure of the closeness between neighborhoods. Two neighborhoods are close if they have similar probabilities p(s) of generating links between node pairs with feature vector s, for any s \u2208 S. We could simply compare point estimates p(s) =\n\u03b7+. (s) /\u03b7. (s), but this does not account for the variance in these estimates. Instead, we consider the full posterior of p(s) (a Beta distribution), and use the total variation distance between these Betas as a measure of the closeness:\nK(dt (i) , dt\u2032 ( i\u2032 ) ) = bD(dt(i),dt\u2032(i \u2032)) (0 < b < 1) (1)\nD(dt (i) , dt\u2032 ( i\u2032 ) ) = \u2211 s\u2208S TV(X,Y )\nX \u223c B ( \u03b7+it (s) , \u03b7it (s)\u2212 \u03b7 + it (s) ) Y \u223c B ( \u03b7+i\u2032t\u2032 (s) , \u03b7i\u2032t\u2032 (s)\u2212 \u03b7 + i\u2032t\u2032 (s) ) ,\nwhere TV(., .) denotes the total variation distance between the distributions of its two argument random variables, and b \u2208 (0, 1) is a bandwidth parameter.\nDealing with Sparsity. For sparse graphs, or short time series, two practical problems can arise. First, a node i could have zero degree and hence an empty neighborhood. In order to get around this, we define the neighborhood of node i as the union of 2-hop neighborhoods over the last p timesteps.\nSecond, the \u03b7. (s) and \u03b7 + . (s) values obtained from kernel regression could be too small, and so the estimated linkage probability \u03b7+. (s) /\u03b7. (s) is too unreliable for prediction and ranking. We offer a threefold solution. (a) We combine \u03b7. (s) and \u03b7 + . (s) with a weighted average of the corresponding values for any s\u2032 that are \u201cclose\u201d to s, the weights encoding the similarity between s\u2032 and s. This is in essence the same as replacing the indicator in Eq. (1) with a kernel that measures similarity between features. (b) Instead of ranking node pairs using \u03b7+. (s) /\u03b7. (s), we use the lower end of the 95% Wilson score interval (Wilson, 1927), which is a widely used binomial proportion confidence interval. The node pairs that are ranked highest according to this \u201cWilson score\u201d are those that have high estimated linkage probability \u03b7+. (s) /\u03b7. (s) and \u03b7. (s) is high (implying a reliable estimate). (c) Last but not the least, we maintain a \u201cprior\u201d datacube, which is average of all historical datacubes. The Wilson score of each node pair is smoothed with the corresponding score derived from the prior datacube, with the degree of smoothing depending on \u03b7. (s). This can be thought of as a simple hierarchical model, where the lower level (set of individual datacubes) smooths its estimates using the higher level (the prior datacube)."}, {"heading": "3. Consistency of Kernel Estimator", "text": "Now, we prove that the estimator g\u0302 defined in Eq. (1) is consistent. Recall that our model is as follows:\nYt+1(i, j)|G \u223c Ber(g(\u03c8t(i, j))), (2)\nwhere \u03c8t(i, j) equals {st (i, j) , dt (i)}. Assume that all graphs have n nodes (n is finite). Let Q represent the\nquery datacube dT (q). We want to obtain predictions for timestep T + 1. From Eq. (1), the kernel estimator of g for query pair (q, q\u2032) at time T + 1 can be written as:\ng\u0302(s,Q) = h\u0302(s,Q)\nf\u0302(s,Q) (where s = sT (q, q\n\u2032))\nh\u0302(s,Q) = 1\nn(T \u2212 p) T\u22121\u2211 t=p n\u2211 i=1 Kb(dt (i) , Q)\u03b7 + it+1 (s)\nf\u0302(s,Q) = 1\nn(T \u2212 p) T\u22121\u2211 t=p n\u2211 i=1 Kb(dt (i) , Q)\u03b7it+1 (s) .\nThe estimator g\u0302 is defined only when f\u0302 > 0. The kernel was defined earlier as Kb(dt (i) , Q) = b\nD(dt(i),Q), where the bandwidth b tends to 0 as T \u2192 \u221e, and D(.) is the distance function defined in Eq. (1). This is similar to other discrete kernels (Aitchison & Aitken, 1976), and has the following property\nlim b\u21920 Kb(dt (i) , Q) =\n{ 1 if dt (i) = Q\n0 otherwise. (3)\nTheorem 3.1 (Consistency). g\u0302 is a consistent estimator of g, i.e., g\u0302 P\u2212\u2192 g as T \u2192\u221e.\nProof. The proof is in two parts. Lemma 3.3 will show that var(h\u0302) and var(f\u0302) tend to 0 with T \u2192 \u221e. Lemma 3.4 shows that their expectations converge to g(s,Q)R and R respectively, for some constant R > 0. Hence, (h\u0302, f\u0302) P\u2212\u2192 (g(s,Q)R,R). By the continuous mapping theorem, g\u0302 = h\u0302/f\u0302 P\u2212\u2192 g.\nThe next lemma upper bounds the growth of variance terms. We first recall the concept of strong mixing. For a Markov chain St, define the strong mixing coefficients \u03b1(k) . = sup|t\u2212t\u2032|\u2265k{|P (A\u2229B)\u2212P (A)P (B)| : A \u2208 F\u2264t, B \u2208 F\u2265t\u2032}, where F\u2264t and F\u2265t\u2032 are the sigma algebras generated by events in \u22c3 i\u2264t S \u2032 i and \u22c3 i\u2265t\u2032 S \u2032 i respectively. Intuitively, small values of \u03b1(k) imply that states that are k apart in the Markov chain are almost independent. For bounded A and B, this also limits their covariance: |cov(A,B)| \u2264 c\u03b1(k) for some constant c (Durrett, 1995).\nLemma 3.2. Let qit be a bounded function of \u03b7it+1 (s),\u03b7 + it+1 (s) and dt (i). Then, (1/T 2)var [\u2211T\nt=1 \u2211n i=1 qit ] \u2192 0 as T \u2192\u221e.\nProof Sketch. Our graph evolution model is Markovian; assuming each \u201cstate\u201d to represent the past p+1 graphs, the next graph (and hence the next state) is a function only of the current state. The state space is also finite, since each graph has bounded size. Thus,\nthe state space may be partitioned as S = TR \u22c3 Ci, where TR is a set of transient states, each Ci is an irreducible closed communication class, and there exists at least one Ci (Grimmett & Stirzaker, 2001).\nThe Markov chain must eventually enter some Ci. First assume that this class is aperiodic. Irreducibility and aperiodicity implies geometric ergodicity (Fill, 1991), which implies strong mixing with exponential decay (Pham, 1986): \u03b1(k) \u223c e\u2212\u03b2k for some \u03b2 > 0. Thus,\n\u2211 t,t\u2032 cov(qit, qjt\u2032) \u2248\u2211\nt \u2211T |t\u2212t\u2032|=0 cov(qit, qjt\u2032) \u2264 \u2211 t \u2211\u221e k=0 c\u03b1(k) =\u2211\nt \u2211 k ce \u2212\u03b2k = O(T ). Thus, var( \u2211 t \u2211 i qit)/T\n2 = O(1/T ), which goes to zero as T \u2192\u221e. The proof for a cyclic communication class, while similar in principle, is more involved and is deferred to the appendix.\nLemma 3.3. var(h\u0302) and var(f\u0302) tend to 0 as T \u2192\u221e.\nProof. The result follows by applying Lemma 3.2 with q(.) equal to Kb(dt (i) , Q)\u03b7 + it+1 (s) and Kb(dt (i) , Q)\u03b7it+1 (s) respectively.\nLemma 3.4. As T \u2192\u221e, for some R > 0, E[h\u0302(s,Q)]\u2192 g(s,Q)R, E[f\u0302(s,Q)]\u2192 R.\nProof. Let denote the minimum distance between two datacubes that are not identical; since the set of all possible datacubes is finite, > 0. E[h\u0302(s,Q)] is an average of terms E[Kb(dt (i) , Q)\u03b7 + it+1 (s)], over i \u2208 {1, . . . , n} and t \u2208 {p, . . . , T \u2212 1}. Now, E[Kb(dt (i) , Q)\u03b7+it+1 (s)] = E [ bD(dt(i),Q)E [ \u03b7+it+1 (s) |dt (i) ]] , and the inner expectation is E [\u03b7it+1 (s) \u00b7 g(s, dt (i))], as can be seen by summing Eq. (2) over all pairs (i, j) in a neighborhood with identical st (i, j), and then taking expectations. Writing the expectation in terms of a sum over all possible datacubes, and noting that everything is bounded, gives the following:\nE [ bD(dt(i),Q)\u03b7it+1 (s) \u00b7 g(s, dt (i)) ] = E[\u03b7it+1 (s) |dt (i) = Q] \u00b7 g(s,Q)P (dt (i) = Q) +O(b ).\nRecalling that E[h\u0302(s,Q)] was an average of the above\nterms, E[h\u0302(s,Q)] equals the following. g(s,Q) \u2211 t,i E[\u03b7it+1 (s) |dt (i) = Q] \u00b7 P (dt (i) = Q)\nn(T \u2212 p) +O(b ).\nUsing the argument of Lemma 3.2, we will eventually hit a closed communication class. Also, the query datacube at T is a function of the state ST , which belongs to a closed irreducible set C with probability 1. Hence, using standard properties of finite state space Markov chains (in particular positive recurrence of states in\nC), we can show that the above average converges to a positive constant R times g(s,Q). An identical argu-\nment yields E[f\u0302(s,Q)] converges to R. The full proof can be found in the appendix."}, {"heading": "4. Fast search using LSH", "text": "A naive implementation of the nonparametric estimator in Eq. (1) searches over all n datacubes for each of the T timesteps for each prediction, which can be very slow for large graphs. In most practical situations, the top-r closest neighborhoods should suffice (in our case r = 20). Thus, we need a fast sublinear-time method to quickly find the top-r closest neighborhoods.\nWe achieve this via locality sensitive hashing (LSH) (Indyk & Motwani, 1998). The standard LSH operates on bit sequences, and maps sequences with small Hamming distance to the same hash bucket. However, we must hash datacubes, and use the total variation distance metric. Our solution is based on the fact that total variation distance between discrete distributions is half the L1 distance between the corresponding probability mass functions. If we could approximate the probability distributions in each cell with bit sequences, then the L1 distance would just be the Hamming distance between these sequences, and standard LSH could be used for our datacubes.\nConversion to bit sequence. The key idea is to approximate the linkage probability distribution by its histogram. We first discretize the range [0, 1] (since we deal with probabilities) into B1 buckets. For each bucket we compute the probability mass p falling inside it. This p is encoded using B2 bits by setting the first bpB2c bits to 1, and the others to 0. Thus, the entire distribution (i.e., one cell) is represented by B1B2 bits. The entire datacube can be stored in |S|B1B2 bits. However, in all our experiments, datacubes were very sparse with only M |S| cells ever being nonempty (usually, 10-50); thus, we use only MB1B2 bits in practice. The Hamming distance between two pairs of MB1B2 bit vectors yields the total variation distance between datacubes (modulo a constant factor).\nDistances via LSH. We create a hash function by just picking a uniformly random sample of k bits out of MB1B2. For each hash function, we create a hash table that stores all datacubes whose hashes are identical in these k bits. We use ` such hash functions. Given a query datacube, we hash it using each of these ` functions, and then create a candidate set of up to O(max(`, r)) of distinct datacubes that share any of these ` hashes. The total variation distance of these candidates to the query datacube is computed explicitly, yielding the closest matching historical datacubes.\nPicking k. The number of bits k is crucial in balancing accuracy versus query time: a large k sends all datacubes to their own hash bucket, so any query can find only a few matches, while a small k bunches many datacubes into the same bucket, forcing costly and unnecessary computations of the exact total variation distance. We do a binary search to find the k for which the average hash-bucket size over a query workload is just enough to provide the desired top-20 matches. Its accuracy is shown in Section 5.\nFinally, we underscore a few points. First, the entire bit representation of MB1B2 bits never needs to be created explicitly; only the hashes need to be computed, and this takes O(k`) time. Second, the main cost in the algorithm is in creating the hash table, which needs to be done once as a preprocessing step. Query processing is extremely fast and sublinear, since the candidate set is much smaller than the size of the training set. Finally, we have found the loss due to approximation to be minimal in all our experiments."}, {"heading": "5. Experiments", "text": "We first introduce several baseline algorithms, and the evaluation metric. We then show via simulations that our algorithm outperforms prior work in a variety of situations modeling nonlinearities in linkage patterns, such as seasonality in link formation. These findings are confirmed on several evolving real-world graphs: a sensor network, two co-authorship graphs, and a stock return correlation graph. Finally, we demonstrate the improvement in timing achieved via LSH over exact search, and the effect of LSH bit-size k on accuracy.\nBaselines and metrics. We compare our nonparametric link prediction algorithm (NonParam) to the following baselines which, though seemingly simple, are extremely hard to beat in practice (Liben-Nowell & Kleinberg, 2003; Tylenda et al., 2009):\nLL: ranks pairs using ascending order of last time of linkage (Tylenda et al., 2009).\nCN (last timestep): ranks pairs using descending order of the number of common neighbors (Liben-Nowell & Kleinberg, 2003).\nAA (last timestep): ranks pairs using descending order of the Adamic-Adar score (Adamic & Adar, 2003), a weighted variant of common neighbors which it has been shown to outperform (Liben-Nowell & Kleinberg, 2003).\nKatz (last timestep): extends CN to paths with length greater than two, but with longer paths getting exponentially smaller weights (Katz, 1953).\nCN-all, AA-all, Katz-all: CN, AA, and Katz computed on the union of all graphs until the last timestep.\nRecall that, for NonParam, we only predict on pairs which are in the neighborhood (generated by the union of 2-hop neighborhoods of last p timesteps) of each other. We deliberately used a simple feature set for NonParam, setting st (i, j) = {cnt(i, j), ``t(i, j)} (i.e., common neighbors and last-link) and not using any external \u201cmeta-data\u201d (e.g., stock sectors, university affiliations, etc.). All feature values are binned logarithmically in order to combat sparsity in the tails of the feature distributions. Mathematically, our feature `t(i, j) should be capped at p. However, since the common heuristic LL uses no such capping, for fairness, we used the uncapped \u2018last time a link appeared\u2019 as `t(i, j), for the pairs we predict on. The bandwidth b is picked by cross-validation.\nFor any graph sequence (G1, . . . , GT ), we test link prediction accuracy on GT for a subset S>0 of nodes with non-zero degree in GT . Each algorithm is provided training data until timestep T \u2212 1, and must output, for each node i \u2208 S>0, a ranked list of nodes in descending order of probability of linking with i in GT . For purposes of efficiency, we only require a ranking on the nodes that have ever been within two hops of i (call these the candidate pairs); all algorithms under consideration predict the absence of a link for nodes outside this subset. We compute the AUC score for predicted scores for all candidate pairs against their actual edges formed in GT ."}, {"heading": "5.1. Prediction Accuracy", "text": "We compare accuracy on (a) simulations, (b) a real-world sensor network with periodicities, and (c) broadly stationary real-world graphs.\nSimulations. One unique aspect of NonParam is its ability to predict even in the presence of sharp fluctuations. As an example, we focus on seasonal patterns, simulating a model of Hoff (personal communication) that posits an independently drawn \u201cfeature vector\u201d for each node. Time moves over a repeating sequence of seasons, with a different set of features being \u201cactive\u201d in each. Nodes with these features are more likely to be linked in that season, though noisy links also exist. The user features also change smoothly over time, to reflect changing user preferences.\nWe generated 100-node graphs over 20 timesteps using 3 seasons, and plotted AUC averaged over 10 random runs for several noise-to-signal ratios (Fig. 1). NonParam consistently outperforms all other baselines by a large margin. Clearly, seasonal graphs have nonlinear linkage patterns: the best predictor of links at time T are the links at times T \u2212 3, T \u2212 6, etc., and NonParam is able to automatically learn this pattern. However, CN, AA, Katz are biased towards predicting\nlinks between pairs which were linked (or had short paths connecting them) at the previous timestep T\u22121; this implicit smoothness assumption makes them suffer heavily. This is why they behave as bad as a random predictor (AUC 0.5).\nBaselines LL, CN-all, AA-all and Katz-all use information from the union of all graphs until time T \u2212 1. Since the off-seasonal noise edges are not sufficiently large to form communities, most of the new edges come from communities of nodes created in season. This is why CN-all, AA-all and Katz-all outperform their \u2018lasttimestep\u2019 counterparts. As for LL, since links are more likely to come from the last seasons, it performs well, although poorly compared to NonParam. Also note that the changing user features forces the community structures to change slowly over time; in our experiments, CN-all performs worse that it would when there was no change in the user features, since the communities stayed the same.\nTable 1 compares average AUC scores for graphs with and without seasonality, using the lowest noise setting from Fig. 1. As already mentioned, CN, AA, Katz perform very poorly on the seasonal graphs, because of their implicit assumption of smoothness. Their variants CN-all, AA-all and Katz-all on the other hand take into account all the community structures seen in the data until the last timestep, and hence are better. On the other hand, for Stationary, links formed in the last few timesteps of the training data are good predictors of future links, and so LL, CN, AA and Katz all perform extremely well. Interestingly, CN-all, AA-all and Katz-all are worse than their \u2018last time-step\u2019 variants owing to the slow movement of the user features. We note, however, that NonParam performs very well in all cases, the margin of improvement being most for\nthe seasonal networks.\nReal-world graphs. We first present results on a 24 node sensor network where each edge represents the successful transmission of a message 1. We look at up to 82 consecutive measurements. These networks exhibit clear periodicity; in particular, a different set of sensors turn on and communicate during different periods (as our earlier \u201cseasons\u201d). Fig. 5.1 shows our results for these four seasons averaged over several cycles. The maximum standard deviation, averaged over these seasons is .07. We do not show CN, AA and Katz which perform like a random predictor. NonParam again significantly outperforms the baselines, confirming the simulation results.\nAdditional experiments were performed on three evolving co-authorship graphs: the Physics \u201cHepTh\u201d community (n = 14, 737 nodes, e = 31, 189 total edges, and T = 8 timesteps), NIPS (n = 2, 865, e = 5, 247, T = 9), and authors of papers on Citeseer (n = 20, 912, e = 45, 672, T = 11) with \u201cmachine learning\u201d in their abstracts. Each timestep looks at 1 \u2212 2 years of papers (so that the median degree at any timestep is at least 1). We also considered an evolving stockcorrelation network: the nodes are a subset of stocks in the S&P500, and two stocks are linked if the correlation of their daily returns over a two month window exceeds 0.8 (n = 424, e = 41, 699, T = 49).\n1http://www.select.cs.cmu.edu/data\nTable 2 shows the average AUC for all the algorithms. In the co-authorship graphs most authors keep working with a similar set of co-authors, which hides seasonal variations, if any. On these graphs we perform as well or better than LL, which has been shown to be the best heuristic by Tylenda et al. (2009). On the other hand, S&P500 is a correlation graph, so it is not surprising that all the common-neighbor or Katz measures perform very well on them. In particular CN-all and AA-all have the best AUC scores. This is primarily because they count paths through edges that exist in different timesteps, which we do not.\nThus, for graphs lacking a clear seasonal trend, LL is the best baseline on co-authorship graphs but not on the correlation graphs, whereas Katz-all works better on correlation graphs, but poorly on co-authorship graphs. NonParam, however, is the best by a large margin in seasonal graphs, and is better or close to the winner in others."}, {"heading": "5.2. Usefulness of LSH", "text": "The query time per datacube using LSH is extremely small: 0.3s for Citeseer, 0.4s for NIPS, 0.6s for HepTh, and 2s for S&P500. Since exact search is intractable in our large-scale real world data, we demonstrate the speedup of LSH over exact search using simulated data. We also show that the hash bitsize k picked adaptively is the largest value that still gives excellent AUC scores. Since larger k translates to fewer entries per hash bucket and hence faster searches, our k yields the fastest runtime performance as well.\nExact search vs. LSH. In Fig. 3(a) we plot the time taken to do top-20 nearest neighbor search for a query datacube. We fix the number of nodes at 100, and increase the number of timesteps. As expected, the exact search time increases linearly with the total number of datacubes, whereas LSH searches in nearly constant time. Also, the AUC score of NonParam with LSH is within 0.4% of that of the exact algorithm on average, implying minimal loss of accuracy from LSH.\nNumber of Bits in Hashing. Fig. 3(b) shows the effectiveness of our adaptive scheme to select the num-\nber of hash bits (Section 4). For these experiments, we turned off the smoothing based on the prior datacube. As k increases, the accuracy goes down to 50%, as a result of the fact that NonParam fails to find any matches of the query datacube. Our adaptive scheme finds k \u223c 170, which yields the highest accuracy."}, {"heading": "6. Related Work", "text": "Existing work on link prediction in dynamic networks can be broadly divided into two categories: generative model based and graph structure based.\nGenerative models. These include extensions of Exponential Family Random Graph models (Hanneke & Xing, 2006) by using evolution statistics like edge stability, reciprocity, transitivity; extension of latent space models for static networks by allowing smooth transitions in latent space (Sarkar & Moore, 2005), and extensions of the mixed membership block model to allow a linear Gaussian trend in the model parameters (Fu et al., 2010). In other work, the structure of evolving networks is learned from node attributes changing over time (Kolar et al., 2010). Although these models are intuitive and rich, they generally a) make strong model assumptions, b) require computationally intractable posterior inference, and c) explicitly model linear trends in the network dynamics.\nModels based on structure. Huang & Lin (2009) proposed a linear autoregressive model for individual links, and also built hybrids using static graph similarity features. In Tylenda et al. (2009) the authors examined simple temporal extensions of existing static measures for link prediction in dynamic networks. In both of these works it was shown empirically that LL and its variants are often better or among the best heuristic measures for link prediction. Our nonparametric method has the advantage of presenting a formal model, with consistency guarantees, that also performs as well as LL."}, {"heading": "7. Conclusions", "text": "We proposed a nonparametric model for link prediction in dynamic graphs, and showed that it performs as well as the state of the art for several real-world graphs, and exhibits important advantages over them in the presence of nonlinearities such as seasonality patterns. NonParam also allows us to incorporate features external to graph topology into the link prediction algorithm, and its asymptotic convergence to the true link probability is guaranteed under our fairly general model assumptions. Together, these make NonParam a useful tool for link prediction in dynamic graphs."}], "references": [{"title": "Friends and neighbors on the web", "author": ["L. Adamic", "E. Adar"], "venue": "Social Networks,", "citeRegEx": "Adamic and Adar,? \\Q2003\\E", "shortCiteRegEx": "Adamic and Adar", "year": 2003}, {"title": "Multivariate binary discrimination by the kernel method", "author": ["J. Aitchison", "C.G.G. Aitken"], "venue": null, "citeRegEx": "Aitchison and Aitken,? \\Q1976\\E", "shortCiteRegEx": "Aitchison and Aitken", "year": 1976}, {"title": "Eigenvalue bounds on convergence to stationarity for nonreversible Markov chains, with an application to the exclusion process", "author": ["J. Fill"], "venue": "Ann. Appl. Prob.,", "citeRegEx": "Fill,? \\Q1991\\E", "shortCiteRegEx": "Fill", "year": 1991}, {"title": "A state-space mixed membership blockmodel for dynamic network tomography", "author": ["W. Fu", "E.P. Xing", "L. Song"], "venue": "Annals of Applied Statistics,", "citeRegEx": "Fu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fu et al\\.", "year": 2010}, {"title": "Discrete temporal models of social networks", "author": ["S. Hanneke", "E.P. Xing"], "venue": "Electron. J. Statist.,", "citeRegEx": "Hanneke and Xing,? \\Q2006\\E", "shortCiteRegEx": "Hanneke and Xing", "year": 2006}, {"title": "The time-series link prediction problem with applications in communication surveillance", "author": ["Z. Huang", "D.K.J. Lin"], "venue": "INFORMS J. on Computing,", "citeRegEx": "Huang and Lin,? \\Q2009\\E", "shortCiteRegEx": "Huang and Lin", "year": 2009}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In VLDB,", "citeRegEx": "Indyk and Motwani,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani", "year": 1998}, {"title": "A new status index derived from sociometric analysis", "author": ["L. Katz"], "venue": "In Psychometrika,", "citeRegEx": "Katz,? \\Q1953\\E", "shortCiteRegEx": "Katz", "year": 1953}, {"title": "Estimating time-varying networks", "author": ["M. Kolar", "L. Song", "A. Ahmed", "E. Xing"], "venue": "Annals of Appl. Stat.,", "citeRegEx": "Kolar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kolar et al\\.", "year": 2010}, {"title": "The link prediction problem for social networks", "author": ["D. Liben-Nowell", "J. Kleinberg"], "venue": "In CIKM,", "citeRegEx": "Liben.Nowell and Kleinberg,? \\Q2003\\E", "shortCiteRegEx": "Liben.Nowell and Kleinberg", "year": 2003}, {"title": "Nonparametric estimation and identification of nonlinear ARCH time series", "author": ["E. Masry", "D. Tj\u00f8stheim"], "venue": "Econometric Theory,", "citeRegEx": "Masry and Tj\u00f8stheim,? \\Q1995\\E", "shortCiteRegEx": "Masry and Tj\u00f8stheim", "year": 1995}, {"title": "The mixing property of bilinear and generalised random coefficient autoregressive models", "author": ["D. Pham"], "venue": "Stochastic Processes and their Applications,", "citeRegEx": "Pham,? \\Q1986\\E", "shortCiteRegEx": "Pham", "year": 1986}, {"title": "Dynamic social network analysis using latent space models", "author": ["P. Sarkar", "A. Moore"], "venue": "In NIPS", "citeRegEx": "Sarkar and Moore,? \\Q2005\\E", "shortCiteRegEx": "Sarkar and Moore", "year": 2005}, {"title": "Theoretical justification of popular link prediction heuristics", "author": ["P. Sarkar", "D. Chakrabarti", "A. Moore"], "venue": "In COLT,", "citeRegEx": "Sarkar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sarkar et al\\.", "year": 2010}, {"title": "Towards timeaware link prediction in evolving social networks", "author": ["T. Tylenda", "R. Angelova", "S. Bedathur"], "venue": "In SNAKDD. ACM,", "citeRegEx": "Tylenda et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tylenda et al\\.", "year": 2009}, {"title": "Continuous-time regression models for longitudinal networks", "author": ["D. Vu", "A. Asuncion", "D. Hunter", "P. Smyth"], "venue": "In NIPS,", "citeRegEx": "Vu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vu et al\\.", "year": 2011}, {"title": "Probable inference, the law of succession, and statistical inference", "author": ["E. Wilson"], "venue": "J. Am. Stat. Assoc.,", "citeRegEx": "Wilson,? \\Q1927\\E", "shortCiteRegEx": "Wilson", "year": 1927}], "referenceMentions": [{"referenceID": 15, "context": "However, graph datasets often carry additional temporal information such as the creation and deletion times of nodes and edges, so the data is better viewed as a sequence of snapshots of an evolving graph or as a continuous time process (Vu et al., 2011).", "startOffset": 237, "endOffset": 254}, {"referenceID": 13, "context": "(Sarkar et al. (2010) is one step in this direction).", "startOffset": 1, "endOffset": 22}, {"referenceID": 16, "context": "(s), we use the lower end of the 95% Wilson score interval (Wilson, 1927), which is a widely used binomial proportion confidence interval.", "startOffset": 59, "endOffset": 73}, {"referenceID": 2, "context": "Irreducibility and aperiodicity implies geometric ergodicity (Fill, 1991), which implies strong mixing with exponential decay (Pham, 1986): \u03b1(k) \u223c e\u2212\u03b2k for some \u03b2 > 0.", "startOffset": 61, "endOffset": 73}, {"referenceID": 11, "context": "Irreducibility and aperiodicity implies geometric ergodicity (Fill, 1991), which implies strong mixing with exponential decay (Pham, 1986): \u03b1(k) \u223c e\u2212\u03b2k for some \u03b2 > 0.", "startOffset": 126, "endOffset": 138}, {"referenceID": 14, "context": "We compare our nonparametric link prediction algorithm (NonParam) to the following baselines which, though seemingly simple, are extremely hard to beat in practice (Liben-Nowell & Kleinberg, 2003; Tylenda et al., 2009):", "startOffset": 164, "endOffset": 218}, {"referenceID": 14, "context": "LL: ranks pairs using ascending order of last time of linkage (Tylenda et al., 2009).", "startOffset": 62, "endOffset": 84}, {"referenceID": 7, "context": "Katz (last timestep): extends CN to paths with length greater than two, but with longer paths getting exponentially smaller weights (Katz, 1953).", "startOffset": 132, "endOffset": 144}, {"referenceID": 13, "context": "On these graphs we perform as well or better than LL, which has been shown to be the best heuristic by Tylenda et al. (2009). On the other hand, S&P500 is a correlation graph, so it is not surprising that all the common-neighbor or Katz measures perform very well on them.", "startOffset": 103, "endOffset": 125}, {"referenceID": 3, "context": "These include extensions of Exponential Family Random Graph models (Hanneke & Xing, 2006) by using evolution statistics like edge stability, reciprocity, transitivity; extension of latent space models for static networks by allowing smooth transitions in latent space (Sarkar & Moore, 2005), and extensions of the mixed membership block model to allow a linear Gaussian trend in the model parameters (Fu et al., 2010).", "startOffset": 400, "endOffset": 417}, {"referenceID": 8, "context": "In other work, the structure of evolving networks is learned from node attributes changing over time (Kolar et al., 2010).", "startOffset": 101, "endOffset": 121}, {"referenceID": 14, "context": "In Tylenda et al. (2009) the authors examined simple temporal extensions of existing static measures for link prediction in dynamic networks.", "startOffset": 3, "endOffset": 25}], "year": 2012, "abstractText": "We propose a nonparametric link prediction algorithm for a sequence of graph snapshots over time. The model predicts links based on the features of its endpoints, as well as those of the local neighborhood around the endpoints. This allows for different types of neighborhoods in a graph, each with its own dynamics (e.g, growing or shrinking communities). We prove the consistency of our estimator, and give a fast implementation based on locality-sensitive hashing. Experiments with simulated as well as five real-world dynamic graphs show that we outperform the state of the art, especially when sharp fluctuations or nonlinearities are present.", "creator": "TeX"}}}