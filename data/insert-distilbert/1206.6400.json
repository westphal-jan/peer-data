{"id": "1206.6400", "review": {"conference": "ICML", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret", "abstract": "online semantic learning algorithms are designed to learn even when their input is generated incomplete by an adversary. roughly the widely - followed accepted formal definition of an online algorithm's ability to internally learn is the current game - theoretic notion of regret. we argue that the standard definition technique of system regret becomes inadequate if the adversary is allowed to adapt to the online algorithm's actions. we define the alternative notion of policy regret, which rather attempts individually to provide managers a more meaningful way to measure an online algorithm's performance against inherently adaptive adversaries. focusing substantially on the online sabre bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. on the other hand, if the adversary's memory is bounded, we actually present a general technique that converts any bandit algorithm with a static sublinear regret bound into an updated algorithm with a sublinear policy regret bound. we extend hence this modified result to other variants of regret, such as switching regret, internal regret, and swap regret.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (296kb)", "http://arxiv.org/abs/1206.6400v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.LG stat.ML", "authors": ["ofer dekel", "ambuj tewari", "raman arora"], "accepted": true, "id": "1206.6400"}, "pdf": {"name": "1206.6400.pdf", "metadata": {"source": "CRF", "title": "Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret", "authors": ["Raman Arora", "Ofer Dekel", "Ambuj Tewari"], "emails": ["arora@ttic.edu", "oferd@microsoft.com", "ambuj@cs.utexas.edu"], "sections": [{"heading": "1. Introduction", "text": "Online learning with bandit feedback is commonly described as a repeated game between a player and an adversary. On each round of the game, the player chooses an action Xt from an action set X , the ad-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nversary chooses a loss function ft, and the player suffers a loss of ft(Xt). We often assume that ft(Xt) is bounded in [0, 1]. The player observes the loss value ft(Xt) and uses it to update its strategy for subsequent rounds. Unlike the full-information player, the bandit player does not observe the entire loss function ft. The player\u2019s goal is to accumulate the smallest possible loss over T rounds of play.\nWhile this presentation is intuitively appealing, it hides the details on what information the adversary may use when choosing ft. Since this aspect of the problem is the main focus of our paper, we opt for a less common, yet entirely equivalent, definition of the online bandit problem.\nWe think of online prediction with bandit feedback as an iterative process where only the player makes active choices on each round. The adversary, on the other hand, prepares his entire sequence of loss functions in advance. To ensure that this assumption does not weaken the adversary, we make the additional assumption that the loss function ft takes, as input, the player\u2019s entire sequence of past and current actions (X1, . . . , Xt), which we abbreviate by X1,...,t. More formally, for each t, Ft is a class of loss functions from X t to the unit interval [0, 1], and the adversary chooses each ft from the respective class Ft.\nWe model the player as a randomized algorithm that defines a distribution over X on each round and samples Xt from this distribution. Therefore, even though ft is a deterministic function fixed in advance, the loss ft(X1,...,t) is a bounded random variable. The player observes the value of this random variable and nothing else, and uses this value to define a new distribution over the action space.\nInteresting special cases of online learning with bandit feedback are the k-armed bandit (Robbins, 1952; Auer et al., 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009). In the k-armed bandit problem, the action space X is the discrete set {1, . . . , k} and each Ft contains all functions from X t to [0, 1]. In bandit convex optimization, X is a predefined convex set and each Ft is the set of functions that are convex in their last argument. A special case of bandit convex optimization is bandit linear optimization (Awerbuch & Kleinberg, 2004; Bartlett et al., 2008), where the functions in Ft are linear in their last argument. In bandit submodular minimization, X is the power-set of {1, . . . , k} and each Ft contains all of the functions that are submodular in their last argument.\nVarious different adversary types have been proposed in the literature (Borodin & El-Yaniv, 1998; CesaBianchi & Lugosi, 2006). All adversary types are strategic and possibly malicious, have unlimited computational power, and are free to use random bits when choosing their loss functions. If the adversary is not restricted beyond the setting described above, he is called an adaptive adversary. Other adversary types are restricted in various ways. For example, an oblivious adversary is restricted to choose a sequence of loss functions such that each ft is oblivious to the first t\u22121 arguments in its input. In other words, ft can only be a function of the current action. Formally,\nft(x1, . . . , xt) = ft(x \u2032 1, . . . , x \u2032 t\u22121, xt) ,\nfor all x1, . . . , xt and x \u2032 1, . . . , x \u2032 t\u22121 in X .\nThe expected cumulative loss suffered by the player after T rounds (which we abbreviate simply as loss) is\nE [\u2211T t=1 ft(X1,...,t) ] . To evaluate how good this loss is, we compare it to a baseline. To this end, we choose a competitor class CT , which is simply a set of deterministic action sequences of length T . Intuitively, we would like to compare the player\u2019s loss with the cumulative loss of the best action sequence in CT . In practice, the most common way to evaluate the player\u2019s performance is to measure his external pseudo-regret compared to CT (Auer et al., 2002) (which we abbreviate as regret), defined as\nmax (y1,...,yT )\u2208CT\nE [ T\u2211 t=1 ft(X1,...,t)\u2212 ft(X1,...,t\u22121, yt) ] . (1)\nMost of the theoretical work on online learning uses this definition, both in the bandit setting (e.g., (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg,\n2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.g., (Zinkevich, 2003; CesaBianchi & Lugosi, 2006; Hazan et al., 2006; Blum & Mansour, 2007; Hazan & Kale, 2009)).\nIf the adversary is oblivious, regret has a simple and intuitive meaning. In this special case, we can slightly overload our notation and rewrite ft(x1, . . . , xt) as ft(xt). With this simplified notation, the regret defined in Eq. (1) becomes\nE [ T\u2211 t=1 ft(Xt) ] \u2212 min (y1,...,yT )\u2208CT T\u2211 t=1 ft(yt) .\nThe above is the difference between the player\u2019s loss and the loss of the best sequence in the competitor class CT . Intuitively, this difference measures how much the player regrets choosing his action-sequence over the best sequence in CT .\nHowever, if the adversary is adaptive, this simple intuition no longer applies, and the standard notion of regret losses much of its meaning. To observe the problem, note that if the player would have chosen a sequence from the competitor class, say (y1, . . . , yT ), then his loss would have been\u2211T t=1 ft(y1, . . . , yt). However, the definition of regret in Eq. (1) instead compares the player\u2019s loss to the\nterm E [\u2211T t=1 ft(X1,...,t\u22121, yt) ] . We can attempt to articulate the meaning of this term: it is the loss in the peculiar situation where the adversary reacts to the player\u2019s original sequence (X1, . . . , XT ), but the player somehow manages to secretly play the sequence (yi, . . . , yT ). This is not a feasible situation and it is unclear why this quantity is an interesting baseline for comparison.\nAs designers of online learning algorithms, we actually have two different ways to obtain a small regret: we can either design an algorithm that attempts to minimize its loss \u2211T t=1 ft(X1,...,t), or we can cheat\nby designing an algorithm that attempts to maximize\u2211T t=1 ft(X1,...,t\u22121, yt). For example, consider an algorithm that identifies an action to which the adversary always responds (on the next round) with a loss function that constantly equals 1 (here we use our assumption that the adversary may play any strategy, not necessarily the most malicious one). Repeatedly playing that action would cause regret to asymptote to a constant (the best possible outcome), since the player\u2019s loss would grow at an identical rate to the loss of all of its competitors. While this algorithm minimizes regret, it certainly isn\u2019t learning how to choose good actions. It is merely learning how to make its competitors look bad.\nThe problem described above seems to be largely overlooked in the online learning literature, with the exception of two important yet isolated papers (Merhav et al., 2002; de Farias & Megiddo, 2006). To overcome this problem, we define the policy regret of the player as the difference between his loss after T rounds and the loss that he would have suffered had he played the best sequence from a competitor class. Policy regret captures the idea that the adversary may react differently to different action sequences. We focus on the bandit setting and start with the class of constantaction competitors. We first prove a negative result: no online bandit algorithm can guarantee a sublinear policy regret. However, if the adversary has a bounded memory, we show how a simple mini-batching technique converts an online bandit algorithm with a regret bound of O(T q) into an algorithm with a policy regret bound of O(T 1/(2\u2212q)). We use this technique to derive a policy-regret bound of O(T 2/3) for the karmed bandit problem, O(T 4/5) for bandit convex optimization, O(T 3/4) for bandit linear optimization (or O(T 2/3) if the player knows the adversary\u2019s memory size), and O(T 3/4) for bandit submodular optimization. We then extend our technique to other notions of regret, namely, switching regret, internal regret, and swap regret."}, {"heading": "1.1. Related Work", "text": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T 2/3) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al. (2002) present one concrete algorithm with a policy regret bound, we show a general technique that endows any existing bandit algorithm with a policy regret bound. Despite the wider scope of our result, our proofs are simpler and shorter than those in Merhav et al. (2002) and our bound is just as good. Our extensions to switching regret, internal regret, and swap regret are also entirely novel.\nThe work of de Farias & Megiddo (2006) is even more closely related to ours as it presents a family of algorithms that deal with adaptive adversaries in the\nbandit setting. However, it is difficult to compare the results in de Farias & Megiddo (2006) with our results. While we stick with the widely accepted notion of online regret, de Farias & Megiddo (2006) forsake the notion of regret and instead analyze their algorithms using a non-standard formalization. Moreover, their analysis makes the assumption that the true value of any constant action can be estimated by repeating that action for a sufficiently large number of rounds, at any point in the game.\nThe reinforcement learning (RL) literature is also related to our work, at least in spirit. Specifically, the PAC-MDP framework (Szepesva\u0301ri, 2010, section 2.4.2) models the player\u2019s state on each round; typically, there is a finite number S of states and the player\u2019s actions both incur a loss and cause him to transition from one state to another. The PAC-MDP bounds typically hold when the comparison is with all kS policies (mappings from states to actions), not just the k constant-action policies. Our work is still substantially different from RL. The state transitions in RL are often assumed to be stochastic, whereas our setting is adversarial. An adversarial variant of the MDP setting was studied in Even-Dar et al. (2009), however, it assumes that all loss functions across all states are observed by the player. There are recent extensions (Yu et al., 2009; Neu et al., 2010) to the partial feedback or bandit setting but they either give asymptotic rates or make even more stringent assumptions on the underlying state transition dynamics. Also, the dependence on the number of states, S, tends to be of the form \u0398(S\u03b1) for some \u03b1 > 0. In the case of an m-memory bounded adaptive adversary, associating a state with each possible m-length history results in an exponential number of states.\nIn other related work, Ryabko & Hutter (2008) address the question of learnability in a general adaptive stochastic environment. They prove that environments that allow a rapid recovery from mistakes are asymptotically learnable. At a high level, the assumption that the adaptive adversary is memory bounded serves the same purpose. Our results differ from theirs in various ways: we consider adversarial environments rather than stochastic ones, we present a concrete tractable algorithm whereas their algorithm is intractable, and we prove finite-horizon convergence rates while their analysis is asymptotic.\nMore recently, Maillard & Munos (2010) considered adaptive adversaries in the k-armed bandit setting. They define a framework where the set of all actionhistories is partitioned into equivalence classes. For example, assuming that the adversary is m-memory-\nbounded is the same as assuming that two actionhistories with a common suffix of length m are equivalent. Within this framework, they study adversaries whose losses are functions of the equivalence classes and competitors whose actions are functions of the equivalence classes. However, they still use the standard notion of regret and do not address its intuitive problems. As mentioned above, this makes their regret bounds difficult to interpret. Moreover, when faced with an m-memory-bounded adversary, their bounds and running time both grow exponentially with m."}, {"heading": "2. Policy regret", "text": "Define the player\u2019s policy regret compared to a competitor class CT as\nE [ T\u2211 t=1 ft(X1,...,t) ] \u2212 min (y1,...,yT )\u2208CT T\u2211 t=1 ft(y1, . . . , yt) .\nThis coincides with Eq. (1) for oblivious adversaries.\nFirst, we show a negative result. Let CT be the set of constant action sequences, namely, sequences of the form (y, . . . , y) for y \u2208 X . We prove that it is impossible to obtain a non-trivial (sublinear) upper-bound on policy regret that holds for all adaptive adversaries.\nTheorem 1. For any player there exists an adaptive adversary such that the player\u2019s policy regret compared to the best constant action sequence is \u2126(T ).\nProof. Let y \u2208 X and p \u2208 (0, 1] be such that Pr(X1 = y) = p. Define an adaptive adversary that chooses the loss functions f1(x1) = 0 and\n\u2200 t \u2265 2 ft(x1, . . . , xt) =\n{ 1 if x1 = y\n0 if x1 6= y .\nAll of the loss functions defined above are constant functions of the current action. From round two and on, the value of the loss function depends entirely on whether the player\u2019s first action was y or not. The player\u2019s expected cumulative loss against this adversary equals pT , since the probability that X1 = y equals p. On the other hand, if the player were to play any constant sequence other than (y, . . . , y), it would accumulate a loss of zero. Therefore, the player\u2019s policy regret is at least pT . For comparison, note that the player\u2019s (standard) regret is zero.\nOther adversarial strategies can cause specific algorithms to suffer a linear regret. For example, the popular EXP3 algorithm (Auer et al., 2002) for the k-armed bandit problem maintains a distribution (p1,t, . . . , pk,t)\nover the k arms on each round. This distribution is a deterministic function of the algorithm\u2019s past observations. If the adversary mimics EXP3\u2019s computations and sets the loss to be ft(j) = pj,t we can prove that this distribution converges to the uniform distribution and EXP3 suffers a linear loss. In contrast, playing any constant arm against this adversary results in a sublinear loss, which implies a linear policy regret.\nGiven that no algorithm can guarantee a small policy regret against all adaptive adversaries, we must restrict the set of possible adversaries. We consider an adversary that lies between oblivious and adaptive: An m-memory-bounded adaptive adversary is an adversary that is constrained to choose loss functions that depend only on the m + 1 most recent actions. Formally,\nft(x1, . . . , xt) = ft(x \u2032 1, . . . , x \u2032 t\u2212m\u22121, xt\u2212m, . . . , xt) ,\nfor all x1, . . . , xt and x \u2032 1, . . . , x \u2032 t\u2212m\u22121 in X . An oblivious adversary is 0-memory-bounded, while a general adaptive adversary is \u221e-memory-bounded. We note that m-memory-bounded adversaries arrise in many natural scenarios. For example, the friction cost associated with switching from one action to another can be modeled using a 1-memory-bounded adversary.\nFor m-memory-bounded adaptive adversaries we prove a positive result, in the form of a reduction. Again, let the competitor class CT be the set of all constant action sequences of length T . We show how an algorithm A with a sublinear (standard) regret bound against an adaptive adversary can be transformed into another algorithm with a (slightly-inferior) policy regret bound against an m-memory-bounded adaptive adversary. We note that this new algorithm does not need to know m, but m does appear as a constant in our analysis.\nWe define a new algorithm by wrapping A with a minibatching loop (e.g., Dekel et al. (2011)). We specify a batch size \u03c4 and name the new algorithm A\u03c4 . The algorithm A\u03c4 groups the online rounds 1, . . . , T into consecutive and disjoint mini-batches of size \u03c4 : The j\u2019th mini-batch begins on round (j \u2212 1)\u03c4 + 1 and ends on round j\u03c4 . At the beginning of mini-batch j, A\u03c4 invokes A and receives an action Zj drawn from A\u2019s internal distribution over the action space. Then, A\u03c4 plays this action for \u03c4 rounds, namely, X(j\u22121)\u03c4+1 = . . . = Xj\u03c4 = Zj . During the mini-batch, A does not observe any feedback, does not update its internal state, and is generally unaware that \u03c4 rounds are going by. At the end of the mini-batch, A\u03c4 feeds A with a single loss value, the average loss suffered during the mini-batch, 1\u03c4 \u2211j\u03c4 t=(j\u22121)\u03c4+1 ft(X1,...,t).\nFrom A\u2019s point of view, every mini-batch feels like a single round: it chooses a single action Zj , receives a single loss value as feedback, and updates its internal state once. Put more formally, A is performing standard online learning with bandit feedback against the loss sequence f\u03021, . . . , f\u0302J , where J = bT/\u03c4c,\nf\u0302j(z1, . . . , zj) = 1\n\u03c4 \u03c4\u2211 k=1 f(j\u22121)\u03c4+k(z \u03c4 1 , ..., z \u03c4 j\u22121, z k j ), (2)\nand zi denotes i repetitions of the action z. By assumption, A\u2019s regret against f\u03021, . . . , f\u0302J is upper bounded by a sublinear function of J . The following theorem transforms this bound into a bound on the policy regret of A\u03c4 . Theorem 2. Let A be an algorithm whose (standard) regret, compared to constant actions, against any sequence of J loss functions generated by an adaptive adversary, is upper bounded by a monotonic function R(J). Let \u03c4 > 0 be a mini-batch size and let A\u03c4 be the mini-batched version of A. Let (ft)Tt=1 be a sequence of loss functions generated by an m-memory-bounded adaptive adversary, let X1, . . . , XT be the sequence of actions played by A\u03c4 against this sequence, and let y be any action in X . If \u03c4 > m, the policy regret of A\u03c4 , compared to the constant action y, is bounded by\nE [ T\u2211 t=1 ft(X1,...,t)\u2212 ft(yt) ] \u2264 \u03c4R ( T \u03c4 ) + Tm \u03c4 + \u03c4.\nSpecifically, if R(J) = CJq + o(Jq) for some C > 0 and q \u2208 (0, 1), and \u03c4 is set to C \u22121 2\u2212q T 1\u2212q 2\u2212q , then\nE [ T\u2211 t=1 ft(X1,...,t)\u2212 ft(yt) ] \u2264 C \u2032T 1 2\u2212q + o(T 1 2\u2212q ) ,\nwhere C \u2032 = (m+ 1)C 1 2\u2212q .\nProof. Assume that \u03c4 > m, otherwise the theorem makes no claim. Let J = bT/\u03c4c and let ( f\u0302j )J j=1 be the sequence of loss functions defined in Eq. (2). Let Z1, . . . , ZJ+1 be the sequence of actions played by A against the loss sequence ( f\u0302j )J j=1 . Our assumption on A implies that\nE  J\u2211 j=1 f\u0302j(Z1,...,j)\u2212f\u0302j(Z1,...,j -1, y)  \u2264 R(J). (3) From the definitions of A\u03c4 and f\u0302j ,\nJ\u2211 j=1 f\u0302j(Z1,...,j) = 1 \u03c4 J\u03c4\u2211 t=1 ft(X1,...,t) . (4)\nIntroducing the notation tj = (j \u2212 1)\u03c4 , we rewrite\nJ\u2211 j=1 f\u0302j(Z1,...,j -1, y) = 1 \u03c4 J\u2211 j=1 \u03c4\u2211 k=1 ftj+k(X1,...,tj ,y k). (5)\nFor any j \u2264 J , the bound on the loss implies m\u2211 k=1 ( ftj+k(X1,...,tj ,y k)\u2212 ftj+k(ytj+k) ) \u2264 m , (6)\nand our assumption that the adversary is m-memorybounded implies\n\u03c4\u2211 k=m+1 ftj+k(X1,...,tj ,y k) = \u03c4\u2211 k=m+1 ftj+k(y tj+k). (7)\nCombining Eqs.(5-7) gives the bound\nJ\u2211 j=1 f\u0302j(Z1,...,j\u22121, y) \u2264 1 \u03c4 ( J\u03c4\u2211 t=1 ft(y t) + Jm ) .\nTogether with Eq. (3) and Eq. (4), we have\nE [ J\u03c4\u2211 t=1 ft(X1,...,t)\u2212 ft(yt) ] \u2264 \u03c4R(J) + Jm.\nWe can bound the regret on rounds J\u03c4 + 1, . . . , T by \u03c4 . Plugging in J \u2264 T/\u03c4 gives an overall policy regret bound of \u03c4R(T/\u03c4)+Tm/\u03c4+\u03c4 . Focusing on the special case where R(J) = CJq + o(Jq), the bound becomes\nCT q\u03c41\u2212q + Tm\u03c4\u22121 + \u03c4 + o ( T q\u03c41\u2212q ) .\nPlugging in \u03c4 = C \u22121 2\u2212q T 1\u2212q 2\u2212q concludes the proof."}, {"heading": "3. Applying the Result", "text": "With Thm. 2 in hand, we prove that the policy regret of existing online bandit algorithms grows sublinearly with T . We begin with the EXP3 algorithm (Auer et al., 2002) in the classic k-armed bandit setting, with its regret bound of \u221a 7Jk log k against any sequence of J loss functions generated by an adaptive adversary. Applying Thm. 2 with C = \u221a 7k log k and q = 1/2 proves the following result.\nCorollary 1 (k-armed bandit). Let X = {1, . . . , k} and let Ft consist of all functions from X t to [0, 1]. The policy regret of the mini-batched version of the EXP3 algorithm (Auer et al., 2002), with batch size \u03c4 = (7k log k)\u22121/3T 1/3, against an m-memory bounded adaptive adversary, is upper bounded by\n(m+ 1)(7k log k)1/3T 2/3 + o(T 2/3).\nWe move on to the bandit convex optimization problem. The algorithm and analysis in Flaxman et al. (2005) guarantees a regret bound of 18d( \u221a LD+1)J3/4 against any sequence of J loss functions generated by an adaptive adversary, where d is the dimension, D is the diameter of X , and L is the Lipschitz coefficient of the loss functions. Applying Thm. 2 with C = 18d( \u221a LD + 1) and q = 3/4 proves the following result.\nCorollary 2 (Bandit convex optimization). Let X \u2282 Rd be a closed bounded convex set with diameter D and let Ft be the class of functions from X t to [0, 1] that are convex and L-Lipschitz in their last argument. The policy regret of the mini-batched version of Flaxman, Kalai, and McMahan\u2019s algorithm (Flaxman et al., 2005), with batch size \u03c4 = (18d( \u221a LD + 1))\u22124/5T 1/5, against an m-memory bounded adaptive adversary is upper bounded by\n(m+ 1) ( 18d( \u221a LD + 1)T )4/5 + o(T 4/5).\nAn important special case of bandit convex optimization is bandit linear optimization. The analysis in Dani & Hayes (2006) proves a regret bound of 15dJ2/3 for the algorithm of McMahan & Blum (2004) against any sequence of J loss functions generated by an adaptive adversary. Applying Thm. 2 with C = 15d and q = 2/3 proves the following result.\nCorollary 3 (Bandit linear optimization). Let X \u2282 [\u22122, 2]d be a polytope (or, more generally, let X \u2286 Rd be a convex set over which linear optimization can be done efficiently). Let Ft be the class of functions from X t to [0, 1] that are linear in their last argument. The policy regret of the mini-batched version of McMahan and Blum\u2019s algorithm (McMahan & Blum, 2004), with batch size \u03c4 = (15d)\u22123/4T 1/4, against an m-memory bounded adaptive adversary is upper bounded by\n(m+ 1)(15dT )3/4 + o(T 3/4).\nFinally, we apply our result to bandit submodular minimization over a ground set {1, . . . , k}. Recall that a set function f is submodular if for any two subsets of the ground set A,B it holds that f(A\u222aB)+f(A\u2229B) \u2264 f(A) + f(B). The algorithm in Hazan & Kale (2009) has a regret bound of 12kJ2/3 against any sequence of J loss functions generated by an adaptive adversary. Applying Thm. 2 with C = 12k and q = 2/3 proves the following result.\nCorollary 4 (Bandit submodular minimization). Let X be the power set of {1, . . . , k} and let Ft be the class of functions from X t to [0, 1] that are submodular in their last argument. The policy regret of the minibatched version of Hazan and Kale\u2019s algorithm (Hazan\n& Kale, 2009), with batch size \u03c4 = (12k)\u22123/4T 1/4, against an m-memory bounded adaptive adversary is\n(m+ 1)(12kT )3/4 + o(T 3/4)."}, {"heading": "4. Extensions", "text": "Theorem 2 is presented in its simplest form, and we can extend it in various interesting ways."}, {"heading": "4.1. Relaxing the Adaptive Assumption", "text": "Recall that we assumed that A has a (standard) regret bound that holds for any loss sequence generated by an adaptive adversary. A closer look at the proof of Thm. 2 reveals that it suffices to assume that A\u2019s regret bound holds against any loss sequence generated by a 1-memory-bounded adaptive adversary. To see why, note that the assumption that each ft is mmemory-bounded, the assumption that \u03c4 > m, and the definition of f\u0302j in Eq. (2) together imply that each f\u0302j is 1-memory-bounded."}, {"heading": "4.2. When m is Known", "text": "We can strengthen Thm. 2 in two ways if the memory bound m is given to A\u03c4 . First, we redefine f\u0302j(z1, . . . , zj) as\n1\n\u03c4 \u2212m \u03c4\u2211 k=m+1 f(j\u22121)\u03c4+k(z \u03c4 1 , ..., z \u03c4 j\u22121, z k j ). (8)\nNote that the first m rounds in the mini-batch are omitted. This makes the sequence (f\u0302j) J j=1 a 0- memory-bounded sequence. In other words, we only needA\u2019s regret bound to hold for oblivious adversaries. In addition to relaxing the assumption on the regret of the original algorithm A, we use m to further optimize the value of \u03c4 . This reduces the linear dependence on m in our policy regret bounds, as seen in the following example. We focus, once again, on the bandit linear optimization setting. We use the algorithm of Abernethy, Hazan, and Rakhlin (Abernethy et al., 2008), whose regret bound is 16n \u221a \u03d1J log J when J > 8\u03d1 log J , for any sequence of J loss functions generated by an oblivious adversary. The constant \u03d1 is associated with a self-concordant barrier on X . In the current context, understanding the nature of this constant is unimportant, and it suffices to know that \u03d1 = O(n) when X is a closed convex set (Nesterov & Nemirovsky, 1994).\nTheorem 3 (Bandit linear optimization, known m). Let X be a convex set and let Ft be the class of functions from X t to [0, 1] that are linear in their last argument. In this setting, The policy regret of the minibatched version of Abernethy, Hazan, and Rakhlin\u2019s\nalgorithm (Abernethy et al., 2008) where the first m loss values in each mini-batch are ignored, with batch size \u03c4 = m2/3(16n \u221a \u03d1 log T )\u22122/3T 1/3, against an mmemory-bounded adaptive adversary, is upper bounded for all T > 8\u03d1 log T by\n2m1/3(16n \u221a \u03d1T log T )2/3 +O(T 1/3)."}, {"heading": "4.3. Switching Competitors", "text": "So far, we defined CT to be the simplest competitor class possible, the class of constant action sequences. We now redefine CT to include all piece-wise constant sequences with at most s switches (Auer et al., 2002). Namely, a sequence in CT is a concatenation of at most s shorter constant sequences, whose total length is T . In this case, we assume that A\u2019s regret bound holds compared to sequences with s switches and we obtain a policy regret bound that holds compared to sequences with s switches.\nTheorem 4. Repeat the assumptions of Thm. 2, except that CT is the set of action sequences with at most s switches (where s is fixed and independent of T ) and A\u2019s regret bound of R(J) holds compared to actionsequences in CJ . Then, the policy regret of A\u03c4 , compared to action-sequences in CT , against the loss sequence (ft) T t=1, is upper bounded by\n\u03c4R(T/\u03c4) + Tm/\u03c4 + (s+ 1)\u03c4.\nThe main observation required to prove this lemma is that our proof of Thm. 2 bounds the regret batch-bybatch. The s switches of the competitor\u2019s sequence may affect at most s batches. We can trivially upper bound the regret on these batches using the fact that the loss is bounded, adding s\u03c4 to the overall bound.\nIn the k-armed bandit setting, (Auer et al., 2002) defines an algorithm named EXP3.S and proves a regret bound compared to sequences with s switches. Combining the guarantee of EXP3.S with the lemma above gives the following result.\nTheorem 5 (k-armed bandit with switches). Let X = {1, . . . , k} and let Ft consist of all functions from X t to [0, 1]. The policy regret of the mini-batched version of the EXP3.S algorithm, with batch size \u03c4 = (7ks log(kT ))\u22121/3T 1/3, compared to action sequences with at most s switches, against an m-memory bounded adaptive adversary, is upper bounded by\n(m+ 1) ( 7ks log(kT ) )1/3 T 2/3 +O(T 1/3).\nIt is possible to give similar guarantees for settings such as bandit convex optimization, provided that regret guarantees under action switches are available.\nFor instance, Flaxman et al. (Flaxman et al., 2005, Section 4) talk about (but do not explicitly derive) extensions of their bandit convex optimization regret guarantees that incorporate switches."}, {"heading": "4.4. Internal Regret, Swap Regret, \u03a6-Regret", "text": "We have so far considered the standard notion of external pseudo-regret, where the player\u2019s action-sequence is compared to action sequences in a class CT , where CT is commonly chosen to be the set of constant sequences. Other standard (yet less common) ways to analyze the performance of the player use the notions of internal regret (Blum & Mansour, 2007) and swap regret (Blum & Mansour, 2007). To define these notions, let \u03a6 be a set of action transformations, namely each \u03c6 \u2208 \u03a6 is a function of the form \u03c6 : X \u2192 X . The player\u2019s \u03a6-regret is then defined as:\nmax \u03c6\u2208\u03a6\nE [ T\u2211 t=1 ft(X1,...,t)\u2212 ft(X1,...,t\u22121, \u03c6(Xt)) ] .\nIn words, we compare the player\u2019s loss to the loss that would have been attained if the player had replaced his current action according to one of the transformations in \u03a6. We recover external regret compared to constant action sequences by letting \u03a6 be the set of constant functions, that map all actions to a constant action y. Internal regret is defined by setting \u03a6 = {\u03c6y\u2192y\u2032 : y, y\u2032 \u2208 X}, where\n\u03c6y\u2192y\u2032(x) =\n{ y\u2032 if x = y\nx otherwise .\nIn other words, \u03c6y\u2192y\u2032 replaces all occurrences of action y with action y\u2032, but leaves all other actions unmodified. To define swap regret, we specialize to the k-armed bandit case, where X = {1, . . . , k}. Swap regret is defined by setting \u03a6 = {\u03c6y1,...,yk : \u2200j yj \u2208 X}, where \u03c6y1,...,yk(x) = yx. In other words, this function replaces every action with a different action.\n\u03a6-regret suffers from the same intuitive difficulty as external regret, when the adversary is adaptive. Define the policy \u03a6-regret as\nmax \u03c6\u2208\u03a6\nE [ T\u2211 t=1 ft(X1,...,t)\u2212 ft ( \u03c6(X1), . . . , \u03c6(Xt) )] .\nWe repeat our technique to prove the following.\nTheorem 6. Repeat the assumptions of Thm. 2, except that now let \u03a6 be any set of action transformations and assume that A\u2019s \u03a6-regret against any sequence of J loss functions generated by an adaptive\nadversary is upper bounded by R(J). Then, the policy \u03a6-regret of A\u03c4 against (ft)Tt=1 generated by an m-memory-bounded adaptive adversary is bounded by \u03c4R(T/\u03c4) + Tm/\u03c4 + \u03c4 .\nThe proof is omitted due to space constraints.\nBlum & Mansour (2007) presents a technique of converting any online learning algorithm with an external regret bound into an algorithm with an internal regret bound. Combining that technique with ours endows any of the online learning algorithms mentioned in this paper with a bound on internal policy regret."}, {"heading": "5. Discussion", "text": "We highlighted a problem with the standard definition of regret when facing an adaptive adversary. We defined the notion of policy regret and argued that it captures the intuitive semantics of the word \u201cregret\u201d better than the standard definition. We then went ahead to prove non-trivial upper bounds on the policy regret of various bandit algorithms.\nThe main gap in our current understanding of policy regret is the absence of lower bounds (in both the bandit and the full-information settings). In other words, we do not know how tight our upper bounds are. It is conceivable that bandit algorithms that are specifically designed to minimize policy regret will have superior bounds, but we are yet unable to show this. On a related issue, we do not know if our mini-batching technique is really necessary: perhaps one could prove a non-trivial policy regret bound for the original (unmodified) EXP3 algorithm. We leave these questions as open problems for future research."}, {"heading": "Abernethy, J., Hazan, E., and Rakhlin, A. Competing in", "text": "the dark: An efficient algorithm for bandit linear optimization. In COLT, pp. 263\u2013274, 2008."}, {"heading": "Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.", "text": "The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48\u201377, 2002.\nAwerbuch, B. and Kleinberg, R. D. Adaptive routing with end-to-end feedback: distributed learning and geometric approaches. In STOC, pp. 45\u201353, 2004."}, {"heading": "Bartlett, P. L., Dani, V., Hayes, T. P., Kakade, S., Rakhlin,", "text": "A., and Tewari, A. High-probability regret bounds for bandit online linear optimization. In COLT, pp. 335\u2013 342, 2008.\nBlum, A. and Mansour, Y. From external to internal regret. JMLR, 8:1307\u20131324, 2007.\nBorodin, A. and El-Yaniv, R. Online computation and competitive analysis. Cambridge University Press, 1998.\nCesa-Bianchi, N. and Lugosi, G. Prediction, learning, and games. Cambridge University Press, 2006.\nDani, V. and Hayes, T. P. Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary. In SODA, 2006.\nde Farias, D. P. and Megiddo, N. Combining expert advice in reactive environments. Journal of the ACM, 53(5): 762\u2013799, 2006."}, {"heading": "Dekel, O., Gilad-Bachrach, R., Shamir, Ohad, and Xiao,", "text": "Lin. Optimal distributed online prediction. In ICML, 2011."}, {"heading": "Even-Dar, E., Kakade, S. M., and Mansour, Y. Online", "text": "Markov decision processes. Math. of Operations Research, 34(3):726\u2013736, 2009."}, {"heading": "Flaxman, A. D., Kalai, A. Tauman, and McMahan, B. H.", "text": "Online convex optimization in the bandit setting: gradient descent without a gradient. In SODA, pp. 385\u2013394, 2005.\nHazan, E. and Kale, S. Online submodular minimization. In Advances in Neural Information Processing Systems (NIPS), 2009.\nHazan, E., Kalai, A., Kale, S., and Agarwal, A. Logarithmic regret algorithms for online convex optimization. In COLT, 2006.\nKleinberg, R. Nearly tight bounds for the continuumarmed bandit problem. In NIPS, pp. 697\u2013704, 2004."}, {"heading": "Maillard, O. and Munos, R. Adaptive bandits: Towards", "text": "the best history-dependent strategy. In AISTATS, 2010.\nMcMahan, H. B. and Blum, A. Online geometric optimization in the bandit setting against an adaptive adversary. In COLT, 2004."}, {"heading": "Merhav, N., Ordentlich, E., Seroussi, G., and Weinberger,", "text": "M.J. Sequential strategies for loss functions with memory. IEEE IT, 48(7):1947\u20131958, 2002.\nNesterov, Y. E. and Nemirovsky, A. S. Interior point polynomial algorithms in convex programming. SIAM, 1994."}, {"heading": "Neu, G., Gyo\u0308rgy, A., Szepesva\u0301ri, C., and Antos, A. Online", "text": "Markov decision processes under bandit feedback. In NIPS, pp. 1804\u20131812, 2010.\nRobbins, H. Some aspects of the sequential design of experiments. Bulletin of the AMS, 58:527\u2013535, 1952.\nRyabko, D. and Hutter, M. On the possibility of learning in reactive environments with arbitrary dependence. Theor. Comput. Sci., 405(3):274\u2013284, 2008.\nSzepesva\u0301ri, C. Algorithms for Reinforcement Learning. Synth. Lectures in A.I. and Machine Learning. Morgan & Claypool Publishers, 2010."}, {"heading": "Yu, J. Y., Mannor, S., and Shimkin, N. Markov decision", "text": "processes with arbitrary reward processes. Math. of Operations Research, 34(3):737\u2013757, 2009.\nZinkevich, M. Online convex programming and generalized infinitesimal gradient ascent. In ICML, 2003."}], "references": [{"title": "Competing in the dark: An efficient algorithm for bandit linear optimization", "author": ["J. Abernethy", "E. Hazan", "A. Rakhlin"], "venue": "In COLT, pp", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: distributed learning and geometric approaches", "author": ["B. Awerbuch", "R.D. Kleinberg"], "venue": "In STOC, pp", "citeRegEx": "Awerbuch and Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg", "year": 2004}, {"title": "High-probability regret bounds for bandit online linear optimization", "author": ["P.L. Bartlett", "V. Dani", "T.P. Hayes", "S. Kakade", "A. Rakhlin", "A. Tewari"], "venue": "In COLT,", "citeRegEx": "Bartlett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2008}, {"title": "Online computation and competitive analysis", "author": ["A. Borodin", "R. El-Yaniv"], "venue": null, "citeRegEx": "Borodin and El.Yaniv,? \\Q1998\\E", "shortCiteRegEx": "Borodin and El.Yaniv", "year": 1998}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Robbing the bandit: Less regret in online geometric optimization against an adaptive adversary", "author": ["V. Dani", "T.P. Hayes"], "venue": "In SODA,", "citeRegEx": "Dani and Hayes,? \\Q2006\\E", "shortCiteRegEx": "Dani and Hayes", "year": 2006}, {"title": "Combining expert advice in reactive environments", "author": ["D.P. de Farias", "N. Megiddo"], "venue": "Journal of the ACM,", "citeRegEx": "Farias and Megiddo,? \\Q2006\\E", "shortCiteRegEx": "Farias and Megiddo", "year": 2006}, {"title": "Optimal distributed online prediction", "author": ["O. Dekel", "R. Gilad-Bachrach", "Shamir", "Ohad", "Xiao", "Lin"], "venue": "In ICML,", "citeRegEx": "Dekel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2011}, {"title": "Online convex optimization in the bandit setting: gradient descent without a gradient", "author": ["A.D. Flaxman", "Kalai", "A. Tauman", "B.H. McMahan"], "venue": "In SODA, pp", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "Online submodular minimization", "author": ["E. Hazan", "S. Kale"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hazan and Kale,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Kale", "year": 2009}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "Nearly tight bounds for the continuumarmed bandit problem", "author": ["R. Kleinberg"], "venue": "In NIPS, pp", "citeRegEx": "Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Kleinberg", "year": 2004}, {"title": "Adaptive bandits: Towards the best history-dependent strategy", "author": ["O. Maillard", "R. Munos"], "venue": "In AISTATS,", "citeRegEx": "Maillard and Munos,? \\Q2010\\E", "shortCiteRegEx": "Maillard and Munos", "year": 2010}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H.B. McMahan", "A. Blum"], "venue": "In COLT,", "citeRegEx": "McMahan and Blum,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum", "year": 2004}, {"title": "Sequential strategies for loss functions with memory", "author": ["N. Merhav", "E. Ordentlich", "G. Seroussi", "M.J. Weinberger"], "venue": "IEEE IT,", "citeRegEx": "Merhav et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Merhav et al\\.", "year": 2002}, {"title": "Interior point polynomial algorithms in convex programming", "author": ["Y.E. Nesterov", "A.S. Nemirovsky"], "venue": null, "citeRegEx": "Nesterov and Nemirovsky,? \\Q1994\\E", "shortCiteRegEx": "Nesterov and Nemirovsky", "year": 1994}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "A. Gy\u00f6rgy", "C. Szepesv\u00e1ri", "A. Antos"], "venue": "In NIPS, pp", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the AMS,", "citeRegEx": "Robbins,? \\Q1952\\E", "shortCiteRegEx": "Robbins", "year": 1952}, {"title": "On the possibility of learning in reactive environments with arbitrary dependence", "author": ["D. Ryabko", "M. Hutter"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "Ryabko and Hutter,? \\Q2008\\E", "shortCiteRegEx": "Ryabko and Hutter", "year": 2008}, {"title": "Algorithms for Reinforcement Learning. Synth", "author": ["C. Szepesv\u00e1ri"], "venue": "Lectures in A.I. and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "Szepesv\u00e1ri,? \\Q2010\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2010}, {"title": "Markov decision processes with arbitrary reward processes", "author": ["J.Y. Yu", "S. Mannor", "N. Shimkin"], "venue": "Math. of Operations Research,", "citeRegEx": "Yu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2009}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Interesting special cases of online learning with bandit feedback are the k-armed bandit (Robbins, 1952; Auer et al., 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al.", "startOffset": 89, "endOffset": 123}, {"referenceID": 1, "context": "Interesting special cases of online learning with bandit feedback are the k-armed bandit (Robbins, 1952; Auer et al., 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al.", "startOffset": 89, "endOffset": 123}, {"referenceID": 12, "context": ", 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009).", "startOffset": 36, "endOffset": 99}, {"referenceID": 9, "context": ", 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009).", "startOffset": 36, "endOffset": 99}, {"referenceID": 0, "context": ", 2002), bandit convex optimization (Kleinberg, 2004; Flaxman et al., 2005; Abernethy et al., 2008), and bandit submodular minimization (Hazan & Kale, 2009).", "startOffset": 36, "endOffset": 99}, {"referenceID": 3, "context": "A special case of bandit convex optimization is bandit linear optimization (Awerbuch & Kleinberg, 2004; Bartlett et al., 2008), where the functions in Ft are linear in their last argument.", "startOffset": 75, "endOffset": 126}, {"referenceID": 1, "context": "In practice, the most common way to evaluate the player\u2019s performance is to measure his external pseudo-regret compared to CT (Auer et al., 2002) (which we abbreviate as regret), defined as", "startOffset": 126, "endOffset": 145}, {"referenceID": 1, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 12, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 9, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 3, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 0, "context": ", (Auer et al., 2002; Awerbuch & Kleinberg, 2004; Kleinberg, 2004; Flaxman et al., 2005; Bartlett et al., 2008; Abernethy et al., 2008; Hazan & Kale, 2009)) and in the full information setting (e.", "startOffset": 2, "endOffset": 155}, {"referenceID": 22, "context": ", (Zinkevich, 2003; CesaBianchi & Lugosi, 2006; Hazan et al., 2006; Blum & Mansour, 2007; Hazan & Kale, 2009)).", "startOffset": 2, "endOffset": 109}, {"referenceID": 11, "context": ", (Zinkevich, 2003; CesaBianchi & Lugosi, 2006; Hazan et al., 2006; Blum & Mansour, 2007; Hazan & Kale, 2009)).", "startOffset": 2, "endOffset": 109}, {"referenceID": 15, "context": "The problem described above seems to be largely overlooked in the online learning literature, with the exception of two important yet isolated papers (Merhav et al., 2002; de Farias & Megiddo, 2006).", "startOffset": 150, "endOffset": 198}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries.", "startOffset": 23, "endOffset": 44}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways.", "startOffset": 23, "endOffset": 345}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition.", "startOffset": 23, "endOffset": 390}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al.", "startOffset": 23, "endOffset": 702}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al. (2002) present one concrete algorithm with a policy regret bound, we show a general technique that endows any existing bandit algorithm with a policy regret bound.", "startOffset": 23, "endOffset": 730}, {"referenceID": 15, "context": "The pioneering work of Merhav et al. (2002) addresses the problem discussed above in the experts setting (the full-information version of the k-armed bandit problem) and presents a concrete full-information algorithm with a policy regret of O(T ) against memorybounded adaptive adversaries. Our work extends and improves on Merhav et al. (2002) in various ways. First, Merhav et al. (2002) are not explicit about the shortcomings of the standard regret definition. Second, note that a bandit algorithm can always be run in the full-information setting (by ignoring the extra feedback) so all of our results also apply to the fullinformation setting and can be compared to those of Merhav et al. (2002). While Merhav et al. (2002) present one concrete algorithm with a policy regret bound, we show a general technique that endows any existing bandit algorithm with a policy regret bound. Despite the wider scope of our result, our proofs are simpler and shorter than those in Merhav et al. (2002) and our bound is just as good.", "startOffset": 23, "endOffset": 996}, {"referenceID": 21, "context": "There are recent extensions (Yu et al., 2009; Neu et al., 2010) to the partial feedback or bandit setting but they either give asymptotic rates or make even more stringent assumptions on the underlying state transition dynamics.", "startOffset": 28, "endOffset": 63}, {"referenceID": 17, "context": "There are recent extensions (Yu et al., 2009; Neu et al., 2010) to the partial feedback or bandit setting but they either give asymptotic rates or make even more stringent assumptions on the underlying state transition dynamics.", "startOffset": 28, "endOffset": 63}, {"referenceID": 19, "context": "Specifically, the PAC-MDP framework (Szepesv\u00e1ri, 2010, section 2.4.2) models the player\u2019s state on each round; typically, there is a finite number S of states and the player\u2019s actions both incur a loss and cause him to transition from one state to another. The PAC-MDP bounds typically hold when the comparison is with all k policies (mappings from states to actions), not just the k constant-action policies. Our work is still substantially different from RL. The state transitions in RL are often assumed to be stochastic, whereas our setting is adversarial. An adversarial variant of the MDP setting was studied in Even-Dar et al. (2009), however, it assumes that all loss functions across all states are observed by the player.", "startOffset": 37, "endOffset": 641}, {"referenceID": 1, "context": "For example, the popular EXP3 algorithm (Auer et al., 2002) for the k-armed bandit problem maintains a distribution (p1,t, .", "startOffset": 40, "endOffset": 59}, {"referenceID": 8, "context": ", Dekel et al. (2011)).", "startOffset": 2, "endOffset": 22}, {"referenceID": 1, "context": "We begin with the EXP3 algorithm (Auer et al., 2002) in the classic k-armed bandit setting, with its regret bound of \u221a 7Jk log k against any sequence of J loss functions generated by an adaptive adversary.", "startOffset": 33, "endOffset": 52}, {"referenceID": 1, "context": "The policy regret of the mini-batched version of the EXP3 algorithm (Auer et al., 2002), with batch size \u03c4 = (7k log k)\u22121/3T , against an m-memory bounded adaptive adversary, is upper bounded by", "startOffset": 68, "endOffset": 87}, {"referenceID": 9, "context": "The policy regret of the mini-batched version of Flaxman, Kalai, and McMahan\u2019s algorithm (Flaxman et al., 2005), with batch size \u03c4 = (18d( \u221a LD + 1))\u22124/5T , against an m-memory bounded adaptive adversary is upper bounded by", "startOffset": 89, "endOffset": 111}, {"referenceID": 9, "context": "The algorithm and analysis in Flaxman et al. (2005) guarantees a regret bound of 18d( \u221a LD+1)J against any sequence of J loss functions generated by an adaptive adversary, where d is the dimension, D is the diameter of X , and L is the Lipschitz coefficient of the loss functions.", "startOffset": 30, "endOffset": 52}, {"referenceID": 0, "context": "We use the algorithm of Abernethy, Hazan, and Rakhlin (Abernethy et al., 2008), whose regret bound is 16n \u221a \u03b8J log J when J > 8\u03b8 log J , for any sequence of J loss functions generated by an oblivious adversary.", "startOffset": 54, "endOffset": 78}, {"referenceID": 0, "context": "algorithm (Abernethy et al., 2008) where the first m loss values in each mini-batch are ignored, with batch size \u03c4 = m(16n \u221a \u03b8 log T )\u22122/3T , against an mmemory-bounded adaptive adversary, is upper bounded for all T > 8\u03b8 log T by", "startOffset": 10, "endOffset": 34}, {"referenceID": 1, "context": "We now redefine CT to include all piece-wise constant sequences with at most s switches (Auer et al., 2002).", "startOffset": 88, "endOffset": 107}, {"referenceID": 1, "context": "In the k-armed bandit setting, (Auer et al., 2002) defines an algorithm named EXP3.", "startOffset": 31, "endOffset": 50}], "year": 2012, "abstractText": "Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm\u2019s ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm\u2019s actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm\u2019s performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary\u2019s memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret.", "creator": "TeX"}}}